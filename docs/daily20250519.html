<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250518.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language\n  Navigation", "author": "Zihan Wang and Seungjun Lee and Gim Hee Lee", "abstract": "  Vision-and-Language Navigation (VLN) is a core task where embodied agents\nleverage their spatial mobility to navigate in 3D environments toward\ndesignated destinations based on natural language instructions. Recently,\nvideo-language large models (Video-VLMs) with strong generalization\ncapabilities and rich commonsense knowledge have shown remarkable performance\nwhen applied to VLN tasks. However, these models still encounter the following\nchallenges when applied to real-world 3D navigation: 1) Insufficient\nunderstanding of 3D geometry and spatial semantics; 2) Limited capacity for\nlarge-scale exploration and long-term environmental memory; 3) Poor\nadaptability to dynamic and changing environments.To address these limitations,\nwe propose Dynam3D, a dynamic layered 3D representation model that leverages\nlanguage-aligned, generalizable, and hierarchical 3D representations as visual\ninput to train 3D-VLM in navigation action prediction. Given posed RGB-D\nimages, our Dynam3D projects 2D CLIP features into 3D space and constructs\nmulti-level 3D patch-instance-zone representations for 3D geometric and\nsemantic understanding with a dynamic and layer-wise update strategy. Our\nDynam3D is capable of online encoding and localization of 3D instances, and\ndynamically updates them in changing environments to provide large-scale\nexploration and long-term memory capabilities for navigation. By leveraging\nlarge-scale 3D-language pretraining and task-specific adaptation, our Dynam3D\nsets new state-of-the-art performance on VLN benchmarks including R2R-CE,\nREVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for\npre-exploration, lifelong memory, and real-world robot validate the\neffectiveness of practical deployment.\n", "link": "http://arxiv.org/abs/2505.11383v1", "date": "2025-05-16", "relevancy": 3.1955, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynam3D%3A%20Dynamic%20Layered%203D%20Tokens%20Empower%20VLM%20for%20Vision-and-Language%0A%20%20Navigation&body=Title%3A%20Dynam3D%3A%20Dynamic%20Layered%203D%20Tokens%20Empower%20VLM%20for%20Vision-and-Language%0A%20%20Navigation%0AAuthor%3A%20Zihan%20Wang%20and%20Seungjun%20Lee%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%20is%20a%20core%20task%20where%20embodied%20agents%0Aleverage%20their%20spatial%20mobility%20to%20navigate%20in%203D%20environments%20toward%0Adesignated%20destinations%20based%20on%20natural%20language%20instructions.%20Recently%2C%0Avideo-language%20large%20models%20%28Video-VLMs%29%20with%20strong%20generalization%0Acapabilities%20and%20rich%20commonsense%20knowledge%20have%20shown%20remarkable%20performance%0Awhen%20applied%20to%20VLN%20tasks.%20However%2C%20these%20models%20still%20encounter%20the%20following%0Achallenges%20when%20applied%20to%20real-world%203D%20navigation%3A%201%29%20Insufficient%0Aunderstanding%20of%203D%20geometry%20and%20spatial%20semantics%3B%202%29%20Limited%20capacity%20for%0Alarge-scale%20exploration%20and%20long-term%20environmental%20memory%3B%203%29%20Poor%0Aadaptability%20to%20dynamic%20and%20changing%20environments.To%20address%20these%20limitations%2C%0Awe%20propose%20Dynam3D%2C%20a%20dynamic%20layered%203D%20representation%20model%20that%20leverages%0Alanguage-aligned%2C%20generalizable%2C%20and%20hierarchical%203D%20representations%20as%20visual%0Ainput%20to%20train%203D-VLM%20in%20navigation%20action%20prediction.%20Given%20posed%20RGB-D%0Aimages%2C%20our%20Dynam3D%20projects%202D%20CLIP%20features%20into%203D%20space%20and%20constructs%0Amulti-level%203D%20patch-instance-zone%20representations%20for%203D%20geometric%20and%0Asemantic%20understanding%20with%20a%20dynamic%20and%20layer-wise%20update%20strategy.%20Our%0ADynam3D%20is%20capable%20of%20online%20encoding%20and%20localization%20of%203D%20instances%2C%20and%0Adynamically%20updates%20them%20in%20changing%20environments%20to%20provide%20large-scale%0Aexploration%20and%20long-term%20memory%20capabilities%20for%20navigation.%20By%20leveraging%0Alarge-scale%203D-language%20pretraining%20and%20task-specific%20adaptation%2C%20our%20Dynam3D%0Asets%20new%20state-of-the-art%20performance%20on%20VLN%20benchmarks%20including%20R2R-CE%2C%0AREVERIE-CE%20and%20NavRAG-CE%20under%20monocular%20settings.%20Furthermore%2C%20experiments%20for%0Apre-exploration%2C%20lifelong%20memory%2C%20and%20real-world%20robot%20validate%20the%0Aeffectiveness%20of%20practical%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynam3D%253A%2520Dynamic%2520Layered%25203D%2520Tokens%2520Empower%2520VLM%2520for%2520Vision-and-Language%250A%2520%2520Navigation%26entry.906535625%3DZihan%2520Wang%2520and%2520Seungjun%2520Lee%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520is%2520a%2520core%2520task%2520where%2520embodied%2520agents%250Aleverage%2520their%2520spatial%2520mobility%2520to%2520navigate%2520in%25203D%2520environments%2520toward%250Adesignated%2520destinations%2520based%2520on%2520natural%2520language%2520instructions.%2520Recently%252C%250Avideo-language%2520large%2520models%2520%2528Video-VLMs%2529%2520with%2520strong%2520generalization%250Acapabilities%2520and%2520rich%2520commonsense%2520knowledge%2520have%2520shown%2520remarkable%2520performance%250Awhen%2520applied%2520to%2520VLN%2520tasks.%2520However%252C%2520these%2520models%2520still%2520encounter%2520the%2520following%250Achallenges%2520when%2520applied%2520to%2520real-world%25203D%2520navigation%253A%25201%2529%2520Insufficient%250Aunderstanding%2520of%25203D%2520geometry%2520and%2520spatial%2520semantics%253B%25202%2529%2520Limited%2520capacity%2520for%250Alarge-scale%2520exploration%2520and%2520long-term%2520environmental%2520memory%253B%25203%2529%2520Poor%250Aadaptability%2520to%2520dynamic%2520and%2520changing%2520environments.To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520Dynam3D%252C%2520a%2520dynamic%2520layered%25203D%2520representation%2520model%2520that%2520leverages%250Alanguage-aligned%252C%2520generalizable%252C%2520and%2520hierarchical%25203D%2520representations%2520as%2520visual%250Ainput%2520to%2520train%25203D-VLM%2520in%2520navigation%2520action%2520prediction.%2520Given%2520posed%2520RGB-D%250Aimages%252C%2520our%2520Dynam3D%2520projects%25202D%2520CLIP%2520features%2520into%25203D%2520space%2520and%2520constructs%250Amulti-level%25203D%2520patch-instance-zone%2520representations%2520for%25203D%2520geometric%2520and%250Asemantic%2520understanding%2520with%2520a%2520dynamic%2520and%2520layer-wise%2520update%2520strategy.%2520Our%250ADynam3D%2520is%2520capable%2520of%2520online%2520encoding%2520and%2520localization%2520of%25203D%2520instances%252C%2520and%250Adynamically%2520updates%2520them%2520in%2520changing%2520environments%2520to%2520provide%2520large-scale%250Aexploration%2520and%2520long-term%2520memory%2520capabilities%2520for%2520navigation.%2520By%2520leveraging%250Alarge-scale%25203D-language%2520pretraining%2520and%2520task-specific%2520adaptation%252C%2520our%2520Dynam3D%250Asets%2520new%2520state-of-the-art%2520performance%2520on%2520VLN%2520benchmarks%2520including%2520R2R-CE%252C%250AREVERIE-CE%2520and%2520NavRAG-CE%2520under%2520monocular%2520settings.%2520Furthermore%252C%2520experiments%2520for%250Apre-exploration%252C%2520lifelong%2520memory%252C%2520and%2520real-world%2520robot%2520validate%2520the%250Aeffectiveness%2520of%2520practical%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynam3D%3A%20Dynamic%20Layered%203D%20Tokens%20Empower%20VLM%20for%20Vision-and-Language%0A%20%20Navigation&entry.906535625=Zihan%20Wang%20and%20Seungjun%20Lee%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%20is%20a%20core%20task%20where%20embodied%20agents%0Aleverage%20their%20spatial%20mobility%20to%20navigate%20in%203D%20environments%20toward%0Adesignated%20destinations%20based%20on%20natural%20language%20instructions.%20Recently%2C%0Avideo-language%20large%20models%20%28Video-VLMs%29%20with%20strong%20generalization%0Acapabilities%20and%20rich%20commonsense%20knowledge%20have%20shown%20remarkable%20performance%0Awhen%20applied%20to%20VLN%20tasks.%20However%2C%20these%20models%20still%20encounter%20the%20following%0Achallenges%20when%20applied%20to%20real-world%203D%20navigation%3A%201%29%20Insufficient%0Aunderstanding%20of%203D%20geometry%20and%20spatial%20semantics%3B%202%29%20Limited%20capacity%20for%0Alarge-scale%20exploration%20and%20long-term%20environmental%20memory%3B%203%29%20Poor%0Aadaptability%20to%20dynamic%20and%20changing%20environments.To%20address%20these%20limitations%2C%0Awe%20propose%20Dynam3D%2C%20a%20dynamic%20layered%203D%20representation%20model%20that%20leverages%0Alanguage-aligned%2C%20generalizable%2C%20and%20hierarchical%203D%20representations%20as%20visual%0Ainput%20to%20train%203D-VLM%20in%20navigation%20action%20prediction.%20Given%20posed%20RGB-D%0Aimages%2C%20our%20Dynam3D%20projects%202D%20CLIP%20features%20into%203D%20space%20and%20constructs%0Amulti-level%203D%20patch-instance-zone%20representations%20for%203D%20geometric%20and%0Asemantic%20understanding%20with%20a%20dynamic%20and%20layer-wise%20update%20strategy.%20Our%0ADynam3D%20is%20capable%20of%20online%20encoding%20and%20localization%20of%203D%20instances%2C%20and%0Adynamically%20updates%20them%20in%20changing%20environments%20to%20provide%20large-scale%0Aexploration%20and%20long-term%20memory%20capabilities%20for%20navigation.%20By%20leveraging%0Alarge-scale%203D-language%20pretraining%20and%20task-specific%20adaptation%2C%20our%20Dynam3D%0Asets%20new%20state-of-the-art%20performance%20on%20VLN%20benchmarks%20including%20R2R-CE%2C%0AREVERIE-CE%20and%20NavRAG-CE%20under%20monocular%20settings.%20Furthermore%2C%20experiments%20for%0Apre-exploration%2C%20lifelong%20memory%2C%20and%20real-world%20robot%20validate%20the%0Aeffectiveness%20of%20practical%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11383v1&entry.124074799=Read"},
{"title": "Disentangling CLIP for Multi-Object Perception", "author": "Samyak Rawlekar and Yujun Cai and Yiwei Wang and Ming-Hsuan Yang and Narendra Ahuja", "abstract": "  Vision-language models like CLIP excel at recognizing the single, prominent\nobject in a scene. However, they struggle in complex scenes containing multiple\nobjects. We identify a fundamental reason behind this limitation: VLMs features\nspace exhibits significant semantic entanglement, where features of one class\ncontain substantial information about other unrelated classes, a phenomenon we\nterm mutual feature information (MFI). This entanglement becomes evident during\nclass-specific queries, as unrelated objects are activated alongside the\nqueried class. To address this limitation, we propose DCLIP, a framework that\ndisentangles CLIP features using two complementary objectives: a novel MFI Loss\nthat orthogonalizes the text (class) features to reduce inter-class similarity,\nand the Asymmetric Loss (ASL) that aligns image features with the disentangled\ntext features. Our experiment demonstrates that DCLIP reduces inter-class\nfeature similarity by 30\\% compared to CLIP, leading to significant performance\ngains on multi-label recognition (MLR) and zero-shot semantic segmentation\n(ZS3). In MLR, DCLIP outperforms SOTA approaches on VOC2007 and COCO-14 while\nusing 75\\% fewer parameters, and surpasses SOTA ZS3 methods by 3.4 mIoU on\nVOC2012 and 2.8 mIoU on COCO-17. These results establish feature\ndisentanglement as a critical factor for effective multi-object perception in\nvision-language models.\n", "link": "http://arxiv.org/abs/2502.02977v3", "date": "2025-05-16", "relevancy": 3.0711, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20CLIP%20for%20Multi-Object%20Perception&body=Title%3A%20Disentangling%20CLIP%20for%20Multi-Object%20Perception%0AAuthor%3A%20Samyak%20Rawlekar%20and%20Yujun%20Cai%20and%20Yiwei%20Wang%20and%20Ming-Hsuan%20Yang%20and%20Narendra%20Ahuja%0AAbstract%3A%20%20%20Vision-language%20models%20like%20CLIP%20excel%20at%20recognizing%20the%20single%2C%20prominent%0Aobject%20in%20a%20scene.%20However%2C%20they%20struggle%20in%20complex%20scenes%20containing%20multiple%0Aobjects.%20We%20identify%20a%20fundamental%20reason%20behind%20this%20limitation%3A%20VLMs%20features%0Aspace%20exhibits%20significant%20semantic%20entanglement%2C%20where%20features%20of%20one%20class%0Acontain%20substantial%20information%20about%20other%20unrelated%20classes%2C%20a%20phenomenon%20we%0Aterm%20mutual%20feature%20information%20%28MFI%29.%20This%20entanglement%20becomes%20evident%20during%0Aclass-specific%20queries%2C%20as%20unrelated%20objects%20are%20activated%20alongside%20the%0Aqueried%20class.%20To%20address%20this%20limitation%2C%20we%20propose%20DCLIP%2C%20a%20framework%20that%0Adisentangles%20CLIP%20features%20using%20two%20complementary%20objectives%3A%20a%20novel%20MFI%20Loss%0Athat%20orthogonalizes%20the%20text%20%28class%29%20features%20to%20reduce%20inter-class%20similarity%2C%0Aand%20the%20Asymmetric%20Loss%20%28ASL%29%20that%20aligns%20image%20features%20with%20the%20disentangled%0Atext%20features.%20Our%20experiment%20demonstrates%20that%20DCLIP%20reduces%20inter-class%0Afeature%20similarity%20by%2030%5C%25%20compared%20to%20CLIP%2C%20leading%20to%20significant%20performance%0Agains%20on%20multi-label%20recognition%20%28MLR%29%20and%20zero-shot%20semantic%20segmentation%0A%28ZS3%29.%20In%20MLR%2C%20DCLIP%20outperforms%20SOTA%20approaches%20on%20VOC2007%20and%20COCO-14%20while%0Ausing%2075%5C%25%20fewer%20parameters%2C%20and%20surpasses%20SOTA%20ZS3%20methods%20by%203.4%20mIoU%20on%0AVOC2012%20and%202.8%20mIoU%20on%20COCO-17.%20These%20results%20establish%20feature%0Adisentanglement%20as%20a%20critical%20factor%20for%20effective%20multi-object%20perception%20in%0Avision-language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02977v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520CLIP%2520for%2520Multi-Object%2520Perception%26entry.906535625%3DSamyak%2520Rawlekar%2520and%2520Yujun%2520Cai%2520and%2520Yiwei%2520Wang%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Narendra%2520Ahuja%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520like%2520CLIP%2520excel%2520at%2520recognizing%2520the%2520single%252C%2520prominent%250Aobject%2520in%2520a%2520scene.%2520However%252C%2520they%2520struggle%2520in%2520complex%2520scenes%2520containing%2520multiple%250Aobjects.%2520We%2520identify%2520a%2520fundamental%2520reason%2520behind%2520this%2520limitation%253A%2520VLMs%2520features%250Aspace%2520exhibits%2520significant%2520semantic%2520entanglement%252C%2520where%2520features%2520of%2520one%2520class%250Acontain%2520substantial%2520information%2520about%2520other%2520unrelated%2520classes%252C%2520a%2520phenomenon%2520we%250Aterm%2520mutual%2520feature%2520information%2520%2528MFI%2529.%2520This%2520entanglement%2520becomes%2520evident%2520during%250Aclass-specific%2520queries%252C%2520as%2520unrelated%2520objects%2520are%2520activated%2520alongside%2520the%250Aqueried%2520class.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520DCLIP%252C%2520a%2520framework%2520that%250Adisentangles%2520CLIP%2520features%2520using%2520two%2520complementary%2520objectives%253A%2520a%2520novel%2520MFI%2520Loss%250Athat%2520orthogonalizes%2520the%2520text%2520%2528class%2529%2520features%2520to%2520reduce%2520inter-class%2520similarity%252C%250Aand%2520the%2520Asymmetric%2520Loss%2520%2528ASL%2529%2520that%2520aligns%2520image%2520features%2520with%2520the%2520disentangled%250Atext%2520features.%2520Our%2520experiment%2520demonstrates%2520that%2520DCLIP%2520reduces%2520inter-class%250Afeature%2520similarity%2520by%252030%255C%2525%2520compared%2520to%2520CLIP%252C%2520leading%2520to%2520significant%2520performance%250Agains%2520on%2520multi-label%2520recognition%2520%2528MLR%2529%2520and%2520zero-shot%2520semantic%2520segmentation%250A%2528ZS3%2529.%2520In%2520MLR%252C%2520DCLIP%2520outperforms%2520SOTA%2520approaches%2520on%2520VOC2007%2520and%2520COCO-14%2520while%250Ausing%252075%255C%2525%2520fewer%2520parameters%252C%2520and%2520surpasses%2520SOTA%2520ZS3%2520methods%2520by%25203.4%2520mIoU%2520on%250AVOC2012%2520and%25202.8%2520mIoU%2520on%2520COCO-17.%2520These%2520results%2520establish%2520feature%250Adisentanglement%2520as%2520a%2520critical%2520factor%2520for%2520effective%2520multi-object%2520perception%2520in%250Avision-language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02977v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20CLIP%20for%20Multi-Object%20Perception&entry.906535625=Samyak%20Rawlekar%20and%20Yujun%20Cai%20and%20Yiwei%20Wang%20and%20Ming-Hsuan%20Yang%20and%20Narendra%20Ahuja&entry.1292438233=%20%20Vision-language%20models%20like%20CLIP%20excel%20at%20recognizing%20the%20single%2C%20prominent%0Aobject%20in%20a%20scene.%20However%2C%20they%20struggle%20in%20complex%20scenes%20containing%20multiple%0Aobjects.%20We%20identify%20a%20fundamental%20reason%20behind%20this%20limitation%3A%20VLMs%20features%0Aspace%20exhibits%20significant%20semantic%20entanglement%2C%20where%20features%20of%20one%20class%0Acontain%20substantial%20information%20about%20other%20unrelated%20classes%2C%20a%20phenomenon%20we%0Aterm%20mutual%20feature%20information%20%28MFI%29.%20This%20entanglement%20becomes%20evident%20during%0Aclass-specific%20queries%2C%20as%20unrelated%20objects%20are%20activated%20alongside%20the%0Aqueried%20class.%20To%20address%20this%20limitation%2C%20we%20propose%20DCLIP%2C%20a%20framework%20that%0Adisentangles%20CLIP%20features%20using%20two%20complementary%20objectives%3A%20a%20novel%20MFI%20Loss%0Athat%20orthogonalizes%20the%20text%20%28class%29%20features%20to%20reduce%20inter-class%20similarity%2C%0Aand%20the%20Asymmetric%20Loss%20%28ASL%29%20that%20aligns%20image%20features%20with%20the%20disentangled%0Atext%20features.%20Our%20experiment%20demonstrates%20that%20DCLIP%20reduces%20inter-class%0Afeature%20similarity%20by%2030%5C%25%20compared%20to%20CLIP%2C%20leading%20to%20significant%20performance%0Agains%20on%20multi-label%20recognition%20%28MLR%29%20and%20zero-shot%20semantic%20segmentation%0A%28ZS3%29.%20In%20MLR%2C%20DCLIP%20outperforms%20SOTA%20approaches%20on%20VOC2007%20and%20COCO-14%20while%0Ausing%2075%5C%25%20fewer%20parameters%2C%20and%20surpasses%20SOTA%20ZS3%20methods%20by%203.4%20mIoU%20on%0AVOC2012%20and%202.8%20mIoU%20on%20COCO-17.%20These%20results%20establish%20feature%0Adisentanglement%20as%20a%20critical%20factor%20for%20effective%20multi-object%20perception%20in%0Avision-language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02977v3&entry.124074799=Read"},
{"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "author": "Yitong Chen and Lingchen Meng and Wujian Peng and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to accommodate visual inputs with different\nresolutions, and an Alignment Loss between visual and textual features for\nbetter cross-modal alignment. After continual pre-training, leading VFMs like\nDINOv2, SigLIP and AIMv2 achieve remarkable improvements not only in multimodal\nunderstanding tasks but also in generic classification and segmentation tasks.\nRemarkably, CoMP-AIMv2 achieves scores of 64.9 on ChartQA with a 0.5B LLM,\nwhile maintaining an 87.3% accuracy on ImageNet-1K and a 51.8 mIoU on ADE20K\nunder frozen chunk evaluation.\n", "link": "http://arxiv.org/abs/2503.18931v2", "date": "2025-05-16", "relevancy": 3.0117, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoMP%3A%20Continual%20Multimodal%20Pre-training%20for%20Vision%20Foundation%20Models&body=Title%3A%20CoMP%3A%20Continual%20Multimodal%20Pre-training%20for%20Vision%20Foundation%20Models%0AAuthor%3A%20Yitong%20Chen%20and%20Lingchen%20Meng%20and%20Wujian%20Peng%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Pre-trained%20Vision%20Foundation%20Models%20%28VFMs%29%20provide%20strong%20visual%0Arepresentations%20for%20a%20wide%20range%20of%20applications.%20In%20this%20paper%2C%20we%20continually%0Apre-train%20prevailing%20VFMs%20in%20a%20multimodal%20manner%20such%20that%20they%20can%0Aeffortlessly%20process%20visual%20inputs%20of%20varying%20sizes%20and%20produce%20visual%0Arepresentations%20that%20are%20more%20aligned%20with%20language%20representations%2C%20regardless%0Aof%20their%20original%20pre-training%20process.%20To%20this%20end%2C%20we%20introduce%20CoMP%2C%20a%0Acarefully%20designed%20multimodal%20pre-training%20pipeline.%20CoMP%20uses%20a%20Continual%0ARotary%20Position%20Embedding%20to%20accommodate%20visual%20inputs%20with%20different%0Aresolutions%2C%20and%20an%20Alignment%20Loss%20between%20visual%20and%20textual%20features%20for%0Abetter%20cross-modal%20alignment.%20After%20continual%20pre-training%2C%20leading%20VFMs%20like%0ADINOv2%2C%20SigLIP%20and%20AIMv2%20achieve%20remarkable%20improvements%20not%20only%20in%20multimodal%0Aunderstanding%20tasks%20but%20also%20in%20generic%20classification%20and%20segmentation%20tasks.%0ARemarkably%2C%20CoMP-AIMv2%20achieves%20scores%20of%2064.9%20on%20ChartQA%20with%20a%200.5B%20LLM%2C%0Awhile%20maintaining%20an%2087.3%25%20accuracy%20on%20ImageNet-1K%20and%20a%2051.8%20mIoU%20on%20ADE20K%0Aunder%20frozen%20chunk%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18931v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoMP%253A%2520Continual%2520Multimodal%2520Pre-training%2520for%2520Vision%2520Foundation%2520Models%26entry.906535625%3DYitong%2520Chen%2520and%2520Lingchen%2520Meng%2520and%2520Wujian%2520Peng%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Pre-trained%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%2520provide%2520strong%2520visual%250Arepresentations%2520for%2520a%2520wide%2520range%2520of%2520applications.%2520In%2520this%2520paper%252C%2520we%2520continually%250Apre-train%2520prevailing%2520VFMs%2520in%2520a%2520multimodal%2520manner%2520such%2520that%2520they%2520can%250Aeffortlessly%2520process%2520visual%2520inputs%2520of%2520varying%2520sizes%2520and%2520produce%2520visual%250Arepresentations%2520that%2520are%2520more%2520aligned%2520with%2520language%2520representations%252C%2520regardless%250Aof%2520their%2520original%2520pre-training%2520process.%2520To%2520this%2520end%252C%2520we%2520introduce%2520CoMP%252C%2520a%250Acarefully%2520designed%2520multimodal%2520pre-training%2520pipeline.%2520CoMP%2520uses%2520a%2520Continual%250ARotary%2520Position%2520Embedding%2520to%2520accommodate%2520visual%2520inputs%2520with%2520different%250Aresolutions%252C%2520and%2520an%2520Alignment%2520Loss%2520between%2520visual%2520and%2520textual%2520features%2520for%250Abetter%2520cross-modal%2520alignment.%2520After%2520continual%2520pre-training%252C%2520leading%2520VFMs%2520like%250ADINOv2%252C%2520SigLIP%2520and%2520AIMv2%2520achieve%2520remarkable%2520improvements%2520not%2520only%2520in%2520multimodal%250Aunderstanding%2520tasks%2520but%2520also%2520in%2520generic%2520classification%2520and%2520segmentation%2520tasks.%250ARemarkably%252C%2520CoMP-AIMv2%2520achieves%2520scores%2520of%252064.9%2520on%2520ChartQA%2520with%2520a%25200.5B%2520LLM%252C%250Awhile%2520maintaining%2520an%252087.3%2525%2520accuracy%2520on%2520ImageNet-1K%2520and%2520a%252051.8%2520mIoU%2520on%2520ADE20K%250Aunder%2520frozen%2520chunk%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18931v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoMP%3A%20Continual%20Multimodal%20Pre-training%20for%20Vision%20Foundation%20Models&entry.906535625=Yitong%20Chen%20and%20Lingchen%20Meng%20and%20Wujian%20Peng%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Pre-trained%20Vision%20Foundation%20Models%20%28VFMs%29%20provide%20strong%20visual%0Arepresentations%20for%20a%20wide%20range%20of%20applications.%20In%20this%20paper%2C%20we%20continually%0Apre-train%20prevailing%20VFMs%20in%20a%20multimodal%20manner%20such%20that%20they%20can%0Aeffortlessly%20process%20visual%20inputs%20of%20varying%20sizes%20and%20produce%20visual%0Arepresentations%20that%20are%20more%20aligned%20with%20language%20representations%2C%20regardless%0Aof%20their%20original%20pre-training%20process.%20To%20this%20end%2C%20we%20introduce%20CoMP%2C%20a%0Acarefully%20designed%20multimodal%20pre-training%20pipeline.%20CoMP%20uses%20a%20Continual%0ARotary%20Position%20Embedding%20to%20accommodate%20visual%20inputs%20with%20different%0Aresolutions%2C%20and%20an%20Alignment%20Loss%20between%20visual%20and%20textual%20features%20for%0Abetter%20cross-modal%20alignment.%20After%20continual%20pre-training%2C%20leading%20VFMs%20like%0ADINOv2%2C%20SigLIP%20and%20AIMv2%20achieve%20remarkable%20improvements%20not%20only%20in%20multimodal%0Aunderstanding%20tasks%20but%20also%20in%20generic%20classification%20and%20segmentation%20tasks.%0ARemarkably%2C%20CoMP-AIMv2%20achieves%20scores%20of%2064.9%20on%20ChartQA%20with%20a%200.5B%20LLM%2C%0Awhile%20maintaining%20an%2087.3%25%20accuracy%20on%20ImageNet-1K%20and%20a%2051.8%20mIoU%20on%20ADE20K%0Aunder%20frozen%20chunk%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18931v2&entry.124074799=Read"},
{"title": "Inspiring the Next Generation of Segment Anything Models:\n  Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards\n  Context-Dependent Concepts under Different Scenes", "author": "Xiaoqi Zhao and Youwei Pang and Shijie Chang and Yuan Zhao and Lihe Zhang and Huchuan Lu and Georges El Fakhri and Xiaofeng Liu", "abstract": "  As a foundational model, SAM has significantly influenced multiple fields\nwithin computer vision, and its upgraded version, SAM 2, enhances capabilities\nin video segmentation, poised to make a substantial impact once again. While\nSAMs (SAM and SAM 2) have demonstrated excellent performance in segmenting\ncontext-independent concepts like people, cars, and roads, they overlook more\nchallenging context-dependent (CD) concepts, such as visual saliency,\ncamouflage, product defects, and medical lesions. CD concepts rely heavily on\nglobal and local contextual information, making them susceptible to shifts in\ndifferent contexts, which requires strong discriminative capabilities from the\nmodel. The lack of comprehensive evaluation of SAMs limits understanding of\ntheir performance boundaries, which may hinder the design of future models. In\nthis paper, we conduct a thorough quantitative evaluation of SAMs on 11 CD\nconcepts across 2D and 3D images and videos in various visual modalities within\nnatural, medical, and industrial scenes. We develop a unified evaluation\nframework for SAM and SAM 2 that supports manual, automatic, and intermediate\nself-prompting, aided by our specific prompt generation and interaction\nstrategies. We further explore the potential of SAM 2 for in-context learning\nand introduce prompt robustness testing to simulate real-world imperfect\nprompts. Finally, we analyze the benefits and limitations of SAMs in\nunderstanding CD concepts and discuss their future development in segmentation\ntasks. This work aims to provide valuable insights to guide future research in\nboth context-independent and context-dependent concepts segmentation,\npotentially informing the development of the next version -- SAM 3.\n", "link": "http://arxiv.org/abs/2412.01240v2", "date": "2025-05-16", "relevancy": 3.0019, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inspiring%20the%20Next%20Generation%20of%20Segment%20Anything%20Models%3A%0A%20%20Comprehensively%20Evaluate%20SAM%20and%20SAM%202%20with%20Diverse%20Prompts%20Towards%0A%20%20Context-Dependent%20Concepts%20under%20Different%20Scenes&body=Title%3A%20Inspiring%20the%20Next%20Generation%20of%20Segment%20Anything%20Models%3A%0A%20%20Comprehensively%20Evaluate%20SAM%20and%20SAM%202%20with%20Diverse%20Prompts%20Towards%0A%20%20Context-Dependent%20Concepts%20under%20Different%20Scenes%0AAuthor%3A%20Xiaoqi%20Zhao%20and%20Youwei%20Pang%20and%20Shijie%20Chang%20and%20Yuan%20Zhao%20and%20Lihe%20Zhang%20and%20Huchuan%20Lu%20and%20Georges%20El%20Fakhri%20and%20Xiaofeng%20Liu%0AAbstract%3A%20%20%20As%20a%20foundational%20model%2C%20SAM%20has%20significantly%20influenced%20multiple%20fields%0Awithin%20computer%20vision%2C%20and%20its%20upgraded%20version%2C%20SAM%202%2C%20enhances%20capabilities%0Ain%20video%20segmentation%2C%20poised%20to%20make%20a%20substantial%20impact%20once%20again.%20While%0ASAMs%20%28SAM%20and%20SAM%202%29%20have%20demonstrated%20excellent%20performance%20in%20segmenting%0Acontext-independent%20concepts%20like%20people%2C%20cars%2C%20and%20roads%2C%20they%20overlook%20more%0Achallenging%20context-dependent%20%28CD%29%20concepts%2C%20such%20as%20visual%20saliency%2C%0Acamouflage%2C%20product%20defects%2C%20and%20medical%20lesions.%20CD%20concepts%20rely%20heavily%20on%0Aglobal%20and%20local%20contextual%20information%2C%20making%20them%20susceptible%20to%20shifts%20in%0Adifferent%20contexts%2C%20which%20requires%20strong%20discriminative%20capabilities%20from%20the%0Amodel.%20The%20lack%20of%20comprehensive%20evaluation%20of%20SAMs%20limits%20understanding%20of%0Atheir%20performance%20boundaries%2C%20which%20may%20hinder%20the%20design%20of%20future%20models.%20In%0Athis%20paper%2C%20we%20conduct%20a%20thorough%20quantitative%20evaluation%20of%20SAMs%20on%2011%20CD%0Aconcepts%20across%202D%20and%203D%20images%20and%20videos%20in%20various%20visual%20modalities%20within%0Anatural%2C%20medical%2C%20and%20industrial%20scenes.%20We%20develop%20a%20unified%20evaluation%0Aframework%20for%20SAM%20and%20SAM%202%20that%20supports%20manual%2C%20automatic%2C%20and%20intermediate%0Aself-prompting%2C%20aided%20by%20our%20specific%20prompt%20generation%20and%20interaction%0Astrategies.%20We%20further%20explore%20the%20potential%20of%20SAM%202%20for%20in-context%20learning%0Aand%20introduce%20prompt%20robustness%20testing%20to%20simulate%20real-world%20imperfect%0Aprompts.%20Finally%2C%20we%20analyze%20the%20benefits%20and%20limitations%20of%20SAMs%20in%0Aunderstanding%20CD%20concepts%20and%20discuss%20their%20future%20development%20in%20segmentation%0Atasks.%20This%20work%20aims%20to%20provide%20valuable%20insights%20to%20guide%20future%20research%20in%0Aboth%20context-independent%20and%20context-dependent%20concepts%20segmentation%2C%0Apotentially%20informing%20the%20development%20of%20the%20next%20version%20--%20SAM%203.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01240v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInspiring%2520the%2520Next%2520Generation%2520of%2520Segment%2520Anything%2520Models%253A%250A%2520%2520Comprehensively%2520Evaluate%2520SAM%2520and%2520SAM%25202%2520with%2520Diverse%2520Prompts%2520Towards%250A%2520%2520Context-Dependent%2520Concepts%2520under%2520Different%2520Scenes%26entry.906535625%3DXiaoqi%2520Zhao%2520and%2520Youwei%2520Pang%2520and%2520Shijie%2520Chang%2520and%2520Yuan%2520Zhao%2520and%2520Lihe%2520Zhang%2520and%2520Huchuan%2520Lu%2520and%2520Georges%2520El%2520Fakhri%2520and%2520Xiaofeng%2520Liu%26entry.1292438233%3D%2520%2520As%2520a%2520foundational%2520model%252C%2520SAM%2520has%2520significantly%2520influenced%2520multiple%2520fields%250Awithin%2520computer%2520vision%252C%2520and%2520its%2520upgraded%2520version%252C%2520SAM%25202%252C%2520enhances%2520capabilities%250Ain%2520video%2520segmentation%252C%2520poised%2520to%2520make%2520a%2520substantial%2520impact%2520once%2520again.%2520While%250ASAMs%2520%2528SAM%2520and%2520SAM%25202%2529%2520have%2520demonstrated%2520excellent%2520performance%2520in%2520segmenting%250Acontext-independent%2520concepts%2520like%2520people%252C%2520cars%252C%2520and%2520roads%252C%2520they%2520overlook%2520more%250Achallenging%2520context-dependent%2520%2528CD%2529%2520concepts%252C%2520such%2520as%2520visual%2520saliency%252C%250Acamouflage%252C%2520product%2520defects%252C%2520and%2520medical%2520lesions.%2520CD%2520concepts%2520rely%2520heavily%2520on%250Aglobal%2520and%2520local%2520contextual%2520information%252C%2520making%2520them%2520susceptible%2520to%2520shifts%2520in%250Adifferent%2520contexts%252C%2520which%2520requires%2520strong%2520discriminative%2520capabilities%2520from%2520the%250Amodel.%2520The%2520lack%2520of%2520comprehensive%2520evaluation%2520of%2520SAMs%2520limits%2520understanding%2520of%250Atheir%2520performance%2520boundaries%252C%2520which%2520may%2520hinder%2520the%2520design%2520of%2520future%2520models.%2520In%250Athis%2520paper%252C%2520we%2520conduct%2520a%2520thorough%2520quantitative%2520evaluation%2520of%2520SAMs%2520on%252011%2520CD%250Aconcepts%2520across%25202D%2520and%25203D%2520images%2520and%2520videos%2520in%2520various%2520visual%2520modalities%2520within%250Anatural%252C%2520medical%252C%2520and%2520industrial%2520scenes.%2520We%2520develop%2520a%2520unified%2520evaluation%250Aframework%2520for%2520SAM%2520and%2520SAM%25202%2520that%2520supports%2520manual%252C%2520automatic%252C%2520and%2520intermediate%250Aself-prompting%252C%2520aided%2520by%2520our%2520specific%2520prompt%2520generation%2520and%2520interaction%250Astrategies.%2520We%2520further%2520explore%2520the%2520potential%2520of%2520SAM%25202%2520for%2520in-context%2520learning%250Aand%2520introduce%2520prompt%2520robustness%2520testing%2520to%2520simulate%2520real-world%2520imperfect%250Aprompts.%2520Finally%252C%2520we%2520analyze%2520the%2520benefits%2520and%2520limitations%2520of%2520SAMs%2520in%250Aunderstanding%2520CD%2520concepts%2520and%2520discuss%2520their%2520future%2520development%2520in%2520segmentation%250Atasks.%2520This%2520work%2520aims%2520to%2520provide%2520valuable%2520insights%2520to%2520guide%2520future%2520research%2520in%250Aboth%2520context-independent%2520and%2520context-dependent%2520concepts%2520segmentation%252C%250Apotentially%2520informing%2520the%2520development%2520of%2520the%2520next%2520version%2520--%2520SAM%25203.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01240v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inspiring%20the%20Next%20Generation%20of%20Segment%20Anything%20Models%3A%0A%20%20Comprehensively%20Evaluate%20SAM%20and%20SAM%202%20with%20Diverse%20Prompts%20Towards%0A%20%20Context-Dependent%20Concepts%20under%20Different%20Scenes&entry.906535625=Xiaoqi%20Zhao%20and%20Youwei%20Pang%20and%20Shijie%20Chang%20and%20Yuan%20Zhao%20and%20Lihe%20Zhang%20and%20Huchuan%20Lu%20and%20Georges%20El%20Fakhri%20and%20Xiaofeng%20Liu&entry.1292438233=%20%20As%20a%20foundational%20model%2C%20SAM%20has%20significantly%20influenced%20multiple%20fields%0Awithin%20computer%20vision%2C%20and%20its%20upgraded%20version%2C%20SAM%202%2C%20enhances%20capabilities%0Ain%20video%20segmentation%2C%20poised%20to%20make%20a%20substantial%20impact%20once%20again.%20While%0ASAMs%20%28SAM%20and%20SAM%202%29%20have%20demonstrated%20excellent%20performance%20in%20segmenting%0Acontext-independent%20concepts%20like%20people%2C%20cars%2C%20and%20roads%2C%20they%20overlook%20more%0Achallenging%20context-dependent%20%28CD%29%20concepts%2C%20such%20as%20visual%20saliency%2C%0Acamouflage%2C%20product%20defects%2C%20and%20medical%20lesions.%20CD%20concepts%20rely%20heavily%20on%0Aglobal%20and%20local%20contextual%20information%2C%20making%20them%20susceptible%20to%20shifts%20in%0Adifferent%20contexts%2C%20which%20requires%20strong%20discriminative%20capabilities%20from%20the%0Amodel.%20The%20lack%20of%20comprehensive%20evaluation%20of%20SAMs%20limits%20understanding%20of%0Atheir%20performance%20boundaries%2C%20which%20may%20hinder%20the%20design%20of%20future%20models.%20In%0Athis%20paper%2C%20we%20conduct%20a%20thorough%20quantitative%20evaluation%20of%20SAMs%20on%2011%20CD%0Aconcepts%20across%202D%20and%203D%20images%20and%20videos%20in%20various%20visual%20modalities%20within%0Anatural%2C%20medical%2C%20and%20industrial%20scenes.%20We%20develop%20a%20unified%20evaluation%0Aframework%20for%20SAM%20and%20SAM%202%20that%20supports%20manual%2C%20automatic%2C%20and%20intermediate%0Aself-prompting%2C%20aided%20by%20our%20specific%20prompt%20generation%20and%20interaction%0Astrategies.%20We%20further%20explore%20the%20potential%20of%20SAM%202%20for%20in-context%20learning%0Aand%20introduce%20prompt%20robustness%20testing%20to%20simulate%20real-world%20imperfect%0Aprompts.%20Finally%2C%20we%20analyze%20the%20benefits%20and%20limitations%20of%20SAMs%20in%0Aunderstanding%20CD%20concepts%20and%20discuss%20their%20future%20development%20in%20segmentation%0Atasks.%20This%20work%20aims%20to%20provide%20valuable%20insights%20to%20guide%20future%20research%20in%0Aboth%20context-independent%20and%20context-dependent%20concepts%20segmentation%2C%0Apotentially%20informing%20the%20development%20of%20the%20next%20version%20--%20SAM%203.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01240v2&entry.124074799=Read"},
{"title": "Visual Planning: Let's Think Only with Images", "author": "Yi Xu and Chengzu Li and Han Zhou and Xingchen Wan and Caiqi Zhang and Anna Korhonen and Ivan Vuli\u0107", "abstract": "  Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.\n", "link": "http://arxiv.org/abs/2505.11409v1", "date": "2025-05-16", "relevancy": 2.9757, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Planning%3A%20Let%27s%20Think%20Only%20with%20Images&body=Title%3A%20Visual%20Planning%3A%20Let%27s%20Think%20Only%20with%20Images%0AAuthor%3A%20Yi%20Xu%20and%20Chengzu%20Li%20and%20Han%20Zhou%20and%20Xingchen%20Wan%20and%20Caiqi%20Zhang%20and%20Anna%20Korhonen%20and%20Ivan%20Vuli%C4%87%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20their%20multimodal%0Aextensions%20%28MLLMs%29%20have%20substantially%20enhanced%20machine%20reasoning%20across%20diverse%0Atasks.%20However%2C%20these%20models%20predominantly%20rely%20on%20pure%20text%20as%20the%20medium%20for%0Aboth%20expressing%20and%20structuring%20reasoning%2C%20even%20when%20visual%20information%20is%0Apresent.%20In%20this%20work%2C%20we%20argue%20that%20language%20may%20not%20always%20be%20the%20most%0Anatural%20or%20effective%20modality%20for%20reasoning%2C%20particularly%20in%20tasks%20involving%0Aspatial%20and%20geometrical%20information.%20Motivated%20by%20this%2C%20we%20propose%20a%20new%0Aparadigm%2C%20Visual%20Planning%2C%20which%20enables%20planning%20through%20purely%20visual%0Arepresentations%2C%20independent%20of%20text.%20In%20this%20paradigm%2C%20planning%20is%20executed%0Avia%20sequences%20of%20images%20that%20encode%20step-by-step%20inference%20in%20the%20visual%0Adomain%2C%20akin%20to%20how%20humans%20sketch%20or%20visualize%20future%20actions.%20We%20introduce%20a%0Anovel%20reinforcement%20learning%20framework%2C%20Visual%20Planning%20via%20Reinforcement%0ALearning%20%28VPRL%29%2C%20empowered%20by%20GRPO%20for%20post-training%20large%20vision%20models%2C%0Aleading%20to%20substantial%20improvements%20in%20planning%20in%20a%20selection%20of%0Arepresentative%20visual%20navigation%20tasks%2C%20FrozenLake%2C%20Maze%2C%20and%20MiniBehavior.%20Our%0Avisual%20planning%20paradigm%20outperforms%20all%20other%20planning%20variants%20that%20conduct%0Areasoning%20in%20the%20text-only%20space.%20Our%20results%20establish%20Visual%20Planning%20as%20a%0Aviable%20and%20promising%20alternative%20to%20language-based%20reasoning%2C%20opening%20new%0Aavenues%20for%20tasks%20that%20benefit%20from%20intuitive%2C%20image-based%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Planning%253A%2520Let%2527s%2520Think%2520Only%2520with%2520Images%26entry.906535625%3DYi%2520Xu%2520and%2520Chengzu%2520Li%2520and%2520Han%2520Zhou%2520and%2520Xingchen%2520Wan%2520and%2520Caiqi%2520Zhang%2520and%2520Anna%2520Korhonen%2520and%2520Ivan%2520Vuli%25C4%2587%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520their%2520multimodal%250Aextensions%2520%2528MLLMs%2529%2520have%2520substantially%2520enhanced%2520machine%2520reasoning%2520across%2520diverse%250Atasks.%2520However%252C%2520these%2520models%2520predominantly%2520rely%2520on%2520pure%2520text%2520as%2520the%2520medium%2520for%250Aboth%2520expressing%2520and%2520structuring%2520reasoning%252C%2520even%2520when%2520visual%2520information%2520is%250Apresent.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520language%2520may%2520not%2520always%2520be%2520the%2520most%250Anatural%2520or%2520effective%2520modality%2520for%2520reasoning%252C%2520particularly%2520in%2520tasks%2520involving%250Aspatial%2520and%2520geometrical%2520information.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520new%250Aparadigm%252C%2520Visual%2520Planning%252C%2520which%2520enables%2520planning%2520through%2520purely%2520visual%250Arepresentations%252C%2520independent%2520of%2520text.%2520In%2520this%2520paradigm%252C%2520planning%2520is%2520executed%250Avia%2520sequences%2520of%2520images%2520that%2520encode%2520step-by-step%2520inference%2520in%2520the%2520visual%250Adomain%252C%2520akin%2520to%2520how%2520humans%2520sketch%2520or%2520visualize%2520future%2520actions.%2520We%2520introduce%2520a%250Anovel%2520reinforcement%2520learning%2520framework%252C%2520Visual%2520Planning%2520via%2520Reinforcement%250ALearning%2520%2528VPRL%2529%252C%2520empowered%2520by%2520GRPO%2520for%2520post-training%2520large%2520vision%2520models%252C%250Aleading%2520to%2520substantial%2520improvements%2520in%2520planning%2520in%2520a%2520selection%2520of%250Arepresentative%2520visual%2520navigation%2520tasks%252C%2520FrozenLake%252C%2520Maze%252C%2520and%2520MiniBehavior.%2520Our%250Avisual%2520planning%2520paradigm%2520outperforms%2520all%2520other%2520planning%2520variants%2520that%2520conduct%250Areasoning%2520in%2520the%2520text-only%2520space.%2520Our%2520results%2520establish%2520Visual%2520Planning%2520as%2520a%250Aviable%2520and%2520promising%2520alternative%2520to%2520language-based%2520reasoning%252C%2520opening%2520new%250Aavenues%2520for%2520tasks%2520that%2520benefit%2520from%2520intuitive%252C%2520image-based%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Planning%3A%20Let%27s%20Think%20Only%20with%20Images&entry.906535625=Yi%20Xu%20and%20Chengzu%20Li%20and%20Han%20Zhou%20and%20Xingchen%20Wan%20and%20Caiqi%20Zhang%20and%20Anna%20Korhonen%20and%20Ivan%20Vuli%C4%87&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20their%20multimodal%0Aextensions%20%28MLLMs%29%20have%20substantially%20enhanced%20machine%20reasoning%20across%20diverse%0Atasks.%20However%2C%20these%20models%20predominantly%20rely%20on%20pure%20text%20as%20the%20medium%20for%0Aboth%20expressing%20and%20structuring%20reasoning%2C%20even%20when%20visual%20information%20is%0Apresent.%20In%20this%20work%2C%20we%20argue%20that%20language%20may%20not%20always%20be%20the%20most%0Anatural%20or%20effective%20modality%20for%20reasoning%2C%20particularly%20in%20tasks%20involving%0Aspatial%20and%20geometrical%20information.%20Motivated%20by%20this%2C%20we%20propose%20a%20new%0Aparadigm%2C%20Visual%20Planning%2C%20which%20enables%20planning%20through%20purely%20visual%0Arepresentations%2C%20independent%20of%20text.%20In%20this%20paradigm%2C%20planning%20is%20executed%0Avia%20sequences%20of%20images%20that%20encode%20step-by-step%20inference%20in%20the%20visual%0Adomain%2C%20akin%20to%20how%20humans%20sketch%20or%20visualize%20future%20actions.%20We%20introduce%20a%0Anovel%20reinforcement%20learning%20framework%2C%20Visual%20Planning%20via%20Reinforcement%0ALearning%20%28VPRL%29%2C%20empowered%20by%20GRPO%20for%20post-training%20large%20vision%20models%2C%0Aleading%20to%20substantial%20improvements%20in%20planning%20in%20a%20selection%20of%0Arepresentative%20visual%20navigation%20tasks%2C%20FrozenLake%2C%20Maze%2C%20and%20MiniBehavior.%20Our%0Avisual%20planning%20paradigm%20outperforms%20all%20other%20planning%20variants%20that%20conduct%0Areasoning%20in%20the%20text-only%20space.%20Our%20results%20establish%20Visual%20Planning%20as%20a%0Aviable%20and%20promising%20alternative%20to%20language-based%20reasoning%2C%20opening%20new%0Aavenues%20for%20tasks%20that%20benefit%20from%20intuitive%2C%20image-based%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11409v1&entry.124074799=Read"},
{"title": "Multi-view dense image matching with similarity learning and geometry\n  priors", "author": "Mohamed Ali Chebbi and Ewelina Rupnik and Paul Lopes and Marc Pierrot-Deseilligny", "abstract": "  We introduce MV-DeepSimNets, a comprehensive suite of deep neural networks\ndesigned for multi-view similarity learning, leveraging epipolar geometry for\ntraining. Our approach incorporates an online geometry prior to characterize\npixel relationships, either along the epipolar line or through homography\nrectification. This enables the generation of geometry-aware features from\nnative images, which are then projected across candidate depth hypotheses using\nplane sweeping. Our method geometric preconditioning effectively adapts\nepipolar-based features for enhanced multi-view reconstruction, without\nrequiring the laborious multi-view training dataset creation. By aggregating\nlearned similarities, we construct and regularize the cost volume, leading to\nimproved multi-view surface reconstruction over traditional dense matching\napproaches. MV-DeepSimNets demonstrates superior performance against leading\nsimilarity learning networks and end-to-end regression models, especially in\nterms of generalization capabilities across both aerial and satellite imagery\nwith varied ground sampling distances. Our pipeline is integrated into MicMac\nsoftware and can be readily adopted in standard multi-resolution image matching\npipelines.\n", "link": "http://arxiv.org/abs/2505.11264v1", "date": "2025-05-16", "relevancy": 2.8755, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5957}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5651}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20dense%20image%20matching%20with%20similarity%20learning%20and%20geometry%0A%20%20priors&body=Title%3A%20Multi-view%20dense%20image%20matching%20with%20similarity%20learning%20and%20geometry%0A%20%20priors%0AAuthor%3A%20Mohamed%20Ali%20Chebbi%20and%20Ewelina%20Rupnik%20and%20Paul%20Lopes%20and%20Marc%20Pierrot-Deseilligny%0AAbstract%3A%20%20%20We%20introduce%20MV-DeepSimNets%2C%20a%20comprehensive%20suite%20of%20deep%20neural%20networks%0Adesigned%20for%20multi-view%20similarity%20learning%2C%20leveraging%20epipolar%20geometry%20for%0Atraining.%20Our%20approach%20incorporates%20an%20online%20geometry%20prior%20to%20characterize%0Apixel%20relationships%2C%20either%20along%20the%20epipolar%20line%20or%20through%20homography%0Arectification.%20This%20enables%20the%20generation%20of%20geometry-aware%20features%20from%0Anative%20images%2C%20which%20are%20then%20projected%20across%20candidate%20depth%20hypotheses%20using%0Aplane%20sweeping.%20Our%20method%20geometric%20preconditioning%20effectively%20adapts%0Aepipolar-based%20features%20for%20enhanced%20multi-view%20reconstruction%2C%20without%0Arequiring%20the%20laborious%20multi-view%20training%20dataset%20creation.%20By%20aggregating%0Alearned%20similarities%2C%20we%20construct%20and%20regularize%20the%20cost%20volume%2C%20leading%20to%0Aimproved%20multi-view%20surface%20reconstruction%20over%20traditional%20dense%20matching%0Aapproaches.%20MV-DeepSimNets%20demonstrates%20superior%20performance%20against%20leading%0Asimilarity%20learning%20networks%20and%20end-to-end%20regression%20models%2C%20especially%20in%0Aterms%20of%20generalization%20capabilities%20across%20both%20aerial%20and%20satellite%20imagery%0Awith%20varied%20ground%20sampling%20distances.%20Our%20pipeline%20is%20integrated%20into%20MicMac%0Asoftware%20and%20can%20be%20readily%20adopted%20in%20standard%20multi-resolution%20image%20matching%0Apipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520dense%2520image%2520matching%2520with%2520similarity%2520learning%2520and%2520geometry%250A%2520%2520priors%26entry.906535625%3DMohamed%2520Ali%2520Chebbi%2520and%2520Ewelina%2520Rupnik%2520and%2520Paul%2520Lopes%2520and%2520Marc%2520Pierrot-Deseilligny%26entry.1292438233%3D%2520%2520We%2520introduce%2520MV-DeepSimNets%252C%2520a%2520comprehensive%2520suite%2520of%2520deep%2520neural%2520networks%250Adesigned%2520for%2520multi-view%2520similarity%2520learning%252C%2520leveraging%2520epipolar%2520geometry%2520for%250Atraining.%2520Our%2520approach%2520incorporates%2520an%2520online%2520geometry%2520prior%2520to%2520characterize%250Apixel%2520relationships%252C%2520either%2520along%2520the%2520epipolar%2520line%2520or%2520through%2520homography%250Arectification.%2520This%2520enables%2520the%2520generation%2520of%2520geometry-aware%2520features%2520from%250Anative%2520images%252C%2520which%2520are%2520then%2520projected%2520across%2520candidate%2520depth%2520hypotheses%2520using%250Aplane%2520sweeping.%2520Our%2520method%2520geometric%2520preconditioning%2520effectively%2520adapts%250Aepipolar-based%2520features%2520for%2520enhanced%2520multi-view%2520reconstruction%252C%2520without%250Arequiring%2520the%2520laborious%2520multi-view%2520training%2520dataset%2520creation.%2520By%2520aggregating%250Alearned%2520similarities%252C%2520we%2520construct%2520and%2520regularize%2520the%2520cost%2520volume%252C%2520leading%2520to%250Aimproved%2520multi-view%2520surface%2520reconstruction%2520over%2520traditional%2520dense%2520matching%250Aapproaches.%2520MV-DeepSimNets%2520demonstrates%2520superior%2520performance%2520against%2520leading%250Asimilarity%2520learning%2520networks%2520and%2520end-to-end%2520regression%2520models%252C%2520especially%2520in%250Aterms%2520of%2520generalization%2520capabilities%2520across%2520both%2520aerial%2520and%2520satellite%2520imagery%250Awith%2520varied%2520ground%2520sampling%2520distances.%2520Our%2520pipeline%2520is%2520integrated%2520into%2520MicMac%250Asoftware%2520and%2520can%2520be%2520readily%2520adopted%2520in%2520standard%2520multi-resolution%2520image%2520matching%250Apipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20dense%20image%20matching%20with%20similarity%20learning%20and%20geometry%0A%20%20priors&entry.906535625=Mohamed%20Ali%20Chebbi%20and%20Ewelina%20Rupnik%20and%20Paul%20Lopes%20and%20Marc%20Pierrot-Deseilligny&entry.1292438233=%20%20We%20introduce%20MV-DeepSimNets%2C%20a%20comprehensive%20suite%20of%20deep%20neural%20networks%0Adesigned%20for%20multi-view%20similarity%20learning%2C%20leveraging%20epipolar%20geometry%20for%0Atraining.%20Our%20approach%20incorporates%20an%20online%20geometry%20prior%20to%20characterize%0Apixel%20relationships%2C%20either%20along%20the%20epipolar%20line%20or%20through%20homography%0Arectification.%20This%20enables%20the%20generation%20of%20geometry-aware%20features%20from%0Anative%20images%2C%20which%20are%20then%20projected%20across%20candidate%20depth%20hypotheses%20using%0Aplane%20sweeping.%20Our%20method%20geometric%20preconditioning%20effectively%20adapts%0Aepipolar-based%20features%20for%20enhanced%20multi-view%20reconstruction%2C%20without%0Arequiring%20the%20laborious%20multi-view%20training%20dataset%20creation.%20By%20aggregating%0Alearned%20similarities%2C%20we%20construct%20and%20regularize%20the%20cost%20volume%2C%20leading%20to%0Aimproved%20multi-view%20surface%20reconstruction%20over%20traditional%20dense%20matching%0Aapproaches.%20MV-DeepSimNets%20demonstrates%20superior%20performance%20against%20leading%0Asimilarity%20learning%20networks%20and%20end-to-end%20regression%20models%2C%20especially%20in%0Aterms%20of%20generalization%20capabilities%20across%20both%20aerial%20and%20satellite%20imagery%0Awith%20varied%20ground%20sampling%20distances.%20Our%20pipeline%20is%20integrated%20into%20MicMac%0Asoftware%20and%20can%20be%20readily%20adopted%20in%20standard%20multi-resolution%20image%20matching%0Apipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11264v1&entry.124074799=Read"},
{"title": "Self-Supervised Representation Learning for Nerve Fiber Distribution\n  Patterns in 3D-PLI", "author": "Alexander Oberstrass and Sascha E. A. Muenzing and Meiqi Niu and Nicola Palomero-Gallagher and Christian Schiffer and Markus Axer and Katrin Amunts and Timo Dickscheid", "abstract": "  A comprehensive understanding of the organizational principles in the human\nbrain requires, among other factors, well-quantifiable descriptors of nerve\nfiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a\nmicroscopic imaging technique that enables insights into the fine-grained\norganization of myelinated nerve fibers with high resolution. Descriptors\ncharacterizing the fiber architecture observed in 3D-PLI would enable\ndownstream analysis tasks such as multimodal correlation studies, clustering,\nand mapping. However, best practices for observer-independent characterization\nof fiber architecture in 3D-PLI are not yet available. To this end, we propose\nthe application of a fully data-driven approach to characterize nerve fiber\narchitecture in 3D-PLI images using self-supervised representation learning. We\nintroduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the\nspatial neighborhood of texture examples across histological brain sections of\na 3D reconstructed volume to sample positive pairs for contrastive learning. We\ncombine this sampling strategy with specifically designed image augmentations\nto gain robustness to typical variations in 3D-PLI parameter maps. The approach\nis demonstrated for the 3D reconstructed occipital lobe of a vervet monkey\nbrain. We show that extracted features are highly sensitive to different\nconfigurations of nerve fibers, yet robust to variations between consecutive\nbrain sections arising from histological processing. We demonstrate their\npractical applicability for retrieving clusters of homogeneous fiber\narchitecture, performing classification with minimal annotations, and\nquery-based retrieval of characteristic components of fiber architecture such\nas U-fibers.\n", "link": "http://arxiv.org/abs/2401.17207v2", "date": "2025-05-16", "relevancy": 2.8619, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6076}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Representation%20Learning%20for%20Nerve%20Fiber%20Distribution%0A%20%20Patterns%20in%203D-PLI&body=Title%3A%20Self-Supervised%20Representation%20Learning%20for%20Nerve%20Fiber%20Distribution%0A%20%20Patterns%20in%203D-PLI%0AAuthor%3A%20Alexander%20Oberstrass%20and%20Sascha%20E.%20A.%20Muenzing%20and%20Meiqi%20Niu%20and%20Nicola%20Palomero-Gallagher%20and%20Christian%20Schiffer%20and%20Markus%20Axer%20and%20Katrin%20Amunts%20and%20Timo%20Dickscheid%0AAbstract%3A%20%20%20A%20comprehensive%20understanding%20of%20the%20organizational%20principles%20in%20the%20human%0Abrain%20requires%2C%20among%20other%20factors%2C%20well-quantifiable%20descriptors%20of%20nerve%0Afiber%20architecture.%20Three-dimensional%20polarized%20light%20imaging%20%283D-PLI%29%20is%20a%0Amicroscopic%20imaging%20technique%20that%20enables%20insights%20into%20the%20fine-grained%0Aorganization%20of%20myelinated%20nerve%20fibers%20with%20high%20resolution.%20Descriptors%0Acharacterizing%20the%20fiber%20architecture%20observed%20in%203D-PLI%20would%20enable%0Adownstream%20analysis%20tasks%20such%20as%20multimodal%20correlation%20studies%2C%20clustering%2C%0Aand%20mapping.%20However%2C%20best%20practices%20for%20observer-independent%20characterization%0Aof%20fiber%20architecture%20in%203D-PLI%20are%20not%20yet%20available.%20To%20this%20end%2C%20we%20propose%0Athe%20application%20of%20a%20fully%20data-driven%20approach%20to%20characterize%20nerve%20fiber%0Aarchitecture%20in%203D-PLI%20images%20using%20self-supervised%20representation%20learning.%20We%0Aintroduce%20a%203D-Context%20Contrastive%20Learning%20%28CL-3D%29%20objective%20that%20utilizes%20the%0Aspatial%20neighborhood%20of%20texture%20examples%20across%20histological%20brain%20sections%20of%0Aa%203D%20reconstructed%20volume%20to%20sample%20positive%20pairs%20for%20contrastive%20learning.%20We%0Acombine%20this%20sampling%20strategy%20with%20specifically%20designed%20image%20augmentations%0Ato%20gain%20robustness%20to%20typical%20variations%20in%203D-PLI%20parameter%20maps.%20The%20approach%0Ais%20demonstrated%20for%20the%203D%20reconstructed%20occipital%20lobe%20of%20a%20vervet%20monkey%0Abrain.%20We%20show%20that%20extracted%20features%20are%20highly%20sensitive%20to%20different%0Aconfigurations%20of%20nerve%20fibers%2C%20yet%20robust%20to%20variations%20between%20consecutive%0Abrain%20sections%20arising%20from%20histological%20processing.%20We%20demonstrate%20their%0Apractical%20applicability%20for%20retrieving%20clusters%20of%20homogeneous%20fiber%0Aarchitecture%2C%20performing%20classification%20with%20minimal%20annotations%2C%20and%0Aquery-based%20retrieval%20of%20characteristic%20components%20of%20fiber%20architecture%20such%0Aas%20U-fibers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17207v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Representation%2520Learning%2520for%2520Nerve%2520Fiber%2520Distribution%250A%2520%2520Patterns%2520in%25203D-PLI%26entry.906535625%3DAlexander%2520Oberstrass%2520and%2520Sascha%2520E.%2520A.%2520Muenzing%2520and%2520Meiqi%2520Niu%2520and%2520Nicola%2520Palomero-Gallagher%2520and%2520Christian%2520Schiffer%2520and%2520Markus%2520Axer%2520and%2520Katrin%2520Amunts%2520and%2520Timo%2520Dickscheid%26entry.1292438233%3D%2520%2520A%2520comprehensive%2520understanding%2520of%2520the%2520organizational%2520principles%2520in%2520the%2520human%250Abrain%2520requires%252C%2520among%2520other%2520factors%252C%2520well-quantifiable%2520descriptors%2520of%2520nerve%250Afiber%2520architecture.%2520Three-dimensional%2520polarized%2520light%2520imaging%2520%25283D-PLI%2529%2520is%2520a%250Amicroscopic%2520imaging%2520technique%2520that%2520enables%2520insights%2520into%2520the%2520fine-grained%250Aorganization%2520of%2520myelinated%2520nerve%2520fibers%2520with%2520high%2520resolution.%2520Descriptors%250Acharacterizing%2520the%2520fiber%2520architecture%2520observed%2520in%25203D-PLI%2520would%2520enable%250Adownstream%2520analysis%2520tasks%2520such%2520as%2520multimodal%2520correlation%2520studies%252C%2520clustering%252C%250Aand%2520mapping.%2520However%252C%2520best%2520practices%2520for%2520observer-independent%2520characterization%250Aof%2520fiber%2520architecture%2520in%25203D-PLI%2520are%2520not%2520yet%2520available.%2520To%2520this%2520end%252C%2520we%2520propose%250Athe%2520application%2520of%2520a%2520fully%2520data-driven%2520approach%2520to%2520characterize%2520nerve%2520fiber%250Aarchitecture%2520in%25203D-PLI%2520images%2520using%2520self-supervised%2520representation%2520learning.%2520We%250Aintroduce%2520a%25203D-Context%2520Contrastive%2520Learning%2520%2528CL-3D%2529%2520objective%2520that%2520utilizes%2520the%250Aspatial%2520neighborhood%2520of%2520texture%2520examples%2520across%2520histological%2520brain%2520sections%2520of%250Aa%25203D%2520reconstructed%2520volume%2520to%2520sample%2520positive%2520pairs%2520for%2520contrastive%2520learning.%2520We%250Acombine%2520this%2520sampling%2520strategy%2520with%2520specifically%2520designed%2520image%2520augmentations%250Ato%2520gain%2520robustness%2520to%2520typical%2520variations%2520in%25203D-PLI%2520parameter%2520maps.%2520The%2520approach%250Ais%2520demonstrated%2520for%2520the%25203D%2520reconstructed%2520occipital%2520lobe%2520of%2520a%2520vervet%2520monkey%250Abrain.%2520We%2520show%2520that%2520extracted%2520features%2520are%2520highly%2520sensitive%2520to%2520different%250Aconfigurations%2520of%2520nerve%2520fibers%252C%2520yet%2520robust%2520to%2520variations%2520between%2520consecutive%250Abrain%2520sections%2520arising%2520from%2520histological%2520processing.%2520We%2520demonstrate%2520their%250Apractical%2520applicability%2520for%2520retrieving%2520clusters%2520of%2520homogeneous%2520fiber%250Aarchitecture%252C%2520performing%2520classification%2520with%2520minimal%2520annotations%252C%2520and%250Aquery-based%2520retrieval%2520of%2520characteristic%2520components%2520of%2520fiber%2520architecture%2520such%250Aas%2520U-fibers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.17207v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Representation%20Learning%20for%20Nerve%20Fiber%20Distribution%0A%20%20Patterns%20in%203D-PLI&entry.906535625=Alexander%20Oberstrass%20and%20Sascha%20E.%20A.%20Muenzing%20and%20Meiqi%20Niu%20and%20Nicola%20Palomero-Gallagher%20and%20Christian%20Schiffer%20and%20Markus%20Axer%20and%20Katrin%20Amunts%20and%20Timo%20Dickscheid&entry.1292438233=%20%20A%20comprehensive%20understanding%20of%20the%20organizational%20principles%20in%20the%20human%0Abrain%20requires%2C%20among%20other%20factors%2C%20well-quantifiable%20descriptors%20of%20nerve%0Afiber%20architecture.%20Three-dimensional%20polarized%20light%20imaging%20%283D-PLI%29%20is%20a%0Amicroscopic%20imaging%20technique%20that%20enables%20insights%20into%20the%20fine-grained%0Aorganization%20of%20myelinated%20nerve%20fibers%20with%20high%20resolution.%20Descriptors%0Acharacterizing%20the%20fiber%20architecture%20observed%20in%203D-PLI%20would%20enable%0Adownstream%20analysis%20tasks%20such%20as%20multimodal%20correlation%20studies%2C%20clustering%2C%0Aand%20mapping.%20However%2C%20best%20practices%20for%20observer-independent%20characterization%0Aof%20fiber%20architecture%20in%203D-PLI%20are%20not%20yet%20available.%20To%20this%20end%2C%20we%20propose%0Athe%20application%20of%20a%20fully%20data-driven%20approach%20to%20characterize%20nerve%20fiber%0Aarchitecture%20in%203D-PLI%20images%20using%20self-supervised%20representation%20learning.%20We%0Aintroduce%20a%203D-Context%20Contrastive%20Learning%20%28CL-3D%29%20objective%20that%20utilizes%20the%0Aspatial%20neighborhood%20of%20texture%20examples%20across%20histological%20brain%20sections%20of%0Aa%203D%20reconstructed%20volume%20to%20sample%20positive%20pairs%20for%20contrastive%20learning.%20We%0Acombine%20this%20sampling%20strategy%20with%20specifically%20designed%20image%20augmentations%0Ato%20gain%20robustness%20to%20typical%20variations%20in%203D-PLI%20parameter%20maps.%20The%20approach%0Ais%20demonstrated%20for%20the%203D%20reconstructed%20occipital%20lobe%20of%20a%20vervet%20monkey%0Abrain.%20We%20show%20that%20extracted%20features%20are%20highly%20sensitive%20to%20different%0Aconfigurations%20of%20nerve%20fibers%2C%20yet%20robust%20to%20variations%20between%20consecutive%0Abrain%20sections%20arising%20from%20histological%20processing.%20We%20demonstrate%20their%0Apractical%20applicability%20for%20retrieving%20clusters%20of%20homogeneous%20fiber%0Aarchitecture%2C%20performing%20classification%20with%20minimal%20annotations%2C%20and%0Aquery-based%20retrieval%20of%20characteristic%20components%20of%20fiber%20architecture%20such%0Aas%20U-fibers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17207v2&entry.124074799=Read"},
{"title": "Open-Source Multi-Viewpoint Surgical Telerobotics", "author": "Guido Caccianiga and Yarden Sharon and Bernard Javot and Senya Polikovsky and G\u00f6kce Erg\u00fcn and Ivan Capobianco and Andr\u00e9 L. Mihaljevic and Anton Deguet and Katherine J. Kuchenbecker", "abstract": "  As robots for minimally invasive surgery (MIS) gradually become more\naccessible and modular, we believe there is a great opportunity to rethink and\nexpand the visualization and control paradigms that have characterized surgical\nteleoperation since its inception. We conjecture that introducing one or more\nadditional adjustable viewpoints in the abdominal cavity would not only unlock\nnovel visualization and collaboration strategies for surgeons but also\nsubstantially boost the robustness of machine perception toward shared\nautonomy. Immediate advantages include controlling a second viewpoint and\nteleoperating surgical tools from a different perspective, which would allow\ncollaborating surgeons to adjust their views independently and still maneuver\ntheir robotic instruments intuitively. Furthermore, we believe that capturing\nsynchronized multi-view 3D measurements of the patient's anatomy would unlock\nadvanced scene representations. Accurate real-time intraoperative 3D perception\nwill allow algorithmic assistants to directly control one or more robotic\ninstruments and/or robotic cameras. Toward these goals, we are building a\nsynchronized multi-viewpoint, multi-sensor robotic surgery system by\nintegrating high-performance vision components and upgrading the da Vinci\nResearch Kit control logic. This short paper reports a functional summary of\nour setup and elaborates on its potential impacts in research and future\nclinical practice. By fully open-sourcing our system, we will enable the\nresearch community to reproduce our setup, improve it, and develop powerful\nalgorithms, effectively boosting clinical translation of cutting-edge research.\n", "link": "http://arxiv.org/abs/2505.11142v1", "date": "2025-05-16", "relevancy": 2.7966, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5875}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5452}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Source%20Multi-Viewpoint%20Surgical%20Telerobotics&body=Title%3A%20Open-Source%20Multi-Viewpoint%20Surgical%20Telerobotics%0AAuthor%3A%20Guido%20Caccianiga%20and%20Yarden%20Sharon%20and%20Bernard%20Javot%20and%20Senya%20Polikovsky%20and%20G%C3%B6kce%20Erg%C3%BCn%20and%20Ivan%20Capobianco%20and%20Andr%C3%A9%20L.%20Mihaljevic%20and%20Anton%20Deguet%20and%20Katherine%20J.%20Kuchenbecker%0AAbstract%3A%20%20%20As%20robots%20for%20minimally%20invasive%20surgery%20%28MIS%29%20gradually%20become%20more%0Aaccessible%20and%20modular%2C%20we%20believe%20there%20is%20a%20great%20opportunity%20to%20rethink%20and%0Aexpand%20the%20visualization%20and%20control%20paradigms%20that%20have%20characterized%20surgical%0Ateleoperation%20since%20its%20inception.%20We%20conjecture%20that%20introducing%20one%20or%20more%0Aadditional%20adjustable%20viewpoints%20in%20the%20abdominal%20cavity%20would%20not%20only%20unlock%0Anovel%20visualization%20and%20collaboration%20strategies%20for%20surgeons%20but%20also%0Asubstantially%20boost%20the%20robustness%20of%20machine%20perception%20toward%20shared%0Aautonomy.%20Immediate%20advantages%20include%20controlling%20a%20second%20viewpoint%20and%0Ateleoperating%20surgical%20tools%20from%20a%20different%20perspective%2C%20which%20would%20allow%0Acollaborating%20surgeons%20to%20adjust%20their%20views%20independently%20and%20still%20maneuver%0Atheir%20robotic%20instruments%20intuitively.%20Furthermore%2C%20we%20believe%20that%20capturing%0Asynchronized%20multi-view%203D%20measurements%20of%20the%20patient%27s%20anatomy%20would%20unlock%0Aadvanced%20scene%20representations.%20Accurate%20real-time%20intraoperative%203D%20perception%0Awill%20allow%20algorithmic%20assistants%20to%20directly%20control%20one%20or%20more%20robotic%0Ainstruments%20and/or%20robotic%20cameras.%20Toward%20these%20goals%2C%20we%20are%20building%20a%0Asynchronized%20multi-viewpoint%2C%20multi-sensor%20robotic%20surgery%20system%20by%0Aintegrating%20high-performance%20vision%20components%20and%20upgrading%20the%20da%20Vinci%0AResearch%20Kit%20control%20logic.%20This%20short%20paper%20reports%20a%20functional%20summary%20of%0Aour%20setup%20and%20elaborates%20on%20its%20potential%20impacts%20in%20research%20and%20future%0Aclinical%20practice.%20By%20fully%20open-sourcing%20our%20system%2C%20we%20will%20enable%20the%0Aresearch%20community%20to%20reproduce%20our%20setup%2C%20improve%20it%2C%20and%20develop%20powerful%0Aalgorithms%2C%20effectively%20boosting%20clinical%20translation%20of%20cutting-edge%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Source%2520Multi-Viewpoint%2520Surgical%2520Telerobotics%26entry.906535625%3DGuido%2520Caccianiga%2520and%2520Yarden%2520Sharon%2520and%2520Bernard%2520Javot%2520and%2520Senya%2520Polikovsky%2520and%2520G%25C3%25B6kce%2520Erg%25C3%25BCn%2520and%2520Ivan%2520Capobianco%2520and%2520Andr%25C3%25A9%2520L.%2520Mihaljevic%2520and%2520Anton%2520Deguet%2520and%2520Katherine%2520J.%2520Kuchenbecker%26entry.1292438233%3D%2520%2520As%2520robots%2520for%2520minimally%2520invasive%2520surgery%2520%2528MIS%2529%2520gradually%2520become%2520more%250Aaccessible%2520and%2520modular%252C%2520we%2520believe%2520there%2520is%2520a%2520great%2520opportunity%2520to%2520rethink%2520and%250Aexpand%2520the%2520visualization%2520and%2520control%2520paradigms%2520that%2520have%2520characterized%2520surgical%250Ateleoperation%2520since%2520its%2520inception.%2520We%2520conjecture%2520that%2520introducing%2520one%2520or%2520more%250Aadditional%2520adjustable%2520viewpoints%2520in%2520the%2520abdominal%2520cavity%2520would%2520not%2520only%2520unlock%250Anovel%2520visualization%2520and%2520collaboration%2520strategies%2520for%2520surgeons%2520but%2520also%250Asubstantially%2520boost%2520the%2520robustness%2520of%2520machine%2520perception%2520toward%2520shared%250Aautonomy.%2520Immediate%2520advantages%2520include%2520controlling%2520a%2520second%2520viewpoint%2520and%250Ateleoperating%2520surgical%2520tools%2520from%2520a%2520different%2520perspective%252C%2520which%2520would%2520allow%250Acollaborating%2520surgeons%2520to%2520adjust%2520their%2520views%2520independently%2520and%2520still%2520maneuver%250Atheir%2520robotic%2520instruments%2520intuitively.%2520Furthermore%252C%2520we%2520believe%2520that%2520capturing%250Asynchronized%2520multi-view%25203D%2520measurements%2520of%2520the%2520patient%2527s%2520anatomy%2520would%2520unlock%250Aadvanced%2520scene%2520representations.%2520Accurate%2520real-time%2520intraoperative%25203D%2520perception%250Awill%2520allow%2520algorithmic%2520assistants%2520to%2520directly%2520control%2520one%2520or%2520more%2520robotic%250Ainstruments%2520and/or%2520robotic%2520cameras.%2520Toward%2520these%2520goals%252C%2520we%2520are%2520building%2520a%250Asynchronized%2520multi-viewpoint%252C%2520multi-sensor%2520robotic%2520surgery%2520system%2520by%250Aintegrating%2520high-performance%2520vision%2520components%2520and%2520upgrading%2520the%2520da%2520Vinci%250AResearch%2520Kit%2520control%2520logic.%2520This%2520short%2520paper%2520reports%2520a%2520functional%2520summary%2520of%250Aour%2520setup%2520and%2520elaborates%2520on%2520its%2520potential%2520impacts%2520in%2520research%2520and%2520future%250Aclinical%2520practice.%2520By%2520fully%2520open-sourcing%2520our%2520system%252C%2520we%2520will%2520enable%2520the%250Aresearch%2520community%2520to%2520reproduce%2520our%2520setup%252C%2520improve%2520it%252C%2520and%2520develop%2520powerful%250Aalgorithms%252C%2520effectively%2520boosting%2520clinical%2520translation%2520of%2520cutting-edge%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Source%20Multi-Viewpoint%20Surgical%20Telerobotics&entry.906535625=Guido%20Caccianiga%20and%20Yarden%20Sharon%20and%20Bernard%20Javot%20and%20Senya%20Polikovsky%20and%20G%C3%B6kce%20Erg%C3%BCn%20and%20Ivan%20Capobianco%20and%20Andr%C3%A9%20L.%20Mihaljevic%20and%20Anton%20Deguet%20and%20Katherine%20J.%20Kuchenbecker&entry.1292438233=%20%20As%20robots%20for%20minimally%20invasive%20surgery%20%28MIS%29%20gradually%20become%20more%0Aaccessible%20and%20modular%2C%20we%20believe%20there%20is%20a%20great%20opportunity%20to%20rethink%20and%0Aexpand%20the%20visualization%20and%20control%20paradigms%20that%20have%20characterized%20surgical%0Ateleoperation%20since%20its%20inception.%20We%20conjecture%20that%20introducing%20one%20or%20more%0Aadditional%20adjustable%20viewpoints%20in%20the%20abdominal%20cavity%20would%20not%20only%20unlock%0Anovel%20visualization%20and%20collaboration%20strategies%20for%20surgeons%20but%20also%0Asubstantially%20boost%20the%20robustness%20of%20machine%20perception%20toward%20shared%0Aautonomy.%20Immediate%20advantages%20include%20controlling%20a%20second%20viewpoint%20and%0Ateleoperating%20surgical%20tools%20from%20a%20different%20perspective%2C%20which%20would%20allow%0Acollaborating%20surgeons%20to%20adjust%20their%20views%20independently%20and%20still%20maneuver%0Atheir%20robotic%20instruments%20intuitively.%20Furthermore%2C%20we%20believe%20that%20capturing%0Asynchronized%20multi-view%203D%20measurements%20of%20the%20patient%27s%20anatomy%20would%20unlock%0Aadvanced%20scene%20representations.%20Accurate%20real-time%20intraoperative%203D%20perception%0Awill%20allow%20algorithmic%20assistants%20to%20directly%20control%20one%20or%20more%20robotic%0Ainstruments%20and/or%20robotic%20cameras.%20Toward%20these%20goals%2C%20we%20are%20building%20a%0Asynchronized%20multi-viewpoint%2C%20multi-sensor%20robotic%20surgery%20system%20by%0Aintegrating%20high-performance%20vision%20components%20and%20upgrading%20the%20da%20Vinci%0AResearch%20Kit%20control%20logic.%20This%20short%20paper%20reports%20a%20functional%20summary%20of%0Aour%20setup%20and%20elaborates%20on%20its%20potential%20impacts%20in%20research%20and%20future%0Aclinical%20practice.%20By%20fully%20open-sourcing%20our%20system%2C%20we%20will%20enable%20the%0Aresearch%20community%20to%20reproduce%20our%20setup%2C%20improve%20it%2C%20and%20develop%20powerful%0Aalgorithms%2C%20effectively%20boosting%20clinical%20translation%20of%20cutting-edge%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11142v1&entry.124074799=Read"},
{"title": "SurgPose: Generalisable Surgical Instrument Pose Estimation using\n  Zero-Shot Learning and Stereo Vision", "author": "Utsav Rai and Haozheng Xu and Stamatia Giannarou", "abstract": "  Accurate pose estimation of surgical tools in Robot-assisted Minimally\nInvasive Surgery (RMIS) is essential for surgical navigation and robot control.\nWhile traditional marker-based methods offer accuracy, they face challenges\nwith occlusions, reflections, and tool-specific designs. Similarly, supervised\nlearning methods require extensive training on annotated datasets, limiting\ntheir adaptability to new tools. Despite their success in other domains,\nzero-shot pose estimation models remain unexplored in RMIS for pose estimation\nof surgical instruments, creating a gap in generalising to unseen surgical\ntools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation\npipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D\nmodels like the FoundationPose and SAM-6D. We advanced these models by\nincorporating vision-based depth estimation using the RAFT-Stereo method, for\nrobust depth estimation in reflective and textureless environments.\nAdditionally, we enhanced SAM-6D by replacing its instance segmentation module,\nSegment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly\nboosting segmentation accuracy in occluded and complex conditions. Extensive\nvalidation reveals that our enhanced SAM-6D surpasses FoundationPose in\nzero-shot pose estimation of unseen surgical instruments, setting a new\nbenchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the\ngeneralisability of pose estimation for unseen objects and pioneers the\napplication of RGB-D zero-shot methods in RMIS.\n", "link": "http://arxiv.org/abs/2505.11439v1", "date": "2025-05-16", "relevancy": 2.785, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5687}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5593}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurgPose%3A%20Generalisable%20Surgical%20Instrument%20Pose%20Estimation%20using%0A%20%20Zero-Shot%20Learning%20and%20Stereo%20Vision&body=Title%3A%20SurgPose%3A%20Generalisable%20Surgical%20Instrument%20Pose%20Estimation%20using%0A%20%20Zero-Shot%20Learning%20and%20Stereo%20Vision%0AAuthor%3A%20Utsav%20Rai%20and%20Haozheng%20Xu%20and%20Stamatia%20Giannarou%0AAbstract%3A%20%20%20Accurate%20pose%20estimation%20of%20surgical%20tools%20in%20Robot-assisted%20Minimally%0AInvasive%20Surgery%20%28RMIS%29%20is%20essential%20for%20surgical%20navigation%20and%20robot%20control.%0AWhile%20traditional%20marker-based%20methods%20offer%20accuracy%2C%20they%20face%20challenges%0Awith%20occlusions%2C%20reflections%2C%20and%20tool-specific%20designs.%20Similarly%2C%20supervised%0Alearning%20methods%20require%20extensive%20training%20on%20annotated%20datasets%2C%20limiting%0Atheir%20adaptability%20to%20new%20tools.%20Despite%20their%20success%20in%20other%20domains%2C%0Azero-shot%20pose%20estimation%20models%20remain%20unexplored%20in%20RMIS%20for%20pose%20estimation%0Aof%20surgical%20instruments%2C%20creating%20a%20gap%20in%20generalising%20to%20unseen%20surgical%0Atools.%20This%20paper%20presents%20a%20novel%206%20Degrees%20of%20Freedom%20%28DoF%29%20pose%20estimation%0Apipeline%20for%20surgical%20instruments%2C%20leveraging%20state-of-the-art%20zero-shot%20RGB-D%0Amodels%20like%20the%20FoundationPose%20and%20SAM-6D.%20We%20advanced%20these%20models%20by%0Aincorporating%20vision-based%20depth%20estimation%20using%20the%20RAFT-Stereo%20method%2C%20for%0Arobust%20depth%20estimation%20in%20reflective%20and%20textureless%20environments.%0AAdditionally%2C%20we%20enhanced%20SAM-6D%20by%20replacing%20its%20instance%20segmentation%20module%2C%0ASegment%20Anything%20Model%20%28SAM%29%2C%20with%20a%20fine-tuned%20Mask%20R-CNN%2C%20significantly%0Aboosting%20segmentation%20accuracy%20in%20occluded%20and%20complex%20conditions.%20Extensive%0Avalidation%20reveals%20that%20our%20enhanced%20SAM-6D%20surpasses%20FoundationPose%20in%0Azero-shot%20pose%20estimation%20of%20unseen%20surgical%20instruments%2C%20setting%20a%20new%0Abenchmark%20for%20zero-shot%20RGB-D%20pose%20estimation%20in%20RMIS.%20This%20work%20enhances%20the%0Ageneralisability%20of%20pose%20estimation%20for%20unseen%20objects%20and%20pioneers%20the%0Aapplication%20of%20RGB-D%20zero-shot%20methods%20in%20RMIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgPose%253A%2520Generalisable%2520Surgical%2520Instrument%2520Pose%2520Estimation%2520using%250A%2520%2520Zero-Shot%2520Learning%2520and%2520Stereo%2520Vision%26entry.906535625%3DUtsav%2520Rai%2520and%2520Haozheng%2520Xu%2520and%2520Stamatia%2520Giannarou%26entry.1292438233%3D%2520%2520Accurate%2520pose%2520estimation%2520of%2520surgical%2520tools%2520in%2520Robot-assisted%2520Minimally%250AInvasive%2520Surgery%2520%2528RMIS%2529%2520is%2520essential%2520for%2520surgical%2520navigation%2520and%2520robot%2520control.%250AWhile%2520traditional%2520marker-based%2520methods%2520offer%2520accuracy%252C%2520they%2520face%2520challenges%250Awith%2520occlusions%252C%2520reflections%252C%2520and%2520tool-specific%2520designs.%2520Similarly%252C%2520supervised%250Alearning%2520methods%2520require%2520extensive%2520training%2520on%2520annotated%2520datasets%252C%2520limiting%250Atheir%2520adaptability%2520to%2520new%2520tools.%2520Despite%2520their%2520success%2520in%2520other%2520domains%252C%250Azero-shot%2520pose%2520estimation%2520models%2520remain%2520unexplored%2520in%2520RMIS%2520for%2520pose%2520estimation%250Aof%2520surgical%2520instruments%252C%2520creating%2520a%2520gap%2520in%2520generalising%2520to%2520unseen%2520surgical%250Atools.%2520This%2520paper%2520presents%2520a%2520novel%25206%2520Degrees%2520of%2520Freedom%2520%2528DoF%2529%2520pose%2520estimation%250Apipeline%2520for%2520surgical%2520instruments%252C%2520leveraging%2520state-of-the-art%2520zero-shot%2520RGB-D%250Amodels%2520like%2520the%2520FoundationPose%2520and%2520SAM-6D.%2520We%2520advanced%2520these%2520models%2520by%250Aincorporating%2520vision-based%2520depth%2520estimation%2520using%2520the%2520RAFT-Stereo%2520method%252C%2520for%250Arobust%2520depth%2520estimation%2520in%2520reflective%2520and%2520textureless%2520environments.%250AAdditionally%252C%2520we%2520enhanced%2520SAM-6D%2520by%2520replacing%2520its%2520instance%2520segmentation%2520module%252C%250ASegment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520with%2520a%2520fine-tuned%2520Mask%2520R-CNN%252C%2520significantly%250Aboosting%2520segmentation%2520accuracy%2520in%2520occluded%2520and%2520complex%2520conditions.%2520Extensive%250Avalidation%2520reveals%2520that%2520our%2520enhanced%2520SAM-6D%2520surpasses%2520FoundationPose%2520in%250Azero-shot%2520pose%2520estimation%2520of%2520unseen%2520surgical%2520instruments%252C%2520setting%2520a%2520new%250Abenchmark%2520for%2520zero-shot%2520RGB-D%2520pose%2520estimation%2520in%2520RMIS.%2520This%2520work%2520enhances%2520the%250Ageneralisability%2520of%2520pose%2520estimation%2520for%2520unseen%2520objects%2520and%2520pioneers%2520the%250Aapplication%2520of%2520RGB-D%2520zero-shot%2520methods%2520in%2520RMIS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurgPose%3A%20Generalisable%20Surgical%20Instrument%20Pose%20Estimation%20using%0A%20%20Zero-Shot%20Learning%20and%20Stereo%20Vision&entry.906535625=Utsav%20Rai%20and%20Haozheng%20Xu%20and%20Stamatia%20Giannarou&entry.1292438233=%20%20Accurate%20pose%20estimation%20of%20surgical%20tools%20in%20Robot-assisted%20Minimally%0AInvasive%20Surgery%20%28RMIS%29%20is%20essential%20for%20surgical%20navigation%20and%20robot%20control.%0AWhile%20traditional%20marker-based%20methods%20offer%20accuracy%2C%20they%20face%20challenges%0Awith%20occlusions%2C%20reflections%2C%20and%20tool-specific%20designs.%20Similarly%2C%20supervised%0Alearning%20methods%20require%20extensive%20training%20on%20annotated%20datasets%2C%20limiting%0Atheir%20adaptability%20to%20new%20tools.%20Despite%20their%20success%20in%20other%20domains%2C%0Azero-shot%20pose%20estimation%20models%20remain%20unexplored%20in%20RMIS%20for%20pose%20estimation%0Aof%20surgical%20instruments%2C%20creating%20a%20gap%20in%20generalising%20to%20unseen%20surgical%0Atools.%20This%20paper%20presents%20a%20novel%206%20Degrees%20of%20Freedom%20%28DoF%29%20pose%20estimation%0Apipeline%20for%20surgical%20instruments%2C%20leveraging%20state-of-the-art%20zero-shot%20RGB-D%0Amodels%20like%20the%20FoundationPose%20and%20SAM-6D.%20We%20advanced%20these%20models%20by%0Aincorporating%20vision-based%20depth%20estimation%20using%20the%20RAFT-Stereo%20method%2C%20for%0Arobust%20depth%20estimation%20in%20reflective%20and%20textureless%20environments.%0AAdditionally%2C%20we%20enhanced%20SAM-6D%20by%20replacing%20its%20instance%20segmentation%20module%2C%0ASegment%20Anything%20Model%20%28SAM%29%2C%20with%20a%20fine-tuned%20Mask%20R-CNN%2C%20significantly%0Aboosting%20segmentation%20accuracy%20in%20occluded%20and%20complex%20conditions.%20Extensive%0Avalidation%20reveals%20that%20our%20enhanced%20SAM-6D%20surpasses%20FoundationPose%20in%0Azero-shot%20pose%20estimation%20of%20unseen%20surgical%20instruments%2C%20setting%20a%20new%0Abenchmark%20for%20zero-shot%20RGB-D%20pose%20estimation%20in%20RMIS.%20This%20work%20enhances%20the%0Ageneralisability%20of%20pose%20estimation%20for%20unseen%20objects%20and%20pioneers%20the%0Aapplication%20of%20RGB-D%20zero-shot%20methods%20in%20RMIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11439v1&entry.124074799=Read"},
{"title": "Imputation-free and Alignment-free: Incomplete Multi-view Clustering\n  Driven by Consensus Semantic Learning", "author": "Yuzhuo Dai and Jiaqi Jin and Zhibin Dong and Siwei Wang and Xinwang Liu and En Zhu and Xihong Yang and Xinbiao Gan and Yu Feng", "abstract": "  In incomplete multi-view clustering (IMVC), missing data induce prototype\nshifts within views and semantic inconsistencies across views. A feasible\nsolution is to explore cross-view consistency in paired complete observations,\nfurther imputing and aligning the similarity relationships inherently shared\nacross views. Nevertheless, existing methods are constrained by two-tiered\nlimitations: (1) Neither instance- nor cluster-level consistency learning\nconstruct a semantic space shared across views to learn consensus semantics.\nThe former enforces cross-view instances alignment, and wrongly regards\nunpaired observations with semantic consistency as negative pairs; the latter\nfocuses on cross-view cluster counterparts while coarsely handling fine-grained\nintra-cluster relationships within views. (2) Excessive reliance on consistency\nresults in unreliable imputation and alignment without incorporating\nview-specific cluster information. Thus, we propose an IMVC framework,\nimputation- and alignment-free for consensus semantics learning (FreeCSL). To\nbridge semantic gaps across all observations, we learn consensus prototypes\nfrom available data to discover a shared space, where semantically similar\nobservations are pulled closer for consensus semantics learning. To capture\nsemantic relationships within specific views, we design a heuristic graph\nclustering based on modularity to recover cluster structure with intra-cluster\ncompactness and inter-cluster separation for cluster semantics enhancement.\nExtensive experiments demonstrate, compared to state-of-the-art competitors,\nFreeCSL achieves more confident and robust assignments on IMVC task.\n", "link": "http://arxiv.org/abs/2505.11182v1", "date": "2025-05-16", "relevancy": 2.7749, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imputation-free%20and%20Alignment-free%3A%20Incomplete%20Multi-view%20Clustering%0A%20%20Driven%20by%20Consensus%20Semantic%20Learning&body=Title%3A%20Imputation-free%20and%20Alignment-free%3A%20Incomplete%20Multi-view%20Clustering%0A%20%20Driven%20by%20Consensus%20Semantic%20Learning%0AAuthor%3A%20Yuzhuo%20Dai%20and%20Jiaqi%20Jin%20and%20Zhibin%20Dong%20and%20Siwei%20Wang%20and%20Xinwang%20Liu%20and%20En%20Zhu%20and%20Xihong%20Yang%20and%20Xinbiao%20Gan%20and%20Yu%20Feng%0AAbstract%3A%20%20%20In%20incomplete%20multi-view%20clustering%20%28IMVC%29%2C%20missing%20data%20induce%20prototype%0Ashifts%20within%20views%20and%20semantic%20inconsistencies%20across%20views.%20A%20feasible%0Asolution%20is%20to%20explore%20cross-view%20consistency%20in%20paired%20complete%20observations%2C%0Afurther%20imputing%20and%20aligning%20the%20similarity%20relationships%20inherently%20shared%0Aacross%20views.%20Nevertheless%2C%20existing%20methods%20are%20constrained%20by%20two-tiered%0Alimitations%3A%20%281%29%20Neither%20instance-%20nor%20cluster-level%20consistency%20learning%0Aconstruct%20a%20semantic%20space%20shared%20across%20views%20to%20learn%20consensus%20semantics.%0AThe%20former%20enforces%20cross-view%20instances%20alignment%2C%20and%20wrongly%20regards%0Aunpaired%20observations%20with%20semantic%20consistency%20as%20negative%20pairs%3B%20the%20latter%0Afocuses%20on%20cross-view%20cluster%20counterparts%20while%20coarsely%20handling%20fine-grained%0Aintra-cluster%20relationships%20within%20views.%20%282%29%20Excessive%20reliance%20on%20consistency%0Aresults%20in%20unreliable%20imputation%20and%20alignment%20without%20incorporating%0Aview-specific%20cluster%20information.%20Thus%2C%20we%20propose%20an%20IMVC%20framework%2C%0Aimputation-%20and%20alignment-free%20for%20consensus%20semantics%20learning%20%28FreeCSL%29.%20To%0Abridge%20semantic%20gaps%20across%20all%20observations%2C%20we%20learn%20consensus%20prototypes%0Afrom%20available%20data%20to%20discover%20a%20shared%20space%2C%20where%20semantically%20similar%0Aobservations%20are%20pulled%20closer%20for%20consensus%20semantics%20learning.%20To%20capture%0Asemantic%20relationships%20within%20specific%20views%2C%20we%20design%20a%20heuristic%20graph%0Aclustering%20based%20on%20modularity%20to%20recover%20cluster%20structure%20with%20intra-cluster%0Acompactness%20and%20inter-cluster%20separation%20for%20cluster%20semantics%20enhancement.%0AExtensive%20experiments%20demonstrate%2C%20compared%20to%20state-of-the-art%20competitors%2C%0AFreeCSL%20achieves%20more%20confident%20and%20robust%20assignments%20on%20IMVC%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImputation-free%2520and%2520Alignment-free%253A%2520Incomplete%2520Multi-view%2520Clustering%250A%2520%2520Driven%2520by%2520Consensus%2520Semantic%2520Learning%26entry.906535625%3DYuzhuo%2520Dai%2520and%2520Jiaqi%2520Jin%2520and%2520Zhibin%2520Dong%2520and%2520Siwei%2520Wang%2520and%2520Xinwang%2520Liu%2520and%2520En%2520Zhu%2520and%2520Xihong%2520Yang%2520and%2520Xinbiao%2520Gan%2520and%2520Yu%2520Feng%26entry.1292438233%3D%2520%2520In%2520incomplete%2520multi-view%2520clustering%2520%2528IMVC%2529%252C%2520missing%2520data%2520induce%2520prototype%250Ashifts%2520within%2520views%2520and%2520semantic%2520inconsistencies%2520across%2520views.%2520A%2520feasible%250Asolution%2520is%2520to%2520explore%2520cross-view%2520consistency%2520in%2520paired%2520complete%2520observations%252C%250Afurther%2520imputing%2520and%2520aligning%2520the%2520similarity%2520relationships%2520inherently%2520shared%250Aacross%2520views.%2520Nevertheless%252C%2520existing%2520methods%2520are%2520constrained%2520by%2520two-tiered%250Alimitations%253A%2520%25281%2529%2520Neither%2520instance-%2520nor%2520cluster-level%2520consistency%2520learning%250Aconstruct%2520a%2520semantic%2520space%2520shared%2520across%2520views%2520to%2520learn%2520consensus%2520semantics.%250AThe%2520former%2520enforces%2520cross-view%2520instances%2520alignment%252C%2520and%2520wrongly%2520regards%250Aunpaired%2520observations%2520with%2520semantic%2520consistency%2520as%2520negative%2520pairs%253B%2520the%2520latter%250Afocuses%2520on%2520cross-view%2520cluster%2520counterparts%2520while%2520coarsely%2520handling%2520fine-grained%250Aintra-cluster%2520relationships%2520within%2520views.%2520%25282%2529%2520Excessive%2520reliance%2520on%2520consistency%250Aresults%2520in%2520unreliable%2520imputation%2520and%2520alignment%2520without%2520incorporating%250Aview-specific%2520cluster%2520information.%2520Thus%252C%2520we%2520propose%2520an%2520IMVC%2520framework%252C%250Aimputation-%2520and%2520alignment-free%2520for%2520consensus%2520semantics%2520learning%2520%2528FreeCSL%2529.%2520To%250Abridge%2520semantic%2520gaps%2520across%2520all%2520observations%252C%2520we%2520learn%2520consensus%2520prototypes%250Afrom%2520available%2520data%2520to%2520discover%2520a%2520shared%2520space%252C%2520where%2520semantically%2520similar%250Aobservations%2520are%2520pulled%2520closer%2520for%2520consensus%2520semantics%2520learning.%2520To%2520capture%250Asemantic%2520relationships%2520within%2520specific%2520views%252C%2520we%2520design%2520a%2520heuristic%2520graph%250Aclustering%2520based%2520on%2520modularity%2520to%2520recover%2520cluster%2520structure%2520with%2520intra-cluster%250Acompactness%2520and%2520inter-cluster%2520separation%2520for%2520cluster%2520semantics%2520enhancement.%250AExtensive%2520experiments%2520demonstrate%252C%2520compared%2520to%2520state-of-the-art%2520competitors%252C%250AFreeCSL%2520achieves%2520more%2520confident%2520and%2520robust%2520assignments%2520on%2520IMVC%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imputation-free%20and%20Alignment-free%3A%20Incomplete%20Multi-view%20Clustering%0A%20%20Driven%20by%20Consensus%20Semantic%20Learning&entry.906535625=Yuzhuo%20Dai%20and%20Jiaqi%20Jin%20and%20Zhibin%20Dong%20and%20Siwei%20Wang%20and%20Xinwang%20Liu%20and%20En%20Zhu%20and%20Xihong%20Yang%20and%20Xinbiao%20Gan%20and%20Yu%20Feng&entry.1292438233=%20%20In%20incomplete%20multi-view%20clustering%20%28IMVC%29%2C%20missing%20data%20induce%20prototype%0Ashifts%20within%20views%20and%20semantic%20inconsistencies%20across%20views.%20A%20feasible%0Asolution%20is%20to%20explore%20cross-view%20consistency%20in%20paired%20complete%20observations%2C%0Afurther%20imputing%20and%20aligning%20the%20similarity%20relationships%20inherently%20shared%0Aacross%20views.%20Nevertheless%2C%20existing%20methods%20are%20constrained%20by%20two-tiered%0Alimitations%3A%20%281%29%20Neither%20instance-%20nor%20cluster-level%20consistency%20learning%0Aconstruct%20a%20semantic%20space%20shared%20across%20views%20to%20learn%20consensus%20semantics.%0AThe%20former%20enforces%20cross-view%20instances%20alignment%2C%20and%20wrongly%20regards%0Aunpaired%20observations%20with%20semantic%20consistency%20as%20negative%20pairs%3B%20the%20latter%0Afocuses%20on%20cross-view%20cluster%20counterparts%20while%20coarsely%20handling%20fine-grained%0Aintra-cluster%20relationships%20within%20views.%20%282%29%20Excessive%20reliance%20on%20consistency%0Aresults%20in%20unreliable%20imputation%20and%20alignment%20without%20incorporating%0Aview-specific%20cluster%20information.%20Thus%2C%20we%20propose%20an%20IMVC%20framework%2C%0Aimputation-%20and%20alignment-free%20for%20consensus%20semantics%20learning%20%28FreeCSL%29.%20To%0Abridge%20semantic%20gaps%20across%20all%20observations%2C%20we%20learn%20consensus%20prototypes%0Afrom%20available%20data%20to%20discover%20a%20shared%20space%2C%20where%20semantically%20similar%0Aobservations%20are%20pulled%20closer%20for%20consensus%20semantics%20learning.%20To%20capture%0Asemantic%20relationships%20within%20specific%20views%2C%20we%20design%20a%20heuristic%20graph%0Aclustering%20based%20on%20modularity%20to%20recover%20cluster%20structure%20with%20intra-cluster%0Acompactness%20and%20inter-cluster%20separation%20for%20cluster%20semantics%20enhancement.%0AExtensive%20experiments%20demonstrate%2C%20compared%20to%20state-of-the-art%20competitors%2C%0AFreeCSL%20achieves%20more%20confident%20and%20robust%20assignments%20on%20IMVC%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11182v1&entry.124074799=Read"},
{"title": "X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic\n  Humanoid Imitation", "author": "Peizhen Li and Longbing Cao and Xiao-Ming Wu and Runze Yang and Xiaohan Yu", "abstract": "  The ability to imitate realistic facial expressions is essential for humanoid\nrobots engaged in affective human-robot communication. However, the lack of\ndatasets containing diverse humanoid facial expressions with proper annotations\nhinders progress in realistic humanoid facial expression imitation. To address\nthese challenges, we introduce X2C (Anything to Control), a dataset featuring\nnuanced facial expressions for realistic humanoid imitation. With X2C, we\ncontribute: 1) a high-quality, high-diversity, large-scale dataset comprising\n100,000 (image, control value) pairs. Each image depicts a humanoid robot\ndisplaying a diverse range of facial expressions, annotated with 30 control\nvalues representing the ground-truth expression configuration; 2) X2CNet, a\nnovel human-to-humanoid facial expression imitation framework that learns the\ncorrespondence between nuanced humanoid expressions and their underlying\ncontrol values from X2C. It enables facial expression imitation in the wild for\ndifferent human performers, providing a baseline for the imitation task,\nshowcasing the potential value of our dataset; 3) real-world demonstrations on\na physical humanoid robot, highlighting its capability to advance realistic\nhumanoid facial expression imitation. Code and Data:\nhttps://lipzh5.github.io/X2CNet/\n", "link": "http://arxiv.org/abs/2505.11146v1", "date": "2025-05-16", "relevancy": 2.7713, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5657}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5502}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X2C%3A%20A%20Dataset%20Featuring%20Nuanced%20Facial%20Expressions%20for%20Realistic%0A%20%20Humanoid%20Imitation&body=Title%3A%20X2C%3A%20A%20Dataset%20Featuring%20Nuanced%20Facial%20Expressions%20for%20Realistic%0A%20%20Humanoid%20Imitation%0AAuthor%3A%20Peizhen%20Li%20and%20Longbing%20Cao%20and%20Xiao-Ming%20Wu%20and%20Runze%20Yang%20and%20Xiaohan%20Yu%0AAbstract%3A%20%20%20The%20ability%20to%20imitate%20realistic%20facial%20expressions%20is%20essential%20for%20humanoid%0Arobots%20engaged%20in%20affective%20human-robot%20communication.%20However%2C%20the%20lack%20of%0Adatasets%20containing%20diverse%20humanoid%20facial%20expressions%20with%20proper%20annotations%0Ahinders%20progress%20in%20realistic%20humanoid%20facial%20expression%20imitation.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20X2C%20%28Anything%20to%20Control%29%2C%20a%20dataset%20featuring%0Anuanced%20facial%20expressions%20for%20realistic%20humanoid%20imitation.%20With%20X2C%2C%20we%0Acontribute%3A%201%29%20a%20high-quality%2C%20high-diversity%2C%20large-scale%20dataset%20comprising%0A100%2C000%20%28image%2C%20control%20value%29%20pairs.%20Each%20image%20depicts%20a%20humanoid%20robot%0Adisplaying%20a%20diverse%20range%20of%20facial%20expressions%2C%20annotated%20with%2030%20control%0Avalues%20representing%20the%20ground-truth%20expression%20configuration%3B%202%29%20X2CNet%2C%20a%0Anovel%20human-to-humanoid%20facial%20expression%20imitation%20framework%20that%20learns%20the%0Acorrespondence%20between%20nuanced%20humanoid%20expressions%20and%20their%20underlying%0Acontrol%20values%20from%20X2C.%20It%20enables%20facial%20expression%20imitation%20in%20the%20wild%20for%0Adifferent%20human%20performers%2C%20providing%20a%20baseline%20for%20the%20imitation%20task%2C%0Ashowcasing%20the%20potential%20value%20of%20our%20dataset%3B%203%29%20real-world%20demonstrations%20on%0Aa%20physical%20humanoid%20robot%2C%20highlighting%20its%20capability%20to%20advance%20realistic%0Ahumanoid%20facial%20expression%20imitation.%20Code%20and%20Data%3A%0Ahttps%3A//lipzh5.github.io/X2CNet/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX2C%253A%2520A%2520Dataset%2520Featuring%2520Nuanced%2520Facial%2520Expressions%2520for%2520Realistic%250A%2520%2520Humanoid%2520Imitation%26entry.906535625%3DPeizhen%2520Li%2520and%2520Longbing%2520Cao%2520and%2520Xiao-Ming%2520Wu%2520and%2520Runze%2520Yang%2520and%2520Xiaohan%2520Yu%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520imitate%2520realistic%2520facial%2520expressions%2520is%2520essential%2520for%2520humanoid%250Arobots%2520engaged%2520in%2520affective%2520human-robot%2520communication.%2520However%252C%2520the%2520lack%2520of%250Adatasets%2520containing%2520diverse%2520humanoid%2520facial%2520expressions%2520with%2520proper%2520annotations%250Ahinders%2520progress%2520in%2520realistic%2520humanoid%2520facial%2520expression%2520imitation.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520X2C%2520%2528Anything%2520to%2520Control%2529%252C%2520a%2520dataset%2520featuring%250Anuanced%2520facial%2520expressions%2520for%2520realistic%2520humanoid%2520imitation.%2520With%2520X2C%252C%2520we%250Acontribute%253A%25201%2529%2520a%2520high-quality%252C%2520high-diversity%252C%2520large-scale%2520dataset%2520comprising%250A100%252C000%2520%2528image%252C%2520control%2520value%2529%2520pairs.%2520Each%2520image%2520depicts%2520a%2520humanoid%2520robot%250Adisplaying%2520a%2520diverse%2520range%2520of%2520facial%2520expressions%252C%2520annotated%2520with%252030%2520control%250Avalues%2520representing%2520the%2520ground-truth%2520expression%2520configuration%253B%25202%2529%2520X2CNet%252C%2520a%250Anovel%2520human-to-humanoid%2520facial%2520expression%2520imitation%2520framework%2520that%2520learns%2520the%250Acorrespondence%2520between%2520nuanced%2520humanoid%2520expressions%2520and%2520their%2520underlying%250Acontrol%2520values%2520from%2520X2C.%2520It%2520enables%2520facial%2520expression%2520imitation%2520in%2520the%2520wild%2520for%250Adifferent%2520human%2520performers%252C%2520providing%2520a%2520baseline%2520for%2520the%2520imitation%2520task%252C%250Ashowcasing%2520the%2520potential%2520value%2520of%2520our%2520dataset%253B%25203%2529%2520real-world%2520demonstrations%2520on%250Aa%2520physical%2520humanoid%2520robot%252C%2520highlighting%2520its%2520capability%2520to%2520advance%2520realistic%250Ahumanoid%2520facial%2520expression%2520imitation.%2520Code%2520and%2520Data%253A%250Ahttps%253A//lipzh5.github.io/X2CNet/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X2C%3A%20A%20Dataset%20Featuring%20Nuanced%20Facial%20Expressions%20for%20Realistic%0A%20%20Humanoid%20Imitation&entry.906535625=Peizhen%20Li%20and%20Longbing%20Cao%20and%20Xiao-Ming%20Wu%20and%20Runze%20Yang%20and%20Xiaohan%20Yu&entry.1292438233=%20%20The%20ability%20to%20imitate%20realistic%20facial%20expressions%20is%20essential%20for%20humanoid%0Arobots%20engaged%20in%20affective%20human-robot%20communication.%20However%2C%20the%20lack%20of%0Adatasets%20containing%20diverse%20humanoid%20facial%20expressions%20with%20proper%20annotations%0Ahinders%20progress%20in%20realistic%20humanoid%20facial%20expression%20imitation.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20X2C%20%28Anything%20to%20Control%29%2C%20a%20dataset%20featuring%0Anuanced%20facial%20expressions%20for%20realistic%20humanoid%20imitation.%20With%20X2C%2C%20we%0Acontribute%3A%201%29%20a%20high-quality%2C%20high-diversity%2C%20large-scale%20dataset%20comprising%0A100%2C000%20%28image%2C%20control%20value%29%20pairs.%20Each%20image%20depicts%20a%20humanoid%20robot%0Adisplaying%20a%20diverse%20range%20of%20facial%20expressions%2C%20annotated%20with%2030%20control%0Avalues%20representing%20the%20ground-truth%20expression%20configuration%3B%202%29%20X2CNet%2C%20a%0Anovel%20human-to-humanoid%20facial%20expression%20imitation%20framework%20that%20learns%20the%0Acorrespondence%20between%20nuanced%20humanoid%20expressions%20and%20their%20underlying%0Acontrol%20values%20from%20X2C.%20It%20enables%20facial%20expression%20imitation%20in%20the%20wild%20for%0Adifferent%20human%20performers%2C%20providing%20a%20baseline%20for%20the%20imitation%20task%2C%0Ashowcasing%20the%20potential%20value%20of%20our%20dataset%3B%203%29%20real-world%20demonstrations%20on%0Aa%20physical%20humanoid%20robot%2C%20highlighting%20its%20capability%20to%20advance%20realistic%0Ahumanoid%20facial%20expression%20imitation.%20Code%20and%20Data%3A%0Ahttps%3A//lipzh5.github.io/X2CNet/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11146v1&entry.124074799=Read"},
{"title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time\n  Vision-Language Models", "author": "Keunwoo Peter Yu and Joyce Chai", "abstract": "  Vision-language models (VLMs) have shown remarkable progress in offline tasks\nsuch as image captioning and video question answering. However, real-time\ninteractive environments impose new demands on VLMs, requiring them to generate\nutterances that are not only semantically accurate but also precisely timed. We\nidentify two core capabilities necessary for such settings --\n$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and\npropose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation\n(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in\nresponse to streaming video such that both content and timing align with\ndynamic visual input. To support this benchmark, we curate evaluation datasets\nfrom sports broadcasting and egocentric human interaction domains, and\nintroduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring\nsemantic similarity and temporal alignment. Finally, we present\n$\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,\na model that interleaves visual and linguistic tokens in a time-synchronized\nmanner, enabling real-time language generation without relying on turn-based\nassumptions. Experimental results show that VLM-TSI significantly outperforms a\nstrong baseline, yet overall performance remains modest -- highlighting the\ndifficulty of TGLG and motivating further research in real-time VLMs. Code and\ndata available $\\href{https://github.com/yukw777/tglg}{here}$.\n", "link": "http://arxiv.org/abs/2505.11326v1", "date": "2025-05-16", "relevancy": 2.7475, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5533}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporally-Grounded%20Language%20Generation%3A%20A%20Benchmark%20for%20Real-Time%0A%20%20Vision-Language%20Models&body=Title%3A%20Temporally-Grounded%20Language%20Generation%3A%20A%20Benchmark%20for%20Real-Time%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Keunwoo%20Peter%20Yu%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20remarkable%20progress%20in%20offline%20tasks%0Asuch%20as%20image%20captioning%20and%20video%20question%20answering.%20However%2C%20real-time%0Ainteractive%20environments%20impose%20new%20demands%20on%20VLMs%2C%20requiring%20them%20to%20generate%0Autterances%20that%20are%20not%20only%20semantically%20accurate%20but%20also%20precisely%20timed.%20We%0Aidentify%20two%20core%20capabilities%20necessary%20for%20such%20settings%20--%0A%24%5Ctextit%7Bperceptual%20updating%7D%24%20and%20%24%5Ctextit%7Bcontingency%20awareness%7D%24%20--%20and%0Apropose%20a%20new%20benchmark%20task%2C%20%24%5Ctextbf%7BTemporally-Grounded%20Language%20Generation%0A%28TGLG%29%7D%24%2C%20to%20evaluate%20them.%20TGLG%20requires%20models%20to%20generate%20utterances%20in%0Aresponse%20to%20streaming%20video%20such%20that%20both%20content%20and%20timing%20align%20with%0Adynamic%20visual%20input.%20To%20support%20this%20benchmark%2C%20we%20curate%20evaluation%20datasets%0Afrom%20sports%20broadcasting%20and%20egocentric%20human%20interaction%20domains%2C%20and%0Aintroduce%20a%20new%20metric%2C%20%24%5Ctextbf%7BTRACE%7D%24%2C%20to%20evaluate%20TGLG%20by%20jointly%20measuring%0Asemantic%20similarity%20and%20temporal%20alignment.%20Finally%2C%20we%20present%0A%24%5Ctextbf%7BVision-Language%20Model%20with%20Time-Synchronized%20Interleaving%20%28VLM-TSI%29%7D%24%2C%0Aa%20model%20that%20interleaves%20visual%20and%20linguistic%20tokens%20in%20a%20time-synchronized%0Amanner%2C%20enabling%20real-time%20language%20generation%20without%20relying%20on%20turn-based%0Aassumptions.%20Experimental%20results%20show%20that%20VLM-TSI%20significantly%20outperforms%20a%0Astrong%20baseline%2C%20yet%20overall%20performance%20remains%20modest%20--%20highlighting%20the%0Adifficulty%20of%20TGLG%20and%20motivating%20further%20research%20in%20real-time%20VLMs.%20Code%20and%0Adata%20available%20%24%5Chref%7Bhttps%3A//github.com/yukw777/tglg%7D%7Bhere%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporally-Grounded%2520Language%2520Generation%253A%2520A%2520Benchmark%2520for%2520Real-Time%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DKeunwoo%2520Peter%2520Yu%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520remarkable%2520progress%2520in%2520offline%2520tasks%250Asuch%2520as%2520image%2520captioning%2520and%2520video%2520question%2520answering.%2520However%252C%2520real-time%250Ainteractive%2520environments%2520impose%2520new%2520demands%2520on%2520VLMs%252C%2520requiring%2520them%2520to%2520generate%250Autterances%2520that%2520are%2520not%2520only%2520semantically%2520accurate%2520but%2520also%2520precisely%2520timed.%2520We%250Aidentify%2520two%2520core%2520capabilities%2520necessary%2520for%2520such%2520settings%2520--%250A%2524%255Ctextit%257Bperceptual%2520updating%257D%2524%2520and%2520%2524%255Ctextit%257Bcontingency%2520awareness%257D%2524%2520--%2520and%250Apropose%2520a%2520new%2520benchmark%2520task%252C%2520%2524%255Ctextbf%257BTemporally-Grounded%2520Language%2520Generation%250A%2528TGLG%2529%257D%2524%252C%2520to%2520evaluate%2520them.%2520TGLG%2520requires%2520models%2520to%2520generate%2520utterances%2520in%250Aresponse%2520to%2520streaming%2520video%2520such%2520that%2520both%2520content%2520and%2520timing%2520align%2520with%250Adynamic%2520visual%2520input.%2520To%2520support%2520this%2520benchmark%252C%2520we%2520curate%2520evaluation%2520datasets%250Afrom%2520sports%2520broadcasting%2520and%2520egocentric%2520human%2520interaction%2520domains%252C%2520and%250Aintroduce%2520a%2520new%2520metric%252C%2520%2524%255Ctextbf%257BTRACE%257D%2524%252C%2520to%2520evaluate%2520TGLG%2520by%2520jointly%2520measuring%250Asemantic%2520similarity%2520and%2520temporal%2520alignment.%2520Finally%252C%2520we%2520present%250A%2524%255Ctextbf%257BVision-Language%2520Model%2520with%2520Time-Synchronized%2520Interleaving%2520%2528VLM-TSI%2529%257D%2524%252C%250Aa%2520model%2520that%2520interleaves%2520visual%2520and%2520linguistic%2520tokens%2520in%2520a%2520time-synchronized%250Amanner%252C%2520enabling%2520real-time%2520language%2520generation%2520without%2520relying%2520on%2520turn-based%250Aassumptions.%2520Experimental%2520results%2520show%2520that%2520VLM-TSI%2520significantly%2520outperforms%2520a%250Astrong%2520baseline%252C%2520yet%2520overall%2520performance%2520remains%2520modest%2520--%2520highlighting%2520the%250Adifficulty%2520of%2520TGLG%2520and%2520motivating%2520further%2520research%2520in%2520real-time%2520VLMs.%2520Code%2520and%250Adata%2520available%2520%2524%255Chref%257Bhttps%253A//github.com/yukw777/tglg%257D%257Bhere%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporally-Grounded%20Language%20Generation%3A%20A%20Benchmark%20for%20Real-Time%0A%20%20Vision-Language%20Models&entry.906535625=Keunwoo%20Peter%20Yu%20and%20Joyce%20Chai&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20remarkable%20progress%20in%20offline%20tasks%0Asuch%20as%20image%20captioning%20and%20video%20question%20answering.%20However%2C%20real-time%0Ainteractive%20environments%20impose%20new%20demands%20on%20VLMs%2C%20requiring%20them%20to%20generate%0Autterances%20that%20are%20not%20only%20semantically%20accurate%20but%20also%20precisely%20timed.%20We%0Aidentify%20two%20core%20capabilities%20necessary%20for%20such%20settings%20--%0A%24%5Ctextit%7Bperceptual%20updating%7D%24%20and%20%24%5Ctextit%7Bcontingency%20awareness%7D%24%20--%20and%0Apropose%20a%20new%20benchmark%20task%2C%20%24%5Ctextbf%7BTemporally-Grounded%20Language%20Generation%0A%28TGLG%29%7D%24%2C%20to%20evaluate%20them.%20TGLG%20requires%20models%20to%20generate%20utterances%20in%0Aresponse%20to%20streaming%20video%20such%20that%20both%20content%20and%20timing%20align%20with%0Adynamic%20visual%20input.%20To%20support%20this%20benchmark%2C%20we%20curate%20evaluation%20datasets%0Afrom%20sports%20broadcasting%20and%20egocentric%20human%20interaction%20domains%2C%20and%0Aintroduce%20a%20new%20metric%2C%20%24%5Ctextbf%7BTRACE%7D%24%2C%20to%20evaluate%20TGLG%20by%20jointly%20measuring%0Asemantic%20similarity%20and%20temporal%20alignment.%20Finally%2C%20we%20present%0A%24%5Ctextbf%7BVision-Language%20Model%20with%20Time-Synchronized%20Interleaving%20%28VLM-TSI%29%7D%24%2C%0Aa%20model%20that%20interleaves%20visual%20and%20linguistic%20tokens%20in%20a%20time-synchronized%0Amanner%2C%20enabling%20real-time%20language%20generation%20without%20relying%20on%20turn-based%0Aassumptions.%20Experimental%20results%20show%20that%20VLM-TSI%20significantly%20outperforms%20a%0Astrong%20baseline%2C%20yet%20overall%20performance%20remains%20modest%20--%20highlighting%20the%0Adifficulty%20of%20TGLG%20and%20motivating%20further%20research%20in%20real-time%20VLMs.%20Code%20and%0Adata%20available%20%24%5Chref%7Bhttps%3A//github.com/yukw777/tglg%7D%7Bhere%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11326v1&entry.124074799=Read"},
{"title": "Face Consistency Benchmark for GenAI Video", "author": "Michal Podstawski and Malgorzata Kudelska and Haohong Wang", "abstract": "  Video generation driven by artificial intelligence has advanced\nsignificantly, enabling the creation of dynamic and realistic content. However,\nmaintaining character consistency across video sequences remains a major\nchallenge, with current models struggling to ensure coherence in appearance and\nattributes. This paper introduces the Face Consistency Benchmark (FCB), a\nframework for evaluating and comparing the consistency of characters in\nAI-generated videos. By providing standardized metrics, the benchmark\nhighlights gaps in existing solutions and promotes the development of more\nreliable approaches. This work represents a crucial step toward improving\ncharacter consistency in AI video generation technologies.\n", "link": "http://arxiv.org/abs/2505.11425v1", "date": "2025-05-16", "relevancy": 2.7393, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5698}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5587}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Face%20Consistency%20Benchmark%20for%20GenAI%20Video&body=Title%3A%20Face%20Consistency%20Benchmark%20for%20GenAI%20Video%0AAuthor%3A%20Michal%20Podstawski%20and%20Malgorzata%20Kudelska%20and%20Haohong%20Wang%0AAbstract%3A%20%20%20Video%20generation%20driven%20by%20artificial%20intelligence%20has%20advanced%0Asignificantly%2C%20enabling%20the%20creation%20of%20dynamic%20and%20realistic%20content.%20However%2C%0Amaintaining%20character%20consistency%20across%20video%20sequences%20remains%20a%20major%0Achallenge%2C%20with%20current%20models%20struggling%20to%20ensure%20coherence%20in%20appearance%20and%0Aattributes.%20This%20paper%20introduces%20the%20Face%20Consistency%20Benchmark%20%28FCB%29%2C%20a%0Aframework%20for%20evaluating%20and%20comparing%20the%20consistency%20of%20characters%20in%0AAI-generated%20videos.%20By%20providing%20standardized%20metrics%2C%20the%20benchmark%0Ahighlights%20gaps%20in%20existing%20solutions%20and%20promotes%20the%20development%20of%20more%0Areliable%20approaches.%20This%20work%20represents%20a%20crucial%20step%20toward%20improving%0Acharacter%20consistency%20in%20AI%20video%20generation%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFace%2520Consistency%2520Benchmark%2520for%2520GenAI%2520Video%26entry.906535625%3DMichal%2520Podstawski%2520and%2520Malgorzata%2520Kudelska%2520and%2520Haohong%2520Wang%26entry.1292438233%3D%2520%2520Video%2520generation%2520driven%2520by%2520artificial%2520intelligence%2520has%2520advanced%250Asignificantly%252C%2520enabling%2520the%2520creation%2520of%2520dynamic%2520and%2520realistic%2520content.%2520However%252C%250Amaintaining%2520character%2520consistency%2520across%2520video%2520sequences%2520remains%2520a%2520major%250Achallenge%252C%2520with%2520current%2520models%2520struggling%2520to%2520ensure%2520coherence%2520in%2520appearance%2520and%250Aattributes.%2520This%2520paper%2520introduces%2520the%2520Face%2520Consistency%2520Benchmark%2520%2528FCB%2529%252C%2520a%250Aframework%2520for%2520evaluating%2520and%2520comparing%2520the%2520consistency%2520of%2520characters%2520in%250AAI-generated%2520videos.%2520By%2520providing%2520standardized%2520metrics%252C%2520the%2520benchmark%250Ahighlights%2520gaps%2520in%2520existing%2520solutions%2520and%2520promotes%2520the%2520development%2520of%2520more%250Areliable%2520approaches.%2520This%2520work%2520represents%2520a%2520crucial%2520step%2520toward%2520improving%250Acharacter%2520consistency%2520in%2520AI%2520video%2520generation%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Face%20Consistency%20Benchmark%20for%20GenAI%20Video&entry.906535625=Michal%20Podstawski%20and%20Malgorzata%20Kudelska%20and%20Haohong%20Wang&entry.1292438233=%20%20Video%20generation%20driven%20by%20artificial%20intelligence%20has%20advanced%0Asignificantly%2C%20enabling%20the%20creation%20of%20dynamic%20and%20realistic%20content.%20However%2C%0Amaintaining%20character%20consistency%20across%20video%20sequences%20remains%20a%20major%0Achallenge%2C%20with%20current%20models%20struggling%20to%20ensure%20coherence%20in%20appearance%20and%0Aattributes.%20This%20paper%20introduces%20the%20Face%20Consistency%20Benchmark%20%28FCB%29%2C%20a%0Aframework%20for%20evaluating%20and%20comparing%20the%20consistency%20of%20characters%20in%0AAI-generated%20videos.%20By%20providing%20standardized%20metrics%2C%20the%20benchmark%0Ahighlights%20gaps%20in%20existing%20solutions%20and%20promotes%20the%20development%20of%20more%0Areliable%20approaches.%20This%20work%20represents%20a%20crucial%20step%20toward%20improving%0Acharacter%20consistency%20in%20AI%20video%20generation%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11425v1&entry.124074799=Read"},
{"title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models", "author": "Justus Westerhoff and Erblina Purelku and Jakob Hackstein and Jonas Loos and Leo Pinetzki and Lorenz Hufe", "abstract": "  Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper along with the code for evaluations at\nwww.bliss.berlin/research/scam.\n", "link": "http://arxiv.org/abs/2504.04893v4", "date": "2025-05-16", "relevancy": 2.7253, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models&body=Title%3A%20SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models%0AAuthor%3A%20Justus%20Westerhoff%20and%20Erblina%20Purelku%20and%20Jakob%20Hackstein%20and%20Jonas%20Loos%20and%20Leo%20Pinetzki%20and%20Lorenz%20Hufe%0AAbstract%3A%20%20%20Typographic%20attacks%20exploit%20the%20interplay%20between%20text%20and%20visual%20content%20in%0Amultimodal%20foundation%20models%2C%20causing%20misclassifications%20when%20misleading%20text%0Ais%20embedded%20within%20images.%20However%2C%20existing%20datasets%20are%20limited%20in%20size%20and%0Adiversity%2C%20making%20it%20difficult%20to%20study%20such%20vulnerabilities.%20In%20this%20paper%2C%20we%0Aintroduce%20SCAM%2C%20the%20largest%20and%20most%20diverse%20dataset%20of%20real-world%20typographic%0Aattack%20images%20to%20date%2C%20containing%201%2C162%20images%20across%20hundreds%20of%20object%0Acategories%20and%20attack%20words.%20Through%20extensive%20benchmarking%20of%20Vision-Language%0AModels%20%28VLMs%29%20on%20SCAM%2C%20we%20demonstrate%20that%20typographic%20attacks%20significantly%0Adegrade%20performance%2C%20and%20identify%20that%20training%20data%20and%20model%20architecture%0Ainfluence%20the%20susceptibility%20to%20these%20attacks.%20Our%20findings%20reveal%20that%0Atypographic%20attacks%20persist%20in%20state-of-the-art%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20due%20to%20the%20choice%20of%20their%20vision%20encoder%2C%20though%20larger%20Large%20Language%0AModels%20%28LLMs%29%20backbones%20help%20mitigate%20their%20vulnerability.%20Additionally%2C%20we%0Ademonstrate%20that%20synthetic%20attacks%20closely%20resemble%20real-world%20%28handwritten%29%0Aattacks%2C%20validating%20their%20use%20in%20research.%20Our%20work%20provides%20a%20comprehensive%0Aresource%20and%20empirical%20insights%20to%20facilitate%20future%20research%20toward%20robust%20and%0Atrustworthy%20multimodal%20AI%20systems.%20We%20publicly%20release%20the%20datasets%20introduced%0Ain%20this%20paper%20along%20with%20the%20code%20for%20evaluations%20at%0Awww.bliss.berlin/research/scam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04893v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCAM%253A%2520A%2520Real-World%2520Typographic%2520Robustness%2520Evaluation%2520for%2520Multimodal%250A%2520%2520Foundation%2520Models%26entry.906535625%3DJustus%2520Westerhoff%2520and%2520Erblina%2520Purelku%2520and%2520Jakob%2520Hackstein%2520and%2520Jonas%2520Loos%2520and%2520Leo%2520Pinetzki%2520and%2520Lorenz%2520Hufe%26entry.1292438233%3D%2520%2520Typographic%2520attacks%2520exploit%2520the%2520interplay%2520between%2520text%2520and%2520visual%2520content%2520in%250Amultimodal%2520foundation%2520models%252C%2520causing%2520misclassifications%2520when%2520misleading%2520text%250Ais%2520embedded%2520within%2520images.%2520However%252C%2520existing%2520datasets%2520are%2520limited%2520in%2520size%2520and%250Adiversity%252C%2520making%2520it%2520difficult%2520to%2520study%2520such%2520vulnerabilities.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520SCAM%252C%2520the%2520largest%2520and%2520most%2520diverse%2520dataset%2520of%2520real-world%2520typographic%250Aattack%2520images%2520to%2520date%252C%2520containing%25201%252C162%2520images%2520across%2520hundreds%2520of%2520object%250Acategories%2520and%2520attack%2520words.%2520Through%2520extensive%2520benchmarking%2520of%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520on%2520SCAM%252C%2520we%2520demonstrate%2520that%2520typographic%2520attacks%2520significantly%250Adegrade%2520performance%252C%2520and%2520identify%2520that%2520training%2520data%2520and%2520model%2520architecture%250Ainfluence%2520the%2520susceptibility%2520to%2520these%2520attacks.%2520Our%2520findings%2520reveal%2520that%250Atypographic%2520attacks%2520persist%2520in%2520state-of-the-art%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529%2520due%2520to%2520the%2520choice%2520of%2520their%2520vision%2520encoder%252C%2520though%2520larger%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520backbones%2520help%2520mitigate%2520their%2520vulnerability.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520synthetic%2520attacks%2520closely%2520resemble%2520real-world%2520%2528handwritten%2529%250Aattacks%252C%2520validating%2520their%2520use%2520in%2520research.%2520Our%2520work%2520provides%2520a%2520comprehensive%250Aresource%2520and%2520empirical%2520insights%2520to%2520facilitate%2520future%2520research%2520toward%2520robust%2520and%250Atrustworthy%2520multimodal%2520AI%2520systems.%2520We%2520publicly%2520release%2520the%2520datasets%2520introduced%250Ain%2520this%2520paper%2520along%2520with%2520the%2520code%2520for%2520evaluations%2520at%250Awww.bliss.berlin/research/scam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04893v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models&entry.906535625=Justus%20Westerhoff%20and%20Erblina%20Purelku%20and%20Jakob%20Hackstein%20and%20Jonas%20Loos%20and%20Leo%20Pinetzki%20and%20Lorenz%20Hufe&entry.1292438233=%20%20Typographic%20attacks%20exploit%20the%20interplay%20between%20text%20and%20visual%20content%20in%0Amultimodal%20foundation%20models%2C%20causing%20misclassifications%20when%20misleading%20text%0Ais%20embedded%20within%20images.%20However%2C%20existing%20datasets%20are%20limited%20in%20size%20and%0Adiversity%2C%20making%20it%20difficult%20to%20study%20such%20vulnerabilities.%20In%20this%20paper%2C%20we%0Aintroduce%20SCAM%2C%20the%20largest%20and%20most%20diverse%20dataset%20of%20real-world%20typographic%0Aattack%20images%20to%20date%2C%20containing%201%2C162%20images%20across%20hundreds%20of%20object%0Acategories%20and%20attack%20words.%20Through%20extensive%20benchmarking%20of%20Vision-Language%0AModels%20%28VLMs%29%20on%20SCAM%2C%20we%20demonstrate%20that%20typographic%20attacks%20significantly%0Adegrade%20performance%2C%20and%20identify%20that%20training%20data%20and%20model%20architecture%0Ainfluence%20the%20susceptibility%20to%20these%20attacks.%20Our%20findings%20reveal%20that%0Atypographic%20attacks%20persist%20in%20state-of-the-art%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20due%20to%20the%20choice%20of%20their%20vision%20encoder%2C%20though%20larger%20Large%20Language%0AModels%20%28LLMs%29%20backbones%20help%20mitigate%20their%20vulnerability.%20Additionally%2C%20we%0Ademonstrate%20that%20synthetic%20attacks%20closely%20resemble%20real-world%20%28handwritten%29%0Aattacks%2C%20validating%20their%20use%20in%20research.%20Our%20work%20provides%20a%20comprehensive%0Aresource%20and%20empirical%20insights%20to%20facilitate%20future%20research%20toward%20robust%20and%0Atrustworthy%20multimodal%20AI%20systems.%20We%20publicly%20release%20the%20datasets%20introduced%0Ain%20this%20paper%20along%20with%20the%20code%20for%20evaluations%20at%0Awww.bliss.berlin/research/scam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04893v4&entry.124074799=Read"},
{"title": "TwinTURBO: Semi-Supervised Fine-Tuning of Foundation Models via Mutual\n  Information Decompositions for Downstream Task and Latent Spaces", "author": "Guillaume Qu\u00e9tant and Pavlo Molchanov and Slava Voloshynovskiy", "abstract": "  We present a semi-supervised fine-tuning framework for foundation models that\nutilises mutual information decomposition to address the challenges of training\nfor a limited amount of labelled data. Our approach derives two distinct lower\nbounds: i) for the downstream task space, such as classification, optimised\nusing conditional and marginal cross-entropy alongside Kullback-Leibler\ndivergence, and ii) for the latent space representation, regularised and\naligned using a contrastive-like decomposition. This fine-tuning strategy\nretains the pre-trained structure of the foundation model, modifying only a\nspecialised projector module comprising a small transformer and a token\naggregation technique. Experiments on several datasets demonstrate significant\nimprovements in classification tasks under extremely low-labelled conditions by\neffectively leveraging unlabelled data.\n", "link": "http://arxiv.org/abs/2503.07851v2", "date": "2025-05-16", "relevancy": 2.7038, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5471}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5423}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TwinTURBO%3A%20Semi-Supervised%20Fine-Tuning%20of%20Foundation%20Models%20via%20Mutual%0A%20%20Information%20Decompositions%20for%20Downstream%20Task%20and%20Latent%20Spaces&body=Title%3A%20TwinTURBO%3A%20Semi-Supervised%20Fine-Tuning%20of%20Foundation%20Models%20via%20Mutual%0A%20%20Information%20Decompositions%20for%20Downstream%20Task%20and%20Latent%20Spaces%0AAuthor%3A%20Guillaume%20Qu%C3%A9tant%20and%20Pavlo%20Molchanov%20and%20Slava%20Voloshynovskiy%0AAbstract%3A%20%20%20We%20present%20a%20semi-supervised%20fine-tuning%20framework%20for%20foundation%20models%20that%0Autilises%20mutual%20information%20decomposition%20to%20address%20the%20challenges%20of%20training%0Afor%20a%20limited%20amount%20of%20labelled%20data.%20Our%20approach%20derives%20two%20distinct%20lower%0Abounds%3A%20i%29%20for%20the%20downstream%20task%20space%2C%20such%20as%20classification%2C%20optimised%0Ausing%20conditional%20and%20marginal%20cross-entropy%20alongside%20Kullback-Leibler%0Adivergence%2C%20and%20ii%29%20for%20the%20latent%20space%20representation%2C%20regularised%20and%0Aaligned%20using%20a%20contrastive-like%20decomposition.%20This%20fine-tuning%20strategy%0Aretains%20the%20pre-trained%20structure%20of%20the%20foundation%20model%2C%20modifying%20only%20a%0Aspecialised%20projector%20module%20comprising%20a%20small%20transformer%20and%20a%20token%0Aaggregation%20technique.%20Experiments%20on%20several%20datasets%20demonstrate%20significant%0Aimprovements%20in%20classification%20tasks%20under%20extremely%20low-labelled%20conditions%20by%0Aeffectively%20leveraging%20unlabelled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwinTURBO%253A%2520Semi-Supervised%2520Fine-Tuning%2520of%2520Foundation%2520Models%2520via%2520Mutual%250A%2520%2520Information%2520Decompositions%2520for%2520Downstream%2520Task%2520and%2520Latent%2520Spaces%26entry.906535625%3DGuillaume%2520Qu%25C3%25A9tant%2520and%2520Pavlo%2520Molchanov%2520and%2520Slava%2520Voloshynovskiy%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520semi-supervised%2520fine-tuning%2520framework%2520for%2520foundation%2520models%2520that%250Autilises%2520mutual%2520information%2520decomposition%2520to%2520address%2520the%2520challenges%2520of%2520training%250Afor%2520a%2520limited%2520amount%2520of%2520labelled%2520data.%2520Our%2520approach%2520derives%2520two%2520distinct%2520lower%250Abounds%253A%2520i%2529%2520for%2520the%2520downstream%2520task%2520space%252C%2520such%2520as%2520classification%252C%2520optimised%250Ausing%2520conditional%2520and%2520marginal%2520cross-entropy%2520alongside%2520Kullback-Leibler%250Adivergence%252C%2520and%2520ii%2529%2520for%2520the%2520latent%2520space%2520representation%252C%2520regularised%2520and%250Aaligned%2520using%2520a%2520contrastive-like%2520decomposition.%2520This%2520fine-tuning%2520strategy%250Aretains%2520the%2520pre-trained%2520structure%2520of%2520the%2520foundation%2520model%252C%2520modifying%2520only%2520a%250Aspecialised%2520projector%2520module%2520comprising%2520a%2520small%2520transformer%2520and%2520a%2520token%250Aaggregation%2520technique.%2520Experiments%2520on%2520several%2520datasets%2520demonstrate%2520significant%250Aimprovements%2520in%2520classification%2520tasks%2520under%2520extremely%2520low-labelled%2520conditions%2520by%250Aeffectively%2520leveraging%2520unlabelled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TwinTURBO%3A%20Semi-Supervised%20Fine-Tuning%20of%20Foundation%20Models%20via%20Mutual%0A%20%20Information%20Decompositions%20for%20Downstream%20Task%20and%20Latent%20Spaces&entry.906535625=Guillaume%20Qu%C3%A9tant%20and%20Pavlo%20Molchanov%20and%20Slava%20Voloshynovskiy&entry.1292438233=%20%20We%20present%20a%20semi-supervised%20fine-tuning%20framework%20for%20foundation%20models%20that%0Autilises%20mutual%20information%20decomposition%20to%20address%20the%20challenges%20of%20training%0Afor%20a%20limited%20amount%20of%20labelled%20data.%20Our%20approach%20derives%20two%20distinct%20lower%0Abounds%3A%20i%29%20for%20the%20downstream%20task%20space%2C%20such%20as%20classification%2C%20optimised%0Ausing%20conditional%20and%20marginal%20cross-entropy%20alongside%20Kullback-Leibler%0Adivergence%2C%20and%20ii%29%20for%20the%20latent%20space%20representation%2C%20regularised%20and%0Aaligned%20using%20a%20contrastive-like%20decomposition.%20This%20fine-tuning%20strategy%0Aretains%20the%20pre-trained%20structure%20of%20the%20foundation%20model%2C%20modifying%20only%20a%0Aspecialised%20projector%20module%20comprising%20a%20small%20transformer%20and%20a%20token%0Aaggregation%20technique.%20Experiments%20on%20several%20datasets%20demonstrate%20significant%0Aimprovements%20in%20classification%20tasks%20under%20extremely%20low-labelled%20conditions%20by%0Aeffectively%20leveraging%20unlabelled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07851v2&entry.124074799=Read"},
{"title": "TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding\n  Capabilities of MLLMs", "author": "Pengju Xu and Yan Wang and Shuyuan Zhang and Xuan Zhou and Xin Li and Yue Yuan and Fengzhao Li and Shunyuan Zhou and Xingyu Wang and Yi Zhang and Haiying Zhao", "abstract": "  Recent progress in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the ability of artificial intelligence systems to\nunderstand and generate multimodal content. However, these models often exhibit\nlimited effectiveness when applied to non-Western cultural contexts, which\nraises concerns about their wider applicability. To address this limitation, we\npropose the \\textbf{T}raditional \\textbf{C}hinese \\textbf{C}ulture\nunderstanding \\textbf{Bench}mark (\\textbf{TCC-Bench}), a bilingual\n(\\textit{i.e.}, Chinese and English) Visual Question Answering (VQA) benchmark\nspecifically designed for assessing the understanding of traditional Chinese\nculture by MLLMs. TCC-Bench comprises culturally rich and visually diverse\ndata, incorporating images from museum artifacts, everyday life scenes, comics,\nand other culturally significant contexts. We adopt a semi-automated pipeline\nthat utilizes GPT-4o in text-only mode to generate candidate questions,\nfollowed by human curation to ensure data quality and avoid potential data\nleakage. The benchmark also avoids language bias by preventing direct\ndisclosure of cultural concepts within question texts. Experimental evaluations\nacross a wide range of MLLMs demonstrate that current models still face\nsignificant challenges when reasoning about culturally grounded visual content.\nThe results highlight the need for further research in developing culturally\ninclusive and context-aware multimodal systems. The code and data can be found\nat: https://github.com/Morty-Xu/TCC-Bench.\n", "link": "http://arxiv.org/abs/2505.11275v1", "date": "2025-05-16", "relevancy": 2.6985, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TCC-Bench%3A%20Benchmarking%20the%20Traditional%20Chinese%20Culture%20Understanding%0A%20%20Capabilities%20of%20MLLMs&body=Title%3A%20TCC-Bench%3A%20Benchmarking%20the%20Traditional%20Chinese%20Culture%20Understanding%0A%20%20Capabilities%20of%20MLLMs%0AAuthor%3A%20Pengju%20Xu%20and%20Yan%20Wang%20and%20Shuyuan%20Zhang%20and%20Xuan%20Zhou%20and%20Xin%20Li%20and%20Yue%20Yuan%20and%20Fengzhao%20Li%20and%20Shunyuan%20Zhou%20and%20Xingyu%20Wang%20and%20Yi%20Zhang%20and%20Haiying%20Zhao%0AAbstract%3A%20%20%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Asignificantly%20enhanced%20the%20ability%20of%20artificial%20intelligence%20systems%20to%0Aunderstand%20and%20generate%20multimodal%20content.%20However%2C%20these%20models%20often%20exhibit%0Alimited%20effectiveness%20when%20applied%20to%20non-Western%20cultural%20contexts%2C%20which%0Araises%20concerns%20about%20their%20wider%20applicability.%20To%20address%20this%20limitation%2C%20we%0Apropose%20the%20%5Ctextbf%7BT%7Draditional%20%5Ctextbf%7BC%7Dhinese%20%5Ctextbf%7BC%7Dulture%0Aunderstanding%20%5Ctextbf%7BBench%7Dmark%20%28%5Ctextbf%7BTCC-Bench%7D%29%2C%20a%20bilingual%0A%28%5Ctextit%7Bi.e.%7D%2C%20Chinese%20and%20English%29%20Visual%20Question%20Answering%20%28VQA%29%20benchmark%0Aspecifically%20designed%20for%20assessing%20the%20understanding%20of%20traditional%20Chinese%0Aculture%20by%20MLLMs.%20TCC-Bench%20comprises%20culturally%20rich%20and%20visually%20diverse%0Adata%2C%20incorporating%20images%20from%20museum%20artifacts%2C%20everyday%20life%20scenes%2C%20comics%2C%0Aand%20other%20culturally%20significant%20contexts.%20We%20adopt%20a%20semi-automated%20pipeline%0Athat%20utilizes%20GPT-4o%20in%20text-only%20mode%20to%20generate%20candidate%20questions%2C%0Afollowed%20by%20human%20curation%20to%20ensure%20data%20quality%20and%20avoid%20potential%20data%0Aleakage.%20The%20benchmark%20also%20avoids%20language%20bias%20by%20preventing%20direct%0Adisclosure%20of%20cultural%20concepts%20within%20question%20texts.%20Experimental%20evaluations%0Aacross%20a%20wide%20range%20of%20MLLMs%20demonstrate%20that%20current%20models%20still%20face%0Asignificant%20challenges%20when%20reasoning%20about%20culturally%20grounded%20visual%20content.%0AThe%20results%20highlight%20the%20need%20for%20further%20research%20in%20developing%20culturally%0Ainclusive%20and%20context-aware%20multimodal%20systems.%20The%20code%20and%20data%20can%20be%20found%0Aat%3A%20https%3A//github.com/Morty-Xu/TCC-Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTCC-Bench%253A%2520Benchmarking%2520the%2520Traditional%2520Chinese%2520Culture%2520Understanding%250A%2520%2520Capabilities%2520of%2520MLLMs%26entry.906535625%3DPengju%2520Xu%2520and%2520Yan%2520Wang%2520and%2520Shuyuan%2520Zhang%2520and%2520Xuan%2520Zhou%2520and%2520Xin%2520Li%2520and%2520Yue%2520Yuan%2520and%2520Fengzhao%2520Li%2520and%2520Shunyuan%2520Zhou%2520and%2520Xingyu%2520Wang%2520and%2520Yi%2520Zhang%2520and%2520Haiying%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%250Asignificantly%2520enhanced%2520the%2520ability%2520of%2520artificial%2520intelligence%2520systems%2520to%250Aunderstand%2520and%2520generate%2520multimodal%2520content.%2520However%252C%2520these%2520models%2520often%2520exhibit%250Alimited%2520effectiveness%2520when%2520applied%2520to%2520non-Western%2520cultural%2520contexts%252C%2520which%250Araises%2520concerns%2520about%2520their%2520wider%2520applicability.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520the%2520%255Ctextbf%257BT%257Draditional%2520%255Ctextbf%257BC%257Dhinese%2520%255Ctextbf%257BC%257Dulture%250Aunderstanding%2520%255Ctextbf%257BBench%257Dmark%2520%2528%255Ctextbf%257BTCC-Bench%257D%2529%252C%2520a%2520bilingual%250A%2528%255Ctextit%257Bi.e.%257D%252C%2520Chinese%2520and%2520English%2529%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520benchmark%250Aspecifically%2520designed%2520for%2520assessing%2520the%2520understanding%2520of%2520traditional%2520Chinese%250Aculture%2520by%2520MLLMs.%2520TCC-Bench%2520comprises%2520culturally%2520rich%2520and%2520visually%2520diverse%250Adata%252C%2520incorporating%2520images%2520from%2520museum%2520artifacts%252C%2520everyday%2520life%2520scenes%252C%2520comics%252C%250Aand%2520other%2520culturally%2520significant%2520contexts.%2520We%2520adopt%2520a%2520semi-automated%2520pipeline%250Athat%2520utilizes%2520GPT-4o%2520in%2520text-only%2520mode%2520to%2520generate%2520candidate%2520questions%252C%250Afollowed%2520by%2520human%2520curation%2520to%2520ensure%2520data%2520quality%2520and%2520avoid%2520potential%2520data%250Aleakage.%2520The%2520benchmark%2520also%2520avoids%2520language%2520bias%2520by%2520preventing%2520direct%250Adisclosure%2520of%2520cultural%2520concepts%2520within%2520question%2520texts.%2520Experimental%2520evaluations%250Aacross%2520a%2520wide%2520range%2520of%2520MLLMs%2520demonstrate%2520that%2520current%2520models%2520still%2520face%250Asignificant%2520challenges%2520when%2520reasoning%2520about%2520culturally%2520grounded%2520visual%2520content.%250AThe%2520results%2520highlight%2520the%2520need%2520for%2520further%2520research%2520in%2520developing%2520culturally%250Ainclusive%2520and%2520context-aware%2520multimodal%2520systems.%2520The%2520code%2520and%2520data%2520can%2520be%2520found%250Aat%253A%2520https%253A//github.com/Morty-Xu/TCC-Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TCC-Bench%3A%20Benchmarking%20the%20Traditional%20Chinese%20Culture%20Understanding%0A%20%20Capabilities%20of%20MLLMs&entry.906535625=Pengju%20Xu%20and%20Yan%20Wang%20and%20Shuyuan%20Zhang%20and%20Xuan%20Zhou%20and%20Xin%20Li%20and%20Yue%20Yuan%20and%20Fengzhao%20Li%20and%20Shunyuan%20Zhou%20and%20Xingyu%20Wang%20and%20Yi%20Zhang%20and%20Haiying%20Zhao&entry.1292438233=%20%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Asignificantly%20enhanced%20the%20ability%20of%20artificial%20intelligence%20systems%20to%0Aunderstand%20and%20generate%20multimodal%20content.%20However%2C%20these%20models%20often%20exhibit%0Alimited%20effectiveness%20when%20applied%20to%20non-Western%20cultural%20contexts%2C%20which%0Araises%20concerns%20about%20their%20wider%20applicability.%20To%20address%20this%20limitation%2C%20we%0Apropose%20the%20%5Ctextbf%7BT%7Draditional%20%5Ctextbf%7BC%7Dhinese%20%5Ctextbf%7BC%7Dulture%0Aunderstanding%20%5Ctextbf%7BBench%7Dmark%20%28%5Ctextbf%7BTCC-Bench%7D%29%2C%20a%20bilingual%0A%28%5Ctextit%7Bi.e.%7D%2C%20Chinese%20and%20English%29%20Visual%20Question%20Answering%20%28VQA%29%20benchmark%0Aspecifically%20designed%20for%20assessing%20the%20understanding%20of%20traditional%20Chinese%0Aculture%20by%20MLLMs.%20TCC-Bench%20comprises%20culturally%20rich%20and%20visually%20diverse%0Adata%2C%20incorporating%20images%20from%20museum%20artifacts%2C%20everyday%20life%20scenes%2C%20comics%2C%0Aand%20other%20culturally%20significant%20contexts.%20We%20adopt%20a%20semi-automated%20pipeline%0Athat%20utilizes%20GPT-4o%20in%20text-only%20mode%20to%20generate%20candidate%20questions%2C%0Afollowed%20by%20human%20curation%20to%20ensure%20data%20quality%20and%20avoid%20potential%20data%0Aleakage.%20The%20benchmark%20also%20avoids%20language%20bias%20by%20preventing%20direct%0Adisclosure%20of%20cultural%20concepts%20within%20question%20texts.%20Experimental%20evaluations%0Aacross%20a%20wide%20range%20of%20MLLMs%20demonstrate%20that%20current%20models%20still%20face%0Asignificant%20challenges%20when%20reasoning%20about%20culturally%20grounded%20visual%20content.%0AThe%20results%20highlight%20the%20need%20for%20further%20research%20in%20developing%20culturally%0Ainclusive%20and%20context-aware%20multimodal%20systems.%20The%20code%20and%20data%20can%20be%20found%0Aat%3A%20https%3A//github.com/Morty-Xu/TCC-Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11275v1&entry.124074799=Read"},
{"title": "STRIDE: Sparse Techniques for Regression in Deep Gaussian Processes", "author": "Simon Urbainczyk and Aretha L. Teckentrup and Jonas Latz", "abstract": "  Gaussian processes (GPs) have gained popularity as flexible machine learning\nmodels for regression and function approximation with an in-built method for\nuncertainty quantification. However, GPs suffer when the amount of training\ndata is large or when the underlying function contains multi-scale features\nthat are difficult to represent by a stationary kernel. To address the former,\ntraining of GPs with large-scale data is often performed through inducing point\napproximations (also known as sparse GP regression (GPR)), where the size of\nthe covariance matrices in GPR is reduced considerably through a greedy search\non the data set. To aid the latter, deep GPs have gained traction as\nhierarchical models that resolve multi-scale features by combining multiple\nGPs. Posterior inference in deep GPs requires a sampling or, more usual, a\nvariational approximation. Variational approximations lead to large-scale\nstochastic, non-convex optimisation problems and the resulting approximation\ntends to represent uncertainty incorrectly. In this work, we combine\nvariational learning with MCMC to develop a particle-based\nexpectation-maximisation method to simultaneously find inducing points within\nthe large-scale data (variationally) and accurately train the GPs\n(sampling-based). The result is a highly efficient and accurate methodology for\ndeep GP training on large-scale data. We test our method on standard benchmark\nproblems.\n", "link": "http://arxiv.org/abs/2505.11355v1", "date": "2025-05-16", "relevancy": 2.6935, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5515}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5358}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STRIDE%3A%20Sparse%20Techniques%20for%20Regression%20in%20Deep%20Gaussian%20Processes&body=Title%3A%20STRIDE%3A%20Sparse%20Techniques%20for%20Regression%20in%20Deep%20Gaussian%20Processes%0AAuthor%3A%20Simon%20Urbainczyk%20and%20Aretha%20L.%20Teckentrup%20and%20Jonas%20Latz%0AAbstract%3A%20%20%20Gaussian%20processes%20%28GPs%29%20have%20gained%20popularity%20as%20flexible%20machine%20learning%0Amodels%20for%20regression%20and%20function%20approximation%20with%20an%20in-built%20method%20for%0Auncertainty%20quantification.%20However%2C%20GPs%20suffer%20when%20the%20amount%20of%20training%0Adata%20is%20large%20or%20when%20the%20underlying%20function%20contains%20multi-scale%20features%0Athat%20are%20difficult%20to%20represent%20by%20a%20stationary%20kernel.%20To%20address%20the%20former%2C%0Atraining%20of%20GPs%20with%20large-scale%20data%20is%20often%20performed%20through%20inducing%20point%0Aapproximations%20%28also%20known%20as%20sparse%20GP%20regression%20%28GPR%29%29%2C%20where%20the%20size%20of%0Athe%20covariance%20matrices%20in%20GPR%20is%20reduced%20considerably%20through%20a%20greedy%20search%0Aon%20the%20data%20set.%20To%20aid%20the%20latter%2C%20deep%20GPs%20have%20gained%20traction%20as%0Ahierarchical%20models%20that%20resolve%20multi-scale%20features%20by%20combining%20multiple%0AGPs.%20Posterior%20inference%20in%20deep%20GPs%20requires%20a%20sampling%20or%2C%20more%20usual%2C%20a%0Avariational%20approximation.%20Variational%20approximations%20lead%20to%20large-scale%0Astochastic%2C%20non-convex%20optimisation%20problems%20and%20the%20resulting%20approximation%0Atends%20to%20represent%20uncertainty%20incorrectly.%20In%20this%20work%2C%20we%20combine%0Avariational%20learning%20with%20MCMC%20to%20develop%20a%20particle-based%0Aexpectation-maximisation%20method%20to%20simultaneously%20find%20inducing%20points%20within%0Athe%20large-scale%20data%20%28variationally%29%20and%20accurately%20train%20the%20GPs%0A%28sampling-based%29.%20The%20result%20is%20a%20highly%20efficient%20and%20accurate%20methodology%20for%0Adeep%20GP%20training%20on%20large-scale%20data.%20We%20test%20our%20method%20on%20standard%20benchmark%0Aproblems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTRIDE%253A%2520Sparse%2520Techniques%2520for%2520Regression%2520in%2520Deep%2520Gaussian%2520Processes%26entry.906535625%3DSimon%2520Urbainczyk%2520and%2520Aretha%2520L.%2520Teckentrup%2520and%2520Jonas%2520Latz%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520%2528GPs%2529%2520have%2520gained%2520popularity%2520as%2520flexible%2520machine%2520learning%250Amodels%2520for%2520regression%2520and%2520function%2520approximation%2520with%2520an%2520in-built%2520method%2520for%250Auncertainty%2520quantification.%2520However%252C%2520GPs%2520suffer%2520when%2520the%2520amount%2520of%2520training%250Adata%2520is%2520large%2520or%2520when%2520the%2520underlying%2520function%2520contains%2520multi-scale%2520features%250Athat%2520are%2520difficult%2520to%2520represent%2520by%2520a%2520stationary%2520kernel.%2520To%2520address%2520the%2520former%252C%250Atraining%2520of%2520GPs%2520with%2520large-scale%2520data%2520is%2520often%2520performed%2520through%2520inducing%2520point%250Aapproximations%2520%2528also%2520known%2520as%2520sparse%2520GP%2520regression%2520%2528GPR%2529%2529%252C%2520where%2520the%2520size%2520of%250Athe%2520covariance%2520matrices%2520in%2520GPR%2520is%2520reduced%2520considerably%2520through%2520a%2520greedy%2520search%250Aon%2520the%2520data%2520set.%2520To%2520aid%2520the%2520latter%252C%2520deep%2520GPs%2520have%2520gained%2520traction%2520as%250Ahierarchical%2520models%2520that%2520resolve%2520multi-scale%2520features%2520by%2520combining%2520multiple%250AGPs.%2520Posterior%2520inference%2520in%2520deep%2520GPs%2520requires%2520a%2520sampling%2520or%252C%2520more%2520usual%252C%2520a%250Avariational%2520approximation.%2520Variational%2520approximations%2520lead%2520to%2520large-scale%250Astochastic%252C%2520non-convex%2520optimisation%2520problems%2520and%2520the%2520resulting%2520approximation%250Atends%2520to%2520represent%2520uncertainty%2520incorrectly.%2520In%2520this%2520work%252C%2520we%2520combine%250Avariational%2520learning%2520with%2520MCMC%2520to%2520develop%2520a%2520particle-based%250Aexpectation-maximisation%2520method%2520to%2520simultaneously%2520find%2520inducing%2520points%2520within%250Athe%2520large-scale%2520data%2520%2528variationally%2529%2520and%2520accurately%2520train%2520the%2520GPs%250A%2528sampling-based%2529.%2520The%2520result%2520is%2520a%2520highly%2520efficient%2520and%2520accurate%2520methodology%2520for%250Adeep%2520GP%2520training%2520on%2520large-scale%2520data.%2520We%2520test%2520our%2520method%2520on%2520standard%2520benchmark%250Aproblems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STRIDE%3A%20Sparse%20Techniques%20for%20Regression%20in%20Deep%20Gaussian%20Processes&entry.906535625=Simon%20Urbainczyk%20and%20Aretha%20L.%20Teckentrup%20and%20Jonas%20Latz&entry.1292438233=%20%20Gaussian%20processes%20%28GPs%29%20have%20gained%20popularity%20as%20flexible%20machine%20learning%0Amodels%20for%20regression%20and%20function%20approximation%20with%20an%20in-built%20method%20for%0Auncertainty%20quantification.%20However%2C%20GPs%20suffer%20when%20the%20amount%20of%20training%0Adata%20is%20large%20or%20when%20the%20underlying%20function%20contains%20multi-scale%20features%0Athat%20are%20difficult%20to%20represent%20by%20a%20stationary%20kernel.%20To%20address%20the%20former%2C%0Atraining%20of%20GPs%20with%20large-scale%20data%20is%20often%20performed%20through%20inducing%20point%0Aapproximations%20%28also%20known%20as%20sparse%20GP%20regression%20%28GPR%29%29%2C%20where%20the%20size%20of%0Athe%20covariance%20matrices%20in%20GPR%20is%20reduced%20considerably%20through%20a%20greedy%20search%0Aon%20the%20data%20set.%20To%20aid%20the%20latter%2C%20deep%20GPs%20have%20gained%20traction%20as%0Ahierarchical%20models%20that%20resolve%20multi-scale%20features%20by%20combining%20multiple%0AGPs.%20Posterior%20inference%20in%20deep%20GPs%20requires%20a%20sampling%20or%2C%20more%20usual%2C%20a%0Avariational%20approximation.%20Variational%20approximations%20lead%20to%20large-scale%0Astochastic%2C%20non-convex%20optimisation%20problems%20and%20the%20resulting%20approximation%0Atends%20to%20represent%20uncertainty%20incorrectly.%20In%20this%20work%2C%20we%20combine%0Avariational%20learning%20with%20MCMC%20to%20develop%20a%20particle-based%0Aexpectation-maximisation%20method%20to%20simultaneously%20find%20inducing%20points%20within%0Athe%20large-scale%20data%20%28variationally%29%20and%20accurately%20train%20the%20GPs%0A%28sampling-based%29.%20The%20result%20is%20a%20highly%20efficient%20and%20accurate%20methodology%20for%0Adeep%20GP%20training%20on%20large-scale%20data.%20We%20test%20our%20method%20on%20standard%20benchmark%0Aproblems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11355v1&entry.124074799=Read"},
{"title": "Redundancy-Aware Pretraining of Vision-Language Foundation Models in\n  Remote Sensing", "author": "Mathis J\u00fcrgen Adler and Leonard Hackel and Gencer Sumbul and Beg\u00fcm Demir", "abstract": "  The development of foundation models through pretraining of vision-language\nmodels (VLMs) has recently attracted great attention in remote sensing (RS).\nVLM pretraining aims to learn image and language alignments from a large number\nof image-text pairs. Each pretraining image is often associated with multiple\ncaptions containing redundant information due to repeated or semantically\nsimilar phrases, resulting in increased pretraining and inference time. To\novercome this, we introduce a weighted feature aggregation (WFA) strategy for\nVLM pretraining in RS. Our strategy aims to extract and exploit complementary\ninformation from multiple captions per image while reducing redundancies\nthrough feature aggregation with importance weighting. To calculate adaptive\nimportance weights for different captions of each image, we propose two\ntechniques: (i) non-parametric uniqueness and (ii) learning-based attention. In\nthe first technique, importance weights are calculated based on the bilingual\nevaluation understudy (BLEU) scores of the captions to emphasize unique\nsentences and reduce the influence of repetitive ones. In the second technique,\nimportance weights are learned through an attention mechanism instead of\nrelying on hand-crafted features. The effectiveness of the proposed WFA\nstrategy with the two techniques is analyzed in terms of downstream performance\non text-to-image retrieval in RS. Experimental results show that the proposed\nstrategy enables efficient and effective pretraining of VLMs in RS. Based on\nthe experimental analysis, we derive guidelines for selecting appropriate\ntechniques depending on downstream task requirements and resource constraints.\nThe code of this work is publicly available at\nhttps://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.\n", "link": "http://arxiv.org/abs/2505.11121v1", "date": "2025-05-16", "relevancy": 2.6681, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redundancy-Aware%20Pretraining%20of%20Vision-Language%20Foundation%20Models%20in%0A%20%20Remote%20Sensing&body=Title%3A%20Redundancy-Aware%20Pretraining%20of%20Vision-Language%20Foundation%20Models%20in%0A%20%20Remote%20Sensing%0AAuthor%3A%20Mathis%20J%C3%BCrgen%20Adler%20and%20Leonard%20Hackel%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20The%20development%20of%20foundation%20models%20through%20pretraining%20of%20vision-language%0Amodels%20%28VLMs%29%20has%20recently%20attracted%20great%20attention%20in%20remote%20sensing%20%28RS%29.%0AVLM%20pretraining%20aims%20to%20learn%20image%20and%20language%20alignments%20from%20a%20large%20number%0Aof%20image-text%20pairs.%20Each%20pretraining%20image%20is%20often%20associated%20with%20multiple%0Acaptions%20containing%20redundant%20information%20due%20to%20repeated%20or%20semantically%0Asimilar%20phrases%2C%20resulting%20in%20increased%20pretraining%20and%20inference%20time.%20To%0Aovercome%20this%2C%20we%20introduce%20a%20weighted%20feature%20aggregation%20%28WFA%29%20strategy%20for%0AVLM%20pretraining%20in%20RS.%20Our%20strategy%20aims%20to%20extract%20and%20exploit%20complementary%0Ainformation%20from%20multiple%20captions%20per%20image%20while%20reducing%20redundancies%0Athrough%20feature%20aggregation%20with%20importance%20weighting.%20To%20calculate%20adaptive%0Aimportance%20weights%20for%20different%20captions%20of%20each%20image%2C%20we%20propose%20two%0Atechniques%3A%20%28i%29%20non-parametric%20uniqueness%20and%20%28ii%29%20learning-based%20attention.%20In%0Athe%20first%20technique%2C%20importance%20weights%20are%20calculated%20based%20on%20the%20bilingual%0Aevaluation%20understudy%20%28BLEU%29%20scores%20of%20the%20captions%20to%20emphasize%20unique%0Asentences%20and%20reduce%20the%20influence%20of%20repetitive%20ones.%20In%20the%20second%20technique%2C%0Aimportance%20weights%20are%20learned%20through%20an%20attention%20mechanism%20instead%20of%0Arelying%20on%20hand-crafted%20features.%20The%20effectiveness%20of%20the%20proposed%20WFA%0Astrategy%20with%20the%20two%20techniques%20is%20analyzed%20in%20terms%20of%20downstream%20performance%0Aon%20text-to-image%20retrieval%20in%20RS.%20Experimental%20results%20show%20that%20the%20proposed%0Astrategy%20enables%20efficient%20and%20effective%20pretraining%20of%20VLMs%20in%20RS.%20Based%20on%0Athe%20experimental%20analysis%2C%20we%20derive%20guidelines%20for%20selecting%20appropriate%0Atechniques%20depending%20on%20downstream%20task%20requirements%20and%20resource%20constraints.%0AThe%20code%20of%20this%20work%20is%20publicly%20available%20at%0Ahttps%3A//git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedundancy-Aware%2520Pretraining%2520of%2520Vision-Language%2520Foundation%2520Models%2520in%250A%2520%2520Remote%2520Sensing%26entry.906535625%3DMathis%2520J%25C3%25BCrgen%2520Adler%2520and%2520Leonard%2520Hackel%2520and%2520Gencer%2520Sumbul%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520foundation%2520models%2520through%2520pretraining%2520of%2520vision-language%250Amodels%2520%2528VLMs%2529%2520has%2520recently%2520attracted%2520great%2520attention%2520in%2520remote%2520sensing%2520%2528RS%2529.%250AVLM%2520pretraining%2520aims%2520to%2520learn%2520image%2520and%2520language%2520alignments%2520from%2520a%2520large%2520number%250Aof%2520image-text%2520pairs.%2520Each%2520pretraining%2520image%2520is%2520often%2520associated%2520with%2520multiple%250Acaptions%2520containing%2520redundant%2520information%2520due%2520to%2520repeated%2520or%2520semantically%250Asimilar%2520phrases%252C%2520resulting%2520in%2520increased%2520pretraining%2520and%2520inference%2520time.%2520To%250Aovercome%2520this%252C%2520we%2520introduce%2520a%2520weighted%2520feature%2520aggregation%2520%2528WFA%2529%2520strategy%2520for%250AVLM%2520pretraining%2520in%2520RS.%2520Our%2520strategy%2520aims%2520to%2520extract%2520and%2520exploit%2520complementary%250Ainformation%2520from%2520multiple%2520captions%2520per%2520image%2520while%2520reducing%2520redundancies%250Athrough%2520feature%2520aggregation%2520with%2520importance%2520weighting.%2520To%2520calculate%2520adaptive%250Aimportance%2520weights%2520for%2520different%2520captions%2520of%2520each%2520image%252C%2520we%2520propose%2520two%250Atechniques%253A%2520%2528i%2529%2520non-parametric%2520uniqueness%2520and%2520%2528ii%2529%2520learning-based%2520attention.%2520In%250Athe%2520first%2520technique%252C%2520importance%2520weights%2520are%2520calculated%2520based%2520on%2520the%2520bilingual%250Aevaluation%2520understudy%2520%2528BLEU%2529%2520scores%2520of%2520the%2520captions%2520to%2520emphasize%2520unique%250Asentences%2520and%2520reduce%2520the%2520influence%2520of%2520repetitive%2520ones.%2520In%2520the%2520second%2520technique%252C%250Aimportance%2520weights%2520are%2520learned%2520through%2520an%2520attention%2520mechanism%2520instead%2520of%250Arelying%2520on%2520hand-crafted%2520features.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520WFA%250Astrategy%2520with%2520the%2520two%2520techniques%2520is%2520analyzed%2520in%2520terms%2520of%2520downstream%2520performance%250Aon%2520text-to-image%2520retrieval%2520in%2520RS.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%250Astrategy%2520enables%2520efficient%2520and%2520effective%2520pretraining%2520of%2520VLMs%2520in%2520RS.%2520Based%2520on%250Athe%2520experimental%2520analysis%252C%2520we%2520derive%2520guidelines%2520for%2520selecting%2520appropriate%250Atechniques%2520depending%2520on%2520downstream%2520task%2520requirements%2520and%2520resource%2520constraints.%250AThe%2520code%2520of%2520this%2520work%2520is%2520publicly%2520available%2520at%250Ahttps%253A//git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redundancy-Aware%20Pretraining%20of%20Vision-Language%20Foundation%20Models%20in%0A%20%20Remote%20Sensing&entry.906535625=Mathis%20J%C3%BCrgen%20Adler%20and%20Leonard%20Hackel%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20The%20development%20of%20foundation%20models%20through%20pretraining%20of%20vision-language%0Amodels%20%28VLMs%29%20has%20recently%20attracted%20great%20attention%20in%20remote%20sensing%20%28RS%29.%0AVLM%20pretraining%20aims%20to%20learn%20image%20and%20language%20alignments%20from%20a%20large%20number%0Aof%20image-text%20pairs.%20Each%20pretraining%20image%20is%20often%20associated%20with%20multiple%0Acaptions%20containing%20redundant%20information%20due%20to%20repeated%20or%20semantically%0Asimilar%20phrases%2C%20resulting%20in%20increased%20pretraining%20and%20inference%20time.%20To%0Aovercome%20this%2C%20we%20introduce%20a%20weighted%20feature%20aggregation%20%28WFA%29%20strategy%20for%0AVLM%20pretraining%20in%20RS.%20Our%20strategy%20aims%20to%20extract%20and%20exploit%20complementary%0Ainformation%20from%20multiple%20captions%20per%20image%20while%20reducing%20redundancies%0Athrough%20feature%20aggregation%20with%20importance%20weighting.%20To%20calculate%20adaptive%0Aimportance%20weights%20for%20different%20captions%20of%20each%20image%2C%20we%20propose%20two%0Atechniques%3A%20%28i%29%20non-parametric%20uniqueness%20and%20%28ii%29%20learning-based%20attention.%20In%0Athe%20first%20technique%2C%20importance%20weights%20are%20calculated%20based%20on%20the%20bilingual%0Aevaluation%20understudy%20%28BLEU%29%20scores%20of%20the%20captions%20to%20emphasize%20unique%0Asentences%20and%20reduce%20the%20influence%20of%20repetitive%20ones.%20In%20the%20second%20technique%2C%0Aimportance%20weights%20are%20learned%20through%20an%20attention%20mechanism%20instead%20of%0Arelying%20on%20hand-crafted%20features.%20The%20effectiveness%20of%20the%20proposed%20WFA%0Astrategy%20with%20the%20two%20techniques%20is%20analyzed%20in%20terms%20of%20downstream%20performance%0Aon%20text-to-image%20retrieval%20in%20RS.%20Experimental%20results%20show%20that%20the%20proposed%0Astrategy%20enables%20efficient%20and%20effective%20pretraining%20of%20VLMs%20in%20RS.%20Based%20on%0Athe%20experimental%20analysis%2C%20we%20derive%20guidelines%20for%20selecting%20appropriate%0Atechniques%20depending%20on%20downstream%20task%20requirements%20and%20resource%20constraints.%0AThe%20code%20of%20this%20work%20is%20publicly%20available%20at%0Ahttps%3A//git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11121v1&entry.124074799=Read"},
{"title": "Autoencoder-Based Hybrid Replay for Class-Incremental Learning", "author": "Milad Khademi Nori and Il-Min Kim and Guanghui Wang", "abstract": "  In class-incremental learning (CIL), effective incremental learning\nstrategies are essential to mitigate task confusion and catastrophic\nforgetting, especially as the number of tasks $t$ increases. Current exemplar\nreplay strategies impose $\\mathcal{O}(t)$ memory/compute complexities. We\npropose an autoencoder-based hybrid replay (AHR) strategy that leverages our\nnew hybrid autoencoder (HAE) to function as a compressor to alleviate the\nrequirement for large memory, achieving $\\mathcal{O}(0.1 t)$ at the worst case\nwith the computing complexity of $\\mathcal{O}(t)$ while accomplishing\nstate-of-the-art performance. The decoder later recovers the exemplar data\nstored in the latent space, rather than in raw format. Additionally, HAE is\ndesigned for both discriminative and generative modeling, enabling\nclassification and replay capabilities, respectively. HAE adopts the charged\nparticle system energy minimization equations and repulsive force algorithm for\nthe incremental embedding and distribution of new class centroids in its latent\nspace. Our results demonstrate that AHR consistently outperforms recent\nbaselines across multiple benchmarks while operating with the same\nmemory/compute budgets. The source code is included in the supplementary\nmaterial and will be open-sourced upon publication.\n", "link": "http://arxiv.org/abs/2505.05926v3", "date": "2025-05-16", "relevancy": 2.6485, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5836}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5167}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoencoder-Based%20Hybrid%20Replay%20for%20Class-Incremental%20Learning&body=Title%3A%20Autoencoder-Based%20Hybrid%20Replay%20for%20Class-Incremental%20Learning%0AAuthor%3A%20Milad%20Khademi%20Nori%20and%20Il-Min%20Kim%20and%20Guanghui%20Wang%0AAbstract%3A%20%20%20In%20class-incremental%20learning%20%28CIL%29%2C%20effective%20incremental%20learning%0Astrategies%20are%20essential%20to%20mitigate%20task%20confusion%20and%20catastrophic%0Aforgetting%2C%20especially%20as%20the%20number%20of%20tasks%20%24t%24%20increases.%20Current%20exemplar%0Areplay%20strategies%20impose%20%24%5Cmathcal%7BO%7D%28t%29%24%20memory/compute%20complexities.%20We%0Apropose%20an%20autoencoder-based%20hybrid%20replay%20%28AHR%29%20strategy%20that%20leverages%20our%0Anew%20hybrid%20autoencoder%20%28HAE%29%20to%20function%20as%20a%20compressor%20to%20alleviate%20the%0Arequirement%20for%20large%20memory%2C%20achieving%20%24%5Cmathcal%7BO%7D%280.1%20t%29%24%20at%20the%20worst%20case%0Awith%20the%20computing%20complexity%20of%20%24%5Cmathcal%7BO%7D%28t%29%24%20while%20accomplishing%0Astate-of-the-art%20performance.%20The%20decoder%20later%20recovers%20the%20exemplar%20data%0Astored%20in%20the%20latent%20space%2C%20rather%20than%20in%20raw%20format.%20Additionally%2C%20HAE%20is%0Adesigned%20for%20both%20discriminative%20and%20generative%20modeling%2C%20enabling%0Aclassification%20and%20replay%20capabilities%2C%20respectively.%20HAE%20adopts%20the%20charged%0Aparticle%20system%20energy%20minimization%20equations%20and%20repulsive%20force%20algorithm%20for%0Athe%20incremental%20embedding%20and%20distribution%20of%20new%20class%20centroids%20in%20its%20latent%0Aspace.%20Our%20results%20demonstrate%20that%20AHR%20consistently%20outperforms%20recent%0Abaselines%20across%20multiple%20benchmarks%20while%20operating%20with%20the%20same%0Amemory/compute%20budgets.%20The%20source%20code%20is%20included%20in%20the%20supplementary%0Amaterial%20and%20will%20be%20open-sourced%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05926v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoencoder-Based%2520Hybrid%2520Replay%2520for%2520Class-Incremental%2520Learning%26entry.906535625%3DMilad%2520Khademi%2520Nori%2520and%2520Il-Min%2520Kim%2520and%2520Guanghui%2520Wang%26entry.1292438233%3D%2520%2520In%2520class-incremental%2520learning%2520%2528CIL%2529%252C%2520effective%2520incremental%2520learning%250Astrategies%2520are%2520essential%2520to%2520mitigate%2520task%2520confusion%2520and%2520catastrophic%250Aforgetting%252C%2520especially%2520as%2520the%2520number%2520of%2520tasks%2520%2524t%2524%2520increases.%2520Current%2520exemplar%250Areplay%2520strategies%2520impose%2520%2524%255Cmathcal%257BO%257D%2528t%2529%2524%2520memory/compute%2520complexities.%2520We%250Apropose%2520an%2520autoencoder-based%2520hybrid%2520replay%2520%2528AHR%2529%2520strategy%2520that%2520leverages%2520our%250Anew%2520hybrid%2520autoencoder%2520%2528HAE%2529%2520to%2520function%2520as%2520a%2520compressor%2520to%2520alleviate%2520the%250Arequirement%2520for%2520large%2520memory%252C%2520achieving%2520%2524%255Cmathcal%257BO%257D%25280.1%2520t%2529%2524%2520at%2520the%2520worst%2520case%250Awith%2520the%2520computing%2520complexity%2520of%2520%2524%255Cmathcal%257BO%257D%2528t%2529%2524%2520while%2520accomplishing%250Astate-of-the-art%2520performance.%2520The%2520decoder%2520later%2520recovers%2520the%2520exemplar%2520data%250Astored%2520in%2520the%2520latent%2520space%252C%2520rather%2520than%2520in%2520raw%2520format.%2520Additionally%252C%2520HAE%2520is%250Adesigned%2520for%2520both%2520discriminative%2520and%2520generative%2520modeling%252C%2520enabling%250Aclassification%2520and%2520replay%2520capabilities%252C%2520respectively.%2520HAE%2520adopts%2520the%2520charged%250Aparticle%2520system%2520energy%2520minimization%2520equations%2520and%2520repulsive%2520force%2520algorithm%2520for%250Athe%2520incremental%2520embedding%2520and%2520distribution%2520of%2520new%2520class%2520centroids%2520in%2520its%2520latent%250Aspace.%2520Our%2520results%2520demonstrate%2520that%2520AHR%2520consistently%2520outperforms%2520recent%250Abaselines%2520across%2520multiple%2520benchmarks%2520while%2520operating%2520with%2520the%2520same%250Amemory/compute%2520budgets.%2520The%2520source%2520code%2520is%2520included%2520in%2520the%2520supplementary%250Amaterial%2520and%2520will%2520be%2520open-sourced%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05926v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoencoder-Based%20Hybrid%20Replay%20for%20Class-Incremental%20Learning&entry.906535625=Milad%20Khademi%20Nori%20and%20Il-Min%20Kim%20and%20Guanghui%20Wang&entry.1292438233=%20%20In%20class-incremental%20learning%20%28CIL%29%2C%20effective%20incremental%20learning%0Astrategies%20are%20essential%20to%20mitigate%20task%20confusion%20and%20catastrophic%0Aforgetting%2C%20especially%20as%20the%20number%20of%20tasks%20%24t%24%20increases.%20Current%20exemplar%0Areplay%20strategies%20impose%20%24%5Cmathcal%7BO%7D%28t%29%24%20memory/compute%20complexities.%20We%0Apropose%20an%20autoencoder-based%20hybrid%20replay%20%28AHR%29%20strategy%20that%20leverages%20our%0Anew%20hybrid%20autoencoder%20%28HAE%29%20to%20function%20as%20a%20compressor%20to%20alleviate%20the%0Arequirement%20for%20large%20memory%2C%20achieving%20%24%5Cmathcal%7BO%7D%280.1%20t%29%24%20at%20the%20worst%20case%0Awith%20the%20computing%20complexity%20of%20%24%5Cmathcal%7BO%7D%28t%29%24%20while%20accomplishing%0Astate-of-the-art%20performance.%20The%20decoder%20later%20recovers%20the%20exemplar%20data%0Astored%20in%20the%20latent%20space%2C%20rather%20than%20in%20raw%20format.%20Additionally%2C%20HAE%20is%0Adesigned%20for%20both%20discriminative%20and%20generative%20modeling%2C%20enabling%0Aclassification%20and%20replay%20capabilities%2C%20respectively.%20HAE%20adopts%20the%20charged%0Aparticle%20system%20energy%20minimization%20equations%20and%20repulsive%20force%20algorithm%20for%0Athe%20incremental%20embedding%20and%20distribution%20of%20new%20class%20centroids%20in%20its%20latent%0Aspace.%20Our%20results%20demonstrate%20that%20AHR%20consistently%20outperforms%20recent%0Abaselines%20across%20multiple%20benchmarks%20while%20operating%20with%20the%20same%0Amemory/compute%20budgets.%20The%20source%20code%20is%20included%20in%20the%20supplementary%0Amaterial%20and%20will%20be%20open-sourced%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05926v3&entry.124074799=Read"},
{"title": "A Review on Discriminative Self-supervised Learning Methods in Computer\n  Vision", "author": "Nikolaos Giakoumoglou and Tania Stathaki and Athanasios Gkelias", "abstract": "  Self-supervised learning (SSL) has rapidly emerged as a transformative\napproach in computer vision, enabling the extraction of rich feature\nrepresentations from vast amounts of unlabeled data and reducing reliance on\ncostly manual annotations. This review presents a comprehensive analysis of\ndiscriminative SSL methods, which focus on learning representations by solving\npretext tasks that do not require human labels. The paper systematically\ncategorizes discriminative SSL approaches into five main groups: contrastive\nmethods, clustering methods, self-distillation methods, knowledge distillation\nmethods, and feature decorrelation methods. For each category, the review\ndetails the underlying principles, architectural components, loss functions,\nand representative algorithms, highlighting their unique mechanisms and\ncontributions to the field. Extensive comparative evaluations are provided,\nincluding linear and semi-supervised protocols on standard benchmarks such as\nImageNet, as well as transfer learning performance across diverse downstream\ntasks. The review also discusses theoretical foundations, scalability,\nefficiency, and practical challenges, such as computational demands and\naccessibility. By synthesizing recent advancements and identifying key trends,\nopen challenges, and future research directions, this work serves as a valuable\nresource for researchers and practitioners aiming to leverage discriminative\nSSL for robust and generalizable computer vision models.\n", "link": "http://arxiv.org/abs/2405.04969v2", "date": "2025-05-16", "relevancy": 2.6421, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20on%20Discriminative%20Self-supervised%20Learning%20Methods%20in%20Computer%0A%20%20Vision&body=Title%3A%20A%20Review%20on%20Discriminative%20Self-supervised%20Learning%20Methods%20in%20Computer%0A%20%20Vision%0AAuthor%3A%20Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%20and%20Athanasios%20Gkelias%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20rapidly%20emerged%20as%20a%20transformative%0Aapproach%20in%20computer%20vision%2C%20enabling%20the%20extraction%20of%20rich%20feature%0Arepresentations%20from%20vast%20amounts%20of%20unlabeled%20data%20and%20reducing%20reliance%20on%0Acostly%20manual%20annotations.%20This%20review%20presents%20a%20comprehensive%20analysis%20of%0Adiscriminative%20SSL%20methods%2C%20which%20focus%20on%20learning%20representations%20by%20solving%0Apretext%20tasks%20that%20do%20not%20require%20human%20labels.%20The%20paper%20systematically%0Acategorizes%20discriminative%20SSL%20approaches%20into%20five%20main%20groups%3A%20contrastive%0Amethods%2C%20clustering%20methods%2C%20self-distillation%20methods%2C%20knowledge%20distillation%0Amethods%2C%20and%20feature%20decorrelation%20methods.%20For%20each%20category%2C%20the%20review%0Adetails%20the%20underlying%20principles%2C%20architectural%20components%2C%20loss%20functions%2C%0Aand%20representative%20algorithms%2C%20highlighting%20their%20unique%20mechanisms%20and%0Acontributions%20to%20the%20field.%20Extensive%20comparative%20evaluations%20are%20provided%2C%0Aincluding%20linear%20and%20semi-supervised%20protocols%20on%20standard%20benchmarks%20such%20as%0AImageNet%2C%20as%20well%20as%20transfer%20learning%20performance%20across%20diverse%20downstream%0Atasks.%20The%20review%20also%20discusses%20theoretical%20foundations%2C%20scalability%2C%0Aefficiency%2C%20and%20practical%20challenges%2C%20such%20as%20computational%20demands%20and%0Aaccessibility.%20By%20synthesizing%20recent%20advancements%20and%20identifying%20key%20trends%2C%0Aopen%20challenges%2C%20and%20future%20research%20directions%2C%20this%20work%20serves%20as%20a%20valuable%0Aresource%20for%20researchers%20and%20practitioners%20aiming%20to%20leverage%20discriminative%0ASSL%20for%20robust%20and%20generalizable%20computer%20vision%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520on%2520Discriminative%2520Self-supervised%2520Learning%2520Methods%2520in%2520Computer%250A%2520%2520Vision%26entry.906535625%3DNikolaos%2520Giakoumoglou%2520and%2520Tania%2520Stathaki%2520and%2520Athanasios%2520Gkelias%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520rapidly%2520emerged%2520as%2520a%2520transformative%250Aapproach%2520in%2520computer%2520vision%252C%2520enabling%2520the%2520extraction%2520of%2520rich%2520feature%250Arepresentations%2520from%2520vast%2520amounts%2520of%2520unlabeled%2520data%2520and%2520reducing%2520reliance%2520on%250Acostly%2520manual%2520annotations.%2520This%2520review%2520presents%2520a%2520comprehensive%2520analysis%2520of%250Adiscriminative%2520SSL%2520methods%252C%2520which%2520focus%2520on%2520learning%2520representations%2520by%2520solving%250Apretext%2520tasks%2520that%2520do%2520not%2520require%2520human%2520labels.%2520The%2520paper%2520systematically%250Acategorizes%2520discriminative%2520SSL%2520approaches%2520into%2520five%2520main%2520groups%253A%2520contrastive%250Amethods%252C%2520clustering%2520methods%252C%2520self-distillation%2520methods%252C%2520knowledge%2520distillation%250Amethods%252C%2520and%2520feature%2520decorrelation%2520methods.%2520For%2520each%2520category%252C%2520the%2520review%250Adetails%2520the%2520underlying%2520principles%252C%2520architectural%2520components%252C%2520loss%2520functions%252C%250Aand%2520representative%2520algorithms%252C%2520highlighting%2520their%2520unique%2520mechanisms%2520and%250Acontributions%2520to%2520the%2520field.%2520Extensive%2520comparative%2520evaluations%2520are%2520provided%252C%250Aincluding%2520linear%2520and%2520semi-supervised%2520protocols%2520on%2520standard%2520benchmarks%2520such%2520as%250AImageNet%252C%2520as%2520well%2520as%2520transfer%2520learning%2520performance%2520across%2520diverse%2520downstream%250Atasks.%2520The%2520review%2520also%2520discusses%2520theoretical%2520foundations%252C%2520scalability%252C%250Aefficiency%252C%2520and%2520practical%2520challenges%252C%2520such%2520as%2520computational%2520demands%2520and%250Aaccessibility.%2520By%2520synthesizing%2520recent%2520advancements%2520and%2520identifying%2520key%2520trends%252C%250Aopen%2520challenges%252C%2520and%2520future%2520research%2520directions%252C%2520this%2520work%2520serves%2520as%2520a%2520valuable%250Aresource%2520for%2520researchers%2520and%2520practitioners%2520aiming%2520to%2520leverage%2520discriminative%250ASSL%2520for%2520robust%2520and%2520generalizable%2520computer%2520vision%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20on%20Discriminative%20Self-supervised%20Learning%20Methods%20in%20Computer%0A%20%20Vision&entry.906535625=Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%20and%20Athanasios%20Gkelias&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20rapidly%20emerged%20as%20a%20transformative%0Aapproach%20in%20computer%20vision%2C%20enabling%20the%20extraction%20of%20rich%20feature%0Arepresentations%20from%20vast%20amounts%20of%20unlabeled%20data%20and%20reducing%20reliance%20on%0Acostly%20manual%20annotations.%20This%20review%20presents%20a%20comprehensive%20analysis%20of%0Adiscriminative%20SSL%20methods%2C%20which%20focus%20on%20learning%20representations%20by%20solving%0Apretext%20tasks%20that%20do%20not%20require%20human%20labels.%20The%20paper%20systematically%0Acategorizes%20discriminative%20SSL%20approaches%20into%20five%20main%20groups%3A%20contrastive%0Amethods%2C%20clustering%20methods%2C%20self-distillation%20methods%2C%20knowledge%20distillation%0Amethods%2C%20and%20feature%20decorrelation%20methods.%20For%20each%20category%2C%20the%20review%0Adetails%20the%20underlying%20principles%2C%20architectural%20components%2C%20loss%20functions%2C%0Aand%20representative%20algorithms%2C%20highlighting%20their%20unique%20mechanisms%20and%0Acontributions%20to%20the%20field.%20Extensive%20comparative%20evaluations%20are%20provided%2C%0Aincluding%20linear%20and%20semi-supervised%20protocols%20on%20standard%20benchmarks%20such%20as%0AImageNet%2C%20as%20well%20as%20transfer%20learning%20performance%20across%20diverse%20downstream%0Atasks.%20The%20review%20also%20discusses%20theoretical%20foundations%2C%20scalability%2C%0Aefficiency%2C%20and%20practical%20challenges%2C%20such%20as%20computational%20demands%20and%0Aaccessibility.%20By%20synthesizing%20recent%20advancements%20and%20identifying%20key%20trends%2C%0Aopen%20challenges%2C%20and%20future%20research%20directions%2C%20this%20work%20serves%20as%20a%20valuable%0Aresource%20for%20researchers%20and%20practitioners%20aiming%20to%20leverage%20discriminative%0ASSL%20for%20robust%20and%20generalizable%20computer%20vision%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04969v2&entry.124074799=Read"},
{"title": "From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl\n  Violet Staining From 3D Polarized Light Imaging", "author": "Alexander Oberstrass and Esteban Vaca and Eric Upschulte and Meiqi Niu and Nicola Palomero-Gallagher and David Graessel and Christian Schiffer and Markus Axer and Katrin Amunts and Timo Dickscheid", "abstract": "  Comprehensive assessment of the various aspects of the brain's microstructure\nrequires the use of complementary imaging techniques. This includes measuring\nthe spatial distribution of cell bodies (cytoarchitecture) and nerve fibers\n(myeloarchitecture). The gold standard for cytoarchitectonic analysis is light\nmicroscopic imaging of cell-body stained tissue sections. To reveal the 3D\norientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been\nintroduced as a reliable technique providing a resolution in the micrometer\nrange while allowing processing of series of complete brain sections. 3D-PLI\nacquisition is label-free and allows subsequent staining of sections after\nmeasurement. By post-staining for cell bodies, a direct link between fiber- and\ncytoarchitecture can potentially be established within the same section.\nHowever, inevitable distortions introduced during the staining process make a\nnonlinear and cross-modal registration necessary in order to study the detailed\nrelationships between cells and fibers in the images. In addition, the\ncomplexity of processing histological sections for post-staining only allows\nfor a limited number of samples. In this work, we take advantage of deep\nlearning methods for image-to-image translation to generate a virtual staining\nof 3D-PLI that is spatially aligned at the cellular level. In a supervised\nsetting, we build on a unique dataset of brain sections, to which Cresyl violet\nstaining has been applied after 3D-PLI measurement. To ensure high\ncorrespondence between both modalities, we address the misalignment of training\ndata using Fourier-based registration methods. In this way, registration can be\nefficiently calculated during training for local image patches of target and\npredicted staining. We demonstrate that the proposed method enables prediction\nof a Cresyl violet staining from 3D-PLI, matching individual cell instances.\n", "link": "http://arxiv.org/abs/2505.11394v1", "date": "2025-05-16", "relevancy": 2.5966, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5219}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Fibers%20to%20Cells%3A%20Fourier-Based%20Registration%20Enables%20Virtual%20Cresyl%0A%20%20Violet%20Staining%20From%203D%20Polarized%20Light%20Imaging&body=Title%3A%20From%20Fibers%20to%20Cells%3A%20Fourier-Based%20Registration%20Enables%20Virtual%20Cresyl%0A%20%20Violet%20Staining%20From%203D%20Polarized%20Light%20Imaging%0AAuthor%3A%20Alexander%20Oberstrass%20and%20Esteban%20Vaca%20and%20Eric%20Upschulte%20and%20Meiqi%20Niu%20and%20Nicola%20Palomero-Gallagher%20and%20David%20Graessel%20and%20Christian%20Schiffer%20and%20Markus%20Axer%20and%20Katrin%20Amunts%20and%20Timo%20Dickscheid%0AAbstract%3A%20%20%20Comprehensive%20assessment%20of%20the%20various%20aspects%20of%20the%20brain%27s%20microstructure%0Arequires%20the%20use%20of%20complementary%20imaging%20techniques.%20This%20includes%20measuring%0Athe%20spatial%20distribution%20of%20cell%20bodies%20%28cytoarchitecture%29%20and%20nerve%20fibers%0A%28myeloarchitecture%29.%20The%20gold%20standard%20for%20cytoarchitectonic%20analysis%20is%20light%0Amicroscopic%20imaging%20of%20cell-body%20stained%20tissue%20sections.%20To%20reveal%20the%203D%0Aorientations%20of%20nerve%20fibers%2C%203D%20Polarized%20Light%20Imaging%20%283D-PLI%29%20has%20been%0Aintroduced%20as%20a%20reliable%20technique%20providing%20a%20resolution%20in%20the%20micrometer%0Arange%20while%20allowing%20processing%20of%20series%20of%20complete%20brain%20sections.%203D-PLI%0Aacquisition%20is%20label-free%20and%20allows%20subsequent%20staining%20of%20sections%20after%0Ameasurement.%20By%20post-staining%20for%20cell%20bodies%2C%20a%20direct%20link%20between%20fiber-%20and%0Acytoarchitecture%20can%20potentially%20be%20established%20within%20the%20same%20section.%0AHowever%2C%20inevitable%20distortions%20introduced%20during%20the%20staining%20process%20make%20a%0Anonlinear%20and%20cross-modal%20registration%20necessary%20in%20order%20to%20study%20the%20detailed%0Arelationships%20between%20cells%20and%20fibers%20in%20the%20images.%20In%20addition%2C%20the%0Acomplexity%20of%20processing%20histological%20sections%20for%20post-staining%20only%20allows%0Afor%20a%20limited%20number%20of%20samples.%20In%20this%20work%2C%20we%20take%20advantage%20of%20deep%0Alearning%20methods%20for%20image-to-image%20translation%20to%20generate%20a%20virtual%20staining%0Aof%203D-PLI%20that%20is%20spatially%20aligned%20at%20the%20cellular%20level.%20In%20a%20supervised%0Asetting%2C%20we%20build%20on%20a%20unique%20dataset%20of%20brain%20sections%2C%20to%20which%20Cresyl%20violet%0Astaining%20has%20been%20applied%20after%203D-PLI%20measurement.%20To%20ensure%20high%0Acorrespondence%20between%20both%20modalities%2C%20we%20address%20the%20misalignment%20of%20training%0Adata%20using%20Fourier-based%20registration%20methods.%20In%20this%20way%2C%20registration%20can%20be%0Aefficiently%20calculated%20during%20training%20for%20local%20image%20patches%20of%20target%20and%0Apredicted%20staining.%20We%20demonstrate%20that%20the%20proposed%20method%20enables%20prediction%0Aof%20a%20Cresyl%20violet%20staining%20from%203D-PLI%2C%20matching%20individual%20cell%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Fibers%2520to%2520Cells%253A%2520Fourier-Based%2520Registration%2520Enables%2520Virtual%2520Cresyl%250A%2520%2520Violet%2520Staining%2520From%25203D%2520Polarized%2520Light%2520Imaging%26entry.906535625%3DAlexander%2520Oberstrass%2520and%2520Esteban%2520Vaca%2520and%2520Eric%2520Upschulte%2520and%2520Meiqi%2520Niu%2520and%2520Nicola%2520Palomero-Gallagher%2520and%2520David%2520Graessel%2520and%2520Christian%2520Schiffer%2520and%2520Markus%2520Axer%2520and%2520Katrin%2520Amunts%2520and%2520Timo%2520Dickscheid%26entry.1292438233%3D%2520%2520Comprehensive%2520assessment%2520of%2520the%2520various%2520aspects%2520of%2520the%2520brain%2527s%2520microstructure%250Arequires%2520the%2520use%2520of%2520complementary%2520imaging%2520techniques.%2520This%2520includes%2520measuring%250Athe%2520spatial%2520distribution%2520of%2520cell%2520bodies%2520%2528cytoarchitecture%2529%2520and%2520nerve%2520fibers%250A%2528myeloarchitecture%2529.%2520The%2520gold%2520standard%2520for%2520cytoarchitectonic%2520analysis%2520is%2520light%250Amicroscopic%2520imaging%2520of%2520cell-body%2520stained%2520tissue%2520sections.%2520To%2520reveal%2520the%25203D%250Aorientations%2520of%2520nerve%2520fibers%252C%25203D%2520Polarized%2520Light%2520Imaging%2520%25283D-PLI%2529%2520has%2520been%250Aintroduced%2520as%2520a%2520reliable%2520technique%2520providing%2520a%2520resolution%2520in%2520the%2520micrometer%250Arange%2520while%2520allowing%2520processing%2520of%2520series%2520of%2520complete%2520brain%2520sections.%25203D-PLI%250Aacquisition%2520is%2520label-free%2520and%2520allows%2520subsequent%2520staining%2520of%2520sections%2520after%250Ameasurement.%2520By%2520post-staining%2520for%2520cell%2520bodies%252C%2520a%2520direct%2520link%2520between%2520fiber-%2520and%250Acytoarchitecture%2520can%2520potentially%2520be%2520established%2520within%2520the%2520same%2520section.%250AHowever%252C%2520inevitable%2520distortions%2520introduced%2520during%2520the%2520staining%2520process%2520make%2520a%250Anonlinear%2520and%2520cross-modal%2520registration%2520necessary%2520in%2520order%2520to%2520study%2520the%2520detailed%250Arelationships%2520between%2520cells%2520and%2520fibers%2520in%2520the%2520images.%2520In%2520addition%252C%2520the%250Acomplexity%2520of%2520processing%2520histological%2520sections%2520for%2520post-staining%2520only%2520allows%250Afor%2520a%2520limited%2520number%2520of%2520samples.%2520In%2520this%2520work%252C%2520we%2520take%2520advantage%2520of%2520deep%250Alearning%2520methods%2520for%2520image-to-image%2520translation%2520to%2520generate%2520a%2520virtual%2520staining%250Aof%25203D-PLI%2520that%2520is%2520spatially%2520aligned%2520at%2520the%2520cellular%2520level.%2520In%2520a%2520supervised%250Asetting%252C%2520we%2520build%2520on%2520a%2520unique%2520dataset%2520of%2520brain%2520sections%252C%2520to%2520which%2520Cresyl%2520violet%250Astaining%2520has%2520been%2520applied%2520after%25203D-PLI%2520measurement.%2520To%2520ensure%2520high%250Acorrespondence%2520between%2520both%2520modalities%252C%2520we%2520address%2520the%2520misalignment%2520of%2520training%250Adata%2520using%2520Fourier-based%2520registration%2520methods.%2520In%2520this%2520way%252C%2520registration%2520can%2520be%250Aefficiently%2520calculated%2520during%2520training%2520for%2520local%2520image%2520patches%2520of%2520target%2520and%250Apredicted%2520staining.%2520We%2520demonstrate%2520that%2520the%2520proposed%2520method%2520enables%2520prediction%250Aof%2520a%2520Cresyl%2520violet%2520staining%2520from%25203D-PLI%252C%2520matching%2520individual%2520cell%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Fibers%20to%20Cells%3A%20Fourier-Based%20Registration%20Enables%20Virtual%20Cresyl%0A%20%20Violet%20Staining%20From%203D%20Polarized%20Light%20Imaging&entry.906535625=Alexander%20Oberstrass%20and%20Esteban%20Vaca%20and%20Eric%20Upschulte%20and%20Meiqi%20Niu%20and%20Nicola%20Palomero-Gallagher%20and%20David%20Graessel%20and%20Christian%20Schiffer%20and%20Markus%20Axer%20and%20Katrin%20Amunts%20and%20Timo%20Dickscheid&entry.1292438233=%20%20Comprehensive%20assessment%20of%20the%20various%20aspects%20of%20the%20brain%27s%20microstructure%0Arequires%20the%20use%20of%20complementary%20imaging%20techniques.%20This%20includes%20measuring%0Athe%20spatial%20distribution%20of%20cell%20bodies%20%28cytoarchitecture%29%20and%20nerve%20fibers%0A%28myeloarchitecture%29.%20The%20gold%20standard%20for%20cytoarchitectonic%20analysis%20is%20light%0Amicroscopic%20imaging%20of%20cell-body%20stained%20tissue%20sections.%20To%20reveal%20the%203D%0Aorientations%20of%20nerve%20fibers%2C%203D%20Polarized%20Light%20Imaging%20%283D-PLI%29%20has%20been%0Aintroduced%20as%20a%20reliable%20technique%20providing%20a%20resolution%20in%20the%20micrometer%0Arange%20while%20allowing%20processing%20of%20series%20of%20complete%20brain%20sections.%203D-PLI%0Aacquisition%20is%20label-free%20and%20allows%20subsequent%20staining%20of%20sections%20after%0Ameasurement.%20By%20post-staining%20for%20cell%20bodies%2C%20a%20direct%20link%20between%20fiber-%20and%0Acytoarchitecture%20can%20potentially%20be%20established%20within%20the%20same%20section.%0AHowever%2C%20inevitable%20distortions%20introduced%20during%20the%20staining%20process%20make%20a%0Anonlinear%20and%20cross-modal%20registration%20necessary%20in%20order%20to%20study%20the%20detailed%0Arelationships%20between%20cells%20and%20fibers%20in%20the%20images.%20In%20addition%2C%20the%0Acomplexity%20of%20processing%20histological%20sections%20for%20post-staining%20only%20allows%0Afor%20a%20limited%20number%20of%20samples.%20In%20this%20work%2C%20we%20take%20advantage%20of%20deep%0Alearning%20methods%20for%20image-to-image%20translation%20to%20generate%20a%20virtual%20staining%0Aof%203D-PLI%20that%20is%20spatially%20aligned%20at%20the%20cellular%20level.%20In%20a%20supervised%0Asetting%2C%20we%20build%20on%20a%20unique%20dataset%20of%20brain%20sections%2C%20to%20which%20Cresyl%20violet%0Astaining%20has%20been%20applied%20after%203D-PLI%20measurement.%20To%20ensure%20high%0Acorrespondence%20between%20both%20modalities%2C%20we%20address%20the%20misalignment%20of%20training%0Adata%20using%20Fourier-based%20registration%20methods.%20In%20this%20way%2C%20registration%20can%20be%0Aefficiently%20calculated%20during%20training%20for%20local%20image%20patches%20of%20target%20and%0Apredicted%20staining.%20We%20demonstrate%20that%20the%20proposed%20method%20enables%20prediction%0Aof%20a%20Cresyl%20violet%20staining%20from%203D-PLI%2C%20matching%20individual%20cell%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11394v1&entry.124074799=Read"},
{"title": "Training of Scaffolded Language Models with Language Supervision: A\n  Survey", "author": "Matthieu Lin and Jenny Sheng and Andrew Zhao and Shenzhi Wang and Yang Yue and Victor Shea Jay Huang and Huan Liu and Jun Liu and Gao Huang and Yong-Jin Liu", "abstract": "  This survey organizes the intricate literature on the design and optimization\nof emerging structures around post-trained LMs. We refer to this overarching\nstructure as scaffolded LMs and focus on LMs that are integrated into\nmulti-step processes with tools. We view scaffolded LMs as semi-parametric\nmodels wherein we train non-parametric variables, including the prompt, tools,\nand scaffold's code. In particular, they interpret instructions, use tools, and\nreceive feedback all in language. Recent works use an LM as an optimizer to\ninterpret language supervision and update non-parametric variables according to\nintricate objectives. In this survey, we refer to this paradigm as training of\nscaffolded LMs with language supervision. A key feature of non-parametric\ntraining is the ability to learn from language. Parametric training excels in\nlearning from demonstration (supervised learning), exploration (reinforcement\nlearning), or observations (unsupervised learning), using well-defined loss\nfunctions. Language-based optimization enables rich, interpretable, and\nexpressive objectives, while mitigating issues like catastrophic forgetting and\nsupporting compatibility with closed-source models. Furthermore, agents are\nincreasingly deployed as co-workers in real-world applications such as Copilot\nin Office tools or software development. In these mixed-autonomy settings,\nwhere control and decision-making are shared between human and AI, users point\nout errors or suggest corrections. Accordingly, we discuss agents that\ncontinuously improve by learning from this real-time, language-based feedback\nand refer to this setting as streaming learning from language supervision.\n", "link": "http://arxiv.org/abs/2410.16392v2", "date": "2025-05-16", "relevancy": 2.5958, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20of%20Scaffolded%20Language%20Models%20with%20Language%20Supervision%3A%20A%0A%20%20Survey&body=Title%3A%20Training%20of%20Scaffolded%20Language%20Models%20with%20Language%20Supervision%3A%20A%0A%20%20Survey%0AAuthor%3A%20Matthieu%20Lin%20and%20Jenny%20Sheng%20and%20Andrew%20Zhao%20and%20Shenzhi%20Wang%20and%20Yang%20Yue%20and%20Victor%20Shea%20Jay%20Huang%20and%20Huan%20Liu%20and%20Jun%20Liu%20and%20Gao%20Huang%20and%20Yong-Jin%20Liu%0AAbstract%3A%20%20%20This%20survey%20organizes%20the%20intricate%20literature%20on%20the%20design%20and%20optimization%0Aof%20emerging%20structures%20around%20post-trained%20LMs.%20We%20refer%20to%20this%20overarching%0Astructure%20as%20scaffolded%20LMs%20and%20focus%20on%20LMs%20that%20are%20integrated%20into%0Amulti-step%20processes%20with%20tools.%20We%20view%20scaffolded%20LMs%20as%20semi-parametric%0Amodels%20wherein%20we%20train%20non-parametric%20variables%2C%20including%20the%20prompt%2C%20tools%2C%0Aand%20scaffold%27s%20code.%20In%20particular%2C%20they%20interpret%20instructions%2C%20use%20tools%2C%20and%0Areceive%20feedback%20all%20in%20language.%20Recent%20works%20use%20an%20LM%20as%20an%20optimizer%20to%0Ainterpret%20language%20supervision%20and%20update%20non-parametric%20variables%20according%20to%0Aintricate%20objectives.%20In%20this%20survey%2C%20we%20refer%20to%20this%20paradigm%20as%20training%20of%0Ascaffolded%20LMs%20with%20language%20supervision.%20A%20key%20feature%20of%20non-parametric%0Atraining%20is%20the%20ability%20to%20learn%20from%20language.%20Parametric%20training%20excels%20in%0Alearning%20from%20demonstration%20%28supervised%20learning%29%2C%20exploration%20%28reinforcement%0Alearning%29%2C%20or%20observations%20%28unsupervised%20learning%29%2C%20using%20well-defined%20loss%0Afunctions.%20Language-based%20optimization%20enables%20rich%2C%20interpretable%2C%20and%0Aexpressive%20objectives%2C%20while%20mitigating%20issues%20like%20catastrophic%20forgetting%20and%0Asupporting%20compatibility%20with%20closed-source%20models.%20Furthermore%2C%20agents%20are%0Aincreasingly%20deployed%20as%20co-workers%20in%20real-world%20applications%20such%20as%20Copilot%0Ain%20Office%20tools%20or%20software%20development.%20In%20these%20mixed-autonomy%20settings%2C%0Awhere%20control%20and%20decision-making%20are%20shared%20between%20human%20and%20AI%2C%20users%20point%0Aout%20errors%20or%20suggest%20corrections.%20Accordingly%2C%20we%20discuss%20agents%20that%0Acontinuously%20improve%20by%20learning%20from%20this%20real-time%2C%20language-based%20feedback%0Aand%20refer%20to%20this%20setting%20as%20streaming%20learning%20from%20language%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16392v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520of%2520Scaffolded%2520Language%2520Models%2520with%2520Language%2520Supervision%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DMatthieu%2520Lin%2520and%2520Jenny%2520Sheng%2520and%2520Andrew%2520Zhao%2520and%2520Shenzhi%2520Wang%2520and%2520Yang%2520Yue%2520and%2520Victor%2520Shea%2520Jay%2520Huang%2520and%2520Huan%2520Liu%2520and%2520Jun%2520Liu%2520and%2520Gao%2520Huang%2520and%2520Yong-Jin%2520Liu%26entry.1292438233%3D%2520%2520This%2520survey%2520organizes%2520the%2520intricate%2520literature%2520on%2520the%2520design%2520and%2520optimization%250Aof%2520emerging%2520structures%2520around%2520post-trained%2520LMs.%2520We%2520refer%2520to%2520this%2520overarching%250Astructure%2520as%2520scaffolded%2520LMs%2520and%2520focus%2520on%2520LMs%2520that%2520are%2520integrated%2520into%250Amulti-step%2520processes%2520with%2520tools.%2520We%2520view%2520scaffolded%2520LMs%2520as%2520semi-parametric%250Amodels%2520wherein%2520we%2520train%2520non-parametric%2520variables%252C%2520including%2520the%2520prompt%252C%2520tools%252C%250Aand%2520scaffold%2527s%2520code.%2520In%2520particular%252C%2520they%2520interpret%2520instructions%252C%2520use%2520tools%252C%2520and%250Areceive%2520feedback%2520all%2520in%2520language.%2520Recent%2520works%2520use%2520an%2520LM%2520as%2520an%2520optimizer%2520to%250Ainterpret%2520language%2520supervision%2520and%2520update%2520non-parametric%2520variables%2520according%2520to%250Aintricate%2520objectives.%2520In%2520this%2520survey%252C%2520we%2520refer%2520to%2520this%2520paradigm%2520as%2520training%2520of%250Ascaffolded%2520LMs%2520with%2520language%2520supervision.%2520A%2520key%2520feature%2520of%2520non-parametric%250Atraining%2520is%2520the%2520ability%2520to%2520learn%2520from%2520language.%2520Parametric%2520training%2520excels%2520in%250Alearning%2520from%2520demonstration%2520%2528supervised%2520learning%2529%252C%2520exploration%2520%2528reinforcement%250Alearning%2529%252C%2520or%2520observations%2520%2528unsupervised%2520learning%2529%252C%2520using%2520well-defined%2520loss%250Afunctions.%2520Language-based%2520optimization%2520enables%2520rich%252C%2520interpretable%252C%2520and%250Aexpressive%2520objectives%252C%2520while%2520mitigating%2520issues%2520like%2520catastrophic%2520forgetting%2520and%250Asupporting%2520compatibility%2520with%2520closed-source%2520models.%2520Furthermore%252C%2520agents%2520are%250Aincreasingly%2520deployed%2520as%2520co-workers%2520in%2520real-world%2520applications%2520such%2520as%2520Copilot%250Ain%2520Office%2520tools%2520or%2520software%2520development.%2520In%2520these%2520mixed-autonomy%2520settings%252C%250Awhere%2520control%2520and%2520decision-making%2520are%2520shared%2520between%2520human%2520and%2520AI%252C%2520users%2520point%250Aout%2520errors%2520or%2520suggest%2520corrections.%2520Accordingly%252C%2520we%2520discuss%2520agents%2520that%250Acontinuously%2520improve%2520by%2520learning%2520from%2520this%2520real-time%252C%2520language-based%2520feedback%250Aand%2520refer%2520to%2520this%2520setting%2520as%2520streaming%2520learning%2520from%2520language%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16392v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20of%20Scaffolded%20Language%20Models%20with%20Language%20Supervision%3A%20A%0A%20%20Survey&entry.906535625=Matthieu%20Lin%20and%20Jenny%20Sheng%20and%20Andrew%20Zhao%20and%20Shenzhi%20Wang%20and%20Yang%20Yue%20and%20Victor%20Shea%20Jay%20Huang%20and%20Huan%20Liu%20and%20Jun%20Liu%20and%20Gao%20Huang%20and%20Yong-Jin%20Liu&entry.1292438233=%20%20This%20survey%20organizes%20the%20intricate%20literature%20on%20the%20design%20and%20optimization%0Aof%20emerging%20structures%20around%20post-trained%20LMs.%20We%20refer%20to%20this%20overarching%0Astructure%20as%20scaffolded%20LMs%20and%20focus%20on%20LMs%20that%20are%20integrated%20into%0Amulti-step%20processes%20with%20tools.%20We%20view%20scaffolded%20LMs%20as%20semi-parametric%0Amodels%20wherein%20we%20train%20non-parametric%20variables%2C%20including%20the%20prompt%2C%20tools%2C%0Aand%20scaffold%27s%20code.%20In%20particular%2C%20they%20interpret%20instructions%2C%20use%20tools%2C%20and%0Areceive%20feedback%20all%20in%20language.%20Recent%20works%20use%20an%20LM%20as%20an%20optimizer%20to%0Ainterpret%20language%20supervision%20and%20update%20non-parametric%20variables%20according%20to%0Aintricate%20objectives.%20In%20this%20survey%2C%20we%20refer%20to%20this%20paradigm%20as%20training%20of%0Ascaffolded%20LMs%20with%20language%20supervision.%20A%20key%20feature%20of%20non-parametric%0Atraining%20is%20the%20ability%20to%20learn%20from%20language.%20Parametric%20training%20excels%20in%0Alearning%20from%20demonstration%20%28supervised%20learning%29%2C%20exploration%20%28reinforcement%0Alearning%29%2C%20or%20observations%20%28unsupervised%20learning%29%2C%20using%20well-defined%20loss%0Afunctions.%20Language-based%20optimization%20enables%20rich%2C%20interpretable%2C%20and%0Aexpressive%20objectives%2C%20while%20mitigating%20issues%20like%20catastrophic%20forgetting%20and%0Asupporting%20compatibility%20with%20closed-source%20models.%20Furthermore%2C%20agents%20are%0Aincreasingly%20deployed%20as%20co-workers%20in%20real-world%20applications%20such%20as%20Copilot%0Ain%20Office%20tools%20or%20software%20development.%20In%20these%20mixed-autonomy%20settings%2C%0Awhere%20control%20and%20decision-making%20are%20shared%20between%20human%20and%20AI%2C%20users%20point%0Aout%20errors%20or%20suggest%20corrections.%20Accordingly%2C%20we%20discuss%20agents%20that%0Acontinuously%20improve%20by%20learning%20from%20this%20real-time%2C%20language-based%20feedback%0Aand%20refer%20to%20this%20setting%20as%20streaming%20learning%20from%20language%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16392v2&entry.124074799=Read"},
{"title": "Prototype Augmented Hypernetworks for Continual Learning", "author": "Neil De La Fuente and Maria Pilligua and Daniel Vidal and Albin Soutiff and Cecilia Curreli and Daniel Cremers and Andrey Barsky", "abstract": "  Continual learning (CL) aims to learn a sequence of tasks without forgetting\nprior knowledge, but gradient updates for a new task often overwrite the\nweights learned earlier, causing catastrophic forgetting (CF). We propose\nPrototype-Augmented Hypernetworks (PAH), a framework where a single\nhypernetwork, conditioned on learnable task prototypes, dynamically generates\ntask-specific classifier heads on demand. To mitigate forgetting, PAH combines\ncross-entropy with dual distillation losses, one to align logits and another to\nalign prototypes, ensuring stable feature representations across tasks.\nEvaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves\nstate-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7\n% and 4.4 % forgetting, respectively, surpassing prior methods without storing\nsamples or heads.\n", "link": "http://arxiv.org/abs/2505.07450v3", "date": "2025-05-16", "relevancy": 2.5934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5282}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5176}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototype%20Augmented%20Hypernetworks%20for%20Continual%20Learning&body=Title%3A%20Prototype%20Augmented%20Hypernetworks%20for%20Continual%20Learning%0AAuthor%3A%20Neil%20De%20La%20Fuente%20and%20Maria%20Pilligua%20and%20Daniel%20Vidal%20and%20Albin%20Soutiff%20and%20Cecilia%20Curreli%20and%20Daniel%20Cremers%20and%20Andrey%20Barsky%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20aims%20to%20learn%20a%20sequence%20of%20tasks%20without%20forgetting%0Aprior%20knowledge%2C%20but%20gradient%20updates%20for%20a%20new%20task%20often%20overwrite%20the%0Aweights%20learned%20earlier%2C%20causing%20catastrophic%20forgetting%20%28CF%29.%20We%20propose%0APrototype-Augmented%20Hypernetworks%20%28PAH%29%2C%20a%20framework%20where%20a%20single%0Ahypernetwork%2C%20conditioned%20on%20learnable%20task%20prototypes%2C%20dynamically%20generates%0Atask-specific%20classifier%20heads%20on%20demand.%20To%20mitigate%20forgetting%2C%20PAH%20combines%0Across-entropy%20with%20dual%20distillation%20losses%2C%20one%20to%20align%20logits%20and%20another%20to%0Aalign%20prototypes%2C%20ensuring%20stable%20feature%20representations%20across%20tasks.%0AEvaluations%20on%20Split-CIFAR100%20and%20TinyImageNet%20demonstrate%20that%20PAH%20achieves%0Astate-of-the-art%20performance%2C%20reaching%2074.5%20%25%20and%2063.7%20%25%20accuracy%20with%20only%201.7%0A%25%20and%204.4%20%25%20forgetting%2C%20respectively%2C%20surpassing%20prior%20methods%20without%20storing%0Asamples%20or%20heads.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07450v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototype%2520Augmented%2520Hypernetworks%2520for%2520Continual%2520Learning%26entry.906535625%3DNeil%2520De%2520La%2520Fuente%2520and%2520Maria%2520Pilligua%2520and%2520Daniel%2520Vidal%2520and%2520Albin%2520Soutiff%2520and%2520Cecilia%2520Curreli%2520and%2520Daniel%2520Cremers%2520and%2520Andrey%2520Barsky%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520aims%2520to%2520learn%2520a%2520sequence%2520of%2520tasks%2520without%2520forgetting%250Aprior%2520knowledge%252C%2520but%2520gradient%2520updates%2520for%2520a%2520new%2520task%2520often%2520overwrite%2520the%250Aweights%2520learned%2520earlier%252C%2520causing%2520catastrophic%2520forgetting%2520%2528CF%2529.%2520We%2520propose%250APrototype-Augmented%2520Hypernetworks%2520%2528PAH%2529%252C%2520a%2520framework%2520where%2520a%2520single%250Ahypernetwork%252C%2520conditioned%2520on%2520learnable%2520task%2520prototypes%252C%2520dynamically%2520generates%250Atask-specific%2520classifier%2520heads%2520on%2520demand.%2520To%2520mitigate%2520forgetting%252C%2520PAH%2520combines%250Across-entropy%2520with%2520dual%2520distillation%2520losses%252C%2520one%2520to%2520align%2520logits%2520and%2520another%2520to%250Aalign%2520prototypes%252C%2520ensuring%2520stable%2520feature%2520representations%2520across%2520tasks.%250AEvaluations%2520on%2520Split-CIFAR100%2520and%2520TinyImageNet%2520demonstrate%2520that%2520PAH%2520achieves%250Astate-of-the-art%2520performance%252C%2520reaching%252074.5%2520%2525%2520and%252063.7%2520%2525%2520accuracy%2520with%2520only%25201.7%250A%2525%2520and%25204.4%2520%2525%2520forgetting%252C%2520respectively%252C%2520surpassing%2520prior%2520methods%2520without%2520storing%250Asamples%2520or%2520heads.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07450v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype%20Augmented%20Hypernetworks%20for%20Continual%20Learning&entry.906535625=Neil%20De%20La%20Fuente%20and%20Maria%20Pilligua%20and%20Daniel%20Vidal%20and%20Albin%20Soutiff%20and%20Cecilia%20Curreli%20and%20Daniel%20Cremers%20and%20Andrey%20Barsky&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20aims%20to%20learn%20a%20sequence%20of%20tasks%20without%20forgetting%0Aprior%20knowledge%2C%20but%20gradient%20updates%20for%20a%20new%20task%20often%20overwrite%20the%0Aweights%20learned%20earlier%2C%20causing%20catastrophic%20forgetting%20%28CF%29.%20We%20propose%0APrototype-Augmented%20Hypernetworks%20%28PAH%29%2C%20a%20framework%20where%20a%20single%0Ahypernetwork%2C%20conditioned%20on%20learnable%20task%20prototypes%2C%20dynamically%20generates%0Atask-specific%20classifier%20heads%20on%20demand.%20To%20mitigate%20forgetting%2C%20PAH%20combines%0Across-entropy%20with%20dual%20distillation%20losses%2C%20one%20to%20align%20logits%20and%20another%20to%0Aalign%20prototypes%2C%20ensuring%20stable%20feature%20representations%20across%20tasks.%0AEvaluations%20on%20Split-CIFAR100%20and%20TinyImageNet%20demonstrate%20that%20PAH%20achieves%0Astate-of-the-art%20performance%2C%20reaching%2074.5%20%25%20and%2063.7%20%25%20accuracy%20with%20only%201.7%0A%25%20and%204.4%20%25%20forgetting%2C%20respectively%2C%20surpassing%20prior%20methods%20without%20storing%0Asamples%20or%20heads.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07450v3&entry.124074799=Read"},
{"title": "The Final Layer Holds the Key: A Unified and Efficient GNN Calibration\n  Framework", "author": "Jincheng Huang and Jie Xu and Xiaoshuang Shi and Ping Hu and Lei Feng and Xiaofeng Zhu", "abstract": "  Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on\ngraph-based tasks. However, their predictive confidence is often miscalibrated,\ntypically exhibiting under-confidence, which harms the reliability of their\ndecisions. Existing calibration methods for GNNs normally introduce additional\ncalibration components, which fail to capture the intrinsic relationship\nbetween the model and the prediction confidence, resulting in limited\ntheoretical guarantees and increased computational overhead. To address this\nissue, we propose a simple yet efficient graph calibration method. We establish\na unified theoretical framework revealing that model confidence is jointly\ngoverned by class-centroid-level and node-level calibration at the final layer.\nBased on this insight, we theoretically show that reducing the weight decay of\nthe final-layer parameters alleviates GNN under-confidence by acting on the\nclass-centroid level, while node-level calibration acts as a finer-grained\ncomplement to class-centroid level calibration, which encourages each test node\nto be closer to its predicted class centroid at the final-layer\nrepresentations. Extensive experiments validate the superiority of our method.\n", "link": "http://arxiv.org/abs/2505.11335v1", "date": "2025-05-16", "relevancy": 2.5815, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5342}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5292}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Final%20Layer%20Holds%20the%20Key%3A%20A%20Unified%20and%20Efficient%20GNN%20Calibration%0A%20%20Framework&body=Title%3A%20The%20Final%20Layer%20Holds%20the%20Key%3A%20A%20Unified%20and%20Efficient%20GNN%20Calibration%0A%20%20Framework%0AAuthor%3A%20Jincheng%20Huang%20and%20Jie%20Xu%20and%20Xiaoshuang%20Shi%20and%20Ping%20Hu%20and%20Lei%20Feng%20and%20Xiaofeng%20Zhu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20effectiveness%20on%0Agraph-based%20tasks.%20However%2C%20their%20predictive%20confidence%20is%20often%20miscalibrated%2C%0Atypically%20exhibiting%20under-confidence%2C%20which%20harms%20the%20reliability%20of%20their%0Adecisions.%20Existing%20calibration%20methods%20for%20GNNs%20normally%20introduce%20additional%0Acalibration%20components%2C%20which%20fail%20to%20capture%20the%20intrinsic%20relationship%0Abetween%20the%20model%20and%20the%20prediction%20confidence%2C%20resulting%20in%20limited%0Atheoretical%20guarantees%20and%20increased%20computational%20overhead.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20simple%20yet%20efficient%20graph%20calibration%20method.%20We%20establish%0Aa%20unified%20theoretical%20framework%20revealing%20that%20model%20confidence%20is%20jointly%0Agoverned%20by%20class-centroid-level%20and%20node-level%20calibration%20at%20the%20final%20layer.%0ABased%20on%20this%20insight%2C%20we%20theoretically%20show%20that%20reducing%20the%20weight%20decay%20of%0Athe%20final-layer%20parameters%20alleviates%20GNN%20under-confidence%20by%20acting%20on%20the%0Aclass-centroid%20level%2C%20while%20node-level%20calibration%20acts%20as%20a%20finer-grained%0Acomplement%20to%20class-centroid%20level%20calibration%2C%20which%20encourages%20each%20test%20node%0Ato%20be%20closer%20to%20its%20predicted%20class%20centroid%20at%20the%20final-layer%0Arepresentations.%20Extensive%20experiments%20validate%20the%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Final%2520Layer%2520Holds%2520the%2520Key%253A%2520A%2520Unified%2520and%2520Efficient%2520GNN%2520Calibration%250A%2520%2520Framework%26entry.906535625%3DJincheng%2520Huang%2520and%2520Jie%2520Xu%2520and%2520Xiaoshuang%2520Shi%2520and%2520Ping%2520Hu%2520and%2520Lei%2520Feng%2520and%2520Xiaofeng%2520Zhu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520remarkable%2520effectiveness%2520on%250Agraph-based%2520tasks.%2520However%252C%2520their%2520predictive%2520confidence%2520is%2520often%2520miscalibrated%252C%250Atypically%2520exhibiting%2520under-confidence%252C%2520which%2520harms%2520the%2520reliability%2520of%2520their%250Adecisions.%2520Existing%2520calibration%2520methods%2520for%2520GNNs%2520normally%2520introduce%2520additional%250Acalibration%2520components%252C%2520which%2520fail%2520to%2520capture%2520the%2520intrinsic%2520relationship%250Abetween%2520the%2520model%2520and%2520the%2520prediction%2520confidence%252C%2520resulting%2520in%2520limited%250Atheoretical%2520guarantees%2520and%2520increased%2520computational%2520overhead.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520a%2520simple%2520yet%2520efficient%2520graph%2520calibration%2520method.%2520We%2520establish%250Aa%2520unified%2520theoretical%2520framework%2520revealing%2520that%2520model%2520confidence%2520is%2520jointly%250Agoverned%2520by%2520class-centroid-level%2520and%2520node-level%2520calibration%2520at%2520the%2520final%2520layer.%250ABased%2520on%2520this%2520insight%252C%2520we%2520theoretically%2520show%2520that%2520reducing%2520the%2520weight%2520decay%2520of%250Athe%2520final-layer%2520parameters%2520alleviates%2520GNN%2520under-confidence%2520by%2520acting%2520on%2520the%250Aclass-centroid%2520level%252C%2520while%2520node-level%2520calibration%2520acts%2520as%2520a%2520finer-grained%250Acomplement%2520to%2520class-centroid%2520level%2520calibration%252C%2520which%2520encourages%2520each%2520test%2520node%250Ato%2520be%2520closer%2520to%2520its%2520predicted%2520class%2520centroid%2520at%2520the%2520final-layer%250Arepresentations.%2520Extensive%2520experiments%2520validate%2520the%2520superiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Final%20Layer%20Holds%20the%20Key%3A%20A%20Unified%20and%20Efficient%20GNN%20Calibration%0A%20%20Framework&entry.906535625=Jincheng%20Huang%20and%20Jie%20Xu%20and%20Xiaoshuang%20Shi%20and%20Ping%20Hu%20and%20Lei%20Feng%20and%20Xiaofeng%20Zhu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20effectiveness%20on%0Agraph-based%20tasks.%20However%2C%20their%20predictive%20confidence%20is%20often%20miscalibrated%2C%0Atypically%20exhibiting%20under-confidence%2C%20which%20harms%20the%20reliability%20of%20their%0Adecisions.%20Existing%20calibration%20methods%20for%20GNNs%20normally%20introduce%20additional%0Acalibration%20components%2C%20which%20fail%20to%20capture%20the%20intrinsic%20relationship%0Abetween%20the%20model%20and%20the%20prediction%20confidence%2C%20resulting%20in%20limited%0Atheoretical%20guarantees%20and%20increased%20computational%20overhead.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20simple%20yet%20efficient%20graph%20calibration%20method.%20We%20establish%0Aa%20unified%20theoretical%20framework%20revealing%20that%20model%20confidence%20is%20jointly%0Agoverned%20by%20class-centroid-level%20and%20node-level%20calibration%20at%20the%20final%20layer.%0ABased%20on%20this%20insight%2C%20we%20theoretically%20show%20that%20reducing%20the%20weight%20decay%20of%0Athe%20final-layer%20parameters%20alleviates%20GNN%20under-confidence%20by%20acting%20on%20the%0Aclass-centroid%20level%2C%20while%20node-level%20calibration%20acts%20as%20a%20finer-grained%0Acomplement%20to%20class-centroid%20level%20calibration%2C%20which%20encourages%20each%20test%20node%0Ato%20be%20closer%20to%20its%20predicted%20class%20centroid%20at%20the%20final-layer%0Arepresentations.%20Extensive%20experiments%20validate%20the%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11335v1&entry.124074799=Read"},
{"title": "CUBIC: Concept Embeddings for Unsupervised Bias Identification using\n  VLMs", "author": "David M\u00e9ndez and Gianpaolo Bontempo and Elisa Ficarra and Roberto Confalonieri and Natalia D\u00edaz-Rodr\u00edguez", "abstract": "  Deep vision models often rely on biases learned from spurious correlations in\ndatasets. To identify these biases, methods that interpret high-level,\nhuman-understandable concepts are more effective than those relying primarily\non low-level features like heatmaps. A major challenge for these concept-based\nmethods is the lack of image annotations indicating potentially bias-inducing\nconcepts, since creating such annotations requires detailed labeling for each\ndataset and concept, which is highly labor-intensive. We present CUBIC (Concept\nembeddings for Unsupervised Bias IdentifiCation), a novel method that\nautomatically discovers interpretable concepts that may bias classifier\nbehavior. Unlike existing approaches, CUBIC does not rely on predefined bias\ncandidates or examples of model failures tied to specific biases, as such\ninformation is not always available. Instead, it leverages image-text latent\nspace and linear classifier probes to examine how the latent representation of\na superclass label$\\unicode{x2014}$shared by all instances in the\ndataset$\\unicode{x2014}$is influenced by the presence of a given concept. By\nmeasuring these shifts against the normal vector to the classifier's decision\nboundary, CUBIC identifies concepts that significantly influence model\npredictions. Our experiments demonstrate that CUBIC effectively uncovers\npreviously unknown biases using Vision-Language Models (VLMs) without requiring\nthe samples in the dataset where the classifier underperforms or prior\nknowledge of potential biases.\n", "link": "http://arxiv.org/abs/2505.11060v1", "date": "2025-05-16", "relevancy": 2.5745, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CUBIC%3A%20Concept%20Embeddings%20for%20Unsupervised%20Bias%20Identification%20using%0A%20%20VLMs&body=Title%3A%20CUBIC%3A%20Concept%20Embeddings%20for%20Unsupervised%20Bias%20Identification%20using%0A%20%20VLMs%0AAuthor%3A%20David%20M%C3%A9ndez%20and%20Gianpaolo%20Bontempo%20and%20Elisa%20Ficarra%20and%20Roberto%20Confalonieri%20and%20Natalia%20D%C3%ADaz-Rodr%C3%ADguez%0AAbstract%3A%20%20%20Deep%20vision%20models%20often%20rely%20on%20biases%20learned%20from%20spurious%20correlations%20in%0Adatasets.%20To%20identify%20these%20biases%2C%20methods%20that%20interpret%20high-level%2C%0Ahuman-understandable%20concepts%20are%20more%20effective%20than%20those%20relying%20primarily%0Aon%20low-level%20features%20like%20heatmaps.%20A%20major%20challenge%20for%20these%20concept-based%0Amethods%20is%20the%20lack%20of%20image%20annotations%20indicating%20potentially%20bias-inducing%0Aconcepts%2C%20since%20creating%20such%20annotations%20requires%20detailed%20labeling%20for%20each%0Adataset%20and%20concept%2C%20which%20is%20highly%20labor-intensive.%20We%20present%20CUBIC%20%28Concept%0Aembeddings%20for%20Unsupervised%20Bias%20IdentifiCation%29%2C%20a%20novel%20method%20that%0Aautomatically%20discovers%20interpretable%20concepts%20that%20may%20bias%20classifier%0Abehavior.%20Unlike%20existing%20approaches%2C%20CUBIC%20does%20not%20rely%20on%20predefined%20bias%0Acandidates%20or%20examples%20of%20model%20failures%20tied%20to%20specific%20biases%2C%20as%20such%0Ainformation%20is%20not%20always%20available.%20Instead%2C%20it%20leverages%20image-text%20latent%0Aspace%20and%20linear%20classifier%20probes%20to%20examine%20how%20the%20latent%20representation%20of%0Aa%20superclass%20label%24%5Cunicode%7Bx2014%7D%24shared%20by%20all%20instances%20in%20the%0Adataset%24%5Cunicode%7Bx2014%7D%24is%20influenced%20by%20the%20presence%20of%20a%20given%20concept.%20By%0Ameasuring%20these%20shifts%20against%20the%20normal%20vector%20to%20the%20classifier%27s%20decision%0Aboundary%2C%20CUBIC%20identifies%20concepts%20that%20significantly%20influence%20model%0Apredictions.%20Our%20experiments%20demonstrate%20that%20CUBIC%20effectively%20uncovers%0Apreviously%20unknown%20biases%20using%20Vision-Language%20Models%20%28VLMs%29%20without%20requiring%0Athe%20samples%20in%20the%20dataset%20where%20the%20classifier%20underperforms%20or%20prior%0Aknowledge%20of%20potential%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCUBIC%253A%2520Concept%2520Embeddings%2520for%2520Unsupervised%2520Bias%2520Identification%2520using%250A%2520%2520VLMs%26entry.906535625%3DDavid%2520M%25C3%25A9ndez%2520and%2520Gianpaolo%2520Bontempo%2520and%2520Elisa%2520Ficarra%2520and%2520Roberto%2520Confalonieri%2520and%2520Natalia%2520D%25C3%25ADaz-Rodr%25C3%25ADguez%26entry.1292438233%3D%2520%2520Deep%2520vision%2520models%2520often%2520rely%2520on%2520biases%2520learned%2520from%2520spurious%2520correlations%2520in%250Adatasets.%2520To%2520identify%2520these%2520biases%252C%2520methods%2520that%2520interpret%2520high-level%252C%250Ahuman-understandable%2520concepts%2520are%2520more%2520effective%2520than%2520those%2520relying%2520primarily%250Aon%2520low-level%2520features%2520like%2520heatmaps.%2520A%2520major%2520challenge%2520for%2520these%2520concept-based%250Amethods%2520is%2520the%2520lack%2520of%2520image%2520annotations%2520indicating%2520potentially%2520bias-inducing%250Aconcepts%252C%2520since%2520creating%2520such%2520annotations%2520requires%2520detailed%2520labeling%2520for%2520each%250Adataset%2520and%2520concept%252C%2520which%2520is%2520highly%2520labor-intensive.%2520We%2520present%2520CUBIC%2520%2528Concept%250Aembeddings%2520for%2520Unsupervised%2520Bias%2520IdentifiCation%2529%252C%2520a%2520novel%2520method%2520that%250Aautomatically%2520discovers%2520interpretable%2520concepts%2520that%2520may%2520bias%2520classifier%250Abehavior.%2520Unlike%2520existing%2520approaches%252C%2520CUBIC%2520does%2520not%2520rely%2520on%2520predefined%2520bias%250Acandidates%2520or%2520examples%2520of%2520model%2520failures%2520tied%2520to%2520specific%2520biases%252C%2520as%2520such%250Ainformation%2520is%2520not%2520always%2520available.%2520Instead%252C%2520it%2520leverages%2520image-text%2520latent%250Aspace%2520and%2520linear%2520classifier%2520probes%2520to%2520examine%2520how%2520the%2520latent%2520representation%2520of%250Aa%2520superclass%2520label%2524%255Cunicode%257Bx2014%257D%2524shared%2520by%2520all%2520instances%2520in%2520the%250Adataset%2524%255Cunicode%257Bx2014%257D%2524is%2520influenced%2520by%2520the%2520presence%2520of%2520a%2520given%2520concept.%2520By%250Ameasuring%2520these%2520shifts%2520against%2520the%2520normal%2520vector%2520to%2520the%2520classifier%2527s%2520decision%250Aboundary%252C%2520CUBIC%2520identifies%2520concepts%2520that%2520significantly%2520influence%2520model%250Apredictions.%2520Our%2520experiments%2520demonstrate%2520that%2520CUBIC%2520effectively%2520uncovers%250Apreviously%2520unknown%2520biases%2520using%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520without%2520requiring%250Athe%2520samples%2520in%2520the%2520dataset%2520where%2520the%2520classifier%2520underperforms%2520or%2520prior%250Aknowledge%2520of%2520potential%2520biases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CUBIC%3A%20Concept%20Embeddings%20for%20Unsupervised%20Bias%20Identification%20using%0A%20%20VLMs&entry.906535625=David%20M%C3%A9ndez%20and%20Gianpaolo%20Bontempo%20and%20Elisa%20Ficarra%20and%20Roberto%20Confalonieri%20and%20Natalia%20D%C3%ADaz-Rodr%C3%ADguez&entry.1292438233=%20%20Deep%20vision%20models%20often%20rely%20on%20biases%20learned%20from%20spurious%20correlations%20in%0Adatasets.%20To%20identify%20these%20biases%2C%20methods%20that%20interpret%20high-level%2C%0Ahuman-understandable%20concepts%20are%20more%20effective%20than%20those%20relying%20primarily%0Aon%20low-level%20features%20like%20heatmaps.%20A%20major%20challenge%20for%20these%20concept-based%0Amethods%20is%20the%20lack%20of%20image%20annotations%20indicating%20potentially%20bias-inducing%0Aconcepts%2C%20since%20creating%20such%20annotations%20requires%20detailed%20labeling%20for%20each%0Adataset%20and%20concept%2C%20which%20is%20highly%20labor-intensive.%20We%20present%20CUBIC%20%28Concept%0Aembeddings%20for%20Unsupervised%20Bias%20IdentifiCation%29%2C%20a%20novel%20method%20that%0Aautomatically%20discovers%20interpretable%20concepts%20that%20may%20bias%20classifier%0Abehavior.%20Unlike%20existing%20approaches%2C%20CUBIC%20does%20not%20rely%20on%20predefined%20bias%0Acandidates%20or%20examples%20of%20model%20failures%20tied%20to%20specific%20biases%2C%20as%20such%0Ainformation%20is%20not%20always%20available.%20Instead%2C%20it%20leverages%20image-text%20latent%0Aspace%20and%20linear%20classifier%20probes%20to%20examine%20how%20the%20latent%20representation%20of%0Aa%20superclass%20label%24%5Cunicode%7Bx2014%7D%24shared%20by%20all%20instances%20in%20the%0Adataset%24%5Cunicode%7Bx2014%7D%24is%20influenced%20by%20the%20presence%20of%20a%20given%20concept.%20By%0Ameasuring%20these%20shifts%20against%20the%20normal%20vector%20to%20the%20classifier%27s%20decision%0Aboundary%2C%20CUBIC%20identifies%20concepts%20that%20significantly%20influence%20model%0Apredictions.%20Our%20experiments%20demonstrate%20that%20CUBIC%20effectively%20uncovers%0Apreviously%20unknown%20biases%20using%20Vision-Language%20Models%20%28VLMs%29%20without%20requiring%0Athe%20samples%20in%20the%20dataset%20where%20the%20classifier%20underperforms%20or%20prior%0Aknowledge%20of%20potential%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11060v1&entry.124074799=Read"},
{"title": "Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene\n  Understanding", "author": "Yuchen Rao and Stefan Ainetter and Sinisa Stekovic and Vincent Lepetit and Friedrich Fraundorfer", "abstract": "  High-level 3D scene understanding is essential in many applications. However,\nthe challenges of generating accurate 3D annotations make development of deep\nlearning models difficult. We turn to recent advancements in automatic\nretrieval of synthetic CAD models, and show that data generated by such methods\ncan be used as high-quality ground truth for training supervised deep learning\nmodels. More exactly, we employ a pipeline akin to the one previously used to\nautomatically annotate objects in ScanNet scenes with their 9D poses and CAD\nmodels. This time, we apply it to the recent ScanNet++ v1 dataset, which\npreviously lacked such annotations. Our findings demonstrate that it is not\nonly possible to train deep learning models on these automatically-obtained\nannotations but that the resulting models outperform those trained on manually\nannotated data. We validate this on two distinct tasks: point cloud completion\nand single-view CAD model retrieval and alignment. Our results underscore the\npotential of automatic 3D annotations to enhance model performance while\nsignificantly reducing annotation costs. To support future research in 3D scene\nunderstanding, we will release our annotations, which we call SCANnotate++,\nalong with our trained models.\n", "link": "http://arxiv.org/abs/2504.13580v4", "date": "2025-05-16", "relevancy": 2.564, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6457}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Automatic%20CAD%20Annotations%20for%20Supervised%20Learning%20in%203D%20Scene%0A%20%20Understanding&body=Title%3A%20Leveraging%20Automatic%20CAD%20Annotations%20for%20Supervised%20Learning%20in%203D%20Scene%0A%20%20Understanding%0AAuthor%3A%20Yuchen%20Rao%20and%20Stefan%20Ainetter%20and%20Sinisa%20Stekovic%20and%20Vincent%20Lepetit%20and%20Friedrich%20Fraundorfer%0AAbstract%3A%20%20%20High-level%203D%20scene%20understanding%20is%20essential%20in%20many%20applications.%20However%2C%0Athe%20challenges%20of%20generating%20accurate%203D%20annotations%20make%20development%20of%20deep%0Alearning%20models%20difficult.%20We%20turn%20to%20recent%20advancements%20in%20automatic%0Aretrieval%20of%20synthetic%20CAD%20models%2C%20and%20show%20that%20data%20generated%20by%20such%20methods%0Acan%20be%20used%20as%20high-quality%20ground%20truth%20for%20training%20supervised%20deep%20learning%0Amodels.%20More%20exactly%2C%20we%20employ%20a%20pipeline%20akin%20to%20the%20one%20previously%20used%20to%0Aautomatically%20annotate%20objects%20in%20ScanNet%20scenes%20with%20their%209D%20poses%20and%20CAD%0Amodels.%20This%20time%2C%20we%20apply%20it%20to%20the%20recent%20ScanNet%2B%2B%20v1%20dataset%2C%20which%0Apreviously%20lacked%20such%20annotations.%20Our%20findings%20demonstrate%20that%20it%20is%20not%0Aonly%20possible%20to%20train%20deep%20learning%20models%20on%20these%20automatically-obtained%0Aannotations%20but%20that%20the%20resulting%20models%20outperform%20those%20trained%20on%20manually%0Aannotated%20data.%20We%20validate%20this%20on%20two%20distinct%20tasks%3A%20point%20cloud%20completion%0Aand%20single-view%20CAD%20model%20retrieval%20and%20alignment.%20Our%20results%20underscore%20the%0Apotential%20of%20automatic%203D%20annotations%20to%20enhance%20model%20performance%20while%0Asignificantly%20reducing%20annotation%20costs.%20To%20support%20future%20research%20in%203D%20scene%0Aunderstanding%2C%20we%20will%20release%20our%20annotations%2C%20which%20we%20call%20SCANnotate%2B%2B%2C%0Aalong%20with%20our%20trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13580v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Automatic%2520CAD%2520Annotations%2520for%2520Supervised%2520Learning%2520in%25203D%2520Scene%250A%2520%2520Understanding%26entry.906535625%3DYuchen%2520Rao%2520and%2520Stefan%2520Ainetter%2520and%2520Sinisa%2520Stekovic%2520and%2520Vincent%2520Lepetit%2520and%2520Friedrich%2520Fraundorfer%26entry.1292438233%3D%2520%2520High-level%25203D%2520scene%2520understanding%2520is%2520essential%2520in%2520many%2520applications.%2520However%252C%250Athe%2520challenges%2520of%2520generating%2520accurate%25203D%2520annotations%2520make%2520development%2520of%2520deep%250Alearning%2520models%2520difficult.%2520We%2520turn%2520to%2520recent%2520advancements%2520in%2520automatic%250Aretrieval%2520of%2520synthetic%2520CAD%2520models%252C%2520and%2520show%2520that%2520data%2520generated%2520by%2520such%2520methods%250Acan%2520be%2520used%2520as%2520high-quality%2520ground%2520truth%2520for%2520training%2520supervised%2520deep%2520learning%250Amodels.%2520More%2520exactly%252C%2520we%2520employ%2520a%2520pipeline%2520akin%2520to%2520the%2520one%2520previously%2520used%2520to%250Aautomatically%2520annotate%2520objects%2520in%2520ScanNet%2520scenes%2520with%2520their%25209D%2520poses%2520and%2520CAD%250Amodels.%2520This%2520time%252C%2520we%2520apply%2520it%2520to%2520the%2520recent%2520ScanNet%252B%252B%2520v1%2520dataset%252C%2520which%250Apreviously%2520lacked%2520such%2520annotations.%2520Our%2520findings%2520demonstrate%2520that%2520it%2520is%2520not%250Aonly%2520possible%2520to%2520train%2520deep%2520learning%2520models%2520on%2520these%2520automatically-obtained%250Aannotations%2520but%2520that%2520the%2520resulting%2520models%2520outperform%2520those%2520trained%2520on%2520manually%250Aannotated%2520data.%2520We%2520validate%2520this%2520on%2520two%2520distinct%2520tasks%253A%2520point%2520cloud%2520completion%250Aand%2520single-view%2520CAD%2520model%2520retrieval%2520and%2520alignment.%2520Our%2520results%2520underscore%2520the%250Apotential%2520of%2520automatic%25203D%2520annotations%2520to%2520enhance%2520model%2520performance%2520while%250Asignificantly%2520reducing%2520annotation%2520costs.%2520To%2520support%2520future%2520research%2520in%25203D%2520scene%250Aunderstanding%252C%2520we%2520will%2520release%2520our%2520annotations%252C%2520which%2520we%2520call%2520SCANnotate%252B%252B%252C%250Aalong%2520with%2520our%2520trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13580v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Automatic%20CAD%20Annotations%20for%20Supervised%20Learning%20in%203D%20Scene%0A%20%20Understanding&entry.906535625=Yuchen%20Rao%20and%20Stefan%20Ainetter%20and%20Sinisa%20Stekovic%20and%20Vincent%20Lepetit%20and%20Friedrich%20Fraundorfer&entry.1292438233=%20%20High-level%203D%20scene%20understanding%20is%20essential%20in%20many%20applications.%20However%2C%0Athe%20challenges%20of%20generating%20accurate%203D%20annotations%20make%20development%20of%20deep%0Alearning%20models%20difficult.%20We%20turn%20to%20recent%20advancements%20in%20automatic%0Aretrieval%20of%20synthetic%20CAD%20models%2C%20and%20show%20that%20data%20generated%20by%20such%20methods%0Acan%20be%20used%20as%20high-quality%20ground%20truth%20for%20training%20supervised%20deep%20learning%0Amodels.%20More%20exactly%2C%20we%20employ%20a%20pipeline%20akin%20to%20the%20one%20previously%20used%20to%0Aautomatically%20annotate%20objects%20in%20ScanNet%20scenes%20with%20their%209D%20poses%20and%20CAD%0Amodels.%20This%20time%2C%20we%20apply%20it%20to%20the%20recent%20ScanNet%2B%2B%20v1%20dataset%2C%20which%0Apreviously%20lacked%20such%20annotations.%20Our%20findings%20demonstrate%20that%20it%20is%20not%0Aonly%20possible%20to%20train%20deep%20learning%20models%20on%20these%20automatically-obtained%0Aannotations%20but%20that%20the%20resulting%20models%20outperform%20those%20trained%20on%20manually%0Aannotated%20data.%20We%20validate%20this%20on%20two%20distinct%20tasks%3A%20point%20cloud%20completion%0Aand%20single-view%20CAD%20model%20retrieval%20and%20alignment.%20Our%20results%20underscore%20the%0Apotential%20of%20automatic%203D%20annotations%20to%20enhance%20model%20performance%20while%0Asignificantly%20reducing%20annotation%20costs.%20To%20support%20future%20research%20in%203D%20scene%0Aunderstanding%2C%20we%20will%20release%20our%20annotations%2C%20which%20we%20call%20SCANnotate%2B%2B%2C%0Aalong%20with%20our%20trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13580v4&entry.124074799=Read"},
{"title": "FALCON: False-Negative Aware Learning of Contrastive Negatives in\n  Vision-Language Pretraining", "author": "Myunsoo Kim and Seong-Woong Shim and Byung-Jun Lee", "abstract": "  False negatives pose a critical challenge in vision-language pretraining\n(VLP) due to the many-to-many correspondence between images and texts in\nlarge-scale datasets. These false negatives introduce conflicting supervision\nsignals that degrade the learned embedding space and diminish the effectiveness\nof hard negative sampling. In this paper, we propose FALCON (False-negative\nAware Learning of COntrastive Negatives), a learning-based mini-batch\nconstruction strategy that adaptively balances the trade-off between hard and\nfalse negatives during VLP. Rather than relying on fixed heuristics, FALCON\nemploys a negative mining scheduler that dynamically selects negative samples\nof appropriate hardness for each anchor instance during mini-batch\nconstruction, guided by a proxy for cross-modal alignment improvement.\nExperimental results demonstrate that FALCON significantly improves performance\nacross two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of\ndownstream tasks and evaluation settings, underscoring its effectiveness and\nrobustness in mitigating the impact of false negatives.\n", "link": "http://arxiv.org/abs/2505.11192v1", "date": "2025-05-16", "relevancy": 2.5624, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5147}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FALCON%3A%20False-Negative%20Aware%20Learning%20of%20Contrastive%20Negatives%20in%0A%20%20Vision-Language%20Pretraining&body=Title%3A%20FALCON%3A%20False-Negative%20Aware%20Learning%20of%20Contrastive%20Negatives%20in%0A%20%20Vision-Language%20Pretraining%0AAuthor%3A%20Myunsoo%20Kim%20and%20Seong-Woong%20Shim%20and%20Byung-Jun%20Lee%0AAbstract%3A%20%20%20False%20negatives%20pose%20a%20critical%20challenge%20in%20vision-language%20pretraining%0A%28VLP%29%20due%20to%20the%20many-to-many%20correspondence%20between%20images%20and%20texts%20in%0Alarge-scale%20datasets.%20These%20false%20negatives%20introduce%20conflicting%20supervision%0Asignals%20that%20degrade%20the%20learned%20embedding%20space%20and%20diminish%20the%20effectiveness%0Aof%20hard%20negative%20sampling.%20In%20this%20paper%2C%20we%20propose%20FALCON%20%28False-negative%0AAware%20Learning%20of%20COntrastive%20Negatives%29%2C%20a%20learning-based%20mini-batch%0Aconstruction%20strategy%20that%20adaptively%20balances%20the%20trade-off%20between%20hard%20and%0Afalse%20negatives%20during%20VLP.%20Rather%20than%20relying%20on%20fixed%20heuristics%2C%20FALCON%0Aemploys%20a%20negative%20mining%20scheduler%20that%20dynamically%20selects%20negative%20samples%0Aof%20appropriate%20hardness%20for%20each%20anchor%20instance%20during%20mini-batch%0Aconstruction%2C%20guided%20by%20a%20proxy%20for%20cross-modal%20alignment%20improvement.%0AExperimental%20results%20demonstrate%20that%20FALCON%20significantly%20improves%20performance%0Aacross%20two%20widely%20adopted%20VLP%20frameworks%20%28ALBEF%2C%20BLIP-2%29%20and%20a%20broad%20range%20of%0Adownstream%20tasks%20and%20evaluation%20settings%2C%20underscoring%20its%20effectiveness%20and%0Arobustness%20in%20mitigating%20the%20impact%20of%20false%20negatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFALCON%253A%2520False-Negative%2520Aware%2520Learning%2520of%2520Contrastive%2520Negatives%2520in%250A%2520%2520Vision-Language%2520Pretraining%26entry.906535625%3DMyunsoo%2520Kim%2520and%2520Seong-Woong%2520Shim%2520and%2520Byung-Jun%2520Lee%26entry.1292438233%3D%2520%2520False%2520negatives%2520pose%2520a%2520critical%2520challenge%2520in%2520vision-language%2520pretraining%250A%2528VLP%2529%2520due%2520to%2520the%2520many-to-many%2520correspondence%2520between%2520images%2520and%2520texts%2520in%250Alarge-scale%2520datasets.%2520These%2520false%2520negatives%2520introduce%2520conflicting%2520supervision%250Asignals%2520that%2520degrade%2520the%2520learned%2520embedding%2520space%2520and%2520diminish%2520the%2520effectiveness%250Aof%2520hard%2520negative%2520sampling.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FALCON%2520%2528False-negative%250AAware%2520Learning%2520of%2520COntrastive%2520Negatives%2529%252C%2520a%2520learning-based%2520mini-batch%250Aconstruction%2520strategy%2520that%2520adaptively%2520balances%2520the%2520trade-off%2520between%2520hard%2520and%250Afalse%2520negatives%2520during%2520VLP.%2520Rather%2520than%2520relying%2520on%2520fixed%2520heuristics%252C%2520FALCON%250Aemploys%2520a%2520negative%2520mining%2520scheduler%2520that%2520dynamically%2520selects%2520negative%2520samples%250Aof%2520appropriate%2520hardness%2520for%2520each%2520anchor%2520instance%2520during%2520mini-batch%250Aconstruction%252C%2520guided%2520by%2520a%2520proxy%2520for%2520cross-modal%2520alignment%2520improvement.%250AExperimental%2520results%2520demonstrate%2520that%2520FALCON%2520significantly%2520improves%2520performance%250Aacross%2520two%2520widely%2520adopted%2520VLP%2520frameworks%2520%2528ALBEF%252C%2520BLIP-2%2529%2520and%2520a%2520broad%2520range%2520of%250Adownstream%2520tasks%2520and%2520evaluation%2520settings%252C%2520underscoring%2520its%2520effectiveness%2520and%250Arobustness%2520in%2520mitigating%2520the%2520impact%2520of%2520false%2520negatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FALCON%3A%20False-Negative%20Aware%20Learning%20of%20Contrastive%20Negatives%20in%0A%20%20Vision-Language%20Pretraining&entry.906535625=Myunsoo%20Kim%20and%20Seong-Woong%20Shim%20and%20Byung-Jun%20Lee&entry.1292438233=%20%20False%20negatives%20pose%20a%20critical%20challenge%20in%20vision-language%20pretraining%0A%28VLP%29%20due%20to%20the%20many-to-many%20correspondence%20between%20images%20and%20texts%20in%0Alarge-scale%20datasets.%20These%20false%20negatives%20introduce%20conflicting%20supervision%0Asignals%20that%20degrade%20the%20learned%20embedding%20space%20and%20diminish%20the%20effectiveness%0Aof%20hard%20negative%20sampling.%20In%20this%20paper%2C%20we%20propose%20FALCON%20%28False-negative%0AAware%20Learning%20of%20COntrastive%20Negatives%29%2C%20a%20learning-based%20mini-batch%0Aconstruction%20strategy%20that%20adaptively%20balances%20the%20trade-off%20between%20hard%20and%0Afalse%20negatives%20during%20VLP.%20Rather%20than%20relying%20on%20fixed%20heuristics%2C%20FALCON%0Aemploys%20a%20negative%20mining%20scheduler%20that%20dynamically%20selects%20negative%20samples%0Aof%20appropriate%20hardness%20for%20each%20anchor%20instance%20during%20mini-batch%0Aconstruction%2C%20guided%20by%20a%20proxy%20for%20cross-modal%20alignment%20improvement.%0AExperimental%20results%20demonstrate%20that%20FALCON%20significantly%20improves%20performance%0Aacross%20two%20widely%20adopted%20VLP%20frameworks%20%28ALBEF%2C%20BLIP-2%29%20and%20a%20broad%20range%20of%0Adownstream%20tasks%20and%20evaluation%20settings%2C%20underscoring%20its%20effectiveness%20and%0Arobustness%20in%20mitigating%20the%20impact%20of%20false%20negatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11192v1&entry.124074799=Read"},
{"title": "Towards Anomaly-Aware Pre-Training and Fine-Tuning for Graph Anomaly\n  Detection", "author": "Yunhui Liu and Jiashun Cheng and Yiqing Lin and Qizhuo Xie and Jia Li and Fugee Tsung and Hongzhi Yin and Tao Zheng and Jianhua Zhao and Tieke He", "abstract": "  Graph anomaly detection (GAD) has garnered increasing attention in recent\nyears, yet remains challenging due to two key factors: (1) label scarcity\nstemming from the high cost of annotations and (2) homophily disparity at node\nand class levels. In this paper, we introduce Anomaly-Aware Pre-Training and\nFine-Tuning (APF), a targeted and effective framework to mitigate the above\nchallenges in GAD. In the pre-training stage, APF incorporates node-specific\nsubgraphs selected via the Rayleigh Quotient, a label-free anomaly metric, into\nthe learning objective to enhance anomaly awareness. It further introduces two\nlearnable spectral polynomial filters to jointly learn dual representations\nthat capture both general semantics and subtle anomaly cues. During\nfine-tuning, a gated fusion mechanism adaptively integrates pre-trained\nrepresentations across nodes and dimensions, while an anomaly-aware\nregularization loss encourages abnormal nodes to preserve more anomaly-relevant\ninformation. Furthermore, we theoretically show that APF tends to achieve\nlinear separability under mild conditions. Comprehensive experiments on 10\nbenchmark datasets validate the superior performance of APF in comparison to\nstate-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2504.14250v2", "date": "2025-05-16", "relevancy": 2.5278, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5205}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5123}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Anomaly-Aware%20Pre-Training%20and%20Fine-Tuning%20for%20Graph%20Anomaly%0A%20%20Detection&body=Title%3A%20Towards%20Anomaly-Aware%20Pre-Training%20and%20Fine-Tuning%20for%20Graph%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Yunhui%20Liu%20and%20Jiashun%20Cheng%20and%20Yiqing%20Lin%20and%20Qizhuo%20Xie%20and%20Jia%20Li%20and%20Fugee%20Tsung%20and%20Hongzhi%20Yin%20and%20Tao%20Zheng%20and%20Jianhua%20Zhao%20and%20Tieke%20He%0AAbstract%3A%20%20%20Graph%20anomaly%20detection%20%28GAD%29%20has%20garnered%20increasing%20attention%20in%20recent%0Ayears%2C%20yet%20remains%20challenging%20due%20to%20two%20key%20factors%3A%20%281%29%20label%20scarcity%0Astemming%20from%20the%20high%20cost%20of%20annotations%20and%20%282%29%20homophily%20disparity%20at%20node%0Aand%20class%20levels.%20In%20this%20paper%2C%20we%20introduce%20Anomaly-Aware%20Pre-Training%20and%0AFine-Tuning%20%28APF%29%2C%20a%20targeted%20and%20effective%20framework%20to%20mitigate%20the%20above%0Achallenges%20in%20GAD.%20In%20the%20pre-training%20stage%2C%20APF%20incorporates%20node-specific%0Asubgraphs%20selected%20via%20the%20Rayleigh%20Quotient%2C%20a%20label-free%20anomaly%20metric%2C%20into%0Athe%20learning%20objective%20to%20enhance%20anomaly%20awareness.%20It%20further%20introduces%20two%0Alearnable%20spectral%20polynomial%20filters%20to%20jointly%20learn%20dual%20representations%0Athat%20capture%20both%20general%20semantics%20and%20subtle%20anomaly%20cues.%20During%0Afine-tuning%2C%20a%20gated%20fusion%20mechanism%20adaptively%20integrates%20pre-trained%0Arepresentations%20across%20nodes%20and%20dimensions%2C%20while%20an%20anomaly-aware%0Aregularization%20loss%20encourages%20abnormal%20nodes%20to%20preserve%20more%20anomaly-relevant%0Ainformation.%20Furthermore%2C%20we%20theoretically%20show%20that%20APF%20tends%20to%20achieve%0Alinear%20separability%20under%20mild%20conditions.%20Comprehensive%20experiments%20on%2010%0Abenchmark%20datasets%20validate%20the%20superior%20performance%20of%20APF%20in%20comparison%20to%0Astate-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14250v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Anomaly-Aware%2520Pre-Training%2520and%2520Fine-Tuning%2520for%2520Graph%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DYunhui%2520Liu%2520and%2520Jiashun%2520Cheng%2520and%2520Yiqing%2520Lin%2520and%2520Qizhuo%2520Xie%2520and%2520Jia%2520Li%2520and%2520Fugee%2520Tsung%2520and%2520Hongzhi%2520Yin%2520and%2520Tao%2520Zheng%2520and%2520Jianhua%2520Zhao%2520and%2520Tieke%2520He%26entry.1292438233%3D%2520%2520Graph%2520anomaly%2520detection%2520%2528GAD%2529%2520has%2520garnered%2520increasing%2520attention%2520in%2520recent%250Ayears%252C%2520yet%2520remains%2520challenging%2520due%2520to%2520two%2520key%2520factors%253A%2520%25281%2529%2520label%2520scarcity%250Astemming%2520from%2520the%2520high%2520cost%2520of%2520annotations%2520and%2520%25282%2529%2520homophily%2520disparity%2520at%2520node%250Aand%2520class%2520levels.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Anomaly-Aware%2520Pre-Training%2520and%250AFine-Tuning%2520%2528APF%2529%252C%2520a%2520targeted%2520and%2520effective%2520framework%2520to%2520mitigate%2520the%2520above%250Achallenges%2520in%2520GAD.%2520In%2520the%2520pre-training%2520stage%252C%2520APF%2520incorporates%2520node-specific%250Asubgraphs%2520selected%2520via%2520the%2520Rayleigh%2520Quotient%252C%2520a%2520label-free%2520anomaly%2520metric%252C%2520into%250Athe%2520learning%2520objective%2520to%2520enhance%2520anomaly%2520awareness.%2520It%2520further%2520introduces%2520two%250Alearnable%2520spectral%2520polynomial%2520filters%2520to%2520jointly%2520learn%2520dual%2520representations%250Athat%2520capture%2520both%2520general%2520semantics%2520and%2520subtle%2520anomaly%2520cues.%2520During%250Afine-tuning%252C%2520a%2520gated%2520fusion%2520mechanism%2520adaptively%2520integrates%2520pre-trained%250Arepresentations%2520across%2520nodes%2520and%2520dimensions%252C%2520while%2520an%2520anomaly-aware%250Aregularization%2520loss%2520encourages%2520abnormal%2520nodes%2520to%2520preserve%2520more%2520anomaly-relevant%250Ainformation.%2520Furthermore%252C%2520we%2520theoretically%2520show%2520that%2520APF%2520tends%2520to%2520achieve%250Alinear%2520separability%2520under%2520mild%2520conditions.%2520Comprehensive%2520experiments%2520on%252010%250Abenchmark%2520datasets%2520validate%2520the%2520superior%2520performance%2520of%2520APF%2520in%2520comparison%2520to%250Astate-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14250v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Anomaly-Aware%20Pre-Training%20and%20Fine-Tuning%20for%20Graph%20Anomaly%0A%20%20Detection&entry.906535625=Yunhui%20Liu%20and%20Jiashun%20Cheng%20and%20Yiqing%20Lin%20and%20Qizhuo%20Xie%20and%20Jia%20Li%20and%20Fugee%20Tsung%20and%20Hongzhi%20Yin%20and%20Tao%20Zheng%20and%20Jianhua%20Zhao%20and%20Tieke%20He&entry.1292438233=%20%20Graph%20anomaly%20detection%20%28GAD%29%20has%20garnered%20increasing%20attention%20in%20recent%0Ayears%2C%20yet%20remains%20challenging%20due%20to%20two%20key%20factors%3A%20%281%29%20label%20scarcity%0Astemming%20from%20the%20high%20cost%20of%20annotations%20and%20%282%29%20homophily%20disparity%20at%20node%0Aand%20class%20levels.%20In%20this%20paper%2C%20we%20introduce%20Anomaly-Aware%20Pre-Training%20and%0AFine-Tuning%20%28APF%29%2C%20a%20targeted%20and%20effective%20framework%20to%20mitigate%20the%20above%0Achallenges%20in%20GAD.%20In%20the%20pre-training%20stage%2C%20APF%20incorporates%20node-specific%0Asubgraphs%20selected%20via%20the%20Rayleigh%20Quotient%2C%20a%20label-free%20anomaly%20metric%2C%20into%0Athe%20learning%20objective%20to%20enhance%20anomaly%20awareness.%20It%20further%20introduces%20two%0Alearnable%20spectral%20polynomial%20filters%20to%20jointly%20learn%20dual%20representations%0Athat%20capture%20both%20general%20semantics%20and%20subtle%20anomaly%20cues.%20During%0Afine-tuning%2C%20a%20gated%20fusion%20mechanism%20adaptively%20integrates%20pre-trained%0Arepresentations%20across%20nodes%20and%20dimensions%2C%20while%20an%20anomaly-aware%0Aregularization%20loss%20encourages%20abnormal%20nodes%20to%20preserve%20more%20anomaly-relevant%0Ainformation.%20Furthermore%2C%20we%20theoretically%20show%20that%20APF%20tends%20to%20achieve%0Alinear%20separability%20under%20mild%20conditions.%20Comprehensive%20experiments%20on%2010%0Abenchmark%20datasets%20validate%20the%20superior%20performance%20of%20APF%20in%20comparison%20to%0Astate-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14250v2&entry.124074799=Read"},
{"title": "Audio Turing Test: Benchmarking the Human-likeness of Large Language\n  Model-based Text-to-Speech Systems in Chinese", "author": "Xihuai Wang and Ziyi Zhao and Siyu Ren and Shao Zhang and Song Li and Xiaoyu Li and Ziwen Wang and Lin Qiu and Guanglu Wan and Xuezhi Cao and Xunliang Cai and Weinan Zhang", "abstract": "  Recent advances in large language models (LLMs) have significantly improved\ntext-to-speech (TTS) systems, enhancing control over speech style, naturalness,\nand emotional expression, which brings TTS Systems closer to human-level\nperformance. Although the Mean Opinion Score (MOS) remains the standard for TTS\nSystem evaluation, it suffers from subjectivity, environmental inconsistencies,\nand limited interpretability. Existing evaluation datasets also lack a\nmulti-dimensional design, often neglecting factors such as speaking styles,\ncontext diversity, and trap utterances, which is particularly evident in\nChinese TTS evaluation. To address these challenges, we introduce the Audio\nTuring Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired\nwith a simple, Turing-Test-inspired evaluation protocol. Instead of relying on\ncomplex MOS scales or direct model comparisons, ATT asks evaluators to judge\nwhether a voice sounds human. This simplification reduces rating bias and\nimproves evaluation robustness. To further support rapid model development, we\nalso finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for\nautomatic evaluation. Experimental results show that ATT effectively\ndifferentiates models across specific capability dimensions using its\nmulti-dimensional design. Auto-ATT also demonstrates strong alignment with\nhuman evaluations, confirming its value as a fast and reliable assessment tool.\nThe white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face\nCollection\n(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).\n", "link": "http://arxiv.org/abs/2505.11200v1", "date": "2025-05-16", "relevancy": 2.5173, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio%20Turing%20Test%3A%20Benchmarking%20the%20Human-likeness%20of%20Large%20Language%0A%20%20Model-based%20Text-to-Speech%20Systems%20in%20Chinese&body=Title%3A%20Audio%20Turing%20Test%3A%20Benchmarking%20the%20Human-likeness%20of%20Large%20Language%0A%20%20Model-based%20Text-to-Speech%20Systems%20in%20Chinese%0AAuthor%3A%20Xihuai%20Wang%20and%20Ziyi%20Zhao%20and%20Siyu%20Ren%20and%20Shao%20Zhang%20and%20Song%20Li%20and%20Xiaoyu%20Li%20and%20Ziwen%20Wang%20and%20Lin%20Qiu%20and%20Guanglu%20Wan%20and%20Xuezhi%20Cao%20and%20Xunliang%20Cai%20and%20Weinan%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%20improved%0Atext-to-speech%20%28TTS%29%20systems%2C%20enhancing%20control%20over%20speech%20style%2C%20naturalness%2C%0Aand%20emotional%20expression%2C%20which%20brings%20TTS%20Systems%20closer%20to%20human-level%0Aperformance.%20Although%20the%20Mean%20Opinion%20Score%20%28MOS%29%20remains%20the%20standard%20for%20TTS%0ASystem%20evaluation%2C%20it%20suffers%20from%20subjectivity%2C%20environmental%20inconsistencies%2C%0Aand%20limited%20interpretability.%20Existing%20evaluation%20datasets%20also%20lack%20a%0Amulti-dimensional%20design%2C%20often%20neglecting%20factors%20such%20as%20speaking%20styles%2C%0Acontext%20diversity%2C%20and%20trap%20utterances%2C%20which%20is%20particularly%20evident%20in%0AChinese%20TTS%20evaluation.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%20Audio%0ATuring%20Test%20%28ATT%29%2C%20a%20multi-dimensional%20Chinese%20corpus%20dataset%20ATT-Corpus%20paired%0Awith%20a%20simple%2C%20Turing-Test-inspired%20evaluation%20protocol.%20Instead%20of%20relying%20on%0Acomplex%20MOS%20scales%20or%20direct%20model%20comparisons%2C%20ATT%20asks%20evaluators%20to%20judge%0Awhether%20a%20voice%20sounds%20human.%20This%20simplification%20reduces%20rating%20bias%20and%0Aimproves%20evaluation%20robustness.%20To%20further%20support%20rapid%20model%20development%2C%20we%0Aalso%20finetune%20Qwen2-Audio-Instruct%20with%20human%20judgment%20data%20as%20Auto-ATT%20for%0Aautomatic%20evaluation.%20Experimental%20results%20show%20that%20ATT%20effectively%0Adifferentiates%20models%20across%20specific%20capability%20dimensions%20using%20its%0Amulti-dimensional%20design.%20Auto-ATT%20also%20demonstrates%20strong%20alignment%20with%0Ahuman%20evaluations%2C%20confirming%20its%20value%20as%20a%20fast%20and%20reliable%20assessment%20tool.%0AThe%20white-box%20ATT-Corpus%20and%20Auto-ATT%20can%20be%20found%20in%20ATT%20Hugging%20Face%0ACollection%0A%28https%3A//huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio%2520Turing%2520Test%253A%2520Benchmarking%2520the%2520Human-likeness%2520of%2520Large%2520Language%250A%2520%2520Model-based%2520Text-to-Speech%2520Systems%2520in%2520Chinese%26entry.906535625%3DXihuai%2520Wang%2520and%2520Ziyi%2520Zhao%2520and%2520Siyu%2520Ren%2520and%2520Shao%2520Zhang%2520and%2520Song%2520Li%2520and%2520Xiaoyu%2520Li%2520and%2520Ziwen%2520Wang%2520and%2520Lin%2520Qiu%2520and%2520Guanglu%2520Wan%2520and%2520Xuezhi%2520Cao%2520and%2520Xunliang%2520Cai%2520and%2520Weinan%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520improved%250Atext-to-speech%2520%2528TTS%2529%2520systems%252C%2520enhancing%2520control%2520over%2520speech%2520style%252C%2520naturalness%252C%250Aand%2520emotional%2520expression%252C%2520which%2520brings%2520TTS%2520Systems%2520closer%2520to%2520human-level%250Aperformance.%2520Although%2520the%2520Mean%2520Opinion%2520Score%2520%2528MOS%2529%2520remains%2520the%2520standard%2520for%2520TTS%250ASystem%2520evaluation%252C%2520it%2520suffers%2520from%2520subjectivity%252C%2520environmental%2520inconsistencies%252C%250Aand%2520limited%2520interpretability.%2520Existing%2520evaluation%2520datasets%2520also%2520lack%2520a%250Amulti-dimensional%2520design%252C%2520often%2520neglecting%2520factors%2520such%2520as%2520speaking%2520styles%252C%250Acontext%2520diversity%252C%2520and%2520trap%2520utterances%252C%2520which%2520is%2520particularly%2520evident%2520in%250AChinese%2520TTS%2520evaluation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520Audio%250ATuring%2520Test%2520%2528ATT%2529%252C%2520a%2520multi-dimensional%2520Chinese%2520corpus%2520dataset%2520ATT-Corpus%2520paired%250Awith%2520a%2520simple%252C%2520Turing-Test-inspired%2520evaluation%2520protocol.%2520Instead%2520of%2520relying%2520on%250Acomplex%2520MOS%2520scales%2520or%2520direct%2520model%2520comparisons%252C%2520ATT%2520asks%2520evaluators%2520to%2520judge%250Awhether%2520a%2520voice%2520sounds%2520human.%2520This%2520simplification%2520reduces%2520rating%2520bias%2520and%250Aimproves%2520evaluation%2520robustness.%2520To%2520further%2520support%2520rapid%2520model%2520development%252C%2520we%250Aalso%2520finetune%2520Qwen2-Audio-Instruct%2520with%2520human%2520judgment%2520data%2520as%2520Auto-ATT%2520for%250Aautomatic%2520evaluation.%2520Experimental%2520results%2520show%2520that%2520ATT%2520effectively%250Adifferentiates%2520models%2520across%2520specific%2520capability%2520dimensions%2520using%2520its%250Amulti-dimensional%2520design.%2520Auto-ATT%2520also%2520demonstrates%2520strong%2520alignment%2520with%250Ahuman%2520evaluations%252C%2520confirming%2520its%2520value%2520as%2520a%2520fast%2520and%2520reliable%2520assessment%2520tool.%250AThe%2520white-box%2520ATT-Corpus%2520and%2520Auto-ATT%2520can%2520be%2520found%2520in%2520ATT%2520Hugging%2520Face%250ACollection%250A%2528https%253A//huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio%20Turing%20Test%3A%20Benchmarking%20the%20Human-likeness%20of%20Large%20Language%0A%20%20Model-based%20Text-to-Speech%20Systems%20in%20Chinese&entry.906535625=Xihuai%20Wang%20and%20Ziyi%20Zhao%20and%20Siyu%20Ren%20and%20Shao%20Zhang%20and%20Song%20Li%20and%20Xiaoyu%20Li%20and%20Ziwen%20Wang%20and%20Lin%20Qiu%20and%20Guanglu%20Wan%20and%20Xuezhi%20Cao%20and%20Xunliang%20Cai%20and%20Weinan%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%20improved%0Atext-to-speech%20%28TTS%29%20systems%2C%20enhancing%20control%20over%20speech%20style%2C%20naturalness%2C%0Aand%20emotional%20expression%2C%20which%20brings%20TTS%20Systems%20closer%20to%20human-level%0Aperformance.%20Although%20the%20Mean%20Opinion%20Score%20%28MOS%29%20remains%20the%20standard%20for%20TTS%0ASystem%20evaluation%2C%20it%20suffers%20from%20subjectivity%2C%20environmental%20inconsistencies%2C%0Aand%20limited%20interpretability.%20Existing%20evaluation%20datasets%20also%20lack%20a%0Amulti-dimensional%20design%2C%20often%20neglecting%20factors%20such%20as%20speaking%20styles%2C%0Acontext%20diversity%2C%20and%20trap%20utterances%2C%20which%20is%20particularly%20evident%20in%0AChinese%20TTS%20evaluation.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%20Audio%0ATuring%20Test%20%28ATT%29%2C%20a%20multi-dimensional%20Chinese%20corpus%20dataset%20ATT-Corpus%20paired%0Awith%20a%20simple%2C%20Turing-Test-inspired%20evaluation%20protocol.%20Instead%20of%20relying%20on%0Acomplex%20MOS%20scales%20or%20direct%20model%20comparisons%2C%20ATT%20asks%20evaluators%20to%20judge%0Awhether%20a%20voice%20sounds%20human.%20This%20simplification%20reduces%20rating%20bias%20and%0Aimproves%20evaluation%20robustness.%20To%20further%20support%20rapid%20model%20development%2C%20we%0Aalso%20finetune%20Qwen2-Audio-Instruct%20with%20human%20judgment%20data%20as%20Auto-ATT%20for%0Aautomatic%20evaluation.%20Experimental%20results%20show%20that%20ATT%20effectively%0Adifferentiates%20models%20across%20specific%20capability%20dimensions%20using%20its%0Amulti-dimensional%20design.%20Auto-ATT%20also%20demonstrates%20strong%20alignment%20with%0Ahuman%20evaluations%2C%20confirming%20its%20value%20as%20a%20fast%20and%20reliable%20assessment%20tool.%0AThe%20white-box%20ATT-Corpus%20and%20Auto-ATT%20can%20be%20found%20in%20ATT%20Hugging%20Face%0ACollection%0A%28https%3A//huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11200v1&entry.124074799=Read"},
{"title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long\n  Preference Optimization", "author": "Huashan Sun and Shengyi Liao and Yansen Han and Yu Bai and Yang Gao and Cheng Fu and Weizhou Shen and Fanqi Wan and Ming Yan and Ji Zhang and Fei Huang", "abstract": "  Despite advances in pretraining with extended context lengths, large language\nmodels (LLMs) still face challenges in effectively utilizing real-world\nlong-context information, primarily due to insufficient long-context alignment\ncaused by data quality issues, training inefficiencies, and the lack of\nwell-designed optimization objectives. To address these limitations, we propose\na framework named $\\textbf{S}$h$\\textbf{o}$rt-to-$\\textbf{Lo}$ng\n$\\textbf{P}$reference $\\textbf{O}$ptimization ($\\textbf{SoLoPO}$), decoupling\nlong-context preference optimization (PO) into two components: short-context PO\nand short-to-long reward alignment (SoLo-RA), supported by both theoretical and\nempirical evidence. Specifically, short-context PO leverages preference pairs\nsampled from short contexts to enhance the model's contextual knowledge\nutilization ability. Meanwhile, SoLo-RA explicitly encourages reward score\nconsistency utilization for the responses when conditioned on both short and\nlong contexts that contain identical task-relevant information. This\nfacilitates transferring the model's ability to handle short contexts into\nlong-context scenarios. SoLoPO is compatible with mainstream preference\noptimization algorithms, while substantially improving the efficiency of data\nconstruction and training processes. Experimental results show that SoLoPO\nenhances all these algorithms with respect to stronger length and domain\ngeneralization abilities across various long-context benchmarks, while\nachieving notable improvements in both computational and memory efficiency.\n", "link": "http://arxiv.org/abs/2505.11166v1", "date": "2025-05-16", "relevancy": 2.5076, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoLoPO%3A%20Unlocking%20Long-Context%20Capabilities%20in%20LLMs%20via%20Short-to-Long%0A%20%20Preference%20Optimization&body=Title%3A%20SoLoPO%3A%20Unlocking%20Long-Context%20Capabilities%20in%20LLMs%20via%20Short-to-Long%0A%20%20Preference%20Optimization%0AAuthor%3A%20Huashan%20Sun%20and%20Shengyi%20Liao%20and%20Yansen%20Han%20and%20Yu%20Bai%20and%20Yang%20Gao%20and%20Cheng%20Fu%20and%20Weizhou%20Shen%20and%20Fanqi%20Wan%20and%20Ming%20Yan%20and%20Ji%20Zhang%20and%20Fei%20Huang%0AAbstract%3A%20%20%20Despite%20advances%20in%20pretraining%20with%20extended%20context%20lengths%2C%20large%20language%0Amodels%20%28LLMs%29%20still%20face%20challenges%20in%20effectively%20utilizing%20real-world%0Along-context%20information%2C%20primarily%20due%20to%20insufficient%20long-context%20alignment%0Acaused%20by%20data%20quality%20issues%2C%20training%20inefficiencies%2C%20and%20the%20lack%20of%0Awell-designed%20optimization%20objectives.%20To%20address%20these%20limitations%2C%20we%20propose%0Aa%20framework%20named%20%24%5Ctextbf%7BS%7D%24h%24%5Ctextbf%7Bo%7D%24rt-to-%24%5Ctextbf%7BLo%7D%24ng%0A%24%5Ctextbf%7BP%7D%24reference%20%24%5Ctextbf%7BO%7D%24ptimization%20%28%24%5Ctextbf%7BSoLoPO%7D%24%29%2C%20decoupling%0Along-context%20preference%20optimization%20%28PO%29%20into%20two%20components%3A%20short-context%20PO%0Aand%20short-to-long%20reward%20alignment%20%28SoLo-RA%29%2C%20supported%20by%20both%20theoretical%20and%0Aempirical%20evidence.%20Specifically%2C%20short-context%20PO%20leverages%20preference%20pairs%0Asampled%20from%20short%20contexts%20to%20enhance%20the%20model%27s%20contextual%20knowledge%0Autilization%20ability.%20Meanwhile%2C%20SoLo-RA%20explicitly%20encourages%20reward%20score%0Aconsistency%20utilization%20for%20the%20responses%20when%20conditioned%20on%20both%20short%20and%0Along%20contexts%20that%20contain%20identical%20task-relevant%20information.%20This%0Afacilitates%20transferring%20the%20model%27s%20ability%20to%20handle%20short%20contexts%20into%0Along-context%20scenarios.%20SoLoPO%20is%20compatible%20with%20mainstream%20preference%0Aoptimization%20algorithms%2C%20while%20substantially%20improving%20the%20efficiency%20of%20data%0Aconstruction%20and%20training%20processes.%20Experimental%20results%20show%20that%20SoLoPO%0Aenhances%20all%20these%20algorithms%20with%20respect%20to%20stronger%20length%20and%20domain%0Ageneralization%20abilities%20across%20various%20long-context%20benchmarks%2C%20while%0Aachieving%20notable%20improvements%20in%20both%20computational%20and%20memory%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoLoPO%253A%2520Unlocking%2520Long-Context%2520Capabilities%2520in%2520LLMs%2520via%2520Short-to-Long%250A%2520%2520Preference%2520Optimization%26entry.906535625%3DHuashan%2520Sun%2520and%2520Shengyi%2520Liao%2520and%2520Yansen%2520Han%2520and%2520Yu%2520Bai%2520and%2520Yang%2520Gao%2520and%2520Cheng%2520Fu%2520and%2520Weizhou%2520Shen%2520and%2520Fanqi%2520Wan%2520and%2520Ming%2520Yan%2520and%2520Ji%2520Zhang%2520and%2520Fei%2520Huang%26entry.1292438233%3D%2520%2520Despite%2520advances%2520in%2520pretraining%2520with%2520extended%2520context%2520lengths%252C%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520still%2520face%2520challenges%2520in%2520effectively%2520utilizing%2520real-world%250Along-context%2520information%252C%2520primarily%2520due%2520to%2520insufficient%2520long-context%2520alignment%250Acaused%2520by%2520data%2520quality%2520issues%252C%2520training%2520inefficiencies%252C%2520and%2520the%2520lack%2520of%250Awell-designed%2520optimization%2520objectives.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250Aa%2520framework%2520named%2520%2524%255Ctextbf%257BS%257D%2524h%2524%255Ctextbf%257Bo%257D%2524rt-to-%2524%255Ctextbf%257BLo%257D%2524ng%250A%2524%255Ctextbf%257BP%257D%2524reference%2520%2524%255Ctextbf%257BO%257D%2524ptimization%2520%2528%2524%255Ctextbf%257BSoLoPO%257D%2524%2529%252C%2520decoupling%250Along-context%2520preference%2520optimization%2520%2528PO%2529%2520into%2520two%2520components%253A%2520short-context%2520PO%250Aand%2520short-to-long%2520reward%2520alignment%2520%2528SoLo-RA%2529%252C%2520supported%2520by%2520both%2520theoretical%2520and%250Aempirical%2520evidence.%2520Specifically%252C%2520short-context%2520PO%2520leverages%2520preference%2520pairs%250Asampled%2520from%2520short%2520contexts%2520to%2520enhance%2520the%2520model%2527s%2520contextual%2520knowledge%250Autilization%2520ability.%2520Meanwhile%252C%2520SoLo-RA%2520explicitly%2520encourages%2520reward%2520score%250Aconsistency%2520utilization%2520for%2520the%2520responses%2520when%2520conditioned%2520on%2520both%2520short%2520and%250Along%2520contexts%2520that%2520contain%2520identical%2520task-relevant%2520information.%2520This%250Afacilitates%2520transferring%2520the%2520model%2527s%2520ability%2520to%2520handle%2520short%2520contexts%2520into%250Along-context%2520scenarios.%2520SoLoPO%2520is%2520compatible%2520with%2520mainstream%2520preference%250Aoptimization%2520algorithms%252C%2520while%2520substantially%2520improving%2520the%2520efficiency%2520of%2520data%250Aconstruction%2520and%2520training%2520processes.%2520Experimental%2520results%2520show%2520that%2520SoLoPO%250Aenhances%2520all%2520these%2520algorithms%2520with%2520respect%2520to%2520stronger%2520length%2520and%2520domain%250Ageneralization%2520abilities%2520across%2520various%2520long-context%2520benchmarks%252C%2520while%250Aachieving%2520notable%2520improvements%2520in%2520both%2520computational%2520and%2520memory%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoLoPO%3A%20Unlocking%20Long-Context%20Capabilities%20in%20LLMs%20via%20Short-to-Long%0A%20%20Preference%20Optimization&entry.906535625=Huashan%20Sun%20and%20Shengyi%20Liao%20and%20Yansen%20Han%20and%20Yu%20Bai%20and%20Yang%20Gao%20and%20Cheng%20Fu%20and%20Weizhou%20Shen%20and%20Fanqi%20Wan%20and%20Ming%20Yan%20and%20Ji%20Zhang%20and%20Fei%20Huang&entry.1292438233=%20%20Despite%20advances%20in%20pretraining%20with%20extended%20context%20lengths%2C%20large%20language%0Amodels%20%28LLMs%29%20still%20face%20challenges%20in%20effectively%20utilizing%20real-world%0Along-context%20information%2C%20primarily%20due%20to%20insufficient%20long-context%20alignment%0Acaused%20by%20data%20quality%20issues%2C%20training%20inefficiencies%2C%20and%20the%20lack%20of%0Awell-designed%20optimization%20objectives.%20To%20address%20these%20limitations%2C%20we%20propose%0Aa%20framework%20named%20%24%5Ctextbf%7BS%7D%24h%24%5Ctextbf%7Bo%7D%24rt-to-%24%5Ctextbf%7BLo%7D%24ng%0A%24%5Ctextbf%7BP%7D%24reference%20%24%5Ctextbf%7BO%7D%24ptimization%20%28%24%5Ctextbf%7BSoLoPO%7D%24%29%2C%20decoupling%0Along-context%20preference%20optimization%20%28PO%29%20into%20two%20components%3A%20short-context%20PO%0Aand%20short-to-long%20reward%20alignment%20%28SoLo-RA%29%2C%20supported%20by%20both%20theoretical%20and%0Aempirical%20evidence.%20Specifically%2C%20short-context%20PO%20leverages%20preference%20pairs%0Asampled%20from%20short%20contexts%20to%20enhance%20the%20model%27s%20contextual%20knowledge%0Autilization%20ability.%20Meanwhile%2C%20SoLo-RA%20explicitly%20encourages%20reward%20score%0Aconsistency%20utilization%20for%20the%20responses%20when%20conditioned%20on%20both%20short%20and%0Along%20contexts%20that%20contain%20identical%20task-relevant%20information.%20This%0Afacilitates%20transferring%20the%20model%27s%20ability%20to%20handle%20short%20contexts%20into%0Along-context%20scenarios.%20SoLoPO%20is%20compatible%20with%20mainstream%20preference%0Aoptimization%20algorithms%2C%20while%20substantially%20improving%20the%20efficiency%20of%20data%0Aconstruction%20and%20training%20processes.%20Experimental%20results%20show%20that%20SoLoPO%0Aenhances%20all%20these%20algorithms%20with%20respect%20to%20stronger%20length%20and%20domain%0Ageneralization%20abilities%20across%20various%20long-context%20benchmarks%2C%20while%0Aachieving%20notable%20improvements%20in%20both%20computational%20and%20memory%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11166v1&entry.124074799=Read"},
{"title": "On the Universality of Self-Supervised Learning", "author": "Wenwen Qiang and Jingyao Wang and Changwen Zheng and Hui Xiong and Gang Hua", "abstract": "  In this paper, we investigate what constitutes a good representation or model\nin self-supervised learning (SSL). We argue that a good representation should\nexhibit universality, characterized by three essential properties:\ndiscriminability, generalizability, and transferability. While these\ncapabilities are implicitly desired in most SSL frameworks, existing methods\nlack an explicit modeling of universality, and its theoretical foundations\nremain underexplored. To address these gaps, we propose General SSL (GeSSL), a\nnovel framework that explicitly models universality from three complementary\ndimensions: the optimization objective, the parameter update mechanism, and the\nlearning paradigm. GeSSL integrates a bi-level optimization structure that\njointly models task-specific adaptation and cross-task consistency, thereby\ncapturing all three aspects of universality within a unified SSL objective.\nFurthermore, we derive a theoretical generalization bound, ensuring that the\noptimization process of GeSSL consistently leads to representations that\ngeneralize well to unseen tasks. Empirical results on multiple benchmark\ndatasets demonstrate that GeSSL consistently achieves superior performance\nacross diverse downstream tasks, validating its effectiveness in modeling\nuniversal representations.\n", "link": "http://arxiv.org/abs/2405.01053v5", "date": "2025-05-16", "relevancy": 2.4958, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5275}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4862}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Universality%20of%20Self-Supervised%20Learning&body=Title%3A%20On%20the%20Universality%20of%20Self-Supervised%20Learning%0AAuthor%3A%20Wenwen%20Qiang%20and%20Jingyao%20Wang%20and%20Changwen%20Zheng%20and%20Hui%20Xiong%20and%20Gang%20Hua%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20what%20constitutes%20a%20good%20representation%20or%20model%0Ain%20self-supervised%20learning%20%28SSL%29.%20We%20argue%20that%20a%20good%20representation%20should%0Aexhibit%20universality%2C%20characterized%20by%20three%20essential%20properties%3A%0Adiscriminability%2C%20generalizability%2C%20and%20transferability.%20While%20these%0Acapabilities%20are%20implicitly%20desired%20in%20most%20SSL%20frameworks%2C%20existing%20methods%0Alack%20an%20explicit%20modeling%20of%20universality%2C%20and%20its%20theoretical%20foundations%0Aremain%20underexplored.%20To%20address%20these%20gaps%2C%20we%20propose%20General%20SSL%20%28GeSSL%29%2C%20a%0Anovel%20framework%20that%20explicitly%20models%20universality%20from%20three%20complementary%0Adimensions%3A%20the%20optimization%20objective%2C%20the%20parameter%20update%20mechanism%2C%20and%20the%0Alearning%20paradigm.%20GeSSL%20integrates%20a%20bi-level%20optimization%20structure%20that%0Ajointly%20models%20task-specific%20adaptation%20and%20cross-task%20consistency%2C%20thereby%0Acapturing%20all%20three%20aspects%20of%20universality%20within%20a%20unified%20SSL%20objective.%0AFurthermore%2C%20we%20derive%20a%20theoretical%20generalization%20bound%2C%20ensuring%20that%20the%0Aoptimization%20process%20of%20GeSSL%20consistently%20leads%20to%20representations%20that%0Ageneralize%20well%20to%20unseen%20tasks.%20Empirical%20results%20on%20multiple%20benchmark%0Adatasets%20demonstrate%20that%20GeSSL%20consistently%20achieves%20superior%20performance%0Aacross%20diverse%20downstream%20tasks%2C%20validating%20its%20effectiveness%20in%20modeling%0Auniversal%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01053v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Universality%2520of%2520Self-Supervised%2520Learning%26entry.906535625%3DWenwen%2520Qiang%2520and%2520Jingyao%2520Wang%2520and%2520Changwen%2520Zheng%2520and%2520Hui%2520Xiong%2520and%2520Gang%2520Hua%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520what%2520constitutes%2520a%2520good%2520representation%2520or%2520model%250Ain%2520self-supervised%2520learning%2520%2528SSL%2529.%2520We%2520argue%2520that%2520a%2520good%2520representation%2520should%250Aexhibit%2520universality%252C%2520characterized%2520by%2520three%2520essential%2520properties%253A%250Adiscriminability%252C%2520generalizability%252C%2520and%2520transferability.%2520While%2520these%250Acapabilities%2520are%2520implicitly%2520desired%2520in%2520most%2520SSL%2520frameworks%252C%2520existing%2520methods%250Alack%2520an%2520explicit%2520modeling%2520of%2520universality%252C%2520and%2520its%2520theoretical%2520foundations%250Aremain%2520underexplored.%2520To%2520address%2520these%2520gaps%252C%2520we%2520propose%2520General%2520SSL%2520%2528GeSSL%2529%252C%2520a%250Anovel%2520framework%2520that%2520explicitly%2520models%2520universality%2520from%2520three%2520complementary%250Adimensions%253A%2520the%2520optimization%2520objective%252C%2520the%2520parameter%2520update%2520mechanism%252C%2520and%2520the%250Alearning%2520paradigm.%2520GeSSL%2520integrates%2520a%2520bi-level%2520optimization%2520structure%2520that%250Ajointly%2520models%2520task-specific%2520adaptation%2520and%2520cross-task%2520consistency%252C%2520thereby%250Acapturing%2520all%2520three%2520aspects%2520of%2520universality%2520within%2520a%2520unified%2520SSL%2520objective.%250AFurthermore%252C%2520we%2520derive%2520a%2520theoretical%2520generalization%2520bound%252C%2520ensuring%2520that%2520the%250Aoptimization%2520process%2520of%2520GeSSL%2520consistently%2520leads%2520to%2520representations%2520that%250Ageneralize%2520well%2520to%2520unseen%2520tasks.%2520Empirical%2520results%2520on%2520multiple%2520benchmark%250Adatasets%2520demonstrate%2520that%2520GeSSL%2520consistently%2520achieves%2520superior%2520performance%250Aacross%2520diverse%2520downstream%2520tasks%252C%2520validating%2520its%2520effectiveness%2520in%2520modeling%250Auniversal%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01053v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Universality%20of%20Self-Supervised%20Learning&entry.906535625=Wenwen%20Qiang%20and%20Jingyao%20Wang%20and%20Changwen%20Zheng%20and%20Hui%20Xiong%20and%20Gang%20Hua&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20what%20constitutes%20a%20good%20representation%20or%20model%0Ain%20self-supervised%20learning%20%28SSL%29.%20We%20argue%20that%20a%20good%20representation%20should%0Aexhibit%20universality%2C%20characterized%20by%20three%20essential%20properties%3A%0Adiscriminability%2C%20generalizability%2C%20and%20transferability.%20While%20these%0Acapabilities%20are%20implicitly%20desired%20in%20most%20SSL%20frameworks%2C%20existing%20methods%0Alack%20an%20explicit%20modeling%20of%20universality%2C%20and%20its%20theoretical%20foundations%0Aremain%20underexplored.%20To%20address%20these%20gaps%2C%20we%20propose%20General%20SSL%20%28GeSSL%29%2C%20a%0Anovel%20framework%20that%20explicitly%20models%20universality%20from%20three%20complementary%0Adimensions%3A%20the%20optimization%20objective%2C%20the%20parameter%20update%20mechanism%2C%20and%20the%0Alearning%20paradigm.%20GeSSL%20integrates%20a%20bi-level%20optimization%20structure%20that%0Ajointly%20models%20task-specific%20adaptation%20and%20cross-task%20consistency%2C%20thereby%0Acapturing%20all%20three%20aspects%20of%20universality%20within%20a%20unified%20SSL%20objective.%0AFurthermore%2C%20we%20derive%20a%20theoretical%20generalization%20bound%2C%20ensuring%20that%20the%0Aoptimization%20process%20of%20GeSSL%20consistently%20leads%20to%20representations%20that%0Ageneralize%20well%20to%20unseen%20tasks.%20Empirical%20results%20on%20multiple%20benchmark%0Adatasets%20demonstrate%20that%20GeSSL%20consistently%20achieves%20superior%20performance%0Aacross%20diverse%20downstream%20tasks%2C%20validating%20its%20effectiveness%20in%20modeling%0Auniversal%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01053v5&entry.124074799=Read"},
{"title": "Customizing Visual-Language Foundation Models for Multi-modal Anomaly\n  Detection and Reasoning", "author": "Xiaohao Xu and Yunkang Cao and Huaxin Zhang and Nong Sang and Xiaonan Huang", "abstract": "  Anomaly detection is vital in various industrial scenarios, including the\nidentification of unusual patterns in production lines and the detection of\nmanufacturing defects for quality control. Existing techniques tend to be\nspecialized in individual scenarios and lack generalization capacities. In this\nstudy, our objective is to develop a generic anomaly detection model that can\nbe applied in multiple scenarios. To achieve this, we custom-build generic\nvisual language foundation models that possess extensive knowledge and robust\nreasoning abilities as anomaly detectors and reasoners. Specifically, we\nintroduce a multi-modal prompting strategy that incorporates domain knowledge\nfrom experts as conditions to guide the models. Our approach considers diverse\nprompt types, including task descriptions, class context, normality rules, and\nreference images. In addition, we unify the input representation of\nmulti-modality into a 2D image format, enabling multi-modal anomaly detection\nand reasoning. Our preliminary studies demonstrate that combining visual and\nlanguage prompts as conditions for customizing the models enhances anomaly\ndetection performance. The customized models showcase the ability to detect\nanomalies across different data modalities such as images, point clouds, and\nvideos. Qualitative case studies further highlight the anomaly detection and\nreasoning capabilities, particularly for multi-object scenes and temporal data.\nOur code is publicly available at\nhttps://github.com/Xiaohao-Xu/Customizable-VLM\n", "link": "http://arxiv.org/abs/2403.11083v2", "date": "2025-05-16", "relevancy": 2.4892, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Customizing%20Visual-Language%20Foundation%20Models%20for%20Multi-modal%20Anomaly%0A%20%20Detection%20and%20Reasoning&body=Title%3A%20Customizing%20Visual-Language%20Foundation%20Models%20for%20Multi-modal%20Anomaly%0A%20%20Detection%20and%20Reasoning%0AAuthor%3A%20Xiaohao%20Xu%20and%20Yunkang%20Cao%20and%20Huaxin%20Zhang%20and%20Nong%20Sang%20and%20Xiaonan%20Huang%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20vital%20in%20various%20industrial%20scenarios%2C%20including%20the%0Aidentification%20of%20unusual%20patterns%20in%20production%20lines%20and%20the%20detection%20of%0Amanufacturing%20defects%20for%20quality%20control.%20Existing%20techniques%20tend%20to%20be%0Aspecialized%20in%20individual%20scenarios%20and%20lack%20generalization%20capacities.%20In%20this%0Astudy%2C%20our%20objective%20is%20to%20develop%20a%20generic%20anomaly%20detection%20model%20that%20can%0Abe%20applied%20in%20multiple%20scenarios.%20To%20achieve%20this%2C%20we%20custom-build%20generic%0Avisual%20language%20foundation%20models%20that%20possess%20extensive%20knowledge%20and%20robust%0Areasoning%20abilities%20as%20anomaly%20detectors%20and%20reasoners.%20Specifically%2C%20we%0Aintroduce%20a%20multi-modal%20prompting%20strategy%20that%20incorporates%20domain%20knowledge%0Afrom%20experts%20as%20conditions%20to%20guide%20the%20models.%20Our%20approach%20considers%20diverse%0Aprompt%20types%2C%20including%20task%20descriptions%2C%20class%20context%2C%20normality%20rules%2C%20and%0Areference%20images.%20In%20addition%2C%20we%20unify%20the%20input%20representation%20of%0Amulti-modality%20into%20a%202D%20image%20format%2C%20enabling%20multi-modal%20anomaly%20detection%0Aand%20reasoning.%20Our%20preliminary%20studies%20demonstrate%20that%20combining%20visual%20and%0Alanguage%20prompts%20as%20conditions%20for%20customizing%20the%20models%20enhances%20anomaly%0Adetection%20performance.%20The%20customized%20models%20showcase%20the%20ability%20to%20detect%0Aanomalies%20across%20different%20data%20modalities%20such%20as%20images%2C%20point%20clouds%2C%20and%0Avideos.%20Qualitative%20case%20studies%20further%20highlight%20the%20anomaly%20detection%20and%0Areasoning%20capabilities%2C%20particularly%20for%20multi-object%20scenes%20and%20temporal%20data.%0AOur%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Xiaohao-Xu/Customizable-VLM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustomizing%2520Visual-Language%2520Foundation%2520Models%2520for%2520Multi-modal%2520Anomaly%250A%2520%2520Detection%2520and%2520Reasoning%26entry.906535625%3DXiaohao%2520Xu%2520and%2520Yunkang%2520Cao%2520and%2520Huaxin%2520Zhang%2520and%2520Nong%2520Sang%2520and%2520Xiaonan%2520Huang%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520vital%2520in%2520various%2520industrial%2520scenarios%252C%2520including%2520the%250Aidentification%2520of%2520unusual%2520patterns%2520in%2520production%2520lines%2520and%2520the%2520detection%2520of%250Amanufacturing%2520defects%2520for%2520quality%2520control.%2520Existing%2520techniques%2520tend%2520to%2520be%250Aspecialized%2520in%2520individual%2520scenarios%2520and%2520lack%2520generalization%2520capacities.%2520In%2520this%250Astudy%252C%2520our%2520objective%2520is%2520to%2520develop%2520a%2520generic%2520anomaly%2520detection%2520model%2520that%2520can%250Abe%2520applied%2520in%2520multiple%2520scenarios.%2520To%2520achieve%2520this%252C%2520we%2520custom-build%2520generic%250Avisual%2520language%2520foundation%2520models%2520that%2520possess%2520extensive%2520knowledge%2520and%2520robust%250Areasoning%2520abilities%2520as%2520anomaly%2520detectors%2520and%2520reasoners.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520multi-modal%2520prompting%2520strategy%2520that%2520incorporates%2520domain%2520knowledge%250Afrom%2520experts%2520as%2520conditions%2520to%2520guide%2520the%2520models.%2520Our%2520approach%2520considers%2520diverse%250Aprompt%2520types%252C%2520including%2520task%2520descriptions%252C%2520class%2520context%252C%2520normality%2520rules%252C%2520and%250Areference%2520images.%2520In%2520addition%252C%2520we%2520unify%2520the%2520input%2520representation%2520of%250Amulti-modality%2520into%2520a%25202D%2520image%2520format%252C%2520enabling%2520multi-modal%2520anomaly%2520detection%250Aand%2520reasoning.%2520Our%2520preliminary%2520studies%2520demonstrate%2520that%2520combining%2520visual%2520and%250Alanguage%2520prompts%2520as%2520conditions%2520for%2520customizing%2520the%2520models%2520enhances%2520anomaly%250Adetection%2520performance.%2520The%2520customized%2520models%2520showcase%2520the%2520ability%2520to%2520detect%250Aanomalies%2520across%2520different%2520data%2520modalities%2520such%2520as%2520images%252C%2520point%2520clouds%252C%2520and%250Avideos.%2520Qualitative%2520case%2520studies%2520further%2520highlight%2520the%2520anomaly%2520detection%2520and%250Areasoning%2520capabilities%252C%2520particularly%2520for%2520multi-object%2520scenes%2520and%2520temporal%2520data.%250AOur%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Xiaohao-Xu/Customizable-VLM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customizing%20Visual-Language%20Foundation%20Models%20for%20Multi-modal%20Anomaly%0A%20%20Detection%20and%20Reasoning&entry.906535625=Xiaohao%20Xu%20and%20Yunkang%20Cao%20and%20Huaxin%20Zhang%20and%20Nong%20Sang%20and%20Xiaonan%20Huang&entry.1292438233=%20%20Anomaly%20detection%20is%20vital%20in%20various%20industrial%20scenarios%2C%20including%20the%0Aidentification%20of%20unusual%20patterns%20in%20production%20lines%20and%20the%20detection%20of%0Amanufacturing%20defects%20for%20quality%20control.%20Existing%20techniques%20tend%20to%20be%0Aspecialized%20in%20individual%20scenarios%20and%20lack%20generalization%20capacities.%20In%20this%0Astudy%2C%20our%20objective%20is%20to%20develop%20a%20generic%20anomaly%20detection%20model%20that%20can%0Abe%20applied%20in%20multiple%20scenarios.%20To%20achieve%20this%2C%20we%20custom-build%20generic%0Avisual%20language%20foundation%20models%20that%20possess%20extensive%20knowledge%20and%20robust%0Areasoning%20abilities%20as%20anomaly%20detectors%20and%20reasoners.%20Specifically%2C%20we%0Aintroduce%20a%20multi-modal%20prompting%20strategy%20that%20incorporates%20domain%20knowledge%0Afrom%20experts%20as%20conditions%20to%20guide%20the%20models.%20Our%20approach%20considers%20diverse%0Aprompt%20types%2C%20including%20task%20descriptions%2C%20class%20context%2C%20normality%20rules%2C%20and%0Areference%20images.%20In%20addition%2C%20we%20unify%20the%20input%20representation%20of%0Amulti-modality%20into%20a%202D%20image%20format%2C%20enabling%20multi-modal%20anomaly%20detection%0Aand%20reasoning.%20Our%20preliminary%20studies%20demonstrate%20that%20combining%20visual%20and%0Alanguage%20prompts%20as%20conditions%20for%20customizing%20the%20models%20enhances%20anomaly%0Adetection%20performance.%20The%20customized%20models%20showcase%20the%20ability%20to%20detect%0Aanomalies%20across%20different%20data%20modalities%20such%20as%20images%2C%20point%20clouds%2C%20and%0Avideos.%20Qualitative%20case%20studies%20further%20highlight%20the%20anomaly%20detection%20and%0Areasoning%20capabilities%2C%20particularly%20for%20multi-object%20scenes%20and%20temporal%20data.%0AOur%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Xiaohao-Xu/Customizable-VLM%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11083v2&entry.124074799=Read"},
{"title": "Low-Resource Language Processing: An OCR-Driven Summarization and\n  Translation Pipeline", "author": "Hrishit Madhavi and Jacob Cherian and Yuvraj Khamkar and Dhananjay Bhagat", "abstract": "  This paper presents an end-to-end suite for multilingual information\nextraction and processing from image-based documents. The system uses Optical\nCharacter Recognition (Tesseract) to extract text in languages such as English,\nHindi, and Tamil, and then a pipeline involving large language model APIs\n(Gemini) for cross-lingual translation, abstractive summarization, and\nre-translation into a target language. Additional modules add sentiment\nanalysis (TensorFlow), topic classification (Transformers), and date extraction\n(Regex) for better document comprehension. Made available in an accessible\nGradio interface, the current research shows a real-world application of\nlibraries, models, and APIs to close the language gap and enhance access to\ninformation in image media across different linguistic environments\n", "link": "http://arxiv.org/abs/2505.11177v1", "date": "2025-05-16", "relevancy": 2.4835, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Resource%20Language%20Processing%3A%20An%20OCR-Driven%20Summarization%20and%0A%20%20Translation%20Pipeline&body=Title%3A%20Low-Resource%20Language%20Processing%3A%20An%20OCR-Driven%20Summarization%20and%0A%20%20Translation%20Pipeline%0AAuthor%3A%20Hrishit%20Madhavi%20and%20Jacob%20Cherian%20and%20Yuvraj%20Khamkar%20and%20Dhananjay%20Bhagat%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20end-to-end%20suite%20for%20multilingual%20information%0Aextraction%20and%20processing%20from%20image-based%20documents.%20The%20system%20uses%20Optical%0ACharacter%20Recognition%20%28Tesseract%29%20to%20extract%20text%20in%20languages%20such%20as%20English%2C%0AHindi%2C%20and%20Tamil%2C%20and%20then%20a%20pipeline%20involving%20large%20language%20model%20APIs%0A%28Gemini%29%20for%20cross-lingual%20translation%2C%20abstractive%20summarization%2C%20and%0Are-translation%20into%20a%20target%20language.%20Additional%20modules%20add%20sentiment%0Aanalysis%20%28TensorFlow%29%2C%20topic%20classification%20%28Transformers%29%2C%20and%20date%20extraction%0A%28Regex%29%20for%20better%20document%20comprehension.%20Made%20available%20in%20an%20accessible%0AGradio%20interface%2C%20the%20current%20research%20shows%20a%20real-world%20application%20of%0Alibraries%2C%20models%2C%20and%20APIs%20to%20close%20the%20language%20gap%20and%20enhance%20access%20to%0Ainformation%20in%20image%20media%20across%20different%20linguistic%20environments%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Resource%2520Language%2520Processing%253A%2520An%2520OCR-Driven%2520Summarization%2520and%250A%2520%2520Translation%2520Pipeline%26entry.906535625%3DHrishit%2520Madhavi%2520and%2520Jacob%2520Cherian%2520and%2520Yuvraj%2520Khamkar%2520and%2520Dhananjay%2520Bhagat%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520end-to-end%2520suite%2520for%2520multilingual%2520information%250Aextraction%2520and%2520processing%2520from%2520image-based%2520documents.%2520The%2520system%2520uses%2520Optical%250ACharacter%2520Recognition%2520%2528Tesseract%2529%2520to%2520extract%2520text%2520in%2520languages%2520such%2520as%2520English%252C%250AHindi%252C%2520and%2520Tamil%252C%2520and%2520then%2520a%2520pipeline%2520involving%2520large%2520language%2520model%2520APIs%250A%2528Gemini%2529%2520for%2520cross-lingual%2520translation%252C%2520abstractive%2520summarization%252C%2520and%250Are-translation%2520into%2520a%2520target%2520language.%2520Additional%2520modules%2520add%2520sentiment%250Aanalysis%2520%2528TensorFlow%2529%252C%2520topic%2520classification%2520%2528Transformers%2529%252C%2520and%2520date%2520extraction%250A%2528Regex%2529%2520for%2520better%2520document%2520comprehension.%2520Made%2520available%2520in%2520an%2520accessible%250AGradio%2520interface%252C%2520the%2520current%2520research%2520shows%2520a%2520real-world%2520application%2520of%250Alibraries%252C%2520models%252C%2520and%2520APIs%2520to%2520close%2520the%2520language%2520gap%2520and%2520enhance%2520access%2520to%250Ainformation%2520in%2520image%2520media%2520across%2520different%2520linguistic%2520environments%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Resource%20Language%20Processing%3A%20An%20OCR-Driven%20Summarization%20and%0A%20%20Translation%20Pipeline&entry.906535625=Hrishit%20Madhavi%20and%20Jacob%20Cherian%20and%20Yuvraj%20Khamkar%20and%20Dhananjay%20Bhagat&entry.1292438233=%20%20This%20paper%20presents%20an%20end-to-end%20suite%20for%20multilingual%20information%0Aextraction%20and%20processing%20from%20image-based%20documents.%20The%20system%20uses%20Optical%0ACharacter%20Recognition%20%28Tesseract%29%20to%20extract%20text%20in%20languages%20such%20as%20English%2C%0AHindi%2C%20and%20Tamil%2C%20and%20then%20a%20pipeline%20involving%20large%20language%20model%20APIs%0A%28Gemini%29%20for%20cross-lingual%20translation%2C%20abstractive%20summarization%2C%20and%0Are-translation%20into%20a%20target%20language.%20Additional%20modules%20add%20sentiment%0Aanalysis%20%28TensorFlow%29%2C%20topic%20classification%20%28Transformers%29%2C%20and%20date%20extraction%0A%28Regex%29%20for%20better%20document%20comprehension.%20Made%20available%20in%20an%20accessible%0AGradio%20interface%2C%20the%20current%20research%20shows%20a%20real-world%20application%20of%0Alibraries%2C%20models%2C%20and%20APIs%20to%20close%20the%20language%20gap%20and%20enhance%20access%20to%0Ainformation%20in%20image%20media%20across%20different%20linguistic%20environments%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11177v1&entry.124074799=Read"},
{"title": "Is Grokking a Computational Glass Relaxation?", "author": "Xiaotian Zhang and Yue Shang and Entao Yang and Ge Zhang", "abstract": "  Understanding neural network's (NN) generalizability remains a central\nquestion in deep learning research. The special phenomenon of grokking, where\nNNs abruptly generalize long after the training performance reaches a\nnear-perfect level, offers a unique window to investigate the underlying\nmechanisms of NNs' generalizability. Here we propose an interpretation for\ngrokking by framing it as a computational glass relaxation: viewing NNs as a\nphysical system where parameters are the degrees of freedom and train loss is\nthe system energy, we find memorization process resembles a rapid cooling of\nliquid into non-equilibrium glassy state at low temperature and the later\ngeneralization is like a slow relaxation towards a more stable configuration.\nThis mapping enables us to sample NNs' Boltzmann entropy (states of density)\nlandscape as a function of training loss and test accuracy. Our experiments in\ntransformers on arithmetic tasks suggests that there is NO entropy barrier in\nthe memorization-to-generalization transition of grokking, challenging previous\ntheory that defines grokking as a first-order phase transition. We identify a\nhigh-entropy advantage under grokking, an extension of prior work linking\nentropy to generalizability but much more significant. Inspired by grokking's\nfar-from-equilibrium nature, we develop a toy optimizer WanD based on\nWang-landau molecular dynamics, which can eliminate grokking without any\nconstraints and find high-norm generalizing solutions. This provides\nstrictly-defined counterexamples to theory attributing grokking solely to\nweight norm evolution towards the Goldilocks zone and also suggests new\npotential ways for optimizer design.\n", "link": "http://arxiv.org/abs/2505.11411v1", "date": "2025-05-16", "relevancy": 2.479, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5216}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4881}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Grokking%20a%20Computational%20Glass%20Relaxation%3F&body=Title%3A%20Is%20Grokking%20a%20Computational%20Glass%20Relaxation%3F%0AAuthor%3A%20Xiaotian%20Zhang%20and%20Yue%20Shang%20and%20Entao%20Yang%20and%20Ge%20Zhang%0AAbstract%3A%20%20%20Understanding%20neural%20network%27s%20%28NN%29%20generalizability%20remains%20a%20central%0Aquestion%20in%20deep%20learning%20research.%20The%20special%20phenomenon%20of%20grokking%2C%20where%0ANNs%20abruptly%20generalize%20long%20after%20the%20training%20performance%20reaches%20a%0Anear-perfect%20level%2C%20offers%20a%20unique%20window%20to%20investigate%20the%20underlying%0Amechanisms%20of%20NNs%27%20generalizability.%20Here%20we%20propose%20an%20interpretation%20for%0Agrokking%20by%20framing%20it%20as%20a%20computational%20glass%20relaxation%3A%20viewing%20NNs%20as%20a%0Aphysical%20system%20where%20parameters%20are%20the%20degrees%20of%20freedom%20and%20train%20loss%20is%0Athe%20system%20energy%2C%20we%20find%20memorization%20process%20resembles%20a%20rapid%20cooling%20of%0Aliquid%20into%20non-equilibrium%20glassy%20state%20at%20low%20temperature%20and%20the%20later%0Ageneralization%20is%20like%20a%20slow%20relaxation%20towards%20a%20more%20stable%20configuration.%0AThis%20mapping%20enables%20us%20to%20sample%20NNs%27%20Boltzmann%20entropy%20%28states%20of%20density%29%0Alandscape%20as%20a%20function%20of%20training%20loss%20and%20test%20accuracy.%20Our%20experiments%20in%0Atransformers%20on%20arithmetic%20tasks%20suggests%20that%20there%20is%20NO%20entropy%20barrier%20in%0Athe%20memorization-to-generalization%20transition%20of%20grokking%2C%20challenging%20previous%0Atheory%20that%20defines%20grokking%20as%20a%20first-order%20phase%20transition.%20We%20identify%20a%0Ahigh-entropy%20advantage%20under%20grokking%2C%20an%20extension%20of%20prior%20work%20linking%0Aentropy%20to%20generalizability%20but%20much%20more%20significant.%20Inspired%20by%20grokking%27s%0Afar-from-equilibrium%20nature%2C%20we%20develop%20a%20toy%20optimizer%20WanD%20based%20on%0AWang-landau%20molecular%20dynamics%2C%20which%20can%20eliminate%20grokking%20without%20any%0Aconstraints%20and%20find%20high-norm%20generalizing%20solutions.%20This%20provides%0Astrictly-defined%20counterexamples%20to%20theory%20attributing%20grokking%20solely%20to%0Aweight%20norm%20evolution%20towards%20the%20Goldilocks%20zone%20and%20also%20suggests%20new%0Apotential%20ways%20for%20optimizer%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Grokking%2520a%2520Computational%2520Glass%2520Relaxation%253F%26entry.906535625%3DXiaotian%2520Zhang%2520and%2520Yue%2520Shang%2520and%2520Entao%2520Yang%2520and%2520Ge%2520Zhang%26entry.1292438233%3D%2520%2520Understanding%2520neural%2520network%2527s%2520%2528NN%2529%2520generalizability%2520remains%2520a%2520central%250Aquestion%2520in%2520deep%2520learning%2520research.%2520The%2520special%2520phenomenon%2520of%2520grokking%252C%2520where%250ANNs%2520abruptly%2520generalize%2520long%2520after%2520the%2520training%2520performance%2520reaches%2520a%250Anear-perfect%2520level%252C%2520offers%2520a%2520unique%2520window%2520to%2520investigate%2520the%2520underlying%250Amechanisms%2520of%2520NNs%2527%2520generalizability.%2520Here%2520we%2520propose%2520an%2520interpretation%2520for%250Agrokking%2520by%2520framing%2520it%2520as%2520a%2520computational%2520glass%2520relaxation%253A%2520viewing%2520NNs%2520as%2520a%250Aphysical%2520system%2520where%2520parameters%2520are%2520the%2520degrees%2520of%2520freedom%2520and%2520train%2520loss%2520is%250Athe%2520system%2520energy%252C%2520we%2520find%2520memorization%2520process%2520resembles%2520a%2520rapid%2520cooling%2520of%250Aliquid%2520into%2520non-equilibrium%2520glassy%2520state%2520at%2520low%2520temperature%2520and%2520the%2520later%250Ageneralization%2520is%2520like%2520a%2520slow%2520relaxation%2520towards%2520a%2520more%2520stable%2520configuration.%250AThis%2520mapping%2520enables%2520us%2520to%2520sample%2520NNs%2527%2520Boltzmann%2520entropy%2520%2528states%2520of%2520density%2529%250Alandscape%2520as%2520a%2520function%2520of%2520training%2520loss%2520and%2520test%2520accuracy.%2520Our%2520experiments%2520in%250Atransformers%2520on%2520arithmetic%2520tasks%2520suggests%2520that%2520there%2520is%2520NO%2520entropy%2520barrier%2520in%250Athe%2520memorization-to-generalization%2520transition%2520of%2520grokking%252C%2520challenging%2520previous%250Atheory%2520that%2520defines%2520grokking%2520as%2520a%2520first-order%2520phase%2520transition.%2520We%2520identify%2520a%250Ahigh-entropy%2520advantage%2520under%2520grokking%252C%2520an%2520extension%2520of%2520prior%2520work%2520linking%250Aentropy%2520to%2520generalizability%2520but%2520much%2520more%2520significant.%2520Inspired%2520by%2520grokking%2527s%250Afar-from-equilibrium%2520nature%252C%2520we%2520develop%2520a%2520toy%2520optimizer%2520WanD%2520based%2520on%250AWang-landau%2520molecular%2520dynamics%252C%2520which%2520can%2520eliminate%2520grokking%2520without%2520any%250Aconstraints%2520and%2520find%2520high-norm%2520generalizing%2520solutions.%2520This%2520provides%250Astrictly-defined%2520counterexamples%2520to%2520theory%2520attributing%2520grokking%2520solely%2520to%250Aweight%2520norm%2520evolution%2520towards%2520the%2520Goldilocks%2520zone%2520and%2520also%2520suggests%2520new%250Apotential%2520ways%2520for%2520optimizer%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Grokking%20a%20Computational%20Glass%20Relaxation%3F&entry.906535625=Xiaotian%20Zhang%20and%20Yue%20Shang%20and%20Entao%20Yang%20and%20Ge%20Zhang&entry.1292438233=%20%20Understanding%20neural%20network%27s%20%28NN%29%20generalizability%20remains%20a%20central%0Aquestion%20in%20deep%20learning%20research.%20The%20special%20phenomenon%20of%20grokking%2C%20where%0ANNs%20abruptly%20generalize%20long%20after%20the%20training%20performance%20reaches%20a%0Anear-perfect%20level%2C%20offers%20a%20unique%20window%20to%20investigate%20the%20underlying%0Amechanisms%20of%20NNs%27%20generalizability.%20Here%20we%20propose%20an%20interpretation%20for%0Agrokking%20by%20framing%20it%20as%20a%20computational%20glass%20relaxation%3A%20viewing%20NNs%20as%20a%0Aphysical%20system%20where%20parameters%20are%20the%20degrees%20of%20freedom%20and%20train%20loss%20is%0Athe%20system%20energy%2C%20we%20find%20memorization%20process%20resembles%20a%20rapid%20cooling%20of%0Aliquid%20into%20non-equilibrium%20glassy%20state%20at%20low%20temperature%20and%20the%20later%0Ageneralization%20is%20like%20a%20slow%20relaxation%20towards%20a%20more%20stable%20configuration.%0AThis%20mapping%20enables%20us%20to%20sample%20NNs%27%20Boltzmann%20entropy%20%28states%20of%20density%29%0Alandscape%20as%20a%20function%20of%20training%20loss%20and%20test%20accuracy.%20Our%20experiments%20in%0Atransformers%20on%20arithmetic%20tasks%20suggests%20that%20there%20is%20NO%20entropy%20barrier%20in%0Athe%20memorization-to-generalization%20transition%20of%20grokking%2C%20challenging%20previous%0Atheory%20that%20defines%20grokking%20as%20a%20first-order%20phase%20transition.%20We%20identify%20a%0Ahigh-entropy%20advantage%20under%20grokking%2C%20an%20extension%20of%20prior%20work%20linking%0Aentropy%20to%20generalizability%20but%20much%20more%20significant.%20Inspired%20by%20grokking%27s%0Afar-from-equilibrium%20nature%2C%20we%20develop%20a%20toy%20optimizer%20WanD%20based%20on%0AWang-landau%20molecular%20dynamics%2C%20which%20can%20eliminate%20grokking%20without%20any%0Aconstraints%20and%20find%20high-norm%20generalizing%20solutions.%20This%20provides%0Astrictly-defined%20counterexamples%20to%20theory%20attributing%20grokking%20solely%20to%0Aweight%20norm%20evolution%20towards%20the%20Goldilocks%20zone%20and%20also%20suggests%20new%0Apotential%20ways%20for%20optimizer%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11411v1&entry.124074799=Read"},
{"title": "QVGen: Pushing the Limit of Quantized Video Generative Models", "author": "Yushi Huang and Ruihao Gong and Jing Liu and Yifu Ding and Chengtao Lv and Haotong Qin and Jun Zhang", "abstract": "  Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,\ntheir substantial computational and memory demands pose serious challenges to\nreal-world deployment, even on high-end GPUs. As a commonly adopted solution,\nquantization has proven notable success in reducing cost for image DMs, while\nits direct application to video DMs remains ineffective. In this paper, we\npresent QVGen, a novel quantization-aware training (QAT) framework tailored for\nhigh-performance and inference-efficient video DMs under extremely low-bit\nquantization (e.g., 4-bit or below). We begin with a theoretical analysis\ndemonstrating that reducing the gradient norm is essential to facilitate\nconvergence for QAT. To this end, we introduce auxiliary modules ($\\Phi$) to\nmitigate large quantization errors, leading to significantly enhanced\nconvergence. To eliminate the inference overhead of $\\Phi$, we propose a\nrank-decay strategy that progressively eliminates $\\Phi$. Specifically, we\nrepeatedly employ singular value decomposition (SVD) and a proposed rank-based\nregularization $\\mathbf{\\gamma}$ to identify and decay low-contributing\ncomponents. This strategy retains performance while zeroing out inference\noverhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,\nwith parameter sizes ranging from $1.3$B $\\sim14$B, show that QVGen is the\nfirst to reach full-precision comparable quality under 4-bit settings.\nMoreover, it significantly outperforms existing methods. For instance, our\n3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and\n$+8.43$ in Scene Consistency on VBench.\n", "link": "http://arxiv.org/abs/2505.11497v1", "date": "2025-05-16", "relevancy": 2.4545, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6325}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6044}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QVGen%3A%20Pushing%20the%20Limit%20of%20Quantized%20Video%20Generative%20Models&body=Title%3A%20QVGen%3A%20Pushing%20the%20Limit%20of%20Quantized%20Video%20Generative%20Models%0AAuthor%3A%20Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Yifu%20Ding%20and%20Chengtao%20Lv%20and%20Haotong%20Qin%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20Video%20diffusion%20models%20%28DMs%29%20have%20enabled%20high-quality%20video%20synthesis.%20Yet%2C%0Atheir%20substantial%20computational%20and%20memory%20demands%20pose%20serious%20challenges%20to%0Areal-world%20deployment%2C%20even%20on%20high-end%20GPUs.%20As%20a%20commonly%20adopted%20solution%2C%0Aquantization%20has%20proven%20notable%20success%20in%20reducing%20cost%20for%20image%20DMs%2C%20while%0Aits%20direct%20application%20to%20video%20DMs%20remains%20ineffective.%20In%20this%20paper%2C%20we%0Apresent%20QVGen%2C%20a%20novel%20quantization-aware%20training%20%28QAT%29%20framework%20tailored%20for%0Ahigh-performance%20and%20inference-efficient%20video%20DMs%20under%20extremely%20low-bit%0Aquantization%20%28e.g.%2C%204-bit%20or%20below%29.%20We%20begin%20with%20a%20theoretical%20analysis%0Ademonstrating%20that%20reducing%20the%20gradient%20norm%20is%20essential%20to%20facilitate%0Aconvergence%20for%20QAT.%20To%20this%20end%2C%20we%20introduce%20auxiliary%20modules%20%28%24%5CPhi%24%29%20to%0Amitigate%20large%20quantization%20errors%2C%20leading%20to%20significantly%20enhanced%0Aconvergence.%20To%20eliminate%20the%20inference%20overhead%20of%20%24%5CPhi%24%2C%20we%20propose%20a%0Arank-decay%20strategy%20that%20progressively%20eliminates%20%24%5CPhi%24.%20Specifically%2C%20we%0Arepeatedly%20employ%20singular%20value%20decomposition%20%28SVD%29%20and%20a%20proposed%20rank-based%0Aregularization%20%24%5Cmathbf%7B%5Cgamma%7D%24%20to%20identify%20and%20decay%20low-contributing%0Acomponents.%20This%20strategy%20retains%20performance%20while%20zeroing%20out%20inference%0Aoverhead.%20Extensive%20experiments%20across%20%244%24%20state-of-the-art%20%28SOTA%29%20video%20DMs%2C%0Awith%20parameter%20sizes%20ranging%20from%20%241.3%24B%20%24%5Csim14%24B%2C%20show%20that%20QVGen%20is%20the%0Afirst%20to%20reach%20full-precision%20comparable%20quality%20under%204-bit%20settings.%0AMoreover%2C%20it%20significantly%20outperforms%20existing%20methods.%20For%20instance%2C%20our%0A3-bit%20CogVideoX-2B%20achieves%20improvements%20of%20%24%2B25.28%24%20in%20Dynamic%20Degree%20and%0A%24%2B8.43%24%20in%20Scene%20Consistency%20on%20VBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQVGen%253A%2520Pushing%2520the%2520Limit%2520of%2520Quantized%2520Video%2520Generative%2520Models%26entry.906535625%3DYushi%2520Huang%2520and%2520Ruihao%2520Gong%2520and%2520Jing%2520Liu%2520and%2520Yifu%2520Ding%2520and%2520Chengtao%2520Lv%2520and%2520Haotong%2520Qin%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520diffusion%2520models%2520%2528DMs%2529%2520have%2520enabled%2520high-quality%2520video%2520synthesis.%2520Yet%252C%250Atheir%2520substantial%2520computational%2520and%2520memory%2520demands%2520pose%2520serious%2520challenges%2520to%250Areal-world%2520deployment%252C%2520even%2520on%2520high-end%2520GPUs.%2520As%2520a%2520commonly%2520adopted%2520solution%252C%250Aquantization%2520has%2520proven%2520notable%2520success%2520in%2520reducing%2520cost%2520for%2520image%2520DMs%252C%2520while%250Aits%2520direct%2520application%2520to%2520video%2520DMs%2520remains%2520ineffective.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520QVGen%252C%2520a%2520novel%2520quantization-aware%2520training%2520%2528QAT%2529%2520framework%2520tailored%2520for%250Ahigh-performance%2520and%2520inference-efficient%2520video%2520DMs%2520under%2520extremely%2520low-bit%250Aquantization%2520%2528e.g.%252C%25204-bit%2520or%2520below%2529.%2520We%2520begin%2520with%2520a%2520theoretical%2520analysis%250Ademonstrating%2520that%2520reducing%2520the%2520gradient%2520norm%2520is%2520essential%2520to%2520facilitate%250Aconvergence%2520for%2520QAT.%2520To%2520this%2520end%252C%2520we%2520introduce%2520auxiliary%2520modules%2520%2528%2524%255CPhi%2524%2529%2520to%250Amitigate%2520large%2520quantization%2520errors%252C%2520leading%2520to%2520significantly%2520enhanced%250Aconvergence.%2520To%2520eliminate%2520the%2520inference%2520overhead%2520of%2520%2524%255CPhi%2524%252C%2520we%2520propose%2520a%250Arank-decay%2520strategy%2520that%2520progressively%2520eliminates%2520%2524%255CPhi%2524.%2520Specifically%252C%2520we%250Arepeatedly%2520employ%2520singular%2520value%2520decomposition%2520%2528SVD%2529%2520and%2520a%2520proposed%2520rank-based%250Aregularization%2520%2524%255Cmathbf%257B%255Cgamma%257D%2524%2520to%2520identify%2520and%2520decay%2520low-contributing%250Acomponents.%2520This%2520strategy%2520retains%2520performance%2520while%2520zeroing%2520out%2520inference%250Aoverhead.%2520Extensive%2520experiments%2520across%2520%25244%2524%2520state-of-the-art%2520%2528SOTA%2529%2520video%2520DMs%252C%250Awith%2520parameter%2520sizes%2520ranging%2520from%2520%25241.3%2524B%2520%2524%255Csim14%2524B%252C%2520show%2520that%2520QVGen%2520is%2520the%250Afirst%2520to%2520reach%2520full-precision%2520comparable%2520quality%2520under%25204-bit%2520settings.%250AMoreover%252C%2520it%2520significantly%2520outperforms%2520existing%2520methods.%2520For%2520instance%252C%2520our%250A3-bit%2520CogVideoX-2B%2520achieves%2520improvements%2520of%2520%2524%252B25.28%2524%2520in%2520Dynamic%2520Degree%2520and%250A%2524%252B8.43%2524%2520in%2520Scene%2520Consistency%2520on%2520VBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QVGen%3A%20Pushing%20the%20Limit%20of%20Quantized%20Video%20Generative%20Models&entry.906535625=Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Yifu%20Ding%20and%20Chengtao%20Lv%20and%20Haotong%20Qin%20and%20Jun%20Zhang&entry.1292438233=%20%20Video%20diffusion%20models%20%28DMs%29%20have%20enabled%20high-quality%20video%20synthesis.%20Yet%2C%0Atheir%20substantial%20computational%20and%20memory%20demands%20pose%20serious%20challenges%20to%0Areal-world%20deployment%2C%20even%20on%20high-end%20GPUs.%20As%20a%20commonly%20adopted%20solution%2C%0Aquantization%20has%20proven%20notable%20success%20in%20reducing%20cost%20for%20image%20DMs%2C%20while%0Aits%20direct%20application%20to%20video%20DMs%20remains%20ineffective.%20In%20this%20paper%2C%20we%0Apresent%20QVGen%2C%20a%20novel%20quantization-aware%20training%20%28QAT%29%20framework%20tailored%20for%0Ahigh-performance%20and%20inference-efficient%20video%20DMs%20under%20extremely%20low-bit%0Aquantization%20%28e.g.%2C%204-bit%20or%20below%29.%20We%20begin%20with%20a%20theoretical%20analysis%0Ademonstrating%20that%20reducing%20the%20gradient%20norm%20is%20essential%20to%20facilitate%0Aconvergence%20for%20QAT.%20To%20this%20end%2C%20we%20introduce%20auxiliary%20modules%20%28%24%5CPhi%24%29%20to%0Amitigate%20large%20quantization%20errors%2C%20leading%20to%20significantly%20enhanced%0Aconvergence.%20To%20eliminate%20the%20inference%20overhead%20of%20%24%5CPhi%24%2C%20we%20propose%20a%0Arank-decay%20strategy%20that%20progressively%20eliminates%20%24%5CPhi%24.%20Specifically%2C%20we%0Arepeatedly%20employ%20singular%20value%20decomposition%20%28SVD%29%20and%20a%20proposed%20rank-based%0Aregularization%20%24%5Cmathbf%7B%5Cgamma%7D%24%20to%20identify%20and%20decay%20low-contributing%0Acomponents.%20This%20strategy%20retains%20performance%20while%20zeroing%20out%20inference%0Aoverhead.%20Extensive%20experiments%20across%20%244%24%20state-of-the-art%20%28SOTA%29%20video%20DMs%2C%0Awith%20parameter%20sizes%20ranging%20from%20%241.3%24B%20%24%5Csim14%24B%2C%20show%20that%20QVGen%20is%20the%0Afirst%20to%20reach%20full-precision%20comparable%20quality%20under%204-bit%20settings.%0AMoreover%2C%20it%20significantly%20outperforms%20existing%20methods.%20For%20instance%2C%20our%0A3-bit%20CogVideoX-2B%20achieves%20improvements%20of%20%24%2B25.28%24%20in%20Dynamic%20Degree%20and%0A%24%2B8.43%24%20in%20Scene%20Consistency%20on%20VBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11497v1&entry.124074799=Read"},
{"title": "On Next-Token Prediction in LLMs: How End Goals Determine the\n  Consistency of Decoding Algorithms", "author": "Jacob Trauger and Ambuj Tewari", "abstract": "  Probabilistic next-token prediction trained using cross-entropy loss is the\nbasis of most large language models. Given a sequence of previous values,\nnext-token prediction assigns a probability to each possible next value in the\nvocabulary. There are many ways to use next-token prediction to output token\nsequences. This paper examines a few of these algorithms (greedy, lookahead,\nrandom sampling, and temperature-scaled random sampling) and studies their\nconsistency with respect to various goals encoded as loss functions. Although\nconsistency of surrogate losses with respect to a target loss function is a\nwell researched topic, we are the first to study it in the context of LLMs (to\nthe best of our knowledge). We find that, so long as next-token prediction\nconverges to its true probability distribution, random sampling is consistent\nwith outputting sequences that mimic sampling from the true probability\ndistribution. For the other goals, such as minimizing the 0-1 loss on the\nentire sequence, we show no polynomial-time algorithm is optimal for all\nprobability distributions and all decoding algorithms studied are only optimal\nfor a subset of probability distributions. When analyzing these results, we see\nthat there is a dichotomy created between the goals of information retrieval\nand creative generation for the decoding algorithms. This shows that choosing\nthe correct decoding algorithm based on the desired goal is extremely important\nand many of the ones used are lacking theoretical grounding in numerous\nscenarios.\n", "link": "http://arxiv.org/abs/2505.11183v1", "date": "2025-05-16", "relevancy": 2.4504, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Next-Token%20Prediction%20in%20LLMs%3A%20How%20End%20Goals%20Determine%20the%0A%20%20Consistency%20of%20Decoding%20Algorithms&body=Title%3A%20On%20Next-Token%20Prediction%20in%20LLMs%3A%20How%20End%20Goals%20Determine%20the%0A%20%20Consistency%20of%20Decoding%20Algorithms%0AAuthor%3A%20Jacob%20Trauger%20and%20Ambuj%20Tewari%0AAbstract%3A%20%20%20Probabilistic%20next-token%20prediction%20trained%20using%20cross-entropy%20loss%20is%20the%0Abasis%20of%20most%20large%20language%20models.%20Given%20a%20sequence%20of%20previous%20values%2C%0Anext-token%20prediction%20assigns%20a%20probability%20to%20each%20possible%20next%20value%20in%20the%0Avocabulary.%20There%20are%20many%20ways%20to%20use%20next-token%20prediction%20to%20output%20token%0Asequences.%20This%20paper%20examines%20a%20few%20of%20these%20algorithms%20%28greedy%2C%20lookahead%2C%0Arandom%20sampling%2C%20and%20temperature-scaled%20random%20sampling%29%20and%20studies%20their%0Aconsistency%20with%20respect%20to%20various%20goals%20encoded%20as%20loss%20functions.%20Although%0Aconsistency%20of%20surrogate%20losses%20with%20respect%20to%20a%20target%20loss%20function%20is%20a%0Awell%20researched%20topic%2C%20we%20are%20the%20first%20to%20study%20it%20in%20the%20context%20of%20LLMs%20%28to%0Athe%20best%20of%20our%20knowledge%29.%20We%20find%20that%2C%20so%20long%20as%20next-token%20prediction%0Aconverges%20to%20its%20true%20probability%20distribution%2C%20random%20sampling%20is%20consistent%0Awith%20outputting%20sequences%20that%20mimic%20sampling%20from%20the%20true%20probability%0Adistribution.%20For%20the%20other%20goals%2C%20such%20as%20minimizing%20the%200-1%20loss%20on%20the%0Aentire%20sequence%2C%20we%20show%20no%20polynomial-time%20algorithm%20is%20optimal%20for%20all%0Aprobability%20distributions%20and%20all%20decoding%20algorithms%20studied%20are%20only%20optimal%0Afor%20a%20subset%20of%20probability%20distributions.%20When%20analyzing%20these%20results%2C%20we%20see%0Athat%20there%20is%20a%20dichotomy%20created%20between%20the%20goals%20of%20information%20retrieval%0Aand%20creative%20generation%20for%20the%20decoding%20algorithms.%20This%20shows%20that%20choosing%0Athe%20correct%20decoding%20algorithm%20based%20on%20the%20desired%20goal%20is%20extremely%20important%0Aand%20many%20of%20the%20ones%20used%20are%20lacking%20theoretical%20grounding%20in%20numerous%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Next-Token%2520Prediction%2520in%2520LLMs%253A%2520How%2520End%2520Goals%2520Determine%2520the%250A%2520%2520Consistency%2520of%2520Decoding%2520Algorithms%26entry.906535625%3DJacob%2520Trauger%2520and%2520Ambuj%2520Tewari%26entry.1292438233%3D%2520%2520Probabilistic%2520next-token%2520prediction%2520trained%2520using%2520cross-entropy%2520loss%2520is%2520the%250Abasis%2520of%2520most%2520large%2520language%2520models.%2520Given%2520a%2520sequence%2520of%2520previous%2520values%252C%250Anext-token%2520prediction%2520assigns%2520a%2520probability%2520to%2520each%2520possible%2520next%2520value%2520in%2520the%250Avocabulary.%2520There%2520are%2520many%2520ways%2520to%2520use%2520next-token%2520prediction%2520to%2520output%2520token%250Asequences.%2520This%2520paper%2520examines%2520a%2520few%2520of%2520these%2520algorithms%2520%2528greedy%252C%2520lookahead%252C%250Arandom%2520sampling%252C%2520and%2520temperature-scaled%2520random%2520sampling%2529%2520and%2520studies%2520their%250Aconsistency%2520with%2520respect%2520to%2520various%2520goals%2520encoded%2520as%2520loss%2520functions.%2520Although%250Aconsistency%2520of%2520surrogate%2520losses%2520with%2520respect%2520to%2520a%2520target%2520loss%2520function%2520is%2520a%250Awell%2520researched%2520topic%252C%2520we%2520are%2520the%2520first%2520to%2520study%2520it%2520in%2520the%2520context%2520of%2520LLMs%2520%2528to%250Athe%2520best%2520of%2520our%2520knowledge%2529.%2520We%2520find%2520that%252C%2520so%2520long%2520as%2520next-token%2520prediction%250Aconverges%2520to%2520its%2520true%2520probability%2520distribution%252C%2520random%2520sampling%2520is%2520consistent%250Awith%2520outputting%2520sequences%2520that%2520mimic%2520sampling%2520from%2520the%2520true%2520probability%250Adistribution.%2520For%2520the%2520other%2520goals%252C%2520such%2520as%2520minimizing%2520the%25200-1%2520loss%2520on%2520the%250Aentire%2520sequence%252C%2520we%2520show%2520no%2520polynomial-time%2520algorithm%2520is%2520optimal%2520for%2520all%250Aprobability%2520distributions%2520and%2520all%2520decoding%2520algorithms%2520studied%2520are%2520only%2520optimal%250Afor%2520a%2520subset%2520of%2520probability%2520distributions.%2520When%2520analyzing%2520these%2520results%252C%2520we%2520see%250Athat%2520there%2520is%2520a%2520dichotomy%2520created%2520between%2520the%2520goals%2520of%2520information%2520retrieval%250Aand%2520creative%2520generation%2520for%2520the%2520decoding%2520algorithms.%2520This%2520shows%2520that%2520choosing%250Athe%2520correct%2520decoding%2520algorithm%2520based%2520on%2520the%2520desired%2520goal%2520is%2520extremely%2520important%250Aand%2520many%2520of%2520the%2520ones%2520used%2520are%2520lacking%2520theoretical%2520grounding%2520in%2520numerous%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Next-Token%20Prediction%20in%20LLMs%3A%20How%20End%20Goals%20Determine%20the%0A%20%20Consistency%20of%20Decoding%20Algorithms&entry.906535625=Jacob%20Trauger%20and%20Ambuj%20Tewari&entry.1292438233=%20%20Probabilistic%20next-token%20prediction%20trained%20using%20cross-entropy%20loss%20is%20the%0Abasis%20of%20most%20large%20language%20models.%20Given%20a%20sequence%20of%20previous%20values%2C%0Anext-token%20prediction%20assigns%20a%20probability%20to%20each%20possible%20next%20value%20in%20the%0Avocabulary.%20There%20are%20many%20ways%20to%20use%20next-token%20prediction%20to%20output%20token%0Asequences.%20This%20paper%20examines%20a%20few%20of%20these%20algorithms%20%28greedy%2C%20lookahead%2C%0Arandom%20sampling%2C%20and%20temperature-scaled%20random%20sampling%29%20and%20studies%20their%0Aconsistency%20with%20respect%20to%20various%20goals%20encoded%20as%20loss%20functions.%20Although%0Aconsistency%20of%20surrogate%20losses%20with%20respect%20to%20a%20target%20loss%20function%20is%20a%0Awell%20researched%20topic%2C%20we%20are%20the%20first%20to%20study%20it%20in%20the%20context%20of%20LLMs%20%28to%0Athe%20best%20of%20our%20knowledge%29.%20We%20find%20that%2C%20so%20long%20as%20next-token%20prediction%0Aconverges%20to%20its%20true%20probability%20distribution%2C%20random%20sampling%20is%20consistent%0Awith%20outputting%20sequences%20that%20mimic%20sampling%20from%20the%20true%20probability%0Adistribution.%20For%20the%20other%20goals%2C%20such%20as%20minimizing%20the%200-1%20loss%20on%20the%0Aentire%20sequence%2C%20we%20show%20no%20polynomial-time%20algorithm%20is%20optimal%20for%20all%0Aprobability%20distributions%20and%20all%20decoding%20algorithms%20studied%20are%20only%20optimal%0Afor%20a%20subset%20of%20probability%20distributions.%20When%20analyzing%20these%20results%2C%20we%20see%0Athat%20there%20is%20a%20dichotomy%20created%20between%20the%20goals%20of%20information%20retrieval%0Aand%20creative%20generation%20for%20the%20decoding%20algorithms.%20This%20shows%20that%20choosing%0Athe%20correct%20decoding%20algorithm%20based%20on%20the%20desired%20goal%20is%20extremely%20important%0Aand%20many%20of%20the%20ones%20used%20are%20lacking%20theoretical%20grounding%20in%20numerous%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11183v1&entry.124074799=Read"},
{"title": "Degree-Conscious Spiking Graph for Cross-Domain Adaptation", "author": "Yingxu Wang and Mengzhu Wang and Siwei Liu and Houcheng Su and Nan Yin and James Kwok", "abstract": "  Spiking Graph Networks (SGNs) have demonstrated significant potential in\ngraph classification by emulating brain-inspired neural dynamics to achieve\nenergy-efficient computation. However, existing SGNs are generally constrained\nto in-distribution scenarios and struggle with distribution shifts. In this\npaper, we first propose the domain adaptation problem in SGNs, and introduce a\nnovel framework named Degree-Consicious Spiking Graph for Cross-Domain\nAdaptation. DeSGraDA enhances generalization across domains with three key\ncomponents. First, we introduce the degree-conscious spiking representation\nmodule by adapting spike thresholds based on node degrees, enabling more\nexpressive and structure-aware signal encoding. Then, we perform temporal\ndistribution alignment by adversarially matching membrane potentials between\ndomains, ensuring effective performance under domain shift while preserving\nenergy efficiency. Additionally, we extract consistent predictions across two\nspaces to create reliable pseudo-labels, effectively leveraging unlabeled data\nto enhance graph classification performance. Furthermore, we establish the\nfirst generalization bound for SGDA, providing theoretical insights into its\nadaptation performance. Extensive experiments on benchmark datasets validate\nthat DeSGraDA consistently outperforms state-of-the-art methods in both\nclassification accuracy and energy efficiency.\n", "link": "http://arxiv.org/abs/2410.06883v4", "date": "2025-05-16", "relevancy": 2.4471, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4937}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4873}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Degree-Conscious%20Spiking%20Graph%20for%20Cross-Domain%20Adaptation&body=Title%3A%20Degree-Conscious%20Spiking%20Graph%20for%20Cross-Domain%20Adaptation%0AAuthor%3A%20Yingxu%20Wang%20and%20Mengzhu%20Wang%20and%20Siwei%20Liu%20and%20Houcheng%20Su%20and%20Nan%20Yin%20and%20James%20Kwok%0AAbstract%3A%20%20%20Spiking%20Graph%20Networks%20%28SGNs%29%20have%20demonstrated%20significant%20potential%20in%0Agraph%20classification%20by%20emulating%20brain-inspired%20neural%20dynamics%20to%20achieve%0Aenergy-efficient%20computation.%20However%2C%20existing%20SGNs%20are%20generally%20constrained%0Ato%20in-distribution%20scenarios%20and%20struggle%20with%20distribution%20shifts.%20In%20this%0Apaper%2C%20we%20first%20propose%20the%20domain%20adaptation%20problem%20in%20SGNs%2C%20and%20introduce%20a%0Anovel%20framework%20named%20Degree-Consicious%20Spiking%20Graph%20for%20Cross-Domain%0AAdaptation.%20DeSGraDA%20enhances%20generalization%20across%20domains%20with%20three%20key%0Acomponents.%20First%2C%20we%20introduce%20the%20degree-conscious%20spiking%20representation%0Amodule%20by%20adapting%20spike%20thresholds%20based%20on%20node%20degrees%2C%20enabling%20more%0Aexpressive%20and%20structure-aware%20signal%20encoding.%20Then%2C%20we%20perform%20temporal%0Adistribution%20alignment%20by%20adversarially%20matching%20membrane%20potentials%20between%0Adomains%2C%20ensuring%20effective%20performance%20under%20domain%20shift%20while%20preserving%0Aenergy%20efficiency.%20Additionally%2C%20we%20extract%20consistent%20predictions%20across%20two%0Aspaces%20to%20create%20reliable%20pseudo-labels%2C%20effectively%20leveraging%20unlabeled%20data%0Ato%20enhance%20graph%20classification%20performance.%20Furthermore%2C%20we%20establish%20the%0Afirst%20generalization%20bound%20for%20SGDA%2C%20providing%20theoretical%20insights%20into%20its%0Aadaptation%20performance.%20Extensive%20experiments%20on%20benchmark%20datasets%20validate%0Athat%20DeSGraDA%20consistently%20outperforms%20state-of-the-art%20methods%20in%20both%0Aclassification%20accuracy%20and%20energy%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06883v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegree-Conscious%2520Spiking%2520Graph%2520for%2520Cross-Domain%2520Adaptation%26entry.906535625%3DYingxu%2520Wang%2520and%2520Mengzhu%2520Wang%2520and%2520Siwei%2520Liu%2520and%2520Houcheng%2520Su%2520and%2520Nan%2520Yin%2520and%2520James%2520Kwok%26entry.1292438233%3D%2520%2520Spiking%2520Graph%2520Networks%2520%2528SGNs%2529%2520have%2520demonstrated%2520significant%2520potential%2520in%250Agraph%2520classification%2520by%2520emulating%2520brain-inspired%2520neural%2520dynamics%2520to%2520achieve%250Aenergy-efficient%2520computation.%2520However%252C%2520existing%2520SGNs%2520are%2520generally%2520constrained%250Ato%2520in-distribution%2520scenarios%2520and%2520struggle%2520with%2520distribution%2520shifts.%2520In%2520this%250Apaper%252C%2520we%2520first%2520propose%2520the%2520domain%2520adaptation%2520problem%2520in%2520SGNs%252C%2520and%2520introduce%2520a%250Anovel%2520framework%2520named%2520Degree-Consicious%2520Spiking%2520Graph%2520for%2520Cross-Domain%250AAdaptation.%2520DeSGraDA%2520enhances%2520generalization%2520across%2520domains%2520with%2520three%2520key%250Acomponents.%2520First%252C%2520we%2520introduce%2520the%2520degree-conscious%2520spiking%2520representation%250Amodule%2520by%2520adapting%2520spike%2520thresholds%2520based%2520on%2520node%2520degrees%252C%2520enabling%2520more%250Aexpressive%2520and%2520structure-aware%2520signal%2520encoding.%2520Then%252C%2520we%2520perform%2520temporal%250Adistribution%2520alignment%2520by%2520adversarially%2520matching%2520membrane%2520potentials%2520between%250Adomains%252C%2520ensuring%2520effective%2520performance%2520under%2520domain%2520shift%2520while%2520preserving%250Aenergy%2520efficiency.%2520Additionally%252C%2520we%2520extract%2520consistent%2520predictions%2520across%2520two%250Aspaces%2520to%2520create%2520reliable%2520pseudo-labels%252C%2520effectively%2520leveraging%2520unlabeled%2520data%250Ato%2520enhance%2520graph%2520classification%2520performance.%2520Furthermore%252C%2520we%2520establish%2520the%250Afirst%2520generalization%2520bound%2520for%2520SGDA%252C%2520providing%2520theoretical%2520insights%2520into%2520its%250Aadaptation%2520performance.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520validate%250Athat%2520DeSGraDA%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%250Aclassification%2520accuracy%2520and%2520energy%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06883v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Degree-Conscious%20Spiking%20Graph%20for%20Cross-Domain%20Adaptation&entry.906535625=Yingxu%20Wang%20and%20Mengzhu%20Wang%20and%20Siwei%20Liu%20and%20Houcheng%20Su%20and%20Nan%20Yin%20and%20James%20Kwok&entry.1292438233=%20%20Spiking%20Graph%20Networks%20%28SGNs%29%20have%20demonstrated%20significant%20potential%20in%0Agraph%20classification%20by%20emulating%20brain-inspired%20neural%20dynamics%20to%20achieve%0Aenergy-efficient%20computation.%20However%2C%20existing%20SGNs%20are%20generally%20constrained%0Ato%20in-distribution%20scenarios%20and%20struggle%20with%20distribution%20shifts.%20In%20this%0Apaper%2C%20we%20first%20propose%20the%20domain%20adaptation%20problem%20in%20SGNs%2C%20and%20introduce%20a%0Anovel%20framework%20named%20Degree-Consicious%20Spiking%20Graph%20for%20Cross-Domain%0AAdaptation.%20DeSGraDA%20enhances%20generalization%20across%20domains%20with%20three%20key%0Acomponents.%20First%2C%20we%20introduce%20the%20degree-conscious%20spiking%20representation%0Amodule%20by%20adapting%20spike%20thresholds%20based%20on%20node%20degrees%2C%20enabling%20more%0Aexpressive%20and%20structure-aware%20signal%20encoding.%20Then%2C%20we%20perform%20temporal%0Adistribution%20alignment%20by%20adversarially%20matching%20membrane%20potentials%20between%0Adomains%2C%20ensuring%20effective%20performance%20under%20domain%20shift%20while%20preserving%0Aenergy%20efficiency.%20Additionally%2C%20we%20extract%20consistent%20predictions%20across%20two%0Aspaces%20to%20create%20reliable%20pseudo-labels%2C%20effectively%20leveraging%20unlabeled%20data%0Ato%20enhance%20graph%20classification%20performance.%20Furthermore%2C%20we%20establish%20the%0Afirst%20generalization%20bound%20for%20SGDA%2C%20providing%20theoretical%20insights%20into%20its%0Aadaptation%20performance.%20Extensive%20experiments%20on%20benchmark%20datasets%20validate%0Athat%20DeSGraDA%20consistently%20outperforms%20state-of-the-art%20methods%20in%20both%0Aclassification%20accuracy%20and%20energy%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06883v4&entry.124074799=Read"},
{"title": "reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis", "author": "Kai Norman Clasen and Leonard Hackel and Tom Burgert and Gencer Sumbul and Beg\u00fcm Demir and Volker Markl", "abstract": "  This paper presents refined BigEarthNet (reBEN) that is a large-scale,\nmulti-modal remote sensing dataset constructed to support deep learning (DL)\nstudies for remote sensing image analysis. The reBEN dataset consists of\n549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN,\nwe initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the\nBigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m.\nWe apply atmospheric correction to the Sentinel-2 patches using the latest\nversion of the sen2cor tool, resulting in higher-quality patches compared to\nthose present in BigEarthNet. Each patch is then associated with a pixel-level\nreference map and scene-level multi-labels. This makes reBEN suitable for\npixel- and scene-based learning tasks. The labels are derived from the most\nrecent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class\nnomenclature as in BigEarthNet. The use of the most recent CLC map results in\novercoming the label noise present in BigEarthNet. Furthermore, we introduce a\nnew geographical-based split assignment algorithm that significantly reduces\nthe spatial correlation among the train, validation, and test sets with respect\nto those present in BigEarthNet. This increases the reliability of the\nevaluation of DL models. To minimize the DL model training time, we introduce\nsoftware tools that convert the reBEN dataset into a DL-optimized data format.\nIn our experiments, we show the potential of reBEN for multi-modal multi-label\nimage classification problems by considering several state-of-the-art DL\nmodels. The pre-trained model weights, associated code, and complete dataset\nare available at https://bigearth.net.\n", "link": "http://arxiv.org/abs/2407.03653v5", "date": "2025-05-16", "relevancy": 2.4384, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20reBEN%3A%20Refined%20BigEarthNet%20Dataset%20for%20Remote%20Sensing%20Image%20Analysis&body=Title%3A%20reBEN%3A%20Refined%20BigEarthNet%20Dataset%20for%20Remote%20Sensing%20Image%20Analysis%0AAuthor%3A%20Kai%20Norman%20Clasen%20and%20Leonard%20Hackel%20and%20Tom%20Burgert%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir%20and%20Volker%20Markl%0AAbstract%3A%20%20%20This%20paper%20presents%20refined%20BigEarthNet%20%28reBEN%29%20that%20is%20a%20large-scale%2C%0Amulti-modal%20remote%20sensing%20dataset%20constructed%20to%20support%20deep%20learning%20%28DL%29%0Astudies%20for%20remote%20sensing%20image%20analysis.%20The%20reBEN%20dataset%20consists%20of%0A549%2C488%20pairs%20of%20Sentinel-1%20and%20Sentinel-2%20image%20patches.%20To%20construct%20reBEN%2C%0Awe%20initially%20consider%20the%20Sentinel-1%20and%20Sentinel-2%20tiles%20used%20to%20construct%20the%0ABigEarthNet%20dataset%20and%20then%20divide%20them%20into%20patches%20of%20size%201200%20m%20x%201200%20m.%0AWe%20apply%20atmospheric%20correction%20to%20the%20Sentinel-2%20patches%20using%20the%20latest%0Aversion%20of%20the%20sen2cor%20tool%2C%20resulting%20in%20higher-quality%20patches%20compared%20to%0Athose%20present%20in%20BigEarthNet.%20Each%20patch%20is%20then%20associated%20with%20a%20pixel-level%0Areference%20map%20and%20scene-level%20multi-labels.%20This%20makes%20reBEN%20suitable%20for%0Apixel-%20and%20scene-based%20learning%20tasks.%20The%20labels%20are%20derived%20from%20the%20most%0Arecent%20CORINE%20Land%20Cover%20%28CLC%29%20map%20of%202018%20by%20utilizing%20the%2019-class%0Anomenclature%20as%20in%20BigEarthNet.%20The%20use%20of%20the%20most%20recent%20CLC%20map%20results%20in%0Aovercoming%20the%20label%20noise%20present%20in%20BigEarthNet.%20Furthermore%2C%20we%20introduce%20a%0Anew%20geographical-based%20split%20assignment%20algorithm%20that%20significantly%20reduces%0Athe%20spatial%20correlation%20among%20the%20train%2C%20validation%2C%20and%20test%20sets%20with%20respect%0Ato%20those%20present%20in%20BigEarthNet.%20This%20increases%20the%20reliability%20of%20the%0Aevaluation%20of%20DL%20models.%20To%20minimize%20the%20DL%20model%20training%20time%2C%20we%20introduce%0Asoftware%20tools%20that%20convert%20the%20reBEN%20dataset%20into%20a%20DL-optimized%20data%20format.%0AIn%20our%20experiments%2C%20we%20show%20the%20potential%20of%20reBEN%20for%20multi-modal%20multi-label%0Aimage%20classification%20problems%20by%20considering%20several%20state-of-the-art%20DL%0Amodels.%20The%20pre-trained%20model%20weights%2C%20associated%20code%2C%20and%20complete%20dataset%0Aare%20available%20at%20https%3A//bigearth.net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03653v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DreBEN%253A%2520Refined%2520BigEarthNet%2520Dataset%2520for%2520Remote%2520Sensing%2520Image%2520Analysis%26entry.906535625%3DKai%2520Norman%2520Clasen%2520and%2520Leonard%2520Hackel%2520and%2520Tom%2520Burgert%2520and%2520Gencer%2520Sumbul%2520and%2520Beg%25C3%25BCm%2520Demir%2520and%2520Volker%2520Markl%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520refined%2520BigEarthNet%2520%2528reBEN%2529%2520that%2520is%2520a%2520large-scale%252C%250Amulti-modal%2520remote%2520sensing%2520dataset%2520constructed%2520to%2520support%2520deep%2520learning%2520%2528DL%2529%250Astudies%2520for%2520remote%2520sensing%2520image%2520analysis.%2520The%2520reBEN%2520dataset%2520consists%2520of%250A549%252C488%2520pairs%2520of%2520Sentinel-1%2520and%2520Sentinel-2%2520image%2520patches.%2520To%2520construct%2520reBEN%252C%250Awe%2520initially%2520consider%2520the%2520Sentinel-1%2520and%2520Sentinel-2%2520tiles%2520used%2520to%2520construct%2520the%250ABigEarthNet%2520dataset%2520and%2520then%2520divide%2520them%2520into%2520patches%2520of%2520size%25201200%2520m%2520x%25201200%2520m.%250AWe%2520apply%2520atmospheric%2520correction%2520to%2520the%2520Sentinel-2%2520patches%2520using%2520the%2520latest%250Aversion%2520of%2520the%2520sen2cor%2520tool%252C%2520resulting%2520in%2520higher-quality%2520patches%2520compared%2520to%250Athose%2520present%2520in%2520BigEarthNet.%2520Each%2520patch%2520is%2520then%2520associated%2520with%2520a%2520pixel-level%250Areference%2520map%2520and%2520scene-level%2520multi-labels.%2520This%2520makes%2520reBEN%2520suitable%2520for%250Apixel-%2520and%2520scene-based%2520learning%2520tasks.%2520The%2520labels%2520are%2520derived%2520from%2520the%2520most%250Arecent%2520CORINE%2520Land%2520Cover%2520%2528CLC%2529%2520map%2520of%25202018%2520by%2520utilizing%2520the%252019-class%250Anomenclature%2520as%2520in%2520BigEarthNet.%2520The%2520use%2520of%2520the%2520most%2520recent%2520CLC%2520map%2520results%2520in%250Aovercoming%2520the%2520label%2520noise%2520present%2520in%2520BigEarthNet.%2520Furthermore%252C%2520we%2520introduce%2520a%250Anew%2520geographical-based%2520split%2520assignment%2520algorithm%2520that%2520significantly%2520reduces%250Athe%2520spatial%2520correlation%2520among%2520the%2520train%252C%2520validation%252C%2520and%2520test%2520sets%2520with%2520respect%250Ato%2520those%2520present%2520in%2520BigEarthNet.%2520This%2520increases%2520the%2520reliability%2520of%2520the%250Aevaluation%2520of%2520DL%2520models.%2520To%2520minimize%2520the%2520DL%2520model%2520training%2520time%252C%2520we%2520introduce%250Asoftware%2520tools%2520that%2520convert%2520the%2520reBEN%2520dataset%2520into%2520a%2520DL-optimized%2520data%2520format.%250AIn%2520our%2520experiments%252C%2520we%2520show%2520the%2520potential%2520of%2520reBEN%2520for%2520multi-modal%2520multi-label%250Aimage%2520classification%2520problems%2520by%2520considering%2520several%2520state-of-the-art%2520DL%250Amodels.%2520The%2520pre-trained%2520model%2520weights%252C%2520associated%2520code%252C%2520and%2520complete%2520dataset%250Aare%2520available%2520at%2520https%253A//bigearth.net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03653v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=reBEN%3A%20Refined%20BigEarthNet%20Dataset%20for%20Remote%20Sensing%20Image%20Analysis&entry.906535625=Kai%20Norman%20Clasen%20and%20Leonard%20Hackel%20and%20Tom%20Burgert%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir%20and%20Volker%20Markl&entry.1292438233=%20%20This%20paper%20presents%20refined%20BigEarthNet%20%28reBEN%29%20that%20is%20a%20large-scale%2C%0Amulti-modal%20remote%20sensing%20dataset%20constructed%20to%20support%20deep%20learning%20%28DL%29%0Astudies%20for%20remote%20sensing%20image%20analysis.%20The%20reBEN%20dataset%20consists%20of%0A549%2C488%20pairs%20of%20Sentinel-1%20and%20Sentinel-2%20image%20patches.%20To%20construct%20reBEN%2C%0Awe%20initially%20consider%20the%20Sentinel-1%20and%20Sentinel-2%20tiles%20used%20to%20construct%20the%0ABigEarthNet%20dataset%20and%20then%20divide%20them%20into%20patches%20of%20size%201200%20m%20x%201200%20m.%0AWe%20apply%20atmospheric%20correction%20to%20the%20Sentinel-2%20patches%20using%20the%20latest%0Aversion%20of%20the%20sen2cor%20tool%2C%20resulting%20in%20higher-quality%20patches%20compared%20to%0Athose%20present%20in%20BigEarthNet.%20Each%20patch%20is%20then%20associated%20with%20a%20pixel-level%0Areference%20map%20and%20scene-level%20multi-labels.%20This%20makes%20reBEN%20suitable%20for%0Apixel-%20and%20scene-based%20learning%20tasks.%20The%20labels%20are%20derived%20from%20the%20most%0Arecent%20CORINE%20Land%20Cover%20%28CLC%29%20map%20of%202018%20by%20utilizing%20the%2019-class%0Anomenclature%20as%20in%20BigEarthNet.%20The%20use%20of%20the%20most%20recent%20CLC%20map%20results%20in%0Aovercoming%20the%20label%20noise%20present%20in%20BigEarthNet.%20Furthermore%2C%20we%20introduce%20a%0Anew%20geographical-based%20split%20assignment%20algorithm%20that%20significantly%20reduces%0Athe%20spatial%20correlation%20among%20the%20train%2C%20validation%2C%20and%20test%20sets%20with%20respect%0Ato%20those%20present%20in%20BigEarthNet.%20This%20increases%20the%20reliability%20of%20the%0Aevaluation%20of%20DL%20models.%20To%20minimize%20the%20DL%20model%20training%20time%2C%20we%20introduce%0Asoftware%20tools%20that%20convert%20the%20reBEN%20dataset%20into%20a%20DL-optimized%20data%20format.%0AIn%20our%20experiments%2C%20we%20show%20the%20potential%20of%20reBEN%20for%20multi-modal%20multi-label%0Aimage%20classification%20problems%20by%20considering%20several%20state-of-the-art%20DL%0Amodels.%20The%20pre-trained%20model%20weights%2C%20associated%20code%2C%20and%20complete%20dataset%0Aare%20available%20at%20https%3A//bigearth.net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03653v5&entry.124074799=Read"},
{"title": "Espresso: High Compression For Rich Extraction From Videos for Your\n  Vision-Language Model", "author": "Keunwoo Peter Yu and Achal Dave and Rares Ambrus and Jean Mercat", "abstract": "  Recent advances in vision-language models (VLMs) have shown great promise in\nconnecting images and text, but extending these models to long videos remains\nchallenging due to the rapid growth in token counts. Models that compress\nvideos by local aggregation in time or space have become popular for handling\nlong-form inputs; however, these pooling-based projectors sacrifice the\nbenefits of fixed-length representations that are crucial for streaming and\nefficient video understanding. We introduce $\\texttt{Espresso}$, a new\narchitecture that separately compresses spatial and temporal features into\nfixed-length sequences. $\\texttt{Espresso}$ enables efficient video encoding\nwhile maintaining strong long-form reasoning capabilities. Experiments show\nthat fixed-length compression combined with segment-wise processing offers a\nscalable and competitive alternative to pooling-based approaches. Our results\ndemonstrate that fixed-length projectors, when properly designed and trained,\nremain a viable foundation for video-language modeling.\n", "link": "http://arxiv.org/abs/2412.04729v3", "date": "2025-05-16", "relevancy": 2.4369, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6173}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Espresso%3A%20High%20Compression%20For%20Rich%20Extraction%20From%20Videos%20for%20Your%0A%20%20Vision-Language%20Model&body=Title%3A%20Espresso%3A%20High%20Compression%20For%20Rich%20Extraction%20From%20Videos%20for%20Your%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Keunwoo%20Peter%20Yu%20and%20Achal%20Dave%20and%20Rares%20Ambrus%20and%20Jean%20Mercat%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20shown%20great%20promise%20in%0Aconnecting%20images%20and%20text%2C%20but%20extending%20these%20models%20to%20long%20videos%20remains%0Achallenging%20due%20to%20the%20rapid%20growth%20in%20token%20counts.%20Models%20that%20compress%0Avideos%20by%20local%20aggregation%20in%20time%20or%20space%20have%20become%20popular%20for%20handling%0Along-form%20inputs%3B%20however%2C%20these%20pooling-based%20projectors%20sacrifice%20the%0Abenefits%20of%20fixed-length%20representations%20that%20are%20crucial%20for%20streaming%20and%0Aefficient%20video%20understanding.%20We%20introduce%20%24%5Ctexttt%7BEspresso%7D%24%2C%20a%20new%0Aarchitecture%20that%20separately%20compresses%20spatial%20and%20temporal%20features%20into%0Afixed-length%20sequences.%20%24%5Ctexttt%7BEspresso%7D%24%20enables%20efficient%20video%20encoding%0Awhile%20maintaining%20strong%20long-form%20reasoning%20capabilities.%20Experiments%20show%0Athat%20fixed-length%20compression%20combined%20with%20segment-wise%20processing%20offers%20a%0Ascalable%20and%20competitive%20alternative%20to%20pooling-based%20approaches.%20Our%20results%0Ademonstrate%20that%20fixed-length%20projectors%2C%20when%20properly%20designed%20and%20trained%2C%0Aremain%20a%20viable%20foundation%20for%20video-language%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04729v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEspresso%253A%2520High%2520Compression%2520For%2520Rich%2520Extraction%2520From%2520Videos%2520for%2520Your%250A%2520%2520Vision-Language%2520Model%26entry.906535625%3DKeunwoo%2520Peter%2520Yu%2520and%2520Achal%2520Dave%2520and%2520Rares%2520Ambrus%2520and%2520Jean%2520Mercat%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520great%2520promise%2520in%250Aconnecting%2520images%2520and%2520text%252C%2520but%2520extending%2520these%2520models%2520to%2520long%2520videos%2520remains%250Achallenging%2520due%2520to%2520the%2520rapid%2520growth%2520in%2520token%2520counts.%2520Models%2520that%2520compress%250Avideos%2520by%2520local%2520aggregation%2520in%2520time%2520or%2520space%2520have%2520become%2520popular%2520for%2520handling%250Along-form%2520inputs%253B%2520however%252C%2520these%2520pooling-based%2520projectors%2520sacrifice%2520the%250Abenefits%2520of%2520fixed-length%2520representations%2520that%2520are%2520crucial%2520for%2520streaming%2520and%250Aefficient%2520video%2520understanding.%2520We%2520introduce%2520%2524%255Ctexttt%257BEspresso%257D%2524%252C%2520a%2520new%250Aarchitecture%2520that%2520separately%2520compresses%2520spatial%2520and%2520temporal%2520features%2520into%250Afixed-length%2520sequences.%2520%2524%255Ctexttt%257BEspresso%257D%2524%2520enables%2520efficient%2520video%2520encoding%250Awhile%2520maintaining%2520strong%2520long-form%2520reasoning%2520capabilities.%2520Experiments%2520show%250Athat%2520fixed-length%2520compression%2520combined%2520with%2520segment-wise%2520processing%2520offers%2520a%250Ascalable%2520and%2520competitive%2520alternative%2520to%2520pooling-based%2520approaches.%2520Our%2520results%250Ademonstrate%2520that%2520fixed-length%2520projectors%252C%2520when%2520properly%2520designed%2520and%2520trained%252C%250Aremain%2520a%2520viable%2520foundation%2520for%2520video-language%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04729v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Espresso%3A%20High%20Compression%20For%20Rich%20Extraction%20From%20Videos%20for%20Your%0A%20%20Vision-Language%20Model&entry.906535625=Keunwoo%20Peter%20Yu%20and%20Achal%20Dave%20and%20Rares%20Ambrus%20and%20Jean%20Mercat&entry.1292438233=%20%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20shown%20great%20promise%20in%0Aconnecting%20images%20and%20text%2C%20but%20extending%20these%20models%20to%20long%20videos%20remains%0Achallenging%20due%20to%20the%20rapid%20growth%20in%20token%20counts.%20Models%20that%20compress%0Avideos%20by%20local%20aggregation%20in%20time%20or%20space%20have%20become%20popular%20for%20handling%0Along-form%20inputs%3B%20however%2C%20these%20pooling-based%20projectors%20sacrifice%20the%0Abenefits%20of%20fixed-length%20representations%20that%20are%20crucial%20for%20streaming%20and%0Aefficient%20video%20understanding.%20We%20introduce%20%24%5Ctexttt%7BEspresso%7D%24%2C%20a%20new%0Aarchitecture%20that%20separately%20compresses%20spatial%20and%20temporal%20features%20into%0Afixed-length%20sequences.%20%24%5Ctexttt%7BEspresso%7D%24%20enables%20efficient%20video%20encoding%0Awhile%20maintaining%20strong%20long-form%20reasoning%20capabilities.%20Experiments%20show%0Athat%20fixed-length%20compression%20combined%20with%20segment-wise%20processing%20offers%20a%0Ascalable%20and%20competitive%20alternative%20to%20pooling-based%20approaches.%20Our%20results%0Ademonstrate%20that%20fixed-length%20projectors%2C%20when%20properly%20designed%20and%20trained%2C%0Aremain%20a%20viable%20foundation%20for%20video-language%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04729v3&entry.124074799=Read"},
{"title": "Memory-Efficient Orthogonal Fine-Tuning with Principal Subspace\n  Adaptation", "author": "Fei Wu and Jia Hu and Geyong Min and Shiqiang Wang", "abstract": "  Driven by the relentless growth in model parameters, which renders full\nfine-tuning prohibitively expensive for large-scale deployment,\nparameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for\nrapidly adapting large models to a wide range of downstream tasks. Among the\nPEFT family, orthogonal fine-tuning and its variants have demonstrated\nremarkable performance by preserving hyperspherical energy, which encodes\npairwise angular similarity between neurons. However, these methods are\ninherently memory-inefficient due to the need to store intermediate activations\nfrom multiple full-dimensional sparse matrices. To address this limitation, we\npropose Memory-efficient Orthogonal Fine-Tuning (MOFT) with principal subspace\nadaptation. Specifically, we first establish a theoretical condition under\nwhich orthogonal transformations within a low-rank subspace preserve\nhyperspherical energy. Based on this insight, we constrain orthogonal\nfine-tuning to the principal subspace defined by the top-r components obtained\nthrough singular value decomposition and impose an additional constraint on the\nprojection matrix to satisfy the preservation condition. To enhance MOFT's\nflexibility across tasks, we relax strict orthogonality by introducing two\nlearnable scaling vectors. Extensive experiments on 37 diverse tasks and four\nmodels across NLP and CV demonstrate that MOFT consistently outperforms key\nbaselines while significantly reducing the memory footprint of orthogonal\nfine-tuning.\n", "link": "http://arxiv.org/abs/2505.11235v1", "date": "2025-05-16", "relevancy": 2.4287, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4913}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4873}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-Efficient%20Orthogonal%20Fine-Tuning%20with%20Principal%20Subspace%0A%20%20Adaptation&body=Title%3A%20Memory-Efficient%20Orthogonal%20Fine-Tuning%20with%20Principal%20Subspace%0A%20%20Adaptation%0AAuthor%3A%20Fei%20Wu%20and%20Jia%20Hu%20and%20Geyong%20Min%20and%20Shiqiang%20Wang%0AAbstract%3A%20%20%20Driven%20by%20the%20relentless%20growth%20in%20model%20parameters%2C%20which%20renders%20full%0Afine-tuning%20prohibitively%20expensive%20for%20large-scale%20deployment%2C%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20has%20emerged%20as%20a%20crucial%20approach%20for%0Arapidly%20adapting%20large%20models%20to%20a%20wide%20range%20of%20downstream%20tasks.%20Among%20the%0APEFT%20family%2C%20orthogonal%20fine-tuning%20and%20its%20variants%20have%20demonstrated%0Aremarkable%20performance%20by%20preserving%20hyperspherical%20energy%2C%20which%20encodes%0Apairwise%20angular%20similarity%20between%20neurons.%20However%2C%20these%20methods%20are%0Ainherently%20memory-inefficient%20due%20to%20the%20need%20to%20store%20intermediate%20activations%0Afrom%20multiple%20full-dimensional%20sparse%20matrices.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Memory-efficient%20Orthogonal%20Fine-Tuning%20%28MOFT%29%20with%20principal%20subspace%0Aadaptation.%20Specifically%2C%20we%20first%20establish%20a%20theoretical%20condition%20under%0Awhich%20orthogonal%20transformations%20within%20a%20low-rank%20subspace%20preserve%0Ahyperspherical%20energy.%20Based%20on%20this%20insight%2C%20we%20constrain%20orthogonal%0Afine-tuning%20to%20the%20principal%20subspace%20defined%20by%20the%20top-r%20components%20obtained%0Athrough%20singular%20value%20decomposition%20and%20impose%20an%20additional%20constraint%20on%20the%0Aprojection%20matrix%20to%20satisfy%20the%20preservation%20condition.%20To%20enhance%20MOFT%27s%0Aflexibility%20across%20tasks%2C%20we%20relax%20strict%20orthogonality%20by%20introducing%20two%0Alearnable%20scaling%20vectors.%20Extensive%20experiments%20on%2037%20diverse%20tasks%20and%20four%0Amodels%20across%20NLP%20and%20CV%20demonstrate%20that%20MOFT%20consistently%20outperforms%20key%0Abaselines%20while%20significantly%20reducing%20the%20memory%20footprint%20of%20orthogonal%0Afine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-Efficient%2520Orthogonal%2520Fine-Tuning%2520with%2520Principal%2520Subspace%250A%2520%2520Adaptation%26entry.906535625%3DFei%2520Wu%2520and%2520Jia%2520Hu%2520and%2520Geyong%2520Min%2520and%2520Shiqiang%2520Wang%26entry.1292438233%3D%2520%2520Driven%2520by%2520the%2520relentless%2520growth%2520in%2520model%2520parameters%252C%2520which%2520renders%2520full%250Afine-tuning%2520prohibitively%2520expensive%2520for%2520large-scale%2520deployment%252C%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520has%2520emerged%2520as%2520a%2520crucial%2520approach%2520for%250Arapidly%2520adapting%2520large%2520models%2520to%2520a%2520wide%2520range%2520of%2520downstream%2520tasks.%2520Among%2520the%250APEFT%2520family%252C%2520orthogonal%2520fine-tuning%2520and%2520its%2520variants%2520have%2520demonstrated%250Aremarkable%2520performance%2520by%2520preserving%2520hyperspherical%2520energy%252C%2520which%2520encodes%250Apairwise%2520angular%2520similarity%2520between%2520neurons.%2520However%252C%2520these%2520methods%2520are%250Ainherently%2520memory-inefficient%2520due%2520to%2520the%2520need%2520to%2520store%2520intermediate%2520activations%250Afrom%2520multiple%2520full-dimensional%2520sparse%2520matrices.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520Memory-efficient%2520Orthogonal%2520Fine-Tuning%2520%2528MOFT%2529%2520with%2520principal%2520subspace%250Aadaptation.%2520Specifically%252C%2520we%2520first%2520establish%2520a%2520theoretical%2520condition%2520under%250Awhich%2520orthogonal%2520transformations%2520within%2520a%2520low-rank%2520subspace%2520preserve%250Ahyperspherical%2520energy.%2520Based%2520on%2520this%2520insight%252C%2520we%2520constrain%2520orthogonal%250Afine-tuning%2520to%2520the%2520principal%2520subspace%2520defined%2520by%2520the%2520top-r%2520components%2520obtained%250Athrough%2520singular%2520value%2520decomposition%2520and%2520impose%2520an%2520additional%2520constraint%2520on%2520the%250Aprojection%2520matrix%2520to%2520satisfy%2520the%2520preservation%2520condition.%2520To%2520enhance%2520MOFT%2527s%250Aflexibility%2520across%2520tasks%252C%2520we%2520relax%2520strict%2520orthogonality%2520by%2520introducing%2520two%250Alearnable%2520scaling%2520vectors.%2520Extensive%2520experiments%2520on%252037%2520diverse%2520tasks%2520and%2520four%250Amodels%2520across%2520NLP%2520and%2520CV%2520demonstrate%2520that%2520MOFT%2520consistently%2520outperforms%2520key%250Abaselines%2520while%2520significantly%2520reducing%2520the%2520memory%2520footprint%2520of%2520orthogonal%250Afine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-Efficient%20Orthogonal%20Fine-Tuning%20with%20Principal%20Subspace%0A%20%20Adaptation&entry.906535625=Fei%20Wu%20and%20Jia%20Hu%20and%20Geyong%20Min%20and%20Shiqiang%20Wang&entry.1292438233=%20%20Driven%20by%20the%20relentless%20growth%20in%20model%20parameters%2C%20which%20renders%20full%0Afine-tuning%20prohibitively%20expensive%20for%20large-scale%20deployment%2C%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20has%20emerged%20as%20a%20crucial%20approach%20for%0Arapidly%20adapting%20large%20models%20to%20a%20wide%20range%20of%20downstream%20tasks.%20Among%20the%0APEFT%20family%2C%20orthogonal%20fine-tuning%20and%20its%20variants%20have%20demonstrated%0Aremarkable%20performance%20by%20preserving%20hyperspherical%20energy%2C%20which%20encodes%0Apairwise%20angular%20similarity%20between%20neurons.%20However%2C%20these%20methods%20are%0Ainherently%20memory-inefficient%20due%20to%20the%20need%20to%20store%20intermediate%20activations%0Afrom%20multiple%20full-dimensional%20sparse%20matrices.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Memory-efficient%20Orthogonal%20Fine-Tuning%20%28MOFT%29%20with%20principal%20subspace%0Aadaptation.%20Specifically%2C%20we%20first%20establish%20a%20theoretical%20condition%20under%0Awhich%20orthogonal%20transformations%20within%20a%20low-rank%20subspace%20preserve%0Ahyperspherical%20energy.%20Based%20on%20this%20insight%2C%20we%20constrain%20orthogonal%0Afine-tuning%20to%20the%20principal%20subspace%20defined%20by%20the%20top-r%20components%20obtained%0Athrough%20singular%20value%20decomposition%20and%20impose%20an%20additional%20constraint%20on%20the%0Aprojection%20matrix%20to%20satisfy%20the%20preservation%20condition.%20To%20enhance%20MOFT%27s%0Aflexibility%20across%20tasks%2C%20we%20relax%20strict%20orthogonality%20by%20introducing%20two%0Alearnable%20scaling%20vectors.%20Extensive%20experiments%20on%2037%20diverse%20tasks%20and%20four%0Amodels%20across%20NLP%20and%20CV%20demonstrate%20that%20MOFT%20consistently%20outperforms%20key%0Abaselines%20while%20significantly%20reducing%20the%20memory%20footprint%20of%20orthogonal%0Afine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11235v1&entry.124074799=Read"},
{"title": "Nash: Neural Adaptive Shrinkage for Structured High-Dimensional\n  Regression", "author": "William R. P. Denault", "abstract": "  Sparse linear regression is a fundamental tool in data analysis. However,\ntraditional approaches often fall short when covariates exhibit structure or\narise from heterogeneous sources. In biomedical applications, covariates may\nstem from distinct modalities or be structured according to an underlying\ngraph. We introduce Neural Adaptive Shrinkage (Nash), a unified framework that\nintegrates covariate-specific side information into sparse regression via\nneural networks. Nash adaptively modulates penalties on a per-covariate basis,\nlearning to tailor regularization without cross-validation. We develop a\nvariational inference algorithm for efficient training and establish\nconnections to empirical Bayes regression. Experiments on real data demonstrate\nthat Nash can improve accuracy and adaptability over existing methods.\n", "link": "http://arxiv.org/abs/2505.11143v1", "date": "2025-05-16", "relevancy": 2.4251, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4928}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4824}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nash%3A%20Neural%20Adaptive%20Shrinkage%20for%20Structured%20High-Dimensional%0A%20%20Regression&body=Title%3A%20Nash%3A%20Neural%20Adaptive%20Shrinkage%20for%20Structured%20High-Dimensional%0A%20%20Regression%0AAuthor%3A%20William%20R.%20P.%20Denault%0AAbstract%3A%20%20%20Sparse%20linear%20regression%20is%20a%20fundamental%20tool%20in%20data%20analysis.%20However%2C%0Atraditional%20approaches%20often%20fall%20short%20when%20covariates%20exhibit%20structure%20or%0Aarise%20from%20heterogeneous%20sources.%20In%20biomedical%20applications%2C%20covariates%20may%0Astem%20from%20distinct%20modalities%20or%20be%20structured%20according%20to%20an%20underlying%0Agraph.%20We%20introduce%20Neural%20Adaptive%20Shrinkage%20%28Nash%29%2C%20a%20unified%20framework%20that%0Aintegrates%20covariate-specific%20side%20information%20into%20sparse%20regression%20via%0Aneural%20networks.%20Nash%20adaptively%20modulates%20penalties%20on%20a%20per-covariate%20basis%2C%0Alearning%20to%20tailor%20regularization%20without%20cross-validation.%20We%20develop%20a%0Avariational%20inference%20algorithm%20for%20efficient%20training%20and%20establish%0Aconnections%20to%20empirical%20Bayes%20regression.%20Experiments%20on%20real%20data%20demonstrate%0Athat%20Nash%20can%20improve%20accuracy%20and%20adaptability%20over%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNash%253A%2520Neural%2520Adaptive%2520Shrinkage%2520for%2520Structured%2520High-Dimensional%250A%2520%2520Regression%26entry.906535625%3DWilliam%2520R.%2520P.%2520Denault%26entry.1292438233%3D%2520%2520Sparse%2520linear%2520regression%2520is%2520a%2520fundamental%2520tool%2520in%2520data%2520analysis.%2520However%252C%250Atraditional%2520approaches%2520often%2520fall%2520short%2520when%2520covariates%2520exhibit%2520structure%2520or%250Aarise%2520from%2520heterogeneous%2520sources.%2520In%2520biomedical%2520applications%252C%2520covariates%2520may%250Astem%2520from%2520distinct%2520modalities%2520or%2520be%2520structured%2520according%2520to%2520an%2520underlying%250Agraph.%2520We%2520introduce%2520Neural%2520Adaptive%2520Shrinkage%2520%2528Nash%2529%252C%2520a%2520unified%2520framework%2520that%250Aintegrates%2520covariate-specific%2520side%2520information%2520into%2520sparse%2520regression%2520via%250Aneural%2520networks.%2520Nash%2520adaptively%2520modulates%2520penalties%2520on%2520a%2520per-covariate%2520basis%252C%250Alearning%2520to%2520tailor%2520regularization%2520without%2520cross-validation.%2520We%2520develop%2520a%250Avariational%2520inference%2520algorithm%2520for%2520efficient%2520training%2520and%2520establish%250Aconnections%2520to%2520empirical%2520Bayes%2520regression.%2520Experiments%2520on%2520real%2520data%2520demonstrate%250Athat%2520Nash%2520can%2520improve%2520accuracy%2520and%2520adaptability%2520over%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nash%3A%20Neural%20Adaptive%20Shrinkage%20for%20Structured%20High-Dimensional%0A%20%20Regression&entry.906535625=William%20R.%20P.%20Denault&entry.1292438233=%20%20Sparse%20linear%20regression%20is%20a%20fundamental%20tool%20in%20data%20analysis.%20However%2C%0Atraditional%20approaches%20often%20fall%20short%20when%20covariates%20exhibit%20structure%20or%0Aarise%20from%20heterogeneous%20sources.%20In%20biomedical%20applications%2C%20covariates%20may%0Astem%20from%20distinct%20modalities%20or%20be%20structured%20according%20to%20an%20underlying%0Agraph.%20We%20introduce%20Neural%20Adaptive%20Shrinkage%20%28Nash%29%2C%20a%20unified%20framework%20that%0Aintegrates%20covariate-specific%20side%20information%20into%20sparse%20regression%20via%0Aneural%20networks.%20Nash%20adaptively%20modulates%20penalties%20on%20a%20per-covariate%20basis%2C%0Alearning%20to%20tailor%20regularization%20without%20cross-validation.%20We%20develop%20a%0Avariational%20inference%20algorithm%20for%20efficient%20training%20and%20establish%0Aconnections%20to%20empirical%20Bayes%20regression.%20Experiments%20on%20real%20data%20demonstrate%0Athat%20Nash%20can%20improve%20accuracy%20and%20adaptability%20over%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11143v1&entry.124074799=Read"},
{"title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned\n  Guidance", "author": "Jiazi Bu and Pengyang Ling and Yujie Zhou and Pan Zhang and Tong Wu and Xiaoyi Dong and Yuhang Zang and Yuhang Cao and Dahua Lin and Jiaqi Wang", "abstract": "  Text-to-image (T2I) diffusion/flow models have drawn considerable attention\nrecently due to their remarkable ability to deliver flexible visual creations.\nStill, high-resolution image synthesis presents formidable challenges due to\nthe scarcity and complexity of high-resolution content. Recent approaches have\ninvestigated training-free strategies to enable high-resolution image synthesis\nwith pre-trained models. However, these techniques often struggle with\ngenerating high-quality visuals and tend to exhibit artifacts or low-fidelity\ndetails, as they typically rely solely on the endpoint of the low-resolution\nsampling trajectory while neglecting intermediate states that are critical for\npreserving structure and synthesizing finer detail. To this end, we present\nHiFlow, a training-free and model-agnostic framework to unlock the resolution\npotential of pre-trained flow models. Specifically, HiFlow establishes a\nvirtual reference flow within the high-resolution space that effectively\ncaptures the characteristics of low-resolution flow information, offering\nguidance for high-resolution generation through three key aspects:\ninitialization alignment for low-frequency consistency, direction alignment for\nstructure preservation, and acceleration alignment for detail fidelity. By\nleveraging such flow-aligned guidance, HiFlow substantially elevates the\nquality of high-resolution image synthesis of T2I models and demonstrates\nversatility across their personalized variants. Extensive experiments validate\nHiFlow's capability in achieving superior high-resolution image quality over\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2504.06232v2", "date": "2025-05-16", "relevancy": 2.4249, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7088}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5917}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiFlow%3A%20Training-free%20High-Resolution%20Image%20Generation%20with%20Flow-Aligned%0A%20%20Guidance&body=Title%3A%20HiFlow%3A%20Training-free%20High-Resolution%20Image%20Generation%20with%20Flow-Aligned%0A%20%20Guidance%0AAuthor%3A%20Jiazi%20Bu%20and%20Pengyang%20Ling%20and%20Yujie%20Zhou%20and%20Pan%20Zhang%20and%20Tong%20Wu%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion/flow%20models%20have%20drawn%20considerable%20attention%0Arecently%20due%20to%20their%20remarkable%20ability%20to%20deliver%20flexible%20visual%20creations.%0AStill%2C%20high-resolution%20image%20synthesis%20presents%20formidable%20challenges%20due%20to%0Athe%20scarcity%20and%20complexity%20of%20high-resolution%20content.%20Recent%20approaches%20have%0Ainvestigated%20training-free%20strategies%20to%20enable%20high-resolution%20image%20synthesis%0Awith%20pre-trained%20models.%20However%2C%20these%20techniques%20often%20struggle%20with%0Agenerating%20high-quality%20visuals%20and%20tend%20to%20exhibit%20artifacts%20or%20low-fidelity%0Adetails%2C%20as%20they%20typically%20rely%20solely%20on%20the%20endpoint%20of%20the%20low-resolution%0Asampling%20trajectory%20while%20neglecting%20intermediate%20states%20that%20are%20critical%20for%0Apreserving%20structure%20and%20synthesizing%20finer%20detail.%20To%20this%20end%2C%20we%20present%0AHiFlow%2C%20a%20training-free%20and%20model-agnostic%20framework%20to%20unlock%20the%20resolution%0Apotential%20of%20pre-trained%20flow%20models.%20Specifically%2C%20HiFlow%20establishes%20a%0Avirtual%20reference%20flow%20within%20the%20high-resolution%20space%20that%20effectively%0Acaptures%20the%20characteristics%20of%20low-resolution%20flow%20information%2C%20offering%0Aguidance%20for%20high-resolution%20generation%20through%20three%20key%20aspects%3A%0Ainitialization%20alignment%20for%20low-frequency%20consistency%2C%20direction%20alignment%20for%0Astructure%20preservation%2C%20and%20acceleration%20alignment%20for%20detail%20fidelity.%20By%0Aleveraging%20such%20flow-aligned%20guidance%2C%20HiFlow%20substantially%20elevates%20the%0Aquality%20of%20high-resolution%20image%20synthesis%20of%20T2I%20models%20and%20demonstrates%0Aversatility%20across%20their%20personalized%20variants.%20Extensive%20experiments%20validate%0AHiFlow%27s%20capability%20in%20achieving%20superior%20high-resolution%20image%20quality%20over%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06232v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiFlow%253A%2520Training-free%2520High-Resolution%2520Image%2520Generation%2520with%2520Flow-Aligned%250A%2520%2520Guidance%26entry.906535625%3DJiazi%2520Bu%2520and%2520Pengyang%2520Ling%2520and%2520Yujie%2520Zhou%2520and%2520Pan%2520Zhang%2520and%2520Tong%2520Wu%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion/flow%2520models%2520have%2520drawn%2520considerable%2520attention%250Arecently%2520due%2520to%2520their%2520remarkable%2520ability%2520to%2520deliver%2520flexible%2520visual%2520creations.%250AStill%252C%2520high-resolution%2520image%2520synthesis%2520presents%2520formidable%2520challenges%2520due%2520to%250Athe%2520scarcity%2520and%2520complexity%2520of%2520high-resolution%2520content.%2520Recent%2520approaches%2520have%250Ainvestigated%2520training-free%2520strategies%2520to%2520enable%2520high-resolution%2520image%2520synthesis%250Awith%2520pre-trained%2520models.%2520However%252C%2520these%2520techniques%2520often%2520struggle%2520with%250Agenerating%2520high-quality%2520visuals%2520and%2520tend%2520to%2520exhibit%2520artifacts%2520or%2520low-fidelity%250Adetails%252C%2520as%2520they%2520typically%2520rely%2520solely%2520on%2520the%2520endpoint%2520of%2520the%2520low-resolution%250Asampling%2520trajectory%2520while%2520neglecting%2520intermediate%2520states%2520that%2520are%2520critical%2520for%250Apreserving%2520structure%2520and%2520synthesizing%2520finer%2520detail.%2520To%2520this%2520end%252C%2520we%2520present%250AHiFlow%252C%2520a%2520training-free%2520and%2520model-agnostic%2520framework%2520to%2520unlock%2520the%2520resolution%250Apotential%2520of%2520pre-trained%2520flow%2520models.%2520Specifically%252C%2520HiFlow%2520establishes%2520a%250Avirtual%2520reference%2520flow%2520within%2520the%2520high-resolution%2520space%2520that%2520effectively%250Acaptures%2520the%2520characteristics%2520of%2520low-resolution%2520flow%2520information%252C%2520offering%250Aguidance%2520for%2520high-resolution%2520generation%2520through%2520three%2520key%2520aspects%253A%250Ainitialization%2520alignment%2520for%2520low-frequency%2520consistency%252C%2520direction%2520alignment%2520for%250Astructure%2520preservation%252C%2520and%2520acceleration%2520alignment%2520for%2520detail%2520fidelity.%2520By%250Aleveraging%2520such%2520flow-aligned%2520guidance%252C%2520HiFlow%2520substantially%2520elevates%2520the%250Aquality%2520of%2520high-resolution%2520image%2520synthesis%2520of%2520T2I%2520models%2520and%2520demonstrates%250Aversatility%2520across%2520their%2520personalized%2520variants.%2520Extensive%2520experiments%2520validate%250AHiFlow%2527s%2520capability%2520in%2520achieving%2520superior%2520high-resolution%2520image%2520quality%2520over%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06232v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiFlow%3A%20Training-free%20High-Resolution%20Image%20Generation%20with%20Flow-Aligned%0A%20%20Guidance&entry.906535625=Jiazi%20Bu%20and%20Pengyang%20Ling%20and%20Yujie%20Zhou%20and%20Pan%20Zhang%20and%20Tong%20Wu%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion/flow%20models%20have%20drawn%20considerable%20attention%0Arecently%20due%20to%20their%20remarkable%20ability%20to%20deliver%20flexible%20visual%20creations.%0AStill%2C%20high-resolution%20image%20synthesis%20presents%20formidable%20challenges%20due%20to%0Athe%20scarcity%20and%20complexity%20of%20high-resolution%20content.%20Recent%20approaches%20have%0Ainvestigated%20training-free%20strategies%20to%20enable%20high-resolution%20image%20synthesis%0Awith%20pre-trained%20models.%20However%2C%20these%20techniques%20often%20struggle%20with%0Agenerating%20high-quality%20visuals%20and%20tend%20to%20exhibit%20artifacts%20or%20low-fidelity%0Adetails%2C%20as%20they%20typically%20rely%20solely%20on%20the%20endpoint%20of%20the%20low-resolution%0Asampling%20trajectory%20while%20neglecting%20intermediate%20states%20that%20are%20critical%20for%0Apreserving%20structure%20and%20synthesizing%20finer%20detail.%20To%20this%20end%2C%20we%20present%0AHiFlow%2C%20a%20training-free%20and%20model-agnostic%20framework%20to%20unlock%20the%20resolution%0Apotential%20of%20pre-trained%20flow%20models.%20Specifically%2C%20HiFlow%20establishes%20a%0Avirtual%20reference%20flow%20within%20the%20high-resolution%20space%20that%20effectively%0Acaptures%20the%20characteristics%20of%20low-resolution%20flow%20information%2C%20offering%0Aguidance%20for%20high-resolution%20generation%20through%20three%20key%20aspects%3A%0Ainitialization%20alignment%20for%20low-frequency%20consistency%2C%20direction%20alignment%20for%0Astructure%20preservation%2C%20and%20acceleration%20alignment%20for%20detail%20fidelity.%20By%0Aleveraging%20such%20flow-aligned%20guidance%2C%20HiFlow%20substantially%20elevates%20the%0Aquality%20of%20high-resolution%20image%20synthesis%20of%20T2I%20models%20and%20demonstrates%0Aversatility%20across%20their%20personalized%20variants.%20Extensive%20experiments%20validate%0AHiFlow%27s%20capability%20in%20achieving%20superior%20high-resolution%20image%20quality%20over%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06232v2&entry.124074799=Read"},
{"title": "SynCL: A Synergistic Training Strategy with Instance-Aware Contrastive\n  Learning for End-to-End Multi-Camera 3D Tracking", "author": "Shubo Lin and Yutong Kou and Zirui Wu and Shaoru Wang and Bing Li and Weiming Hu and Jin Gao", "abstract": "  While existing query-based 3D end-to-end visual trackers integrate detection\nand tracking via the tracking-by-attention paradigm, these two chicken-and-egg\ntasks encounter optimization difficulties when sharing the same parameters. Our\nfindings reveal that these difficulties arise due to two inherent constraints\non the self-attention mechanism, i.e., over-deduplication for object queries\nand self-centric attention for track queries. In contrast, removing the\nself-attention mechanism not only minimally impacts regression predictions of\nthe tracker, but also tends to generate more latent candidate boxes. Based on\nthese analyses, we present SynCL, a novel plug-and-play synergistic training\nstrategy designed to co-facilitate multi-task learning for detection and\ntracking. Specifically, we propose a Task-specific Hybrid Matching module for a\nweight-shared cross-attention-based decoder that matches the targets of track\nqueries with multiple object queries to exploit promising candidates overlooked\nby the self-attention mechanism. To flexibly select optimal candidates for the\none-to-many matching, we also design a Dynamic Query Filtering module\ncontrolled by model training status. Moreover, we introduce Instance-aware\nContrastive Learning to break through the barrier of self-centric attention for\ntrack queries, effectively bridging the gap between detection and tracking.\nWithout additional inference costs, SynCL consistently delivers improvements in\nvarious benchmarks and achieves state-of-the-art performance with $58.9\\%$\nAMOTA on the nuScenes dataset. Code and raw results will be publicly available.\n", "link": "http://arxiv.org/abs/2411.06780v3", "date": "2025-05-16", "relevancy": 2.4181, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6093}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6057}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynCL%3A%20A%20Synergistic%20Training%20Strategy%20with%20Instance-Aware%20Contrastive%0A%20%20Learning%20for%20End-to-End%20Multi-Camera%203D%20Tracking&body=Title%3A%20SynCL%3A%20A%20Synergistic%20Training%20Strategy%20with%20Instance-Aware%20Contrastive%0A%20%20Learning%20for%20End-to-End%20Multi-Camera%203D%20Tracking%0AAuthor%3A%20Shubo%20Lin%20and%20Yutong%20Kou%20and%20Zirui%20Wu%20and%20Shaoru%20Wang%20and%20Bing%20Li%20and%20Weiming%20Hu%20and%20Jin%20Gao%0AAbstract%3A%20%20%20While%20existing%20query-based%203D%20end-to-end%20visual%20trackers%20integrate%20detection%0Aand%20tracking%20via%20the%20tracking-by-attention%20paradigm%2C%20these%20two%20chicken-and-egg%0Atasks%20encounter%20optimization%20difficulties%20when%20sharing%20the%20same%20parameters.%20Our%0Afindings%20reveal%20that%20these%20difficulties%20arise%20due%20to%20two%20inherent%20constraints%0Aon%20the%20self-attention%20mechanism%2C%20i.e.%2C%20over-deduplication%20for%20object%20queries%0Aand%20self-centric%20attention%20for%20track%20queries.%20In%20contrast%2C%20removing%20the%0Aself-attention%20mechanism%20not%20only%20minimally%20impacts%20regression%20predictions%20of%0Athe%20tracker%2C%20but%20also%20tends%20to%20generate%20more%20latent%20candidate%20boxes.%20Based%20on%0Athese%20analyses%2C%20we%20present%20SynCL%2C%20a%20novel%20plug-and-play%20synergistic%20training%0Astrategy%20designed%20to%20co-facilitate%20multi-task%20learning%20for%20detection%20and%0Atracking.%20Specifically%2C%20we%20propose%20a%20Task-specific%20Hybrid%20Matching%20module%20for%20a%0Aweight-shared%20cross-attention-based%20decoder%20that%20matches%20the%20targets%20of%20track%0Aqueries%20with%20multiple%20object%20queries%20to%20exploit%20promising%20candidates%20overlooked%0Aby%20the%20self-attention%20mechanism.%20To%20flexibly%20select%20optimal%20candidates%20for%20the%0Aone-to-many%20matching%2C%20we%20also%20design%20a%20Dynamic%20Query%20Filtering%20module%0Acontrolled%20by%20model%20training%20status.%20Moreover%2C%20we%20introduce%20Instance-aware%0AContrastive%20Learning%20to%20break%20through%20the%20barrier%20of%20self-centric%20attention%20for%0Atrack%20queries%2C%20effectively%20bridging%20the%20gap%20between%20detection%20and%20tracking.%0AWithout%20additional%20inference%20costs%2C%20SynCL%20consistently%20delivers%20improvements%20in%0Avarious%20benchmarks%20and%20achieves%20state-of-the-art%20performance%20with%20%2458.9%5C%25%24%0AAMOTA%20on%20the%20nuScenes%20dataset.%20Code%20and%20raw%20results%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06780v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynCL%253A%2520A%2520Synergistic%2520Training%2520Strategy%2520with%2520Instance-Aware%2520Contrastive%250A%2520%2520Learning%2520for%2520End-to-End%2520Multi-Camera%25203D%2520Tracking%26entry.906535625%3DShubo%2520Lin%2520and%2520Yutong%2520Kou%2520and%2520Zirui%2520Wu%2520and%2520Shaoru%2520Wang%2520and%2520Bing%2520Li%2520and%2520Weiming%2520Hu%2520and%2520Jin%2520Gao%26entry.1292438233%3D%2520%2520While%2520existing%2520query-based%25203D%2520end-to-end%2520visual%2520trackers%2520integrate%2520detection%250Aand%2520tracking%2520via%2520the%2520tracking-by-attention%2520paradigm%252C%2520these%2520two%2520chicken-and-egg%250Atasks%2520encounter%2520optimization%2520difficulties%2520when%2520sharing%2520the%2520same%2520parameters.%2520Our%250Afindings%2520reveal%2520that%2520these%2520difficulties%2520arise%2520due%2520to%2520two%2520inherent%2520constraints%250Aon%2520the%2520self-attention%2520mechanism%252C%2520i.e.%252C%2520over-deduplication%2520for%2520object%2520queries%250Aand%2520self-centric%2520attention%2520for%2520track%2520queries.%2520In%2520contrast%252C%2520removing%2520the%250Aself-attention%2520mechanism%2520not%2520only%2520minimally%2520impacts%2520regression%2520predictions%2520of%250Athe%2520tracker%252C%2520but%2520also%2520tends%2520to%2520generate%2520more%2520latent%2520candidate%2520boxes.%2520Based%2520on%250Athese%2520analyses%252C%2520we%2520present%2520SynCL%252C%2520a%2520novel%2520plug-and-play%2520synergistic%2520training%250Astrategy%2520designed%2520to%2520co-facilitate%2520multi-task%2520learning%2520for%2520detection%2520and%250Atracking.%2520Specifically%252C%2520we%2520propose%2520a%2520Task-specific%2520Hybrid%2520Matching%2520module%2520for%2520a%250Aweight-shared%2520cross-attention-based%2520decoder%2520that%2520matches%2520the%2520targets%2520of%2520track%250Aqueries%2520with%2520multiple%2520object%2520queries%2520to%2520exploit%2520promising%2520candidates%2520overlooked%250Aby%2520the%2520self-attention%2520mechanism.%2520To%2520flexibly%2520select%2520optimal%2520candidates%2520for%2520the%250Aone-to-many%2520matching%252C%2520we%2520also%2520design%2520a%2520Dynamic%2520Query%2520Filtering%2520module%250Acontrolled%2520by%2520model%2520training%2520status.%2520Moreover%252C%2520we%2520introduce%2520Instance-aware%250AContrastive%2520Learning%2520to%2520break%2520through%2520the%2520barrier%2520of%2520self-centric%2520attention%2520for%250Atrack%2520queries%252C%2520effectively%2520bridging%2520the%2520gap%2520between%2520detection%2520and%2520tracking.%250AWithout%2520additional%2520inference%2520costs%252C%2520SynCL%2520consistently%2520delivers%2520improvements%2520in%250Avarious%2520benchmarks%2520and%2520achieves%2520state-of-the-art%2520performance%2520with%2520%252458.9%255C%2525%2524%250AAMOTA%2520on%2520the%2520nuScenes%2520dataset.%2520Code%2520and%2520raw%2520results%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06780v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynCL%3A%20A%20Synergistic%20Training%20Strategy%20with%20Instance-Aware%20Contrastive%0A%20%20Learning%20for%20End-to-End%20Multi-Camera%203D%20Tracking&entry.906535625=Shubo%20Lin%20and%20Yutong%20Kou%20and%20Zirui%20Wu%20and%20Shaoru%20Wang%20and%20Bing%20Li%20and%20Weiming%20Hu%20and%20Jin%20Gao&entry.1292438233=%20%20While%20existing%20query-based%203D%20end-to-end%20visual%20trackers%20integrate%20detection%0Aand%20tracking%20via%20the%20tracking-by-attention%20paradigm%2C%20these%20two%20chicken-and-egg%0Atasks%20encounter%20optimization%20difficulties%20when%20sharing%20the%20same%20parameters.%20Our%0Afindings%20reveal%20that%20these%20difficulties%20arise%20due%20to%20two%20inherent%20constraints%0Aon%20the%20self-attention%20mechanism%2C%20i.e.%2C%20over-deduplication%20for%20object%20queries%0Aand%20self-centric%20attention%20for%20track%20queries.%20In%20contrast%2C%20removing%20the%0Aself-attention%20mechanism%20not%20only%20minimally%20impacts%20regression%20predictions%20of%0Athe%20tracker%2C%20but%20also%20tends%20to%20generate%20more%20latent%20candidate%20boxes.%20Based%20on%0Athese%20analyses%2C%20we%20present%20SynCL%2C%20a%20novel%20plug-and-play%20synergistic%20training%0Astrategy%20designed%20to%20co-facilitate%20multi-task%20learning%20for%20detection%20and%0Atracking.%20Specifically%2C%20we%20propose%20a%20Task-specific%20Hybrid%20Matching%20module%20for%20a%0Aweight-shared%20cross-attention-based%20decoder%20that%20matches%20the%20targets%20of%20track%0Aqueries%20with%20multiple%20object%20queries%20to%20exploit%20promising%20candidates%20overlooked%0Aby%20the%20self-attention%20mechanism.%20To%20flexibly%20select%20optimal%20candidates%20for%20the%0Aone-to-many%20matching%2C%20we%20also%20design%20a%20Dynamic%20Query%20Filtering%20module%0Acontrolled%20by%20model%20training%20status.%20Moreover%2C%20we%20introduce%20Instance-aware%0AContrastive%20Learning%20to%20break%20through%20the%20barrier%20of%20self-centric%20attention%20for%0Atrack%20queries%2C%20effectively%20bridging%20the%20gap%20between%20detection%20and%20tracking.%0AWithout%20additional%20inference%20costs%2C%20SynCL%20consistently%20delivers%20improvements%20in%0Avarious%20benchmarks%20and%20achieves%20state-of-the-art%20performance%20with%20%2458.9%5C%25%24%0AAMOTA%20on%20the%20nuScenes%20dataset.%20Code%20and%20raw%20results%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06780v3&entry.124074799=Read"},
{"title": "INSIGHT: Enhancing Autonomous Driving Safety through Vision-Language\n  Models on Context-Aware Hazard Detection and Edge Case Evaluation", "author": "Dianwei Chen and Zifan Zhang and Yuchen Liu and Xianfeng Terry Yang", "abstract": "  Autonomous driving systems face significant challenges in handling\nunpredictable edge-case scenarios, such as adversarial pedestrian movements,\ndangerous vehicle maneuvers, and sudden environmental changes. Current\nend-to-end driving models struggle with generalization to these rare events due\nto limitations in traditional detection and prediction approaches. To address\nthis, we propose INSIGHT (Integration of Semantic and Visual Inputs for\nGeneralized Hazard Tracking), a hierarchical vision-language model (VLM)\nframework designed to enhance hazard detection and edge-case evaluation. By\nusing multimodal data fusion, our approach integrates semantic and visual\nrepresentations, enabling precise interpretation of driving scenarios and\naccurate forecasting of potential dangers. Through supervised fine-tuning of\nVLMs, we optimize spatial hazard localization using attention-based mechanisms\nand coordinate regression techniques. Experimental results on the BDD100K\ndataset demonstrate a substantial improvement in hazard prediction\nstraightforwardness and accuracy over existing models, achieving a notable\nincrease in generalization performance. This advancement enhances the\nrobustness and safety of autonomous driving systems, ensuring improved\nsituational awareness and potential decision-making in complex real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2502.00262v3", "date": "2025-05-16", "relevancy": 2.4134, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20INSIGHT%3A%20Enhancing%20Autonomous%20Driving%20Safety%20through%20Vision-Language%0A%20%20Models%20on%20Context-Aware%20Hazard%20Detection%20and%20Edge%20Case%20Evaluation&body=Title%3A%20INSIGHT%3A%20Enhancing%20Autonomous%20Driving%20Safety%20through%20Vision-Language%0A%20%20Models%20on%20Context-Aware%20Hazard%20Detection%20and%20Edge%20Case%20Evaluation%0AAuthor%3A%20Dianwei%20Chen%20and%20Zifan%20Zhang%20and%20Yuchen%20Liu%20and%20Xianfeng%20Terry%20Yang%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20face%20significant%20challenges%20in%20handling%0Aunpredictable%20edge-case%20scenarios%2C%20such%20as%20adversarial%20pedestrian%20movements%2C%0Adangerous%20vehicle%20maneuvers%2C%20and%20sudden%20environmental%20changes.%20Current%0Aend-to-end%20driving%20models%20struggle%20with%20generalization%20to%20these%20rare%20events%20due%0Ato%20limitations%20in%20traditional%20detection%20and%20prediction%20approaches.%20To%20address%0Athis%2C%20we%20propose%20INSIGHT%20%28Integration%20of%20Semantic%20and%20Visual%20Inputs%20for%0AGeneralized%20Hazard%20Tracking%29%2C%20a%20hierarchical%20vision-language%20model%20%28VLM%29%0Aframework%20designed%20to%20enhance%20hazard%20detection%20and%20edge-case%20evaluation.%20By%0Ausing%20multimodal%20data%20fusion%2C%20our%20approach%20integrates%20semantic%20and%20visual%0Arepresentations%2C%20enabling%20precise%20interpretation%20of%20driving%20scenarios%20and%0Aaccurate%20forecasting%20of%20potential%20dangers.%20Through%20supervised%20fine-tuning%20of%0AVLMs%2C%20we%20optimize%20spatial%20hazard%20localization%20using%20attention-based%20mechanisms%0Aand%20coordinate%20regression%20techniques.%20Experimental%20results%20on%20the%20BDD100K%0Adataset%20demonstrate%20a%20substantial%20improvement%20in%20hazard%20prediction%0Astraightforwardness%20and%20accuracy%20over%20existing%20models%2C%20achieving%20a%20notable%0Aincrease%20in%20generalization%20performance.%20This%20advancement%20enhances%20the%0Arobustness%20and%20safety%20of%20autonomous%20driving%20systems%2C%20ensuring%20improved%0Asituational%20awareness%20and%20potential%20decision-making%20in%20complex%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00262v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DINSIGHT%253A%2520Enhancing%2520Autonomous%2520Driving%2520Safety%2520through%2520Vision-Language%250A%2520%2520Models%2520on%2520Context-Aware%2520Hazard%2520Detection%2520and%2520Edge%2520Case%2520Evaluation%26entry.906535625%3DDianwei%2520Chen%2520and%2520Zifan%2520Zhang%2520and%2520Yuchen%2520Liu%2520and%2520Xianfeng%2520Terry%2520Yang%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520systems%2520face%2520significant%2520challenges%2520in%2520handling%250Aunpredictable%2520edge-case%2520scenarios%252C%2520such%2520as%2520adversarial%2520pedestrian%2520movements%252C%250Adangerous%2520vehicle%2520maneuvers%252C%2520and%2520sudden%2520environmental%2520changes.%2520Current%250Aend-to-end%2520driving%2520models%2520struggle%2520with%2520generalization%2520to%2520these%2520rare%2520events%2520due%250Ato%2520limitations%2520in%2520traditional%2520detection%2520and%2520prediction%2520approaches.%2520To%2520address%250Athis%252C%2520we%2520propose%2520INSIGHT%2520%2528Integration%2520of%2520Semantic%2520and%2520Visual%2520Inputs%2520for%250AGeneralized%2520Hazard%2520Tracking%2529%252C%2520a%2520hierarchical%2520vision-language%2520model%2520%2528VLM%2529%250Aframework%2520designed%2520to%2520enhance%2520hazard%2520detection%2520and%2520edge-case%2520evaluation.%2520By%250Ausing%2520multimodal%2520data%2520fusion%252C%2520our%2520approach%2520integrates%2520semantic%2520and%2520visual%250Arepresentations%252C%2520enabling%2520precise%2520interpretation%2520of%2520driving%2520scenarios%2520and%250Aaccurate%2520forecasting%2520of%2520potential%2520dangers.%2520Through%2520supervised%2520fine-tuning%2520of%250AVLMs%252C%2520we%2520optimize%2520spatial%2520hazard%2520localization%2520using%2520attention-based%2520mechanisms%250Aand%2520coordinate%2520regression%2520techniques.%2520Experimental%2520results%2520on%2520the%2520BDD100K%250Adataset%2520demonstrate%2520a%2520substantial%2520improvement%2520in%2520hazard%2520prediction%250Astraightforwardness%2520and%2520accuracy%2520over%2520existing%2520models%252C%2520achieving%2520a%2520notable%250Aincrease%2520in%2520generalization%2520performance.%2520This%2520advancement%2520enhances%2520the%250Arobustness%2520and%2520safety%2520of%2520autonomous%2520driving%2520systems%252C%2520ensuring%2520improved%250Asituational%2520awareness%2520and%2520potential%2520decision-making%2520in%2520complex%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00262v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=INSIGHT%3A%20Enhancing%20Autonomous%20Driving%20Safety%20through%20Vision-Language%0A%20%20Models%20on%20Context-Aware%20Hazard%20Detection%20and%20Edge%20Case%20Evaluation&entry.906535625=Dianwei%20Chen%20and%20Zifan%20Zhang%20and%20Yuchen%20Liu%20and%20Xianfeng%20Terry%20Yang&entry.1292438233=%20%20Autonomous%20driving%20systems%20face%20significant%20challenges%20in%20handling%0Aunpredictable%20edge-case%20scenarios%2C%20such%20as%20adversarial%20pedestrian%20movements%2C%0Adangerous%20vehicle%20maneuvers%2C%20and%20sudden%20environmental%20changes.%20Current%0Aend-to-end%20driving%20models%20struggle%20with%20generalization%20to%20these%20rare%20events%20due%0Ato%20limitations%20in%20traditional%20detection%20and%20prediction%20approaches.%20To%20address%0Athis%2C%20we%20propose%20INSIGHT%20%28Integration%20of%20Semantic%20and%20Visual%20Inputs%20for%0AGeneralized%20Hazard%20Tracking%29%2C%20a%20hierarchical%20vision-language%20model%20%28VLM%29%0Aframework%20designed%20to%20enhance%20hazard%20detection%20and%20edge-case%20evaluation.%20By%0Ausing%20multimodal%20data%20fusion%2C%20our%20approach%20integrates%20semantic%20and%20visual%0Arepresentations%2C%20enabling%20precise%20interpretation%20of%20driving%20scenarios%20and%0Aaccurate%20forecasting%20of%20potential%20dangers.%20Through%20supervised%20fine-tuning%20of%0AVLMs%2C%20we%20optimize%20spatial%20hazard%20localization%20using%20attention-based%20mechanisms%0Aand%20coordinate%20regression%20techniques.%20Experimental%20results%20on%20the%20BDD100K%0Adataset%20demonstrate%20a%20substantial%20improvement%20in%20hazard%20prediction%0Astraightforwardness%20and%20accuracy%20over%20existing%20models%2C%20achieving%20a%20notable%0Aincrease%20in%20generalization%20performance.%20This%20advancement%20enhances%20the%0Arobustness%20and%20safety%20of%20autonomous%20driving%20systems%2C%20ensuring%20improved%0Asituational%20awareness%20and%20potential%20decision-making%20in%20complex%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00262v3&entry.124074799=Read"},
{"title": "CompAlign: Improving Compositional Text-to-Image Generation with a\n  Complex Benchmark and Fine-Grained Feedback", "author": "Yixin Wan and Kai-Wei Chang", "abstract": "  State-of-the-art T2I models are capable of generating high-resolution images\ngiven textual prompts. However, they still struggle with accurately depicting\ncompositional scenes that specify multiple objects, attributes, and spatial\nrelations. We present CompAlign, a challenging benchmark with an emphasis on\nassessing the depiction of 3D-spatial relationships, for evaluating and\nimproving models on compositional image generation. CompAlign consists of 900\ncomplex multi-subject image generation prompts that combine numerical and\n3D-spatial relationships with varied attribute bindings. Our benchmark is\nremarkably challenging, incorporating generation tasks with 3+ generation\nsubjects with complex 3D-spatial relationships. Additionally, we propose\nCompQuest, an interpretable and accurate evaluation framework that decomposes\ncomplex prompts into atomic sub-questions, then utilizes a MLLM to provide\nfine-grained binary feedback on the correctness of each aspect of generation\nelements in model-generated images. This enables precise quantification of\nalignment between generated images and compositional prompts. Furthermore, we\npropose an alignment framework that uses CompQuest's feedback as preference\nsignals to improve diffusion models' compositional image generation abilities.\nUsing adjustable per-image preferences, our method is easily scalable and\nflexible for different tasks. Evaluation of 9 T2I models reveals that: (1)\nmodels remarkable struggle more with compositional tasks with more complex\n3D-spatial configurations, and (2) a noticeable performance gap exists between\nopen-source accessible models and closed-source commercial models. Further\nempirical study on using CompAlign for model alignment yield promising results:\npost-alignment diffusion models achieve remarkable improvements in\ncompositional accuracy, especially on complex generation tasks, outperforming\nprevious approaches.\n", "link": "http://arxiv.org/abs/2505.11178v1", "date": "2025-05-16", "relevancy": 2.4109, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5887}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompAlign%3A%20Improving%20Compositional%20Text-to-Image%20Generation%20with%20a%0A%20%20Complex%20Benchmark%20and%20Fine-Grained%20Feedback&body=Title%3A%20CompAlign%3A%20Improving%20Compositional%20Text-to-Image%20Generation%20with%20a%0A%20%20Complex%20Benchmark%20and%20Fine-Grained%20Feedback%0AAuthor%3A%20Yixin%20Wan%20and%20Kai-Wei%20Chang%0AAbstract%3A%20%20%20State-of-the-art%20T2I%20models%20are%20capable%20of%20generating%20high-resolution%20images%0Agiven%20textual%20prompts.%20However%2C%20they%20still%20struggle%20with%20accurately%20depicting%0Acompositional%20scenes%20that%20specify%20multiple%20objects%2C%20attributes%2C%20and%20spatial%0Arelations.%20We%20present%20CompAlign%2C%20a%20challenging%20benchmark%20with%20an%20emphasis%20on%0Aassessing%20the%20depiction%20of%203D-spatial%20relationships%2C%20for%20evaluating%20and%0Aimproving%20models%20on%20compositional%20image%20generation.%20CompAlign%20consists%20of%20900%0Acomplex%20multi-subject%20image%20generation%20prompts%20that%20combine%20numerical%20and%0A3D-spatial%20relationships%20with%20varied%20attribute%20bindings.%20Our%20benchmark%20is%0Aremarkably%20challenging%2C%20incorporating%20generation%20tasks%20with%203%2B%20generation%0Asubjects%20with%20complex%203D-spatial%20relationships.%20Additionally%2C%20we%20propose%0ACompQuest%2C%20an%20interpretable%20and%20accurate%20evaluation%20framework%20that%20decomposes%0Acomplex%20prompts%20into%20atomic%20sub-questions%2C%20then%20utilizes%20a%20MLLM%20to%20provide%0Afine-grained%20binary%20feedback%20on%20the%20correctness%20of%20each%20aspect%20of%20generation%0Aelements%20in%20model-generated%20images.%20This%20enables%20precise%20quantification%20of%0Aalignment%20between%20generated%20images%20and%20compositional%20prompts.%20Furthermore%2C%20we%0Apropose%20an%20alignment%20framework%20that%20uses%20CompQuest%27s%20feedback%20as%20preference%0Asignals%20to%20improve%20diffusion%20models%27%20compositional%20image%20generation%20abilities.%0AUsing%20adjustable%20per-image%20preferences%2C%20our%20method%20is%20easily%20scalable%20and%0Aflexible%20for%20different%20tasks.%20Evaluation%20of%209%20T2I%20models%20reveals%20that%3A%20%281%29%0Amodels%20remarkable%20struggle%20more%20with%20compositional%20tasks%20with%20more%20complex%0A3D-spatial%20configurations%2C%20and%20%282%29%20a%20noticeable%20performance%20gap%20exists%20between%0Aopen-source%20accessible%20models%20and%20closed-source%20commercial%20models.%20Further%0Aempirical%20study%20on%20using%20CompAlign%20for%20model%20alignment%20yield%20promising%20results%3A%0Apost-alignment%20diffusion%20models%20achieve%20remarkable%20improvements%20in%0Acompositional%20accuracy%2C%20especially%20on%20complex%20generation%20tasks%2C%20outperforming%0Aprevious%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompAlign%253A%2520Improving%2520Compositional%2520Text-to-Image%2520Generation%2520with%2520a%250A%2520%2520Complex%2520Benchmark%2520and%2520Fine-Grained%2520Feedback%26entry.906535625%3DYixin%2520Wan%2520and%2520Kai-Wei%2520Chang%26entry.1292438233%3D%2520%2520State-of-the-art%2520T2I%2520models%2520are%2520capable%2520of%2520generating%2520high-resolution%2520images%250Agiven%2520textual%2520prompts.%2520However%252C%2520they%2520still%2520struggle%2520with%2520accurately%2520depicting%250Acompositional%2520scenes%2520that%2520specify%2520multiple%2520objects%252C%2520attributes%252C%2520and%2520spatial%250Arelations.%2520We%2520present%2520CompAlign%252C%2520a%2520challenging%2520benchmark%2520with%2520an%2520emphasis%2520on%250Aassessing%2520the%2520depiction%2520of%25203D-spatial%2520relationships%252C%2520for%2520evaluating%2520and%250Aimproving%2520models%2520on%2520compositional%2520image%2520generation.%2520CompAlign%2520consists%2520of%2520900%250Acomplex%2520multi-subject%2520image%2520generation%2520prompts%2520that%2520combine%2520numerical%2520and%250A3D-spatial%2520relationships%2520with%2520varied%2520attribute%2520bindings.%2520Our%2520benchmark%2520is%250Aremarkably%2520challenging%252C%2520incorporating%2520generation%2520tasks%2520with%25203%252B%2520generation%250Asubjects%2520with%2520complex%25203D-spatial%2520relationships.%2520Additionally%252C%2520we%2520propose%250ACompQuest%252C%2520an%2520interpretable%2520and%2520accurate%2520evaluation%2520framework%2520that%2520decomposes%250Acomplex%2520prompts%2520into%2520atomic%2520sub-questions%252C%2520then%2520utilizes%2520a%2520MLLM%2520to%2520provide%250Afine-grained%2520binary%2520feedback%2520on%2520the%2520correctness%2520of%2520each%2520aspect%2520of%2520generation%250Aelements%2520in%2520model-generated%2520images.%2520This%2520enables%2520precise%2520quantification%2520of%250Aalignment%2520between%2520generated%2520images%2520and%2520compositional%2520prompts.%2520Furthermore%252C%2520we%250Apropose%2520an%2520alignment%2520framework%2520that%2520uses%2520CompQuest%2527s%2520feedback%2520as%2520preference%250Asignals%2520to%2520improve%2520diffusion%2520models%2527%2520compositional%2520image%2520generation%2520abilities.%250AUsing%2520adjustable%2520per-image%2520preferences%252C%2520our%2520method%2520is%2520easily%2520scalable%2520and%250Aflexible%2520for%2520different%2520tasks.%2520Evaluation%2520of%25209%2520T2I%2520models%2520reveals%2520that%253A%2520%25281%2529%250Amodels%2520remarkable%2520struggle%2520more%2520with%2520compositional%2520tasks%2520with%2520more%2520complex%250A3D-spatial%2520configurations%252C%2520and%2520%25282%2529%2520a%2520noticeable%2520performance%2520gap%2520exists%2520between%250Aopen-source%2520accessible%2520models%2520and%2520closed-source%2520commercial%2520models.%2520Further%250Aempirical%2520study%2520on%2520using%2520CompAlign%2520for%2520model%2520alignment%2520yield%2520promising%2520results%253A%250Apost-alignment%2520diffusion%2520models%2520achieve%2520remarkable%2520improvements%2520in%250Acompositional%2520accuracy%252C%2520especially%2520on%2520complex%2520generation%2520tasks%252C%2520outperforming%250Aprevious%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompAlign%3A%20Improving%20Compositional%20Text-to-Image%20Generation%20with%20a%0A%20%20Complex%20Benchmark%20and%20Fine-Grained%20Feedback&entry.906535625=Yixin%20Wan%20and%20Kai-Wei%20Chang&entry.1292438233=%20%20State-of-the-art%20T2I%20models%20are%20capable%20of%20generating%20high-resolution%20images%0Agiven%20textual%20prompts.%20However%2C%20they%20still%20struggle%20with%20accurately%20depicting%0Acompositional%20scenes%20that%20specify%20multiple%20objects%2C%20attributes%2C%20and%20spatial%0Arelations.%20We%20present%20CompAlign%2C%20a%20challenging%20benchmark%20with%20an%20emphasis%20on%0Aassessing%20the%20depiction%20of%203D-spatial%20relationships%2C%20for%20evaluating%20and%0Aimproving%20models%20on%20compositional%20image%20generation.%20CompAlign%20consists%20of%20900%0Acomplex%20multi-subject%20image%20generation%20prompts%20that%20combine%20numerical%20and%0A3D-spatial%20relationships%20with%20varied%20attribute%20bindings.%20Our%20benchmark%20is%0Aremarkably%20challenging%2C%20incorporating%20generation%20tasks%20with%203%2B%20generation%0Asubjects%20with%20complex%203D-spatial%20relationships.%20Additionally%2C%20we%20propose%0ACompQuest%2C%20an%20interpretable%20and%20accurate%20evaluation%20framework%20that%20decomposes%0Acomplex%20prompts%20into%20atomic%20sub-questions%2C%20then%20utilizes%20a%20MLLM%20to%20provide%0Afine-grained%20binary%20feedback%20on%20the%20correctness%20of%20each%20aspect%20of%20generation%0Aelements%20in%20model-generated%20images.%20This%20enables%20precise%20quantification%20of%0Aalignment%20between%20generated%20images%20and%20compositional%20prompts.%20Furthermore%2C%20we%0Apropose%20an%20alignment%20framework%20that%20uses%20CompQuest%27s%20feedback%20as%20preference%0Asignals%20to%20improve%20diffusion%20models%27%20compositional%20image%20generation%20abilities.%0AUsing%20adjustable%20per-image%20preferences%2C%20our%20method%20is%20easily%20scalable%20and%0Aflexible%20for%20different%20tasks.%20Evaluation%20of%209%20T2I%20models%20reveals%20that%3A%20%281%29%0Amodels%20remarkable%20struggle%20more%20with%20compositional%20tasks%20with%20more%20complex%0A3D-spatial%20configurations%2C%20and%20%282%29%20a%20noticeable%20performance%20gap%20exists%20between%0Aopen-source%20accessible%20models%20and%20closed-source%20commercial%20models.%20Further%0Aempirical%20study%20on%20using%20CompAlign%20for%20model%20alignment%20yield%20promising%20results%3A%0Apost-alignment%20diffusion%20models%20achieve%20remarkable%20improvements%20in%0Acompositional%20accuracy%2C%20especially%20on%20complex%20generation%20tasks%2C%20outperforming%0Aprevious%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11178v1&entry.124074799=Read"},
{"title": "Training NTK to Generalize with KARE", "author": "Johannes Schwab and Bryan Kelly and Semyon Malamud and Teng Andrea Xu", "abstract": "  The performance of the data-dependent neural tangent kernel (NTK; Jacot et\nal. (2018)) associated with a trained deep neural network (DNN) often matches\nor exceeds that of the full network. This implies that DNN training via\ngradient descent implicitly performs kernel learning by optimizing the NTK. In\nthis paper, we propose instead to optimize the NTK explicitly. Rather than\nminimizing empirical risk, we train the NTK to minimize its generalization\nerror using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot\net al. (2020)). Our simulations and real data experiments show that NTKs\ntrained with KARE consistently match or significantly outperform the original\nDNN and the DNN- induced NTK (the after-kernel). These results suggest that\nexplicitly trained kernels can outperform traditional end-to-end DNN\noptimization in certain settings, challenging the conventional dominance of\nDNNs. We argue that explicit training of NTK is a form of over-parametrized\nfeature learning.\n", "link": "http://arxiv.org/abs/2505.11347v1", "date": "2025-05-16", "relevancy": 2.4009, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5201}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4779}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20NTK%20to%20Generalize%20with%20KARE&body=Title%3A%20Training%20NTK%20to%20Generalize%20with%20KARE%0AAuthor%3A%20Johannes%20Schwab%20and%20Bryan%20Kelly%20and%20Semyon%20Malamud%20and%20Teng%20Andrea%20Xu%0AAbstract%3A%20%20%20The%20performance%20of%20the%20data-dependent%20neural%20tangent%20kernel%20%28NTK%3B%20Jacot%20et%0Aal.%20%282018%29%29%20associated%20with%20a%20trained%20deep%20neural%20network%20%28DNN%29%20often%20matches%0Aor%20exceeds%20that%20of%20the%20full%20network.%20This%20implies%20that%20DNN%20training%20via%0Agradient%20descent%20implicitly%20performs%20kernel%20learning%20by%20optimizing%20the%20NTK.%20In%0Athis%20paper%2C%20we%20propose%20instead%20to%20optimize%20the%20NTK%20explicitly.%20Rather%20than%0Aminimizing%20empirical%20risk%2C%20we%20train%20the%20NTK%20to%20minimize%20its%20generalization%0Aerror%20using%20the%20recently%20developed%20Kernel%20Alignment%20Risk%20Estimator%20%28KARE%3B%20Jacot%0Aet%20al.%20%282020%29%29.%20Our%20simulations%20and%20real%20data%20experiments%20show%20that%20NTKs%0Atrained%20with%20KARE%20consistently%20match%20or%20significantly%20outperform%20the%20original%0ADNN%20and%20the%20DNN-%20induced%20NTK%20%28the%20after-kernel%29.%20These%20results%20suggest%20that%0Aexplicitly%20trained%20kernels%20can%20outperform%20traditional%20end-to-end%20DNN%0Aoptimization%20in%20certain%20settings%2C%20challenging%20the%20conventional%20dominance%20of%0ADNNs.%20We%20argue%20that%20explicit%20training%20of%20NTK%20is%20a%20form%20of%20over-parametrized%0Afeature%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520NTK%2520to%2520Generalize%2520with%2520KARE%26entry.906535625%3DJohannes%2520Schwab%2520and%2520Bryan%2520Kelly%2520and%2520Semyon%2520Malamud%2520and%2520Teng%2520Andrea%2520Xu%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520the%2520data-dependent%2520neural%2520tangent%2520kernel%2520%2528NTK%253B%2520Jacot%2520et%250Aal.%2520%25282018%2529%2529%2520associated%2520with%2520a%2520trained%2520deep%2520neural%2520network%2520%2528DNN%2529%2520often%2520matches%250Aor%2520exceeds%2520that%2520of%2520the%2520full%2520network.%2520This%2520implies%2520that%2520DNN%2520training%2520via%250Agradient%2520descent%2520implicitly%2520performs%2520kernel%2520learning%2520by%2520optimizing%2520the%2520NTK.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520instead%2520to%2520optimize%2520the%2520NTK%2520explicitly.%2520Rather%2520than%250Aminimizing%2520empirical%2520risk%252C%2520we%2520train%2520the%2520NTK%2520to%2520minimize%2520its%2520generalization%250Aerror%2520using%2520the%2520recently%2520developed%2520Kernel%2520Alignment%2520Risk%2520Estimator%2520%2528KARE%253B%2520Jacot%250Aet%2520al.%2520%25282020%2529%2529.%2520Our%2520simulations%2520and%2520real%2520data%2520experiments%2520show%2520that%2520NTKs%250Atrained%2520with%2520KARE%2520consistently%2520match%2520or%2520significantly%2520outperform%2520the%2520original%250ADNN%2520and%2520the%2520DNN-%2520induced%2520NTK%2520%2528the%2520after-kernel%2529.%2520These%2520results%2520suggest%2520that%250Aexplicitly%2520trained%2520kernels%2520can%2520outperform%2520traditional%2520end-to-end%2520DNN%250Aoptimization%2520in%2520certain%2520settings%252C%2520challenging%2520the%2520conventional%2520dominance%2520of%250ADNNs.%2520We%2520argue%2520that%2520explicit%2520training%2520of%2520NTK%2520is%2520a%2520form%2520of%2520over-parametrized%250Afeature%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20NTK%20to%20Generalize%20with%20KARE&entry.906535625=Johannes%20Schwab%20and%20Bryan%20Kelly%20and%20Semyon%20Malamud%20and%20Teng%20Andrea%20Xu&entry.1292438233=%20%20The%20performance%20of%20the%20data-dependent%20neural%20tangent%20kernel%20%28NTK%3B%20Jacot%20et%0Aal.%20%282018%29%29%20associated%20with%20a%20trained%20deep%20neural%20network%20%28DNN%29%20often%20matches%0Aor%20exceeds%20that%20of%20the%20full%20network.%20This%20implies%20that%20DNN%20training%20via%0Agradient%20descent%20implicitly%20performs%20kernel%20learning%20by%20optimizing%20the%20NTK.%20In%0Athis%20paper%2C%20we%20propose%20instead%20to%20optimize%20the%20NTK%20explicitly.%20Rather%20than%0Aminimizing%20empirical%20risk%2C%20we%20train%20the%20NTK%20to%20minimize%20its%20generalization%0Aerror%20using%20the%20recently%20developed%20Kernel%20Alignment%20Risk%20Estimator%20%28KARE%3B%20Jacot%0Aet%20al.%20%282020%29%29.%20Our%20simulations%20and%20real%20data%20experiments%20show%20that%20NTKs%0Atrained%20with%20KARE%20consistently%20match%20or%20significantly%20outperform%20the%20original%0ADNN%20and%20the%20DNN-%20induced%20NTK%20%28the%20after-kernel%29.%20These%20results%20suggest%20that%0Aexplicitly%20trained%20kernels%20can%20outperform%20traditional%20end-to-end%20DNN%0Aoptimization%20in%20certain%20settings%2C%20challenging%20the%20conventional%20dominance%20of%0ADNNs.%20We%20argue%20that%20explicit%20training%20of%20NTK%20is%20a%20form%20of%20over-parametrized%0Afeature%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11347v1&entry.124074799=Read"},
{"title": "Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent\n  Federated Learning", "author": "Shudi Weng and Chao Ren and Ming Xiao and Mikael Skoglund", "abstract": "  Federated learning (FL) commonly involves clients with diverse communication\nand computational capabilities. Such heterogeneity can significantly distort\nthe optimization dynamics and lead to objective inconsistency, where the global\nmodel converges to an incorrect stationary point potentially far from the\npursued optimum. Despite its critical impact, the joint effect of communication\nand computation heterogeneity has remained largely unexplored, due to the\nintrinsic complexity of their interaction. In this paper, we reveal the\nfundamentally distinct mechanisms through which heterogeneous communication and\ncomputation drive inconsistency in FL. To the best of our knowledge, this is\nthe first unified theoretical analysis of general heterogeneous FL, offering a\nprincipled understanding of how these two forms of heterogeneity jointly\ndistort the optimization trajectory under arbitrary choices of local solvers.\nMotivated by these insights, we propose Federated Heterogeneity-Aware Client\nSampling, FedACS, a universal method to eliminate all types of objective\ninconsistency. We theoretically prove that FedACS converges to the correct\noptimum at a rate of $O(1/\\sqrt{R})$, even in dynamic heterogeneous\nenvironments. Extensive experiments across multiple datasets show that FedACS\noutperforms state-of-the-art and category-specific baselines by 4.3%-36%, while\nreducing communication costs by 22%-89% and computation loads by 14%-105%,\nrespectively.\n", "link": "http://arxiv.org/abs/2505.11304v1", "date": "2025-05-16", "relevancy": 2.3986, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4926}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4856}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneity-Aware%20Client%20Sampling%3A%20A%20Unified%20Solution%20for%20Consistent%0A%20%20Federated%20Learning&body=Title%3A%20Heterogeneity-Aware%20Client%20Sampling%3A%20A%20Unified%20Solution%20for%20Consistent%0A%20%20Federated%20Learning%0AAuthor%3A%20Shudi%20Weng%20and%20Chao%20Ren%20and%20Ming%20Xiao%20and%20Mikael%20Skoglund%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20commonly%20involves%20clients%20with%20diverse%20communication%0Aand%20computational%20capabilities.%20Such%20heterogeneity%20can%20significantly%20distort%0Athe%20optimization%20dynamics%20and%20lead%20to%20objective%20inconsistency%2C%20where%20the%20global%0Amodel%20converges%20to%20an%20incorrect%20stationary%20point%20potentially%20far%20from%20the%0Apursued%20optimum.%20Despite%20its%20critical%20impact%2C%20the%20joint%20effect%20of%20communication%0Aand%20computation%20heterogeneity%20has%20remained%20largely%20unexplored%2C%20due%20to%20the%0Aintrinsic%20complexity%20of%20their%20interaction.%20In%20this%20paper%2C%20we%20reveal%20the%0Afundamentally%20distinct%20mechanisms%20through%20which%20heterogeneous%20communication%20and%0Acomputation%20drive%20inconsistency%20in%20FL.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20unified%20theoretical%20analysis%20of%20general%20heterogeneous%20FL%2C%20offering%20a%0Aprincipled%20understanding%20of%20how%20these%20two%20forms%20of%20heterogeneity%20jointly%0Adistort%20the%20optimization%20trajectory%20under%20arbitrary%20choices%20of%20local%20solvers.%0AMotivated%20by%20these%20insights%2C%20we%20propose%20Federated%20Heterogeneity-Aware%20Client%0ASampling%2C%20FedACS%2C%20a%20universal%20method%20to%20eliminate%20all%20types%20of%20objective%0Ainconsistency.%20We%20theoretically%20prove%20that%20FedACS%20converges%20to%20the%20correct%0Aoptimum%20at%20a%20rate%20of%20%24O%281/%5Csqrt%7BR%7D%29%24%2C%20even%20in%20dynamic%20heterogeneous%0Aenvironments.%20Extensive%20experiments%20across%20multiple%20datasets%20show%20that%20FedACS%0Aoutperforms%20state-of-the-art%20and%20category-specific%20baselines%20by%204.3%25-36%25%2C%20while%0Areducing%20communication%20costs%20by%2022%25-89%25%20and%20computation%20loads%20by%2014%25-105%25%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneity-Aware%2520Client%2520Sampling%253A%2520A%2520Unified%2520Solution%2520for%2520Consistent%250A%2520%2520Federated%2520Learning%26entry.906535625%3DShudi%2520Weng%2520and%2520Chao%2520Ren%2520and%2520Ming%2520Xiao%2520and%2520Mikael%2520Skoglund%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520commonly%2520involves%2520clients%2520with%2520diverse%2520communication%250Aand%2520computational%2520capabilities.%2520Such%2520heterogeneity%2520can%2520significantly%2520distort%250Athe%2520optimization%2520dynamics%2520and%2520lead%2520to%2520objective%2520inconsistency%252C%2520where%2520the%2520global%250Amodel%2520converges%2520to%2520an%2520incorrect%2520stationary%2520point%2520potentially%2520far%2520from%2520the%250Apursued%2520optimum.%2520Despite%2520its%2520critical%2520impact%252C%2520the%2520joint%2520effect%2520of%2520communication%250Aand%2520computation%2520heterogeneity%2520has%2520remained%2520largely%2520unexplored%252C%2520due%2520to%2520the%250Aintrinsic%2520complexity%2520of%2520their%2520interaction.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520the%250Afundamentally%2520distinct%2520mechanisms%2520through%2520which%2520heterogeneous%2520communication%2520and%250Acomputation%2520drive%2520inconsistency%2520in%2520FL.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520unified%2520theoretical%2520analysis%2520of%2520general%2520heterogeneous%2520FL%252C%2520offering%2520a%250Aprincipled%2520understanding%2520of%2520how%2520these%2520two%2520forms%2520of%2520heterogeneity%2520jointly%250Adistort%2520the%2520optimization%2520trajectory%2520under%2520arbitrary%2520choices%2520of%2520local%2520solvers.%250AMotivated%2520by%2520these%2520insights%252C%2520we%2520propose%2520Federated%2520Heterogeneity-Aware%2520Client%250ASampling%252C%2520FedACS%252C%2520a%2520universal%2520method%2520to%2520eliminate%2520all%2520types%2520of%2520objective%250Ainconsistency.%2520We%2520theoretically%2520prove%2520that%2520FedACS%2520converges%2520to%2520the%2520correct%250Aoptimum%2520at%2520a%2520rate%2520of%2520%2524O%25281/%255Csqrt%257BR%257D%2529%2524%252C%2520even%2520in%2520dynamic%2520heterogeneous%250Aenvironments.%2520Extensive%2520experiments%2520across%2520multiple%2520datasets%2520show%2520that%2520FedACS%250Aoutperforms%2520state-of-the-art%2520and%2520category-specific%2520baselines%2520by%25204.3%2525-36%2525%252C%2520while%250Areducing%2520communication%2520costs%2520by%252022%2525-89%2525%2520and%2520computation%2520loads%2520by%252014%2525-105%2525%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneity-Aware%20Client%20Sampling%3A%20A%20Unified%20Solution%20for%20Consistent%0A%20%20Federated%20Learning&entry.906535625=Shudi%20Weng%20and%20Chao%20Ren%20and%20Ming%20Xiao%20and%20Mikael%20Skoglund&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20commonly%20involves%20clients%20with%20diverse%20communication%0Aand%20computational%20capabilities.%20Such%20heterogeneity%20can%20significantly%20distort%0Athe%20optimization%20dynamics%20and%20lead%20to%20objective%20inconsistency%2C%20where%20the%20global%0Amodel%20converges%20to%20an%20incorrect%20stationary%20point%20potentially%20far%20from%20the%0Apursued%20optimum.%20Despite%20its%20critical%20impact%2C%20the%20joint%20effect%20of%20communication%0Aand%20computation%20heterogeneity%20has%20remained%20largely%20unexplored%2C%20due%20to%20the%0Aintrinsic%20complexity%20of%20their%20interaction.%20In%20this%20paper%2C%20we%20reveal%20the%0Afundamentally%20distinct%20mechanisms%20through%20which%20heterogeneous%20communication%20and%0Acomputation%20drive%20inconsistency%20in%20FL.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20unified%20theoretical%20analysis%20of%20general%20heterogeneous%20FL%2C%20offering%20a%0Aprincipled%20understanding%20of%20how%20these%20two%20forms%20of%20heterogeneity%20jointly%0Adistort%20the%20optimization%20trajectory%20under%20arbitrary%20choices%20of%20local%20solvers.%0AMotivated%20by%20these%20insights%2C%20we%20propose%20Federated%20Heterogeneity-Aware%20Client%0ASampling%2C%20FedACS%2C%20a%20universal%20method%20to%20eliminate%20all%20types%20of%20objective%0Ainconsistency.%20We%20theoretically%20prove%20that%20FedACS%20converges%20to%20the%20correct%0Aoptimum%20at%20a%20rate%20of%20%24O%281/%5Csqrt%7BR%7D%29%24%2C%20even%20in%20dynamic%20heterogeneous%0Aenvironments.%20Extensive%20experiments%20across%20multiple%20datasets%20show%20that%20FedACS%0Aoutperforms%20state-of-the-art%20and%20category-specific%20baselines%20by%204.3%25-36%25%2C%20while%0Areducing%20communication%20costs%20by%2022%25-89%25%20and%20computation%20loads%20by%2014%25-105%25%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11304v1&entry.124074799=Read"},
{"title": "V-MAGE: A Game Evaluation Framework for Assessing Vision-Centric\n  Capabilities in Multimodal Large Language Models", "author": "Xiangxi Zheng and Linjie Li and Zhengyuan Yang and Ping Yu and Alex Jinpeng Wang and Rui Yan and Yuan Yao and Lijuan Wang", "abstract": "  Recent advancements in Multimodal Large Language Models (MLLMs) have\ndemonstrated impressive capabilities in visual-text processing. However,\nexisting static image-text benchmarks are insufficient for evaluating their\ndynamic perception and interactive reasoning abilities. We introduce\nVision-centric Multiple Abilities Game Evaluation(V-MAGE), a novel game-based\nevaluation framework designed to systematically assess MLLMs' visual reasoning\nin interactive, continuous-space environments. V-MAGE features five distinct\nvideo games comprising over 30 carefully constructed evaluation scenarios.\nThese scenarios are set in free-form, visually complex environments that\nrequire models to interpret dynamic game states and make decisions based solely\non visual input, thereby closely reflecting the conditions encountered by human\nplayers. To ensure robust and interpretable comparisons across models, V-MAGE\nemploys a dynamic Elo-based ranking system that accounts for varying difficulty\nlevels and task diversity. Benchmarking state-of-the-art MLLMs against human\nbaselines reveals that while leading models approach human-level performance in\nsimple tasks, their performance drops significantly in complex scenarios\nrequiring advanced reasoning and task orchestration. This persistent\nperformance gap highlights fundamental limitations in current MLLMs' ability to\nperform real-time, vision-grounded interactions. Through extensive analyses, we\ndemonstrate the utility of V-MAGE in uncovering these limitations and providing\nactionable insights for improving the visual and reasoning capabilities of\nMLLMs in dynamic, interactive settings. Code is publicly available at\nhttps://github.com/CSU-JPG/V-MAGE.\n", "link": "http://arxiv.org/abs/2504.06148v2", "date": "2025-05-16", "relevancy": 2.3954, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6074}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-MAGE%3A%20A%20Game%20Evaluation%20Framework%20for%20Assessing%20Vision-Centric%0A%20%20Capabilities%20in%20Multimodal%20Large%20Language%20Models&body=Title%3A%20V-MAGE%3A%20A%20Game%20Evaluation%20Framework%20for%20Assessing%20Vision-Centric%0A%20%20Capabilities%20in%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Xiangxi%20Zheng%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Ping%20Yu%20and%20Alex%20Jinpeng%20Wang%20and%20Rui%20Yan%20and%20Yuan%20Yao%20and%20Lijuan%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Ademonstrated%20impressive%20capabilities%20in%20visual-text%20processing.%20However%2C%0Aexisting%20static%20image-text%20benchmarks%20are%20insufficient%20for%20evaluating%20their%0Adynamic%20perception%20and%20interactive%20reasoning%20abilities.%20We%20introduce%0AVision-centric%20Multiple%20Abilities%20Game%20Evaluation%28V-MAGE%29%2C%20a%20novel%20game-based%0Aevaluation%20framework%20designed%20to%20systematically%20assess%20MLLMs%27%20visual%20reasoning%0Ain%20interactive%2C%20continuous-space%20environments.%20V-MAGE%20features%20five%20distinct%0Avideo%20games%20comprising%20over%2030%20carefully%20constructed%20evaluation%20scenarios.%0AThese%20scenarios%20are%20set%20in%20free-form%2C%20visually%20complex%20environments%20that%0Arequire%20models%20to%20interpret%20dynamic%20game%20states%20and%20make%20decisions%20based%20solely%0Aon%20visual%20input%2C%20thereby%20closely%20reflecting%20the%20conditions%20encountered%20by%20human%0Aplayers.%20To%20ensure%20robust%20and%20interpretable%20comparisons%20across%20models%2C%20V-MAGE%0Aemploys%20a%20dynamic%20Elo-based%20ranking%20system%20that%20accounts%20for%20varying%20difficulty%0Alevels%20and%20task%20diversity.%20Benchmarking%20state-of-the-art%20MLLMs%20against%20human%0Abaselines%20reveals%20that%20while%20leading%20models%20approach%20human-level%20performance%20in%0Asimple%20tasks%2C%20their%20performance%20drops%20significantly%20in%20complex%20scenarios%0Arequiring%20advanced%20reasoning%20and%20task%20orchestration.%20This%20persistent%0Aperformance%20gap%20highlights%20fundamental%20limitations%20in%20current%20MLLMs%27%20ability%20to%0Aperform%20real-time%2C%20vision-grounded%20interactions.%20Through%20extensive%20analyses%2C%20we%0Ademonstrate%20the%20utility%20of%20V-MAGE%20in%20uncovering%20these%20limitations%20and%20providing%0Aactionable%20insights%20for%20improving%20the%20visual%20and%20reasoning%20capabilities%20of%0AMLLMs%20in%20dynamic%2C%20interactive%20settings.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/CSU-JPG/V-MAGE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06148v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-MAGE%253A%2520A%2520Game%2520Evaluation%2520Framework%2520for%2520Assessing%2520Vision-Centric%250A%2520%2520Capabilities%2520in%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DXiangxi%2520Zheng%2520and%2520Linjie%2520Li%2520and%2520Zhengyuan%2520Yang%2520and%2520Ping%2520Yu%2520and%2520Alex%2520Jinpeng%2520Wang%2520and%2520Rui%2520Yan%2520and%2520Yuan%2520Yao%2520and%2520Lijuan%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%250Ademonstrated%2520impressive%2520capabilities%2520in%2520visual-text%2520processing.%2520However%252C%250Aexisting%2520static%2520image-text%2520benchmarks%2520are%2520insufficient%2520for%2520evaluating%2520their%250Adynamic%2520perception%2520and%2520interactive%2520reasoning%2520abilities.%2520We%2520introduce%250AVision-centric%2520Multiple%2520Abilities%2520Game%2520Evaluation%2528V-MAGE%2529%252C%2520a%2520novel%2520game-based%250Aevaluation%2520framework%2520designed%2520to%2520systematically%2520assess%2520MLLMs%2527%2520visual%2520reasoning%250Ain%2520interactive%252C%2520continuous-space%2520environments.%2520V-MAGE%2520features%2520five%2520distinct%250Avideo%2520games%2520comprising%2520over%252030%2520carefully%2520constructed%2520evaluation%2520scenarios.%250AThese%2520scenarios%2520are%2520set%2520in%2520free-form%252C%2520visually%2520complex%2520environments%2520that%250Arequire%2520models%2520to%2520interpret%2520dynamic%2520game%2520states%2520and%2520make%2520decisions%2520based%2520solely%250Aon%2520visual%2520input%252C%2520thereby%2520closely%2520reflecting%2520the%2520conditions%2520encountered%2520by%2520human%250Aplayers.%2520To%2520ensure%2520robust%2520and%2520interpretable%2520comparisons%2520across%2520models%252C%2520V-MAGE%250Aemploys%2520a%2520dynamic%2520Elo-based%2520ranking%2520system%2520that%2520accounts%2520for%2520varying%2520difficulty%250Alevels%2520and%2520task%2520diversity.%2520Benchmarking%2520state-of-the-art%2520MLLMs%2520against%2520human%250Abaselines%2520reveals%2520that%2520while%2520leading%2520models%2520approach%2520human-level%2520performance%2520in%250Asimple%2520tasks%252C%2520their%2520performance%2520drops%2520significantly%2520in%2520complex%2520scenarios%250Arequiring%2520advanced%2520reasoning%2520and%2520task%2520orchestration.%2520This%2520persistent%250Aperformance%2520gap%2520highlights%2520fundamental%2520limitations%2520in%2520current%2520MLLMs%2527%2520ability%2520to%250Aperform%2520real-time%252C%2520vision-grounded%2520interactions.%2520Through%2520extensive%2520analyses%252C%2520we%250Ademonstrate%2520the%2520utility%2520of%2520V-MAGE%2520in%2520uncovering%2520these%2520limitations%2520and%2520providing%250Aactionable%2520insights%2520for%2520improving%2520the%2520visual%2520and%2520reasoning%2520capabilities%2520of%250AMLLMs%2520in%2520dynamic%252C%2520interactive%2520settings.%2520Code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/CSU-JPG/V-MAGE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06148v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-MAGE%3A%20A%20Game%20Evaluation%20Framework%20for%20Assessing%20Vision-Centric%0A%20%20Capabilities%20in%20Multimodal%20Large%20Language%20Models&entry.906535625=Xiangxi%20Zheng%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Ping%20Yu%20and%20Alex%20Jinpeng%20Wang%20and%20Rui%20Yan%20and%20Yuan%20Yao%20and%20Lijuan%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Ademonstrated%20impressive%20capabilities%20in%20visual-text%20processing.%20However%2C%0Aexisting%20static%20image-text%20benchmarks%20are%20insufficient%20for%20evaluating%20their%0Adynamic%20perception%20and%20interactive%20reasoning%20abilities.%20We%20introduce%0AVision-centric%20Multiple%20Abilities%20Game%20Evaluation%28V-MAGE%29%2C%20a%20novel%20game-based%0Aevaluation%20framework%20designed%20to%20systematically%20assess%20MLLMs%27%20visual%20reasoning%0Ain%20interactive%2C%20continuous-space%20environments.%20V-MAGE%20features%20five%20distinct%0Avideo%20games%20comprising%20over%2030%20carefully%20constructed%20evaluation%20scenarios.%0AThese%20scenarios%20are%20set%20in%20free-form%2C%20visually%20complex%20environments%20that%0Arequire%20models%20to%20interpret%20dynamic%20game%20states%20and%20make%20decisions%20based%20solely%0Aon%20visual%20input%2C%20thereby%20closely%20reflecting%20the%20conditions%20encountered%20by%20human%0Aplayers.%20To%20ensure%20robust%20and%20interpretable%20comparisons%20across%20models%2C%20V-MAGE%0Aemploys%20a%20dynamic%20Elo-based%20ranking%20system%20that%20accounts%20for%20varying%20difficulty%0Alevels%20and%20task%20diversity.%20Benchmarking%20state-of-the-art%20MLLMs%20against%20human%0Abaselines%20reveals%20that%20while%20leading%20models%20approach%20human-level%20performance%20in%0Asimple%20tasks%2C%20their%20performance%20drops%20significantly%20in%20complex%20scenarios%0Arequiring%20advanced%20reasoning%20and%20task%20orchestration.%20This%20persistent%0Aperformance%20gap%20highlights%20fundamental%20limitations%20in%20current%20MLLMs%27%20ability%20to%0Aperform%20real-time%2C%20vision-grounded%20interactions.%20Through%20extensive%20analyses%2C%20we%0Ademonstrate%20the%20utility%20of%20V-MAGE%20in%20uncovering%20these%20limitations%20and%20providing%0Aactionable%20insights%20for%20improving%20the%20visual%20and%20reasoning%20capabilities%20of%0AMLLMs%20in%20dynamic%2C%20interactive%20settings.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/CSU-JPG/V-MAGE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06148v2&entry.124074799=Read"},
{"title": "Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual\n  Search in the Wild", "author": "Derek Ming Siang Tan and  Shailesh and Boyang Liu and Alok Raj and Qi Xuan Ang and Weiheng Dai and Tanishq Duhan and Jimmy Chiun and Yuhong Cao and Florian Shkurti and Guillaume Sartoretti", "abstract": "  To perform autonomous visual search for environmental monitoring, a robot may\nleverage satellite imagery as a prior map. This can help inform coarse,\nhigh-level search and exploration strategies, even when such images lack\nsufficient resolution to allow fine-grained, explicit visual recognition of\ntargets. However, there are some challenges to overcome with using satellite\nimages to direct visual search. For one, targets that are unseen in satellite\nimages are underrepresented (compared to ground images) in most existing\ndatasets, and thus vision models trained on these datasets fail to reason\neffectively based on indirect visual cues. Furthermore, approaches which\nleverage large Vision Language Models (VLMs) for generalization may yield\ninaccurate outputs due to hallucination, leading to inefficient search. To\naddress these challenges, we introduce Search-TTA, a multimodal test-time\nadaptation framework that can accept text and/or image input. First, we\npretrain a remote sensing image encoder to align with CLIP's visual encoder to\noutput probability distributions of target presence used for visual search.\nSecond, our framework dynamically refines CLIP's predictions during search\nusing a test-time adaptation mechanism. Through a feedback loop inspired by\nSpatial Poisson Point Processes, gradient updates (weighted by uncertainty) are\nused to correct (potentially inaccurate) predictions and improve search\nperformance. To validate Search-TTA's performance, we curate a visual search\ndataset based on internet-scale ecological data. We find that Search-TTA\nimproves planner performance by up to 9.7%, particularly in cases with poor\ninitial CLIP predictions. It also achieves comparable performance to\nstate-of-the-art VLMs. Finally, we deploy Search-TTA on a real UAV via\nhardware-in-the-loop testing, by simulating its operation within a large-scale\nsimulation that provides onboard sensing.\n", "link": "http://arxiv.org/abs/2505.11350v1", "date": "2025-05-16", "relevancy": 2.3874, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6085}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5935}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Search-TTA%3A%20A%20Multimodal%20Test-Time%20Adaptation%20Framework%20for%20Visual%0A%20%20Search%20in%20the%20Wild&body=Title%3A%20Search-TTA%3A%20A%20Multimodal%20Test-Time%20Adaptation%20Framework%20for%20Visual%0A%20%20Search%20in%20the%20Wild%0AAuthor%3A%20Derek%20Ming%20Siang%20Tan%20and%20%20Shailesh%20and%20Boyang%20Liu%20and%20Alok%20Raj%20and%20Qi%20Xuan%20Ang%20and%20Weiheng%20Dai%20and%20Tanishq%20Duhan%20and%20Jimmy%20Chiun%20and%20Yuhong%20Cao%20and%20Florian%20Shkurti%20and%20Guillaume%20Sartoretti%0AAbstract%3A%20%20%20To%20perform%20autonomous%20visual%20search%20for%20environmental%20monitoring%2C%20a%20robot%20may%0Aleverage%20satellite%20imagery%20as%20a%20prior%20map.%20This%20can%20help%20inform%20coarse%2C%0Ahigh-level%20search%20and%20exploration%20strategies%2C%20even%20when%20such%20images%20lack%0Asufficient%20resolution%20to%20allow%20fine-grained%2C%20explicit%20visual%20recognition%20of%0Atargets.%20However%2C%20there%20are%20some%20challenges%20to%20overcome%20with%20using%20satellite%0Aimages%20to%20direct%20visual%20search.%20For%20one%2C%20targets%20that%20are%20unseen%20in%20satellite%0Aimages%20are%20underrepresented%20%28compared%20to%20ground%20images%29%20in%20most%20existing%0Adatasets%2C%20and%20thus%20vision%20models%20trained%20on%20these%20datasets%20fail%20to%20reason%0Aeffectively%20based%20on%20indirect%20visual%20cues.%20Furthermore%2C%20approaches%20which%0Aleverage%20large%20Vision%20Language%20Models%20%28VLMs%29%20for%20generalization%20may%20yield%0Ainaccurate%20outputs%20due%20to%20hallucination%2C%20leading%20to%20inefficient%20search.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20Search-TTA%2C%20a%20multimodal%20test-time%0Aadaptation%20framework%20that%20can%20accept%20text%20and/or%20image%20input.%20First%2C%20we%0Apretrain%20a%20remote%20sensing%20image%20encoder%20to%20align%20with%20CLIP%27s%20visual%20encoder%20to%0Aoutput%20probability%20distributions%20of%20target%20presence%20used%20for%20visual%20search.%0ASecond%2C%20our%20framework%20dynamically%20refines%20CLIP%27s%20predictions%20during%20search%0Ausing%20a%20test-time%20adaptation%20mechanism.%20Through%20a%20feedback%20loop%20inspired%20by%0ASpatial%20Poisson%20Point%20Processes%2C%20gradient%20updates%20%28weighted%20by%20uncertainty%29%20are%0Aused%20to%20correct%20%28potentially%20inaccurate%29%20predictions%20and%20improve%20search%0Aperformance.%20To%20validate%20Search-TTA%27s%20performance%2C%20we%20curate%20a%20visual%20search%0Adataset%20based%20on%20internet-scale%20ecological%20data.%20We%20find%20that%20Search-TTA%0Aimproves%20planner%20performance%20by%20up%20to%209.7%25%2C%20particularly%20in%20cases%20with%20poor%0Ainitial%20CLIP%20predictions.%20It%20also%20achieves%20comparable%20performance%20to%0Astate-of-the-art%20VLMs.%20Finally%2C%20we%20deploy%20Search-TTA%20on%20a%20real%20UAV%20via%0Ahardware-in-the-loop%20testing%2C%20by%20simulating%20its%20operation%20within%20a%20large-scale%0Asimulation%20that%20provides%20onboard%20sensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearch-TTA%253A%2520A%2520Multimodal%2520Test-Time%2520Adaptation%2520Framework%2520for%2520Visual%250A%2520%2520Search%2520in%2520the%2520Wild%26entry.906535625%3DDerek%2520Ming%2520Siang%2520Tan%2520and%2520%2520Shailesh%2520and%2520Boyang%2520Liu%2520and%2520Alok%2520Raj%2520and%2520Qi%2520Xuan%2520Ang%2520and%2520Weiheng%2520Dai%2520and%2520Tanishq%2520Duhan%2520and%2520Jimmy%2520Chiun%2520and%2520Yuhong%2520Cao%2520and%2520Florian%2520Shkurti%2520and%2520Guillaume%2520Sartoretti%26entry.1292438233%3D%2520%2520To%2520perform%2520autonomous%2520visual%2520search%2520for%2520environmental%2520monitoring%252C%2520a%2520robot%2520may%250Aleverage%2520satellite%2520imagery%2520as%2520a%2520prior%2520map.%2520This%2520can%2520help%2520inform%2520coarse%252C%250Ahigh-level%2520search%2520and%2520exploration%2520strategies%252C%2520even%2520when%2520such%2520images%2520lack%250Asufficient%2520resolution%2520to%2520allow%2520fine-grained%252C%2520explicit%2520visual%2520recognition%2520of%250Atargets.%2520However%252C%2520there%2520are%2520some%2520challenges%2520to%2520overcome%2520with%2520using%2520satellite%250Aimages%2520to%2520direct%2520visual%2520search.%2520For%2520one%252C%2520targets%2520that%2520are%2520unseen%2520in%2520satellite%250Aimages%2520are%2520underrepresented%2520%2528compared%2520to%2520ground%2520images%2529%2520in%2520most%2520existing%250Adatasets%252C%2520and%2520thus%2520vision%2520models%2520trained%2520on%2520these%2520datasets%2520fail%2520to%2520reason%250Aeffectively%2520based%2520on%2520indirect%2520visual%2520cues.%2520Furthermore%252C%2520approaches%2520which%250Aleverage%2520large%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520for%2520generalization%2520may%2520yield%250Ainaccurate%2520outputs%2520due%2520to%2520hallucination%252C%2520leading%2520to%2520inefficient%2520search.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520Search-TTA%252C%2520a%2520multimodal%2520test-time%250Aadaptation%2520framework%2520that%2520can%2520accept%2520text%2520and/or%2520image%2520input.%2520First%252C%2520we%250Apretrain%2520a%2520remote%2520sensing%2520image%2520encoder%2520to%2520align%2520with%2520CLIP%2527s%2520visual%2520encoder%2520to%250Aoutput%2520probability%2520distributions%2520of%2520target%2520presence%2520used%2520for%2520visual%2520search.%250ASecond%252C%2520our%2520framework%2520dynamically%2520refines%2520CLIP%2527s%2520predictions%2520during%2520search%250Ausing%2520a%2520test-time%2520adaptation%2520mechanism.%2520Through%2520a%2520feedback%2520loop%2520inspired%2520by%250ASpatial%2520Poisson%2520Point%2520Processes%252C%2520gradient%2520updates%2520%2528weighted%2520by%2520uncertainty%2529%2520are%250Aused%2520to%2520correct%2520%2528potentially%2520inaccurate%2529%2520predictions%2520and%2520improve%2520search%250Aperformance.%2520To%2520validate%2520Search-TTA%2527s%2520performance%252C%2520we%2520curate%2520a%2520visual%2520search%250Adataset%2520based%2520on%2520internet-scale%2520ecological%2520data.%2520We%2520find%2520that%2520Search-TTA%250Aimproves%2520planner%2520performance%2520by%2520up%2520to%25209.7%2525%252C%2520particularly%2520in%2520cases%2520with%2520poor%250Ainitial%2520CLIP%2520predictions.%2520It%2520also%2520achieves%2520comparable%2520performance%2520to%250Astate-of-the-art%2520VLMs.%2520Finally%252C%2520we%2520deploy%2520Search-TTA%2520on%2520a%2520real%2520UAV%2520via%250Ahardware-in-the-loop%2520testing%252C%2520by%2520simulating%2520its%2520operation%2520within%2520a%2520large-scale%250Asimulation%2520that%2520provides%2520onboard%2520sensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search-TTA%3A%20A%20Multimodal%20Test-Time%20Adaptation%20Framework%20for%20Visual%0A%20%20Search%20in%20the%20Wild&entry.906535625=Derek%20Ming%20Siang%20Tan%20and%20%20Shailesh%20and%20Boyang%20Liu%20and%20Alok%20Raj%20and%20Qi%20Xuan%20Ang%20and%20Weiheng%20Dai%20and%20Tanishq%20Duhan%20and%20Jimmy%20Chiun%20and%20Yuhong%20Cao%20and%20Florian%20Shkurti%20and%20Guillaume%20Sartoretti&entry.1292438233=%20%20To%20perform%20autonomous%20visual%20search%20for%20environmental%20monitoring%2C%20a%20robot%20may%0Aleverage%20satellite%20imagery%20as%20a%20prior%20map.%20This%20can%20help%20inform%20coarse%2C%0Ahigh-level%20search%20and%20exploration%20strategies%2C%20even%20when%20such%20images%20lack%0Asufficient%20resolution%20to%20allow%20fine-grained%2C%20explicit%20visual%20recognition%20of%0Atargets.%20However%2C%20there%20are%20some%20challenges%20to%20overcome%20with%20using%20satellite%0Aimages%20to%20direct%20visual%20search.%20For%20one%2C%20targets%20that%20are%20unseen%20in%20satellite%0Aimages%20are%20underrepresented%20%28compared%20to%20ground%20images%29%20in%20most%20existing%0Adatasets%2C%20and%20thus%20vision%20models%20trained%20on%20these%20datasets%20fail%20to%20reason%0Aeffectively%20based%20on%20indirect%20visual%20cues.%20Furthermore%2C%20approaches%20which%0Aleverage%20large%20Vision%20Language%20Models%20%28VLMs%29%20for%20generalization%20may%20yield%0Ainaccurate%20outputs%20due%20to%20hallucination%2C%20leading%20to%20inefficient%20search.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20Search-TTA%2C%20a%20multimodal%20test-time%0Aadaptation%20framework%20that%20can%20accept%20text%20and/or%20image%20input.%20First%2C%20we%0Apretrain%20a%20remote%20sensing%20image%20encoder%20to%20align%20with%20CLIP%27s%20visual%20encoder%20to%0Aoutput%20probability%20distributions%20of%20target%20presence%20used%20for%20visual%20search.%0ASecond%2C%20our%20framework%20dynamically%20refines%20CLIP%27s%20predictions%20during%20search%0Ausing%20a%20test-time%20adaptation%20mechanism.%20Through%20a%20feedback%20loop%20inspired%20by%0ASpatial%20Poisson%20Point%20Processes%2C%20gradient%20updates%20%28weighted%20by%20uncertainty%29%20are%0Aused%20to%20correct%20%28potentially%20inaccurate%29%20predictions%20and%20improve%20search%0Aperformance.%20To%20validate%20Search-TTA%27s%20performance%2C%20we%20curate%20a%20visual%20search%0Adataset%20based%20on%20internet-scale%20ecological%20data.%20We%20find%20that%20Search-TTA%0Aimproves%20planner%20performance%20by%20up%20to%209.7%25%2C%20particularly%20in%20cases%20with%20poor%0Ainitial%20CLIP%20predictions.%20It%20also%20achieves%20comparable%20performance%20to%0Astate-of-the-art%20VLMs.%20Finally%2C%20we%20deploy%20Search-TTA%20on%20a%20real%20UAV%20via%0Ahardware-in-the-loop%20testing%2C%20by%20simulating%20its%20operation%20within%20a%20large-scale%0Asimulation%20that%20provides%20onboard%20sensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11350v1&entry.124074799=Read"},
{"title": "Evaluating Vision-Language Models as Evaluators in Path Planning", "author": "Mohamed Aghzal and Xiang Yue and Erion Plaku and Ziyu Yao", "abstract": "  Despite their promise to perform complex reasoning, large language models\n(LLMs) have been shown to have limited effectiveness in end-to-end planning.\nThis has inspired an intriguing question: if these models cannot plan well, can\nthey still contribute to the planning framework as a helpful plan evaluator? In\nthis work, we generalize this question to consider LLMs augmented with visual\nunderstanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a\nnovel benchmark evaluating VLMs as plan evaluators in complex path-planning\nscenarios. Succeeding in the benchmark requires a VLM to be able to abstract\ntraits of optimal paths from the scenario description, demonstrate precise\nlow-level perception on each path, and integrate this information to decide the\nbetter path. Our analysis of state-of-the-art VLMs reveals that these models\nface significant challenges on the benchmark. We observe that the VLMs can\nprecisely abstract given scenarios to identify the desired traits and exhibit\nmixed performance in integrating the provided information. Yet, their vision\ncomponent presents a critical bottleneck, with models struggling to perceive\nlow-level details about a path. Our experimental results show that this issue\ncannot be trivially addressed via end-to-end fine-tuning; rather, task-specific\ndiscriminative adaptation of these vision encoders is needed for these VLMs to\nbecome effective path evaluators.\n", "link": "http://arxiv.org/abs/2411.18711v4", "date": "2025-05-16", "relevancy": 2.3816, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6117}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6117}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Vision-Language%20Models%20as%20Evaluators%20in%20Path%20Planning&body=Title%3A%20Evaluating%20Vision-Language%20Models%20as%20Evaluators%20in%20Path%20Planning%0AAuthor%3A%20Mohamed%20Aghzal%20and%20Xiang%20Yue%20and%20Erion%20Plaku%20and%20Ziyu%20Yao%0AAbstract%3A%20%20%20Despite%20their%20promise%20to%20perform%20complex%20reasoning%2C%20large%20language%20models%0A%28LLMs%29%20have%20been%20shown%20to%20have%20limited%20effectiveness%20in%20end-to-end%20planning.%0AThis%20has%20inspired%20an%20intriguing%20question%3A%20if%20these%20models%20cannot%20plan%20well%2C%20can%0Athey%20still%20contribute%20to%20the%20planning%20framework%20as%20a%20helpful%20plan%20evaluator%3F%20In%0Athis%20work%2C%20we%20generalize%20this%20question%20to%20consider%20LLMs%20augmented%20with%20visual%0Aunderstanding%2C%20i.e.%2C%20Vision-Language%20Models%20%28VLMs%29.%20We%20introduce%20PathEval%2C%20a%0Anovel%20benchmark%20evaluating%20VLMs%20as%20plan%20evaluators%20in%20complex%20path-planning%0Ascenarios.%20Succeeding%20in%20the%20benchmark%20requires%20a%20VLM%20to%20be%20able%20to%20abstract%0Atraits%20of%20optimal%20paths%20from%20the%20scenario%20description%2C%20demonstrate%20precise%0Alow-level%20perception%20on%20each%20path%2C%20and%20integrate%20this%20information%20to%20decide%20the%0Abetter%20path.%20Our%20analysis%20of%20state-of-the-art%20VLMs%20reveals%20that%20these%20models%0Aface%20significant%20challenges%20on%20the%20benchmark.%20We%20observe%20that%20the%20VLMs%20can%0Aprecisely%20abstract%20given%20scenarios%20to%20identify%20the%20desired%20traits%20and%20exhibit%0Amixed%20performance%20in%20integrating%20the%20provided%20information.%20Yet%2C%20their%20vision%0Acomponent%20presents%20a%20critical%20bottleneck%2C%20with%20models%20struggling%20to%20perceive%0Alow-level%20details%20about%20a%20path.%20Our%20experimental%20results%20show%20that%20this%20issue%0Acannot%20be%20trivially%20addressed%20via%20end-to-end%20fine-tuning%3B%20rather%2C%20task-specific%0Adiscriminative%20adaptation%20of%20these%20vision%20encoders%20is%20needed%20for%20these%20VLMs%20to%0Abecome%20effective%20path%20evaluators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18711v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Vision-Language%2520Models%2520as%2520Evaluators%2520in%2520Path%2520Planning%26entry.906535625%3DMohamed%2520Aghzal%2520and%2520Xiang%2520Yue%2520and%2520Erion%2520Plaku%2520and%2520Ziyu%2520Yao%26entry.1292438233%3D%2520%2520Despite%2520their%2520promise%2520to%2520perform%2520complex%2520reasoning%252C%2520large%2520language%2520models%250A%2528LLMs%2529%2520have%2520been%2520shown%2520to%2520have%2520limited%2520effectiveness%2520in%2520end-to-end%2520planning.%250AThis%2520has%2520inspired%2520an%2520intriguing%2520question%253A%2520if%2520these%2520models%2520cannot%2520plan%2520well%252C%2520can%250Athey%2520still%2520contribute%2520to%2520the%2520planning%2520framework%2520as%2520a%2520helpful%2520plan%2520evaluator%253F%2520In%250Athis%2520work%252C%2520we%2520generalize%2520this%2520question%2520to%2520consider%2520LLMs%2520augmented%2520with%2520visual%250Aunderstanding%252C%2520i.e.%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520We%2520introduce%2520PathEval%252C%2520a%250Anovel%2520benchmark%2520evaluating%2520VLMs%2520as%2520plan%2520evaluators%2520in%2520complex%2520path-planning%250Ascenarios.%2520Succeeding%2520in%2520the%2520benchmark%2520requires%2520a%2520VLM%2520to%2520be%2520able%2520to%2520abstract%250Atraits%2520of%2520optimal%2520paths%2520from%2520the%2520scenario%2520description%252C%2520demonstrate%2520precise%250Alow-level%2520perception%2520on%2520each%2520path%252C%2520and%2520integrate%2520this%2520information%2520to%2520decide%2520the%250Abetter%2520path.%2520Our%2520analysis%2520of%2520state-of-the-art%2520VLMs%2520reveals%2520that%2520these%2520models%250Aface%2520significant%2520challenges%2520on%2520the%2520benchmark.%2520We%2520observe%2520that%2520the%2520VLMs%2520can%250Aprecisely%2520abstract%2520given%2520scenarios%2520to%2520identify%2520the%2520desired%2520traits%2520and%2520exhibit%250Amixed%2520performance%2520in%2520integrating%2520the%2520provided%2520information.%2520Yet%252C%2520their%2520vision%250Acomponent%2520presents%2520a%2520critical%2520bottleneck%252C%2520with%2520models%2520struggling%2520to%2520perceive%250Alow-level%2520details%2520about%2520a%2520path.%2520Our%2520experimental%2520results%2520show%2520that%2520this%2520issue%250Acannot%2520be%2520trivially%2520addressed%2520via%2520end-to-end%2520fine-tuning%253B%2520rather%252C%2520task-specific%250Adiscriminative%2520adaptation%2520of%2520these%2520vision%2520encoders%2520is%2520needed%2520for%2520these%2520VLMs%2520to%250Abecome%2520effective%2520path%2520evaluators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18711v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Vision-Language%20Models%20as%20Evaluators%20in%20Path%20Planning&entry.906535625=Mohamed%20Aghzal%20and%20Xiang%20Yue%20and%20Erion%20Plaku%20and%20Ziyu%20Yao&entry.1292438233=%20%20Despite%20their%20promise%20to%20perform%20complex%20reasoning%2C%20large%20language%20models%0A%28LLMs%29%20have%20been%20shown%20to%20have%20limited%20effectiveness%20in%20end-to-end%20planning.%0AThis%20has%20inspired%20an%20intriguing%20question%3A%20if%20these%20models%20cannot%20plan%20well%2C%20can%0Athey%20still%20contribute%20to%20the%20planning%20framework%20as%20a%20helpful%20plan%20evaluator%3F%20In%0Athis%20work%2C%20we%20generalize%20this%20question%20to%20consider%20LLMs%20augmented%20with%20visual%0Aunderstanding%2C%20i.e.%2C%20Vision-Language%20Models%20%28VLMs%29.%20We%20introduce%20PathEval%2C%20a%0Anovel%20benchmark%20evaluating%20VLMs%20as%20plan%20evaluators%20in%20complex%20path-planning%0Ascenarios.%20Succeeding%20in%20the%20benchmark%20requires%20a%20VLM%20to%20be%20able%20to%20abstract%0Atraits%20of%20optimal%20paths%20from%20the%20scenario%20description%2C%20demonstrate%20precise%0Alow-level%20perception%20on%20each%20path%2C%20and%20integrate%20this%20information%20to%20decide%20the%0Abetter%20path.%20Our%20analysis%20of%20state-of-the-art%20VLMs%20reveals%20that%20these%20models%0Aface%20significant%20challenges%20on%20the%20benchmark.%20We%20observe%20that%20the%20VLMs%20can%0Aprecisely%20abstract%20given%20scenarios%20to%20identify%20the%20desired%20traits%20and%20exhibit%0Amixed%20performance%20in%20integrating%20the%20provided%20information.%20Yet%2C%20their%20vision%0Acomponent%20presents%20a%20critical%20bottleneck%2C%20with%20models%20struggling%20to%20perceive%0Alow-level%20details%20about%20a%20path.%20Our%20experimental%20results%20show%20that%20this%20issue%0Acannot%20be%20trivially%20addressed%20via%20end-to-end%20fine-tuning%3B%20rather%2C%20task-specific%0Adiscriminative%20adaptation%20of%20these%20vision%20encoders%20is%20needed%20for%20these%20VLMs%20to%0Abecome%20effective%20path%20evaluators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18711v4&entry.124074799=Read"},
{"title": "Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views", "author": "Abhishek Kashyap and Henrik Andreasson and Todor Stoyanov", "abstract": "  Vision based robot manipulation uses cameras to capture one or more images of\na scene containing the objects to be manipulated. Taking multiple images can\nhelp if any object is occluded from one viewpoint but more visible from another\nviewpoint. However, the camera has to be moved to a sequence of suitable\npositions for capturing multiple images, which requires time and may not always\nbe possible, due to reachability constraints. So while additional images can\nproduce more accurate grasp poses due to the extra information available, the\ntime-cost goes up with the number of additional views sampled. Scene\nrepresentations like Gaussian Splatting are capable of rendering accurate\nphotorealistic virtual images from user-specified novel viewpoints. In this\nwork, we show initial results which indicate that novel view synthesis can\nprovide additional context in generating grasp poses. Our experiments on the\nGraspnet-1billion dataset show that novel views contributed force-closure\ngrasps in addition to the force-closure grasps obtained from sparsely sampled\nreal views while also improving grasp coverage. In the future we hope this work\ncan be extended to improve grasp extraction from radiance fields constructed\nwith a single input image, using for example diffusion models or generalizable\nradiance fields.\n", "link": "http://arxiv.org/abs/2505.11467v1", "date": "2025-05-16", "relevancy": 2.3585, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.611}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5761}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Radiance%20Fields%20for%20Grasp%20Generation%20on%20Novel%20Synthetic%20Views&body=Title%3A%20Exploiting%20Radiance%20Fields%20for%20Grasp%20Generation%20on%20Novel%20Synthetic%20Views%0AAuthor%3A%20Abhishek%20Kashyap%20and%20Henrik%20Andreasson%20and%20Todor%20Stoyanov%0AAbstract%3A%20%20%20Vision%20based%20robot%20manipulation%20uses%20cameras%20to%20capture%20one%20or%20more%20images%20of%0Aa%20scene%20containing%20the%20objects%20to%20be%20manipulated.%20Taking%20multiple%20images%20can%0Ahelp%20if%20any%20object%20is%20occluded%20from%20one%20viewpoint%20but%20more%20visible%20from%20another%0Aviewpoint.%20However%2C%20the%20camera%20has%20to%20be%20moved%20to%20a%20sequence%20of%20suitable%0Apositions%20for%20capturing%20multiple%20images%2C%20which%20requires%20time%20and%20may%20not%20always%0Abe%20possible%2C%20due%20to%20reachability%20constraints.%20So%20while%20additional%20images%20can%0Aproduce%20more%20accurate%20grasp%20poses%20due%20to%20the%20extra%20information%20available%2C%20the%0Atime-cost%20goes%20up%20with%20the%20number%20of%20additional%20views%20sampled.%20Scene%0Arepresentations%20like%20Gaussian%20Splatting%20are%20capable%20of%20rendering%20accurate%0Aphotorealistic%20virtual%20images%20from%20user-specified%20novel%20viewpoints.%20In%20this%0Awork%2C%20we%20show%20initial%20results%20which%20indicate%20that%20novel%20view%20synthesis%20can%0Aprovide%20additional%20context%20in%20generating%20grasp%20poses.%20Our%20experiments%20on%20the%0AGraspnet-1billion%20dataset%20show%20that%20novel%20views%20contributed%20force-closure%0Agrasps%20in%20addition%20to%20the%20force-closure%20grasps%20obtained%20from%20sparsely%20sampled%0Areal%20views%20while%20also%20improving%20grasp%20coverage.%20In%20the%20future%20we%20hope%20this%20work%0Acan%20be%20extended%20to%20improve%20grasp%20extraction%20from%20radiance%20fields%20constructed%0Awith%20a%20single%20input%20image%2C%20using%20for%20example%20diffusion%20models%20or%20generalizable%0Aradiance%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Radiance%2520Fields%2520for%2520Grasp%2520Generation%2520on%2520Novel%2520Synthetic%2520Views%26entry.906535625%3DAbhishek%2520Kashyap%2520and%2520Henrik%2520Andreasson%2520and%2520Todor%2520Stoyanov%26entry.1292438233%3D%2520%2520Vision%2520based%2520robot%2520manipulation%2520uses%2520cameras%2520to%2520capture%2520one%2520or%2520more%2520images%2520of%250Aa%2520scene%2520containing%2520the%2520objects%2520to%2520be%2520manipulated.%2520Taking%2520multiple%2520images%2520can%250Ahelp%2520if%2520any%2520object%2520is%2520occluded%2520from%2520one%2520viewpoint%2520but%2520more%2520visible%2520from%2520another%250Aviewpoint.%2520However%252C%2520the%2520camera%2520has%2520to%2520be%2520moved%2520to%2520a%2520sequence%2520of%2520suitable%250Apositions%2520for%2520capturing%2520multiple%2520images%252C%2520which%2520requires%2520time%2520and%2520may%2520not%2520always%250Abe%2520possible%252C%2520due%2520to%2520reachability%2520constraints.%2520So%2520while%2520additional%2520images%2520can%250Aproduce%2520more%2520accurate%2520grasp%2520poses%2520due%2520to%2520the%2520extra%2520information%2520available%252C%2520the%250Atime-cost%2520goes%2520up%2520with%2520the%2520number%2520of%2520additional%2520views%2520sampled.%2520Scene%250Arepresentations%2520like%2520Gaussian%2520Splatting%2520are%2520capable%2520of%2520rendering%2520accurate%250Aphotorealistic%2520virtual%2520images%2520from%2520user-specified%2520novel%2520viewpoints.%2520In%2520this%250Awork%252C%2520we%2520show%2520initial%2520results%2520which%2520indicate%2520that%2520novel%2520view%2520synthesis%2520can%250Aprovide%2520additional%2520context%2520in%2520generating%2520grasp%2520poses.%2520Our%2520experiments%2520on%2520the%250AGraspnet-1billion%2520dataset%2520show%2520that%2520novel%2520views%2520contributed%2520force-closure%250Agrasps%2520in%2520addition%2520to%2520the%2520force-closure%2520grasps%2520obtained%2520from%2520sparsely%2520sampled%250Areal%2520views%2520while%2520also%2520improving%2520grasp%2520coverage.%2520In%2520the%2520future%2520we%2520hope%2520this%2520work%250Acan%2520be%2520extended%2520to%2520improve%2520grasp%2520extraction%2520from%2520radiance%2520fields%2520constructed%250Awith%2520a%2520single%2520input%2520image%252C%2520using%2520for%2520example%2520diffusion%2520models%2520or%2520generalizable%250Aradiance%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Radiance%20Fields%20for%20Grasp%20Generation%20on%20Novel%20Synthetic%20Views&entry.906535625=Abhishek%20Kashyap%20and%20Henrik%20Andreasson%20and%20Todor%20Stoyanov&entry.1292438233=%20%20Vision%20based%20robot%20manipulation%20uses%20cameras%20to%20capture%20one%20or%20more%20images%20of%0Aa%20scene%20containing%20the%20objects%20to%20be%20manipulated.%20Taking%20multiple%20images%20can%0Ahelp%20if%20any%20object%20is%20occluded%20from%20one%20viewpoint%20but%20more%20visible%20from%20another%0Aviewpoint.%20However%2C%20the%20camera%20has%20to%20be%20moved%20to%20a%20sequence%20of%20suitable%0Apositions%20for%20capturing%20multiple%20images%2C%20which%20requires%20time%20and%20may%20not%20always%0Abe%20possible%2C%20due%20to%20reachability%20constraints.%20So%20while%20additional%20images%20can%0Aproduce%20more%20accurate%20grasp%20poses%20due%20to%20the%20extra%20information%20available%2C%20the%0Atime-cost%20goes%20up%20with%20the%20number%20of%20additional%20views%20sampled.%20Scene%0Arepresentations%20like%20Gaussian%20Splatting%20are%20capable%20of%20rendering%20accurate%0Aphotorealistic%20virtual%20images%20from%20user-specified%20novel%20viewpoints.%20In%20this%0Awork%2C%20we%20show%20initial%20results%20which%20indicate%20that%20novel%20view%20synthesis%20can%0Aprovide%20additional%20context%20in%20generating%20grasp%20poses.%20Our%20experiments%20on%20the%0AGraspnet-1billion%20dataset%20show%20that%20novel%20views%20contributed%20force-closure%0Agrasps%20in%20addition%20to%20the%20force-closure%20grasps%20obtained%20from%20sparsely%20sampled%0Areal%20views%20while%20also%20improving%20grasp%20coverage.%20In%20the%20future%20we%20hope%20this%20work%0Acan%20be%20extended%20to%20improve%20grasp%20extraction%20from%20radiance%20fields%20constructed%0Awith%20a%20single%20input%20image%2C%20using%20for%20example%20diffusion%20models%20or%20generalizable%0Aradiance%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11467v1&entry.124074799=Read"},
{"title": "Unveiling the Potential of Vision-Language-Action Models with Open-Ended\n  Multimodal Instructions", "author": "Wei Zhao and Gongsheng Li and Zhefei Gong and Pengxiang Ding and Han Zhao and Donglin Wang", "abstract": "  Vision-Language-Action (VLA) models have recently become highly prominent in\nthe field of robotics. Leveraging vision-language foundation models trained on\nlarge-scale internet data, the VLA model can generate robotic actions directly\nfrom visual observations and human instructions through a single end-to-end\nneural network. Despite their effectiveness, current VLA models usually accept\nonly one form of human prompting, language instructions, which may constrain\ntheir applicability in open-ended human-robot interactions. For example, a user\nmight expect the robot to retrieve an object shown in an image, follow an\ninstruction written on the whiteboard, or imitate a behavior demonstrated in a\nvideo, rather than relying solely on language-based descriptions. To address\nthis gap, we introduce OE-VLA, which explores the potential of VLA models for\nopen-ended multimodal instructions. Extensive results demonstrate that our\nOE-VLA not only achieves comparable performance to traditional VLA models with\nlinguistic input but also delivers impressive results across four additional\ncategories of open-ended tasks. The proposed methodology could significantly\nexpand the applications of VLA models across various everyday scenarios and\nfacilitate human-robot interaction.\n", "link": "http://arxiv.org/abs/2505.11214v1", "date": "2025-05-16", "relevancy": 2.3547, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Potential%20of%20Vision-Language-Action%20Models%20with%20Open-Ended%0A%20%20Multimodal%20Instructions&body=Title%3A%20Unveiling%20the%20Potential%20of%20Vision-Language-Action%20Models%20with%20Open-Ended%0A%20%20Multimodal%20Instructions%0AAuthor%3A%20Wei%20Zhao%20and%20Gongsheng%20Li%20and%20Zhefei%20Gong%20and%20Pengxiang%20Ding%20and%20Han%20Zhao%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20recently%20become%20highly%20prominent%20in%0Athe%20field%20of%20robotics.%20Leveraging%20vision-language%20foundation%20models%20trained%20on%0Alarge-scale%20internet%20data%2C%20the%20VLA%20model%20can%20generate%20robotic%20actions%20directly%0Afrom%20visual%20observations%20and%20human%20instructions%20through%20a%20single%20end-to-end%0Aneural%20network.%20Despite%20their%20effectiveness%2C%20current%20VLA%20models%20usually%20accept%0Aonly%20one%20form%20of%20human%20prompting%2C%20language%20instructions%2C%20which%20may%20constrain%0Atheir%20applicability%20in%20open-ended%20human-robot%20interactions.%20For%20example%2C%20a%20user%0Amight%20expect%20the%20robot%20to%20retrieve%20an%20object%20shown%20in%20an%20image%2C%20follow%20an%0Ainstruction%20written%20on%20the%20whiteboard%2C%20or%20imitate%20a%20behavior%20demonstrated%20in%20a%0Avideo%2C%20rather%20than%20relying%20solely%20on%20language-based%20descriptions.%20To%20address%0Athis%20gap%2C%20we%20introduce%20OE-VLA%2C%20which%20explores%20the%20potential%20of%20VLA%20models%20for%0Aopen-ended%20multimodal%20instructions.%20Extensive%20results%20demonstrate%20that%20our%0AOE-VLA%20not%20only%20achieves%20comparable%20performance%20to%20traditional%20VLA%20models%20with%0Alinguistic%20input%20but%20also%20delivers%20impressive%20results%20across%20four%20additional%0Acategories%20of%20open-ended%20tasks.%20The%20proposed%20methodology%20could%20significantly%0Aexpand%20the%20applications%20of%20VLA%20models%20across%20various%20everyday%20scenarios%20and%0Afacilitate%20human-robot%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Potential%2520of%2520Vision-Language-Action%2520Models%2520with%2520Open-Ended%250A%2520%2520Multimodal%2520Instructions%26entry.906535625%3DWei%2520Zhao%2520and%2520Gongsheng%2520Li%2520and%2520Zhefei%2520Gong%2520and%2520Pengxiang%2520Ding%2520and%2520Han%2520Zhao%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520recently%2520become%2520highly%2520prominent%2520in%250Athe%2520field%2520of%2520robotics.%2520Leveraging%2520vision-language%2520foundation%2520models%2520trained%2520on%250Alarge-scale%2520internet%2520data%252C%2520the%2520VLA%2520model%2520can%2520generate%2520robotic%2520actions%2520directly%250Afrom%2520visual%2520observations%2520and%2520human%2520instructions%2520through%2520a%2520single%2520end-to-end%250Aneural%2520network.%2520Despite%2520their%2520effectiveness%252C%2520current%2520VLA%2520models%2520usually%2520accept%250Aonly%2520one%2520form%2520of%2520human%2520prompting%252C%2520language%2520instructions%252C%2520which%2520may%2520constrain%250Atheir%2520applicability%2520in%2520open-ended%2520human-robot%2520interactions.%2520For%2520example%252C%2520a%2520user%250Amight%2520expect%2520the%2520robot%2520to%2520retrieve%2520an%2520object%2520shown%2520in%2520an%2520image%252C%2520follow%2520an%250Ainstruction%2520written%2520on%2520the%2520whiteboard%252C%2520or%2520imitate%2520a%2520behavior%2520demonstrated%2520in%2520a%250Avideo%252C%2520rather%2520than%2520relying%2520solely%2520on%2520language-based%2520descriptions.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520OE-VLA%252C%2520which%2520explores%2520the%2520potential%2520of%2520VLA%2520models%2520for%250Aopen-ended%2520multimodal%2520instructions.%2520Extensive%2520results%2520demonstrate%2520that%2520our%250AOE-VLA%2520not%2520only%2520achieves%2520comparable%2520performance%2520to%2520traditional%2520VLA%2520models%2520with%250Alinguistic%2520input%2520but%2520also%2520delivers%2520impressive%2520results%2520across%2520four%2520additional%250Acategories%2520of%2520open-ended%2520tasks.%2520The%2520proposed%2520methodology%2520could%2520significantly%250Aexpand%2520the%2520applications%2520of%2520VLA%2520models%2520across%2520various%2520everyday%2520scenarios%2520and%250Afacilitate%2520human-robot%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Potential%20of%20Vision-Language-Action%20Models%20with%20Open-Ended%0A%20%20Multimodal%20Instructions&entry.906535625=Wei%20Zhao%20and%20Gongsheng%20Li%20and%20Zhefei%20Gong%20and%20Pengxiang%20Ding%20and%20Han%20Zhao%20and%20Donglin%20Wang&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20recently%20become%20highly%20prominent%20in%0Athe%20field%20of%20robotics.%20Leveraging%20vision-language%20foundation%20models%20trained%20on%0Alarge-scale%20internet%20data%2C%20the%20VLA%20model%20can%20generate%20robotic%20actions%20directly%0Afrom%20visual%20observations%20and%20human%20instructions%20through%20a%20single%20end-to-end%0Aneural%20network.%20Despite%20their%20effectiveness%2C%20current%20VLA%20models%20usually%20accept%0Aonly%20one%20form%20of%20human%20prompting%2C%20language%20instructions%2C%20which%20may%20constrain%0Atheir%20applicability%20in%20open-ended%20human-robot%20interactions.%20For%20example%2C%20a%20user%0Amight%20expect%20the%20robot%20to%20retrieve%20an%20object%20shown%20in%20an%20image%2C%20follow%20an%0Ainstruction%20written%20on%20the%20whiteboard%2C%20or%20imitate%20a%20behavior%20demonstrated%20in%20a%0Avideo%2C%20rather%20than%20relying%20solely%20on%20language-based%20descriptions.%20To%20address%0Athis%20gap%2C%20we%20introduce%20OE-VLA%2C%20which%20explores%20the%20potential%20of%20VLA%20models%20for%0Aopen-ended%20multimodal%20instructions.%20Extensive%20results%20demonstrate%20that%20our%0AOE-VLA%20not%20only%20achieves%20comparable%20performance%20to%20traditional%20VLA%20models%20with%0Alinguistic%20input%20but%20also%20delivers%20impressive%20results%20across%20four%20additional%0Acategories%20of%20open-ended%20tasks.%20The%20proposed%20methodology%20could%20significantly%0Aexpand%20the%20applications%20of%20VLA%20models%20across%20various%20everyday%20scenarios%20and%0Afacilitate%20human-robot%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11214v1&entry.124074799=Read"},
{"title": "Real-Time Verification of Embodied Reasoning for Generative Skill\n  Acquisition", "author": "Bo Yue and Shuqi Guo and Kaiyu Hu and Chujiao Wang and Benyou Wang and Kui Jia and Guiliang Liu", "abstract": "  Generative skill acquisition enables embodied agents to actively learn a\nscalable and evolving repertoire of control skills, crucial for the advancement\nof large decision models. While prior approaches often rely on supervision\nsignals from generalist agents (e.g., LLMs), their effectiveness in complex 3D\nenvironments remains unclear; exhaustive evaluation incurs substantial\ncomputational costs, significantly hindering the efficiency of skill learning.\nInspired by recent successes in verification models for mathematical reasoning,\nwe propose VERGSA (Verifying Embodied Reasoning in Generative Skill\nAcquisition), a framework that systematically integrates real-time verification\nprinciples into embodied skill learning. VERGSA establishes 1) a seamless\nextension from verification of mathematical reasoning into embodied learning by\ndynamically incorporating contextually relevant tasks into prompts and defining\nsuccess metrics for both subtasks and overall tasks, and 2) an automated,\nscalable reward labeling scheme that synthesizes dense reward signals by\niteratively finalizing the contribution of scene configuration and subtask\nlearning to overall skill acquisition. To the best of our knowledge, this\napproach constitutes the first comprehensive training dataset for\nverification-driven generative skill acquisition, eliminating arduous manual\nreward engineering. Experiments validate the efficacy of our approach: 1) the\nexemplar task pool improves the average task success rates by 21%, 2) our\nverification model boosts success rates by 24% for novel tasks and 36% for\nencountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification\nquality.\n", "link": "http://arxiv.org/abs/2505.11175v1", "date": "2025-05-16", "relevancy": 2.3465, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6119}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5871}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Verification%20of%20Embodied%20Reasoning%20for%20Generative%20Skill%0A%20%20Acquisition&body=Title%3A%20Real-Time%20Verification%20of%20Embodied%20Reasoning%20for%20Generative%20Skill%0A%20%20Acquisition%0AAuthor%3A%20Bo%20Yue%20and%20Shuqi%20Guo%20and%20Kaiyu%20Hu%20and%20Chujiao%20Wang%20and%20Benyou%20Wang%20and%20Kui%20Jia%20and%20Guiliang%20Liu%0AAbstract%3A%20%20%20Generative%20skill%20acquisition%20enables%20embodied%20agents%20to%20actively%20learn%20a%0Ascalable%20and%20evolving%20repertoire%20of%20control%20skills%2C%20crucial%20for%20the%20advancement%0Aof%20large%20decision%20models.%20While%20prior%20approaches%20often%20rely%20on%20supervision%0Asignals%20from%20generalist%20agents%20%28e.g.%2C%20LLMs%29%2C%20their%20effectiveness%20in%20complex%203D%0Aenvironments%20remains%20unclear%3B%20exhaustive%20evaluation%20incurs%20substantial%0Acomputational%20costs%2C%20significantly%20hindering%20the%20efficiency%20of%20skill%20learning.%0AInspired%20by%20recent%20successes%20in%20verification%20models%20for%20mathematical%20reasoning%2C%0Awe%20propose%20VERGSA%20%28Verifying%20Embodied%20Reasoning%20in%20Generative%20Skill%0AAcquisition%29%2C%20a%20framework%20that%20systematically%20integrates%20real-time%20verification%0Aprinciples%20into%20embodied%20skill%20learning.%20VERGSA%20establishes%201%29%20a%20seamless%0Aextension%20from%20verification%20of%20mathematical%20reasoning%20into%20embodied%20learning%20by%0Adynamically%20incorporating%20contextually%20relevant%20tasks%20into%20prompts%20and%20defining%0Asuccess%20metrics%20for%20both%20subtasks%20and%20overall%20tasks%2C%20and%202%29%20an%20automated%2C%0Ascalable%20reward%20labeling%20scheme%20that%20synthesizes%20dense%20reward%20signals%20by%0Aiteratively%20finalizing%20the%20contribution%20of%20scene%20configuration%20and%20subtask%0Alearning%20to%20overall%20skill%20acquisition.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Aapproach%20constitutes%20the%20first%20comprehensive%20training%20dataset%20for%0Averification-driven%20generative%20skill%20acquisition%2C%20eliminating%20arduous%20manual%0Areward%20engineering.%20Experiments%20validate%20the%20efficacy%20of%20our%20approach%3A%201%29%20the%0Aexemplar%20task%20pool%20improves%20the%20average%20task%20success%20rates%20by%2021%25%2C%202%29%20our%0Averification%20model%20boosts%20success%20rates%20by%2024%25%20for%20novel%20tasks%20and%2036%25%20for%0Aencountered%20tasks%2C%20and%203%29%20outperforms%20LLM-as-a-Judge%20baselines%20in%20verification%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Verification%2520of%2520Embodied%2520Reasoning%2520for%2520Generative%2520Skill%250A%2520%2520Acquisition%26entry.906535625%3DBo%2520Yue%2520and%2520Shuqi%2520Guo%2520and%2520Kaiyu%2520Hu%2520and%2520Chujiao%2520Wang%2520and%2520Benyou%2520Wang%2520and%2520Kui%2520Jia%2520and%2520Guiliang%2520Liu%26entry.1292438233%3D%2520%2520Generative%2520skill%2520acquisition%2520enables%2520embodied%2520agents%2520to%2520actively%2520learn%2520a%250Ascalable%2520and%2520evolving%2520repertoire%2520of%2520control%2520skills%252C%2520crucial%2520for%2520the%2520advancement%250Aof%2520large%2520decision%2520models.%2520While%2520prior%2520approaches%2520often%2520rely%2520on%2520supervision%250Asignals%2520from%2520generalist%2520agents%2520%2528e.g.%252C%2520LLMs%2529%252C%2520their%2520effectiveness%2520in%2520complex%25203D%250Aenvironments%2520remains%2520unclear%253B%2520exhaustive%2520evaluation%2520incurs%2520substantial%250Acomputational%2520costs%252C%2520significantly%2520hindering%2520the%2520efficiency%2520of%2520skill%2520learning.%250AInspired%2520by%2520recent%2520successes%2520in%2520verification%2520models%2520for%2520mathematical%2520reasoning%252C%250Awe%2520propose%2520VERGSA%2520%2528Verifying%2520Embodied%2520Reasoning%2520in%2520Generative%2520Skill%250AAcquisition%2529%252C%2520a%2520framework%2520that%2520systematically%2520integrates%2520real-time%2520verification%250Aprinciples%2520into%2520embodied%2520skill%2520learning.%2520VERGSA%2520establishes%25201%2529%2520a%2520seamless%250Aextension%2520from%2520verification%2520of%2520mathematical%2520reasoning%2520into%2520embodied%2520learning%2520by%250Adynamically%2520incorporating%2520contextually%2520relevant%2520tasks%2520into%2520prompts%2520and%2520defining%250Asuccess%2520metrics%2520for%2520both%2520subtasks%2520and%2520overall%2520tasks%252C%2520and%25202%2529%2520an%2520automated%252C%250Ascalable%2520reward%2520labeling%2520scheme%2520that%2520synthesizes%2520dense%2520reward%2520signals%2520by%250Aiteratively%2520finalizing%2520the%2520contribution%2520of%2520scene%2520configuration%2520and%2520subtask%250Alearning%2520to%2520overall%2520skill%2520acquisition.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%250Aapproach%2520constitutes%2520the%2520first%2520comprehensive%2520training%2520dataset%2520for%250Averification-driven%2520generative%2520skill%2520acquisition%252C%2520eliminating%2520arduous%2520manual%250Areward%2520engineering.%2520Experiments%2520validate%2520the%2520efficacy%2520of%2520our%2520approach%253A%25201%2529%2520the%250Aexemplar%2520task%2520pool%2520improves%2520the%2520average%2520task%2520success%2520rates%2520by%252021%2525%252C%25202%2529%2520our%250Averification%2520model%2520boosts%2520success%2520rates%2520by%252024%2525%2520for%2520novel%2520tasks%2520and%252036%2525%2520for%250Aencountered%2520tasks%252C%2520and%25203%2529%2520outperforms%2520LLM-as-a-Judge%2520baselines%2520in%2520verification%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Verification%20of%20Embodied%20Reasoning%20for%20Generative%20Skill%0A%20%20Acquisition&entry.906535625=Bo%20Yue%20and%20Shuqi%20Guo%20and%20Kaiyu%20Hu%20and%20Chujiao%20Wang%20and%20Benyou%20Wang%20and%20Kui%20Jia%20and%20Guiliang%20Liu&entry.1292438233=%20%20Generative%20skill%20acquisition%20enables%20embodied%20agents%20to%20actively%20learn%20a%0Ascalable%20and%20evolving%20repertoire%20of%20control%20skills%2C%20crucial%20for%20the%20advancement%0Aof%20large%20decision%20models.%20While%20prior%20approaches%20often%20rely%20on%20supervision%0Asignals%20from%20generalist%20agents%20%28e.g.%2C%20LLMs%29%2C%20their%20effectiveness%20in%20complex%203D%0Aenvironments%20remains%20unclear%3B%20exhaustive%20evaluation%20incurs%20substantial%0Acomputational%20costs%2C%20significantly%20hindering%20the%20efficiency%20of%20skill%20learning.%0AInspired%20by%20recent%20successes%20in%20verification%20models%20for%20mathematical%20reasoning%2C%0Awe%20propose%20VERGSA%20%28Verifying%20Embodied%20Reasoning%20in%20Generative%20Skill%0AAcquisition%29%2C%20a%20framework%20that%20systematically%20integrates%20real-time%20verification%0Aprinciples%20into%20embodied%20skill%20learning.%20VERGSA%20establishes%201%29%20a%20seamless%0Aextension%20from%20verification%20of%20mathematical%20reasoning%20into%20embodied%20learning%20by%0Adynamically%20incorporating%20contextually%20relevant%20tasks%20into%20prompts%20and%20defining%0Asuccess%20metrics%20for%20both%20subtasks%20and%20overall%20tasks%2C%20and%202%29%20an%20automated%2C%0Ascalable%20reward%20labeling%20scheme%20that%20synthesizes%20dense%20reward%20signals%20by%0Aiteratively%20finalizing%20the%20contribution%20of%20scene%20configuration%20and%20subtask%0Alearning%20to%20overall%20skill%20acquisition.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Aapproach%20constitutes%20the%20first%20comprehensive%20training%20dataset%20for%0Averification-driven%20generative%20skill%20acquisition%2C%20eliminating%20arduous%20manual%0Areward%20engineering.%20Experiments%20validate%20the%20efficacy%20of%20our%20approach%3A%201%29%20the%0Aexemplar%20task%20pool%20improves%20the%20average%20task%20success%20rates%20by%2021%25%2C%202%29%20our%0Averification%20model%20boosts%20success%20rates%20by%2024%25%20for%20novel%20tasks%20and%2036%25%20for%0Aencountered%20tasks%2C%20and%203%29%20outperforms%20LLM-as-a-Judge%20baselines%20in%20verification%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11175v1&entry.124074799=Read"},
{"title": "Learning Equivariant Non-Local Electron Density Functionals", "author": "Nicholas Gao and Eike Eberhard and Stephan G\u00fcnnemann", "abstract": "  The accuracy of density functional theory hinges on the approximation of\nnon-local contributions to the exchange-correlation (XC) functional. To date,\nmachine-learned and human-designed approximations suffer from insufficient\naccuracy, limited scalability, or dependence on costly reference data. To\naddress these issues, we introduce Equivariant Graph Exchange Correlation\n(EG-XC), a novel non-local XC functional based on equivariant graph neural\nnetworks (GNNs). Where previous works relied on semi-local functionals or\nfixed-size descriptors of the density, we compress the electron density into an\nSO(3)-equivariant nuclei-centered point cloud for efficient non-local\natomic-range interactions. By applying an equivariant GNN on this point cloud,\nwe capture molecular-range interactions in a scalable and accurate manner. To\ntrain EG-XC, we differentiate through a self-consistent field solver requiring\nonly energy targets. In our empirical evaluation, we find EG-XC to accurately\nreconstruct `gold-standard' CCSD(T) energies on MD17. On out-of-distribution\nconformations of 3BPA, EG-XC reduces the relative MAE by 35% to 50%.\nRemarkably, EG-XC excels in data efficiency and molecular size extrapolation on\nQM9, matching force fields trained on 5 times more and larger molecules. On\nidentical training sets, EG-XC yields on average 51% lower MAEs.\n", "link": "http://arxiv.org/abs/2410.07972v3", "date": "2025-05-16", "relevancy": 2.3447, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.482}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4683}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Equivariant%20Non-Local%20Electron%20Density%20Functionals&body=Title%3A%20Learning%20Equivariant%20Non-Local%20Electron%20Density%20Functionals%0AAuthor%3A%20Nicholas%20Gao%20and%20Eike%20Eberhard%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20The%20accuracy%20of%20density%20functional%20theory%20hinges%20on%20the%20approximation%20of%0Anon-local%20contributions%20to%20the%20exchange-correlation%20%28XC%29%20functional.%20To%20date%2C%0Amachine-learned%20and%20human-designed%20approximations%20suffer%20from%20insufficient%0Aaccuracy%2C%20limited%20scalability%2C%20or%20dependence%20on%20costly%20reference%20data.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20Equivariant%20Graph%20Exchange%20Correlation%0A%28EG-XC%29%2C%20a%20novel%20non-local%20XC%20functional%20based%20on%20equivariant%20graph%20neural%0Anetworks%20%28GNNs%29.%20Where%20previous%20works%20relied%20on%20semi-local%20functionals%20or%0Afixed-size%20descriptors%20of%20the%20density%2C%20we%20compress%20the%20electron%20density%20into%20an%0ASO%283%29-equivariant%20nuclei-centered%20point%20cloud%20for%20efficient%20non-local%0Aatomic-range%20interactions.%20By%20applying%20an%20equivariant%20GNN%20on%20this%20point%20cloud%2C%0Awe%20capture%20molecular-range%20interactions%20in%20a%20scalable%20and%20accurate%20manner.%20To%0Atrain%20EG-XC%2C%20we%20differentiate%20through%20a%20self-consistent%20field%20solver%20requiring%0Aonly%20energy%20targets.%20In%20our%20empirical%20evaluation%2C%20we%20find%20EG-XC%20to%20accurately%0Areconstruct%20%60gold-standard%27%20CCSD%28T%29%20energies%20on%20MD17.%20On%20out-of-distribution%0Aconformations%20of%203BPA%2C%20EG-XC%20reduces%20the%20relative%20MAE%20by%2035%25%20to%2050%25.%0ARemarkably%2C%20EG-XC%20excels%20in%20data%20efficiency%20and%20molecular%20size%20extrapolation%20on%0AQM9%2C%20matching%20force%20fields%20trained%20on%205%20times%20more%20and%20larger%20molecules.%20On%0Aidentical%20training%20sets%2C%20EG-XC%20yields%20on%20average%2051%25%20lower%20MAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07972v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Equivariant%2520Non-Local%2520Electron%2520Density%2520Functionals%26entry.906535625%3DNicholas%2520Gao%2520and%2520Eike%2520Eberhard%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520The%2520accuracy%2520of%2520density%2520functional%2520theory%2520hinges%2520on%2520the%2520approximation%2520of%250Anon-local%2520contributions%2520to%2520the%2520exchange-correlation%2520%2528XC%2529%2520functional.%2520To%2520date%252C%250Amachine-learned%2520and%2520human-designed%2520approximations%2520suffer%2520from%2520insufficient%250Aaccuracy%252C%2520limited%2520scalability%252C%2520or%2520dependence%2520on%2520costly%2520reference%2520data.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520introduce%2520Equivariant%2520Graph%2520Exchange%2520Correlation%250A%2528EG-XC%2529%252C%2520a%2520novel%2520non-local%2520XC%2520functional%2520based%2520on%2520equivariant%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529.%2520Where%2520previous%2520works%2520relied%2520on%2520semi-local%2520functionals%2520or%250Afixed-size%2520descriptors%2520of%2520the%2520density%252C%2520we%2520compress%2520the%2520electron%2520density%2520into%2520an%250ASO%25283%2529-equivariant%2520nuclei-centered%2520point%2520cloud%2520for%2520efficient%2520non-local%250Aatomic-range%2520interactions.%2520By%2520applying%2520an%2520equivariant%2520GNN%2520on%2520this%2520point%2520cloud%252C%250Awe%2520capture%2520molecular-range%2520interactions%2520in%2520a%2520scalable%2520and%2520accurate%2520manner.%2520To%250Atrain%2520EG-XC%252C%2520we%2520differentiate%2520through%2520a%2520self-consistent%2520field%2520solver%2520requiring%250Aonly%2520energy%2520targets.%2520In%2520our%2520empirical%2520evaluation%252C%2520we%2520find%2520EG-XC%2520to%2520accurately%250Areconstruct%2520%2560gold-standard%2527%2520CCSD%2528T%2529%2520energies%2520on%2520MD17.%2520On%2520out-of-distribution%250Aconformations%2520of%25203BPA%252C%2520EG-XC%2520reduces%2520the%2520relative%2520MAE%2520by%252035%2525%2520to%252050%2525.%250ARemarkably%252C%2520EG-XC%2520excels%2520in%2520data%2520efficiency%2520and%2520molecular%2520size%2520extrapolation%2520on%250AQM9%252C%2520matching%2520force%2520fields%2520trained%2520on%25205%2520times%2520more%2520and%2520larger%2520molecules.%2520On%250Aidentical%2520training%2520sets%252C%2520EG-XC%2520yields%2520on%2520average%252051%2525%2520lower%2520MAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07972v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Equivariant%20Non-Local%20Electron%20Density%20Functionals&entry.906535625=Nicholas%20Gao%20and%20Eike%20Eberhard%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20The%20accuracy%20of%20density%20functional%20theory%20hinges%20on%20the%20approximation%20of%0Anon-local%20contributions%20to%20the%20exchange-correlation%20%28XC%29%20functional.%20To%20date%2C%0Amachine-learned%20and%20human-designed%20approximations%20suffer%20from%20insufficient%0Aaccuracy%2C%20limited%20scalability%2C%20or%20dependence%20on%20costly%20reference%20data.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20Equivariant%20Graph%20Exchange%20Correlation%0A%28EG-XC%29%2C%20a%20novel%20non-local%20XC%20functional%20based%20on%20equivariant%20graph%20neural%0Anetworks%20%28GNNs%29.%20Where%20previous%20works%20relied%20on%20semi-local%20functionals%20or%0Afixed-size%20descriptors%20of%20the%20density%2C%20we%20compress%20the%20electron%20density%20into%20an%0ASO%283%29-equivariant%20nuclei-centered%20point%20cloud%20for%20efficient%20non-local%0Aatomic-range%20interactions.%20By%20applying%20an%20equivariant%20GNN%20on%20this%20point%20cloud%2C%0Awe%20capture%20molecular-range%20interactions%20in%20a%20scalable%20and%20accurate%20manner.%20To%0Atrain%20EG-XC%2C%20we%20differentiate%20through%20a%20self-consistent%20field%20solver%20requiring%0Aonly%20energy%20targets.%20In%20our%20empirical%20evaluation%2C%20we%20find%20EG-XC%20to%20accurately%0Areconstruct%20%60gold-standard%27%20CCSD%28T%29%20energies%20on%20MD17.%20On%20out-of-distribution%0Aconformations%20of%203BPA%2C%20EG-XC%20reduces%20the%20relative%20MAE%20by%2035%25%20to%2050%25.%0ARemarkably%2C%20EG-XC%20excels%20in%20data%20efficiency%20and%20molecular%20size%20extrapolation%20on%0AQM9%2C%20matching%20force%20fields%20trained%20on%205%20times%20more%20and%20larger%20molecules.%20On%0Aidentical%20training%20sets%2C%20EG-XC%20yields%20on%20average%2051%25%20lower%20MAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07972v3&entry.124074799=Read"},
{"title": "Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning", "author": "Anjiang Wei and Tarun Suresh and Huanmi Tan and Yinglun Xu and Gagandeep Singh and Ke Wang and Alex Aiken", "abstract": "  Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.\n", "link": "http://arxiv.org/abs/2505.11480v1", "date": "2025-05-16", "relevancy": 2.3415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4765}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Assembly%20Code%20Performance%20with%20Large%20Language%20Models%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20Improving%20Assembly%20Code%20Performance%20with%20Large%20Language%20Models%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Anjiang%20Wei%20and%20Tarun%20Suresh%20and%20Huanmi%20Tan%20and%20Yinglun%20Xu%20and%20Gagandeep%20Singh%20and%20Ke%20Wang%20and%20Alex%20Aiken%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20performance%20across%20a%0Awide%20range%20of%20programming%20tasks%2C%20yet%20their%20potential%20for%20code%20optimization%0Aremains%20underexplored.%20This%20work%20investigates%20whether%20LLMs%20can%20optimize%20the%0Aperformance%20of%20assembly%20code%2C%20where%20fine-grained%20control%20over%20execution%20enables%0Aimprovements%20that%20are%20difficult%20to%20express%20in%20high-level%20languages.%20We%20present%0Aa%20reinforcement%20learning%20framework%20that%20trains%20LLMs%20using%20Proximal%20Policy%0AOptimization%20%28PPO%29%2C%20guided%20by%20a%20reward%20function%20that%20considers%20both%20functional%0Acorrectness%2C%20validated%20through%20test%20cases%2C%20and%20execution%20performance%20relative%0Ato%20the%20industry-standard%20compiler%20gcc%20-O3.%20To%20support%20this%20study%2C%20we%20introduce%0Aa%20benchmark%20of%208%2C072%20real-world%20programs.%20Our%20model%2C%20Qwen2.5-Coder-7B-PPO%2C%0Aachieves%2096.0%25%20test%20pass%20rates%20and%20an%20average%20speedup%20of%201.47x%20over%20the%20gcc%20-O3%0Abaseline%2C%20outperforming%20all%2020%20other%20models%20evaluated%2C%20including%0AClaude-3.7-sonnet.%20These%20results%20indicate%20that%20reinforcement%20learning%20can%0Aunlock%20the%20potential%20of%20LLMs%20to%20serve%20as%20effective%20optimizers%20for%20assembly%20code%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Assembly%2520Code%2520Performance%2520with%2520Large%2520Language%2520Models%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DAnjiang%2520Wei%2520and%2520Tarun%2520Suresh%2520and%2520Huanmi%2520Tan%2520and%2520Yinglun%2520Xu%2520and%2520Gagandeep%2520Singh%2520and%2520Ke%2520Wang%2520and%2520Alex%2520Aiken%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520performance%2520across%2520a%250Awide%2520range%2520of%2520programming%2520tasks%252C%2520yet%2520their%2520potential%2520for%2520code%2520optimization%250Aremains%2520underexplored.%2520This%2520work%2520investigates%2520whether%2520LLMs%2520can%2520optimize%2520the%250Aperformance%2520of%2520assembly%2520code%252C%2520where%2520fine-grained%2520control%2520over%2520execution%2520enables%250Aimprovements%2520that%2520are%2520difficult%2520to%2520express%2520in%2520high-level%2520languages.%2520We%2520present%250Aa%2520reinforcement%2520learning%2520framework%2520that%2520trains%2520LLMs%2520using%2520Proximal%2520Policy%250AOptimization%2520%2528PPO%2529%252C%2520guided%2520by%2520a%2520reward%2520function%2520that%2520considers%2520both%2520functional%250Acorrectness%252C%2520validated%2520through%2520test%2520cases%252C%2520and%2520execution%2520performance%2520relative%250Ato%2520the%2520industry-standard%2520compiler%2520gcc%2520-O3.%2520To%2520support%2520this%2520study%252C%2520we%2520introduce%250Aa%2520benchmark%2520of%25208%252C072%2520real-world%2520programs.%2520Our%2520model%252C%2520Qwen2.5-Coder-7B-PPO%252C%250Aachieves%252096.0%2525%2520test%2520pass%2520rates%2520and%2520an%2520average%2520speedup%2520of%25201.47x%2520over%2520the%2520gcc%2520-O3%250Abaseline%252C%2520outperforming%2520all%252020%2520other%2520models%2520evaluated%252C%2520including%250AClaude-3.7-sonnet.%2520These%2520results%2520indicate%2520that%2520reinforcement%2520learning%2520can%250Aunlock%2520the%2520potential%2520of%2520LLMs%2520to%2520serve%2520as%2520effective%2520optimizers%2520for%2520assembly%2520code%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Assembly%20Code%20Performance%20with%20Large%20Language%20Models%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Anjiang%20Wei%20and%20Tarun%20Suresh%20and%20Huanmi%20Tan%20and%20Yinglun%20Xu%20and%20Gagandeep%20Singh%20and%20Ke%20Wang%20and%20Alex%20Aiken&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20performance%20across%20a%0Awide%20range%20of%20programming%20tasks%2C%20yet%20their%20potential%20for%20code%20optimization%0Aremains%20underexplored.%20This%20work%20investigates%20whether%20LLMs%20can%20optimize%20the%0Aperformance%20of%20assembly%20code%2C%20where%20fine-grained%20control%20over%20execution%20enables%0Aimprovements%20that%20are%20difficult%20to%20express%20in%20high-level%20languages.%20We%20present%0Aa%20reinforcement%20learning%20framework%20that%20trains%20LLMs%20using%20Proximal%20Policy%0AOptimization%20%28PPO%29%2C%20guided%20by%20a%20reward%20function%20that%20considers%20both%20functional%0Acorrectness%2C%20validated%20through%20test%20cases%2C%20and%20execution%20performance%20relative%0Ato%20the%20industry-standard%20compiler%20gcc%20-O3.%20To%20support%20this%20study%2C%20we%20introduce%0Aa%20benchmark%20of%208%2C072%20real-world%20programs.%20Our%20model%2C%20Qwen2.5-Coder-7B-PPO%2C%0Aachieves%2096.0%25%20test%20pass%20rates%20and%20an%20average%20speedup%20of%201.47x%20over%20the%20gcc%20-O3%0Abaseline%2C%20outperforming%20all%2020%20other%20models%20evaluated%2C%20including%0AClaude-3.7-sonnet.%20These%20results%20indicate%20that%20reinforcement%20learning%20can%0Aunlock%20the%20potential%20of%20LLMs%20to%20serve%20as%20effective%20optimizers%20for%20assembly%20code%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11480v1&entry.124074799=Read"},
{"title": "Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion", "author": "Jingyang Li and Jiuqian Shang and Yang Chen", "abstract": "  Tensor completion is crucial in many scientific domains with missing data\nproblems. Traditional low-rank tensor models, including CP, Tucker, and\nTensor-Train, exploit low-dimensional structures to recover missing data.\nHowever, these methods often treat all tensor modes symmetrically, failing to\ncapture the unique spatiotemporal patterns inherent in scientific data, where\nthe temporal component exhibits both low-frequency stability and high-frequency\nvariations. To address this, we propose a novel model, \\underline{F}ourier\n\\underline{Lo}w-rank and \\underline{S}parse \\underline{T}ensor (FLoST), which\ndecomposes the tensor along the temporal dimension using a Fourier transform.\nThis approach captures low-frequency components with low-rank matrices and\nhigh-frequency fluctuations with sparsity, resulting in a hybrid structure that\nefficiently models both smooth and localized variations. Compared to the\nwell-known tubal-rank model, which assumes low-rankness across all frequency\ncomponents, FLoST requires significantly fewer parameters, making it\ncomputationally more efficient, particularly when the time dimension is large.\nThrough theoretical analysis and empirical experiments, we demonstrate that\nFLoST outperforms existing tensor completion models in terms of both accuracy\nand computational efficiency, offering a more interpretable solution for\nspatiotemporal data reconstruction.\n", "link": "http://arxiv.org/abs/2505.11261v1", "date": "2025-05-16", "relevancy": 2.3382, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4717}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4705}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fourier%20Low-rank%20and%20Sparse%20Tensor%20for%20Efficient%20Tensor%20Completion&body=Title%3A%20Fourier%20Low-rank%20and%20Sparse%20Tensor%20for%20Efficient%20Tensor%20Completion%0AAuthor%3A%20Jingyang%20Li%20and%20Jiuqian%20Shang%20and%20Yang%20Chen%0AAbstract%3A%20%20%20Tensor%20completion%20is%20crucial%20in%20many%20scientific%20domains%20with%20missing%20data%0Aproblems.%20Traditional%20low-rank%20tensor%20models%2C%20including%20CP%2C%20Tucker%2C%20and%0ATensor-Train%2C%20exploit%20low-dimensional%20structures%20to%20recover%20missing%20data.%0AHowever%2C%20these%20methods%20often%20treat%20all%20tensor%20modes%20symmetrically%2C%20failing%20to%0Acapture%20the%20unique%20spatiotemporal%20patterns%20inherent%20in%20scientific%20data%2C%20where%0Athe%20temporal%20component%20exhibits%20both%20low-frequency%20stability%20and%20high-frequency%0Avariations.%20To%20address%20this%2C%20we%20propose%20a%20novel%20model%2C%20%5Cunderline%7BF%7Dourier%0A%5Cunderline%7BLo%7Dw-rank%20and%20%5Cunderline%7BS%7Dparse%20%5Cunderline%7BT%7Densor%20%28FLoST%29%2C%20which%0Adecomposes%20the%20tensor%20along%20the%20temporal%20dimension%20using%20a%20Fourier%20transform.%0AThis%20approach%20captures%20low-frequency%20components%20with%20low-rank%20matrices%20and%0Ahigh-frequency%20fluctuations%20with%20sparsity%2C%20resulting%20in%20a%20hybrid%20structure%20that%0Aefficiently%20models%20both%20smooth%20and%20localized%20variations.%20Compared%20to%20the%0Awell-known%20tubal-rank%20model%2C%20which%20assumes%20low-rankness%20across%20all%20frequency%0Acomponents%2C%20FLoST%20requires%20significantly%20fewer%20parameters%2C%20making%20it%0Acomputationally%20more%20efficient%2C%20particularly%20when%20the%20time%20dimension%20is%20large.%0AThrough%20theoretical%20analysis%20and%20empirical%20experiments%2C%20we%20demonstrate%20that%0AFLoST%20outperforms%20existing%20tensor%20completion%20models%20in%20terms%20of%20both%20accuracy%0Aand%20computational%20efficiency%2C%20offering%20a%20more%20interpretable%20solution%20for%0Aspatiotemporal%20data%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFourier%2520Low-rank%2520and%2520Sparse%2520Tensor%2520for%2520Efficient%2520Tensor%2520Completion%26entry.906535625%3DJingyang%2520Li%2520and%2520Jiuqian%2520Shang%2520and%2520Yang%2520Chen%26entry.1292438233%3D%2520%2520Tensor%2520completion%2520is%2520crucial%2520in%2520many%2520scientific%2520domains%2520with%2520missing%2520data%250Aproblems.%2520Traditional%2520low-rank%2520tensor%2520models%252C%2520including%2520CP%252C%2520Tucker%252C%2520and%250ATensor-Train%252C%2520exploit%2520low-dimensional%2520structures%2520to%2520recover%2520missing%2520data.%250AHowever%252C%2520these%2520methods%2520often%2520treat%2520all%2520tensor%2520modes%2520symmetrically%252C%2520failing%2520to%250Acapture%2520the%2520unique%2520spatiotemporal%2520patterns%2520inherent%2520in%2520scientific%2520data%252C%2520where%250Athe%2520temporal%2520component%2520exhibits%2520both%2520low-frequency%2520stability%2520and%2520high-frequency%250Avariations.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520model%252C%2520%255Cunderline%257BF%257Dourier%250A%255Cunderline%257BLo%257Dw-rank%2520and%2520%255Cunderline%257BS%257Dparse%2520%255Cunderline%257BT%257Densor%2520%2528FLoST%2529%252C%2520which%250Adecomposes%2520the%2520tensor%2520along%2520the%2520temporal%2520dimension%2520using%2520a%2520Fourier%2520transform.%250AThis%2520approach%2520captures%2520low-frequency%2520components%2520with%2520low-rank%2520matrices%2520and%250Ahigh-frequency%2520fluctuations%2520with%2520sparsity%252C%2520resulting%2520in%2520a%2520hybrid%2520structure%2520that%250Aefficiently%2520models%2520both%2520smooth%2520and%2520localized%2520variations.%2520Compared%2520to%2520the%250Awell-known%2520tubal-rank%2520model%252C%2520which%2520assumes%2520low-rankness%2520across%2520all%2520frequency%250Acomponents%252C%2520FLoST%2520requires%2520significantly%2520fewer%2520parameters%252C%2520making%2520it%250Acomputationally%2520more%2520efficient%252C%2520particularly%2520when%2520the%2520time%2520dimension%2520is%2520large.%250AThrough%2520theoretical%2520analysis%2520and%2520empirical%2520experiments%252C%2520we%2520demonstrate%2520that%250AFLoST%2520outperforms%2520existing%2520tensor%2520completion%2520models%2520in%2520terms%2520of%2520both%2520accuracy%250Aand%2520computational%2520efficiency%252C%2520offering%2520a%2520more%2520interpretable%2520solution%2520for%250Aspatiotemporal%2520data%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fourier%20Low-rank%20and%20Sparse%20Tensor%20for%20Efficient%20Tensor%20Completion&entry.906535625=Jingyang%20Li%20and%20Jiuqian%20Shang%20and%20Yang%20Chen&entry.1292438233=%20%20Tensor%20completion%20is%20crucial%20in%20many%20scientific%20domains%20with%20missing%20data%0Aproblems.%20Traditional%20low-rank%20tensor%20models%2C%20including%20CP%2C%20Tucker%2C%20and%0ATensor-Train%2C%20exploit%20low-dimensional%20structures%20to%20recover%20missing%20data.%0AHowever%2C%20these%20methods%20often%20treat%20all%20tensor%20modes%20symmetrically%2C%20failing%20to%0Acapture%20the%20unique%20spatiotemporal%20patterns%20inherent%20in%20scientific%20data%2C%20where%0Athe%20temporal%20component%20exhibits%20both%20low-frequency%20stability%20and%20high-frequency%0Avariations.%20To%20address%20this%2C%20we%20propose%20a%20novel%20model%2C%20%5Cunderline%7BF%7Dourier%0A%5Cunderline%7BLo%7Dw-rank%20and%20%5Cunderline%7BS%7Dparse%20%5Cunderline%7BT%7Densor%20%28FLoST%29%2C%20which%0Adecomposes%20the%20tensor%20along%20the%20temporal%20dimension%20using%20a%20Fourier%20transform.%0AThis%20approach%20captures%20low-frequency%20components%20with%20low-rank%20matrices%20and%0Ahigh-frequency%20fluctuations%20with%20sparsity%2C%20resulting%20in%20a%20hybrid%20structure%20that%0Aefficiently%20models%20both%20smooth%20and%20localized%20variations.%20Compared%20to%20the%0Awell-known%20tubal-rank%20model%2C%20which%20assumes%20low-rankness%20across%20all%20frequency%0Acomponents%2C%20FLoST%20requires%20significantly%20fewer%20parameters%2C%20making%20it%0Acomputationally%20more%20efficient%2C%20particularly%20when%20the%20time%20dimension%20is%20large.%0AThrough%20theoretical%20analysis%20and%20empirical%20experiments%2C%20we%20demonstrate%20that%0AFLoST%20outperforms%20existing%20tensor%20completion%20models%20in%20terms%20of%20both%20accuracy%0Aand%20computational%20efficiency%2C%20offering%20a%20more%20interpretable%20solution%20for%0Aspatiotemporal%20data%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11261v1&entry.124074799=Read"},
{"title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "author": "Yansheng Qiu and Haoquan Zhang and Zhaopan Xu and Ming Li and Diping Song and Zheng Wang and Kaipeng Zhang", "abstract": "  Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery.\n", "link": "http://arxiv.org/abs/2504.14191v2", "date": "2025-05-16", "relevancy": 2.3363, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4748}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4635}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Idea%20Bench%202025%3A%20AI%20Research%20Idea%20Generation%20Benchmark&body=Title%3A%20AI%20Idea%20Bench%202025%3A%20AI%20Research%20Idea%20Generation%20Benchmark%0AAuthor%3A%20Yansheng%20Qiu%20and%20Haoquan%20Zhang%20and%20Zhaopan%20Xu%20and%20Ming%20Li%20and%20Diping%20Song%20and%20Zheng%20Wang%20and%20Kaipeng%20Zhang%0AAbstract%3A%20%20%20Large-scale%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20human-AI%20interaction%0Aand%20achieved%20significant%20success%20in%20the%20generation%20of%20novel%20ideas.%20However%2C%0Acurrent%20assessments%20of%20idea%20generation%20overlook%20crucial%20factors%20such%20as%0Aknowledge%20leakage%20in%20LLMs%2C%20the%20absence%20of%20open-ended%20benchmarks%20with%20grounded%0Atruth%2C%20and%20the%20limited%20scope%20of%20feasibility%20analysis%20constrained%20by%20prompt%0Adesign.%20These%20limitations%20hinder%20the%20potential%20of%20uncovering%20groundbreaking%0Aresearch%20ideas.%20In%20this%20paper%2C%20we%20present%20AI%20Idea%20Bench%202025%2C%20a%20framework%0Adesigned%20to%20quantitatively%20evaluate%20and%20compare%20the%20ideas%20generated%20by%20LLMs%0Awithin%20the%20domain%20of%20AI%20research%20from%20diverse%20perspectives.%20The%20framework%0Acomprises%20a%20comprehensive%20dataset%20of%203%2C495%20AI%20papers%20and%20their%20associated%0Ainspired%20works%2C%20along%20with%20a%20robust%20evaluation%20methodology.%20This%20evaluation%0Asystem%20gauges%20idea%20quality%20in%20two%20dimensions%3A%20alignment%20with%20the%20ground-truth%0Acontent%20of%20the%20original%20papers%20and%20judgment%20based%20on%20general%20reference%0Amaterial.%20AI%20Idea%20Bench%202025%27s%20benchmarking%20system%20stands%20to%20be%20an%20invaluable%0Aresource%20for%20assessing%20and%20comparing%20idea-generation%20techniques%2C%20thereby%0Afacilitating%20the%20automation%20of%20scientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14191v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Idea%2520Bench%25202025%253A%2520AI%2520Research%2520Idea%2520Generation%2520Benchmark%26entry.906535625%3DYansheng%2520Qiu%2520and%2520Haoquan%2520Zhang%2520and%2520Zhaopan%2520Xu%2520and%2520Ming%2520Li%2520and%2520Diping%2520Song%2520and%2520Zheng%2520Wang%2520and%2520Kaipeng%2520Zhang%26entry.1292438233%3D%2520%2520Large-scale%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520human-AI%2520interaction%250Aand%2520achieved%2520significant%2520success%2520in%2520the%2520generation%2520of%2520novel%2520ideas.%2520However%252C%250Acurrent%2520assessments%2520of%2520idea%2520generation%2520overlook%2520crucial%2520factors%2520such%2520as%250Aknowledge%2520leakage%2520in%2520LLMs%252C%2520the%2520absence%2520of%2520open-ended%2520benchmarks%2520with%2520grounded%250Atruth%252C%2520and%2520the%2520limited%2520scope%2520of%2520feasibility%2520analysis%2520constrained%2520by%2520prompt%250Adesign.%2520These%2520limitations%2520hinder%2520the%2520potential%2520of%2520uncovering%2520groundbreaking%250Aresearch%2520ideas.%2520In%2520this%2520paper%252C%2520we%2520present%2520AI%2520Idea%2520Bench%25202025%252C%2520a%2520framework%250Adesigned%2520to%2520quantitatively%2520evaluate%2520and%2520compare%2520the%2520ideas%2520generated%2520by%2520LLMs%250Awithin%2520the%2520domain%2520of%2520AI%2520research%2520from%2520diverse%2520perspectives.%2520The%2520framework%250Acomprises%2520a%2520comprehensive%2520dataset%2520of%25203%252C495%2520AI%2520papers%2520and%2520their%2520associated%250Ainspired%2520works%252C%2520along%2520with%2520a%2520robust%2520evaluation%2520methodology.%2520This%2520evaluation%250Asystem%2520gauges%2520idea%2520quality%2520in%2520two%2520dimensions%253A%2520alignment%2520with%2520the%2520ground-truth%250Acontent%2520of%2520the%2520original%2520papers%2520and%2520judgment%2520based%2520on%2520general%2520reference%250Amaterial.%2520AI%2520Idea%2520Bench%25202025%2527s%2520benchmarking%2520system%2520stands%2520to%2520be%2520an%2520invaluable%250Aresource%2520for%2520assessing%2520and%2520comparing%2520idea-generation%2520techniques%252C%2520thereby%250Afacilitating%2520the%2520automation%2520of%2520scientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14191v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Idea%20Bench%202025%3A%20AI%20Research%20Idea%20Generation%20Benchmark&entry.906535625=Yansheng%20Qiu%20and%20Haoquan%20Zhang%20and%20Zhaopan%20Xu%20and%20Ming%20Li%20and%20Diping%20Song%20and%20Zheng%20Wang%20and%20Kaipeng%20Zhang&entry.1292438233=%20%20Large-scale%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20human-AI%20interaction%0Aand%20achieved%20significant%20success%20in%20the%20generation%20of%20novel%20ideas.%20However%2C%0Acurrent%20assessments%20of%20idea%20generation%20overlook%20crucial%20factors%20such%20as%0Aknowledge%20leakage%20in%20LLMs%2C%20the%20absence%20of%20open-ended%20benchmarks%20with%20grounded%0Atruth%2C%20and%20the%20limited%20scope%20of%20feasibility%20analysis%20constrained%20by%20prompt%0Adesign.%20These%20limitations%20hinder%20the%20potential%20of%20uncovering%20groundbreaking%0Aresearch%20ideas.%20In%20this%20paper%2C%20we%20present%20AI%20Idea%20Bench%202025%2C%20a%20framework%0Adesigned%20to%20quantitatively%20evaluate%20and%20compare%20the%20ideas%20generated%20by%20LLMs%0Awithin%20the%20domain%20of%20AI%20research%20from%20diverse%20perspectives.%20The%20framework%0Acomprises%20a%20comprehensive%20dataset%20of%203%2C495%20AI%20papers%20and%20their%20associated%0Ainspired%20works%2C%20along%20with%20a%20robust%20evaluation%20methodology.%20This%20evaluation%0Asystem%20gauges%20idea%20quality%20in%20two%20dimensions%3A%20alignment%20with%20the%20ground-truth%0Acontent%20of%20the%20original%20papers%20and%20judgment%20based%20on%20general%20reference%0Amaterial.%20AI%20Idea%20Bench%202025%27s%20benchmarking%20system%20stands%20to%20be%20an%20invaluable%0Aresource%20for%20assessing%20and%20comparing%20idea-generation%20techniques%2C%20thereby%0Afacilitating%20the%20automation%20of%20scientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14191v2&entry.124074799=Read"},
{"title": "LGBQPC: Local Granular-Ball Quality Peaks Clustering", "author": "Zihang Jia and Zhen Zhang and Witold Pedrycz", "abstract": "  The density peaks clustering (DPC) algorithm has attracted considerable\nattention for its ability to detect arbitrarily shaped clusters based on a\nsimple yet effective assumption. Recent advancements integrating granular-ball\n(GB) computing with DPC have led to the GB-based DPC (GBDPC) algorithm, which\nimproves computational efficiency. However, GBDPC demonstrates limitations when\nhandling complex clustering tasks, particularly those involving data with\ncomplex manifold structures or non-uniform density distributions. To overcome\nthese challenges, this paper proposes the local GB quality peaks clustering\n(LGBQPC) algorithm, which offers comprehensive improvements to GBDPC in both GB\ngeneration and clustering processes based on the principle of justifiable\ngranularity (POJG). Firstly, an improved GB generation method, termed GB-POJG+,\nis developed, which systematically refines the original GB-POJG in four key\naspects: the objective function, termination criterion for GB division,\ndefinition of abnormal GB, and granularity level adaptation strategy. GB-POJG+\nsimplifies parameter configuration by requiring only a single penalty\ncoefficient and ensures high-quality GB generation while maintaining the number\nof generated GBs within an acceptable range. In the clustering phase, two key\ninnovations are introduced based on the GB k-nearest neighbor graph: relative\nGB quality for density estimation and geodesic distance for GB distance metric.\nThese modifications substantially improve the performance of GBDPC on datasets\nwith complex manifold structures or non-uniform density distributions.\nExtensive numerical experiments on 40 benchmark datasets, including both\nsynthetic and publicly available datasets, validate the superior performance of\nthe proposed LGBQPC algorithm.\n", "link": "http://arxiv.org/abs/2505.11359v1", "date": "2025-05-16", "relevancy": 2.3361, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4734}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4729}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LGBQPC%3A%20Local%20Granular-Ball%20Quality%20Peaks%20Clustering&body=Title%3A%20LGBQPC%3A%20Local%20Granular-Ball%20Quality%20Peaks%20Clustering%0AAuthor%3A%20Zihang%20Jia%20and%20Zhen%20Zhang%20and%20Witold%20Pedrycz%0AAbstract%3A%20%20%20The%20density%20peaks%20clustering%20%28DPC%29%20algorithm%20has%20attracted%20considerable%0Aattention%20for%20its%20ability%20to%20detect%20arbitrarily%20shaped%20clusters%20based%20on%20a%0Asimple%20yet%20effective%20assumption.%20Recent%20advancements%20integrating%20granular-ball%0A%28GB%29%20computing%20with%20DPC%20have%20led%20to%20the%20GB-based%20DPC%20%28GBDPC%29%20algorithm%2C%20which%0Aimproves%20computational%20efficiency.%20However%2C%20GBDPC%20demonstrates%20limitations%20when%0Ahandling%20complex%20clustering%20tasks%2C%20particularly%20those%20involving%20data%20with%0Acomplex%20manifold%20structures%20or%20non-uniform%20density%20distributions.%20To%20overcome%0Athese%20challenges%2C%20this%20paper%20proposes%20the%20local%20GB%20quality%20peaks%20clustering%0A%28LGBQPC%29%20algorithm%2C%20which%20offers%20comprehensive%20improvements%20to%20GBDPC%20in%20both%20GB%0Ageneration%20and%20clustering%20processes%20based%20on%20the%20principle%20of%20justifiable%0Agranularity%20%28POJG%29.%20Firstly%2C%20an%20improved%20GB%20generation%20method%2C%20termed%20GB-POJG%2B%2C%0Ais%20developed%2C%20which%20systematically%20refines%20the%20original%20GB-POJG%20in%20four%20key%0Aaspects%3A%20the%20objective%20function%2C%20termination%20criterion%20for%20GB%20division%2C%0Adefinition%20of%20abnormal%20GB%2C%20and%20granularity%20level%20adaptation%20strategy.%20GB-POJG%2B%0Asimplifies%20parameter%20configuration%20by%20requiring%20only%20a%20single%20penalty%0Acoefficient%20and%20ensures%20high-quality%20GB%20generation%20while%20maintaining%20the%20number%0Aof%20generated%20GBs%20within%20an%20acceptable%20range.%20In%20the%20clustering%20phase%2C%20two%20key%0Ainnovations%20are%20introduced%20based%20on%20the%20GB%20k-nearest%20neighbor%20graph%3A%20relative%0AGB%20quality%20for%20density%20estimation%20and%20geodesic%20distance%20for%20GB%20distance%20metric.%0AThese%20modifications%20substantially%20improve%20the%20performance%20of%20GBDPC%20on%20datasets%0Awith%20complex%20manifold%20structures%20or%20non-uniform%20density%20distributions.%0AExtensive%20numerical%20experiments%20on%2040%20benchmark%20datasets%2C%20including%20both%0Asynthetic%20and%20publicly%20available%20datasets%2C%20validate%20the%20superior%20performance%20of%0Athe%20proposed%20LGBQPC%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLGBQPC%253A%2520Local%2520Granular-Ball%2520Quality%2520Peaks%2520Clustering%26entry.906535625%3DZihang%2520Jia%2520and%2520Zhen%2520Zhang%2520and%2520Witold%2520Pedrycz%26entry.1292438233%3D%2520%2520The%2520density%2520peaks%2520clustering%2520%2528DPC%2529%2520algorithm%2520has%2520attracted%2520considerable%250Aattention%2520for%2520its%2520ability%2520to%2520detect%2520arbitrarily%2520shaped%2520clusters%2520based%2520on%2520a%250Asimple%2520yet%2520effective%2520assumption.%2520Recent%2520advancements%2520integrating%2520granular-ball%250A%2528GB%2529%2520computing%2520with%2520DPC%2520have%2520led%2520to%2520the%2520GB-based%2520DPC%2520%2528GBDPC%2529%2520algorithm%252C%2520which%250Aimproves%2520computational%2520efficiency.%2520However%252C%2520GBDPC%2520demonstrates%2520limitations%2520when%250Ahandling%2520complex%2520clustering%2520tasks%252C%2520particularly%2520those%2520involving%2520data%2520with%250Acomplex%2520manifold%2520structures%2520or%2520non-uniform%2520density%2520distributions.%2520To%2520overcome%250Athese%2520challenges%252C%2520this%2520paper%2520proposes%2520the%2520local%2520GB%2520quality%2520peaks%2520clustering%250A%2528LGBQPC%2529%2520algorithm%252C%2520which%2520offers%2520comprehensive%2520improvements%2520to%2520GBDPC%2520in%2520both%2520GB%250Ageneration%2520and%2520clustering%2520processes%2520based%2520on%2520the%2520principle%2520of%2520justifiable%250Agranularity%2520%2528POJG%2529.%2520Firstly%252C%2520an%2520improved%2520GB%2520generation%2520method%252C%2520termed%2520GB-POJG%252B%252C%250Ais%2520developed%252C%2520which%2520systematically%2520refines%2520the%2520original%2520GB-POJG%2520in%2520four%2520key%250Aaspects%253A%2520the%2520objective%2520function%252C%2520termination%2520criterion%2520for%2520GB%2520division%252C%250Adefinition%2520of%2520abnormal%2520GB%252C%2520and%2520granularity%2520level%2520adaptation%2520strategy.%2520GB-POJG%252B%250Asimplifies%2520parameter%2520configuration%2520by%2520requiring%2520only%2520a%2520single%2520penalty%250Acoefficient%2520and%2520ensures%2520high-quality%2520GB%2520generation%2520while%2520maintaining%2520the%2520number%250Aof%2520generated%2520GBs%2520within%2520an%2520acceptable%2520range.%2520In%2520the%2520clustering%2520phase%252C%2520two%2520key%250Ainnovations%2520are%2520introduced%2520based%2520on%2520the%2520GB%2520k-nearest%2520neighbor%2520graph%253A%2520relative%250AGB%2520quality%2520for%2520density%2520estimation%2520and%2520geodesic%2520distance%2520for%2520GB%2520distance%2520metric.%250AThese%2520modifications%2520substantially%2520improve%2520the%2520performance%2520of%2520GBDPC%2520on%2520datasets%250Awith%2520complex%2520manifold%2520structures%2520or%2520non-uniform%2520density%2520distributions.%250AExtensive%2520numerical%2520experiments%2520on%252040%2520benchmark%2520datasets%252C%2520including%2520both%250Asynthetic%2520and%2520publicly%2520available%2520datasets%252C%2520validate%2520the%2520superior%2520performance%2520of%250Athe%2520proposed%2520LGBQPC%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LGBQPC%3A%20Local%20Granular-Ball%20Quality%20Peaks%20Clustering&entry.906535625=Zihang%20Jia%20and%20Zhen%20Zhang%20and%20Witold%20Pedrycz&entry.1292438233=%20%20The%20density%20peaks%20clustering%20%28DPC%29%20algorithm%20has%20attracted%20considerable%0Aattention%20for%20its%20ability%20to%20detect%20arbitrarily%20shaped%20clusters%20based%20on%20a%0Asimple%20yet%20effective%20assumption.%20Recent%20advancements%20integrating%20granular-ball%0A%28GB%29%20computing%20with%20DPC%20have%20led%20to%20the%20GB-based%20DPC%20%28GBDPC%29%20algorithm%2C%20which%0Aimproves%20computational%20efficiency.%20However%2C%20GBDPC%20demonstrates%20limitations%20when%0Ahandling%20complex%20clustering%20tasks%2C%20particularly%20those%20involving%20data%20with%0Acomplex%20manifold%20structures%20or%20non-uniform%20density%20distributions.%20To%20overcome%0Athese%20challenges%2C%20this%20paper%20proposes%20the%20local%20GB%20quality%20peaks%20clustering%0A%28LGBQPC%29%20algorithm%2C%20which%20offers%20comprehensive%20improvements%20to%20GBDPC%20in%20both%20GB%0Ageneration%20and%20clustering%20processes%20based%20on%20the%20principle%20of%20justifiable%0Agranularity%20%28POJG%29.%20Firstly%2C%20an%20improved%20GB%20generation%20method%2C%20termed%20GB-POJG%2B%2C%0Ais%20developed%2C%20which%20systematically%20refines%20the%20original%20GB-POJG%20in%20four%20key%0Aaspects%3A%20the%20objective%20function%2C%20termination%20criterion%20for%20GB%20division%2C%0Adefinition%20of%20abnormal%20GB%2C%20and%20granularity%20level%20adaptation%20strategy.%20GB-POJG%2B%0Asimplifies%20parameter%20configuration%20by%20requiring%20only%20a%20single%20penalty%0Acoefficient%20and%20ensures%20high-quality%20GB%20generation%20while%20maintaining%20the%20number%0Aof%20generated%20GBs%20within%20an%20acceptable%20range.%20In%20the%20clustering%20phase%2C%20two%20key%0Ainnovations%20are%20introduced%20based%20on%20the%20GB%20k-nearest%20neighbor%20graph%3A%20relative%0AGB%20quality%20for%20density%20estimation%20and%20geodesic%20distance%20for%20GB%20distance%20metric.%0AThese%20modifications%20substantially%20improve%20the%20performance%20of%20GBDPC%20on%20datasets%0Awith%20complex%20manifold%20structures%20or%20non-uniform%20density%20distributions.%0AExtensive%20numerical%20experiments%20on%2040%20benchmark%20datasets%2C%20including%20both%0Asynthetic%20and%20publicly%20available%20datasets%2C%20validate%20the%20superior%20performance%20of%0Athe%20proposed%20LGBQPC%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11359v1&entry.124074799=Read"},
{"title": "BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks", "author": "Aditya Panangat", "abstract": "  Over the past decade, the use of machine learning has increased\nexponentially. Models are far more complex than ever before, growing to\ngargantuan sizes and housing millions of weights. Unfortunately, the fact that\nlarge models have become the state of the art means that it often costs\nmillions of dollars to train and operate them. These expenses not only hurt\ncompanies but also bar non-wealthy individuals from contributing to new\ndevelopments and force consumers to pay greater prices for AI. Current methods\nused to prune models, such as iterative magnitude pruning, have shown great\naccuracy but require an iterative training sequence that is incredibly\ncomputationally and environmentally taxing. To solve this problem, BINGO is\nintroduced. BINGO, during the training pass, studies specific subsets of a\nneural network one at a time to gauge how significant of a role each weight\nplays in contributing to a network's accuracy. By the time training is done,\nBINGO generates a significance score for each weight, allowing for\ninsignificant weights to be pruned in one shot. BINGO provides an\naccuracy-preserving pruning technique that is less computationally intensive\nthan current methods, allowing for a world where AI growth does not have to\nmean model growth, as well.\n", "link": "http://arxiv.org/abs/2505.09864v2", "date": "2025-05-16", "relevancy": 2.3336, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4947}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4644}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BINGO%3A%20A%20Novel%20Pruning%20Mechanism%20to%20Reduce%20the%20Size%20of%20Neural%20Networks&body=Title%3A%20BINGO%3A%20A%20Novel%20Pruning%20Mechanism%20to%20Reduce%20the%20Size%20of%20Neural%20Networks%0AAuthor%3A%20Aditya%20Panangat%0AAbstract%3A%20%20%20Over%20the%20past%20decade%2C%20the%20use%20of%20machine%20learning%20has%20increased%0Aexponentially.%20Models%20are%20far%20more%20complex%20than%20ever%20before%2C%20growing%20to%0Agargantuan%20sizes%20and%20housing%20millions%20of%20weights.%20Unfortunately%2C%20the%20fact%20that%0Alarge%20models%20have%20become%20the%20state%20of%20the%20art%20means%20that%20it%20often%20costs%0Amillions%20of%20dollars%20to%20train%20and%20operate%20them.%20These%20expenses%20not%20only%20hurt%0Acompanies%20but%20also%20bar%20non-wealthy%20individuals%20from%20contributing%20to%20new%0Adevelopments%20and%20force%20consumers%20to%20pay%20greater%20prices%20for%20AI.%20Current%20methods%0Aused%20to%20prune%20models%2C%20such%20as%20iterative%20magnitude%20pruning%2C%20have%20shown%20great%0Aaccuracy%20but%20require%20an%20iterative%20training%20sequence%20that%20is%20incredibly%0Acomputationally%20and%20environmentally%20taxing.%20To%20solve%20this%20problem%2C%20BINGO%20is%0Aintroduced.%20BINGO%2C%20during%20the%20training%20pass%2C%20studies%20specific%20subsets%20of%20a%0Aneural%20network%20one%20at%20a%20time%20to%20gauge%20how%20significant%20of%20a%20role%20each%20weight%0Aplays%20in%20contributing%20to%20a%20network%27s%20accuracy.%20By%20the%20time%20training%20is%20done%2C%0ABINGO%20generates%20a%20significance%20score%20for%20each%20weight%2C%20allowing%20for%0Ainsignificant%20weights%20to%20be%20pruned%20in%20one%20shot.%20BINGO%20provides%20an%0Aaccuracy-preserving%20pruning%20technique%20that%20is%20less%20computationally%20intensive%0Athan%20current%20methods%2C%20allowing%20for%20a%20world%20where%20AI%20growth%20does%20not%20have%20to%0Amean%20model%20growth%2C%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBINGO%253A%2520A%2520Novel%2520Pruning%2520Mechanism%2520to%2520Reduce%2520the%2520Size%2520of%2520Neural%2520Networks%26entry.906535625%3DAditya%2520Panangat%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decade%252C%2520the%2520use%2520of%2520machine%2520learning%2520has%2520increased%250Aexponentially.%2520Models%2520are%2520far%2520more%2520complex%2520than%2520ever%2520before%252C%2520growing%2520to%250Agargantuan%2520sizes%2520and%2520housing%2520millions%2520of%2520weights.%2520Unfortunately%252C%2520the%2520fact%2520that%250Alarge%2520models%2520have%2520become%2520the%2520state%2520of%2520the%2520art%2520means%2520that%2520it%2520often%2520costs%250Amillions%2520of%2520dollars%2520to%2520train%2520and%2520operate%2520them.%2520These%2520expenses%2520not%2520only%2520hurt%250Acompanies%2520but%2520also%2520bar%2520non-wealthy%2520individuals%2520from%2520contributing%2520to%2520new%250Adevelopments%2520and%2520force%2520consumers%2520to%2520pay%2520greater%2520prices%2520for%2520AI.%2520Current%2520methods%250Aused%2520to%2520prune%2520models%252C%2520such%2520as%2520iterative%2520magnitude%2520pruning%252C%2520have%2520shown%2520great%250Aaccuracy%2520but%2520require%2520an%2520iterative%2520training%2520sequence%2520that%2520is%2520incredibly%250Acomputationally%2520and%2520environmentally%2520taxing.%2520To%2520solve%2520this%2520problem%252C%2520BINGO%2520is%250Aintroduced.%2520BINGO%252C%2520during%2520the%2520training%2520pass%252C%2520studies%2520specific%2520subsets%2520of%2520a%250Aneural%2520network%2520one%2520at%2520a%2520time%2520to%2520gauge%2520how%2520significant%2520of%2520a%2520role%2520each%2520weight%250Aplays%2520in%2520contributing%2520to%2520a%2520network%2527s%2520accuracy.%2520By%2520the%2520time%2520training%2520is%2520done%252C%250ABINGO%2520generates%2520a%2520significance%2520score%2520for%2520each%2520weight%252C%2520allowing%2520for%250Ainsignificant%2520weights%2520to%2520be%2520pruned%2520in%2520one%2520shot.%2520BINGO%2520provides%2520an%250Aaccuracy-preserving%2520pruning%2520technique%2520that%2520is%2520less%2520computationally%2520intensive%250Athan%2520current%2520methods%252C%2520allowing%2520for%2520a%2520world%2520where%2520AI%2520growth%2520does%2520not%2520have%2520to%250Amean%2520model%2520growth%252C%2520as%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BINGO%3A%20A%20Novel%20Pruning%20Mechanism%20to%20Reduce%20the%20Size%20of%20Neural%20Networks&entry.906535625=Aditya%20Panangat&entry.1292438233=%20%20Over%20the%20past%20decade%2C%20the%20use%20of%20machine%20learning%20has%20increased%0Aexponentially.%20Models%20are%20far%20more%20complex%20than%20ever%20before%2C%20growing%20to%0Agargantuan%20sizes%20and%20housing%20millions%20of%20weights.%20Unfortunately%2C%20the%20fact%20that%0Alarge%20models%20have%20become%20the%20state%20of%20the%20art%20means%20that%20it%20often%20costs%0Amillions%20of%20dollars%20to%20train%20and%20operate%20them.%20These%20expenses%20not%20only%20hurt%0Acompanies%20but%20also%20bar%20non-wealthy%20individuals%20from%20contributing%20to%20new%0Adevelopments%20and%20force%20consumers%20to%20pay%20greater%20prices%20for%20AI.%20Current%20methods%0Aused%20to%20prune%20models%2C%20such%20as%20iterative%20magnitude%20pruning%2C%20have%20shown%20great%0Aaccuracy%20but%20require%20an%20iterative%20training%20sequence%20that%20is%20incredibly%0Acomputationally%20and%20environmentally%20taxing.%20To%20solve%20this%20problem%2C%20BINGO%20is%0Aintroduced.%20BINGO%2C%20during%20the%20training%20pass%2C%20studies%20specific%20subsets%20of%20a%0Aneural%20network%20one%20at%20a%20time%20to%20gauge%20how%20significant%20of%20a%20role%20each%20weight%0Aplays%20in%20contributing%20to%20a%20network%27s%20accuracy.%20By%20the%20time%20training%20is%20done%2C%0ABINGO%20generates%20a%20significance%20score%20for%20each%20weight%2C%20allowing%20for%0Ainsignificant%20weights%20to%20be%20pruned%20in%20one%20shot.%20BINGO%20provides%20an%0Aaccuracy-preserving%20pruning%20technique%20that%20is%20less%20computationally%20intensive%0Athan%20current%20methods%2C%20allowing%20for%20a%20world%20where%20AI%20growth%20does%20not%20have%20to%0Amean%20model%20growth%2C%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09864v2&entry.124074799=Read"},
{"title": "Efficient and Comprehensive Feature Extraction in Large Vision-Language\n  Model for Pathology Analysis", "author": "Shengxuming Zhang and Weihan Li and Tianhong Gao and Jiacong Hu and Haoming Luo and Xiuming Zhang and Jing Zhang and Mingli Song and Zunlei Feng", "abstract": "  Pathological diagnosis is vital for determining disease characteristics,\nguiding treatment, and assessing prognosis, relying heavily on detailed,\nmulti-scale analysis of high-resolution whole slide images (WSI). However,\nexisting large vision-language models (LVLMs) are limited by input resolution\nconstraints, hindering their efficiency and accuracy in pathology image\nanalysis. To overcome these issues, we propose two innovative strategies: the\nmixed task-guided feature enhancement, which directs feature extraction toward\nlesion-related details across scales, and the prompt-guided detail feature\ncompletion, which integrates coarse- and fine-grained features from WSI based\non specific prompts without compromising inference speed. Leveraging a\ncomprehensive dataset of 490K samples from diverse pathology tasks, we trained\nthe pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate\nthat this model significantly outperforms existing methods in diagnostic\naccuracy and efficiency, providing an interactive, clinically aligned approach\nfor auxiliary diagnosis in a wide range of pathology applications.\n", "link": "http://arxiv.org/abs/2412.09521v3", "date": "2025-05-16", "relevancy": 2.317, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5946}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Comprehensive%20Feature%20Extraction%20in%20Large%20Vision-Language%0A%20%20Model%20for%20Pathology%20Analysis&body=Title%3A%20Efficient%20and%20Comprehensive%20Feature%20Extraction%20in%20Large%20Vision-Language%0A%20%20Model%20for%20Pathology%20Analysis%0AAuthor%3A%20Shengxuming%20Zhang%20and%20Weihan%20Li%20and%20Tianhong%20Gao%20and%20Jiacong%20Hu%20and%20Haoming%20Luo%20and%20Xiuming%20Zhang%20and%20Jing%20Zhang%20and%20Mingli%20Song%20and%20Zunlei%20Feng%0AAbstract%3A%20%20%20Pathological%20diagnosis%20is%20vital%20for%20determining%20disease%20characteristics%2C%0Aguiding%20treatment%2C%20and%20assessing%20prognosis%2C%20relying%20heavily%20on%20detailed%2C%0Amulti-scale%20analysis%20of%20high-resolution%20whole%20slide%20images%20%28WSI%29.%20However%2C%0Aexisting%20large%20vision-language%20models%20%28LVLMs%29%20are%20limited%20by%20input%20resolution%0Aconstraints%2C%20hindering%20their%20efficiency%20and%20accuracy%20in%20pathology%20image%0Aanalysis.%20To%20overcome%20these%20issues%2C%20we%20propose%20two%20innovative%20strategies%3A%20the%0Amixed%20task-guided%20feature%20enhancement%2C%20which%20directs%20feature%20extraction%20toward%0Alesion-related%20details%20across%20scales%2C%20and%20the%20prompt-guided%20detail%20feature%0Acompletion%2C%20which%20integrates%20coarse-%20and%20fine-grained%20features%20from%20WSI%20based%0Aon%20specific%20prompts%20without%20compromising%20inference%20speed.%20Leveraging%20a%0Acomprehensive%20dataset%20of%20490K%20samples%20from%20diverse%20pathology%20tasks%2C%20we%20trained%0Athe%20pathology-specialized%20LVLM%2C%20OmniPath.%20Extensive%20experiments%20demonstrate%0Athat%20this%20model%20significantly%20outperforms%20existing%20methods%20in%20diagnostic%0Aaccuracy%20and%20efficiency%2C%20providing%20an%20interactive%2C%20clinically%20aligned%20approach%0Afor%20auxiliary%20diagnosis%20in%20a%20wide%20range%20of%20pathology%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09521v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Comprehensive%2520Feature%2520Extraction%2520in%2520Large%2520Vision-Language%250A%2520%2520Model%2520for%2520Pathology%2520Analysis%26entry.906535625%3DShengxuming%2520Zhang%2520and%2520Weihan%2520Li%2520and%2520Tianhong%2520Gao%2520and%2520Jiacong%2520Hu%2520and%2520Haoming%2520Luo%2520and%2520Xiuming%2520Zhang%2520and%2520Jing%2520Zhang%2520and%2520Mingli%2520Song%2520and%2520Zunlei%2520Feng%26entry.1292438233%3D%2520%2520Pathological%2520diagnosis%2520is%2520vital%2520for%2520determining%2520disease%2520characteristics%252C%250Aguiding%2520treatment%252C%2520and%2520assessing%2520prognosis%252C%2520relying%2520heavily%2520on%2520detailed%252C%250Amulti-scale%2520analysis%2520of%2520high-resolution%2520whole%2520slide%2520images%2520%2528WSI%2529.%2520However%252C%250Aexisting%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520limited%2520by%2520input%2520resolution%250Aconstraints%252C%2520hindering%2520their%2520efficiency%2520and%2520accuracy%2520in%2520pathology%2520image%250Aanalysis.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520two%2520innovative%2520strategies%253A%2520the%250Amixed%2520task-guided%2520feature%2520enhancement%252C%2520which%2520directs%2520feature%2520extraction%2520toward%250Alesion-related%2520details%2520across%2520scales%252C%2520and%2520the%2520prompt-guided%2520detail%2520feature%250Acompletion%252C%2520which%2520integrates%2520coarse-%2520and%2520fine-grained%2520features%2520from%2520WSI%2520based%250Aon%2520specific%2520prompts%2520without%2520compromising%2520inference%2520speed.%2520Leveraging%2520a%250Acomprehensive%2520dataset%2520of%2520490K%2520samples%2520from%2520diverse%2520pathology%2520tasks%252C%2520we%2520trained%250Athe%2520pathology-specialized%2520LVLM%252C%2520OmniPath.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520this%2520model%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520diagnostic%250Aaccuracy%2520and%2520efficiency%252C%2520providing%2520an%2520interactive%252C%2520clinically%2520aligned%2520approach%250Afor%2520auxiliary%2520diagnosis%2520in%2520a%2520wide%2520range%2520of%2520pathology%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09521v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Comprehensive%20Feature%20Extraction%20in%20Large%20Vision-Language%0A%20%20Model%20for%20Pathology%20Analysis&entry.906535625=Shengxuming%20Zhang%20and%20Weihan%20Li%20and%20Tianhong%20Gao%20and%20Jiacong%20Hu%20and%20Haoming%20Luo%20and%20Xiuming%20Zhang%20and%20Jing%20Zhang%20and%20Mingli%20Song%20and%20Zunlei%20Feng&entry.1292438233=%20%20Pathological%20diagnosis%20is%20vital%20for%20determining%20disease%20characteristics%2C%0Aguiding%20treatment%2C%20and%20assessing%20prognosis%2C%20relying%20heavily%20on%20detailed%2C%0Amulti-scale%20analysis%20of%20high-resolution%20whole%20slide%20images%20%28WSI%29.%20However%2C%0Aexisting%20large%20vision-language%20models%20%28LVLMs%29%20are%20limited%20by%20input%20resolution%0Aconstraints%2C%20hindering%20their%20efficiency%20and%20accuracy%20in%20pathology%20image%0Aanalysis.%20To%20overcome%20these%20issues%2C%20we%20propose%20two%20innovative%20strategies%3A%20the%0Amixed%20task-guided%20feature%20enhancement%2C%20which%20directs%20feature%20extraction%20toward%0Alesion-related%20details%20across%20scales%2C%20and%20the%20prompt-guided%20detail%20feature%0Acompletion%2C%20which%20integrates%20coarse-%20and%20fine-grained%20features%20from%20WSI%20based%0Aon%20specific%20prompts%20without%20compromising%20inference%20speed.%20Leveraging%20a%0Acomprehensive%20dataset%20of%20490K%20samples%20from%20diverse%20pathology%20tasks%2C%20we%20trained%0Athe%20pathology-specialized%20LVLM%2C%20OmniPath.%20Extensive%20experiments%20demonstrate%0Athat%20this%20model%20significantly%20outperforms%20existing%20methods%20in%20diagnostic%0Aaccuracy%20and%20efficiency%2C%20providing%20an%20interactive%2C%20clinically%20aligned%20approach%0Afor%20auxiliary%20diagnosis%20in%20a%20wide%20range%20of%20pathology%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09521v3&entry.124074799=Read"},
{"title": "Automating High Quality RT Planning at Scale", "author": "Riqiang Gao and Mamadou Diallo and Han Liu and Anthony Magliari and Jonathan Sackett and Wilko Verbakel and Sandra Meyers and Rafe Mcbeth and Masoud Zarepisheh and Simon Arberet and Martin Kraus and Florin C. Ghesu and Ali Kamen", "abstract": "  Radiotherapy (RT) planning is complex, subjective, and time-intensive.\nAdvances with artificial intelligence (AI) promise to improve its precision and\nefficiency, but progress is often limited by the scarcity of large,\nstandardized datasets. To address this, we introduce the Automated Iterative RT\nPlanning (AIRTP) system, a scalable solution for generating high-quality\ntreatment plans. This scalable solution is designed to generate substantial\nvolumes of consistently high-quality treatment plans, overcoming a key obstacle\nin the advancement of AI-driven RT planning. Our AIRTP pipeline adheres to\nclinical guidelines and automates essential steps, including organ-at-risk\n(OAR) contouring, helper structure creation, beam setup, optimization, and plan\nquality improvement, using AI integrated with RT planning software like Varian\nEclipse. Furthermore, a novel approach for determining optimization parameters\nto reproduce 3D dose distributions, i.e. a method to convert dose predictions\nto deliverable treatment plans constrained by machine limitations is proposed.\nA comparative analysis of plan quality reveals that our automated pipeline\nproduces treatment plans of quality comparable to those generated manually,\nwhich traditionally require several hours of labor per plan. Committed to\npublic research, the first data release of our AIRTP pipeline includes nine\ncohorts covering head-and-neck and lung cancer sites to support an AAPM 2025\nchallenge. To our best knowledge, this dataset features more than 10 times\nnumber of plans compared to the largest existing well-curated public dataset.\nRepo: https://github.com/RiqiangGao/GDP-HMM_AAPMChallenge.\n", "link": "http://arxiv.org/abs/2501.11803v3", "date": "2025-05-16", "relevancy": 2.3131, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4944}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4467}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20High%20Quality%20RT%20Planning%20at%20Scale&body=Title%3A%20Automating%20High%20Quality%20RT%20Planning%20at%20Scale%0AAuthor%3A%20Riqiang%20Gao%20and%20Mamadou%20Diallo%20and%20Han%20Liu%20and%20Anthony%20Magliari%20and%20Jonathan%20Sackett%20and%20Wilko%20Verbakel%20and%20Sandra%20Meyers%20and%20Rafe%20Mcbeth%20and%20Masoud%20Zarepisheh%20and%20Simon%20Arberet%20and%20Martin%20Kraus%20and%20Florin%20C.%20Ghesu%20and%20Ali%20Kamen%0AAbstract%3A%20%20%20Radiotherapy%20%28RT%29%20planning%20is%20complex%2C%20subjective%2C%20and%20time-intensive.%0AAdvances%20with%20artificial%20intelligence%20%28AI%29%20promise%20to%20improve%20its%20precision%20and%0Aefficiency%2C%20but%20progress%20is%20often%20limited%20by%20the%20scarcity%20of%20large%2C%0Astandardized%20datasets.%20To%20address%20this%2C%20we%20introduce%20the%20Automated%20Iterative%20RT%0APlanning%20%28AIRTP%29%20system%2C%20a%20scalable%20solution%20for%20generating%20high-quality%0Atreatment%20plans.%20This%20scalable%20solution%20is%20designed%20to%20generate%20substantial%0Avolumes%20of%20consistently%20high-quality%20treatment%20plans%2C%20overcoming%20a%20key%20obstacle%0Ain%20the%20advancement%20of%20AI-driven%20RT%20planning.%20Our%20AIRTP%20pipeline%20adheres%20to%0Aclinical%20guidelines%20and%20automates%20essential%20steps%2C%20including%20organ-at-risk%0A%28OAR%29%20contouring%2C%20helper%20structure%20creation%2C%20beam%20setup%2C%20optimization%2C%20and%20plan%0Aquality%20improvement%2C%20using%20AI%20integrated%20with%20RT%20planning%20software%20like%20Varian%0AEclipse.%20Furthermore%2C%20a%20novel%20approach%20for%20determining%20optimization%20parameters%0Ato%20reproduce%203D%20dose%20distributions%2C%20i.e.%20a%20method%20to%20convert%20dose%20predictions%0Ato%20deliverable%20treatment%20plans%20constrained%20by%20machine%20limitations%20is%20proposed.%0AA%20comparative%20analysis%20of%20plan%20quality%20reveals%20that%20our%20automated%20pipeline%0Aproduces%20treatment%20plans%20of%20quality%20comparable%20to%20those%20generated%20manually%2C%0Awhich%20traditionally%20require%20several%20hours%20of%20labor%20per%20plan.%20Committed%20to%0Apublic%20research%2C%20the%20first%20data%20release%20of%20our%20AIRTP%20pipeline%20includes%20nine%0Acohorts%20covering%20head-and-neck%20and%20lung%20cancer%20sites%20to%20support%20an%20AAPM%202025%0Achallenge.%20To%20our%20best%20knowledge%2C%20this%20dataset%20features%20more%20than%2010%20times%0Anumber%20of%20plans%20compared%20to%20the%20largest%20existing%20well-curated%20public%20dataset.%0ARepo%3A%20https%3A//github.com/RiqiangGao/GDP-HMM_AAPMChallenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11803v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520High%2520Quality%2520RT%2520Planning%2520at%2520Scale%26entry.906535625%3DRiqiang%2520Gao%2520and%2520Mamadou%2520Diallo%2520and%2520Han%2520Liu%2520and%2520Anthony%2520Magliari%2520and%2520Jonathan%2520Sackett%2520and%2520Wilko%2520Verbakel%2520and%2520Sandra%2520Meyers%2520and%2520Rafe%2520Mcbeth%2520and%2520Masoud%2520Zarepisheh%2520and%2520Simon%2520Arberet%2520and%2520Martin%2520Kraus%2520and%2520Florin%2520C.%2520Ghesu%2520and%2520Ali%2520Kamen%26entry.1292438233%3D%2520%2520Radiotherapy%2520%2528RT%2529%2520planning%2520is%2520complex%252C%2520subjective%252C%2520and%2520time-intensive.%250AAdvances%2520with%2520artificial%2520intelligence%2520%2528AI%2529%2520promise%2520to%2520improve%2520its%2520precision%2520and%250Aefficiency%252C%2520but%2520progress%2520is%2520often%2520limited%2520by%2520the%2520scarcity%2520of%2520large%252C%250Astandardized%2520datasets.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Automated%2520Iterative%2520RT%250APlanning%2520%2528AIRTP%2529%2520system%252C%2520a%2520scalable%2520solution%2520for%2520generating%2520high-quality%250Atreatment%2520plans.%2520This%2520scalable%2520solution%2520is%2520designed%2520to%2520generate%2520substantial%250Avolumes%2520of%2520consistently%2520high-quality%2520treatment%2520plans%252C%2520overcoming%2520a%2520key%2520obstacle%250Ain%2520the%2520advancement%2520of%2520AI-driven%2520RT%2520planning.%2520Our%2520AIRTP%2520pipeline%2520adheres%2520to%250Aclinical%2520guidelines%2520and%2520automates%2520essential%2520steps%252C%2520including%2520organ-at-risk%250A%2528OAR%2529%2520contouring%252C%2520helper%2520structure%2520creation%252C%2520beam%2520setup%252C%2520optimization%252C%2520and%2520plan%250Aquality%2520improvement%252C%2520using%2520AI%2520integrated%2520with%2520RT%2520planning%2520software%2520like%2520Varian%250AEclipse.%2520Furthermore%252C%2520a%2520novel%2520approach%2520for%2520determining%2520optimization%2520parameters%250Ato%2520reproduce%25203D%2520dose%2520distributions%252C%2520i.e.%2520a%2520method%2520to%2520convert%2520dose%2520predictions%250Ato%2520deliverable%2520treatment%2520plans%2520constrained%2520by%2520machine%2520limitations%2520is%2520proposed.%250AA%2520comparative%2520analysis%2520of%2520plan%2520quality%2520reveals%2520that%2520our%2520automated%2520pipeline%250Aproduces%2520treatment%2520plans%2520of%2520quality%2520comparable%2520to%2520those%2520generated%2520manually%252C%250Awhich%2520traditionally%2520require%2520several%2520hours%2520of%2520labor%2520per%2520plan.%2520Committed%2520to%250Apublic%2520research%252C%2520the%2520first%2520data%2520release%2520of%2520our%2520AIRTP%2520pipeline%2520includes%2520nine%250Acohorts%2520covering%2520head-and-neck%2520and%2520lung%2520cancer%2520sites%2520to%2520support%2520an%2520AAPM%25202025%250Achallenge.%2520To%2520our%2520best%2520knowledge%252C%2520this%2520dataset%2520features%2520more%2520than%252010%2520times%250Anumber%2520of%2520plans%2520compared%2520to%2520the%2520largest%2520existing%2520well-curated%2520public%2520dataset.%250ARepo%253A%2520https%253A//github.com/RiqiangGao/GDP-HMM_AAPMChallenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11803v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20High%20Quality%20RT%20Planning%20at%20Scale&entry.906535625=Riqiang%20Gao%20and%20Mamadou%20Diallo%20and%20Han%20Liu%20and%20Anthony%20Magliari%20and%20Jonathan%20Sackett%20and%20Wilko%20Verbakel%20and%20Sandra%20Meyers%20and%20Rafe%20Mcbeth%20and%20Masoud%20Zarepisheh%20and%20Simon%20Arberet%20and%20Martin%20Kraus%20and%20Florin%20C.%20Ghesu%20and%20Ali%20Kamen&entry.1292438233=%20%20Radiotherapy%20%28RT%29%20planning%20is%20complex%2C%20subjective%2C%20and%20time-intensive.%0AAdvances%20with%20artificial%20intelligence%20%28AI%29%20promise%20to%20improve%20its%20precision%20and%0Aefficiency%2C%20but%20progress%20is%20often%20limited%20by%20the%20scarcity%20of%20large%2C%0Astandardized%20datasets.%20To%20address%20this%2C%20we%20introduce%20the%20Automated%20Iterative%20RT%0APlanning%20%28AIRTP%29%20system%2C%20a%20scalable%20solution%20for%20generating%20high-quality%0Atreatment%20plans.%20This%20scalable%20solution%20is%20designed%20to%20generate%20substantial%0Avolumes%20of%20consistently%20high-quality%20treatment%20plans%2C%20overcoming%20a%20key%20obstacle%0Ain%20the%20advancement%20of%20AI-driven%20RT%20planning.%20Our%20AIRTP%20pipeline%20adheres%20to%0Aclinical%20guidelines%20and%20automates%20essential%20steps%2C%20including%20organ-at-risk%0A%28OAR%29%20contouring%2C%20helper%20structure%20creation%2C%20beam%20setup%2C%20optimization%2C%20and%20plan%0Aquality%20improvement%2C%20using%20AI%20integrated%20with%20RT%20planning%20software%20like%20Varian%0AEclipse.%20Furthermore%2C%20a%20novel%20approach%20for%20determining%20optimization%20parameters%0Ato%20reproduce%203D%20dose%20distributions%2C%20i.e.%20a%20method%20to%20convert%20dose%20predictions%0Ato%20deliverable%20treatment%20plans%20constrained%20by%20machine%20limitations%20is%20proposed.%0AA%20comparative%20analysis%20of%20plan%20quality%20reveals%20that%20our%20automated%20pipeline%0Aproduces%20treatment%20plans%20of%20quality%20comparable%20to%20those%20generated%20manually%2C%0Awhich%20traditionally%20require%20several%20hours%20of%20labor%20per%20plan.%20Committed%20to%0Apublic%20research%2C%20the%20first%20data%20release%20of%20our%20AIRTP%20pipeline%20includes%20nine%0Acohorts%20covering%20head-and-neck%20and%20lung%20cancer%20sites%20to%20support%20an%20AAPM%202025%0Achallenge.%20To%20our%20best%20knowledge%2C%20this%20dataset%20features%20more%20than%2010%20times%0Anumber%20of%20plans%20compared%20to%20the%20largest%20existing%20well-curated%20public%20dataset.%0ARepo%3A%20https%3A//github.com/RiqiangGao/GDP-HMM_AAPMChallenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11803v3&entry.124074799=Read"},
{"title": "Concept Drift Guided LayerNorm Tuning for Efficient Multimodal Metaphor\n  Identification", "author": "Wenhao Qian and Zhenzhen Hu and Zijie Song and Jia Li", "abstract": "  Metaphorical imagination, the ability to connect seemingly unrelated\nconcepts, is fundamental to human cognition and communication. While\nunderstanding linguistic metaphors has advanced significantly, grasping\nmultimodal metaphors, such as those found in internet memes, presents unique\nchallenges due to their unconventional expressions and implied meanings.\nExisting methods for multimodal metaphor identification often struggle to\nbridge the gap between literal and figurative interpretations. Additionally,\ngenerative approaches that utilize large language models or text-to-image\nmodels, while promising, suffer from high computational costs. This paper\nintroduces \\textbf{C}oncept \\textbf{D}rift \\textbf{G}uided \\textbf{L}ayerNorm\n\\textbf{T}uning (\\textbf{CDGLT}), a novel and training-efficient framework for\nmultimodal metaphor identification. CDGLT incorporates two key innovations: (1)\nConcept Drift, a mechanism that leverages Spherical Linear Interpolation\n(SLERP) of cross-modal embeddings from a CLIP encoder to generate a new,\ndivergent concept embedding. This drifted concept helps to alleviate the gap\nbetween literal features and the figurative task. (2) A prompt construction\nstrategy, that adapts the method of feature extraction and fusion using\npre-trained language models for the multimodal metaphor identification task.\nCDGLT achieves state-of-the-art performance on the MET-Meme benchmark while\nsignificantly reducing training costs compared to existing generative methods.\nAblation studies demonstrate the effectiveness of both Concept Drift and our\nadapted LN Tuning approach. Our method represents a significant step towards\nefficient and accurate multimodal metaphor understanding. The code is\navailable:\n\\href{https://github.com/Qianvenh/CDGLT}{https://github.com/Qianvenh/CDGLT}.\n", "link": "http://arxiv.org/abs/2505.11237v1", "date": "2025-05-16", "relevancy": 2.296, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5879}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5662}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept%20Drift%20Guided%20LayerNorm%20Tuning%20for%20Efficient%20Multimodal%20Metaphor%0A%20%20Identification&body=Title%3A%20Concept%20Drift%20Guided%20LayerNorm%20Tuning%20for%20Efficient%20Multimodal%20Metaphor%0A%20%20Identification%0AAuthor%3A%20Wenhao%20Qian%20and%20Zhenzhen%20Hu%20and%20Zijie%20Song%20and%20Jia%20Li%0AAbstract%3A%20%20%20Metaphorical%20imagination%2C%20the%20ability%20to%20connect%20seemingly%20unrelated%0Aconcepts%2C%20is%20fundamental%20to%20human%20cognition%20and%20communication.%20While%0Aunderstanding%20linguistic%20metaphors%20has%20advanced%20significantly%2C%20grasping%0Amultimodal%20metaphors%2C%20such%20as%20those%20found%20in%20internet%20memes%2C%20presents%20unique%0Achallenges%20due%20to%20their%20unconventional%20expressions%20and%20implied%20meanings.%0AExisting%20methods%20for%20multimodal%20metaphor%20identification%20often%20struggle%20to%0Abridge%20the%20gap%20between%20literal%20and%20figurative%20interpretations.%20Additionally%2C%0Agenerative%20approaches%20that%20utilize%20large%20language%20models%20or%20text-to-image%0Amodels%2C%20while%20promising%2C%20suffer%20from%20high%20computational%20costs.%20This%20paper%0Aintroduces%20%5Ctextbf%7BC%7Doncept%20%5Ctextbf%7BD%7Drift%20%5Ctextbf%7BG%7Duided%20%5Ctextbf%7BL%7DayerNorm%0A%5Ctextbf%7BT%7Duning%20%28%5Ctextbf%7BCDGLT%7D%29%2C%20a%20novel%20and%20training-efficient%20framework%20for%0Amultimodal%20metaphor%20identification.%20CDGLT%20incorporates%20two%20key%20innovations%3A%20%281%29%0AConcept%20Drift%2C%20a%20mechanism%20that%20leverages%20Spherical%20Linear%20Interpolation%0A%28SLERP%29%20of%20cross-modal%20embeddings%20from%20a%20CLIP%20encoder%20to%20generate%20a%20new%2C%0Adivergent%20concept%20embedding.%20This%20drifted%20concept%20helps%20to%20alleviate%20the%20gap%0Abetween%20literal%20features%20and%20the%20figurative%20task.%20%282%29%20A%20prompt%20construction%0Astrategy%2C%20that%20adapts%20the%20method%20of%20feature%20extraction%20and%20fusion%20using%0Apre-trained%20language%20models%20for%20the%20multimodal%20metaphor%20identification%20task.%0ACDGLT%20achieves%20state-of-the-art%20performance%20on%20the%20MET-Meme%20benchmark%20while%0Asignificantly%20reducing%20training%20costs%20compared%20to%20existing%20generative%20methods.%0AAblation%20studies%20demonstrate%20the%20effectiveness%20of%20both%20Concept%20Drift%20and%20our%0Aadapted%20LN%20Tuning%20approach.%20Our%20method%20represents%20a%20significant%20step%20towards%0Aefficient%20and%20accurate%20multimodal%20metaphor%20understanding.%20The%20code%20is%0Aavailable%3A%0A%5Chref%7Bhttps%3A//github.com/Qianvenh/CDGLT%7D%7Bhttps%3A//github.com/Qianvenh/CDGLT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept%2520Drift%2520Guided%2520LayerNorm%2520Tuning%2520for%2520Efficient%2520Multimodal%2520Metaphor%250A%2520%2520Identification%26entry.906535625%3DWenhao%2520Qian%2520and%2520Zhenzhen%2520Hu%2520and%2520Zijie%2520Song%2520and%2520Jia%2520Li%26entry.1292438233%3D%2520%2520Metaphorical%2520imagination%252C%2520the%2520ability%2520to%2520connect%2520seemingly%2520unrelated%250Aconcepts%252C%2520is%2520fundamental%2520to%2520human%2520cognition%2520and%2520communication.%2520While%250Aunderstanding%2520linguistic%2520metaphors%2520has%2520advanced%2520significantly%252C%2520grasping%250Amultimodal%2520metaphors%252C%2520such%2520as%2520those%2520found%2520in%2520internet%2520memes%252C%2520presents%2520unique%250Achallenges%2520due%2520to%2520their%2520unconventional%2520expressions%2520and%2520implied%2520meanings.%250AExisting%2520methods%2520for%2520multimodal%2520metaphor%2520identification%2520often%2520struggle%2520to%250Abridge%2520the%2520gap%2520between%2520literal%2520and%2520figurative%2520interpretations.%2520Additionally%252C%250Agenerative%2520approaches%2520that%2520utilize%2520large%2520language%2520models%2520or%2520text-to-image%250Amodels%252C%2520while%2520promising%252C%2520suffer%2520from%2520high%2520computational%2520costs.%2520This%2520paper%250Aintroduces%2520%255Ctextbf%257BC%257Doncept%2520%255Ctextbf%257BD%257Drift%2520%255Ctextbf%257BG%257Duided%2520%255Ctextbf%257BL%257DayerNorm%250A%255Ctextbf%257BT%257Duning%2520%2528%255Ctextbf%257BCDGLT%257D%2529%252C%2520a%2520novel%2520and%2520training-efficient%2520framework%2520for%250Amultimodal%2520metaphor%2520identification.%2520CDGLT%2520incorporates%2520two%2520key%2520innovations%253A%2520%25281%2529%250AConcept%2520Drift%252C%2520a%2520mechanism%2520that%2520leverages%2520Spherical%2520Linear%2520Interpolation%250A%2528SLERP%2529%2520of%2520cross-modal%2520embeddings%2520from%2520a%2520CLIP%2520encoder%2520to%2520generate%2520a%2520new%252C%250Adivergent%2520concept%2520embedding.%2520This%2520drifted%2520concept%2520helps%2520to%2520alleviate%2520the%2520gap%250Abetween%2520literal%2520features%2520and%2520the%2520figurative%2520task.%2520%25282%2529%2520A%2520prompt%2520construction%250Astrategy%252C%2520that%2520adapts%2520the%2520method%2520of%2520feature%2520extraction%2520and%2520fusion%2520using%250Apre-trained%2520language%2520models%2520for%2520the%2520multimodal%2520metaphor%2520identification%2520task.%250ACDGLT%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520MET-Meme%2520benchmark%2520while%250Asignificantly%2520reducing%2520training%2520costs%2520compared%2520to%2520existing%2520generative%2520methods.%250AAblation%2520studies%2520demonstrate%2520the%2520effectiveness%2520of%2520both%2520Concept%2520Drift%2520and%2520our%250Aadapted%2520LN%2520Tuning%2520approach.%2520Our%2520method%2520represents%2520a%2520significant%2520step%2520towards%250Aefficient%2520and%2520accurate%2520multimodal%2520metaphor%2520understanding.%2520The%2520code%2520is%250Aavailable%253A%250A%255Chref%257Bhttps%253A//github.com/Qianvenh/CDGLT%257D%257Bhttps%253A//github.com/Qianvenh/CDGLT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20Drift%20Guided%20LayerNorm%20Tuning%20for%20Efficient%20Multimodal%20Metaphor%0A%20%20Identification&entry.906535625=Wenhao%20Qian%20and%20Zhenzhen%20Hu%20and%20Zijie%20Song%20and%20Jia%20Li&entry.1292438233=%20%20Metaphorical%20imagination%2C%20the%20ability%20to%20connect%20seemingly%20unrelated%0Aconcepts%2C%20is%20fundamental%20to%20human%20cognition%20and%20communication.%20While%0Aunderstanding%20linguistic%20metaphors%20has%20advanced%20significantly%2C%20grasping%0Amultimodal%20metaphors%2C%20such%20as%20those%20found%20in%20internet%20memes%2C%20presents%20unique%0Achallenges%20due%20to%20their%20unconventional%20expressions%20and%20implied%20meanings.%0AExisting%20methods%20for%20multimodal%20metaphor%20identification%20often%20struggle%20to%0Abridge%20the%20gap%20between%20literal%20and%20figurative%20interpretations.%20Additionally%2C%0Agenerative%20approaches%20that%20utilize%20large%20language%20models%20or%20text-to-image%0Amodels%2C%20while%20promising%2C%20suffer%20from%20high%20computational%20costs.%20This%20paper%0Aintroduces%20%5Ctextbf%7BC%7Doncept%20%5Ctextbf%7BD%7Drift%20%5Ctextbf%7BG%7Duided%20%5Ctextbf%7BL%7DayerNorm%0A%5Ctextbf%7BT%7Duning%20%28%5Ctextbf%7BCDGLT%7D%29%2C%20a%20novel%20and%20training-efficient%20framework%20for%0Amultimodal%20metaphor%20identification.%20CDGLT%20incorporates%20two%20key%20innovations%3A%20%281%29%0AConcept%20Drift%2C%20a%20mechanism%20that%20leverages%20Spherical%20Linear%20Interpolation%0A%28SLERP%29%20of%20cross-modal%20embeddings%20from%20a%20CLIP%20encoder%20to%20generate%20a%20new%2C%0Adivergent%20concept%20embedding.%20This%20drifted%20concept%20helps%20to%20alleviate%20the%20gap%0Abetween%20literal%20features%20and%20the%20figurative%20task.%20%282%29%20A%20prompt%20construction%0Astrategy%2C%20that%20adapts%20the%20method%20of%20feature%20extraction%20and%20fusion%20using%0Apre-trained%20language%20models%20for%20the%20multimodal%20metaphor%20identification%20task.%0ACDGLT%20achieves%20state-of-the-art%20performance%20on%20the%20MET-Meme%20benchmark%20while%0Asignificantly%20reducing%20training%20costs%20compared%20to%20existing%20generative%20methods.%0AAblation%20studies%20demonstrate%20the%20effectiveness%20of%20both%20Concept%20Drift%20and%20our%0Aadapted%20LN%20Tuning%20approach.%20Our%20method%20represents%20a%20significant%20step%20towards%0Aefficient%20and%20accurate%20multimodal%20metaphor%20understanding.%20The%20code%20is%0Aavailable%3A%0A%5Chref%7Bhttps%3A//github.com/Qianvenh/CDGLT%7D%7Bhttps%3A//github.com/Qianvenh/CDGLT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11237v1&entry.124074799=Read"},
{"title": "What Can We Learn From MIMO Graph Convolutions?", "author": "Andreas Roth and Thomas Liebig", "abstract": "  Most graph neural networks (GNNs) utilize approximations of the general graph\nconvolution derived in the graph Fourier domain. While GNNs are typically\napplied in the multi-input multi-output (MIMO) case, the approximations are\nperformed in the single-input single-output (SISO) case. In this work, we first\nderive the MIMO graph convolution through the convolution theorem and\napproximate it directly in the MIMO case. We find the key MIMO-specific\nproperty of the graph convolution to be operating on multiple computational\ngraphs, or equivalently, applying distinct feature transformations for each\npair of nodes. As a localized approximation, we introduce localized MIMO graph\nconvolutions (LMGCs), which generalize many linear message-passing neural\nnetworks. For almost every choice of edge weights, we prove that LMGCs with a\nsingle computational graph are injective on multisets, and the resulting\nrepresentations are linearly independent when more than one computational graph\nis used. Our experimental results confirm that an LMGC can combine the benefits\nof various methods.\n", "link": "http://arxiv.org/abs/2505.11346v1", "date": "2025-05-16", "relevancy": 2.2936, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4703}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4601}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Can%20We%20Learn%20From%20MIMO%20Graph%20Convolutions%3F&body=Title%3A%20What%20Can%20We%20Learn%20From%20MIMO%20Graph%20Convolutions%3F%0AAuthor%3A%20Andreas%20Roth%20and%20Thomas%20Liebig%0AAbstract%3A%20%20%20Most%20graph%20neural%20networks%20%28GNNs%29%20utilize%20approximations%20of%20the%20general%20graph%0Aconvolution%20derived%20in%20the%20graph%20Fourier%20domain.%20While%20GNNs%20are%20typically%0Aapplied%20in%20the%20multi-input%20multi-output%20%28MIMO%29%20case%2C%20the%20approximations%20are%0Aperformed%20in%20the%20single-input%20single-output%20%28SISO%29%20case.%20In%20this%20work%2C%20we%20first%0Aderive%20the%20MIMO%20graph%20convolution%20through%20the%20convolution%20theorem%20and%0Aapproximate%20it%20directly%20in%20the%20MIMO%20case.%20We%20find%20the%20key%20MIMO-specific%0Aproperty%20of%20the%20graph%20convolution%20to%20be%20operating%20on%20multiple%20computational%0Agraphs%2C%20or%20equivalently%2C%20applying%20distinct%20feature%20transformations%20for%20each%0Apair%20of%20nodes.%20As%20a%20localized%20approximation%2C%20we%20introduce%20localized%20MIMO%20graph%0Aconvolutions%20%28LMGCs%29%2C%20which%20generalize%20many%20linear%20message-passing%20neural%0Anetworks.%20For%20almost%20every%20choice%20of%20edge%20weights%2C%20we%20prove%20that%20LMGCs%20with%20a%0Asingle%20computational%20graph%20are%20injective%20on%20multisets%2C%20and%20the%20resulting%0Arepresentations%20are%20linearly%20independent%20when%20more%20than%20one%20computational%20graph%0Ais%20used.%20Our%20experimental%20results%20confirm%20that%20an%20LMGC%20can%20combine%20the%20benefits%0Aof%20various%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Can%2520We%2520Learn%2520From%2520MIMO%2520Graph%2520Convolutions%253F%26entry.906535625%3DAndreas%2520Roth%2520and%2520Thomas%2520Liebig%26entry.1292438233%3D%2520%2520Most%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520utilize%2520approximations%2520of%2520the%2520general%2520graph%250Aconvolution%2520derived%2520in%2520the%2520graph%2520Fourier%2520domain.%2520While%2520GNNs%2520are%2520typically%250Aapplied%2520in%2520the%2520multi-input%2520multi-output%2520%2528MIMO%2529%2520case%252C%2520the%2520approximations%2520are%250Aperformed%2520in%2520the%2520single-input%2520single-output%2520%2528SISO%2529%2520case.%2520In%2520this%2520work%252C%2520we%2520first%250Aderive%2520the%2520MIMO%2520graph%2520convolution%2520through%2520the%2520convolution%2520theorem%2520and%250Aapproximate%2520it%2520directly%2520in%2520the%2520MIMO%2520case.%2520We%2520find%2520the%2520key%2520MIMO-specific%250Aproperty%2520of%2520the%2520graph%2520convolution%2520to%2520be%2520operating%2520on%2520multiple%2520computational%250Agraphs%252C%2520or%2520equivalently%252C%2520applying%2520distinct%2520feature%2520transformations%2520for%2520each%250Apair%2520of%2520nodes.%2520As%2520a%2520localized%2520approximation%252C%2520we%2520introduce%2520localized%2520MIMO%2520graph%250Aconvolutions%2520%2528LMGCs%2529%252C%2520which%2520generalize%2520many%2520linear%2520message-passing%2520neural%250Anetworks.%2520For%2520almost%2520every%2520choice%2520of%2520edge%2520weights%252C%2520we%2520prove%2520that%2520LMGCs%2520with%2520a%250Asingle%2520computational%2520graph%2520are%2520injective%2520on%2520multisets%252C%2520and%2520the%2520resulting%250Arepresentations%2520are%2520linearly%2520independent%2520when%2520more%2520than%2520one%2520computational%2520graph%250Ais%2520used.%2520Our%2520experimental%2520results%2520confirm%2520that%2520an%2520LMGC%2520can%2520combine%2520the%2520benefits%250Aof%2520various%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Can%20We%20Learn%20From%20MIMO%20Graph%20Convolutions%3F&entry.906535625=Andreas%20Roth%20and%20Thomas%20Liebig&entry.1292438233=%20%20Most%20graph%20neural%20networks%20%28GNNs%29%20utilize%20approximations%20of%20the%20general%20graph%0Aconvolution%20derived%20in%20the%20graph%20Fourier%20domain.%20While%20GNNs%20are%20typically%0Aapplied%20in%20the%20multi-input%20multi-output%20%28MIMO%29%20case%2C%20the%20approximations%20are%0Aperformed%20in%20the%20single-input%20single-output%20%28SISO%29%20case.%20In%20this%20work%2C%20we%20first%0Aderive%20the%20MIMO%20graph%20convolution%20through%20the%20convolution%20theorem%20and%0Aapproximate%20it%20directly%20in%20the%20MIMO%20case.%20We%20find%20the%20key%20MIMO-specific%0Aproperty%20of%20the%20graph%20convolution%20to%20be%20operating%20on%20multiple%20computational%0Agraphs%2C%20or%20equivalently%2C%20applying%20distinct%20feature%20transformations%20for%20each%0Apair%20of%20nodes.%20As%20a%20localized%20approximation%2C%20we%20introduce%20localized%20MIMO%20graph%0Aconvolutions%20%28LMGCs%29%2C%20which%20generalize%20many%20linear%20message-passing%20neural%0Anetworks.%20For%20almost%20every%20choice%20of%20edge%20weights%2C%20we%20prove%20that%20LMGCs%20with%20a%0Asingle%20computational%20graph%20are%20injective%20on%20multisets%2C%20and%20the%20resulting%0Arepresentations%20are%20linearly%20independent%20when%20more%20than%20one%20computational%20graph%0Ais%20used.%20Our%20experimental%20results%20confirm%20that%20an%20LMGC%20can%20combine%20the%20benefits%0Aof%20various%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11346v1&entry.124074799=Read"},
{"title": "VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on\n  Synthetic Video Understanding", "author": "Zongxia Li and Xiyang Wu and Guangyao Shi and Yubin Qin and Hongyang Du and Tianyi Zhou and Dinesh Manocha and Jordan Lee Boyd-Graber", "abstract": "  Synthetic video generation has gained significant attention for its realism\nand broad applications, but remains prone to violations of common sense and\nphysical laws. This highlights the need for reliable abnormality detectors that\nunderstand such principles and are robust to hallucinations. To address this,\nwe introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from\nsynthetic videos generated by models like Veo2, Sora, and Kling, paired with\nexpert-crafted counterintuitive QA to evaluate the critical thinking abilities\nof Multi-modal Large Language Models (MLLMs) on abnormalities that are\nperceptually obvious to humans but often hallucinated due to language priors.\nVideoHallu evaluates MLLMs' abnormality detection abilities with examples\nacross alignment, consistency, commonsense, and physics. We benchmark SOTA\nMLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and\nVideoChat-R1. We observe that these models perform well on many real-world\nbenchmarks like MVBench and MovieChat, but still struggle with basic\nphysics-based and commonsense reasoning in synthetic videos. We further show\nthat post-training with Group Relative Policy Optimization (GRPO), using\ncurriculum learning on datasets combining video QA with counterintuitive\ncommonsense and physics reasoning over real and synthetic videos, improves\nMLLMs' abnormality detection and critical thinking, demonstrating the value of\ntargeted training for improving their understanding of commonsense and physical\nlaws.\n", "link": "http://arxiv.org/abs/2505.01481v2", "date": "2025-05-16", "relevancy": 2.2844, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoHallu%3A%20Evaluating%20and%20Mitigating%20Multi-modal%20Hallucinations%20on%0A%20%20Synthetic%20Video%20Understanding&body=Title%3A%20VideoHallu%3A%20Evaluating%20and%20Mitigating%20Multi-modal%20Hallucinations%20on%0A%20%20Synthetic%20Video%20Understanding%0AAuthor%3A%20Zongxia%20Li%20and%20Xiyang%20Wu%20and%20Guangyao%20Shi%20and%20Yubin%20Qin%20and%20Hongyang%20Du%20and%20Tianyi%20Zhou%20and%20Dinesh%20Manocha%20and%20Jordan%20Lee%20Boyd-Graber%0AAbstract%3A%20%20%20Synthetic%20video%20generation%20has%20gained%20significant%20attention%20for%20its%20realism%0Aand%20broad%20applications%2C%20but%20remains%20prone%20to%20violations%20of%20common%20sense%20and%0Aphysical%20laws.%20This%20highlights%20the%20need%20for%20reliable%20abnormality%20detectors%20that%0Aunderstand%20such%20principles%20and%20are%20robust%20to%20hallucinations.%20To%20address%20this%2C%0Awe%20introduce%20VideoHallu%2C%20a%20benchmark%20of%20over%203%2C000%20video%20QA%20pairs%20built%20from%0Asynthetic%20videos%20generated%20by%20models%20like%20Veo2%2C%20Sora%2C%20and%20Kling%2C%20paired%20with%0Aexpert-crafted%20counterintuitive%20QA%20to%20evaluate%20the%20critical%20thinking%20abilities%0Aof%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20on%20abnormalities%20that%20are%0Aperceptually%20obvious%20to%20humans%20but%20often%20hallucinated%20due%20to%20language%20priors.%0AVideoHallu%20evaluates%20MLLMs%27%20abnormality%20detection%20abilities%20with%20examples%0Aacross%20alignment%2C%20consistency%2C%20commonsense%2C%20and%20physics.%20We%20benchmark%20SOTA%0AMLLMs%2C%20including%20GPT-4o%2C%20Gemini-2.5-Pro%2C%20Qwen2.5-VL%2C%20Video-R1%2C%20and%0AVideoChat-R1.%20We%20observe%20that%20these%20models%20perform%20well%20on%20many%20real-world%0Abenchmarks%20like%20MVBench%20and%20MovieChat%2C%20but%20still%20struggle%20with%20basic%0Aphysics-based%20and%20commonsense%20reasoning%20in%20synthetic%20videos.%20We%20further%20show%0Athat%20post-training%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20using%0Acurriculum%20learning%20on%20datasets%20combining%20video%20QA%20with%20counterintuitive%0Acommonsense%20and%20physics%20reasoning%20over%20real%20and%20synthetic%20videos%2C%20improves%0AMLLMs%27%20abnormality%20detection%20and%20critical%20thinking%2C%20demonstrating%20the%20value%20of%0Atargeted%20training%20for%20improving%20their%20understanding%20of%20commonsense%20and%20physical%0Alaws.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01481v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoHallu%253A%2520Evaluating%2520and%2520Mitigating%2520Multi-modal%2520Hallucinations%2520on%250A%2520%2520Synthetic%2520Video%2520Understanding%26entry.906535625%3DZongxia%2520Li%2520and%2520Xiyang%2520Wu%2520and%2520Guangyao%2520Shi%2520and%2520Yubin%2520Qin%2520and%2520Hongyang%2520Du%2520and%2520Tianyi%2520Zhou%2520and%2520Dinesh%2520Manocha%2520and%2520Jordan%2520Lee%2520Boyd-Graber%26entry.1292438233%3D%2520%2520Synthetic%2520video%2520generation%2520has%2520gained%2520significant%2520attention%2520for%2520its%2520realism%250Aand%2520broad%2520applications%252C%2520but%2520remains%2520prone%2520to%2520violations%2520of%2520common%2520sense%2520and%250Aphysical%2520laws.%2520This%2520highlights%2520the%2520need%2520for%2520reliable%2520abnormality%2520detectors%2520that%250Aunderstand%2520such%2520principles%2520and%2520are%2520robust%2520to%2520hallucinations.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520VideoHallu%252C%2520a%2520benchmark%2520of%2520over%25203%252C000%2520video%2520QA%2520pairs%2520built%2520from%250Asynthetic%2520videos%2520generated%2520by%2520models%2520like%2520Veo2%252C%2520Sora%252C%2520and%2520Kling%252C%2520paired%2520with%250Aexpert-crafted%2520counterintuitive%2520QA%2520to%2520evaluate%2520the%2520critical%2520thinking%2520abilities%250Aof%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520on%2520abnormalities%2520that%2520are%250Aperceptually%2520obvious%2520to%2520humans%2520but%2520often%2520hallucinated%2520due%2520to%2520language%2520priors.%250AVideoHallu%2520evaluates%2520MLLMs%2527%2520abnormality%2520detection%2520abilities%2520with%2520examples%250Aacross%2520alignment%252C%2520consistency%252C%2520commonsense%252C%2520and%2520physics.%2520We%2520benchmark%2520SOTA%250AMLLMs%252C%2520including%2520GPT-4o%252C%2520Gemini-2.5-Pro%252C%2520Qwen2.5-VL%252C%2520Video-R1%252C%2520and%250AVideoChat-R1.%2520We%2520observe%2520that%2520these%2520models%2520perform%2520well%2520on%2520many%2520real-world%250Abenchmarks%2520like%2520MVBench%2520and%2520MovieChat%252C%2520but%2520still%2520struggle%2520with%2520basic%250Aphysics-based%2520and%2520commonsense%2520reasoning%2520in%2520synthetic%2520videos.%2520We%2520further%2520show%250Athat%2520post-training%2520with%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520using%250Acurriculum%2520learning%2520on%2520datasets%2520combining%2520video%2520QA%2520with%2520counterintuitive%250Acommonsense%2520and%2520physics%2520reasoning%2520over%2520real%2520and%2520synthetic%2520videos%252C%2520improves%250AMLLMs%2527%2520abnormality%2520detection%2520and%2520critical%2520thinking%252C%2520demonstrating%2520the%2520value%2520of%250Atargeted%2520training%2520for%2520improving%2520their%2520understanding%2520of%2520commonsense%2520and%2520physical%250Alaws.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01481v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoHallu%3A%20Evaluating%20and%20Mitigating%20Multi-modal%20Hallucinations%20on%0A%20%20Synthetic%20Video%20Understanding&entry.906535625=Zongxia%20Li%20and%20Xiyang%20Wu%20and%20Guangyao%20Shi%20and%20Yubin%20Qin%20and%20Hongyang%20Du%20and%20Tianyi%20Zhou%20and%20Dinesh%20Manocha%20and%20Jordan%20Lee%20Boyd-Graber&entry.1292438233=%20%20Synthetic%20video%20generation%20has%20gained%20significant%20attention%20for%20its%20realism%0Aand%20broad%20applications%2C%20but%20remains%20prone%20to%20violations%20of%20common%20sense%20and%0Aphysical%20laws.%20This%20highlights%20the%20need%20for%20reliable%20abnormality%20detectors%20that%0Aunderstand%20such%20principles%20and%20are%20robust%20to%20hallucinations.%20To%20address%20this%2C%0Awe%20introduce%20VideoHallu%2C%20a%20benchmark%20of%20over%203%2C000%20video%20QA%20pairs%20built%20from%0Asynthetic%20videos%20generated%20by%20models%20like%20Veo2%2C%20Sora%2C%20and%20Kling%2C%20paired%20with%0Aexpert-crafted%20counterintuitive%20QA%20to%20evaluate%20the%20critical%20thinking%20abilities%0Aof%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20on%20abnormalities%20that%20are%0Aperceptually%20obvious%20to%20humans%20but%20often%20hallucinated%20due%20to%20language%20priors.%0AVideoHallu%20evaluates%20MLLMs%27%20abnormality%20detection%20abilities%20with%20examples%0Aacross%20alignment%2C%20consistency%2C%20commonsense%2C%20and%20physics.%20We%20benchmark%20SOTA%0AMLLMs%2C%20including%20GPT-4o%2C%20Gemini-2.5-Pro%2C%20Qwen2.5-VL%2C%20Video-R1%2C%20and%0AVideoChat-R1.%20We%20observe%20that%20these%20models%20perform%20well%20on%20many%20real-world%0Abenchmarks%20like%20MVBench%20and%20MovieChat%2C%20but%20still%20struggle%20with%20basic%0Aphysics-based%20and%20commonsense%20reasoning%20in%20synthetic%20videos.%20We%20further%20show%0Athat%20post-training%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20using%0Acurriculum%20learning%20on%20datasets%20combining%20video%20QA%20with%20counterintuitive%0Acommonsense%20and%20physics%20reasoning%20over%20real%20and%20synthetic%20videos%2C%20improves%0AMLLMs%27%20abnormality%20detection%20and%20critical%20thinking%2C%20demonstrating%20the%20value%20of%0Atargeted%20training%20for%20improving%20their%20understanding%20of%20commonsense%20and%20physical%0Alaws.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01481v2&entry.124074799=Read"},
{"title": "FOReCAst: The Future Outcome Reasoning and Confidence Assessment\n  Benchmark", "author": "Zhangdie Yuan and Zifeng Ding and Andreas Vlachos", "abstract": "  Forecasting is an important task in many domains, such as technology and\neconomics. However existing forecasting benchmarks largely lack comprehensive\nconfidence assessment, focus on limited question types, and often consist of\nartificial questions that do not align with real-world human forecasting needs.\nTo address these gaps, we introduce FOReCAst (Future Outcome Reasoning and\nConfidence Assessment), a benchmark that evaluates models' ability to make\npredictions and their confidence in them. FOReCAst spans diverse forecasting\nscenarios involving Boolean questions, timeframe prediction, and quantity\nestimation, enabling a comprehensive evaluation of both prediction accuracy and\nconfidence calibration for real-world applications.\n", "link": "http://arxiv.org/abs/2502.19676v4", "date": "2025-05-16", "relevancy": 2.2832, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4567}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOReCAst%3A%20The%20Future%20Outcome%20Reasoning%20and%20Confidence%20Assessment%0A%20%20Benchmark&body=Title%3A%20FOReCAst%3A%20The%20Future%20Outcome%20Reasoning%20and%20Confidence%20Assessment%0A%20%20Benchmark%0AAuthor%3A%20Zhangdie%20Yuan%20and%20Zifeng%20Ding%20and%20Andreas%20Vlachos%0AAbstract%3A%20%20%20Forecasting%20is%20an%20important%20task%20in%20many%20domains%2C%20such%20as%20technology%20and%0Aeconomics.%20However%20existing%20forecasting%20benchmarks%20largely%20lack%20comprehensive%0Aconfidence%20assessment%2C%20focus%20on%20limited%20question%20types%2C%20and%20often%20consist%20of%0Aartificial%20questions%20that%20do%20not%20align%20with%20real-world%20human%20forecasting%20needs.%0ATo%20address%20these%20gaps%2C%20we%20introduce%20FOReCAst%20%28Future%20Outcome%20Reasoning%20and%0AConfidence%20Assessment%29%2C%20a%20benchmark%20that%20evaluates%20models%27%20ability%20to%20make%0Apredictions%20and%20their%20confidence%20in%20them.%20FOReCAst%20spans%20diverse%20forecasting%0Ascenarios%20involving%20Boolean%20questions%2C%20timeframe%20prediction%2C%20and%20quantity%0Aestimation%2C%20enabling%20a%20comprehensive%20evaluation%20of%20both%20prediction%20accuracy%20and%0Aconfidence%20calibration%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19676v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOReCAst%253A%2520The%2520Future%2520Outcome%2520Reasoning%2520and%2520Confidence%2520Assessment%250A%2520%2520Benchmark%26entry.906535625%3DZhangdie%2520Yuan%2520and%2520Zifeng%2520Ding%2520and%2520Andreas%2520Vlachos%26entry.1292438233%3D%2520%2520Forecasting%2520is%2520an%2520important%2520task%2520in%2520many%2520domains%252C%2520such%2520as%2520technology%2520and%250Aeconomics.%2520However%2520existing%2520forecasting%2520benchmarks%2520largely%2520lack%2520comprehensive%250Aconfidence%2520assessment%252C%2520focus%2520on%2520limited%2520question%2520types%252C%2520and%2520often%2520consist%2520of%250Aartificial%2520questions%2520that%2520do%2520not%2520align%2520with%2520real-world%2520human%2520forecasting%2520needs.%250ATo%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520FOReCAst%2520%2528Future%2520Outcome%2520Reasoning%2520and%250AConfidence%2520Assessment%2529%252C%2520a%2520benchmark%2520that%2520evaluates%2520models%2527%2520ability%2520to%2520make%250Apredictions%2520and%2520their%2520confidence%2520in%2520them.%2520FOReCAst%2520spans%2520diverse%2520forecasting%250Ascenarios%2520involving%2520Boolean%2520questions%252C%2520timeframe%2520prediction%252C%2520and%2520quantity%250Aestimation%252C%2520enabling%2520a%2520comprehensive%2520evaluation%2520of%2520both%2520prediction%2520accuracy%2520and%250Aconfidence%2520calibration%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19676v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOReCAst%3A%20The%20Future%20Outcome%20Reasoning%20and%20Confidence%20Assessment%0A%20%20Benchmark&entry.906535625=Zhangdie%20Yuan%20and%20Zifeng%20Ding%20and%20Andreas%20Vlachos&entry.1292438233=%20%20Forecasting%20is%20an%20important%20task%20in%20many%20domains%2C%20such%20as%20technology%20and%0Aeconomics.%20However%20existing%20forecasting%20benchmarks%20largely%20lack%20comprehensive%0Aconfidence%20assessment%2C%20focus%20on%20limited%20question%20types%2C%20and%20often%20consist%20of%0Aartificial%20questions%20that%20do%20not%20align%20with%20real-world%20human%20forecasting%20needs.%0ATo%20address%20these%20gaps%2C%20we%20introduce%20FOReCAst%20%28Future%20Outcome%20Reasoning%20and%0AConfidence%20Assessment%29%2C%20a%20benchmark%20that%20evaluates%20models%27%20ability%20to%20make%0Apredictions%20and%20their%20confidence%20in%20them.%20FOReCAst%20spans%20diverse%20forecasting%0Ascenarios%20involving%20Boolean%20questions%2C%20timeframe%20prediction%2C%20and%20quantity%0Aestimation%2C%20enabling%20a%20comprehensive%20evaluation%20of%20both%20prediction%20accuracy%20and%0Aconfidence%20calibration%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19676v4&entry.124074799=Read"},
{"title": "FairSHAP: Preprocessing for Fairness Through Attribution-Based Data\n  Augmentation", "author": "Lin Zhu and Yijun Bian and Lei You", "abstract": "  Ensuring fairness in machine learning models is critical, particularly in\nhigh-stakes domains where biased decisions can lead to serious societal\nconsequences. Existing preprocessing approaches generally lack transparent\nmechanisms for identifying which features or instances are responsible for\nunfairness. This obscures the rationale behind data modifications. We introduce\nFairSHAP, a novel pre-processing framework that leverages Shapley value\nattribution to improve both individual and group fairness. FairSHAP identifies\nfairness-critical instances in the training data using an interpretable measure\nof feature importance, and systematically modifies them through instance-level\nmatching across sensitive groups. This process reduces discriminative risk - an\nindividual fairness metric - while preserving data integrity and model\naccuracy. We demonstrate that FairSHAP significantly improves demographic\nparity and equality of opportunity across diverse tabular datasets, achieving\nfairness gains with minimal data perturbation and, in some cases, improved\npredictive performance. As a model-agnostic and transparent method, FairSHAP\nintegrates seamlessly into existing machine learning pipelines and provides\nactionable insights into the sources of bias.Our code is on\nhttps://github.com/youlei202/FairSHAP.\n", "link": "http://arxiv.org/abs/2505.11111v1", "date": "2025-05-16", "relevancy": 2.2808, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4678}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4574}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairSHAP%3A%20Preprocessing%20for%20Fairness%20Through%20Attribution-Based%20Data%0A%20%20Augmentation&body=Title%3A%20FairSHAP%3A%20Preprocessing%20for%20Fairness%20Through%20Attribution-Based%20Data%0A%20%20Augmentation%0AAuthor%3A%20Lin%20Zhu%20and%20Yijun%20Bian%20and%20Lei%20You%0AAbstract%3A%20%20%20Ensuring%20fairness%20in%20machine%20learning%20models%20is%20critical%2C%20particularly%20in%0Ahigh-stakes%20domains%20where%20biased%20decisions%20can%20lead%20to%20serious%20societal%0Aconsequences.%20Existing%20preprocessing%20approaches%20generally%20lack%20transparent%0Amechanisms%20for%20identifying%20which%20features%20or%20instances%20are%20responsible%20for%0Aunfairness.%20This%20obscures%20the%20rationale%20behind%20data%20modifications.%20We%20introduce%0AFairSHAP%2C%20a%20novel%20pre-processing%20framework%20that%20leverages%20Shapley%20value%0Aattribution%20to%20improve%20both%20individual%20and%20group%20fairness.%20FairSHAP%20identifies%0Afairness-critical%20instances%20in%20the%20training%20data%20using%20an%20interpretable%20measure%0Aof%20feature%20importance%2C%20and%20systematically%20modifies%20them%20through%20instance-level%0Amatching%20across%20sensitive%20groups.%20This%20process%20reduces%20discriminative%20risk%20-%20an%0Aindividual%20fairness%20metric%20-%20while%20preserving%20data%20integrity%20and%20model%0Aaccuracy.%20We%20demonstrate%20that%20FairSHAP%20significantly%20improves%20demographic%0Aparity%20and%20equality%20of%20opportunity%20across%20diverse%20tabular%20datasets%2C%20achieving%0Afairness%20gains%20with%20minimal%20data%20perturbation%20and%2C%20in%20some%20cases%2C%20improved%0Apredictive%20performance.%20As%20a%20model-agnostic%20and%20transparent%20method%2C%20FairSHAP%0Aintegrates%20seamlessly%20into%20existing%20machine%20learning%20pipelines%20and%20provides%0Aactionable%20insights%20into%20the%20sources%20of%20bias.Our%20code%20is%20on%0Ahttps%3A//github.com/youlei202/FairSHAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairSHAP%253A%2520Preprocessing%2520for%2520Fairness%2520Through%2520Attribution-Based%2520Data%250A%2520%2520Augmentation%26entry.906535625%3DLin%2520Zhu%2520and%2520Yijun%2520Bian%2520and%2520Lei%2520You%26entry.1292438233%3D%2520%2520Ensuring%2520fairness%2520in%2520machine%2520learning%2520models%2520is%2520critical%252C%2520particularly%2520in%250Ahigh-stakes%2520domains%2520where%2520biased%2520decisions%2520can%2520lead%2520to%2520serious%2520societal%250Aconsequences.%2520Existing%2520preprocessing%2520approaches%2520generally%2520lack%2520transparent%250Amechanisms%2520for%2520identifying%2520which%2520features%2520or%2520instances%2520are%2520responsible%2520for%250Aunfairness.%2520This%2520obscures%2520the%2520rationale%2520behind%2520data%2520modifications.%2520We%2520introduce%250AFairSHAP%252C%2520a%2520novel%2520pre-processing%2520framework%2520that%2520leverages%2520Shapley%2520value%250Aattribution%2520to%2520improve%2520both%2520individual%2520and%2520group%2520fairness.%2520FairSHAP%2520identifies%250Afairness-critical%2520instances%2520in%2520the%2520training%2520data%2520using%2520an%2520interpretable%2520measure%250Aof%2520feature%2520importance%252C%2520and%2520systematically%2520modifies%2520them%2520through%2520instance-level%250Amatching%2520across%2520sensitive%2520groups.%2520This%2520process%2520reduces%2520discriminative%2520risk%2520-%2520an%250Aindividual%2520fairness%2520metric%2520-%2520while%2520preserving%2520data%2520integrity%2520and%2520model%250Aaccuracy.%2520We%2520demonstrate%2520that%2520FairSHAP%2520significantly%2520improves%2520demographic%250Aparity%2520and%2520equality%2520of%2520opportunity%2520across%2520diverse%2520tabular%2520datasets%252C%2520achieving%250Afairness%2520gains%2520with%2520minimal%2520data%2520perturbation%2520and%252C%2520in%2520some%2520cases%252C%2520improved%250Apredictive%2520performance.%2520As%2520a%2520model-agnostic%2520and%2520transparent%2520method%252C%2520FairSHAP%250Aintegrates%2520seamlessly%2520into%2520existing%2520machine%2520learning%2520pipelines%2520and%2520provides%250Aactionable%2520insights%2520into%2520the%2520sources%2520of%2520bias.Our%2520code%2520is%2520on%250Ahttps%253A//github.com/youlei202/FairSHAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairSHAP%3A%20Preprocessing%20for%20Fairness%20Through%20Attribution-Based%20Data%0A%20%20Augmentation&entry.906535625=Lin%20Zhu%20and%20Yijun%20Bian%20and%20Lei%20You&entry.1292438233=%20%20Ensuring%20fairness%20in%20machine%20learning%20models%20is%20critical%2C%20particularly%20in%0Ahigh-stakes%20domains%20where%20biased%20decisions%20can%20lead%20to%20serious%20societal%0Aconsequences.%20Existing%20preprocessing%20approaches%20generally%20lack%20transparent%0Amechanisms%20for%20identifying%20which%20features%20or%20instances%20are%20responsible%20for%0Aunfairness.%20This%20obscures%20the%20rationale%20behind%20data%20modifications.%20We%20introduce%0AFairSHAP%2C%20a%20novel%20pre-processing%20framework%20that%20leverages%20Shapley%20value%0Aattribution%20to%20improve%20both%20individual%20and%20group%20fairness.%20FairSHAP%20identifies%0Afairness-critical%20instances%20in%20the%20training%20data%20using%20an%20interpretable%20measure%0Aof%20feature%20importance%2C%20and%20systematically%20modifies%20them%20through%20instance-level%0Amatching%20across%20sensitive%20groups.%20This%20process%20reduces%20discriminative%20risk%20-%20an%0Aindividual%20fairness%20metric%20-%20while%20preserving%20data%20integrity%20and%20model%0Aaccuracy.%20We%20demonstrate%20that%20FairSHAP%20significantly%20improves%20demographic%0Aparity%20and%20equality%20of%20opportunity%20across%20diverse%20tabular%20datasets%2C%20achieving%0Afairness%20gains%20with%20minimal%20data%20perturbation%20and%2C%20in%20some%20cases%2C%20improved%0Apredictive%20performance.%20As%20a%20model-agnostic%20and%20transparent%20method%2C%20FairSHAP%0Aintegrates%20seamlessly%20into%20existing%20machine%20learning%20pipelines%20and%20provides%0Aactionable%20insights%20into%20the%20sources%20of%20bias.Our%20code%20is%20on%0Ahttps%3A//github.com/youlei202/FairSHAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11111v1&entry.124074799=Read"},
{"title": "Flex: End-to-End Text-Instructed Visual Navigation from Foundation Model\n  Features", "author": "Makram Chahine and Alex Quach and Alaa Maalouf and Tsun-Hsuan Wang and Daniela Rus", "abstract": "  End-to-end learning directly maps sensory inputs to actions, creating highly\nintegrated and efficient policies for complex robotics tasks. However, such\nmodels often struggle to generalize beyond their training scenarios, limiting\nadaptability to new environments, tasks, and concepts. In this work, we\ninvestigate the minimal data requirements and architectural adaptations\nnecessary to achieve robust closed-loop performance with vision-based control\npolicies under unseen text instructions and visual distribution shifts. Our\nfindings are synthesized in Flex (Fly lexically), a framework that uses\npre-trained Vision Language Models (VLMs) as frozen patch-wise feature\nextractors, generating spatially aware embeddings that integrate semantic and\nvisual information. We demonstrate the effectiveness of this approach on a\nquadrotor fly-to-target task, where agents trained via behavior cloning on a\nsmall simulated dataset successfully generalize to real-world scenes with\ndiverse novel goals and command formulations.\n", "link": "http://arxiv.org/abs/2410.13002v2", "date": "2025-05-16", "relevancy": 2.2793, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flex%3A%20End-to-End%20Text-Instructed%20Visual%20Navigation%20from%20Foundation%20Model%0A%20%20Features&body=Title%3A%20Flex%3A%20End-to-End%20Text-Instructed%20Visual%20Navigation%20from%20Foundation%20Model%0A%20%20Features%0AAuthor%3A%20Makram%20Chahine%20and%20Alex%20Quach%20and%20Alaa%20Maalouf%20and%20Tsun-Hsuan%20Wang%20and%20Daniela%20Rus%0AAbstract%3A%20%20%20End-to-end%20learning%20directly%20maps%20sensory%20inputs%20to%20actions%2C%20creating%20highly%0Aintegrated%20and%20efficient%20policies%20for%20complex%20robotics%20tasks.%20However%2C%20such%0Amodels%20often%20struggle%20to%20generalize%20beyond%20their%20training%20scenarios%2C%20limiting%0Aadaptability%20to%20new%20environments%2C%20tasks%2C%20and%20concepts.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20minimal%20data%20requirements%20and%20architectural%20adaptations%0Anecessary%20to%20achieve%20robust%20closed-loop%20performance%20with%20vision-based%20control%0Apolicies%20under%20unseen%20text%20instructions%20and%20visual%20distribution%20shifts.%20Our%0Afindings%20are%20synthesized%20in%20Flex%20%28Fly%20lexically%29%2C%20a%20framework%20that%20uses%0Apre-trained%20Vision%20Language%20Models%20%28VLMs%29%20as%20frozen%20patch-wise%20feature%0Aextractors%2C%20generating%20spatially%20aware%20embeddings%20that%20integrate%20semantic%20and%0Avisual%20information.%20We%20demonstrate%20the%20effectiveness%20of%20this%20approach%20on%20a%0Aquadrotor%20fly-to-target%20task%2C%20where%20agents%20trained%20via%20behavior%20cloning%20on%20a%0Asmall%20simulated%20dataset%20successfully%20generalize%20to%20real-world%20scenes%20with%0Adiverse%20novel%20goals%20and%20command%20formulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlex%253A%2520End-to-End%2520Text-Instructed%2520Visual%2520Navigation%2520from%2520Foundation%2520Model%250A%2520%2520Features%26entry.906535625%3DMakram%2520Chahine%2520and%2520Alex%2520Quach%2520and%2520Alaa%2520Maalouf%2520and%2520Tsun-Hsuan%2520Wang%2520and%2520Daniela%2520Rus%26entry.1292438233%3D%2520%2520End-to-end%2520learning%2520directly%2520maps%2520sensory%2520inputs%2520to%2520actions%252C%2520creating%2520highly%250Aintegrated%2520and%2520efficient%2520policies%2520for%2520complex%2520robotics%2520tasks.%2520However%252C%2520such%250Amodels%2520often%2520struggle%2520to%2520generalize%2520beyond%2520their%2520training%2520scenarios%252C%2520limiting%250Aadaptability%2520to%2520new%2520environments%252C%2520tasks%252C%2520and%2520concepts.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520the%2520minimal%2520data%2520requirements%2520and%2520architectural%2520adaptations%250Anecessary%2520to%2520achieve%2520robust%2520closed-loop%2520performance%2520with%2520vision-based%2520control%250Apolicies%2520under%2520unseen%2520text%2520instructions%2520and%2520visual%2520distribution%2520shifts.%2520Our%250Afindings%2520are%2520synthesized%2520in%2520Flex%2520%2528Fly%2520lexically%2529%252C%2520a%2520framework%2520that%2520uses%250Apre-trained%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520as%2520frozen%2520patch-wise%2520feature%250Aextractors%252C%2520generating%2520spatially%2520aware%2520embeddings%2520that%2520integrate%2520semantic%2520and%250Avisual%2520information.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520approach%2520on%2520a%250Aquadrotor%2520fly-to-target%2520task%252C%2520where%2520agents%2520trained%2520via%2520behavior%2520cloning%2520on%2520a%250Asmall%2520simulated%2520dataset%2520successfully%2520generalize%2520to%2520real-world%2520scenes%2520with%250Adiverse%2520novel%2520goals%2520and%2520command%2520formulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flex%3A%20End-to-End%20Text-Instructed%20Visual%20Navigation%20from%20Foundation%20Model%0A%20%20Features&entry.906535625=Makram%20Chahine%20and%20Alex%20Quach%20and%20Alaa%20Maalouf%20and%20Tsun-Hsuan%20Wang%20and%20Daniela%20Rus&entry.1292438233=%20%20End-to-end%20learning%20directly%20maps%20sensory%20inputs%20to%20actions%2C%20creating%20highly%0Aintegrated%20and%20efficient%20policies%20for%20complex%20robotics%20tasks.%20However%2C%20such%0Amodels%20often%20struggle%20to%20generalize%20beyond%20their%20training%20scenarios%2C%20limiting%0Aadaptability%20to%20new%20environments%2C%20tasks%2C%20and%20concepts.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20minimal%20data%20requirements%20and%20architectural%20adaptations%0Anecessary%20to%20achieve%20robust%20closed-loop%20performance%20with%20vision-based%20control%0Apolicies%20under%20unseen%20text%20instructions%20and%20visual%20distribution%20shifts.%20Our%0Afindings%20are%20synthesized%20in%20Flex%20%28Fly%20lexically%29%2C%20a%20framework%20that%20uses%0Apre-trained%20Vision%20Language%20Models%20%28VLMs%29%20as%20frozen%20patch-wise%20feature%0Aextractors%2C%20generating%20spatially%20aware%20embeddings%20that%20integrate%20semantic%20and%0Avisual%20information.%20We%20demonstrate%20the%20effectiveness%20of%20this%20approach%20on%20a%0Aquadrotor%20fly-to-target%20task%2C%20where%20agents%20trained%20via%20behavior%20cloning%20on%20a%0Asmall%20simulated%20dataset%20successfully%20generalize%20to%20real-world%20scenes%20with%0Adiverse%20novel%20goals%20and%20command%20formulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13002v2&entry.124074799=Read"},
{"title": "GLOVA: Global and Local Variation-Aware Analog Circuit Design with\n  Risk-Sensitive Reinforcement Learning", "author": "Dongjun Kim and Junwoo Park and Chaehyeon Shin and Jaeheon Jung and Kyungho Shin and Seungheon Baek and Sanghyuk Heo and Woongrae Kim and Inchul Jeong and Joohwan Cho and Jongsun Park", "abstract": "  Analog/mixed-signal circuit design encounters significant challenges due to\nperformance degradation from process, voltage, and temperature (PVT)\nvariations. To achieve commercial-grade reliability, iterative manual design\nrevisions and extensive statistical simulations are required. While several\nstudies have aimed to automate variation aware analog design to reduce\ntime-to-market, the substantial mismatches in real-world wafers have not been\nthoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing\nframework that effectively manages the impact of diverse random mismatches to\nimprove robustness against PVT variations. In the proposed approach,\nrisk-sensitive reinforcement learning is leveraged to account for the\nreliability bound affected by PVT variations, and ensemble-based critic is\nintroduced to achieve sample-efficient learning. For design verification, we\nalso propose $\\mu$-$\\sigma$ evaluation and simulation reordering method to\nreduce simulation costs of identifying failed designs. GLOVA supports\nverification through industrial-level PVT variation evaluation methods,\nincluding corner simulation as well as global and local Monte Carlo (MC)\nsimulations. Compared to previous state-of-the-art variation-aware analog\nsizing frameworks, GLOVA achieves up to 80.5$\\times$ improvement in sample\nefficiency and 76.0$\\times$ reduction in time.\n", "link": "http://arxiv.org/abs/2505.11208v1", "date": "2025-05-16", "relevancy": 2.2725, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4618}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4528}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLOVA%3A%20Global%20and%20Local%20Variation-Aware%20Analog%20Circuit%20Design%20with%0A%20%20Risk-Sensitive%20Reinforcement%20Learning&body=Title%3A%20GLOVA%3A%20Global%20and%20Local%20Variation-Aware%20Analog%20Circuit%20Design%20with%0A%20%20Risk-Sensitive%20Reinforcement%20Learning%0AAuthor%3A%20Dongjun%20Kim%20and%20Junwoo%20Park%20and%20Chaehyeon%20Shin%20and%20Jaeheon%20Jung%20and%20Kyungho%20Shin%20and%20Seungheon%20Baek%20and%20Sanghyuk%20Heo%20and%20Woongrae%20Kim%20and%20Inchul%20Jeong%20and%20Joohwan%20Cho%20and%20Jongsun%20Park%0AAbstract%3A%20%20%20Analog/mixed-signal%20circuit%20design%20encounters%20significant%20challenges%20due%20to%0Aperformance%20degradation%20from%20process%2C%20voltage%2C%20and%20temperature%20%28PVT%29%0Avariations.%20To%20achieve%20commercial-grade%20reliability%2C%20iterative%20manual%20design%0Arevisions%20and%20extensive%20statistical%20simulations%20are%20required.%20While%20several%0Astudies%20have%20aimed%20to%20automate%20variation%20aware%20analog%20design%20to%20reduce%0Atime-to-market%2C%20the%20substantial%20mismatches%20in%20real-world%20wafers%20have%20not%20been%0Athoroughly%20addressed.%20In%20this%20paper%2C%20we%20present%20GLOVA%2C%20an%20analog%20circuit%20sizing%0Aframework%20that%20effectively%20manages%20the%20impact%20of%20diverse%20random%20mismatches%20to%0Aimprove%20robustness%20against%20PVT%20variations.%20In%20the%20proposed%20approach%2C%0Arisk-sensitive%20reinforcement%20learning%20is%20leveraged%20to%20account%20for%20the%0Areliability%20bound%20affected%20by%20PVT%20variations%2C%20and%20ensemble-based%20critic%20is%0Aintroduced%20to%20achieve%20sample-efficient%20learning.%20For%20design%20verification%2C%20we%0Aalso%20propose%20%24%5Cmu%24-%24%5Csigma%24%20evaluation%20and%20simulation%20reordering%20method%20to%0Areduce%20simulation%20costs%20of%20identifying%20failed%20designs.%20GLOVA%20supports%0Averification%20through%20industrial-level%20PVT%20variation%20evaluation%20methods%2C%0Aincluding%20corner%20simulation%20as%20well%20as%20global%20and%20local%20Monte%20Carlo%20%28MC%29%0Asimulations.%20Compared%20to%20previous%20state-of-the-art%20variation-aware%20analog%0Asizing%20frameworks%2C%20GLOVA%20achieves%20up%20to%2080.5%24%5Ctimes%24%20improvement%20in%20sample%0Aefficiency%20and%2076.0%24%5Ctimes%24%20reduction%20in%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLOVA%253A%2520Global%2520and%2520Local%2520Variation-Aware%2520Analog%2520Circuit%2520Design%2520with%250A%2520%2520Risk-Sensitive%2520Reinforcement%2520Learning%26entry.906535625%3DDongjun%2520Kim%2520and%2520Junwoo%2520Park%2520and%2520Chaehyeon%2520Shin%2520and%2520Jaeheon%2520Jung%2520and%2520Kyungho%2520Shin%2520and%2520Seungheon%2520Baek%2520and%2520Sanghyuk%2520Heo%2520and%2520Woongrae%2520Kim%2520and%2520Inchul%2520Jeong%2520and%2520Joohwan%2520Cho%2520and%2520Jongsun%2520Park%26entry.1292438233%3D%2520%2520Analog/mixed-signal%2520circuit%2520design%2520encounters%2520significant%2520challenges%2520due%2520to%250Aperformance%2520degradation%2520from%2520process%252C%2520voltage%252C%2520and%2520temperature%2520%2528PVT%2529%250Avariations.%2520To%2520achieve%2520commercial-grade%2520reliability%252C%2520iterative%2520manual%2520design%250Arevisions%2520and%2520extensive%2520statistical%2520simulations%2520are%2520required.%2520While%2520several%250Astudies%2520have%2520aimed%2520to%2520automate%2520variation%2520aware%2520analog%2520design%2520to%2520reduce%250Atime-to-market%252C%2520the%2520substantial%2520mismatches%2520in%2520real-world%2520wafers%2520have%2520not%2520been%250Athoroughly%2520addressed.%2520In%2520this%2520paper%252C%2520we%2520present%2520GLOVA%252C%2520an%2520analog%2520circuit%2520sizing%250Aframework%2520that%2520effectively%2520manages%2520the%2520impact%2520of%2520diverse%2520random%2520mismatches%2520to%250Aimprove%2520robustness%2520against%2520PVT%2520variations.%2520In%2520the%2520proposed%2520approach%252C%250Arisk-sensitive%2520reinforcement%2520learning%2520is%2520leveraged%2520to%2520account%2520for%2520the%250Areliability%2520bound%2520affected%2520by%2520PVT%2520variations%252C%2520and%2520ensemble-based%2520critic%2520is%250Aintroduced%2520to%2520achieve%2520sample-efficient%2520learning.%2520For%2520design%2520verification%252C%2520we%250Aalso%2520propose%2520%2524%255Cmu%2524-%2524%255Csigma%2524%2520evaluation%2520and%2520simulation%2520reordering%2520method%2520to%250Areduce%2520simulation%2520costs%2520of%2520identifying%2520failed%2520designs.%2520GLOVA%2520supports%250Averification%2520through%2520industrial-level%2520PVT%2520variation%2520evaluation%2520methods%252C%250Aincluding%2520corner%2520simulation%2520as%2520well%2520as%2520global%2520and%2520local%2520Monte%2520Carlo%2520%2528MC%2529%250Asimulations.%2520Compared%2520to%2520previous%2520state-of-the-art%2520variation-aware%2520analog%250Asizing%2520frameworks%252C%2520GLOVA%2520achieves%2520up%2520to%252080.5%2524%255Ctimes%2524%2520improvement%2520in%2520sample%250Aefficiency%2520and%252076.0%2524%255Ctimes%2524%2520reduction%2520in%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLOVA%3A%20Global%20and%20Local%20Variation-Aware%20Analog%20Circuit%20Design%20with%0A%20%20Risk-Sensitive%20Reinforcement%20Learning&entry.906535625=Dongjun%20Kim%20and%20Junwoo%20Park%20and%20Chaehyeon%20Shin%20and%20Jaeheon%20Jung%20and%20Kyungho%20Shin%20and%20Seungheon%20Baek%20and%20Sanghyuk%20Heo%20and%20Woongrae%20Kim%20and%20Inchul%20Jeong%20and%20Joohwan%20Cho%20and%20Jongsun%20Park&entry.1292438233=%20%20Analog/mixed-signal%20circuit%20design%20encounters%20significant%20challenges%20due%20to%0Aperformance%20degradation%20from%20process%2C%20voltage%2C%20and%20temperature%20%28PVT%29%0Avariations.%20To%20achieve%20commercial-grade%20reliability%2C%20iterative%20manual%20design%0Arevisions%20and%20extensive%20statistical%20simulations%20are%20required.%20While%20several%0Astudies%20have%20aimed%20to%20automate%20variation%20aware%20analog%20design%20to%20reduce%0Atime-to-market%2C%20the%20substantial%20mismatches%20in%20real-world%20wafers%20have%20not%20been%0Athoroughly%20addressed.%20In%20this%20paper%2C%20we%20present%20GLOVA%2C%20an%20analog%20circuit%20sizing%0Aframework%20that%20effectively%20manages%20the%20impact%20of%20diverse%20random%20mismatches%20to%0Aimprove%20robustness%20against%20PVT%20variations.%20In%20the%20proposed%20approach%2C%0Arisk-sensitive%20reinforcement%20learning%20is%20leveraged%20to%20account%20for%20the%0Areliability%20bound%20affected%20by%20PVT%20variations%2C%20and%20ensemble-based%20critic%20is%0Aintroduced%20to%20achieve%20sample-efficient%20learning.%20For%20design%20verification%2C%20we%0Aalso%20propose%20%24%5Cmu%24-%24%5Csigma%24%20evaluation%20and%20simulation%20reordering%20method%20to%0Areduce%20simulation%20costs%20of%20identifying%20failed%20designs.%20GLOVA%20supports%0Averification%20through%20industrial-level%20PVT%20variation%20evaluation%20methods%2C%0Aincluding%20corner%20simulation%20as%20well%20as%20global%20and%20local%20Monte%20Carlo%20%28MC%29%0Asimulations.%20Compared%20to%20previous%20state-of-the-art%20variation-aware%20analog%0Asizing%20frameworks%2C%20GLOVA%20achieves%20up%20to%2080.5%24%5Ctimes%24%20improvement%20in%20sample%0Aefficiency%20and%2076.0%24%5Ctimes%24%20reduction%20in%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11208v1&entry.124074799=Read"},
{"title": "Planar Velocity Estimation for Fast-Moving Mobile Robots Using\n  Event-Based Optical Flow", "author": "Liam Boyle and Jonas K\u00fchne and Nicolas Baumann and Niklas Bastuck and Michele Magno", "abstract": "  Accurate velocity estimation is critical in mobile robotics, particularly for\ndriver assistance systems and autonomous driving. Wheel odometry fused with\nInertial Measurement Unit (IMU) data is a widely used method for velocity\nestimation; however, it typically requires strong assumptions, such as non-slip\nsteering, or complex vehicle dynamics models that do not hold under varying\nenvironmental conditions like slippery surfaces. We introduce an approach to\nvelocity estimation that is decoupled from wheel-to-surface traction\nassumptions by leveraging planar kinematics in combination with optical flow\nfrom event cameras pointed perpendicularly at the ground. The asynchronous\nmicro-second latency and high dynamic range of event cameras make them highly\nrobust to motion blur, a common challenge in vision-based perception techniques\nfor autonomous driving. The proposed method is evaluated through in-field\nexperiments on a 1:10 scale autonomous racing platform and compared to precise\nmotion capture data, demonstrating not only performance on par with the\nstate-of-the-art Event-VIO method but also a 38.3 % improvement in lateral\nerror. Qualitative experiments at highway speeds of up to 32 m/s further\nconfirm the effectiveness of our approach, indicating significant potential for\nreal-world deployment.\n", "link": "http://arxiv.org/abs/2505.11116v1", "date": "2025-05-16", "relevancy": 2.2719, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6051}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5623}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planar%20Velocity%20Estimation%20for%20Fast-Moving%20Mobile%20Robots%20Using%0A%20%20Event-Based%20Optical%20Flow&body=Title%3A%20Planar%20Velocity%20Estimation%20for%20Fast-Moving%20Mobile%20Robots%20Using%0A%20%20Event-Based%20Optical%20Flow%0AAuthor%3A%20Liam%20Boyle%20and%20Jonas%20K%C3%BChne%20and%20Nicolas%20Baumann%20and%20Niklas%20Bastuck%20and%20Michele%20Magno%0AAbstract%3A%20%20%20Accurate%20velocity%20estimation%20is%20critical%20in%20mobile%20robotics%2C%20particularly%20for%0Adriver%20assistance%20systems%20and%20autonomous%20driving.%20Wheel%20odometry%20fused%20with%0AInertial%20Measurement%20Unit%20%28IMU%29%20data%20is%20a%20widely%20used%20method%20for%20velocity%0Aestimation%3B%20however%2C%20it%20typically%20requires%20strong%20assumptions%2C%20such%20as%20non-slip%0Asteering%2C%20or%20complex%20vehicle%20dynamics%20models%20that%20do%20not%20hold%20under%20varying%0Aenvironmental%20conditions%20like%20slippery%20surfaces.%20We%20introduce%20an%20approach%20to%0Avelocity%20estimation%20that%20is%20decoupled%20from%20wheel-to-surface%20traction%0Aassumptions%20by%20leveraging%20planar%20kinematics%20in%20combination%20with%20optical%20flow%0Afrom%20event%20cameras%20pointed%20perpendicularly%20at%20the%20ground.%20The%20asynchronous%0Amicro-second%20latency%20and%20high%20dynamic%20range%20of%20event%20cameras%20make%20them%20highly%0Arobust%20to%20motion%20blur%2C%20a%20common%20challenge%20in%20vision-based%20perception%20techniques%0Afor%20autonomous%20driving.%20The%20proposed%20method%20is%20evaluated%20through%20in-field%0Aexperiments%20on%20a%201%3A10%20scale%20autonomous%20racing%20platform%20and%20compared%20to%20precise%0Amotion%20capture%20data%2C%20demonstrating%20not%20only%20performance%20on%20par%20with%20the%0Astate-of-the-art%20Event-VIO%20method%20but%20also%20a%2038.3%20%25%20improvement%20in%20lateral%0Aerror.%20Qualitative%20experiments%20at%20highway%20speeds%20of%20up%20to%2032%20m/s%20further%0Aconfirm%20the%20effectiveness%20of%20our%20approach%2C%20indicating%20significant%20potential%20for%0Areal-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanar%2520Velocity%2520Estimation%2520for%2520Fast-Moving%2520Mobile%2520Robots%2520Using%250A%2520%2520Event-Based%2520Optical%2520Flow%26entry.906535625%3DLiam%2520Boyle%2520and%2520Jonas%2520K%25C3%25BChne%2520and%2520Nicolas%2520Baumann%2520and%2520Niklas%2520Bastuck%2520and%2520Michele%2520Magno%26entry.1292438233%3D%2520%2520Accurate%2520velocity%2520estimation%2520is%2520critical%2520in%2520mobile%2520robotics%252C%2520particularly%2520for%250Adriver%2520assistance%2520systems%2520and%2520autonomous%2520driving.%2520Wheel%2520odometry%2520fused%2520with%250AInertial%2520Measurement%2520Unit%2520%2528IMU%2529%2520data%2520is%2520a%2520widely%2520used%2520method%2520for%2520velocity%250Aestimation%253B%2520however%252C%2520it%2520typically%2520requires%2520strong%2520assumptions%252C%2520such%2520as%2520non-slip%250Asteering%252C%2520or%2520complex%2520vehicle%2520dynamics%2520models%2520that%2520do%2520not%2520hold%2520under%2520varying%250Aenvironmental%2520conditions%2520like%2520slippery%2520surfaces.%2520We%2520introduce%2520an%2520approach%2520to%250Avelocity%2520estimation%2520that%2520is%2520decoupled%2520from%2520wheel-to-surface%2520traction%250Aassumptions%2520by%2520leveraging%2520planar%2520kinematics%2520in%2520combination%2520with%2520optical%2520flow%250Afrom%2520event%2520cameras%2520pointed%2520perpendicularly%2520at%2520the%2520ground.%2520The%2520asynchronous%250Amicro-second%2520latency%2520and%2520high%2520dynamic%2520range%2520of%2520event%2520cameras%2520make%2520them%2520highly%250Arobust%2520to%2520motion%2520blur%252C%2520a%2520common%2520challenge%2520in%2520vision-based%2520perception%2520techniques%250Afor%2520autonomous%2520driving.%2520The%2520proposed%2520method%2520is%2520evaluated%2520through%2520in-field%250Aexperiments%2520on%2520a%25201%253A10%2520scale%2520autonomous%2520racing%2520platform%2520and%2520compared%2520to%2520precise%250Amotion%2520capture%2520data%252C%2520demonstrating%2520not%2520only%2520performance%2520on%2520par%2520with%2520the%250Astate-of-the-art%2520Event-VIO%2520method%2520but%2520also%2520a%252038.3%2520%2525%2520improvement%2520in%2520lateral%250Aerror.%2520Qualitative%2520experiments%2520at%2520highway%2520speeds%2520of%2520up%2520to%252032%2520m/s%2520further%250Aconfirm%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520indicating%2520significant%2520potential%2520for%250Areal-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planar%20Velocity%20Estimation%20for%20Fast-Moving%20Mobile%20Robots%20Using%0A%20%20Event-Based%20Optical%20Flow&entry.906535625=Liam%20Boyle%20and%20Jonas%20K%C3%BChne%20and%20Nicolas%20Baumann%20and%20Niklas%20Bastuck%20and%20Michele%20Magno&entry.1292438233=%20%20Accurate%20velocity%20estimation%20is%20critical%20in%20mobile%20robotics%2C%20particularly%20for%0Adriver%20assistance%20systems%20and%20autonomous%20driving.%20Wheel%20odometry%20fused%20with%0AInertial%20Measurement%20Unit%20%28IMU%29%20data%20is%20a%20widely%20used%20method%20for%20velocity%0Aestimation%3B%20however%2C%20it%20typically%20requires%20strong%20assumptions%2C%20such%20as%20non-slip%0Asteering%2C%20or%20complex%20vehicle%20dynamics%20models%20that%20do%20not%20hold%20under%20varying%0Aenvironmental%20conditions%20like%20slippery%20surfaces.%20We%20introduce%20an%20approach%20to%0Avelocity%20estimation%20that%20is%20decoupled%20from%20wheel-to-surface%20traction%0Aassumptions%20by%20leveraging%20planar%20kinematics%20in%20combination%20with%20optical%20flow%0Afrom%20event%20cameras%20pointed%20perpendicularly%20at%20the%20ground.%20The%20asynchronous%0Amicro-second%20latency%20and%20high%20dynamic%20range%20of%20event%20cameras%20make%20them%20highly%0Arobust%20to%20motion%20blur%2C%20a%20common%20challenge%20in%20vision-based%20perception%20techniques%0Afor%20autonomous%20driving.%20The%20proposed%20method%20is%20evaluated%20through%20in-field%0Aexperiments%20on%20a%201%3A10%20scale%20autonomous%20racing%20platform%20and%20compared%20to%20precise%0Amotion%20capture%20data%2C%20demonstrating%20not%20only%20performance%20on%20par%20with%20the%0Astate-of-the-art%20Event-VIO%20method%20but%20also%20a%2038.3%20%25%20improvement%20in%20lateral%0Aerror.%20Qualitative%20experiments%20at%20highway%20speeds%20of%20up%20to%2032%20m/s%20further%0Aconfirm%20the%20effectiveness%20of%20our%20approach%2C%20indicating%20significant%20potential%20for%0Areal-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11116v1&entry.124074799=Read"},
{"title": "Empowering Agentic Video Analytics Systems with Video Language Models", "author": "Yuxuan Yan and Shiqi Jiang and Ting Cao and Yifan Yang and Qianqian Yang and Yuanchao Shu and Yuqing Yang and Lili Qiu", "abstract": "  AI-driven video analytics has become increasingly pivotal across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Video-Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVAS, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVAS incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively, significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an\naccuracy of 75.8%.\n", "link": "http://arxiv.org/abs/2505.00254v3", "date": "2025-05-16", "relevancy": 2.2694, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5709}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Agentic%20Video%20Analytics%20Systems%20with%20Video%20Language%20Models&body=Title%3A%20Empowering%20Agentic%20Video%20Analytics%20Systems%20with%20Video%20Language%20Models%0AAuthor%3A%20Yuxuan%20Yan%20and%20Shiqi%20Jiang%20and%20Ting%20Cao%20and%20Yifan%20Yang%20and%20Qianqian%20Yang%20and%20Yuanchao%20Shu%20and%20Yuqing%20Yang%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20AI-driven%20video%20analytics%20has%20become%20increasingly%20pivotal%20across%20diverse%0Adomains.%20However%2C%20existing%20systems%20are%20often%20constrained%20to%20specific%2C%0Apredefined%20tasks%2C%20limiting%20their%20adaptability%20in%20open-ended%20analytical%0Ascenarios.%20The%20recent%20emergence%20of%20Video-Language%20Models%20%28VLMs%29%20as%0Atransformative%20technologies%20offers%20significant%20potential%20for%20enabling%0Aopen-ended%20video%20understanding%2C%20reasoning%2C%20and%20analytics.%20Nevertheless%2C%20their%0Alimited%20context%20windows%20present%20challenges%20when%20processing%20ultra-long%20video%0Acontent%2C%20which%20is%20prevalent%20in%20real-world%20applications.%20To%20address%20this%2C%20we%0Aintroduce%20AVAS%2C%20a%20VLM-powered%20system%20designed%20for%20open-ended%2C%20advanced%20video%0Aanalytics.%20AVAS%20incorporates%20two%20key%20innovations%3A%20%281%29%20the%20near%20real-time%0Aconstruction%20of%20Event%20Knowledge%20Graphs%20%28EKGs%29%20for%20efficient%20indexing%20of%20long%20or%0Acontinuous%20video%20streams%2C%20and%20%282%29%20an%20agentic%20retrieval-generation%20mechanism%0Athat%20leverages%20EKGs%20to%20handle%20complex%20and%20diverse%20queries.%20Comprehensive%0Aevaluations%20on%20public%20benchmarks%2C%20LVBench%20and%20VideoMME-Long%2C%20demonstrate%20that%0AAVAS%20achieves%20state-of-the-art%20performance%2C%20attaining%2062.3%25%20and%2064.1%25%20accuracy%2C%0Arespectively%2C%20significantly%20surpassing%20existing%20VLM%20and%20video%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems.%20Furthermore%2C%20to%20evaluate%20video%0Aanalytics%20in%20ultra-long%20and%20open-world%20video%20scenarios%2C%20we%20introduce%20a%20new%0Abenchmark%2C%20AVAS-100.%20This%20benchmark%20comprises%208%20videos%2C%20each%20exceeding%2010%20hours%0Ain%20duration%2C%20along%20with%20120%20manually%20annotated%2C%20diverse%2C%20and%20complex%0Aquestion-answer%20pairs.%20On%20AVAS-100%2C%20AVAS%20achieves%20top-tier%20performance%20with%20an%0Aaccuracy%20of%2075.8%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00254v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Agentic%2520Video%2520Analytics%2520Systems%2520with%2520Video%2520Language%2520Models%26entry.906535625%3DYuxuan%2520Yan%2520and%2520Shiqi%2520Jiang%2520and%2520Ting%2520Cao%2520and%2520Yifan%2520Yang%2520and%2520Qianqian%2520Yang%2520and%2520Yuanchao%2520Shu%2520and%2520Yuqing%2520Yang%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520AI-driven%2520video%2520analytics%2520has%2520become%2520increasingly%2520pivotal%2520across%2520diverse%250Adomains.%2520However%252C%2520existing%2520systems%2520are%2520often%2520constrained%2520to%2520specific%252C%250Apredefined%2520tasks%252C%2520limiting%2520their%2520adaptability%2520in%2520open-ended%2520analytical%250Ascenarios.%2520The%2520recent%2520emergence%2520of%2520Video-Language%2520Models%2520%2528VLMs%2529%2520as%250Atransformative%2520technologies%2520offers%2520significant%2520potential%2520for%2520enabling%250Aopen-ended%2520video%2520understanding%252C%2520reasoning%252C%2520and%2520analytics.%2520Nevertheless%252C%2520their%250Alimited%2520context%2520windows%2520present%2520challenges%2520when%2520processing%2520ultra-long%2520video%250Acontent%252C%2520which%2520is%2520prevalent%2520in%2520real-world%2520applications.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520AVAS%252C%2520a%2520VLM-powered%2520system%2520designed%2520for%2520open-ended%252C%2520advanced%2520video%250Aanalytics.%2520AVAS%2520incorporates%2520two%2520key%2520innovations%253A%2520%25281%2529%2520the%2520near%2520real-time%250Aconstruction%2520of%2520Event%2520Knowledge%2520Graphs%2520%2528EKGs%2529%2520for%2520efficient%2520indexing%2520of%2520long%2520or%250Acontinuous%2520video%2520streams%252C%2520and%2520%25282%2529%2520an%2520agentic%2520retrieval-generation%2520mechanism%250Athat%2520leverages%2520EKGs%2520to%2520handle%2520complex%2520and%2520diverse%2520queries.%2520Comprehensive%250Aevaluations%2520on%2520public%2520benchmarks%252C%2520LVBench%2520and%2520VideoMME-Long%252C%2520demonstrate%2520that%250AAVAS%2520achieves%2520state-of-the-art%2520performance%252C%2520attaining%252062.3%2525%2520and%252064.1%2525%2520accuracy%252C%250Arespectively%252C%2520significantly%2520surpassing%2520existing%2520VLM%2520and%2520video%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems.%2520Furthermore%252C%2520to%2520evaluate%2520video%250Aanalytics%2520in%2520ultra-long%2520and%2520open-world%2520video%2520scenarios%252C%2520we%2520introduce%2520a%2520new%250Abenchmark%252C%2520AVAS-100.%2520This%2520benchmark%2520comprises%25208%2520videos%252C%2520each%2520exceeding%252010%2520hours%250Ain%2520duration%252C%2520along%2520with%2520120%2520manually%2520annotated%252C%2520diverse%252C%2520and%2520complex%250Aquestion-answer%2520pairs.%2520On%2520AVAS-100%252C%2520AVAS%2520achieves%2520top-tier%2520performance%2520with%2520an%250Aaccuracy%2520of%252075.8%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00254v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Agentic%20Video%20Analytics%20Systems%20with%20Video%20Language%20Models&entry.906535625=Yuxuan%20Yan%20and%20Shiqi%20Jiang%20and%20Ting%20Cao%20and%20Yifan%20Yang%20and%20Qianqian%20Yang%20and%20Yuanchao%20Shu%20and%20Yuqing%20Yang%20and%20Lili%20Qiu&entry.1292438233=%20%20AI-driven%20video%20analytics%20has%20become%20increasingly%20pivotal%20across%20diverse%0Adomains.%20However%2C%20existing%20systems%20are%20often%20constrained%20to%20specific%2C%0Apredefined%20tasks%2C%20limiting%20their%20adaptability%20in%20open-ended%20analytical%0Ascenarios.%20The%20recent%20emergence%20of%20Video-Language%20Models%20%28VLMs%29%20as%0Atransformative%20technologies%20offers%20significant%20potential%20for%20enabling%0Aopen-ended%20video%20understanding%2C%20reasoning%2C%20and%20analytics.%20Nevertheless%2C%20their%0Alimited%20context%20windows%20present%20challenges%20when%20processing%20ultra-long%20video%0Acontent%2C%20which%20is%20prevalent%20in%20real-world%20applications.%20To%20address%20this%2C%20we%0Aintroduce%20AVAS%2C%20a%20VLM-powered%20system%20designed%20for%20open-ended%2C%20advanced%20video%0Aanalytics.%20AVAS%20incorporates%20two%20key%20innovations%3A%20%281%29%20the%20near%20real-time%0Aconstruction%20of%20Event%20Knowledge%20Graphs%20%28EKGs%29%20for%20efficient%20indexing%20of%20long%20or%0Acontinuous%20video%20streams%2C%20and%20%282%29%20an%20agentic%20retrieval-generation%20mechanism%0Athat%20leverages%20EKGs%20to%20handle%20complex%20and%20diverse%20queries.%20Comprehensive%0Aevaluations%20on%20public%20benchmarks%2C%20LVBench%20and%20VideoMME-Long%2C%20demonstrate%20that%0AAVAS%20achieves%20state-of-the-art%20performance%2C%20attaining%2062.3%25%20and%2064.1%25%20accuracy%2C%0Arespectively%2C%20significantly%20surpassing%20existing%20VLM%20and%20video%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems.%20Furthermore%2C%20to%20evaluate%20video%0Aanalytics%20in%20ultra-long%20and%20open-world%20video%20scenarios%2C%20we%20introduce%20a%20new%0Abenchmark%2C%20AVAS-100.%20This%20benchmark%20comprises%208%20videos%2C%20each%20exceeding%2010%20hours%0Ain%20duration%2C%20along%20with%20120%20manually%20annotated%2C%20diverse%2C%20and%20complex%0Aquestion-answer%20pairs.%20On%20AVAS-100%2C%20AVAS%20achieves%20top-tier%20performance%20with%20an%0Aaccuracy%20of%2075.8%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00254v3&entry.124074799=Read"},
{"title": "PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video", "author": "Makoto Yamada and Kian Ming A. Chai and Ayoub Rhim and Satoki Ishikawa and Mohammad Sabokrou and Yao-Hung Hubert Tsai", "abstract": "  Recent advances in self-supervised learning (SSL) have revolutionized\ncomputer vision through innovative architectures and learning objectives, yet\nthey have not fully leveraged insights from biological visual processing\nsystems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is\nbased on a ResNet backbone and operates on static image inputs with strong\naugmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based\narchitecture that processes temporal visual input (that is, sequences of\nimages) without relying on strong augmentation. Our model leverages variational\ninference to learn robust visual representations from continuous input streams,\nsimilar to human visual processing. Through extensive experimentation, we\ndemonstrate that PhiNet v2 achieves competitive performance compared to\nstate-of-the-art vision foundation models, while maintaining the ability to\nlearn from sequential input without strong data augmentation. This work\nrepresents a significant step toward more biologically plausible computer\nvision systems that process visual information in a manner more closely aligned\nwith human cognitive processes.\n", "link": "http://arxiv.org/abs/2505.11129v1", "date": "2025-05-16", "relevancy": 2.2488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhiNet%20v2%3A%20A%20Mask-Free%20Brain-Inspired%20Vision%20Foundation%20Model%20from%20Video&body=Title%3A%20PhiNet%20v2%3A%20A%20Mask-Free%20Brain-Inspired%20Vision%20Foundation%20Model%20from%20Video%0AAuthor%3A%20Makoto%20Yamada%20and%20Kian%20Ming%20A.%20Chai%20and%20Ayoub%20Rhim%20and%20Satoki%20Ishikawa%20and%20Mohammad%20Sabokrou%20and%20Yao-Hung%20Hubert%20Tsai%0AAbstract%3A%20%20%20Recent%20advances%20in%20self-supervised%20learning%20%28SSL%29%20have%20revolutionized%0Acomputer%20vision%20through%20innovative%20architectures%20and%20learning%20objectives%2C%20yet%0Athey%20have%20not%20fully%20leveraged%20insights%20from%20biological%20visual%20processing%0Asystems.%20Recently%2C%20a%20brain-inspired%20SSL%20model%20named%20PhiNet%20was%20proposed%3B%20it%20is%0Abased%20on%20a%20ResNet%20backbone%20and%20operates%20on%20static%20image%20inputs%20with%20strong%0Aaugmentation.%20In%20this%20paper%2C%20we%20introduce%20PhiNet%20v2%2C%20a%20novel%20Transformer-based%0Aarchitecture%20that%20processes%20temporal%20visual%20input%20%28that%20is%2C%20sequences%20of%0Aimages%29%20without%20relying%20on%20strong%20augmentation.%20Our%20model%20leverages%20variational%0Ainference%20to%20learn%20robust%20visual%20representations%20from%20continuous%20input%20streams%2C%0Asimilar%20to%20human%20visual%20processing.%20Through%20extensive%20experimentation%2C%20we%0Ademonstrate%20that%20PhiNet%20v2%20achieves%20competitive%20performance%20compared%20to%0Astate-of-the-art%20vision%20foundation%20models%2C%20while%20maintaining%20the%20ability%20to%0Alearn%20from%20sequential%20input%20without%20strong%20data%20augmentation.%20This%20work%0Arepresents%20a%20significant%20step%20toward%20more%20biologically%20plausible%20computer%0Avision%20systems%20that%20process%20visual%20information%20in%20a%20manner%20more%20closely%20aligned%0Awith%20human%20cognitive%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhiNet%2520v2%253A%2520A%2520Mask-Free%2520Brain-Inspired%2520Vision%2520Foundation%2520Model%2520from%2520Video%26entry.906535625%3DMakoto%2520Yamada%2520and%2520Kian%2520Ming%2520A.%2520Chai%2520and%2520Ayoub%2520Rhim%2520and%2520Satoki%2520Ishikawa%2520and%2520Mohammad%2520Sabokrou%2520and%2520Yao-Hung%2520Hubert%2520Tsai%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520self-supervised%2520learning%2520%2528SSL%2529%2520have%2520revolutionized%250Acomputer%2520vision%2520through%2520innovative%2520architectures%2520and%2520learning%2520objectives%252C%2520yet%250Athey%2520have%2520not%2520fully%2520leveraged%2520insights%2520from%2520biological%2520visual%2520processing%250Asystems.%2520Recently%252C%2520a%2520brain-inspired%2520SSL%2520model%2520named%2520PhiNet%2520was%2520proposed%253B%2520it%2520is%250Abased%2520on%2520a%2520ResNet%2520backbone%2520and%2520operates%2520on%2520static%2520image%2520inputs%2520with%2520strong%250Aaugmentation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PhiNet%2520v2%252C%2520a%2520novel%2520Transformer-based%250Aarchitecture%2520that%2520processes%2520temporal%2520visual%2520input%2520%2528that%2520is%252C%2520sequences%2520of%250Aimages%2529%2520without%2520relying%2520on%2520strong%2520augmentation.%2520Our%2520model%2520leverages%2520variational%250Ainference%2520to%2520learn%2520robust%2520visual%2520representations%2520from%2520continuous%2520input%2520streams%252C%250Asimilar%2520to%2520human%2520visual%2520processing.%2520Through%2520extensive%2520experimentation%252C%2520we%250Ademonstrate%2520that%2520PhiNet%2520v2%2520achieves%2520competitive%2520performance%2520compared%2520to%250Astate-of-the-art%2520vision%2520foundation%2520models%252C%2520while%2520maintaining%2520the%2520ability%2520to%250Alearn%2520from%2520sequential%2520input%2520without%2520strong%2520data%2520augmentation.%2520This%2520work%250Arepresents%2520a%2520significant%2520step%2520toward%2520more%2520biologically%2520plausible%2520computer%250Avision%2520systems%2520that%2520process%2520visual%2520information%2520in%2520a%2520manner%2520more%2520closely%2520aligned%250Awith%2520human%2520cognitive%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhiNet%20v2%3A%20A%20Mask-Free%20Brain-Inspired%20Vision%20Foundation%20Model%20from%20Video&entry.906535625=Makoto%20Yamada%20and%20Kian%20Ming%20A.%20Chai%20and%20Ayoub%20Rhim%20and%20Satoki%20Ishikawa%20and%20Mohammad%20Sabokrou%20and%20Yao-Hung%20Hubert%20Tsai&entry.1292438233=%20%20Recent%20advances%20in%20self-supervised%20learning%20%28SSL%29%20have%20revolutionized%0Acomputer%20vision%20through%20innovative%20architectures%20and%20learning%20objectives%2C%20yet%0Athey%20have%20not%20fully%20leveraged%20insights%20from%20biological%20visual%20processing%0Asystems.%20Recently%2C%20a%20brain-inspired%20SSL%20model%20named%20PhiNet%20was%20proposed%3B%20it%20is%0Abased%20on%20a%20ResNet%20backbone%20and%20operates%20on%20static%20image%20inputs%20with%20strong%0Aaugmentation.%20In%20this%20paper%2C%20we%20introduce%20PhiNet%20v2%2C%20a%20novel%20Transformer-based%0Aarchitecture%20that%20processes%20temporal%20visual%20input%20%28that%20is%2C%20sequences%20of%0Aimages%29%20without%20relying%20on%20strong%20augmentation.%20Our%20model%20leverages%20variational%0Ainference%20to%20learn%20robust%20visual%20representations%20from%20continuous%20input%20streams%2C%0Asimilar%20to%20human%20visual%20processing.%20Through%20extensive%20experimentation%2C%20we%0Ademonstrate%20that%20PhiNet%20v2%20achieves%20competitive%20performance%20compared%20to%0Astate-of-the-art%20vision%20foundation%20models%2C%20while%20maintaining%20the%20ability%20to%0Alearn%20from%20sequential%20input%20without%20strong%20data%20augmentation.%20This%20work%0Arepresents%20a%20significant%20step%20toward%20more%20biologically%20plausible%20computer%0Avision%20systems%20that%20process%20visual%20information%20in%20a%20manner%20more%20closely%20aligned%0Awith%20human%20cognitive%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11129v1&entry.124074799=Read"},
{"title": "Conditioning Matters: Training Diffusion Policies is Faster Than You\n  Think", "author": "Zibin Dong and Yicheng Liu and Yinchuan Li and Hang Zhao and Jianye Hao", "abstract": "  Diffusion policies have emerged as a mainstream paradigm for building\nvision-language-action (VLA) models. Although they demonstrate strong robot\ncontrol capabilities, their training efficiency remains suboptimal. In this\nwork, we identify a fundamental challenge in conditional diffusion policy\ntraining: when generative conditions are hard to distinguish, the training\nobjective degenerates into modeling the marginal action distribution, a\nphenomenon we term loss collapse. To overcome this, we propose Cocos, a simple\nyet general solution that modifies the source distribution in the conditional\nflow matching to be condition-dependent. By anchoring the source distribution\naround semantics extracted from condition inputs, Cocos encourages stronger\ncondition integration and prevents the loss collapse. We provide theoretical\njustification and extensive empirical results across simulation and real-world\nbenchmarks. Our method achieves faster convergence and higher success rates\nthan existing approaches, matching the performance of large-scale pre-trained\nVLAs using significantly fewer gradient steps and parameters. Cocos is\nlightweight, easy to implement, and compatible with diverse policy\narchitectures, offering a general-purpose improvement to diffusion policy\ntraining.\n", "link": "http://arxiv.org/abs/2505.11123v1", "date": "2025-05-16", "relevancy": 2.2488, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6264}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5497}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditioning%20Matters%3A%20Training%20Diffusion%20Policies%20is%20Faster%20Than%20You%0A%20%20Think&body=Title%3A%20Conditioning%20Matters%3A%20Training%20Diffusion%20Policies%20is%20Faster%20Than%20You%0A%20%20Think%0AAuthor%3A%20Zibin%20Dong%20and%20Yicheng%20Liu%20and%20Yinchuan%20Li%20and%20Hang%20Zhao%20and%20Jianye%20Hao%0AAbstract%3A%20%20%20Diffusion%20policies%20have%20emerged%20as%20a%20mainstream%20paradigm%20for%20building%0Avision-language-action%20%28VLA%29%20models.%20Although%20they%20demonstrate%20strong%20robot%0Acontrol%20capabilities%2C%20their%20training%20efficiency%20remains%20suboptimal.%20In%20this%0Awork%2C%20we%20identify%20a%20fundamental%20challenge%20in%20conditional%20diffusion%20policy%0Atraining%3A%20when%20generative%20conditions%20are%20hard%20to%20distinguish%2C%20the%20training%0Aobjective%20degenerates%20into%20modeling%20the%20marginal%20action%20distribution%2C%20a%0Aphenomenon%20we%20term%20loss%20collapse.%20To%20overcome%20this%2C%20we%20propose%20Cocos%2C%20a%20simple%0Ayet%20general%20solution%20that%20modifies%20the%20source%20distribution%20in%20the%20conditional%0Aflow%20matching%20to%20be%20condition-dependent.%20By%20anchoring%20the%20source%20distribution%0Aaround%20semantics%20extracted%20from%20condition%20inputs%2C%20Cocos%20encourages%20stronger%0Acondition%20integration%20and%20prevents%20the%20loss%20collapse.%20We%20provide%20theoretical%0Ajustification%20and%20extensive%20empirical%20results%20across%20simulation%20and%20real-world%0Abenchmarks.%20Our%20method%20achieves%20faster%20convergence%20and%20higher%20success%20rates%0Athan%20existing%20approaches%2C%20matching%20the%20performance%20of%20large-scale%20pre-trained%0AVLAs%20using%20significantly%20fewer%20gradient%20steps%20and%20parameters.%20Cocos%20is%0Alightweight%2C%20easy%20to%20implement%2C%20and%20compatible%20with%20diverse%20policy%0Aarchitectures%2C%20offering%20a%20general-purpose%20improvement%20to%20diffusion%20policy%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditioning%2520Matters%253A%2520Training%2520Diffusion%2520Policies%2520is%2520Faster%2520Than%2520You%250A%2520%2520Think%26entry.906535625%3DZibin%2520Dong%2520and%2520Yicheng%2520Liu%2520and%2520Yinchuan%2520Li%2520and%2520Hang%2520Zhao%2520and%2520Jianye%2520Hao%26entry.1292438233%3D%2520%2520Diffusion%2520policies%2520have%2520emerged%2520as%2520a%2520mainstream%2520paradigm%2520for%2520building%250Avision-language-action%2520%2528VLA%2529%2520models.%2520Although%2520they%2520demonstrate%2520strong%2520robot%250Acontrol%2520capabilities%252C%2520their%2520training%2520efficiency%2520remains%2520suboptimal.%2520In%2520this%250Awork%252C%2520we%2520identify%2520a%2520fundamental%2520challenge%2520in%2520conditional%2520diffusion%2520policy%250Atraining%253A%2520when%2520generative%2520conditions%2520are%2520hard%2520to%2520distinguish%252C%2520the%2520training%250Aobjective%2520degenerates%2520into%2520modeling%2520the%2520marginal%2520action%2520distribution%252C%2520a%250Aphenomenon%2520we%2520term%2520loss%2520collapse.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520Cocos%252C%2520a%2520simple%250Ayet%2520general%2520solution%2520that%2520modifies%2520the%2520source%2520distribution%2520in%2520the%2520conditional%250Aflow%2520matching%2520to%2520be%2520condition-dependent.%2520By%2520anchoring%2520the%2520source%2520distribution%250Aaround%2520semantics%2520extracted%2520from%2520condition%2520inputs%252C%2520Cocos%2520encourages%2520stronger%250Acondition%2520integration%2520and%2520prevents%2520the%2520loss%2520collapse.%2520We%2520provide%2520theoretical%250Ajustification%2520and%2520extensive%2520empirical%2520results%2520across%2520simulation%2520and%2520real-world%250Abenchmarks.%2520Our%2520method%2520achieves%2520faster%2520convergence%2520and%2520higher%2520success%2520rates%250Athan%2520existing%2520approaches%252C%2520matching%2520the%2520performance%2520of%2520large-scale%2520pre-trained%250AVLAs%2520using%2520significantly%2520fewer%2520gradient%2520steps%2520and%2520parameters.%2520Cocos%2520is%250Alightweight%252C%2520easy%2520to%2520implement%252C%2520and%2520compatible%2520with%2520diverse%2520policy%250Aarchitectures%252C%2520offering%2520a%2520general-purpose%2520improvement%2520to%2520diffusion%2520policy%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditioning%20Matters%3A%20Training%20Diffusion%20Policies%20is%20Faster%20Than%20You%0A%20%20Think&entry.906535625=Zibin%20Dong%20and%20Yicheng%20Liu%20and%20Yinchuan%20Li%20and%20Hang%20Zhao%20and%20Jianye%20Hao&entry.1292438233=%20%20Diffusion%20policies%20have%20emerged%20as%20a%20mainstream%20paradigm%20for%20building%0Avision-language-action%20%28VLA%29%20models.%20Although%20they%20demonstrate%20strong%20robot%0Acontrol%20capabilities%2C%20their%20training%20efficiency%20remains%20suboptimal.%20In%20this%0Awork%2C%20we%20identify%20a%20fundamental%20challenge%20in%20conditional%20diffusion%20policy%0Atraining%3A%20when%20generative%20conditions%20are%20hard%20to%20distinguish%2C%20the%20training%0Aobjective%20degenerates%20into%20modeling%20the%20marginal%20action%20distribution%2C%20a%0Aphenomenon%20we%20term%20loss%20collapse.%20To%20overcome%20this%2C%20we%20propose%20Cocos%2C%20a%20simple%0Ayet%20general%20solution%20that%20modifies%20the%20source%20distribution%20in%20the%20conditional%0Aflow%20matching%20to%20be%20condition-dependent.%20By%20anchoring%20the%20source%20distribution%0Aaround%20semantics%20extracted%20from%20condition%20inputs%2C%20Cocos%20encourages%20stronger%0Acondition%20integration%20and%20prevents%20the%20loss%20collapse.%20We%20provide%20theoretical%0Ajustification%20and%20extensive%20empirical%20results%20across%20simulation%20and%20real-world%0Abenchmarks.%20Our%20method%20achieves%20faster%20convergence%20and%20higher%20success%20rates%0Athan%20existing%20approaches%2C%20matching%20the%20performance%20of%20large-scale%20pre-trained%0AVLAs%20using%20significantly%20fewer%20gradient%20steps%20and%20parameters.%20Cocos%20is%0Alightweight%2C%20easy%20to%20implement%2C%20and%20compatible%20with%20diverse%20policy%0Aarchitectures%2C%20offering%20a%20general-purpose%20improvement%20to%20diffusion%20policy%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11123v1&entry.124074799=Read"},
{"title": "Context parroting: A simple but tough-to-beat baseline for foundation\n  models in scientific machine learning", "author": "Yuanzhao Zhang and William Gilpin", "abstract": "  Recently-developed time series foundation models for scientific machine\nlearning exhibit emergent abilities to predict physical systems. These\nabilities include zero-shot forecasting, in which a model forecasts future\nstates of a system given only a short trajectory as context. Here, we show that\nfoundation models applied to physical systems can give accurate predictions,\nbut that they fail to develop meaningful representations of the underlying\nphysics. Instead, foundation models often forecast by context parroting, a\nsimple zero-shot forecasting strategy that copies directly from the context. As\na result, a naive direct context parroting model scores higher than\nstate-of-the-art time-series foundation models on predicting a diverse range of\ndynamical systems, at a tiny fraction of the computational cost. We draw a\nparallel between context parroting and induction heads, which explains why\nlarge language models trained on text can be repurposed for time series\nforecasting. Our dynamical systems perspective also ties the scaling between\nforecast accuracy and context length to the fractal dimension of the attractor,\nproviding insight into the previously observed in-context neural scaling laws.\nContext parroting thus serves as a simple but tough-to-beat baseline for future\ntime-series foundation models and can help identify in-context learning\nstrategies beyond parroting.\n", "link": "http://arxiv.org/abs/2505.11349v1", "date": "2025-05-16", "relevancy": 2.2447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20parroting%3A%20A%20simple%20but%20tough-to-beat%20baseline%20for%20foundation%0A%20%20models%20in%20scientific%20machine%20learning&body=Title%3A%20Context%20parroting%3A%20A%20simple%20but%20tough-to-beat%20baseline%20for%20foundation%0A%20%20models%20in%20scientific%20machine%20learning%0AAuthor%3A%20Yuanzhao%20Zhang%20and%20William%20Gilpin%0AAbstract%3A%20%20%20Recently-developed%20time%20series%20foundation%20models%20for%20scientific%20machine%0Alearning%20exhibit%20emergent%20abilities%20to%20predict%20physical%20systems.%20These%0Aabilities%20include%20zero-shot%20forecasting%2C%20in%20which%20a%20model%20forecasts%20future%0Astates%20of%20a%20system%20given%20only%20a%20short%20trajectory%20as%20context.%20Here%2C%20we%20show%20that%0Afoundation%20models%20applied%20to%20physical%20systems%20can%20give%20accurate%20predictions%2C%0Abut%20that%20they%20fail%20to%20develop%20meaningful%20representations%20of%20the%20underlying%0Aphysics.%20Instead%2C%20foundation%20models%20often%20forecast%20by%20context%20parroting%2C%20a%0Asimple%20zero-shot%20forecasting%20strategy%20that%20copies%20directly%20from%20the%20context.%20As%0Aa%20result%2C%20a%20naive%20direct%20context%20parroting%20model%20scores%20higher%20than%0Astate-of-the-art%20time-series%20foundation%20models%20on%20predicting%20a%20diverse%20range%20of%0Adynamical%20systems%2C%20at%20a%20tiny%20fraction%20of%20the%20computational%20cost.%20We%20draw%20a%0Aparallel%20between%20context%20parroting%20and%20induction%20heads%2C%20which%20explains%20why%0Alarge%20language%20models%20trained%20on%20text%20can%20be%20repurposed%20for%20time%20series%0Aforecasting.%20Our%20dynamical%20systems%20perspective%20also%20ties%20the%20scaling%20between%0Aforecast%20accuracy%20and%20context%20length%20to%20the%20fractal%20dimension%20of%20the%20attractor%2C%0Aproviding%20insight%20into%20the%20previously%20observed%20in-context%20neural%20scaling%20laws.%0AContext%20parroting%20thus%20serves%20as%20a%20simple%20but%20tough-to-beat%20baseline%20for%20future%0Atime-series%20foundation%20models%20and%20can%20help%20identify%20in-context%20learning%0Astrategies%20beyond%20parroting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520parroting%253A%2520A%2520simple%2520but%2520tough-to-beat%2520baseline%2520for%2520foundation%250A%2520%2520models%2520in%2520scientific%2520machine%2520learning%26entry.906535625%3DYuanzhao%2520Zhang%2520and%2520William%2520Gilpin%26entry.1292438233%3D%2520%2520Recently-developed%2520time%2520series%2520foundation%2520models%2520for%2520scientific%2520machine%250Alearning%2520exhibit%2520emergent%2520abilities%2520to%2520predict%2520physical%2520systems.%2520These%250Aabilities%2520include%2520zero-shot%2520forecasting%252C%2520in%2520which%2520a%2520model%2520forecasts%2520future%250Astates%2520of%2520a%2520system%2520given%2520only%2520a%2520short%2520trajectory%2520as%2520context.%2520Here%252C%2520we%2520show%2520that%250Afoundation%2520models%2520applied%2520to%2520physical%2520systems%2520can%2520give%2520accurate%2520predictions%252C%250Abut%2520that%2520they%2520fail%2520to%2520develop%2520meaningful%2520representations%2520of%2520the%2520underlying%250Aphysics.%2520Instead%252C%2520foundation%2520models%2520often%2520forecast%2520by%2520context%2520parroting%252C%2520a%250Asimple%2520zero-shot%2520forecasting%2520strategy%2520that%2520copies%2520directly%2520from%2520the%2520context.%2520As%250Aa%2520result%252C%2520a%2520naive%2520direct%2520context%2520parroting%2520model%2520scores%2520higher%2520than%250Astate-of-the-art%2520time-series%2520foundation%2520models%2520on%2520predicting%2520a%2520diverse%2520range%2520of%250Adynamical%2520systems%252C%2520at%2520a%2520tiny%2520fraction%2520of%2520the%2520computational%2520cost.%2520We%2520draw%2520a%250Aparallel%2520between%2520context%2520parroting%2520and%2520induction%2520heads%252C%2520which%2520explains%2520why%250Alarge%2520language%2520models%2520trained%2520on%2520text%2520can%2520be%2520repurposed%2520for%2520time%2520series%250Aforecasting.%2520Our%2520dynamical%2520systems%2520perspective%2520also%2520ties%2520the%2520scaling%2520between%250Aforecast%2520accuracy%2520and%2520context%2520length%2520to%2520the%2520fractal%2520dimension%2520of%2520the%2520attractor%252C%250Aproviding%2520insight%2520into%2520the%2520previously%2520observed%2520in-context%2520neural%2520scaling%2520laws.%250AContext%2520parroting%2520thus%2520serves%2520as%2520a%2520simple%2520but%2520tough-to-beat%2520baseline%2520for%2520future%250Atime-series%2520foundation%2520models%2520and%2520can%2520help%2520identify%2520in-context%2520learning%250Astrategies%2520beyond%2520parroting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20parroting%3A%20A%20simple%20but%20tough-to-beat%20baseline%20for%20foundation%0A%20%20models%20in%20scientific%20machine%20learning&entry.906535625=Yuanzhao%20Zhang%20and%20William%20Gilpin&entry.1292438233=%20%20Recently-developed%20time%20series%20foundation%20models%20for%20scientific%20machine%0Alearning%20exhibit%20emergent%20abilities%20to%20predict%20physical%20systems.%20These%0Aabilities%20include%20zero-shot%20forecasting%2C%20in%20which%20a%20model%20forecasts%20future%0Astates%20of%20a%20system%20given%20only%20a%20short%20trajectory%20as%20context.%20Here%2C%20we%20show%20that%0Afoundation%20models%20applied%20to%20physical%20systems%20can%20give%20accurate%20predictions%2C%0Abut%20that%20they%20fail%20to%20develop%20meaningful%20representations%20of%20the%20underlying%0Aphysics.%20Instead%2C%20foundation%20models%20often%20forecast%20by%20context%20parroting%2C%20a%0Asimple%20zero-shot%20forecasting%20strategy%20that%20copies%20directly%20from%20the%20context.%20As%0Aa%20result%2C%20a%20naive%20direct%20context%20parroting%20model%20scores%20higher%20than%0Astate-of-the-art%20time-series%20foundation%20models%20on%20predicting%20a%20diverse%20range%20of%0Adynamical%20systems%2C%20at%20a%20tiny%20fraction%20of%20the%20computational%20cost.%20We%20draw%20a%0Aparallel%20between%20context%20parroting%20and%20induction%20heads%2C%20which%20explains%20why%0Alarge%20language%20models%20trained%20on%20text%20can%20be%20repurposed%20for%20time%20series%0Aforecasting.%20Our%20dynamical%20systems%20perspective%20also%20ties%20the%20scaling%20between%0Aforecast%20accuracy%20and%20context%20length%20to%20the%20fractal%20dimension%20of%20the%20attractor%2C%0Aproviding%20insight%20into%20the%20previously%20observed%20in-context%20neural%20scaling%20laws.%0AContext%20parroting%20thus%20serves%20as%20a%20simple%20but%20tough-to-beat%20baseline%20for%20future%0Atime-series%20foundation%20models%20and%20can%20help%20identify%20in-context%20learning%0Astrategies%20beyond%20parroting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11349v1&entry.124074799=Read"},
{"title": "Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration\n  for Image-Guided Liver Surgery with Patches-to-Partial Matching", "author": "Zixin Yang and Jon S. Heiselman and Cheng Han and Kelly Merrell and Richard Simon and Cristian. A. Linte", "abstract": "  In image-guided liver surgery, the initial rigid alignment between\npreoperative and intraoperative data, often represented as point clouds, is\ncrucial for providing sub-surface information from preoperative CT/MRI images\nto the surgeon during the procedure. Currently, this alignment is typically\nperformed using semi-automatic methods, which, while effective to some extent,\nare prone to errors that demand manual correction. Point cloud\ncorrespondence-based registration methods are promising to serve as a fully\nautomatic solution. However, they may struggle in scenarios with limited\nintraoperative surface visibility, a common challenge in liver surgery,\nparticularly in laparoscopic procedures, which we refer to as\ncomplete-to-partial ambiguity. We first illustrate this ambiguity by evaluating\nthe performance of state-of-the-art learning-based point cloud registration\nmethods on our carefully constructed in silico and in vitro datasets. Then, we\npropose a patches-to-partial matching strategy as a plug-and-play module to\nresolve the ambiguity, which can be seamlessly integrated into learning-based\nregistration methods without disrupting their end-to-end structure. It has\nproven effective and efficient in improving registration performance for cases\nwith limited intraoperative visibility. The constructed benchmark and the\nproposed module establish a solid foundation for advancing applications of\npoint cloud correspondence-based registration methods in image-guided liver\nsurgery.\n", "link": "http://arxiv.org/abs/2412.19328v2", "date": "2025-05-16", "relevancy": 2.2437, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5747}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resolving%20the%20Ambiguity%20of%20Complete-to-Partial%20Point%20Cloud%20Registration%0A%20%20for%20Image-Guided%20Liver%20Surgery%20with%20Patches-to-Partial%20Matching&body=Title%3A%20Resolving%20the%20Ambiguity%20of%20Complete-to-Partial%20Point%20Cloud%20Registration%0A%20%20for%20Image-Guided%20Liver%20Surgery%20with%20Patches-to-Partial%20Matching%0AAuthor%3A%20Zixin%20Yang%20and%20Jon%20S.%20Heiselman%20and%20Cheng%20Han%20and%20Kelly%20Merrell%20and%20Richard%20Simon%20and%20Cristian.%20A.%20Linte%0AAbstract%3A%20%20%20In%20image-guided%20liver%20surgery%2C%20the%20initial%20rigid%20alignment%20between%0Apreoperative%20and%20intraoperative%20data%2C%20often%20represented%20as%20point%20clouds%2C%20is%0Acrucial%20for%20providing%20sub-surface%20information%20from%20preoperative%20CT/MRI%20images%0Ato%20the%20surgeon%20during%20the%20procedure.%20Currently%2C%20this%20alignment%20is%20typically%0Aperformed%20using%20semi-automatic%20methods%2C%20which%2C%20while%20effective%20to%20some%20extent%2C%0Aare%20prone%20to%20errors%20that%20demand%20manual%20correction.%20Point%20cloud%0Acorrespondence-based%20registration%20methods%20are%20promising%20to%20serve%20as%20a%20fully%0Aautomatic%20solution.%20However%2C%20they%20may%20struggle%20in%20scenarios%20with%20limited%0Aintraoperative%20surface%20visibility%2C%20a%20common%20challenge%20in%20liver%20surgery%2C%0Aparticularly%20in%20laparoscopic%20procedures%2C%20which%20we%20refer%20to%20as%0Acomplete-to-partial%20ambiguity.%20We%20first%20illustrate%20this%20ambiguity%20by%20evaluating%0Athe%20performance%20of%20state-of-the-art%20learning-based%20point%20cloud%20registration%0Amethods%20on%20our%20carefully%20constructed%20in%20silico%20and%20in%20vitro%20datasets.%20Then%2C%20we%0Apropose%20a%20patches-to-partial%20matching%20strategy%20as%20a%20plug-and-play%20module%20to%0Aresolve%20the%20ambiguity%2C%20which%20can%20be%20seamlessly%20integrated%20into%20learning-based%0Aregistration%20methods%20without%20disrupting%20their%20end-to-end%20structure.%20It%20has%0Aproven%20effective%20and%20efficient%20in%20improving%20registration%20performance%20for%20cases%0Awith%20limited%20intraoperative%20visibility.%20The%20constructed%20benchmark%20and%20the%0Aproposed%20module%20establish%20a%20solid%20foundation%20for%20advancing%20applications%20of%0Apoint%20cloud%20correspondence-based%20registration%20methods%20in%20image-guided%20liver%0Asurgery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19328v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResolving%2520the%2520Ambiguity%2520of%2520Complete-to-Partial%2520Point%2520Cloud%2520Registration%250A%2520%2520for%2520Image-Guided%2520Liver%2520Surgery%2520with%2520Patches-to-Partial%2520Matching%26entry.906535625%3DZixin%2520Yang%2520and%2520Jon%2520S.%2520Heiselman%2520and%2520Cheng%2520Han%2520and%2520Kelly%2520Merrell%2520and%2520Richard%2520Simon%2520and%2520Cristian.%2520A.%2520Linte%26entry.1292438233%3D%2520%2520In%2520image-guided%2520liver%2520surgery%252C%2520the%2520initial%2520rigid%2520alignment%2520between%250Apreoperative%2520and%2520intraoperative%2520data%252C%2520often%2520represented%2520as%2520point%2520clouds%252C%2520is%250Acrucial%2520for%2520providing%2520sub-surface%2520information%2520from%2520preoperative%2520CT/MRI%2520images%250Ato%2520the%2520surgeon%2520during%2520the%2520procedure.%2520Currently%252C%2520this%2520alignment%2520is%2520typically%250Aperformed%2520using%2520semi-automatic%2520methods%252C%2520which%252C%2520while%2520effective%2520to%2520some%2520extent%252C%250Aare%2520prone%2520to%2520errors%2520that%2520demand%2520manual%2520correction.%2520Point%2520cloud%250Acorrespondence-based%2520registration%2520methods%2520are%2520promising%2520to%2520serve%2520as%2520a%2520fully%250Aautomatic%2520solution.%2520However%252C%2520they%2520may%2520struggle%2520in%2520scenarios%2520with%2520limited%250Aintraoperative%2520surface%2520visibility%252C%2520a%2520common%2520challenge%2520in%2520liver%2520surgery%252C%250Aparticularly%2520in%2520laparoscopic%2520procedures%252C%2520which%2520we%2520refer%2520to%2520as%250Acomplete-to-partial%2520ambiguity.%2520We%2520first%2520illustrate%2520this%2520ambiguity%2520by%2520evaluating%250Athe%2520performance%2520of%2520state-of-the-art%2520learning-based%2520point%2520cloud%2520registration%250Amethods%2520on%2520our%2520carefully%2520constructed%2520in%2520silico%2520and%2520in%2520vitro%2520datasets.%2520Then%252C%2520we%250Apropose%2520a%2520patches-to-partial%2520matching%2520strategy%2520as%2520a%2520plug-and-play%2520module%2520to%250Aresolve%2520the%2520ambiguity%252C%2520which%2520can%2520be%2520seamlessly%2520integrated%2520into%2520learning-based%250Aregistration%2520methods%2520without%2520disrupting%2520their%2520end-to-end%2520structure.%2520It%2520has%250Aproven%2520effective%2520and%2520efficient%2520in%2520improving%2520registration%2520performance%2520for%2520cases%250Awith%2520limited%2520intraoperative%2520visibility.%2520The%2520constructed%2520benchmark%2520and%2520the%250Aproposed%2520module%2520establish%2520a%2520solid%2520foundation%2520for%2520advancing%2520applications%2520of%250Apoint%2520cloud%2520correspondence-based%2520registration%2520methods%2520in%2520image-guided%2520liver%250Asurgery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19328v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resolving%20the%20Ambiguity%20of%20Complete-to-Partial%20Point%20Cloud%20Registration%0A%20%20for%20Image-Guided%20Liver%20Surgery%20with%20Patches-to-Partial%20Matching&entry.906535625=Zixin%20Yang%20and%20Jon%20S.%20Heiselman%20and%20Cheng%20Han%20and%20Kelly%20Merrell%20and%20Richard%20Simon%20and%20Cristian.%20A.%20Linte&entry.1292438233=%20%20In%20image-guided%20liver%20surgery%2C%20the%20initial%20rigid%20alignment%20between%0Apreoperative%20and%20intraoperative%20data%2C%20often%20represented%20as%20point%20clouds%2C%20is%0Acrucial%20for%20providing%20sub-surface%20information%20from%20preoperative%20CT/MRI%20images%0Ato%20the%20surgeon%20during%20the%20procedure.%20Currently%2C%20this%20alignment%20is%20typically%0Aperformed%20using%20semi-automatic%20methods%2C%20which%2C%20while%20effective%20to%20some%20extent%2C%0Aare%20prone%20to%20errors%20that%20demand%20manual%20correction.%20Point%20cloud%0Acorrespondence-based%20registration%20methods%20are%20promising%20to%20serve%20as%20a%20fully%0Aautomatic%20solution.%20However%2C%20they%20may%20struggle%20in%20scenarios%20with%20limited%0Aintraoperative%20surface%20visibility%2C%20a%20common%20challenge%20in%20liver%20surgery%2C%0Aparticularly%20in%20laparoscopic%20procedures%2C%20which%20we%20refer%20to%20as%0Acomplete-to-partial%20ambiguity.%20We%20first%20illustrate%20this%20ambiguity%20by%20evaluating%0Athe%20performance%20of%20state-of-the-art%20learning-based%20point%20cloud%20registration%0Amethods%20on%20our%20carefully%20constructed%20in%20silico%20and%20in%20vitro%20datasets.%20Then%2C%20we%0Apropose%20a%20patches-to-partial%20matching%20strategy%20as%20a%20plug-and-play%20module%20to%0Aresolve%20the%20ambiguity%2C%20which%20can%20be%20seamlessly%20integrated%20into%20learning-based%0Aregistration%20methods%20without%20disrupting%20their%20end-to-end%20structure.%20It%20has%0Aproven%20effective%20and%20efficient%20in%20improving%20registration%20performance%20for%20cases%0Awith%20limited%20intraoperative%20visibility.%20The%20constructed%20benchmark%20and%20the%0Aproposed%20module%20establish%20a%20solid%20foundation%20for%20advancing%20applications%20of%0Apoint%20cloud%20correspondence-based%20registration%20methods%20in%20image-guided%20liver%0Asurgery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19328v2&entry.124074799=Read"},
{"title": "RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive\n  and Reflective Objects", "author": "Yue Yin and Enze Tao and Weijian Deng and Dylan Campbell", "abstract": "  Modern 3D reconstruction and novel view synthesis approaches have\ndemonstrated strong performance on scenes with opaque Lambertian objects.\nHowever, most assume straight light paths and therefore cannot properly handle\nrefractive and reflective materials. Moreover, datasets specialized for these\neffects are limited, stymieing efforts to evaluate performance and develop\nsuitable techniques. In this work, we introduce a synthetic RefRef dataset and\nbenchmark for reconstructing scenes with refractive and reflective objects from\nposed images. Our dataset has 50 such objects of varying complexity, from\nsingle-material convex shapes to multi-material non-convex shapes, each placed\nin three different background types, resulting in 150 scenes. We also propose\nan oracle method that, given the object geometry and refractive indices,\ncalculates accurate light paths for neural rendering, and an approach based on\nthis that avoids these assumptions. We benchmark these against several\nstate-of-the-art methods and show that all methods lag significantly behind the\noracle, highlighting the challenges of the task and dataset.\n", "link": "http://arxiv.org/abs/2505.05848v2", "date": "2025-05-16", "relevancy": 2.2375, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.56}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.56}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefRef%3A%20A%20Synthetic%20Dataset%20and%20Benchmark%20for%20Reconstructing%20Refractive%0A%20%20and%20Reflective%20Objects&body=Title%3A%20RefRef%3A%20A%20Synthetic%20Dataset%20and%20Benchmark%20for%20Reconstructing%20Refractive%0A%20%20and%20Reflective%20Objects%0AAuthor%3A%20Yue%20Yin%20and%20Enze%20Tao%20and%20Weijian%20Deng%20and%20Dylan%20Campbell%0AAbstract%3A%20%20%20Modern%203D%20reconstruction%20and%20novel%20view%20synthesis%20approaches%20have%0Ademonstrated%20strong%20performance%20on%20scenes%20with%20opaque%20Lambertian%20objects.%0AHowever%2C%20most%20assume%20straight%20light%20paths%20and%20therefore%20cannot%20properly%20handle%0Arefractive%20and%20reflective%20materials.%20Moreover%2C%20datasets%20specialized%20for%20these%0Aeffects%20are%20limited%2C%20stymieing%20efforts%20to%20evaluate%20performance%20and%20develop%0Asuitable%20techniques.%20In%20this%20work%2C%20we%20introduce%20a%20synthetic%20RefRef%20dataset%20and%0Abenchmark%20for%20reconstructing%20scenes%20with%20refractive%20and%20reflective%20objects%20from%0Aposed%20images.%20Our%20dataset%20has%2050%20such%20objects%20of%20varying%20complexity%2C%20from%0Asingle-material%20convex%20shapes%20to%20multi-material%20non-convex%20shapes%2C%20each%20placed%0Ain%20three%20different%20background%20types%2C%20resulting%20in%20150%20scenes.%20We%20also%20propose%0Aan%20oracle%20method%20that%2C%20given%20the%20object%20geometry%20and%20refractive%20indices%2C%0Acalculates%20accurate%20light%20paths%20for%20neural%20rendering%2C%20and%20an%20approach%20based%20on%0Athis%20that%20avoids%20these%20assumptions.%20We%20benchmark%20these%20against%20several%0Astate-of-the-art%20methods%20and%20show%20that%20all%20methods%20lag%20significantly%20behind%20the%0Aoracle%2C%20highlighting%20the%20challenges%20of%20the%20task%20and%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05848v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefRef%253A%2520A%2520Synthetic%2520Dataset%2520and%2520Benchmark%2520for%2520Reconstructing%2520Refractive%250A%2520%2520and%2520Reflective%2520Objects%26entry.906535625%3DYue%2520Yin%2520and%2520Enze%2520Tao%2520and%2520Weijian%2520Deng%2520and%2520Dylan%2520Campbell%26entry.1292438233%3D%2520%2520Modern%25203D%2520reconstruction%2520and%2520novel%2520view%2520synthesis%2520approaches%2520have%250Ademonstrated%2520strong%2520performance%2520on%2520scenes%2520with%2520opaque%2520Lambertian%2520objects.%250AHowever%252C%2520most%2520assume%2520straight%2520light%2520paths%2520and%2520therefore%2520cannot%2520properly%2520handle%250Arefractive%2520and%2520reflective%2520materials.%2520Moreover%252C%2520datasets%2520specialized%2520for%2520these%250Aeffects%2520are%2520limited%252C%2520stymieing%2520efforts%2520to%2520evaluate%2520performance%2520and%2520develop%250Asuitable%2520techniques.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520synthetic%2520RefRef%2520dataset%2520and%250Abenchmark%2520for%2520reconstructing%2520scenes%2520with%2520refractive%2520and%2520reflective%2520objects%2520from%250Aposed%2520images.%2520Our%2520dataset%2520has%252050%2520such%2520objects%2520of%2520varying%2520complexity%252C%2520from%250Asingle-material%2520convex%2520shapes%2520to%2520multi-material%2520non-convex%2520shapes%252C%2520each%2520placed%250Ain%2520three%2520different%2520background%2520types%252C%2520resulting%2520in%2520150%2520scenes.%2520We%2520also%2520propose%250Aan%2520oracle%2520method%2520that%252C%2520given%2520the%2520object%2520geometry%2520and%2520refractive%2520indices%252C%250Acalculates%2520accurate%2520light%2520paths%2520for%2520neural%2520rendering%252C%2520and%2520an%2520approach%2520based%2520on%250Athis%2520that%2520avoids%2520these%2520assumptions.%2520We%2520benchmark%2520these%2520against%2520several%250Astate-of-the-art%2520methods%2520and%2520show%2520that%2520all%2520methods%2520lag%2520significantly%2520behind%2520the%250Aoracle%252C%2520highlighting%2520the%2520challenges%2520of%2520the%2520task%2520and%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05848v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefRef%3A%20A%20Synthetic%20Dataset%20and%20Benchmark%20for%20Reconstructing%20Refractive%0A%20%20and%20Reflective%20Objects&entry.906535625=Yue%20Yin%20and%20Enze%20Tao%20and%20Weijian%20Deng%20and%20Dylan%20Campbell&entry.1292438233=%20%20Modern%203D%20reconstruction%20and%20novel%20view%20synthesis%20approaches%20have%0Ademonstrated%20strong%20performance%20on%20scenes%20with%20opaque%20Lambertian%20objects.%0AHowever%2C%20most%20assume%20straight%20light%20paths%20and%20therefore%20cannot%20properly%20handle%0Arefractive%20and%20reflective%20materials.%20Moreover%2C%20datasets%20specialized%20for%20these%0Aeffects%20are%20limited%2C%20stymieing%20efforts%20to%20evaluate%20performance%20and%20develop%0Asuitable%20techniques.%20In%20this%20work%2C%20we%20introduce%20a%20synthetic%20RefRef%20dataset%20and%0Abenchmark%20for%20reconstructing%20scenes%20with%20refractive%20and%20reflective%20objects%20from%0Aposed%20images.%20Our%20dataset%20has%2050%20such%20objects%20of%20varying%20complexity%2C%20from%0Asingle-material%20convex%20shapes%20to%20multi-material%20non-convex%20shapes%2C%20each%20placed%0Ain%20three%20different%20background%20types%2C%20resulting%20in%20150%20scenes.%20We%20also%20propose%0Aan%20oracle%20method%20that%2C%20given%20the%20object%20geometry%20and%20refractive%20indices%2C%0Acalculates%20accurate%20light%20paths%20for%20neural%20rendering%2C%20and%20an%20approach%20based%20on%0Athis%20that%20avoids%20these%20assumptions.%20We%20benchmark%20these%20against%20several%0Astate-of-the-art%20methods%20and%20show%20that%20all%20methods%20lag%20significantly%20behind%20the%0Aoracle%2C%20highlighting%20the%20challenges%20of%20the%20task%20and%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05848v2&entry.124074799=Read"},
{"title": "FreeA: Human-object Interaction Detection using Free Annotation Labels", "author": "Qi Liu and Yuxiao Wang and Xinyu Jiang and Wolin Liang and Zhenao Wei and Yu Lei and Nan Zhuang and Weiying Xue", "abstract": "  Recent human-object interaction (HOI) detection methods depend on extensively\nannotated image datasets, which require a significant amount of manpower. In\nthis paper, we propose a novel self-adaptive, language-driven HOI detection\nmethod, termed FreeA. This method leverages the adaptability of the text-image\nmodel to generate latent HOI labels without requiring manual annotation.\nSpecifically, FreeA aligns image features of human-object pairs with HOI text\ntemplates and employs a knowledge-based masking technique to decrease\nimprobable interactions. Furthermore, FreeA implements a proposed method for\nmatching interaction correlations to increase the probability of actions\nassociated with a particular action, thereby improving the generated HOI\nlabels. Experiments on two benchmark datasets showcase that FreeA achieves\nstate-of-the-art performance among weakly supervised HOI competitors. Our\nproposal gets +\\textbf{13.29} (\\textbf{159\\%$\\uparrow$}) mAP and\n+\\textbf{17.30} (\\textbf{98\\%$\\uparrow$}) mAP than the newest ``Weakly''\nsupervised model, and +\\textbf{7.19} (\\textbf{28\\%$\\uparrow$}) mAP and\n+\\textbf{14.69} (\\textbf{34\\%$\\uparrow$}) mAP than the latest ``Weakly+''\nsupervised model, respectively, on HICO-DET and V-COCO datasets, more accurate\nin localizing and classifying the interactive actions. The source code will be\nmade public.\n", "link": "http://arxiv.org/abs/2403.01840v2", "date": "2025-05-16", "relevancy": 2.2345, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.573}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5565}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeA%3A%20Human-object%20Interaction%20Detection%20using%20Free%20Annotation%20Labels&body=Title%3A%20FreeA%3A%20Human-object%20Interaction%20Detection%20using%20Free%20Annotation%20Labels%0AAuthor%3A%20Qi%20Liu%20and%20Yuxiao%20Wang%20and%20Xinyu%20Jiang%20and%20Wolin%20Liang%20and%20Zhenao%20Wei%20and%20Yu%20Lei%20and%20Nan%20Zhuang%20and%20Weiying%20Xue%0AAbstract%3A%20%20%20Recent%20human-object%20interaction%20%28HOI%29%20detection%20methods%20depend%20on%20extensively%0Aannotated%20image%20datasets%2C%20which%20require%20a%20significant%20amount%20of%20manpower.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20self-adaptive%2C%20language-driven%20HOI%20detection%0Amethod%2C%20termed%20FreeA.%20This%20method%20leverages%20the%20adaptability%20of%20the%20text-image%0Amodel%20to%20generate%20latent%20HOI%20labels%20without%20requiring%20manual%20annotation.%0ASpecifically%2C%20FreeA%20aligns%20image%20features%20of%20human-object%20pairs%20with%20HOI%20text%0Atemplates%20and%20employs%20a%20knowledge-based%20masking%20technique%20to%20decrease%0Aimprobable%20interactions.%20Furthermore%2C%20FreeA%20implements%20a%20proposed%20method%20for%0Amatching%20interaction%20correlations%20to%20increase%20the%20probability%20of%20actions%0Aassociated%20with%20a%20particular%20action%2C%20thereby%20improving%20the%20generated%20HOI%0Alabels.%20Experiments%20on%20two%20benchmark%20datasets%20showcase%20that%20FreeA%20achieves%0Astate-of-the-art%20performance%20among%20weakly%20supervised%20HOI%20competitors.%20Our%0Aproposal%20gets%20%2B%5Ctextbf%7B13.29%7D%20%28%5Ctextbf%7B159%5C%25%24%5Cuparrow%24%7D%29%20mAP%20and%0A%2B%5Ctextbf%7B17.30%7D%20%28%5Ctextbf%7B98%5C%25%24%5Cuparrow%24%7D%29%20mAP%20than%20the%20newest%20%60%60Weakly%27%27%0Asupervised%20model%2C%20and%20%2B%5Ctextbf%7B7.19%7D%20%28%5Ctextbf%7B28%5C%25%24%5Cuparrow%24%7D%29%20mAP%20and%0A%2B%5Ctextbf%7B14.69%7D%20%28%5Ctextbf%7B34%5C%25%24%5Cuparrow%24%7D%29%20mAP%20than%20the%20latest%20%60%60Weakly%2B%27%27%0Asupervised%20model%2C%20respectively%2C%20on%20HICO-DET%20and%20V-COCO%20datasets%2C%20more%20accurate%0Ain%20localizing%20and%20classifying%20the%20interactive%20actions.%20The%20source%20code%20will%20be%0Amade%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01840v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeA%253A%2520Human-object%2520Interaction%2520Detection%2520using%2520Free%2520Annotation%2520Labels%26entry.906535625%3DQi%2520Liu%2520and%2520Yuxiao%2520Wang%2520and%2520Xinyu%2520Jiang%2520and%2520Wolin%2520Liang%2520and%2520Zhenao%2520Wei%2520and%2520Yu%2520Lei%2520and%2520Nan%2520Zhuang%2520and%2520Weiying%2520Xue%26entry.1292438233%3D%2520%2520Recent%2520human-object%2520interaction%2520%2528HOI%2529%2520detection%2520methods%2520depend%2520on%2520extensively%250Aannotated%2520image%2520datasets%252C%2520which%2520require%2520a%2520significant%2520amount%2520of%2520manpower.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520self-adaptive%252C%2520language-driven%2520HOI%2520detection%250Amethod%252C%2520termed%2520FreeA.%2520This%2520method%2520leverages%2520the%2520adaptability%2520of%2520the%2520text-image%250Amodel%2520to%2520generate%2520latent%2520HOI%2520labels%2520without%2520requiring%2520manual%2520annotation.%250ASpecifically%252C%2520FreeA%2520aligns%2520image%2520features%2520of%2520human-object%2520pairs%2520with%2520HOI%2520text%250Atemplates%2520and%2520employs%2520a%2520knowledge-based%2520masking%2520technique%2520to%2520decrease%250Aimprobable%2520interactions.%2520Furthermore%252C%2520FreeA%2520implements%2520a%2520proposed%2520method%2520for%250Amatching%2520interaction%2520correlations%2520to%2520increase%2520the%2520probability%2520of%2520actions%250Aassociated%2520with%2520a%2520particular%2520action%252C%2520thereby%2520improving%2520the%2520generated%2520HOI%250Alabels.%2520Experiments%2520on%2520two%2520benchmark%2520datasets%2520showcase%2520that%2520FreeA%2520achieves%250Astate-of-the-art%2520performance%2520among%2520weakly%2520supervised%2520HOI%2520competitors.%2520Our%250Aproposal%2520gets%2520%252B%255Ctextbf%257B13.29%257D%2520%2528%255Ctextbf%257B159%255C%2525%2524%255Cuparrow%2524%257D%2529%2520mAP%2520and%250A%252B%255Ctextbf%257B17.30%257D%2520%2528%255Ctextbf%257B98%255C%2525%2524%255Cuparrow%2524%257D%2529%2520mAP%2520than%2520the%2520newest%2520%2560%2560Weakly%2527%2527%250Asupervised%2520model%252C%2520and%2520%252B%255Ctextbf%257B7.19%257D%2520%2528%255Ctextbf%257B28%255C%2525%2524%255Cuparrow%2524%257D%2529%2520mAP%2520and%250A%252B%255Ctextbf%257B14.69%257D%2520%2528%255Ctextbf%257B34%255C%2525%2524%255Cuparrow%2524%257D%2529%2520mAP%2520than%2520the%2520latest%2520%2560%2560Weakly%252B%2527%2527%250Asupervised%2520model%252C%2520respectively%252C%2520on%2520HICO-DET%2520and%2520V-COCO%2520datasets%252C%2520more%2520accurate%250Ain%2520localizing%2520and%2520classifying%2520the%2520interactive%2520actions.%2520The%2520source%2520code%2520will%2520be%250Amade%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01840v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeA%3A%20Human-object%20Interaction%20Detection%20using%20Free%20Annotation%20Labels&entry.906535625=Qi%20Liu%20and%20Yuxiao%20Wang%20and%20Xinyu%20Jiang%20and%20Wolin%20Liang%20and%20Zhenao%20Wei%20and%20Yu%20Lei%20and%20Nan%20Zhuang%20and%20Weiying%20Xue&entry.1292438233=%20%20Recent%20human-object%20interaction%20%28HOI%29%20detection%20methods%20depend%20on%20extensively%0Aannotated%20image%20datasets%2C%20which%20require%20a%20significant%20amount%20of%20manpower.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20self-adaptive%2C%20language-driven%20HOI%20detection%0Amethod%2C%20termed%20FreeA.%20This%20method%20leverages%20the%20adaptability%20of%20the%20text-image%0Amodel%20to%20generate%20latent%20HOI%20labels%20without%20requiring%20manual%20annotation.%0ASpecifically%2C%20FreeA%20aligns%20image%20features%20of%20human-object%20pairs%20with%20HOI%20text%0Atemplates%20and%20employs%20a%20knowledge-based%20masking%20technique%20to%20decrease%0Aimprobable%20interactions.%20Furthermore%2C%20FreeA%20implements%20a%20proposed%20method%20for%0Amatching%20interaction%20correlations%20to%20increase%20the%20probability%20of%20actions%0Aassociated%20with%20a%20particular%20action%2C%20thereby%20improving%20the%20generated%20HOI%0Alabels.%20Experiments%20on%20two%20benchmark%20datasets%20showcase%20that%20FreeA%20achieves%0Astate-of-the-art%20performance%20among%20weakly%20supervised%20HOI%20competitors.%20Our%0Aproposal%20gets%20%2B%5Ctextbf%7B13.29%7D%20%28%5Ctextbf%7B159%5C%25%24%5Cuparrow%24%7D%29%20mAP%20and%0A%2B%5Ctextbf%7B17.30%7D%20%28%5Ctextbf%7B98%5C%25%24%5Cuparrow%24%7D%29%20mAP%20than%20the%20newest%20%60%60Weakly%27%27%0Asupervised%20model%2C%20and%20%2B%5Ctextbf%7B7.19%7D%20%28%5Ctextbf%7B28%5C%25%24%5Cuparrow%24%7D%29%20mAP%20and%0A%2B%5Ctextbf%7B14.69%7D%20%28%5Ctextbf%7B34%5C%25%24%5Cuparrow%24%7D%29%20mAP%20than%20the%20latest%20%60%60Weakly%2B%27%27%0Asupervised%20model%2C%20respectively%2C%20on%20HICO-DET%20and%20V-COCO%20datasets%2C%20more%20accurate%0Ain%20localizing%20and%20classifying%20the%20interactive%20actions.%20The%20source%20code%20will%20be%0Amade%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01840v2&entry.124074799=Read"},
{"title": "GeoMM: On Geodesic Perspective for Multi-modal Learning", "author": "Shibin Mei and Hang Wang and Bingbing Ni", "abstract": "  Geodesic distance serves as a reliable means of measuring distance in\nnonlinear spaces, and such nonlinear manifolds are prevalent in the current\nmultimodal learning. In these scenarios, some samples may exhibit high\nsimilarity, yet they convey different semantics, making traditional distance\nmetrics inadequate for distinguishing between positive and negative samples.\nThis paper introduces geodesic distance as a novel distance metric in\nmulti-modal learning for the first time, to mine correlations between samples,\naiming to address the limitations of common distance metric. Our approach\nincorporates a comprehensive series of strategies to adapt geodesic distance\nfor the current multimodal learning. Specifically, we construct a graph\nstructure to represent the adjacency relationships among samples by\nthresholding distances between them and then apply the shortest-path algorithm\nto obtain geodesic distance within this graph. To facilitate efficient\ncomputation, we further propose a hierarchical graph structure through\nclustering and combined with incremental update strategies for dynamic status\nupdates. Extensive experiments across various downstream tasks validate the\neffectiveness of our proposed method, demonstrating its capability to capture\ncomplex relationships between samples and improve the performance of multimodal\nlearning models.\n", "link": "http://arxiv.org/abs/2505.11216v1", "date": "2025-05-16", "relevancy": 2.2201, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5654}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoMM%3A%20On%20Geodesic%20Perspective%20for%20Multi-modal%20Learning&body=Title%3A%20GeoMM%3A%20On%20Geodesic%20Perspective%20for%20Multi-modal%20Learning%0AAuthor%3A%20Shibin%20Mei%20and%20Hang%20Wang%20and%20Bingbing%20Ni%0AAbstract%3A%20%20%20Geodesic%20distance%20serves%20as%20a%20reliable%20means%20of%20measuring%20distance%20in%0Anonlinear%20spaces%2C%20and%20such%20nonlinear%20manifolds%20are%20prevalent%20in%20the%20current%0Amultimodal%20learning.%20In%20these%20scenarios%2C%20some%20samples%20may%20exhibit%20high%0Asimilarity%2C%20yet%20they%20convey%20different%20semantics%2C%20making%20traditional%20distance%0Ametrics%20inadequate%20for%20distinguishing%20between%20positive%20and%20negative%20samples.%0AThis%20paper%20introduces%20geodesic%20distance%20as%20a%20novel%20distance%20metric%20in%0Amulti-modal%20learning%20for%20the%20first%20time%2C%20to%20mine%20correlations%20between%20samples%2C%0Aaiming%20to%20address%20the%20limitations%20of%20common%20distance%20metric.%20Our%20approach%0Aincorporates%20a%20comprehensive%20series%20of%20strategies%20to%20adapt%20geodesic%20distance%0Afor%20the%20current%20multimodal%20learning.%20Specifically%2C%20we%20construct%20a%20graph%0Astructure%20to%20represent%20the%20adjacency%20relationships%20among%20samples%20by%0Athresholding%20distances%20between%20them%20and%20then%20apply%20the%20shortest-path%20algorithm%0Ato%20obtain%20geodesic%20distance%20within%20this%20graph.%20To%20facilitate%20efficient%0Acomputation%2C%20we%20further%20propose%20a%20hierarchical%20graph%20structure%20through%0Aclustering%20and%20combined%20with%20incremental%20update%20strategies%20for%20dynamic%20status%0Aupdates.%20Extensive%20experiments%20across%20various%20downstream%20tasks%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20demonstrating%20its%20capability%20to%20capture%0Acomplex%20relationships%20between%20samples%20and%20improve%20the%20performance%20of%20multimodal%0Alearning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoMM%253A%2520On%2520Geodesic%2520Perspective%2520for%2520Multi-modal%2520Learning%26entry.906535625%3DShibin%2520Mei%2520and%2520Hang%2520Wang%2520and%2520Bingbing%2520Ni%26entry.1292438233%3D%2520%2520Geodesic%2520distance%2520serves%2520as%2520a%2520reliable%2520means%2520of%2520measuring%2520distance%2520in%250Anonlinear%2520spaces%252C%2520and%2520such%2520nonlinear%2520manifolds%2520are%2520prevalent%2520in%2520the%2520current%250Amultimodal%2520learning.%2520In%2520these%2520scenarios%252C%2520some%2520samples%2520may%2520exhibit%2520high%250Asimilarity%252C%2520yet%2520they%2520convey%2520different%2520semantics%252C%2520making%2520traditional%2520distance%250Ametrics%2520inadequate%2520for%2520distinguishing%2520between%2520positive%2520and%2520negative%2520samples.%250AThis%2520paper%2520introduces%2520geodesic%2520distance%2520as%2520a%2520novel%2520distance%2520metric%2520in%250Amulti-modal%2520learning%2520for%2520the%2520first%2520time%252C%2520to%2520mine%2520correlations%2520between%2520samples%252C%250Aaiming%2520to%2520address%2520the%2520limitations%2520of%2520common%2520distance%2520metric.%2520Our%2520approach%250Aincorporates%2520a%2520comprehensive%2520series%2520of%2520strategies%2520to%2520adapt%2520geodesic%2520distance%250Afor%2520the%2520current%2520multimodal%2520learning.%2520Specifically%252C%2520we%2520construct%2520a%2520graph%250Astructure%2520to%2520represent%2520the%2520adjacency%2520relationships%2520among%2520samples%2520by%250Athresholding%2520distances%2520between%2520them%2520and%2520then%2520apply%2520the%2520shortest-path%2520algorithm%250Ato%2520obtain%2520geodesic%2520distance%2520within%2520this%2520graph.%2520To%2520facilitate%2520efficient%250Acomputation%252C%2520we%2520further%2520propose%2520a%2520hierarchical%2520graph%2520structure%2520through%250Aclustering%2520and%2520combined%2520with%2520incremental%2520update%2520strategies%2520for%2520dynamic%2520status%250Aupdates.%2520Extensive%2520experiments%2520across%2520various%2520downstream%2520tasks%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520method%252C%2520demonstrating%2520its%2520capability%2520to%2520capture%250Acomplex%2520relationships%2520between%2520samples%2520and%2520improve%2520the%2520performance%2520of%2520multimodal%250Alearning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoMM%3A%20On%20Geodesic%20Perspective%20for%20Multi-modal%20Learning&entry.906535625=Shibin%20Mei%20and%20Hang%20Wang%20and%20Bingbing%20Ni&entry.1292438233=%20%20Geodesic%20distance%20serves%20as%20a%20reliable%20means%20of%20measuring%20distance%20in%0Anonlinear%20spaces%2C%20and%20such%20nonlinear%20manifolds%20are%20prevalent%20in%20the%20current%0Amultimodal%20learning.%20In%20these%20scenarios%2C%20some%20samples%20may%20exhibit%20high%0Asimilarity%2C%20yet%20they%20convey%20different%20semantics%2C%20making%20traditional%20distance%0Ametrics%20inadequate%20for%20distinguishing%20between%20positive%20and%20negative%20samples.%0AThis%20paper%20introduces%20geodesic%20distance%20as%20a%20novel%20distance%20metric%20in%0Amulti-modal%20learning%20for%20the%20first%20time%2C%20to%20mine%20correlations%20between%20samples%2C%0Aaiming%20to%20address%20the%20limitations%20of%20common%20distance%20metric.%20Our%20approach%0Aincorporates%20a%20comprehensive%20series%20of%20strategies%20to%20adapt%20geodesic%20distance%0Afor%20the%20current%20multimodal%20learning.%20Specifically%2C%20we%20construct%20a%20graph%0Astructure%20to%20represent%20the%20adjacency%20relationships%20among%20samples%20by%0Athresholding%20distances%20between%20them%20and%20then%20apply%20the%20shortest-path%20algorithm%0Ato%20obtain%20geodesic%20distance%20within%20this%20graph.%20To%20facilitate%20efficient%0Acomputation%2C%20we%20further%20propose%20a%20hierarchical%20graph%20structure%20through%0Aclustering%20and%20combined%20with%20incremental%20update%20strategies%20for%20dynamic%20status%0Aupdates.%20Extensive%20experiments%20across%20various%20downstream%20tasks%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20demonstrating%20its%20capability%20to%20capture%0Acomplex%20relationships%20between%20samples%20and%20improve%20the%20performance%20of%20multimodal%0Alearning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11216v1&entry.124074799=Read"},
{"title": "Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI\n  models in Sound Localization", "author": "Yanhao Jia and Ji Xie and S Jivaganesh and Hao Li and Xu Wu and Mengmi Zhang", "abstract": "  Imagine hearing a dog bark and turning toward the sound only to see a parked\ncar, while the real, silent dog sits elsewhere. Such sensory conflicts test\nperception, yet humans reliably resolve them by prioritizing sound over\nmisleading visuals. Despite advances in multimodal AI integrating vision and\naudio, little is known about how these systems handle cross-modal conflicts or\nwhether they favor one modality. In this study, we systematically examine\nmodality bias and conflict resolution in AI sound localization. We assess\nleading multimodal models and benchmark them against human performance in\npsychophysics experiments across six audiovisual conditions, including\ncongruent, conflicting, and absent cues. Humans consistently outperform AI,\ndemonstrating superior resilience to conflicting or missing visuals by relying\non auditory information. In contrast, AI models often default to visual input,\ndegrading performance to near chance levels. To address this, we finetune a\nstate-of-the-art model using a stereo audio-image dataset generated via 3D\nsimulations. Even with limited training data, the refined model surpasses\nexisting benchmarks. Notably, it also mirrors human-like horizontal\nlocalization bias favoring left-right precision-likely due to the stereo audio\nstructure reflecting human ear placement. These findings underscore how sensory\ninput quality and system architecture shape multimodal representation accuracy.\n", "link": "http://arxiv.org/abs/2505.11217v1", "date": "2025-05-16", "relevancy": 2.219, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Sound%2C%20Hearing%20Sight%3A%20Uncovering%20Modality%20Bias%20and%20Conflict%20of%20AI%0A%20%20models%20in%20Sound%20Localization&body=Title%3A%20Seeing%20Sound%2C%20Hearing%20Sight%3A%20Uncovering%20Modality%20Bias%20and%20Conflict%20of%20AI%0A%20%20models%20in%20Sound%20Localization%0AAuthor%3A%20Yanhao%20Jia%20and%20Ji%20Xie%20and%20S%20Jivaganesh%20and%20Hao%20Li%20and%20Xu%20Wu%20and%20Mengmi%20Zhang%0AAbstract%3A%20%20%20Imagine%20hearing%20a%20dog%20bark%20and%20turning%20toward%20the%20sound%20only%20to%20see%20a%20parked%0Acar%2C%20while%20the%20real%2C%20silent%20dog%20sits%20elsewhere.%20Such%20sensory%20conflicts%20test%0Aperception%2C%20yet%20humans%20reliably%20resolve%20them%20by%20prioritizing%20sound%20over%0Amisleading%20visuals.%20Despite%20advances%20in%20multimodal%20AI%20integrating%20vision%20and%0Aaudio%2C%20little%20is%20known%20about%20how%20these%20systems%20handle%20cross-modal%20conflicts%20or%0Awhether%20they%20favor%20one%20modality.%20In%20this%20study%2C%20we%20systematically%20examine%0Amodality%20bias%20and%20conflict%20resolution%20in%20AI%20sound%20localization.%20We%20assess%0Aleading%20multimodal%20models%20and%20benchmark%20them%20against%20human%20performance%20in%0Apsychophysics%20experiments%20across%20six%20audiovisual%20conditions%2C%20including%0Acongruent%2C%20conflicting%2C%20and%20absent%20cues.%20Humans%20consistently%20outperform%20AI%2C%0Ademonstrating%20superior%20resilience%20to%20conflicting%20or%20missing%20visuals%20by%20relying%0Aon%20auditory%20information.%20In%20contrast%2C%20AI%20models%20often%20default%20to%20visual%20input%2C%0Adegrading%20performance%20to%20near%20chance%20levels.%20To%20address%20this%2C%20we%20finetune%20a%0Astate-of-the-art%20model%20using%20a%20stereo%20audio-image%20dataset%20generated%20via%203D%0Asimulations.%20Even%20with%20limited%20training%20data%2C%20the%20refined%20model%20surpasses%0Aexisting%20benchmarks.%20Notably%2C%20it%20also%20mirrors%20human-like%20horizontal%0Alocalization%20bias%20favoring%20left-right%20precision-likely%20due%20to%20the%20stereo%20audio%0Astructure%20reflecting%20human%20ear%20placement.%20These%20findings%20underscore%20how%20sensory%0Ainput%20quality%20and%20system%20architecture%20shape%20multimodal%20representation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Sound%252C%2520Hearing%2520Sight%253A%2520Uncovering%2520Modality%2520Bias%2520and%2520Conflict%2520of%2520AI%250A%2520%2520models%2520in%2520Sound%2520Localization%26entry.906535625%3DYanhao%2520Jia%2520and%2520Ji%2520Xie%2520and%2520S%2520Jivaganesh%2520and%2520Hao%2520Li%2520and%2520Xu%2520Wu%2520and%2520Mengmi%2520Zhang%26entry.1292438233%3D%2520%2520Imagine%2520hearing%2520a%2520dog%2520bark%2520and%2520turning%2520toward%2520the%2520sound%2520only%2520to%2520see%2520a%2520parked%250Acar%252C%2520while%2520the%2520real%252C%2520silent%2520dog%2520sits%2520elsewhere.%2520Such%2520sensory%2520conflicts%2520test%250Aperception%252C%2520yet%2520humans%2520reliably%2520resolve%2520them%2520by%2520prioritizing%2520sound%2520over%250Amisleading%2520visuals.%2520Despite%2520advances%2520in%2520multimodal%2520AI%2520integrating%2520vision%2520and%250Aaudio%252C%2520little%2520is%2520known%2520about%2520how%2520these%2520systems%2520handle%2520cross-modal%2520conflicts%2520or%250Awhether%2520they%2520favor%2520one%2520modality.%2520In%2520this%2520study%252C%2520we%2520systematically%2520examine%250Amodality%2520bias%2520and%2520conflict%2520resolution%2520in%2520AI%2520sound%2520localization.%2520We%2520assess%250Aleading%2520multimodal%2520models%2520and%2520benchmark%2520them%2520against%2520human%2520performance%2520in%250Apsychophysics%2520experiments%2520across%2520six%2520audiovisual%2520conditions%252C%2520including%250Acongruent%252C%2520conflicting%252C%2520and%2520absent%2520cues.%2520Humans%2520consistently%2520outperform%2520AI%252C%250Ademonstrating%2520superior%2520resilience%2520to%2520conflicting%2520or%2520missing%2520visuals%2520by%2520relying%250Aon%2520auditory%2520information.%2520In%2520contrast%252C%2520AI%2520models%2520often%2520default%2520to%2520visual%2520input%252C%250Adegrading%2520performance%2520to%2520near%2520chance%2520levels.%2520To%2520address%2520this%252C%2520we%2520finetune%2520a%250Astate-of-the-art%2520model%2520using%2520a%2520stereo%2520audio-image%2520dataset%2520generated%2520via%25203D%250Asimulations.%2520Even%2520with%2520limited%2520training%2520data%252C%2520the%2520refined%2520model%2520surpasses%250Aexisting%2520benchmarks.%2520Notably%252C%2520it%2520also%2520mirrors%2520human-like%2520horizontal%250Alocalization%2520bias%2520favoring%2520left-right%2520precision-likely%2520due%2520to%2520the%2520stereo%2520audio%250Astructure%2520reflecting%2520human%2520ear%2520placement.%2520These%2520findings%2520underscore%2520how%2520sensory%250Ainput%2520quality%2520and%2520system%2520architecture%2520shape%2520multimodal%2520representation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Sound%2C%20Hearing%20Sight%3A%20Uncovering%20Modality%20Bias%20and%20Conflict%20of%20AI%0A%20%20models%20in%20Sound%20Localization&entry.906535625=Yanhao%20Jia%20and%20Ji%20Xie%20and%20S%20Jivaganesh%20and%20Hao%20Li%20and%20Xu%20Wu%20and%20Mengmi%20Zhang&entry.1292438233=%20%20Imagine%20hearing%20a%20dog%20bark%20and%20turning%20toward%20the%20sound%20only%20to%20see%20a%20parked%0Acar%2C%20while%20the%20real%2C%20silent%20dog%20sits%20elsewhere.%20Such%20sensory%20conflicts%20test%0Aperception%2C%20yet%20humans%20reliably%20resolve%20them%20by%20prioritizing%20sound%20over%0Amisleading%20visuals.%20Despite%20advances%20in%20multimodal%20AI%20integrating%20vision%20and%0Aaudio%2C%20little%20is%20known%20about%20how%20these%20systems%20handle%20cross-modal%20conflicts%20or%0Awhether%20they%20favor%20one%20modality.%20In%20this%20study%2C%20we%20systematically%20examine%0Amodality%20bias%20and%20conflict%20resolution%20in%20AI%20sound%20localization.%20We%20assess%0Aleading%20multimodal%20models%20and%20benchmark%20them%20against%20human%20performance%20in%0Apsychophysics%20experiments%20across%20six%20audiovisual%20conditions%2C%20including%0Acongruent%2C%20conflicting%2C%20and%20absent%20cues.%20Humans%20consistently%20outperform%20AI%2C%0Ademonstrating%20superior%20resilience%20to%20conflicting%20or%20missing%20visuals%20by%20relying%0Aon%20auditory%20information.%20In%20contrast%2C%20AI%20models%20often%20default%20to%20visual%20input%2C%0Adegrading%20performance%20to%20near%20chance%20levels.%20To%20address%20this%2C%20we%20finetune%20a%0Astate-of-the-art%20model%20using%20a%20stereo%20audio-image%20dataset%20generated%20via%203D%0Asimulations.%20Even%20with%20limited%20training%20data%2C%20the%20refined%20model%20surpasses%0Aexisting%20benchmarks.%20Notably%2C%20it%20also%20mirrors%20human-like%20horizontal%0Alocalization%20bias%20favoring%20left-right%20precision-likely%20due%20to%20the%20stereo%20audio%0Astructure%20reflecting%20human%20ear%20placement.%20These%20findings%20underscore%20how%20sensory%0Ainput%20quality%20and%20system%20architecture%20shape%20multimodal%20representation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11217v1&entry.124074799=Read"},
{"title": "Normalized Matching Transformer", "author": "Abtin Pourhadi and Paul Swoboda", "abstract": "  We present a new state of the art approach for sparse keypoint matching\nbetween pairs of images. Our method consists of a fully deep learning based\napproach combining a visual backbone coupled with a SplineCNN graph neural\nnetwork for feature processing and a normalized transformer decoder for\ndecoding keypoint correspondences together with the Sinkhorn algorithm. Our\nmethod is trained using a contrastive and a hyperspherical loss for better\nfeature representations. We additionally use data augmentation during training.\nThis comparatively simple architecture combining extensive normalization and\nadvanced losses outperforms current state of the art approaches on PascalVOC\nand SPair-71k datasets by $5.1\\%$ and $2.2\\%$ respectively compared to BBGM,\nASAR, COMMON and GMTR while training for at least $1.7x$ fewer epochs.\n", "link": "http://arxiv.org/abs/2503.17715v2", "date": "2025-05-16", "relevancy": 2.2158, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5909}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5287}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normalized%20Matching%20Transformer&body=Title%3A%20Normalized%20Matching%20Transformer%0AAuthor%3A%20Abtin%20Pourhadi%20and%20Paul%20Swoboda%0AAbstract%3A%20%20%20We%20present%20a%20new%20state%20of%20the%20art%20approach%20for%20sparse%20keypoint%20matching%0Abetween%20pairs%20of%20images.%20Our%20method%20consists%20of%20a%20fully%20deep%20learning%20based%0Aapproach%20combining%20a%20visual%20backbone%20coupled%20with%20a%20SplineCNN%20graph%20neural%0Anetwork%20for%20feature%20processing%20and%20a%20normalized%20transformer%20decoder%20for%0Adecoding%20keypoint%20correspondences%20together%20with%20the%20Sinkhorn%20algorithm.%20Our%0Amethod%20is%20trained%20using%20a%20contrastive%20and%20a%20hyperspherical%20loss%20for%20better%0Afeature%20representations.%20We%20additionally%20use%20data%20augmentation%20during%20training.%0AThis%20comparatively%20simple%20architecture%20combining%20extensive%20normalization%20and%0Aadvanced%20losses%20outperforms%20current%20state%20of%20the%20art%20approaches%20on%20PascalVOC%0Aand%20SPair-71k%20datasets%20by%20%245.1%5C%25%24%20and%20%242.2%5C%25%24%20respectively%20compared%20to%20BBGM%2C%0AASAR%2C%20COMMON%20and%20GMTR%20while%20training%20for%20at%20least%20%241.7x%24%20fewer%20epochs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalized%2520Matching%2520Transformer%26entry.906535625%3DAbtin%2520Pourhadi%2520and%2520Paul%2520Swoboda%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520state%2520of%2520the%2520art%2520approach%2520for%2520sparse%2520keypoint%2520matching%250Abetween%2520pairs%2520of%2520images.%2520Our%2520method%2520consists%2520of%2520a%2520fully%2520deep%2520learning%2520based%250Aapproach%2520combining%2520a%2520visual%2520backbone%2520coupled%2520with%2520a%2520SplineCNN%2520graph%2520neural%250Anetwork%2520for%2520feature%2520processing%2520and%2520a%2520normalized%2520transformer%2520decoder%2520for%250Adecoding%2520keypoint%2520correspondences%2520together%2520with%2520the%2520Sinkhorn%2520algorithm.%2520Our%250Amethod%2520is%2520trained%2520using%2520a%2520contrastive%2520and%2520a%2520hyperspherical%2520loss%2520for%2520better%250Afeature%2520representations.%2520We%2520additionally%2520use%2520data%2520augmentation%2520during%2520training.%250AThis%2520comparatively%2520simple%2520architecture%2520combining%2520extensive%2520normalization%2520and%250Aadvanced%2520losses%2520outperforms%2520current%2520state%2520of%2520the%2520art%2520approaches%2520on%2520PascalVOC%250Aand%2520SPair-71k%2520datasets%2520by%2520%25245.1%255C%2525%2524%2520and%2520%25242.2%255C%2525%2524%2520respectively%2520compared%2520to%2520BBGM%252C%250AASAR%252C%2520COMMON%2520and%2520GMTR%2520while%2520training%2520for%2520at%2520least%2520%25241.7x%2524%2520fewer%2520epochs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalized%20Matching%20Transformer&entry.906535625=Abtin%20Pourhadi%20and%20Paul%20Swoboda&entry.1292438233=%20%20We%20present%20a%20new%20state%20of%20the%20art%20approach%20for%20sparse%20keypoint%20matching%0Abetween%20pairs%20of%20images.%20Our%20method%20consists%20of%20a%20fully%20deep%20learning%20based%0Aapproach%20combining%20a%20visual%20backbone%20coupled%20with%20a%20SplineCNN%20graph%20neural%0Anetwork%20for%20feature%20processing%20and%20a%20normalized%20transformer%20decoder%20for%0Adecoding%20keypoint%20correspondences%20together%20with%20the%20Sinkhorn%20algorithm.%20Our%0Amethod%20is%20trained%20using%20a%20contrastive%20and%20a%20hyperspherical%20loss%20for%20better%0Afeature%20representations.%20We%20additionally%20use%20data%20augmentation%20during%20training.%0AThis%20comparatively%20simple%20architecture%20combining%20extensive%20normalization%20and%0Aadvanced%20losses%20outperforms%20current%20state%20of%20the%20art%20approaches%20on%20PascalVOC%0Aand%20SPair-71k%20datasets%20by%20%245.1%5C%25%24%20and%20%242.2%5C%25%24%20respectively%20compared%20to%20BBGM%2C%0AASAR%2C%20COMMON%20and%20GMTR%20while%20training%20for%20at%20least%20%241.7x%24%20fewer%20epochs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17715v2&entry.124074799=Read"},
{"title": "Modeling cognitive processes of natural reading with transformer-based\n  Language Models", "author": "Bruno Bianchi and Ferm\u00edn Travi and Juan E. Kamienkowski", "abstract": "  Recent advances in Natural Language Processing (NLP) have led to the\ndevelopment of highly sophisticated language models for text generation. In\nparallel, neuroscience has increasingly employed these models to explore\ncognitive processes involved in language comprehension. Previous research has\nshown that models such as N-grams and LSTM networks can partially account for\npredictability effects in explaining eye movement behaviors, specifically Gaze\nDuration, during reading. In this study, we extend these findings by evaluating\ntransformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate\nthis relationship. Our results indicate that these architectures outperform\nearlier models in explaining the variance in Gaze Durations recorded from\nRioplantense Spanish readers. However, similar to previous studies, these\nmodels still fail to account for the entirety of the variance captured by human\npredictability. These findings suggest that, despite their advancements,\nstate-of-the-art language models continue to predict language in ways that\ndiffer from human readers.\n", "link": "http://arxiv.org/abs/2505.11485v1", "date": "2025-05-16", "relevancy": 2.2156, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5833}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20cognitive%20processes%20of%20natural%20reading%20with%20transformer-based%0A%20%20Language%20Models&body=Title%3A%20Modeling%20cognitive%20processes%20of%20natural%20reading%20with%20transformer-based%0A%20%20Language%20Models%0AAuthor%3A%20Bruno%20Bianchi%20and%20Ferm%C3%ADn%20Travi%20and%20Juan%20E.%20Kamienkowski%0AAbstract%3A%20%20%20Recent%20advances%20in%20Natural%20Language%20Processing%20%28NLP%29%20have%20led%20to%20the%0Adevelopment%20of%20highly%20sophisticated%20language%20models%20for%20text%20generation.%20In%0Aparallel%2C%20neuroscience%20has%20increasingly%20employed%20these%20models%20to%20explore%0Acognitive%20processes%20involved%20in%20language%20comprehension.%20Previous%20research%20has%0Ashown%20that%20models%20such%20as%20N-grams%20and%20LSTM%20networks%20can%20partially%20account%20for%0Apredictability%20effects%20in%20explaining%20eye%20movement%20behaviors%2C%20specifically%20Gaze%0ADuration%2C%20during%20reading.%20In%20this%20study%2C%20we%20extend%20these%20findings%20by%20evaluating%0Atransformer-based%20models%20%28GPT2%2C%20LLaMA-7B%2C%20and%20LLaMA2-7B%29%20to%20further%20investigate%0Athis%20relationship.%20Our%20results%20indicate%20that%20these%20architectures%20outperform%0Aearlier%20models%20in%20explaining%20the%20variance%20in%20Gaze%20Durations%20recorded%20from%0ARioplantense%20Spanish%20readers.%20However%2C%20similar%20to%20previous%20studies%2C%20these%0Amodels%20still%20fail%20to%20account%20for%20the%20entirety%20of%20the%20variance%20captured%20by%20human%0Apredictability.%20These%20findings%20suggest%20that%2C%20despite%20their%20advancements%2C%0Astate-of-the-art%20language%20models%20continue%20to%20predict%20language%20in%20ways%20that%0Adiffer%20from%20human%20readers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520cognitive%2520processes%2520of%2520natural%2520reading%2520with%2520transformer-based%250A%2520%2520Language%2520Models%26entry.906535625%3DBruno%2520Bianchi%2520and%2520Ferm%25C3%25ADn%2520Travi%2520and%2520Juan%2520E.%2520Kamienkowski%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520have%2520led%2520to%2520the%250Adevelopment%2520of%2520highly%2520sophisticated%2520language%2520models%2520for%2520text%2520generation.%2520In%250Aparallel%252C%2520neuroscience%2520has%2520increasingly%2520employed%2520these%2520models%2520to%2520explore%250Acognitive%2520processes%2520involved%2520in%2520language%2520comprehension.%2520Previous%2520research%2520has%250Ashown%2520that%2520models%2520such%2520as%2520N-grams%2520and%2520LSTM%2520networks%2520can%2520partially%2520account%2520for%250Apredictability%2520effects%2520in%2520explaining%2520eye%2520movement%2520behaviors%252C%2520specifically%2520Gaze%250ADuration%252C%2520during%2520reading.%2520In%2520this%2520study%252C%2520we%2520extend%2520these%2520findings%2520by%2520evaluating%250Atransformer-based%2520models%2520%2528GPT2%252C%2520LLaMA-7B%252C%2520and%2520LLaMA2-7B%2529%2520to%2520further%2520investigate%250Athis%2520relationship.%2520Our%2520results%2520indicate%2520that%2520these%2520architectures%2520outperform%250Aearlier%2520models%2520in%2520explaining%2520the%2520variance%2520in%2520Gaze%2520Durations%2520recorded%2520from%250ARioplantense%2520Spanish%2520readers.%2520However%252C%2520similar%2520to%2520previous%2520studies%252C%2520these%250Amodels%2520still%2520fail%2520to%2520account%2520for%2520the%2520entirety%2520of%2520the%2520variance%2520captured%2520by%2520human%250Apredictability.%2520These%2520findings%2520suggest%2520that%252C%2520despite%2520their%2520advancements%252C%250Astate-of-the-art%2520language%2520models%2520continue%2520to%2520predict%2520language%2520in%2520ways%2520that%250Adiffer%2520from%2520human%2520readers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20cognitive%20processes%20of%20natural%20reading%20with%20transformer-based%0A%20%20Language%20Models&entry.906535625=Bruno%20Bianchi%20and%20Ferm%C3%ADn%20Travi%20and%20Juan%20E.%20Kamienkowski&entry.1292438233=%20%20Recent%20advances%20in%20Natural%20Language%20Processing%20%28NLP%29%20have%20led%20to%20the%0Adevelopment%20of%20highly%20sophisticated%20language%20models%20for%20text%20generation.%20In%0Aparallel%2C%20neuroscience%20has%20increasingly%20employed%20these%20models%20to%20explore%0Acognitive%20processes%20involved%20in%20language%20comprehension.%20Previous%20research%20has%0Ashown%20that%20models%20such%20as%20N-grams%20and%20LSTM%20networks%20can%20partially%20account%20for%0Apredictability%20effects%20in%20explaining%20eye%20movement%20behaviors%2C%20specifically%20Gaze%0ADuration%2C%20during%20reading.%20In%20this%20study%2C%20we%20extend%20these%20findings%20by%20evaluating%0Atransformer-based%20models%20%28GPT2%2C%20LLaMA-7B%2C%20and%20LLaMA2-7B%29%20to%20further%20investigate%0Athis%20relationship.%20Our%20results%20indicate%20that%20these%20architectures%20outperform%0Aearlier%20models%20in%20explaining%20the%20variance%20in%20Gaze%20Durations%20recorded%20from%0ARioplantense%20Spanish%20readers.%20However%2C%20similar%20to%20previous%20studies%2C%20these%0Amodels%20still%20fail%20to%20account%20for%20the%20entirety%20of%20the%20variance%20captured%20by%20human%0Apredictability.%20These%20findings%20suggest%20that%2C%20despite%20their%20advancements%2C%0Astate-of-the-art%20language%20models%20continue%20to%20predict%20language%20in%20ways%20that%0Adiffer%20from%20human%20readers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11485v1&entry.124074799=Read"},
{"title": "L-WISE: Boosting Human Visual Category Learning Through Model-Based\n  Image Selection and Enhancement", "author": "Morgan B. Talbot and Gabriel Kreiman and James J. DiCarlo and Guy Gaziv", "abstract": "  The currently leading artificial neural network models of the visual ventral\nstream - which are derived from a combination of performance optimization and\nrobustification methods - have demonstrated a remarkable degree of behavioral\nalignment with humans on visual categorization tasks. We show that image\nperturbations generated by these models can enhance the ability of humans to\naccurately report the ground truth class. Furthermore, we find that the same\nmodels can also be used out-of-the-box to predict the proportion of correct\nhuman responses to individual images, providing a simple, human-aligned\nestimator of the relative difficulty of each image. Motivated by these\nobservations, we propose to augment visual learning in humans in a way that\nimproves human categorization accuracy at test time. Our learning augmentation\napproach consists of (i) selecting images based on their model-estimated\nrecognition difficulty, and (ii) applying image perturbations that aid\nrecognition for novice learners. We find that combining these model-based\nstrategies leads to categorization accuracy gains of 33-72% relative to control\nsubjects without these interventions, on unmodified, randomly selected held-out\ntest images. Beyond the accuracy gain, the training time for the augmented\nlearning group was also shortened by 20-23%, despite both groups completing the\nsame number of training trials. We demonstrate the efficacy of our approach in\na fine-grained categorization task with natural images, as well as two tasks in\nclinically relevant image domains - histology and dermoscopy - where visual\nlearning is notoriously challenging. To the best of our knowledge, our work is\nthe first application of artificial neural networks to increase visual learning\nperformance in humans by enhancing category-specific image features.\n", "link": "http://arxiv.org/abs/2412.09765v4", "date": "2025-05-16", "relevancy": 2.2143, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5586}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5554}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L-WISE%3A%20Boosting%20Human%20Visual%20Category%20Learning%20Through%20Model-Based%0A%20%20Image%20Selection%20and%20Enhancement&body=Title%3A%20L-WISE%3A%20Boosting%20Human%20Visual%20Category%20Learning%20Through%20Model-Based%0A%20%20Image%20Selection%20and%20Enhancement%0AAuthor%3A%20Morgan%20B.%20Talbot%20and%20Gabriel%20Kreiman%20and%20James%20J.%20DiCarlo%20and%20Guy%20Gaziv%0AAbstract%3A%20%20%20The%20currently%20leading%20artificial%20neural%20network%20models%20of%20the%20visual%20ventral%0Astream%20-%20which%20are%20derived%20from%20a%20combination%20of%20performance%20optimization%20and%0Arobustification%20methods%20-%20have%20demonstrated%20a%20remarkable%20degree%20of%20behavioral%0Aalignment%20with%20humans%20on%20visual%20categorization%20tasks.%20We%20show%20that%20image%0Aperturbations%20generated%20by%20these%20models%20can%20enhance%20the%20ability%20of%20humans%20to%0Aaccurately%20report%20the%20ground%20truth%20class.%20Furthermore%2C%20we%20find%20that%20the%20same%0Amodels%20can%20also%20be%20used%20out-of-the-box%20to%20predict%20the%20proportion%20of%20correct%0Ahuman%20responses%20to%20individual%20images%2C%20providing%20a%20simple%2C%20human-aligned%0Aestimator%20of%20the%20relative%20difficulty%20of%20each%20image.%20Motivated%20by%20these%0Aobservations%2C%20we%20propose%20to%20augment%20visual%20learning%20in%20humans%20in%20a%20way%20that%0Aimproves%20human%20categorization%20accuracy%20at%20test%20time.%20Our%20learning%20augmentation%0Aapproach%20consists%20of%20%28i%29%20selecting%20images%20based%20on%20their%20model-estimated%0Arecognition%20difficulty%2C%20and%20%28ii%29%20applying%20image%20perturbations%20that%20aid%0Arecognition%20for%20novice%20learners.%20We%20find%20that%20combining%20these%20model-based%0Astrategies%20leads%20to%20categorization%20accuracy%20gains%20of%2033-72%25%20relative%20to%20control%0Asubjects%20without%20these%20interventions%2C%20on%20unmodified%2C%20randomly%20selected%20held-out%0Atest%20images.%20Beyond%20the%20accuracy%20gain%2C%20the%20training%20time%20for%20the%20augmented%0Alearning%20group%20was%20also%20shortened%20by%2020-23%25%2C%20despite%20both%20groups%20completing%20the%0Asame%20number%20of%20training%20trials.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%0Aa%20fine-grained%20categorization%20task%20with%20natural%20images%2C%20as%20well%20as%20two%20tasks%20in%0Aclinically%20relevant%20image%20domains%20-%20histology%20and%20dermoscopy%20-%20where%20visual%0Alearning%20is%20notoriously%20challenging.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20work%20is%0Athe%20first%20application%20of%20artificial%20neural%20networks%20to%20increase%20visual%20learning%0Aperformance%20in%20humans%20by%20enhancing%20category-specific%20image%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09765v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL-WISE%253A%2520Boosting%2520Human%2520Visual%2520Category%2520Learning%2520Through%2520Model-Based%250A%2520%2520Image%2520Selection%2520and%2520Enhancement%26entry.906535625%3DMorgan%2520B.%2520Talbot%2520and%2520Gabriel%2520Kreiman%2520and%2520James%2520J.%2520DiCarlo%2520and%2520Guy%2520Gaziv%26entry.1292438233%3D%2520%2520The%2520currently%2520leading%2520artificial%2520neural%2520network%2520models%2520of%2520the%2520visual%2520ventral%250Astream%2520-%2520which%2520are%2520derived%2520from%2520a%2520combination%2520of%2520performance%2520optimization%2520and%250Arobustification%2520methods%2520-%2520have%2520demonstrated%2520a%2520remarkable%2520degree%2520of%2520behavioral%250Aalignment%2520with%2520humans%2520on%2520visual%2520categorization%2520tasks.%2520We%2520show%2520that%2520image%250Aperturbations%2520generated%2520by%2520these%2520models%2520can%2520enhance%2520the%2520ability%2520of%2520humans%2520to%250Aaccurately%2520report%2520the%2520ground%2520truth%2520class.%2520Furthermore%252C%2520we%2520find%2520that%2520the%2520same%250Amodels%2520can%2520also%2520be%2520used%2520out-of-the-box%2520to%2520predict%2520the%2520proportion%2520of%2520correct%250Ahuman%2520responses%2520to%2520individual%2520images%252C%2520providing%2520a%2520simple%252C%2520human-aligned%250Aestimator%2520of%2520the%2520relative%2520difficulty%2520of%2520each%2520image.%2520Motivated%2520by%2520these%250Aobservations%252C%2520we%2520propose%2520to%2520augment%2520visual%2520learning%2520in%2520humans%2520in%2520a%2520way%2520that%250Aimproves%2520human%2520categorization%2520accuracy%2520at%2520test%2520time.%2520Our%2520learning%2520augmentation%250Aapproach%2520consists%2520of%2520%2528i%2529%2520selecting%2520images%2520based%2520on%2520their%2520model-estimated%250Arecognition%2520difficulty%252C%2520and%2520%2528ii%2529%2520applying%2520image%2520perturbations%2520that%2520aid%250Arecognition%2520for%2520novice%2520learners.%2520We%2520find%2520that%2520combining%2520these%2520model-based%250Astrategies%2520leads%2520to%2520categorization%2520accuracy%2520gains%2520of%252033-72%2525%2520relative%2520to%2520control%250Asubjects%2520without%2520these%2520interventions%252C%2520on%2520unmodified%252C%2520randomly%2520selected%2520held-out%250Atest%2520images.%2520Beyond%2520the%2520accuracy%2520gain%252C%2520the%2520training%2520time%2520for%2520the%2520augmented%250Alearning%2520group%2520was%2520also%2520shortened%2520by%252020-23%2525%252C%2520despite%2520both%2520groups%2520completing%2520the%250Asame%2520number%2520of%2520training%2520trials.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach%2520in%250Aa%2520fine-grained%2520categorization%2520task%2520with%2520natural%2520images%252C%2520as%2520well%2520as%2520two%2520tasks%2520in%250Aclinically%2520relevant%2520image%2520domains%2520-%2520histology%2520and%2520dermoscopy%2520-%2520where%2520visual%250Alearning%2520is%2520notoriously%2520challenging.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520work%2520is%250Athe%2520first%2520application%2520of%2520artificial%2520neural%2520networks%2520to%2520increase%2520visual%2520learning%250Aperformance%2520in%2520humans%2520by%2520enhancing%2520category-specific%2520image%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09765v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L-WISE%3A%20Boosting%20Human%20Visual%20Category%20Learning%20Through%20Model-Based%0A%20%20Image%20Selection%20and%20Enhancement&entry.906535625=Morgan%20B.%20Talbot%20and%20Gabriel%20Kreiman%20and%20James%20J.%20DiCarlo%20and%20Guy%20Gaziv&entry.1292438233=%20%20The%20currently%20leading%20artificial%20neural%20network%20models%20of%20the%20visual%20ventral%0Astream%20-%20which%20are%20derived%20from%20a%20combination%20of%20performance%20optimization%20and%0Arobustification%20methods%20-%20have%20demonstrated%20a%20remarkable%20degree%20of%20behavioral%0Aalignment%20with%20humans%20on%20visual%20categorization%20tasks.%20We%20show%20that%20image%0Aperturbations%20generated%20by%20these%20models%20can%20enhance%20the%20ability%20of%20humans%20to%0Aaccurately%20report%20the%20ground%20truth%20class.%20Furthermore%2C%20we%20find%20that%20the%20same%0Amodels%20can%20also%20be%20used%20out-of-the-box%20to%20predict%20the%20proportion%20of%20correct%0Ahuman%20responses%20to%20individual%20images%2C%20providing%20a%20simple%2C%20human-aligned%0Aestimator%20of%20the%20relative%20difficulty%20of%20each%20image.%20Motivated%20by%20these%0Aobservations%2C%20we%20propose%20to%20augment%20visual%20learning%20in%20humans%20in%20a%20way%20that%0Aimproves%20human%20categorization%20accuracy%20at%20test%20time.%20Our%20learning%20augmentation%0Aapproach%20consists%20of%20%28i%29%20selecting%20images%20based%20on%20their%20model-estimated%0Arecognition%20difficulty%2C%20and%20%28ii%29%20applying%20image%20perturbations%20that%20aid%0Arecognition%20for%20novice%20learners.%20We%20find%20that%20combining%20these%20model-based%0Astrategies%20leads%20to%20categorization%20accuracy%20gains%20of%2033-72%25%20relative%20to%20control%0Asubjects%20without%20these%20interventions%2C%20on%20unmodified%2C%20randomly%20selected%20held-out%0Atest%20images.%20Beyond%20the%20accuracy%20gain%2C%20the%20training%20time%20for%20the%20augmented%0Alearning%20group%20was%20also%20shortened%20by%2020-23%25%2C%20despite%20both%20groups%20completing%20the%0Asame%20number%20of%20training%20trials.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%0Aa%20fine-grained%20categorization%20task%20with%20natural%20images%2C%20as%20well%20as%20two%20tasks%20in%0Aclinically%20relevant%20image%20domains%20-%20histology%20and%20dermoscopy%20-%20where%20visual%0Alearning%20is%20notoriously%20challenging.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20work%20is%0Athe%20first%20application%20of%20artificial%20neural%20networks%20to%20increase%20visual%20learning%0Aperformance%20in%20humans%20by%20enhancing%20category-specific%20image%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09765v4&entry.124074799=Read"},
{"title": "AW-GATCN: Adaptive Weighted Graph Attention Convolutional Network for\n  Event Camera Data Joint Denoising and Object Recognition", "author": "Haiyu Li and Charith Abhayaratne", "abstract": "  Event cameras, which capture brightness changes with high temporal\nresolution, inherently generate a significant amount of redundant and noisy\ndata beyond essential object structures. The primary challenge in event-based\nobject recognition lies in effectively removing this noise without losing\ncritical spatial-temporal information. To address this, we propose an Adaptive\nGraph-based Noisy Data Removal framework for Event-based Object Recognition.\nSpecifically, our approach integrates adaptive event segmentation based on\nnormalized density analysis, a multifactorial edge-weighting mechanism, and\nadaptive graph-based denoising strategies. These innovations significantly\nenhance the integration of spatiotemporal information, effectively filtering\nnoise while preserving critical structural features for robust recognition.\nExperimental evaluations on four challenging datasets demonstrate that our\nmethod achieves superior recognition accuracies of 83.77%, 76.79%, 99.30%, and\n96.89%, surpassing existing graph-based methods by up to 8.79%, and improving\nnoise reduction performance by up to 19.57%, with an additional accuracy gain\nof 6.26% compared to traditional Euclidean-based techniques.\n", "link": "http://arxiv.org/abs/2505.11232v1", "date": "2025-05-16", "relevancy": 2.2058, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5525}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.551}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AW-GATCN%3A%20Adaptive%20Weighted%20Graph%20Attention%20Convolutional%20Network%20for%0A%20%20Event%20Camera%20Data%20Joint%20Denoising%20and%20Object%20Recognition&body=Title%3A%20AW-GATCN%3A%20Adaptive%20Weighted%20Graph%20Attention%20Convolutional%20Network%20for%0A%20%20Event%20Camera%20Data%20Joint%20Denoising%20and%20Object%20Recognition%0AAuthor%3A%20Haiyu%20Li%20and%20Charith%20Abhayaratne%0AAbstract%3A%20%20%20Event%20cameras%2C%20which%20capture%20brightness%20changes%20with%20high%20temporal%0Aresolution%2C%20inherently%20generate%20a%20significant%20amount%20of%20redundant%20and%20noisy%0Adata%20beyond%20essential%20object%20structures.%20The%20primary%20challenge%20in%20event-based%0Aobject%20recognition%20lies%20in%20effectively%20removing%20this%20noise%20without%20losing%0Acritical%20spatial-temporal%20information.%20To%20address%20this%2C%20we%20propose%20an%20Adaptive%0AGraph-based%20Noisy%20Data%20Removal%20framework%20for%20Event-based%20Object%20Recognition.%0ASpecifically%2C%20our%20approach%20integrates%20adaptive%20event%20segmentation%20based%20on%0Anormalized%20density%20analysis%2C%20a%20multifactorial%20edge-weighting%20mechanism%2C%20and%0Aadaptive%20graph-based%20denoising%20strategies.%20These%20innovations%20significantly%0Aenhance%20the%20integration%20of%20spatiotemporal%20information%2C%20effectively%20filtering%0Anoise%20while%20preserving%20critical%20structural%20features%20for%20robust%20recognition.%0AExperimental%20evaluations%20on%20four%20challenging%20datasets%20demonstrate%20that%20our%0Amethod%20achieves%20superior%20recognition%20accuracies%20of%2083.77%25%2C%2076.79%25%2C%2099.30%25%2C%20and%0A96.89%25%2C%20surpassing%20existing%20graph-based%20methods%20by%20up%20to%208.79%25%2C%20and%20improving%0Anoise%20reduction%20performance%20by%20up%20to%2019.57%25%2C%20with%20an%20additional%20accuracy%20gain%0Aof%206.26%25%20compared%20to%20traditional%20Euclidean-based%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAW-GATCN%253A%2520Adaptive%2520Weighted%2520Graph%2520Attention%2520Convolutional%2520Network%2520for%250A%2520%2520Event%2520Camera%2520Data%2520Joint%2520Denoising%2520and%2520Object%2520Recognition%26entry.906535625%3DHaiyu%2520Li%2520and%2520Charith%2520Abhayaratne%26entry.1292438233%3D%2520%2520Event%2520cameras%252C%2520which%2520capture%2520brightness%2520changes%2520with%2520high%2520temporal%250Aresolution%252C%2520inherently%2520generate%2520a%2520significant%2520amount%2520of%2520redundant%2520and%2520noisy%250Adata%2520beyond%2520essential%2520object%2520structures.%2520The%2520primary%2520challenge%2520in%2520event-based%250Aobject%2520recognition%2520lies%2520in%2520effectively%2520removing%2520this%2520noise%2520without%2520losing%250Acritical%2520spatial-temporal%2520information.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520Adaptive%250AGraph-based%2520Noisy%2520Data%2520Removal%2520framework%2520for%2520Event-based%2520Object%2520Recognition.%250ASpecifically%252C%2520our%2520approach%2520integrates%2520adaptive%2520event%2520segmentation%2520based%2520on%250Anormalized%2520density%2520analysis%252C%2520a%2520multifactorial%2520edge-weighting%2520mechanism%252C%2520and%250Aadaptive%2520graph-based%2520denoising%2520strategies.%2520These%2520innovations%2520significantly%250Aenhance%2520the%2520integration%2520of%2520spatiotemporal%2520information%252C%2520effectively%2520filtering%250Anoise%2520while%2520preserving%2520critical%2520structural%2520features%2520for%2520robust%2520recognition.%250AExperimental%2520evaluations%2520on%2520four%2520challenging%2520datasets%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520superior%2520recognition%2520accuracies%2520of%252083.77%2525%252C%252076.79%2525%252C%252099.30%2525%252C%2520and%250A96.89%2525%252C%2520surpassing%2520existing%2520graph-based%2520methods%2520by%2520up%2520to%25208.79%2525%252C%2520and%2520improving%250Anoise%2520reduction%2520performance%2520by%2520up%2520to%252019.57%2525%252C%2520with%2520an%2520additional%2520accuracy%2520gain%250Aof%25206.26%2525%2520compared%2520to%2520traditional%2520Euclidean-based%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AW-GATCN%3A%20Adaptive%20Weighted%20Graph%20Attention%20Convolutional%20Network%20for%0A%20%20Event%20Camera%20Data%20Joint%20Denoising%20and%20Object%20Recognition&entry.906535625=Haiyu%20Li%20and%20Charith%20Abhayaratne&entry.1292438233=%20%20Event%20cameras%2C%20which%20capture%20brightness%20changes%20with%20high%20temporal%0Aresolution%2C%20inherently%20generate%20a%20significant%20amount%20of%20redundant%20and%20noisy%0Adata%20beyond%20essential%20object%20structures.%20The%20primary%20challenge%20in%20event-based%0Aobject%20recognition%20lies%20in%20effectively%20removing%20this%20noise%20without%20losing%0Acritical%20spatial-temporal%20information.%20To%20address%20this%2C%20we%20propose%20an%20Adaptive%0AGraph-based%20Noisy%20Data%20Removal%20framework%20for%20Event-based%20Object%20Recognition.%0ASpecifically%2C%20our%20approach%20integrates%20adaptive%20event%20segmentation%20based%20on%0Anormalized%20density%20analysis%2C%20a%20multifactorial%20edge-weighting%20mechanism%2C%20and%0Aadaptive%20graph-based%20denoising%20strategies.%20These%20innovations%20significantly%0Aenhance%20the%20integration%20of%20spatiotemporal%20information%2C%20effectively%20filtering%0Anoise%20while%20preserving%20critical%20structural%20features%20for%20robust%20recognition.%0AExperimental%20evaluations%20on%20four%20challenging%20datasets%20demonstrate%20that%20our%0Amethod%20achieves%20superior%20recognition%20accuracies%20of%2083.77%25%2C%2076.79%25%2C%2099.30%25%2C%20and%0A96.89%25%2C%20surpassing%20existing%20graph-based%20methods%20by%20up%20to%208.79%25%2C%20and%20improving%0Anoise%20reduction%20performance%20by%20up%20to%2019.57%25%2C%20with%20an%20additional%20accuracy%20gain%0Aof%206.26%25%20compared%20to%20traditional%20Euclidean-based%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11232v1&entry.124074799=Read"},
{"title": "VIN-NBV: A View Introspection Network for Next-Best-View Selection for\n  Resource-Efficient 3D Reconstruction", "author": "Noah Frahm and Dongxu Zhao and Andrea Dunn Beltran and Ron Alterovitz and Jan-Michael Frahm and Junier Oliva and Roni Sengupta", "abstract": "  Next Best View (NBV) algorithms aim to acquire an optimal set of images using\nminimal resources, time, or number of captures to enable efficient 3D\nreconstruction of a scene. Existing approaches often rely on prior scene\nknowledge or additional image captures and often develop policies that maximize\ncoverage. Yet, for many real scenes with complex geometry and self-occlusions,\ncoverage maximization does not lead to better reconstruction quality directly.\nIn this paper, we propose the View Introspection Network (VIN), which is\ntrained to predict the reconstruction quality improvement of views directly,\nand the VIN-NBV policy. A greedy sequential sampling-based policy, where at\neach acquisition step, we sample multiple query views and choose the one with\nthe highest VIN predicted improvement score. We design the VIN to perform\n3D-aware featurization of the reconstruction built from prior acquisitions, and\nfor each query view create a feature that can be decoded into an improvement\nscore. We then train the VIN using imitation learning to predict the\nreconstruction improvement score. We show that VIN-NBV improves reconstruction\nquality by ~30% over a coverage maximization baseline when operating with\nconstraints on the number of acquisitions or the time in motion.\n", "link": "http://arxiv.org/abs/2505.06219v2", "date": "2025-05-16", "relevancy": 2.1964, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5506}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5506}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIN-NBV%3A%20A%20View%20Introspection%20Network%20for%20Next-Best-View%20Selection%20for%0A%20%20Resource-Efficient%203D%20Reconstruction&body=Title%3A%20VIN-NBV%3A%20A%20View%20Introspection%20Network%20for%20Next-Best-View%20Selection%20for%0A%20%20Resource-Efficient%203D%20Reconstruction%0AAuthor%3A%20Noah%20Frahm%20and%20Dongxu%20Zhao%20and%20Andrea%20Dunn%20Beltran%20and%20Ron%20Alterovitz%20and%20Jan-Michael%20Frahm%20and%20Junier%20Oliva%20and%20Roni%20Sengupta%0AAbstract%3A%20%20%20Next%20Best%20View%20%28NBV%29%20algorithms%20aim%20to%20acquire%20an%20optimal%20set%20of%20images%20using%0Aminimal%20resources%2C%20time%2C%20or%20number%20of%20captures%20to%20enable%20efficient%203D%0Areconstruction%20of%20a%20scene.%20Existing%20approaches%20often%20rely%20on%20prior%20scene%0Aknowledge%20or%20additional%20image%20captures%20and%20often%20develop%20policies%20that%20maximize%0Acoverage.%20Yet%2C%20for%20many%20real%20scenes%20with%20complex%20geometry%20and%20self-occlusions%2C%0Acoverage%20maximization%20does%20not%20lead%20to%20better%20reconstruction%20quality%20directly.%0AIn%20this%20paper%2C%20we%20propose%20the%20View%20Introspection%20Network%20%28VIN%29%2C%20which%20is%0Atrained%20to%20predict%20the%20reconstruction%20quality%20improvement%20of%20views%20directly%2C%0Aand%20the%20VIN-NBV%20policy.%20A%20greedy%20sequential%20sampling-based%20policy%2C%20where%20at%0Aeach%20acquisition%20step%2C%20we%20sample%20multiple%20query%20views%20and%20choose%20the%20one%20with%0Athe%20highest%20VIN%20predicted%20improvement%20score.%20We%20design%20the%20VIN%20to%20perform%0A3D-aware%20featurization%20of%20the%20reconstruction%20built%20from%20prior%20acquisitions%2C%20and%0Afor%20each%20query%20view%20create%20a%20feature%20that%20can%20be%20decoded%20into%20an%20improvement%0Ascore.%20We%20then%20train%20the%20VIN%20using%20imitation%20learning%20to%20predict%20the%0Areconstruction%20improvement%20score.%20We%20show%20that%20VIN-NBV%20improves%20reconstruction%0Aquality%20by%20~30%25%20over%20a%20coverage%20maximization%20baseline%20when%20operating%20with%0Aconstraints%20on%20the%20number%20of%20acquisitions%20or%20the%20time%20in%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06219v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIN-NBV%253A%2520A%2520View%2520Introspection%2520Network%2520for%2520Next-Best-View%2520Selection%2520for%250A%2520%2520Resource-Efficient%25203D%2520Reconstruction%26entry.906535625%3DNoah%2520Frahm%2520and%2520Dongxu%2520Zhao%2520and%2520Andrea%2520Dunn%2520Beltran%2520and%2520Ron%2520Alterovitz%2520and%2520Jan-Michael%2520Frahm%2520and%2520Junier%2520Oliva%2520and%2520Roni%2520Sengupta%26entry.1292438233%3D%2520%2520Next%2520Best%2520View%2520%2528NBV%2529%2520algorithms%2520aim%2520to%2520acquire%2520an%2520optimal%2520set%2520of%2520images%2520using%250Aminimal%2520resources%252C%2520time%252C%2520or%2520number%2520of%2520captures%2520to%2520enable%2520efficient%25203D%250Areconstruction%2520of%2520a%2520scene.%2520Existing%2520approaches%2520often%2520rely%2520on%2520prior%2520scene%250Aknowledge%2520or%2520additional%2520image%2520captures%2520and%2520often%2520develop%2520policies%2520that%2520maximize%250Acoverage.%2520Yet%252C%2520for%2520many%2520real%2520scenes%2520with%2520complex%2520geometry%2520and%2520self-occlusions%252C%250Acoverage%2520maximization%2520does%2520not%2520lead%2520to%2520better%2520reconstruction%2520quality%2520directly.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520the%2520View%2520Introspection%2520Network%2520%2528VIN%2529%252C%2520which%2520is%250Atrained%2520to%2520predict%2520the%2520reconstruction%2520quality%2520improvement%2520of%2520views%2520directly%252C%250Aand%2520the%2520VIN-NBV%2520policy.%2520A%2520greedy%2520sequential%2520sampling-based%2520policy%252C%2520where%2520at%250Aeach%2520acquisition%2520step%252C%2520we%2520sample%2520multiple%2520query%2520views%2520and%2520choose%2520the%2520one%2520with%250Athe%2520highest%2520VIN%2520predicted%2520improvement%2520score.%2520We%2520design%2520the%2520VIN%2520to%2520perform%250A3D-aware%2520featurization%2520of%2520the%2520reconstruction%2520built%2520from%2520prior%2520acquisitions%252C%2520and%250Afor%2520each%2520query%2520view%2520create%2520a%2520feature%2520that%2520can%2520be%2520decoded%2520into%2520an%2520improvement%250Ascore.%2520We%2520then%2520train%2520the%2520VIN%2520using%2520imitation%2520learning%2520to%2520predict%2520the%250Areconstruction%2520improvement%2520score.%2520We%2520show%2520that%2520VIN-NBV%2520improves%2520reconstruction%250Aquality%2520by%2520~30%2525%2520over%2520a%2520coverage%2520maximization%2520baseline%2520when%2520operating%2520with%250Aconstraints%2520on%2520the%2520number%2520of%2520acquisitions%2520or%2520the%2520time%2520in%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06219v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIN-NBV%3A%20A%20View%20Introspection%20Network%20for%20Next-Best-View%20Selection%20for%0A%20%20Resource-Efficient%203D%20Reconstruction&entry.906535625=Noah%20Frahm%20and%20Dongxu%20Zhao%20and%20Andrea%20Dunn%20Beltran%20and%20Ron%20Alterovitz%20and%20Jan-Michael%20Frahm%20and%20Junier%20Oliva%20and%20Roni%20Sengupta&entry.1292438233=%20%20Next%20Best%20View%20%28NBV%29%20algorithms%20aim%20to%20acquire%20an%20optimal%20set%20of%20images%20using%0Aminimal%20resources%2C%20time%2C%20or%20number%20of%20captures%20to%20enable%20efficient%203D%0Areconstruction%20of%20a%20scene.%20Existing%20approaches%20often%20rely%20on%20prior%20scene%0Aknowledge%20or%20additional%20image%20captures%20and%20often%20develop%20policies%20that%20maximize%0Acoverage.%20Yet%2C%20for%20many%20real%20scenes%20with%20complex%20geometry%20and%20self-occlusions%2C%0Acoverage%20maximization%20does%20not%20lead%20to%20better%20reconstruction%20quality%20directly.%0AIn%20this%20paper%2C%20we%20propose%20the%20View%20Introspection%20Network%20%28VIN%29%2C%20which%20is%0Atrained%20to%20predict%20the%20reconstruction%20quality%20improvement%20of%20views%20directly%2C%0Aand%20the%20VIN-NBV%20policy.%20A%20greedy%20sequential%20sampling-based%20policy%2C%20where%20at%0Aeach%20acquisition%20step%2C%20we%20sample%20multiple%20query%20views%20and%20choose%20the%20one%20with%0Athe%20highest%20VIN%20predicted%20improvement%20score.%20We%20design%20the%20VIN%20to%20perform%0A3D-aware%20featurization%20of%20the%20reconstruction%20built%20from%20prior%20acquisitions%2C%20and%0Afor%20each%20query%20view%20create%20a%20feature%20that%20can%20be%20decoded%20into%20an%20improvement%0Ascore.%20We%20then%20train%20the%20VIN%20using%20imitation%20learning%20to%20predict%20the%0Areconstruction%20improvement%20score.%20We%20show%20that%20VIN-NBV%20improves%20reconstruction%0Aquality%20by%20~30%25%20over%20a%20coverage%20maximization%20baseline%20when%20operating%20with%0Aconstraints%20on%20the%20number%20of%20acquisitions%20or%20the%20time%20in%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06219v2&entry.124074799=Read"},
{"title": "Entropy-Driven Genetic Optimization for Deep-Feature-Guided Low-Light\n  Image Enhancement", "author": "Nirjhor Datta and Afroza Akther and M. Sohel Rahman", "abstract": "  Image enhancement methods often prioritize pixel level information,\noverlooking the semantic features. We propose a novel, unsupervised,\nfuzzy-inspired image enhancement framework guided by NSGA-II algorithm that\noptimizes image brightness, contrast, and gamma parameters to achieve a balance\nbetween visual quality and semantic fidelity. Central to our proposed method is\nthe use of a pre trained deep neural network as a feature extractor. To find\nthe best enhancement settings, we use a GPU-accelerated NSGA-II algorithm that\nbalances multiple objectives, namely, increasing image entropy, improving\nperceptual similarity, and maintaining appropriate brightness. We further\nimprove the results by applying a local search phase to fine-tune the top\ncandidates from the genetic algorithm. Our approach operates entirely without\npaired training data making it broadly applicable across domains with limited\nor noisy labels. Quantitatively, our model achieves excellent performance with\naverage BRISQUE and NIQE scores of 19.82 and 3.652, respectively, in all\nunpaired datasets. Qualitatively, enhanced images by our model exhibit\nsignificantly improved visibility in shadowed regions, natural balance of\ncontrast and also preserve the richer fine detail without introducing noticable\nartifacts. This work opens new directions for unsupervised image enhancement\nwhere semantic consistency is critical.\n", "link": "http://arxiv.org/abs/2505.11246v1", "date": "2025-05-16", "relevancy": 2.1936, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5564}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.555}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Driven%20Genetic%20Optimization%20for%20Deep-Feature-Guided%20Low-Light%0A%20%20Image%20Enhancement&body=Title%3A%20Entropy-Driven%20Genetic%20Optimization%20for%20Deep-Feature-Guided%20Low-Light%0A%20%20Image%20Enhancement%0AAuthor%3A%20Nirjhor%20Datta%20and%20Afroza%20Akther%20and%20M.%20Sohel%20Rahman%0AAbstract%3A%20%20%20Image%20enhancement%20methods%20often%20prioritize%20pixel%20level%20information%2C%0Aoverlooking%20the%20semantic%20features.%20We%20propose%20a%20novel%2C%20unsupervised%2C%0Afuzzy-inspired%20image%20enhancement%20framework%20guided%20by%20NSGA-II%20algorithm%20that%0Aoptimizes%20image%20brightness%2C%20contrast%2C%20and%20gamma%20parameters%20to%20achieve%20a%20balance%0Abetween%20visual%20quality%20and%20semantic%20fidelity.%20Central%20to%20our%20proposed%20method%20is%0Athe%20use%20of%20a%20pre%20trained%20deep%20neural%20network%20as%20a%20feature%20extractor.%20To%20find%0Athe%20best%20enhancement%20settings%2C%20we%20use%20a%20GPU-accelerated%20NSGA-II%20algorithm%20that%0Abalances%20multiple%20objectives%2C%20namely%2C%20increasing%20image%20entropy%2C%20improving%0Aperceptual%20similarity%2C%20and%20maintaining%20appropriate%20brightness.%20We%20further%0Aimprove%20the%20results%20by%20applying%20a%20local%20search%20phase%20to%20fine-tune%20the%20top%0Acandidates%20from%20the%20genetic%20algorithm.%20Our%20approach%20operates%20entirely%20without%0Apaired%20training%20data%20making%20it%20broadly%20applicable%20across%20domains%20with%20limited%0Aor%20noisy%20labels.%20Quantitatively%2C%20our%20model%20achieves%20excellent%20performance%20with%0Aaverage%20BRISQUE%20and%20NIQE%20scores%20of%2019.82%20and%203.652%2C%20respectively%2C%20in%20all%0Aunpaired%20datasets.%20Qualitatively%2C%20enhanced%20images%20by%20our%20model%20exhibit%0Asignificantly%20improved%20visibility%20in%20shadowed%20regions%2C%20natural%20balance%20of%0Acontrast%20and%20also%20preserve%20the%20richer%20fine%20detail%20without%20introducing%20noticable%0Aartifacts.%20This%20work%20opens%20new%20directions%20for%20unsupervised%20image%20enhancement%0Awhere%20semantic%20consistency%20is%20critical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Driven%2520Genetic%2520Optimization%2520for%2520Deep-Feature-Guided%2520Low-Light%250A%2520%2520Image%2520Enhancement%26entry.906535625%3DNirjhor%2520Datta%2520and%2520Afroza%2520Akther%2520and%2520M.%2520Sohel%2520Rahman%26entry.1292438233%3D%2520%2520Image%2520enhancement%2520methods%2520often%2520prioritize%2520pixel%2520level%2520information%252C%250Aoverlooking%2520the%2520semantic%2520features.%2520We%2520propose%2520a%2520novel%252C%2520unsupervised%252C%250Afuzzy-inspired%2520image%2520enhancement%2520framework%2520guided%2520by%2520NSGA-II%2520algorithm%2520that%250Aoptimizes%2520image%2520brightness%252C%2520contrast%252C%2520and%2520gamma%2520parameters%2520to%2520achieve%2520a%2520balance%250Abetween%2520visual%2520quality%2520and%2520semantic%2520fidelity.%2520Central%2520to%2520our%2520proposed%2520method%2520is%250Athe%2520use%2520of%2520a%2520pre%2520trained%2520deep%2520neural%2520network%2520as%2520a%2520feature%2520extractor.%2520To%2520find%250Athe%2520best%2520enhancement%2520settings%252C%2520we%2520use%2520a%2520GPU-accelerated%2520NSGA-II%2520algorithm%2520that%250Abalances%2520multiple%2520objectives%252C%2520namely%252C%2520increasing%2520image%2520entropy%252C%2520improving%250Aperceptual%2520similarity%252C%2520and%2520maintaining%2520appropriate%2520brightness.%2520We%2520further%250Aimprove%2520the%2520results%2520by%2520applying%2520a%2520local%2520search%2520phase%2520to%2520fine-tune%2520the%2520top%250Acandidates%2520from%2520the%2520genetic%2520algorithm.%2520Our%2520approach%2520operates%2520entirely%2520without%250Apaired%2520training%2520data%2520making%2520it%2520broadly%2520applicable%2520across%2520domains%2520with%2520limited%250Aor%2520noisy%2520labels.%2520Quantitatively%252C%2520our%2520model%2520achieves%2520excellent%2520performance%2520with%250Aaverage%2520BRISQUE%2520and%2520NIQE%2520scores%2520of%252019.82%2520and%25203.652%252C%2520respectively%252C%2520in%2520all%250Aunpaired%2520datasets.%2520Qualitatively%252C%2520enhanced%2520images%2520by%2520our%2520model%2520exhibit%250Asignificantly%2520improved%2520visibility%2520in%2520shadowed%2520regions%252C%2520natural%2520balance%2520of%250Acontrast%2520and%2520also%2520preserve%2520the%2520richer%2520fine%2520detail%2520without%2520introducing%2520noticable%250Aartifacts.%2520This%2520work%2520opens%2520new%2520directions%2520for%2520unsupervised%2520image%2520enhancement%250Awhere%2520semantic%2520consistency%2520is%2520critical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Driven%20Genetic%20Optimization%20for%20Deep-Feature-Guided%20Low-Light%0A%20%20Image%20Enhancement&entry.906535625=Nirjhor%20Datta%20and%20Afroza%20Akther%20and%20M.%20Sohel%20Rahman&entry.1292438233=%20%20Image%20enhancement%20methods%20often%20prioritize%20pixel%20level%20information%2C%0Aoverlooking%20the%20semantic%20features.%20We%20propose%20a%20novel%2C%20unsupervised%2C%0Afuzzy-inspired%20image%20enhancement%20framework%20guided%20by%20NSGA-II%20algorithm%20that%0Aoptimizes%20image%20brightness%2C%20contrast%2C%20and%20gamma%20parameters%20to%20achieve%20a%20balance%0Abetween%20visual%20quality%20and%20semantic%20fidelity.%20Central%20to%20our%20proposed%20method%20is%0Athe%20use%20of%20a%20pre%20trained%20deep%20neural%20network%20as%20a%20feature%20extractor.%20To%20find%0Athe%20best%20enhancement%20settings%2C%20we%20use%20a%20GPU-accelerated%20NSGA-II%20algorithm%20that%0Abalances%20multiple%20objectives%2C%20namely%2C%20increasing%20image%20entropy%2C%20improving%0Aperceptual%20similarity%2C%20and%20maintaining%20appropriate%20brightness.%20We%20further%0Aimprove%20the%20results%20by%20applying%20a%20local%20search%20phase%20to%20fine-tune%20the%20top%0Acandidates%20from%20the%20genetic%20algorithm.%20Our%20approach%20operates%20entirely%20without%0Apaired%20training%20data%20making%20it%20broadly%20applicable%20across%20domains%20with%20limited%0Aor%20noisy%20labels.%20Quantitatively%2C%20our%20model%20achieves%20excellent%20performance%20with%0Aaverage%20BRISQUE%20and%20NIQE%20scores%20of%2019.82%20and%203.652%2C%20respectively%2C%20in%20all%0Aunpaired%20datasets.%20Qualitatively%2C%20enhanced%20images%20by%20our%20model%20exhibit%0Asignificantly%20improved%20visibility%20in%20shadowed%20regions%2C%20natural%20balance%20of%0Acontrast%20and%20also%20preserve%20the%20richer%20fine%20detail%20without%20introducing%20noticable%0Aartifacts.%20This%20work%20opens%20new%20directions%20for%20unsupervised%20image%20enhancement%0Awhere%20semantic%20consistency%20is%20critical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11246v1&entry.124074799=Read"},
{"title": "IMPACT: A Generic Semantic Loss for Multimodal Medical Image\n  Registration", "author": "Valentin Boussot and C\u00e9dric H\u00e9mon and Jean-Claude Nunes and Jason Dowling and Simon Rouz\u00e9 and Caroline Lafond and Ana\u00efs Barateau and Jean-Louis Dillenseger", "abstract": "  Image registration is fundamental in medical imaging, enabling precise\nalignment of anatomical structures for diagnosis, treatment planning,\nimage-guided interventions, and longitudinal monitoring. This work introduces\nIMPACT (Image Metric with Pretrained model-Agnostic Comparison for\nTransmodality registration), a novel similarity metric designed for robust\nmultimodal image registration. Rather than relying on raw intensities,\nhandcrafted descriptors, or task-specific training, IMPACT defines a semantic\nsimilarity measure based on the comparison of deep features extracted from\nlarge-scale pretrained segmentation models. By leveraging representations from\nmodels such as TotalSegmentator, Segment Anything (SAM), and other foundation\nnetworks, IMPACT provides a task-agnostic, training-free solution that\ngeneralizes across imaging modalities. These features, originally trained for\nsegmentation, offer strong spatial correspondence and semantic alignment\ncapabilities, making them naturally suited for registration. The method\nintegrates seamlessly into both algorithmic (Elastix) and learning-based\n(VoxelMorph) frameworks, leveraging the strengths of each. IMPACT was evaluated\non five challenging 3D registration tasks involving thoracic CT/CBCT and pelvic\nMR/CT datasets. Quantitative metrics, including Target Registration Error and\nDice Similarity Coefficient, demonstrated consistent improvements in anatomical\nalignment over baseline methods. Qualitative analyses further highlighted the\nrobustness of the proposed metric in the presence of noise, artifacts, and\nmodality variations. With its versatility, efficiency, and strong performance\nacross diverse tasks, IMPACT offers a powerful solution for advancing\nmultimodal image registration in both clinical and research settings.\n", "link": "http://arxiv.org/abs/2503.24121v3", "date": "2025-05-16", "relevancy": 2.1934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5303}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMPACT%3A%20A%20Generic%20Semantic%20Loss%20for%20Multimodal%20Medical%20Image%0A%20%20Registration&body=Title%3A%20IMPACT%3A%20A%20Generic%20Semantic%20Loss%20for%20Multimodal%20Medical%20Image%0A%20%20Registration%0AAuthor%3A%20Valentin%20Boussot%20and%20C%C3%A9dric%20H%C3%A9mon%20and%20Jean-Claude%20Nunes%20and%20Jason%20Dowling%20and%20Simon%20Rouz%C3%A9%20and%20Caroline%20Lafond%20and%20Ana%C3%AFs%20Barateau%20and%20Jean-Louis%20Dillenseger%0AAbstract%3A%20%20%20Image%20registration%20is%20fundamental%20in%20medical%20imaging%2C%20enabling%20precise%0Aalignment%20of%20anatomical%20structures%20for%20diagnosis%2C%20treatment%20planning%2C%0Aimage-guided%20interventions%2C%20and%20longitudinal%20monitoring.%20This%20work%20introduces%0AIMPACT%20%28Image%20Metric%20with%20Pretrained%20model-Agnostic%20Comparison%20for%0ATransmodality%20registration%29%2C%20a%20novel%20similarity%20metric%20designed%20for%20robust%0Amultimodal%20image%20registration.%20Rather%20than%20relying%20on%20raw%20intensities%2C%0Ahandcrafted%20descriptors%2C%20or%20task-specific%20training%2C%20IMPACT%20defines%20a%20semantic%0Asimilarity%20measure%20based%20on%20the%20comparison%20of%20deep%20features%20extracted%20from%0Alarge-scale%20pretrained%20segmentation%20models.%20By%20leveraging%20representations%20from%0Amodels%20such%20as%20TotalSegmentator%2C%20Segment%20Anything%20%28SAM%29%2C%20and%20other%20foundation%0Anetworks%2C%20IMPACT%20provides%20a%20task-agnostic%2C%20training-free%20solution%20that%0Ageneralizes%20across%20imaging%20modalities.%20These%20features%2C%20originally%20trained%20for%0Asegmentation%2C%20offer%20strong%20spatial%20correspondence%20and%20semantic%20alignment%0Acapabilities%2C%20making%20them%20naturally%20suited%20for%20registration.%20The%20method%0Aintegrates%20seamlessly%20into%20both%20algorithmic%20%28Elastix%29%20and%20learning-based%0A%28VoxelMorph%29%20frameworks%2C%20leveraging%20the%20strengths%20of%20each.%20IMPACT%20was%20evaluated%0Aon%20five%20challenging%203D%20registration%20tasks%20involving%20thoracic%20CT/CBCT%20and%20pelvic%0AMR/CT%20datasets.%20Quantitative%20metrics%2C%20including%20Target%20Registration%20Error%20and%0ADice%20Similarity%20Coefficient%2C%20demonstrated%20consistent%20improvements%20in%20anatomical%0Aalignment%20over%20baseline%20methods.%20Qualitative%20analyses%20further%20highlighted%20the%0Arobustness%20of%20the%20proposed%20metric%20in%20the%20presence%20of%20noise%2C%20artifacts%2C%20and%0Amodality%20variations.%20With%20its%20versatility%2C%20efficiency%2C%20and%20strong%20performance%0Aacross%20diverse%20tasks%2C%20IMPACT%20offers%20a%20powerful%20solution%20for%20advancing%0Amultimodal%20image%20registration%20in%20both%20clinical%20and%20research%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.24121v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMPACT%253A%2520A%2520Generic%2520Semantic%2520Loss%2520for%2520Multimodal%2520Medical%2520Image%250A%2520%2520Registration%26entry.906535625%3DValentin%2520Boussot%2520and%2520C%25C3%25A9dric%2520H%25C3%25A9mon%2520and%2520Jean-Claude%2520Nunes%2520and%2520Jason%2520Dowling%2520and%2520Simon%2520Rouz%25C3%25A9%2520and%2520Caroline%2520Lafond%2520and%2520Ana%25C3%25AFs%2520Barateau%2520and%2520Jean-Louis%2520Dillenseger%26entry.1292438233%3D%2520%2520Image%2520registration%2520is%2520fundamental%2520in%2520medical%2520imaging%252C%2520enabling%2520precise%250Aalignment%2520of%2520anatomical%2520structures%2520for%2520diagnosis%252C%2520treatment%2520planning%252C%250Aimage-guided%2520interventions%252C%2520and%2520longitudinal%2520monitoring.%2520This%2520work%2520introduces%250AIMPACT%2520%2528Image%2520Metric%2520with%2520Pretrained%2520model-Agnostic%2520Comparison%2520for%250ATransmodality%2520registration%2529%252C%2520a%2520novel%2520similarity%2520metric%2520designed%2520for%2520robust%250Amultimodal%2520image%2520registration.%2520Rather%2520than%2520relying%2520on%2520raw%2520intensities%252C%250Ahandcrafted%2520descriptors%252C%2520or%2520task-specific%2520training%252C%2520IMPACT%2520defines%2520a%2520semantic%250Asimilarity%2520measure%2520based%2520on%2520the%2520comparison%2520of%2520deep%2520features%2520extracted%2520from%250Alarge-scale%2520pretrained%2520segmentation%2520models.%2520By%2520leveraging%2520representations%2520from%250Amodels%2520such%2520as%2520TotalSegmentator%252C%2520Segment%2520Anything%2520%2528SAM%2529%252C%2520and%2520other%2520foundation%250Anetworks%252C%2520IMPACT%2520provides%2520a%2520task-agnostic%252C%2520training-free%2520solution%2520that%250Ageneralizes%2520across%2520imaging%2520modalities.%2520These%2520features%252C%2520originally%2520trained%2520for%250Asegmentation%252C%2520offer%2520strong%2520spatial%2520correspondence%2520and%2520semantic%2520alignment%250Acapabilities%252C%2520making%2520them%2520naturally%2520suited%2520for%2520registration.%2520The%2520method%250Aintegrates%2520seamlessly%2520into%2520both%2520algorithmic%2520%2528Elastix%2529%2520and%2520learning-based%250A%2528VoxelMorph%2529%2520frameworks%252C%2520leveraging%2520the%2520strengths%2520of%2520each.%2520IMPACT%2520was%2520evaluated%250Aon%2520five%2520challenging%25203D%2520registration%2520tasks%2520involving%2520thoracic%2520CT/CBCT%2520and%2520pelvic%250AMR/CT%2520datasets.%2520Quantitative%2520metrics%252C%2520including%2520Target%2520Registration%2520Error%2520and%250ADice%2520Similarity%2520Coefficient%252C%2520demonstrated%2520consistent%2520improvements%2520in%2520anatomical%250Aalignment%2520over%2520baseline%2520methods.%2520Qualitative%2520analyses%2520further%2520highlighted%2520the%250Arobustness%2520of%2520the%2520proposed%2520metric%2520in%2520the%2520presence%2520of%2520noise%252C%2520artifacts%252C%2520and%250Amodality%2520variations.%2520With%2520its%2520versatility%252C%2520efficiency%252C%2520and%2520strong%2520performance%250Aacross%2520diverse%2520tasks%252C%2520IMPACT%2520offers%2520a%2520powerful%2520solution%2520for%2520advancing%250Amultimodal%2520image%2520registration%2520in%2520both%2520clinical%2520and%2520research%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.24121v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMPACT%3A%20A%20Generic%20Semantic%20Loss%20for%20Multimodal%20Medical%20Image%0A%20%20Registration&entry.906535625=Valentin%20Boussot%20and%20C%C3%A9dric%20H%C3%A9mon%20and%20Jean-Claude%20Nunes%20and%20Jason%20Dowling%20and%20Simon%20Rouz%C3%A9%20and%20Caroline%20Lafond%20and%20Ana%C3%AFs%20Barateau%20and%20Jean-Louis%20Dillenseger&entry.1292438233=%20%20Image%20registration%20is%20fundamental%20in%20medical%20imaging%2C%20enabling%20precise%0Aalignment%20of%20anatomical%20structures%20for%20diagnosis%2C%20treatment%20planning%2C%0Aimage-guided%20interventions%2C%20and%20longitudinal%20monitoring.%20This%20work%20introduces%0AIMPACT%20%28Image%20Metric%20with%20Pretrained%20model-Agnostic%20Comparison%20for%0ATransmodality%20registration%29%2C%20a%20novel%20similarity%20metric%20designed%20for%20robust%0Amultimodal%20image%20registration.%20Rather%20than%20relying%20on%20raw%20intensities%2C%0Ahandcrafted%20descriptors%2C%20or%20task-specific%20training%2C%20IMPACT%20defines%20a%20semantic%0Asimilarity%20measure%20based%20on%20the%20comparison%20of%20deep%20features%20extracted%20from%0Alarge-scale%20pretrained%20segmentation%20models.%20By%20leveraging%20representations%20from%0Amodels%20such%20as%20TotalSegmentator%2C%20Segment%20Anything%20%28SAM%29%2C%20and%20other%20foundation%0Anetworks%2C%20IMPACT%20provides%20a%20task-agnostic%2C%20training-free%20solution%20that%0Ageneralizes%20across%20imaging%20modalities.%20These%20features%2C%20originally%20trained%20for%0Asegmentation%2C%20offer%20strong%20spatial%20correspondence%20and%20semantic%20alignment%0Acapabilities%2C%20making%20them%20naturally%20suited%20for%20registration.%20The%20method%0Aintegrates%20seamlessly%20into%20both%20algorithmic%20%28Elastix%29%20and%20learning-based%0A%28VoxelMorph%29%20frameworks%2C%20leveraging%20the%20strengths%20of%20each.%20IMPACT%20was%20evaluated%0Aon%20five%20challenging%203D%20registration%20tasks%20involving%20thoracic%20CT/CBCT%20and%20pelvic%0AMR/CT%20datasets.%20Quantitative%20metrics%2C%20including%20Target%20Registration%20Error%20and%0ADice%20Similarity%20Coefficient%2C%20demonstrated%20consistent%20improvements%20in%20anatomical%0Aalignment%20over%20baseline%20methods.%20Qualitative%20analyses%20further%20highlighted%20the%0Arobustness%20of%20the%20proposed%20metric%20in%20the%20presence%20of%20noise%2C%20artifacts%2C%20and%0Amodality%20variations.%20With%20its%20versatility%2C%20efficiency%2C%20and%20strong%20performance%0Aacross%20diverse%20tasks%2C%20IMPACT%20offers%20a%20powerful%20solution%20for%20advancing%0Amultimodal%20image%20registration%20in%20both%20clinical%20and%20research%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.24121v3&entry.124074799=Read"},
{"title": "Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert\n  Reasoner", "author": "Wenchuan Zhang and Penghao Zhang and Jingru Guo and Tao Cheng and Jie Chen and Shuwan Zhang and Zhang Zhang and Yuhao Yi and Hong Bu", "abstract": "  Recent advances in vision language models (VLMs) have enabled broad progress\nin the general medical field. However, pathology still remains a more\nchallenging subdomain, with current pathology specific VLMs exhibiting\nlimitations in both diagnostic accuracy and reasoning plausibility. Such\nshortcomings are largely attributable to the nature of current pathology\ndatasets, which are primarily composed of image description pairs that lack the\ndepth and structured diagnostic paradigms employed by real world pathologists.\nIn this study, we leverage pathology textbooks and real world pathology experts\nto construct high-quality, reasoning-oriented datasets. Building on this, we\nintroduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a\nthree-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs\nfor knowledge infusion; (2) supervised fine-tuning on 500k high-quality\nChain-of-Thought samples for reasoning incentivizing; (3) reinforcement\nlearning using Group Relative Policy Optimization and Decoupled Clip and\nDynamic sAmpling Policy Optimization strategies for multimodal reasoning\nquality refinement. To further assess the alignment quality of our dataset, we\npropose PathoCLIP, trained on the same figure-caption corpus used for continued\npretraining. Comprehensive experimental results demonstrate that both PathoCLIP\nand Patho-R1 achieve robust performance across a wide range of\npathology-related tasks, including zero-shot classification, cross-modal\nretrieval, Visual Question Answering, and Multiple Choice Question. Our project\nis available at the Patho-R1 repository:\nhttps://github.com/Wenchuan-Zhang/Patho-R1.\n", "link": "http://arxiv.org/abs/2505.11404v1", "date": "2025-05-16", "relevancy": 2.1913, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patho-R1%3A%20A%20Multimodal%20Reinforcement%20Learning-Based%20Pathology%20Expert%0A%20%20Reasoner&body=Title%3A%20Patho-R1%3A%20A%20Multimodal%20Reinforcement%20Learning-Based%20Pathology%20Expert%0A%20%20Reasoner%0AAuthor%3A%20Wenchuan%20Zhang%20and%20Penghao%20Zhang%20and%20Jingru%20Guo%20and%20Tao%20Cheng%20and%20Jie%20Chen%20and%20Shuwan%20Zhang%20and%20Zhang%20Zhang%20and%20Yuhao%20Yi%20and%20Hong%20Bu%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision%20language%20models%20%28VLMs%29%20have%20enabled%20broad%20progress%0Ain%20the%20general%20medical%20field.%20However%2C%20pathology%20still%20remains%20a%20more%0Achallenging%20subdomain%2C%20with%20current%20pathology%20specific%20VLMs%20exhibiting%0Alimitations%20in%20both%20diagnostic%20accuracy%20and%20reasoning%20plausibility.%20Such%0Ashortcomings%20are%20largely%20attributable%20to%20the%20nature%20of%20current%20pathology%0Adatasets%2C%20which%20are%20primarily%20composed%20of%20image%20description%20pairs%20that%20lack%20the%0Adepth%20and%20structured%20diagnostic%20paradigms%20employed%20by%20real%20world%20pathologists.%0AIn%20this%20study%2C%20we%20leverage%20pathology%20textbooks%20and%20real%20world%20pathology%20experts%0Ato%20construct%20high-quality%2C%20reasoning-oriented%20datasets.%20Building%20on%20this%2C%20we%0Aintroduce%20Patho-R1%2C%20a%20multimodal%20RL-based%20pathology%20Reasoner%2C%20trained%20through%20a%0Athree-stage%20pipeline%3A%20%281%29%20continued%20pretraining%20on%203.5%20million%20image-text%20pairs%0Afor%20knowledge%20infusion%3B%20%282%29%20supervised%20fine-tuning%20on%20500k%20high-quality%0AChain-of-Thought%20samples%20for%20reasoning%20incentivizing%3B%20%283%29%20reinforcement%0Alearning%20using%20Group%20Relative%20Policy%20Optimization%20and%20Decoupled%20Clip%20and%0ADynamic%20sAmpling%20Policy%20Optimization%20strategies%20for%20multimodal%20reasoning%0Aquality%20refinement.%20To%20further%20assess%20the%20alignment%20quality%20of%20our%20dataset%2C%20we%0Apropose%20PathoCLIP%2C%20trained%20on%20the%20same%20figure-caption%20corpus%20used%20for%20continued%0Apretraining.%20Comprehensive%20experimental%20results%20demonstrate%20that%20both%20PathoCLIP%0Aand%20Patho-R1%20achieve%20robust%20performance%20across%20a%20wide%20range%20of%0Apathology-related%20tasks%2C%20including%20zero-shot%20classification%2C%20cross-modal%0Aretrieval%2C%20Visual%20Question%20Answering%2C%20and%20Multiple%20Choice%20Question.%20Our%20project%0Ais%20available%20at%20the%20Patho-R1%20repository%3A%0Ahttps%3A//github.com/Wenchuan-Zhang/Patho-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatho-R1%253A%2520A%2520Multimodal%2520Reinforcement%2520Learning-Based%2520Pathology%2520Expert%250A%2520%2520Reasoner%26entry.906535625%3DWenchuan%2520Zhang%2520and%2520Penghao%2520Zhang%2520and%2520Jingru%2520Guo%2520and%2520Tao%2520Cheng%2520and%2520Jie%2520Chen%2520and%2520Shuwan%2520Zhang%2520and%2520Zhang%2520Zhang%2520and%2520Yuhao%2520Yi%2520and%2520Hong%2520Bu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520enabled%2520broad%2520progress%250Ain%2520the%2520general%2520medical%2520field.%2520However%252C%2520pathology%2520still%2520remains%2520a%2520more%250Achallenging%2520subdomain%252C%2520with%2520current%2520pathology%2520specific%2520VLMs%2520exhibiting%250Alimitations%2520in%2520both%2520diagnostic%2520accuracy%2520and%2520reasoning%2520plausibility.%2520Such%250Ashortcomings%2520are%2520largely%2520attributable%2520to%2520the%2520nature%2520of%2520current%2520pathology%250Adatasets%252C%2520which%2520are%2520primarily%2520composed%2520of%2520image%2520description%2520pairs%2520that%2520lack%2520the%250Adepth%2520and%2520structured%2520diagnostic%2520paradigms%2520employed%2520by%2520real%2520world%2520pathologists.%250AIn%2520this%2520study%252C%2520we%2520leverage%2520pathology%2520textbooks%2520and%2520real%2520world%2520pathology%2520experts%250Ato%2520construct%2520high-quality%252C%2520reasoning-oriented%2520datasets.%2520Building%2520on%2520this%252C%2520we%250Aintroduce%2520Patho-R1%252C%2520a%2520multimodal%2520RL-based%2520pathology%2520Reasoner%252C%2520trained%2520through%2520a%250Athree-stage%2520pipeline%253A%2520%25281%2529%2520continued%2520pretraining%2520on%25203.5%2520million%2520image-text%2520pairs%250Afor%2520knowledge%2520infusion%253B%2520%25282%2529%2520supervised%2520fine-tuning%2520on%2520500k%2520high-quality%250AChain-of-Thought%2520samples%2520for%2520reasoning%2520incentivizing%253B%2520%25283%2529%2520reinforcement%250Alearning%2520using%2520Group%2520Relative%2520Policy%2520Optimization%2520and%2520Decoupled%2520Clip%2520and%250ADynamic%2520sAmpling%2520Policy%2520Optimization%2520strategies%2520for%2520multimodal%2520reasoning%250Aquality%2520refinement.%2520To%2520further%2520assess%2520the%2520alignment%2520quality%2520of%2520our%2520dataset%252C%2520we%250Apropose%2520PathoCLIP%252C%2520trained%2520on%2520the%2520same%2520figure-caption%2520corpus%2520used%2520for%2520continued%250Apretraining.%2520Comprehensive%2520experimental%2520results%2520demonstrate%2520that%2520both%2520PathoCLIP%250Aand%2520Patho-R1%2520achieve%2520robust%2520performance%2520across%2520a%2520wide%2520range%2520of%250Apathology-related%2520tasks%252C%2520including%2520zero-shot%2520classification%252C%2520cross-modal%250Aretrieval%252C%2520Visual%2520Question%2520Answering%252C%2520and%2520Multiple%2520Choice%2520Question.%2520Our%2520project%250Ais%2520available%2520at%2520the%2520Patho-R1%2520repository%253A%250Ahttps%253A//github.com/Wenchuan-Zhang/Patho-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patho-R1%3A%20A%20Multimodal%20Reinforcement%20Learning-Based%20Pathology%20Expert%0A%20%20Reasoner&entry.906535625=Wenchuan%20Zhang%20and%20Penghao%20Zhang%20and%20Jingru%20Guo%20and%20Tao%20Cheng%20and%20Jie%20Chen%20and%20Shuwan%20Zhang%20and%20Zhang%20Zhang%20and%20Yuhao%20Yi%20and%20Hong%20Bu&entry.1292438233=%20%20Recent%20advances%20in%20vision%20language%20models%20%28VLMs%29%20have%20enabled%20broad%20progress%0Ain%20the%20general%20medical%20field.%20However%2C%20pathology%20still%20remains%20a%20more%0Achallenging%20subdomain%2C%20with%20current%20pathology%20specific%20VLMs%20exhibiting%0Alimitations%20in%20both%20diagnostic%20accuracy%20and%20reasoning%20plausibility.%20Such%0Ashortcomings%20are%20largely%20attributable%20to%20the%20nature%20of%20current%20pathology%0Adatasets%2C%20which%20are%20primarily%20composed%20of%20image%20description%20pairs%20that%20lack%20the%0Adepth%20and%20structured%20diagnostic%20paradigms%20employed%20by%20real%20world%20pathologists.%0AIn%20this%20study%2C%20we%20leverage%20pathology%20textbooks%20and%20real%20world%20pathology%20experts%0Ato%20construct%20high-quality%2C%20reasoning-oriented%20datasets.%20Building%20on%20this%2C%20we%0Aintroduce%20Patho-R1%2C%20a%20multimodal%20RL-based%20pathology%20Reasoner%2C%20trained%20through%20a%0Athree-stage%20pipeline%3A%20%281%29%20continued%20pretraining%20on%203.5%20million%20image-text%20pairs%0Afor%20knowledge%20infusion%3B%20%282%29%20supervised%20fine-tuning%20on%20500k%20high-quality%0AChain-of-Thought%20samples%20for%20reasoning%20incentivizing%3B%20%283%29%20reinforcement%0Alearning%20using%20Group%20Relative%20Policy%20Optimization%20and%20Decoupled%20Clip%20and%0ADynamic%20sAmpling%20Policy%20Optimization%20strategies%20for%20multimodal%20reasoning%0Aquality%20refinement.%20To%20further%20assess%20the%20alignment%20quality%20of%20our%20dataset%2C%20we%0Apropose%20PathoCLIP%2C%20trained%20on%20the%20same%20figure-caption%20corpus%20used%20for%20continued%0Apretraining.%20Comprehensive%20experimental%20results%20demonstrate%20that%20both%20PathoCLIP%0Aand%20Patho-R1%20achieve%20robust%20performance%20across%20a%20wide%20range%20of%0Apathology-related%20tasks%2C%20including%20zero-shot%20classification%2C%20cross-modal%0Aretrieval%2C%20Visual%20Question%20Answering%2C%20and%20Multiple%20Choice%20Question.%20Our%20project%0Ais%20available%20at%20the%20Patho-R1%20repository%3A%0Ahttps%3A//github.com/Wenchuan-Zhang/Patho-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11404v1&entry.124074799=Read"},
{"title": "A Multi-modal Fusion Network for Terrain Perception Based on\n  Illumination Aware", "author": "Rui Wang and Shichun Yang and Yuyi Chen and Zhuoyang Li and Zexiang Tong and Jianyi Xu and Jiayi Lu and Xinjie Feng and Yaoguang Cao", "abstract": "  Road terrains play a crucial role in ensuring the driving safety of\nautonomous vehicles (AVs). However, existing sensors of AVs, including cameras\nand Lidars, are susceptible to variations in lighting and weather conditions,\nmaking it challenging to achieve real-time perception of road conditions. In\nthis paper, we propose an illumination-aware multi-modal fusion network (IMF),\nwhich leverages both exteroceptive and proprioceptive perception and optimizes\nthe fusion process based on illumination features. We introduce an\nillumination-perception sub-network to accurately estimate illumination\nfeatures. Moreover, we design a multi-modal fusion network which is able to\ndynamically adjust weights of different modalities according to illumination\nfeatures. We enhance the optimization process by pre-training of the\nillumination-perception sub-network and incorporating illumination loss as one\nof the training constraints. Extensive experiments demonstrate that the IMF\nshows a superior performance compared to state-of-the-art methods. The\ncomparison results with single modality perception methods highlight the\ncomprehensive advantages of multi-modal fusion in accurately perceiving road\nterrains under varying lighting conditions. Our dataset is available at:\nhttps://github.com/lindawang2016/IMF.\n", "link": "http://arxiv.org/abs/2505.11066v1", "date": "2025-05-16", "relevancy": 2.1842, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5794}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5502}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-modal%20Fusion%20Network%20for%20Terrain%20Perception%20Based%20on%0A%20%20Illumination%20Aware&body=Title%3A%20A%20Multi-modal%20Fusion%20Network%20for%20Terrain%20Perception%20Based%20on%0A%20%20Illumination%20Aware%0AAuthor%3A%20Rui%20Wang%20and%20Shichun%20Yang%20and%20Yuyi%20Chen%20and%20Zhuoyang%20Li%20and%20Zexiang%20Tong%20and%20Jianyi%20Xu%20and%20Jiayi%20Lu%20and%20Xinjie%20Feng%20and%20Yaoguang%20Cao%0AAbstract%3A%20%20%20Road%20terrains%20play%20a%20crucial%20role%20in%20ensuring%20the%20driving%20safety%20of%0Aautonomous%20vehicles%20%28AVs%29.%20However%2C%20existing%20sensors%20of%20AVs%2C%20including%20cameras%0Aand%20Lidars%2C%20are%20susceptible%20to%20variations%20in%20lighting%20and%20weather%20conditions%2C%0Amaking%20it%20challenging%20to%20achieve%20real-time%20perception%20of%20road%20conditions.%20In%0Athis%20paper%2C%20we%20propose%20an%20illumination-aware%20multi-modal%20fusion%20network%20%28IMF%29%2C%0Awhich%20leverages%20both%20exteroceptive%20and%20proprioceptive%20perception%20and%20optimizes%0Athe%20fusion%20process%20based%20on%20illumination%20features.%20We%20introduce%20an%0Aillumination-perception%20sub-network%20to%20accurately%20estimate%20illumination%0Afeatures.%20Moreover%2C%20we%20design%20a%20multi-modal%20fusion%20network%20which%20is%20able%20to%0Adynamically%20adjust%20weights%20of%20different%20modalities%20according%20to%20illumination%0Afeatures.%20We%20enhance%20the%20optimization%20process%20by%20pre-training%20of%20the%0Aillumination-perception%20sub-network%20and%20incorporating%20illumination%20loss%20as%20one%0Aof%20the%20training%20constraints.%20Extensive%20experiments%20demonstrate%20that%20the%20IMF%0Ashows%20a%20superior%20performance%20compared%20to%20state-of-the-art%20methods.%20The%0Acomparison%20results%20with%20single%20modality%20perception%20methods%20highlight%20the%0Acomprehensive%20advantages%20of%20multi-modal%20fusion%20in%20accurately%20perceiving%20road%0Aterrains%20under%20varying%20lighting%20conditions.%20Our%20dataset%20is%20available%20at%3A%0Ahttps%3A//github.com/lindawang2016/IMF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-modal%2520Fusion%2520Network%2520for%2520Terrain%2520Perception%2520Based%2520on%250A%2520%2520Illumination%2520Aware%26entry.906535625%3DRui%2520Wang%2520and%2520Shichun%2520Yang%2520and%2520Yuyi%2520Chen%2520and%2520Zhuoyang%2520Li%2520and%2520Zexiang%2520Tong%2520and%2520Jianyi%2520Xu%2520and%2520Jiayi%2520Lu%2520and%2520Xinjie%2520Feng%2520and%2520Yaoguang%2520Cao%26entry.1292438233%3D%2520%2520Road%2520terrains%2520play%2520a%2520crucial%2520role%2520in%2520ensuring%2520the%2520driving%2520safety%2520of%250Aautonomous%2520vehicles%2520%2528AVs%2529.%2520However%252C%2520existing%2520sensors%2520of%2520AVs%252C%2520including%2520cameras%250Aand%2520Lidars%252C%2520are%2520susceptible%2520to%2520variations%2520in%2520lighting%2520and%2520weather%2520conditions%252C%250Amaking%2520it%2520challenging%2520to%2520achieve%2520real-time%2520perception%2520of%2520road%2520conditions.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520an%2520illumination-aware%2520multi-modal%2520fusion%2520network%2520%2528IMF%2529%252C%250Awhich%2520leverages%2520both%2520exteroceptive%2520and%2520proprioceptive%2520perception%2520and%2520optimizes%250Athe%2520fusion%2520process%2520based%2520on%2520illumination%2520features.%2520We%2520introduce%2520an%250Aillumination-perception%2520sub-network%2520to%2520accurately%2520estimate%2520illumination%250Afeatures.%2520Moreover%252C%2520we%2520design%2520a%2520multi-modal%2520fusion%2520network%2520which%2520is%2520able%2520to%250Adynamically%2520adjust%2520weights%2520of%2520different%2520modalities%2520according%2520to%2520illumination%250Afeatures.%2520We%2520enhance%2520the%2520optimization%2520process%2520by%2520pre-training%2520of%2520the%250Aillumination-perception%2520sub-network%2520and%2520incorporating%2520illumination%2520loss%2520as%2520one%250Aof%2520the%2520training%2520constraints.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520IMF%250Ashows%2520a%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520methods.%2520The%250Acomparison%2520results%2520with%2520single%2520modality%2520perception%2520methods%2520highlight%2520the%250Acomprehensive%2520advantages%2520of%2520multi-modal%2520fusion%2520in%2520accurately%2520perceiving%2520road%250Aterrains%2520under%2520varying%2520lighting%2520conditions.%2520Our%2520dataset%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/lindawang2016/IMF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-modal%20Fusion%20Network%20for%20Terrain%20Perception%20Based%20on%0A%20%20Illumination%20Aware&entry.906535625=Rui%20Wang%20and%20Shichun%20Yang%20and%20Yuyi%20Chen%20and%20Zhuoyang%20Li%20and%20Zexiang%20Tong%20and%20Jianyi%20Xu%20and%20Jiayi%20Lu%20and%20Xinjie%20Feng%20and%20Yaoguang%20Cao&entry.1292438233=%20%20Road%20terrains%20play%20a%20crucial%20role%20in%20ensuring%20the%20driving%20safety%20of%0Aautonomous%20vehicles%20%28AVs%29.%20However%2C%20existing%20sensors%20of%20AVs%2C%20including%20cameras%0Aand%20Lidars%2C%20are%20susceptible%20to%20variations%20in%20lighting%20and%20weather%20conditions%2C%0Amaking%20it%20challenging%20to%20achieve%20real-time%20perception%20of%20road%20conditions.%20In%0Athis%20paper%2C%20we%20propose%20an%20illumination-aware%20multi-modal%20fusion%20network%20%28IMF%29%2C%0Awhich%20leverages%20both%20exteroceptive%20and%20proprioceptive%20perception%20and%20optimizes%0Athe%20fusion%20process%20based%20on%20illumination%20features.%20We%20introduce%20an%0Aillumination-perception%20sub-network%20to%20accurately%20estimate%20illumination%0Afeatures.%20Moreover%2C%20we%20design%20a%20multi-modal%20fusion%20network%20which%20is%20able%20to%0Adynamically%20adjust%20weights%20of%20different%20modalities%20according%20to%20illumination%0Afeatures.%20We%20enhance%20the%20optimization%20process%20by%20pre-training%20of%20the%0Aillumination-perception%20sub-network%20and%20incorporating%20illumination%20loss%20as%20one%0Aof%20the%20training%20constraints.%20Extensive%20experiments%20demonstrate%20that%20the%20IMF%0Ashows%20a%20superior%20performance%20compared%20to%20state-of-the-art%20methods.%20The%0Acomparison%20results%20with%20single%20modality%20perception%20methods%20highlight%20the%0Acomprehensive%20advantages%20of%20multi-modal%20fusion%20in%20accurately%20perceiving%20road%0Aterrains%20under%20varying%20lighting%20conditions.%20Our%20dataset%20is%20available%20at%3A%0Ahttps%3A//github.com/lindawang2016/IMF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11066v1&entry.124074799=Read"},
{"title": "Self-supervised perception for tactile skin covered dexterous hands", "author": "Akash Sharma and Carolina Higuera and Chaithanya Krishna Bodduluri and Zixi Liu and Taosha Fan and Tess Hellebrekers and Mike Lambeta and Byron Boots and Michael Kaess and Tingfan Wu and Francois Robert Hogan and Mustafa Mukadam", "abstract": "  We present Sparsh-skin, a pre-trained encoder for magnetic skin sensors\ndistributed across the fingertips, phalanges, and palm of a dexterous robot\nhand. Magnetic tactile skins offer a flexible form factor for hand-wide\ncoverage with fast response times, in contrast to vision-based tactile sensors\nthat are restricted to the fingertips and limited by bandwidth. Full hand\ntactile perception is crucial for robot dexterity. However, a lack of\ngeneral-purpose models, challenges with interpreting magnetic flux and\ncalibration have limited the adoption of these sensors. Sparsh-skin, given a\nhistory of kinematic and tactile sensing across a hand, outputs a latent\ntactile embedding that can be used in any downstream task. The encoder is\nself-supervised via self-distillation on a variety of unlabeled hand-object\ninteractions using an Allegro hand sensorized with Xela uSkin. In experiments\nacross several benchmark tasks, from state estimation to policy learning, we\nfind that pretrained Sparsh-skin representations are both sample efficient in\nlearning downstream tasks and improve task performance by over 41% compared to\nprior work and over 56% compared to end-to-end learning.\n", "link": "http://arxiv.org/abs/2505.11420v1", "date": "2025-05-16", "relevancy": 2.1837, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5594}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5493}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20perception%20for%20tactile%20skin%20covered%20dexterous%20hands&body=Title%3A%20Self-supervised%20perception%20for%20tactile%20skin%20covered%20dexterous%20hands%0AAuthor%3A%20Akash%20Sharma%20and%20Carolina%20Higuera%20and%20Chaithanya%20Krishna%20Bodduluri%20and%20Zixi%20Liu%20and%20Taosha%20Fan%20and%20Tess%20Hellebrekers%20and%20Mike%20Lambeta%20and%20Byron%20Boots%20and%20Michael%20Kaess%20and%20Tingfan%20Wu%20and%20Francois%20Robert%20Hogan%20and%20Mustafa%20Mukadam%0AAbstract%3A%20%20%20We%20present%20Sparsh-skin%2C%20a%20pre-trained%20encoder%20for%20magnetic%20skin%20sensors%0Adistributed%20across%20the%20fingertips%2C%20phalanges%2C%20and%20palm%20of%20a%20dexterous%20robot%0Ahand.%20Magnetic%20tactile%20skins%20offer%20a%20flexible%20form%20factor%20for%20hand-wide%0Acoverage%20with%20fast%20response%20times%2C%20in%20contrast%20to%20vision-based%20tactile%20sensors%0Athat%20are%20restricted%20to%20the%20fingertips%20and%20limited%20by%20bandwidth.%20Full%20hand%0Atactile%20perception%20is%20crucial%20for%20robot%20dexterity.%20However%2C%20a%20lack%20of%0Ageneral-purpose%20models%2C%20challenges%20with%20interpreting%20magnetic%20flux%20and%0Acalibration%20have%20limited%20the%20adoption%20of%20these%20sensors.%20Sparsh-skin%2C%20given%20a%0Ahistory%20of%20kinematic%20and%20tactile%20sensing%20across%20a%20hand%2C%20outputs%20a%20latent%0Atactile%20embedding%20that%20can%20be%20used%20in%20any%20downstream%20task.%20The%20encoder%20is%0Aself-supervised%20via%20self-distillation%20on%20a%20variety%20of%20unlabeled%20hand-object%0Ainteractions%20using%20an%20Allegro%20hand%20sensorized%20with%20Xela%20uSkin.%20In%20experiments%0Aacross%20several%20benchmark%20tasks%2C%20from%20state%20estimation%20to%20policy%20learning%2C%20we%0Afind%20that%20pretrained%20Sparsh-skin%20representations%20are%20both%20sample%20efficient%20in%0Alearning%20downstream%20tasks%20and%20improve%20task%20performance%20by%20over%2041%25%20compared%20to%0Aprior%20work%20and%20over%2056%25%20compared%20to%20end-to-end%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520perception%2520for%2520tactile%2520skin%2520covered%2520dexterous%2520hands%26entry.906535625%3DAkash%2520Sharma%2520and%2520Carolina%2520Higuera%2520and%2520Chaithanya%2520Krishna%2520Bodduluri%2520and%2520Zixi%2520Liu%2520and%2520Taosha%2520Fan%2520and%2520Tess%2520Hellebrekers%2520and%2520Mike%2520Lambeta%2520and%2520Byron%2520Boots%2520and%2520Michael%2520Kaess%2520and%2520Tingfan%2520Wu%2520and%2520Francois%2520Robert%2520Hogan%2520and%2520Mustafa%2520Mukadam%26entry.1292438233%3D%2520%2520We%2520present%2520Sparsh-skin%252C%2520a%2520pre-trained%2520encoder%2520for%2520magnetic%2520skin%2520sensors%250Adistributed%2520across%2520the%2520fingertips%252C%2520phalanges%252C%2520and%2520palm%2520of%2520a%2520dexterous%2520robot%250Ahand.%2520Magnetic%2520tactile%2520skins%2520offer%2520a%2520flexible%2520form%2520factor%2520for%2520hand-wide%250Acoverage%2520with%2520fast%2520response%2520times%252C%2520in%2520contrast%2520to%2520vision-based%2520tactile%2520sensors%250Athat%2520are%2520restricted%2520to%2520the%2520fingertips%2520and%2520limited%2520by%2520bandwidth.%2520Full%2520hand%250Atactile%2520perception%2520is%2520crucial%2520for%2520robot%2520dexterity.%2520However%252C%2520a%2520lack%2520of%250Ageneral-purpose%2520models%252C%2520challenges%2520with%2520interpreting%2520magnetic%2520flux%2520and%250Acalibration%2520have%2520limited%2520the%2520adoption%2520of%2520these%2520sensors.%2520Sparsh-skin%252C%2520given%2520a%250Ahistory%2520of%2520kinematic%2520and%2520tactile%2520sensing%2520across%2520a%2520hand%252C%2520outputs%2520a%2520latent%250Atactile%2520embedding%2520that%2520can%2520be%2520used%2520in%2520any%2520downstream%2520task.%2520The%2520encoder%2520is%250Aself-supervised%2520via%2520self-distillation%2520on%2520a%2520variety%2520of%2520unlabeled%2520hand-object%250Ainteractions%2520using%2520an%2520Allegro%2520hand%2520sensorized%2520with%2520Xela%2520uSkin.%2520In%2520experiments%250Aacross%2520several%2520benchmark%2520tasks%252C%2520from%2520state%2520estimation%2520to%2520policy%2520learning%252C%2520we%250Afind%2520that%2520pretrained%2520Sparsh-skin%2520representations%2520are%2520both%2520sample%2520efficient%2520in%250Alearning%2520downstream%2520tasks%2520and%2520improve%2520task%2520performance%2520by%2520over%252041%2525%2520compared%2520to%250Aprior%2520work%2520and%2520over%252056%2525%2520compared%2520to%2520end-to-end%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20perception%20for%20tactile%20skin%20covered%20dexterous%20hands&entry.906535625=Akash%20Sharma%20and%20Carolina%20Higuera%20and%20Chaithanya%20Krishna%20Bodduluri%20and%20Zixi%20Liu%20and%20Taosha%20Fan%20and%20Tess%20Hellebrekers%20and%20Mike%20Lambeta%20and%20Byron%20Boots%20and%20Michael%20Kaess%20and%20Tingfan%20Wu%20and%20Francois%20Robert%20Hogan%20and%20Mustafa%20Mukadam&entry.1292438233=%20%20We%20present%20Sparsh-skin%2C%20a%20pre-trained%20encoder%20for%20magnetic%20skin%20sensors%0Adistributed%20across%20the%20fingertips%2C%20phalanges%2C%20and%20palm%20of%20a%20dexterous%20robot%0Ahand.%20Magnetic%20tactile%20skins%20offer%20a%20flexible%20form%20factor%20for%20hand-wide%0Acoverage%20with%20fast%20response%20times%2C%20in%20contrast%20to%20vision-based%20tactile%20sensors%0Athat%20are%20restricted%20to%20the%20fingertips%20and%20limited%20by%20bandwidth.%20Full%20hand%0Atactile%20perception%20is%20crucial%20for%20robot%20dexterity.%20However%2C%20a%20lack%20of%0Ageneral-purpose%20models%2C%20challenges%20with%20interpreting%20magnetic%20flux%20and%0Acalibration%20have%20limited%20the%20adoption%20of%20these%20sensors.%20Sparsh-skin%2C%20given%20a%0Ahistory%20of%20kinematic%20and%20tactile%20sensing%20across%20a%20hand%2C%20outputs%20a%20latent%0Atactile%20embedding%20that%20can%20be%20used%20in%20any%20downstream%20task.%20The%20encoder%20is%0Aself-supervised%20via%20self-distillation%20on%20a%20variety%20of%20unlabeled%20hand-object%0Ainteractions%20using%20an%20Allegro%20hand%20sensorized%20with%20Xela%20uSkin.%20In%20experiments%0Aacross%20several%20benchmark%20tasks%2C%20from%20state%20estimation%20to%20policy%20learning%2C%20we%0Afind%20that%20pretrained%20Sparsh-skin%20representations%20are%20both%20sample%20efficient%20in%0Alearning%20downstream%20tasks%20and%20improve%20task%20performance%20by%20over%2041%25%20compared%20to%0Aprior%20work%20and%20over%2056%25%20compared%20to%20end-to-end%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11420v1&entry.124074799=Read"},
{"title": "Hybrid-Emba3D: Geometry-Aware and Cross-Path Feature Hybrid Enhanced\n  State Space Model for Point Cloud Classification", "author": "Bin Liu and Chunyang Wang and Xuelian Liu and Guan Xi and Ge Zhang and Ziteng Yao and Mengxue Dong", "abstract": "  The point cloud classification tasks face the dual challenge of efficiently\nextracting local geometric features while maintaining model complexity. The\nMamba architecture utilizes the linear complexity advantage of state space\nmodels (SSMs) to overcome the computational bottleneck of Transformers while\nbalancing global modeling capabilities. However, the inherent contradiction\nbetween its unidirectional dependency and the unordered nature of point clouds\nimpedes modeling spatial correlation in local neighborhoods, thus constraining\ngeometric feature extraction. This paper proposes Hybrid-Emba3D, a\nbidirectional Mamba model enhanced by geometry-feature coupling and cross-path\nfeature hybridization. The Local geometric pooling with geometry-feature\ncoupling mechanism significantly enhances local feature discriminative power\nvia coordinated propagation and dynamic aggregation of geometric information\nbetween local center points and their neighborhoods, without introducing\nadditional parameters. The designed Collaborative feature enhancer adopts\ndual-path hybridization, effectively handling local mutations and sparse key\nsignals, breaking through the limitations of traditional SSM long-range\nmodeling. Experimental results demonstrate that the proposed model achieves a\nnew SOTA classification accuracy of 95.99% on ModelNet40 with only 0.03M\nadditional.\n", "link": "http://arxiv.org/abs/2505.11099v1", "date": "2025-05-16", "relevancy": 2.1827, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.553}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5432}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid-Emba3D%3A%20Geometry-Aware%20and%20Cross-Path%20Feature%20Hybrid%20Enhanced%0A%20%20State%20Space%20Model%20for%20Point%20Cloud%20Classification&body=Title%3A%20Hybrid-Emba3D%3A%20Geometry-Aware%20and%20Cross-Path%20Feature%20Hybrid%20Enhanced%0A%20%20State%20Space%20Model%20for%20Point%20Cloud%20Classification%0AAuthor%3A%20Bin%20Liu%20and%20Chunyang%20Wang%20and%20Xuelian%20Liu%20and%20Guan%20Xi%20and%20Ge%20Zhang%20and%20Ziteng%20Yao%20and%20Mengxue%20Dong%0AAbstract%3A%20%20%20The%20point%20cloud%20classification%20tasks%20face%20the%20dual%20challenge%20of%20efficiently%0Aextracting%20local%20geometric%20features%20while%20maintaining%20model%20complexity.%20The%0AMamba%20architecture%20utilizes%20the%20linear%20complexity%20advantage%20of%20state%20space%0Amodels%20%28SSMs%29%20to%20overcome%20the%20computational%20bottleneck%20of%20Transformers%20while%0Abalancing%20global%20modeling%20capabilities.%20However%2C%20the%20inherent%20contradiction%0Abetween%20its%20unidirectional%20dependency%20and%20the%20unordered%20nature%20of%20point%20clouds%0Aimpedes%20modeling%20spatial%20correlation%20in%20local%20neighborhoods%2C%20thus%20constraining%0Ageometric%20feature%20extraction.%20This%20paper%20proposes%20Hybrid-Emba3D%2C%20a%0Abidirectional%20Mamba%20model%20enhanced%20by%20geometry-feature%20coupling%20and%20cross-path%0Afeature%20hybridization.%20The%20Local%20geometric%20pooling%20with%20geometry-feature%0Acoupling%20mechanism%20significantly%20enhances%20local%20feature%20discriminative%20power%0Avia%20coordinated%20propagation%20and%20dynamic%20aggregation%20of%20geometric%20information%0Abetween%20local%20center%20points%20and%20their%20neighborhoods%2C%20without%20introducing%0Aadditional%20parameters.%20The%20designed%20Collaborative%20feature%20enhancer%20adopts%0Adual-path%20hybridization%2C%20effectively%20handling%20local%20mutations%20and%20sparse%20key%0Asignals%2C%20breaking%20through%20the%20limitations%20of%20traditional%20SSM%20long-range%0Amodeling.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20model%20achieves%20a%0Anew%20SOTA%20classification%20accuracy%20of%2095.99%25%20on%20ModelNet40%20with%20only%200.03M%0Aadditional.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid-Emba3D%253A%2520Geometry-Aware%2520and%2520Cross-Path%2520Feature%2520Hybrid%2520Enhanced%250A%2520%2520State%2520Space%2520Model%2520for%2520Point%2520Cloud%2520Classification%26entry.906535625%3DBin%2520Liu%2520and%2520Chunyang%2520Wang%2520and%2520Xuelian%2520Liu%2520and%2520Guan%2520Xi%2520and%2520Ge%2520Zhang%2520and%2520Ziteng%2520Yao%2520and%2520Mengxue%2520Dong%26entry.1292438233%3D%2520%2520The%2520point%2520cloud%2520classification%2520tasks%2520face%2520the%2520dual%2520challenge%2520of%2520efficiently%250Aextracting%2520local%2520geometric%2520features%2520while%2520maintaining%2520model%2520complexity.%2520The%250AMamba%2520architecture%2520utilizes%2520the%2520linear%2520complexity%2520advantage%2520of%2520state%2520space%250Amodels%2520%2528SSMs%2529%2520to%2520overcome%2520the%2520computational%2520bottleneck%2520of%2520Transformers%2520while%250Abalancing%2520global%2520modeling%2520capabilities.%2520However%252C%2520the%2520inherent%2520contradiction%250Abetween%2520its%2520unidirectional%2520dependency%2520and%2520the%2520unordered%2520nature%2520of%2520point%2520clouds%250Aimpedes%2520modeling%2520spatial%2520correlation%2520in%2520local%2520neighborhoods%252C%2520thus%2520constraining%250Ageometric%2520feature%2520extraction.%2520This%2520paper%2520proposes%2520Hybrid-Emba3D%252C%2520a%250Abidirectional%2520Mamba%2520model%2520enhanced%2520by%2520geometry-feature%2520coupling%2520and%2520cross-path%250Afeature%2520hybridization.%2520The%2520Local%2520geometric%2520pooling%2520with%2520geometry-feature%250Acoupling%2520mechanism%2520significantly%2520enhances%2520local%2520feature%2520discriminative%2520power%250Avia%2520coordinated%2520propagation%2520and%2520dynamic%2520aggregation%2520of%2520geometric%2520information%250Abetween%2520local%2520center%2520points%2520and%2520their%2520neighborhoods%252C%2520without%2520introducing%250Aadditional%2520parameters.%2520The%2520designed%2520Collaborative%2520feature%2520enhancer%2520adopts%250Adual-path%2520hybridization%252C%2520effectively%2520handling%2520local%2520mutations%2520and%2520sparse%2520key%250Asignals%252C%2520breaking%2520through%2520the%2520limitations%2520of%2520traditional%2520SSM%2520long-range%250Amodeling.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520model%2520achieves%2520a%250Anew%2520SOTA%2520classification%2520accuracy%2520of%252095.99%2525%2520on%2520ModelNet40%2520with%2520only%25200.03M%250Aadditional.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid-Emba3D%3A%20Geometry-Aware%20and%20Cross-Path%20Feature%20Hybrid%20Enhanced%0A%20%20State%20Space%20Model%20for%20Point%20Cloud%20Classification&entry.906535625=Bin%20Liu%20and%20Chunyang%20Wang%20and%20Xuelian%20Liu%20and%20Guan%20Xi%20and%20Ge%20Zhang%20and%20Ziteng%20Yao%20and%20Mengxue%20Dong&entry.1292438233=%20%20The%20point%20cloud%20classification%20tasks%20face%20the%20dual%20challenge%20of%20efficiently%0Aextracting%20local%20geometric%20features%20while%20maintaining%20model%20complexity.%20The%0AMamba%20architecture%20utilizes%20the%20linear%20complexity%20advantage%20of%20state%20space%0Amodels%20%28SSMs%29%20to%20overcome%20the%20computational%20bottleneck%20of%20Transformers%20while%0Abalancing%20global%20modeling%20capabilities.%20However%2C%20the%20inherent%20contradiction%0Abetween%20its%20unidirectional%20dependency%20and%20the%20unordered%20nature%20of%20point%20clouds%0Aimpedes%20modeling%20spatial%20correlation%20in%20local%20neighborhoods%2C%20thus%20constraining%0Ageometric%20feature%20extraction.%20This%20paper%20proposes%20Hybrid-Emba3D%2C%20a%0Abidirectional%20Mamba%20model%20enhanced%20by%20geometry-feature%20coupling%20and%20cross-path%0Afeature%20hybridization.%20The%20Local%20geometric%20pooling%20with%20geometry-feature%0Acoupling%20mechanism%20significantly%20enhances%20local%20feature%20discriminative%20power%0Avia%20coordinated%20propagation%20and%20dynamic%20aggregation%20of%20geometric%20information%0Abetween%20local%20center%20points%20and%20their%20neighborhoods%2C%20without%20introducing%0Aadditional%20parameters.%20The%20designed%20Collaborative%20feature%20enhancer%20adopts%0Adual-path%20hybridization%2C%20effectively%20handling%20local%20mutations%20and%20sparse%20key%0Asignals%2C%20breaking%20through%20the%20limitations%20of%20traditional%20SSM%20long-range%0Amodeling.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20model%20achieves%20a%0Anew%20SOTA%20classification%20accuracy%20of%2095.99%25%20on%20ModelNet40%20with%20only%200.03M%0Aadditional.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11099v1&entry.124074799=Read"},
{"title": "MutualNeRF: Improve the Performance of NeRF under Limited Samples with\n  Mutual Information Theory", "author": "Zifan Wang and Jingwei Li and Yitang Li and Yunze Liu", "abstract": "  This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field\n(NeRF) performance under limited samples using Mutual Information Theory. While\nNeRF excels in 3D scene synthesis, challenges arise with limited data and\nexisting methods that aim to introduce prior knowledge lack theoretical support\nin a unified framework. We introduce a simple but theoretically robust concept,\nMutual Information, as a metric to uniformly measure the correlation between\nimages, considering both macro (semantic) and micro (pixel) levels.\n  For sparse view sampling, we strategically select additional viewpoints\ncontaining more non-overlapping scene information by minimizing mutual\ninformation without knowing ground truth images beforehand. Our framework\nemploys a greedy algorithm, offering a near-optimal solution.\n  For few-shot view synthesis, we maximize the mutual information between\ninferred images and ground truth, expecting inferred images to gain more\nrelevant information from known images. This is achieved by incorporating\nefficient, plug-and-play regularization terms.\n  Experiments under limited samples show consistent improvement over\nstate-of-the-art baselines in different settings, affirming the efficacy of our\nframework.\n", "link": "http://arxiv.org/abs/2505.11386v1", "date": "2025-05-16", "relevancy": 2.1751, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5555}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5427}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MutualNeRF%3A%20Improve%20the%20Performance%20of%20NeRF%20under%20Limited%20Samples%20with%0A%20%20Mutual%20Information%20Theory&body=Title%3A%20MutualNeRF%3A%20Improve%20the%20Performance%20of%20NeRF%20under%20Limited%20Samples%20with%0A%20%20Mutual%20Information%20Theory%0AAuthor%3A%20Zifan%20Wang%20and%20Jingwei%20Li%20and%20Yitang%20Li%20and%20Yunze%20Liu%0AAbstract%3A%20%20%20This%20paper%20introduces%20MutualNeRF%2C%20a%20framework%20enhancing%20Neural%20Radiance%20Field%0A%28NeRF%29%20performance%20under%20limited%20samples%20using%20Mutual%20Information%20Theory.%20While%0ANeRF%20excels%20in%203D%20scene%20synthesis%2C%20challenges%20arise%20with%20limited%20data%20and%0Aexisting%20methods%20that%20aim%20to%20introduce%20prior%20knowledge%20lack%20theoretical%20support%0Ain%20a%20unified%20framework.%20We%20introduce%20a%20simple%20but%20theoretically%20robust%20concept%2C%0AMutual%20Information%2C%20as%20a%20metric%20to%20uniformly%20measure%20the%20correlation%20between%0Aimages%2C%20considering%20both%20macro%20%28semantic%29%20and%20micro%20%28pixel%29%20levels.%0A%20%20For%20sparse%20view%20sampling%2C%20we%20strategically%20select%20additional%20viewpoints%0Acontaining%20more%20non-overlapping%20scene%20information%20by%20minimizing%20mutual%0Ainformation%20without%20knowing%20ground%20truth%20images%20beforehand.%20Our%20framework%0Aemploys%20a%20greedy%20algorithm%2C%20offering%20a%20near-optimal%20solution.%0A%20%20For%20few-shot%20view%20synthesis%2C%20we%20maximize%20the%20mutual%20information%20between%0Ainferred%20images%20and%20ground%20truth%2C%20expecting%20inferred%20images%20to%20gain%20more%0Arelevant%20information%20from%20known%20images.%20This%20is%20achieved%20by%20incorporating%0Aefficient%2C%20plug-and-play%20regularization%20terms.%0A%20%20Experiments%20under%20limited%20samples%20show%20consistent%20improvement%20over%0Astate-of-the-art%20baselines%20in%20different%20settings%2C%20affirming%20the%20efficacy%20of%20our%0Aframework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMutualNeRF%253A%2520Improve%2520the%2520Performance%2520of%2520NeRF%2520under%2520Limited%2520Samples%2520with%250A%2520%2520Mutual%2520Information%2520Theory%26entry.906535625%3DZifan%2520Wang%2520and%2520Jingwei%2520Li%2520and%2520Yitang%2520Li%2520and%2520Yunze%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520MutualNeRF%252C%2520a%2520framework%2520enhancing%2520Neural%2520Radiance%2520Field%250A%2528NeRF%2529%2520performance%2520under%2520limited%2520samples%2520using%2520Mutual%2520Information%2520Theory.%2520While%250ANeRF%2520excels%2520in%25203D%2520scene%2520synthesis%252C%2520challenges%2520arise%2520with%2520limited%2520data%2520and%250Aexisting%2520methods%2520that%2520aim%2520to%2520introduce%2520prior%2520knowledge%2520lack%2520theoretical%2520support%250Ain%2520a%2520unified%2520framework.%2520We%2520introduce%2520a%2520simple%2520but%2520theoretically%2520robust%2520concept%252C%250AMutual%2520Information%252C%2520as%2520a%2520metric%2520to%2520uniformly%2520measure%2520the%2520correlation%2520between%250Aimages%252C%2520considering%2520both%2520macro%2520%2528semantic%2529%2520and%2520micro%2520%2528pixel%2529%2520levels.%250A%2520%2520For%2520sparse%2520view%2520sampling%252C%2520we%2520strategically%2520select%2520additional%2520viewpoints%250Acontaining%2520more%2520non-overlapping%2520scene%2520information%2520by%2520minimizing%2520mutual%250Ainformation%2520without%2520knowing%2520ground%2520truth%2520images%2520beforehand.%2520Our%2520framework%250Aemploys%2520a%2520greedy%2520algorithm%252C%2520offering%2520a%2520near-optimal%2520solution.%250A%2520%2520For%2520few-shot%2520view%2520synthesis%252C%2520we%2520maximize%2520the%2520mutual%2520information%2520between%250Ainferred%2520images%2520and%2520ground%2520truth%252C%2520expecting%2520inferred%2520images%2520to%2520gain%2520more%250Arelevant%2520information%2520from%2520known%2520images.%2520This%2520is%2520achieved%2520by%2520incorporating%250Aefficient%252C%2520plug-and-play%2520regularization%2520terms.%250A%2520%2520Experiments%2520under%2520limited%2520samples%2520show%2520consistent%2520improvement%2520over%250Astate-of-the-art%2520baselines%2520in%2520different%2520settings%252C%2520affirming%2520the%2520efficacy%2520of%2520our%250Aframework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MutualNeRF%3A%20Improve%20the%20Performance%20of%20NeRF%20under%20Limited%20Samples%20with%0A%20%20Mutual%20Information%20Theory&entry.906535625=Zifan%20Wang%20and%20Jingwei%20Li%20and%20Yitang%20Li%20and%20Yunze%20Liu&entry.1292438233=%20%20This%20paper%20introduces%20MutualNeRF%2C%20a%20framework%20enhancing%20Neural%20Radiance%20Field%0A%28NeRF%29%20performance%20under%20limited%20samples%20using%20Mutual%20Information%20Theory.%20While%0ANeRF%20excels%20in%203D%20scene%20synthesis%2C%20challenges%20arise%20with%20limited%20data%20and%0Aexisting%20methods%20that%20aim%20to%20introduce%20prior%20knowledge%20lack%20theoretical%20support%0Ain%20a%20unified%20framework.%20We%20introduce%20a%20simple%20but%20theoretically%20robust%20concept%2C%0AMutual%20Information%2C%20as%20a%20metric%20to%20uniformly%20measure%20the%20correlation%20between%0Aimages%2C%20considering%20both%20macro%20%28semantic%29%20and%20micro%20%28pixel%29%20levels.%0A%20%20For%20sparse%20view%20sampling%2C%20we%20strategically%20select%20additional%20viewpoints%0Acontaining%20more%20non-overlapping%20scene%20information%20by%20minimizing%20mutual%0Ainformation%20without%20knowing%20ground%20truth%20images%20beforehand.%20Our%20framework%0Aemploys%20a%20greedy%20algorithm%2C%20offering%20a%20near-optimal%20solution.%0A%20%20For%20few-shot%20view%20synthesis%2C%20we%20maximize%20the%20mutual%20information%20between%0Ainferred%20images%20and%20ground%20truth%2C%20expecting%20inferred%20images%20to%20gain%20more%0Arelevant%20information%20from%20known%20images.%20This%20is%20achieved%20by%20incorporating%0Aefficient%2C%20plug-and-play%20regularization%20terms.%0A%20%20Experiments%20under%20limited%20samples%20show%20consistent%20improvement%20over%0Astate-of-the-art%20baselines%20in%20different%20settings%2C%20affirming%20the%20efficacy%20of%20our%0Aframework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11386v1&entry.124074799=Read"},
{"title": "LLMs unlock new paths to monetizing exploits", "author": "Nicholas Carlini and Milad Nasr and Edoardo Debenedetti and Barry Wang and Christopher A. Choquette-Choo and Daphne Ippolito and Florian Tram\u00e8r and Matthew Jagielski", "abstract": "  We argue that Large language models (LLMs) will soon alter the economics of\ncyberattacks. Instead of attacking the most commonly used software and\nmonetizing exploits by targeting the lowest common denominator among victims,\nLLMs enable adversaries to launch tailored attacks on a user-by-user basis. On\nthe exploitation front, instead of human attackers manually searching for one\ndifficult-to-identify bug in a product with millions of users, LLMs can find\nthousands of easy-to-identify bugs in products with thousands of users. And on\nthe monetization front, instead of generic ransomware that always performs the\nsame attack (encrypt all your data and request payment to decrypt), an\nLLM-driven ransomware attack could tailor the ransom demand based on the\nparticular content of each exploited device.\n  We show that these two attacks (and several others) are imminently practical\nusing state-of-the-art LLMs. For example, we show that without any human\nintervention, an LLM finds highly sensitive personal information in the Enron\nemail dataset (e.g., an executive having an affair with another employee) that\ncould be used for blackmail. While some of our attacks are still too expensive\nto scale widely today, the incentives to implement these attacks will only\nincrease as LLMs get cheaper. Thus, we argue that LLMs create a need for new\ndefense-in-depth approaches.\n", "link": "http://arxiv.org/abs/2505.11449v1", "date": "2025-05-16", "relevancy": 1.5691, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20unlock%20new%20paths%20to%20monetizing%20exploits&body=Title%3A%20LLMs%20unlock%20new%20paths%20to%20monetizing%20exploits%0AAuthor%3A%20Nicholas%20Carlini%20and%20Milad%20Nasr%20and%20Edoardo%20Debenedetti%20and%20Barry%20Wang%20and%20Christopher%20A.%20Choquette-Choo%20and%20Daphne%20Ippolito%20and%20Florian%20Tram%C3%A8r%20and%20Matthew%20Jagielski%0AAbstract%3A%20%20%20We%20argue%20that%20Large%20language%20models%20%28LLMs%29%20will%20soon%20alter%20the%20economics%20of%0Acyberattacks.%20Instead%20of%20attacking%20the%20most%20commonly%20used%20software%20and%0Amonetizing%20exploits%20by%20targeting%20the%20lowest%20common%20denominator%20among%20victims%2C%0ALLMs%20enable%20adversaries%20to%20launch%20tailored%20attacks%20on%20a%20user-by-user%20basis.%20On%0Athe%20exploitation%20front%2C%20instead%20of%20human%20attackers%20manually%20searching%20for%20one%0Adifficult-to-identify%20bug%20in%20a%20product%20with%20millions%20of%20users%2C%20LLMs%20can%20find%0Athousands%20of%20easy-to-identify%20bugs%20in%20products%20with%20thousands%20of%20users.%20And%20on%0Athe%20monetization%20front%2C%20instead%20of%20generic%20ransomware%20that%20always%20performs%20the%0Asame%20attack%20%28encrypt%20all%20your%20data%20and%20request%20payment%20to%20decrypt%29%2C%20an%0ALLM-driven%20ransomware%20attack%20could%20tailor%20the%20ransom%20demand%20based%20on%20the%0Aparticular%20content%20of%20each%20exploited%20device.%0A%20%20We%20show%20that%20these%20two%20attacks%20%28and%20several%20others%29%20are%20imminently%20practical%0Ausing%20state-of-the-art%20LLMs.%20For%20example%2C%20we%20show%20that%20without%20any%20human%0Aintervention%2C%20an%20LLM%20finds%20highly%20sensitive%20personal%20information%20in%20the%20Enron%0Aemail%20dataset%20%28e.g.%2C%20an%20executive%20having%20an%20affair%20with%20another%20employee%29%20that%0Acould%20be%20used%20for%20blackmail.%20While%20some%20of%20our%20attacks%20are%20still%20too%20expensive%0Ato%20scale%20widely%20today%2C%20the%20incentives%20to%20implement%20these%20attacks%20will%20only%0Aincrease%20as%20LLMs%20get%20cheaper.%20Thus%2C%20we%20argue%20that%20LLMs%20create%20a%20need%20for%20new%0Adefense-in-depth%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520unlock%2520new%2520paths%2520to%2520monetizing%2520exploits%26entry.906535625%3DNicholas%2520Carlini%2520and%2520Milad%2520Nasr%2520and%2520Edoardo%2520Debenedetti%2520and%2520Barry%2520Wang%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520Daphne%2520Ippolito%2520and%2520Florian%2520Tram%25C3%25A8r%2520and%2520Matthew%2520Jagielski%26entry.1292438233%3D%2520%2520We%2520argue%2520that%2520Large%2520language%2520models%2520%2528LLMs%2529%2520will%2520soon%2520alter%2520the%2520economics%2520of%250Acyberattacks.%2520Instead%2520of%2520attacking%2520the%2520most%2520commonly%2520used%2520software%2520and%250Amonetizing%2520exploits%2520by%2520targeting%2520the%2520lowest%2520common%2520denominator%2520among%2520victims%252C%250ALLMs%2520enable%2520adversaries%2520to%2520launch%2520tailored%2520attacks%2520on%2520a%2520user-by-user%2520basis.%2520On%250Athe%2520exploitation%2520front%252C%2520instead%2520of%2520human%2520attackers%2520manually%2520searching%2520for%2520one%250Adifficult-to-identify%2520bug%2520in%2520a%2520product%2520with%2520millions%2520of%2520users%252C%2520LLMs%2520can%2520find%250Athousands%2520of%2520easy-to-identify%2520bugs%2520in%2520products%2520with%2520thousands%2520of%2520users.%2520And%2520on%250Athe%2520monetization%2520front%252C%2520instead%2520of%2520generic%2520ransomware%2520that%2520always%2520performs%2520the%250Asame%2520attack%2520%2528encrypt%2520all%2520your%2520data%2520and%2520request%2520payment%2520to%2520decrypt%2529%252C%2520an%250ALLM-driven%2520ransomware%2520attack%2520could%2520tailor%2520the%2520ransom%2520demand%2520based%2520on%2520the%250Aparticular%2520content%2520of%2520each%2520exploited%2520device.%250A%2520%2520We%2520show%2520that%2520these%2520two%2520attacks%2520%2528and%2520several%2520others%2529%2520are%2520imminently%2520practical%250Ausing%2520state-of-the-art%2520LLMs.%2520For%2520example%252C%2520we%2520show%2520that%2520without%2520any%2520human%250Aintervention%252C%2520an%2520LLM%2520finds%2520highly%2520sensitive%2520personal%2520information%2520in%2520the%2520Enron%250Aemail%2520dataset%2520%2528e.g.%252C%2520an%2520executive%2520having%2520an%2520affair%2520with%2520another%2520employee%2529%2520that%250Acould%2520be%2520used%2520for%2520blackmail.%2520While%2520some%2520of%2520our%2520attacks%2520are%2520still%2520too%2520expensive%250Ato%2520scale%2520widely%2520today%252C%2520the%2520incentives%2520to%2520implement%2520these%2520attacks%2520will%2520only%250Aincrease%2520as%2520LLMs%2520get%2520cheaper.%2520Thus%252C%2520we%2520argue%2520that%2520LLMs%2520create%2520a%2520need%2520for%2520new%250Adefense-in-depth%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20unlock%20new%20paths%20to%20monetizing%20exploits&entry.906535625=Nicholas%20Carlini%20and%20Milad%20Nasr%20and%20Edoardo%20Debenedetti%20and%20Barry%20Wang%20and%20Christopher%20A.%20Choquette-Choo%20and%20Daphne%20Ippolito%20and%20Florian%20Tram%C3%A8r%20and%20Matthew%20Jagielski&entry.1292438233=%20%20We%20argue%20that%20Large%20language%20models%20%28LLMs%29%20will%20soon%20alter%20the%20economics%20of%0Acyberattacks.%20Instead%20of%20attacking%20the%20most%20commonly%20used%20software%20and%0Amonetizing%20exploits%20by%20targeting%20the%20lowest%20common%20denominator%20among%20victims%2C%0ALLMs%20enable%20adversaries%20to%20launch%20tailored%20attacks%20on%20a%20user-by-user%20basis.%20On%0Athe%20exploitation%20front%2C%20instead%20of%20human%20attackers%20manually%20searching%20for%20one%0Adifficult-to-identify%20bug%20in%20a%20product%20with%20millions%20of%20users%2C%20LLMs%20can%20find%0Athousands%20of%20easy-to-identify%20bugs%20in%20products%20with%20thousands%20of%20users.%20And%20on%0Athe%20monetization%20front%2C%20instead%20of%20generic%20ransomware%20that%20always%20performs%20the%0Asame%20attack%20%28encrypt%20all%20your%20data%20and%20request%20payment%20to%20decrypt%29%2C%20an%0ALLM-driven%20ransomware%20attack%20could%20tailor%20the%20ransom%20demand%20based%20on%20the%0Aparticular%20content%20of%20each%20exploited%20device.%0A%20%20We%20show%20that%20these%20two%20attacks%20%28and%20several%20others%29%20are%20imminently%20practical%0Ausing%20state-of-the-art%20LLMs.%20For%20example%2C%20we%20show%20that%20without%20any%20human%0Aintervention%2C%20an%20LLM%20finds%20highly%20sensitive%20personal%20information%20in%20the%20Enron%0Aemail%20dataset%20%28e.g.%2C%20an%20executive%20having%20an%20affair%20with%20another%20employee%29%20that%0Acould%20be%20used%20for%20blackmail.%20While%20some%20of%20our%20attacks%20are%20still%20too%20expensive%0Ato%20scale%20widely%20today%2C%20the%20incentives%20to%20implement%20these%20attacks%20will%20only%0Aincrease%20as%20LLMs%20get%20cheaper.%20Thus%2C%20we%20argue%20that%20LLMs%20create%20a%20need%20for%20new%0Adefense-in-depth%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11449v1&entry.124074799=Read"},
{"title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity", "author": "Chan-Jan Hsu and Davide Buffelli and Jamie McGowan and Feng-Ting Liao and Yi-Chang Chen and Sattar Vakili and Da-shan Shiu", "abstract": "  Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.\n", "link": "http://arxiv.org/abs/2505.11107v1", "date": "2025-05-16", "relevancy": 1.5116, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5161}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5091}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group%20Think%3A%20Multiple%20Concurrent%20Reasoning%20Agents%20Collaborating%20at%20Token%0A%20%20Level%20Granularity&body=Title%3A%20Group%20Think%3A%20Multiple%20Concurrent%20Reasoning%20Agents%20Collaborating%20at%20Token%0A%20%20Level%20Granularity%0AAuthor%3A%20Chan-Jan%20Hsu%20and%20Davide%20Buffelli%20and%20Jamie%20McGowan%20and%20Feng-Ting%20Liao%20and%20Yi-Chang%20Chen%20and%20Sattar%20Vakili%20and%20Da-shan%20Shiu%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20the%20power%0Aof%20reasoning%20through%20self-generated%20chains%20of%20thought.%20Multiple%20reasoning%0Aagents%20can%20collaborate%20to%20raise%20joint%20reasoning%20quality%20above%20individual%0Aoutcomes.%20However%2C%20such%20agents%20typically%20interact%20in%20a%20turn-based%20manner%2C%0Atrading%20increased%20latency%20for%20improved%20quality.%20In%20this%20paper%2C%20we%20propose%20Group%0AThink--a%20single%20LLM%20that%20acts%20as%20multiple%20concurrent%20reasoning%20agents%2C%20or%0Athinkers.%20With%20shared%20visibility%20into%20each%20other%27s%20partial%20generation%20progress%2C%0AGroup%20Think%20introduces%20a%20new%20concurrent-reasoning%20paradigm%20in%20which%20multiple%0Areasoning%20trajectories%20adapt%20dynamically%20to%20one%20another%20at%20the%20token%20level.%20For%0Aexample%2C%20a%20reasoning%20thread%20may%20shift%20its%20generation%20mid-sentence%20upon%0Adetecting%20that%20another%20thread%20is%20better%20positioned%20to%20continue.%20This%0Afine-grained%2C%20token-level%20collaboration%20enables%20Group%20Think%20to%20reduce%20redundant%0Areasoning%20and%20improve%20quality%20while%20achieving%20significantly%20lower%20latency.%0AMoreover%2C%20its%20concurrent%20nature%20allows%20for%20efficient%20utilization%20of%20idle%0Acomputational%20resources%2C%20making%20it%20especially%20suitable%20for%20edge%20inference%2C%0Awhere%20very%20small%20batch%20size%20often%20underutilizes%20local~GPUs.%20We%20give%20a%20simple%0Aand%20generalizable%20modification%20that%20enables%20any%20existing%20LLM%20to%20perform%20Group%0AThink%20on%20a%20local%20GPU.%20We%20also%20present%20an%20evaluation%20strategy%20to%20benchmark%0Areasoning%20latency%20and%20empirically%20demonstrate%20latency%20improvements%20using%0Aopen-source%20LLMs%20that%20were%20not%20explicitly%20trained%20for%20Group%20Think.%20We%20hope%20this%0Awork%20paves%20the%20way%20for%20future%20LLMs%20to%20exhibit%20more%20sophisticated%20and%20more%0Aefficient%20collaborative%20behavior%20for%20higher%20quality%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup%2520Think%253A%2520Multiple%2520Concurrent%2520Reasoning%2520Agents%2520Collaborating%2520at%2520Token%250A%2520%2520Level%2520Granularity%26entry.906535625%3DChan-Jan%2520Hsu%2520and%2520Davide%2520Buffelli%2520and%2520Jamie%2520McGowan%2520and%2520Feng-Ting%2520Liao%2520and%2520Yi-Chang%2520Chen%2520and%2520Sattar%2520Vakili%2520and%2520Da-shan%2520Shiu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520the%2520power%250Aof%2520reasoning%2520through%2520self-generated%2520chains%2520of%2520thought.%2520Multiple%2520reasoning%250Aagents%2520can%2520collaborate%2520to%2520raise%2520joint%2520reasoning%2520quality%2520above%2520individual%250Aoutcomes.%2520However%252C%2520such%2520agents%2520typically%2520interact%2520in%2520a%2520turn-based%2520manner%252C%250Atrading%2520increased%2520latency%2520for%2520improved%2520quality.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Group%250AThink--a%2520single%2520LLM%2520that%2520acts%2520as%2520multiple%2520concurrent%2520reasoning%2520agents%252C%2520or%250Athinkers.%2520With%2520shared%2520visibility%2520into%2520each%2520other%2527s%2520partial%2520generation%2520progress%252C%250AGroup%2520Think%2520introduces%2520a%2520new%2520concurrent-reasoning%2520paradigm%2520in%2520which%2520multiple%250Areasoning%2520trajectories%2520adapt%2520dynamically%2520to%2520one%2520another%2520at%2520the%2520token%2520level.%2520For%250Aexample%252C%2520a%2520reasoning%2520thread%2520may%2520shift%2520its%2520generation%2520mid-sentence%2520upon%250Adetecting%2520that%2520another%2520thread%2520is%2520better%2520positioned%2520to%2520continue.%2520This%250Afine-grained%252C%2520token-level%2520collaboration%2520enables%2520Group%2520Think%2520to%2520reduce%2520redundant%250Areasoning%2520and%2520improve%2520quality%2520while%2520achieving%2520significantly%2520lower%2520latency.%250AMoreover%252C%2520its%2520concurrent%2520nature%2520allows%2520for%2520efficient%2520utilization%2520of%2520idle%250Acomputational%2520resources%252C%2520making%2520it%2520especially%2520suitable%2520for%2520edge%2520inference%252C%250Awhere%2520very%2520small%2520batch%2520size%2520often%2520underutilizes%2520local~GPUs.%2520We%2520give%2520a%2520simple%250Aand%2520generalizable%2520modification%2520that%2520enables%2520any%2520existing%2520LLM%2520to%2520perform%2520Group%250AThink%2520on%2520a%2520local%2520GPU.%2520We%2520also%2520present%2520an%2520evaluation%2520strategy%2520to%2520benchmark%250Areasoning%2520latency%2520and%2520empirically%2520demonstrate%2520latency%2520improvements%2520using%250Aopen-source%2520LLMs%2520that%2520were%2520not%2520explicitly%2520trained%2520for%2520Group%2520Think.%2520We%2520hope%2520this%250Awork%2520paves%2520the%2520way%2520for%2520future%2520LLMs%2520to%2520exhibit%2520more%2520sophisticated%2520and%2520more%250Aefficient%2520collaborative%2520behavior%2520for%2520higher%2520quality%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group%20Think%3A%20Multiple%20Concurrent%20Reasoning%20Agents%20Collaborating%20at%20Token%0A%20%20Level%20Granularity&entry.906535625=Chan-Jan%20Hsu%20and%20Davide%20Buffelli%20and%20Jamie%20McGowan%20and%20Feng-Ting%20Liao%20and%20Yi-Chang%20Chen%20and%20Sattar%20Vakili%20and%20Da-shan%20Shiu&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20the%20power%0Aof%20reasoning%20through%20self-generated%20chains%20of%20thought.%20Multiple%20reasoning%0Aagents%20can%20collaborate%20to%20raise%20joint%20reasoning%20quality%20above%20individual%0Aoutcomes.%20However%2C%20such%20agents%20typically%20interact%20in%20a%20turn-based%20manner%2C%0Atrading%20increased%20latency%20for%20improved%20quality.%20In%20this%20paper%2C%20we%20propose%20Group%0AThink--a%20single%20LLM%20that%20acts%20as%20multiple%20concurrent%20reasoning%20agents%2C%20or%0Athinkers.%20With%20shared%20visibility%20into%20each%20other%27s%20partial%20generation%20progress%2C%0AGroup%20Think%20introduces%20a%20new%20concurrent-reasoning%20paradigm%20in%20which%20multiple%0Areasoning%20trajectories%20adapt%20dynamically%20to%20one%20another%20at%20the%20token%20level.%20For%0Aexample%2C%20a%20reasoning%20thread%20may%20shift%20its%20generation%20mid-sentence%20upon%0Adetecting%20that%20another%20thread%20is%20better%20positioned%20to%20continue.%20This%0Afine-grained%2C%20token-level%20collaboration%20enables%20Group%20Think%20to%20reduce%20redundant%0Areasoning%20and%20improve%20quality%20while%20achieving%20significantly%20lower%20latency.%0AMoreover%2C%20its%20concurrent%20nature%20allows%20for%20efficient%20utilization%20of%20idle%0Acomputational%20resources%2C%20making%20it%20especially%20suitable%20for%20edge%20inference%2C%0Awhere%20very%20small%20batch%20size%20often%20underutilizes%20local~GPUs.%20We%20give%20a%20simple%0Aand%20generalizable%20modification%20that%20enables%20any%20existing%20LLM%20to%20perform%20Group%0AThink%20on%20a%20local%20GPU.%20We%20also%20present%20an%20evaluation%20strategy%20to%20benchmark%0Areasoning%20latency%20and%20empirically%20demonstrate%20latency%20improvements%20using%0Aopen-source%20LLMs%20that%20were%20not%20explicitly%20trained%20for%20Group%20Think.%20We%20hope%20this%0Awork%20paves%20the%20way%20for%20future%20LLMs%20to%20exhibit%20more%20sophisticated%20and%20more%0Aefficient%20collaborative%20behavior%20for%20higher%20quality%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11107v1&entry.124074799=Read"},
{"title": "Attention on the Sphere", "author": "Boris Bonev and Max Rietmann and Andrea Paris and Alberto Carpentieri and Thorsten Kurth", "abstract": "  We introduce a generalized attention mechanism for spherical domains,\nenabling Transformer architectures to natively process data defined on the\ntwo-dimensional sphere - a critical need in fields such as atmospheric physics,\ncosmology, and robotics, where preserving spherical symmetries and topology is\nessential for physical accuracy. By integrating numerical quadrature weights\ninto the attention mechanism, we obtain a geometrically faithful spherical\nattention that is approximately rotationally equivariant, providing strong\ninductive biases and leading to better performance than Cartesian approaches.\nTo further enhance both scalability and model performance, we propose\nneighborhood attention on the sphere, which confines interactions to geodesic\nneighborhoods. This approach reduces computational complexity and introduces\nthe additional inductive bias for locality, while retaining the symmetry\nproperties of our method. We provide optimized CUDA kernels and\nmemory-efficient implementations to ensure practical applicability. The method\nis validated on three diverse tasks: simulating shallow water equations on the\nrotating sphere, spherical image segmentation, and spherical depth estimation.\nAcross all tasks, our spherical Transformers consistently outperform their\nplanar counterparts, highlighting the advantage of geometric priors for\nlearning on spherical domains.\n", "link": "http://arxiv.org/abs/2505.11157v1", "date": "2025-05-16", "relevancy": 1.5924, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5375}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5266}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20on%20the%20Sphere&body=Title%3A%20Attention%20on%20the%20Sphere%0AAuthor%3A%20Boris%20Bonev%20and%20Max%20Rietmann%20and%20Andrea%20Paris%20and%20Alberto%20Carpentieri%20and%20Thorsten%20Kurth%0AAbstract%3A%20%20%20We%20introduce%20a%20generalized%20attention%20mechanism%20for%20spherical%20domains%2C%0Aenabling%20Transformer%20architectures%20to%20natively%20process%20data%20defined%20on%20the%0Atwo-dimensional%20sphere%20-%20a%20critical%20need%20in%20fields%20such%20as%20atmospheric%20physics%2C%0Acosmology%2C%20and%20robotics%2C%20where%20preserving%20spherical%20symmetries%20and%20topology%20is%0Aessential%20for%20physical%20accuracy.%20By%20integrating%20numerical%20quadrature%20weights%0Ainto%20the%20attention%20mechanism%2C%20we%20obtain%20a%20geometrically%20faithful%20spherical%0Aattention%20that%20is%20approximately%20rotationally%20equivariant%2C%20providing%20strong%0Ainductive%20biases%20and%20leading%20to%20better%20performance%20than%20Cartesian%20approaches.%0ATo%20further%20enhance%20both%20scalability%20and%20model%20performance%2C%20we%20propose%0Aneighborhood%20attention%20on%20the%20sphere%2C%20which%20confines%20interactions%20to%20geodesic%0Aneighborhoods.%20This%20approach%20reduces%20computational%20complexity%20and%20introduces%0Athe%20additional%20inductive%20bias%20for%20locality%2C%20while%20retaining%20the%20symmetry%0Aproperties%20of%20our%20method.%20We%20provide%20optimized%20CUDA%20kernels%20and%0Amemory-efficient%20implementations%20to%20ensure%20practical%20applicability.%20The%20method%0Ais%20validated%20on%20three%20diverse%20tasks%3A%20simulating%20shallow%20water%20equations%20on%20the%0Arotating%20sphere%2C%20spherical%20image%20segmentation%2C%20and%20spherical%20depth%20estimation.%0AAcross%20all%20tasks%2C%20our%20spherical%20Transformers%20consistently%20outperform%20their%0Aplanar%20counterparts%2C%20highlighting%20the%20advantage%20of%20geometric%20priors%20for%0Alearning%20on%20spherical%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520on%2520the%2520Sphere%26entry.906535625%3DBoris%2520Bonev%2520and%2520Max%2520Rietmann%2520and%2520Andrea%2520Paris%2520and%2520Alberto%2520Carpentieri%2520and%2520Thorsten%2520Kurth%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520generalized%2520attention%2520mechanism%2520for%2520spherical%2520domains%252C%250Aenabling%2520Transformer%2520architectures%2520to%2520natively%2520process%2520data%2520defined%2520on%2520the%250Atwo-dimensional%2520sphere%2520-%2520a%2520critical%2520need%2520in%2520fields%2520such%2520as%2520atmospheric%2520physics%252C%250Acosmology%252C%2520and%2520robotics%252C%2520where%2520preserving%2520spherical%2520symmetries%2520and%2520topology%2520is%250Aessential%2520for%2520physical%2520accuracy.%2520By%2520integrating%2520numerical%2520quadrature%2520weights%250Ainto%2520the%2520attention%2520mechanism%252C%2520we%2520obtain%2520a%2520geometrically%2520faithful%2520spherical%250Aattention%2520that%2520is%2520approximately%2520rotationally%2520equivariant%252C%2520providing%2520strong%250Ainductive%2520biases%2520and%2520leading%2520to%2520better%2520performance%2520than%2520Cartesian%2520approaches.%250ATo%2520further%2520enhance%2520both%2520scalability%2520and%2520model%2520performance%252C%2520we%2520propose%250Aneighborhood%2520attention%2520on%2520the%2520sphere%252C%2520which%2520confines%2520interactions%2520to%2520geodesic%250Aneighborhoods.%2520This%2520approach%2520reduces%2520computational%2520complexity%2520and%2520introduces%250Athe%2520additional%2520inductive%2520bias%2520for%2520locality%252C%2520while%2520retaining%2520the%2520symmetry%250Aproperties%2520of%2520our%2520method.%2520We%2520provide%2520optimized%2520CUDA%2520kernels%2520and%250Amemory-efficient%2520implementations%2520to%2520ensure%2520practical%2520applicability.%2520The%2520method%250Ais%2520validated%2520on%2520three%2520diverse%2520tasks%253A%2520simulating%2520shallow%2520water%2520equations%2520on%2520the%250Arotating%2520sphere%252C%2520spherical%2520image%2520segmentation%252C%2520and%2520spherical%2520depth%2520estimation.%250AAcross%2520all%2520tasks%252C%2520our%2520spherical%2520Transformers%2520consistently%2520outperform%2520their%250Aplanar%2520counterparts%252C%2520highlighting%2520the%2520advantage%2520of%2520geometric%2520priors%2520for%250Alearning%2520on%2520spherical%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20on%20the%20Sphere&entry.906535625=Boris%20Bonev%20and%20Max%20Rietmann%20and%20Andrea%20Paris%20and%20Alberto%20Carpentieri%20and%20Thorsten%20Kurth&entry.1292438233=%20%20We%20introduce%20a%20generalized%20attention%20mechanism%20for%20spherical%20domains%2C%0Aenabling%20Transformer%20architectures%20to%20natively%20process%20data%20defined%20on%20the%0Atwo-dimensional%20sphere%20-%20a%20critical%20need%20in%20fields%20such%20as%20atmospheric%20physics%2C%0Acosmology%2C%20and%20robotics%2C%20where%20preserving%20spherical%20symmetries%20and%20topology%20is%0Aessential%20for%20physical%20accuracy.%20By%20integrating%20numerical%20quadrature%20weights%0Ainto%20the%20attention%20mechanism%2C%20we%20obtain%20a%20geometrically%20faithful%20spherical%0Aattention%20that%20is%20approximately%20rotationally%20equivariant%2C%20providing%20strong%0Ainductive%20biases%20and%20leading%20to%20better%20performance%20than%20Cartesian%20approaches.%0ATo%20further%20enhance%20both%20scalability%20and%20model%20performance%2C%20we%20propose%0Aneighborhood%20attention%20on%20the%20sphere%2C%20which%20confines%20interactions%20to%20geodesic%0Aneighborhoods.%20This%20approach%20reduces%20computational%20complexity%20and%20introduces%0Athe%20additional%20inductive%20bias%20for%20locality%2C%20while%20retaining%20the%20symmetry%0Aproperties%20of%20our%20method.%20We%20provide%20optimized%20CUDA%20kernels%20and%0Amemory-efficient%20implementations%20to%20ensure%20practical%20applicability.%20The%20method%0Ais%20validated%20on%20three%20diverse%20tasks%3A%20simulating%20shallow%20water%20equations%20on%20the%0Arotating%20sphere%2C%20spherical%20image%20segmentation%2C%20and%20spherical%20depth%20estimation.%0AAcross%20all%20tasks%2C%20our%20spherical%20Transformers%20consistently%20outperform%20their%0Aplanar%20counterparts%2C%20highlighting%20the%20advantage%20of%20geometric%20priors%20for%0Alearning%20on%20spherical%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11157v1&entry.124074799=Read"},
{"title": "MegaScale-MoE: Large-Scale Communication-Efficient Training of\n  Mixture-of-Experts Models in Production", "author": "Chao Jin and Ziheng Jiang and Zhihao Bai and Zheng Zhong and Juncai Liu and Xiang Li and Ningxin Zheng and Xi Wang and Cong Xie and Wen Heng and Yiyuan Ma and Wenlei Bao and Size Zheng and Yanghua Peng and Haibin Lin and Xuanzhe Liu and Xin Jin and Xin Liu", "abstract": "  We present MegaScale-MoE, a production system tailored for the efficient\ntraining of large-scale mixture-of-experts (MoE) models. MoE emerges as a\npromising architecture to scale large language models (LLMs) to unprecedented\nsizes, thereby enhancing model performance. However, existing MoE training\nsystems experience a degradation in training efficiency, exacerbated by the\nescalating scale of MoE models and the continuous evolution of hardware.\n  Recognizing the pivotal role of efficient communication in enhancing MoE\ntraining, MegaScale-MoE customizes communication-efficient parallelism\nstrategies for attention and FFNs in each MoE layer and adopts a holistic\napproach to overlap communication with computation at both inter- and\nintra-operator levels. Additionally, MegaScale-MoE applies communication\ncompression with adjusted communication patterns to lower precision, further\nimproving training efficiency. When training a 352B MoE model on 1,440 NVIDIA\nHopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,\nimproving the efficiency by 1.88$\\times$ compared to Megatron-LM. We share our\noperational experience in accelerating MoE training and hope that by offering\nour insights in system design, this work will motivate future research in MoE\nsystems.\n", "link": "http://arxiv.org/abs/2505.11432v1", "date": "2025-05-16", "relevancy": 1.9562, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegaScale-MoE%3A%20Large-Scale%20Communication-Efficient%20Training%20of%0A%20%20Mixture-of-Experts%20Models%20in%20Production&body=Title%3A%20MegaScale-MoE%3A%20Large-Scale%20Communication-Efficient%20Training%20of%0A%20%20Mixture-of-Experts%20Models%20in%20Production%0AAuthor%3A%20Chao%20Jin%20and%20Ziheng%20Jiang%20and%20Zhihao%20Bai%20and%20Zheng%20Zhong%20and%20Juncai%20Liu%20and%20Xiang%20Li%20and%20Ningxin%20Zheng%20and%20Xi%20Wang%20and%20Cong%20Xie%20and%20Wen%20Heng%20and%20Yiyuan%20Ma%20and%20Wenlei%20Bao%20and%20Size%20Zheng%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Xuanzhe%20Liu%20and%20Xin%20Jin%20and%20Xin%20Liu%0AAbstract%3A%20%20%20We%20present%20MegaScale-MoE%2C%20a%20production%20system%20tailored%20for%20the%20efficient%0Atraining%20of%20large-scale%20mixture-of-experts%20%28MoE%29%20models.%20MoE%20emerges%20as%20a%0Apromising%20architecture%20to%20scale%20large%20language%20models%20%28LLMs%29%20to%20unprecedented%0Asizes%2C%20thereby%20enhancing%20model%20performance.%20However%2C%20existing%20MoE%20training%0Asystems%20experience%20a%20degradation%20in%20training%20efficiency%2C%20exacerbated%20by%20the%0Aescalating%20scale%20of%20MoE%20models%20and%20the%20continuous%20evolution%20of%20hardware.%0A%20%20Recognizing%20the%20pivotal%20role%20of%20efficient%20communication%20in%20enhancing%20MoE%0Atraining%2C%20MegaScale-MoE%20customizes%20communication-efficient%20parallelism%0Astrategies%20for%20attention%20and%20FFNs%20in%20each%20MoE%20layer%20and%20adopts%20a%20holistic%0Aapproach%20to%20overlap%20communication%20with%20computation%20at%20both%20inter-%20and%0Aintra-operator%20levels.%20Additionally%2C%20MegaScale-MoE%20applies%20communication%0Acompression%20with%20adjusted%20communication%20patterns%20to%20lower%20precision%2C%20further%0Aimproving%20training%20efficiency.%20When%20training%20a%20352B%20MoE%20model%20on%201%2C440%20NVIDIA%0AHopper%20GPUs%2C%20MegaScale-MoE%20achieves%20a%20training%20throughput%20of%201.41M%20tokens/s%2C%0Aimproving%20the%20efficiency%20by%201.88%24%5Ctimes%24%20compared%20to%20Megatron-LM.%20We%20share%20our%0Aoperational%20experience%20in%20accelerating%20MoE%20training%20and%20hope%20that%20by%20offering%0Aour%20insights%20in%20system%20design%2C%20this%20work%20will%20motivate%20future%20research%20in%20MoE%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegaScale-MoE%253A%2520Large-Scale%2520Communication-Efficient%2520Training%2520of%250A%2520%2520Mixture-of-Experts%2520Models%2520in%2520Production%26entry.906535625%3DChao%2520Jin%2520and%2520Ziheng%2520Jiang%2520and%2520Zhihao%2520Bai%2520and%2520Zheng%2520Zhong%2520and%2520Juncai%2520Liu%2520and%2520Xiang%2520Li%2520and%2520Ningxin%2520Zheng%2520and%2520Xi%2520Wang%2520and%2520Cong%2520Xie%2520and%2520Wen%2520Heng%2520and%2520Yiyuan%2520Ma%2520and%2520Wenlei%2520Bao%2520and%2520Size%2520Zheng%2520and%2520Yanghua%2520Peng%2520and%2520Haibin%2520Lin%2520and%2520Xuanzhe%2520Liu%2520and%2520Xin%2520Jin%2520and%2520Xin%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520MegaScale-MoE%252C%2520a%2520production%2520system%2520tailored%2520for%2520the%2520efficient%250Atraining%2520of%2520large-scale%2520mixture-of-experts%2520%2528MoE%2529%2520models.%2520MoE%2520emerges%2520as%2520a%250Apromising%2520architecture%2520to%2520scale%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520unprecedented%250Asizes%252C%2520thereby%2520enhancing%2520model%2520performance.%2520However%252C%2520existing%2520MoE%2520training%250Asystems%2520experience%2520a%2520degradation%2520in%2520training%2520efficiency%252C%2520exacerbated%2520by%2520the%250Aescalating%2520scale%2520of%2520MoE%2520models%2520and%2520the%2520continuous%2520evolution%2520of%2520hardware.%250A%2520%2520Recognizing%2520the%2520pivotal%2520role%2520of%2520efficient%2520communication%2520in%2520enhancing%2520MoE%250Atraining%252C%2520MegaScale-MoE%2520customizes%2520communication-efficient%2520parallelism%250Astrategies%2520for%2520attention%2520and%2520FFNs%2520in%2520each%2520MoE%2520layer%2520and%2520adopts%2520a%2520holistic%250Aapproach%2520to%2520overlap%2520communication%2520with%2520computation%2520at%2520both%2520inter-%2520and%250Aintra-operator%2520levels.%2520Additionally%252C%2520MegaScale-MoE%2520applies%2520communication%250Acompression%2520with%2520adjusted%2520communication%2520patterns%2520to%2520lower%2520precision%252C%2520further%250Aimproving%2520training%2520efficiency.%2520When%2520training%2520a%2520352B%2520MoE%2520model%2520on%25201%252C440%2520NVIDIA%250AHopper%2520GPUs%252C%2520MegaScale-MoE%2520achieves%2520a%2520training%2520throughput%2520of%25201.41M%2520tokens/s%252C%250Aimproving%2520the%2520efficiency%2520by%25201.88%2524%255Ctimes%2524%2520compared%2520to%2520Megatron-LM.%2520We%2520share%2520our%250Aoperational%2520experience%2520in%2520accelerating%2520MoE%2520training%2520and%2520hope%2520that%2520by%2520offering%250Aour%2520insights%2520in%2520system%2520design%252C%2520this%2520work%2520will%2520motivate%2520future%2520research%2520in%2520MoE%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegaScale-MoE%3A%20Large-Scale%20Communication-Efficient%20Training%20of%0A%20%20Mixture-of-Experts%20Models%20in%20Production&entry.906535625=Chao%20Jin%20and%20Ziheng%20Jiang%20and%20Zhihao%20Bai%20and%20Zheng%20Zhong%20and%20Juncai%20Liu%20and%20Xiang%20Li%20and%20Ningxin%20Zheng%20and%20Xi%20Wang%20and%20Cong%20Xie%20and%20Wen%20Heng%20and%20Yiyuan%20Ma%20and%20Wenlei%20Bao%20and%20Size%20Zheng%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Xuanzhe%20Liu%20and%20Xin%20Jin%20and%20Xin%20Liu&entry.1292438233=%20%20We%20present%20MegaScale-MoE%2C%20a%20production%20system%20tailored%20for%20the%20efficient%0Atraining%20of%20large-scale%20mixture-of-experts%20%28MoE%29%20models.%20MoE%20emerges%20as%20a%0Apromising%20architecture%20to%20scale%20large%20language%20models%20%28LLMs%29%20to%20unprecedented%0Asizes%2C%20thereby%20enhancing%20model%20performance.%20However%2C%20existing%20MoE%20training%0Asystems%20experience%20a%20degradation%20in%20training%20efficiency%2C%20exacerbated%20by%20the%0Aescalating%20scale%20of%20MoE%20models%20and%20the%20continuous%20evolution%20of%20hardware.%0A%20%20Recognizing%20the%20pivotal%20role%20of%20efficient%20communication%20in%20enhancing%20MoE%0Atraining%2C%20MegaScale-MoE%20customizes%20communication-efficient%20parallelism%0Astrategies%20for%20attention%20and%20FFNs%20in%20each%20MoE%20layer%20and%20adopts%20a%20holistic%0Aapproach%20to%20overlap%20communication%20with%20computation%20at%20both%20inter-%20and%0Aintra-operator%20levels.%20Additionally%2C%20MegaScale-MoE%20applies%20communication%0Acompression%20with%20adjusted%20communication%20patterns%20to%20lower%20precision%2C%20further%0Aimproving%20training%20efficiency.%20When%20training%20a%20352B%20MoE%20model%20on%201%2C440%20NVIDIA%0AHopper%20GPUs%2C%20MegaScale-MoE%20achieves%20a%20training%20throughput%20of%201.41M%20tokens/s%2C%0Aimproving%20the%20efficiency%20by%201.88%24%5Ctimes%24%20compared%20to%20Megatron-LM.%20We%20share%20our%0Aoperational%20experience%20in%20accelerating%20MoE%20training%20and%20hope%20that%20by%20offering%0Aour%20insights%20in%20system%20design%2C%20this%20work%20will%20motivate%20future%20research%20in%20MoE%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11432v1&entry.124074799=Read"},
{"title": "Diffusion Learning with Partial Agent Participation and Local Updates", "author": "Elsa Rizk and Kun Yuan and Ali H. Sayed", "abstract": "  Diffusion learning is a framework that endows edge devices with advanced\nintelligence. By processing and analyzing data locally and allowing each agent\nto communicate with its immediate neighbors, diffusion effectively protects the\nprivacy of edge devices, enables real-time response, and reduces reliance on\ncentral servers. However, traditional diffusion learning relies on\ncommunication at every iteration, leading to communication overhead, especially\nwith large learning models. Furthermore, the inherent volatility of edge\ndevices, stemming from power outages or signal loss, poses challenges to\nreliable communication between neighboring agents. To mitigate these issues,\nthis paper investigates an enhanced diffusion learning approach incorporating\nlocal updates and partial agent participation. Local updates will curtail\ncommunication frequency, while partial agent participation will allow for the\ninclusion of agents based on their availability. We prove that the resulting\nalgorithm is stable in the mean-square error sense and provide a tight analysis\nof its Mean-Square-Deviation (MSD) performance. Various numerical experiments\nare conducted to illustrate our theoretical findings.\n", "link": "http://arxiv.org/abs/2505.11307v1", "date": "2025-05-16", "relevancy": 2.0814, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5727}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5145}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Learning%20with%20Partial%20Agent%20Participation%20and%20Local%20Updates&body=Title%3A%20Diffusion%20Learning%20with%20Partial%20Agent%20Participation%20and%20Local%20Updates%0AAuthor%3A%20Elsa%20Rizk%20and%20Kun%20Yuan%20and%20Ali%20H.%20Sayed%0AAbstract%3A%20%20%20Diffusion%20learning%20is%20a%20framework%20that%20endows%20edge%20devices%20with%20advanced%0Aintelligence.%20By%20processing%20and%20analyzing%20data%20locally%20and%20allowing%20each%20agent%0Ato%20communicate%20with%20its%20immediate%20neighbors%2C%20diffusion%20effectively%20protects%20the%0Aprivacy%20of%20edge%20devices%2C%20enables%20real-time%20response%2C%20and%20reduces%20reliance%20on%0Acentral%20servers.%20However%2C%20traditional%20diffusion%20learning%20relies%20on%0Acommunication%20at%20every%20iteration%2C%20leading%20to%20communication%20overhead%2C%20especially%0Awith%20large%20learning%20models.%20Furthermore%2C%20the%20inherent%20volatility%20of%20edge%0Adevices%2C%20stemming%20from%20power%20outages%20or%20signal%20loss%2C%20poses%20challenges%20to%0Areliable%20communication%20between%20neighboring%20agents.%20To%20mitigate%20these%20issues%2C%0Athis%20paper%20investigates%20an%20enhanced%20diffusion%20learning%20approach%20incorporating%0Alocal%20updates%20and%20partial%20agent%20participation.%20Local%20updates%20will%20curtail%0Acommunication%20frequency%2C%20while%20partial%20agent%20participation%20will%20allow%20for%20the%0Ainclusion%20of%20agents%20based%20on%20their%20availability.%20We%20prove%20that%20the%20resulting%0Aalgorithm%20is%20stable%20in%20the%20mean-square%20error%20sense%20and%20provide%20a%20tight%20analysis%0Aof%20its%20Mean-Square-Deviation%20%28MSD%29%20performance.%20Various%20numerical%20experiments%0Aare%20conducted%20to%20illustrate%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Learning%2520with%2520Partial%2520Agent%2520Participation%2520and%2520Local%2520Updates%26entry.906535625%3DElsa%2520Rizk%2520and%2520Kun%2520Yuan%2520and%2520Ali%2520H.%2520Sayed%26entry.1292438233%3D%2520%2520Diffusion%2520learning%2520is%2520a%2520framework%2520that%2520endows%2520edge%2520devices%2520with%2520advanced%250Aintelligence.%2520By%2520processing%2520and%2520analyzing%2520data%2520locally%2520and%2520allowing%2520each%2520agent%250Ato%2520communicate%2520with%2520its%2520immediate%2520neighbors%252C%2520diffusion%2520effectively%2520protects%2520the%250Aprivacy%2520of%2520edge%2520devices%252C%2520enables%2520real-time%2520response%252C%2520and%2520reduces%2520reliance%2520on%250Acentral%2520servers.%2520However%252C%2520traditional%2520diffusion%2520learning%2520relies%2520on%250Acommunication%2520at%2520every%2520iteration%252C%2520leading%2520to%2520communication%2520overhead%252C%2520especially%250Awith%2520large%2520learning%2520models.%2520Furthermore%252C%2520the%2520inherent%2520volatility%2520of%2520edge%250Adevices%252C%2520stemming%2520from%2520power%2520outages%2520or%2520signal%2520loss%252C%2520poses%2520challenges%2520to%250Areliable%2520communication%2520between%2520neighboring%2520agents.%2520To%2520mitigate%2520these%2520issues%252C%250Athis%2520paper%2520investigates%2520an%2520enhanced%2520diffusion%2520learning%2520approach%2520incorporating%250Alocal%2520updates%2520and%2520partial%2520agent%2520participation.%2520Local%2520updates%2520will%2520curtail%250Acommunication%2520frequency%252C%2520while%2520partial%2520agent%2520participation%2520will%2520allow%2520for%2520the%250Ainclusion%2520of%2520agents%2520based%2520on%2520their%2520availability.%2520We%2520prove%2520that%2520the%2520resulting%250Aalgorithm%2520is%2520stable%2520in%2520the%2520mean-square%2520error%2520sense%2520and%2520provide%2520a%2520tight%2520analysis%250Aof%2520its%2520Mean-Square-Deviation%2520%2528MSD%2529%2520performance.%2520Various%2520numerical%2520experiments%250Aare%2520conducted%2520to%2520illustrate%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Learning%20with%20Partial%20Agent%20Participation%20and%20Local%20Updates&entry.906535625=Elsa%20Rizk%20and%20Kun%20Yuan%20and%20Ali%20H.%20Sayed&entry.1292438233=%20%20Diffusion%20learning%20is%20a%20framework%20that%20endows%20edge%20devices%20with%20advanced%0Aintelligence.%20By%20processing%20and%20analyzing%20data%20locally%20and%20allowing%20each%20agent%0Ato%20communicate%20with%20its%20immediate%20neighbors%2C%20diffusion%20effectively%20protects%20the%0Aprivacy%20of%20edge%20devices%2C%20enables%20real-time%20response%2C%20and%20reduces%20reliance%20on%0Acentral%20servers.%20However%2C%20traditional%20diffusion%20learning%20relies%20on%0Acommunication%20at%20every%20iteration%2C%20leading%20to%20communication%20overhead%2C%20especially%0Awith%20large%20learning%20models.%20Furthermore%2C%20the%20inherent%20volatility%20of%20edge%0Adevices%2C%20stemming%20from%20power%20outages%20or%20signal%20loss%2C%20poses%20challenges%20to%0Areliable%20communication%20between%20neighboring%20agents.%20To%20mitigate%20these%20issues%2C%0Athis%20paper%20investigates%20an%20enhanced%20diffusion%20learning%20approach%20incorporating%0Alocal%20updates%20and%20partial%20agent%20participation.%20Local%20updates%20will%20curtail%0Acommunication%20frequency%2C%20while%20partial%20agent%20participation%20will%20allow%20for%20the%0Ainclusion%20of%20agents%20based%20on%20their%20availability.%20We%20prove%20that%20the%20resulting%0Aalgorithm%20is%20stable%20in%20the%20mean-square%20error%20sense%20and%20provide%20a%20tight%20analysis%0Aof%20its%20Mean-Square-Deviation%20%28MSD%29%20performance.%20Various%20numerical%20experiments%0Aare%20conducted%20to%20illustrate%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11307v1&entry.124074799=Read"},
{"title": "Phare: A Safety Probe for Large Language Models", "author": "Pierre Le Jeune and Beno\u00eet Mal\u00e9sieux and Weixuan Xiao and Matteo Dora", "abstract": "  Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.\n", "link": "http://arxiv.org/abs/2505.11365v1", "date": "2025-05-16", "relevancy": 2.0389, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phare%3A%20A%20Safety%20Probe%20for%20Large%20Language%20Models&body=Title%3A%20Phare%3A%20A%20Safety%20Probe%20for%20Large%20Language%20Models%0AAuthor%3A%20Pierre%20Le%20Jeune%20and%20Beno%C3%AEt%20Mal%C3%A9sieux%20and%20Weixuan%20Xiao%20and%20Matteo%20Dora%0AAbstract%3A%20%20%20Ensuring%20the%20safety%20of%20large%20language%20models%20%28LLMs%29%20is%20critical%20for%0Aresponsible%20deployment%2C%20yet%20existing%20evaluations%20often%20prioritize%20performance%0Aover%20identifying%20failure%20modes.%20We%20introduce%20Phare%2C%20a%20multilingual%20diagnostic%0Aframework%20to%20probe%20and%20evaluate%20LLM%20behavior%20across%20three%20critical%20dimensions%3A%0Ahallucination%20and%20reliability%2C%20social%20biases%2C%20and%20harmful%20content%20generation.%0AOur%20evaluation%20of%2017%20state-of-the-art%20LLMs%20reveals%20patterns%20of%20systematic%0Avulnerabilities%20across%20all%20safety%20dimensions%2C%20including%20sycophancy%2C%20prompt%0Asensitivity%2C%20and%20stereotype%20reproduction.%20By%20highlighting%20these%20specific%0Afailure%20modes%20rather%20than%20simply%20ranking%20models%2C%20Phare%20provides%20researchers%20and%0Apractitioners%20with%20actionable%20insights%20to%20build%20more%20robust%2C%20aligned%2C%20and%0Atrustworthy%20language%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhare%253A%2520A%2520Safety%2520Probe%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DPierre%2520Le%2520Jeune%2520and%2520Beno%25C3%25AEt%2520Mal%25C3%25A9sieux%2520and%2520Weixuan%2520Xiao%2520and%2520Matteo%2520Dora%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520safety%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520critical%2520for%250Aresponsible%2520deployment%252C%2520yet%2520existing%2520evaluations%2520often%2520prioritize%2520performance%250Aover%2520identifying%2520failure%2520modes.%2520We%2520introduce%2520Phare%252C%2520a%2520multilingual%2520diagnostic%250Aframework%2520to%2520probe%2520and%2520evaluate%2520LLM%2520behavior%2520across%2520three%2520critical%2520dimensions%253A%250Ahallucination%2520and%2520reliability%252C%2520social%2520biases%252C%2520and%2520harmful%2520content%2520generation.%250AOur%2520evaluation%2520of%252017%2520state-of-the-art%2520LLMs%2520reveals%2520patterns%2520of%2520systematic%250Avulnerabilities%2520across%2520all%2520safety%2520dimensions%252C%2520including%2520sycophancy%252C%2520prompt%250Asensitivity%252C%2520and%2520stereotype%2520reproduction.%2520By%2520highlighting%2520these%2520specific%250Afailure%2520modes%2520rather%2520than%2520simply%2520ranking%2520models%252C%2520Phare%2520provides%2520researchers%2520and%250Apractitioners%2520with%2520actionable%2520insights%2520to%2520build%2520more%2520robust%252C%2520aligned%252C%2520and%250Atrustworthy%2520language%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phare%3A%20A%20Safety%20Probe%20for%20Large%20Language%20Models&entry.906535625=Pierre%20Le%20Jeune%20and%20Beno%C3%AEt%20Mal%C3%A9sieux%20and%20Weixuan%20Xiao%20and%20Matteo%20Dora&entry.1292438233=%20%20Ensuring%20the%20safety%20of%20large%20language%20models%20%28LLMs%29%20is%20critical%20for%0Aresponsible%20deployment%2C%20yet%20existing%20evaluations%20often%20prioritize%20performance%0Aover%20identifying%20failure%20modes.%20We%20introduce%20Phare%2C%20a%20multilingual%20diagnostic%0Aframework%20to%20probe%20and%20evaluate%20LLM%20behavior%20across%20three%20critical%20dimensions%3A%0Ahallucination%20and%20reliability%2C%20social%20biases%2C%20and%20harmful%20content%20generation.%0AOur%20evaluation%20of%2017%20state-of-the-art%20LLMs%20reveals%20patterns%20of%20systematic%0Avulnerabilities%20across%20all%20safety%20dimensions%2C%20including%20sycophancy%2C%20prompt%0Asensitivity%2C%20and%20stereotype%20reproduction.%20By%20highlighting%20these%20specific%0Afailure%20modes%20rather%20than%20simply%20ranking%20models%2C%20Phare%20provides%20researchers%20and%0Apractitioners%20with%20actionable%20insights%20to%20build%20more%20robust%2C%20aligned%2C%20and%0Atrustworthy%20language%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11365v1&entry.124074799=Read"},
{"title": "PSDiffusion: Harmonized Multi-Layer Image Generation via Layout and\n  Appearance Alignment", "author": "Dingbang Huang and Wenbo Li and Yifei Zhao and Xinyu Pan and Yanhong Zeng and Bo Dai", "abstract": "  Diffusion models have made remarkable advancements in generating high-quality\nimages from textual descriptions. Recent works like LayerDiffuse have extended\nthe previous single-layer, unified image generation paradigm to transparent\nimage layer generation. However, existing multi-layer generation methods fail\nto handle the interactions among multiple layers such as rational global\nlayout, physics-plausible contacts and visual effects like shadows and\nreflections while maintaining high alpha quality. To solve this problem, we\npropose PSDiffusion, a unified diffusion framework for simultaneous multi-layer\ntext-to-image generation. Our model can automatically generate multi-layer\nimages with one RGB background and multiple RGBA foregrounds through a single\nfeed-forward process. Unlike existing methods that combine multiple tools for\npost-decomposition or generate layers sequentially and separately, our method\nintroduces a global-layer interactive mechanism that generates layered-images\nconcurrently and collaboratively, ensuring not only high quality and\ncompleteness for each layer, but also spatial and visual interactions among\nlayers for global coherence.\n", "link": "http://arxiv.org/abs/2505.11468v1", "date": "2025-05-16", "relevancy": 1.2678, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6524}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6252}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSDiffusion%3A%20Harmonized%20Multi-Layer%20Image%20Generation%20via%20Layout%20and%0A%20%20Appearance%20Alignment&body=Title%3A%20PSDiffusion%3A%20Harmonized%20Multi-Layer%20Image%20Generation%20via%20Layout%20and%0A%20%20Appearance%20Alignment%0AAuthor%3A%20Dingbang%20Huang%20and%20Wenbo%20Li%20and%20Yifei%20Zhao%20and%20Xinyu%20Pan%20and%20Yanhong%20Zeng%20and%20Bo%20Dai%0AAbstract%3A%20%20%20Diffusion%20models%20have%20made%20remarkable%20advancements%20in%20generating%20high-quality%0Aimages%20from%20textual%20descriptions.%20Recent%20works%20like%20LayerDiffuse%20have%20extended%0Athe%20previous%20single-layer%2C%20unified%20image%20generation%20paradigm%20to%20transparent%0Aimage%20layer%20generation.%20However%2C%20existing%20multi-layer%20generation%20methods%20fail%0Ato%20handle%20the%20interactions%20among%20multiple%20layers%20such%20as%20rational%20global%0Alayout%2C%20physics-plausible%20contacts%20and%20visual%20effects%20like%20shadows%20and%0Areflections%20while%20maintaining%20high%20alpha%20quality.%20To%20solve%20this%20problem%2C%20we%0Apropose%20PSDiffusion%2C%20a%20unified%20diffusion%20framework%20for%20simultaneous%20multi-layer%0Atext-to-image%20generation.%20Our%20model%20can%20automatically%20generate%20multi-layer%0Aimages%20with%20one%20RGB%20background%20and%20multiple%20RGBA%20foregrounds%20through%20a%20single%0Afeed-forward%20process.%20Unlike%20existing%20methods%20that%20combine%20multiple%20tools%20for%0Apost-decomposition%20or%20generate%20layers%20sequentially%20and%20separately%2C%20our%20method%0Aintroduces%20a%20global-layer%20interactive%20mechanism%20that%20generates%20layered-images%0Aconcurrently%20and%20collaboratively%2C%20ensuring%20not%20only%20high%20quality%20and%0Acompleteness%20for%20each%20layer%2C%20but%20also%20spatial%20and%20visual%20interactions%20among%0Alayers%20for%20global%20coherence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSDiffusion%253A%2520Harmonized%2520Multi-Layer%2520Image%2520Generation%2520via%2520Layout%2520and%250A%2520%2520Appearance%2520Alignment%26entry.906535625%3DDingbang%2520Huang%2520and%2520Wenbo%2520Li%2520and%2520Yifei%2520Zhao%2520and%2520Xinyu%2520Pan%2520and%2520Yanhong%2520Zeng%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520made%2520remarkable%2520advancements%2520in%2520generating%2520high-quality%250Aimages%2520from%2520textual%2520descriptions.%2520Recent%2520works%2520like%2520LayerDiffuse%2520have%2520extended%250Athe%2520previous%2520single-layer%252C%2520unified%2520image%2520generation%2520paradigm%2520to%2520transparent%250Aimage%2520layer%2520generation.%2520However%252C%2520existing%2520multi-layer%2520generation%2520methods%2520fail%250Ato%2520handle%2520the%2520interactions%2520among%2520multiple%2520layers%2520such%2520as%2520rational%2520global%250Alayout%252C%2520physics-plausible%2520contacts%2520and%2520visual%2520effects%2520like%2520shadows%2520and%250Areflections%2520while%2520maintaining%2520high%2520alpha%2520quality.%2520To%2520solve%2520this%2520problem%252C%2520we%250Apropose%2520PSDiffusion%252C%2520a%2520unified%2520diffusion%2520framework%2520for%2520simultaneous%2520multi-layer%250Atext-to-image%2520generation.%2520Our%2520model%2520can%2520automatically%2520generate%2520multi-layer%250Aimages%2520with%2520one%2520RGB%2520background%2520and%2520multiple%2520RGBA%2520foregrounds%2520through%2520a%2520single%250Afeed-forward%2520process.%2520Unlike%2520existing%2520methods%2520that%2520combine%2520multiple%2520tools%2520for%250Apost-decomposition%2520or%2520generate%2520layers%2520sequentially%2520and%2520separately%252C%2520our%2520method%250Aintroduces%2520a%2520global-layer%2520interactive%2520mechanism%2520that%2520generates%2520layered-images%250Aconcurrently%2520and%2520collaboratively%252C%2520ensuring%2520not%2520only%2520high%2520quality%2520and%250Acompleteness%2520for%2520each%2520layer%252C%2520but%2520also%2520spatial%2520and%2520visual%2520interactions%2520among%250Alayers%2520for%2520global%2520coherence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSDiffusion%3A%20Harmonized%20Multi-Layer%20Image%20Generation%20via%20Layout%20and%0A%20%20Appearance%20Alignment&entry.906535625=Dingbang%20Huang%20and%20Wenbo%20Li%20and%20Yifei%20Zhao%20and%20Xinyu%20Pan%20and%20Yanhong%20Zeng%20and%20Bo%20Dai&entry.1292438233=%20%20Diffusion%20models%20have%20made%20remarkable%20advancements%20in%20generating%20high-quality%0Aimages%20from%20textual%20descriptions.%20Recent%20works%20like%20LayerDiffuse%20have%20extended%0Athe%20previous%20single-layer%2C%20unified%20image%20generation%20paradigm%20to%20transparent%0Aimage%20layer%20generation.%20However%2C%20existing%20multi-layer%20generation%20methods%20fail%0Ato%20handle%20the%20interactions%20among%20multiple%20layers%20such%20as%20rational%20global%0Alayout%2C%20physics-plausible%20contacts%20and%20visual%20effects%20like%20shadows%20and%0Areflections%20while%20maintaining%20high%20alpha%20quality.%20To%20solve%20this%20problem%2C%20we%0Apropose%20PSDiffusion%2C%20a%20unified%20diffusion%20framework%20for%20simultaneous%20multi-layer%0Atext-to-image%20generation.%20Our%20model%20can%20automatically%20generate%20multi-layer%0Aimages%20with%20one%20RGB%20background%20and%20multiple%20RGBA%20foregrounds%20through%20a%20single%0Afeed-forward%20process.%20Unlike%20existing%20methods%20that%20combine%20multiple%20tools%20for%0Apost-decomposition%20or%20generate%20layers%20sequentially%20and%20separately%2C%20our%20method%0Aintroduces%20a%20global-layer%20interactive%20mechanism%20that%20generates%20layered-images%0Aconcurrently%20and%20collaboratively%2C%20ensuring%20not%20only%20high%20quality%20and%0Acompleteness%20for%20each%20layer%2C%20but%20also%20spatial%20and%20visual%20interactions%20among%0Alayers%20for%20global%20coherence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11468v1&entry.124074799=Read"},
{"title": "Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral\n  Changes Approach", "author": "Jiabei Cheng and Zhen-Qun Yang and Jiannong Cao and Yu Yang and Xinzhe Zheng", "abstract": "  Timely prediction of students at high risk of dropout is critical for early\nintervention and improving educational outcomes. However, in offline\neducational settings, poor data quality, limited scale, and high heterogeneity\noften hinder the application of advanced machine learning models. Furthermore,\nwhile educational theories provide valuable insights into dropout phenomena,\nthe lack of quantifiable metrics for key indicators limits their use in\ndata-driven modeling. Through data analysis and a review of educational\nliterature, we identified abrupt changes in student behavior as key early\nsignals of dropout risk. To address this, we propose the Dual-Modal Multiscale\nSliding Window (DMSW) Model, which integrates academic performance and\nbehavioral data to dynamically capture behavior patterns using minimal data.\nThe DMSW model improves prediction accuracy by 15% compared to traditional\nmethods, enabling educators to identify high-risk students earlier, provide\ntimely support, and foster a more inclusive learning environment. Our analysis\nhighlights key behavior patterns, offering practical insights for preventive\nstrategies and tailored support. These findings bridge the gap between theory\nand practice in dropout prediction, giving educators an innovative tool to\nenhance student retention and outcomes.\n", "link": "http://arxiv.org/abs/2505.11119v1", "date": "2025-05-16", "relevancy": 0.9604, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5043}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4759}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Student%20Dropout%20Risk%20With%20A%20Dual-Modal%20Abrupt%20Behavioral%0A%20%20Changes%20Approach&body=Title%3A%20Predicting%20Student%20Dropout%20Risk%20With%20A%20Dual-Modal%20Abrupt%20Behavioral%0A%20%20Changes%20Approach%0AAuthor%3A%20Jiabei%20Cheng%20and%20Zhen-Qun%20Yang%20and%20Jiannong%20Cao%20and%20Yu%20Yang%20and%20Xinzhe%20Zheng%0AAbstract%3A%20%20%20Timely%20prediction%20of%20students%20at%20high%20risk%20of%20dropout%20is%20critical%20for%20early%0Aintervention%20and%20improving%20educational%20outcomes.%20However%2C%20in%20offline%0Aeducational%20settings%2C%20poor%20data%20quality%2C%20limited%20scale%2C%20and%20high%20heterogeneity%0Aoften%20hinder%20the%20application%20of%20advanced%20machine%20learning%20models.%20Furthermore%2C%0Awhile%20educational%20theories%20provide%20valuable%20insights%20into%20dropout%20phenomena%2C%0Athe%20lack%20of%20quantifiable%20metrics%20for%20key%20indicators%20limits%20their%20use%20in%0Adata-driven%20modeling.%20Through%20data%20analysis%20and%20a%20review%20of%20educational%0Aliterature%2C%20we%20identified%20abrupt%20changes%20in%20student%20behavior%20as%20key%20early%0Asignals%20of%20dropout%20risk.%20To%20address%20this%2C%20we%20propose%20the%20Dual-Modal%20Multiscale%0ASliding%20Window%20%28DMSW%29%20Model%2C%20which%20integrates%20academic%20performance%20and%0Abehavioral%20data%20to%20dynamically%20capture%20behavior%20patterns%20using%20minimal%20data.%0AThe%20DMSW%20model%20improves%20prediction%20accuracy%20by%2015%25%20compared%20to%20traditional%0Amethods%2C%20enabling%20educators%20to%20identify%20high-risk%20students%20earlier%2C%20provide%0Atimely%20support%2C%20and%20foster%20a%20more%20inclusive%20learning%20environment.%20Our%20analysis%0Ahighlights%20key%20behavior%20patterns%2C%20offering%20practical%20insights%20for%20preventive%0Astrategies%20and%20tailored%20support.%20These%20findings%20bridge%20the%20gap%20between%20theory%0Aand%20practice%20in%20dropout%20prediction%2C%20giving%20educators%20an%20innovative%20tool%20to%0Aenhance%20student%20retention%20and%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Student%2520Dropout%2520Risk%2520With%2520A%2520Dual-Modal%2520Abrupt%2520Behavioral%250A%2520%2520Changes%2520Approach%26entry.906535625%3DJiabei%2520Cheng%2520and%2520Zhen-Qun%2520Yang%2520and%2520Jiannong%2520Cao%2520and%2520Yu%2520Yang%2520and%2520Xinzhe%2520Zheng%26entry.1292438233%3D%2520%2520Timely%2520prediction%2520of%2520students%2520at%2520high%2520risk%2520of%2520dropout%2520is%2520critical%2520for%2520early%250Aintervention%2520and%2520improving%2520educational%2520outcomes.%2520However%252C%2520in%2520offline%250Aeducational%2520settings%252C%2520poor%2520data%2520quality%252C%2520limited%2520scale%252C%2520and%2520high%2520heterogeneity%250Aoften%2520hinder%2520the%2520application%2520of%2520advanced%2520machine%2520learning%2520models.%2520Furthermore%252C%250Awhile%2520educational%2520theories%2520provide%2520valuable%2520insights%2520into%2520dropout%2520phenomena%252C%250Athe%2520lack%2520of%2520quantifiable%2520metrics%2520for%2520key%2520indicators%2520limits%2520their%2520use%2520in%250Adata-driven%2520modeling.%2520Through%2520data%2520analysis%2520and%2520a%2520review%2520of%2520educational%250Aliterature%252C%2520we%2520identified%2520abrupt%2520changes%2520in%2520student%2520behavior%2520as%2520key%2520early%250Asignals%2520of%2520dropout%2520risk.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Dual-Modal%2520Multiscale%250ASliding%2520Window%2520%2528DMSW%2529%2520Model%252C%2520which%2520integrates%2520academic%2520performance%2520and%250Abehavioral%2520data%2520to%2520dynamically%2520capture%2520behavior%2520patterns%2520using%2520minimal%2520data.%250AThe%2520DMSW%2520model%2520improves%2520prediction%2520accuracy%2520by%252015%2525%2520compared%2520to%2520traditional%250Amethods%252C%2520enabling%2520educators%2520to%2520identify%2520high-risk%2520students%2520earlier%252C%2520provide%250Atimely%2520support%252C%2520and%2520foster%2520a%2520more%2520inclusive%2520learning%2520environment.%2520Our%2520analysis%250Ahighlights%2520key%2520behavior%2520patterns%252C%2520offering%2520practical%2520insights%2520for%2520preventive%250Astrategies%2520and%2520tailored%2520support.%2520These%2520findings%2520bridge%2520the%2520gap%2520between%2520theory%250Aand%2520practice%2520in%2520dropout%2520prediction%252C%2520giving%2520educators%2520an%2520innovative%2520tool%2520to%250Aenhance%2520student%2520retention%2520and%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Student%20Dropout%20Risk%20With%20A%20Dual-Modal%20Abrupt%20Behavioral%0A%20%20Changes%20Approach&entry.906535625=Jiabei%20Cheng%20and%20Zhen-Qun%20Yang%20and%20Jiannong%20Cao%20and%20Yu%20Yang%20and%20Xinzhe%20Zheng&entry.1292438233=%20%20Timely%20prediction%20of%20students%20at%20high%20risk%20of%20dropout%20is%20critical%20for%20early%0Aintervention%20and%20improving%20educational%20outcomes.%20However%2C%20in%20offline%0Aeducational%20settings%2C%20poor%20data%20quality%2C%20limited%20scale%2C%20and%20high%20heterogeneity%0Aoften%20hinder%20the%20application%20of%20advanced%20machine%20learning%20models.%20Furthermore%2C%0Awhile%20educational%20theories%20provide%20valuable%20insights%20into%20dropout%20phenomena%2C%0Athe%20lack%20of%20quantifiable%20metrics%20for%20key%20indicators%20limits%20their%20use%20in%0Adata-driven%20modeling.%20Through%20data%20analysis%20and%20a%20review%20of%20educational%0Aliterature%2C%20we%20identified%20abrupt%20changes%20in%20student%20behavior%20as%20key%20early%0Asignals%20of%20dropout%20risk.%20To%20address%20this%2C%20we%20propose%20the%20Dual-Modal%20Multiscale%0ASliding%20Window%20%28DMSW%29%20Model%2C%20which%20integrates%20academic%20performance%20and%0Abehavioral%20data%20to%20dynamically%20capture%20behavior%20patterns%20using%20minimal%20data.%0AThe%20DMSW%20model%20improves%20prediction%20accuracy%20by%2015%25%20compared%20to%20traditional%0Amethods%2C%20enabling%20educators%20to%20identify%20high-risk%20students%20earlier%2C%20provide%0Atimely%20support%2C%20and%20foster%20a%20more%20inclusive%20learning%20environment.%20Our%20analysis%0Ahighlights%20key%20behavior%20patterns%2C%20offering%20practical%20insights%20for%20preventive%0Astrategies%20and%20tailored%20support.%20These%20findings%20bridge%20the%20gap%20between%20theory%0Aand%20practice%20in%20dropout%20prediction%2C%20giving%20educators%20an%20innovative%20tool%20to%0Aenhance%20student%20retention%20and%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11119v1&entry.124074799=Read"},
{"title": "Uncertainty quantification with approximate variational learning for\n  wearable photoplethysmography prediction tasks", "author": "Ciaran Bench and Vivek Desai and Mohammad Moulaeifard and Nils Strodthoff and Philip Aston and Andrew Thompson", "abstract": "  Photoplethysmography (PPG) signals encode information about relative changes\nin blood volume that can be used to assess various aspects of cardiac health\nnon-invasively, e.g.\\ to detect atrial fibrillation (AF) or predict blood\npressure (BP). Deep networks are well-equipped to handle the large quantities\nof data acquired from wearable measurement devices. However, they lack\ninterpretability and are prone to overfitting, leaving considerable risk for\npoor performance on unseen data and misdiagnosis. Here, we describe the use of\ntwo scalable uncertainty quantification techniques: Monte Carlo Dropout and the\nrecently proposed Improved Variational Online Newton. These techniques are used\nto assess the trustworthiness of models trained to perform AF classification\nand BP regression from raw PPG time series. We find that the choice of\nhyperparameters has a considerable effect on the predictive performance of the\nmodels and on the quality and composition of predicted uncertainties. E.g. the\nstochasticity of the model parameter sampling determines the proportion of the\ntotal uncertainty that is aleatoric, and has varying effects on predictive\nperformance and calibration quality dependent on the chosen uncertainty\nquantification technique and the chosen expression of uncertainty. We find\nsignificant discrepancy in the quality of uncertainties over the predicted\nclasses, emphasising the need for a thorough evaluation protocol that assesses\nlocal and adaptive calibration. This work suggests that the choice of\nhyperparameters must be carefully tuned to balance predictive performance and\ncalibration quality, and that the optimal parameterisation may vary depending\non the chosen expression of uncertainty.\n", "link": "http://arxiv.org/abs/2505.11412v1", "date": "2025-05-16", "relevancy": 1.6634, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20quantification%20with%20approximate%20variational%20learning%20for%0A%20%20wearable%20photoplethysmography%20prediction%20tasks&body=Title%3A%20Uncertainty%20quantification%20with%20approximate%20variational%20learning%20for%0A%20%20wearable%20photoplethysmography%20prediction%20tasks%0AAuthor%3A%20Ciaran%20Bench%20and%20Vivek%20Desai%20and%20Mohammad%20Moulaeifard%20and%20Nils%20Strodthoff%20and%20Philip%20Aston%20and%20Andrew%20Thompson%0AAbstract%3A%20%20%20Photoplethysmography%20%28PPG%29%20signals%20encode%20information%20about%20relative%20changes%0Ain%20blood%20volume%20that%20can%20be%20used%20to%20assess%20various%20aspects%20of%20cardiac%20health%0Anon-invasively%2C%20e.g.%5C%20to%20detect%20atrial%20fibrillation%20%28AF%29%20or%20predict%20blood%0Apressure%20%28BP%29.%20Deep%20networks%20are%20well-equipped%20to%20handle%20the%20large%20quantities%0Aof%20data%20acquired%20from%20wearable%20measurement%20devices.%20However%2C%20they%20lack%0Ainterpretability%20and%20are%20prone%20to%20overfitting%2C%20leaving%20considerable%20risk%20for%0Apoor%20performance%20on%20unseen%20data%20and%20misdiagnosis.%20Here%2C%20we%20describe%20the%20use%20of%0Atwo%20scalable%20uncertainty%20quantification%20techniques%3A%20Monte%20Carlo%20Dropout%20and%20the%0Arecently%20proposed%20Improved%20Variational%20Online%20Newton.%20These%20techniques%20are%20used%0Ato%20assess%20the%20trustworthiness%20of%20models%20trained%20to%20perform%20AF%20classification%0Aand%20BP%20regression%20from%20raw%20PPG%20time%20series.%20We%20find%20that%20the%20choice%20of%0Ahyperparameters%20has%20a%20considerable%20effect%20on%20the%20predictive%20performance%20of%20the%0Amodels%20and%20on%20the%20quality%20and%20composition%20of%20predicted%20uncertainties.%20E.g.%20the%0Astochasticity%20of%20the%20model%20parameter%20sampling%20determines%20the%20proportion%20of%20the%0Atotal%20uncertainty%20that%20is%20aleatoric%2C%20and%20has%20varying%20effects%20on%20predictive%0Aperformance%20and%20calibration%20quality%20dependent%20on%20the%20chosen%20uncertainty%0Aquantification%20technique%20and%20the%20chosen%20expression%20of%20uncertainty.%20We%20find%0Asignificant%20discrepancy%20in%20the%20quality%20of%20uncertainties%20over%20the%20predicted%0Aclasses%2C%20emphasising%20the%20need%20for%20a%20thorough%20evaluation%20protocol%20that%20assesses%0Alocal%20and%20adaptive%20calibration.%20This%20work%20suggests%20that%20the%20choice%20of%0Ahyperparameters%20must%20be%20carefully%20tuned%20to%20balance%20predictive%20performance%20and%0Acalibration%20quality%2C%20and%20that%20the%20optimal%20parameterisation%20may%20vary%20depending%0Aon%20the%20chosen%20expression%20of%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520quantification%2520with%2520approximate%2520variational%2520learning%2520for%250A%2520%2520wearable%2520photoplethysmography%2520prediction%2520tasks%26entry.906535625%3DCiaran%2520Bench%2520and%2520Vivek%2520Desai%2520and%2520Mohammad%2520Moulaeifard%2520and%2520Nils%2520Strodthoff%2520and%2520Philip%2520Aston%2520and%2520Andrew%2520Thompson%26entry.1292438233%3D%2520%2520Photoplethysmography%2520%2528PPG%2529%2520signals%2520encode%2520information%2520about%2520relative%2520changes%250Ain%2520blood%2520volume%2520that%2520can%2520be%2520used%2520to%2520assess%2520various%2520aspects%2520of%2520cardiac%2520health%250Anon-invasively%252C%2520e.g.%255C%2520to%2520detect%2520atrial%2520fibrillation%2520%2528AF%2529%2520or%2520predict%2520blood%250Apressure%2520%2528BP%2529.%2520Deep%2520networks%2520are%2520well-equipped%2520to%2520handle%2520the%2520large%2520quantities%250Aof%2520data%2520acquired%2520from%2520wearable%2520measurement%2520devices.%2520However%252C%2520they%2520lack%250Ainterpretability%2520and%2520are%2520prone%2520to%2520overfitting%252C%2520leaving%2520considerable%2520risk%2520for%250Apoor%2520performance%2520on%2520unseen%2520data%2520and%2520misdiagnosis.%2520Here%252C%2520we%2520describe%2520the%2520use%2520of%250Atwo%2520scalable%2520uncertainty%2520quantification%2520techniques%253A%2520Monte%2520Carlo%2520Dropout%2520and%2520the%250Arecently%2520proposed%2520Improved%2520Variational%2520Online%2520Newton.%2520These%2520techniques%2520are%2520used%250Ato%2520assess%2520the%2520trustworthiness%2520of%2520models%2520trained%2520to%2520perform%2520AF%2520classification%250Aand%2520BP%2520regression%2520from%2520raw%2520PPG%2520time%2520series.%2520We%2520find%2520that%2520the%2520choice%2520of%250Ahyperparameters%2520has%2520a%2520considerable%2520effect%2520on%2520the%2520predictive%2520performance%2520of%2520the%250Amodels%2520and%2520on%2520the%2520quality%2520and%2520composition%2520of%2520predicted%2520uncertainties.%2520E.g.%2520the%250Astochasticity%2520of%2520the%2520model%2520parameter%2520sampling%2520determines%2520the%2520proportion%2520of%2520the%250Atotal%2520uncertainty%2520that%2520is%2520aleatoric%252C%2520and%2520has%2520varying%2520effects%2520on%2520predictive%250Aperformance%2520and%2520calibration%2520quality%2520dependent%2520on%2520the%2520chosen%2520uncertainty%250Aquantification%2520technique%2520and%2520the%2520chosen%2520expression%2520of%2520uncertainty.%2520We%2520find%250Asignificant%2520discrepancy%2520in%2520the%2520quality%2520of%2520uncertainties%2520over%2520the%2520predicted%250Aclasses%252C%2520emphasising%2520the%2520need%2520for%2520a%2520thorough%2520evaluation%2520protocol%2520that%2520assesses%250Alocal%2520and%2520adaptive%2520calibration.%2520This%2520work%2520suggests%2520that%2520the%2520choice%2520of%250Ahyperparameters%2520must%2520be%2520carefully%2520tuned%2520to%2520balance%2520predictive%2520performance%2520and%250Acalibration%2520quality%252C%2520and%2520that%2520the%2520optimal%2520parameterisation%2520may%2520vary%2520depending%250Aon%2520the%2520chosen%2520expression%2520of%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20quantification%20with%20approximate%20variational%20learning%20for%0A%20%20wearable%20photoplethysmography%20prediction%20tasks&entry.906535625=Ciaran%20Bench%20and%20Vivek%20Desai%20and%20Mohammad%20Moulaeifard%20and%20Nils%20Strodthoff%20and%20Philip%20Aston%20and%20Andrew%20Thompson&entry.1292438233=%20%20Photoplethysmography%20%28PPG%29%20signals%20encode%20information%20about%20relative%20changes%0Ain%20blood%20volume%20that%20can%20be%20used%20to%20assess%20various%20aspects%20of%20cardiac%20health%0Anon-invasively%2C%20e.g.%5C%20to%20detect%20atrial%20fibrillation%20%28AF%29%20or%20predict%20blood%0Apressure%20%28BP%29.%20Deep%20networks%20are%20well-equipped%20to%20handle%20the%20large%20quantities%0Aof%20data%20acquired%20from%20wearable%20measurement%20devices.%20However%2C%20they%20lack%0Ainterpretability%20and%20are%20prone%20to%20overfitting%2C%20leaving%20considerable%20risk%20for%0Apoor%20performance%20on%20unseen%20data%20and%20misdiagnosis.%20Here%2C%20we%20describe%20the%20use%20of%0Atwo%20scalable%20uncertainty%20quantification%20techniques%3A%20Monte%20Carlo%20Dropout%20and%20the%0Arecently%20proposed%20Improved%20Variational%20Online%20Newton.%20These%20techniques%20are%20used%0Ato%20assess%20the%20trustworthiness%20of%20models%20trained%20to%20perform%20AF%20classification%0Aand%20BP%20regression%20from%20raw%20PPG%20time%20series.%20We%20find%20that%20the%20choice%20of%0Ahyperparameters%20has%20a%20considerable%20effect%20on%20the%20predictive%20performance%20of%20the%0Amodels%20and%20on%20the%20quality%20and%20composition%20of%20predicted%20uncertainties.%20E.g.%20the%0Astochasticity%20of%20the%20model%20parameter%20sampling%20determines%20the%20proportion%20of%20the%0Atotal%20uncertainty%20that%20is%20aleatoric%2C%20and%20has%20varying%20effects%20on%20predictive%0Aperformance%20and%20calibration%20quality%20dependent%20on%20the%20chosen%20uncertainty%0Aquantification%20technique%20and%20the%20chosen%20expression%20of%20uncertainty.%20We%20find%0Asignificant%20discrepancy%20in%20the%20quality%20of%20uncertainties%20over%20the%20predicted%0Aclasses%2C%20emphasising%20the%20need%20for%20a%20thorough%20evaluation%20protocol%20that%20assesses%0Alocal%20and%20adaptive%20calibration.%20This%20work%20suggests%20that%20the%20choice%20of%0Ahyperparameters%20must%20be%20carefully%20tuned%20to%20balance%20predictive%20performance%20and%0Acalibration%20quality%2C%20and%20that%20the%20optimal%20parameterisation%20may%20vary%20depending%0Aon%20the%20chosen%20expression%20of%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11412v1&entry.124074799=Read"},
{"title": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse\n  Mixture-of-Experts Systems", "author": "Yinsicheng Jiang and Yao Fu and Yeqi Huang and Ping Nie and Zhan Lu and Leyang Xue and Congjie He and Man-Kit Sit and Jilong Xue and Li Dong and Ziming Miao and Dayou Du and Tairan Xu and Kai Zou and Edoardo Ponti and Luo Mai", "abstract": "  The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for\nscaling Large Language Models (LLMs) efficiently, but it depends on\nheterogeneous compute and memory resources. These factors jointly affect system\nCost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing\nbenchmarks often fail to capture these trade-offs accurately, complicating\npractical deployment decisions. To address this, we introduce MoE-CAP, a\nbenchmark specifically designed for MoE systems. Our analysis reveals that\nachieving an optimal balance across CAP is difficult with current hardware; MoE\nsystems typically optimize two of the three dimensions at the expense of the\nthird-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose\nthe CAP Radar Diagram. We further introduce sparsity-aware performance\nmetrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS\nUtilization (S-MFU)-to enable accurate performance benchmarking of MoE systems\nacross diverse hardware platforms and deployment scenarios.\n", "link": "http://arxiv.org/abs/2505.11415v1", "date": "2025-05-16", "relevancy": 1.8671, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoE-CAP%3A%20Benchmarking%20Cost%2C%20Accuracy%20and%20Performance%20of%20Sparse%0A%20%20Mixture-of-Experts%20Systems&body=Title%3A%20MoE-CAP%3A%20Benchmarking%20Cost%2C%20Accuracy%20and%20Performance%20of%20Sparse%0A%20%20Mixture-of-Experts%20Systems%0AAuthor%3A%20Yinsicheng%20Jiang%20and%20Yao%20Fu%20and%20Yeqi%20Huang%20and%20Ping%20Nie%20and%20Zhan%20Lu%20and%20Leyang%20Xue%20and%20Congjie%20He%20and%20Man-Kit%20Sit%20and%20Jilong%20Xue%20and%20Li%20Dong%20and%20Ziming%20Miao%20and%20Dayou%20Du%20and%20Tairan%20Xu%20and%20Kai%20Zou%20and%20Edoardo%20Ponti%20and%20Luo%20Mai%0AAbstract%3A%20%20%20The%20sparse%20Mixture-of-Experts%20%28MoE%29%20architecture%20is%20increasingly%20favored%20for%0Ascaling%20Large%20Language%20Models%20%28LLMs%29%20efficiently%2C%20but%20it%20depends%20on%0Aheterogeneous%20compute%20and%20memory%20resources.%20These%20factors%20jointly%20affect%20system%0ACost%2C%20Accuracy%2C%20and%20Performance%20%28CAP%29%2C%20making%20trade-offs%20inevitable.%20Existing%0Abenchmarks%20often%20fail%20to%20capture%20these%20trade-offs%20accurately%2C%20complicating%0Apractical%20deployment%20decisions.%20To%20address%20this%2C%20we%20introduce%20MoE-CAP%2C%20a%0Abenchmark%20specifically%20designed%20for%20MoE%20systems.%20Our%20analysis%20reveals%20that%0Aachieving%20an%20optimal%20balance%20across%20CAP%20is%20difficult%20with%20current%20hardware%3B%20MoE%0Asystems%20typically%20optimize%20two%20of%20the%20three%20dimensions%20at%20the%20expense%20of%20the%0Athird-a%20dynamic%20we%20term%20the%20MoE-CAP%20trade-off.%20To%20visualize%20this%2C%20we%20propose%0Athe%20CAP%20Radar%20Diagram.%20We%20further%20introduce%20sparsity-aware%20performance%0Ametrics-Sparse%20Memory%20Bandwidth%20Utilization%20%28S-MBU%29%20and%20Sparse%20Model%20FLOPS%0AUtilization%20%28S-MFU%29-to%20enable%20accurate%20performance%20benchmarking%20of%20MoE%20systems%0Aacross%20diverse%20hardware%20platforms%20and%20deployment%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoE-CAP%253A%2520Benchmarking%2520Cost%252C%2520Accuracy%2520and%2520Performance%2520of%2520Sparse%250A%2520%2520Mixture-of-Experts%2520Systems%26entry.906535625%3DYinsicheng%2520Jiang%2520and%2520Yao%2520Fu%2520and%2520Yeqi%2520Huang%2520and%2520Ping%2520Nie%2520and%2520Zhan%2520Lu%2520and%2520Leyang%2520Xue%2520and%2520Congjie%2520He%2520and%2520Man-Kit%2520Sit%2520and%2520Jilong%2520Xue%2520and%2520Li%2520Dong%2520and%2520Ziming%2520Miao%2520and%2520Dayou%2520Du%2520and%2520Tairan%2520Xu%2520and%2520Kai%2520Zou%2520and%2520Edoardo%2520Ponti%2520and%2520Luo%2520Mai%26entry.1292438233%3D%2520%2520The%2520sparse%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture%2520is%2520increasingly%2520favored%2520for%250Ascaling%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520efficiently%252C%2520but%2520it%2520depends%2520on%250Aheterogeneous%2520compute%2520and%2520memory%2520resources.%2520These%2520factors%2520jointly%2520affect%2520system%250ACost%252C%2520Accuracy%252C%2520and%2520Performance%2520%2528CAP%2529%252C%2520making%2520trade-offs%2520inevitable.%2520Existing%250Abenchmarks%2520often%2520fail%2520to%2520capture%2520these%2520trade-offs%2520accurately%252C%2520complicating%250Apractical%2520deployment%2520decisions.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MoE-CAP%252C%2520a%250Abenchmark%2520specifically%2520designed%2520for%2520MoE%2520systems.%2520Our%2520analysis%2520reveals%2520that%250Aachieving%2520an%2520optimal%2520balance%2520across%2520CAP%2520is%2520difficult%2520with%2520current%2520hardware%253B%2520MoE%250Asystems%2520typically%2520optimize%2520two%2520of%2520the%2520three%2520dimensions%2520at%2520the%2520expense%2520of%2520the%250Athird-a%2520dynamic%2520we%2520term%2520the%2520MoE-CAP%2520trade-off.%2520To%2520visualize%2520this%252C%2520we%2520propose%250Athe%2520CAP%2520Radar%2520Diagram.%2520We%2520further%2520introduce%2520sparsity-aware%2520performance%250Ametrics-Sparse%2520Memory%2520Bandwidth%2520Utilization%2520%2528S-MBU%2529%2520and%2520Sparse%2520Model%2520FLOPS%250AUtilization%2520%2528S-MFU%2529-to%2520enable%2520accurate%2520performance%2520benchmarking%2520of%2520MoE%2520systems%250Aacross%2520diverse%2520hardware%2520platforms%2520and%2520deployment%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoE-CAP%3A%20Benchmarking%20Cost%2C%20Accuracy%20and%20Performance%20of%20Sparse%0A%20%20Mixture-of-Experts%20Systems&entry.906535625=Yinsicheng%20Jiang%20and%20Yao%20Fu%20and%20Yeqi%20Huang%20and%20Ping%20Nie%20and%20Zhan%20Lu%20and%20Leyang%20Xue%20and%20Congjie%20He%20and%20Man-Kit%20Sit%20and%20Jilong%20Xue%20and%20Li%20Dong%20and%20Ziming%20Miao%20and%20Dayou%20Du%20and%20Tairan%20Xu%20and%20Kai%20Zou%20and%20Edoardo%20Ponti%20and%20Luo%20Mai&entry.1292438233=%20%20The%20sparse%20Mixture-of-Experts%20%28MoE%29%20architecture%20is%20increasingly%20favored%20for%0Ascaling%20Large%20Language%20Models%20%28LLMs%29%20efficiently%2C%20but%20it%20depends%20on%0Aheterogeneous%20compute%20and%20memory%20resources.%20These%20factors%20jointly%20affect%20system%0ACost%2C%20Accuracy%2C%20and%20Performance%20%28CAP%29%2C%20making%20trade-offs%20inevitable.%20Existing%0Abenchmarks%20often%20fail%20to%20capture%20these%20trade-offs%20accurately%2C%20complicating%0Apractical%20deployment%20decisions.%20To%20address%20this%2C%20we%20introduce%20MoE-CAP%2C%20a%0Abenchmark%20specifically%20designed%20for%20MoE%20systems.%20Our%20analysis%20reveals%20that%0Aachieving%20an%20optimal%20balance%20across%20CAP%20is%20difficult%20with%20current%20hardware%3B%20MoE%0Asystems%20typically%20optimize%20two%20of%20the%20three%20dimensions%20at%20the%20expense%20of%20the%0Athird-a%20dynamic%20we%20term%20the%20MoE-CAP%20trade-off.%20To%20visualize%20this%2C%20we%20propose%0Athe%20CAP%20Radar%20Diagram.%20We%20further%20introduce%20sparsity-aware%20performance%0Ametrics-Sparse%20Memory%20Bandwidth%20Utilization%20%28S-MBU%29%20and%20Sparse%20Model%20FLOPS%0AUtilization%20%28S-MFU%29-to%20enable%20accurate%20performance%20benchmarking%20of%20MoE%20systems%0Aacross%20diverse%20hardware%20platforms%20and%20deployment%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11415v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


