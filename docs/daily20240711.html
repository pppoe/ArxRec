<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240710.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "CharacterGen: Efficient 3D Character Generation from Single Images with\n  Multi-View Pose Canonicalization", "author": "Hao-Yang Peng and Jia-Peng Zhang and Meng-Hao Guo and Yan-Pei Cao and Shi-Min Hu", "abstract": "  In the field of digital content creation, generating high-quality 3D\ncharacters from single images is challenging, especially given the complexities\nof various body poses and the issues of self-occlusion and pose ambiguity. In\nthis paper, we present CharacterGen, a framework developed to efficiently\ngenerate 3D characters. CharacterGen introduces a streamlined generation\npipeline along with an image-conditioned multi-view diffusion model. This model\neffectively calibrates input poses to a canonical form while retaining key\nattributes of the input image, thereby addressing the challenges posed by\ndiverse poses. A transformer-based, generalizable sparse-view reconstruction\nmodel is the other core component of our approach, facilitating the creation of\ndetailed 3D models from multi-view images. We also adopt a\ntexture-back-projection strategy to produce high-quality texture maps.\nAdditionally, we have curated a dataset of anime characters, rendered in\nmultiple poses and views, to train and evaluate our model. Our approach has\nbeen thoroughly evaluated through quantitative and qualitative experiments,\nshowing its proficiency in generating 3D characters with high-quality shapes\nand textures, ready for downstream applications such as rigging and animation.\n", "link": "http://arxiv.org/abs/2402.17214v3", "date": "2024-07-10", "relevancy": 3.2512, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6553}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6477}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CharacterGen%3A%20Efficient%203D%20Character%20Generation%20from%20Single%20Images%20with%0A%20%20Multi-View%20Pose%20Canonicalization&body=Title%3A%20CharacterGen%3A%20Efficient%203D%20Character%20Generation%20from%20Single%20Images%20with%0A%20%20Multi-View%20Pose%20Canonicalization%0AAuthor%3A%20Hao-Yang%20Peng%20and%20Jia-Peng%20Zhang%20and%20Meng-Hao%20Guo%20and%20Yan-Pei%20Cao%20and%20Shi-Min%20Hu%0AAbstract%3A%20%20%20In%20the%20field%20of%20digital%20content%20creation%2C%20generating%20high-quality%203D%0Acharacters%20from%20single%20images%20is%20challenging%2C%20especially%20given%20the%20complexities%0Aof%20various%20body%20poses%20and%20the%20issues%20of%20self-occlusion%20and%20pose%20ambiguity.%20In%0Athis%20paper%2C%20we%20present%20CharacterGen%2C%20a%20framework%20developed%20to%20efficiently%0Agenerate%203D%20characters.%20CharacterGen%20introduces%20a%20streamlined%20generation%0Apipeline%20along%20with%20an%20image-conditioned%20multi-view%20diffusion%20model.%20This%20model%0Aeffectively%20calibrates%20input%20poses%20to%20a%20canonical%20form%20while%20retaining%20key%0Aattributes%20of%20the%20input%20image%2C%20thereby%20addressing%20the%20challenges%20posed%20by%0Adiverse%20poses.%20A%20transformer-based%2C%20generalizable%20sparse-view%20reconstruction%0Amodel%20is%20the%20other%20core%20component%20of%20our%20approach%2C%20facilitating%20the%20creation%20of%0Adetailed%203D%20models%20from%20multi-view%20images.%20We%20also%20adopt%20a%0Atexture-back-projection%20strategy%20to%20produce%20high-quality%20texture%20maps.%0AAdditionally%2C%20we%20have%20curated%20a%20dataset%20of%20anime%20characters%2C%20rendered%20in%0Amultiple%20poses%20and%20views%2C%20to%20train%20and%20evaluate%20our%20model.%20Our%20approach%20has%0Abeen%20thoroughly%20evaluated%20through%20quantitative%20and%20qualitative%20experiments%2C%0Ashowing%20its%20proficiency%20in%20generating%203D%20characters%20with%20high-quality%20shapes%0Aand%20textures%2C%20ready%20for%20downstream%20applications%20such%20as%20rigging%20and%20animation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17214v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterGen%253A%2520Efficient%25203D%2520Character%2520Generation%2520from%2520Single%2520Images%2520with%250A%2520%2520Multi-View%2520Pose%2520Canonicalization%26entry.906535625%3DHao-Yang%2520Peng%2520and%2520Jia-Peng%2520Zhang%2520and%2520Meng-Hao%2520Guo%2520and%2520Yan-Pei%2520Cao%2520and%2520Shi-Min%2520Hu%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520digital%2520content%2520creation%252C%2520generating%2520high-quality%25203D%250Acharacters%2520from%2520single%2520images%2520is%2520challenging%252C%2520especially%2520given%2520the%2520complexities%250Aof%2520various%2520body%2520poses%2520and%2520the%2520issues%2520of%2520self-occlusion%2520and%2520pose%2520ambiguity.%2520In%250Athis%2520paper%252C%2520we%2520present%2520CharacterGen%252C%2520a%2520framework%2520developed%2520to%2520efficiently%250Agenerate%25203D%2520characters.%2520CharacterGen%2520introduces%2520a%2520streamlined%2520generation%250Apipeline%2520along%2520with%2520an%2520image-conditioned%2520multi-view%2520diffusion%2520model.%2520This%2520model%250Aeffectively%2520calibrates%2520input%2520poses%2520to%2520a%2520canonical%2520form%2520while%2520retaining%2520key%250Aattributes%2520of%2520the%2520input%2520image%252C%2520thereby%2520addressing%2520the%2520challenges%2520posed%2520by%250Adiverse%2520poses.%2520A%2520transformer-based%252C%2520generalizable%2520sparse-view%2520reconstruction%250Amodel%2520is%2520the%2520other%2520core%2520component%2520of%2520our%2520approach%252C%2520facilitating%2520the%2520creation%2520of%250Adetailed%25203D%2520models%2520from%2520multi-view%2520images.%2520We%2520also%2520adopt%2520a%250Atexture-back-projection%2520strategy%2520to%2520produce%2520high-quality%2520texture%2520maps.%250AAdditionally%252C%2520we%2520have%2520curated%2520a%2520dataset%2520of%2520anime%2520characters%252C%2520rendered%2520in%250Amultiple%2520poses%2520and%2520views%252C%2520to%2520train%2520and%2520evaluate%2520our%2520model.%2520Our%2520approach%2520has%250Abeen%2520thoroughly%2520evaluated%2520through%2520quantitative%2520and%2520qualitative%2520experiments%252C%250Ashowing%2520its%2520proficiency%2520in%2520generating%25203D%2520characters%2520with%2520high-quality%2520shapes%250Aand%2520textures%252C%2520ready%2520for%2520downstream%2520applications%2520such%2520as%2520rigging%2520and%2520animation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17214v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CharacterGen%3A%20Efficient%203D%20Character%20Generation%20from%20Single%20Images%20with%0A%20%20Multi-View%20Pose%20Canonicalization&entry.906535625=Hao-Yang%20Peng%20and%20Jia-Peng%20Zhang%20and%20Meng-Hao%20Guo%20and%20Yan-Pei%20Cao%20and%20Shi-Min%20Hu&entry.1292438233=%20%20In%20the%20field%20of%20digital%20content%20creation%2C%20generating%20high-quality%203D%0Acharacters%20from%20single%20images%20is%20challenging%2C%20especially%20given%20the%20complexities%0Aof%20various%20body%20poses%20and%20the%20issues%20of%20self-occlusion%20and%20pose%20ambiguity.%20In%0Athis%20paper%2C%20we%20present%20CharacterGen%2C%20a%20framework%20developed%20to%20efficiently%0Agenerate%203D%20characters.%20CharacterGen%20introduces%20a%20streamlined%20generation%0Apipeline%20along%20with%20an%20image-conditioned%20multi-view%20diffusion%20model.%20This%20model%0Aeffectively%20calibrates%20input%20poses%20to%20a%20canonical%20form%20while%20retaining%20key%0Aattributes%20of%20the%20input%20image%2C%20thereby%20addressing%20the%20challenges%20posed%20by%0Adiverse%20poses.%20A%20transformer-based%2C%20generalizable%20sparse-view%20reconstruction%0Amodel%20is%20the%20other%20core%20component%20of%20our%20approach%2C%20facilitating%20the%20creation%20of%0Adetailed%203D%20models%20from%20multi-view%20images.%20We%20also%20adopt%20a%0Atexture-back-projection%20strategy%20to%20produce%20high-quality%20texture%20maps.%0AAdditionally%2C%20we%20have%20curated%20a%20dataset%20of%20anime%20characters%2C%20rendered%20in%0Amultiple%20poses%20and%20views%2C%20to%20train%20and%20evaluate%20our%20model.%20Our%20approach%20has%0Abeen%20thoroughly%20evaluated%20through%20quantitative%20and%20qualitative%20experiments%2C%0Ashowing%20its%20proficiency%20in%20generating%203D%20characters%20with%20high-quality%20shapes%0Aand%20textures%2C%20ready%20for%20downstream%20applications%20such%20as%20rigging%20and%20animation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17214v3&entry.124074799=Read"},
{"title": "Generative Image as Action Models", "author": "Mohit Shridhar and Yat Long Lo and Stephen James", "abstract": "  Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, a\nbehavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'\nas targets on RGB images. These images are fed into a controller that maps the\nvisual targets into a sequence of joint-positions. We study GENIMA on 25\nRLBench and 9 real-world manipulation tasks. We find that, by lifting actions\ninto image-space, internet pre-trained diffusion models can generate policies\nthat outperform state-of-the-art visuomotor approaches, especially in\nrobustness to scene perturbations and generalizing to novel objects. Our method\nis also competitive with 3D agents, despite lacking priors such as depth,\nkeypoints, or motion-planners.\n", "link": "http://arxiv.org/abs/2407.07875v1", "date": "2024-07-10", "relevancy": 3.2136, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6904}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6277}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Image%20as%20Action%20Models&body=Title%3A%20Generative%20Image%20as%20Action%20Models%0AAuthor%3A%20Mohit%20Shridhar%20and%20Yat%20Long%20Lo%20and%20Stephen%20James%0AAbstract%3A%20%20%20Image-generation%20diffusion%20models%20have%20been%20fine-tuned%20to%20unlock%20new%0Acapabilities%20such%20as%20image-editing%20and%20novel%20view%20synthesis.%20Can%20we%20similarly%0Aunlock%20image-generation%20models%20for%20visuomotor%20control%3F%20We%20present%20GENIMA%2C%20a%0Abehavior-cloning%20agent%20that%20fine-tunes%20Stable%20Diffusion%20to%20%27draw%20joint-actions%27%0Aas%20targets%20on%20RGB%20images.%20These%20images%20are%20fed%20into%20a%20controller%20that%20maps%20the%0Avisual%20targets%20into%20a%20sequence%20of%20joint-positions.%20We%20study%20GENIMA%20on%2025%0ARLBench%20and%209%20real-world%20manipulation%20tasks.%20We%20find%20that%2C%20by%20lifting%20actions%0Ainto%20image-space%2C%20internet%20pre-trained%20diffusion%20models%20can%20generate%20policies%0Athat%20outperform%20state-of-the-art%20visuomotor%20approaches%2C%20especially%20in%0Arobustness%20to%20scene%20perturbations%20and%20generalizing%20to%20novel%20objects.%20Our%20method%0Ais%20also%20competitive%20with%203D%20agents%2C%20despite%20lacking%20priors%20such%20as%20depth%2C%0Akeypoints%2C%20or%20motion-planners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Image%2520as%2520Action%2520Models%26entry.906535625%3DMohit%2520Shridhar%2520and%2520Yat%2520Long%2520Lo%2520and%2520Stephen%2520James%26entry.1292438233%3D%2520%2520Image-generation%2520diffusion%2520models%2520have%2520been%2520fine-tuned%2520to%2520unlock%2520new%250Acapabilities%2520such%2520as%2520image-editing%2520and%2520novel%2520view%2520synthesis.%2520Can%2520we%2520similarly%250Aunlock%2520image-generation%2520models%2520for%2520visuomotor%2520control%253F%2520We%2520present%2520GENIMA%252C%2520a%250Abehavior-cloning%2520agent%2520that%2520fine-tunes%2520Stable%2520Diffusion%2520to%2520%2527draw%2520joint-actions%2527%250Aas%2520targets%2520on%2520RGB%2520images.%2520These%2520images%2520are%2520fed%2520into%2520a%2520controller%2520that%2520maps%2520the%250Avisual%2520targets%2520into%2520a%2520sequence%2520of%2520joint-positions.%2520We%2520study%2520GENIMA%2520on%252025%250ARLBench%2520and%25209%2520real-world%2520manipulation%2520tasks.%2520We%2520find%2520that%252C%2520by%2520lifting%2520actions%250Ainto%2520image-space%252C%2520internet%2520pre-trained%2520diffusion%2520models%2520can%2520generate%2520policies%250Athat%2520outperform%2520state-of-the-art%2520visuomotor%2520approaches%252C%2520especially%2520in%250Arobustness%2520to%2520scene%2520perturbations%2520and%2520generalizing%2520to%2520novel%2520objects.%2520Our%2520method%250Ais%2520also%2520competitive%2520with%25203D%2520agents%252C%2520despite%2520lacking%2520priors%2520such%2520as%2520depth%252C%250Akeypoints%252C%2520or%2520motion-planners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Image%20as%20Action%20Models&entry.906535625=Mohit%20Shridhar%20and%20Yat%20Long%20Lo%20and%20Stephen%20James&entry.1292438233=%20%20Image-generation%20diffusion%20models%20have%20been%20fine-tuned%20to%20unlock%20new%0Acapabilities%20such%20as%20image-editing%20and%20novel%20view%20synthesis.%20Can%20we%20similarly%0Aunlock%20image-generation%20models%20for%20visuomotor%20control%3F%20We%20present%20GENIMA%2C%20a%0Abehavior-cloning%20agent%20that%20fine-tunes%20Stable%20Diffusion%20to%20%27draw%20joint-actions%27%0Aas%20targets%20on%20RGB%20images.%20These%20images%20are%20fed%20into%20a%20controller%20that%20maps%20the%0Avisual%20targets%20into%20a%20sequence%20of%20joint-positions.%20We%20study%20GENIMA%20on%2025%0ARLBench%20and%209%20real-world%20manipulation%20tasks.%20We%20find%20that%2C%20by%20lifting%20actions%0Ainto%20image-space%2C%20internet%20pre-trained%20diffusion%20models%20can%20generate%20policies%0Athat%20outperform%20state-of-the-art%20visuomotor%20approaches%2C%20especially%20in%0Arobustness%20to%20scene%20perturbations%20and%20generalizing%20to%20novel%20objects.%20Our%20method%0Ais%20also%20competitive%20with%203D%20agents%2C%20despite%20lacking%20priors%20such%20as%20depth%2C%0Akeypoints%2C%20or%20motion-planners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07875v1&entry.124074799=Read"},
{"title": "Neural Localizer Fields for Continuous 3D Human Pose and Shape\n  Estimation", "author": "Istv\u00e1n S\u00e1r\u00e1ndi and Gerard Pons-Moll", "abstract": "  With the explosive growth of available training data, single-image 3D human\nmodeling is ahead of a transition to a data-centric paradigm. A key to\nsuccessfully exploiting data scale is to design flexible models that can be\nsupervised from various heterogeneous data sources produced by different\nresearchers or vendors. To this end, we propose a simple yet powerful paradigm\nfor seamlessly unifying different human pose and shape-related tasks and\ndatasets. Our formulation is centered on the ability - both at training and\ntest time - to query any arbitrary point of the human volume, and obtain its\nestimated location in 3D. We achieve this by learning a continuous neural field\nof body point localizer functions, each of which is a differently parameterized\n3D heatmap-based convolutional point localizer (detector). For generating\nparametric output, we propose an efficient post-processing step for fitting\nSMPL-family body models to nonparametric joint and vertex predictions. With\nthis approach, we can naturally exploit differently annotated data sources\nincluding mesh, 2D/3D skeleton and dense pose, without having to convert\nbetween them, and thereby train large-scale 3D human mesh and skeleton\nestimation models that outperform the state-of-the-art on several public\nbenchmarks including 3DPW, EMDB and SSP-3D by a considerable margin.\n", "link": "http://arxiv.org/abs/2407.07532v1", "date": "2024-07-10", "relevancy": 3.0069, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6385}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5863}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Localizer%20Fields%20for%20Continuous%203D%20Human%20Pose%20and%20Shape%0A%20%20Estimation&body=Title%3A%20Neural%20Localizer%20Fields%20for%20Continuous%203D%20Human%20Pose%20and%20Shape%0A%20%20Estimation%0AAuthor%3A%20Istv%C3%A1n%20S%C3%A1r%C3%A1ndi%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20%20%20With%20the%20explosive%20growth%20of%20available%20training%20data%2C%20single-image%203D%20human%0Amodeling%20is%20ahead%20of%20a%20transition%20to%20a%20data-centric%20paradigm.%20A%20key%20to%0Asuccessfully%20exploiting%20data%20scale%20is%20to%20design%20flexible%20models%20that%20can%20be%0Asupervised%20from%20various%20heterogeneous%20data%20sources%20produced%20by%20different%0Aresearchers%20or%20vendors.%20To%20this%20end%2C%20we%20propose%20a%20simple%20yet%20powerful%20paradigm%0Afor%20seamlessly%20unifying%20different%20human%20pose%20and%20shape-related%20tasks%20and%0Adatasets.%20Our%20formulation%20is%20centered%20on%20the%20ability%20-%20both%20at%20training%20and%0Atest%20time%20-%20to%20query%20any%20arbitrary%20point%20of%20the%20human%20volume%2C%20and%20obtain%20its%0Aestimated%20location%20in%203D.%20We%20achieve%20this%20by%20learning%20a%20continuous%20neural%20field%0Aof%20body%20point%20localizer%20functions%2C%20each%20of%20which%20is%20a%20differently%20parameterized%0A3D%20heatmap-based%20convolutional%20point%20localizer%20%28detector%29.%20For%20generating%0Aparametric%20output%2C%20we%20propose%20an%20efficient%20post-processing%20step%20for%20fitting%0ASMPL-family%20body%20models%20to%20nonparametric%20joint%20and%20vertex%20predictions.%20With%0Athis%20approach%2C%20we%20can%20naturally%20exploit%20differently%20annotated%20data%20sources%0Aincluding%20mesh%2C%202D/3D%20skeleton%20and%20dense%20pose%2C%20without%20having%20to%20convert%0Abetween%20them%2C%20and%20thereby%20train%20large-scale%203D%20human%20mesh%20and%20skeleton%0Aestimation%20models%20that%20outperform%20the%20state-of-the-art%20on%20several%20public%0Abenchmarks%20including%203DPW%2C%20EMDB%20and%20SSP-3D%20by%20a%20considerable%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Localizer%2520Fields%2520for%2520Continuous%25203D%2520Human%2520Pose%2520and%2520Shape%250A%2520%2520Estimation%26entry.906535625%3DIstv%25C3%25A1n%2520S%25C3%25A1r%25C3%25A1ndi%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3D%2520%2520With%2520the%2520explosive%2520growth%2520of%2520available%2520training%2520data%252C%2520single-image%25203D%2520human%250Amodeling%2520is%2520ahead%2520of%2520a%2520transition%2520to%2520a%2520data-centric%2520paradigm.%2520A%2520key%2520to%250Asuccessfully%2520exploiting%2520data%2520scale%2520is%2520to%2520design%2520flexible%2520models%2520that%2520can%2520be%250Asupervised%2520from%2520various%2520heterogeneous%2520data%2520sources%2520produced%2520by%2520different%250Aresearchers%2520or%2520vendors.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520simple%2520yet%2520powerful%2520paradigm%250Afor%2520seamlessly%2520unifying%2520different%2520human%2520pose%2520and%2520shape-related%2520tasks%2520and%250Adatasets.%2520Our%2520formulation%2520is%2520centered%2520on%2520the%2520ability%2520-%2520both%2520at%2520training%2520and%250Atest%2520time%2520-%2520to%2520query%2520any%2520arbitrary%2520point%2520of%2520the%2520human%2520volume%252C%2520and%2520obtain%2520its%250Aestimated%2520location%2520in%25203D.%2520We%2520achieve%2520this%2520by%2520learning%2520a%2520continuous%2520neural%2520field%250Aof%2520body%2520point%2520localizer%2520functions%252C%2520each%2520of%2520which%2520is%2520a%2520differently%2520parameterized%250A3D%2520heatmap-based%2520convolutional%2520point%2520localizer%2520%2528detector%2529.%2520For%2520generating%250Aparametric%2520output%252C%2520we%2520propose%2520an%2520efficient%2520post-processing%2520step%2520for%2520fitting%250ASMPL-family%2520body%2520models%2520to%2520nonparametric%2520joint%2520and%2520vertex%2520predictions.%2520With%250Athis%2520approach%252C%2520we%2520can%2520naturally%2520exploit%2520differently%2520annotated%2520data%2520sources%250Aincluding%2520mesh%252C%25202D/3D%2520skeleton%2520and%2520dense%2520pose%252C%2520without%2520having%2520to%2520convert%250Abetween%2520them%252C%2520and%2520thereby%2520train%2520large-scale%25203D%2520human%2520mesh%2520and%2520skeleton%250Aestimation%2520models%2520that%2520outperform%2520the%2520state-of-the-art%2520on%2520several%2520public%250Abenchmarks%2520including%25203DPW%252C%2520EMDB%2520and%2520SSP-3D%2520by%2520a%2520considerable%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Localizer%20Fields%20for%20Continuous%203D%20Human%20Pose%20and%20Shape%0A%20%20Estimation&entry.906535625=Istv%C3%A1n%20S%C3%A1r%C3%A1ndi%20and%20Gerard%20Pons-Moll&entry.1292438233=%20%20With%20the%20explosive%20growth%20of%20available%20training%20data%2C%20single-image%203D%20human%0Amodeling%20is%20ahead%20of%20a%20transition%20to%20a%20data-centric%20paradigm.%20A%20key%20to%0Asuccessfully%20exploiting%20data%20scale%20is%20to%20design%20flexible%20models%20that%20can%20be%0Asupervised%20from%20various%20heterogeneous%20data%20sources%20produced%20by%20different%0Aresearchers%20or%20vendors.%20To%20this%20end%2C%20we%20propose%20a%20simple%20yet%20powerful%20paradigm%0Afor%20seamlessly%20unifying%20different%20human%20pose%20and%20shape-related%20tasks%20and%0Adatasets.%20Our%20formulation%20is%20centered%20on%20the%20ability%20-%20both%20at%20training%20and%0Atest%20time%20-%20to%20query%20any%20arbitrary%20point%20of%20the%20human%20volume%2C%20and%20obtain%20its%0Aestimated%20location%20in%203D.%20We%20achieve%20this%20by%20learning%20a%20continuous%20neural%20field%0Aof%20body%20point%20localizer%20functions%2C%20each%20of%20which%20is%20a%20differently%20parameterized%0A3D%20heatmap-based%20convolutional%20point%20localizer%20%28detector%29.%20For%20generating%0Aparametric%20output%2C%20we%20propose%20an%20efficient%20post-processing%20step%20for%20fitting%0ASMPL-family%20body%20models%20to%20nonparametric%20joint%20and%20vertex%20predictions.%20With%0Athis%20approach%2C%20we%20can%20naturally%20exploit%20differently%20annotated%20data%20sources%0Aincluding%20mesh%2C%202D/3D%20skeleton%20and%20dense%20pose%2C%20without%20having%20to%20convert%0Abetween%20them%2C%20and%20thereby%20train%20large-scale%203D%20human%20mesh%20and%20skeleton%0Aestimation%20models%20that%20outperform%20the%20state-of-the-art%20on%20several%20public%0Abenchmarks%20including%203DPW%2C%20EMDB%20and%20SSP-3D%20by%20a%20considerable%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07532v1&entry.124074799=Read"},
{"title": "3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes", "author": "Nicolas Moenne-Loccoz and Ashkan Mirzaei and Or Perel and Riccardo de Lutio and Janick Martinez Esturo and Gavriel State and Sanja Fidler and Nicholas Sharp and Zan Gojcic", "abstract": "  Particle-based representations of radiance fields such as 3D Gaussian\nSplatting have found great success for reconstructing and re-rendering of\ncomplex scenes. Most existing methods render particles via rasterization,\nprojecting them to screen space tiles for processing in a sorted order. This\nwork instead considers ray tracing the particles, building a bounding volume\nhierarchy and casting a ray for each pixel using high-performance GPU ray\ntracing hardware. To efficiently handle large numbers of semi-transparent\nparticles, we describe a specialized rendering algorithm which encapsulates\nparticles with bounding meshes to leverage fast ray-triangle intersections, and\nshades batches of intersections in depth-order. The benefits of ray tracing are\nwell-known in computer graphics: processing incoherent rays for secondary\nlighting effects such as shadows and reflections, rendering from\nhighly-distorted cameras common in robotics, stochastically sampling rays, and\nmore. With our renderer, this flexibility comes at little cost compared to\nrasterization. Experiments demonstrate the speed and accuracy of our approach,\nas well as several applications in computer graphics and vision. We further\npropose related improvements to the basic Gaussian representation, including a\nsimple use of generalized kernel functions which significantly reduces particle\nhit counts.\n", "link": "http://arxiv.org/abs/2407.07090v2", "date": "2024-07-10", "relevancy": 2.9387, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6682}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5593}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Ray%20Tracing%3A%20Fast%20Tracing%20of%20Particle%20Scenes&body=Title%3A%203D%20Gaussian%20Ray%20Tracing%3A%20Fast%20Tracing%20of%20Particle%20Scenes%0AAuthor%3A%20Nicolas%20Moenne-Loccoz%20and%20Ashkan%20Mirzaei%20and%20Or%20Perel%20and%20Riccardo%20de%20Lutio%20and%20Janick%20Martinez%20Esturo%20and%20Gavriel%20State%20and%20Sanja%20Fidler%20and%20Nicholas%20Sharp%20and%20Zan%20Gojcic%0AAbstract%3A%20%20%20Particle-based%20representations%20of%20radiance%20fields%20such%20as%203D%20Gaussian%0ASplatting%20have%20found%20great%20success%20for%20reconstructing%20and%20re-rendering%20of%0Acomplex%20scenes.%20Most%20existing%20methods%20render%20particles%20via%20rasterization%2C%0Aprojecting%20them%20to%20screen%20space%20tiles%20for%20processing%20in%20a%20sorted%20order.%20This%0Awork%20instead%20considers%20ray%20tracing%20the%20particles%2C%20building%20a%20bounding%20volume%0Ahierarchy%20and%20casting%20a%20ray%20for%20each%20pixel%20using%20high-performance%20GPU%20ray%0Atracing%20hardware.%20To%20efficiently%20handle%20large%20numbers%20of%20semi-transparent%0Aparticles%2C%20we%20describe%20a%20specialized%20rendering%20algorithm%20which%20encapsulates%0Aparticles%20with%20bounding%20meshes%20to%20leverage%20fast%20ray-triangle%20intersections%2C%20and%0Ashades%20batches%20of%20intersections%20in%20depth-order.%20The%20benefits%20of%20ray%20tracing%20are%0Awell-known%20in%20computer%20graphics%3A%20processing%20incoherent%20rays%20for%20secondary%0Alighting%20effects%20such%20as%20shadows%20and%20reflections%2C%20rendering%20from%0Ahighly-distorted%20cameras%20common%20in%20robotics%2C%20stochastically%20sampling%20rays%2C%20and%0Amore.%20With%20our%20renderer%2C%20this%20flexibility%20comes%20at%20little%20cost%20compared%20to%0Arasterization.%20Experiments%20demonstrate%20the%20speed%20and%20accuracy%20of%20our%20approach%2C%0Aas%20well%20as%20several%20applications%20in%20computer%20graphics%20and%20vision.%20We%20further%0Apropose%20related%20improvements%20to%20the%20basic%20Gaussian%20representation%2C%20including%20a%0Asimple%20use%20of%20generalized%20kernel%20functions%20which%20significantly%20reduces%20particle%0Ahit%20counts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520Ray%2520Tracing%253A%2520Fast%2520Tracing%2520of%2520Particle%2520Scenes%26entry.906535625%3DNicolas%2520Moenne-Loccoz%2520and%2520Ashkan%2520Mirzaei%2520and%2520Or%2520Perel%2520and%2520Riccardo%2520de%2520Lutio%2520and%2520Janick%2520Martinez%2520Esturo%2520and%2520Gavriel%2520State%2520and%2520Sanja%2520Fidler%2520and%2520Nicholas%2520Sharp%2520and%2520Zan%2520Gojcic%26entry.1292438233%3D%2520%2520Particle-based%2520representations%2520of%2520radiance%2520fields%2520such%2520as%25203D%2520Gaussian%250ASplatting%2520have%2520found%2520great%2520success%2520for%2520reconstructing%2520and%2520re-rendering%2520of%250Acomplex%2520scenes.%2520Most%2520existing%2520methods%2520render%2520particles%2520via%2520rasterization%252C%250Aprojecting%2520them%2520to%2520screen%2520space%2520tiles%2520for%2520processing%2520in%2520a%2520sorted%2520order.%2520This%250Awork%2520instead%2520considers%2520ray%2520tracing%2520the%2520particles%252C%2520building%2520a%2520bounding%2520volume%250Ahierarchy%2520and%2520casting%2520a%2520ray%2520for%2520each%2520pixel%2520using%2520high-performance%2520GPU%2520ray%250Atracing%2520hardware.%2520To%2520efficiently%2520handle%2520large%2520numbers%2520of%2520semi-transparent%250Aparticles%252C%2520we%2520describe%2520a%2520specialized%2520rendering%2520algorithm%2520which%2520encapsulates%250Aparticles%2520with%2520bounding%2520meshes%2520to%2520leverage%2520fast%2520ray-triangle%2520intersections%252C%2520and%250Ashades%2520batches%2520of%2520intersections%2520in%2520depth-order.%2520The%2520benefits%2520of%2520ray%2520tracing%2520are%250Awell-known%2520in%2520computer%2520graphics%253A%2520processing%2520incoherent%2520rays%2520for%2520secondary%250Alighting%2520effects%2520such%2520as%2520shadows%2520and%2520reflections%252C%2520rendering%2520from%250Ahighly-distorted%2520cameras%2520common%2520in%2520robotics%252C%2520stochastically%2520sampling%2520rays%252C%2520and%250Amore.%2520With%2520our%2520renderer%252C%2520this%2520flexibility%2520comes%2520at%2520little%2520cost%2520compared%2520to%250Arasterization.%2520Experiments%2520demonstrate%2520the%2520speed%2520and%2520accuracy%2520of%2520our%2520approach%252C%250Aas%2520well%2520as%2520several%2520applications%2520in%2520computer%2520graphics%2520and%2520vision.%2520We%2520further%250Apropose%2520related%2520improvements%2520to%2520the%2520basic%2520Gaussian%2520representation%252C%2520including%2520a%250Asimple%2520use%2520of%2520generalized%2520kernel%2520functions%2520which%2520significantly%2520reduces%2520particle%250Ahit%2520counts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Ray%20Tracing%3A%20Fast%20Tracing%20of%20Particle%20Scenes&entry.906535625=Nicolas%20Moenne-Loccoz%20and%20Ashkan%20Mirzaei%20and%20Or%20Perel%20and%20Riccardo%20de%20Lutio%20and%20Janick%20Martinez%20Esturo%20and%20Gavriel%20State%20and%20Sanja%20Fidler%20and%20Nicholas%20Sharp%20and%20Zan%20Gojcic&entry.1292438233=%20%20Particle-based%20representations%20of%20radiance%20fields%20such%20as%203D%20Gaussian%0ASplatting%20have%20found%20great%20success%20for%20reconstructing%20and%20re-rendering%20of%0Acomplex%20scenes.%20Most%20existing%20methods%20render%20particles%20via%20rasterization%2C%0Aprojecting%20them%20to%20screen%20space%20tiles%20for%20processing%20in%20a%20sorted%20order.%20This%0Awork%20instead%20considers%20ray%20tracing%20the%20particles%2C%20building%20a%20bounding%20volume%0Ahierarchy%20and%20casting%20a%20ray%20for%20each%20pixel%20using%20high-performance%20GPU%20ray%0Atracing%20hardware.%20To%20efficiently%20handle%20large%20numbers%20of%20semi-transparent%0Aparticles%2C%20we%20describe%20a%20specialized%20rendering%20algorithm%20which%20encapsulates%0Aparticles%20with%20bounding%20meshes%20to%20leverage%20fast%20ray-triangle%20intersections%2C%20and%0Ashades%20batches%20of%20intersections%20in%20depth-order.%20The%20benefits%20of%20ray%20tracing%20are%0Awell-known%20in%20computer%20graphics%3A%20processing%20incoherent%20rays%20for%20secondary%0Alighting%20effects%20such%20as%20shadows%20and%20reflections%2C%20rendering%20from%0Ahighly-distorted%20cameras%20common%20in%20robotics%2C%20stochastically%20sampling%20rays%2C%20and%0Amore.%20With%20our%20renderer%2C%20this%20flexibility%20comes%20at%20little%20cost%20compared%20to%0Arasterization.%20Experiments%20demonstrate%20the%20speed%20and%20accuracy%20of%20our%20approach%2C%0Aas%20well%20as%20several%20applications%20in%20computer%20graphics%20and%20vision.%20We%20further%0Apropose%20related%20improvements%20to%20the%20basic%20Gaussian%20representation%2C%20including%20a%0Asimple%20use%20of%20generalized%20kernel%20functions%20which%20significantly%20reduces%20particle%0Ahit%20counts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07090v2&entry.124074799=Read"},
{"title": "ColonMapper: topological mapping and localization for colonoscopy", "author": "Javier Morlana and Juan D. Tard\u00f3s and J. M. M. Montiel", "abstract": "  We propose a topological mapping and localization system able to operate on\nreal human colonoscopies, despite significant shape and illumination changes.\nThe map is a graph where each node codes a colon location by a set of real\nimages, while edges represent traversability between nodes. For close-in-time\nimages, where scene changes are minor, place recognition can be successfully\nmanaged with the recent transformers-based local feature matching algorithms.\nHowever, under long-term changes -- such as different colonoscopies of the same\npatient -- feature-based matching fails. To address this, we train on real\ncolonoscopies a deep global descriptor achieving high recall with significant\nchanges in the scene. The addition of a Bayesian filter boosts the accuracy of\nlong-term place recognition, enabling relocalization in a previously built map.\nOur experiments show that ColonMapper is able to autonomously build a map and\nlocalize against it in two important use cases: localization within the same\ncolonoscopy or within different colonoscopies of the same patient. Code:\nhttps://github.com/jmorlana/ColonMapper.\n", "link": "http://arxiv.org/abs/2305.05546v3", "date": "2024-07-10", "relevancy": 2.7906, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5771}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.573}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ColonMapper%3A%20topological%20mapping%20and%20localization%20for%20colonoscopy&body=Title%3A%20ColonMapper%3A%20topological%20mapping%20and%20localization%20for%20colonoscopy%0AAuthor%3A%20Javier%20Morlana%20and%20Juan%20D.%20Tard%C3%B3s%20and%20J.%20M.%20M.%20Montiel%0AAbstract%3A%20%20%20We%20propose%20a%20topological%20mapping%20and%20localization%20system%20able%20to%20operate%20on%0Areal%20human%20colonoscopies%2C%20despite%20significant%20shape%20and%20illumination%20changes.%0AThe%20map%20is%20a%20graph%20where%20each%20node%20codes%20a%20colon%20location%20by%20a%20set%20of%20real%0Aimages%2C%20while%20edges%20represent%20traversability%20between%20nodes.%20For%20close-in-time%0Aimages%2C%20where%20scene%20changes%20are%20minor%2C%20place%20recognition%20can%20be%20successfully%0Amanaged%20with%20the%20recent%20transformers-based%20local%20feature%20matching%20algorithms.%0AHowever%2C%20under%20long-term%20changes%20--%20such%20as%20different%20colonoscopies%20of%20the%20same%0Apatient%20--%20feature-based%20matching%20fails.%20To%20address%20this%2C%20we%20train%20on%20real%0Acolonoscopies%20a%20deep%20global%20descriptor%20achieving%20high%20recall%20with%20significant%0Achanges%20in%20the%20scene.%20The%20addition%20of%20a%20Bayesian%20filter%20boosts%20the%20accuracy%20of%0Along-term%20place%20recognition%2C%20enabling%20relocalization%20in%20a%20previously%20built%20map.%0AOur%20experiments%20show%20that%20ColonMapper%20is%20able%20to%20autonomously%20build%20a%20map%20and%0Alocalize%20against%20it%20in%20two%20important%20use%20cases%3A%20localization%20within%20the%20same%0Acolonoscopy%20or%20within%20different%20colonoscopies%20of%20the%20same%20patient.%20Code%3A%0Ahttps%3A//github.com/jmorlana/ColonMapper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.05546v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColonMapper%253A%2520topological%2520mapping%2520and%2520localization%2520for%2520colonoscopy%26entry.906535625%3DJavier%2520Morlana%2520and%2520Juan%2520D.%2520Tard%25C3%25B3s%2520and%2520J.%2520M.%2520M.%2520Montiel%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520topological%2520mapping%2520and%2520localization%2520system%2520able%2520to%2520operate%2520on%250Areal%2520human%2520colonoscopies%252C%2520despite%2520significant%2520shape%2520and%2520illumination%2520changes.%250AThe%2520map%2520is%2520a%2520graph%2520where%2520each%2520node%2520codes%2520a%2520colon%2520location%2520by%2520a%2520set%2520of%2520real%250Aimages%252C%2520while%2520edges%2520represent%2520traversability%2520between%2520nodes.%2520For%2520close-in-time%250Aimages%252C%2520where%2520scene%2520changes%2520are%2520minor%252C%2520place%2520recognition%2520can%2520be%2520successfully%250Amanaged%2520with%2520the%2520recent%2520transformers-based%2520local%2520feature%2520matching%2520algorithms.%250AHowever%252C%2520under%2520long-term%2520changes%2520--%2520such%2520as%2520different%2520colonoscopies%2520of%2520the%2520same%250Apatient%2520--%2520feature-based%2520matching%2520fails.%2520To%2520address%2520this%252C%2520we%2520train%2520on%2520real%250Acolonoscopies%2520a%2520deep%2520global%2520descriptor%2520achieving%2520high%2520recall%2520with%2520significant%250Achanges%2520in%2520the%2520scene.%2520The%2520addition%2520of%2520a%2520Bayesian%2520filter%2520boosts%2520the%2520accuracy%2520of%250Along-term%2520place%2520recognition%252C%2520enabling%2520relocalization%2520in%2520a%2520previously%2520built%2520map.%250AOur%2520experiments%2520show%2520that%2520ColonMapper%2520is%2520able%2520to%2520autonomously%2520build%2520a%2520map%2520and%250Alocalize%2520against%2520it%2520in%2520two%2520important%2520use%2520cases%253A%2520localization%2520within%2520the%2520same%250Acolonoscopy%2520or%2520within%2520different%2520colonoscopies%2520of%2520the%2520same%2520patient.%2520Code%253A%250Ahttps%253A//github.com/jmorlana/ColonMapper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.05546v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColonMapper%3A%20topological%20mapping%20and%20localization%20for%20colonoscopy&entry.906535625=Javier%20Morlana%20and%20Juan%20D.%20Tard%C3%B3s%20and%20J.%20M.%20M.%20Montiel&entry.1292438233=%20%20We%20propose%20a%20topological%20mapping%20and%20localization%20system%20able%20to%20operate%20on%0Areal%20human%20colonoscopies%2C%20despite%20significant%20shape%20and%20illumination%20changes.%0AThe%20map%20is%20a%20graph%20where%20each%20node%20codes%20a%20colon%20location%20by%20a%20set%20of%20real%0Aimages%2C%20while%20edges%20represent%20traversability%20between%20nodes.%20For%20close-in-time%0Aimages%2C%20where%20scene%20changes%20are%20minor%2C%20place%20recognition%20can%20be%20successfully%0Amanaged%20with%20the%20recent%20transformers-based%20local%20feature%20matching%20algorithms.%0AHowever%2C%20under%20long-term%20changes%20--%20such%20as%20different%20colonoscopies%20of%20the%20same%0Apatient%20--%20feature-based%20matching%20fails.%20To%20address%20this%2C%20we%20train%20on%20real%0Acolonoscopies%20a%20deep%20global%20descriptor%20achieving%20high%20recall%20with%20significant%0Achanges%20in%20the%20scene.%20The%20addition%20of%20a%20Bayesian%20filter%20boosts%20the%20accuracy%20of%0Along-term%20place%20recognition%2C%20enabling%20relocalization%20in%20a%20previously%20built%20map.%0AOur%20experiments%20show%20that%20ColonMapper%20is%20able%20to%20autonomously%20build%20a%20map%20and%0Alocalize%20against%20it%20in%20two%20important%20use%20cases%3A%20localization%20within%20the%20same%0Acolonoscopy%20or%20within%20different%20colonoscopies%20of%20the%20same%20patient.%20Code%3A%0Ahttps%3A//github.com/jmorlana/ColonMapper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.05546v3&entry.124074799=Read"},
{"title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language\n  Model", "author": "Yatai Ji and Shilong Zhang and Jie Wu and Peize Sun and Weifeng Chen and Xuefeng Xiao and Sidi Yang and Yujiu Yang and Ping Luo", "abstract": "  The rapid advancement of Large Vision-Language models (LVLMs) has\ndemonstrated a spectrum of emergent capabilities. Nevertheless, current models\nonly focus on the visual content of a single scenario, while their ability to\nassociate instances across different scenes has not yet been explored, which is\nessential for understanding complex visual content, such as movies with\nmultiple characters and intricate plots. Towards movie understanding, a\ncritical initial step for LVLMs is to unleash the potential of character\nidentities memory and recognition across multiple visual scenarios. To achieve\nthe goal, we propose visual instruction tuning with ID reference and develop an\nID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our research\nintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory and\nrecognition across four dimensions: matching, location, question-answering, and\ncaptioning. Our findings highlight the limitations of existing LVLMs in\nrecognizing and associating instance identities with ID reference. This paper\npaves the way for future artificial intelligence systems to possess\nmulti-identity visual inputs, thereby facilitating the comprehension of complex\nvisual narratives like movies.\n", "link": "http://arxiv.org/abs/2407.07577v1", "date": "2024-07-10", "relevancy": 2.7877, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5866}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDA-VLM%3A%20Towards%20Movie%20Understanding%20via%20ID-Aware%20Large%20Vision-Language%0A%20%20Model&body=Title%3A%20IDA-VLM%3A%20Towards%20Movie%20Understanding%20via%20ID-Aware%20Large%20Vision-Language%0A%20%20Model%0AAuthor%3A%20Yatai%20Ji%20and%20Shilong%20Zhang%20and%20Jie%20Wu%20and%20Peize%20Sun%20and%20Weifeng%20Chen%20and%20Xuefeng%20Xiao%20and%20Sidi%20Yang%20and%20Yujiu%20Yang%20and%20Ping%20Luo%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20Vision-Language%20models%20%28LVLMs%29%20has%0Ademonstrated%20a%20spectrum%20of%20emergent%20capabilities.%20Nevertheless%2C%20current%20models%0Aonly%20focus%20on%20the%20visual%20content%20of%20a%20single%20scenario%2C%20while%20their%20ability%20to%0Aassociate%20instances%20across%20different%20scenes%20has%20not%20yet%20been%20explored%2C%20which%20is%0Aessential%20for%20understanding%20complex%20visual%20content%2C%20such%20as%20movies%20with%0Amultiple%20characters%20and%20intricate%20plots.%20Towards%20movie%20understanding%2C%20a%0Acritical%20initial%20step%20for%20LVLMs%20is%20to%20unleash%20the%20potential%20of%20character%0Aidentities%20memory%20and%20recognition%20across%20multiple%20visual%20scenarios.%20To%20achieve%0Athe%20goal%2C%20we%20propose%20visual%20instruction%20tuning%20with%20ID%20reference%20and%20develop%20an%0AID-Aware%20Large%20Vision-Language%20Model%2C%20IDA-VLM.%20Furthermore%2C%20our%20research%0Aintroduces%20a%20novel%20benchmark%20MM-ID%2C%20to%20examine%20LVLMs%20on%20instance%20IDs%20memory%20and%0Arecognition%20across%20four%20dimensions%3A%20matching%2C%20location%2C%20question-answering%2C%20and%0Acaptioning.%20Our%20findings%20highlight%20the%20limitations%20of%20existing%20LVLMs%20in%0Arecognizing%20and%20associating%20instance%20identities%20with%20ID%20reference.%20This%20paper%0Apaves%20the%20way%20for%20future%20artificial%20intelligence%20systems%20to%20possess%0Amulti-identity%20visual%20inputs%2C%20thereby%20facilitating%20the%20comprehension%20of%20complex%0Avisual%20narratives%20like%20movies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDA-VLM%253A%2520Towards%2520Movie%2520Understanding%2520via%2520ID-Aware%2520Large%2520Vision-Language%250A%2520%2520Model%26entry.906535625%3DYatai%2520Ji%2520and%2520Shilong%2520Zhang%2520and%2520Jie%2520Wu%2520and%2520Peize%2520Sun%2520and%2520Weifeng%2520Chen%2520and%2520Xuefeng%2520Xiao%2520and%2520Sidi%2520Yang%2520and%2520Yujiu%2520Yang%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Large%2520Vision-Language%2520models%2520%2528LVLMs%2529%2520has%250Ademonstrated%2520a%2520spectrum%2520of%2520emergent%2520capabilities.%2520Nevertheless%252C%2520current%2520models%250Aonly%2520focus%2520on%2520the%2520visual%2520content%2520of%2520a%2520single%2520scenario%252C%2520while%2520their%2520ability%2520to%250Aassociate%2520instances%2520across%2520different%2520scenes%2520has%2520not%2520yet%2520been%2520explored%252C%2520which%2520is%250Aessential%2520for%2520understanding%2520complex%2520visual%2520content%252C%2520such%2520as%2520movies%2520with%250Amultiple%2520characters%2520and%2520intricate%2520plots.%2520Towards%2520movie%2520understanding%252C%2520a%250Acritical%2520initial%2520step%2520for%2520LVLMs%2520is%2520to%2520unleash%2520the%2520potential%2520of%2520character%250Aidentities%2520memory%2520and%2520recognition%2520across%2520multiple%2520visual%2520scenarios.%2520To%2520achieve%250Athe%2520goal%252C%2520we%2520propose%2520visual%2520instruction%2520tuning%2520with%2520ID%2520reference%2520and%2520develop%2520an%250AID-Aware%2520Large%2520Vision-Language%2520Model%252C%2520IDA-VLM.%2520Furthermore%252C%2520our%2520research%250Aintroduces%2520a%2520novel%2520benchmark%2520MM-ID%252C%2520to%2520examine%2520LVLMs%2520on%2520instance%2520IDs%2520memory%2520and%250Arecognition%2520across%2520four%2520dimensions%253A%2520matching%252C%2520location%252C%2520question-answering%252C%2520and%250Acaptioning.%2520Our%2520findings%2520highlight%2520the%2520limitations%2520of%2520existing%2520LVLMs%2520in%250Arecognizing%2520and%2520associating%2520instance%2520identities%2520with%2520ID%2520reference.%2520This%2520paper%250Apaves%2520the%2520way%2520for%2520future%2520artificial%2520intelligence%2520systems%2520to%2520possess%250Amulti-identity%2520visual%2520inputs%252C%2520thereby%2520facilitating%2520the%2520comprehension%2520of%2520complex%250Avisual%2520narratives%2520like%2520movies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDA-VLM%3A%20Towards%20Movie%20Understanding%20via%20ID-Aware%20Large%20Vision-Language%0A%20%20Model&entry.906535625=Yatai%20Ji%20and%20Shilong%20Zhang%20and%20Jie%20Wu%20and%20Peize%20Sun%20and%20Weifeng%20Chen%20and%20Xuefeng%20Xiao%20and%20Sidi%20Yang%20and%20Yujiu%20Yang%20and%20Ping%20Luo&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20Vision-Language%20models%20%28LVLMs%29%20has%0Ademonstrated%20a%20spectrum%20of%20emergent%20capabilities.%20Nevertheless%2C%20current%20models%0Aonly%20focus%20on%20the%20visual%20content%20of%20a%20single%20scenario%2C%20while%20their%20ability%20to%0Aassociate%20instances%20across%20different%20scenes%20has%20not%20yet%20been%20explored%2C%20which%20is%0Aessential%20for%20understanding%20complex%20visual%20content%2C%20such%20as%20movies%20with%0Amultiple%20characters%20and%20intricate%20plots.%20Towards%20movie%20understanding%2C%20a%0Acritical%20initial%20step%20for%20LVLMs%20is%20to%20unleash%20the%20potential%20of%20character%0Aidentities%20memory%20and%20recognition%20across%20multiple%20visual%20scenarios.%20To%20achieve%0Athe%20goal%2C%20we%20propose%20visual%20instruction%20tuning%20with%20ID%20reference%20and%20develop%20an%0AID-Aware%20Large%20Vision-Language%20Model%2C%20IDA-VLM.%20Furthermore%2C%20our%20research%0Aintroduces%20a%20novel%20benchmark%20MM-ID%2C%20to%20examine%20LVLMs%20on%20instance%20IDs%20memory%20and%0Arecognition%20across%20four%20dimensions%3A%20matching%2C%20location%2C%20question-answering%2C%20and%0Acaptioning.%20Our%20findings%20highlight%20the%20limitations%20of%20existing%20LVLMs%20in%0Arecognizing%20and%20associating%20instance%20identities%20with%20ID%20reference.%20This%20paper%0Apaves%20the%20way%20for%20future%20artificial%20intelligence%20systems%20to%20possess%0Amulti-identity%20visual%20inputs%2C%20thereby%20facilitating%20the%20comprehension%20of%20complex%0Avisual%20narratives%20like%20movies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07577v1&entry.124074799=Read"},
{"title": "Incremental Multiview Point Cloud Registration with Two-stage Candidate\n  Retrieval", "author": "Shiqi Li and Jihua Zhu and Yifan Xie and Mingchen Zhu", "abstract": "  Multiview point cloud registration serves as a cornerstone of various\ncomputer vision tasks. Previous approaches typically adhere to a global\nparadigm, where a pose graph is initially constructed followed by motion\nsynchronization to determine the absolute pose. However, this separated\napproach may not fully leverage the characteristics of multiview registration\nand might struggle with low-overlap scenarios. In this paper, we propose an\nincremental multiview point cloud registration method that progressively\nregisters all scans to a growing meta-shape. To determine the incremental\nordering, we employ a two-stage coarse-to-fine strategy for point cloud\ncandidate retrieval. The first stage involves the coarse selection of scans\nbased on neighbor fusion-enhanced global aggregation features, while the second\nstage further reranks candidates through geometric-based matching.\nAdditionally, we apply a transformation averaging technique to mitigate\naccumulated errors during the registration process. Finally, we utilize a\nReservoir sampling-based technique to address density variance issues while\nreducing computational load. Comprehensive experimental results across various\nbenchmarks validate the effectiveness and generalization of our approach.\n", "link": "http://arxiv.org/abs/2407.07525v1", "date": "2024-07-10", "relevancy": 2.7569, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5681}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5521}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incremental%20Multiview%20Point%20Cloud%20Registration%20with%20Two-stage%20Candidate%0A%20%20Retrieval&body=Title%3A%20Incremental%20Multiview%20Point%20Cloud%20Registration%20with%20Two-stage%20Candidate%0A%20%20Retrieval%0AAuthor%3A%20Shiqi%20Li%20and%20Jihua%20Zhu%20and%20Yifan%20Xie%20and%20Mingchen%20Zhu%0AAbstract%3A%20%20%20Multiview%20point%20cloud%20registration%20serves%20as%20a%20cornerstone%20of%20various%0Acomputer%20vision%20tasks.%20Previous%20approaches%20typically%20adhere%20to%20a%20global%0Aparadigm%2C%20where%20a%20pose%20graph%20is%20initially%20constructed%20followed%20by%20motion%0Asynchronization%20to%20determine%20the%20absolute%20pose.%20However%2C%20this%20separated%0Aapproach%20may%20not%20fully%20leverage%20the%20characteristics%20of%20multiview%20registration%0Aand%20might%20struggle%20with%20low-overlap%20scenarios.%20In%20this%20paper%2C%20we%20propose%20an%0Aincremental%20multiview%20point%20cloud%20registration%20method%20that%20progressively%0Aregisters%20all%20scans%20to%20a%20growing%20meta-shape.%20To%20determine%20the%20incremental%0Aordering%2C%20we%20employ%20a%20two-stage%20coarse-to-fine%20strategy%20for%20point%20cloud%0Acandidate%20retrieval.%20The%20first%20stage%20involves%20the%20coarse%20selection%20of%20scans%0Abased%20on%20neighbor%20fusion-enhanced%20global%20aggregation%20features%2C%20while%20the%20second%0Astage%20further%20reranks%20candidates%20through%20geometric-based%20matching.%0AAdditionally%2C%20we%20apply%20a%20transformation%20averaging%20technique%20to%20mitigate%0Aaccumulated%20errors%20during%20the%20registration%20process.%20Finally%2C%20we%20utilize%20a%0AReservoir%20sampling-based%20technique%20to%20address%20density%20variance%20issues%20while%0Areducing%20computational%20load.%20Comprehensive%20experimental%20results%20across%20various%0Abenchmarks%20validate%20the%20effectiveness%20and%20generalization%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncremental%2520Multiview%2520Point%2520Cloud%2520Registration%2520with%2520Two-stage%2520Candidate%250A%2520%2520Retrieval%26entry.906535625%3DShiqi%2520Li%2520and%2520Jihua%2520Zhu%2520and%2520Yifan%2520Xie%2520and%2520Mingchen%2520Zhu%26entry.1292438233%3D%2520%2520Multiview%2520point%2520cloud%2520registration%2520serves%2520as%2520a%2520cornerstone%2520of%2520various%250Acomputer%2520vision%2520tasks.%2520Previous%2520approaches%2520typically%2520adhere%2520to%2520a%2520global%250Aparadigm%252C%2520where%2520a%2520pose%2520graph%2520is%2520initially%2520constructed%2520followed%2520by%2520motion%250Asynchronization%2520to%2520determine%2520the%2520absolute%2520pose.%2520However%252C%2520this%2520separated%250Aapproach%2520may%2520not%2520fully%2520leverage%2520the%2520characteristics%2520of%2520multiview%2520registration%250Aand%2520might%2520struggle%2520with%2520low-overlap%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aincremental%2520multiview%2520point%2520cloud%2520registration%2520method%2520that%2520progressively%250Aregisters%2520all%2520scans%2520to%2520a%2520growing%2520meta-shape.%2520To%2520determine%2520the%2520incremental%250Aordering%252C%2520we%2520employ%2520a%2520two-stage%2520coarse-to-fine%2520strategy%2520for%2520point%2520cloud%250Acandidate%2520retrieval.%2520The%2520first%2520stage%2520involves%2520the%2520coarse%2520selection%2520of%2520scans%250Abased%2520on%2520neighbor%2520fusion-enhanced%2520global%2520aggregation%2520features%252C%2520while%2520the%2520second%250Astage%2520further%2520reranks%2520candidates%2520through%2520geometric-based%2520matching.%250AAdditionally%252C%2520we%2520apply%2520a%2520transformation%2520averaging%2520technique%2520to%2520mitigate%250Aaccumulated%2520errors%2520during%2520the%2520registration%2520process.%2520Finally%252C%2520we%2520utilize%2520a%250AReservoir%2520sampling-based%2520technique%2520to%2520address%2520density%2520variance%2520issues%2520while%250Areducing%2520computational%2520load.%2520Comprehensive%2520experimental%2520results%2520across%2520various%250Abenchmarks%2520validate%2520the%2520effectiveness%2520and%2520generalization%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Multiview%20Point%20Cloud%20Registration%20with%20Two-stage%20Candidate%0A%20%20Retrieval&entry.906535625=Shiqi%20Li%20and%20Jihua%20Zhu%20and%20Yifan%20Xie%20and%20Mingchen%20Zhu&entry.1292438233=%20%20Multiview%20point%20cloud%20registration%20serves%20as%20a%20cornerstone%20of%20various%0Acomputer%20vision%20tasks.%20Previous%20approaches%20typically%20adhere%20to%20a%20global%0Aparadigm%2C%20where%20a%20pose%20graph%20is%20initially%20constructed%20followed%20by%20motion%0Asynchronization%20to%20determine%20the%20absolute%20pose.%20However%2C%20this%20separated%0Aapproach%20may%20not%20fully%20leverage%20the%20characteristics%20of%20multiview%20registration%0Aand%20might%20struggle%20with%20low-overlap%20scenarios.%20In%20this%20paper%2C%20we%20propose%20an%0Aincremental%20multiview%20point%20cloud%20registration%20method%20that%20progressively%0Aregisters%20all%20scans%20to%20a%20growing%20meta-shape.%20To%20determine%20the%20incremental%0Aordering%2C%20we%20employ%20a%20two-stage%20coarse-to-fine%20strategy%20for%20point%20cloud%0Acandidate%20retrieval.%20The%20first%20stage%20involves%20the%20coarse%20selection%20of%20scans%0Abased%20on%20neighbor%20fusion-enhanced%20global%20aggregation%20features%2C%20while%20the%20second%0Astage%20further%20reranks%20candidates%20through%20geometric-based%20matching.%0AAdditionally%2C%20we%20apply%20a%20transformation%20averaging%20technique%20to%20mitigate%0Aaccumulated%20errors%20during%20the%20registration%20process.%20Finally%2C%20we%20utilize%20a%0AReservoir%20sampling-based%20technique%20to%20address%20density%20variance%20issues%20while%0Areducing%20computational%20load.%20Comprehensive%20experimental%20results%20across%20various%0Abenchmarks%20validate%20the%20effectiveness%20and%20generalization%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07525v1&entry.124074799=Read"},
{"title": "TIP: Tabular-Image Pre-training for Multimodal Classification with\n  Incomplete Data", "author": "Siyi Du and Shaoming Zheng and Yinsong Wang and Wenjia Bai and Declan P. O'Regan and Chen Qin", "abstract": "  Images and structured tables are essential parts of real-world databases.\nThough tabular-image representation learning is promising to create new\ninsights, it remains a challenging task, as tabular data is typically\nheterogeneous and incomplete, presenting significant modality disparities with\nimages. Earlier works have mainly focused on simple modality fusion strategies\nin complete data scenarios, without considering the missing data issue, and\nthus are limited in practice. In this paper, we propose TIP, a novel\ntabular-image pre-training framework for learning multimodal representations\nrobust to incomplete tabular data. Specifically, TIP investigates a novel\nself-supervised learning (SSL) strategy, including a masked tabular\nreconstruction task for tackling data missingness, and image-tabular matching\nand contrastive learning objectives to capture multimodal information.\nMoreover, TIP proposes a versatile tabular encoder tailored for incomplete,\nheterogeneous tabular data and a multimodal interaction module for\ninter-modality representation learning. Experiments are performed on downstream\nmultimodal classification tasks using both natural and medical image datasets.\nThe results show that TIP outperforms state-of-the-art supervised/SSL\nimage/multimodal algorithms in both complete and incomplete data scenarios. Our\ncode is available at https://github.com/siyi-wind/TIP.\n", "link": "http://arxiv.org/abs/2407.07582v1", "date": "2024-07-10", "relevancy": 2.7499, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6285}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5236}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIP%3A%20Tabular-Image%20Pre-training%20for%20Multimodal%20Classification%20with%0A%20%20Incomplete%20Data&body=Title%3A%20TIP%3A%20Tabular-Image%20Pre-training%20for%20Multimodal%20Classification%20with%0A%20%20Incomplete%20Data%0AAuthor%3A%20Siyi%20Du%20and%20Shaoming%20Zheng%20and%20Yinsong%20Wang%20and%20Wenjia%20Bai%20and%20Declan%20P.%20O%27Regan%20and%20Chen%20Qin%0AAbstract%3A%20%20%20Images%20and%20structured%20tables%20are%20essential%20parts%20of%20real-world%20databases.%0AThough%20tabular-image%20representation%20learning%20is%20promising%20to%20create%20new%0Ainsights%2C%20it%20remains%20a%20challenging%20task%2C%20as%20tabular%20data%20is%20typically%0Aheterogeneous%20and%20incomplete%2C%20presenting%20significant%20modality%20disparities%20with%0Aimages.%20Earlier%20works%20have%20mainly%20focused%20on%20simple%20modality%20fusion%20strategies%0Ain%20complete%20data%20scenarios%2C%20without%20considering%20the%20missing%20data%20issue%2C%20and%0Athus%20are%20limited%20in%20practice.%20In%20this%20paper%2C%20we%20propose%20TIP%2C%20a%20novel%0Atabular-image%20pre-training%20framework%20for%20learning%20multimodal%20representations%0Arobust%20to%20incomplete%20tabular%20data.%20Specifically%2C%20TIP%20investigates%20a%20novel%0Aself-supervised%20learning%20%28SSL%29%20strategy%2C%20including%20a%20masked%20tabular%0Areconstruction%20task%20for%20tackling%20data%20missingness%2C%20and%20image-tabular%20matching%0Aand%20contrastive%20learning%20objectives%20to%20capture%20multimodal%20information.%0AMoreover%2C%20TIP%20proposes%20a%20versatile%20tabular%20encoder%20tailored%20for%20incomplete%2C%0Aheterogeneous%20tabular%20data%20and%20a%20multimodal%20interaction%20module%20for%0Ainter-modality%20representation%20learning.%20Experiments%20are%20performed%20on%20downstream%0Amultimodal%20classification%20tasks%20using%20both%20natural%20and%20medical%20image%20datasets.%0AThe%20results%20show%20that%20TIP%20outperforms%20state-of-the-art%20supervised/SSL%0Aimage/multimodal%20algorithms%20in%20both%20complete%20and%20incomplete%20data%20scenarios.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/siyi-wind/TIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIP%253A%2520Tabular-Image%2520Pre-training%2520for%2520Multimodal%2520Classification%2520with%250A%2520%2520Incomplete%2520Data%26entry.906535625%3DSiyi%2520Du%2520and%2520Shaoming%2520Zheng%2520and%2520Yinsong%2520Wang%2520and%2520Wenjia%2520Bai%2520and%2520Declan%2520P.%2520O%2527Regan%2520and%2520Chen%2520Qin%26entry.1292438233%3D%2520%2520Images%2520and%2520structured%2520tables%2520are%2520essential%2520parts%2520of%2520real-world%2520databases.%250AThough%2520tabular-image%2520representation%2520learning%2520is%2520promising%2520to%2520create%2520new%250Ainsights%252C%2520it%2520remains%2520a%2520challenging%2520task%252C%2520as%2520tabular%2520data%2520is%2520typically%250Aheterogeneous%2520and%2520incomplete%252C%2520presenting%2520significant%2520modality%2520disparities%2520with%250Aimages.%2520Earlier%2520works%2520have%2520mainly%2520focused%2520on%2520simple%2520modality%2520fusion%2520strategies%250Ain%2520complete%2520data%2520scenarios%252C%2520without%2520considering%2520the%2520missing%2520data%2520issue%252C%2520and%250Athus%2520are%2520limited%2520in%2520practice.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TIP%252C%2520a%2520novel%250Atabular-image%2520pre-training%2520framework%2520for%2520learning%2520multimodal%2520representations%250Arobust%2520to%2520incomplete%2520tabular%2520data.%2520Specifically%252C%2520TIP%2520investigates%2520a%2520novel%250Aself-supervised%2520learning%2520%2528SSL%2529%2520strategy%252C%2520including%2520a%2520masked%2520tabular%250Areconstruction%2520task%2520for%2520tackling%2520data%2520missingness%252C%2520and%2520image-tabular%2520matching%250Aand%2520contrastive%2520learning%2520objectives%2520to%2520capture%2520multimodal%2520information.%250AMoreover%252C%2520TIP%2520proposes%2520a%2520versatile%2520tabular%2520encoder%2520tailored%2520for%2520incomplete%252C%250Aheterogeneous%2520tabular%2520data%2520and%2520a%2520multimodal%2520interaction%2520module%2520for%250Ainter-modality%2520representation%2520learning.%2520Experiments%2520are%2520performed%2520on%2520downstream%250Amultimodal%2520classification%2520tasks%2520using%2520both%2520natural%2520and%2520medical%2520image%2520datasets.%250AThe%2520results%2520show%2520that%2520TIP%2520outperforms%2520state-of-the-art%2520supervised/SSL%250Aimage/multimodal%2520algorithms%2520in%2520both%2520complete%2520and%2520incomplete%2520data%2520scenarios.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/siyi-wind/TIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIP%3A%20Tabular-Image%20Pre-training%20for%20Multimodal%20Classification%20with%0A%20%20Incomplete%20Data&entry.906535625=Siyi%20Du%20and%20Shaoming%20Zheng%20and%20Yinsong%20Wang%20and%20Wenjia%20Bai%20and%20Declan%20P.%20O%27Regan%20and%20Chen%20Qin&entry.1292438233=%20%20Images%20and%20structured%20tables%20are%20essential%20parts%20of%20real-world%20databases.%0AThough%20tabular-image%20representation%20learning%20is%20promising%20to%20create%20new%0Ainsights%2C%20it%20remains%20a%20challenging%20task%2C%20as%20tabular%20data%20is%20typically%0Aheterogeneous%20and%20incomplete%2C%20presenting%20significant%20modality%20disparities%20with%0Aimages.%20Earlier%20works%20have%20mainly%20focused%20on%20simple%20modality%20fusion%20strategies%0Ain%20complete%20data%20scenarios%2C%20without%20considering%20the%20missing%20data%20issue%2C%20and%0Athus%20are%20limited%20in%20practice.%20In%20this%20paper%2C%20we%20propose%20TIP%2C%20a%20novel%0Atabular-image%20pre-training%20framework%20for%20learning%20multimodal%20representations%0Arobust%20to%20incomplete%20tabular%20data.%20Specifically%2C%20TIP%20investigates%20a%20novel%0Aself-supervised%20learning%20%28SSL%29%20strategy%2C%20including%20a%20masked%20tabular%0Areconstruction%20task%20for%20tackling%20data%20missingness%2C%20and%20image-tabular%20matching%0Aand%20contrastive%20learning%20objectives%20to%20capture%20multimodal%20information.%0AMoreover%2C%20TIP%20proposes%20a%20versatile%20tabular%20encoder%20tailored%20for%20incomplete%2C%0Aheterogeneous%20tabular%20data%20and%20a%20multimodal%20interaction%20module%20for%0Ainter-modality%20representation%20learning.%20Experiments%20are%20performed%20on%20downstream%0Amultimodal%20classification%20tasks%20using%20both%20natural%20and%20medical%20image%20datasets.%0AThe%20results%20show%20that%20TIP%20outperforms%20state-of-the-art%20supervised/SSL%0Aimage/multimodal%20algorithms%20in%20both%20complete%20and%20incomplete%20data%20scenarios.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/siyi-wind/TIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07582v1&entry.124074799=Read"},
{"title": "Swin SMT: Global Sequential Modeling in 3D Medical Image Segmentation", "author": "Szymon P\u0142otka and Maciej Chrabaszcz and Przemyslaw Biecek", "abstract": "  Recent advances in Vision Transformers (ViTs) have significantly enhanced\nmedical image segmentation by facilitating the learning of global\nrelationships. However, these methods face a notable challenge in capturing\ndiverse local and global long-range sequential feature representations,\nparticularly evident in whole-body CT (WBCT) scans. To overcome this\nlimitation, we introduce Swin Soft Mixture Transformer (Swin SMT), a novel\narchitecture based on Swin UNETR. This model incorporates a Soft\nMixture-of-Experts (Soft MoE) to effectively handle complex and diverse\nlong-range dependencies. The use of Soft MoE allows for scaling up model\nparameters maintaining a balance between computational complexity and\nsegmentation performance in both training and inference modes. We evaluate Swin\nSMT on the publicly available TotalSegmentator-V2 dataset, which includes 117\nmajor anatomical structures in WBCT images. Comprehensive experimental results\ndemonstrate that Swin SMT outperforms several state-of-the-art methods in 3D\nanatomical structure segmentation, achieving an average Dice Similarity\nCoefficient of 85.09%. The code and pre-trained weights of Swin SMT are\npublicly available at https://github.com/MI2DataLab/SwinSMT.\n", "link": "http://arxiv.org/abs/2407.07514v1", "date": "2024-07-10", "relevancy": 2.7344, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5748}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5329}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Swin%20SMT%3A%20Global%20Sequential%20Modeling%20in%203D%20Medical%20Image%20Segmentation&body=Title%3A%20Swin%20SMT%3A%20Global%20Sequential%20Modeling%20in%203D%20Medical%20Image%20Segmentation%0AAuthor%3A%20Szymon%20P%C5%82otka%20and%20Maciej%20Chrabaszcz%20and%20Przemyslaw%20Biecek%0AAbstract%3A%20%20%20Recent%20advances%20in%20Vision%20Transformers%20%28ViTs%29%20have%20significantly%20enhanced%0Amedical%20image%20segmentation%20by%20facilitating%20the%20learning%20of%20global%0Arelationships.%20However%2C%20these%20methods%20face%20a%20notable%20challenge%20in%20capturing%0Adiverse%20local%20and%20global%20long-range%20sequential%20feature%20representations%2C%0Aparticularly%20evident%20in%20whole-body%20CT%20%28WBCT%29%20scans.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20Swin%20Soft%20Mixture%20Transformer%20%28Swin%20SMT%29%2C%20a%20novel%0Aarchitecture%20based%20on%20Swin%20UNETR.%20This%20model%20incorporates%20a%20Soft%0AMixture-of-Experts%20%28Soft%20MoE%29%20to%20effectively%20handle%20complex%20and%20diverse%0Along-range%20dependencies.%20The%20use%20of%20Soft%20MoE%20allows%20for%20scaling%20up%20model%0Aparameters%20maintaining%20a%20balance%20between%20computational%20complexity%20and%0Asegmentation%20performance%20in%20both%20training%20and%20inference%20modes.%20We%20evaluate%20Swin%0ASMT%20on%20the%20publicly%20available%20TotalSegmentator-V2%20dataset%2C%20which%20includes%20117%0Amajor%20anatomical%20structures%20in%20WBCT%20images.%20Comprehensive%20experimental%20results%0Ademonstrate%20that%20Swin%20SMT%20outperforms%20several%20state-of-the-art%20methods%20in%203D%0Aanatomical%20structure%20segmentation%2C%20achieving%20an%20average%20Dice%20Similarity%0ACoefficient%20of%2085.09%25.%20The%20code%20and%20pre-trained%20weights%20of%20Swin%20SMT%20are%0Apublicly%20available%20at%20https%3A//github.com/MI2DataLab/SwinSMT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwin%2520SMT%253A%2520Global%2520Sequential%2520Modeling%2520in%25203D%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DSzymon%2520P%25C5%2582otka%2520and%2520Maciej%2520Chrabaszcz%2520and%2520Przemyslaw%2520Biecek%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520significantly%2520enhanced%250Amedical%2520image%2520segmentation%2520by%2520facilitating%2520the%2520learning%2520of%2520global%250Arelationships.%2520However%252C%2520these%2520methods%2520face%2520a%2520notable%2520challenge%2520in%2520capturing%250Adiverse%2520local%2520and%2520global%2520long-range%2520sequential%2520feature%2520representations%252C%250Aparticularly%2520evident%2520in%2520whole-body%2520CT%2520%2528WBCT%2529%2520scans.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520introduce%2520Swin%2520Soft%2520Mixture%2520Transformer%2520%2528Swin%2520SMT%2529%252C%2520a%2520novel%250Aarchitecture%2520based%2520on%2520Swin%2520UNETR.%2520This%2520model%2520incorporates%2520a%2520Soft%250AMixture-of-Experts%2520%2528Soft%2520MoE%2529%2520to%2520effectively%2520handle%2520complex%2520and%2520diverse%250Along-range%2520dependencies.%2520The%2520use%2520of%2520Soft%2520MoE%2520allows%2520for%2520scaling%2520up%2520model%250Aparameters%2520maintaining%2520a%2520balance%2520between%2520computational%2520complexity%2520and%250Asegmentation%2520performance%2520in%2520both%2520training%2520and%2520inference%2520modes.%2520We%2520evaluate%2520Swin%250ASMT%2520on%2520the%2520publicly%2520available%2520TotalSegmentator-V2%2520dataset%252C%2520which%2520includes%2520117%250Amajor%2520anatomical%2520structures%2520in%2520WBCT%2520images.%2520Comprehensive%2520experimental%2520results%250Ademonstrate%2520that%2520Swin%2520SMT%2520outperforms%2520several%2520state-of-the-art%2520methods%2520in%25203D%250Aanatomical%2520structure%2520segmentation%252C%2520achieving%2520an%2520average%2520Dice%2520Similarity%250ACoefficient%2520of%252085.09%2525.%2520The%2520code%2520and%2520pre-trained%2520weights%2520of%2520Swin%2520SMT%2520are%250Apublicly%2520available%2520at%2520https%253A//github.com/MI2DataLab/SwinSMT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swin%20SMT%3A%20Global%20Sequential%20Modeling%20in%203D%20Medical%20Image%20Segmentation&entry.906535625=Szymon%20P%C5%82otka%20and%20Maciej%20Chrabaszcz%20and%20Przemyslaw%20Biecek&entry.1292438233=%20%20Recent%20advances%20in%20Vision%20Transformers%20%28ViTs%29%20have%20significantly%20enhanced%0Amedical%20image%20segmentation%20by%20facilitating%20the%20learning%20of%20global%0Arelationships.%20However%2C%20these%20methods%20face%20a%20notable%20challenge%20in%20capturing%0Adiverse%20local%20and%20global%20long-range%20sequential%20feature%20representations%2C%0Aparticularly%20evident%20in%20whole-body%20CT%20%28WBCT%29%20scans.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20Swin%20Soft%20Mixture%20Transformer%20%28Swin%20SMT%29%2C%20a%20novel%0Aarchitecture%20based%20on%20Swin%20UNETR.%20This%20model%20incorporates%20a%20Soft%0AMixture-of-Experts%20%28Soft%20MoE%29%20to%20effectively%20handle%20complex%20and%20diverse%0Along-range%20dependencies.%20The%20use%20of%20Soft%20MoE%20allows%20for%20scaling%20up%20model%0Aparameters%20maintaining%20a%20balance%20between%20computational%20complexity%20and%0Asegmentation%20performance%20in%20both%20training%20and%20inference%20modes.%20We%20evaluate%20Swin%0ASMT%20on%20the%20publicly%20available%20TotalSegmentator-V2%20dataset%2C%20which%20includes%20117%0Amajor%20anatomical%20structures%20in%20WBCT%20images.%20Comprehensive%20experimental%20results%0Ademonstrate%20that%20Swin%20SMT%20outperforms%20several%20state-of-the-art%20methods%20in%203D%0Aanatomical%20structure%20segmentation%2C%20achieving%20an%20average%20Dice%20Similarity%0ACoefficient%20of%2085.09%25.%20The%20code%20and%20pre-trained%20weights%20of%20Swin%20SMT%20are%0Apublicly%20available%20at%20https%3A//github.com/MI2DataLab/SwinSMT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07514v1&entry.124074799=Read"},
{"title": "Synthetic to Authentic: Transferring Realism to 3D Face Renderings for\n  Boosting Face Recognition", "author": "Parsa Rahimi and Behrooz Razeghi and Sebastien Marcel", "abstract": "  In this paper, we investigate the potential of image-to-image translation\n(I2I) techniques for transferring realism to 3D-rendered facial images in the\ncontext of Face Recognition (FR) systems. The primary motivation for using\n3D-rendered facial images lies in their ability to circumvent the challenges\nassociated with collecting large real face datasets for training FR systems.\nThese images are generated entirely by 3D rendering engines, facilitating the\ngeneration of synthetic identities. However, it has been observed that FR\nsystems trained on such synthetic datasets underperform when compared to those\ntrained on real datasets, on various FR benchmarks. In this work, we\ndemonstrate that by transferring the realism to 3D-rendered images (i.e.,\nmaking the 3D-rendered images look more real), we can boost the performance of\nFR systems trained on these more photorealistic images. This improvement is\nevident when these systems are evaluated against FR benchmarks utilizing\nreal-world data, thereby paving new pathways for employing synthetic data in\nreal-world applications.\n", "link": "http://arxiv.org/abs/2407.07627v1", "date": "2024-07-10", "relevancy": 2.73, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5586}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5483}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20to%20Authentic%3A%20Transferring%20Realism%20to%203D%20Face%20Renderings%20for%0A%20%20Boosting%20Face%20Recognition&body=Title%3A%20Synthetic%20to%20Authentic%3A%20Transferring%20Realism%20to%203D%20Face%20Renderings%20for%0A%20%20Boosting%20Face%20Recognition%0AAuthor%3A%20Parsa%20Rahimi%20and%20Behrooz%20Razeghi%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20image-to-image%20translation%0A%28I2I%29%20techniques%20for%20transferring%20realism%20to%203D-rendered%20facial%20images%20in%20the%0Acontext%20of%20Face%20Recognition%20%28FR%29%20systems.%20The%20primary%20motivation%20for%20using%0A3D-rendered%20facial%20images%20lies%20in%20their%20ability%20to%20circumvent%20the%20challenges%0Aassociated%20with%20collecting%20large%20real%20face%20datasets%20for%20training%20FR%20systems.%0AThese%20images%20are%20generated%20entirely%20by%203D%20rendering%20engines%2C%20facilitating%20the%0Ageneration%20of%20synthetic%20identities.%20However%2C%20it%20has%20been%20observed%20that%20FR%0Asystems%20trained%20on%20such%20synthetic%20datasets%20underperform%20when%20compared%20to%20those%0Atrained%20on%20real%20datasets%2C%20on%20various%20FR%20benchmarks.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20by%20transferring%20the%20realism%20to%203D-rendered%20images%20%28i.e.%2C%0Amaking%20the%203D-rendered%20images%20look%20more%20real%29%2C%20we%20can%20boost%20the%20performance%20of%0AFR%20systems%20trained%20on%20these%20more%20photorealistic%20images.%20This%20improvement%20is%0Aevident%20when%20these%20systems%20are%20evaluated%20against%20FR%20benchmarks%20utilizing%0Areal-world%20data%2C%20thereby%20paving%20new%20pathways%20for%20employing%20synthetic%20data%20in%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520to%2520Authentic%253A%2520Transferring%2520Realism%2520to%25203D%2520Face%2520Renderings%2520for%250A%2520%2520Boosting%2520Face%2520Recognition%26entry.906535625%3DParsa%2520Rahimi%2520and%2520Behrooz%2520Razeghi%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520potential%2520of%2520image-to-image%2520translation%250A%2528I2I%2529%2520techniques%2520for%2520transferring%2520realism%2520to%25203D-rendered%2520facial%2520images%2520in%2520the%250Acontext%2520of%2520Face%2520Recognition%2520%2528FR%2529%2520systems.%2520The%2520primary%2520motivation%2520for%2520using%250A3D-rendered%2520facial%2520images%2520lies%2520in%2520their%2520ability%2520to%2520circumvent%2520the%2520challenges%250Aassociated%2520with%2520collecting%2520large%2520real%2520face%2520datasets%2520for%2520training%2520FR%2520systems.%250AThese%2520images%2520are%2520generated%2520entirely%2520by%25203D%2520rendering%2520engines%252C%2520facilitating%2520the%250Ageneration%2520of%2520synthetic%2520identities.%2520However%252C%2520it%2520has%2520been%2520observed%2520that%2520FR%250Asystems%2520trained%2520on%2520such%2520synthetic%2520datasets%2520underperform%2520when%2520compared%2520to%2520those%250Atrained%2520on%2520real%2520datasets%252C%2520on%2520various%2520FR%2520benchmarks.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520that%2520by%2520transferring%2520the%2520realism%2520to%25203D-rendered%2520images%2520%2528i.e.%252C%250Amaking%2520the%25203D-rendered%2520images%2520look%2520more%2520real%2529%252C%2520we%2520can%2520boost%2520the%2520performance%2520of%250AFR%2520systems%2520trained%2520on%2520these%2520more%2520photorealistic%2520images.%2520This%2520improvement%2520is%250Aevident%2520when%2520these%2520systems%2520are%2520evaluated%2520against%2520FR%2520benchmarks%2520utilizing%250Areal-world%2520data%252C%2520thereby%2520paving%2520new%2520pathways%2520for%2520employing%2520synthetic%2520data%2520in%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20to%20Authentic%3A%20Transferring%20Realism%20to%203D%20Face%20Renderings%20for%0A%20%20Boosting%20Face%20Recognition&entry.906535625=Parsa%20Rahimi%20and%20Behrooz%20Razeghi%20and%20Sebastien%20Marcel&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20image-to-image%20translation%0A%28I2I%29%20techniques%20for%20transferring%20realism%20to%203D-rendered%20facial%20images%20in%20the%0Acontext%20of%20Face%20Recognition%20%28FR%29%20systems.%20The%20primary%20motivation%20for%20using%0A3D-rendered%20facial%20images%20lies%20in%20their%20ability%20to%20circumvent%20the%20challenges%0Aassociated%20with%20collecting%20large%20real%20face%20datasets%20for%20training%20FR%20systems.%0AThese%20images%20are%20generated%20entirely%20by%203D%20rendering%20engines%2C%20facilitating%20the%0Ageneration%20of%20synthetic%20identities.%20However%2C%20it%20has%20been%20observed%20that%20FR%0Asystems%20trained%20on%20such%20synthetic%20datasets%20underperform%20when%20compared%20to%20those%0Atrained%20on%20real%20datasets%2C%20on%20various%20FR%20benchmarks.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20by%20transferring%20the%20realism%20to%203D-rendered%20images%20%28i.e.%2C%0Amaking%20the%203D-rendered%20images%20look%20more%20real%29%2C%20we%20can%20boost%20the%20performance%20of%0AFR%20systems%20trained%20on%20these%20more%20photorealistic%20images.%20This%20improvement%20is%0Aevident%20when%20these%20systems%20are%20evaluated%20against%20FR%20benchmarks%20utilizing%0Areal-world%20data%2C%20thereby%20paving%20new%20pathways%20for%20employing%20synthetic%20data%20in%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07627v1&entry.124074799=Read"},
{"title": "Raising the Ceiling: Conflict-Free Local Feature Matching with Dynamic\n  View Switching", "author": "Xiaoyong Lu and Songlin Du", "abstract": "  Current feature matching methods prioritize improving modeling capabilities\nto better align outputs with ground-truth matches, which are the theoretical\nupper bound on matching results, metaphorically depicted as the \"ceiling\".\nHowever, these enhancements fail to address the underlying issues that directly\nhinder ground-truth matches, including the scarcity of matchable points in\nsmall scale images, matching conflicts in dense methods, and the\nkeypoint-repeatability reliance in sparse methods. We propose a novel feature\nmatching method named RCM, which Raises the Ceiling of Matching from three\naspects. 1) RCM introduces a dynamic view switching mechanism to address the\nscarcity of matchable points in source images by strategically switching image\npairs. 2) RCM proposes a conflict-free coarse matching module, addressing\nmatching conflicts in the target image through a many-to-one matching strategy.\n3) By integrating the semi-sparse paradigm and the coarse-to-fine architecture,\nRCM preserves the benefits of both high efficiency and global search,\nmitigating the reliance on keypoint repeatability. As a result, RCM enables\nmore matchable points in the source image to be matched in an exhaustive and\nconflict-free manner in the target image, leading to a substantial 260%\nincrease in ground-truth matches. Comprehensive experiments show that RCM\nexhibits remarkable performance and efficiency in comparison to\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.07789v1", "date": "2024-07-10", "relevancy": 2.6941, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5634}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5306}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Raising%20the%20Ceiling%3A%20Conflict-Free%20Local%20Feature%20Matching%20with%20Dynamic%0A%20%20View%20Switching&body=Title%3A%20Raising%20the%20Ceiling%3A%20Conflict-Free%20Local%20Feature%20Matching%20with%20Dynamic%0A%20%20View%20Switching%0AAuthor%3A%20Xiaoyong%20Lu%20and%20Songlin%20Du%0AAbstract%3A%20%20%20Current%20feature%20matching%20methods%20prioritize%20improving%20modeling%20capabilities%0Ato%20better%20align%20outputs%20with%20ground-truth%20matches%2C%20which%20are%20the%20theoretical%0Aupper%20bound%20on%20matching%20results%2C%20metaphorically%20depicted%20as%20the%20%22ceiling%22.%0AHowever%2C%20these%20enhancements%20fail%20to%20address%20the%20underlying%20issues%20that%20directly%0Ahinder%20ground-truth%20matches%2C%20including%20the%20scarcity%20of%20matchable%20points%20in%0Asmall%20scale%20images%2C%20matching%20conflicts%20in%20dense%20methods%2C%20and%20the%0Akeypoint-repeatability%20reliance%20in%20sparse%20methods.%20We%20propose%20a%20novel%20feature%0Amatching%20method%20named%20RCM%2C%20which%20Raises%20the%20Ceiling%20of%20Matching%20from%20three%0Aaspects.%201%29%20RCM%20introduces%20a%20dynamic%20view%20switching%20mechanism%20to%20address%20the%0Ascarcity%20of%20matchable%20points%20in%20source%20images%20by%20strategically%20switching%20image%0Apairs.%202%29%20RCM%20proposes%20a%20conflict-free%20coarse%20matching%20module%2C%20addressing%0Amatching%20conflicts%20in%20the%20target%20image%20through%20a%20many-to-one%20matching%20strategy.%0A3%29%20By%20integrating%20the%20semi-sparse%20paradigm%20and%20the%20coarse-to-fine%20architecture%2C%0ARCM%20preserves%20the%20benefits%20of%20both%20high%20efficiency%20and%20global%20search%2C%0Amitigating%20the%20reliance%20on%20keypoint%20repeatability.%20As%20a%20result%2C%20RCM%20enables%0Amore%20matchable%20points%20in%20the%20source%20image%20to%20be%20matched%20in%20an%20exhaustive%20and%0Aconflict-free%20manner%20in%20the%20target%20image%2C%20leading%20to%20a%20substantial%20260%25%0Aincrease%20in%20ground-truth%20matches.%20Comprehensive%20experiments%20show%20that%20RCM%0Aexhibits%20remarkable%20performance%20and%20efficiency%20in%20comparison%20to%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaising%2520the%2520Ceiling%253A%2520Conflict-Free%2520Local%2520Feature%2520Matching%2520with%2520Dynamic%250A%2520%2520View%2520Switching%26entry.906535625%3DXiaoyong%2520Lu%2520and%2520Songlin%2520Du%26entry.1292438233%3D%2520%2520Current%2520feature%2520matching%2520methods%2520prioritize%2520improving%2520modeling%2520capabilities%250Ato%2520better%2520align%2520outputs%2520with%2520ground-truth%2520matches%252C%2520which%2520are%2520the%2520theoretical%250Aupper%2520bound%2520on%2520matching%2520results%252C%2520metaphorically%2520depicted%2520as%2520the%2520%2522ceiling%2522.%250AHowever%252C%2520these%2520enhancements%2520fail%2520to%2520address%2520the%2520underlying%2520issues%2520that%2520directly%250Ahinder%2520ground-truth%2520matches%252C%2520including%2520the%2520scarcity%2520of%2520matchable%2520points%2520in%250Asmall%2520scale%2520images%252C%2520matching%2520conflicts%2520in%2520dense%2520methods%252C%2520and%2520the%250Akeypoint-repeatability%2520reliance%2520in%2520sparse%2520methods.%2520We%2520propose%2520a%2520novel%2520feature%250Amatching%2520method%2520named%2520RCM%252C%2520which%2520Raises%2520the%2520Ceiling%2520of%2520Matching%2520from%2520three%250Aaspects.%25201%2529%2520RCM%2520introduces%2520a%2520dynamic%2520view%2520switching%2520mechanism%2520to%2520address%2520the%250Ascarcity%2520of%2520matchable%2520points%2520in%2520source%2520images%2520by%2520strategically%2520switching%2520image%250Apairs.%25202%2529%2520RCM%2520proposes%2520a%2520conflict-free%2520coarse%2520matching%2520module%252C%2520addressing%250Amatching%2520conflicts%2520in%2520the%2520target%2520image%2520through%2520a%2520many-to-one%2520matching%2520strategy.%250A3%2529%2520By%2520integrating%2520the%2520semi-sparse%2520paradigm%2520and%2520the%2520coarse-to-fine%2520architecture%252C%250ARCM%2520preserves%2520the%2520benefits%2520of%2520both%2520high%2520efficiency%2520and%2520global%2520search%252C%250Amitigating%2520the%2520reliance%2520on%2520keypoint%2520repeatability.%2520As%2520a%2520result%252C%2520RCM%2520enables%250Amore%2520matchable%2520points%2520in%2520the%2520source%2520image%2520to%2520be%2520matched%2520in%2520an%2520exhaustive%2520and%250Aconflict-free%2520manner%2520in%2520the%2520target%2520image%252C%2520leading%2520to%2520a%2520substantial%2520260%2525%250Aincrease%2520in%2520ground-truth%2520matches.%2520Comprehensive%2520experiments%2520show%2520that%2520RCM%250Aexhibits%2520remarkable%2520performance%2520and%2520efficiency%2520in%2520comparison%2520to%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Raising%20the%20Ceiling%3A%20Conflict-Free%20Local%20Feature%20Matching%20with%20Dynamic%0A%20%20View%20Switching&entry.906535625=Xiaoyong%20Lu%20and%20Songlin%20Du&entry.1292438233=%20%20Current%20feature%20matching%20methods%20prioritize%20improving%20modeling%20capabilities%0Ato%20better%20align%20outputs%20with%20ground-truth%20matches%2C%20which%20are%20the%20theoretical%0Aupper%20bound%20on%20matching%20results%2C%20metaphorically%20depicted%20as%20the%20%22ceiling%22.%0AHowever%2C%20these%20enhancements%20fail%20to%20address%20the%20underlying%20issues%20that%20directly%0Ahinder%20ground-truth%20matches%2C%20including%20the%20scarcity%20of%20matchable%20points%20in%0Asmall%20scale%20images%2C%20matching%20conflicts%20in%20dense%20methods%2C%20and%20the%0Akeypoint-repeatability%20reliance%20in%20sparse%20methods.%20We%20propose%20a%20novel%20feature%0Amatching%20method%20named%20RCM%2C%20which%20Raises%20the%20Ceiling%20of%20Matching%20from%20three%0Aaspects.%201%29%20RCM%20introduces%20a%20dynamic%20view%20switching%20mechanism%20to%20address%20the%0Ascarcity%20of%20matchable%20points%20in%20source%20images%20by%20strategically%20switching%20image%0Apairs.%202%29%20RCM%20proposes%20a%20conflict-free%20coarse%20matching%20module%2C%20addressing%0Amatching%20conflicts%20in%20the%20target%20image%20through%20a%20many-to-one%20matching%20strategy.%0A3%29%20By%20integrating%20the%20semi-sparse%20paradigm%20and%20the%20coarse-to-fine%20architecture%2C%0ARCM%20preserves%20the%20benefits%20of%20both%20high%20efficiency%20and%20global%20search%2C%0Amitigating%20the%20reliance%20on%20keypoint%20repeatability.%20As%20a%20result%2C%20RCM%20enables%0Amore%20matchable%20points%20in%20the%20source%20image%20to%20be%20matched%20in%20an%20exhaustive%20and%0Aconflict-free%20manner%20in%20the%20target%20image%2C%20leading%20to%20a%20substantial%20260%25%0Aincrease%20in%20ground-truth%20matches.%20Comprehensive%20experiments%20show%20that%20RCM%0Aexhibits%20remarkable%20performance%20and%20efficiency%20in%20comparison%20to%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07789v1&entry.124074799=Read"},
{"title": "Controlling Space and Time with Diffusion Models", "author": "Daniel Watson and Saurabh Saxena and Lala Li and Andrea Tagliasacchi and David J. Fleet", "abstract": "  We present 4DiM, a cascaded diffusion model for 4D novel view synthesis\n(NVS), conditioned on one or more images of a general scene, and a set of\ncamera poses and timestamps. To overcome challenges due to limited availability\nof 4D training data, we advocate joint training on 3D (with camera pose), 4D\n(pose+time) and video (time but no pose) data and propose a new architecture\nthat enables the same. We further advocate the calibration of SfM posed data\nusing monocular metric depth estimators for metric scale camera control. For\nmodel evaluation, we introduce new metrics to enrich and overcome shortcomings\nof current evaluation schemes, demonstrating state-of-the-art results in both\nfidelity and pose control compared to existing diffusion models for 3D NVS,\nwhile at the same time adding the ability to handle temporal dynamics. 4DiM is\nalso used for improved panorama stitching, pose-conditioned video to video\ntranslation, and several other tasks. For an overview see\nhttps://4d-diffusion.github.io\n", "link": "http://arxiv.org/abs/2407.07860v1", "date": "2024-07-10", "relevancy": 2.6927, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6791}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6791}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlling%20Space%20and%20Time%20with%20Diffusion%20Models&body=Title%3A%20Controlling%20Space%20and%20Time%20with%20Diffusion%20Models%0AAuthor%3A%20Daniel%20Watson%20and%20Saurabh%20Saxena%20and%20Lala%20Li%20and%20Andrea%20Tagliasacchi%20and%20David%20J.%20Fleet%0AAbstract%3A%20%20%20We%20present%204DiM%2C%20a%20cascaded%20diffusion%20model%20for%204D%20novel%20view%20synthesis%0A%28NVS%29%2C%20conditioned%20on%20one%20or%20more%20images%20of%20a%20general%20scene%2C%20and%20a%20set%20of%0Acamera%20poses%20and%20timestamps.%20To%20overcome%20challenges%20due%20to%20limited%20availability%0Aof%204D%20training%20data%2C%20we%20advocate%20joint%20training%20on%203D%20%28with%20camera%20pose%29%2C%204D%0A%28pose%2Btime%29%20and%20video%20%28time%20but%20no%20pose%29%20data%20and%20propose%20a%20new%20architecture%0Athat%20enables%20the%20same.%20We%20further%20advocate%20the%20calibration%20of%20SfM%20posed%20data%0Ausing%20monocular%20metric%20depth%20estimators%20for%20metric%20scale%20camera%20control.%20For%0Amodel%20evaluation%2C%20we%20introduce%20new%20metrics%20to%20enrich%20and%20overcome%20shortcomings%0Aof%20current%20evaluation%20schemes%2C%20demonstrating%20state-of-the-art%20results%20in%20both%0Afidelity%20and%20pose%20control%20compared%20to%20existing%20diffusion%20models%20for%203D%20NVS%2C%0Awhile%20at%20the%20same%20time%20adding%20the%20ability%20to%20handle%20temporal%20dynamics.%204DiM%20is%0Aalso%20used%20for%20improved%20panorama%20stitching%2C%20pose-conditioned%20video%20to%20video%0Atranslation%2C%20and%20several%20other%20tasks.%20For%20an%20overview%20see%0Ahttps%3A//4d-diffusion.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlling%2520Space%2520and%2520Time%2520with%2520Diffusion%2520Models%26entry.906535625%3DDaniel%2520Watson%2520and%2520Saurabh%2520Saxena%2520and%2520Lala%2520Li%2520and%2520Andrea%2520Tagliasacchi%2520and%2520David%2520J.%2520Fleet%26entry.1292438233%3D%2520%2520We%2520present%25204DiM%252C%2520a%2520cascaded%2520diffusion%2520model%2520for%25204D%2520novel%2520view%2520synthesis%250A%2528NVS%2529%252C%2520conditioned%2520on%2520one%2520or%2520more%2520images%2520of%2520a%2520general%2520scene%252C%2520and%2520a%2520set%2520of%250Acamera%2520poses%2520and%2520timestamps.%2520To%2520overcome%2520challenges%2520due%2520to%2520limited%2520availability%250Aof%25204D%2520training%2520data%252C%2520we%2520advocate%2520joint%2520training%2520on%25203D%2520%2528with%2520camera%2520pose%2529%252C%25204D%250A%2528pose%252Btime%2529%2520and%2520video%2520%2528time%2520but%2520no%2520pose%2529%2520data%2520and%2520propose%2520a%2520new%2520architecture%250Athat%2520enables%2520the%2520same.%2520We%2520further%2520advocate%2520the%2520calibration%2520of%2520SfM%2520posed%2520data%250Ausing%2520monocular%2520metric%2520depth%2520estimators%2520for%2520metric%2520scale%2520camera%2520control.%2520For%250Amodel%2520evaluation%252C%2520we%2520introduce%2520new%2520metrics%2520to%2520enrich%2520and%2520overcome%2520shortcomings%250Aof%2520current%2520evaluation%2520schemes%252C%2520demonstrating%2520state-of-the-art%2520results%2520in%2520both%250Afidelity%2520and%2520pose%2520control%2520compared%2520to%2520existing%2520diffusion%2520models%2520for%25203D%2520NVS%252C%250Awhile%2520at%2520the%2520same%2520time%2520adding%2520the%2520ability%2520to%2520handle%2520temporal%2520dynamics.%25204DiM%2520is%250Aalso%2520used%2520for%2520improved%2520panorama%2520stitching%252C%2520pose-conditioned%2520video%2520to%2520video%250Atranslation%252C%2520and%2520several%2520other%2520tasks.%2520For%2520an%2520overview%2520see%250Ahttps%253A//4d-diffusion.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20Space%20and%20Time%20with%20Diffusion%20Models&entry.906535625=Daniel%20Watson%20and%20Saurabh%20Saxena%20and%20Lala%20Li%20and%20Andrea%20Tagliasacchi%20and%20David%20J.%20Fleet&entry.1292438233=%20%20We%20present%204DiM%2C%20a%20cascaded%20diffusion%20model%20for%204D%20novel%20view%20synthesis%0A%28NVS%29%2C%20conditioned%20on%20one%20or%20more%20images%20of%20a%20general%20scene%2C%20and%20a%20set%20of%0Acamera%20poses%20and%20timestamps.%20To%20overcome%20challenges%20due%20to%20limited%20availability%0Aof%204D%20training%20data%2C%20we%20advocate%20joint%20training%20on%203D%20%28with%20camera%20pose%29%2C%204D%0A%28pose%2Btime%29%20and%20video%20%28time%20but%20no%20pose%29%20data%20and%20propose%20a%20new%20architecture%0Athat%20enables%20the%20same.%20We%20further%20advocate%20the%20calibration%20of%20SfM%20posed%20data%0Ausing%20monocular%20metric%20depth%20estimators%20for%20metric%20scale%20camera%20control.%20For%0Amodel%20evaluation%2C%20we%20introduce%20new%20metrics%20to%20enrich%20and%20overcome%20shortcomings%0Aof%20current%20evaluation%20schemes%2C%20demonstrating%20state-of-the-art%20results%20in%20both%0Afidelity%20and%20pose%20control%20compared%20to%20existing%20diffusion%20models%20for%203D%20NVS%2C%0Awhile%20at%20the%20same%20time%20adding%20the%20ability%20to%20handle%20temporal%20dynamics.%204DiM%20is%0Aalso%20used%20for%20improved%20panorama%20stitching%2C%20pose-conditioned%20video%20to%20video%0Atranslation%2C%20and%20several%20other%20tasks.%20For%20an%20overview%20see%0Ahttps%3A//4d-diffusion.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07860v1&entry.124074799=Read"},
{"title": "Disentangled Representation Learning through Geometry Preservation with\n  the Gromov-Monge Gap", "author": "Th\u00e9o Uscidda and Luca Eyring and Karsten Roth and Fabian Theis and Zeynep Akata and Marco Cuturi", "abstract": "  Learning disentangled representations in an unsupervised manner is a\nfundamental challenge in machine learning. Solving it may unlock other\nproblems, such as generalization, interpretability, or fairness. While\nremarkably difficult to solve in general, recent works have shown that\ndisentanglement is provably achievable under additional assumptions that can\nleverage geometrical constraints, such as local isometry. To use these\ninsights, we propose a novel perspective on disentangled representation\nlearning built on quadratic optimal transport. Specifically, we formulate the\nproblem in the Gromov-Monge setting, which seeks isometric mappings between\ndistributions supported on different spaces. We propose the Gromov-Monge-Gap\n(GMG), a regularizer that quantifies the geometry-preservation of an arbitrary\npush-forward map between two distributions supported on different spaces. We\ndemonstrate the effectiveness of GMG regularization for disentanglement on four\nstandard benchmarks. Moreover, we show that geometry preservation can even\nencourage unsupervised disentanglement without the standard reconstruction\nobjective - making the underlying model decoder-free, and promising a more\npractically viable and scalable perspective on unsupervised disentanglement.\n", "link": "http://arxiv.org/abs/2407.07829v1", "date": "2024-07-10", "relevancy": 2.6801, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5578}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5411}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Representation%20Learning%20through%20Geometry%20Preservation%20with%0A%20%20the%20Gromov-Monge%20Gap&body=Title%3A%20Disentangled%20Representation%20Learning%20through%20Geometry%20Preservation%20with%0A%20%20the%20Gromov-Monge%20Gap%0AAuthor%3A%20Th%C3%A9o%20Uscidda%20and%20Luca%20Eyring%20and%20Karsten%20Roth%20and%20Fabian%20Theis%20and%20Zeynep%20Akata%20and%20Marco%20Cuturi%0AAbstract%3A%20%20%20Learning%20disentangled%20representations%20in%20an%20unsupervised%20manner%20is%20a%0Afundamental%20challenge%20in%20machine%20learning.%20Solving%20it%20may%20unlock%20other%0Aproblems%2C%20such%20as%20generalization%2C%20interpretability%2C%20or%20fairness.%20While%0Aremarkably%20difficult%20to%20solve%20in%20general%2C%20recent%20works%20have%20shown%20that%0Adisentanglement%20is%20provably%20achievable%20under%20additional%20assumptions%20that%20can%0Aleverage%20geometrical%20constraints%2C%20such%20as%20local%20isometry.%20To%20use%20these%0Ainsights%2C%20we%20propose%20a%20novel%20perspective%20on%20disentangled%20representation%0Alearning%20built%20on%20quadratic%20optimal%20transport.%20Specifically%2C%20we%20formulate%20the%0Aproblem%20in%20the%20Gromov-Monge%20setting%2C%20which%20seeks%20isometric%20mappings%20between%0Adistributions%20supported%20on%20different%20spaces.%20We%20propose%20the%20Gromov-Monge-Gap%0A%28GMG%29%2C%20a%20regularizer%20that%20quantifies%20the%20geometry-preservation%20of%20an%20arbitrary%0Apush-forward%20map%20between%20two%20distributions%20supported%20on%20different%20spaces.%20We%0Ademonstrate%20the%20effectiveness%20of%20GMG%20regularization%20for%20disentanglement%20on%20four%0Astandard%20benchmarks.%20Moreover%2C%20we%20show%20that%20geometry%20preservation%20can%20even%0Aencourage%20unsupervised%20disentanglement%20without%20the%20standard%20reconstruction%0Aobjective%20-%20making%20the%20underlying%20model%20decoder-free%2C%20and%20promising%20a%20more%0Apractically%20viable%20and%20scalable%20perspective%20on%20unsupervised%20disentanglement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Representation%2520Learning%2520through%2520Geometry%2520Preservation%2520with%250A%2520%2520the%2520Gromov-Monge%2520Gap%26entry.906535625%3DTh%25C3%25A9o%2520Uscidda%2520and%2520Luca%2520Eyring%2520and%2520Karsten%2520Roth%2520and%2520Fabian%2520Theis%2520and%2520Zeynep%2520Akata%2520and%2520Marco%2520Cuturi%26entry.1292438233%3D%2520%2520Learning%2520disentangled%2520representations%2520in%2520an%2520unsupervised%2520manner%2520is%2520a%250Afundamental%2520challenge%2520in%2520machine%2520learning.%2520Solving%2520it%2520may%2520unlock%2520other%250Aproblems%252C%2520such%2520as%2520generalization%252C%2520interpretability%252C%2520or%2520fairness.%2520While%250Aremarkably%2520difficult%2520to%2520solve%2520in%2520general%252C%2520recent%2520works%2520have%2520shown%2520that%250Adisentanglement%2520is%2520provably%2520achievable%2520under%2520additional%2520assumptions%2520that%2520can%250Aleverage%2520geometrical%2520constraints%252C%2520such%2520as%2520local%2520isometry.%2520To%2520use%2520these%250Ainsights%252C%2520we%2520propose%2520a%2520novel%2520perspective%2520on%2520disentangled%2520representation%250Alearning%2520built%2520on%2520quadratic%2520optimal%2520transport.%2520Specifically%252C%2520we%2520formulate%2520the%250Aproblem%2520in%2520the%2520Gromov-Monge%2520setting%252C%2520which%2520seeks%2520isometric%2520mappings%2520between%250Adistributions%2520supported%2520on%2520different%2520spaces.%2520We%2520propose%2520the%2520Gromov-Monge-Gap%250A%2528GMG%2529%252C%2520a%2520regularizer%2520that%2520quantifies%2520the%2520geometry-preservation%2520of%2520an%2520arbitrary%250Apush-forward%2520map%2520between%2520two%2520distributions%2520supported%2520on%2520different%2520spaces.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520GMG%2520regularization%2520for%2520disentanglement%2520on%2520four%250Astandard%2520benchmarks.%2520Moreover%252C%2520we%2520show%2520that%2520geometry%2520preservation%2520can%2520even%250Aencourage%2520unsupervised%2520disentanglement%2520without%2520the%2520standard%2520reconstruction%250Aobjective%2520-%2520making%2520the%2520underlying%2520model%2520decoder-free%252C%2520and%2520promising%2520a%2520more%250Apractically%2520viable%2520and%2520scalable%2520perspective%2520on%2520unsupervised%2520disentanglement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Representation%20Learning%20through%20Geometry%20Preservation%20with%0A%20%20the%20Gromov-Monge%20Gap&entry.906535625=Th%C3%A9o%20Uscidda%20and%20Luca%20Eyring%20and%20Karsten%20Roth%20and%20Fabian%20Theis%20and%20Zeynep%20Akata%20and%20Marco%20Cuturi&entry.1292438233=%20%20Learning%20disentangled%20representations%20in%20an%20unsupervised%20manner%20is%20a%0Afundamental%20challenge%20in%20machine%20learning.%20Solving%20it%20may%20unlock%20other%0Aproblems%2C%20such%20as%20generalization%2C%20interpretability%2C%20or%20fairness.%20While%0Aremarkably%20difficult%20to%20solve%20in%20general%2C%20recent%20works%20have%20shown%20that%0Adisentanglement%20is%20provably%20achievable%20under%20additional%20assumptions%20that%20can%0Aleverage%20geometrical%20constraints%2C%20such%20as%20local%20isometry.%20To%20use%20these%0Ainsights%2C%20we%20propose%20a%20novel%20perspective%20on%20disentangled%20representation%0Alearning%20built%20on%20quadratic%20optimal%20transport.%20Specifically%2C%20we%20formulate%20the%0Aproblem%20in%20the%20Gromov-Monge%20setting%2C%20which%20seeks%20isometric%20mappings%20between%0Adistributions%20supported%20on%20different%20spaces.%20We%20propose%20the%20Gromov-Monge-Gap%0A%28GMG%29%2C%20a%20regularizer%20that%20quantifies%20the%20geometry-preservation%20of%20an%20arbitrary%0Apush-forward%20map%20between%20two%20distributions%20supported%20on%20different%20spaces.%20We%0Ademonstrate%20the%20effectiveness%20of%20GMG%20regularization%20for%20disentanglement%20on%20four%0Astandard%20benchmarks.%20Moreover%2C%20we%20show%20that%20geometry%20preservation%20can%20even%0Aencourage%20unsupervised%20disentanglement%20without%20the%20standard%20reconstruction%0Aobjective%20-%20making%20the%20underlying%20model%20decoder-free%2C%20and%20promising%20a%20more%0Apractically%20viable%20and%20scalable%20perspective%20on%20unsupervised%20disentanglement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07829v1&entry.124074799=Read"},
{"title": "Neural Geometry Processing via Spherical Neural Surfaces", "author": "Romy Williamson and Niloy J. Mitra", "abstract": "  Neural surfaces (e.g., neural map encoding, deep implicits and neural\nradiance fields) have recently gained popularity because of their generic\nstructure (e.g., multi-layer perceptron) and easy integration with modern\nlearning-based setups. Traditionally, we have a rich toolbox of geometry\nprocessing algorithms designed for polygonal meshes to analyze and operate on\nsurface geometry. However, neural representations are typically discretized and\nconverted into a mesh, before applying any geometry processing algorithm. This\nis unsatisfactory and, as we demonstrate, unnecessary. In this work, we propose\na spherical neural surface representation (a spherical parametrization) for\ngenus-0 surfaces and demonstrate how to compute core geometric operators\ndirectly on this representation. Namely, we show how to construct the normals\nand the first and second fundamental forms of the surface, and how to compute\nthe surface gradient, surface divergence and Laplace Beltrami operator on\nscalar/vector fields defined on the surface. These operators, in turn, enable\nus to create geometry processing tools that act directly on the neural\nrepresentations without any unnecessary meshing. We demonstrate illustrative\napplications in (neural) spectral analysis, heat flow and mean curvature flow,\nand our method shows robustness to isometric shape variations. We both propose\ntheoretical formulations and validate their numerical estimates. By\nsystematically linking neural surface representations with classical geometry\nprocessing algorithms, we believe this work can become a key ingredient in\nenabling neural geometry processing.\n", "link": "http://arxiv.org/abs/2407.07755v1", "date": "2024-07-10", "relevancy": 2.6504, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5626}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5388}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Geometry%20Processing%20via%20Spherical%20Neural%20Surfaces&body=Title%3A%20Neural%20Geometry%20Processing%20via%20Spherical%20Neural%20Surfaces%0AAuthor%3A%20Romy%20Williamson%20and%20Niloy%20J.%20Mitra%0AAbstract%3A%20%20%20Neural%20surfaces%20%28e.g.%2C%20neural%20map%20encoding%2C%20deep%20implicits%20and%20neural%0Aradiance%20fields%29%20have%20recently%20gained%20popularity%20because%20of%20their%20generic%0Astructure%20%28e.g.%2C%20multi-layer%20perceptron%29%20and%20easy%20integration%20with%20modern%0Alearning-based%20setups.%20Traditionally%2C%20we%20have%20a%20rich%20toolbox%20of%20geometry%0Aprocessing%20algorithms%20designed%20for%20polygonal%20meshes%20to%20analyze%20and%20operate%20on%0Asurface%20geometry.%20However%2C%20neural%20representations%20are%20typically%20discretized%20and%0Aconverted%20into%20a%20mesh%2C%20before%20applying%20any%20geometry%20processing%20algorithm.%20This%0Ais%20unsatisfactory%20and%2C%20as%20we%20demonstrate%2C%20unnecessary.%20In%20this%20work%2C%20we%20propose%0Aa%20spherical%20neural%20surface%20representation%20%28a%20spherical%20parametrization%29%20for%0Agenus-0%20surfaces%20and%20demonstrate%20how%20to%20compute%20core%20geometric%20operators%0Adirectly%20on%20this%20representation.%20Namely%2C%20we%20show%20how%20to%20construct%20the%20normals%0Aand%20the%20first%20and%20second%20fundamental%20forms%20of%20the%20surface%2C%20and%20how%20to%20compute%0Athe%20surface%20gradient%2C%20surface%20divergence%20and%20Laplace%20Beltrami%20operator%20on%0Ascalar/vector%20fields%20defined%20on%20the%20surface.%20These%20operators%2C%20in%20turn%2C%20enable%0Aus%20to%20create%20geometry%20processing%20tools%20that%20act%20directly%20on%20the%20neural%0Arepresentations%20without%20any%20unnecessary%20meshing.%20We%20demonstrate%20illustrative%0Aapplications%20in%20%28neural%29%20spectral%20analysis%2C%20heat%20flow%20and%20mean%20curvature%20flow%2C%0Aand%20our%20method%20shows%20robustness%20to%20isometric%20shape%20variations.%20We%20both%20propose%0Atheoretical%20formulations%20and%20validate%20their%20numerical%20estimates.%20By%0Asystematically%20linking%20neural%20surface%20representations%20with%20classical%20geometry%0Aprocessing%20algorithms%2C%20we%20believe%20this%20work%20can%20become%20a%20key%20ingredient%20in%0Aenabling%20neural%20geometry%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Geometry%2520Processing%2520via%2520Spherical%2520Neural%2520Surfaces%26entry.906535625%3DRomy%2520Williamson%2520and%2520Niloy%2520J.%2520Mitra%26entry.1292438233%3D%2520%2520Neural%2520surfaces%2520%2528e.g.%252C%2520neural%2520map%2520encoding%252C%2520deep%2520implicits%2520and%2520neural%250Aradiance%2520fields%2529%2520have%2520recently%2520gained%2520popularity%2520because%2520of%2520their%2520generic%250Astructure%2520%2528e.g.%252C%2520multi-layer%2520perceptron%2529%2520and%2520easy%2520integration%2520with%2520modern%250Alearning-based%2520setups.%2520Traditionally%252C%2520we%2520have%2520a%2520rich%2520toolbox%2520of%2520geometry%250Aprocessing%2520algorithms%2520designed%2520for%2520polygonal%2520meshes%2520to%2520analyze%2520and%2520operate%2520on%250Asurface%2520geometry.%2520However%252C%2520neural%2520representations%2520are%2520typically%2520discretized%2520and%250Aconverted%2520into%2520a%2520mesh%252C%2520before%2520applying%2520any%2520geometry%2520processing%2520algorithm.%2520This%250Ais%2520unsatisfactory%2520and%252C%2520as%2520we%2520demonstrate%252C%2520unnecessary.%2520In%2520this%2520work%252C%2520we%2520propose%250Aa%2520spherical%2520neural%2520surface%2520representation%2520%2528a%2520spherical%2520parametrization%2529%2520for%250Agenus-0%2520surfaces%2520and%2520demonstrate%2520how%2520to%2520compute%2520core%2520geometric%2520operators%250Adirectly%2520on%2520this%2520representation.%2520Namely%252C%2520we%2520show%2520how%2520to%2520construct%2520the%2520normals%250Aand%2520the%2520first%2520and%2520second%2520fundamental%2520forms%2520of%2520the%2520surface%252C%2520and%2520how%2520to%2520compute%250Athe%2520surface%2520gradient%252C%2520surface%2520divergence%2520and%2520Laplace%2520Beltrami%2520operator%2520on%250Ascalar/vector%2520fields%2520defined%2520on%2520the%2520surface.%2520These%2520operators%252C%2520in%2520turn%252C%2520enable%250Aus%2520to%2520create%2520geometry%2520processing%2520tools%2520that%2520act%2520directly%2520on%2520the%2520neural%250Arepresentations%2520without%2520any%2520unnecessary%2520meshing.%2520We%2520demonstrate%2520illustrative%250Aapplications%2520in%2520%2528neural%2529%2520spectral%2520analysis%252C%2520heat%2520flow%2520and%2520mean%2520curvature%2520flow%252C%250Aand%2520our%2520method%2520shows%2520robustness%2520to%2520isometric%2520shape%2520variations.%2520We%2520both%2520propose%250Atheoretical%2520formulations%2520and%2520validate%2520their%2520numerical%2520estimates.%2520By%250Asystematically%2520linking%2520neural%2520surface%2520representations%2520with%2520classical%2520geometry%250Aprocessing%2520algorithms%252C%2520we%2520believe%2520this%2520work%2520can%2520become%2520a%2520key%2520ingredient%2520in%250Aenabling%2520neural%2520geometry%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Geometry%20Processing%20via%20Spherical%20Neural%20Surfaces&entry.906535625=Romy%20Williamson%20and%20Niloy%20J.%20Mitra&entry.1292438233=%20%20Neural%20surfaces%20%28e.g.%2C%20neural%20map%20encoding%2C%20deep%20implicits%20and%20neural%0Aradiance%20fields%29%20have%20recently%20gained%20popularity%20because%20of%20their%20generic%0Astructure%20%28e.g.%2C%20multi-layer%20perceptron%29%20and%20easy%20integration%20with%20modern%0Alearning-based%20setups.%20Traditionally%2C%20we%20have%20a%20rich%20toolbox%20of%20geometry%0Aprocessing%20algorithms%20designed%20for%20polygonal%20meshes%20to%20analyze%20and%20operate%20on%0Asurface%20geometry.%20However%2C%20neural%20representations%20are%20typically%20discretized%20and%0Aconverted%20into%20a%20mesh%2C%20before%20applying%20any%20geometry%20processing%20algorithm.%20This%0Ais%20unsatisfactory%20and%2C%20as%20we%20demonstrate%2C%20unnecessary.%20In%20this%20work%2C%20we%20propose%0Aa%20spherical%20neural%20surface%20representation%20%28a%20spherical%20parametrization%29%20for%0Agenus-0%20surfaces%20and%20demonstrate%20how%20to%20compute%20core%20geometric%20operators%0Adirectly%20on%20this%20representation.%20Namely%2C%20we%20show%20how%20to%20construct%20the%20normals%0Aand%20the%20first%20and%20second%20fundamental%20forms%20of%20the%20surface%2C%20and%20how%20to%20compute%0Athe%20surface%20gradient%2C%20surface%20divergence%20and%20Laplace%20Beltrami%20operator%20on%0Ascalar/vector%20fields%20defined%20on%20the%20surface.%20These%20operators%2C%20in%20turn%2C%20enable%0Aus%20to%20create%20geometry%20processing%20tools%20that%20act%20directly%20on%20the%20neural%0Arepresentations%20without%20any%20unnecessary%20meshing.%20We%20demonstrate%20illustrative%0Aapplications%20in%20%28neural%29%20spectral%20analysis%2C%20heat%20flow%20and%20mean%20curvature%20flow%2C%0Aand%20our%20method%20shows%20robustness%20to%20isometric%20shape%20variations.%20We%20both%20propose%0Atheoretical%20formulations%20and%20validate%20their%20numerical%20estimates.%20By%0Asystematically%20linking%20neural%20surface%20representations%20with%20classical%20geometry%0Aprocessing%20algorithms%2C%20we%20believe%20this%20work%20can%20become%20a%20key%20ingredient%20in%0Aenabling%20neural%20geometry%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07755v1&entry.124074799=Read"},
{"title": "Simplifying Source-Free Domain Adaptation for Object Detection:\n  Effective Self-Training Strategies and Performance Insights", "author": "Yan Hao and Florent Forest and Olga Fink", "abstract": "  This paper focuses on source-free domain adaptation for object detection in\ncomputer vision. This task is challenging and of great practical interest, due\nto the cost of obtaining annotated data sets for every new domain. Recent\nresearch has proposed various solutions for Source-Free Object Detection\n(SFOD), most being variations of teacher-student architectures with diverse\nfeature alignment, regularization and pseudo-label selection strategies. Our\nwork investigates simpler approaches and their performance compared to more\ncomplex SFOD methods in several adaptation scenarios. We highlight the\nimportance of batch normalization layers in the detector backbone, and show\nthat adapting only the batch statistics is a strong baseline for SFOD. We\npropose a simple extension of a Mean Teacher with strong-weak augmentation in\nthe source-free setting, Source-Free Unbiased Teacher (SF-UT), and show that it\nactually outperforms most of the previous SFOD methods. Additionally, we\nshowcase that an even simpler strategy consisting in training on a fixed set of\npseudo-labels can achieve similar performance to the more complex\nteacher-student mutual learning, while being computationally efficient and\nmitigating the major issue of teacher-student collapse. We conduct experiments\non several adaptation tasks using benchmark driving datasets including\n(Foggy)Cityscapes, Sim10k and KITTI, and achieve a notable improvement of 4.7\\%\nAP50 on Cityscapes$\\rightarrow$Foggy-Cityscapes compared with the latest\nstate-of-the-art in SFOD. Source code is available at\nhttps://github.com/EPFL-IMOS/simple-SFOD.\n", "link": "http://arxiv.org/abs/2407.07586v1", "date": "2024-07-10", "relevancy": 2.6476, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5311}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplifying%20Source-Free%20Domain%20Adaptation%20for%20Object%20Detection%3A%0A%20%20Effective%20Self-Training%20Strategies%20and%20Performance%20Insights&body=Title%3A%20Simplifying%20Source-Free%20Domain%20Adaptation%20for%20Object%20Detection%3A%0A%20%20Effective%20Self-Training%20Strategies%20and%20Performance%20Insights%0AAuthor%3A%20Yan%20Hao%20and%20Florent%20Forest%20and%20Olga%20Fink%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20source-free%20domain%20adaptation%20for%20object%20detection%20in%0Acomputer%20vision.%20This%20task%20is%20challenging%20and%20of%20great%20practical%20interest%2C%20due%0Ato%20the%20cost%20of%20obtaining%20annotated%20data%20sets%20for%20every%20new%20domain.%20Recent%0Aresearch%20has%20proposed%20various%20solutions%20for%20Source-Free%20Object%20Detection%0A%28SFOD%29%2C%20most%20being%20variations%20of%20teacher-student%20architectures%20with%20diverse%0Afeature%20alignment%2C%20regularization%20and%20pseudo-label%20selection%20strategies.%20Our%0Awork%20investigates%20simpler%20approaches%20and%20their%20performance%20compared%20to%20more%0Acomplex%20SFOD%20methods%20in%20several%20adaptation%20scenarios.%20We%20highlight%20the%0Aimportance%20of%20batch%20normalization%20layers%20in%20the%20detector%20backbone%2C%20and%20show%0Athat%20adapting%20only%20the%20batch%20statistics%20is%20a%20strong%20baseline%20for%20SFOD.%20We%0Apropose%20a%20simple%20extension%20of%20a%20Mean%20Teacher%20with%20strong-weak%20augmentation%20in%0Athe%20source-free%20setting%2C%20Source-Free%20Unbiased%20Teacher%20%28SF-UT%29%2C%20and%20show%20that%20it%0Aactually%20outperforms%20most%20of%20the%20previous%20SFOD%20methods.%20Additionally%2C%20we%0Ashowcase%20that%20an%20even%20simpler%20strategy%20consisting%20in%20training%20on%20a%20fixed%20set%20of%0Apseudo-labels%20can%20achieve%20similar%20performance%20to%20the%20more%20complex%0Ateacher-student%20mutual%20learning%2C%20while%20being%20computationally%20efficient%20and%0Amitigating%20the%20major%20issue%20of%20teacher-student%20collapse.%20We%20conduct%20experiments%0Aon%20several%20adaptation%20tasks%20using%20benchmark%20driving%20datasets%20including%0A%28Foggy%29Cityscapes%2C%20Sim10k%20and%20KITTI%2C%20and%20achieve%20a%20notable%20improvement%20of%204.7%5C%25%0AAP50%20on%20Cityscapes%24%5Crightarrow%24Foggy-Cityscapes%20compared%20with%20the%20latest%0Astate-of-the-art%20in%20SFOD.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/EPFL-IMOS/simple-SFOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplifying%2520Source-Free%2520Domain%2520Adaptation%2520for%2520Object%2520Detection%253A%250A%2520%2520Effective%2520Self-Training%2520Strategies%2520and%2520Performance%2520Insights%26entry.906535625%3DYan%2520Hao%2520and%2520Florent%2520Forest%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520source-free%2520domain%2520adaptation%2520for%2520object%2520detection%2520in%250Acomputer%2520vision.%2520This%2520task%2520is%2520challenging%2520and%2520of%2520great%2520practical%2520interest%252C%2520due%250Ato%2520the%2520cost%2520of%2520obtaining%2520annotated%2520data%2520sets%2520for%2520every%2520new%2520domain.%2520Recent%250Aresearch%2520has%2520proposed%2520various%2520solutions%2520for%2520Source-Free%2520Object%2520Detection%250A%2528SFOD%2529%252C%2520most%2520being%2520variations%2520of%2520teacher-student%2520architectures%2520with%2520diverse%250Afeature%2520alignment%252C%2520regularization%2520and%2520pseudo-label%2520selection%2520strategies.%2520Our%250Awork%2520investigates%2520simpler%2520approaches%2520and%2520their%2520performance%2520compared%2520to%2520more%250Acomplex%2520SFOD%2520methods%2520in%2520several%2520adaptation%2520scenarios.%2520We%2520highlight%2520the%250Aimportance%2520of%2520batch%2520normalization%2520layers%2520in%2520the%2520detector%2520backbone%252C%2520and%2520show%250Athat%2520adapting%2520only%2520the%2520batch%2520statistics%2520is%2520a%2520strong%2520baseline%2520for%2520SFOD.%2520We%250Apropose%2520a%2520simple%2520extension%2520of%2520a%2520Mean%2520Teacher%2520with%2520strong-weak%2520augmentation%2520in%250Athe%2520source-free%2520setting%252C%2520Source-Free%2520Unbiased%2520Teacher%2520%2528SF-UT%2529%252C%2520and%2520show%2520that%2520it%250Aactually%2520outperforms%2520most%2520of%2520the%2520previous%2520SFOD%2520methods.%2520Additionally%252C%2520we%250Ashowcase%2520that%2520an%2520even%2520simpler%2520strategy%2520consisting%2520in%2520training%2520on%2520a%2520fixed%2520set%2520of%250Apseudo-labels%2520can%2520achieve%2520similar%2520performance%2520to%2520the%2520more%2520complex%250Ateacher-student%2520mutual%2520learning%252C%2520while%2520being%2520computationally%2520efficient%2520and%250Amitigating%2520the%2520major%2520issue%2520of%2520teacher-student%2520collapse.%2520We%2520conduct%2520experiments%250Aon%2520several%2520adaptation%2520tasks%2520using%2520benchmark%2520driving%2520datasets%2520including%250A%2528Foggy%2529Cityscapes%252C%2520Sim10k%2520and%2520KITTI%252C%2520and%2520achieve%2520a%2520notable%2520improvement%2520of%25204.7%255C%2525%250AAP50%2520on%2520Cityscapes%2524%255Crightarrow%2524Foggy-Cityscapes%2520compared%2520with%2520the%2520latest%250Astate-of-the-art%2520in%2520SFOD.%2520Source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/EPFL-IMOS/simple-SFOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplifying%20Source-Free%20Domain%20Adaptation%20for%20Object%20Detection%3A%0A%20%20Effective%20Self-Training%20Strategies%20and%20Performance%20Insights&entry.906535625=Yan%20Hao%20and%20Florent%20Forest%20and%20Olga%20Fink&entry.1292438233=%20%20This%20paper%20focuses%20on%20source-free%20domain%20adaptation%20for%20object%20detection%20in%0Acomputer%20vision.%20This%20task%20is%20challenging%20and%20of%20great%20practical%20interest%2C%20due%0Ato%20the%20cost%20of%20obtaining%20annotated%20data%20sets%20for%20every%20new%20domain.%20Recent%0Aresearch%20has%20proposed%20various%20solutions%20for%20Source-Free%20Object%20Detection%0A%28SFOD%29%2C%20most%20being%20variations%20of%20teacher-student%20architectures%20with%20diverse%0Afeature%20alignment%2C%20regularization%20and%20pseudo-label%20selection%20strategies.%20Our%0Awork%20investigates%20simpler%20approaches%20and%20their%20performance%20compared%20to%20more%0Acomplex%20SFOD%20methods%20in%20several%20adaptation%20scenarios.%20We%20highlight%20the%0Aimportance%20of%20batch%20normalization%20layers%20in%20the%20detector%20backbone%2C%20and%20show%0Athat%20adapting%20only%20the%20batch%20statistics%20is%20a%20strong%20baseline%20for%20SFOD.%20We%0Apropose%20a%20simple%20extension%20of%20a%20Mean%20Teacher%20with%20strong-weak%20augmentation%20in%0Athe%20source-free%20setting%2C%20Source-Free%20Unbiased%20Teacher%20%28SF-UT%29%2C%20and%20show%20that%20it%0Aactually%20outperforms%20most%20of%20the%20previous%20SFOD%20methods.%20Additionally%2C%20we%0Ashowcase%20that%20an%20even%20simpler%20strategy%20consisting%20in%20training%20on%20a%20fixed%20set%20of%0Apseudo-labels%20can%20achieve%20similar%20performance%20to%20the%20more%20complex%0Ateacher-student%20mutual%20learning%2C%20while%20being%20computationally%20efficient%20and%0Amitigating%20the%20major%20issue%20of%20teacher-student%20collapse.%20We%20conduct%20experiments%0Aon%20several%20adaptation%20tasks%20using%20benchmark%20driving%20datasets%20including%0A%28Foggy%29Cityscapes%2C%20Sim10k%20and%20KITTI%2C%20and%20achieve%20a%20notable%20improvement%20of%204.7%5C%25%0AAP50%20on%20Cityscapes%24%5Crightarrow%24Foggy-Cityscapes%20compared%20with%20the%20latest%0Astate-of-the-art%20in%20SFOD.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/EPFL-IMOS/simple-SFOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07586v1&entry.124074799=Read"},
{"title": "Multi-task Prompt Words Learning for Social Media Content Generation", "author": "Haochen Xue and Chong Zhang and Chengzhi Liu and Fangyu Wu and Xiaobo Jin", "abstract": "  The rapid development of the Internet has profoundly changed human life.\nHumans are increasingly expressing themselves and interacting with others on\nsocial media platforms. However, although artificial intelligence technology\nhas been widely used in many aspects of life, its application in social media\ncontent creation is still blank. To solve this problem, we propose a new prompt\nword generation framework based on multi-modal information fusion, which\ncombines multiple tasks including topic classification, sentiment analysis,\nscene recognition and keyword extraction to generate more comprehensive prompt\nwords. Subsequently, we use a template containing a set of prompt words to\nguide ChatGPT to generate high-quality tweets. Furthermore, in the absence of\neffective and objective evaluation criteria in the field of content generation,\nwe use the ChatGPT tool to evaluate the results generated by the algorithm,\nmaking large-scale evaluation of content generation algorithms possible.\nEvaluation results on extensive content generation demonstrate that our cue\nword generation framework generates higher quality content compared to manual\nmethods and other cueing techniques, while topic classification, sentiment\nanalysis, and scene recognition significantly enhance content clarity and its\nconsistency with the image.\n", "link": "http://arxiv.org/abs/2407.07771v1", "date": "2024-07-10", "relevancy": 2.6469, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5418}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5365}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-task%20Prompt%20Words%20Learning%20for%20Social%20Media%20Content%20Generation&body=Title%3A%20Multi-task%20Prompt%20Words%20Learning%20for%20Social%20Media%20Content%20Generation%0AAuthor%3A%20Haochen%20Xue%20and%20Chong%20Zhang%20and%20Chengzhi%20Liu%20and%20Fangyu%20Wu%20and%20Xiaobo%20Jin%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20the%20Internet%20has%20profoundly%20changed%20human%20life.%0AHumans%20are%20increasingly%20expressing%20themselves%20and%20interacting%20with%20others%20on%0Asocial%20media%20platforms.%20However%2C%20although%20artificial%20intelligence%20technology%0Ahas%20been%20widely%20used%20in%20many%20aspects%20of%20life%2C%20its%20application%20in%20social%20media%0Acontent%20creation%20is%20still%20blank.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20new%20prompt%0Aword%20generation%20framework%20based%20on%20multi-modal%20information%20fusion%2C%20which%0Acombines%20multiple%20tasks%20including%20topic%20classification%2C%20sentiment%20analysis%2C%0Ascene%20recognition%20and%20keyword%20extraction%20to%20generate%20more%20comprehensive%20prompt%0Awords.%20Subsequently%2C%20we%20use%20a%20template%20containing%20a%20set%20of%20prompt%20words%20to%0Aguide%20ChatGPT%20to%20generate%20high-quality%20tweets.%20Furthermore%2C%20in%20the%20absence%20of%0Aeffective%20and%20objective%20evaluation%20criteria%20in%20the%20field%20of%20content%20generation%2C%0Awe%20use%20the%20ChatGPT%20tool%20to%20evaluate%20the%20results%20generated%20by%20the%20algorithm%2C%0Amaking%20large-scale%20evaluation%20of%20content%20generation%20algorithms%20possible.%0AEvaluation%20results%20on%20extensive%20content%20generation%20demonstrate%20that%20our%20cue%0Aword%20generation%20framework%20generates%20higher%20quality%20content%20compared%20to%20manual%0Amethods%20and%20other%20cueing%20techniques%2C%20while%20topic%20classification%2C%20sentiment%0Aanalysis%2C%20and%20scene%20recognition%20significantly%20enhance%20content%20clarity%20and%20its%0Aconsistency%20with%20the%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-task%2520Prompt%2520Words%2520Learning%2520for%2520Social%2520Media%2520Content%2520Generation%26entry.906535625%3DHaochen%2520Xue%2520and%2520Chong%2520Zhang%2520and%2520Chengzhi%2520Liu%2520and%2520Fangyu%2520Wu%2520and%2520Xiaobo%2520Jin%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520the%2520Internet%2520has%2520profoundly%2520changed%2520human%2520life.%250AHumans%2520are%2520increasingly%2520expressing%2520themselves%2520and%2520interacting%2520with%2520others%2520on%250Asocial%2520media%2520platforms.%2520However%252C%2520although%2520artificial%2520intelligence%2520technology%250Ahas%2520been%2520widely%2520used%2520in%2520many%2520aspects%2520of%2520life%252C%2520its%2520application%2520in%2520social%2520media%250Acontent%2520creation%2520is%2520still%2520blank.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520a%2520new%2520prompt%250Aword%2520generation%2520framework%2520based%2520on%2520multi-modal%2520information%2520fusion%252C%2520which%250Acombines%2520multiple%2520tasks%2520including%2520topic%2520classification%252C%2520sentiment%2520analysis%252C%250Ascene%2520recognition%2520and%2520keyword%2520extraction%2520to%2520generate%2520more%2520comprehensive%2520prompt%250Awords.%2520Subsequently%252C%2520we%2520use%2520a%2520template%2520containing%2520a%2520set%2520of%2520prompt%2520words%2520to%250Aguide%2520ChatGPT%2520to%2520generate%2520high-quality%2520tweets.%2520Furthermore%252C%2520in%2520the%2520absence%2520of%250Aeffective%2520and%2520objective%2520evaluation%2520criteria%2520in%2520the%2520field%2520of%2520content%2520generation%252C%250Awe%2520use%2520the%2520ChatGPT%2520tool%2520to%2520evaluate%2520the%2520results%2520generated%2520by%2520the%2520algorithm%252C%250Amaking%2520large-scale%2520evaluation%2520of%2520content%2520generation%2520algorithms%2520possible.%250AEvaluation%2520results%2520on%2520extensive%2520content%2520generation%2520demonstrate%2520that%2520our%2520cue%250Aword%2520generation%2520framework%2520generates%2520higher%2520quality%2520content%2520compared%2520to%2520manual%250Amethods%2520and%2520other%2520cueing%2520techniques%252C%2520while%2520topic%2520classification%252C%2520sentiment%250Aanalysis%252C%2520and%2520scene%2520recognition%2520significantly%2520enhance%2520content%2520clarity%2520and%2520its%250Aconsistency%2520with%2520the%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-task%20Prompt%20Words%20Learning%20for%20Social%20Media%20Content%20Generation&entry.906535625=Haochen%20Xue%20and%20Chong%20Zhang%20and%20Chengzhi%20Liu%20and%20Fangyu%20Wu%20and%20Xiaobo%20Jin&entry.1292438233=%20%20The%20rapid%20development%20of%20the%20Internet%20has%20profoundly%20changed%20human%20life.%0AHumans%20are%20increasingly%20expressing%20themselves%20and%20interacting%20with%20others%20on%0Asocial%20media%20platforms.%20However%2C%20although%20artificial%20intelligence%20technology%0Ahas%20been%20widely%20used%20in%20many%20aspects%20of%20life%2C%20its%20application%20in%20social%20media%0Acontent%20creation%20is%20still%20blank.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20new%20prompt%0Aword%20generation%20framework%20based%20on%20multi-modal%20information%20fusion%2C%20which%0Acombines%20multiple%20tasks%20including%20topic%20classification%2C%20sentiment%20analysis%2C%0Ascene%20recognition%20and%20keyword%20extraction%20to%20generate%20more%20comprehensive%20prompt%0Awords.%20Subsequently%2C%20we%20use%20a%20template%20containing%20a%20set%20of%20prompt%20words%20to%0Aguide%20ChatGPT%20to%20generate%20high-quality%20tweets.%20Furthermore%2C%20in%20the%20absence%20of%0Aeffective%20and%20objective%20evaluation%20criteria%20in%20the%20field%20of%20content%20generation%2C%0Awe%20use%20the%20ChatGPT%20tool%20to%20evaluate%20the%20results%20generated%20by%20the%20algorithm%2C%0Amaking%20large-scale%20evaluation%20of%20content%20generation%20algorithms%20possible.%0AEvaluation%20results%20on%20extensive%20content%20generation%20demonstrate%20that%20our%20cue%0Aword%20generation%20framework%20generates%20higher%20quality%20content%20compared%20to%20manual%0Amethods%20and%20other%20cueing%20techniques%2C%20while%20topic%20classification%2C%20sentiment%0Aanalysis%2C%20and%20scene%20recognition%20significantly%20enhance%20content%20clarity%20and%20its%0Aconsistency%20with%20the%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07771v1&entry.124074799=Read"},
{"title": "Domain-Aware Fine-Tuning of Foundation Models", "author": "Ugur Ali Kaplan and Margret Keuper and Anna Khoreva and Dan Zhang and Yumeng Li", "abstract": "  Foundation models (FMs) have revolutionized computer vision, enabling\neffective learning across different domains. However, their performance under\ndomain shift is yet underexplored. This paper investigates the zero-shot domain\nadaptation potential of FMs by comparing different backbone architectures and\nintroducing novel domain-aware components that leverage domain related textual\nembeddings. We propose domain adaptive normalization, termed as Domino, which\nexplicitly leverages domain embeddings during fine-tuning, thus making the\nmodel domain aware. Ultimately, Domino enables more robust computer vision\nmodels that can adapt effectively to various unseen domains.\n", "link": "http://arxiv.org/abs/2407.03482v2", "date": "2024-07-10", "relevancy": 2.6446, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5287}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-Aware%20Fine-Tuning%20of%20Foundation%20Models&body=Title%3A%20Domain-Aware%20Fine-Tuning%20of%20Foundation%20Models%0AAuthor%3A%20Ugur%20Ali%20Kaplan%20and%20Margret%20Keuper%20and%20Anna%20Khoreva%20and%20Dan%20Zhang%20and%20Yumeng%20Li%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20have%20revolutionized%20computer%20vision%2C%20enabling%0Aeffective%20learning%20across%20different%20domains.%20However%2C%20their%20performance%20under%0Adomain%20shift%20is%20yet%20underexplored.%20This%20paper%20investigates%20the%20zero-shot%20domain%0Aadaptation%20potential%20of%20FMs%20by%20comparing%20different%20backbone%20architectures%20and%0Aintroducing%20novel%20domain-aware%20components%20that%20leverage%20domain%20related%20textual%0Aembeddings.%20We%20propose%20domain%20adaptive%20normalization%2C%20termed%20as%20Domino%2C%20which%0Aexplicitly%20leverages%20domain%20embeddings%20during%20fine-tuning%2C%20thus%20making%20the%0Amodel%20domain%20aware.%20Ultimately%2C%20Domino%20enables%20more%20robust%20computer%20vision%0Amodels%20that%20can%20adapt%20effectively%20to%20various%20unseen%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-Aware%2520Fine-Tuning%2520of%2520Foundation%2520Models%26entry.906535625%3DUgur%2520Ali%2520Kaplan%2520and%2520Margret%2520Keuper%2520and%2520Anna%2520Khoreva%2520and%2520Dan%2520Zhang%2520and%2520Yumeng%2520Li%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520have%2520revolutionized%2520computer%2520vision%252C%2520enabling%250Aeffective%2520learning%2520across%2520different%2520domains.%2520However%252C%2520their%2520performance%2520under%250Adomain%2520shift%2520is%2520yet%2520underexplored.%2520This%2520paper%2520investigates%2520the%2520zero-shot%2520domain%250Aadaptation%2520potential%2520of%2520FMs%2520by%2520comparing%2520different%2520backbone%2520architectures%2520and%250Aintroducing%2520novel%2520domain-aware%2520components%2520that%2520leverage%2520domain%2520related%2520textual%250Aembeddings.%2520We%2520propose%2520domain%2520adaptive%2520normalization%252C%2520termed%2520as%2520Domino%252C%2520which%250Aexplicitly%2520leverages%2520domain%2520embeddings%2520during%2520fine-tuning%252C%2520thus%2520making%2520the%250Amodel%2520domain%2520aware.%2520Ultimately%252C%2520Domino%2520enables%2520more%2520robust%2520computer%2520vision%250Amodels%2520that%2520can%2520adapt%2520effectively%2520to%2520various%2520unseen%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Aware%20Fine-Tuning%20of%20Foundation%20Models&entry.906535625=Ugur%20Ali%20Kaplan%20and%20Margret%20Keuper%20and%20Anna%20Khoreva%20and%20Dan%20Zhang%20and%20Yumeng%20Li&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20have%20revolutionized%20computer%20vision%2C%20enabling%0Aeffective%20learning%20across%20different%20domains.%20However%2C%20their%20performance%20under%0Adomain%20shift%20is%20yet%20underexplored.%20This%20paper%20investigates%20the%20zero-shot%20domain%0Aadaptation%20potential%20of%20FMs%20by%20comparing%20different%20backbone%20architectures%20and%0Aintroducing%20novel%20domain-aware%20components%20that%20leverage%20domain%20related%20textual%0Aembeddings.%20We%20propose%20domain%20adaptive%20normalization%2C%20termed%20as%20Domino%2C%20which%0Aexplicitly%20leverages%20domain%20embeddings%20during%20fine-tuning%2C%20thus%20making%20the%0Amodel%20domain%20aware.%20Ultimately%2C%20Domino%20enables%20more%20robust%20computer%20vision%0Amodels%20that%20can%20adapt%20effectively%20to%20various%20unseen%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03482v2&entry.124074799=Read"},
{"title": "VEnhancer: Generative Space-Time Enhancement for Video Generation", "author": "Jingwen He and Tianfan Xue and Dongyang Liu and Xinqi Lin and Peng Gao and Dahua Lin and Yu Qiao and Wanli Ouyang and Ziwei Liu", "abstract": "  We present VEnhancer, a generative space-time enhancement framework that\nimproves the existing text-to-video results by adding more details in spatial\ndomain and synthetic detailed motion in temporal domain. Given a generated\nlow-quality video, our approach can increase its spatial and temporal\nresolution simultaneously with arbitrary up-sampling space and time scales\nthrough a unified video diffusion model. Furthermore, VEnhancer effectively\nremoves generated spatial artifacts and temporal flickering of generated\nvideos. To achieve this, basing on a pretrained video diffusion model, we train\na video ControlNet and inject it to the diffusion model as a condition on low\nframe-rate and low-resolution videos. To effectively train this video\nControlNet, we design space-time data augmentation as well as video-aware\nconditioning. Benefiting from the above designs, VEnhancer yields to be stable\nduring training and shares an elegant end-to-end training manner. Extensive\nexperiments show that VEnhancer surpasses existing state-of-the-art video\nsuper-resolution and space-time super-resolution methods in enhancing\nAI-generated videos. Moreover, with VEnhancer, exisiting open-source\nstate-of-the-art text-to-video method, VideoCrafter-2, reaches the top one in\nvideo generation benchmark -- VBench.\n", "link": "http://arxiv.org/abs/2407.07667v1", "date": "2024-07-10", "relevancy": 2.6408, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6702}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6535}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VEnhancer%3A%20Generative%20Space-Time%20Enhancement%20for%20Video%20Generation&body=Title%3A%20VEnhancer%3A%20Generative%20Space-Time%20Enhancement%20for%20Video%20Generation%0AAuthor%3A%20Jingwen%20He%20and%20Tianfan%20Xue%20and%20Dongyang%20Liu%20and%20Xinqi%20Lin%20and%20Peng%20Gao%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Wanli%20Ouyang%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20We%20present%20VEnhancer%2C%20a%20generative%20space-time%20enhancement%20framework%20that%0Aimproves%20the%20existing%20text-to-video%20results%20by%20adding%20more%20details%20in%20spatial%0Adomain%20and%20synthetic%20detailed%20motion%20in%20temporal%20domain.%20Given%20a%20generated%0Alow-quality%20video%2C%20our%20approach%20can%20increase%20its%20spatial%20and%20temporal%0Aresolution%20simultaneously%20with%20arbitrary%20up-sampling%20space%20and%20time%20scales%0Athrough%20a%20unified%20video%20diffusion%20model.%20Furthermore%2C%20VEnhancer%20effectively%0Aremoves%20generated%20spatial%20artifacts%20and%20temporal%20flickering%20of%20generated%0Avideos.%20To%20achieve%20this%2C%20basing%20on%20a%20pretrained%20video%20diffusion%20model%2C%20we%20train%0Aa%20video%20ControlNet%20and%20inject%20it%20to%20the%20diffusion%20model%20as%20a%20condition%20on%20low%0Aframe-rate%20and%20low-resolution%20videos.%20To%20effectively%20train%20this%20video%0AControlNet%2C%20we%20design%20space-time%20data%20augmentation%20as%20well%20as%20video-aware%0Aconditioning.%20Benefiting%20from%20the%20above%20designs%2C%20VEnhancer%20yields%20to%20be%20stable%0Aduring%20training%20and%20shares%20an%20elegant%20end-to-end%20training%20manner.%20Extensive%0Aexperiments%20show%20that%20VEnhancer%20surpasses%20existing%20state-of-the-art%20video%0Asuper-resolution%20and%20space-time%20super-resolution%20methods%20in%20enhancing%0AAI-generated%20videos.%20Moreover%2C%20with%20VEnhancer%2C%20exisiting%20open-source%0Astate-of-the-art%20text-to-video%20method%2C%20VideoCrafter-2%2C%20reaches%20the%20top%20one%20in%0Avideo%20generation%20benchmark%20--%20VBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVEnhancer%253A%2520Generative%2520Space-Time%2520Enhancement%2520for%2520Video%2520Generation%26entry.906535625%3DJingwen%2520He%2520and%2520Tianfan%2520Xue%2520and%2520Dongyang%2520Liu%2520and%2520Xinqi%2520Lin%2520and%2520Peng%2520Gao%2520and%2520Dahua%2520Lin%2520and%2520Yu%2520Qiao%2520and%2520Wanli%2520Ouyang%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520VEnhancer%252C%2520a%2520generative%2520space-time%2520enhancement%2520framework%2520that%250Aimproves%2520the%2520existing%2520text-to-video%2520results%2520by%2520adding%2520more%2520details%2520in%2520spatial%250Adomain%2520and%2520synthetic%2520detailed%2520motion%2520in%2520temporal%2520domain.%2520Given%2520a%2520generated%250Alow-quality%2520video%252C%2520our%2520approach%2520can%2520increase%2520its%2520spatial%2520and%2520temporal%250Aresolution%2520simultaneously%2520with%2520arbitrary%2520up-sampling%2520space%2520and%2520time%2520scales%250Athrough%2520a%2520unified%2520video%2520diffusion%2520model.%2520Furthermore%252C%2520VEnhancer%2520effectively%250Aremoves%2520generated%2520spatial%2520artifacts%2520and%2520temporal%2520flickering%2520of%2520generated%250Avideos.%2520To%2520achieve%2520this%252C%2520basing%2520on%2520a%2520pretrained%2520video%2520diffusion%2520model%252C%2520we%2520train%250Aa%2520video%2520ControlNet%2520and%2520inject%2520it%2520to%2520the%2520diffusion%2520model%2520as%2520a%2520condition%2520on%2520low%250Aframe-rate%2520and%2520low-resolution%2520videos.%2520To%2520effectively%2520train%2520this%2520video%250AControlNet%252C%2520we%2520design%2520space-time%2520data%2520augmentation%2520as%2520well%2520as%2520video-aware%250Aconditioning.%2520Benefiting%2520from%2520the%2520above%2520designs%252C%2520VEnhancer%2520yields%2520to%2520be%2520stable%250Aduring%2520training%2520and%2520shares%2520an%2520elegant%2520end-to-end%2520training%2520manner.%2520Extensive%250Aexperiments%2520show%2520that%2520VEnhancer%2520surpasses%2520existing%2520state-of-the-art%2520video%250Asuper-resolution%2520and%2520space-time%2520super-resolution%2520methods%2520in%2520enhancing%250AAI-generated%2520videos.%2520Moreover%252C%2520with%2520VEnhancer%252C%2520exisiting%2520open-source%250Astate-of-the-art%2520text-to-video%2520method%252C%2520VideoCrafter-2%252C%2520reaches%2520the%2520top%2520one%2520in%250Avideo%2520generation%2520benchmark%2520--%2520VBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VEnhancer%3A%20Generative%20Space-Time%20Enhancement%20for%20Video%20Generation&entry.906535625=Jingwen%20He%20and%20Tianfan%20Xue%20and%20Dongyang%20Liu%20and%20Xinqi%20Lin%20and%20Peng%20Gao%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Wanli%20Ouyang%20and%20Ziwei%20Liu&entry.1292438233=%20%20We%20present%20VEnhancer%2C%20a%20generative%20space-time%20enhancement%20framework%20that%0Aimproves%20the%20existing%20text-to-video%20results%20by%20adding%20more%20details%20in%20spatial%0Adomain%20and%20synthetic%20detailed%20motion%20in%20temporal%20domain.%20Given%20a%20generated%0Alow-quality%20video%2C%20our%20approach%20can%20increase%20its%20spatial%20and%20temporal%0Aresolution%20simultaneously%20with%20arbitrary%20up-sampling%20space%20and%20time%20scales%0Athrough%20a%20unified%20video%20diffusion%20model.%20Furthermore%2C%20VEnhancer%20effectively%0Aremoves%20generated%20spatial%20artifacts%20and%20temporal%20flickering%20of%20generated%0Avideos.%20To%20achieve%20this%2C%20basing%20on%20a%20pretrained%20video%20diffusion%20model%2C%20we%20train%0Aa%20video%20ControlNet%20and%20inject%20it%20to%20the%20diffusion%20model%20as%20a%20condition%20on%20low%0Aframe-rate%20and%20low-resolution%20videos.%20To%20effectively%20train%20this%20video%0AControlNet%2C%20we%20design%20space-time%20data%20augmentation%20as%20well%20as%20video-aware%0Aconditioning.%20Benefiting%20from%20the%20above%20designs%2C%20VEnhancer%20yields%20to%20be%20stable%0Aduring%20training%20and%20shares%20an%20elegant%20end-to-end%20training%20manner.%20Extensive%0Aexperiments%20show%20that%20VEnhancer%20surpasses%20existing%20state-of-the-art%20video%0Asuper-resolution%20and%20space-time%20super-resolution%20methods%20in%20enhancing%0AAI-generated%20videos.%20Moreover%2C%20with%20VEnhancer%2C%20exisiting%20open-source%0Astate-of-the-art%20text-to-video%20method%2C%20VideoCrafter-2%2C%20reaches%20the%20top%20one%20in%0Avideo%20generation%20benchmark%20--%20VBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07667v1&entry.124074799=Read"},
{"title": "Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent", "author": "Quentin Gallou\u00e9dec and Edward Beeching and Cl\u00e9ment Romac and Emmanuel Dellandr\u00e9a", "abstract": "  The search for a general model that can operate seamlessly across multiple\ndomains remains a key goal in machine learning research. The prevailing\nmethodology in Reinforcement Learning (RL) typically limits models to a single\ntask within a unimodal framework, a limitation that contrasts with the broader\nvision of a versatile, multi-domain model. In this paper, we present Jack of\nAll Trades (JAT), a transformer-based model with a unique design optimized for\nhandling sequential decision-making tasks and multi-modal data types. The JAT\nmodel demonstrates its robust capabilities and versatility by achieving strong\nperformance on very different RL benchmarks, along with promising results on\nComputer Vision (CV) and Natural Language Processing (NLP) tasks, all using a\nsingle set of weights. The JAT model marks a significant step towards more\ngeneral, cross-domain AI model design, and notably, it is the first model of\nits kind to be fully open-sourced at https://huggingface.co/jat-project/jat,\nincluding a pioneering general-purpose dataset.\n", "link": "http://arxiv.org/abs/2402.09844v3", "date": "2024-07-10", "relevancy": 2.6258, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5469}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5218}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jack%20of%20All%20Trades%2C%20Master%20of%20Some%2C%20a%20Multi-Purpose%20Transformer%20Agent&body=Title%3A%20Jack%20of%20All%20Trades%2C%20Master%20of%20Some%2C%20a%20Multi-Purpose%20Transformer%20Agent%0AAuthor%3A%20Quentin%20Gallou%C3%A9dec%20and%20Edward%20Beeching%20and%20Cl%C3%A9ment%20Romac%20and%20Emmanuel%20Dellandr%C3%A9a%0AAbstract%3A%20%20%20The%20search%20for%20a%20general%20model%20that%20can%20operate%20seamlessly%20across%20multiple%0Adomains%20remains%20a%20key%20goal%20in%20machine%20learning%20research.%20The%20prevailing%0Amethodology%20in%20Reinforcement%20Learning%20%28RL%29%20typically%20limits%20models%20to%20a%20single%0Atask%20within%20a%20unimodal%20framework%2C%20a%20limitation%20that%20contrasts%20with%20the%20broader%0Avision%20of%20a%20versatile%2C%20multi-domain%20model.%20In%20this%20paper%2C%20we%20present%20Jack%20of%0AAll%20Trades%20%28JAT%29%2C%20a%20transformer-based%20model%20with%20a%20unique%20design%20optimized%20for%0Ahandling%20sequential%20decision-making%20tasks%20and%20multi-modal%20data%20types.%20The%20JAT%0Amodel%20demonstrates%20its%20robust%20capabilities%20and%20versatility%20by%20achieving%20strong%0Aperformance%20on%20very%20different%20RL%20benchmarks%2C%20along%20with%20promising%20results%20on%0AComputer%20Vision%20%28CV%29%20and%20Natural%20Language%20Processing%20%28NLP%29%20tasks%2C%20all%20using%20a%0Asingle%20set%20of%20weights.%20The%20JAT%20model%20marks%20a%20significant%20step%20towards%20more%0Ageneral%2C%20cross-domain%20AI%20model%20design%2C%20and%20notably%2C%20it%20is%20the%20first%20model%20of%0Aits%20kind%20to%20be%20fully%20open-sourced%20at%20https%3A//huggingface.co/jat-project/jat%2C%0Aincluding%20a%20pioneering%20general-purpose%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09844v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJack%2520of%2520All%2520Trades%252C%2520Master%2520of%2520Some%252C%2520a%2520Multi-Purpose%2520Transformer%2520Agent%26entry.906535625%3DQuentin%2520Gallou%25C3%25A9dec%2520and%2520Edward%2520Beeching%2520and%2520Cl%25C3%25A9ment%2520Romac%2520and%2520Emmanuel%2520Dellandr%25C3%25A9a%26entry.1292438233%3D%2520%2520The%2520search%2520for%2520a%2520general%2520model%2520that%2520can%2520operate%2520seamlessly%2520across%2520multiple%250Adomains%2520remains%2520a%2520key%2520goal%2520in%2520machine%2520learning%2520research.%2520The%2520prevailing%250Amethodology%2520in%2520Reinforcement%2520Learning%2520%2528RL%2529%2520typically%2520limits%2520models%2520to%2520a%2520single%250Atask%2520within%2520a%2520unimodal%2520framework%252C%2520a%2520limitation%2520that%2520contrasts%2520with%2520the%2520broader%250Avision%2520of%2520a%2520versatile%252C%2520multi-domain%2520model.%2520In%2520this%2520paper%252C%2520we%2520present%2520Jack%2520of%250AAll%2520Trades%2520%2528JAT%2529%252C%2520a%2520transformer-based%2520model%2520with%2520a%2520unique%2520design%2520optimized%2520for%250Ahandling%2520sequential%2520decision-making%2520tasks%2520and%2520multi-modal%2520data%2520types.%2520The%2520JAT%250Amodel%2520demonstrates%2520its%2520robust%2520capabilities%2520and%2520versatility%2520by%2520achieving%2520strong%250Aperformance%2520on%2520very%2520different%2520RL%2520benchmarks%252C%2520along%2520with%2520promising%2520results%2520on%250AComputer%2520Vision%2520%2528CV%2529%2520and%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520tasks%252C%2520all%2520using%2520a%250Asingle%2520set%2520of%2520weights.%2520The%2520JAT%2520model%2520marks%2520a%2520significant%2520step%2520towards%2520more%250Ageneral%252C%2520cross-domain%2520AI%2520model%2520design%252C%2520and%2520notably%252C%2520it%2520is%2520the%2520first%2520model%2520of%250Aits%2520kind%2520to%2520be%2520fully%2520open-sourced%2520at%2520https%253A//huggingface.co/jat-project/jat%252C%250Aincluding%2520a%2520pioneering%2520general-purpose%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09844v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jack%20of%20All%20Trades%2C%20Master%20of%20Some%2C%20a%20Multi-Purpose%20Transformer%20Agent&entry.906535625=Quentin%20Gallou%C3%A9dec%20and%20Edward%20Beeching%20and%20Cl%C3%A9ment%20Romac%20and%20Emmanuel%20Dellandr%C3%A9a&entry.1292438233=%20%20The%20search%20for%20a%20general%20model%20that%20can%20operate%20seamlessly%20across%20multiple%0Adomains%20remains%20a%20key%20goal%20in%20machine%20learning%20research.%20The%20prevailing%0Amethodology%20in%20Reinforcement%20Learning%20%28RL%29%20typically%20limits%20models%20to%20a%20single%0Atask%20within%20a%20unimodal%20framework%2C%20a%20limitation%20that%20contrasts%20with%20the%20broader%0Avision%20of%20a%20versatile%2C%20multi-domain%20model.%20In%20this%20paper%2C%20we%20present%20Jack%20of%0AAll%20Trades%20%28JAT%29%2C%20a%20transformer-based%20model%20with%20a%20unique%20design%20optimized%20for%0Ahandling%20sequential%20decision-making%20tasks%20and%20multi-modal%20data%20types.%20The%20JAT%0Amodel%20demonstrates%20its%20robust%20capabilities%20and%20versatility%20by%20achieving%20strong%0Aperformance%20on%20very%20different%20RL%20benchmarks%2C%20along%20with%20promising%20results%20on%0AComputer%20Vision%20%28CV%29%20and%20Natural%20Language%20Processing%20%28NLP%29%20tasks%2C%20all%20using%20a%0Asingle%20set%20of%20weights.%20The%20JAT%20model%20marks%20a%20significant%20step%20towards%20more%0Ageneral%2C%20cross-domain%20AI%20model%20design%2C%20and%20notably%2C%20it%20is%20the%20first%20model%20of%0Aits%20kind%20to%20be%20fully%20open-sourced%20at%20https%3A//huggingface.co/jat-project/jat%2C%0Aincluding%20a%20pioneering%20general-purpose%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09844v3&entry.124074799=Read"},
{"title": "Prompting Language-Informed Distribution for Compositional Zero-Shot\n  Learning", "author": "Wentao Bao and Lichang Chen and Heng Huang and Yu Kong", "abstract": "  Compositional zero-shot learning (CZSL) task aims to recognize unseen\ncompositional visual concepts, e.g., sliced tomatoes, where the model is\nlearned only from the seen compositions, e.g., sliced potatoes and red\ntomatoes. Thanks to the prompt tuning on large pre-trained visual language\nmodels such as CLIP, recent literature shows impressively better CZSL\nperformance than traditional vision-based methods. However, the key aspects\nthat impact the generalization to unseen compositions, including the diversity\nand informativeness of class context, and the entanglement between visual\nprimitives, i.e., state and object, are not properly addressed in existing\nCLIP-based CZSL literature. In this paper, we propose a model by prompting the\nlanguage-informed distribution, aka., PLID, for the CZSL task. Specifically,\nthe PLID leverages pre-trained large language models (LLM) to (i) formulate the\nlanguage-informed class distributions which are diverse and informative, and\n(ii) enhance the compositionality of the class embedding. Moreover, a\nvisual-language primitive decomposition (VLPD) module is proposed to\ndynamically fuse the classification decisions from the compositional and the\nprimitive space. Orthogonal to the existing literature of soft, hard, or\ndistributional prompts, our method advocates prompting the LLM-supported class\ndistributions, leading to a better zero-shot generalization. Experimental\nresults on MIT-States, UT-Zappos, and C-GQA datasets show the superior\nperformance of the PLID to the prior arts. Our code and models are released:\nhttps://github.com/Cogito2012/PLID.\n", "link": "http://arxiv.org/abs/2305.14428v3", "date": "2024-07-10", "relevancy": 2.5951, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5414}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5265}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompting%20Language-Informed%20Distribution%20for%20Compositional%20Zero-Shot%0A%20%20Learning&body=Title%3A%20Prompting%20Language-Informed%20Distribution%20for%20Compositional%20Zero-Shot%0A%20%20Learning%0AAuthor%3A%20Wentao%20Bao%20and%20Lichang%20Chen%20and%20Heng%20Huang%20and%20Yu%20Kong%0AAbstract%3A%20%20%20Compositional%20zero-shot%20learning%20%28CZSL%29%20task%20aims%20to%20recognize%20unseen%0Acompositional%20visual%20concepts%2C%20e.g.%2C%20sliced%20tomatoes%2C%20where%20the%20model%20is%0Alearned%20only%20from%20the%20seen%20compositions%2C%20e.g.%2C%20sliced%20potatoes%20and%20red%0Atomatoes.%20Thanks%20to%20the%20prompt%20tuning%20on%20large%20pre-trained%20visual%20language%0Amodels%20such%20as%20CLIP%2C%20recent%20literature%20shows%20impressively%20better%20CZSL%0Aperformance%20than%20traditional%20vision-based%20methods.%20However%2C%20the%20key%20aspects%0Athat%20impact%20the%20generalization%20to%20unseen%20compositions%2C%20including%20the%20diversity%0Aand%20informativeness%20of%20class%20context%2C%20and%20the%20entanglement%20between%20visual%0Aprimitives%2C%20i.e.%2C%20state%20and%20object%2C%20are%20not%20properly%20addressed%20in%20existing%0ACLIP-based%20CZSL%20literature.%20In%20this%20paper%2C%20we%20propose%20a%20model%20by%20prompting%20the%0Alanguage-informed%20distribution%2C%20aka.%2C%20PLID%2C%20for%20the%20CZSL%20task.%20Specifically%2C%0Athe%20PLID%20leverages%20pre-trained%20large%20language%20models%20%28LLM%29%20to%20%28i%29%20formulate%20the%0Alanguage-informed%20class%20distributions%20which%20are%20diverse%20and%20informative%2C%20and%0A%28ii%29%20enhance%20the%20compositionality%20of%20the%20class%20embedding.%20Moreover%2C%20a%0Avisual-language%20primitive%20decomposition%20%28VLPD%29%20module%20is%20proposed%20to%0Adynamically%20fuse%20the%20classification%20decisions%20from%20the%20compositional%20and%20the%0Aprimitive%20space.%20Orthogonal%20to%20the%20existing%20literature%20of%20soft%2C%20hard%2C%20or%0Adistributional%20prompts%2C%20our%20method%20advocates%20prompting%20the%20LLM-supported%20class%0Adistributions%2C%20leading%20to%20a%20better%20zero-shot%20generalization.%20Experimental%0Aresults%20on%20MIT-States%2C%20UT-Zappos%2C%20and%20C-GQA%20datasets%20show%20the%20superior%0Aperformance%20of%20the%20PLID%20to%20the%20prior%20arts.%20Our%20code%20and%20models%20are%20released%3A%0Ahttps%3A//github.com/Cogito2012/PLID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.14428v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompting%2520Language-Informed%2520Distribution%2520for%2520Compositional%2520Zero-Shot%250A%2520%2520Learning%26entry.906535625%3DWentao%2520Bao%2520and%2520Lichang%2520Chen%2520and%2520Heng%2520Huang%2520and%2520Yu%2520Kong%26entry.1292438233%3D%2520%2520Compositional%2520zero-shot%2520learning%2520%2528CZSL%2529%2520task%2520aims%2520to%2520recognize%2520unseen%250Acompositional%2520visual%2520concepts%252C%2520e.g.%252C%2520sliced%2520tomatoes%252C%2520where%2520the%2520model%2520is%250Alearned%2520only%2520from%2520the%2520seen%2520compositions%252C%2520e.g.%252C%2520sliced%2520potatoes%2520and%2520red%250Atomatoes.%2520Thanks%2520to%2520the%2520prompt%2520tuning%2520on%2520large%2520pre-trained%2520visual%2520language%250Amodels%2520such%2520as%2520CLIP%252C%2520recent%2520literature%2520shows%2520impressively%2520better%2520CZSL%250Aperformance%2520than%2520traditional%2520vision-based%2520methods.%2520However%252C%2520the%2520key%2520aspects%250Athat%2520impact%2520the%2520generalization%2520to%2520unseen%2520compositions%252C%2520including%2520the%2520diversity%250Aand%2520informativeness%2520of%2520class%2520context%252C%2520and%2520the%2520entanglement%2520between%2520visual%250Aprimitives%252C%2520i.e.%252C%2520state%2520and%2520object%252C%2520are%2520not%2520properly%2520addressed%2520in%2520existing%250ACLIP-based%2520CZSL%2520literature.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520model%2520by%2520prompting%2520the%250Alanguage-informed%2520distribution%252C%2520aka.%252C%2520PLID%252C%2520for%2520the%2520CZSL%2520task.%2520Specifically%252C%250Athe%2520PLID%2520leverages%2520pre-trained%2520large%2520language%2520models%2520%2528LLM%2529%2520to%2520%2528i%2529%2520formulate%2520the%250Alanguage-informed%2520class%2520distributions%2520which%2520are%2520diverse%2520and%2520informative%252C%2520and%250A%2528ii%2529%2520enhance%2520the%2520compositionality%2520of%2520the%2520class%2520embedding.%2520Moreover%252C%2520a%250Avisual-language%2520primitive%2520decomposition%2520%2528VLPD%2529%2520module%2520is%2520proposed%2520to%250Adynamically%2520fuse%2520the%2520classification%2520decisions%2520from%2520the%2520compositional%2520and%2520the%250Aprimitive%2520space.%2520Orthogonal%2520to%2520the%2520existing%2520literature%2520of%2520soft%252C%2520hard%252C%2520or%250Adistributional%2520prompts%252C%2520our%2520method%2520advocates%2520prompting%2520the%2520LLM-supported%2520class%250Adistributions%252C%2520leading%2520to%2520a%2520better%2520zero-shot%2520generalization.%2520Experimental%250Aresults%2520on%2520MIT-States%252C%2520UT-Zappos%252C%2520and%2520C-GQA%2520datasets%2520show%2520the%2520superior%250Aperformance%2520of%2520the%2520PLID%2520to%2520the%2520prior%2520arts.%2520Our%2520code%2520and%2520models%2520are%2520released%253A%250Ahttps%253A//github.com/Cogito2012/PLID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.14428v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompting%20Language-Informed%20Distribution%20for%20Compositional%20Zero-Shot%0A%20%20Learning&entry.906535625=Wentao%20Bao%20and%20Lichang%20Chen%20and%20Heng%20Huang%20and%20Yu%20Kong&entry.1292438233=%20%20Compositional%20zero-shot%20learning%20%28CZSL%29%20task%20aims%20to%20recognize%20unseen%0Acompositional%20visual%20concepts%2C%20e.g.%2C%20sliced%20tomatoes%2C%20where%20the%20model%20is%0Alearned%20only%20from%20the%20seen%20compositions%2C%20e.g.%2C%20sliced%20potatoes%20and%20red%0Atomatoes.%20Thanks%20to%20the%20prompt%20tuning%20on%20large%20pre-trained%20visual%20language%0Amodels%20such%20as%20CLIP%2C%20recent%20literature%20shows%20impressively%20better%20CZSL%0Aperformance%20than%20traditional%20vision-based%20methods.%20However%2C%20the%20key%20aspects%0Athat%20impact%20the%20generalization%20to%20unseen%20compositions%2C%20including%20the%20diversity%0Aand%20informativeness%20of%20class%20context%2C%20and%20the%20entanglement%20between%20visual%0Aprimitives%2C%20i.e.%2C%20state%20and%20object%2C%20are%20not%20properly%20addressed%20in%20existing%0ACLIP-based%20CZSL%20literature.%20In%20this%20paper%2C%20we%20propose%20a%20model%20by%20prompting%20the%0Alanguage-informed%20distribution%2C%20aka.%2C%20PLID%2C%20for%20the%20CZSL%20task.%20Specifically%2C%0Athe%20PLID%20leverages%20pre-trained%20large%20language%20models%20%28LLM%29%20to%20%28i%29%20formulate%20the%0Alanguage-informed%20class%20distributions%20which%20are%20diverse%20and%20informative%2C%20and%0A%28ii%29%20enhance%20the%20compositionality%20of%20the%20class%20embedding.%20Moreover%2C%20a%0Avisual-language%20primitive%20decomposition%20%28VLPD%29%20module%20is%20proposed%20to%0Adynamically%20fuse%20the%20classification%20decisions%20from%20the%20compositional%20and%20the%0Aprimitive%20space.%20Orthogonal%20to%20the%20existing%20literature%20of%20soft%2C%20hard%2C%20or%0Adistributional%20prompts%2C%20our%20method%20advocates%20prompting%20the%20LLM-supported%20class%0Adistributions%2C%20leading%20to%20a%20better%20zero-shot%20generalization.%20Experimental%0Aresults%20on%20MIT-States%2C%20UT-Zappos%2C%20and%20C-GQA%20datasets%20show%20the%20superior%0Aperformance%20of%20the%20PLID%20to%20the%20prior%20arts.%20Our%20code%20and%20models%20are%20released%3A%0Ahttps%3A//github.com/Cogito2012/PLID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.14428v3&entry.124074799=Read"},
{"title": "Refine, Discriminate and Align: Stealing Encoders via Sample-Wise\n  Prototypes and Multi-Relational Extraction", "author": "Shuchi Wu and Chuan Ma and Kang Wei and Xiaogang Xu and Ming Ding and Yuwen Qian and Tao Xiang", "abstract": "  This paper introduces RDA, a pioneering approach designed to address two\nprimary deficiencies prevalent in previous endeavors aiming at stealing\npre-trained encoders: (1) suboptimal performances attributed to biased\noptimization objectives, and (2) elevated query costs stemming from the\nend-to-end paradigm that necessitates querying the target encoder every epoch.\nSpecifically, we initially Refine the representations of the target encoder for\neach training sample, thereby establishing a less biased optimization objective\nbefore the steal-training phase. This is accomplished via a sample-wise\nprototype, which consolidates the target encoder's representations for a given\nsample's various perspectives. Demanding exponentially fewer queries compared\nto the end-to-end approach, prototypes can be instantiated to guide subsequent\nquery-free training. For more potent efficacy, we develop a multi-relational\nextraction loss that trains the surrogate encoder to Discriminate mismatched\nembedding-prototype pairs while Aligning those matched ones in terms of both\namplitude and angle. In this way, the trained surrogate encoder achieves\nstate-of-the-art results across the board in various downstream datasets with\nlimited queries. Moreover, RDA is shown to be robust to multiple widely-used\ndefenses.\n", "link": "http://arxiv.org/abs/2312.00855v2", "date": "2024-07-10", "relevancy": 2.5782, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5244}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5194}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refine%2C%20Discriminate%20and%20Align%3A%20Stealing%20Encoders%20via%20Sample-Wise%0A%20%20Prototypes%20and%20Multi-Relational%20Extraction&body=Title%3A%20Refine%2C%20Discriminate%20and%20Align%3A%20Stealing%20Encoders%20via%20Sample-Wise%0A%20%20Prototypes%20and%20Multi-Relational%20Extraction%0AAuthor%3A%20Shuchi%20Wu%20and%20Chuan%20Ma%20and%20Kang%20Wei%20and%20Xiaogang%20Xu%20and%20Ming%20Ding%20and%20Yuwen%20Qian%20and%20Tao%20Xiang%0AAbstract%3A%20%20%20This%20paper%20introduces%20RDA%2C%20a%20pioneering%20approach%20designed%20to%20address%20two%0Aprimary%20deficiencies%20prevalent%20in%20previous%20endeavors%20aiming%20at%20stealing%0Apre-trained%20encoders%3A%20%281%29%20suboptimal%20performances%20attributed%20to%20biased%0Aoptimization%20objectives%2C%20and%20%282%29%20elevated%20query%20costs%20stemming%20from%20the%0Aend-to-end%20paradigm%20that%20necessitates%20querying%20the%20target%20encoder%20every%20epoch.%0ASpecifically%2C%20we%20initially%20Refine%20the%20representations%20of%20the%20target%20encoder%20for%0Aeach%20training%20sample%2C%20thereby%20establishing%20a%20less%20biased%20optimization%20objective%0Abefore%20the%20steal-training%20phase.%20This%20is%20accomplished%20via%20a%20sample-wise%0Aprototype%2C%20which%20consolidates%20the%20target%20encoder%27s%20representations%20for%20a%20given%0Asample%27s%20various%20perspectives.%20Demanding%20exponentially%20fewer%20queries%20compared%0Ato%20the%20end-to-end%20approach%2C%20prototypes%20can%20be%20instantiated%20to%20guide%20subsequent%0Aquery-free%20training.%20For%20more%20potent%20efficacy%2C%20we%20develop%20a%20multi-relational%0Aextraction%20loss%20that%20trains%20the%20surrogate%20encoder%20to%20Discriminate%20mismatched%0Aembedding-prototype%20pairs%20while%20Aligning%20those%20matched%20ones%20in%20terms%20of%20both%0Aamplitude%20and%20angle.%20In%20this%20way%2C%20the%20trained%20surrogate%20encoder%20achieves%0Astate-of-the-art%20results%20across%20the%20board%20in%20various%20downstream%20datasets%20with%0Alimited%20queries.%20Moreover%2C%20RDA%20is%20shown%20to%20be%20robust%20to%20multiple%20widely-used%0Adefenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefine%252C%2520Discriminate%2520and%2520Align%253A%2520Stealing%2520Encoders%2520via%2520Sample-Wise%250A%2520%2520Prototypes%2520and%2520Multi-Relational%2520Extraction%26entry.906535625%3DShuchi%2520Wu%2520and%2520Chuan%2520Ma%2520and%2520Kang%2520Wei%2520and%2520Xiaogang%2520Xu%2520and%2520Ming%2520Ding%2520and%2520Yuwen%2520Qian%2520and%2520Tao%2520Xiang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520RDA%252C%2520a%2520pioneering%2520approach%2520designed%2520to%2520address%2520two%250Aprimary%2520deficiencies%2520prevalent%2520in%2520previous%2520endeavors%2520aiming%2520at%2520stealing%250Apre-trained%2520encoders%253A%2520%25281%2529%2520suboptimal%2520performances%2520attributed%2520to%2520biased%250Aoptimization%2520objectives%252C%2520and%2520%25282%2529%2520elevated%2520query%2520costs%2520stemming%2520from%2520the%250Aend-to-end%2520paradigm%2520that%2520necessitates%2520querying%2520the%2520target%2520encoder%2520every%2520epoch.%250ASpecifically%252C%2520we%2520initially%2520Refine%2520the%2520representations%2520of%2520the%2520target%2520encoder%2520for%250Aeach%2520training%2520sample%252C%2520thereby%2520establishing%2520a%2520less%2520biased%2520optimization%2520objective%250Abefore%2520the%2520steal-training%2520phase.%2520This%2520is%2520accomplished%2520via%2520a%2520sample-wise%250Aprototype%252C%2520which%2520consolidates%2520the%2520target%2520encoder%2527s%2520representations%2520for%2520a%2520given%250Asample%2527s%2520various%2520perspectives.%2520Demanding%2520exponentially%2520fewer%2520queries%2520compared%250Ato%2520the%2520end-to-end%2520approach%252C%2520prototypes%2520can%2520be%2520instantiated%2520to%2520guide%2520subsequent%250Aquery-free%2520training.%2520For%2520more%2520potent%2520efficacy%252C%2520we%2520develop%2520a%2520multi-relational%250Aextraction%2520loss%2520that%2520trains%2520the%2520surrogate%2520encoder%2520to%2520Discriminate%2520mismatched%250Aembedding-prototype%2520pairs%2520while%2520Aligning%2520those%2520matched%2520ones%2520in%2520terms%2520of%2520both%250Aamplitude%2520and%2520angle.%2520In%2520this%2520way%252C%2520the%2520trained%2520surrogate%2520encoder%2520achieves%250Astate-of-the-art%2520results%2520across%2520the%2520board%2520in%2520various%2520downstream%2520datasets%2520with%250Alimited%2520queries.%2520Moreover%252C%2520RDA%2520is%2520shown%2520to%2520be%2520robust%2520to%2520multiple%2520widely-used%250Adefenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refine%2C%20Discriminate%20and%20Align%3A%20Stealing%20Encoders%20via%20Sample-Wise%0A%20%20Prototypes%20and%20Multi-Relational%20Extraction&entry.906535625=Shuchi%20Wu%20and%20Chuan%20Ma%20and%20Kang%20Wei%20and%20Xiaogang%20Xu%20and%20Ming%20Ding%20and%20Yuwen%20Qian%20and%20Tao%20Xiang&entry.1292438233=%20%20This%20paper%20introduces%20RDA%2C%20a%20pioneering%20approach%20designed%20to%20address%20two%0Aprimary%20deficiencies%20prevalent%20in%20previous%20endeavors%20aiming%20at%20stealing%0Apre-trained%20encoders%3A%20%281%29%20suboptimal%20performances%20attributed%20to%20biased%0Aoptimization%20objectives%2C%20and%20%282%29%20elevated%20query%20costs%20stemming%20from%20the%0Aend-to-end%20paradigm%20that%20necessitates%20querying%20the%20target%20encoder%20every%20epoch.%0ASpecifically%2C%20we%20initially%20Refine%20the%20representations%20of%20the%20target%20encoder%20for%0Aeach%20training%20sample%2C%20thereby%20establishing%20a%20less%20biased%20optimization%20objective%0Abefore%20the%20steal-training%20phase.%20This%20is%20accomplished%20via%20a%20sample-wise%0Aprototype%2C%20which%20consolidates%20the%20target%20encoder%27s%20representations%20for%20a%20given%0Asample%27s%20various%20perspectives.%20Demanding%20exponentially%20fewer%20queries%20compared%0Ato%20the%20end-to-end%20approach%2C%20prototypes%20can%20be%20instantiated%20to%20guide%20subsequent%0Aquery-free%20training.%20For%20more%20potent%20efficacy%2C%20we%20develop%20a%20multi-relational%0Aextraction%20loss%20that%20trains%20the%20surrogate%20encoder%20to%20Discriminate%20mismatched%0Aembedding-prototype%20pairs%20while%20Aligning%20those%20matched%20ones%20in%20terms%20of%20both%0Aamplitude%20and%20angle.%20In%20this%20way%2C%20the%20trained%20surrogate%20encoder%20achieves%0Astate-of-the-art%20results%20across%20the%20board%20in%20various%20downstream%20datasets%20with%0Alimited%20queries.%20Moreover%2C%20RDA%20is%20shown%20to%20be%20robust%20to%20multiple%20widely-used%0Adefenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00855v2&entry.124074799=Read"},
{"title": "Multi-branch Collaborative Learning Network for 3D Visual Grounding", "author": "Zhipeng Qian and Yiwei Ma and Zhekai Lin and Jiayi Ji and Xiawu Zheng and Xiaoshuai Sun and Rongrong Ji", "abstract": "  3D referring expression comprehension (3DREC) and segmentation (3DRES) have\noverlapping objectives, indicating their potential for collaboration. However,\nexisting collaborative approaches predominantly depend on the results of one\ntask to make predictions for the other, limiting effective collaboration. We\nargue that employing separate branches for 3DREC and 3DRES tasks enhances the\nmodel's capacity to learn specific information for each task, enabling them to\nacquire complementary knowledge. Thus, we propose the MCLN framework, which\nincludes independent branches for 3DREC and 3DRES tasks. This enables dedicated\nexploration of each task and effective coordination between the branches.\nFurthermore, to facilitate mutual reinforcement between these branches, we\nintroduce a Relative Superpoint Aggregation (RSA) module and an Adaptive Soft\nAlignment (ASA) module. These modules significantly contribute to the precise\nalignment of prediction results from the two branches, directing the module to\nallocate increased attention to key positions. Comprehensive experimental\nevaluation demonstrates that our proposed method achieves state-of-the-art\nperformance on both the 3DREC and 3DRES tasks, with an increase of 2.05% in\nAcc@0.5 for 3DREC and 3.96% in mIoU for 3DRES.\n", "link": "http://arxiv.org/abs/2407.05363v2", "date": "2024-07-10", "relevancy": 2.53, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.645}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6408}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-branch%20Collaborative%20Learning%20Network%20for%203D%20Visual%20Grounding&body=Title%3A%20Multi-branch%20Collaborative%20Learning%20Network%20for%203D%20Visual%20Grounding%0AAuthor%3A%20Zhipeng%20Qian%20and%20Yiwei%20Ma%20and%20Zhekai%20Lin%20and%20Jiayi%20Ji%20and%20Xiawu%20Zheng%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%203D%20referring%20expression%20comprehension%20%283DREC%29%20and%20segmentation%20%283DRES%29%20have%0Aoverlapping%20objectives%2C%20indicating%20their%20potential%20for%20collaboration.%20However%2C%0Aexisting%20collaborative%20approaches%20predominantly%20depend%20on%20the%20results%20of%20one%0Atask%20to%20make%20predictions%20for%20the%20other%2C%20limiting%20effective%20collaboration.%20We%0Aargue%20that%20employing%20separate%20branches%20for%203DREC%20and%203DRES%20tasks%20enhances%20the%0Amodel%27s%20capacity%20to%20learn%20specific%20information%20for%20each%20task%2C%20enabling%20them%20to%0Aacquire%20complementary%20knowledge.%20Thus%2C%20we%20propose%20the%20MCLN%20framework%2C%20which%0Aincludes%20independent%20branches%20for%203DREC%20and%203DRES%20tasks.%20This%20enables%20dedicated%0Aexploration%20of%20each%20task%20and%20effective%20coordination%20between%20the%20branches.%0AFurthermore%2C%20to%20facilitate%20mutual%20reinforcement%20between%20these%20branches%2C%20we%0Aintroduce%20a%20Relative%20Superpoint%20Aggregation%20%28RSA%29%20module%20and%20an%20Adaptive%20Soft%0AAlignment%20%28ASA%29%20module.%20These%20modules%20significantly%20contribute%20to%20the%20precise%0Aalignment%20of%20prediction%20results%20from%20the%20two%20branches%2C%20directing%20the%20module%20to%0Aallocate%20increased%20attention%20to%20key%20positions.%20Comprehensive%20experimental%0Aevaluation%20demonstrates%20that%20our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20on%20both%20the%203DREC%20and%203DRES%20tasks%2C%20with%20an%20increase%20of%202.05%25%20in%0AAcc%400.5%20for%203DREC%20and%203.96%25%20in%20mIoU%20for%203DRES.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05363v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-branch%2520Collaborative%2520Learning%2520Network%2520for%25203D%2520Visual%2520Grounding%26entry.906535625%3DZhipeng%2520Qian%2520and%2520Yiwei%2520Ma%2520and%2520Zhekai%2520Lin%2520and%2520Jiayi%2520Ji%2520and%2520Xiawu%2520Zheng%2520and%2520Xiaoshuai%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%25203D%2520referring%2520expression%2520comprehension%2520%25283DREC%2529%2520and%2520segmentation%2520%25283DRES%2529%2520have%250Aoverlapping%2520objectives%252C%2520indicating%2520their%2520potential%2520for%2520collaboration.%2520However%252C%250Aexisting%2520collaborative%2520approaches%2520predominantly%2520depend%2520on%2520the%2520results%2520of%2520one%250Atask%2520to%2520make%2520predictions%2520for%2520the%2520other%252C%2520limiting%2520effective%2520collaboration.%2520We%250Aargue%2520that%2520employing%2520separate%2520branches%2520for%25203DREC%2520and%25203DRES%2520tasks%2520enhances%2520the%250Amodel%2527s%2520capacity%2520to%2520learn%2520specific%2520information%2520for%2520each%2520task%252C%2520enabling%2520them%2520to%250Aacquire%2520complementary%2520knowledge.%2520Thus%252C%2520we%2520propose%2520the%2520MCLN%2520framework%252C%2520which%250Aincludes%2520independent%2520branches%2520for%25203DREC%2520and%25203DRES%2520tasks.%2520This%2520enables%2520dedicated%250Aexploration%2520of%2520each%2520task%2520and%2520effective%2520coordination%2520between%2520the%2520branches.%250AFurthermore%252C%2520to%2520facilitate%2520mutual%2520reinforcement%2520between%2520these%2520branches%252C%2520we%250Aintroduce%2520a%2520Relative%2520Superpoint%2520Aggregation%2520%2528RSA%2529%2520module%2520and%2520an%2520Adaptive%2520Soft%250AAlignment%2520%2528ASA%2529%2520module.%2520These%2520modules%2520significantly%2520contribute%2520to%2520the%2520precise%250Aalignment%2520of%2520prediction%2520results%2520from%2520the%2520two%2520branches%252C%2520directing%2520the%2520module%2520to%250Aallocate%2520increased%2520attention%2520to%2520key%2520positions.%2520Comprehensive%2520experimental%250Aevaluation%2520demonstrates%2520that%2520our%2520proposed%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520both%2520the%25203DREC%2520and%25203DRES%2520tasks%252C%2520with%2520an%2520increase%2520of%25202.05%2525%2520in%250AAcc%25400.5%2520for%25203DREC%2520and%25203.96%2525%2520in%2520mIoU%2520for%25203DRES.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05363v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-branch%20Collaborative%20Learning%20Network%20for%203D%20Visual%20Grounding&entry.906535625=Zhipeng%20Qian%20and%20Yiwei%20Ma%20and%20Zhekai%20Lin%20and%20Jiayi%20Ji%20and%20Xiawu%20Zheng%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%203D%20referring%20expression%20comprehension%20%283DREC%29%20and%20segmentation%20%283DRES%29%20have%0Aoverlapping%20objectives%2C%20indicating%20their%20potential%20for%20collaboration.%20However%2C%0Aexisting%20collaborative%20approaches%20predominantly%20depend%20on%20the%20results%20of%20one%0Atask%20to%20make%20predictions%20for%20the%20other%2C%20limiting%20effective%20collaboration.%20We%0Aargue%20that%20employing%20separate%20branches%20for%203DREC%20and%203DRES%20tasks%20enhances%20the%0Amodel%27s%20capacity%20to%20learn%20specific%20information%20for%20each%20task%2C%20enabling%20them%20to%0Aacquire%20complementary%20knowledge.%20Thus%2C%20we%20propose%20the%20MCLN%20framework%2C%20which%0Aincludes%20independent%20branches%20for%203DREC%20and%203DRES%20tasks.%20This%20enables%20dedicated%0Aexploration%20of%20each%20task%20and%20effective%20coordination%20between%20the%20branches.%0AFurthermore%2C%20to%20facilitate%20mutual%20reinforcement%20between%20these%20branches%2C%20we%0Aintroduce%20a%20Relative%20Superpoint%20Aggregation%20%28RSA%29%20module%20and%20an%20Adaptive%20Soft%0AAlignment%20%28ASA%29%20module.%20These%20modules%20significantly%20contribute%20to%20the%20precise%0Aalignment%20of%20prediction%20results%20from%20the%20two%20branches%2C%20directing%20the%20module%20to%0Aallocate%20increased%20attention%20to%20key%20positions.%20Comprehensive%20experimental%0Aevaluation%20demonstrates%20that%20our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20on%20both%20the%203DREC%20and%203DRES%20tasks%2C%20with%20an%20increase%20of%202.05%25%20in%0AAcc%400.5%20for%203DREC%20and%203.96%25%20in%20mIoU%20for%203DRES.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05363v2&entry.124074799=Read"},
{"title": "Parameter Efficient Fine Tuning for Multi-scanner PET to PET\n  Reconstruction", "author": "Yumin Kim and Gayoon Choi and Seong Jae Hwang", "abstract": "  Reducing scan time in Positron Emission Tomography (PET) imaging while\nmaintaining high-quality images is crucial for minimizing patient discomfort\nand radiation exposure. Due to the limited size of datasets and distribution\ndiscrepancy across scanners in medical imaging, fine-tuning in a\nparameter-efficient and effective manner is on the rise. Motivated by the\npotential of Parameter-Efficient Fine-Tuning (PEFT), we aim to address these\nissues by effectively leveraging PEFT to improve limited data and GPU resource\nissues in multi-scanner setups. In this paper, we introduce PETITE,\nParameter-Efficient Fine-Tuning for MultI-scanner PET to PET REconstruction\nthat uses fewer than 1% of the parameters. To the best of our knowledge, this\nstudy is the first to systematically explore the efficacy of diverse PEFT\ntechniques in medical imaging reconstruction tasks via prevalent\nencoder-decoder-type deep models. This investigation, in particular, brings\nintriguing insights into PETITE as we show further improvements by treating\nencoder and decoder separately and mixing different PEFT methods, namely,\nMix-PEFT. Using multi-scanner PET datasets comprised of five different\nscanners, we extensively test the cross-scanner PET scan time reduction\nperformances (i.e., a model pre-trained on one scanner is fine-tuned on a\ndifferent scanner) of 21 feasible Mix-PEFT combinations to derive optimal\nPETITE. We show that training with less than 1% parameters using PETITE\nperforms on par with full fine-tuning (i.e., 100% parameter)\n", "link": "http://arxiv.org/abs/2407.07517v1", "date": "2024-07-10", "relevancy": 2.5232, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5075}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5032}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter%20Efficient%20Fine%20Tuning%20for%20Multi-scanner%20PET%20to%20PET%0A%20%20Reconstruction&body=Title%3A%20Parameter%20Efficient%20Fine%20Tuning%20for%20Multi-scanner%20PET%20to%20PET%0A%20%20Reconstruction%0AAuthor%3A%20Yumin%20Kim%20and%20Gayoon%20Choi%20and%20Seong%20Jae%20Hwang%0AAbstract%3A%20%20%20Reducing%20scan%20time%20in%20Positron%20Emission%20Tomography%20%28PET%29%20imaging%20while%0Amaintaining%20high-quality%20images%20is%20crucial%20for%20minimizing%20patient%20discomfort%0Aand%20radiation%20exposure.%20Due%20to%20the%20limited%20size%20of%20datasets%20and%20distribution%0Adiscrepancy%20across%20scanners%20in%20medical%20imaging%2C%20fine-tuning%20in%20a%0Aparameter-efficient%20and%20effective%20manner%20is%20on%20the%20rise.%20Motivated%20by%20the%0Apotential%20of%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%2C%20we%20aim%20to%20address%20these%0Aissues%20by%20effectively%20leveraging%20PEFT%20to%20improve%20limited%20data%20and%20GPU%20resource%0Aissues%20in%20multi-scanner%20setups.%20In%20this%20paper%2C%20we%20introduce%20PETITE%2C%0AParameter-Efficient%20Fine-Tuning%20for%20MultI-scanner%20PET%20to%20PET%20REconstruction%0Athat%20uses%20fewer%20than%201%25%20of%20the%20parameters.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Astudy%20is%20the%20first%20to%20systematically%20explore%20the%20efficacy%20of%20diverse%20PEFT%0Atechniques%20in%20medical%20imaging%20reconstruction%20tasks%20via%20prevalent%0Aencoder-decoder-type%20deep%20models.%20This%20investigation%2C%20in%20particular%2C%20brings%0Aintriguing%20insights%20into%20PETITE%20as%20we%20show%20further%20improvements%20by%20treating%0Aencoder%20and%20decoder%20separately%20and%20mixing%20different%20PEFT%20methods%2C%20namely%2C%0AMix-PEFT.%20Using%20multi-scanner%20PET%20datasets%20comprised%20of%20five%20different%0Ascanners%2C%20we%20extensively%20test%20the%20cross-scanner%20PET%20scan%20time%20reduction%0Aperformances%20%28i.e.%2C%20a%20model%20pre-trained%20on%20one%20scanner%20is%20fine-tuned%20on%20a%0Adifferent%20scanner%29%20of%2021%20feasible%20Mix-PEFT%20combinations%20to%20derive%20optimal%0APETITE.%20We%20show%20that%20training%20with%20less%20than%201%25%20parameters%20using%20PETITE%0Aperforms%20on%20par%20with%20full%20fine-tuning%20%28i.e.%2C%20100%25%20parameter%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter%2520Efficient%2520Fine%2520Tuning%2520for%2520Multi-scanner%2520PET%2520to%2520PET%250A%2520%2520Reconstruction%26entry.906535625%3DYumin%2520Kim%2520and%2520Gayoon%2520Choi%2520and%2520Seong%2520Jae%2520Hwang%26entry.1292438233%3D%2520%2520Reducing%2520scan%2520time%2520in%2520Positron%2520Emission%2520Tomography%2520%2528PET%2529%2520imaging%2520while%250Amaintaining%2520high-quality%2520images%2520is%2520crucial%2520for%2520minimizing%2520patient%2520discomfort%250Aand%2520radiation%2520exposure.%2520Due%2520to%2520the%2520limited%2520size%2520of%2520datasets%2520and%2520distribution%250Adiscrepancy%2520across%2520scanners%2520in%2520medical%2520imaging%252C%2520fine-tuning%2520in%2520a%250Aparameter-efficient%2520and%2520effective%2520manner%2520is%2520on%2520the%2520rise.%2520Motivated%2520by%2520the%250Apotential%2520of%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%252C%2520we%2520aim%2520to%2520address%2520these%250Aissues%2520by%2520effectively%2520leveraging%2520PEFT%2520to%2520improve%2520limited%2520data%2520and%2520GPU%2520resource%250Aissues%2520in%2520multi-scanner%2520setups.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PETITE%252C%250AParameter-Efficient%2520Fine-Tuning%2520for%2520MultI-scanner%2520PET%2520to%2520PET%2520REconstruction%250Athat%2520uses%2520fewer%2520than%25201%2525%2520of%2520the%2520parameters.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%250Astudy%2520is%2520the%2520first%2520to%2520systematically%2520explore%2520the%2520efficacy%2520of%2520diverse%2520PEFT%250Atechniques%2520in%2520medical%2520imaging%2520reconstruction%2520tasks%2520via%2520prevalent%250Aencoder-decoder-type%2520deep%2520models.%2520This%2520investigation%252C%2520in%2520particular%252C%2520brings%250Aintriguing%2520insights%2520into%2520PETITE%2520as%2520we%2520show%2520further%2520improvements%2520by%2520treating%250Aencoder%2520and%2520decoder%2520separately%2520and%2520mixing%2520different%2520PEFT%2520methods%252C%2520namely%252C%250AMix-PEFT.%2520Using%2520multi-scanner%2520PET%2520datasets%2520comprised%2520of%2520five%2520different%250Ascanners%252C%2520we%2520extensively%2520test%2520the%2520cross-scanner%2520PET%2520scan%2520time%2520reduction%250Aperformances%2520%2528i.e.%252C%2520a%2520model%2520pre-trained%2520on%2520one%2520scanner%2520is%2520fine-tuned%2520on%2520a%250Adifferent%2520scanner%2529%2520of%252021%2520feasible%2520Mix-PEFT%2520combinations%2520to%2520derive%2520optimal%250APETITE.%2520We%2520show%2520that%2520training%2520with%2520less%2520than%25201%2525%2520parameters%2520using%2520PETITE%250Aperforms%2520on%2520par%2520with%2520full%2520fine-tuning%2520%2528i.e.%252C%2520100%2525%2520parameter%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter%20Efficient%20Fine%20Tuning%20for%20Multi-scanner%20PET%20to%20PET%0A%20%20Reconstruction&entry.906535625=Yumin%20Kim%20and%20Gayoon%20Choi%20and%20Seong%20Jae%20Hwang&entry.1292438233=%20%20Reducing%20scan%20time%20in%20Positron%20Emission%20Tomography%20%28PET%29%20imaging%20while%0Amaintaining%20high-quality%20images%20is%20crucial%20for%20minimizing%20patient%20discomfort%0Aand%20radiation%20exposure.%20Due%20to%20the%20limited%20size%20of%20datasets%20and%20distribution%0Adiscrepancy%20across%20scanners%20in%20medical%20imaging%2C%20fine-tuning%20in%20a%0Aparameter-efficient%20and%20effective%20manner%20is%20on%20the%20rise.%20Motivated%20by%20the%0Apotential%20of%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%2C%20we%20aim%20to%20address%20these%0Aissues%20by%20effectively%20leveraging%20PEFT%20to%20improve%20limited%20data%20and%20GPU%20resource%0Aissues%20in%20multi-scanner%20setups.%20In%20this%20paper%2C%20we%20introduce%20PETITE%2C%0AParameter-Efficient%20Fine-Tuning%20for%20MultI-scanner%20PET%20to%20PET%20REconstruction%0Athat%20uses%20fewer%20than%201%25%20of%20the%20parameters.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Astudy%20is%20the%20first%20to%20systematically%20explore%20the%20efficacy%20of%20diverse%20PEFT%0Atechniques%20in%20medical%20imaging%20reconstruction%20tasks%20via%20prevalent%0Aencoder-decoder-type%20deep%20models.%20This%20investigation%2C%20in%20particular%2C%20brings%0Aintriguing%20insights%20into%20PETITE%20as%20we%20show%20further%20improvements%20by%20treating%0Aencoder%20and%20decoder%20separately%20and%20mixing%20different%20PEFT%20methods%2C%20namely%2C%0AMix-PEFT.%20Using%20multi-scanner%20PET%20datasets%20comprised%20of%20five%20different%0Ascanners%2C%20we%20extensively%20test%20the%20cross-scanner%20PET%20scan%20time%20reduction%0Aperformances%20%28i.e.%2C%20a%20model%20pre-trained%20on%20one%20scanner%20is%20fine-tuned%20on%20a%0Adifferent%20scanner%29%20of%2021%20feasible%20Mix-PEFT%20combinations%20to%20derive%20optimal%0APETITE.%20We%20show%20that%20training%20with%20less%20than%201%25%20parameters%20using%20PETITE%0Aperforms%20on%20par%20with%20full%20fine-tuning%20%28i.e.%2C%20100%25%20parameter%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07517v1&entry.124074799=Read"},
{"title": "Lie Group Decompositions for Equivariant Neural Networks", "author": "Mircea Mironenco and Patrick Forr\u00e9", "abstract": "  Invariance and equivariance to geometrical transformations have proven to be\nvery useful inductive biases when training (convolutional) neural network\nmodels, especially in the low-data regime. Much work has focused on the case\nwhere the symmetry group employed is compact or abelian, or both. Recent work\nhas explored enlarging the class of transformations used to the case of Lie\ngroups, principally through the use of their Lie algebra, as well as the group\nexponential and logarithm maps. The applicability of such methods is limited by\nthe fact that depending on the group of interest $G$, the exponential map may\nnot be surjective. Further limitations are encountered when $G$ is neither\ncompact nor abelian. Using the structure and geometry of Lie groups and their\nhomogeneous spaces, we present a framework by which it is possible to work with\nsuch groups primarily focusing on the groups $G = \\text{GL}^{+}(n, \\mathbb{R})$\nand $G = \\text{SL}(n, \\mathbb{R})$, as well as their representation as affine\ntransformations $\\mathbb{R}^{n} \\rtimes G$. Invariant integration as well as a\nglobal parametrization is realized by a decomposition into subgroups and\nsubmanifolds which can be handled individually. Under this framework, we show\nhow convolution kernels can be parametrized to build models equivariant with\nrespect to affine transformations. We evaluate the robustness and\nout-of-distribution generalisation capability of our model on the benchmark\naffine-invariant classification task, outperforming previous proposals.\n", "link": "http://arxiv.org/abs/2310.11366v2", "date": "2024-07-10", "relevancy": 2.4642, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4993}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4985}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lie%20Group%20Decompositions%20for%20Equivariant%20Neural%20Networks&body=Title%3A%20Lie%20Group%20Decompositions%20for%20Equivariant%20Neural%20Networks%0AAuthor%3A%20Mircea%20Mironenco%20and%20Patrick%20Forr%C3%A9%0AAbstract%3A%20%20%20Invariance%20and%20equivariance%20to%20geometrical%20transformations%20have%20proven%20to%20be%0Avery%20useful%20inductive%20biases%20when%20training%20%28convolutional%29%20neural%20network%0Amodels%2C%20especially%20in%20the%20low-data%20regime.%20Much%20work%20has%20focused%20on%20the%20case%0Awhere%20the%20symmetry%20group%20employed%20is%20compact%20or%20abelian%2C%20or%20both.%20Recent%20work%0Ahas%20explored%20enlarging%20the%20class%20of%20transformations%20used%20to%20the%20case%20of%20Lie%0Agroups%2C%20principally%20through%20the%20use%20of%20their%20Lie%20algebra%2C%20as%20well%20as%20the%20group%0Aexponential%20and%20logarithm%20maps.%20The%20applicability%20of%20such%20methods%20is%20limited%20by%0Athe%20fact%20that%20depending%20on%20the%20group%20of%20interest%20%24G%24%2C%20the%20exponential%20map%20may%0Anot%20be%20surjective.%20Further%20limitations%20are%20encountered%20when%20%24G%24%20is%20neither%0Acompact%20nor%20abelian.%20Using%20the%20structure%20and%20geometry%20of%20Lie%20groups%20and%20their%0Ahomogeneous%20spaces%2C%20we%20present%20a%20framework%20by%20which%20it%20is%20possible%20to%20work%20with%0Asuch%20groups%20primarily%20focusing%20on%20the%20groups%20%24G%20%3D%20%5Ctext%7BGL%7D%5E%7B%2B%7D%28n%2C%20%5Cmathbb%7BR%7D%29%24%0Aand%20%24G%20%3D%20%5Ctext%7BSL%7D%28n%2C%20%5Cmathbb%7BR%7D%29%24%2C%20as%20well%20as%20their%20representation%20as%20affine%0Atransformations%20%24%5Cmathbb%7BR%7D%5E%7Bn%7D%20%5Crtimes%20G%24.%20Invariant%20integration%20as%20well%20as%20a%0Aglobal%20parametrization%20is%20realized%20by%20a%20decomposition%20into%20subgroups%20and%0Asubmanifolds%20which%20can%20be%20handled%20individually.%20Under%20this%20framework%2C%20we%20show%0Ahow%20convolution%20kernels%20can%20be%20parametrized%20to%20build%20models%20equivariant%20with%0Arespect%20to%20affine%20transformations.%20We%20evaluate%20the%20robustness%20and%0Aout-of-distribution%20generalisation%20capability%20of%20our%20model%20on%20the%20benchmark%0Aaffine-invariant%20classification%20task%2C%20outperforming%20previous%20proposals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11366v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLie%2520Group%2520Decompositions%2520for%2520Equivariant%2520Neural%2520Networks%26entry.906535625%3DMircea%2520Mironenco%2520and%2520Patrick%2520Forr%25C3%25A9%26entry.1292438233%3D%2520%2520Invariance%2520and%2520equivariance%2520to%2520geometrical%2520transformations%2520have%2520proven%2520to%2520be%250Avery%2520useful%2520inductive%2520biases%2520when%2520training%2520%2528convolutional%2529%2520neural%2520network%250Amodels%252C%2520especially%2520in%2520the%2520low-data%2520regime.%2520Much%2520work%2520has%2520focused%2520on%2520the%2520case%250Awhere%2520the%2520symmetry%2520group%2520employed%2520is%2520compact%2520or%2520abelian%252C%2520or%2520both.%2520Recent%2520work%250Ahas%2520explored%2520enlarging%2520the%2520class%2520of%2520transformations%2520used%2520to%2520the%2520case%2520of%2520Lie%250Agroups%252C%2520principally%2520through%2520the%2520use%2520of%2520their%2520Lie%2520algebra%252C%2520as%2520well%2520as%2520the%2520group%250Aexponential%2520and%2520logarithm%2520maps.%2520The%2520applicability%2520of%2520such%2520methods%2520is%2520limited%2520by%250Athe%2520fact%2520that%2520depending%2520on%2520the%2520group%2520of%2520interest%2520%2524G%2524%252C%2520the%2520exponential%2520map%2520may%250Anot%2520be%2520surjective.%2520Further%2520limitations%2520are%2520encountered%2520when%2520%2524G%2524%2520is%2520neither%250Acompact%2520nor%2520abelian.%2520Using%2520the%2520structure%2520and%2520geometry%2520of%2520Lie%2520groups%2520and%2520their%250Ahomogeneous%2520spaces%252C%2520we%2520present%2520a%2520framework%2520by%2520which%2520it%2520is%2520possible%2520to%2520work%2520with%250Asuch%2520groups%2520primarily%2520focusing%2520on%2520the%2520groups%2520%2524G%2520%253D%2520%255Ctext%257BGL%257D%255E%257B%252B%257D%2528n%252C%2520%255Cmathbb%257BR%257D%2529%2524%250Aand%2520%2524G%2520%253D%2520%255Ctext%257BSL%257D%2528n%252C%2520%255Cmathbb%257BR%257D%2529%2524%252C%2520as%2520well%2520as%2520their%2520representation%2520as%2520affine%250Atransformations%2520%2524%255Cmathbb%257BR%257D%255E%257Bn%257D%2520%255Crtimes%2520G%2524.%2520Invariant%2520integration%2520as%2520well%2520as%2520a%250Aglobal%2520parametrization%2520is%2520realized%2520by%2520a%2520decomposition%2520into%2520subgroups%2520and%250Asubmanifolds%2520which%2520can%2520be%2520handled%2520individually.%2520Under%2520this%2520framework%252C%2520we%2520show%250Ahow%2520convolution%2520kernels%2520can%2520be%2520parametrized%2520to%2520build%2520models%2520equivariant%2520with%250Arespect%2520to%2520affine%2520transformations.%2520We%2520evaluate%2520the%2520robustness%2520and%250Aout-of-distribution%2520generalisation%2520capability%2520of%2520our%2520model%2520on%2520the%2520benchmark%250Aaffine-invariant%2520classification%2520task%252C%2520outperforming%2520previous%2520proposals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11366v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lie%20Group%20Decompositions%20for%20Equivariant%20Neural%20Networks&entry.906535625=Mircea%20Mironenco%20and%20Patrick%20Forr%C3%A9&entry.1292438233=%20%20Invariance%20and%20equivariance%20to%20geometrical%20transformations%20have%20proven%20to%20be%0Avery%20useful%20inductive%20biases%20when%20training%20%28convolutional%29%20neural%20network%0Amodels%2C%20especially%20in%20the%20low-data%20regime.%20Much%20work%20has%20focused%20on%20the%20case%0Awhere%20the%20symmetry%20group%20employed%20is%20compact%20or%20abelian%2C%20or%20both.%20Recent%20work%0Ahas%20explored%20enlarging%20the%20class%20of%20transformations%20used%20to%20the%20case%20of%20Lie%0Agroups%2C%20principally%20through%20the%20use%20of%20their%20Lie%20algebra%2C%20as%20well%20as%20the%20group%0Aexponential%20and%20logarithm%20maps.%20The%20applicability%20of%20such%20methods%20is%20limited%20by%0Athe%20fact%20that%20depending%20on%20the%20group%20of%20interest%20%24G%24%2C%20the%20exponential%20map%20may%0Anot%20be%20surjective.%20Further%20limitations%20are%20encountered%20when%20%24G%24%20is%20neither%0Acompact%20nor%20abelian.%20Using%20the%20structure%20and%20geometry%20of%20Lie%20groups%20and%20their%0Ahomogeneous%20spaces%2C%20we%20present%20a%20framework%20by%20which%20it%20is%20possible%20to%20work%20with%0Asuch%20groups%20primarily%20focusing%20on%20the%20groups%20%24G%20%3D%20%5Ctext%7BGL%7D%5E%7B%2B%7D%28n%2C%20%5Cmathbb%7BR%7D%29%24%0Aand%20%24G%20%3D%20%5Ctext%7BSL%7D%28n%2C%20%5Cmathbb%7BR%7D%29%24%2C%20as%20well%20as%20their%20representation%20as%20affine%0Atransformations%20%24%5Cmathbb%7BR%7D%5E%7Bn%7D%20%5Crtimes%20G%24.%20Invariant%20integration%20as%20well%20as%20a%0Aglobal%20parametrization%20is%20realized%20by%20a%20decomposition%20into%20subgroups%20and%0Asubmanifolds%20which%20can%20be%20handled%20individually.%20Under%20this%20framework%2C%20we%20show%0Ahow%20convolution%20kernels%20can%20be%20parametrized%20to%20build%20models%20equivariant%20with%0Arespect%20to%20affine%20transformations.%20We%20evaluate%20the%20robustness%20and%0Aout-of-distribution%20generalisation%20capability%20of%20our%20model%20on%20the%20benchmark%0Aaffine-invariant%20classification%20task%2C%20outperforming%20previous%20proposals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11366v2&entry.124074799=Read"},
{"title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model", "author": "Xinrun Du and Zhouliang Yu and Songyang Gao and Ding Pan and Yuyang Cheng and Ziyang Ma and Ruibin Yuan and Xingwei Qu and Jiaheng Liu and Tianyu Zheng and Xinchen Luo and Guorui Zhou and Wenhu Chen and Ge Zhang", "abstract": "  In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.\n", "link": "http://arxiv.org/abs/2404.04167v4", "date": "2024-07-10", "relevancy": 2.4342, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.47}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model&body=Title%3A%20Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model%0AAuthor%3A%20Xinrun%20Du%20and%20Zhouliang%20Yu%20and%20Songyang%20Gao%20and%20Ding%20Pan%20and%20Yuyang%20Cheng%20and%20Ziyang%20Ma%20and%20Ruibin%20Yuan%20and%20Xingwei%20Qu%20and%20Jiaheng%20Liu%20and%20Tianyu%20Zheng%20and%20Xinchen%20Luo%20and%20Guorui%20Zhou%20and%20Wenhu%20Chen%20and%20Ge%20Zhang%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20CT-LLM%2C%20a%202B%20large%20language%20model%20%28LLM%29%20that%0Aillustrates%20a%20pivotal%20shift%20towards%20prioritizing%20the%20Chinese%20language%20in%0Adeveloping%20LLMs.%20Uniquely%20initiated%20from%20scratch%2C%20CT-LLM%20diverges%20from%20the%0Aconventional%20methodology%20by%20primarily%20incorporating%20Chinese%20textual%20data%2C%0Autilizing%20an%20extensive%20corpus%20of%201%2C200%20billion%20tokens%2C%20including%20800%20billion%0AChinese%20tokens%2C%20300%20billion%20English%20tokens%2C%20and%20100%20billion%20code%20tokens.%20This%0Astrategic%20composition%20facilitates%20the%20model%27s%20exceptional%20proficiency%20in%0Aunderstanding%20and%20processing%20Chinese%2C%20a%20capability%20further%20enhanced%20through%0Aalignment%20techniques.%20Demonstrating%20remarkable%20performance%20on%20the%20CHC-Bench%2C%0ACT-LLM%20excels%20in%20Chinese%20language%20tasks%2C%20and%20showcases%20its%20adeptness%20in%20English%0Athrough%20SFT.%20This%20research%20challenges%20the%20prevailing%20paradigm%20of%20training%20LLMs%0Apredominantly%20on%20English%20corpora%20and%20then%20adapting%20them%20to%20other%20languages%2C%0Abroadening%20the%20horizons%20for%20LLM%20training%20methodologies.%20By%20open-sourcing%20the%0Afull%20process%20of%20training%20a%20Chinese%20LLM%2C%20including%20a%20detailed%20data%20processing%0Aprocedure%20with%20the%20obtained%20Massive%20Appropriate%20Pretraining%20Chinese%20Corpus%0A%28MAP-CC%29%2C%20a%20well-chosen%20multidisciplinary%20Chinese%20Hard%20Case%20Benchmark%0A%28CHC-Bench%29%2C%20and%20the%202B-size%20Chinese%20Tiny%20LLM%20%28CT-LLM%29%2C%20we%20aim%20to%20foster%0Afurther%20exploration%20and%20innovation%20in%20both%20academia%20and%20industry%2C%20paving%20the%0Away%20for%20more%20inclusive%20and%20versatile%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04167v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChinese%2520Tiny%2520LLM%253A%2520Pretraining%2520a%2520Chinese-Centric%2520Large%2520Language%2520Model%26entry.906535625%3DXinrun%2520Du%2520and%2520Zhouliang%2520Yu%2520and%2520Songyang%2520Gao%2520and%2520Ding%2520Pan%2520and%2520Yuyang%2520Cheng%2520and%2520Ziyang%2520Ma%2520and%2520Ruibin%2520Yuan%2520and%2520Xingwei%2520Qu%2520and%2520Jiaheng%2520Liu%2520and%2520Tianyu%2520Zheng%2520and%2520Xinchen%2520Luo%2520and%2520Guorui%2520Zhou%2520and%2520Wenhu%2520Chen%2520and%2520Ge%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520CT-LLM%252C%2520a%25202B%2520large%2520language%2520model%2520%2528LLM%2529%2520that%250Aillustrates%2520a%2520pivotal%2520shift%2520towards%2520prioritizing%2520the%2520Chinese%2520language%2520in%250Adeveloping%2520LLMs.%2520Uniquely%2520initiated%2520from%2520scratch%252C%2520CT-LLM%2520diverges%2520from%2520the%250Aconventional%2520methodology%2520by%2520primarily%2520incorporating%2520Chinese%2520textual%2520data%252C%250Autilizing%2520an%2520extensive%2520corpus%2520of%25201%252C200%2520billion%2520tokens%252C%2520including%2520800%2520billion%250AChinese%2520tokens%252C%2520300%2520billion%2520English%2520tokens%252C%2520and%2520100%2520billion%2520code%2520tokens.%2520This%250Astrategic%2520composition%2520facilitates%2520the%2520model%2527s%2520exceptional%2520proficiency%2520in%250Aunderstanding%2520and%2520processing%2520Chinese%252C%2520a%2520capability%2520further%2520enhanced%2520through%250Aalignment%2520techniques.%2520Demonstrating%2520remarkable%2520performance%2520on%2520the%2520CHC-Bench%252C%250ACT-LLM%2520excels%2520in%2520Chinese%2520language%2520tasks%252C%2520and%2520showcases%2520its%2520adeptness%2520in%2520English%250Athrough%2520SFT.%2520This%2520research%2520challenges%2520the%2520prevailing%2520paradigm%2520of%2520training%2520LLMs%250Apredominantly%2520on%2520English%2520corpora%2520and%2520then%2520adapting%2520them%2520to%2520other%2520languages%252C%250Abroadening%2520the%2520horizons%2520for%2520LLM%2520training%2520methodologies.%2520By%2520open-sourcing%2520the%250Afull%2520process%2520of%2520training%2520a%2520Chinese%2520LLM%252C%2520including%2520a%2520detailed%2520data%2520processing%250Aprocedure%2520with%2520the%2520obtained%2520Massive%2520Appropriate%2520Pretraining%2520Chinese%2520Corpus%250A%2528MAP-CC%2529%252C%2520a%2520well-chosen%2520multidisciplinary%2520Chinese%2520Hard%2520Case%2520Benchmark%250A%2528CHC-Bench%2529%252C%2520and%2520the%25202B-size%2520Chinese%2520Tiny%2520LLM%2520%2528CT-LLM%2529%252C%2520we%2520aim%2520to%2520foster%250Afurther%2520exploration%2520and%2520innovation%2520in%2520both%2520academia%2520and%2520industry%252C%2520paving%2520the%250Away%2520for%2520more%2520inclusive%2520and%2520versatile%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04167v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model&entry.906535625=Xinrun%20Du%20and%20Zhouliang%20Yu%20and%20Songyang%20Gao%20and%20Ding%20Pan%20and%20Yuyang%20Cheng%20and%20Ziyang%20Ma%20and%20Ruibin%20Yuan%20and%20Xingwei%20Qu%20and%20Jiaheng%20Liu%20and%20Tianyu%20Zheng%20and%20Xinchen%20Luo%20and%20Guorui%20Zhou%20and%20Wenhu%20Chen%20and%20Ge%20Zhang&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20CT-LLM%2C%20a%202B%20large%20language%20model%20%28LLM%29%20that%0Aillustrates%20a%20pivotal%20shift%20towards%20prioritizing%20the%20Chinese%20language%20in%0Adeveloping%20LLMs.%20Uniquely%20initiated%20from%20scratch%2C%20CT-LLM%20diverges%20from%20the%0Aconventional%20methodology%20by%20primarily%20incorporating%20Chinese%20textual%20data%2C%0Autilizing%20an%20extensive%20corpus%20of%201%2C200%20billion%20tokens%2C%20including%20800%20billion%0AChinese%20tokens%2C%20300%20billion%20English%20tokens%2C%20and%20100%20billion%20code%20tokens.%20This%0Astrategic%20composition%20facilitates%20the%20model%27s%20exceptional%20proficiency%20in%0Aunderstanding%20and%20processing%20Chinese%2C%20a%20capability%20further%20enhanced%20through%0Aalignment%20techniques.%20Demonstrating%20remarkable%20performance%20on%20the%20CHC-Bench%2C%0ACT-LLM%20excels%20in%20Chinese%20language%20tasks%2C%20and%20showcases%20its%20adeptness%20in%20English%0Athrough%20SFT.%20This%20research%20challenges%20the%20prevailing%20paradigm%20of%20training%20LLMs%0Apredominantly%20on%20English%20corpora%20and%20then%20adapting%20them%20to%20other%20languages%2C%0Abroadening%20the%20horizons%20for%20LLM%20training%20methodologies.%20By%20open-sourcing%20the%0Afull%20process%20of%20training%20a%20Chinese%20LLM%2C%20including%20a%20detailed%20data%20processing%0Aprocedure%20with%20the%20obtained%20Massive%20Appropriate%20Pretraining%20Chinese%20Corpus%0A%28MAP-CC%29%2C%20a%20well-chosen%20multidisciplinary%20Chinese%20Hard%20Case%20Benchmark%0A%28CHC-Bench%29%2C%20and%20the%202B-size%20Chinese%20Tiny%20LLM%20%28CT-LLM%29%2C%20we%20aim%20to%20foster%0Afurther%20exploration%20and%20innovation%20in%20both%20academia%20and%20industry%2C%20paving%20the%0Away%20for%20more%20inclusive%20and%20versatile%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04167v4&entry.124074799=Read"},
{"title": "InstructLayout: Instruction-Driven 2D and 3D Layout Synthesis with\n  Semantic Graph Prior", "author": "Chenguo Lin and Yuchen Lin and Yadong Mu", "abstract": "  Comprehending natural language instructions is a charming property for both\n2D and 3D layout synthesis systems. Existing methods implicitly model object\njoint distributions and express object relations, hindering generation's\ncontrollability. We introduce InstructLayout, a novel generative framework that\nintegrates a semantic graph prior and a layout decoder to improve\ncontrollability and fidelity for 2D and 3D layout synthesis. The proposed\nsemantic graph prior learns layout appearances and object distributions\nsimultaneously, demonstrating versatility across various downstream tasks in a\nzero-shot manner. To facilitate the benchmarking for text-driven 2D and 3D\nscene synthesis, we respectively curate two high-quality datasets of\nlayout-instruction pairs from public Internet resources with large language and\nmultimodal models. Extensive experimental results reveal that the proposed\nmethod outperforms existing state-of-the-art approaches by a large margin in\nboth 2D and 3D layout synthesis tasks. Thorough ablation studies confirm the\nefficacy of crucial design components.\n", "link": "http://arxiv.org/abs/2407.07580v1", "date": "2024-07-10", "relevancy": 2.4094, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6849}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.546}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructLayout%3A%20Instruction-Driven%202D%20and%203D%20Layout%20Synthesis%20with%0A%20%20Semantic%20Graph%20Prior&body=Title%3A%20InstructLayout%3A%20Instruction-Driven%202D%20and%203D%20Layout%20Synthesis%20with%0A%20%20Semantic%20Graph%20Prior%0AAuthor%3A%20Chenguo%20Lin%20and%20Yuchen%20Lin%20and%20Yadong%20Mu%0AAbstract%3A%20%20%20Comprehending%20natural%20language%20instructions%20is%20a%20charming%20property%20for%20both%0A2D%20and%203D%20layout%20synthesis%20systems.%20Existing%20methods%20implicitly%20model%20object%0Ajoint%20distributions%20and%20express%20object%20relations%2C%20hindering%20generation%27s%0Acontrollability.%20We%20introduce%20InstructLayout%2C%20a%20novel%20generative%20framework%20that%0Aintegrates%20a%20semantic%20graph%20prior%20and%20a%20layout%20decoder%20to%20improve%0Acontrollability%20and%20fidelity%20for%202D%20and%203D%20layout%20synthesis.%20The%20proposed%0Asemantic%20graph%20prior%20learns%20layout%20appearances%20and%20object%20distributions%0Asimultaneously%2C%20demonstrating%20versatility%20across%20various%20downstream%20tasks%20in%20a%0Azero-shot%20manner.%20To%20facilitate%20the%20benchmarking%20for%20text-driven%202D%20and%203D%0Ascene%20synthesis%2C%20we%20respectively%20curate%20two%20high-quality%20datasets%20of%0Alayout-instruction%20pairs%20from%20public%20Internet%20resources%20with%20large%20language%20and%0Amultimodal%20models.%20Extensive%20experimental%20results%20reveal%20that%20the%20proposed%0Amethod%20outperforms%20existing%20state-of-the-art%20approaches%20by%20a%20large%20margin%20in%0Aboth%202D%20and%203D%20layout%20synthesis%20tasks.%20Thorough%20ablation%20studies%20confirm%20the%0Aefficacy%20of%20crucial%20design%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructLayout%253A%2520Instruction-Driven%25202D%2520and%25203D%2520Layout%2520Synthesis%2520with%250A%2520%2520Semantic%2520Graph%2520Prior%26entry.906535625%3DChenguo%2520Lin%2520and%2520Yuchen%2520Lin%2520and%2520Yadong%2520Mu%26entry.1292438233%3D%2520%2520Comprehending%2520natural%2520language%2520instructions%2520is%2520a%2520charming%2520property%2520for%2520both%250A2D%2520and%25203D%2520layout%2520synthesis%2520systems.%2520Existing%2520methods%2520implicitly%2520model%2520object%250Ajoint%2520distributions%2520and%2520express%2520object%2520relations%252C%2520hindering%2520generation%2527s%250Acontrollability.%2520We%2520introduce%2520InstructLayout%252C%2520a%2520novel%2520generative%2520framework%2520that%250Aintegrates%2520a%2520semantic%2520graph%2520prior%2520and%2520a%2520layout%2520decoder%2520to%2520improve%250Acontrollability%2520and%2520fidelity%2520for%25202D%2520and%25203D%2520layout%2520synthesis.%2520The%2520proposed%250Asemantic%2520graph%2520prior%2520learns%2520layout%2520appearances%2520and%2520object%2520distributions%250Asimultaneously%252C%2520demonstrating%2520versatility%2520across%2520various%2520downstream%2520tasks%2520in%2520a%250Azero-shot%2520manner.%2520To%2520facilitate%2520the%2520benchmarking%2520for%2520text-driven%25202D%2520and%25203D%250Ascene%2520synthesis%252C%2520we%2520respectively%2520curate%2520two%2520high-quality%2520datasets%2520of%250Alayout-instruction%2520pairs%2520from%2520public%2520Internet%2520resources%2520with%2520large%2520language%2520and%250Amultimodal%2520models.%2520Extensive%2520experimental%2520results%2520reveal%2520that%2520the%2520proposed%250Amethod%2520outperforms%2520existing%2520state-of-the-art%2520approaches%2520by%2520a%2520large%2520margin%2520in%250Aboth%25202D%2520and%25203D%2520layout%2520synthesis%2520tasks.%2520Thorough%2520ablation%2520studies%2520confirm%2520the%250Aefficacy%2520of%2520crucial%2520design%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructLayout%3A%20Instruction-Driven%202D%20and%203D%20Layout%20Synthesis%20with%0A%20%20Semantic%20Graph%20Prior&entry.906535625=Chenguo%20Lin%20and%20Yuchen%20Lin%20and%20Yadong%20Mu&entry.1292438233=%20%20Comprehending%20natural%20language%20instructions%20is%20a%20charming%20property%20for%20both%0A2D%20and%203D%20layout%20synthesis%20systems.%20Existing%20methods%20implicitly%20model%20object%0Ajoint%20distributions%20and%20express%20object%20relations%2C%20hindering%20generation%27s%0Acontrollability.%20We%20introduce%20InstructLayout%2C%20a%20novel%20generative%20framework%20that%0Aintegrates%20a%20semantic%20graph%20prior%20and%20a%20layout%20decoder%20to%20improve%0Acontrollability%20and%20fidelity%20for%202D%20and%203D%20layout%20synthesis.%20The%20proposed%0Asemantic%20graph%20prior%20learns%20layout%20appearances%20and%20object%20distributions%0Asimultaneously%2C%20demonstrating%20versatility%20across%20various%20downstream%20tasks%20in%20a%0Azero-shot%20manner.%20To%20facilitate%20the%20benchmarking%20for%20text-driven%202D%20and%203D%0Ascene%20synthesis%2C%20we%20respectively%20curate%20two%20high-quality%20datasets%20of%0Alayout-instruction%20pairs%20from%20public%20Internet%20resources%20with%20large%20language%20and%0Amultimodal%20models.%20Extensive%20experimental%20results%20reveal%20that%20the%20proposed%0Amethod%20outperforms%20existing%20state-of-the-art%20approaches%20by%20a%20large%20margin%20in%0Aboth%202D%20and%203D%20layout%20synthesis%20tasks.%20Thorough%20ablation%20studies%20confirm%20the%0Aefficacy%20of%20crucial%20design%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07580v1&entry.124074799=Read"},
{"title": "Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal\n  Action Localization", "author": "Feixiang Zhou and Bryan Williams and Hossein Rahmani", "abstract": "  Alleviating noisy pseudo labels remains a key challenge in Semi-Supervised\nTemporal Action Localization (SS-TAL). Existing methods often filter pseudo\nlabels based on strict conditions, but they typically assess classification and\nlocalization quality separately, leading to suboptimal pseudo-label ranking and\nselection. In particular, there might be inaccurate pseudo labels within\nselected positives, alongside reliable counterparts erroneously assigned to\nnegatives. To tackle these problems, we propose a novel Adaptive Pseudo-label\nLearning (APL) framework to facilitate better pseudo-label selection.\nSpecifically, to improve the ranking quality, Adaptive Label Quality Assessment\n(ALQA) is proposed to jointly learn classification confidence and localization\nreliability, followed by dynamically selecting pseudo labels based on the joint\nscore. Additionally, we propose an Instance-level Consistency Discriminator\n(ICD) for eliminating ambiguous positives and mining potential positives\nsimultaneously based on inter-instance intrinsic consistency, thereby leading\nto a more precise selection. We further introduce a general unsupervised\nAction-aware Contrastive Pre-training (ACP) to enhance the discrimination both\nwithin actions and between actions and backgrounds, which benefits SS-TAL.\nExtensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate that our\nmethod achieves state-of-the-art performance under various semi-supervised\nsettings.\n", "link": "http://arxiv.org/abs/2407.07673v1", "date": "2024-07-10", "relevancy": 2.3783, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6315}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Adaptive%20Pseudo-label%20Learning%20for%20Semi-Supervised%20Temporal%0A%20%20Action%20Localization&body=Title%3A%20Towards%20Adaptive%20Pseudo-label%20Learning%20for%20Semi-Supervised%20Temporal%0A%20%20Action%20Localization%0AAuthor%3A%20Feixiang%20Zhou%20and%20Bryan%20Williams%20and%20Hossein%20Rahmani%0AAbstract%3A%20%20%20Alleviating%20noisy%20pseudo%20labels%20remains%20a%20key%20challenge%20in%20Semi-Supervised%0ATemporal%20Action%20Localization%20%28SS-TAL%29.%20Existing%20methods%20often%20filter%20pseudo%0Alabels%20based%20on%20strict%20conditions%2C%20but%20they%20typically%20assess%20classification%20and%0Alocalization%20quality%20separately%2C%20leading%20to%20suboptimal%20pseudo-label%20ranking%20and%0Aselection.%20In%20particular%2C%20there%20might%20be%20inaccurate%20pseudo%20labels%20within%0Aselected%20positives%2C%20alongside%20reliable%20counterparts%20erroneously%20assigned%20to%0Anegatives.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20novel%20Adaptive%20Pseudo-label%0ALearning%20%28APL%29%20framework%20to%20facilitate%20better%20pseudo-label%20selection.%0ASpecifically%2C%20to%20improve%20the%20ranking%20quality%2C%20Adaptive%20Label%20Quality%20Assessment%0A%28ALQA%29%20is%20proposed%20to%20jointly%20learn%20classification%20confidence%20and%20localization%0Areliability%2C%20followed%20by%20dynamically%20selecting%20pseudo%20labels%20based%20on%20the%20joint%0Ascore.%20Additionally%2C%20we%20propose%20an%20Instance-level%20Consistency%20Discriminator%0A%28ICD%29%20for%20eliminating%20ambiguous%20positives%20and%20mining%20potential%20positives%0Asimultaneously%20based%20on%20inter-instance%20intrinsic%20consistency%2C%20thereby%20leading%0Ato%20a%20more%20precise%20selection.%20We%20further%20introduce%20a%20general%20unsupervised%0AAction-aware%20Contrastive%20Pre-training%20%28ACP%29%20to%20enhance%20the%20discrimination%20both%0Awithin%20actions%20and%20between%20actions%20and%20backgrounds%2C%20which%20benefits%20SS-TAL.%0AExtensive%20experiments%20on%20THUMOS14%20and%20ActivityNet%20v1.3%20demonstrate%20that%20our%0Amethod%20achieves%20state-of-the-art%20performance%20under%20various%20semi-supervised%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Adaptive%2520Pseudo-label%2520Learning%2520for%2520Semi-Supervised%2520Temporal%250A%2520%2520Action%2520Localization%26entry.906535625%3DFeixiang%2520Zhou%2520and%2520Bryan%2520Williams%2520and%2520Hossein%2520Rahmani%26entry.1292438233%3D%2520%2520Alleviating%2520noisy%2520pseudo%2520labels%2520remains%2520a%2520key%2520challenge%2520in%2520Semi-Supervised%250ATemporal%2520Action%2520Localization%2520%2528SS-TAL%2529.%2520Existing%2520methods%2520often%2520filter%2520pseudo%250Alabels%2520based%2520on%2520strict%2520conditions%252C%2520but%2520they%2520typically%2520assess%2520classification%2520and%250Alocalization%2520quality%2520separately%252C%2520leading%2520to%2520suboptimal%2520pseudo-label%2520ranking%2520and%250Aselection.%2520In%2520particular%252C%2520there%2520might%2520be%2520inaccurate%2520pseudo%2520labels%2520within%250Aselected%2520positives%252C%2520alongside%2520reliable%2520counterparts%2520erroneously%2520assigned%2520to%250Anegatives.%2520To%2520tackle%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%2520Adaptive%2520Pseudo-label%250ALearning%2520%2528APL%2529%2520framework%2520to%2520facilitate%2520better%2520pseudo-label%2520selection.%250ASpecifically%252C%2520to%2520improve%2520the%2520ranking%2520quality%252C%2520Adaptive%2520Label%2520Quality%2520Assessment%250A%2528ALQA%2529%2520is%2520proposed%2520to%2520jointly%2520learn%2520classification%2520confidence%2520and%2520localization%250Areliability%252C%2520followed%2520by%2520dynamically%2520selecting%2520pseudo%2520labels%2520based%2520on%2520the%2520joint%250Ascore.%2520Additionally%252C%2520we%2520propose%2520an%2520Instance-level%2520Consistency%2520Discriminator%250A%2528ICD%2529%2520for%2520eliminating%2520ambiguous%2520positives%2520and%2520mining%2520potential%2520positives%250Asimultaneously%2520based%2520on%2520inter-instance%2520intrinsic%2520consistency%252C%2520thereby%2520leading%250Ato%2520a%2520more%2520precise%2520selection.%2520We%2520further%2520introduce%2520a%2520general%2520unsupervised%250AAction-aware%2520Contrastive%2520Pre-training%2520%2528ACP%2529%2520to%2520enhance%2520the%2520discrimination%2520both%250Awithin%2520actions%2520and%2520between%2520actions%2520and%2520backgrounds%252C%2520which%2520benefits%2520SS-TAL.%250AExtensive%2520experiments%2520on%2520THUMOS14%2520and%2520ActivityNet%2520v1.3%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520state-of-the-art%2520performance%2520under%2520various%2520semi-supervised%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Adaptive%20Pseudo-label%20Learning%20for%20Semi-Supervised%20Temporal%0A%20%20Action%20Localization&entry.906535625=Feixiang%20Zhou%20and%20Bryan%20Williams%20and%20Hossein%20Rahmani&entry.1292438233=%20%20Alleviating%20noisy%20pseudo%20labels%20remains%20a%20key%20challenge%20in%20Semi-Supervised%0ATemporal%20Action%20Localization%20%28SS-TAL%29.%20Existing%20methods%20often%20filter%20pseudo%0Alabels%20based%20on%20strict%20conditions%2C%20but%20they%20typically%20assess%20classification%20and%0Alocalization%20quality%20separately%2C%20leading%20to%20suboptimal%20pseudo-label%20ranking%20and%0Aselection.%20In%20particular%2C%20there%20might%20be%20inaccurate%20pseudo%20labels%20within%0Aselected%20positives%2C%20alongside%20reliable%20counterparts%20erroneously%20assigned%20to%0Anegatives.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20novel%20Adaptive%20Pseudo-label%0ALearning%20%28APL%29%20framework%20to%20facilitate%20better%20pseudo-label%20selection.%0ASpecifically%2C%20to%20improve%20the%20ranking%20quality%2C%20Adaptive%20Label%20Quality%20Assessment%0A%28ALQA%29%20is%20proposed%20to%20jointly%20learn%20classification%20confidence%20and%20localization%0Areliability%2C%20followed%20by%20dynamically%20selecting%20pseudo%20labels%20based%20on%20the%20joint%0Ascore.%20Additionally%2C%20we%20propose%20an%20Instance-level%20Consistency%20Discriminator%0A%28ICD%29%20for%20eliminating%20ambiguous%20positives%20and%20mining%20potential%20positives%0Asimultaneously%20based%20on%20inter-instance%20intrinsic%20consistency%2C%20thereby%20leading%0Ato%20a%20more%20precise%20selection.%20We%20further%20introduce%20a%20general%20unsupervised%0AAction-aware%20Contrastive%20Pre-training%20%28ACP%29%20to%20enhance%20the%20discrimination%20both%0Awithin%20actions%20and%20between%20actions%20and%20backgrounds%2C%20which%20benefits%20SS-TAL.%0AExtensive%20experiments%20on%20THUMOS14%20and%20ActivityNet%20v1.3%20demonstrate%20that%20our%0Amethod%20achieves%20state-of-the-art%20performance%20under%20various%20semi-supervised%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07673v1&entry.124074799=Read"},
{"title": "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning\n  Instruction Using Language Model", "author": "Wenqi Zhang and Zhenglin Cheng and Yuanyu He and Mengna Wang and Yongliang Shen and Zeqi Tan and Guiyang Hou and Mingqian He and Yanna Ma and Weiming Lu and Yueting Zhuang", "abstract": "  Although most current large multimodal models (LMMs) can already understand\nphotos of natural scenes and portraits, their understanding of abstract images,\ne.g., charts, maps, or layouts, and visual reasoning capabilities remains quite\nrudimentary. They often struggle with simple daily tasks, such as reading time\nfrom a clock, understanding a flowchart, or planning a route using a road map.\nIn light of this, we design a multi-modal self-instruct, utilizing large\nlanguage models and their code capabilities to synthesize massive abstract\nimages and visual reasoning instructions across daily scenarios. Our strategy\neffortlessly creates a multimodal benchmark with 11,193 instructions for eight\nvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,\nrelation graphs, floor plans, and visual puzzles. \\textbf{This benchmark,\nconstructed with simple lines and geometric elements, exposes the shortcomings\nof most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract image\nunderstanding, spatial relations reasoning, and visual element induction.\nBesides, to verify the quality of our synthetic data, we fine-tune an LMM using\n62,476 synthetic chart, table and road map instructions. The results\ndemonstrate improved chart understanding and map navigation performance, and\nalso demonstrate potential benefits for other visual reasoning tasks. Our code\nis available at: \\url{https://github.com/zwq2018/Multi-modal-Self-instruct}.\n", "link": "http://arxiv.org/abs/2407.07053v2", "date": "2024-07-10", "relevancy": 2.3753, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5972}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Self-Instruct%3A%20Synthetic%20Abstract%20Image%20and%20Visual%20Reasoning%0A%20%20Instruction%20Using%20Language%20Model&body=Title%3A%20Multimodal%20Self-Instruct%3A%20Synthetic%20Abstract%20Image%20and%20Visual%20Reasoning%0A%20%20Instruction%20Using%20Language%20Model%0AAuthor%3A%20Wenqi%20Zhang%20and%20Zhenglin%20Cheng%20and%20Yuanyu%20He%20and%20Mengna%20Wang%20and%20Yongliang%20Shen%20and%20Zeqi%20Tan%20and%20Guiyang%20Hou%20and%20Mingqian%20He%20and%20Yanna%20Ma%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Although%20most%20current%20large%20multimodal%20models%20%28LMMs%29%20can%20already%20understand%0Aphotos%20of%20natural%20scenes%20and%20portraits%2C%20their%20understanding%20of%20abstract%20images%2C%0Ae.g.%2C%20charts%2C%20maps%2C%20or%20layouts%2C%20and%20visual%20reasoning%20capabilities%20remains%20quite%0Arudimentary.%20They%20often%20struggle%20with%20simple%20daily%20tasks%2C%20such%20as%20reading%20time%0Afrom%20a%20clock%2C%20understanding%20a%20flowchart%2C%20or%20planning%20a%20route%20using%20a%20road%20map.%0AIn%20light%20of%20this%2C%20we%20design%20a%20multi-modal%20self-instruct%2C%20utilizing%20large%0Alanguage%20models%20and%20their%20code%20capabilities%20to%20synthesize%20massive%20abstract%0Aimages%20and%20visual%20reasoning%20instructions%20across%20daily%20scenarios.%20Our%20strategy%0Aeffortlessly%20creates%20a%20multimodal%20benchmark%20with%2011%2C193%20instructions%20for%20eight%0Avisual%20scenarios%3A%20charts%2C%20tables%2C%20simulated%20maps%2C%20dashboards%2C%20flowcharts%2C%0Arelation%20graphs%2C%20floor%20plans%2C%20and%20visual%20puzzles.%20%5Ctextbf%7BThis%20benchmark%2C%0Aconstructed%20with%20simple%20lines%20and%20geometric%20elements%2C%20exposes%20the%20shortcomings%0Aof%20most%20advanced%20LMMs%7D%20like%20Claude-3.5-Sonnet%20and%20GPT-4o%20in%20abstract%20image%0Aunderstanding%2C%20spatial%20relations%20reasoning%2C%20and%20visual%20element%20induction.%0ABesides%2C%20to%20verify%20the%20quality%20of%20our%20synthetic%20data%2C%20we%20fine-tune%20an%20LMM%20using%0A62%2C476%20synthetic%20chart%2C%20table%20and%20road%20map%20instructions.%20The%20results%0Ademonstrate%20improved%20chart%20understanding%20and%20map%20navigation%20performance%2C%20and%0Aalso%20demonstrate%20potential%20benefits%20for%20other%20visual%20reasoning%20tasks.%20Our%20code%0Ais%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/zwq2018/Multi-modal-Self-instruct%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07053v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Self-Instruct%253A%2520Synthetic%2520Abstract%2520Image%2520and%2520Visual%2520Reasoning%250A%2520%2520Instruction%2520Using%2520Language%2520Model%26entry.906535625%3DWenqi%2520Zhang%2520and%2520Zhenglin%2520Cheng%2520and%2520Yuanyu%2520He%2520and%2520Mengna%2520Wang%2520and%2520Yongliang%2520Shen%2520and%2520Zeqi%2520Tan%2520and%2520Guiyang%2520Hou%2520and%2520Mingqian%2520He%2520and%2520Yanna%2520Ma%2520and%2520Weiming%2520Lu%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Although%2520most%2520current%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520can%2520already%2520understand%250Aphotos%2520of%2520natural%2520scenes%2520and%2520portraits%252C%2520their%2520understanding%2520of%2520abstract%2520images%252C%250Ae.g.%252C%2520charts%252C%2520maps%252C%2520or%2520layouts%252C%2520and%2520visual%2520reasoning%2520capabilities%2520remains%2520quite%250Arudimentary.%2520They%2520often%2520struggle%2520with%2520simple%2520daily%2520tasks%252C%2520such%2520as%2520reading%2520time%250Afrom%2520a%2520clock%252C%2520understanding%2520a%2520flowchart%252C%2520or%2520planning%2520a%2520route%2520using%2520a%2520road%2520map.%250AIn%2520light%2520of%2520this%252C%2520we%2520design%2520a%2520multi-modal%2520self-instruct%252C%2520utilizing%2520large%250Alanguage%2520models%2520and%2520their%2520code%2520capabilities%2520to%2520synthesize%2520massive%2520abstract%250Aimages%2520and%2520visual%2520reasoning%2520instructions%2520across%2520daily%2520scenarios.%2520Our%2520strategy%250Aeffortlessly%2520creates%2520a%2520multimodal%2520benchmark%2520with%252011%252C193%2520instructions%2520for%2520eight%250Avisual%2520scenarios%253A%2520charts%252C%2520tables%252C%2520simulated%2520maps%252C%2520dashboards%252C%2520flowcharts%252C%250Arelation%2520graphs%252C%2520floor%2520plans%252C%2520and%2520visual%2520puzzles.%2520%255Ctextbf%257BThis%2520benchmark%252C%250Aconstructed%2520with%2520simple%2520lines%2520and%2520geometric%2520elements%252C%2520exposes%2520the%2520shortcomings%250Aof%2520most%2520advanced%2520LMMs%257D%2520like%2520Claude-3.5-Sonnet%2520and%2520GPT-4o%2520in%2520abstract%2520image%250Aunderstanding%252C%2520spatial%2520relations%2520reasoning%252C%2520and%2520visual%2520element%2520induction.%250ABesides%252C%2520to%2520verify%2520the%2520quality%2520of%2520our%2520synthetic%2520data%252C%2520we%2520fine-tune%2520an%2520LMM%2520using%250A62%252C476%2520synthetic%2520chart%252C%2520table%2520and%2520road%2520map%2520instructions.%2520The%2520results%250Ademonstrate%2520improved%2520chart%2520understanding%2520and%2520map%2520navigation%2520performance%252C%2520and%250Aalso%2520demonstrate%2520potential%2520benefits%2520for%2520other%2520visual%2520reasoning%2520tasks.%2520Our%2520code%250Ais%2520available%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/zwq2018/Multi-modal-Self-instruct%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07053v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Self-Instruct%3A%20Synthetic%20Abstract%20Image%20and%20Visual%20Reasoning%0A%20%20Instruction%20Using%20Language%20Model&entry.906535625=Wenqi%20Zhang%20and%20Zhenglin%20Cheng%20and%20Yuanyu%20He%20and%20Mengna%20Wang%20and%20Yongliang%20Shen%20and%20Zeqi%20Tan%20and%20Guiyang%20Hou%20and%20Mingqian%20He%20and%20Yanna%20Ma%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Although%20most%20current%20large%20multimodal%20models%20%28LMMs%29%20can%20already%20understand%0Aphotos%20of%20natural%20scenes%20and%20portraits%2C%20their%20understanding%20of%20abstract%20images%2C%0Ae.g.%2C%20charts%2C%20maps%2C%20or%20layouts%2C%20and%20visual%20reasoning%20capabilities%20remains%20quite%0Arudimentary.%20They%20often%20struggle%20with%20simple%20daily%20tasks%2C%20such%20as%20reading%20time%0Afrom%20a%20clock%2C%20understanding%20a%20flowchart%2C%20or%20planning%20a%20route%20using%20a%20road%20map.%0AIn%20light%20of%20this%2C%20we%20design%20a%20multi-modal%20self-instruct%2C%20utilizing%20large%0Alanguage%20models%20and%20their%20code%20capabilities%20to%20synthesize%20massive%20abstract%0Aimages%20and%20visual%20reasoning%20instructions%20across%20daily%20scenarios.%20Our%20strategy%0Aeffortlessly%20creates%20a%20multimodal%20benchmark%20with%2011%2C193%20instructions%20for%20eight%0Avisual%20scenarios%3A%20charts%2C%20tables%2C%20simulated%20maps%2C%20dashboards%2C%20flowcharts%2C%0Arelation%20graphs%2C%20floor%20plans%2C%20and%20visual%20puzzles.%20%5Ctextbf%7BThis%20benchmark%2C%0Aconstructed%20with%20simple%20lines%20and%20geometric%20elements%2C%20exposes%20the%20shortcomings%0Aof%20most%20advanced%20LMMs%7D%20like%20Claude-3.5-Sonnet%20and%20GPT-4o%20in%20abstract%20image%0Aunderstanding%2C%20spatial%20relations%20reasoning%2C%20and%20visual%20element%20induction.%0ABesides%2C%20to%20verify%20the%20quality%20of%20our%20synthetic%20data%2C%20we%20fine-tune%20an%20LMM%20using%0A62%2C476%20synthetic%20chart%2C%20table%20and%20road%20map%20instructions.%20The%20results%0Ademonstrate%20improved%20chart%20understanding%20and%20map%20navigation%20performance%2C%20and%0Aalso%20demonstrate%20potential%20benefits%20for%20other%20visual%20reasoning%20tasks.%20Our%20code%0Ais%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/zwq2018/Multi-modal-Self-instruct%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07053v2&entry.124074799=Read"},
{"title": "Green Screen Augmentation Enables Scene Generalisation in Robotic\n  Manipulation", "author": "Eugene Teoh and Sumit Patidar and Xiao Ma and Stephen James", "abstract": "  Generalising vision-based manipulation policies to novel environments remains\na challenging area with limited exploration. Current practices involve\ncollecting data in one location, training imitation learning or reinforcement\nlearning policies with this data, and deploying the policy in the same\nlocation. However, this approach lacks scalability as it necessitates data\ncollection in multiple locations for each task. This paper proposes a novel\napproach where data is collected in a location predominantly featuring green\nscreens. We introduce Green-screen Augmentation (GreenAug), employing a chroma\nkey algorithm to overlay background textures onto a green screen. Through\nextensive real-world empirical studies with over 850 training demonstrations\nand 8.2k evaluation episodes, we demonstrate that GreenAug surpasses no\naugmentation, standard computer vision augmentation, and prior generative\naugmentation methods in performance. While no algorithmic novelties are\nclaimed, our paper advocates for a fundamental shift in data collection\npractices. We propose that real-world demonstrations in future research should\nutilise green screens, followed by the application of GreenAug. We believe\nGreenAug unlocks policy generalisation to visually distinct novel locations,\naddressing the current scene generalisation limitations in robot learning.\n", "link": "http://arxiv.org/abs/2407.07868v1", "date": "2024-07-10", "relevancy": 2.3408, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6528}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.595}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Green%20Screen%20Augmentation%20Enables%20Scene%20Generalisation%20in%20Robotic%0A%20%20Manipulation&body=Title%3A%20Green%20Screen%20Augmentation%20Enables%20Scene%20Generalisation%20in%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Eugene%20Teoh%20and%20Sumit%20Patidar%20and%20Xiao%20Ma%20and%20Stephen%20James%0AAbstract%3A%20%20%20Generalising%20vision-based%20manipulation%20policies%20to%20novel%20environments%20remains%0Aa%20challenging%20area%20with%20limited%20exploration.%20Current%20practices%20involve%0Acollecting%20data%20in%20one%20location%2C%20training%20imitation%20learning%20or%20reinforcement%0Alearning%20policies%20with%20this%20data%2C%20and%20deploying%20the%20policy%20in%20the%20same%0Alocation.%20However%2C%20this%20approach%20lacks%20scalability%20as%20it%20necessitates%20data%0Acollection%20in%20multiple%20locations%20for%20each%20task.%20This%20paper%20proposes%20a%20novel%0Aapproach%20where%20data%20is%20collected%20in%20a%20location%20predominantly%20featuring%20green%0Ascreens.%20We%20introduce%20Green-screen%20Augmentation%20%28GreenAug%29%2C%20employing%20a%20chroma%0Akey%20algorithm%20to%20overlay%20background%20textures%20onto%20a%20green%20screen.%20Through%0Aextensive%20real-world%20empirical%20studies%20with%20over%20850%20training%20demonstrations%0Aand%208.2k%20evaluation%20episodes%2C%20we%20demonstrate%20that%20GreenAug%20surpasses%20no%0Aaugmentation%2C%20standard%20computer%20vision%20augmentation%2C%20and%20prior%20generative%0Aaugmentation%20methods%20in%20performance.%20While%20no%20algorithmic%20novelties%20are%0Aclaimed%2C%20our%20paper%20advocates%20for%20a%20fundamental%20shift%20in%20data%20collection%0Apractices.%20We%20propose%20that%20real-world%20demonstrations%20in%20future%20research%20should%0Autilise%20green%20screens%2C%20followed%20by%20the%20application%20of%20GreenAug.%20We%20believe%0AGreenAug%20unlocks%20policy%20generalisation%20to%20visually%20distinct%20novel%20locations%2C%0Aaddressing%20the%20current%20scene%20generalisation%20limitations%20in%20robot%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreen%2520Screen%2520Augmentation%2520Enables%2520Scene%2520Generalisation%2520in%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DEugene%2520Teoh%2520and%2520Sumit%2520Patidar%2520and%2520Xiao%2520Ma%2520and%2520Stephen%2520James%26entry.1292438233%3D%2520%2520Generalising%2520vision-based%2520manipulation%2520policies%2520to%2520novel%2520environments%2520remains%250Aa%2520challenging%2520area%2520with%2520limited%2520exploration.%2520Current%2520practices%2520involve%250Acollecting%2520data%2520in%2520one%2520location%252C%2520training%2520imitation%2520learning%2520or%2520reinforcement%250Alearning%2520policies%2520with%2520this%2520data%252C%2520and%2520deploying%2520the%2520policy%2520in%2520the%2520same%250Alocation.%2520However%252C%2520this%2520approach%2520lacks%2520scalability%2520as%2520it%2520necessitates%2520data%250Acollection%2520in%2520multiple%2520locations%2520for%2520each%2520task.%2520This%2520paper%2520proposes%2520a%2520novel%250Aapproach%2520where%2520data%2520is%2520collected%2520in%2520a%2520location%2520predominantly%2520featuring%2520green%250Ascreens.%2520We%2520introduce%2520Green-screen%2520Augmentation%2520%2528GreenAug%2529%252C%2520employing%2520a%2520chroma%250Akey%2520algorithm%2520to%2520overlay%2520background%2520textures%2520onto%2520a%2520green%2520screen.%2520Through%250Aextensive%2520real-world%2520empirical%2520studies%2520with%2520over%2520850%2520training%2520demonstrations%250Aand%25208.2k%2520evaluation%2520episodes%252C%2520we%2520demonstrate%2520that%2520GreenAug%2520surpasses%2520no%250Aaugmentation%252C%2520standard%2520computer%2520vision%2520augmentation%252C%2520and%2520prior%2520generative%250Aaugmentation%2520methods%2520in%2520performance.%2520While%2520no%2520algorithmic%2520novelties%2520are%250Aclaimed%252C%2520our%2520paper%2520advocates%2520for%2520a%2520fundamental%2520shift%2520in%2520data%2520collection%250Apractices.%2520We%2520propose%2520that%2520real-world%2520demonstrations%2520in%2520future%2520research%2520should%250Autilise%2520green%2520screens%252C%2520followed%2520by%2520the%2520application%2520of%2520GreenAug.%2520We%2520believe%250AGreenAug%2520unlocks%2520policy%2520generalisation%2520to%2520visually%2520distinct%2520novel%2520locations%252C%250Aaddressing%2520the%2520current%2520scene%2520generalisation%2520limitations%2520in%2520robot%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Green%20Screen%20Augmentation%20Enables%20Scene%20Generalisation%20in%20Robotic%0A%20%20Manipulation&entry.906535625=Eugene%20Teoh%20and%20Sumit%20Patidar%20and%20Xiao%20Ma%20and%20Stephen%20James&entry.1292438233=%20%20Generalising%20vision-based%20manipulation%20policies%20to%20novel%20environments%20remains%0Aa%20challenging%20area%20with%20limited%20exploration.%20Current%20practices%20involve%0Acollecting%20data%20in%20one%20location%2C%20training%20imitation%20learning%20or%20reinforcement%0Alearning%20policies%20with%20this%20data%2C%20and%20deploying%20the%20policy%20in%20the%20same%0Alocation.%20However%2C%20this%20approach%20lacks%20scalability%20as%20it%20necessitates%20data%0Acollection%20in%20multiple%20locations%20for%20each%20task.%20This%20paper%20proposes%20a%20novel%0Aapproach%20where%20data%20is%20collected%20in%20a%20location%20predominantly%20featuring%20green%0Ascreens.%20We%20introduce%20Green-screen%20Augmentation%20%28GreenAug%29%2C%20employing%20a%20chroma%0Akey%20algorithm%20to%20overlay%20background%20textures%20onto%20a%20green%20screen.%20Through%0Aextensive%20real-world%20empirical%20studies%20with%20over%20850%20training%20demonstrations%0Aand%208.2k%20evaluation%20episodes%2C%20we%20demonstrate%20that%20GreenAug%20surpasses%20no%0Aaugmentation%2C%20standard%20computer%20vision%20augmentation%2C%20and%20prior%20generative%0Aaugmentation%20methods%20in%20performance.%20While%20no%20algorithmic%20novelties%20are%0Aclaimed%2C%20our%20paper%20advocates%20for%20a%20fundamental%20shift%20in%20data%20collection%0Apractices.%20We%20propose%20that%20real-world%20demonstrations%20in%20future%20research%20should%0Autilise%20green%20screens%2C%20followed%20by%20the%20application%20of%20GreenAug.%20We%20believe%0AGreenAug%20unlocks%20policy%20generalisation%20to%20visually%20distinct%20novel%20locations%2C%0Aaddressing%20the%20current%20scene%20generalisation%20limitations%20in%20robot%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07868v1&entry.124074799=Read"},
{"title": "Estimating the stability number of a random graph using convolutional\n  neural networks", "author": "Randy Davila", "abstract": "  Graph combinatorial optimization problems are widely applicable and\nnotoriously difficult to compute; for example, consider the traveling salesman\nor facility location problems. In this paper, we explore the feasibility of\nusing convolutional neural networks (CNNs) on graph images to predict the\ncardinality of combinatorial properties of random graphs and networks.\nSpecifically, we use image representations of modified adjacency matrices of\nrandom graphs as training samples for a CNN model to predict the stability\nnumber of random graphs; where the stability number is the cardinality of a\nmaximum set of vertices containing no pairwise adjacency. Our approach\ndemonstrates the potential for applying deep learning in combinatorial\noptimization problems.\n", "link": "http://arxiv.org/abs/2407.07827v1", "date": "2024-07-10", "relevancy": 2.3368, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4854}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4626}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20the%20stability%20number%20of%20a%20random%20graph%20using%20convolutional%0A%20%20neural%20networks&body=Title%3A%20Estimating%20the%20stability%20number%20of%20a%20random%20graph%20using%20convolutional%0A%20%20neural%20networks%0AAuthor%3A%20Randy%20Davila%0AAbstract%3A%20%20%20Graph%20combinatorial%20optimization%20problems%20are%20widely%20applicable%20and%0Anotoriously%20difficult%20to%20compute%3B%20for%20example%2C%20consider%20the%20traveling%20salesman%0Aor%20facility%20location%20problems.%20In%20this%20paper%2C%20we%20explore%20the%20feasibility%20of%0Ausing%20convolutional%20neural%20networks%20%28CNNs%29%20on%20graph%20images%20to%20predict%20the%0Acardinality%20of%20combinatorial%20properties%20of%20random%20graphs%20and%20networks.%0ASpecifically%2C%20we%20use%20image%20representations%20of%20modified%20adjacency%20matrices%20of%0Arandom%20graphs%20as%20training%20samples%20for%20a%20CNN%20model%20to%20predict%20the%20stability%0Anumber%20of%20random%20graphs%3B%20where%20the%20stability%20number%20is%20the%20cardinality%20of%20a%0Amaximum%20set%20of%20vertices%20containing%20no%20pairwise%20adjacency.%20Our%20approach%0Ademonstrates%20the%20potential%20for%20applying%20deep%20learning%20in%20combinatorial%0Aoptimization%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520the%2520stability%2520number%2520of%2520a%2520random%2520graph%2520using%2520convolutional%250A%2520%2520neural%2520networks%26entry.906535625%3DRandy%2520Davila%26entry.1292438233%3D%2520%2520Graph%2520combinatorial%2520optimization%2520problems%2520are%2520widely%2520applicable%2520and%250Anotoriously%2520difficult%2520to%2520compute%253B%2520for%2520example%252C%2520consider%2520the%2520traveling%2520salesman%250Aor%2520facility%2520location%2520problems.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520feasibility%2520of%250Ausing%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520on%2520graph%2520images%2520to%2520predict%2520the%250Acardinality%2520of%2520combinatorial%2520properties%2520of%2520random%2520graphs%2520and%2520networks.%250ASpecifically%252C%2520we%2520use%2520image%2520representations%2520of%2520modified%2520adjacency%2520matrices%2520of%250Arandom%2520graphs%2520as%2520training%2520samples%2520for%2520a%2520CNN%2520model%2520to%2520predict%2520the%2520stability%250Anumber%2520of%2520random%2520graphs%253B%2520where%2520the%2520stability%2520number%2520is%2520the%2520cardinality%2520of%2520a%250Amaximum%2520set%2520of%2520vertices%2520containing%2520no%2520pairwise%2520adjacency.%2520Our%2520approach%250Ademonstrates%2520the%2520potential%2520for%2520applying%2520deep%2520learning%2520in%2520combinatorial%250Aoptimization%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20the%20stability%20number%20of%20a%20random%20graph%20using%20convolutional%0A%20%20neural%20networks&entry.906535625=Randy%20Davila&entry.1292438233=%20%20Graph%20combinatorial%20optimization%20problems%20are%20widely%20applicable%20and%0Anotoriously%20difficult%20to%20compute%3B%20for%20example%2C%20consider%20the%20traveling%20salesman%0Aor%20facility%20location%20problems.%20In%20this%20paper%2C%20we%20explore%20the%20feasibility%20of%0Ausing%20convolutional%20neural%20networks%20%28CNNs%29%20on%20graph%20images%20to%20predict%20the%0Acardinality%20of%20combinatorial%20properties%20of%20random%20graphs%20and%20networks.%0ASpecifically%2C%20we%20use%20image%20representations%20of%20modified%20adjacency%20matrices%20of%0Arandom%20graphs%20as%20training%20samples%20for%20a%20CNN%20model%20to%20predict%20the%20stability%0Anumber%20of%20random%20graphs%3B%20where%20the%20stability%20number%20is%20the%20cardinality%20of%20a%0Amaximum%20set%20of%20vertices%20containing%20no%20pairwise%20adjacency.%20Our%20approach%0Ademonstrates%20the%20potential%20for%20applying%20deep%20learning%20in%20combinatorial%0Aoptimization%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07827v1&entry.124074799=Read"},
{"title": "Invisible Optical Adversarial Stripes on Traffic Sign against Autonomous\n  Vehicles", "author": "Dongfang Guo and Yuting Wu and Yimin Dai and Pengfei Zhou and Xin Lou and Rui Tan", "abstract": "  Camera-based computer vision is essential to autonomous vehicle's perception.\nThis paper presents an attack that uses light-emitting diodes and exploits the\ncamera's rolling shutter effect to create adversarial stripes in the captured\nimages to mislead traffic sign recognition. The attack is stealthy because the\nstripes on the traffic sign are invisible to human. For the attack to be\nthreatening, the recognition results need to be stable over consecutive image\nframes. To achieve this, we design and implement GhostStripe, an attack system\nthat controls the timing of the modulated light emission to adapt to camera\noperations and victim vehicle movements. Evaluated on real testbeds,\nGhostStripe can stably spoof the traffic sign recognition results for up to\n94\\% of frames to a wrong class when the victim vehicle passes the road\nsection. In reality, such attack effect may fool victim vehicles into\nlife-threatening incidents. We discuss the countermeasures at the levels of\ncamera sensor, perception model, and autonomous driving system.\n", "link": "http://arxiv.org/abs/2407.07510v1", "date": "2024-07-10", "relevancy": 2.3299, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4686}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4651}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invisible%20Optical%20Adversarial%20Stripes%20on%20Traffic%20Sign%20against%20Autonomous%0A%20%20Vehicles&body=Title%3A%20Invisible%20Optical%20Adversarial%20Stripes%20on%20Traffic%20Sign%20against%20Autonomous%0A%20%20Vehicles%0AAuthor%3A%20Dongfang%20Guo%20and%20Yuting%20Wu%20and%20Yimin%20Dai%20and%20Pengfei%20Zhou%20and%20Xin%20Lou%20and%20Rui%20Tan%0AAbstract%3A%20%20%20Camera-based%20computer%20vision%20is%20essential%20to%20autonomous%20vehicle%27s%20perception.%0AThis%20paper%20presents%20an%20attack%20that%20uses%20light-emitting%20diodes%20and%20exploits%20the%0Acamera%27s%20rolling%20shutter%20effect%20to%20create%20adversarial%20stripes%20in%20the%20captured%0Aimages%20to%20mislead%20traffic%20sign%20recognition.%20The%20attack%20is%20stealthy%20because%20the%0Astripes%20on%20the%20traffic%20sign%20are%20invisible%20to%20human.%20For%20the%20attack%20to%20be%0Athreatening%2C%20the%20recognition%20results%20need%20to%20be%20stable%20over%20consecutive%20image%0Aframes.%20To%20achieve%20this%2C%20we%20design%20and%20implement%20GhostStripe%2C%20an%20attack%20system%0Athat%20controls%20the%20timing%20of%20the%20modulated%20light%20emission%20to%20adapt%20to%20camera%0Aoperations%20and%20victim%20vehicle%20movements.%20Evaluated%20on%20real%20testbeds%2C%0AGhostStripe%20can%20stably%20spoof%20the%20traffic%20sign%20recognition%20results%20for%20up%20to%0A94%5C%25%20of%20frames%20to%20a%20wrong%20class%20when%20the%20victim%20vehicle%20passes%20the%20road%0Asection.%20In%20reality%2C%20such%20attack%20effect%20may%20fool%20victim%20vehicles%20into%0Alife-threatening%20incidents.%20We%20discuss%20the%20countermeasures%20at%20the%20levels%20of%0Acamera%20sensor%2C%20perception%20model%2C%20and%20autonomous%20driving%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvisible%2520Optical%2520Adversarial%2520Stripes%2520on%2520Traffic%2520Sign%2520against%2520Autonomous%250A%2520%2520Vehicles%26entry.906535625%3DDongfang%2520Guo%2520and%2520Yuting%2520Wu%2520and%2520Yimin%2520Dai%2520and%2520Pengfei%2520Zhou%2520and%2520Xin%2520Lou%2520and%2520Rui%2520Tan%26entry.1292438233%3D%2520%2520Camera-based%2520computer%2520vision%2520is%2520essential%2520to%2520autonomous%2520vehicle%2527s%2520perception.%250AThis%2520paper%2520presents%2520an%2520attack%2520that%2520uses%2520light-emitting%2520diodes%2520and%2520exploits%2520the%250Acamera%2527s%2520rolling%2520shutter%2520effect%2520to%2520create%2520adversarial%2520stripes%2520in%2520the%2520captured%250Aimages%2520to%2520mislead%2520traffic%2520sign%2520recognition.%2520The%2520attack%2520is%2520stealthy%2520because%2520the%250Astripes%2520on%2520the%2520traffic%2520sign%2520are%2520invisible%2520to%2520human.%2520For%2520the%2520attack%2520to%2520be%250Athreatening%252C%2520the%2520recognition%2520results%2520need%2520to%2520be%2520stable%2520over%2520consecutive%2520image%250Aframes.%2520To%2520achieve%2520this%252C%2520we%2520design%2520and%2520implement%2520GhostStripe%252C%2520an%2520attack%2520system%250Athat%2520controls%2520the%2520timing%2520of%2520the%2520modulated%2520light%2520emission%2520to%2520adapt%2520to%2520camera%250Aoperations%2520and%2520victim%2520vehicle%2520movements.%2520Evaluated%2520on%2520real%2520testbeds%252C%250AGhostStripe%2520can%2520stably%2520spoof%2520the%2520traffic%2520sign%2520recognition%2520results%2520for%2520up%2520to%250A94%255C%2525%2520of%2520frames%2520to%2520a%2520wrong%2520class%2520when%2520the%2520victim%2520vehicle%2520passes%2520the%2520road%250Asection.%2520In%2520reality%252C%2520such%2520attack%2520effect%2520may%2520fool%2520victim%2520vehicles%2520into%250Alife-threatening%2520incidents.%2520We%2520discuss%2520the%2520countermeasures%2520at%2520the%2520levels%2520of%250Acamera%2520sensor%252C%2520perception%2520model%252C%2520and%2520autonomous%2520driving%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invisible%20Optical%20Adversarial%20Stripes%20on%20Traffic%20Sign%20against%20Autonomous%0A%20%20Vehicles&entry.906535625=Dongfang%20Guo%20and%20Yuting%20Wu%20and%20Yimin%20Dai%20and%20Pengfei%20Zhou%20and%20Xin%20Lou%20and%20Rui%20Tan&entry.1292438233=%20%20Camera-based%20computer%20vision%20is%20essential%20to%20autonomous%20vehicle%27s%20perception.%0AThis%20paper%20presents%20an%20attack%20that%20uses%20light-emitting%20diodes%20and%20exploits%20the%0Acamera%27s%20rolling%20shutter%20effect%20to%20create%20adversarial%20stripes%20in%20the%20captured%0Aimages%20to%20mislead%20traffic%20sign%20recognition.%20The%20attack%20is%20stealthy%20because%20the%0Astripes%20on%20the%20traffic%20sign%20are%20invisible%20to%20human.%20For%20the%20attack%20to%20be%0Athreatening%2C%20the%20recognition%20results%20need%20to%20be%20stable%20over%20consecutive%20image%0Aframes.%20To%20achieve%20this%2C%20we%20design%20and%20implement%20GhostStripe%2C%20an%20attack%20system%0Athat%20controls%20the%20timing%20of%20the%20modulated%20light%20emission%20to%20adapt%20to%20camera%0Aoperations%20and%20victim%20vehicle%20movements.%20Evaluated%20on%20real%20testbeds%2C%0AGhostStripe%20can%20stably%20spoof%20the%20traffic%20sign%20recognition%20results%20for%20up%20to%0A94%5C%25%20of%20frames%20to%20a%20wrong%20class%20when%20the%20victim%20vehicle%20passes%20the%20road%0Asection.%20In%20reality%2C%20such%20attack%20effect%20may%20fool%20victim%20vehicles%20into%0Alife-threatening%20incidents.%20We%20discuss%20the%20countermeasures%20at%20the%20levels%20of%0Acamera%20sensor%2C%20perception%20model%2C%20and%20autonomous%20driving%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07510v1&entry.124074799=Read"},
{"title": "PaliGemma: A versatile 3B VLM for transfer", "author": "Lucas Beyer and Andreas Steiner and Andr\u00e9 Susano Pinto and Alexander Kolesnikov and Xiao Wang and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bo\u0161njak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai", "abstract": "  PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation.\n", "link": "http://arxiv.org/abs/2407.07726v1", "date": "2024-07-10", "relevancy": 2.3289, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4931}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaliGemma%3A%20A%20versatile%203B%20VLM%20for%20transfer&body=Title%3A%20PaliGemma%3A%20A%20versatile%203B%20VLM%20for%20transfer%0AAuthor%3A%20Lucas%20Beyer%20and%20Andreas%20Steiner%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Alexander%20Kolesnikov%20and%20Xiao%20Wang%20and%20Daniel%20Salz%20and%20Maxim%20Neumann%20and%20Ibrahim%20Alabdulmohsin%20and%20Michael%20Tschannen%20and%20Emanuele%20Bugliarello%20and%20Thomas%20Unterthiner%20and%20Daniel%20Keysers%20and%20Skanda%20Koppula%20and%20Fangyu%20Liu%20and%20Adam%20Grycner%20and%20Alexey%20Gritsenko%20and%20Neil%20Houlsby%20and%20Manoj%20Kumar%20and%20Keran%20Rong%20and%20Julian%20Eisenschlos%20and%20Rishabh%20Kabra%20and%20Matthias%20Bauer%20and%20Matko%20Bo%C5%A1njak%20and%20Xi%20Chen%20and%20Matthias%20Minderer%20and%20Paul%20Voigtlaender%20and%20Ioana%20Bica%20and%20Ivana%20Balazevic%20and%20Joan%20Puigcerver%20and%20Pinelopi%20Papalampidi%20and%20Olivier%20Henaff%20and%20Xi%20Xiong%20and%20Radu%20Soricut%20and%20Jeremiah%20Harmsen%20and%20Xiaohua%20Zhai%0AAbstract%3A%20%20%20PaliGemma%20is%20an%20open%20Vision-Language%20Model%20%28VLM%29%20that%20is%20based%20on%20the%0ASigLIP-So400m%20vision%20encoder%20and%20the%20Gemma-2B%20language%20model.%20It%20is%20trained%20to%0Abe%20a%20versatile%20and%20broadly%20knowledgeable%20base%20model%20that%20is%20effective%20to%0Atransfer.%20It%20achieves%20strong%20performance%20on%20a%20wide%20variety%20of%20open-world%20tasks.%0AWe%20evaluate%20PaliGemma%20on%20almost%2040%20diverse%20tasks%20including%20standard%20VLM%0Abenchmarks%2C%20but%20also%20more%20specialized%20tasks%20such%20as%20remote-sensing%20and%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaliGemma%253A%2520A%2520versatile%25203B%2520VLM%2520for%2520transfer%26entry.906535625%3DLucas%2520Beyer%2520and%2520Andreas%2520Steiner%2520and%2520Andr%25C3%25A9%2520Susano%2520Pinto%2520and%2520Alexander%2520Kolesnikov%2520and%2520Xiao%2520Wang%2520and%2520Daniel%2520Salz%2520and%2520Maxim%2520Neumann%2520and%2520Ibrahim%2520Alabdulmohsin%2520and%2520Michael%2520Tschannen%2520and%2520Emanuele%2520Bugliarello%2520and%2520Thomas%2520Unterthiner%2520and%2520Daniel%2520Keysers%2520and%2520Skanda%2520Koppula%2520and%2520Fangyu%2520Liu%2520and%2520Adam%2520Grycner%2520and%2520Alexey%2520Gritsenko%2520and%2520Neil%2520Houlsby%2520and%2520Manoj%2520Kumar%2520and%2520Keran%2520Rong%2520and%2520Julian%2520Eisenschlos%2520and%2520Rishabh%2520Kabra%2520and%2520Matthias%2520Bauer%2520and%2520Matko%2520Bo%25C5%25A1njak%2520and%2520Xi%2520Chen%2520and%2520Matthias%2520Minderer%2520and%2520Paul%2520Voigtlaender%2520and%2520Ioana%2520Bica%2520and%2520Ivana%2520Balazevic%2520and%2520Joan%2520Puigcerver%2520and%2520Pinelopi%2520Papalampidi%2520and%2520Olivier%2520Henaff%2520and%2520Xi%2520Xiong%2520and%2520Radu%2520Soricut%2520and%2520Jeremiah%2520Harmsen%2520and%2520Xiaohua%2520Zhai%26entry.1292438233%3D%2520%2520PaliGemma%2520is%2520an%2520open%2520Vision-Language%2520Model%2520%2528VLM%2529%2520that%2520is%2520based%2520on%2520the%250ASigLIP-So400m%2520vision%2520encoder%2520and%2520the%2520Gemma-2B%2520language%2520model.%2520It%2520is%2520trained%2520to%250Abe%2520a%2520versatile%2520and%2520broadly%2520knowledgeable%2520base%2520model%2520that%2520is%2520effective%2520to%250Atransfer.%2520It%2520achieves%2520strong%2520performance%2520on%2520a%2520wide%2520variety%2520of%2520open-world%2520tasks.%250AWe%2520evaluate%2520PaliGemma%2520on%2520almost%252040%2520diverse%2520tasks%2520including%2520standard%2520VLM%250Abenchmarks%252C%2520but%2520also%2520more%2520specialized%2520tasks%2520such%2520as%2520remote-sensing%2520and%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaliGemma%3A%20A%20versatile%203B%20VLM%20for%20transfer&entry.906535625=Lucas%20Beyer%20and%20Andreas%20Steiner%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Alexander%20Kolesnikov%20and%20Xiao%20Wang%20and%20Daniel%20Salz%20and%20Maxim%20Neumann%20and%20Ibrahim%20Alabdulmohsin%20and%20Michael%20Tschannen%20and%20Emanuele%20Bugliarello%20and%20Thomas%20Unterthiner%20and%20Daniel%20Keysers%20and%20Skanda%20Koppula%20and%20Fangyu%20Liu%20and%20Adam%20Grycner%20and%20Alexey%20Gritsenko%20and%20Neil%20Houlsby%20and%20Manoj%20Kumar%20and%20Keran%20Rong%20and%20Julian%20Eisenschlos%20and%20Rishabh%20Kabra%20and%20Matthias%20Bauer%20and%20Matko%20Bo%C5%A1njak%20and%20Xi%20Chen%20and%20Matthias%20Minderer%20and%20Paul%20Voigtlaender%20and%20Ioana%20Bica%20and%20Ivana%20Balazevic%20and%20Joan%20Puigcerver%20and%20Pinelopi%20Papalampidi%20and%20Olivier%20Henaff%20and%20Xi%20Xiong%20and%20Radu%20Soricut%20and%20Jeremiah%20Harmsen%20and%20Xiaohua%20Zhai&entry.1292438233=%20%20PaliGemma%20is%20an%20open%20Vision-Language%20Model%20%28VLM%29%20that%20is%20based%20on%20the%0ASigLIP-So400m%20vision%20encoder%20and%20the%20Gemma-2B%20language%20model.%20It%20is%20trained%20to%0Abe%20a%20versatile%20and%20broadly%20knowledgeable%20base%20model%20that%20is%20effective%20to%0Atransfer.%20It%20achieves%20strong%20performance%20on%20a%20wide%20variety%20of%20open-world%20tasks.%0AWe%20evaluate%20PaliGemma%20on%20almost%2040%20diverse%20tasks%20including%20standard%20VLM%0Abenchmarks%2C%20but%20also%20more%20specialized%20tasks%20such%20as%20remote-sensing%20and%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07726v1&entry.124074799=Read"},
{"title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning\n  Performance of Large Language Models", "author": "Mosh Levy and Alon Jacoby and Yoav Goldberg", "abstract": "  This paper explores the impact of extending input lengths on the capabilities\nof Large Language Models (LLMs). Despite LLMs advancements in recent times,\ntheir performance consistency across different input lengths is not well\nunderstood. We investigate this aspect by introducing a novel QA reasoning\nframework, specifically designed to assess the impact of input length. We\nisolate the effect of input length using multiple versions of the same sample,\neach being extended with padding of different lengths, types and locations. Our\nfindings show a notable degradation in LLMs' reasoning performance at much\nshorter input lengths than their technical maximum. We show that the\ndegradation trend appears in every version of our dataset, although at\ndifferent intensities. Additionally, our study reveals that the traditional\nmetric of next word prediction correlates negatively with performance of LLMs'\non our reasoning dataset. We analyse our results and identify failure modes\nthat can serve as useful guides for future research, potentially informing\nstrategies to address the limitations observed in LLMs.\n", "link": "http://arxiv.org/abs/2402.14848v2", "date": "2024-07-10", "relevancy": 2.3207, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4883}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.454}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Same%20Task%2C%20More%20Tokens%3A%20the%20Impact%20of%20Input%20Length%20on%20the%20Reasoning%0A%20%20Performance%20of%20Large%20Language%20Models&body=Title%3A%20Same%20Task%2C%20More%20Tokens%3A%20the%20Impact%20of%20Input%20Length%20on%20the%20Reasoning%0A%20%20Performance%20of%20Large%20Language%20Models%0AAuthor%3A%20Mosh%20Levy%20and%20Alon%20Jacoby%20and%20Yoav%20Goldberg%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20impact%20of%20extending%20input%20lengths%20on%20the%20capabilities%0Aof%20Large%20Language%20Models%20%28LLMs%29.%20Despite%20LLMs%20advancements%20in%20recent%20times%2C%0Atheir%20performance%20consistency%20across%20different%20input%20lengths%20is%20not%20well%0Aunderstood.%20We%20investigate%20this%20aspect%20by%20introducing%20a%20novel%20QA%20reasoning%0Aframework%2C%20specifically%20designed%20to%20assess%20the%20impact%20of%20input%20length.%20We%0Aisolate%20the%20effect%20of%20input%20length%20using%20multiple%20versions%20of%20the%20same%20sample%2C%0Aeach%20being%20extended%20with%20padding%20of%20different%20lengths%2C%20types%20and%20locations.%20Our%0Afindings%20show%20a%20notable%20degradation%20in%20LLMs%27%20reasoning%20performance%20at%20much%0Ashorter%20input%20lengths%20than%20their%20technical%20maximum.%20We%20show%20that%20the%0Adegradation%20trend%20appears%20in%20every%20version%20of%20our%20dataset%2C%20although%20at%0Adifferent%20intensities.%20Additionally%2C%20our%20study%20reveals%20that%20the%20traditional%0Ametric%20of%20next%20word%20prediction%20correlates%20negatively%20with%20performance%20of%20LLMs%27%0Aon%20our%20reasoning%20dataset.%20We%20analyse%20our%20results%20and%20identify%20failure%20modes%0Athat%20can%20serve%20as%20useful%20guides%20for%20future%20research%2C%20potentially%20informing%0Astrategies%20to%20address%20the%20limitations%20observed%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14848v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSame%2520Task%252C%2520More%2520Tokens%253A%2520the%2520Impact%2520of%2520Input%2520Length%2520on%2520the%2520Reasoning%250A%2520%2520Performance%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DMosh%2520Levy%2520and%2520Alon%2520Jacoby%2520and%2520Yoav%2520Goldberg%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520impact%2520of%2520extending%2520input%2520lengths%2520on%2520the%2520capabilities%250Aof%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Despite%2520LLMs%2520advancements%2520in%2520recent%2520times%252C%250Atheir%2520performance%2520consistency%2520across%2520different%2520input%2520lengths%2520is%2520not%2520well%250Aunderstood.%2520We%2520investigate%2520this%2520aspect%2520by%2520introducing%2520a%2520novel%2520QA%2520reasoning%250Aframework%252C%2520specifically%2520designed%2520to%2520assess%2520the%2520impact%2520of%2520input%2520length.%2520We%250Aisolate%2520the%2520effect%2520of%2520input%2520length%2520using%2520multiple%2520versions%2520of%2520the%2520same%2520sample%252C%250Aeach%2520being%2520extended%2520with%2520padding%2520of%2520different%2520lengths%252C%2520types%2520and%2520locations.%2520Our%250Afindings%2520show%2520a%2520notable%2520degradation%2520in%2520LLMs%2527%2520reasoning%2520performance%2520at%2520much%250Ashorter%2520input%2520lengths%2520than%2520their%2520technical%2520maximum.%2520We%2520show%2520that%2520the%250Adegradation%2520trend%2520appears%2520in%2520every%2520version%2520of%2520our%2520dataset%252C%2520although%2520at%250Adifferent%2520intensities.%2520Additionally%252C%2520our%2520study%2520reveals%2520that%2520the%2520traditional%250Ametric%2520of%2520next%2520word%2520prediction%2520correlates%2520negatively%2520with%2520performance%2520of%2520LLMs%2527%250Aon%2520our%2520reasoning%2520dataset.%2520We%2520analyse%2520our%2520results%2520and%2520identify%2520failure%2520modes%250Athat%2520can%2520serve%2520as%2520useful%2520guides%2520for%2520future%2520research%252C%2520potentially%2520informing%250Astrategies%2520to%2520address%2520the%2520limitations%2520observed%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14848v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Same%20Task%2C%20More%20Tokens%3A%20the%20Impact%20of%20Input%20Length%20on%20the%20Reasoning%0A%20%20Performance%20of%20Large%20Language%20Models&entry.906535625=Mosh%20Levy%20and%20Alon%20Jacoby%20and%20Yoav%20Goldberg&entry.1292438233=%20%20This%20paper%20explores%20the%20impact%20of%20extending%20input%20lengths%20on%20the%20capabilities%0Aof%20Large%20Language%20Models%20%28LLMs%29.%20Despite%20LLMs%20advancements%20in%20recent%20times%2C%0Atheir%20performance%20consistency%20across%20different%20input%20lengths%20is%20not%20well%0Aunderstood.%20We%20investigate%20this%20aspect%20by%20introducing%20a%20novel%20QA%20reasoning%0Aframework%2C%20specifically%20designed%20to%20assess%20the%20impact%20of%20input%20length.%20We%0Aisolate%20the%20effect%20of%20input%20length%20using%20multiple%20versions%20of%20the%20same%20sample%2C%0Aeach%20being%20extended%20with%20padding%20of%20different%20lengths%2C%20types%20and%20locations.%20Our%0Afindings%20show%20a%20notable%20degradation%20in%20LLMs%27%20reasoning%20performance%20at%20much%0Ashorter%20input%20lengths%20than%20their%20technical%20maximum.%20We%20show%20that%20the%0Adegradation%20trend%20appears%20in%20every%20version%20of%20our%20dataset%2C%20although%20at%0Adifferent%20intensities.%20Additionally%2C%20our%20study%20reveals%20that%20the%20traditional%0Ametric%20of%20next%20word%20prediction%20correlates%20negatively%20with%20performance%20of%20LLMs%27%0Aon%20our%20reasoning%20dataset.%20We%20analyse%20our%20results%20and%20identify%20failure%20modes%0Athat%20can%20serve%20as%20useful%20guides%20for%20future%20research%2C%20potentially%20informing%0Astrategies%20to%20address%20the%20limitations%20observed%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14848v2&entry.124074799=Read"},
{"title": "Cross Domain Object Detection via Multi-Granularity Confidence Alignment\n  based Mean Teacher", "author": "Jiangming Chen and Li Liu and Wanxia Deng and Zhen Liu and Yu Liu and Yingmei Wei and Yongxiang Liu", "abstract": "  Cross domain object detection learns an object detector for an unlabeled\ntarget domain by transferring knowledge from an annotated source domain.\nPromising results have been achieved via Mean Teacher, however, pseudo labeling\nwhich is the bottleneck of mutual learning remains to be further explored. In\nthis study, we find that confidence misalignment of the predictions, including\ncategory-level overconfidence, instance-level task confidence inconsistency,\nand image-level confidence misfocusing, leading to the injection of noisy\npseudo label in the training process, will bring suboptimal performance on the\ntarget domain. To tackle this issue, we present a novel general framework\ntermed Multi-Granularity Confidence Alignment Mean Teacher (MGCAMT) for cross\ndomain object detection, which alleviates confidence misalignment across\ncategory-, instance-, and image-levels simultaneously to obtain high quality\npseudo supervision for better teacher-student learning. Specifically, to align\nconfidence with accuracy at category level, we propose Classification\nConfidence Alignment (CCA) to model category uncertainty based on Evidential\nDeep Learning (EDL) and filter out the category incorrect labels via an\nuncertainty-aware selection strategy. Furthermore, to mitigate the\ninstance-level misalignment between classification and localization, we design\nTask Confidence Alignment (TCA) to enhance the interaction between the two task\nbranches and allow each classification feature to adaptively locate the optimal\nfeature for the regression. Finally, we develop imagery Focusing Confidence\nAlignment (FCA) adopting another way of pseudo label learning, i.e., we use the\noriginal outputs from the Mean Teacher network for supervised learning without\nlabel assignment to concentrate on holistic information in the target image.\nThese three procedures benefit from each other from a cooperative learning\nperspective.\n", "link": "http://arxiv.org/abs/2407.07780v1", "date": "2024-07-10", "relevancy": 2.3128, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5917}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5898}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross%20Domain%20Object%20Detection%20via%20Multi-Granularity%20Confidence%20Alignment%0A%20%20based%20Mean%20Teacher&body=Title%3A%20Cross%20Domain%20Object%20Detection%20via%20Multi-Granularity%20Confidence%20Alignment%0A%20%20based%20Mean%20Teacher%0AAuthor%3A%20Jiangming%20Chen%20and%20Li%20Liu%20and%20Wanxia%20Deng%20and%20Zhen%20Liu%20and%20Yu%20Liu%20and%20Yingmei%20Wei%20and%20Yongxiang%20Liu%0AAbstract%3A%20%20%20Cross%20domain%20object%20detection%20learns%20an%20object%20detector%20for%20an%20unlabeled%0Atarget%20domain%20by%20transferring%20knowledge%20from%20an%20annotated%20source%20domain.%0APromising%20results%20have%20been%20achieved%20via%20Mean%20Teacher%2C%20however%2C%20pseudo%20labeling%0Awhich%20is%20the%20bottleneck%20of%20mutual%20learning%20remains%20to%20be%20further%20explored.%20In%0Athis%20study%2C%20we%20find%20that%20confidence%20misalignment%20of%20the%20predictions%2C%20including%0Acategory-level%20overconfidence%2C%20instance-level%20task%20confidence%20inconsistency%2C%0Aand%20image-level%20confidence%20misfocusing%2C%20leading%20to%20the%20injection%20of%20noisy%0Apseudo%20label%20in%20the%20training%20process%2C%20will%20bring%20suboptimal%20performance%20on%20the%0Atarget%20domain.%20To%20tackle%20this%20issue%2C%20we%20present%20a%20novel%20general%20framework%0Atermed%20Multi-Granularity%20Confidence%20Alignment%20Mean%20Teacher%20%28MGCAMT%29%20for%20cross%0Adomain%20object%20detection%2C%20which%20alleviates%20confidence%20misalignment%20across%0Acategory-%2C%20instance-%2C%20and%20image-levels%20simultaneously%20to%20obtain%20high%20quality%0Apseudo%20supervision%20for%20better%20teacher-student%20learning.%20Specifically%2C%20to%20align%0Aconfidence%20with%20accuracy%20at%20category%20level%2C%20we%20propose%20Classification%0AConfidence%20Alignment%20%28CCA%29%20to%20model%20category%20uncertainty%20based%20on%20Evidential%0ADeep%20Learning%20%28EDL%29%20and%20filter%20out%20the%20category%20incorrect%20labels%20via%20an%0Auncertainty-aware%20selection%20strategy.%20Furthermore%2C%20to%20mitigate%20the%0Ainstance-level%20misalignment%20between%20classification%20and%20localization%2C%20we%20design%0ATask%20Confidence%20Alignment%20%28TCA%29%20to%20enhance%20the%20interaction%20between%20the%20two%20task%0Abranches%20and%20allow%20each%20classification%20feature%20to%20adaptively%20locate%20the%20optimal%0Afeature%20for%20the%20regression.%20Finally%2C%20we%20develop%20imagery%20Focusing%20Confidence%0AAlignment%20%28FCA%29%20adopting%20another%20way%20of%20pseudo%20label%20learning%2C%20i.e.%2C%20we%20use%20the%0Aoriginal%20outputs%20from%20the%20Mean%20Teacher%20network%20for%20supervised%20learning%20without%0Alabel%20assignment%20to%20concentrate%20on%20holistic%20information%20in%20the%20target%20image.%0AThese%20three%20procedures%20benefit%20from%20each%20other%20from%20a%20cooperative%20learning%0Aperspective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross%2520Domain%2520Object%2520Detection%2520via%2520Multi-Granularity%2520Confidence%2520Alignment%250A%2520%2520based%2520Mean%2520Teacher%26entry.906535625%3DJiangming%2520Chen%2520and%2520Li%2520Liu%2520and%2520Wanxia%2520Deng%2520and%2520Zhen%2520Liu%2520and%2520Yu%2520Liu%2520and%2520Yingmei%2520Wei%2520and%2520Yongxiang%2520Liu%26entry.1292438233%3D%2520%2520Cross%2520domain%2520object%2520detection%2520learns%2520an%2520object%2520detector%2520for%2520an%2520unlabeled%250Atarget%2520domain%2520by%2520transferring%2520knowledge%2520from%2520an%2520annotated%2520source%2520domain.%250APromising%2520results%2520have%2520been%2520achieved%2520via%2520Mean%2520Teacher%252C%2520however%252C%2520pseudo%2520labeling%250Awhich%2520is%2520the%2520bottleneck%2520of%2520mutual%2520learning%2520remains%2520to%2520be%2520further%2520explored.%2520In%250Athis%2520study%252C%2520we%2520find%2520that%2520confidence%2520misalignment%2520of%2520the%2520predictions%252C%2520including%250Acategory-level%2520overconfidence%252C%2520instance-level%2520task%2520confidence%2520inconsistency%252C%250Aand%2520image-level%2520confidence%2520misfocusing%252C%2520leading%2520to%2520the%2520injection%2520of%2520noisy%250Apseudo%2520label%2520in%2520the%2520training%2520process%252C%2520will%2520bring%2520suboptimal%2520performance%2520on%2520the%250Atarget%2520domain.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520present%2520a%2520novel%2520general%2520framework%250Atermed%2520Multi-Granularity%2520Confidence%2520Alignment%2520Mean%2520Teacher%2520%2528MGCAMT%2529%2520for%2520cross%250Adomain%2520object%2520detection%252C%2520which%2520alleviates%2520confidence%2520misalignment%2520across%250Acategory-%252C%2520instance-%252C%2520and%2520image-levels%2520simultaneously%2520to%2520obtain%2520high%2520quality%250Apseudo%2520supervision%2520for%2520better%2520teacher-student%2520learning.%2520Specifically%252C%2520to%2520align%250Aconfidence%2520with%2520accuracy%2520at%2520category%2520level%252C%2520we%2520propose%2520Classification%250AConfidence%2520Alignment%2520%2528CCA%2529%2520to%2520model%2520category%2520uncertainty%2520based%2520on%2520Evidential%250ADeep%2520Learning%2520%2528EDL%2529%2520and%2520filter%2520out%2520the%2520category%2520incorrect%2520labels%2520via%2520an%250Auncertainty-aware%2520selection%2520strategy.%2520Furthermore%252C%2520to%2520mitigate%2520the%250Ainstance-level%2520misalignment%2520between%2520classification%2520and%2520localization%252C%2520we%2520design%250ATask%2520Confidence%2520Alignment%2520%2528TCA%2529%2520to%2520enhance%2520the%2520interaction%2520between%2520the%2520two%2520task%250Abranches%2520and%2520allow%2520each%2520classification%2520feature%2520to%2520adaptively%2520locate%2520the%2520optimal%250Afeature%2520for%2520the%2520regression.%2520Finally%252C%2520we%2520develop%2520imagery%2520Focusing%2520Confidence%250AAlignment%2520%2528FCA%2529%2520adopting%2520another%2520way%2520of%2520pseudo%2520label%2520learning%252C%2520i.e.%252C%2520we%2520use%2520the%250Aoriginal%2520outputs%2520from%2520the%2520Mean%2520Teacher%2520network%2520for%2520supervised%2520learning%2520without%250Alabel%2520assignment%2520to%2520concentrate%2520on%2520holistic%2520information%2520in%2520the%2520target%2520image.%250AThese%2520three%2520procedures%2520benefit%2520from%2520each%2520other%2520from%2520a%2520cooperative%2520learning%250Aperspective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross%20Domain%20Object%20Detection%20via%20Multi-Granularity%20Confidence%20Alignment%0A%20%20based%20Mean%20Teacher&entry.906535625=Jiangming%20Chen%20and%20Li%20Liu%20and%20Wanxia%20Deng%20and%20Zhen%20Liu%20and%20Yu%20Liu%20and%20Yingmei%20Wei%20and%20Yongxiang%20Liu&entry.1292438233=%20%20Cross%20domain%20object%20detection%20learns%20an%20object%20detector%20for%20an%20unlabeled%0Atarget%20domain%20by%20transferring%20knowledge%20from%20an%20annotated%20source%20domain.%0APromising%20results%20have%20been%20achieved%20via%20Mean%20Teacher%2C%20however%2C%20pseudo%20labeling%0Awhich%20is%20the%20bottleneck%20of%20mutual%20learning%20remains%20to%20be%20further%20explored.%20In%0Athis%20study%2C%20we%20find%20that%20confidence%20misalignment%20of%20the%20predictions%2C%20including%0Acategory-level%20overconfidence%2C%20instance-level%20task%20confidence%20inconsistency%2C%0Aand%20image-level%20confidence%20misfocusing%2C%20leading%20to%20the%20injection%20of%20noisy%0Apseudo%20label%20in%20the%20training%20process%2C%20will%20bring%20suboptimal%20performance%20on%20the%0Atarget%20domain.%20To%20tackle%20this%20issue%2C%20we%20present%20a%20novel%20general%20framework%0Atermed%20Multi-Granularity%20Confidence%20Alignment%20Mean%20Teacher%20%28MGCAMT%29%20for%20cross%0Adomain%20object%20detection%2C%20which%20alleviates%20confidence%20misalignment%20across%0Acategory-%2C%20instance-%2C%20and%20image-levels%20simultaneously%20to%20obtain%20high%20quality%0Apseudo%20supervision%20for%20better%20teacher-student%20learning.%20Specifically%2C%20to%20align%0Aconfidence%20with%20accuracy%20at%20category%20level%2C%20we%20propose%20Classification%0AConfidence%20Alignment%20%28CCA%29%20to%20model%20category%20uncertainty%20based%20on%20Evidential%0ADeep%20Learning%20%28EDL%29%20and%20filter%20out%20the%20category%20incorrect%20labels%20via%20an%0Auncertainty-aware%20selection%20strategy.%20Furthermore%2C%20to%20mitigate%20the%0Ainstance-level%20misalignment%20between%20classification%20and%20localization%2C%20we%20design%0ATask%20Confidence%20Alignment%20%28TCA%29%20to%20enhance%20the%20interaction%20between%20the%20two%20task%0Abranches%20and%20allow%20each%20classification%20feature%20to%20adaptively%20locate%20the%20optimal%0Afeature%20for%20the%20regression.%20Finally%2C%20we%20develop%20imagery%20Focusing%20Confidence%0AAlignment%20%28FCA%29%20adopting%20another%20way%20of%20pseudo%20label%20learning%2C%20i.e.%2C%20we%20use%20the%0Aoriginal%20outputs%20from%20the%20Mean%20Teacher%20network%20for%20supervised%20learning%20without%0Alabel%20assignment%20to%20concentrate%20on%20holistic%20information%20in%20the%20target%20image.%0AThese%20three%20procedures%20benefit%20from%20each%20other%20from%20a%20cooperative%20learning%0Aperspective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07780v1&entry.124074799=Read"},
{"title": "Arabic Automatic Story Generation with Large Language Models", "author": "Ahmed Oumar El-Shangiti and Fakhraddin Alwajih and Muhammad Abdul-Mageed", "abstract": "  Large language models (LLMs) have recently emerged as a powerful tool for a\nwide range of language generation tasks. Nevertheless, this progress has been\nslower in Arabic. In this work, we focus on the task of generating stories from\nLLMs. For our training, we use stories acquired through machine translation\n(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that\nensures we acquire high-quality stories. For our GPT-41 data, we introduce\ncrafted prompts that allow us to generate data well-suited to the Arabic\ncontext in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian\nand Moroccan). For example, we generate stories tailored to various Arab\ncountries on a wide host of topics. Our manual evaluation shows that our model\nfine-tuned on these training datasets can generate coherent stories that adhere\nto our instructions. We also conduct an extensive automatic and human\nevaluation comparing our models against state-of-the-art proprietary and\nopen-source models. Our datasets and models will be made publicly available at\nhttps: //github.com/UBC-NLP/arastories.\n", "link": "http://arxiv.org/abs/2407.07551v1", "date": "2024-07-10", "relevancy": 2.2751, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.469}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.454}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arabic%20Automatic%20Story%20Generation%20with%20Large%20Language%20Models&body=Title%3A%20Arabic%20Automatic%20Story%20Generation%20with%20Large%20Language%20Models%0AAuthor%3A%20Ahmed%20Oumar%20El-Shangiti%20and%20Fakhraddin%20Alwajih%20and%20Muhammad%20Abdul-Mageed%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20emerged%20as%20a%20powerful%20tool%20for%20a%0Awide%20range%20of%20language%20generation%20tasks.%20Nevertheless%2C%20this%20progress%20has%20been%0Aslower%20in%20Arabic.%20In%20this%20work%2C%20we%20focus%20on%20the%20task%20of%20generating%20stories%20from%0ALLMs.%20For%20our%20training%2C%20we%20use%20stories%20acquired%20through%20machine%20translation%0A%28MT%29%20as%20well%20as%20GPT-4.%20For%20the%20MT%20data%2C%20we%20develop%20a%20careful%20pipeline%20that%0Aensures%20we%20acquire%20high-quality%20stories.%20For%20our%20GPT-41%20data%2C%20we%20introduce%0Acrafted%20prompts%20that%20allow%20us%20to%20generate%20data%20well-suited%20to%20the%20Arabic%0Acontext%20in%20both%20Modern%20Standard%20Arabic%20%28MSA%29%20and%20two%20Arabic%20dialects%20%28Egyptian%0Aand%20Moroccan%29.%20For%20example%2C%20we%20generate%20stories%20tailored%20to%20various%20Arab%0Acountries%20on%20a%20wide%20host%20of%20topics.%20Our%20manual%20evaluation%20shows%20that%20our%20model%0Afine-tuned%20on%20these%20training%20datasets%20can%20generate%20coherent%20stories%20that%20adhere%0Ato%20our%20instructions.%20We%20also%20conduct%20an%20extensive%20automatic%20and%20human%0Aevaluation%20comparing%20our%20models%20against%20state-of-the-art%20proprietary%20and%0Aopen-source%20models.%20Our%20datasets%20and%20models%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A%20//github.com/UBC-NLP/arastories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArabic%2520Automatic%2520Story%2520Generation%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DAhmed%2520Oumar%2520El-Shangiti%2520and%2520Fakhraddin%2520Alwajih%2520and%2520Muhammad%2520Abdul-Mageed%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520a%250Awide%2520range%2520of%2520language%2520generation%2520tasks.%2520Nevertheless%252C%2520this%2520progress%2520has%2520been%250Aslower%2520in%2520Arabic.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520task%2520of%2520generating%2520stories%2520from%250ALLMs.%2520For%2520our%2520training%252C%2520we%2520use%2520stories%2520acquired%2520through%2520machine%2520translation%250A%2528MT%2529%2520as%2520well%2520as%2520GPT-4.%2520For%2520the%2520MT%2520data%252C%2520we%2520develop%2520a%2520careful%2520pipeline%2520that%250Aensures%2520we%2520acquire%2520high-quality%2520stories.%2520For%2520our%2520GPT-41%2520data%252C%2520we%2520introduce%250Acrafted%2520prompts%2520that%2520allow%2520us%2520to%2520generate%2520data%2520well-suited%2520to%2520the%2520Arabic%250Acontext%2520in%2520both%2520Modern%2520Standard%2520Arabic%2520%2528MSA%2529%2520and%2520two%2520Arabic%2520dialects%2520%2528Egyptian%250Aand%2520Moroccan%2529.%2520For%2520example%252C%2520we%2520generate%2520stories%2520tailored%2520to%2520various%2520Arab%250Acountries%2520on%2520a%2520wide%2520host%2520of%2520topics.%2520Our%2520manual%2520evaluation%2520shows%2520that%2520our%2520model%250Afine-tuned%2520on%2520these%2520training%2520datasets%2520can%2520generate%2520coherent%2520stories%2520that%2520adhere%250Ato%2520our%2520instructions.%2520We%2520also%2520conduct%2520an%2520extensive%2520automatic%2520and%2520human%250Aevaluation%2520comparing%2520our%2520models%2520against%2520state-of-the-art%2520proprietary%2520and%250Aopen-source%2520models.%2520Our%2520datasets%2520and%2520models%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A%2520//github.com/UBC-NLP/arastories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arabic%20Automatic%20Story%20Generation%20with%20Large%20Language%20Models&entry.906535625=Ahmed%20Oumar%20El-Shangiti%20and%20Fakhraddin%20Alwajih%20and%20Muhammad%20Abdul-Mageed&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20emerged%20as%20a%20powerful%20tool%20for%20a%0Awide%20range%20of%20language%20generation%20tasks.%20Nevertheless%2C%20this%20progress%20has%20been%0Aslower%20in%20Arabic.%20In%20this%20work%2C%20we%20focus%20on%20the%20task%20of%20generating%20stories%20from%0ALLMs.%20For%20our%20training%2C%20we%20use%20stories%20acquired%20through%20machine%20translation%0A%28MT%29%20as%20well%20as%20GPT-4.%20For%20the%20MT%20data%2C%20we%20develop%20a%20careful%20pipeline%20that%0Aensures%20we%20acquire%20high-quality%20stories.%20For%20our%20GPT-41%20data%2C%20we%20introduce%0Acrafted%20prompts%20that%20allow%20us%20to%20generate%20data%20well-suited%20to%20the%20Arabic%0Acontext%20in%20both%20Modern%20Standard%20Arabic%20%28MSA%29%20and%20two%20Arabic%20dialects%20%28Egyptian%0Aand%20Moroccan%29.%20For%20example%2C%20we%20generate%20stories%20tailored%20to%20various%20Arab%0Acountries%20on%20a%20wide%20host%20of%20topics.%20Our%20manual%20evaluation%20shows%20that%20our%20model%0Afine-tuned%20on%20these%20training%20datasets%20can%20generate%20coherent%20stories%20that%20adhere%0Ato%20our%20instructions.%20We%20also%20conduct%20an%20extensive%20automatic%20and%20human%0Aevaluation%20comparing%20our%20models%20against%20state-of-the-art%20proprietary%20and%0Aopen-source%20models.%20Our%20datasets%20and%20models%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A%20//github.com/UBC-NLP/arastories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07551v1&entry.124074799=Read"},
{"title": "Adaptive Multi-head Contrastive Learning", "author": "Lei Wang and Piotr Koniusz and Tom Gedeon and Liang Zheng", "abstract": "  In contrastive learning, two views of an original image, generated by\ndifferent augmentations, are considered a positive pair, and their similarity\nis required to be high. Similarly, two views of distinct images form a negative\npair, with encouraged low similarity. Typically, a single similarity measure,\nprovided by a lone projection head, evaluates positive and negative sample\npairs. However, due to diverse augmentation strategies and varying intra-sample\nsimilarity, views from the same image may not always be similar. Additionally,\nowing to inter-sample similarity, views from different images may be more akin\nthan those from the same image. Consequently, enforcing high similarity for\npositive pairs and low similarity for negative pairs may be unattainable, and\nin some cases, such enforcement could detrimentally impact performance. To\naddress this challenge, we propose using multiple projection heads, each\nproducing a distinct set of features. Our pre-training loss function emerges\nfrom a solution to the maximum likelihood estimation over head-wise posterior\ndistributions of positive samples given observations. This loss incorporates\nthe similarity measure over positive and negative pairs, each re-weighted by an\nindividual adaptive temperature, regulated to prevent ill solutions. Our\napproach, Adaptive Multi-Head Contrastive Learning (AMCL), can be applied to\nand experimentally enhances several popular contrastive learning methods such\nas SimCLR, MoCo, and Barlow Twins. The improvement remains consistent across\nvarious backbones and linear probing epochs, and becomes more significant when\nemploying multiple augmentation methods.\n", "link": "http://arxiv.org/abs/2310.05615v2", "date": "2024-07-10", "relevancy": 2.2661, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6209}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5377}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Multi-head%20Contrastive%20Learning&body=Title%3A%20Adaptive%20Multi-head%20Contrastive%20Learning%0AAuthor%3A%20Lei%20Wang%20and%20Piotr%20Koniusz%20and%20Tom%20Gedeon%20and%20Liang%20Zheng%0AAbstract%3A%20%20%20In%20contrastive%20learning%2C%20two%20views%20of%20an%20original%20image%2C%20generated%20by%0Adifferent%20augmentations%2C%20are%20considered%20a%20positive%20pair%2C%20and%20their%20similarity%0Ais%20required%20to%20be%20high.%20Similarly%2C%20two%20views%20of%20distinct%20images%20form%20a%20negative%0Apair%2C%20with%20encouraged%20low%20similarity.%20Typically%2C%20a%20single%20similarity%20measure%2C%0Aprovided%20by%20a%20lone%20projection%20head%2C%20evaluates%20positive%20and%20negative%20sample%0Apairs.%20However%2C%20due%20to%20diverse%20augmentation%20strategies%20and%20varying%20intra-sample%0Asimilarity%2C%20views%20from%20the%20same%20image%20may%20not%20always%20be%20similar.%20Additionally%2C%0Aowing%20to%20inter-sample%20similarity%2C%20views%20from%20different%20images%20may%20be%20more%20akin%0Athan%20those%20from%20the%20same%20image.%20Consequently%2C%20enforcing%20high%20similarity%20for%0Apositive%20pairs%20and%20low%20similarity%20for%20negative%20pairs%20may%20be%20unattainable%2C%20and%0Ain%20some%20cases%2C%20such%20enforcement%20could%20detrimentally%20impact%20performance.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20using%20multiple%20projection%20heads%2C%20each%0Aproducing%20a%20distinct%20set%20of%20features.%20Our%20pre-training%20loss%20function%20emerges%0Afrom%20a%20solution%20to%20the%20maximum%20likelihood%20estimation%20over%20head-wise%20posterior%0Adistributions%20of%20positive%20samples%20given%20observations.%20This%20loss%20incorporates%0Athe%20similarity%20measure%20over%20positive%20and%20negative%20pairs%2C%20each%20re-weighted%20by%20an%0Aindividual%20adaptive%20temperature%2C%20regulated%20to%20prevent%20ill%20solutions.%20Our%0Aapproach%2C%20Adaptive%20Multi-Head%20Contrastive%20Learning%20%28AMCL%29%2C%20can%20be%20applied%20to%0Aand%20experimentally%20enhances%20several%20popular%20contrastive%20learning%20methods%20such%0Aas%20SimCLR%2C%20MoCo%2C%20and%20Barlow%20Twins.%20The%20improvement%20remains%20consistent%20across%0Avarious%20backbones%20and%20linear%20probing%20epochs%2C%20and%20becomes%20more%20significant%20when%0Aemploying%20multiple%20augmentation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05615v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Multi-head%2520Contrastive%2520Learning%26entry.906535625%3DLei%2520Wang%2520and%2520Piotr%2520Koniusz%2520and%2520Tom%2520Gedeon%2520and%2520Liang%2520Zheng%26entry.1292438233%3D%2520%2520In%2520contrastive%2520learning%252C%2520two%2520views%2520of%2520an%2520original%2520image%252C%2520generated%2520by%250Adifferent%2520augmentations%252C%2520are%2520considered%2520a%2520positive%2520pair%252C%2520and%2520their%2520similarity%250Ais%2520required%2520to%2520be%2520high.%2520Similarly%252C%2520two%2520views%2520of%2520distinct%2520images%2520form%2520a%2520negative%250Apair%252C%2520with%2520encouraged%2520low%2520similarity.%2520Typically%252C%2520a%2520single%2520similarity%2520measure%252C%250Aprovided%2520by%2520a%2520lone%2520projection%2520head%252C%2520evaluates%2520positive%2520and%2520negative%2520sample%250Apairs.%2520However%252C%2520due%2520to%2520diverse%2520augmentation%2520strategies%2520and%2520varying%2520intra-sample%250Asimilarity%252C%2520views%2520from%2520the%2520same%2520image%2520may%2520not%2520always%2520be%2520similar.%2520Additionally%252C%250Aowing%2520to%2520inter-sample%2520similarity%252C%2520views%2520from%2520different%2520images%2520may%2520be%2520more%2520akin%250Athan%2520those%2520from%2520the%2520same%2520image.%2520Consequently%252C%2520enforcing%2520high%2520similarity%2520for%250Apositive%2520pairs%2520and%2520low%2520similarity%2520for%2520negative%2520pairs%2520may%2520be%2520unattainable%252C%2520and%250Ain%2520some%2520cases%252C%2520such%2520enforcement%2520could%2520detrimentally%2520impact%2520performance.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520using%2520multiple%2520projection%2520heads%252C%2520each%250Aproducing%2520a%2520distinct%2520set%2520of%2520features.%2520Our%2520pre-training%2520loss%2520function%2520emerges%250Afrom%2520a%2520solution%2520to%2520the%2520maximum%2520likelihood%2520estimation%2520over%2520head-wise%2520posterior%250Adistributions%2520of%2520positive%2520samples%2520given%2520observations.%2520This%2520loss%2520incorporates%250Athe%2520similarity%2520measure%2520over%2520positive%2520and%2520negative%2520pairs%252C%2520each%2520re-weighted%2520by%2520an%250Aindividual%2520adaptive%2520temperature%252C%2520regulated%2520to%2520prevent%2520ill%2520solutions.%2520Our%250Aapproach%252C%2520Adaptive%2520Multi-Head%2520Contrastive%2520Learning%2520%2528AMCL%2529%252C%2520can%2520be%2520applied%2520to%250Aand%2520experimentally%2520enhances%2520several%2520popular%2520contrastive%2520learning%2520methods%2520such%250Aas%2520SimCLR%252C%2520MoCo%252C%2520and%2520Barlow%2520Twins.%2520The%2520improvement%2520remains%2520consistent%2520across%250Avarious%2520backbones%2520and%2520linear%2520probing%2520epochs%252C%2520and%2520becomes%2520more%2520significant%2520when%250Aemploying%2520multiple%2520augmentation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05615v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Multi-head%20Contrastive%20Learning&entry.906535625=Lei%20Wang%20and%20Piotr%20Koniusz%20and%20Tom%20Gedeon%20and%20Liang%20Zheng&entry.1292438233=%20%20In%20contrastive%20learning%2C%20two%20views%20of%20an%20original%20image%2C%20generated%20by%0Adifferent%20augmentations%2C%20are%20considered%20a%20positive%20pair%2C%20and%20their%20similarity%0Ais%20required%20to%20be%20high.%20Similarly%2C%20two%20views%20of%20distinct%20images%20form%20a%20negative%0Apair%2C%20with%20encouraged%20low%20similarity.%20Typically%2C%20a%20single%20similarity%20measure%2C%0Aprovided%20by%20a%20lone%20projection%20head%2C%20evaluates%20positive%20and%20negative%20sample%0Apairs.%20However%2C%20due%20to%20diverse%20augmentation%20strategies%20and%20varying%20intra-sample%0Asimilarity%2C%20views%20from%20the%20same%20image%20may%20not%20always%20be%20similar.%20Additionally%2C%0Aowing%20to%20inter-sample%20similarity%2C%20views%20from%20different%20images%20may%20be%20more%20akin%0Athan%20those%20from%20the%20same%20image.%20Consequently%2C%20enforcing%20high%20similarity%20for%0Apositive%20pairs%20and%20low%20similarity%20for%20negative%20pairs%20may%20be%20unattainable%2C%20and%0Ain%20some%20cases%2C%20such%20enforcement%20could%20detrimentally%20impact%20performance.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20using%20multiple%20projection%20heads%2C%20each%0Aproducing%20a%20distinct%20set%20of%20features.%20Our%20pre-training%20loss%20function%20emerges%0Afrom%20a%20solution%20to%20the%20maximum%20likelihood%20estimation%20over%20head-wise%20posterior%0Adistributions%20of%20positive%20samples%20given%20observations.%20This%20loss%20incorporates%0Athe%20similarity%20measure%20over%20positive%20and%20negative%20pairs%2C%20each%20re-weighted%20by%20an%0Aindividual%20adaptive%20temperature%2C%20regulated%20to%20prevent%20ill%20solutions.%20Our%0Aapproach%2C%20Adaptive%20Multi-Head%20Contrastive%20Learning%20%28AMCL%29%2C%20can%20be%20applied%20to%0Aand%20experimentally%20enhances%20several%20popular%20contrastive%20learning%20methods%20such%0Aas%20SimCLR%2C%20MoCo%2C%20and%20Barlow%20Twins.%20The%20improvement%20remains%20consistent%20across%0Avarious%20backbones%20and%20linear%20probing%20epochs%2C%20and%20becomes%20more%20significant%20when%0Aemploying%20multiple%20augmentation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05615v2&entry.124074799=Read"},
{"title": "Boosting Transferability in Vision-Language Attacks via Diversification\n  along the Intersection Region of Adversarial Trajectory", "author": "Sensen Gao and Xiaojun Jia and Xuhong Ren and Ivor Tsang and Qing Guo", "abstract": "  Vision-language pre-training (VLP) models exhibit remarkable capabilities in\ncomprehending both images and text, yet they remain susceptible to multimodal\nadversarial examples (AEs).Strengthening attacks and uncovering\nvulnerabilities, especially common issues in VLP models (e.g., high\ntransferable AEs), can advance reliable and practical VLP models. A recent work\n(i.e., Set-level guidance attack) indicates that augmenting image-text pairs to\nincrease AE diversity along the optimization path enhances the transferability\nof adversarial examples significantly. However, this approach predominantly\nemphasizes diversity around the online adversarial examples (i.e., AEs in the\noptimization period), leading to the risk of overfitting the victim model and\naffecting the transferability.In this study, we posit that the diversity of\nadversarial examples towards the clean input and online AEs are both pivotal\nfor enhancing transferability across VLP models. Consequently, we propose using\ndiversification along the intersection region of adversarial trajectory to\nexpand the diversity of AEs.To fully leverage the interaction between\nmodalities, we introduce text-guided adversarial example selection during\noptimization. Furthermore, to further mitigate the potential overfitting, we\ndirect the adversarial text deviating from the last intersection region along\nthe optimization path, rather than adversarial images as in existing\nmethods.Extensive experiments affirm the effectiveness of our method in\nimproving transferability across various VLP models and downstream\nvision-and-language tasks.\n", "link": "http://arxiv.org/abs/2403.12445v2", "date": "2024-07-10", "relevancy": 2.2572, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.575}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5716}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Transferability%20in%20Vision-Language%20Attacks%20via%20Diversification%0A%20%20along%20the%20Intersection%20Region%20of%20Adversarial%20Trajectory&body=Title%3A%20Boosting%20Transferability%20in%20Vision-Language%20Attacks%20via%20Diversification%0A%20%20along%20the%20Intersection%20Region%20of%20Adversarial%20Trajectory%0AAuthor%3A%20Sensen%20Gao%20and%20Xiaojun%20Jia%20and%20Xuhong%20Ren%20and%20Ivor%20Tsang%20and%20Qing%20Guo%0AAbstract%3A%20%20%20Vision-language%20pre-training%20%28VLP%29%20models%20exhibit%20remarkable%20capabilities%20in%0Acomprehending%20both%20images%20and%20text%2C%20yet%20they%20remain%20susceptible%20to%20multimodal%0Aadversarial%20examples%20%28AEs%29.Strengthening%20attacks%20and%20uncovering%0Avulnerabilities%2C%20especially%20common%20issues%20in%20VLP%20models%20%28e.g.%2C%20high%0Atransferable%20AEs%29%2C%20can%20advance%20reliable%20and%20practical%20VLP%20models.%20A%20recent%20work%0A%28i.e.%2C%20Set-level%20guidance%20attack%29%20indicates%20that%20augmenting%20image-text%20pairs%20to%0Aincrease%20AE%20diversity%20along%20the%20optimization%20path%20enhances%20the%20transferability%0Aof%20adversarial%20examples%20significantly.%20However%2C%20this%20approach%20predominantly%0Aemphasizes%20diversity%20around%20the%20online%20adversarial%20examples%20%28i.e.%2C%20AEs%20in%20the%0Aoptimization%20period%29%2C%20leading%20to%20the%20risk%20of%20overfitting%20the%20victim%20model%20and%0Aaffecting%20the%20transferability.In%20this%20study%2C%20we%20posit%20that%20the%20diversity%20of%0Aadversarial%20examples%20towards%20the%20clean%20input%20and%20online%20AEs%20are%20both%20pivotal%0Afor%20enhancing%20transferability%20across%20VLP%20models.%20Consequently%2C%20we%20propose%20using%0Adiversification%20along%20the%20intersection%20region%20of%20adversarial%20trajectory%20to%0Aexpand%20the%20diversity%20of%20AEs.To%20fully%20leverage%20the%20interaction%20between%0Amodalities%2C%20we%20introduce%20text-guided%20adversarial%20example%20selection%20during%0Aoptimization.%20Furthermore%2C%20to%20further%20mitigate%20the%20potential%20overfitting%2C%20we%0Adirect%20the%20adversarial%20text%20deviating%20from%20the%20last%20intersection%20region%20along%0Athe%20optimization%20path%2C%20rather%20than%20adversarial%20images%20as%20in%20existing%0Amethods.Extensive%20experiments%20affirm%20the%20effectiveness%20of%20our%20method%20in%0Aimproving%20transferability%20across%20various%20VLP%20models%20and%20downstream%0Avision-and-language%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Transferability%2520in%2520Vision-Language%2520Attacks%2520via%2520Diversification%250A%2520%2520along%2520the%2520Intersection%2520Region%2520of%2520Adversarial%2520Trajectory%26entry.906535625%3DSensen%2520Gao%2520and%2520Xiaojun%2520Jia%2520and%2520Xuhong%2520Ren%2520and%2520Ivor%2520Tsang%2520and%2520Qing%2520Guo%26entry.1292438233%3D%2520%2520Vision-language%2520pre-training%2520%2528VLP%2529%2520models%2520exhibit%2520remarkable%2520capabilities%2520in%250Acomprehending%2520both%2520images%2520and%2520text%252C%2520yet%2520they%2520remain%2520susceptible%2520to%2520multimodal%250Aadversarial%2520examples%2520%2528AEs%2529.Strengthening%2520attacks%2520and%2520uncovering%250Avulnerabilities%252C%2520especially%2520common%2520issues%2520in%2520VLP%2520models%2520%2528e.g.%252C%2520high%250Atransferable%2520AEs%2529%252C%2520can%2520advance%2520reliable%2520and%2520practical%2520VLP%2520models.%2520A%2520recent%2520work%250A%2528i.e.%252C%2520Set-level%2520guidance%2520attack%2529%2520indicates%2520that%2520augmenting%2520image-text%2520pairs%2520to%250Aincrease%2520AE%2520diversity%2520along%2520the%2520optimization%2520path%2520enhances%2520the%2520transferability%250Aof%2520adversarial%2520examples%2520significantly.%2520However%252C%2520this%2520approach%2520predominantly%250Aemphasizes%2520diversity%2520around%2520the%2520online%2520adversarial%2520examples%2520%2528i.e.%252C%2520AEs%2520in%2520the%250Aoptimization%2520period%2529%252C%2520leading%2520to%2520the%2520risk%2520of%2520overfitting%2520the%2520victim%2520model%2520and%250Aaffecting%2520the%2520transferability.In%2520this%2520study%252C%2520we%2520posit%2520that%2520the%2520diversity%2520of%250Aadversarial%2520examples%2520towards%2520the%2520clean%2520input%2520and%2520online%2520AEs%2520are%2520both%2520pivotal%250Afor%2520enhancing%2520transferability%2520across%2520VLP%2520models.%2520Consequently%252C%2520we%2520propose%2520using%250Adiversification%2520along%2520the%2520intersection%2520region%2520of%2520adversarial%2520trajectory%2520to%250Aexpand%2520the%2520diversity%2520of%2520AEs.To%2520fully%2520leverage%2520the%2520interaction%2520between%250Amodalities%252C%2520we%2520introduce%2520text-guided%2520adversarial%2520example%2520selection%2520during%250Aoptimization.%2520Furthermore%252C%2520to%2520further%2520mitigate%2520the%2520potential%2520overfitting%252C%2520we%250Adirect%2520the%2520adversarial%2520text%2520deviating%2520from%2520the%2520last%2520intersection%2520region%2520along%250Athe%2520optimization%2520path%252C%2520rather%2520than%2520adversarial%2520images%2520as%2520in%2520existing%250Amethods.Extensive%2520experiments%2520affirm%2520the%2520effectiveness%2520of%2520our%2520method%2520in%250Aimproving%2520transferability%2520across%2520various%2520VLP%2520models%2520and%2520downstream%250Avision-and-language%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Transferability%20in%20Vision-Language%20Attacks%20via%20Diversification%0A%20%20along%20the%20Intersection%20Region%20of%20Adversarial%20Trajectory&entry.906535625=Sensen%20Gao%20and%20Xiaojun%20Jia%20and%20Xuhong%20Ren%20and%20Ivor%20Tsang%20and%20Qing%20Guo&entry.1292438233=%20%20Vision-language%20pre-training%20%28VLP%29%20models%20exhibit%20remarkable%20capabilities%20in%0Acomprehending%20both%20images%20and%20text%2C%20yet%20they%20remain%20susceptible%20to%20multimodal%0Aadversarial%20examples%20%28AEs%29.Strengthening%20attacks%20and%20uncovering%0Avulnerabilities%2C%20especially%20common%20issues%20in%20VLP%20models%20%28e.g.%2C%20high%0Atransferable%20AEs%29%2C%20can%20advance%20reliable%20and%20practical%20VLP%20models.%20A%20recent%20work%0A%28i.e.%2C%20Set-level%20guidance%20attack%29%20indicates%20that%20augmenting%20image-text%20pairs%20to%0Aincrease%20AE%20diversity%20along%20the%20optimization%20path%20enhances%20the%20transferability%0Aof%20adversarial%20examples%20significantly.%20However%2C%20this%20approach%20predominantly%0Aemphasizes%20diversity%20around%20the%20online%20adversarial%20examples%20%28i.e.%2C%20AEs%20in%20the%0Aoptimization%20period%29%2C%20leading%20to%20the%20risk%20of%20overfitting%20the%20victim%20model%20and%0Aaffecting%20the%20transferability.In%20this%20study%2C%20we%20posit%20that%20the%20diversity%20of%0Aadversarial%20examples%20towards%20the%20clean%20input%20and%20online%20AEs%20are%20both%20pivotal%0Afor%20enhancing%20transferability%20across%20VLP%20models.%20Consequently%2C%20we%20propose%20using%0Adiversification%20along%20the%20intersection%20region%20of%20adversarial%20trajectory%20to%0Aexpand%20the%20diversity%20of%20AEs.To%20fully%20leverage%20the%20interaction%20between%0Amodalities%2C%20we%20introduce%20text-guided%20adversarial%20example%20selection%20during%0Aoptimization.%20Furthermore%2C%20to%20further%20mitigate%20the%20potential%20overfitting%2C%20we%0Adirect%20the%20adversarial%20text%20deviating%20from%20the%20last%20intersection%20region%20along%0Athe%20optimization%20path%2C%20rather%20than%20adversarial%20images%20as%20in%20existing%0Amethods.Extensive%20experiments%20affirm%20the%20effectiveness%20of%20our%20method%20in%0Aimproving%20transferability%20across%20various%20VLP%20models%20and%20downstream%0Avision-and-language%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12445v2&entry.124074799=Read"},
{"title": "OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy\n  Prediction", "author": "Ji Zhang and Yiran Ding and Zixin Liu", "abstract": "  3D occupancy prediction based on multi-sensor fusion,crucial for a reliable\nautonomous driving system, enables fine-grained understanding of 3D scenes.\nPrevious fusion-based 3D occupancy predictions relied on depth estimation for\nprocessing 2D image features. However, depth estimation is an ill-posed\nproblem, hindering the accuracy and robustness of these methods. Furthermore,\nfine-grained occupancy prediction demands extensive computational resources. To\naddress these issues, we propose OccFusion, a depth estimation free multi-modal\nfusion framework. Additionally, we introduce a generalizable active training\nmethod and an active decoder that can be applied to any occupancy prediction\nmodel, with the potential to enhance their performance. Experiments conducted\non nuScenes-Occupancy and nuScenes-Occ3D demonstrate our framework's superior\nperformance. Detailed ablation studies highlight the effectiveness of each\nproposed method.\n", "link": "http://arxiv.org/abs/2403.05329v2", "date": "2024-07-10", "relevancy": 2.2511, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5889}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5575}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OccFusion%3A%20Depth%20Estimation%20Free%20Multi-sensor%20Fusion%20for%203D%20Occupancy%0A%20%20Prediction&body=Title%3A%20OccFusion%3A%20Depth%20Estimation%20Free%20Multi-sensor%20Fusion%20for%203D%20Occupancy%0A%20%20Prediction%0AAuthor%3A%20Ji%20Zhang%20and%20Yiran%20Ding%20and%20Zixin%20Liu%0AAbstract%3A%20%20%203D%20occupancy%20prediction%20based%20on%20multi-sensor%20fusion%2Ccrucial%20for%20a%20reliable%0Aautonomous%20driving%20system%2C%20enables%20fine-grained%20understanding%20of%203D%20scenes.%0APrevious%20fusion-based%203D%20occupancy%20predictions%20relied%20on%20depth%20estimation%20for%0Aprocessing%202D%20image%20features.%20However%2C%20depth%20estimation%20is%20an%20ill-posed%0Aproblem%2C%20hindering%20the%20accuracy%20and%20robustness%20of%20these%20methods.%20Furthermore%2C%0Afine-grained%20occupancy%20prediction%20demands%20extensive%20computational%20resources.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20OccFusion%2C%20a%20depth%20estimation%20free%20multi-modal%0Afusion%20framework.%20Additionally%2C%20we%20introduce%20a%20generalizable%20active%20training%0Amethod%20and%20an%20active%20decoder%20that%20can%20be%20applied%20to%20any%20occupancy%20prediction%0Amodel%2C%20with%20the%20potential%20to%20enhance%20their%20performance.%20Experiments%20conducted%0Aon%20nuScenes-Occupancy%20and%20nuScenes-Occ3D%20demonstrate%20our%20framework%27s%20superior%0Aperformance.%20Detailed%20ablation%20studies%20highlight%20the%20effectiveness%20of%20each%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05329v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccFusion%253A%2520Depth%2520Estimation%2520Free%2520Multi-sensor%2520Fusion%2520for%25203D%2520Occupancy%250A%2520%2520Prediction%26entry.906535625%3DJi%2520Zhang%2520and%2520Yiran%2520Ding%2520and%2520Zixin%2520Liu%26entry.1292438233%3D%2520%25203D%2520occupancy%2520prediction%2520based%2520on%2520multi-sensor%2520fusion%252Ccrucial%2520for%2520a%2520reliable%250Aautonomous%2520driving%2520system%252C%2520enables%2520fine-grained%2520understanding%2520of%25203D%2520scenes.%250APrevious%2520fusion-based%25203D%2520occupancy%2520predictions%2520relied%2520on%2520depth%2520estimation%2520for%250Aprocessing%25202D%2520image%2520features.%2520However%252C%2520depth%2520estimation%2520is%2520an%2520ill-posed%250Aproblem%252C%2520hindering%2520the%2520accuracy%2520and%2520robustness%2520of%2520these%2520methods.%2520Furthermore%252C%250Afine-grained%2520occupancy%2520prediction%2520demands%2520extensive%2520computational%2520resources.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520OccFusion%252C%2520a%2520depth%2520estimation%2520free%2520multi-modal%250Afusion%2520framework.%2520Additionally%252C%2520we%2520introduce%2520a%2520generalizable%2520active%2520training%250Amethod%2520and%2520an%2520active%2520decoder%2520that%2520can%2520be%2520applied%2520to%2520any%2520occupancy%2520prediction%250Amodel%252C%2520with%2520the%2520potential%2520to%2520enhance%2520their%2520performance.%2520Experiments%2520conducted%250Aon%2520nuScenes-Occupancy%2520and%2520nuScenes-Occ3D%2520demonstrate%2520our%2520framework%2527s%2520superior%250Aperformance.%2520Detailed%2520ablation%2520studies%2520highlight%2520the%2520effectiveness%2520of%2520each%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05329v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccFusion%3A%20Depth%20Estimation%20Free%20Multi-sensor%20Fusion%20for%203D%20Occupancy%0A%20%20Prediction&entry.906535625=Ji%20Zhang%20and%20Yiran%20Ding%20and%20Zixin%20Liu&entry.1292438233=%20%203D%20occupancy%20prediction%20based%20on%20multi-sensor%20fusion%2Ccrucial%20for%20a%20reliable%0Aautonomous%20driving%20system%2C%20enables%20fine-grained%20understanding%20of%203D%20scenes.%0APrevious%20fusion-based%203D%20occupancy%20predictions%20relied%20on%20depth%20estimation%20for%0Aprocessing%202D%20image%20features.%20However%2C%20depth%20estimation%20is%20an%20ill-posed%0Aproblem%2C%20hindering%20the%20accuracy%20and%20robustness%20of%20these%20methods.%20Furthermore%2C%0Afine-grained%20occupancy%20prediction%20demands%20extensive%20computational%20resources.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20OccFusion%2C%20a%20depth%20estimation%20free%20multi-modal%0Afusion%20framework.%20Additionally%2C%20we%20introduce%20a%20generalizable%20active%20training%0Amethod%20and%20an%20active%20decoder%20that%20can%20be%20applied%20to%20any%20occupancy%20prediction%0Amodel%2C%20with%20the%20potential%20to%20enhance%20their%20performance.%20Experiments%20conducted%0Aon%20nuScenes-Occupancy%20and%20nuScenes-Occ3D%20demonstrate%20our%20framework%27s%20superior%0Aperformance.%20Detailed%20ablation%20studies%20highlight%20the%20effectiveness%20of%20each%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05329v2&entry.124074799=Read"},
{"title": "Towards Human-Like Driving: Active Inference in Autonomous Vehicle\n  Control", "author": "Elahe Delavari and John Moore and Junho Hong and Jaerock Kwon", "abstract": "  This paper presents a novel approach to Autonomous Vehicle (AV) control\nthrough the application of active inference, a theory derived from neuroscience\nthat conceptualizes the brain as a predictive machine. Traditional autonomous\ndriving systems rely heavily on Modular Pipelines, Imitation Learning, or\nReinforcement Learning, each with inherent limitations in adaptability,\ngeneralization, and computational efficiency. Active inference addresses these\nchallenges by minimizing prediction error (termed \"surprise\") through a dynamic\nmodel that balances perception and action. Our method integrates active\ninference with deep learning to manage lateral control in AVs, enabling them to\nperform lane following maneuvers within a simulated urban environment. We\ndemonstrate that our model, despite its simplicity, effectively learns and\ngeneralizes from limited data without extensive retraining, significantly\nreducing computational demands. The proposed approach not only enhances the\nadaptability and performance of AVs in dynamic scenarios but also aligns\nclosely with human-like driving behavior, leveraging a generative model to\npredict and adapt to environmental changes. Results from extensive experiments\nin the CARLA simulator show promising outcomes, outperforming traditional\nmethods in terms of adaptability and efficiency, thereby advancing the\npotential of active inference in real-world autonomous driving applications.\n", "link": "http://arxiv.org/abs/2407.07684v1", "date": "2024-07-10", "relevancy": 2.2482, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6049}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5825}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Human-Like%20Driving%3A%20Active%20Inference%20in%20Autonomous%20Vehicle%0A%20%20Control&body=Title%3A%20Towards%20Human-Like%20Driving%3A%20Active%20Inference%20in%20Autonomous%20Vehicle%0A%20%20Control%0AAuthor%3A%20Elahe%20Delavari%20and%20John%20Moore%20and%20Junho%20Hong%20and%20Jaerock%20Kwon%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20Autonomous%20Vehicle%20%28AV%29%20control%0Athrough%20the%20application%20of%20active%20inference%2C%20a%20theory%20derived%20from%20neuroscience%0Athat%20conceptualizes%20the%20brain%20as%20a%20predictive%20machine.%20Traditional%20autonomous%0Adriving%20systems%20rely%20heavily%20on%20Modular%20Pipelines%2C%20Imitation%20Learning%2C%20or%0AReinforcement%20Learning%2C%20each%20with%20inherent%20limitations%20in%20adaptability%2C%0Ageneralization%2C%20and%20computational%20efficiency.%20Active%20inference%20addresses%20these%0Achallenges%20by%20minimizing%20prediction%20error%20%28termed%20%22surprise%22%29%20through%20a%20dynamic%0Amodel%20that%20balances%20perception%20and%20action.%20Our%20method%20integrates%20active%0Ainference%20with%20deep%20learning%20to%20manage%20lateral%20control%20in%20AVs%2C%20enabling%20them%20to%0Aperform%20lane%20following%20maneuvers%20within%20a%20simulated%20urban%20environment.%20We%0Ademonstrate%20that%20our%20model%2C%20despite%20its%20simplicity%2C%20effectively%20learns%20and%0Ageneralizes%20from%20limited%20data%20without%20extensive%20retraining%2C%20significantly%0Areducing%20computational%20demands.%20The%20proposed%20approach%20not%20only%20enhances%20the%0Aadaptability%20and%20performance%20of%20AVs%20in%20dynamic%20scenarios%20but%20also%20aligns%0Aclosely%20with%20human-like%20driving%20behavior%2C%20leveraging%20a%20generative%20model%20to%0Apredict%20and%20adapt%20to%20environmental%20changes.%20Results%20from%20extensive%20experiments%0Ain%20the%20CARLA%20simulator%20show%20promising%20outcomes%2C%20outperforming%20traditional%0Amethods%20in%20terms%20of%20adaptability%20and%20efficiency%2C%20thereby%20advancing%20the%0Apotential%20of%20active%20inference%20in%20real-world%20autonomous%20driving%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Human-Like%2520Driving%253A%2520Active%2520Inference%2520in%2520Autonomous%2520Vehicle%250A%2520%2520Control%26entry.906535625%3DElahe%2520Delavari%2520and%2520John%2520Moore%2520and%2520Junho%2520Hong%2520and%2520Jaerock%2520Kwon%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520Autonomous%2520Vehicle%2520%2528AV%2529%2520control%250Athrough%2520the%2520application%2520of%2520active%2520inference%252C%2520a%2520theory%2520derived%2520from%2520neuroscience%250Athat%2520conceptualizes%2520the%2520brain%2520as%2520a%2520predictive%2520machine.%2520Traditional%2520autonomous%250Adriving%2520systems%2520rely%2520heavily%2520on%2520Modular%2520Pipelines%252C%2520Imitation%2520Learning%252C%2520or%250AReinforcement%2520Learning%252C%2520each%2520with%2520inherent%2520limitations%2520in%2520adaptability%252C%250Ageneralization%252C%2520and%2520computational%2520efficiency.%2520Active%2520inference%2520addresses%2520these%250Achallenges%2520by%2520minimizing%2520prediction%2520error%2520%2528termed%2520%2522surprise%2522%2529%2520through%2520a%2520dynamic%250Amodel%2520that%2520balances%2520perception%2520and%2520action.%2520Our%2520method%2520integrates%2520active%250Ainference%2520with%2520deep%2520learning%2520to%2520manage%2520lateral%2520control%2520in%2520AVs%252C%2520enabling%2520them%2520to%250Aperform%2520lane%2520following%2520maneuvers%2520within%2520a%2520simulated%2520urban%2520environment.%2520We%250Ademonstrate%2520that%2520our%2520model%252C%2520despite%2520its%2520simplicity%252C%2520effectively%2520learns%2520and%250Ageneralizes%2520from%2520limited%2520data%2520without%2520extensive%2520retraining%252C%2520significantly%250Areducing%2520computational%2520demands.%2520The%2520proposed%2520approach%2520not%2520only%2520enhances%2520the%250Aadaptability%2520and%2520performance%2520of%2520AVs%2520in%2520dynamic%2520scenarios%2520but%2520also%2520aligns%250Aclosely%2520with%2520human-like%2520driving%2520behavior%252C%2520leveraging%2520a%2520generative%2520model%2520to%250Apredict%2520and%2520adapt%2520to%2520environmental%2520changes.%2520Results%2520from%2520extensive%2520experiments%250Ain%2520the%2520CARLA%2520simulator%2520show%2520promising%2520outcomes%252C%2520outperforming%2520traditional%250Amethods%2520in%2520terms%2520of%2520adaptability%2520and%2520efficiency%252C%2520thereby%2520advancing%2520the%250Apotential%2520of%2520active%2520inference%2520in%2520real-world%2520autonomous%2520driving%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Human-Like%20Driving%3A%20Active%20Inference%20in%20Autonomous%20Vehicle%0A%20%20Control&entry.906535625=Elahe%20Delavari%20and%20John%20Moore%20and%20Junho%20Hong%20and%20Jaerock%20Kwon&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20Autonomous%20Vehicle%20%28AV%29%20control%0Athrough%20the%20application%20of%20active%20inference%2C%20a%20theory%20derived%20from%20neuroscience%0Athat%20conceptualizes%20the%20brain%20as%20a%20predictive%20machine.%20Traditional%20autonomous%0Adriving%20systems%20rely%20heavily%20on%20Modular%20Pipelines%2C%20Imitation%20Learning%2C%20or%0AReinforcement%20Learning%2C%20each%20with%20inherent%20limitations%20in%20adaptability%2C%0Ageneralization%2C%20and%20computational%20efficiency.%20Active%20inference%20addresses%20these%0Achallenges%20by%20minimizing%20prediction%20error%20%28termed%20%22surprise%22%29%20through%20a%20dynamic%0Amodel%20that%20balances%20perception%20and%20action.%20Our%20method%20integrates%20active%0Ainference%20with%20deep%20learning%20to%20manage%20lateral%20control%20in%20AVs%2C%20enabling%20them%20to%0Aperform%20lane%20following%20maneuvers%20within%20a%20simulated%20urban%20environment.%20We%0Ademonstrate%20that%20our%20model%2C%20despite%20its%20simplicity%2C%20effectively%20learns%20and%0Ageneralizes%20from%20limited%20data%20without%20extensive%20retraining%2C%20significantly%0Areducing%20computational%20demands.%20The%20proposed%20approach%20not%20only%20enhances%20the%0Aadaptability%20and%20performance%20of%20AVs%20in%20dynamic%20scenarios%20but%20also%20aligns%0Aclosely%20with%20human-like%20driving%20behavior%2C%20leveraging%20a%20generative%20model%20to%0Apredict%20and%20adapt%20to%20environmental%20changes.%20Results%20from%20extensive%20experiments%0Ain%20the%20CARLA%20simulator%20show%20promising%20outcomes%2C%20outperforming%20traditional%0Amethods%20in%20terms%20of%20adaptability%20and%20efficiency%2C%20thereby%20advancing%20the%0Apotential%20of%20active%20inference%20in%20real-world%20autonomous%20driving%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07684v1&entry.124074799=Read"},
{"title": "Let Occ Flow: Self-Supervised 3D Occupancy Flow Prediction", "author": "Yili Liu and Linzhan Mou and Xuan Yu and Chenrui Han and Sitong Mao and Rong Xiong and Yue Wang", "abstract": "  Accurate perception of the dynamic environment is a fundamental task for\nautonomous driving and robot systems. This paper introduces Let Occ Flow, the\nfirst self-supervised work for joint 3D occupancy and occupancy flow prediction\nusing only camera inputs, eliminating the need for 3D annotations. Utilizing\nTPV for unified scene representation and deformable attention layers for\nfeature aggregation, our approach incorporates a backward-forward temporal\nattention module to capture dynamic object dependencies, followed by a 3D\nrefine module for fine-gained volumetric representation. Besides, our method\nextends differentiable rendering to 3D volumetric flow fields, leveraging\nzero-shot 2D segmentation and optical flow cues for dynamic decomposition and\nmotion optimization. Extensive experiments on nuScenes and KITTI datasets\ndemonstrate the competitive performance of our approach over prior\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.07587v1", "date": "2024-07-10", "relevancy": 2.2345, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5832}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5556}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%20Occ%20Flow%3A%20Self-Supervised%203D%20Occupancy%20Flow%20Prediction&body=Title%3A%20Let%20Occ%20Flow%3A%20Self-Supervised%203D%20Occupancy%20Flow%20Prediction%0AAuthor%3A%20Yili%20Liu%20and%20Linzhan%20Mou%20and%20Xuan%20Yu%20and%20Chenrui%20Han%20and%20Sitong%20Mao%20and%20Rong%20Xiong%20and%20Yue%20Wang%0AAbstract%3A%20%20%20Accurate%20perception%20of%20the%20dynamic%20environment%20is%20a%20fundamental%20task%20for%0Aautonomous%20driving%20and%20robot%20systems.%20This%20paper%20introduces%20Let%20Occ%20Flow%2C%20the%0Afirst%20self-supervised%20work%20for%20joint%203D%20occupancy%20and%20occupancy%20flow%20prediction%0Ausing%20only%20camera%20inputs%2C%20eliminating%20the%20need%20for%203D%20annotations.%20Utilizing%0ATPV%20for%20unified%20scene%20representation%20and%20deformable%20attention%20layers%20for%0Afeature%20aggregation%2C%20our%20approach%20incorporates%20a%20backward-forward%20temporal%0Aattention%20module%20to%20capture%20dynamic%20object%20dependencies%2C%20followed%20by%20a%203D%0Arefine%20module%20for%20fine-gained%20volumetric%20representation.%20Besides%2C%20our%20method%0Aextends%20differentiable%20rendering%20to%203D%20volumetric%20flow%20fields%2C%20leveraging%0Azero-shot%202D%20segmentation%20and%20optical%20flow%20cues%20for%20dynamic%20decomposition%20and%0Amotion%20optimization.%20Extensive%20experiments%20on%20nuScenes%20and%20KITTI%20datasets%0Ademonstrate%20the%20competitive%20performance%20of%20our%20approach%20over%20prior%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2520Occ%2520Flow%253A%2520Self-Supervised%25203D%2520Occupancy%2520Flow%2520Prediction%26entry.906535625%3DYili%2520Liu%2520and%2520Linzhan%2520Mou%2520and%2520Xuan%2520Yu%2520and%2520Chenrui%2520Han%2520and%2520Sitong%2520Mao%2520and%2520Rong%2520Xiong%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520Accurate%2520perception%2520of%2520the%2520dynamic%2520environment%2520is%2520a%2520fundamental%2520task%2520for%250Aautonomous%2520driving%2520and%2520robot%2520systems.%2520This%2520paper%2520introduces%2520Let%2520Occ%2520Flow%252C%2520the%250Afirst%2520self-supervised%2520work%2520for%2520joint%25203D%2520occupancy%2520and%2520occupancy%2520flow%2520prediction%250Ausing%2520only%2520camera%2520inputs%252C%2520eliminating%2520the%2520need%2520for%25203D%2520annotations.%2520Utilizing%250ATPV%2520for%2520unified%2520scene%2520representation%2520and%2520deformable%2520attention%2520layers%2520for%250Afeature%2520aggregation%252C%2520our%2520approach%2520incorporates%2520a%2520backward-forward%2520temporal%250Aattention%2520module%2520to%2520capture%2520dynamic%2520object%2520dependencies%252C%2520followed%2520by%2520a%25203D%250Arefine%2520module%2520for%2520fine-gained%2520volumetric%2520representation.%2520Besides%252C%2520our%2520method%250Aextends%2520differentiable%2520rendering%2520to%25203D%2520volumetric%2520flow%2520fields%252C%2520leveraging%250Azero-shot%25202D%2520segmentation%2520and%2520optical%2520flow%2520cues%2520for%2520dynamic%2520decomposition%2520and%250Amotion%2520optimization.%2520Extensive%2520experiments%2520on%2520nuScenes%2520and%2520KITTI%2520datasets%250Ademonstrate%2520the%2520competitive%2520performance%2520of%2520our%2520approach%2520over%2520prior%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%20Occ%20Flow%3A%20Self-Supervised%203D%20Occupancy%20Flow%20Prediction&entry.906535625=Yili%20Liu%20and%20Linzhan%20Mou%20and%20Xuan%20Yu%20and%20Chenrui%20Han%20and%20Sitong%20Mao%20and%20Rong%20Xiong%20and%20Yue%20Wang&entry.1292438233=%20%20Accurate%20perception%20of%20the%20dynamic%20environment%20is%20a%20fundamental%20task%20for%0Aautonomous%20driving%20and%20robot%20systems.%20This%20paper%20introduces%20Let%20Occ%20Flow%2C%20the%0Afirst%20self-supervised%20work%20for%20joint%203D%20occupancy%20and%20occupancy%20flow%20prediction%0Ausing%20only%20camera%20inputs%2C%20eliminating%20the%20need%20for%203D%20annotations.%20Utilizing%0ATPV%20for%20unified%20scene%20representation%20and%20deformable%20attention%20layers%20for%0Afeature%20aggregation%2C%20our%20approach%20incorporates%20a%20backward-forward%20temporal%0Aattention%20module%20to%20capture%20dynamic%20object%20dependencies%2C%20followed%20by%20a%203D%0Arefine%20module%20for%20fine-gained%20volumetric%20representation.%20Besides%2C%20our%20method%0Aextends%20differentiable%20rendering%20to%203D%20volumetric%20flow%20fields%2C%20leveraging%0Azero-shot%202D%20segmentation%20and%20optical%20flow%20cues%20for%20dynamic%20decomposition%20and%0Amotion%20optimization.%20Extensive%20experiments%20on%20nuScenes%20and%20KITTI%20datasets%0Ademonstrate%20the%20competitive%20performance%20of%20our%20approach%20over%20prior%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07587v1&entry.124074799=Read"},
{"title": "Disentangling Masked Autoencoders for Unsupervised Domain Generalization", "author": "An Zhang and Han Wang and Xiang Wang and Tat-Seng Chua", "abstract": "  Domain Generalization (DG), designed to enhance out-of-distribution (OOD)\ngeneralization, is all about learning invariance against domain shifts\nutilizing sufficient supervision signals. Yet, the scarcity of such labeled\ndata has led to the rise of unsupervised domain generalization (UDG) - a more\nimportant yet challenging task in that models are trained across diverse\ndomains in an unsupervised manner and eventually tested on unseen domains. UDG\nis fast gaining attention but is still far from well-studied. To close the\nresearch gap, we propose a novel learning framework designed for UDG, termed\nthe Disentangled Masked Auto Encoder (DisMAE), aiming to discover the\ndisentangled representations that faithfully reveal the intrinsic features and\nsuperficial variations without access to the class label. At its core is the\ndistillation of domain-invariant semantic features, which cannot be\ndistinguished by domain classifier, while filtering out the domain-specific\nvariations (for example, color schemes and texture patterns) that are unstable\nand redundant. Notably, DisMAE co-trains the asymmetric dual-branch\narchitecture with semantic and lightweight variation encoders, offering dynamic\ndata manipulation and representation level augmentation capabilities. Extensive\nexperiments on four benchmark datasets (i.e., DomainNet, PACS, VLCS, Colored\nMNIST) with both DG and UDG tasks demonstrate that DisMAE can achieve\ncompetitive OOD performance compared with the state-of-the-art DG and UDG\nbaselines, which shed light on potential research line in improving the\ngeneralization ability with large-scale unlabeled data.\n", "link": "http://arxiv.org/abs/2407.07544v1", "date": "2024-07-10", "relevancy": 2.2141, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5617}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20Masked%20Autoencoders%20for%20Unsupervised%20Domain%20Generalization&body=Title%3A%20Disentangling%20Masked%20Autoencoders%20for%20Unsupervised%20Domain%20Generalization%0AAuthor%3A%20An%20Zhang%20and%20Han%20Wang%20and%20Xiang%20Wang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Domain%20Generalization%20%28DG%29%2C%20designed%20to%20enhance%20out-of-distribution%20%28OOD%29%0Ageneralization%2C%20is%20all%20about%20learning%20invariance%20against%20domain%20shifts%0Autilizing%20sufficient%20supervision%20signals.%20Yet%2C%20the%20scarcity%20of%20such%20labeled%0Adata%20has%20led%20to%20the%20rise%20of%20unsupervised%20domain%20generalization%20%28UDG%29%20-%20a%20more%0Aimportant%20yet%20challenging%20task%20in%20that%20models%20are%20trained%20across%20diverse%0Adomains%20in%20an%20unsupervised%20manner%20and%20eventually%20tested%20on%20unseen%20domains.%20UDG%0Ais%20fast%20gaining%20attention%20but%20is%20still%20far%20from%20well-studied.%20To%20close%20the%0Aresearch%20gap%2C%20we%20propose%20a%20novel%20learning%20framework%20designed%20for%20UDG%2C%20termed%0Athe%20Disentangled%20Masked%20Auto%20Encoder%20%28DisMAE%29%2C%20aiming%20to%20discover%20the%0Adisentangled%20representations%20that%20faithfully%20reveal%20the%20intrinsic%20features%20and%0Asuperficial%20variations%20without%20access%20to%20the%20class%20label.%20At%20its%20core%20is%20the%0Adistillation%20of%20domain-invariant%20semantic%20features%2C%20which%20cannot%20be%0Adistinguished%20by%20domain%20classifier%2C%20while%20filtering%20out%20the%20domain-specific%0Avariations%20%28for%20example%2C%20color%20schemes%20and%20texture%20patterns%29%20that%20are%20unstable%0Aand%20redundant.%20Notably%2C%20DisMAE%20co-trains%20the%20asymmetric%20dual-branch%0Aarchitecture%20with%20semantic%20and%20lightweight%20variation%20encoders%2C%20offering%20dynamic%0Adata%20manipulation%20and%20representation%20level%20augmentation%20capabilities.%20Extensive%0Aexperiments%20on%20four%20benchmark%20datasets%20%28i.e.%2C%20DomainNet%2C%20PACS%2C%20VLCS%2C%20Colored%0AMNIST%29%20with%20both%20DG%20and%20UDG%20tasks%20demonstrate%20that%20DisMAE%20can%20achieve%0Acompetitive%20OOD%20performance%20compared%20with%20the%20state-of-the-art%20DG%20and%20UDG%0Abaselines%2C%20which%20shed%20light%20on%20potential%20research%20line%20in%20improving%20the%0Ageneralization%20ability%20with%20large-scale%20unlabeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520Masked%2520Autoencoders%2520for%2520Unsupervised%2520Domain%2520Generalization%26entry.906535625%3DAn%2520Zhang%2520and%2520Han%2520Wang%2520and%2520Xiang%2520Wang%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Domain%2520Generalization%2520%2528DG%2529%252C%2520designed%2520to%2520enhance%2520out-of-distribution%2520%2528OOD%2529%250Ageneralization%252C%2520is%2520all%2520about%2520learning%2520invariance%2520against%2520domain%2520shifts%250Autilizing%2520sufficient%2520supervision%2520signals.%2520Yet%252C%2520the%2520scarcity%2520of%2520such%2520labeled%250Adata%2520has%2520led%2520to%2520the%2520rise%2520of%2520unsupervised%2520domain%2520generalization%2520%2528UDG%2529%2520-%2520a%2520more%250Aimportant%2520yet%2520challenging%2520task%2520in%2520that%2520models%2520are%2520trained%2520across%2520diverse%250Adomains%2520in%2520an%2520unsupervised%2520manner%2520and%2520eventually%2520tested%2520on%2520unseen%2520domains.%2520UDG%250Ais%2520fast%2520gaining%2520attention%2520but%2520is%2520still%2520far%2520from%2520well-studied.%2520To%2520close%2520the%250Aresearch%2520gap%252C%2520we%2520propose%2520a%2520novel%2520learning%2520framework%2520designed%2520for%2520UDG%252C%2520termed%250Athe%2520Disentangled%2520Masked%2520Auto%2520Encoder%2520%2528DisMAE%2529%252C%2520aiming%2520to%2520discover%2520the%250Adisentangled%2520representations%2520that%2520faithfully%2520reveal%2520the%2520intrinsic%2520features%2520and%250Asuperficial%2520variations%2520without%2520access%2520to%2520the%2520class%2520label.%2520At%2520its%2520core%2520is%2520the%250Adistillation%2520of%2520domain-invariant%2520semantic%2520features%252C%2520which%2520cannot%2520be%250Adistinguished%2520by%2520domain%2520classifier%252C%2520while%2520filtering%2520out%2520the%2520domain-specific%250Avariations%2520%2528for%2520example%252C%2520color%2520schemes%2520and%2520texture%2520patterns%2529%2520that%2520are%2520unstable%250Aand%2520redundant.%2520Notably%252C%2520DisMAE%2520co-trains%2520the%2520asymmetric%2520dual-branch%250Aarchitecture%2520with%2520semantic%2520and%2520lightweight%2520variation%2520encoders%252C%2520offering%2520dynamic%250Adata%2520manipulation%2520and%2520representation%2520level%2520augmentation%2520capabilities.%2520Extensive%250Aexperiments%2520on%2520four%2520benchmark%2520datasets%2520%2528i.e.%252C%2520DomainNet%252C%2520PACS%252C%2520VLCS%252C%2520Colored%250AMNIST%2529%2520with%2520both%2520DG%2520and%2520UDG%2520tasks%2520demonstrate%2520that%2520DisMAE%2520can%2520achieve%250Acompetitive%2520OOD%2520performance%2520compared%2520with%2520the%2520state-of-the-art%2520DG%2520and%2520UDG%250Abaselines%252C%2520which%2520shed%2520light%2520on%2520potential%2520research%2520line%2520in%2520improving%2520the%250Ageneralization%2520ability%2520with%2520large-scale%2520unlabeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20Masked%20Autoencoders%20for%20Unsupervised%20Domain%20Generalization&entry.906535625=An%20Zhang%20and%20Han%20Wang%20and%20Xiang%20Wang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Domain%20Generalization%20%28DG%29%2C%20designed%20to%20enhance%20out-of-distribution%20%28OOD%29%0Ageneralization%2C%20is%20all%20about%20learning%20invariance%20against%20domain%20shifts%0Autilizing%20sufficient%20supervision%20signals.%20Yet%2C%20the%20scarcity%20of%20such%20labeled%0Adata%20has%20led%20to%20the%20rise%20of%20unsupervised%20domain%20generalization%20%28UDG%29%20-%20a%20more%0Aimportant%20yet%20challenging%20task%20in%20that%20models%20are%20trained%20across%20diverse%0Adomains%20in%20an%20unsupervised%20manner%20and%20eventually%20tested%20on%20unseen%20domains.%20UDG%0Ais%20fast%20gaining%20attention%20but%20is%20still%20far%20from%20well-studied.%20To%20close%20the%0Aresearch%20gap%2C%20we%20propose%20a%20novel%20learning%20framework%20designed%20for%20UDG%2C%20termed%0Athe%20Disentangled%20Masked%20Auto%20Encoder%20%28DisMAE%29%2C%20aiming%20to%20discover%20the%0Adisentangled%20representations%20that%20faithfully%20reveal%20the%20intrinsic%20features%20and%0Asuperficial%20variations%20without%20access%20to%20the%20class%20label.%20At%20its%20core%20is%20the%0Adistillation%20of%20domain-invariant%20semantic%20features%2C%20which%20cannot%20be%0Adistinguished%20by%20domain%20classifier%2C%20while%20filtering%20out%20the%20domain-specific%0Avariations%20%28for%20example%2C%20color%20schemes%20and%20texture%20patterns%29%20that%20are%20unstable%0Aand%20redundant.%20Notably%2C%20DisMAE%20co-trains%20the%20asymmetric%20dual-branch%0Aarchitecture%20with%20semantic%20and%20lightweight%20variation%20encoders%2C%20offering%20dynamic%0Adata%20manipulation%20and%20representation%20level%20augmentation%20capabilities.%20Extensive%0Aexperiments%20on%20four%20benchmark%20datasets%20%28i.e.%2C%20DomainNet%2C%20PACS%2C%20VLCS%2C%20Colored%0AMNIST%29%20with%20both%20DG%20and%20UDG%20tasks%20demonstrate%20that%20DisMAE%20can%20achieve%0Acompetitive%20OOD%20performance%20compared%20with%20the%20state-of-the-art%20DG%20and%20UDG%0Abaselines%2C%20which%20shed%20light%20on%20potential%20research%20line%20in%20improving%20the%0Ageneralization%20ability%20with%20large-scale%20unlabeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07544v1&entry.124074799=Read"},
{"title": "Online Video Quality Enhancement with Spatial-Temporal Look-up Tables", "author": "Zefan Qu and Xinyang Jiang and Yifan Yang and Dongsheng Li and Cairong Zhao", "abstract": "  Low latency rates are crucial for online video-based applications, such as\nvideo conferencing and cloud gaming, which make improving video quality in\nonline scenarios increasingly important. However, existing quality enhancement\nmethods are limited by slow inference speed and the requirement for temporal\ninformation contained in future frames, making it challenging to deploy them\ndirectly in online tasks. In this paper, we propose a novel method, STLVQE,\nspecifically designed to address the rarely studied online video quality\nenhancement (Online-VQE) problem. Our STLVQE designs a new VQE framework which\ncontains a Module-Agnostic Feature Extractor that greatly reduces the redundant\ncomputations and redesign the propagation, alignment, and enhancement module of\nthe network. A Spatial-Temporal Look-up Tables (STL) is proposed, which\nextracts spatial-temporal information in videos while saving substantial\ninference time. To the best of our knowledge, we are the first to exploit the\nLUT structure to extract temporal information in video tasks. Extensive\nexperiments on the MFQE 2.0 dataset demonstrate that our STLVQE achieves a\nsatisfactory performance-speed trade-off.\n", "link": "http://arxiv.org/abs/2311.13616v2", "date": "2024-07-10", "relevancy": 2.2133, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5811}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5336}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Video%20Quality%20Enhancement%20with%20Spatial-Temporal%20Look-up%20Tables&body=Title%3A%20Online%20Video%20Quality%20Enhancement%20with%20Spatial-Temporal%20Look-up%20Tables%0AAuthor%3A%20Zefan%20Qu%20and%20Xinyang%20Jiang%20and%20Yifan%20Yang%20and%20Dongsheng%20Li%20and%20Cairong%20Zhao%0AAbstract%3A%20%20%20Low%20latency%20rates%20are%20crucial%20for%20online%20video-based%20applications%2C%20such%20as%0Avideo%20conferencing%20and%20cloud%20gaming%2C%20which%20make%20improving%20video%20quality%20in%0Aonline%20scenarios%20increasingly%20important.%20However%2C%20existing%20quality%20enhancement%0Amethods%20are%20limited%20by%20slow%20inference%20speed%20and%20the%20requirement%20for%20temporal%0Ainformation%20contained%20in%20future%20frames%2C%20making%20it%20challenging%20to%20deploy%20them%0Adirectly%20in%20online%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%20STLVQE%2C%0Aspecifically%20designed%20to%20address%20the%20rarely%20studied%20online%20video%20quality%0Aenhancement%20%28Online-VQE%29%20problem.%20Our%20STLVQE%20designs%20a%20new%20VQE%20framework%20which%0Acontains%20a%20Module-Agnostic%20Feature%20Extractor%20that%20greatly%20reduces%20the%20redundant%0Acomputations%20and%20redesign%20the%20propagation%2C%20alignment%2C%20and%20enhancement%20module%20of%0Athe%20network.%20A%20Spatial-Temporal%20Look-up%20Tables%20%28STL%29%20is%20proposed%2C%20which%0Aextracts%20spatial-temporal%20information%20in%20videos%20while%20saving%20substantial%0Ainference%20time.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20exploit%20the%0ALUT%20structure%20to%20extract%20temporal%20information%20in%20video%20tasks.%20Extensive%0Aexperiments%20on%20the%20MFQE%202.0%20dataset%20demonstrate%20that%20our%20STLVQE%20achieves%20a%0Asatisfactory%20performance-speed%20trade-off.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13616v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Video%2520Quality%2520Enhancement%2520with%2520Spatial-Temporal%2520Look-up%2520Tables%26entry.906535625%3DZefan%2520Qu%2520and%2520Xinyang%2520Jiang%2520and%2520Yifan%2520Yang%2520and%2520Dongsheng%2520Li%2520and%2520Cairong%2520Zhao%26entry.1292438233%3D%2520%2520Low%2520latency%2520rates%2520are%2520crucial%2520for%2520online%2520video-based%2520applications%252C%2520such%2520as%250Avideo%2520conferencing%2520and%2520cloud%2520gaming%252C%2520which%2520make%2520improving%2520video%2520quality%2520in%250Aonline%2520scenarios%2520increasingly%2520important.%2520However%252C%2520existing%2520quality%2520enhancement%250Amethods%2520are%2520limited%2520by%2520slow%2520inference%2520speed%2520and%2520the%2520requirement%2520for%2520temporal%250Ainformation%2520contained%2520in%2520future%2520frames%252C%2520making%2520it%2520challenging%2520to%2520deploy%2520them%250Adirectly%2520in%2520online%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520STLVQE%252C%250Aspecifically%2520designed%2520to%2520address%2520the%2520rarely%2520studied%2520online%2520video%2520quality%250Aenhancement%2520%2528Online-VQE%2529%2520problem.%2520Our%2520STLVQE%2520designs%2520a%2520new%2520VQE%2520framework%2520which%250Acontains%2520a%2520Module-Agnostic%2520Feature%2520Extractor%2520that%2520greatly%2520reduces%2520the%2520redundant%250Acomputations%2520and%2520redesign%2520the%2520propagation%252C%2520alignment%252C%2520and%2520enhancement%2520module%2520of%250Athe%2520network.%2520A%2520Spatial-Temporal%2520Look-up%2520Tables%2520%2528STL%2529%2520is%2520proposed%252C%2520which%250Aextracts%2520spatial-temporal%2520information%2520in%2520videos%2520while%2520saving%2520substantial%250Ainference%2520time.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520exploit%2520the%250ALUT%2520structure%2520to%2520extract%2520temporal%2520information%2520in%2520video%2520tasks.%2520Extensive%250Aexperiments%2520on%2520the%2520MFQE%25202.0%2520dataset%2520demonstrate%2520that%2520our%2520STLVQE%2520achieves%2520a%250Asatisfactory%2520performance-speed%2520trade-off.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13616v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Video%20Quality%20Enhancement%20with%20Spatial-Temporal%20Look-up%20Tables&entry.906535625=Zefan%20Qu%20and%20Xinyang%20Jiang%20and%20Yifan%20Yang%20and%20Dongsheng%20Li%20and%20Cairong%20Zhao&entry.1292438233=%20%20Low%20latency%20rates%20are%20crucial%20for%20online%20video-based%20applications%2C%20such%20as%0Avideo%20conferencing%20and%20cloud%20gaming%2C%20which%20make%20improving%20video%20quality%20in%0Aonline%20scenarios%20increasingly%20important.%20However%2C%20existing%20quality%20enhancement%0Amethods%20are%20limited%20by%20slow%20inference%20speed%20and%20the%20requirement%20for%20temporal%0Ainformation%20contained%20in%20future%20frames%2C%20making%20it%20challenging%20to%20deploy%20them%0Adirectly%20in%20online%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%20STLVQE%2C%0Aspecifically%20designed%20to%20address%20the%20rarely%20studied%20online%20video%20quality%0Aenhancement%20%28Online-VQE%29%20problem.%20Our%20STLVQE%20designs%20a%20new%20VQE%20framework%20which%0Acontains%20a%20Module-Agnostic%20Feature%20Extractor%20that%20greatly%20reduces%20the%20redundant%0Acomputations%20and%20redesign%20the%20propagation%2C%20alignment%2C%20and%20enhancement%20module%20of%0Athe%20network.%20A%20Spatial-Temporal%20Look-up%20Tables%20%28STL%29%20is%20proposed%2C%20which%0Aextracts%20spatial-temporal%20information%20in%20videos%20while%20saving%20substantial%0Ainference%20time.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20exploit%20the%0ALUT%20structure%20to%20extract%20temporal%20information%20in%20video%20tasks.%20Extensive%0Aexperiments%20on%20the%20MFQE%202.0%20dataset%20demonstrate%20that%20our%20STLVQE%20achieves%20a%0Asatisfactory%20performance-speed%20trade-off.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13616v2&entry.124074799=Read"},
{"title": "FedGT: Identification of Malicious Clients in Federated Learning with\n  Secure Aggregation", "author": "Marvin Xhemrishi and Johan \u00d6stman and Antonia Wachter-Zeh and Alexandre Graell i Amat", "abstract": "  We propose FedGT, a novel framework for identifying malicious clients in\nfederated learning with secure aggregation. Inspired by group testing, the\nframework leverages overlapping groups of clients to identify the presence of\nmalicious clients in the groups via a decoding operation. The clients\nidentified as malicious are then removed from the model training, which is\nperformed over the remaining clients. By choosing the size, number, and overlap\nbetween groups, FedGT strikes a balance between privacy and security.\nSpecifically, the server learns the aggregated model of the clients in each\ngroup - vanilla federated learning and secure aggregation correspond to the\nextreme cases of FedGT with group size equal to one and the total number of\nclients, respectively. The effectiveness of FedGT is demonstrated through\nextensive experiments on the MNIST, CIFAR-10, and ISIC2019 datasets in a\ncross-silo setting under different data-poisoning attacks. These experiments\nshowcase FedGT's ability to identify malicious clients, resulting in high model\nutility. We further show that FedGT significantly outperforms the private\nrobust aggregation approach based on the geometric median recently proposed by\nPillutla et al. in multiple settings.\n", "link": "http://arxiv.org/abs/2305.05506v3", "date": "2024-07-10", "relevancy": 2.2078, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4466}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4393}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedGT%3A%20Identification%20of%20Malicious%20Clients%20in%20Federated%20Learning%20with%0A%20%20Secure%20Aggregation&body=Title%3A%20FedGT%3A%20Identification%20of%20Malicious%20Clients%20in%20Federated%20Learning%20with%0A%20%20Secure%20Aggregation%0AAuthor%3A%20Marvin%20Xhemrishi%20and%20Johan%20%C3%96stman%20and%20Antonia%20Wachter-Zeh%20and%20Alexandre%20Graell%20i%20Amat%0AAbstract%3A%20%20%20We%20propose%20FedGT%2C%20a%20novel%20framework%20for%20identifying%20malicious%20clients%20in%0Afederated%20learning%20with%20secure%20aggregation.%20Inspired%20by%20group%20testing%2C%20the%0Aframework%20leverages%20overlapping%20groups%20of%20clients%20to%20identify%20the%20presence%20of%0Amalicious%20clients%20in%20the%20groups%20via%20a%20decoding%20operation.%20The%20clients%0Aidentified%20as%20malicious%20are%20then%20removed%20from%20the%20model%20training%2C%20which%20is%0Aperformed%20over%20the%20remaining%20clients.%20By%20choosing%20the%20size%2C%20number%2C%20and%20overlap%0Abetween%20groups%2C%20FedGT%20strikes%20a%20balance%20between%20privacy%20and%20security.%0ASpecifically%2C%20the%20server%20learns%20the%20aggregated%20model%20of%20the%20clients%20in%20each%0Agroup%20-%20vanilla%20federated%20learning%20and%20secure%20aggregation%20correspond%20to%20the%0Aextreme%20cases%20of%20FedGT%20with%20group%20size%20equal%20to%20one%20and%20the%20total%20number%20of%0Aclients%2C%20respectively.%20The%20effectiveness%20of%20FedGT%20is%20demonstrated%20through%0Aextensive%20experiments%20on%20the%20MNIST%2C%20CIFAR-10%2C%20and%20ISIC2019%20datasets%20in%20a%0Across-silo%20setting%20under%20different%20data-poisoning%20attacks.%20These%20experiments%0Ashowcase%20FedGT%27s%20ability%20to%20identify%20malicious%20clients%2C%20resulting%20in%20high%20model%0Autility.%20We%20further%20show%20that%20FedGT%20significantly%20outperforms%20the%20private%0Arobust%20aggregation%20approach%20based%20on%20the%20geometric%20median%20recently%20proposed%20by%0APillutla%20et%20al.%20in%20multiple%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.05506v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedGT%253A%2520Identification%2520of%2520Malicious%2520Clients%2520in%2520Federated%2520Learning%2520with%250A%2520%2520Secure%2520Aggregation%26entry.906535625%3DMarvin%2520Xhemrishi%2520and%2520Johan%2520%25C3%2596stman%2520and%2520Antonia%2520Wachter-Zeh%2520and%2520Alexandre%2520Graell%2520i%2520Amat%26entry.1292438233%3D%2520%2520We%2520propose%2520FedGT%252C%2520a%2520novel%2520framework%2520for%2520identifying%2520malicious%2520clients%2520in%250Afederated%2520learning%2520with%2520secure%2520aggregation.%2520Inspired%2520by%2520group%2520testing%252C%2520the%250Aframework%2520leverages%2520overlapping%2520groups%2520of%2520clients%2520to%2520identify%2520the%2520presence%2520of%250Amalicious%2520clients%2520in%2520the%2520groups%2520via%2520a%2520decoding%2520operation.%2520The%2520clients%250Aidentified%2520as%2520malicious%2520are%2520then%2520removed%2520from%2520the%2520model%2520training%252C%2520which%2520is%250Aperformed%2520over%2520the%2520remaining%2520clients.%2520By%2520choosing%2520the%2520size%252C%2520number%252C%2520and%2520overlap%250Abetween%2520groups%252C%2520FedGT%2520strikes%2520a%2520balance%2520between%2520privacy%2520and%2520security.%250ASpecifically%252C%2520the%2520server%2520learns%2520the%2520aggregated%2520model%2520of%2520the%2520clients%2520in%2520each%250Agroup%2520-%2520vanilla%2520federated%2520learning%2520and%2520secure%2520aggregation%2520correspond%2520to%2520the%250Aextreme%2520cases%2520of%2520FedGT%2520with%2520group%2520size%2520equal%2520to%2520one%2520and%2520the%2520total%2520number%2520of%250Aclients%252C%2520respectively.%2520The%2520effectiveness%2520of%2520FedGT%2520is%2520demonstrated%2520through%250Aextensive%2520experiments%2520on%2520the%2520MNIST%252C%2520CIFAR-10%252C%2520and%2520ISIC2019%2520datasets%2520in%2520a%250Across-silo%2520setting%2520under%2520different%2520data-poisoning%2520attacks.%2520These%2520experiments%250Ashowcase%2520FedGT%2527s%2520ability%2520to%2520identify%2520malicious%2520clients%252C%2520resulting%2520in%2520high%2520model%250Autility.%2520We%2520further%2520show%2520that%2520FedGT%2520significantly%2520outperforms%2520the%2520private%250Arobust%2520aggregation%2520approach%2520based%2520on%2520the%2520geometric%2520median%2520recently%2520proposed%2520by%250APillutla%2520et%2520al.%2520in%2520multiple%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.05506v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedGT%3A%20Identification%20of%20Malicious%20Clients%20in%20Federated%20Learning%20with%0A%20%20Secure%20Aggregation&entry.906535625=Marvin%20Xhemrishi%20and%20Johan%20%C3%96stman%20and%20Antonia%20Wachter-Zeh%20and%20Alexandre%20Graell%20i%20Amat&entry.1292438233=%20%20We%20propose%20FedGT%2C%20a%20novel%20framework%20for%20identifying%20malicious%20clients%20in%0Afederated%20learning%20with%20secure%20aggregation.%20Inspired%20by%20group%20testing%2C%20the%0Aframework%20leverages%20overlapping%20groups%20of%20clients%20to%20identify%20the%20presence%20of%0Amalicious%20clients%20in%20the%20groups%20via%20a%20decoding%20operation.%20The%20clients%0Aidentified%20as%20malicious%20are%20then%20removed%20from%20the%20model%20training%2C%20which%20is%0Aperformed%20over%20the%20remaining%20clients.%20By%20choosing%20the%20size%2C%20number%2C%20and%20overlap%0Abetween%20groups%2C%20FedGT%20strikes%20a%20balance%20between%20privacy%20and%20security.%0ASpecifically%2C%20the%20server%20learns%20the%20aggregated%20model%20of%20the%20clients%20in%20each%0Agroup%20-%20vanilla%20federated%20learning%20and%20secure%20aggregation%20correspond%20to%20the%0Aextreme%20cases%20of%20FedGT%20with%20group%20size%20equal%20to%20one%20and%20the%20total%20number%20of%0Aclients%2C%20respectively.%20The%20effectiveness%20of%20FedGT%20is%20demonstrated%20through%0Aextensive%20experiments%20on%20the%20MNIST%2C%20CIFAR-10%2C%20and%20ISIC2019%20datasets%20in%20a%0Across-silo%20setting%20under%20different%20data-poisoning%20attacks.%20These%20experiments%0Ashowcase%20FedGT%27s%20ability%20to%20identify%20malicious%20clients%2C%20resulting%20in%20high%20model%0Autility.%20We%20further%20show%20that%20FedGT%20significantly%20outperforms%20the%20private%0Arobust%20aggregation%20approach%20based%20on%20the%20geometric%20median%20recently%20proposed%20by%0APillutla%20et%20al.%20in%20multiple%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.05506v3&entry.124074799=Read"},
{"title": "The Selective G-Bispectrum and its Inversion: Applications to\n  G-Invariant Networks", "author": "Simon Mataigne and Johan Mathe and Sophia Sanborn and Christopher Hillar and Nina Miolane", "abstract": "  An important problem in signal processing and deep learning is to achieve\n\\textit{invariance} to nuisance factors not relevant for the task. Since many\nof these factors are describable as the action of a group $G$ (e.g. rotations,\ntranslations, scalings), we want methods to be $G$-invariant. The\n$G$-Bispectrum extracts every characteristic of a given signal up to group\naction: for example, the shape of an object in an image, but not its\norientation. Consequently, the $G$-Bispectrum has been incorporated into deep\nneural network architectures as a computational primitive for\n$G$-invariance\\textemdash akin to a pooling mechanism, but with greater\nselectivity and robustness. However, the computational cost of the\n$G$-Bispectrum ($\\mathcal{O}(|G|^2)$, with $|G|$ the size of the group) has\nlimited its widespread adoption. Here, we show that the $G$-Bispectrum\ncomputation contains redundancies that can be reduced into a \\textit{selective\n$G$-Bispectrum} with $\\mathcal{O}(|G|)$ complexity. We prove desirable\nmathematical properties of the selective $G$-Bispectrum and demonstrate how its\nintegration in neural networks enhances accuracy and robustness compared to\ntraditional approaches, while enjoying considerable speeds-up compared to the\nfull $G$-Bispectrum.\n", "link": "http://arxiv.org/abs/2407.07655v1", "date": "2024-07-10", "relevancy": 2.2033, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4486}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4457}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Selective%20G-Bispectrum%20and%20its%20Inversion%3A%20Applications%20to%0A%20%20G-Invariant%20Networks&body=Title%3A%20The%20Selective%20G-Bispectrum%20and%20its%20Inversion%3A%20Applications%20to%0A%20%20G-Invariant%20Networks%0AAuthor%3A%20Simon%20Mataigne%20and%20Johan%20Mathe%20and%20Sophia%20Sanborn%20and%20Christopher%20Hillar%20and%20Nina%20Miolane%0AAbstract%3A%20%20%20An%20important%20problem%20in%20signal%20processing%20and%20deep%20learning%20is%20to%20achieve%0A%5Ctextit%7Binvariance%7D%20to%20nuisance%20factors%20not%20relevant%20for%20the%20task.%20Since%20many%0Aof%20these%20factors%20are%20describable%20as%20the%20action%20of%20a%20group%20%24G%24%20%28e.g.%20rotations%2C%0Atranslations%2C%20scalings%29%2C%20we%20want%20methods%20to%20be%20%24G%24-invariant.%20The%0A%24G%24-Bispectrum%20extracts%20every%20characteristic%20of%20a%20given%20signal%20up%20to%20group%0Aaction%3A%20for%20example%2C%20the%20shape%20of%20an%20object%20in%20an%20image%2C%20but%20not%20its%0Aorientation.%20Consequently%2C%20the%20%24G%24-Bispectrum%20has%20been%20incorporated%20into%20deep%0Aneural%20network%20architectures%20as%20a%20computational%20primitive%20for%0A%24G%24-invariance%5Ctextemdash%20akin%20to%20a%20pooling%20mechanism%2C%20but%20with%20greater%0Aselectivity%20and%20robustness.%20However%2C%20the%20computational%20cost%20of%20the%0A%24G%24-Bispectrum%20%28%24%5Cmathcal%7BO%7D%28%7CG%7C%5E2%29%24%2C%20with%20%24%7CG%7C%24%20the%20size%20of%20the%20group%29%20has%0Alimited%20its%20widespread%20adoption.%20Here%2C%20we%20show%20that%20the%20%24G%24-Bispectrum%0Acomputation%20contains%20redundancies%20that%20can%20be%20reduced%20into%20a%20%5Ctextit%7Bselective%0A%24G%24-Bispectrum%7D%20with%20%24%5Cmathcal%7BO%7D%28%7CG%7C%29%24%20complexity.%20We%20prove%20desirable%0Amathematical%20properties%20of%20the%20selective%20%24G%24-Bispectrum%20and%20demonstrate%20how%20its%0Aintegration%20in%20neural%20networks%20enhances%20accuracy%20and%20robustness%20compared%20to%0Atraditional%20approaches%2C%20while%20enjoying%20considerable%20speeds-up%20compared%20to%20the%0Afull%20%24G%24-Bispectrum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Selective%2520G-Bispectrum%2520and%2520its%2520Inversion%253A%2520Applications%2520to%250A%2520%2520G-Invariant%2520Networks%26entry.906535625%3DSimon%2520Mataigne%2520and%2520Johan%2520Mathe%2520and%2520Sophia%2520Sanborn%2520and%2520Christopher%2520Hillar%2520and%2520Nina%2520Miolane%26entry.1292438233%3D%2520%2520An%2520important%2520problem%2520in%2520signal%2520processing%2520and%2520deep%2520learning%2520is%2520to%2520achieve%250A%255Ctextit%257Binvariance%257D%2520to%2520nuisance%2520factors%2520not%2520relevant%2520for%2520the%2520task.%2520Since%2520many%250Aof%2520these%2520factors%2520are%2520describable%2520as%2520the%2520action%2520of%2520a%2520group%2520%2524G%2524%2520%2528e.g.%2520rotations%252C%250Atranslations%252C%2520scalings%2529%252C%2520we%2520want%2520methods%2520to%2520be%2520%2524G%2524-invariant.%2520The%250A%2524G%2524-Bispectrum%2520extracts%2520every%2520characteristic%2520of%2520a%2520given%2520signal%2520up%2520to%2520group%250Aaction%253A%2520for%2520example%252C%2520the%2520shape%2520of%2520an%2520object%2520in%2520an%2520image%252C%2520but%2520not%2520its%250Aorientation.%2520Consequently%252C%2520the%2520%2524G%2524-Bispectrum%2520has%2520been%2520incorporated%2520into%2520deep%250Aneural%2520network%2520architectures%2520as%2520a%2520computational%2520primitive%2520for%250A%2524G%2524-invariance%255Ctextemdash%2520akin%2520to%2520a%2520pooling%2520mechanism%252C%2520but%2520with%2520greater%250Aselectivity%2520and%2520robustness.%2520However%252C%2520the%2520computational%2520cost%2520of%2520the%250A%2524G%2524-Bispectrum%2520%2528%2524%255Cmathcal%257BO%257D%2528%257CG%257C%255E2%2529%2524%252C%2520with%2520%2524%257CG%257C%2524%2520the%2520size%2520of%2520the%2520group%2529%2520has%250Alimited%2520its%2520widespread%2520adoption.%2520Here%252C%2520we%2520show%2520that%2520the%2520%2524G%2524-Bispectrum%250Acomputation%2520contains%2520redundancies%2520that%2520can%2520be%2520reduced%2520into%2520a%2520%255Ctextit%257Bselective%250A%2524G%2524-Bispectrum%257D%2520with%2520%2524%255Cmathcal%257BO%257D%2528%257CG%257C%2529%2524%2520complexity.%2520We%2520prove%2520desirable%250Amathematical%2520properties%2520of%2520the%2520selective%2520%2524G%2524-Bispectrum%2520and%2520demonstrate%2520how%2520its%250Aintegration%2520in%2520neural%2520networks%2520enhances%2520accuracy%2520and%2520robustness%2520compared%2520to%250Atraditional%2520approaches%252C%2520while%2520enjoying%2520considerable%2520speeds-up%2520compared%2520to%2520the%250Afull%2520%2524G%2524-Bispectrum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Selective%20G-Bispectrum%20and%20its%20Inversion%3A%20Applications%20to%0A%20%20G-Invariant%20Networks&entry.906535625=Simon%20Mataigne%20and%20Johan%20Mathe%20and%20Sophia%20Sanborn%20and%20Christopher%20Hillar%20and%20Nina%20Miolane&entry.1292438233=%20%20An%20important%20problem%20in%20signal%20processing%20and%20deep%20learning%20is%20to%20achieve%0A%5Ctextit%7Binvariance%7D%20to%20nuisance%20factors%20not%20relevant%20for%20the%20task.%20Since%20many%0Aof%20these%20factors%20are%20describable%20as%20the%20action%20of%20a%20group%20%24G%24%20%28e.g.%20rotations%2C%0Atranslations%2C%20scalings%29%2C%20we%20want%20methods%20to%20be%20%24G%24-invariant.%20The%0A%24G%24-Bispectrum%20extracts%20every%20characteristic%20of%20a%20given%20signal%20up%20to%20group%0Aaction%3A%20for%20example%2C%20the%20shape%20of%20an%20object%20in%20an%20image%2C%20but%20not%20its%0Aorientation.%20Consequently%2C%20the%20%24G%24-Bispectrum%20has%20been%20incorporated%20into%20deep%0Aneural%20network%20architectures%20as%20a%20computational%20primitive%20for%0A%24G%24-invariance%5Ctextemdash%20akin%20to%20a%20pooling%20mechanism%2C%20but%20with%20greater%0Aselectivity%20and%20robustness.%20However%2C%20the%20computational%20cost%20of%20the%0A%24G%24-Bispectrum%20%28%24%5Cmathcal%7BO%7D%28%7CG%7C%5E2%29%24%2C%20with%20%24%7CG%7C%24%20the%20size%20of%20the%20group%29%20has%0Alimited%20its%20widespread%20adoption.%20Here%2C%20we%20show%20that%20the%20%24G%24-Bispectrum%0Acomputation%20contains%20redundancies%20that%20can%20be%20reduced%20into%20a%20%5Ctextit%7Bselective%0A%24G%24-Bispectrum%7D%20with%20%24%5Cmathcal%7BO%7D%28%7CG%7C%29%24%20complexity.%20We%20prove%20desirable%0Amathematical%20properties%20of%20the%20selective%20%24G%24-Bispectrum%20and%20demonstrate%20how%20its%0Aintegration%20in%20neural%20networks%20enhances%20accuracy%20and%20robustness%20compared%20to%0Atraditional%20approaches%2C%20while%20enjoying%20considerable%20speeds-up%20compared%20to%20the%0Afull%20%24G%24-Bispectrum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07655v1&entry.124074799=Read"},
{"title": "A noise-robust acoustic method for recognizing foraging activities of\n  grazing cattle", "author": "Luciano S. Martinez-Rau and Jos\u00e9 O. Chelotti and Mariano Ferrero and Julio R. Galli and Santiago A. Utsumi and Alejandra M. Planisich and H. Leonardo Rufiner and Leonardo L. Giovanini", "abstract": "  Farmers must continuously improve their livestock production systems to\nremain competitive in the growing dairy market. Precision livestock farming\ntechnologies provide individualized monitoring of animals on commercial farms,\noptimizing livestock production. Continuous acoustic monitoring is a widely\naccepted sensing technique used to estimate the daily rumination and grazing\ntime budget of free-ranging cattle. However, typical environmental and natural\nnoises on pastures noticeably affect the performance limiting the practical\napplication of current acoustic methods. In this study, we present the\noperating principle and generalization capability of an acoustic method called\nNoise-Robust Foraging Activity Recognizer (NRFAR). The proposed method\ndetermines foraging activity bouts by analyzing fixed-length segments of\nidentified jaw movement events produced during grazing and rumination. The\nadditive noise robustness of the NRFAR was evaluated for several\nsignal-to-noise ratios using stationary Gaussian white noise and four different\nnonstationary natural noise sources. In noiseless conditions, NRFAR reached an\naverage balanced accuracy of 86.4%, outperforming two previous acoustic methods\nby more than 7.5%. Furthermore, NRFAR performed better than previous acoustic\nmethods in 77 of 80 evaluated noisy scenarios (53 cases with p<0.05). NRFAR has\nbeen shown to be effective in harsh free-ranging environments and could be used\nas a reliable solution to improve pasture management and monitor the health and\nwelfare of dairy cows. The instrumentation and computational algorithms\npresented in this publication are protected by a pending patent application: AR\nP20220100910. Web demo available at: https://sinc.unl.edu.ar/web-demo/nrfar\n", "link": "http://arxiv.org/abs/2304.14824v3", "date": "2024-07-10", "relevancy": 2.2022, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4474}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4407}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20noise-robust%20acoustic%20method%20for%20recognizing%20foraging%20activities%20of%0A%20%20grazing%20cattle&body=Title%3A%20A%20noise-robust%20acoustic%20method%20for%20recognizing%20foraging%20activities%20of%0A%20%20grazing%20cattle%0AAuthor%3A%20Luciano%20S.%20Martinez-Rau%20and%20Jos%C3%A9%20O.%20Chelotti%20and%20Mariano%20Ferrero%20and%20Julio%20R.%20Galli%20and%20Santiago%20A.%20Utsumi%20and%20Alejandra%20M.%20Planisich%20and%20H.%20Leonardo%20Rufiner%20and%20Leonardo%20L.%20Giovanini%0AAbstract%3A%20%20%20Farmers%20must%20continuously%20improve%20their%20livestock%20production%20systems%20to%0Aremain%20competitive%20in%20the%20growing%20dairy%20market.%20Precision%20livestock%20farming%0Atechnologies%20provide%20individualized%20monitoring%20of%20animals%20on%20commercial%20farms%2C%0Aoptimizing%20livestock%20production.%20Continuous%20acoustic%20monitoring%20is%20a%20widely%0Aaccepted%20sensing%20technique%20used%20to%20estimate%20the%20daily%20rumination%20and%20grazing%0Atime%20budget%20of%20free-ranging%20cattle.%20However%2C%20typical%20environmental%20and%20natural%0Anoises%20on%20pastures%20noticeably%20affect%20the%20performance%20limiting%20the%20practical%0Aapplication%20of%20current%20acoustic%20methods.%20In%20this%20study%2C%20we%20present%20the%0Aoperating%20principle%20and%20generalization%20capability%20of%20an%20acoustic%20method%20called%0ANoise-Robust%20Foraging%20Activity%20Recognizer%20%28NRFAR%29.%20The%20proposed%20method%0Adetermines%20foraging%20activity%20bouts%20by%20analyzing%20fixed-length%20segments%20of%0Aidentified%20jaw%20movement%20events%20produced%20during%20grazing%20and%20rumination.%20The%0Aadditive%20noise%20robustness%20of%20the%20NRFAR%20was%20evaluated%20for%20several%0Asignal-to-noise%20ratios%20using%20stationary%20Gaussian%20white%20noise%20and%20four%20different%0Anonstationary%20natural%20noise%20sources.%20In%20noiseless%20conditions%2C%20NRFAR%20reached%20an%0Aaverage%20balanced%20accuracy%20of%2086.4%25%2C%20outperforming%20two%20previous%20acoustic%20methods%0Aby%20more%20than%207.5%25.%20Furthermore%2C%20NRFAR%20performed%20better%20than%20previous%20acoustic%0Amethods%20in%2077%20of%2080%20evaluated%20noisy%20scenarios%20%2853%20cases%20with%20p%3C0.05%29.%20NRFAR%20has%0Abeen%20shown%20to%20be%20effective%20in%20harsh%20free-ranging%20environments%20and%20could%20be%20used%0Aas%20a%20reliable%20solution%20to%20improve%20pasture%20management%20and%20monitor%20the%20health%20and%0Awelfare%20of%20dairy%20cows.%20The%20instrumentation%20and%20computational%20algorithms%0Apresented%20in%20this%20publication%20are%20protected%20by%20a%20pending%20patent%20application%3A%20AR%0AP20220100910.%20Web%20demo%20available%20at%3A%20https%3A//sinc.unl.edu.ar/web-demo/nrfar%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.14824v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520noise-robust%2520acoustic%2520method%2520for%2520recognizing%2520foraging%2520activities%2520of%250A%2520%2520grazing%2520cattle%26entry.906535625%3DLuciano%2520S.%2520Martinez-Rau%2520and%2520Jos%25C3%25A9%2520O.%2520Chelotti%2520and%2520Mariano%2520Ferrero%2520and%2520Julio%2520R.%2520Galli%2520and%2520Santiago%2520A.%2520Utsumi%2520and%2520Alejandra%2520M.%2520Planisich%2520and%2520H.%2520Leonardo%2520Rufiner%2520and%2520Leonardo%2520L.%2520Giovanini%26entry.1292438233%3D%2520%2520Farmers%2520must%2520continuously%2520improve%2520their%2520livestock%2520production%2520systems%2520to%250Aremain%2520competitive%2520in%2520the%2520growing%2520dairy%2520market.%2520Precision%2520livestock%2520farming%250Atechnologies%2520provide%2520individualized%2520monitoring%2520of%2520animals%2520on%2520commercial%2520farms%252C%250Aoptimizing%2520livestock%2520production.%2520Continuous%2520acoustic%2520monitoring%2520is%2520a%2520widely%250Aaccepted%2520sensing%2520technique%2520used%2520to%2520estimate%2520the%2520daily%2520rumination%2520and%2520grazing%250Atime%2520budget%2520of%2520free-ranging%2520cattle.%2520However%252C%2520typical%2520environmental%2520and%2520natural%250Anoises%2520on%2520pastures%2520noticeably%2520affect%2520the%2520performance%2520limiting%2520the%2520practical%250Aapplication%2520of%2520current%2520acoustic%2520methods.%2520In%2520this%2520study%252C%2520we%2520present%2520the%250Aoperating%2520principle%2520and%2520generalization%2520capability%2520of%2520an%2520acoustic%2520method%2520called%250ANoise-Robust%2520Foraging%2520Activity%2520Recognizer%2520%2528NRFAR%2529.%2520The%2520proposed%2520method%250Adetermines%2520foraging%2520activity%2520bouts%2520by%2520analyzing%2520fixed-length%2520segments%2520of%250Aidentified%2520jaw%2520movement%2520events%2520produced%2520during%2520grazing%2520and%2520rumination.%2520The%250Aadditive%2520noise%2520robustness%2520of%2520the%2520NRFAR%2520was%2520evaluated%2520for%2520several%250Asignal-to-noise%2520ratios%2520using%2520stationary%2520Gaussian%2520white%2520noise%2520and%2520four%2520different%250Anonstationary%2520natural%2520noise%2520sources.%2520In%2520noiseless%2520conditions%252C%2520NRFAR%2520reached%2520an%250Aaverage%2520balanced%2520accuracy%2520of%252086.4%2525%252C%2520outperforming%2520two%2520previous%2520acoustic%2520methods%250Aby%2520more%2520than%25207.5%2525.%2520Furthermore%252C%2520NRFAR%2520performed%2520better%2520than%2520previous%2520acoustic%250Amethods%2520in%252077%2520of%252080%2520evaluated%2520noisy%2520scenarios%2520%252853%2520cases%2520with%2520p%253C0.05%2529.%2520NRFAR%2520has%250Abeen%2520shown%2520to%2520be%2520effective%2520in%2520harsh%2520free-ranging%2520environments%2520and%2520could%2520be%2520used%250Aas%2520a%2520reliable%2520solution%2520to%2520improve%2520pasture%2520management%2520and%2520monitor%2520the%2520health%2520and%250Awelfare%2520of%2520dairy%2520cows.%2520The%2520instrumentation%2520and%2520computational%2520algorithms%250Apresented%2520in%2520this%2520publication%2520are%2520protected%2520by%2520a%2520pending%2520patent%2520application%253A%2520AR%250AP20220100910.%2520Web%2520demo%2520available%2520at%253A%2520https%253A//sinc.unl.edu.ar/web-demo/nrfar%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.14824v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20noise-robust%20acoustic%20method%20for%20recognizing%20foraging%20activities%20of%0A%20%20grazing%20cattle&entry.906535625=Luciano%20S.%20Martinez-Rau%20and%20Jos%C3%A9%20O.%20Chelotti%20and%20Mariano%20Ferrero%20and%20Julio%20R.%20Galli%20and%20Santiago%20A.%20Utsumi%20and%20Alejandra%20M.%20Planisich%20and%20H.%20Leonardo%20Rufiner%20and%20Leonardo%20L.%20Giovanini&entry.1292438233=%20%20Farmers%20must%20continuously%20improve%20their%20livestock%20production%20systems%20to%0Aremain%20competitive%20in%20the%20growing%20dairy%20market.%20Precision%20livestock%20farming%0Atechnologies%20provide%20individualized%20monitoring%20of%20animals%20on%20commercial%20farms%2C%0Aoptimizing%20livestock%20production.%20Continuous%20acoustic%20monitoring%20is%20a%20widely%0Aaccepted%20sensing%20technique%20used%20to%20estimate%20the%20daily%20rumination%20and%20grazing%0Atime%20budget%20of%20free-ranging%20cattle.%20However%2C%20typical%20environmental%20and%20natural%0Anoises%20on%20pastures%20noticeably%20affect%20the%20performance%20limiting%20the%20practical%0Aapplication%20of%20current%20acoustic%20methods.%20In%20this%20study%2C%20we%20present%20the%0Aoperating%20principle%20and%20generalization%20capability%20of%20an%20acoustic%20method%20called%0ANoise-Robust%20Foraging%20Activity%20Recognizer%20%28NRFAR%29.%20The%20proposed%20method%0Adetermines%20foraging%20activity%20bouts%20by%20analyzing%20fixed-length%20segments%20of%0Aidentified%20jaw%20movement%20events%20produced%20during%20grazing%20and%20rumination.%20The%0Aadditive%20noise%20robustness%20of%20the%20NRFAR%20was%20evaluated%20for%20several%0Asignal-to-noise%20ratios%20using%20stationary%20Gaussian%20white%20noise%20and%20four%20different%0Anonstationary%20natural%20noise%20sources.%20In%20noiseless%20conditions%2C%20NRFAR%20reached%20an%0Aaverage%20balanced%20accuracy%20of%2086.4%25%2C%20outperforming%20two%20previous%20acoustic%20methods%0Aby%20more%20than%207.5%25.%20Furthermore%2C%20NRFAR%20performed%20better%20than%20previous%20acoustic%0Amethods%20in%2077%20of%2080%20evaluated%20noisy%20scenarios%20%2853%20cases%20with%20p%3C0.05%29.%20NRFAR%20has%0Abeen%20shown%20to%20be%20effective%20in%20harsh%20free-ranging%20environments%20and%20could%20be%20used%0Aas%20a%20reliable%20solution%20to%20improve%20pasture%20management%20and%20monitor%20the%20health%20and%0Awelfare%20of%20dairy%20cows.%20The%20instrumentation%20and%20computational%20algorithms%0Apresented%20in%20this%20publication%20are%20protected%20by%20a%20pending%20patent%20application%3A%20AR%0AP20220100910.%20Web%20demo%20available%20at%3A%20https%3A//sinc.unl.edu.ar/web-demo/nrfar%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.14824v3&entry.124074799=Read"},
{"title": "iiANET: Inception Inspired Attention Hybrid Network for efficient\n  Long-Range Dependency", "author": "Haruna Yunusa and Qin Shiyin and Abdulrahman Hamman Adama Chukkol and Isah Bello and Adamu Lawan", "abstract": "  The recent emergence of hybrid models has introduced another transformative\napproach to solving computer vision tasks, slowly shifting away from\nconventional CNN (Convolutional Neural Network) and ViT (Vision Transformer).\nHowever, not enough effort has been made to efficiently combine these two\napproaches to improve capturing long-range dependencies prevalent in complex\nimages. In this paper, we introduce iiANET (Inception Inspired Attention\nNetwork), an efficient hybrid model designed to capture long-range dependencies\nin complex images. The fundamental building block, iiABlock, integrates global\n2D-MHSA (Multi-Head Self-Attention) with Registers, MBConv2 (MobileNetV2-based\nconvolution), and dilated convolution in parallel, enabling the model to\nadeptly leverage self-attention for capturing long-range dependencies while\nutilizing MBConv2 for effective local-detail extraction and dilated convolution\nfor efficiently expanding the kernel receptive field to capture more contextual\ninformation. Lastly, we serially integrate an ECANET (Efficient Channel\nAttention Network) at the end of each iiABlock to calibrate channel-wise\nattention for enhanced model performance. Extensive qualitative and\nquantitative comparative evaluation on various benchmarks demonstrates improved\nperformance over some state-of-the-art models.\n", "link": "http://arxiv.org/abs/2407.07603v1", "date": "2024-07-10", "relevancy": 2.195, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.56}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5447}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iiANET%3A%20Inception%20Inspired%20Attention%20Hybrid%20Network%20for%20efficient%0A%20%20Long-Range%20Dependency&body=Title%3A%20iiANET%3A%20Inception%20Inspired%20Attention%20Hybrid%20Network%20for%20efficient%0A%20%20Long-Range%20Dependency%0AAuthor%3A%20Haruna%20Yunusa%20and%20Qin%20Shiyin%20and%20Abdulrahman%20Hamman%20Adama%20Chukkol%20and%20Isah%20Bello%20and%20Adamu%20Lawan%0AAbstract%3A%20%20%20The%20recent%20emergence%20of%20hybrid%20models%20has%20introduced%20another%20transformative%0Aapproach%20to%20solving%20computer%20vision%20tasks%2C%20slowly%20shifting%20away%20from%0Aconventional%20CNN%20%28Convolutional%20Neural%20Network%29%20and%20ViT%20%28Vision%20Transformer%29.%0AHowever%2C%20not%20enough%20effort%20has%20been%20made%20to%20efficiently%20combine%20these%20two%0Aapproaches%20to%20improve%20capturing%20long-range%20dependencies%20prevalent%20in%20complex%0Aimages.%20In%20this%20paper%2C%20we%20introduce%20iiANET%20%28Inception%20Inspired%20Attention%0ANetwork%29%2C%20an%20efficient%20hybrid%20model%20designed%20to%20capture%20long-range%20dependencies%0Ain%20complex%20images.%20The%20fundamental%20building%20block%2C%20iiABlock%2C%20integrates%20global%0A2D-MHSA%20%28Multi-Head%20Self-Attention%29%20with%20Registers%2C%20MBConv2%20%28MobileNetV2-based%0Aconvolution%29%2C%20and%20dilated%20convolution%20in%20parallel%2C%20enabling%20the%20model%20to%0Aadeptly%20leverage%20self-attention%20for%20capturing%20long-range%20dependencies%20while%0Autilizing%20MBConv2%20for%20effective%20local-detail%20extraction%20and%20dilated%20convolution%0Afor%20efficiently%20expanding%20the%20kernel%20receptive%20field%20to%20capture%20more%20contextual%0Ainformation.%20Lastly%2C%20we%20serially%20integrate%20an%20ECANET%20%28Efficient%20Channel%0AAttention%20Network%29%20at%20the%20end%20of%20each%20iiABlock%20to%20calibrate%20channel-wise%0Aattention%20for%20enhanced%20model%20performance.%20Extensive%20qualitative%20and%0Aquantitative%20comparative%20evaluation%20on%20various%20benchmarks%20demonstrates%20improved%0Aperformance%20over%20some%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiiANET%253A%2520Inception%2520Inspired%2520Attention%2520Hybrid%2520Network%2520for%2520efficient%250A%2520%2520Long-Range%2520Dependency%26entry.906535625%3DHaruna%2520Yunusa%2520and%2520Qin%2520Shiyin%2520and%2520Abdulrahman%2520Hamman%2520Adama%2520Chukkol%2520and%2520Isah%2520Bello%2520and%2520Adamu%2520Lawan%26entry.1292438233%3D%2520%2520The%2520recent%2520emergence%2520of%2520hybrid%2520models%2520has%2520introduced%2520another%2520transformative%250Aapproach%2520to%2520solving%2520computer%2520vision%2520tasks%252C%2520slowly%2520shifting%2520away%2520from%250Aconventional%2520CNN%2520%2528Convolutional%2520Neural%2520Network%2529%2520and%2520ViT%2520%2528Vision%2520Transformer%2529.%250AHowever%252C%2520not%2520enough%2520effort%2520has%2520been%2520made%2520to%2520efficiently%2520combine%2520these%2520two%250Aapproaches%2520to%2520improve%2520capturing%2520long-range%2520dependencies%2520prevalent%2520in%2520complex%250Aimages.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520iiANET%2520%2528Inception%2520Inspired%2520Attention%250ANetwork%2529%252C%2520an%2520efficient%2520hybrid%2520model%2520designed%2520to%2520capture%2520long-range%2520dependencies%250Ain%2520complex%2520images.%2520The%2520fundamental%2520building%2520block%252C%2520iiABlock%252C%2520integrates%2520global%250A2D-MHSA%2520%2528Multi-Head%2520Self-Attention%2529%2520with%2520Registers%252C%2520MBConv2%2520%2528MobileNetV2-based%250Aconvolution%2529%252C%2520and%2520dilated%2520convolution%2520in%2520parallel%252C%2520enabling%2520the%2520model%2520to%250Aadeptly%2520leverage%2520self-attention%2520for%2520capturing%2520long-range%2520dependencies%2520while%250Autilizing%2520MBConv2%2520for%2520effective%2520local-detail%2520extraction%2520and%2520dilated%2520convolution%250Afor%2520efficiently%2520expanding%2520the%2520kernel%2520receptive%2520field%2520to%2520capture%2520more%2520contextual%250Ainformation.%2520Lastly%252C%2520we%2520serially%2520integrate%2520an%2520ECANET%2520%2528Efficient%2520Channel%250AAttention%2520Network%2529%2520at%2520the%2520end%2520of%2520each%2520iiABlock%2520to%2520calibrate%2520channel-wise%250Aattention%2520for%2520enhanced%2520model%2520performance.%2520Extensive%2520qualitative%2520and%250Aquantitative%2520comparative%2520evaluation%2520on%2520various%2520benchmarks%2520demonstrates%2520improved%250Aperformance%2520over%2520some%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iiANET%3A%20Inception%20Inspired%20Attention%20Hybrid%20Network%20for%20efficient%0A%20%20Long-Range%20Dependency&entry.906535625=Haruna%20Yunusa%20and%20Qin%20Shiyin%20and%20Abdulrahman%20Hamman%20Adama%20Chukkol%20and%20Isah%20Bello%20and%20Adamu%20Lawan&entry.1292438233=%20%20The%20recent%20emergence%20of%20hybrid%20models%20has%20introduced%20another%20transformative%0Aapproach%20to%20solving%20computer%20vision%20tasks%2C%20slowly%20shifting%20away%20from%0Aconventional%20CNN%20%28Convolutional%20Neural%20Network%29%20and%20ViT%20%28Vision%20Transformer%29.%0AHowever%2C%20not%20enough%20effort%20has%20been%20made%20to%20efficiently%20combine%20these%20two%0Aapproaches%20to%20improve%20capturing%20long-range%20dependencies%20prevalent%20in%20complex%0Aimages.%20In%20this%20paper%2C%20we%20introduce%20iiANET%20%28Inception%20Inspired%20Attention%0ANetwork%29%2C%20an%20efficient%20hybrid%20model%20designed%20to%20capture%20long-range%20dependencies%0Ain%20complex%20images.%20The%20fundamental%20building%20block%2C%20iiABlock%2C%20integrates%20global%0A2D-MHSA%20%28Multi-Head%20Self-Attention%29%20with%20Registers%2C%20MBConv2%20%28MobileNetV2-based%0Aconvolution%29%2C%20and%20dilated%20convolution%20in%20parallel%2C%20enabling%20the%20model%20to%0Aadeptly%20leverage%20self-attention%20for%20capturing%20long-range%20dependencies%20while%0Autilizing%20MBConv2%20for%20effective%20local-detail%20extraction%20and%20dilated%20convolution%0Afor%20efficiently%20expanding%20the%20kernel%20receptive%20field%20to%20capture%20more%20contextual%0Ainformation.%20Lastly%2C%20we%20serially%20integrate%20an%20ECANET%20%28Efficient%20Channel%0AAttention%20Network%29%20at%20the%20end%20of%20each%20iiABlock%20to%20calibrate%20channel-wise%0Aattention%20for%20enhanced%20model%20performance.%20Extensive%20qualitative%20and%0Aquantitative%20comparative%20evaluation%20on%20various%20benchmarks%20demonstrates%20improved%0Aperformance%20over%20some%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07603v1&entry.124074799=Read"},
{"title": "Learning In-Hand Translation Using Tactile Skin With Shear and Normal\n  Force Sensing", "author": "Jessica Yin and Haozhi Qi and Jitendra Malik and James Pikul and Mark Yim and Tess Hellebrekers", "abstract": "  Recent progress in reinforcement learning (RL) and tactile sensing has\nsignificantly advanced dexterous manipulation. However, these methods often\nutilize simplified tactile signals due to the gap between tactile simulation\nand the real world. We introduce a sensor model for tactile skin that enables\nzero-shot sim-to-real transfer of ternary shear and binary normal forces. Using\nthis model, we develop an RL policy that leverages sliding contact for\ndexterous in-hand translation. We conduct extensive real-world experiments to\nassess how tactile sensing facilitates policy adaptation to various unseen\nobject properties and robot hand orientations. We demonstrate that our 3-axis\ntactile policies consistently outperform baselines that use only shear forces,\nonly normal forces, or only proprioception. Website:\nhttps://jessicayin.github.io/tactile-skin-rl/\n", "link": "http://arxiv.org/abs/2407.07885v1", "date": "2024-07-10", "relevancy": 2.1883, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6108}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5446}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20In-Hand%20Translation%20Using%20Tactile%20Skin%20With%20Shear%20and%20Normal%0A%20%20Force%20Sensing&body=Title%3A%20Learning%20In-Hand%20Translation%20Using%20Tactile%20Skin%20With%20Shear%20and%20Normal%0A%20%20Force%20Sensing%0AAuthor%3A%20Jessica%20Yin%20and%20Haozhi%20Qi%20and%20Jitendra%20Malik%20and%20James%20Pikul%20and%20Mark%20Yim%20and%20Tess%20Hellebrekers%0AAbstract%3A%20%20%20Recent%20progress%20in%20reinforcement%20learning%20%28RL%29%20and%20tactile%20sensing%20has%0Asignificantly%20advanced%20dexterous%20manipulation.%20However%2C%20these%20methods%20often%0Autilize%20simplified%20tactile%20signals%20due%20to%20the%20gap%20between%20tactile%20simulation%0Aand%20the%20real%20world.%20We%20introduce%20a%20sensor%20model%20for%20tactile%20skin%20that%20enables%0Azero-shot%20sim-to-real%20transfer%20of%20ternary%20shear%20and%20binary%20normal%20forces.%20Using%0Athis%20model%2C%20we%20develop%20an%20RL%20policy%20that%20leverages%20sliding%20contact%20for%0Adexterous%20in-hand%20translation.%20We%20conduct%20extensive%20real-world%20experiments%20to%0Aassess%20how%20tactile%20sensing%20facilitates%20policy%20adaptation%20to%20various%20unseen%0Aobject%20properties%20and%20robot%20hand%20orientations.%20We%20demonstrate%20that%20our%203-axis%0Atactile%20policies%20consistently%20outperform%20baselines%20that%20use%20only%20shear%20forces%2C%0Aonly%20normal%20forces%2C%20or%20only%20proprioception.%20Website%3A%0Ahttps%3A//jessicayin.github.io/tactile-skin-rl/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520In-Hand%2520Translation%2520Using%2520Tactile%2520Skin%2520With%2520Shear%2520and%2520Normal%250A%2520%2520Force%2520Sensing%26entry.906535625%3DJessica%2520Yin%2520and%2520Haozhi%2520Qi%2520and%2520Jitendra%2520Malik%2520and%2520James%2520Pikul%2520and%2520Mark%2520Yim%2520and%2520Tess%2520Hellebrekers%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520tactile%2520sensing%2520has%250Asignificantly%2520advanced%2520dexterous%2520manipulation.%2520However%252C%2520these%2520methods%2520often%250Autilize%2520simplified%2520tactile%2520signals%2520due%2520to%2520the%2520gap%2520between%2520tactile%2520simulation%250Aand%2520the%2520real%2520world.%2520We%2520introduce%2520a%2520sensor%2520model%2520for%2520tactile%2520skin%2520that%2520enables%250Azero-shot%2520sim-to-real%2520transfer%2520of%2520ternary%2520shear%2520and%2520binary%2520normal%2520forces.%2520Using%250Athis%2520model%252C%2520we%2520develop%2520an%2520RL%2520policy%2520that%2520leverages%2520sliding%2520contact%2520for%250Adexterous%2520in-hand%2520translation.%2520We%2520conduct%2520extensive%2520real-world%2520experiments%2520to%250Aassess%2520how%2520tactile%2520sensing%2520facilitates%2520policy%2520adaptation%2520to%2520various%2520unseen%250Aobject%2520properties%2520and%2520robot%2520hand%2520orientations.%2520We%2520demonstrate%2520that%2520our%25203-axis%250Atactile%2520policies%2520consistently%2520outperform%2520baselines%2520that%2520use%2520only%2520shear%2520forces%252C%250Aonly%2520normal%2520forces%252C%2520or%2520only%2520proprioception.%2520Website%253A%250Ahttps%253A//jessicayin.github.io/tactile-skin-rl/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20In-Hand%20Translation%20Using%20Tactile%20Skin%20With%20Shear%20and%20Normal%0A%20%20Force%20Sensing&entry.906535625=Jessica%20Yin%20and%20Haozhi%20Qi%20and%20Jitendra%20Malik%20and%20James%20Pikul%20and%20Mark%20Yim%20and%20Tess%20Hellebrekers&entry.1292438233=%20%20Recent%20progress%20in%20reinforcement%20learning%20%28RL%29%20and%20tactile%20sensing%20has%0Asignificantly%20advanced%20dexterous%20manipulation.%20However%2C%20these%20methods%20often%0Autilize%20simplified%20tactile%20signals%20due%20to%20the%20gap%20between%20tactile%20simulation%0Aand%20the%20real%20world.%20We%20introduce%20a%20sensor%20model%20for%20tactile%20skin%20that%20enables%0Azero-shot%20sim-to-real%20transfer%20of%20ternary%20shear%20and%20binary%20normal%20forces.%20Using%0Athis%20model%2C%20we%20develop%20an%20RL%20policy%20that%20leverages%20sliding%20contact%20for%0Adexterous%20in-hand%20translation.%20We%20conduct%20extensive%20real-world%20experiments%20to%0Aassess%20how%20tactile%20sensing%20facilitates%20policy%20adaptation%20to%20various%20unseen%0Aobject%20properties%20and%20robot%20hand%20orientations.%20We%20demonstrate%20that%20our%203-axis%0Atactile%20policies%20consistently%20outperform%20baselines%20that%20use%20only%20shear%20forces%2C%0Aonly%20normal%20forces%2C%20or%20only%20proprioception.%20Website%3A%0Ahttps%3A//jessicayin.github.io/tactile-skin-rl/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07885v1&entry.124074799=Read"},
{"title": "Federated Foundation Model for Cardiac CT Imaging", "author": "Malte T\u00f6lle and Philipp Garthe and Clemens Scherer and Jan Moritz Seliger and Andreas Leha and Nina Kr\u00fcger and Stefan Simm and Simon Martin and Sebastian Eble and Halvar Kelm and Moritz Bednorz and Florian Andr\u00e9 and Peter Bannas and Gerhard Diller and Norbert Frey and Stefan Gro\u00df and Anja Hennemuth and Lars Kaderali and Alexander Meyer and Eike Nagel and Stefan Orwat and Moritz Seiffert and Tim Friede and Tim Seidler and Sandy Engelhardt", "abstract": "  Federated learning (FL) is a renowned technique for utilizing decentralized\ndata while preserving privacy. However, real-world applications often involve\ninherent challenges such as partially labeled datasets, where not all clients\npossess expert annotations of all labels of interest, leaving large portions of\nunlabeled data unused. In this study, we conduct the largest federated cardiac\nCT imaging analysis to date, focusing on partially labeled datasets ($n=8,124$)\nof Transcatheter Aortic Valve Implantation (TAVI) patients over eight hospital\nclients. Transformer architectures, which are the major building blocks of\ncurrent foundation models, have shown superior performance when trained on\nlarger cohorts than traditional CNNs. However, when trained on small\ntask-specific labeled sample sizes, it is currently not feasible to exploit\ntheir underlying attention mechanism for improved performance. Therefore, we\ndeveloped a two-stage semi-supervised learning strategy that distills knowledge\nfrom several task-specific CNNs (landmark detection and segmentation of\ncalcification) into a single transformer model by utilizing large amounts of\nunlabeled data typically residing unused in hospitals to mitigate these issues.\nThis method not only improves the predictive accuracy and generalizability of\ntransformer-based architectures but also facilitates the simultaneous learning\nof all partial labels within a single transformer model across the federation.\nAdditionally, we show that our transformer-based model extracts more meaningful\nfeatures for further downstream tasks than the UNet-based one by only training\nthe last layer to also solve segmentation of coronary arteries. We make the\ncode and weights of the final model openly available, which can serve as a\nfoundation model for further research in cardiac CT imaging.\n", "link": "http://arxiv.org/abs/2407.07557v1", "date": "2024-07-10", "relevancy": 2.1855, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5781}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5607}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Foundation%20Model%20for%20Cardiac%20CT%20Imaging&body=Title%3A%20Federated%20Foundation%20Model%20for%20Cardiac%20CT%20Imaging%0AAuthor%3A%20Malte%20T%C3%B6lle%20and%20Philipp%20Garthe%20and%20Clemens%20Scherer%20and%20Jan%20Moritz%20Seliger%20and%20Andreas%20Leha%20and%20Nina%20Kr%C3%BCger%20and%20Stefan%20Simm%20and%20Simon%20Martin%20and%20Sebastian%20Eble%20and%20Halvar%20Kelm%20and%20Moritz%20Bednorz%20and%20Florian%20Andr%C3%A9%20and%20Peter%20Bannas%20and%20Gerhard%20Diller%20and%20Norbert%20Frey%20and%20Stefan%20Gro%C3%9F%20and%20Anja%20Hennemuth%20and%20Lars%20Kaderali%20and%20Alexander%20Meyer%20and%20Eike%20Nagel%20and%20Stefan%20Orwat%20and%20Moritz%20Seiffert%20and%20Tim%20Friede%20and%20Tim%20Seidler%20and%20Sandy%20Engelhardt%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20renowned%20technique%20for%20utilizing%20decentralized%0Adata%20while%20preserving%20privacy.%20However%2C%20real-world%20applications%20often%20involve%0Ainherent%20challenges%20such%20as%20partially%20labeled%20datasets%2C%20where%20not%20all%20clients%0Apossess%20expert%20annotations%20of%20all%20labels%20of%20interest%2C%20leaving%20large%20portions%20of%0Aunlabeled%20data%20unused.%20In%20this%20study%2C%20we%20conduct%20the%20largest%20federated%20cardiac%0ACT%20imaging%20analysis%20to%20date%2C%20focusing%20on%20partially%20labeled%20datasets%20%28%24n%3D8%2C124%24%29%0Aof%20Transcatheter%20Aortic%20Valve%20Implantation%20%28TAVI%29%20patients%20over%20eight%20hospital%0Aclients.%20Transformer%20architectures%2C%20which%20are%20the%20major%20building%20blocks%20of%0Acurrent%20foundation%20models%2C%20have%20shown%20superior%20performance%20when%20trained%20on%0Alarger%20cohorts%20than%20traditional%20CNNs.%20However%2C%20when%20trained%20on%20small%0Atask-specific%20labeled%20sample%20sizes%2C%20it%20is%20currently%20not%20feasible%20to%20exploit%0Atheir%20underlying%20attention%20mechanism%20for%20improved%20performance.%20Therefore%2C%20we%0Adeveloped%20a%20two-stage%20semi-supervised%20learning%20strategy%20that%20distills%20knowledge%0Afrom%20several%20task-specific%20CNNs%20%28landmark%20detection%20and%20segmentation%20of%0Acalcification%29%20into%20a%20single%20transformer%20model%20by%20utilizing%20large%20amounts%20of%0Aunlabeled%20data%20typically%20residing%20unused%20in%20hospitals%20to%20mitigate%20these%20issues.%0AThis%20method%20not%20only%20improves%20the%20predictive%20accuracy%20and%20generalizability%20of%0Atransformer-based%20architectures%20but%20also%20facilitates%20the%20simultaneous%20learning%0Aof%20all%20partial%20labels%20within%20a%20single%20transformer%20model%20across%20the%20federation.%0AAdditionally%2C%20we%20show%20that%20our%20transformer-based%20model%20extracts%20more%20meaningful%0Afeatures%20for%20further%20downstream%20tasks%20than%20the%20UNet-based%20one%20by%20only%20training%0Athe%20last%20layer%20to%20also%20solve%20segmentation%20of%20coronary%20arteries.%20We%20make%20the%0Acode%20and%20weights%20of%20the%20final%20model%20openly%20available%2C%20which%20can%20serve%20as%20a%0Afoundation%20model%20for%20further%20research%20in%20cardiac%20CT%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Foundation%2520Model%2520for%2520Cardiac%2520CT%2520Imaging%26entry.906535625%3DMalte%2520T%25C3%25B6lle%2520and%2520Philipp%2520Garthe%2520and%2520Clemens%2520Scherer%2520and%2520Jan%2520Moritz%2520Seliger%2520and%2520Andreas%2520Leha%2520and%2520Nina%2520Kr%25C3%25BCger%2520and%2520Stefan%2520Simm%2520and%2520Simon%2520Martin%2520and%2520Sebastian%2520Eble%2520and%2520Halvar%2520Kelm%2520and%2520Moritz%2520Bednorz%2520and%2520Florian%2520Andr%25C3%25A9%2520and%2520Peter%2520Bannas%2520and%2520Gerhard%2520Diller%2520and%2520Norbert%2520Frey%2520and%2520Stefan%2520Gro%25C3%259F%2520and%2520Anja%2520Hennemuth%2520and%2520Lars%2520Kaderali%2520and%2520Alexander%2520Meyer%2520and%2520Eike%2520Nagel%2520and%2520Stefan%2520Orwat%2520and%2520Moritz%2520Seiffert%2520and%2520Tim%2520Friede%2520and%2520Tim%2520Seidler%2520and%2520Sandy%2520Engelhardt%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520renowned%2520technique%2520for%2520utilizing%2520decentralized%250Adata%2520while%2520preserving%2520privacy.%2520However%252C%2520real-world%2520applications%2520often%2520involve%250Ainherent%2520challenges%2520such%2520as%2520partially%2520labeled%2520datasets%252C%2520where%2520not%2520all%2520clients%250Apossess%2520expert%2520annotations%2520of%2520all%2520labels%2520of%2520interest%252C%2520leaving%2520large%2520portions%2520of%250Aunlabeled%2520data%2520unused.%2520In%2520this%2520study%252C%2520we%2520conduct%2520the%2520largest%2520federated%2520cardiac%250ACT%2520imaging%2520analysis%2520to%2520date%252C%2520focusing%2520on%2520partially%2520labeled%2520datasets%2520%2528%2524n%253D8%252C124%2524%2529%250Aof%2520Transcatheter%2520Aortic%2520Valve%2520Implantation%2520%2528TAVI%2529%2520patients%2520over%2520eight%2520hospital%250Aclients.%2520Transformer%2520architectures%252C%2520which%2520are%2520the%2520major%2520building%2520blocks%2520of%250Acurrent%2520foundation%2520models%252C%2520have%2520shown%2520superior%2520performance%2520when%2520trained%2520on%250Alarger%2520cohorts%2520than%2520traditional%2520CNNs.%2520However%252C%2520when%2520trained%2520on%2520small%250Atask-specific%2520labeled%2520sample%2520sizes%252C%2520it%2520is%2520currently%2520not%2520feasible%2520to%2520exploit%250Atheir%2520underlying%2520attention%2520mechanism%2520for%2520improved%2520performance.%2520Therefore%252C%2520we%250Adeveloped%2520a%2520two-stage%2520semi-supervised%2520learning%2520strategy%2520that%2520distills%2520knowledge%250Afrom%2520several%2520task-specific%2520CNNs%2520%2528landmark%2520detection%2520and%2520segmentation%2520of%250Acalcification%2529%2520into%2520a%2520single%2520transformer%2520model%2520by%2520utilizing%2520large%2520amounts%2520of%250Aunlabeled%2520data%2520typically%2520residing%2520unused%2520in%2520hospitals%2520to%2520mitigate%2520these%2520issues.%250AThis%2520method%2520not%2520only%2520improves%2520the%2520predictive%2520accuracy%2520and%2520generalizability%2520of%250Atransformer-based%2520architectures%2520but%2520also%2520facilitates%2520the%2520simultaneous%2520learning%250Aof%2520all%2520partial%2520labels%2520within%2520a%2520single%2520transformer%2520model%2520across%2520the%2520federation.%250AAdditionally%252C%2520we%2520show%2520that%2520our%2520transformer-based%2520model%2520extracts%2520more%2520meaningful%250Afeatures%2520for%2520further%2520downstream%2520tasks%2520than%2520the%2520UNet-based%2520one%2520by%2520only%2520training%250Athe%2520last%2520layer%2520to%2520also%2520solve%2520segmentation%2520of%2520coronary%2520arteries.%2520We%2520make%2520the%250Acode%2520and%2520weights%2520of%2520the%2520final%2520model%2520openly%2520available%252C%2520which%2520can%2520serve%2520as%2520a%250Afoundation%2520model%2520for%2520further%2520research%2520in%2520cardiac%2520CT%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Foundation%20Model%20for%20Cardiac%20CT%20Imaging&entry.906535625=Malte%20T%C3%B6lle%20and%20Philipp%20Garthe%20and%20Clemens%20Scherer%20and%20Jan%20Moritz%20Seliger%20and%20Andreas%20Leha%20and%20Nina%20Kr%C3%BCger%20and%20Stefan%20Simm%20and%20Simon%20Martin%20and%20Sebastian%20Eble%20and%20Halvar%20Kelm%20and%20Moritz%20Bednorz%20and%20Florian%20Andr%C3%A9%20and%20Peter%20Bannas%20and%20Gerhard%20Diller%20and%20Norbert%20Frey%20and%20Stefan%20Gro%C3%9F%20and%20Anja%20Hennemuth%20and%20Lars%20Kaderali%20and%20Alexander%20Meyer%20and%20Eike%20Nagel%20and%20Stefan%20Orwat%20and%20Moritz%20Seiffert%20and%20Tim%20Friede%20and%20Tim%20Seidler%20and%20Sandy%20Engelhardt&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20renowned%20technique%20for%20utilizing%20decentralized%0Adata%20while%20preserving%20privacy.%20However%2C%20real-world%20applications%20often%20involve%0Ainherent%20challenges%20such%20as%20partially%20labeled%20datasets%2C%20where%20not%20all%20clients%0Apossess%20expert%20annotations%20of%20all%20labels%20of%20interest%2C%20leaving%20large%20portions%20of%0Aunlabeled%20data%20unused.%20In%20this%20study%2C%20we%20conduct%20the%20largest%20federated%20cardiac%0ACT%20imaging%20analysis%20to%20date%2C%20focusing%20on%20partially%20labeled%20datasets%20%28%24n%3D8%2C124%24%29%0Aof%20Transcatheter%20Aortic%20Valve%20Implantation%20%28TAVI%29%20patients%20over%20eight%20hospital%0Aclients.%20Transformer%20architectures%2C%20which%20are%20the%20major%20building%20blocks%20of%0Acurrent%20foundation%20models%2C%20have%20shown%20superior%20performance%20when%20trained%20on%0Alarger%20cohorts%20than%20traditional%20CNNs.%20However%2C%20when%20trained%20on%20small%0Atask-specific%20labeled%20sample%20sizes%2C%20it%20is%20currently%20not%20feasible%20to%20exploit%0Atheir%20underlying%20attention%20mechanism%20for%20improved%20performance.%20Therefore%2C%20we%0Adeveloped%20a%20two-stage%20semi-supervised%20learning%20strategy%20that%20distills%20knowledge%0Afrom%20several%20task-specific%20CNNs%20%28landmark%20detection%20and%20segmentation%20of%0Acalcification%29%20into%20a%20single%20transformer%20model%20by%20utilizing%20large%20amounts%20of%0Aunlabeled%20data%20typically%20residing%20unused%20in%20hospitals%20to%20mitigate%20these%20issues.%0AThis%20method%20not%20only%20improves%20the%20predictive%20accuracy%20and%20generalizability%20of%0Atransformer-based%20architectures%20but%20also%20facilitates%20the%20simultaneous%20learning%0Aof%20all%20partial%20labels%20within%20a%20single%20transformer%20model%20across%20the%20federation.%0AAdditionally%2C%20we%20show%20that%20our%20transformer-based%20model%20extracts%20more%20meaningful%0Afeatures%20for%20further%20downstream%20tasks%20than%20the%20UNet-based%20one%20by%20only%20training%0Athe%20last%20layer%20to%20also%20solve%20segmentation%20of%20coronary%20arteries.%20We%20make%20the%0Acode%20and%20weights%20of%20the%20final%20model%20openly%20available%2C%20which%20can%20serve%20as%20a%0Afoundation%20model%20for%20further%20research%20in%20cardiac%20CT%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07557v1&entry.124074799=Read"},
{"title": "RT-LA-VocE: Real-Time Low-SNR Audio-Visual Speech Enhancement", "author": "Honglie Chen and Rodrigo Mira and Stavros Petridis and Maja Pantic", "abstract": "  In this paper, we aim to generate clean speech frame by frame from a live\nvideo stream and a noisy audio stream without relying on future inputs. To this\nend, we propose RT-LA-VocE, which completely re-designs every component of\nLA-VocE, a state-of-the-art non-causal audio-visual speech enhancement model,\nto perform causal real-time inference with a 40ms input frame. We do so by\ndevising new visual and audio encoders that rely solely on past frames,\nreplacing the Transformer encoder with the Emformer, and designing a new causal\nneural vocoder C-HiFi-GAN. On the popular AVSpeech dataset, we show that our\nalgorithm achieves state-of-the-art results in all real-time scenarios. More\nimportantly, each component is carefully tuned to minimize the algorithm\nlatency to the theoretical minimum (40ms) while maintaining a low end-to-end\nprocessing latency of 28.15ms per frame, enabling real-time frame-by-frame\nenhancement with minimal delay.\n", "link": "http://arxiv.org/abs/2407.07825v1", "date": "2024-07-10", "relevancy": 2.1785, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5783}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5308}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RT-LA-VocE%3A%20Real-Time%20Low-SNR%20Audio-Visual%20Speech%20Enhancement&body=Title%3A%20RT-LA-VocE%3A%20Real-Time%20Low-SNR%20Audio-Visual%20Speech%20Enhancement%0AAuthor%3A%20Honglie%20Chen%20and%20Rodrigo%20Mira%20and%20Stavros%20Petridis%20and%20Maja%20Pantic%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20generate%20clean%20speech%20frame%20by%20frame%20from%20a%20live%0Avideo%20stream%20and%20a%20noisy%20audio%20stream%20without%20relying%20on%20future%20inputs.%20To%20this%0Aend%2C%20we%20propose%20RT-LA-VocE%2C%20which%20completely%20re-designs%20every%20component%20of%0ALA-VocE%2C%20a%20state-of-the-art%20non-causal%20audio-visual%20speech%20enhancement%20model%2C%0Ato%20perform%20causal%20real-time%20inference%20with%20a%2040ms%20input%20frame.%20We%20do%20so%20by%0Adevising%20new%20visual%20and%20audio%20encoders%20that%20rely%20solely%20on%20past%20frames%2C%0Areplacing%20the%20Transformer%20encoder%20with%20the%20Emformer%2C%20and%20designing%20a%20new%20causal%0Aneural%20vocoder%20C-HiFi-GAN.%20On%20the%20popular%20AVSpeech%20dataset%2C%20we%20show%20that%20our%0Aalgorithm%20achieves%20state-of-the-art%20results%20in%20all%20real-time%20scenarios.%20More%0Aimportantly%2C%20each%20component%20is%20carefully%20tuned%20to%20minimize%20the%20algorithm%0Alatency%20to%20the%20theoretical%20minimum%20%2840ms%29%20while%20maintaining%20a%20low%20end-to-end%0Aprocessing%20latency%20of%2028.15ms%20per%20frame%2C%20enabling%20real-time%20frame-by-frame%0Aenhancement%20with%20minimal%20delay.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRT-LA-VocE%253A%2520Real-Time%2520Low-SNR%2520Audio-Visual%2520Speech%2520Enhancement%26entry.906535625%3DHonglie%2520Chen%2520and%2520Rodrigo%2520Mira%2520and%2520Stavros%2520Petridis%2520and%2520Maja%2520Pantic%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520generate%2520clean%2520speech%2520frame%2520by%2520frame%2520from%2520a%2520live%250Avideo%2520stream%2520and%2520a%2520noisy%2520audio%2520stream%2520without%2520relying%2520on%2520future%2520inputs.%2520To%2520this%250Aend%252C%2520we%2520propose%2520RT-LA-VocE%252C%2520which%2520completely%2520re-designs%2520every%2520component%2520of%250ALA-VocE%252C%2520a%2520state-of-the-art%2520non-causal%2520audio-visual%2520speech%2520enhancement%2520model%252C%250Ato%2520perform%2520causal%2520real-time%2520inference%2520with%2520a%252040ms%2520input%2520frame.%2520We%2520do%2520so%2520by%250Adevising%2520new%2520visual%2520and%2520audio%2520encoders%2520that%2520rely%2520solely%2520on%2520past%2520frames%252C%250Areplacing%2520the%2520Transformer%2520encoder%2520with%2520the%2520Emformer%252C%2520and%2520designing%2520a%2520new%2520causal%250Aneural%2520vocoder%2520C-HiFi-GAN.%2520On%2520the%2520popular%2520AVSpeech%2520dataset%252C%2520we%2520show%2520that%2520our%250Aalgorithm%2520achieves%2520state-of-the-art%2520results%2520in%2520all%2520real-time%2520scenarios.%2520More%250Aimportantly%252C%2520each%2520component%2520is%2520carefully%2520tuned%2520to%2520minimize%2520the%2520algorithm%250Alatency%2520to%2520the%2520theoretical%2520minimum%2520%252840ms%2529%2520while%2520maintaining%2520a%2520low%2520end-to-end%250Aprocessing%2520latency%2520of%252028.15ms%2520per%2520frame%252C%2520enabling%2520real-time%2520frame-by-frame%250Aenhancement%2520with%2520minimal%2520delay.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RT-LA-VocE%3A%20Real-Time%20Low-SNR%20Audio-Visual%20Speech%20Enhancement&entry.906535625=Honglie%20Chen%20and%20Rodrigo%20Mira%20and%20Stavros%20Petridis%20and%20Maja%20Pantic&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20generate%20clean%20speech%20frame%20by%20frame%20from%20a%20live%0Avideo%20stream%20and%20a%20noisy%20audio%20stream%20without%20relying%20on%20future%20inputs.%20To%20this%0Aend%2C%20we%20propose%20RT-LA-VocE%2C%20which%20completely%20re-designs%20every%20component%20of%0ALA-VocE%2C%20a%20state-of-the-art%20non-causal%20audio-visual%20speech%20enhancement%20model%2C%0Ato%20perform%20causal%20real-time%20inference%20with%20a%2040ms%20input%20frame.%20We%20do%20so%20by%0Adevising%20new%20visual%20and%20audio%20encoders%20that%20rely%20solely%20on%20past%20frames%2C%0Areplacing%20the%20Transformer%20encoder%20with%20the%20Emformer%2C%20and%20designing%20a%20new%20causal%0Aneural%20vocoder%20C-HiFi-GAN.%20On%20the%20popular%20AVSpeech%20dataset%2C%20we%20show%20that%20our%0Aalgorithm%20achieves%20state-of-the-art%20results%20in%20all%20real-time%20scenarios.%20More%0Aimportantly%2C%20each%20component%20is%20carefully%20tuned%20to%20minimize%20the%20algorithm%0Alatency%20to%20the%20theoretical%20minimum%20%2840ms%29%20while%20maintaining%20a%20low%20end-to-end%0Aprocessing%20latency%20of%2028.15ms%20per%20frame%2C%20enabling%20real-time%20frame-by-frame%0Aenhancement%20with%20minimal%20delay.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07825v1&entry.124074799=Read"},
{"title": "Study on Aspect Ratio Variability toward Robustness of Vision\n  Transformer-based Vehicle Re-identification", "author": "Mei Qiu and Lauren Christopher and Lingxi Li", "abstract": "  Vision Transformers (ViTs) have excelled in vehicle re-identification (ReID)\ntasks. However, non-square aspect ratios of image or video input might\nsignificantly affect the re-identification performance. To address this issue,\nwe propose a novel ViT-based ReID framework in this paper, which fuses models\ntrained on a variety of aspect ratios. Our main contributions are threefold:\n(i) We analyze aspect ratio performance on VeRi-776 and VehicleID datasets,\nguiding input settings based on aspect ratios of original images. (ii) We\nintroduce patch-wise mixup intra-image during ViT patchification (guided by\nspatial attention scores) and implement uneven stride for better object aspect\nratio matching. (iii) We propose a dynamic feature fusing ReID network,\nenhancing model robustness. Our ReID method achieves a significantly improved\nmean Average Precision (mAP) of 91.0\\% compared to the the closest\nstate-of-the-art (CAL) result of 80.9\\% on VehicleID dataset.\n", "link": "http://arxiv.org/abs/2407.07842v1", "date": "2024-07-10", "relevancy": 2.1712, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5468}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5427}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Study%20on%20Aspect%20Ratio%20Variability%20toward%20Robustness%20of%20Vision%0A%20%20Transformer-based%20Vehicle%20Re-identification&body=Title%3A%20Study%20on%20Aspect%20Ratio%20Variability%20toward%20Robustness%20of%20Vision%0A%20%20Transformer-based%20Vehicle%20Re-identification%0AAuthor%3A%20Mei%20Qiu%20and%20Lauren%20Christopher%20and%20Lingxi%20Li%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20excelled%20in%20vehicle%20re-identification%20%28ReID%29%0Atasks.%20However%2C%20non-square%20aspect%20ratios%20of%20image%20or%20video%20input%20might%0Asignificantly%20affect%20the%20re-identification%20performance.%20To%20address%20this%20issue%2C%0Awe%20propose%20a%20novel%20ViT-based%20ReID%20framework%20in%20this%20paper%2C%20which%20fuses%20models%0Atrained%20on%20a%20variety%20of%20aspect%20ratios.%20Our%20main%20contributions%20are%20threefold%3A%0A%28i%29%20We%20analyze%20aspect%20ratio%20performance%20on%20VeRi-776%20and%20VehicleID%20datasets%2C%0Aguiding%20input%20settings%20based%20on%20aspect%20ratios%20of%20original%20images.%20%28ii%29%20We%0Aintroduce%20patch-wise%20mixup%20intra-image%20during%20ViT%20patchification%20%28guided%20by%0Aspatial%20attention%20scores%29%20and%20implement%20uneven%20stride%20for%20better%20object%20aspect%0Aratio%20matching.%20%28iii%29%20We%20propose%20a%20dynamic%20feature%20fusing%20ReID%20network%2C%0Aenhancing%20model%20robustness.%20Our%20ReID%20method%20achieves%20a%20significantly%20improved%0Amean%20Average%20Precision%20%28mAP%29%20of%2091.0%5C%25%20compared%20to%20the%20the%20closest%0Astate-of-the-art%20%28CAL%29%20result%20of%2080.9%5C%25%20on%20VehicleID%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudy%2520on%2520Aspect%2520Ratio%2520Variability%2520toward%2520Robustness%2520of%2520Vision%250A%2520%2520Transformer-based%2520Vehicle%2520Re-identification%26entry.906535625%3DMei%2520Qiu%2520and%2520Lauren%2520Christopher%2520and%2520Lingxi%2520Li%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520excelled%2520in%2520vehicle%2520re-identification%2520%2528ReID%2529%250Atasks.%2520However%252C%2520non-square%2520aspect%2520ratios%2520of%2520image%2520or%2520video%2520input%2520might%250Asignificantly%2520affect%2520the%2520re-identification%2520performance.%2520To%2520address%2520this%2520issue%252C%250Awe%2520propose%2520a%2520novel%2520ViT-based%2520ReID%2520framework%2520in%2520this%2520paper%252C%2520which%2520fuses%2520models%250Atrained%2520on%2520a%2520variety%2520of%2520aspect%2520ratios.%2520Our%2520main%2520contributions%2520are%2520threefold%253A%250A%2528i%2529%2520We%2520analyze%2520aspect%2520ratio%2520performance%2520on%2520VeRi-776%2520and%2520VehicleID%2520datasets%252C%250Aguiding%2520input%2520settings%2520based%2520on%2520aspect%2520ratios%2520of%2520original%2520images.%2520%2528ii%2529%2520We%250Aintroduce%2520patch-wise%2520mixup%2520intra-image%2520during%2520ViT%2520patchification%2520%2528guided%2520by%250Aspatial%2520attention%2520scores%2529%2520and%2520implement%2520uneven%2520stride%2520for%2520better%2520object%2520aspect%250Aratio%2520matching.%2520%2528iii%2529%2520We%2520propose%2520a%2520dynamic%2520feature%2520fusing%2520ReID%2520network%252C%250Aenhancing%2520model%2520robustness.%2520Our%2520ReID%2520method%2520achieves%2520a%2520significantly%2520improved%250Amean%2520Average%2520Precision%2520%2528mAP%2529%2520of%252091.0%255C%2525%2520compared%2520to%2520the%2520the%2520closest%250Astate-of-the-art%2520%2528CAL%2529%2520result%2520of%252080.9%255C%2525%2520on%2520VehicleID%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Study%20on%20Aspect%20Ratio%20Variability%20toward%20Robustness%20of%20Vision%0A%20%20Transformer-based%20Vehicle%20Re-identification&entry.906535625=Mei%20Qiu%20and%20Lauren%20Christopher%20and%20Lingxi%20Li&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20excelled%20in%20vehicle%20re-identification%20%28ReID%29%0Atasks.%20However%2C%20non-square%20aspect%20ratios%20of%20image%20or%20video%20input%20might%0Asignificantly%20affect%20the%20re-identification%20performance.%20To%20address%20this%20issue%2C%0Awe%20propose%20a%20novel%20ViT-based%20ReID%20framework%20in%20this%20paper%2C%20which%20fuses%20models%0Atrained%20on%20a%20variety%20of%20aspect%20ratios.%20Our%20main%20contributions%20are%20threefold%3A%0A%28i%29%20We%20analyze%20aspect%20ratio%20performance%20on%20VeRi-776%20and%20VehicleID%20datasets%2C%0Aguiding%20input%20settings%20based%20on%20aspect%20ratios%20of%20original%20images.%20%28ii%29%20We%0Aintroduce%20patch-wise%20mixup%20intra-image%20during%20ViT%20patchification%20%28guided%20by%0Aspatial%20attention%20scores%29%20and%20implement%20uneven%20stride%20for%20better%20object%20aspect%0Aratio%20matching.%20%28iii%29%20We%20propose%20a%20dynamic%20feature%20fusing%20ReID%20network%2C%0Aenhancing%20model%20robustness.%20Our%20ReID%20method%20achieves%20a%20significantly%20improved%0Amean%20Average%20Precision%20%28mAP%29%20of%2091.0%5C%25%20compared%20to%20the%20the%20closest%0Astate-of-the-art%20%28CAL%29%20result%20of%2080.9%5C%25%20on%20VehicleID%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07842v1&entry.124074799=Read"},
{"title": "SUMix: Mixup with Semantic and Uncertain Information", "author": "Huafeng Qin and Xin Jin and Hongyu Zhu and Hongchao Liao and Moun\u00eem A. El-Yacoubi and Xinbo Gao", "abstract": "  Mixup data augmentation approaches have been applied for various tasks of\ndeep learning to improve the generalization ability of deep neural networks.\nSome existing approaches CutMix, SaliencyMix, etc. randomly replace a patch in\none image with patches from another to generate the mixed image. Similarly, the\ncorresponding labels are linearly combined by a fixed ratio $\\lambda$ by l. The\nobjects in two images may be overlapped during the mixing process, so some\nsemantic information is corrupted in the mixed samples. In this case, the mixed\nimage does not match the mixed label information. Besides, such a label may\nmislead the deep learning model training, which results in poor performance. To\nsolve this problem, we proposed a novel approach named SUMix to learn the\nmixing ratio as well as the uncertainty for the mixed samples during the\ntraining process. First, we design a learnable similarity function to compute\nan accurate mix ratio. Second, an approach is investigated as a regularized\nterm to model the uncertainty of the mixed samples. We conduct experiments on\nfive image benchmarks, and extensive experimental results imply that our method\nis capable of improving the performance of classifiers with different\ncutting-based mixup approaches. The source code is available at\nhttps://github.com/JinXins/SUMix.\n", "link": "http://arxiv.org/abs/2407.07805v1", "date": "2024-07-10", "relevancy": 2.1498, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5767}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5444}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUMix%3A%20Mixup%20with%20Semantic%20and%20Uncertain%20Information&body=Title%3A%20SUMix%3A%20Mixup%20with%20Semantic%20and%20Uncertain%20Information%0AAuthor%3A%20Huafeng%20Qin%20and%20Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Hongchao%20Liao%20and%20Moun%C3%AEm%20A.%20El-Yacoubi%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Mixup%20data%20augmentation%20approaches%20have%20been%20applied%20for%20various%20tasks%20of%0Adeep%20learning%20to%20improve%20the%20generalization%20ability%20of%20deep%20neural%20networks.%0ASome%20existing%20approaches%20CutMix%2C%20SaliencyMix%2C%20etc.%20randomly%20replace%20a%20patch%20in%0Aone%20image%20with%20patches%20from%20another%20to%20generate%20the%20mixed%20image.%20Similarly%2C%20the%0Acorresponding%20labels%20are%20linearly%20combined%20by%20a%20fixed%20ratio%20%24%5Clambda%24%20by%20l.%20The%0Aobjects%20in%20two%20images%20may%20be%20overlapped%20during%20the%20mixing%20process%2C%20so%20some%0Asemantic%20information%20is%20corrupted%20in%20the%20mixed%20samples.%20In%20this%20case%2C%20the%20mixed%0Aimage%20does%20not%20match%20the%20mixed%20label%20information.%20Besides%2C%20such%20a%20label%20may%0Amislead%20the%20deep%20learning%20model%20training%2C%20which%20results%20in%20poor%20performance.%20To%0Asolve%20this%20problem%2C%20we%20proposed%20a%20novel%20approach%20named%20SUMix%20to%20learn%20the%0Amixing%20ratio%20as%20well%20as%20the%20uncertainty%20for%20the%20mixed%20samples%20during%20the%0Atraining%20process.%20First%2C%20we%20design%20a%20learnable%20similarity%20function%20to%20compute%0Aan%20accurate%20mix%20ratio.%20Second%2C%20an%20approach%20is%20investigated%20as%20a%20regularized%0Aterm%20to%20model%20the%20uncertainty%20of%20the%20mixed%20samples.%20We%20conduct%20experiments%20on%0Afive%20image%20benchmarks%2C%20and%20extensive%20experimental%20results%20imply%20that%20our%20method%0Ais%20capable%20of%20improving%20the%20performance%20of%20classifiers%20with%20different%0Acutting-based%20mixup%20approaches.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/JinXins/SUMix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUMix%253A%2520Mixup%2520with%2520Semantic%2520and%2520Uncertain%2520Information%26entry.906535625%3DHuafeng%2520Qin%2520and%2520Xin%2520Jin%2520and%2520Hongyu%2520Zhu%2520and%2520Hongchao%2520Liao%2520and%2520Moun%25C3%25AEm%2520A.%2520El-Yacoubi%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Mixup%2520data%2520augmentation%2520approaches%2520have%2520been%2520applied%2520for%2520various%2520tasks%2520of%250Adeep%2520learning%2520to%2520improve%2520the%2520generalization%2520ability%2520of%2520deep%2520neural%2520networks.%250ASome%2520existing%2520approaches%2520CutMix%252C%2520SaliencyMix%252C%2520etc.%2520randomly%2520replace%2520a%2520patch%2520in%250Aone%2520image%2520with%2520patches%2520from%2520another%2520to%2520generate%2520the%2520mixed%2520image.%2520Similarly%252C%2520the%250Acorresponding%2520labels%2520are%2520linearly%2520combined%2520by%2520a%2520fixed%2520ratio%2520%2524%255Clambda%2524%2520by%2520l.%2520The%250Aobjects%2520in%2520two%2520images%2520may%2520be%2520overlapped%2520during%2520the%2520mixing%2520process%252C%2520so%2520some%250Asemantic%2520information%2520is%2520corrupted%2520in%2520the%2520mixed%2520samples.%2520In%2520this%2520case%252C%2520the%2520mixed%250Aimage%2520does%2520not%2520match%2520the%2520mixed%2520label%2520information.%2520Besides%252C%2520such%2520a%2520label%2520may%250Amislead%2520the%2520deep%2520learning%2520model%2520training%252C%2520which%2520results%2520in%2520poor%2520performance.%2520To%250Asolve%2520this%2520problem%252C%2520we%2520proposed%2520a%2520novel%2520approach%2520named%2520SUMix%2520to%2520learn%2520the%250Amixing%2520ratio%2520as%2520well%2520as%2520the%2520uncertainty%2520for%2520the%2520mixed%2520samples%2520during%2520the%250Atraining%2520process.%2520First%252C%2520we%2520design%2520a%2520learnable%2520similarity%2520function%2520to%2520compute%250Aan%2520accurate%2520mix%2520ratio.%2520Second%252C%2520an%2520approach%2520is%2520investigated%2520as%2520a%2520regularized%250Aterm%2520to%2520model%2520the%2520uncertainty%2520of%2520the%2520mixed%2520samples.%2520We%2520conduct%2520experiments%2520on%250Afive%2520image%2520benchmarks%252C%2520and%2520extensive%2520experimental%2520results%2520imply%2520that%2520our%2520method%250Ais%2520capable%2520of%2520improving%2520the%2520performance%2520of%2520classifiers%2520with%2520different%250Acutting-based%2520mixup%2520approaches.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/JinXins/SUMix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUMix%3A%20Mixup%20with%20Semantic%20and%20Uncertain%20Information&entry.906535625=Huafeng%20Qin%20and%20Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Hongchao%20Liao%20and%20Moun%C3%AEm%20A.%20El-Yacoubi%20and%20Xinbo%20Gao&entry.1292438233=%20%20Mixup%20data%20augmentation%20approaches%20have%20been%20applied%20for%20various%20tasks%20of%0Adeep%20learning%20to%20improve%20the%20generalization%20ability%20of%20deep%20neural%20networks.%0ASome%20existing%20approaches%20CutMix%2C%20SaliencyMix%2C%20etc.%20randomly%20replace%20a%20patch%20in%0Aone%20image%20with%20patches%20from%20another%20to%20generate%20the%20mixed%20image.%20Similarly%2C%20the%0Acorresponding%20labels%20are%20linearly%20combined%20by%20a%20fixed%20ratio%20%24%5Clambda%24%20by%20l.%20The%0Aobjects%20in%20two%20images%20may%20be%20overlapped%20during%20the%20mixing%20process%2C%20so%20some%0Asemantic%20information%20is%20corrupted%20in%20the%20mixed%20samples.%20In%20this%20case%2C%20the%20mixed%0Aimage%20does%20not%20match%20the%20mixed%20label%20information.%20Besides%2C%20such%20a%20label%20may%0Amislead%20the%20deep%20learning%20model%20training%2C%20which%20results%20in%20poor%20performance.%20To%0Asolve%20this%20problem%2C%20we%20proposed%20a%20novel%20approach%20named%20SUMix%20to%20learn%20the%0Amixing%20ratio%20as%20well%20as%20the%20uncertainty%20for%20the%20mixed%20samples%20during%20the%0Atraining%20process.%20First%2C%20we%20design%20a%20learnable%20similarity%20function%20to%20compute%0Aan%20accurate%20mix%20ratio.%20Second%2C%20an%20approach%20is%20investigated%20as%20a%20regularized%0Aterm%20to%20model%20the%20uncertainty%20of%20the%20mixed%20samples.%20We%20conduct%20experiments%20on%0Afive%20image%20benchmarks%2C%20and%20extensive%20experimental%20results%20imply%20that%20our%20method%0Ais%20capable%20of%20improving%20the%20performance%20of%20classifiers%20with%20different%0Acutting-based%20mixup%20approaches.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/JinXins/SUMix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07805v1&entry.124074799=Read"},
{"title": "Boosting Medical Image Synthesis via Registration-guided Consistency and\n  Disentanglement Learning", "author": "Chuanpu Li and Zeli Chen and Yiwen Zhang and Liming Zhong and Wei Yang", "abstract": "  Medical image synthesis remains challenging due to misalignment noise during\ntraining. Existing methods have attempted to address this challenge by\nincorporating a registration-guided module. However, these methods tend to\noverlook the task-specific constraints on the synthetic and registration\nmodules, which may cause the synthetic module to still generate spatially\naligned images with misaligned target images during training, regardless of the\nregistration module's function. Therefore, this paper proposes\nregistration-guided consistency and incorporates disentanglement learning for\nmedical image synthesis. The proposed registration-guided consistency\narchitecture fosters task-specificity within the synthetic and registration\nmodules by applying identical deformation fields before and after synthesis,\nwhile enforcing output consistency through an alignment loss. Moreover, the\nsynthetic module is designed to possess the capability of disentangling\nanatomical structures and specific styles across various modalities. An anatomy\nconsistency loss is introduced to further compel the synthetic module to\npreserve geometrical integrity within latent spaces. Experiments conducted on\nboth an in-house abdominal CECT-CT dataset and a publicly available pelvic\nMR-CT dataset have demonstrated the superiority of the proposed method.\n", "link": "http://arxiv.org/abs/2407.07660v1", "date": "2024-07-10", "relevancy": 2.1372, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5503}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Medical%20Image%20Synthesis%20via%20Registration-guided%20Consistency%20and%0A%20%20Disentanglement%20Learning&body=Title%3A%20Boosting%20Medical%20Image%20Synthesis%20via%20Registration-guided%20Consistency%20and%0A%20%20Disentanglement%20Learning%0AAuthor%3A%20Chuanpu%20Li%20and%20Zeli%20Chen%20and%20Yiwen%20Zhang%20and%20Liming%20Zhong%20and%20Wei%20Yang%0AAbstract%3A%20%20%20Medical%20image%20synthesis%20remains%20challenging%20due%20to%20misalignment%20noise%20during%0Atraining.%20Existing%20methods%20have%20attempted%20to%20address%20this%20challenge%20by%0Aincorporating%20a%20registration-guided%20module.%20However%2C%20these%20methods%20tend%20to%0Aoverlook%20the%20task-specific%20constraints%20on%20the%20synthetic%20and%20registration%0Amodules%2C%20which%20may%20cause%20the%20synthetic%20module%20to%20still%20generate%20spatially%0Aaligned%20images%20with%20misaligned%20target%20images%20during%20training%2C%20regardless%20of%20the%0Aregistration%20module%27s%20function.%20Therefore%2C%20this%20paper%20proposes%0Aregistration-guided%20consistency%20and%20incorporates%20disentanglement%20learning%20for%0Amedical%20image%20synthesis.%20The%20proposed%20registration-guided%20consistency%0Aarchitecture%20fosters%20task-specificity%20within%20the%20synthetic%20and%20registration%0Amodules%20by%20applying%20identical%20deformation%20fields%20before%20and%20after%20synthesis%2C%0Awhile%20enforcing%20output%20consistency%20through%20an%20alignment%20loss.%20Moreover%2C%20the%0Asynthetic%20module%20is%20designed%20to%20possess%20the%20capability%20of%20disentangling%0Aanatomical%20structures%20and%20specific%20styles%20across%20various%20modalities.%20An%20anatomy%0Aconsistency%20loss%20is%20introduced%20to%20further%20compel%20the%20synthetic%20module%20to%0Apreserve%20geometrical%20integrity%20within%20latent%20spaces.%20Experiments%20conducted%20on%0Aboth%20an%20in-house%20abdominal%20CECT-CT%20dataset%20and%20a%20publicly%20available%20pelvic%0AMR-CT%20dataset%20have%20demonstrated%20the%20superiority%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Medical%2520Image%2520Synthesis%2520via%2520Registration-guided%2520Consistency%2520and%250A%2520%2520Disentanglement%2520Learning%26entry.906535625%3DChuanpu%2520Li%2520and%2520Zeli%2520Chen%2520and%2520Yiwen%2520Zhang%2520and%2520Liming%2520Zhong%2520and%2520Wei%2520Yang%26entry.1292438233%3D%2520%2520Medical%2520image%2520synthesis%2520remains%2520challenging%2520due%2520to%2520misalignment%2520noise%2520during%250Atraining.%2520Existing%2520methods%2520have%2520attempted%2520to%2520address%2520this%2520challenge%2520by%250Aincorporating%2520a%2520registration-guided%2520module.%2520However%252C%2520these%2520methods%2520tend%2520to%250Aoverlook%2520the%2520task-specific%2520constraints%2520on%2520the%2520synthetic%2520and%2520registration%250Amodules%252C%2520which%2520may%2520cause%2520the%2520synthetic%2520module%2520to%2520still%2520generate%2520spatially%250Aaligned%2520images%2520with%2520misaligned%2520target%2520images%2520during%2520training%252C%2520regardless%2520of%2520the%250Aregistration%2520module%2527s%2520function.%2520Therefore%252C%2520this%2520paper%2520proposes%250Aregistration-guided%2520consistency%2520and%2520incorporates%2520disentanglement%2520learning%2520for%250Amedical%2520image%2520synthesis.%2520The%2520proposed%2520registration-guided%2520consistency%250Aarchitecture%2520fosters%2520task-specificity%2520within%2520the%2520synthetic%2520and%2520registration%250Amodules%2520by%2520applying%2520identical%2520deformation%2520fields%2520before%2520and%2520after%2520synthesis%252C%250Awhile%2520enforcing%2520output%2520consistency%2520through%2520an%2520alignment%2520loss.%2520Moreover%252C%2520the%250Asynthetic%2520module%2520is%2520designed%2520to%2520possess%2520the%2520capability%2520of%2520disentangling%250Aanatomical%2520structures%2520and%2520specific%2520styles%2520across%2520various%2520modalities.%2520An%2520anatomy%250Aconsistency%2520loss%2520is%2520introduced%2520to%2520further%2520compel%2520the%2520synthetic%2520module%2520to%250Apreserve%2520geometrical%2520integrity%2520within%2520latent%2520spaces.%2520Experiments%2520conducted%2520on%250Aboth%2520an%2520in-house%2520abdominal%2520CECT-CT%2520dataset%2520and%2520a%2520publicly%2520available%2520pelvic%250AMR-CT%2520dataset%2520have%2520demonstrated%2520the%2520superiority%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Medical%20Image%20Synthesis%20via%20Registration-guided%20Consistency%20and%0A%20%20Disentanglement%20Learning&entry.906535625=Chuanpu%20Li%20and%20Zeli%20Chen%20and%20Yiwen%20Zhang%20and%20Liming%20Zhong%20and%20Wei%20Yang&entry.1292438233=%20%20Medical%20image%20synthesis%20remains%20challenging%20due%20to%20misalignment%20noise%20during%0Atraining.%20Existing%20methods%20have%20attempted%20to%20address%20this%20challenge%20by%0Aincorporating%20a%20registration-guided%20module.%20However%2C%20these%20methods%20tend%20to%0Aoverlook%20the%20task-specific%20constraints%20on%20the%20synthetic%20and%20registration%0Amodules%2C%20which%20may%20cause%20the%20synthetic%20module%20to%20still%20generate%20spatially%0Aaligned%20images%20with%20misaligned%20target%20images%20during%20training%2C%20regardless%20of%20the%0Aregistration%20module%27s%20function.%20Therefore%2C%20this%20paper%20proposes%0Aregistration-guided%20consistency%20and%20incorporates%20disentanglement%20learning%20for%0Amedical%20image%20synthesis.%20The%20proposed%20registration-guided%20consistency%0Aarchitecture%20fosters%20task-specificity%20within%20the%20synthetic%20and%20registration%0Amodules%20by%20applying%20identical%20deformation%20fields%20before%20and%20after%20synthesis%2C%0Awhile%20enforcing%20output%20consistency%20through%20an%20alignment%20loss.%20Moreover%2C%20the%0Asynthetic%20module%20is%20designed%20to%20possess%20the%20capability%20of%20disentangling%0Aanatomical%20structures%20and%20specific%20styles%20across%20various%20modalities.%20An%20anatomy%0Aconsistency%20loss%20is%20introduced%20to%20further%20compel%20the%20synthetic%20module%20to%0Apreserve%20geometrical%20integrity%20within%20latent%20spaces.%20Experiments%20conducted%20on%0Aboth%20an%20in-house%20abdominal%20CECT-CT%20dataset%20and%20a%20publicly%20available%20pelvic%0AMR-CT%20dataset%20have%20demonstrated%20the%20superiority%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07660v1&entry.124074799=Read"},
{"title": "AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning", "author": "Jongsuk Kim and Jiwon Shin and Junmo Kim", "abstract": "  In recent years, advancements in representation learning and language models\nhave propelled Automated Captioning (AC) to new heights, enabling the\ngeneration of human-level descriptions. Leveraging these advancements, we\npropose \\textbf{AVCap}, an \\textbf{A}udio-\\textbf{V}isual \\textbf{Cap}tioning\nframework, a simple yet powerful baseline approach applicable to audio-visual\ncaptioning. AVCap utilizes audio-visual features as text tokens, which has many\nadvantages not only in performance but also in the extensibility and\nscalability of the model. AVCap is designed around three pivotal dimensions:\nthe exploration of optimal audio-visual encoder architectures, the adaptation\nof pre-trained models according to the characteristics of generated text, and\nthe investigation into the efficacy of modality fusion in captioning. Our\nmethod outperforms existing audio-visual captioning methods across all metrics\nand the code is available on https://github.com/JongSuk1/AVCap\n", "link": "http://arxiv.org/abs/2407.07801v1", "date": "2024-07-10", "relevancy": 2.1338, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5443}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5412}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AVCap%3A%20Leveraging%20Audio-Visual%20Features%20as%20Text%20Tokens%20for%20Captioning&body=Title%3A%20AVCap%3A%20Leveraging%20Audio-Visual%20Features%20as%20Text%20Tokens%20for%20Captioning%0AAuthor%3A%20Jongsuk%20Kim%20and%20Jiwon%20Shin%20and%20Junmo%20Kim%0AAbstract%3A%20%20%20In%20recent%20years%2C%20advancements%20in%20representation%20learning%20and%20language%20models%0Ahave%20propelled%20Automated%20Captioning%20%28AC%29%20to%20new%20heights%2C%20enabling%20the%0Ageneration%20of%20human-level%20descriptions.%20Leveraging%20these%20advancements%2C%20we%0Apropose%20%5Ctextbf%7BAVCap%7D%2C%20an%20%5Ctextbf%7BA%7Dudio-%5Ctextbf%7BV%7Disual%20%5Ctextbf%7BCap%7Dtioning%0Aframework%2C%20a%20simple%20yet%20powerful%20baseline%20approach%20applicable%20to%20audio-visual%0Acaptioning.%20AVCap%20utilizes%20audio-visual%20features%20as%20text%20tokens%2C%20which%20has%20many%0Aadvantages%20not%20only%20in%20performance%20but%20also%20in%20the%20extensibility%20and%0Ascalability%20of%20the%20model.%20AVCap%20is%20designed%20around%20three%20pivotal%20dimensions%3A%0Athe%20exploration%20of%20optimal%20audio-visual%20encoder%20architectures%2C%20the%20adaptation%0Aof%20pre-trained%20models%20according%20to%20the%20characteristics%20of%20generated%20text%2C%20and%0Athe%20investigation%20into%20the%20efficacy%20of%20modality%20fusion%20in%20captioning.%20Our%0Amethod%20outperforms%20existing%20audio-visual%20captioning%20methods%20across%20all%20metrics%0Aand%20the%20code%20is%20available%20on%20https%3A//github.com/JongSuk1/AVCap%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAVCap%253A%2520Leveraging%2520Audio-Visual%2520Features%2520as%2520Text%2520Tokens%2520for%2520Captioning%26entry.906535625%3DJongsuk%2520Kim%2520and%2520Jiwon%2520Shin%2520and%2520Junmo%2520Kim%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520advancements%2520in%2520representation%2520learning%2520and%2520language%2520models%250Ahave%2520propelled%2520Automated%2520Captioning%2520%2528AC%2529%2520to%2520new%2520heights%252C%2520enabling%2520the%250Ageneration%2520of%2520human-level%2520descriptions.%2520Leveraging%2520these%2520advancements%252C%2520we%250Apropose%2520%255Ctextbf%257BAVCap%257D%252C%2520an%2520%255Ctextbf%257BA%257Dudio-%255Ctextbf%257BV%257Disual%2520%255Ctextbf%257BCap%257Dtioning%250Aframework%252C%2520a%2520simple%2520yet%2520powerful%2520baseline%2520approach%2520applicable%2520to%2520audio-visual%250Acaptioning.%2520AVCap%2520utilizes%2520audio-visual%2520features%2520as%2520text%2520tokens%252C%2520which%2520has%2520many%250Aadvantages%2520not%2520only%2520in%2520performance%2520but%2520also%2520in%2520the%2520extensibility%2520and%250Ascalability%2520of%2520the%2520model.%2520AVCap%2520is%2520designed%2520around%2520three%2520pivotal%2520dimensions%253A%250Athe%2520exploration%2520of%2520optimal%2520audio-visual%2520encoder%2520architectures%252C%2520the%2520adaptation%250Aof%2520pre-trained%2520models%2520according%2520to%2520the%2520characteristics%2520of%2520generated%2520text%252C%2520and%250Athe%2520investigation%2520into%2520the%2520efficacy%2520of%2520modality%2520fusion%2520in%2520captioning.%2520Our%250Amethod%2520outperforms%2520existing%2520audio-visual%2520captioning%2520methods%2520across%2520all%2520metrics%250Aand%2520the%2520code%2520is%2520available%2520on%2520https%253A//github.com/JongSuk1/AVCap%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AVCap%3A%20Leveraging%20Audio-Visual%20Features%20as%20Text%20Tokens%20for%20Captioning&entry.906535625=Jongsuk%20Kim%20and%20Jiwon%20Shin%20and%20Junmo%20Kim&entry.1292438233=%20%20In%20recent%20years%2C%20advancements%20in%20representation%20learning%20and%20language%20models%0Ahave%20propelled%20Automated%20Captioning%20%28AC%29%20to%20new%20heights%2C%20enabling%20the%0Ageneration%20of%20human-level%20descriptions.%20Leveraging%20these%20advancements%2C%20we%0Apropose%20%5Ctextbf%7BAVCap%7D%2C%20an%20%5Ctextbf%7BA%7Dudio-%5Ctextbf%7BV%7Disual%20%5Ctextbf%7BCap%7Dtioning%0Aframework%2C%20a%20simple%20yet%20powerful%20baseline%20approach%20applicable%20to%20audio-visual%0Acaptioning.%20AVCap%20utilizes%20audio-visual%20features%20as%20text%20tokens%2C%20which%20has%20many%0Aadvantages%20not%20only%20in%20performance%20but%20also%20in%20the%20extensibility%20and%0Ascalability%20of%20the%20model.%20AVCap%20is%20designed%20around%20three%20pivotal%20dimensions%3A%0Athe%20exploration%20of%20optimal%20audio-visual%20encoder%20architectures%2C%20the%20adaptation%0Aof%20pre-trained%20models%20according%20to%20the%20characteristics%20of%20generated%20text%2C%20and%0Athe%20investigation%20into%20the%20efficacy%20of%20modality%20fusion%20in%20captioning.%20Our%0Amethod%20outperforms%20existing%20audio-visual%20captioning%20methods%20across%20all%20metrics%0Aand%20the%20code%20is%20available%20on%20https%3A//github.com/JongSuk1/AVCap%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07801v1&entry.124074799=Read"},
{"title": "OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective\n  Fusion", "author": "Hao Wang and Pengzhen Ren and Zequn Jie and Xiao Dong and Chengjian Feng and Yinlong Qian and Lin Ma and Dongmei Jiang and Yaowei Wang and Xiangyuan Lan and Xiaodan Liang", "abstract": "  Open-vocabulary detection is a challenging task due to the requirement of\ndetecting objects based on class names, including those not encountered during\ntraining. Existing methods have shown strong zero-shot detection capabilities\nthrough pre-training on diverse large-scale datasets. However, these approaches\nstill face two primary challenges: (i) how to universally integrate diverse\ndata sources for end-to-end training, and (ii) how to effectively leverage the\nlanguage-aware capability for region-level cross-modality understanding. To\naddress these challenges, we propose a novel unified open-vocabulary detection\nmethod called OV-DINO, which pre-trains on diverse large-scale datasets with\nlanguage-aware selective fusion in a unified framework. Specifically, we\nintroduce a Unified Data Integration (UniDI) pipeline to enable end-to-end\ntraining and eliminate noise from pseudo-label generation by unifying different\ndata sources into detection-centric data. In addition, we propose a\nLanguage-Aware Selective Fusion (LASF) module to enable the language-aware\nability of the model through a language-aware query selection and fusion\nprocess. We evaluate the performance of the proposed OV-DINO on popular\nopen-vocabulary detection benchmark datasets, achieving state-of-the-art\nresults with an AP of 50.6\\% on the COCO dataset and 40.0\\% on the LVIS dataset\nin a zero-shot manner, demonstrating its strong generalization ability.\nFurthermore, the fine-tuned OV-DINO on COCO achieves 58.4\\% AP, outperforming\nmany existing methods with the same backbone. The code for OV-DINO will be\navailable at\n\\href{https://github.com/wanghao9610/OV-DINO}{https://github.com/wanghao9610/OV-DINO}.\n", "link": "http://arxiv.org/abs/2407.07844v1", "date": "2024-07-10", "relevancy": 2.1283, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OV-DINO%3A%20Unified%20Open-Vocabulary%20Detection%20with%20Language-Aware%20Selective%0A%20%20Fusion&body=Title%3A%20OV-DINO%3A%20Unified%20Open-Vocabulary%20Detection%20with%20Language-Aware%20Selective%0A%20%20Fusion%0AAuthor%3A%20Hao%20Wang%20and%20Pengzhen%20Ren%20and%20Zequn%20Jie%20and%20Xiao%20Dong%20and%20Chengjian%20Feng%20and%20Yinlong%20Qian%20and%20Lin%20Ma%20and%20Dongmei%20Jiang%20and%20Yaowei%20Wang%20and%20Xiangyuan%20Lan%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Open-vocabulary%20detection%20is%20a%20challenging%20task%20due%20to%20the%20requirement%20of%0Adetecting%20objects%20based%20on%20class%20names%2C%20including%20those%20not%20encountered%20during%0Atraining.%20Existing%20methods%20have%20shown%20strong%20zero-shot%20detection%20capabilities%0Athrough%20pre-training%20on%20diverse%20large-scale%20datasets.%20However%2C%20these%20approaches%0Astill%20face%20two%20primary%20challenges%3A%20%28i%29%20how%20to%20universally%20integrate%20diverse%0Adata%20sources%20for%20end-to-end%20training%2C%20and%20%28ii%29%20how%20to%20effectively%20leverage%20the%0Alanguage-aware%20capability%20for%20region-level%20cross-modality%20understanding.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20unified%20open-vocabulary%20detection%0Amethod%20called%20OV-DINO%2C%20which%20pre-trains%20on%20diverse%20large-scale%20datasets%20with%0Alanguage-aware%20selective%20fusion%20in%20a%20unified%20framework.%20Specifically%2C%20we%0Aintroduce%20a%20Unified%20Data%20Integration%20%28UniDI%29%20pipeline%20to%20enable%20end-to-end%0Atraining%20and%20eliminate%20noise%20from%20pseudo-label%20generation%20by%20unifying%20different%0Adata%20sources%20into%20detection-centric%20data.%20In%20addition%2C%20we%20propose%20a%0ALanguage-Aware%20Selective%20Fusion%20%28LASF%29%20module%20to%20enable%20the%20language-aware%0Aability%20of%20the%20model%20through%20a%20language-aware%20query%20selection%20and%20fusion%0Aprocess.%20We%20evaluate%20the%20performance%20of%20the%20proposed%20OV-DINO%20on%20popular%0Aopen-vocabulary%20detection%20benchmark%20datasets%2C%20achieving%20state-of-the-art%0Aresults%20with%20an%20AP%20of%2050.6%5C%25%20on%20the%20COCO%20dataset%20and%2040.0%5C%25%20on%20the%20LVIS%20dataset%0Ain%20a%20zero-shot%20manner%2C%20demonstrating%20its%20strong%20generalization%20ability.%0AFurthermore%2C%20the%20fine-tuned%20OV-DINO%20on%20COCO%20achieves%2058.4%5C%25%20AP%2C%20outperforming%0Amany%20existing%20methods%20with%20the%20same%20backbone.%20The%20code%20for%20OV-DINO%20will%20be%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/wanghao9610/OV-DINO%7D%7Bhttps%3A//github.com/wanghao9610/OV-DINO%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOV-DINO%253A%2520Unified%2520Open-Vocabulary%2520Detection%2520with%2520Language-Aware%2520Selective%250A%2520%2520Fusion%26entry.906535625%3DHao%2520Wang%2520and%2520Pengzhen%2520Ren%2520and%2520Zequn%2520Jie%2520and%2520Xiao%2520Dong%2520and%2520Chengjian%2520Feng%2520and%2520Yinlong%2520Qian%2520and%2520Lin%2520Ma%2520and%2520Dongmei%2520Jiang%2520and%2520Yaowei%2520Wang%2520and%2520Xiangyuan%2520Lan%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Open-vocabulary%2520detection%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%2520requirement%2520of%250Adetecting%2520objects%2520based%2520on%2520class%2520names%252C%2520including%2520those%2520not%2520encountered%2520during%250Atraining.%2520Existing%2520methods%2520have%2520shown%2520strong%2520zero-shot%2520detection%2520capabilities%250Athrough%2520pre-training%2520on%2520diverse%2520large-scale%2520datasets.%2520However%252C%2520these%2520approaches%250Astill%2520face%2520two%2520primary%2520challenges%253A%2520%2528i%2529%2520how%2520to%2520universally%2520integrate%2520diverse%250Adata%2520sources%2520for%2520end-to-end%2520training%252C%2520and%2520%2528ii%2529%2520how%2520to%2520effectively%2520leverage%2520the%250Alanguage-aware%2520capability%2520for%2520region-level%2520cross-modality%2520understanding.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520unified%2520open-vocabulary%2520detection%250Amethod%2520called%2520OV-DINO%252C%2520which%2520pre-trains%2520on%2520diverse%2520large-scale%2520datasets%2520with%250Alanguage-aware%2520selective%2520fusion%2520in%2520a%2520unified%2520framework.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520Unified%2520Data%2520Integration%2520%2528UniDI%2529%2520pipeline%2520to%2520enable%2520end-to-end%250Atraining%2520and%2520eliminate%2520noise%2520from%2520pseudo-label%2520generation%2520by%2520unifying%2520different%250Adata%2520sources%2520into%2520detection-centric%2520data.%2520In%2520addition%252C%2520we%2520propose%2520a%250ALanguage-Aware%2520Selective%2520Fusion%2520%2528LASF%2529%2520module%2520to%2520enable%2520the%2520language-aware%250Aability%2520of%2520the%2520model%2520through%2520a%2520language-aware%2520query%2520selection%2520and%2520fusion%250Aprocess.%2520We%2520evaluate%2520the%2520performance%2520of%2520the%2520proposed%2520OV-DINO%2520on%2520popular%250Aopen-vocabulary%2520detection%2520benchmark%2520datasets%252C%2520achieving%2520state-of-the-art%250Aresults%2520with%2520an%2520AP%2520of%252050.6%255C%2525%2520on%2520the%2520COCO%2520dataset%2520and%252040.0%255C%2525%2520on%2520the%2520LVIS%2520dataset%250Ain%2520a%2520zero-shot%2520manner%252C%2520demonstrating%2520its%2520strong%2520generalization%2520ability.%250AFurthermore%252C%2520the%2520fine-tuned%2520OV-DINO%2520on%2520COCO%2520achieves%252058.4%255C%2525%2520AP%252C%2520outperforming%250Amany%2520existing%2520methods%2520with%2520the%2520same%2520backbone.%2520The%2520code%2520for%2520OV-DINO%2520will%2520be%250Aavailable%2520at%250A%255Chref%257Bhttps%253A//github.com/wanghao9610/OV-DINO%257D%257Bhttps%253A//github.com/wanghao9610/OV-DINO%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OV-DINO%3A%20Unified%20Open-Vocabulary%20Detection%20with%20Language-Aware%20Selective%0A%20%20Fusion&entry.906535625=Hao%20Wang%20and%20Pengzhen%20Ren%20and%20Zequn%20Jie%20and%20Xiao%20Dong%20and%20Chengjian%20Feng%20and%20Yinlong%20Qian%20and%20Lin%20Ma%20and%20Dongmei%20Jiang%20and%20Yaowei%20Wang%20and%20Xiangyuan%20Lan%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Open-vocabulary%20detection%20is%20a%20challenging%20task%20due%20to%20the%20requirement%20of%0Adetecting%20objects%20based%20on%20class%20names%2C%20including%20those%20not%20encountered%20during%0Atraining.%20Existing%20methods%20have%20shown%20strong%20zero-shot%20detection%20capabilities%0Athrough%20pre-training%20on%20diverse%20large-scale%20datasets.%20However%2C%20these%20approaches%0Astill%20face%20two%20primary%20challenges%3A%20%28i%29%20how%20to%20universally%20integrate%20diverse%0Adata%20sources%20for%20end-to-end%20training%2C%20and%20%28ii%29%20how%20to%20effectively%20leverage%20the%0Alanguage-aware%20capability%20for%20region-level%20cross-modality%20understanding.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20unified%20open-vocabulary%20detection%0Amethod%20called%20OV-DINO%2C%20which%20pre-trains%20on%20diverse%20large-scale%20datasets%20with%0Alanguage-aware%20selective%20fusion%20in%20a%20unified%20framework.%20Specifically%2C%20we%0Aintroduce%20a%20Unified%20Data%20Integration%20%28UniDI%29%20pipeline%20to%20enable%20end-to-end%0Atraining%20and%20eliminate%20noise%20from%20pseudo-label%20generation%20by%20unifying%20different%0Adata%20sources%20into%20detection-centric%20data.%20In%20addition%2C%20we%20propose%20a%0ALanguage-Aware%20Selective%20Fusion%20%28LASF%29%20module%20to%20enable%20the%20language-aware%0Aability%20of%20the%20model%20through%20a%20language-aware%20query%20selection%20and%20fusion%0Aprocess.%20We%20evaluate%20the%20performance%20of%20the%20proposed%20OV-DINO%20on%20popular%0Aopen-vocabulary%20detection%20benchmark%20datasets%2C%20achieving%20state-of-the-art%0Aresults%20with%20an%20AP%20of%2050.6%5C%25%20on%20the%20COCO%20dataset%20and%2040.0%5C%25%20on%20the%20LVIS%20dataset%0Ain%20a%20zero-shot%20manner%2C%20demonstrating%20its%20strong%20generalization%20ability.%0AFurthermore%2C%20the%20fine-tuned%20OV-DINO%20on%20COCO%20achieves%2058.4%5C%25%20AP%2C%20outperforming%0Amany%20existing%20methods%20with%20the%20same%20backbone.%20The%20code%20for%20OV-DINO%20will%20be%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/wanghao9610/OV-DINO%7D%7Bhttps%3A//github.com/wanghao9610/OV-DINO%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07844v1&entry.124074799=Read"},
{"title": "Agent Lumos: Unified and Modular Training for Open-Source Language\n  Agents", "author": "Da Yin and Faeze Brahman and Abhilasha Ravichander and Khyathi Chandu and Kai-Wei Chang and Yejin Choi and Bill Yuchen Lin", "abstract": "  Closed-source agents suffer from several issues such as a lack of\naffordability, transparency, and reproducibility, particularly on complex\ninteractive tasks. This motivates the development of open-source alternatives.\nWe introduce LUMOS, one of the first frameworks for training open-source\nLLM-based agents. LUMOS features a learnable, unified, and modular architecture\nwith a planning module that learns high-level subgoal generation, and a\ngrounding module trained to translate these into actions using various tools in\nthe execution module. The design allows for modular upgrades and wider\napplicability to diverse interactive tasks. To foster generalizable agent\nlearning, we collect large-scale, unified, and high-quality training\nannotations derived from diverse ground-truth reasoning rationales across\nvarious complex interactive tasks. On 9 datasets, LUMOS exhibits several key\nadvantages: (1) LUMOS excels multiple larger open-source agents on the held-out\ndatasets (unused for training) for each task type. LUMOS even surpasses GPT\nagents on QA and web tasks; (2) LUMOS outperforms open-source agents produced\nby chain-of-thoughts and unmodularized integrated training; and (3) LUMOS\neffectively generalizes to unseen tasks, outperforming 33B-scale agents and\ndomain-specific agents.\n", "link": "http://arxiv.org/abs/2311.05657v3", "date": "2024-07-10", "relevancy": 2.1273, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5484}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5257}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Lumos%3A%20Unified%20and%20Modular%20Training%20for%20Open-Source%20Language%0A%20%20Agents&body=Title%3A%20Agent%20Lumos%3A%20Unified%20and%20Modular%20Training%20for%20Open-Source%20Language%0A%20%20Agents%0AAuthor%3A%20Da%20Yin%20and%20Faeze%20Brahman%20and%20Abhilasha%20Ravichander%20and%20Khyathi%20Chandu%20and%20Kai-Wei%20Chang%20and%20Yejin%20Choi%20and%20Bill%20Yuchen%20Lin%0AAbstract%3A%20%20%20Closed-source%20agents%20suffer%20from%20several%20issues%20such%20as%20a%20lack%20of%0Aaffordability%2C%20transparency%2C%20and%20reproducibility%2C%20particularly%20on%20complex%0Ainteractive%20tasks.%20This%20motivates%20the%20development%20of%20open-source%20alternatives.%0AWe%20introduce%20LUMOS%2C%20one%20of%20the%20first%20frameworks%20for%20training%20open-source%0ALLM-based%20agents.%20LUMOS%20features%20a%20learnable%2C%20unified%2C%20and%20modular%20architecture%0Awith%20a%20planning%20module%20that%20learns%20high-level%20subgoal%20generation%2C%20and%20a%0Agrounding%20module%20trained%20to%20translate%20these%20into%20actions%20using%20various%20tools%20in%0Athe%20execution%20module.%20The%20design%20allows%20for%20modular%20upgrades%20and%20wider%0Aapplicability%20to%20diverse%20interactive%20tasks.%20To%20foster%20generalizable%20agent%0Alearning%2C%20we%20collect%20large-scale%2C%20unified%2C%20and%20high-quality%20training%0Aannotations%20derived%20from%20diverse%20ground-truth%20reasoning%20rationales%20across%0Avarious%20complex%20interactive%20tasks.%20On%209%20datasets%2C%20LUMOS%20exhibits%20several%20key%0Aadvantages%3A%20%281%29%20LUMOS%20excels%20multiple%20larger%20open-source%20agents%20on%20the%20held-out%0Adatasets%20%28unused%20for%20training%29%20for%20each%20task%20type.%20LUMOS%20even%20surpasses%20GPT%0Aagents%20on%20QA%20and%20web%20tasks%3B%20%282%29%20LUMOS%20outperforms%20open-source%20agents%20produced%0Aby%20chain-of-thoughts%20and%20unmodularized%20integrated%20training%3B%20and%20%283%29%20LUMOS%0Aeffectively%20generalizes%20to%20unseen%20tasks%2C%20outperforming%2033B-scale%20agents%20and%0Adomain-specific%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05657v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Lumos%253A%2520Unified%2520and%2520Modular%2520Training%2520for%2520Open-Source%2520Language%250A%2520%2520Agents%26entry.906535625%3DDa%2520Yin%2520and%2520Faeze%2520Brahman%2520and%2520Abhilasha%2520Ravichander%2520and%2520Khyathi%2520Chandu%2520and%2520Kai-Wei%2520Chang%2520and%2520Yejin%2520Choi%2520and%2520Bill%2520Yuchen%2520Lin%26entry.1292438233%3D%2520%2520Closed-source%2520agents%2520suffer%2520from%2520several%2520issues%2520such%2520as%2520a%2520lack%2520of%250Aaffordability%252C%2520transparency%252C%2520and%2520reproducibility%252C%2520particularly%2520on%2520complex%250Ainteractive%2520tasks.%2520This%2520motivates%2520the%2520development%2520of%2520open-source%2520alternatives.%250AWe%2520introduce%2520LUMOS%252C%2520one%2520of%2520the%2520first%2520frameworks%2520for%2520training%2520open-source%250ALLM-based%2520agents.%2520LUMOS%2520features%2520a%2520learnable%252C%2520unified%252C%2520and%2520modular%2520architecture%250Awith%2520a%2520planning%2520module%2520that%2520learns%2520high-level%2520subgoal%2520generation%252C%2520and%2520a%250Agrounding%2520module%2520trained%2520to%2520translate%2520these%2520into%2520actions%2520using%2520various%2520tools%2520in%250Athe%2520execution%2520module.%2520The%2520design%2520allows%2520for%2520modular%2520upgrades%2520and%2520wider%250Aapplicability%2520to%2520diverse%2520interactive%2520tasks.%2520To%2520foster%2520generalizable%2520agent%250Alearning%252C%2520we%2520collect%2520large-scale%252C%2520unified%252C%2520and%2520high-quality%2520training%250Aannotations%2520derived%2520from%2520diverse%2520ground-truth%2520reasoning%2520rationales%2520across%250Avarious%2520complex%2520interactive%2520tasks.%2520On%25209%2520datasets%252C%2520LUMOS%2520exhibits%2520several%2520key%250Aadvantages%253A%2520%25281%2529%2520LUMOS%2520excels%2520multiple%2520larger%2520open-source%2520agents%2520on%2520the%2520held-out%250Adatasets%2520%2528unused%2520for%2520training%2529%2520for%2520each%2520task%2520type.%2520LUMOS%2520even%2520surpasses%2520GPT%250Aagents%2520on%2520QA%2520and%2520web%2520tasks%253B%2520%25282%2529%2520LUMOS%2520outperforms%2520open-source%2520agents%2520produced%250Aby%2520chain-of-thoughts%2520and%2520unmodularized%2520integrated%2520training%253B%2520and%2520%25283%2529%2520LUMOS%250Aeffectively%2520generalizes%2520to%2520unseen%2520tasks%252C%2520outperforming%252033B-scale%2520agents%2520and%250Adomain-specific%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.05657v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Lumos%3A%20Unified%20and%20Modular%20Training%20for%20Open-Source%20Language%0A%20%20Agents&entry.906535625=Da%20Yin%20and%20Faeze%20Brahman%20and%20Abhilasha%20Ravichander%20and%20Khyathi%20Chandu%20and%20Kai-Wei%20Chang%20and%20Yejin%20Choi%20and%20Bill%20Yuchen%20Lin&entry.1292438233=%20%20Closed-source%20agents%20suffer%20from%20several%20issues%20such%20as%20a%20lack%20of%0Aaffordability%2C%20transparency%2C%20and%20reproducibility%2C%20particularly%20on%20complex%0Ainteractive%20tasks.%20This%20motivates%20the%20development%20of%20open-source%20alternatives.%0AWe%20introduce%20LUMOS%2C%20one%20of%20the%20first%20frameworks%20for%20training%20open-source%0ALLM-based%20agents.%20LUMOS%20features%20a%20learnable%2C%20unified%2C%20and%20modular%20architecture%0Awith%20a%20planning%20module%20that%20learns%20high-level%20subgoal%20generation%2C%20and%20a%0Agrounding%20module%20trained%20to%20translate%20these%20into%20actions%20using%20various%20tools%20in%0Athe%20execution%20module.%20The%20design%20allows%20for%20modular%20upgrades%20and%20wider%0Aapplicability%20to%20diverse%20interactive%20tasks.%20To%20foster%20generalizable%20agent%0Alearning%2C%20we%20collect%20large-scale%2C%20unified%2C%20and%20high-quality%20training%0Aannotations%20derived%20from%20diverse%20ground-truth%20reasoning%20rationales%20across%0Avarious%20complex%20interactive%20tasks.%20On%209%20datasets%2C%20LUMOS%20exhibits%20several%20key%0Aadvantages%3A%20%281%29%20LUMOS%20excels%20multiple%20larger%20open-source%20agents%20on%20the%20held-out%0Adatasets%20%28unused%20for%20training%29%20for%20each%20task%20type.%20LUMOS%20even%20surpasses%20GPT%0Aagents%20on%20QA%20and%20web%20tasks%3B%20%282%29%20LUMOS%20outperforms%20open-source%20agents%20produced%0Aby%20chain-of-thoughts%20and%20unmodularized%20integrated%20training%3B%20and%20%283%29%20LUMOS%0Aeffectively%20generalizes%20to%20unseen%20tasks%2C%20outperforming%2033B-scale%20agents%20and%0Adomain-specific%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05657v3&entry.124074799=Read"},
{"title": "Deep-Graph-Sprints: Accelerated Representation Learning in\n  Continuous-Time Dynamic Graphs", "author": "Ahmad Naser Eddin and Jacopo Bono and David Apar\u00edcio and Hugo Ferreira and Pedro Ribeiro and Pedro Bizarro", "abstract": "  Continuous-time dynamic graphs (CTDGs) are essential for modeling\ninterconnected, evolving systems. Traditional methods for extracting knowledge\nfrom these graphs often depend on feature engineering or deep learning. Feature\nengineering is limited by the manual and time-intensive nature of crafting\nfeatures, while deep learning approaches suffer from high inference latency,\nmaking them impractical for real-time applications. This paper introduces\nDeep-Graph-Sprints (DGS), a novel deep learning architecture designed for\nefficient representation learning on CTDGs with low-latency inference\nrequirements. We benchmark DGS against state-of-the-art feature engineering and\ngraph neural network methods using five diverse datasets. The results indicate\nthat DGS achieves competitive performance while improving inference speed up to\n12x compared to other deep learning approaches on our tested benchmarks. Our\nmethod effectively bridges the gap between deep representation learning and\nlow-latency application requirements for CTDGs.\n", "link": "http://arxiv.org/abs/2407.07712v1", "date": "2024-07-10", "relevancy": 2.1206, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.542}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5283}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-Graph-Sprints%3A%20Accelerated%20Representation%20Learning%20in%0A%20%20Continuous-Time%20Dynamic%20Graphs&body=Title%3A%20Deep-Graph-Sprints%3A%20Accelerated%20Representation%20Learning%20in%0A%20%20Continuous-Time%20Dynamic%20Graphs%0AAuthor%3A%20Ahmad%20Naser%20Eddin%20and%20Jacopo%20Bono%20and%20David%20Apar%C3%ADcio%20and%20Hugo%20Ferreira%20and%20Pedro%20Ribeiro%20and%20Pedro%20Bizarro%0AAbstract%3A%20%20%20Continuous-time%20dynamic%20graphs%20%28CTDGs%29%20are%20essential%20for%20modeling%0Ainterconnected%2C%20evolving%20systems.%20Traditional%20methods%20for%20extracting%20knowledge%0Afrom%20these%20graphs%20often%20depend%20on%20feature%20engineering%20or%20deep%20learning.%20Feature%0Aengineering%20is%20limited%20by%20the%20manual%20and%20time-intensive%20nature%20of%20crafting%0Afeatures%2C%20while%20deep%20learning%20approaches%20suffer%20from%20high%20inference%20latency%2C%0Amaking%20them%20impractical%20for%20real-time%20applications.%20This%20paper%20introduces%0ADeep-Graph-Sprints%20%28DGS%29%2C%20a%20novel%20deep%20learning%20architecture%20designed%20for%0Aefficient%20representation%20learning%20on%20CTDGs%20with%20low-latency%20inference%0Arequirements.%20We%20benchmark%20DGS%20against%20state-of-the-art%20feature%20engineering%20and%0Agraph%20neural%20network%20methods%20using%20five%20diverse%20datasets.%20The%20results%20indicate%0Athat%20DGS%20achieves%20competitive%20performance%20while%20improving%20inference%20speed%20up%20to%0A12x%20compared%20to%20other%20deep%20learning%20approaches%20on%20our%20tested%20benchmarks.%20Our%0Amethod%20effectively%20bridges%20the%20gap%20between%20deep%20representation%20learning%20and%0Alow-latency%20application%20requirements%20for%20CTDGs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-Graph-Sprints%253A%2520Accelerated%2520Representation%2520Learning%2520in%250A%2520%2520Continuous-Time%2520Dynamic%2520Graphs%26entry.906535625%3DAhmad%2520Naser%2520Eddin%2520and%2520Jacopo%2520Bono%2520and%2520David%2520Apar%25C3%25ADcio%2520and%2520Hugo%2520Ferreira%2520and%2520Pedro%2520Ribeiro%2520and%2520Pedro%2520Bizarro%26entry.1292438233%3D%2520%2520Continuous-time%2520dynamic%2520graphs%2520%2528CTDGs%2529%2520are%2520essential%2520for%2520modeling%250Ainterconnected%252C%2520evolving%2520systems.%2520Traditional%2520methods%2520for%2520extracting%2520knowledge%250Afrom%2520these%2520graphs%2520often%2520depend%2520on%2520feature%2520engineering%2520or%2520deep%2520learning.%2520Feature%250Aengineering%2520is%2520limited%2520by%2520the%2520manual%2520and%2520time-intensive%2520nature%2520of%2520crafting%250Afeatures%252C%2520while%2520deep%2520learning%2520approaches%2520suffer%2520from%2520high%2520inference%2520latency%252C%250Amaking%2520them%2520impractical%2520for%2520real-time%2520applications.%2520This%2520paper%2520introduces%250ADeep-Graph-Sprints%2520%2528DGS%2529%252C%2520a%2520novel%2520deep%2520learning%2520architecture%2520designed%2520for%250Aefficient%2520representation%2520learning%2520on%2520CTDGs%2520with%2520low-latency%2520inference%250Arequirements.%2520We%2520benchmark%2520DGS%2520against%2520state-of-the-art%2520feature%2520engineering%2520and%250Agraph%2520neural%2520network%2520methods%2520using%2520five%2520diverse%2520datasets.%2520The%2520results%2520indicate%250Athat%2520DGS%2520achieves%2520competitive%2520performance%2520while%2520improving%2520inference%2520speed%2520up%2520to%250A12x%2520compared%2520to%2520other%2520deep%2520learning%2520approaches%2520on%2520our%2520tested%2520benchmarks.%2520Our%250Amethod%2520effectively%2520bridges%2520the%2520gap%2520between%2520deep%2520representation%2520learning%2520and%250Alow-latency%2520application%2520requirements%2520for%2520CTDGs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-Graph-Sprints%3A%20Accelerated%20Representation%20Learning%20in%0A%20%20Continuous-Time%20Dynamic%20Graphs&entry.906535625=Ahmad%20Naser%20Eddin%20and%20Jacopo%20Bono%20and%20David%20Apar%C3%ADcio%20and%20Hugo%20Ferreira%20and%20Pedro%20Ribeiro%20and%20Pedro%20Bizarro&entry.1292438233=%20%20Continuous-time%20dynamic%20graphs%20%28CTDGs%29%20are%20essential%20for%20modeling%0Ainterconnected%2C%20evolving%20systems.%20Traditional%20methods%20for%20extracting%20knowledge%0Afrom%20these%20graphs%20often%20depend%20on%20feature%20engineering%20or%20deep%20learning.%20Feature%0Aengineering%20is%20limited%20by%20the%20manual%20and%20time-intensive%20nature%20of%20crafting%0Afeatures%2C%20while%20deep%20learning%20approaches%20suffer%20from%20high%20inference%20latency%2C%0Amaking%20them%20impractical%20for%20real-time%20applications.%20This%20paper%20introduces%0ADeep-Graph-Sprints%20%28DGS%29%2C%20a%20novel%20deep%20learning%20architecture%20designed%20for%0Aefficient%20representation%20learning%20on%20CTDGs%20with%20low-latency%20inference%0Arequirements.%20We%20benchmark%20DGS%20against%20state-of-the-art%20feature%20engineering%20and%0Agraph%20neural%20network%20methods%20using%20five%20diverse%20datasets.%20The%20results%20indicate%0Athat%20DGS%20achieves%20competitive%20performance%20while%20improving%20inference%20speed%20up%20to%0A12x%20compared%20to%20other%20deep%20learning%20approaches%20on%20our%20tested%20benchmarks.%20Our%0Amethod%20effectively%20bridges%20the%20gap%20between%20deep%20representation%20learning%20and%0Alow-latency%20application%20requirements%20for%20CTDGs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07712v1&entry.124074799=Read"},
{"title": "HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical\n  Image Classification", "author": "Omar S. EL-Assiouti and Ghada Hamed and Dina Khattab and Hala M. Ebied", "abstract": "  Vision Transformers (ViTs) have achieved significant advancement in computer\nvision tasks due to their powerful modeling capacity. However, their\nperformance notably degrades when trained with insufficient data due to lack of\ninherent inductive biases. Distilling knowledge and inductive biases from a\nConvolutional Neural Network (CNN) teacher has emerged as an effective strategy\nfor enhancing the generalization of ViTs on limited datasets. Previous\napproaches to Knowledge Distillation (KD) have pursued two primary paths: some\nfocused solely on distilling the logit distribution from CNN teacher to ViT\nstudent, neglecting the rich semantic information present in intermediate\nfeatures due to the structural differences between them. Others integrated\nfeature distillation along with logit distillation, yet this introduced\nalignment operations that limits the amount of knowledge transferred due to\nmismatched architectures and increased the computational overhead. To this end,\nthis paper presents Hybrid Data-efficient Knowledge Distillation (HDKD)\nparadigm which employs a CNN teacher and a hybrid student. The choice of hybrid\nstudent serves two main aspects. First, it leverages the strengths of both\nconvolutions and transformers while sharing the convolutional structure with\nthe teacher model. Second, this shared structure enables the direct application\nof feature distillation without any information loss or additional\ncomputational overhead. Additionally, we propose an efficient light-weight\nconvolutional block named Mobile Channel-Spatial Attention (MBCSA), which\nserves as the primary convolutional block in both teacher and student models.\nExtensive experiments on two medical public datasets showcase the superiority\nof HDKD over other state-of-the-art models and its computational efficiency.\nSource code at: https://github.com/omarsherif200/HDKD\n", "link": "http://arxiv.org/abs/2407.07516v1", "date": "2024-07-10", "relevancy": 2.1205, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5379}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5301}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HDKD%3A%20Hybrid%20Data-Efficient%20Knowledge%20Distillation%20Network%20for%20Medical%0A%20%20Image%20Classification&body=Title%3A%20HDKD%3A%20Hybrid%20Data-Efficient%20Knowledge%20Distillation%20Network%20for%20Medical%0A%20%20Image%20Classification%0AAuthor%3A%20Omar%20S.%20EL-Assiouti%20and%20Ghada%20Hamed%20and%20Dina%20Khattab%20and%20Hala%20M.%20Ebied%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20significant%20advancement%20in%20computer%0Avision%20tasks%20due%20to%20their%20powerful%20modeling%20capacity.%20However%2C%20their%0Aperformance%20notably%20degrades%20when%20trained%20with%20insufficient%20data%20due%20to%20lack%20of%0Ainherent%20inductive%20biases.%20Distilling%20knowledge%20and%20inductive%20biases%20from%20a%0AConvolutional%20Neural%20Network%20%28CNN%29%20teacher%20has%20emerged%20as%20an%20effective%20strategy%0Afor%20enhancing%20the%20generalization%20of%20ViTs%20on%20limited%20datasets.%20Previous%0Aapproaches%20to%20Knowledge%20Distillation%20%28KD%29%20have%20pursued%20two%20primary%20paths%3A%20some%0Afocused%20solely%20on%20distilling%20the%20logit%20distribution%20from%20CNN%20teacher%20to%20ViT%0Astudent%2C%20neglecting%20the%20rich%20semantic%20information%20present%20in%20intermediate%0Afeatures%20due%20to%20the%20structural%20differences%20between%20them.%20Others%20integrated%0Afeature%20distillation%20along%20with%20logit%20distillation%2C%20yet%20this%20introduced%0Aalignment%20operations%20that%20limits%20the%20amount%20of%20knowledge%20transferred%20due%20to%0Amismatched%20architectures%20and%20increased%20the%20computational%20overhead.%20To%20this%20end%2C%0Athis%20paper%20presents%20Hybrid%20Data-efficient%20Knowledge%20Distillation%20%28HDKD%29%0Aparadigm%20which%20employs%20a%20CNN%20teacher%20and%20a%20hybrid%20student.%20The%20choice%20of%20hybrid%0Astudent%20serves%20two%20main%20aspects.%20First%2C%20it%20leverages%20the%20strengths%20of%20both%0Aconvolutions%20and%20transformers%20while%20sharing%20the%20convolutional%20structure%20with%0Athe%20teacher%20model.%20Second%2C%20this%20shared%20structure%20enables%20the%20direct%20application%0Aof%20feature%20distillation%20without%20any%20information%20loss%20or%20additional%0Acomputational%20overhead.%20Additionally%2C%20we%20propose%20an%20efficient%20light-weight%0Aconvolutional%20block%20named%20Mobile%20Channel-Spatial%20Attention%20%28MBCSA%29%2C%20which%0Aserves%20as%20the%20primary%20convolutional%20block%20in%20both%20teacher%20and%20student%20models.%0AExtensive%20experiments%20on%20two%20medical%20public%20datasets%20showcase%20the%20superiority%0Aof%20HDKD%20over%20other%20state-of-the-art%20models%20and%20its%20computational%20efficiency.%0ASource%20code%20at%3A%20https%3A//github.com/omarsherif200/HDKD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHDKD%253A%2520Hybrid%2520Data-Efficient%2520Knowledge%2520Distillation%2520Network%2520for%2520Medical%250A%2520%2520Image%2520Classification%26entry.906535625%3DOmar%2520S.%2520EL-Assiouti%2520and%2520Ghada%2520Hamed%2520and%2520Dina%2520Khattab%2520and%2520Hala%2520M.%2520Ebied%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520achieved%2520significant%2520advancement%2520in%2520computer%250Avision%2520tasks%2520due%2520to%2520their%2520powerful%2520modeling%2520capacity.%2520However%252C%2520their%250Aperformance%2520notably%2520degrades%2520when%2520trained%2520with%2520insufficient%2520data%2520due%2520to%2520lack%2520of%250Ainherent%2520inductive%2520biases.%2520Distilling%2520knowledge%2520and%2520inductive%2520biases%2520from%2520a%250AConvolutional%2520Neural%2520Network%2520%2528CNN%2529%2520teacher%2520has%2520emerged%2520as%2520an%2520effective%2520strategy%250Afor%2520enhancing%2520the%2520generalization%2520of%2520ViTs%2520on%2520limited%2520datasets.%2520Previous%250Aapproaches%2520to%2520Knowledge%2520Distillation%2520%2528KD%2529%2520have%2520pursued%2520two%2520primary%2520paths%253A%2520some%250Afocused%2520solely%2520on%2520distilling%2520the%2520logit%2520distribution%2520from%2520CNN%2520teacher%2520to%2520ViT%250Astudent%252C%2520neglecting%2520the%2520rich%2520semantic%2520information%2520present%2520in%2520intermediate%250Afeatures%2520due%2520to%2520the%2520structural%2520differences%2520between%2520them.%2520Others%2520integrated%250Afeature%2520distillation%2520along%2520with%2520logit%2520distillation%252C%2520yet%2520this%2520introduced%250Aalignment%2520operations%2520that%2520limits%2520the%2520amount%2520of%2520knowledge%2520transferred%2520due%2520to%250Amismatched%2520architectures%2520and%2520increased%2520the%2520computational%2520overhead.%2520To%2520this%2520end%252C%250Athis%2520paper%2520presents%2520Hybrid%2520Data-efficient%2520Knowledge%2520Distillation%2520%2528HDKD%2529%250Aparadigm%2520which%2520employs%2520a%2520CNN%2520teacher%2520and%2520a%2520hybrid%2520student.%2520The%2520choice%2520of%2520hybrid%250Astudent%2520serves%2520two%2520main%2520aspects.%2520First%252C%2520it%2520leverages%2520the%2520strengths%2520of%2520both%250Aconvolutions%2520and%2520transformers%2520while%2520sharing%2520the%2520convolutional%2520structure%2520with%250Athe%2520teacher%2520model.%2520Second%252C%2520this%2520shared%2520structure%2520enables%2520the%2520direct%2520application%250Aof%2520feature%2520distillation%2520without%2520any%2520information%2520loss%2520or%2520additional%250Acomputational%2520overhead.%2520Additionally%252C%2520we%2520propose%2520an%2520efficient%2520light-weight%250Aconvolutional%2520block%2520named%2520Mobile%2520Channel-Spatial%2520Attention%2520%2528MBCSA%2529%252C%2520which%250Aserves%2520as%2520the%2520primary%2520convolutional%2520block%2520in%2520both%2520teacher%2520and%2520student%2520models.%250AExtensive%2520experiments%2520on%2520two%2520medical%2520public%2520datasets%2520showcase%2520the%2520superiority%250Aof%2520HDKD%2520over%2520other%2520state-of-the-art%2520models%2520and%2520its%2520computational%2520efficiency.%250ASource%2520code%2520at%253A%2520https%253A//github.com/omarsherif200/HDKD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HDKD%3A%20Hybrid%20Data-Efficient%20Knowledge%20Distillation%20Network%20for%20Medical%0A%20%20Image%20Classification&entry.906535625=Omar%20S.%20EL-Assiouti%20and%20Ghada%20Hamed%20and%20Dina%20Khattab%20and%20Hala%20M.%20Ebied&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20significant%20advancement%20in%20computer%0Avision%20tasks%20due%20to%20their%20powerful%20modeling%20capacity.%20However%2C%20their%0Aperformance%20notably%20degrades%20when%20trained%20with%20insufficient%20data%20due%20to%20lack%20of%0Ainherent%20inductive%20biases.%20Distilling%20knowledge%20and%20inductive%20biases%20from%20a%0AConvolutional%20Neural%20Network%20%28CNN%29%20teacher%20has%20emerged%20as%20an%20effective%20strategy%0Afor%20enhancing%20the%20generalization%20of%20ViTs%20on%20limited%20datasets.%20Previous%0Aapproaches%20to%20Knowledge%20Distillation%20%28KD%29%20have%20pursued%20two%20primary%20paths%3A%20some%0Afocused%20solely%20on%20distilling%20the%20logit%20distribution%20from%20CNN%20teacher%20to%20ViT%0Astudent%2C%20neglecting%20the%20rich%20semantic%20information%20present%20in%20intermediate%0Afeatures%20due%20to%20the%20structural%20differences%20between%20them.%20Others%20integrated%0Afeature%20distillation%20along%20with%20logit%20distillation%2C%20yet%20this%20introduced%0Aalignment%20operations%20that%20limits%20the%20amount%20of%20knowledge%20transferred%20due%20to%0Amismatched%20architectures%20and%20increased%20the%20computational%20overhead.%20To%20this%20end%2C%0Athis%20paper%20presents%20Hybrid%20Data-efficient%20Knowledge%20Distillation%20%28HDKD%29%0Aparadigm%20which%20employs%20a%20CNN%20teacher%20and%20a%20hybrid%20student.%20The%20choice%20of%20hybrid%0Astudent%20serves%20two%20main%20aspects.%20First%2C%20it%20leverages%20the%20strengths%20of%20both%0Aconvolutions%20and%20transformers%20while%20sharing%20the%20convolutional%20structure%20with%0Athe%20teacher%20model.%20Second%2C%20this%20shared%20structure%20enables%20the%20direct%20application%0Aof%20feature%20distillation%20without%20any%20information%20loss%20or%20additional%0Acomputational%20overhead.%20Additionally%2C%20we%20propose%20an%20efficient%20light-weight%0Aconvolutional%20block%20named%20Mobile%20Channel-Spatial%20Attention%20%28MBCSA%29%2C%20which%0Aserves%20as%20the%20primary%20convolutional%20block%20in%20both%20teacher%20and%20student%20models.%0AExtensive%20experiments%20on%20two%20medical%20public%20datasets%20showcase%20the%20superiority%0Aof%20HDKD%20over%20other%20state-of-the-art%20models%20and%20its%20computational%20efficiency.%0ASource%20code%20at%3A%20https%3A//github.com/omarsherif200/HDKD%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07516v1&entry.124074799=Read"},
{"title": "How to Leverage Predictive Uncertainty Estimates for Reducing\n  Catastrophic Forgetting in Online Continual Learning", "author": "Giuseppe Serra and Ben Werner and Florian Buettner", "abstract": "  Many real-world applications require machine-learning models to be able to\ndeal with non-stationary data distributions and thus learn autonomously over an\nextended period of time, often in an online setting. One of the main challenges\nin this scenario is the so-called catastrophic forgetting (CF) for which the\nlearning model tends to focus on the most recent tasks while experiencing\npredictive degradation on older ones. In the online setting, the most effective\nsolutions employ a fixed-size memory buffer to store old samples used for\nreplay when training on new tasks. Many approaches have been presented to\ntackle this problem. However, it is not clear how predictive uncertainty\ninformation for memory management can be leveraged in the most effective manner\nand conflicting strategies are proposed to populate the memory. Are the\neasiest-to-forget or the easiest-to-remember samples more effective in\ncombating CF? Starting from the intuition that predictive uncertainty provides\nan idea of the samples' location in the decision space, this work presents an\nin-depth analysis of different uncertainty estimates and strategies for\npopulating the memory. The investigation provides a better understanding of the\ncharacteristics data points should have for alleviating CF. Then, we propose an\nalternative method for estimating predictive uncertainty via the generalised\nvariance induced by the negative log-likelihood. Finally, we demonstrate that\nthe use of predictive uncertainty measures helps in reducing CF in different\nsettings.\n", "link": "http://arxiv.org/abs/2407.07668v1", "date": "2024-07-10", "relevancy": 2.1171, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5314}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.529}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Leverage%20Predictive%20Uncertainty%20Estimates%20for%20Reducing%0A%20%20Catastrophic%20Forgetting%20in%20Online%20Continual%20Learning&body=Title%3A%20How%20to%20Leverage%20Predictive%20Uncertainty%20Estimates%20for%20Reducing%0A%20%20Catastrophic%20Forgetting%20in%20Online%20Continual%20Learning%0AAuthor%3A%20Giuseppe%20Serra%20and%20Ben%20Werner%20and%20Florian%20Buettner%0AAbstract%3A%20%20%20Many%20real-world%20applications%20require%20machine-learning%20models%20to%20be%20able%20to%0Adeal%20with%20non-stationary%20data%20distributions%20and%20thus%20learn%20autonomously%20over%20an%0Aextended%20period%20of%20time%2C%20often%20in%20an%20online%20setting.%20One%20of%20the%20main%20challenges%0Ain%20this%20scenario%20is%20the%20so-called%20catastrophic%20forgetting%20%28CF%29%20for%20which%20the%0Alearning%20model%20tends%20to%20focus%20on%20the%20most%20recent%20tasks%20while%20experiencing%0Apredictive%20degradation%20on%20older%20ones.%20In%20the%20online%20setting%2C%20the%20most%20effective%0Asolutions%20employ%20a%20fixed-size%20memory%20buffer%20to%20store%20old%20samples%20used%20for%0Areplay%20when%20training%20on%20new%20tasks.%20Many%20approaches%20have%20been%20presented%20to%0Atackle%20this%20problem.%20However%2C%20it%20is%20not%20clear%20how%20predictive%20uncertainty%0Ainformation%20for%20memory%20management%20can%20be%20leveraged%20in%20the%20most%20effective%20manner%0Aand%20conflicting%20strategies%20are%20proposed%20to%20populate%20the%20memory.%20Are%20the%0Aeasiest-to-forget%20or%20the%20easiest-to-remember%20samples%20more%20effective%20in%0Acombating%20CF%3F%20Starting%20from%20the%20intuition%20that%20predictive%20uncertainty%20provides%0Aan%20idea%20of%20the%20samples%27%20location%20in%20the%20decision%20space%2C%20this%20work%20presents%20an%0Ain-depth%20analysis%20of%20different%20uncertainty%20estimates%20and%20strategies%20for%0Apopulating%20the%20memory.%20The%20investigation%20provides%20a%20better%20understanding%20of%20the%0Acharacteristics%20data%20points%20should%20have%20for%20alleviating%20CF.%20Then%2C%20we%20propose%20an%0Aalternative%20method%20for%20estimating%20predictive%20uncertainty%20via%20the%20generalised%0Avariance%20induced%20by%20the%20negative%20log-likelihood.%20Finally%2C%20we%20demonstrate%20that%0Athe%20use%20of%20predictive%20uncertainty%20measures%20helps%20in%20reducing%20CF%20in%20different%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Leverage%2520Predictive%2520Uncertainty%2520Estimates%2520for%2520Reducing%250A%2520%2520Catastrophic%2520Forgetting%2520in%2520Online%2520Continual%2520Learning%26entry.906535625%3DGiuseppe%2520Serra%2520and%2520Ben%2520Werner%2520and%2520Florian%2520Buettner%26entry.1292438233%3D%2520%2520Many%2520real-world%2520applications%2520require%2520machine-learning%2520models%2520to%2520be%2520able%2520to%250Adeal%2520with%2520non-stationary%2520data%2520distributions%2520and%2520thus%2520learn%2520autonomously%2520over%2520an%250Aextended%2520period%2520of%2520time%252C%2520often%2520in%2520an%2520online%2520setting.%2520One%2520of%2520the%2520main%2520challenges%250Ain%2520this%2520scenario%2520is%2520the%2520so-called%2520catastrophic%2520forgetting%2520%2528CF%2529%2520for%2520which%2520the%250Alearning%2520model%2520tends%2520to%2520focus%2520on%2520the%2520most%2520recent%2520tasks%2520while%2520experiencing%250Apredictive%2520degradation%2520on%2520older%2520ones.%2520In%2520the%2520online%2520setting%252C%2520the%2520most%2520effective%250Asolutions%2520employ%2520a%2520fixed-size%2520memory%2520buffer%2520to%2520store%2520old%2520samples%2520used%2520for%250Areplay%2520when%2520training%2520on%2520new%2520tasks.%2520Many%2520approaches%2520have%2520been%2520presented%2520to%250Atackle%2520this%2520problem.%2520However%252C%2520it%2520is%2520not%2520clear%2520how%2520predictive%2520uncertainty%250Ainformation%2520for%2520memory%2520management%2520can%2520be%2520leveraged%2520in%2520the%2520most%2520effective%2520manner%250Aand%2520conflicting%2520strategies%2520are%2520proposed%2520to%2520populate%2520the%2520memory.%2520Are%2520the%250Aeasiest-to-forget%2520or%2520the%2520easiest-to-remember%2520samples%2520more%2520effective%2520in%250Acombating%2520CF%253F%2520Starting%2520from%2520the%2520intuition%2520that%2520predictive%2520uncertainty%2520provides%250Aan%2520idea%2520of%2520the%2520samples%2527%2520location%2520in%2520the%2520decision%2520space%252C%2520this%2520work%2520presents%2520an%250Ain-depth%2520analysis%2520of%2520different%2520uncertainty%2520estimates%2520and%2520strategies%2520for%250Apopulating%2520the%2520memory.%2520The%2520investigation%2520provides%2520a%2520better%2520understanding%2520of%2520the%250Acharacteristics%2520data%2520points%2520should%2520have%2520for%2520alleviating%2520CF.%2520Then%252C%2520we%2520propose%2520an%250Aalternative%2520method%2520for%2520estimating%2520predictive%2520uncertainty%2520via%2520the%2520generalised%250Avariance%2520induced%2520by%2520the%2520negative%2520log-likelihood.%2520Finally%252C%2520we%2520demonstrate%2520that%250Athe%2520use%2520of%2520predictive%2520uncertainty%2520measures%2520helps%2520in%2520reducing%2520CF%2520in%2520different%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Leverage%20Predictive%20Uncertainty%20Estimates%20for%20Reducing%0A%20%20Catastrophic%20Forgetting%20in%20Online%20Continual%20Learning&entry.906535625=Giuseppe%20Serra%20and%20Ben%20Werner%20and%20Florian%20Buettner&entry.1292438233=%20%20Many%20real-world%20applications%20require%20machine-learning%20models%20to%20be%20able%20to%0Adeal%20with%20non-stationary%20data%20distributions%20and%20thus%20learn%20autonomously%20over%20an%0Aextended%20period%20of%20time%2C%20often%20in%20an%20online%20setting.%20One%20of%20the%20main%20challenges%0Ain%20this%20scenario%20is%20the%20so-called%20catastrophic%20forgetting%20%28CF%29%20for%20which%20the%0Alearning%20model%20tends%20to%20focus%20on%20the%20most%20recent%20tasks%20while%20experiencing%0Apredictive%20degradation%20on%20older%20ones.%20In%20the%20online%20setting%2C%20the%20most%20effective%0Asolutions%20employ%20a%20fixed-size%20memory%20buffer%20to%20store%20old%20samples%20used%20for%0Areplay%20when%20training%20on%20new%20tasks.%20Many%20approaches%20have%20been%20presented%20to%0Atackle%20this%20problem.%20However%2C%20it%20is%20not%20clear%20how%20predictive%20uncertainty%0Ainformation%20for%20memory%20management%20can%20be%20leveraged%20in%20the%20most%20effective%20manner%0Aand%20conflicting%20strategies%20are%20proposed%20to%20populate%20the%20memory.%20Are%20the%0Aeasiest-to-forget%20or%20the%20easiest-to-remember%20samples%20more%20effective%20in%0Acombating%20CF%3F%20Starting%20from%20the%20intuition%20that%20predictive%20uncertainty%20provides%0Aan%20idea%20of%20the%20samples%27%20location%20in%20the%20decision%20space%2C%20this%20work%20presents%20an%0Ain-depth%20analysis%20of%20different%20uncertainty%20estimates%20and%20strategies%20for%0Apopulating%20the%20memory.%20The%20investigation%20provides%20a%20better%20understanding%20of%20the%0Acharacteristics%20data%20points%20should%20have%20for%20alleviating%20CF.%20Then%2C%20we%20propose%20an%0Aalternative%20method%20for%20estimating%20predictive%20uncertainty%20via%20the%20generalised%0Avariance%20induced%20by%20the%20negative%20log-likelihood.%20Finally%2C%20we%20demonstrate%20that%0Athe%20use%20of%20predictive%20uncertainty%20measures%20helps%20in%20reducing%20CF%20in%20different%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07668v1&entry.124074799=Read"},
{"title": "NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time-Series\n  Pretraining", "author": "Chenguo Lin and Xumeng Wen and Wei Cao and Congrui Huang and Jiang Bian and Stephen Lin and Zhirong Wu", "abstract": "  Recent research on time-series self-supervised models shows great promise in\nlearning semantic representations. However, it has been limited to small-scale\ndatasets, e.g., thousands of temporal sequences. In this work, we make key\ntechnical contributions that are tailored to the numerical properties of\ntime-series data and allow the model to scale to large datasets, e.g., millions\nof temporal sequences. We adopt the Transformer architecture by first\npartitioning the input into non-overlapping windows. Each window is then\ncharacterized by its normalized shape and two scalar values denoting the mean\nand standard deviation within each window. To embed scalar values that may\npossess arbitrary numerical amplitudes in a high-dimensional space, we propose\na numerically multi-scaled embedding module enumerating all possible numerical\nscales for the scalars. The model undergoes pretraining with a simple\ncontrastive objective on a large-scale dataset over a million sequences\ncollected by merging existing public data. We study its transfer performance on\na number of univariate and multivariate classification tasks, few shot\nlearning, unsupervised clustering and anomaly detection benchmarks. Our method\nexhibits remarkable improvement against previous pretraining approaches and\nestablishes the new state of the art, even compared with domain-specific\nnon-learning-based methods. Code is available at:\n\\url{https://github.com/chenguolin/NuTime}.\n", "link": "http://arxiv.org/abs/2310.07402v3", "date": "2024-07-10", "relevancy": 2.1157, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5402}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NuTime%3A%20Numerically%20Multi-Scaled%20Embedding%20for%20Large-Scale%20Time-Series%0A%20%20Pretraining&body=Title%3A%20NuTime%3A%20Numerically%20Multi-Scaled%20Embedding%20for%20Large-Scale%20Time-Series%0A%20%20Pretraining%0AAuthor%3A%20Chenguo%20Lin%20and%20Xumeng%20Wen%20and%20Wei%20Cao%20and%20Congrui%20Huang%20and%20Jiang%20Bian%20and%20Stephen%20Lin%20and%20Zhirong%20Wu%0AAbstract%3A%20%20%20Recent%20research%20on%20time-series%20self-supervised%20models%20shows%20great%20promise%20in%0Alearning%20semantic%20representations.%20However%2C%20it%20has%20been%20limited%20to%20small-scale%0Adatasets%2C%20e.g.%2C%20thousands%20of%20temporal%20sequences.%20In%20this%20work%2C%20we%20make%20key%0Atechnical%20contributions%20that%20are%20tailored%20to%20the%20numerical%20properties%20of%0Atime-series%20data%20and%20allow%20the%20model%20to%20scale%20to%20large%20datasets%2C%20e.g.%2C%20millions%0Aof%20temporal%20sequences.%20We%20adopt%20the%20Transformer%20architecture%20by%20first%0Apartitioning%20the%20input%20into%20non-overlapping%20windows.%20Each%20window%20is%20then%0Acharacterized%20by%20its%20normalized%20shape%20and%20two%20scalar%20values%20denoting%20the%20mean%0Aand%20standard%20deviation%20within%20each%20window.%20To%20embed%20scalar%20values%20that%20may%0Apossess%20arbitrary%20numerical%20amplitudes%20in%20a%20high-dimensional%20space%2C%20we%20propose%0Aa%20numerically%20multi-scaled%20embedding%20module%20enumerating%20all%20possible%20numerical%0Ascales%20for%20the%20scalars.%20The%20model%20undergoes%20pretraining%20with%20a%20simple%0Acontrastive%20objective%20on%20a%20large-scale%20dataset%20over%20a%20million%20sequences%0Acollected%20by%20merging%20existing%20public%20data.%20We%20study%20its%20transfer%20performance%20on%0Aa%20number%20of%20univariate%20and%20multivariate%20classification%20tasks%2C%20few%20shot%0Alearning%2C%20unsupervised%20clustering%20and%20anomaly%20detection%20benchmarks.%20Our%20method%0Aexhibits%20remarkable%20improvement%20against%20previous%20pretraining%20approaches%20and%0Aestablishes%20the%20new%20state%20of%20the%20art%2C%20even%20compared%20with%20domain-specific%0Anon-learning-based%20methods.%20Code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/chenguolin/NuTime%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07402v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNuTime%253A%2520Numerically%2520Multi-Scaled%2520Embedding%2520for%2520Large-Scale%2520Time-Series%250A%2520%2520Pretraining%26entry.906535625%3DChenguo%2520Lin%2520and%2520Xumeng%2520Wen%2520and%2520Wei%2520Cao%2520and%2520Congrui%2520Huang%2520and%2520Jiang%2520Bian%2520and%2520Stephen%2520Lin%2520and%2520Zhirong%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520research%2520on%2520time-series%2520self-supervised%2520models%2520shows%2520great%2520promise%2520in%250Alearning%2520semantic%2520representations.%2520However%252C%2520it%2520has%2520been%2520limited%2520to%2520small-scale%250Adatasets%252C%2520e.g.%252C%2520thousands%2520of%2520temporal%2520sequences.%2520In%2520this%2520work%252C%2520we%2520make%2520key%250Atechnical%2520contributions%2520that%2520are%2520tailored%2520to%2520the%2520numerical%2520properties%2520of%250Atime-series%2520data%2520and%2520allow%2520the%2520model%2520to%2520scale%2520to%2520large%2520datasets%252C%2520e.g.%252C%2520millions%250Aof%2520temporal%2520sequences.%2520We%2520adopt%2520the%2520Transformer%2520architecture%2520by%2520first%250Apartitioning%2520the%2520input%2520into%2520non-overlapping%2520windows.%2520Each%2520window%2520is%2520then%250Acharacterized%2520by%2520its%2520normalized%2520shape%2520and%2520two%2520scalar%2520values%2520denoting%2520the%2520mean%250Aand%2520standard%2520deviation%2520within%2520each%2520window.%2520To%2520embed%2520scalar%2520values%2520that%2520may%250Apossess%2520arbitrary%2520numerical%2520amplitudes%2520in%2520a%2520high-dimensional%2520space%252C%2520we%2520propose%250Aa%2520numerically%2520multi-scaled%2520embedding%2520module%2520enumerating%2520all%2520possible%2520numerical%250Ascales%2520for%2520the%2520scalars.%2520The%2520model%2520undergoes%2520pretraining%2520with%2520a%2520simple%250Acontrastive%2520objective%2520on%2520a%2520large-scale%2520dataset%2520over%2520a%2520million%2520sequences%250Acollected%2520by%2520merging%2520existing%2520public%2520data.%2520We%2520study%2520its%2520transfer%2520performance%2520on%250Aa%2520number%2520of%2520univariate%2520and%2520multivariate%2520classification%2520tasks%252C%2520few%2520shot%250Alearning%252C%2520unsupervised%2520clustering%2520and%2520anomaly%2520detection%2520benchmarks.%2520Our%2520method%250Aexhibits%2520remarkable%2520improvement%2520against%2520previous%2520pretraining%2520approaches%2520and%250Aestablishes%2520the%2520new%2520state%2520of%2520the%2520art%252C%2520even%2520compared%2520with%2520domain-specific%250Anon-learning-based%2520methods.%2520Code%2520is%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/chenguolin/NuTime%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07402v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NuTime%3A%20Numerically%20Multi-Scaled%20Embedding%20for%20Large-Scale%20Time-Series%0A%20%20Pretraining&entry.906535625=Chenguo%20Lin%20and%20Xumeng%20Wen%20and%20Wei%20Cao%20and%20Congrui%20Huang%20and%20Jiang%20Bian%20and%20Stephen%20Lin%20and%20Zhirong%20Wu&entry.1292438233=%20%20Recent%20research%20on%20time-series%20self-supervised%20models%20shows%20great%20promise%20in%0Alearning%20semantic%20representations.%20However%2C%20it%20has%20been%20limited%20to%20small-scale%0Adatasets%2C%20e.g.%2C%20thousands%20of%20temporal%20sequences.%20In%20this%20work%2C%20we%20make%20key%0Atechnical%20contributions%20that%20are%20tailored%20to%20the%20numerical%20properties%20of%0Atime-series%20data%20and%20allow%20the%20model%20to%20scale%20to%20large%20datasets%2C%20e.g.%2C%20millions%0Aof%20temporal%20sequences.%20We%20adopt%20the%20Transformer%20architecture%20by%20first%0Apartitioning%20the%20input%20into%20non-overlapping%20windows.%20Each%20window%20is%20then%0Acharacterized%20by%20its%20normalized%20shape%20and%20two%20scalar%20values%20denoting%20the%20mean%0Aand%20standard%20deviation%20within%20each%20window.%20To%20embed%20scalar%20values%20that%20may%0Apossess%20arbitrary%20numerical%20amplitudes%20in%20a%20high-dimensional%20space%2C%20we%20propose%0Aa%20numerically%20multi-scaled%20embedding%20module%20enumerating%20all%20possible%20numerical%0Ascales%20for%20the%20scalars.%20The%20model%20undergoes%20pretraining%20with%20a%20simple%0Acontrastive%20objective%20on%20a%20large-scale%20dataset%20over%20a%20million%20sequences%0Acollected%20by%20merging%20existing%20public%20data.%20We%20study%20its%20transfer%20performance%20on%0Aa%20number%20of%20univariate%20and%20multivariate%20classification%20tasks%2C%20few%20shot%0Alearning%2C%20unsupervised%20clustering%20and%20anomaly%20detection%20benchmarks.%20Our%20method%0Aexhibits%20remarkable%20improvement%20against%20previous%20pretraining%20approaches%20and%0Aestablishes%20the%20new%20state%20of%20the%20art%2C%20even%20compared%20with%20domain-specific%0Anon-learning-based%20methods.%20Code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/chenguolin/NuTime%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07402v3&entry.124074799=Read"},
{"title": "MMLongBench-Doc: Benchmarking Long-context Document Understanding with\n  Visualizations", "author": "Yubo Ma and Yuhang Zang and Liangyu Chen and Meiqi Chen and Yizhu Jiao and Xinze Li and Xinyuan Lu and Ziyu Liu and Yan Ma and Xiaoyi Dong and Pan Zhang and Liangming Pan and Yu-Gang Jiang and Jiaqi Wang and Yixin Cao and Aixin Sun", "abstract": "  Understanding documents with rich layouts and multi-modal components is a\nlong-standing and practical task. Recent Large Vision-Language Models (LVLMs)\nhave made remarkable strides in various tasks, particularly in single-page\ndocument understanding (DU). However, their abilities on long-context DU remain\nan open problem. This work presents MMLongBench-Doc, a long-context,\nmulti-modal benchmark comprising 1,062 expert-annotated questions. Distinct\nfrom previous datasets, it is constructed upon 130 lengthy PDF-formatted\ndocuments with an average of 49.4 pages and 20,971 textual tokens. Towards\ncomprehensive evaluation, answers to these questions rely on pieces of evidence\nfrom (1) different sources (text, image, chart, table, and layout structure)\nand (2) various locations (i.e. page number). Moreover, 33.2% of the questions\nare cross-page questions requiring evidence across multiple pages. 22.8% of the\nquestions are designed to be unanswerable for detecting potential\nhallucinations. Experiments on 14 LVLMs demonstrate that long-context DU\ngreatly challenges current models. Notably, the best-performing model, GPT-4o,\nachieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores\n31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse\nperformance than their LLM counterparts which are fed with lossy-parsed OCR\ndocuments. These results validate the necessity of future research toward more\ncapable long-context LVLMs. Project Page:\nhttps://mayubo2333.github.io/MMLongBench-Doc\n", "link": "http://arxiv.org/abs/2407.01523v2", "date": "2024-07-10", "relevancy": 2.1077, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5626}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMLongBench-Doc%3A%20Benchmarking%20Long-context%20Document%20Understanding%20with%0A%20%20Visualizations&body=Title%3A%20MMLongBench-Doc%3A%20Benchmarking%20Long-context%20Document%20Understanding%20with%0A%20%20Visualizations%0AAuthor%3A%20Yubo%20Ma%20and%20Yuhang%20Zang%20and%20Liangyu%20Chen%20and%20Meiqi%20Chen%20and%20Yizhu%20Jiao%20and%20Xinze%20Li%20and%20Xinyuan%20Lu%20and%20Ziyu%20Liu%20and%20Yan%20Ma%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Liangming%20Pan%20and%20Yu-Gang%20Jiang%20and%20Jiaqi%20Wang%20and%20Yixin%20Cao%20and%20Aixin%20Sun%0AAbstract%3A%20%20%20Understanding%20documents%20with%20rich%20layouts%20and%20multi-modal%20components%20is%20a%0Along-standing%20and%20practical%20task.%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%0Ahave%20made%20remarkable%20strides%20in%20various%20tasks%2C%20particularly%20in%20single-page%0Adocument%20understanding%20%28DU%29.%20However%2C%20their%20abilities%20on%20long-context%20DU%20remain%0Aan%20open%20problem.%20This%20work%20presents%20MMLongBench-Doc%2C%20a%20long-context%2C%0Amulti-modal%20benchmark%20comprising%201%2C062%20expert-annotated%20questions.%20Distinct%0Afrom%20previous%20datasets%2C%20it%20is%20constructed%20upon%20130%20lengthy%20PDF-formatted%0Adocuments%20with%20an%20average%20of%2049.4%20pages%20and%2020%2C971%20textual%20tokens.%20Towards%0Acomprehensive%20evaluation%2C%20answers%20to%20these%20questions%20rely%20on%20pieces%20of%20evidence%0Afrom%20%281%29%20different%20sources%20%28text%2C%20image%2C%20chart%2C%20table%2C%20and%20layout%20structure%29%0Aand%20%282%29%20various%20locations%20%28i.e.%20page%20number%29.%20Moreover%2C%2033.2%25%20of%20the%20questions%0Aare%20cross-page%20questions%20requiring%20evidence%20across%20multiple%20pages.%2022.8%25%20of%20the%0Aquestions%20are%20designed%20to%20be%20unanswerable%20for%20detecting%20potential%0Ahallucinations.%20Experiments%20on%2014%20LVLMs%20demonstrate%20that%20long-context%20DU%0Agreatly%20challenges%20current%20models.%20Notably%2C%20the%20best-performing%20model%2C%20GPT-4o%2C%0Aachieves%20an%20F1%20score%20of%20only%2042.7%25%2C%20while%20the%20second-best%2C%20GPT-4V%2C%20scores%0A31.4%25.%20Furthermore%2C%2012%20LVLMs%20%28all%20except%20GPT-4o%20and%20GPT-4V%29%20even%20present%20worse%0Aperformance%20than%20their%20LLM%20counterparts%20which%20are%20fed%20with%20lossy-parsed%20OCR%0Adocuments.%20These%20results%20validate%20the%20necessity%20of%20future%20research%20toward%20more%0Acapable%20long-context%20LVLMs.%20Project%20Page%3A%0Ahttps%3A//mayubo2333.github.io/MMLongBench-Doc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMLongBench-Doc%253A%2520Benchmarking%2520Long-context%2520Document%2520Understanding%2520with%250A%2520%2520Visualizations%26entry.906535625%3DYubo%2520Ma%2520and%2520Yuhang%2520Zang%2520and%2520Liangyu%2520Chen%2520and%2520Meiqi%2520Chen%2520and%2520Yizhu%2520Jiao%2520and%2520Xinze%2520Li%2520and%2520Xinyuan%2520Lu%2520and%2520Ziyu%2520Liu%2520and%2520Yan%2520Ma%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Liangming%2520Pan%2520and%2520Yu-Gang%2520Jiang%2520and%2520Jiaqi%2520Wang%2520and%2520Yixin%2520Cao%2520and%2520Aixin%2520Sun%26entry.1292438233%3D%2520%2520Understanding%2520documents%2520with%2520rich%2520layouts%2520and%2520multi-modal%2520components%2520is%2520a%250Along-standing%2520and%2520practical%2520task.%2520Recent%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%250Ahave%2520made%2520remarkable%2520strides%2520in%2520various%2520tasks%252C%2520particularly%2520in%2520single-page%250Adocument%2520understanding%2520%2528DU%2529.%2520However%252C%2520their%2520abilities%2520on%2520long-context%2520DU%2520remain%250Aan%2520open%2520problem.%2520This%2520work%2520presents%2520MMLongBench-Doc%252C%2520a%2520long-context%252C%250Amulti-modal%2520benchmark%2520comprising%25201%252C062%2520expert-annotated%2520questions.%2520Distinct%250Afrom%2520previous%2520datasets%252C%2520it%2520is%2520constructed%2520upon%2520130%2520lengthy%2520PDF-formatted%250Adocuments%2520with%2520an%2520average%2520of%252049.4%2520pages%2520and%252020%252C971%2520textual%2520tokens.%2520Towards%250Acomprehensive%2520evaluation%252C%2520answers%2520to%2520these%2520questions%2520rely%2520on%2520pieces%2520of%2520evidence%250Afrom%2520%25281%2529%2520different%2520sources%2520%2528text%252C%2520image%252C%2520chart%252C%2520table%252C%2520and%2520layout%2520structure%2529%250Aand%2520%25282%2529%2520various%2520locations%2520%2528i.e.%2520page%2520number%2529.%2520Moreover%252C%252033.2%2525%2520of%2520the%2520questions%250Aare%2520cross-page%2520questions%2520requiring%2520evidence%2520across%2520multiple%2520pages.%252022.8%2525%2520of%2520the%250Aquestions%2520are%2520designed%2520to%2520be%2520unanswerable%2520for%2520detecting%2520potential%250Ahallucinations.%2520Experiments%2520on%252014%2520LVLMs%2520demonstrate%2520that%2520long-context%2520DU%250Agreatly%2520challenges%2520current%2520models.%2520Notably%252C%2520the%2520best-performing%2520model%252C%2520GPT-4o%252C%250Aachieves%2520an%2520F1%2520score%2520of%2520only%252042.7%2525%252C%2520while%2520the%2520second-best%252C%2520GPT-4V%252C%2520scores%250A31.4%2525.%2520Furthermore%252C%252012%2520LVLMs%2520%2528all%2520except%2520GPT-4o%2520and%2520GPT-4V%2529%2520even%2520present%2520worse%250Aperformance%2520than%2520their%2520LLM%2520counterparts%2520which%2520are%2520fed%2520with%2520lossy-parsed%2520OCR%250Adocuments.%2520These%2520results%2520validate%2520the%2520necessity%2520of%2520future%2520research%2520toward%2520more%250Acapable%2520long-context%2520LVLMs.%2520Project%2520Page%253A%250Ahttps%253A//mayubo2333.github.io/MMLongBench-Doc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMLongBench-Doc%3A%20Benchmarking%20Long-context%20Document%20Understanding%20with%0A%20%20Visualizations&entry.906535625=Yubo%20Ma%20and%20Yuhang%20Zang%20and%20Liangyu%20Chen%20and%20Meiqi%20Chen%20and%20Yizhu%20Jiao%20and%20Xinze%20Li%20and%20Xinyuan%20Lu%20and%20Ziyu%20Liu%20and%20Yan%20Ma%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Liangming%20Pan%20and%20Yu-Gang%20Jiang%20and%20Jiaqi%20Wang%20and%20Yixin%20Cao%20and%20Aixin%20Sun&entry.1292438233=%20%20Understanding%20documents%20with%20rich%20layouts%20and%20multi-modal%20components%20is%20a%0Along-standing%20and%20practical%20task.%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%0Ahave%20made%20remarkable%20strides%20in%20various%20tasks%2C%20particularly%20in%20single-page%0Adocument%20understanding%20%28DU%29.%20However%2C%20their%20abilities%20on%20long-context%20DU%20remain%0Aan%20open%20problem.%20This%20work%20presents%20MMLongBench-Doc%2C%20a%20long-context%2C%0Amulti-modal%20benchmark%20comprising%201%2C062%20expert-annotated%20questions.%20Distinct%0Afrom%20previous%20datasets%2C%20it%20is%20constructed%20upon%20130%20lengthy%20PDF-formatted%0Adocuments%20with%20an%20average%20of%2049.4%20pages%20and%2020%2C971%20textual%20tokens.%20Towards%0Acomprehensive%20evaluation%2C%20answers%20to%20these%20questions%20rely%20on%20pieces%20of%20evidence%0Afrom%20%281%29%20different%20sources%20%28text%2C%20image%2C%20chart%2C%20table%2C%20and%20layout%20structure%29%0Aand%20%282%29%20various%20locations%20%28i.e.%20page%20number%29.%20Moreover%2C%2033.2%25%20of%20the%20questions%0Aare%20cross-page%20questions%20requiring%20evidence%20across%20multiple%20pages.%2022.8%25%20of%20the%0Aquestions%20are%20designed%20to%20be%20unanswerable%20for%20detecting%20potential%0Ahallucinations.%20Experiments%20on%2014%20LVLMs%20demonstrate%20that%20long-context%20DU%0Agreatly%20challenges%20current%20models.%20Notably%2C%20the%20best-performing%20model%2C%20GPT-4o%2C%0Aachieves%20an%20F1%20score%20of%20only%2042.7%25%2C%20while%20the%20second-best%2C%20GPT-4V%2C%20scores%0A31.4%25.%20Furthermore%2C%2012%20LVLMs%20%28all%20except%20GPT-4o%20and%20GPT-4V%29%20even%20present%20worse%0Aperformance%20than%20their%20LLM%20counterparts%20which%20are%20fed%20with%20lossy-parsed%20OCR%0Adocuments.%20These%20results%20validate%20the%20necessity%20of%20future%20research%20toward%20more%0Acapable%20long-context%20LVLMs.%20Project%20Page%3A%0Ahttps%3A//mayubo2333.github.io/MMLongBench-Doc%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01523v2&entry.124074799=Read"},
{"title": "Trainable Highly-expressive Activation Functions", "author": "Irit Chelly and Shahaf E. Finder and Shira Ifergane and Oren Freifeld", "abstract": "  Nonlinear activation functions are pivotal to the success of deep neural\nnets, and choosing the appropriate activation function can significantly affect\ntheir performance. Most networks use fixed activation functions (e.g., ReLU,\nGELU, etc.), and this choice might limit their expressiveness. Furthermore,\ndifferent layers may benefit from diverse activation functions. Consequently,\nthere has been a growing interest in trainable activation functions. In this\npaper, we introduce DiTAC, a trainable highly-expressive activation function\nbased on an efficient diffeomorphic transformation (called CPAB). Despite\nintroducing only a negligible number of trainable parameters, DiTAC enhances\nmodel expressiveness and performance, often yielding substantial improvements.\nIt also outperforms existing activation functions (regardless whether the\nlatter are fixed or trainable) in tasks such as semantic segmentation, image\ngeneration, regression problems, and image classification. Our code is\navailable at https://github.com/BGU-CS-VIL/DiTAC.\n", "link": "http://arxiv.org/abs/2407.07564v1", "date": "2024-07-10", "relevancy": 2.1046, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.532}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5282}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trainable%20Highly-expressive%20Activation%20Functions&body=Title%3A%20Trainable%20Highly-expressive%20Activation%20Functions%0AAuthor%3A%20Irit%20Chelly%20and%20Shahaf%20E.%20Finder%20and%20Shira%20Ifergane%20and%20Oren%20Freifeld%0AAbstract%3A%20%20%20Nonlinear%20activation%20functions%20are%20pivotal%20to%20the%20success%20of%20deep%20neural%0Anets%2C%20and%20choosing%20the%20appropriate%20activation%20function%20can%20significantly%20affect%0Atheir%20performance.%20Most%20networks%20use%20fixed%20activation%20functions%20%28e.g.%2C%20ReLU%2C%0AGELU%2C%20etc.%29%2C%20and%20this%20choice%20might%20limit%20their%20expressiveness.%20Furthermore%2C%0Adifferent%20layers%20may%20benefit%20from%20diverse%20activation%20functions.%20Consequently%2C%0Athere%20has%20been%20a%20growing%20interest%20in%20trainable%20activation%20functions.%20In%20this%0Apaper%2C%20we%20introduce%20DiTAC%2C%20a%20trainable%20highly-expressive%20activation%20function%0Abased%20on%20an%20efficient%20diffeomorphic%20transformation%20%28called%20CPAB%29.%20Despite%0Aintroducing%20only%20a%20negligible%20number%20of%20trainable%20parameters%2C%20DiTAC%20enhances%0Amodel%20expressiveness%20and%20performance%2C%20often%20yielding%20substantial%20improvements.%0AIt%20also%20outperforms%20existing%20activation%20functions%20%28regardless%20whether%20the%0Alatter%20are%20fixed%20or%20trainable%29%20in%20tasks%20such%20as%20semantic%20segmentation%2C%20image%0Ageneration%2C%20regression%20problems%2C%20and%20image%20classification.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/BGU-CS-VIL/DiTAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrainable%2520Highly-expressive%2520Activation%2520Functions%26entry.906535625%3DIrit%2520Chelly%2520and%2520Shahaf%2520E.%2520Finder%2520and%2520Shira%2520Ifergane%2520and%2520Oren%2520Freifeld%26entry.1292438233%3D%2520%2520Nonlinear%2520activation%2520functions%2520are%2520pivotal%2520to%2520the%2520success%2520of%2520deep%2520neural%250Anets%252C%2520and%2520choosing%2520the%2520appropriate%2520activation%2520function%2520can%2520significantly%2520affect%250Atheir%2520performance.%2520Most%2520networks%2520use%2520fixed%2520activation%2520functions%2520%2528e.g.%252C%2520ReLU%252C%250AGELU%252C%2520etc.%2529%252C%2520and%2520this%2520choice%2520might%2520limit%2520their%2520expressiveness.%2520Furthermore%252C%250Adifferent%2520layers%2520may%2520benefit%2520from%2520diverse%2520activation%2520functions.%2520Consequently%252C%250Athere%2520has%2520been%2520a%2520growing%2520interest%2520in%2520trainable%2520activation%2520functions.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520DiTAC%252C%2520a%2520trainable%2520highly-expressive%2520activation%2520function%250Abased%2520on%2520an%2520efficient%2520diffeomorphic%2520transformation%2520%2528called%2520CPAB%2529.%2520Despite%250Aintroducing%2520only%2520a%2520negligible%2520number%2520of%2520trainable%2520parameters%252C%2520DiTAC%2520enhances%250Amodel%2520expressiveness%2520and%2520performance%252C%2520often%2520yielding%2520substantial%2520improvements.%250AIt%2520also%2520outperforms%2520existing%2520activation%2520functions%2520%2528regardless%2520whether%2520the%250Alatter%2520are%2520fixed%2520or%2520trainable%2529%2520in%2520tasks%2520such%2520as%2520semantic%2520segmentation%252C%2520image%250Ageneration%252C%2520regression%2520problems%252C%2520and%2520image%2520classification.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/BGU-CS-VIL/DiTAC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trainable%20Highly-expressive%20Activation%20Functions&entry.906535625=Irit%20Chelly%20and%20Shahaf%20E.%20Finder%20and%20Shira%20Ifergane%20and%20Oren%20Freifeld&entry.1292438233=%20%20Nonlinear%20activation%20functions%20are%20pivotal%20to%20the%20success%20of%20deep%20neural%0Anets%2C%20and%20choosing%20the%20appropriate%20activation%20function%20can%20significantly%20affect%0Atheir%20performance.%20Most%20networks%20use%20fixed%20activation%20functions%20%28e.g.%2C%20ReLU%2C%0AGELU%2C%20etc.%29%2C%20and%20this%20choice%20might%20limit%20their%20expressiveness.%20Furthermore%2C%0Adifferent%20layers%20may%20benefit%20from%20diverse%20activation%20functions.%20Consequently%2C%0Athere%20has%20been%20a%20growing%20interest%20in%20trainable%20activation%20functions.%20In%20this%0Apaper%2C%20we%20introduce%20DiTAC%2C%20a%20trainable%20highly-expressive%20activation%20function%0Abased%20on%20an%20efficient%20diffeomorphic%20transformation%20%28called%20CPAB%29.%20Despite%0Aintroducing%20only%20a%20negligible%20number%20of%20trainable%20parameters%2C%20DiTAC%20enhances%0Amodel%20expressiveness%20and%20performance%2C%20often%20yielding%20substantial%20improvements.%0AIt%20also%20outperforms%20existing%20activation%20functions%20%28regardless%20whether%20the%0Alatter%20are%20fixed%20or%20trainable%29%20in%20tasks%20such%20as%20semantic%20segmentation%2C%20image%0Ageneration%2C%20regression%20problems%2C%20and%20image%20classification.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/BGU-CS-VIL/DiTAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07564v1&entry.124074799=Read"},
{"title": "Vegetable Peeling: A Case Study in Constrained Dexterous Manipulation", "author": "Tao Chen and Eric Cousineau and Naveen Kuppuswamy and Pulkit Agrawal", "abstract": "  Recent studies have made significant progress in addressing dexterous\nmanipulation problems, particularly in in-hand object reorientation. However,\nthere are few existing works that explore the potential utilization of\ndeveloped dexterous manipulation controllers for downstream tasks. In this\nstudy, we focus on constrained dexterous manipulation for food peeling. Food\npeeling presents various constraints on the reorientation controller, such as\nthe requirement for the hand to securely hold the object after reorientation\nfor peeling. We propose a simple system for learning a reorientation controller\nthat facilitates the subsequent peeling task. Videos are available at:\nhttps://taochenshh.github.io/projects/veg-peeling.\n", "link": "http://arxiv.org/abs/2407.07884v1", "date": "2024-07-10", "relevancy": 2.1025, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.543}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5376}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vegetable%20Peeling%3A%20A%20Case%20Study%20in%20Constrained%20Dexterous%20Manipulation&body=Title%3A%20Vegetable%20Peeling%3A%20A%20Case%20Study%20in%20Constrained%20Dexterous%20Manipulation%0AAuthor%3A%20Tao%20Chen%20and%20Eric%20Cousineau%20and%20Naveen%20Kuppuswamy%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Recent%20studies%20have%20made%20significant%20progress%20in%20addressing%20dexterous%0Amanipulation%20problems%2C%20particularly%20in%20in-hand%20object%20reorientation.%20However%2C%0Athere%20are%20few%20existing%20works%20that%20explore%20the%20potential%20utilization%20of%0Adeveloped%20dexterous%20manipulation%20controllers%20for%20downstream%20tasks.%20In%20this%0Astudy%2C%20we%20focus%20on%20constrained%20dexterous%20manipulation%20for%20food%20peeling.%20Food%0Apeeling%20presents%20various%20constraints%20on%20the%20reorientation%20controller%2C%20such%20as%0Athe%20requirement%20for%20the%20hand%20to%20securely%20hold%20the%20object%20after%20reorientation%0Afor%20peeling.%20We%20propose%20a%20simple%20system%20for%20learning%20a%20reorientation%20controller%0Athat%20facilitates%20the%20subsequent%20peeling%20task.%20Videos%20are%20available%20at%3A%0Ahttps%3A//taochenshh.github.io/projects/veg-peeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVegetable%2520Peeling%253A%2520A%2520Case%2520Study%2520in%2520Constrained%2520Dexterous%2520Manipulation%26entry.906535625%3DTao%2520Chen%2520and%2520Eric%2520Cousineau%2520and%2520Naveen%2520Kuppuswamy%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520made%2520significant%2520progress%2520in%2520addressing%2520dexterous%250Amanipulation%2520problems%252C%2520particularly%2520in%2520in-hand%2520object%2520reorientation.%2520However%252C%250Athere%2520are%2520few%2520existing%2520works%2520that%2520explore%2520the%2520potential%2520utilization%2520of%250Adeveloped%2520dexterous%2520manipulation%2520controllers%2520for%2520downstream%2520tasks.%2520In%2520this%250Astudy%252C%2520we%2520focus%2520on%2520constrained%2520dexterous%2520manipulation%2520for%2520food%2520peeling.%2520Food%250Apeeling%2520presents%2520various%2520constraints%2520on%2520the%2520reorientation%2520controller%252C%2520such%2520as%250Athe%2520requirement%2520for%2520the%2520hand%2520to%2520securely%2520hold%2520the%2520object%2520after%2520reorientation%250Afor%2520peeling.%2520We%2520propose%2520a%2520simple%2520system%2520for%2520learning%2520a%2520reorientation%2520controller%250Athat%2520facilitates%2520the%2520subsequent%2520peeling%2520task.%2520Videos%2520are%2520available%2520at%253A%250Ahttps%253A//taochenshh.github.io/projects/veg-peeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vegetable%20Peeling%3A%20A%20Case%20Study%20in%20Constrained%20Dexterous%20Manipulation&entry.906535625=Tao%20Chen%20and%20Eric%20Cousineau%20and%20Naveen%20Kuppuswamy%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Recent%20studies%20have%20made%20significant%20progress%20in%20addressing%20dexterous%0Amanipulation%20problems%2C%20particularly%20in%20in-hand%20object%20reorientation.%20However%2C%0Athere%20are%20few%20existing%20works%20that%20explore%20the%20potential%20utilization%20of%0Adeveloped%20dexterous%20manipulation%20controllers%20for%20downstream%20tasks.%20In%20this%0Astudy%2C%20we%20focus%20on%20constrained%20dexterous%20manipulation%20for%20food%20peeling.%20Food%0Apeeling%20presents%20various%20constraints%20on%20the%20reorientation%20controller%2C%20such%20as%0Athe%20requirement%20for%20the%20hand%20to%20securely%20hold%20the%20object%20after%20reorientation%0Afor%20peeling.%20We%20propose%20a%20simple%20system%20for%20learning%20a%20reorientation%20controller%0Athat%20facilitates%20the%20subsequent%20peeling%20task.%20Videos%20are%20available%20at%3A%0Ahttps%3A//taochenshh.github.io/projects/veg-peeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07884v1&entry.124074799=Read"},
{"title": "Adversarial Robustness Limits via Scaling-Law and Human-Alignment\n  Studies", "author": "Brian R. Bartoldson and James Diffenderfer and Konstantinos Parasyris and Bhavya Kailkhura", "abstract": "  This paper revisits the simple, long-studied, yet still unsolved problem of\nmaking image classifiers robust to imperceptible perturbations. Taking CIFAR10\nas an example, SOTA clean accuracy is about $100$%, but SOTA robustness to\n$\\ell_{\\infty}$-norm bounded perturbations barely exceeds $70$%. To understand\nthis gap, we analyze how model size, dataset size, and synthetic data quality\naffect robustness by developing the first scaling laws for adversarial\ntraining. Our scaling laws reveal inefficiencies in prior art and provide\nactionable feedback to advance the field. For instance, we discovered that SOTA\nmethods diverge notably from compute-optimal setups, using excess compute for\ntheir level of robustness. Leveraging a compute-efficient setup, we surpass the\nprior SOTA with $20$% ($70$%) fewer training (inference) FLOPs. We trained\nvarious compute-efficient models, with our best achieving $74$% AutoAttack\naccuracy ($+3$% gain). However, our scaling laws also predict robustness slowly\ngrows then plateaus at $90$%: dwarfing our new SOTA by scaling is impractical,\nand perfect robustness is impossible. To better understand this predicted\nlimit, we carry out a small-scale human evaluation on the AutoAttack data that\nfools our top-performing model. Concerningly, we estimate that human\nperformance also plateaus near $90$%, which we show to be attributable to\n$\\ell_{\\infty}$-constrained attacks' generation of invalid images not\nconsistent with their original labels. Having characterized limiting\nroadblocks, we outline promising paths for future research.\n", "link": "http://arxiv.org/abs/2404.09349v2", "date": "2024-07-10", "relevancy": 2.0902, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5313}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5213}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Robustness%20Limits%20via%20Scaling-Law%20and%20Human-Alignment%0A%20%20Studies&body=Title%3A%20Adversarial%20Robustness%20Limits%20via%20Scaling-Law%20and%20Human-Alignment%0A%20%20Studies%0AAuthor%3A%20Brian%20R.%20Bartoldson%20and%20James%20Diffenderfer%20and%20Konstantinos%20Parasyris%20and%20Bhavya%20Kailkhura%0AAbstract%3A%20%20%20This%20paper%20revisits%20the%20simple%2C%20long-studied%2C%20yet%20still%20unsolved%20problem%20of%0Amaking%20image%20classifiers%20robust%20to%20imperceptible%20perturbations.%20Taking%20CIFAR10%0Aas%20an%20example%2C%20SOTA%20clean%20accuracy%20is%20about%20%24100%24%25%2C%20but%20SOTA%20robustness%20to%0A%24%5Cell_%7B%5Cinfty%7D%24-norm%20bounded%20perturbations%20barely%20exceeds%20%2470%24%25.%20To%20understand%0Athis%20gap%2C%20we%20analyze%20how%20model%20size%2C%20dataset%20size%2C%20and%20synthetic%20data%20quality%0Aaffect%20robustness%20by%20developing%20the%20first%20scaling%20laws%20for%20adversarial%0Atraining.%20Our%20scaling%20laws%20reveal%20inefficiencies%20in%20prior%20art%20and%20provide%0Aactionable%20feedback%20to%20advance%20the%20field.%20For%20instance%2C%20we%20discovered%20that%20SOTA%0Amethods%20diverge%20notably%20from%20compute-optimal%20setups%2C%20using%20excess%20compute%20for%0Atheir%20level%20of%20robustness.%20Leveraging%20a%20compute-efficient%20setup%2C%20we%20surpass%20the%0Aprior%20SOTA%20with%20%2420%24%25%20%28%2470%24%25%29%20fewer%20training%20%28inference%29%20FLOPs.%20We%20trained%0Avarious%20compute-efficient%20models%2C%20with%20our%20best%20achieving%20%2474%24%25%20AutoAttack%0Aaccuracy%20%28%24%2B3%24%25%20gain%29.%20However%2C%20our%20scaling%20laws%20also%20predict%20robustness%20slowly%0Agrows%20then%20plateaus%20at%20%2490%24%25%3A%20dwarfing%20our%20new%20SOTA%20by%20scaling%20is%20impractical%2C%0Aand%20perfect%20robustness%20is%20impossible.%20To%20better%20understand%20this%20predicted%0Alimit%2C%20we%20carry%20out%20a%20small-scale%20human%20evaluation%20on%20the%20AutoAttack%20data%20that%0Afools%20our%20top-performing%20model.%20Concerningly%2C%20we%20estimate%20that%20human%0Aperformance%20also%20plateaus%20near%20%2490%24%25%2C%20which%20we%20show%20to%20be%20attributable%20to%0A%24%5Cell_%7B%5Cinfty%7D%24-constrained%20attacks%27%20generation%20of%20invalid%20images%20not%0Aconsistent%20with%20their%20original%20labels.%20Having%20characterized%20limiting%0Aroadblocks%2C%20we%20outline%20promising%20paths%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Robustness%2520Limits%2520via%2520Scaling-Law%2520and%2520Human-Alignment%250A%2520%2520Studies%26entry.906535625%3DBrian%2520R.%2520Bartoldson%2520and%2520James%2520Diffenderfer%2520and%2520Konstantinos%2520Parasyris%2520and%2520Bhavya%2520Kailkhura%26entry.1292438233%3D%2520%2520This%2520paper%2520revisits%2520the%2520simple%252C%2520long-studied%252C%2520yet%2520still%2520unsolved%2520problem%2520of%250Amaking%2520image%2520classifiers%2520robust%2520to%2520imperceptible%2520perturbations.%2520Taking%2520CIFAR10%250Aas%2520an%2520example%252C%2520SOTA%2520clean%2520accuracy%2520is%2520about%2520%2524100%2524%2525%252C%2520but%2520SOTA%2520robustness%2520to%250A%2524%255Cell_%257B%255Cinfty%257D%2524-norm%2520bounded%2520perturbations%2520barely%2520exceeds%2520%252470%2524%2525.%2520To%2520understand%250Athis%2520gap%252C%2520we%2520analyze%2520how%2520model%2520size%252C%2520dataset%2520size%252C%2520and%2520synthetic%2520data%2520quality%250Aaffect%2520robustness%2520by%2520developing%2520the%2520first%2520scaling%2520laws%2520for%2520adversarial%250Atraining.%2520Our%2520scaling%2520laws%2520reveal%2520inefficiencies%2520in%2520prior%2520art%2520and%2520provide%250Aactionable%2520feedback%2520to%2520advance%2520the%2520field.%2520For%2520instance%252C%2520we%2520discovered%2520that%2520SOTA%250Amethods%2520diverge%2520notably%2520from%2520compute-optimal%2520setups%252C%2520using%2520excess%2520compute%2520for%250Atheir%2520level%2520of%2520robustness.%2520Leveraging%2520a%2520compute-efficient%2520setup%252C%2520we%2520surpass%2520the%250Aprior%2520SOTA%2520with%2520%252420%2524%2525%2520%2528%252470%2524%2525%2529%2520fewer%2520training%2520%2528inference%2529%2520FLOPs.%2520We%2520trained%250Avarious%2520compute-efficient%2520models%252C%2520with%2520our%2520best%2520achieving%2520%252474%2524%2525%2520AutoAttack%250Aaccuracy%2520%2528%2524%252B3%2524%2525%2520gain%2529.%2520However%252C%2520our%2520scaling%2520laws%2520also%2520predict%2520robustness%2520slowly%250Agrows%2520then%2520plateaus%2520at%2520%252490%2524%2525%253A%2520dwarfing%2520our%2520new%2520SOTA%2520by%2520scaling%2520is%2520impractical%252C%250Aand%2520perfect%2520robustness%2520is%2520impossible.%2520To%2520better%2520understand%2520this%2520predicted%250Alimit%252C%2520we%2520carry%2520out%2520a%2520small-scale%2520human%2520evaluation%2520on%2520the%2520AutoAttack%2520data%2520that%250Afools%2520our%2520top-performing%2520model.%2520Concerningly%252C%2520we%2520estimate%2520that%2520human%250Aperformance%2520also%2520plateaus%2520near%2520%252490%2524%2525%252C%2520which%2520we%2520show%2520to%2520be%2520attributable%2520to%250A%2524%255Cell_%257B%255Cinfty%257D%2524-constrained%2520attacks%2527%2520generation%2520of%2520invalid%2520images%2520not%250Aconsistent%2520with%2520their%2520original%2520labels.%2520Having%2520characterized%2520limiting%250Aroadblocks%252C%2520we%2520outline%2520promising%2520paths%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Robustness%20Limits%20via%20Scaling-Law%20and%20Human-Alignment%0A%20%20Studies&entry.906535625=Brian%20R.%20Bartoldson%20and%20James%20Diffenderfer%20and%20Konstantinos%20Parasyris%20and%20Bhavya%20Kailkhura&entry.1292438233=%20%20This%20paper%20revisits%20the%20simple%2C%20long-studied%2C%20yet%20still%20unsolved%20problem%20of%0Amaking%20image%20classifiers%20robust%20to%20imperceptible%20perturbations.%20Taking%20CIFAR10%0Aas%20an%20example%2C%20SOTA%20clean%20accuracy%20is%20about%20%24100%24%25%2C%20but%20SOTA%20robustness%20to%0A%24%5Cell_%7B%5Cinfty%7D%24-norm%20bounded%20perturbations%20barely%20exceeds%20%2470%24%25.%20To%20understand%0Athis%20gap%2C%20we%20analyze%20how%20model%20size%2C%20dataset%20size%2C%20and%20synthetic%20data%20quality%0Aaffect%20robustness%20by%20developing%20the%20first%20scaling%20laws%20for%20adversarial%0Atraining.%20Our%20scaling%20laws%20reveal%20inefficiencies%20in%20prior%20art%20and%20provide%0Aactionable%20feedback%20to%20advance%20the%20field.%20For%20instance%2C%20we%20discovered%20that%20SOTA%0Amethods%20diverge%20notably%20from%20compute-optimal%20setups%2C%20using%20excess%20compute%20for%0Atheir%20level%20of%20robustness.%20Leveraging%20a%20compute-efficient%20setup%2C%20we%20surpass%20the%0Aprior%20SOTA%20with%20%2420%24%25%20%28%2470%24%25%29%20fewer%20training%20%28inference%29%20FLOPs.%20We%20trained%0Avarious%20compute-efficient%20models%2C%20with%20our%20best%20achieving%20%2474%24%25%20AutoAttack%0Aaccuracy%20%28%24%2B3%24%25%20gain%29.%20However%2C%20our%20scaling%20laws%20also%20predict%20robustness%20slowly%0Agrows%20then%20plateaus%20at%20%2490%24%25%3A%20dwarfing%20our%20new%20SOTA%20by%20scaling%20is%20impractical%2C%0Aand%20perfect%20robustness%20is%20impossible.%20To%20better%20understand%20this%20predicted%0Alimit%2C%20we%20carry%20out%20a%20small-scale%20human%20evaluation%20on%20the%20AutoAttack%20data%20that%0Afools%20our%20top-performing%20model.%20Concerningly%2C%20we%20estimate%20that%20human%0Aperformance%20also%20plateaus%20near%20%2490%24%25%2C%20which%20we%20show%20to%20be%20attributable%20to%0A%24%5Cell_%7B%5Cinfty%7D%24-constrained%20attacks%27%20generation%20of%20invalid%20images%20not%0Aconsistent%20with%20their%20original%20labels.%20Having%20characterized%20limiting%0Aroadblocks%2C%20we%20outline%20promising%20paths%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09349v2&entry.124074799=Read"},
{"title": "Pentagonal Photonic Crystal Mirrors: Scalable Lightsails with Enhanced\n  Acceleration via Neural Topology Optimization", "author": "L. Norder and S. Yin and M. J. de Jong and F. Stallone and H. Aydogmus and P. M. Sberna and M. A. Bessa and R. A. Norte", "abstract": "  The Starshot Breakthrough Initiative aims to send one-gram microchip probes\nto Alpha Centauri within 20 years, using gram-scale lightsails propelled by\nlaser-based radiation pressure, reaching velocities nearing a fifth of light\nspeed. This mission requires lightsail materials that challenge the\nfundamentals of nanotechnology, requiring innovations in optics, material\nscience and structural engineering. Unlike the microchip payload, which must be\nminimized in every dimension, such lightsails need meter-scale dimensions with\nnanoscale thickness and billions of nanoscale holes to enhance reflectivity and\nreduce mass. Our study employs neural topology optimization, revealing a novel\npentagonal lattice-based photonic crystal (PhC) reflector. The optimized\ndesigns shorten acceleration times, therefore lowering launch costs\nsignificantly. Crucially, these designs also enable lightsail material\nfabrication with orders-of-magnitude reduction in costs. We have fabricated a\n60 x 60 mm$^2$, 200nm thick, single-layer reflector perforated with over a\nbillion nanoscale features; the highest aspect-ratio nanophotonic element to\ndate. We achieve this with nearly 9,000 times cost reduction per m$^2$.\nStarshot lightsails will have several stringent requirements but will\nultimately be driven by costs to build at scale. Here we highlight challenges\nand possible solutions in developing lightsail materials - showcasing the\npotential of scaling nanophotonics for cost-effective next-generation space\nexploration.\n", "link": "http://arxiv.org/abs/2407.07896v1", "date": "2024-07-10", "relevancy": 2.0889, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4225}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4154}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pentagonal%20Photonic%20Crystal%20Mirrors%3A%20Scalable%20Lightsails%20with%20Enhanced%0A%20%20Acceleration%20via%20Neural%20Topology%20Optimization&body=Title%3A%20Pentagonal%20Photonic%20Crystal%20Mirrors%3A%20Scalable%20Lightsails%20with%20Enhanced%0A%20%20Acceleration%20via%20Neural%20Topology%20Optimization%0AAuthor%3A%20L.%20Norder%20and%20S.%20Yin%20and%20M.%20J.%20de%20Jong%20and%20F.%20Stallone%20and%20H.%20Aydogmus%20and%20P.%20M.%20Sberna%20and%20M.%20A.%20Bessa%20and%20R.%20A.%20Norte%0AAbstract%3A%20%20%20The%20Starshot%20Breakthrough%20Initiative%20aims%20to%20send%20one-gram%20microchip%20probes%0Ato%20Alpha%20Centauri%20within%2020%20years%2C%20using%20gram-scale%20lightsails%20propelled%20by%0Alaser-based%20radiation%20pressure%2C%20reaching%20velocities%20nearing%20a%20fifth%20of%20light%0Aspeed.%20This%20mission%20requires%20lightsail%20materials%20that%20challenge%20the%0Afundamentals%20of%20nanotechnology%2C%20requiring%20innovations%20in%20optics%2C%20material%0Ascience%20and%20structural%20engineering.%20Unlike%20the%20microchip%20payload%2C%20which%20must%20be%0Aminimized%20in%20every%20dimension%2C%20such%20lightsails%20need%20meter-scale%20dimensions%20with%0Ananoscale%20thickness%20and%20billions%20of%20nanoscale%20holes%20to%20enhance%20reflectivity%20and%0Areduce%20mass.%20Our%20study%20employs%20neural%20topology%20optimization%2C%20revealing%20a%20novel%0Apentagonal%20lattice-based%20photonic%20crystal%20%28PhC%29%20reflector.%20The%20optimized%0Adesigns%20shorten%20acceleration%20times%2C%20therefore%20lowering%20launch%20costs%0Asignificantly.%20Crucially%2C%20these%20designs%20also%20enable%20lightsail%20material%0Afabrication%20with%20orders-of-magnitude%20reduction%20in%20costs.%20We%20have%20fabricated%20a%0A60%20x%2060%20mm%24%5E2%24%2C%20200nm%20thick%2C%20single-layer%20reflector%20perforated%20with%20over%20a%0Abillion%20nanoscale%20features%3B%20the%20highest%20aspect-ratio%20nanophotonic%20element%20to%0Adate.%20We%20achieve%20this%20with%20nearly%209%2C000%20times%20cost%20reduction%20per%20m%24%5E2%24.%0AStarshot%20lightsails%20will%20have%20several%20stringent%20requirements%20but%20will%0Aultimately%20be%20driven%20by%20costs%20to%20build%20at%20scale.%20Here%20we%20highlight%20challenges%0Aand%20possible%20solutions%20in%20developing%20lightsail%20materials%20-%20showcasing%20the%0Apotential%20of%20scaling%20nanophotonics%20for%20cost-effective%20next-generation%20space%0Aexploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPentagonal%2520Photonic%2520Crystal%2520Mirrors%253A%2520Scalable%2520Lightsails%2520with%2520Enhanced%250A%2520%2520Acceleration%2520via%2520Neural%2520Topology%2520Optimization%26entry.906535625%3DL.%2520Norder%2520and%2520S.%2520Yin%2520and%2520M.%2520J.%2520de%2520Jong%2520and%2520F.%2520Stallone%2520and%2520H.%2520Aydogmus%2520and%2520P.%2520M.%2520Sberna%2520and%2520M.%2520A.%2520Bessa%2520and%2520R.%2520A.%2520Norte%26entry.1292438233%3D%2520%2520The%2520Starshot%2520Breakthrough%2520Initiative%2520aims%2520to%2520send%2520one-gram%2520microchip%2520probes%250Ato%2520Alpha%2520Centauri%2520within%252020%2520years%252C%2520using%2520gram-scale%2520lightsails%2520propelled%2520by%250Alaser-based%2520radiation%2520pressure%252C%2520reaching%2520velocities%2520nearing%2520a%2520fifth%2520of%2520light%250Aspeed.%2520This%2520mission%2520requires%2520lightsail%2520materials%2520that%2520challenge%2520the%250Afundamentals%2520of%2520nanotechnology%252C%2520requiring%2520innovations%2520in%2520optics%252C%2520material%250Ascience%2520and%2520structural%2520engineering.%2520Unlike%2520the%2520microchip%2520payload%252C%2520which%2520must%2520be%250Aminimized%2520in%2520every%2520dimension%252C%2520such%2520lightsails%2520need%2520meter-scale%2520dimensions%2520with%250Ananoscale%2520thickness%2520and%2520billions%2520of%2520nanoscale%2520holes%2520to%2520enhance%2520reflectivity%2520and%250Areduce%2520mass.%2520Our%2520study%2520employs%2520neural%2520topology%2520optimization%252C%2520revealing%2520a%2520novel%250Apentagonal%2520lattice-based%2520photonic%2520crystal%2520%2528PhC%2529%2520reflector.%2520The%2520optimized%250Adesigns%2520shorten%2520acceleration%2520times%252C%2520therefore%2520lowering%2520launch%2520costs%250Asignificantly.%2520Crucially%252C%2520these%2520designs%2520also%2520enable%2520lightsail%2520material%250Afabrication%2520with%2520orders-of-magnitude%2520reduction%2520in%2520costs.%2520We%2520have%2520fabricated%2520a%250A60%2520x%252060%2520mm%2524%255E2%2524%252C%2520200nm%2520thick%252C%2520single-layer%2520reflector%2520perforated%2520with%2520over%2520a%250Abillion%2520nanoscale%2520features%253B%2520the%2520highest%2520aspect-ratio%2520nanophotonic%2520element%2520to%250Adate.%2520We%2520achieve%2520this%2520with%2520nearly%25209%252C000%2520times%2520cost%2520reduction%2520per%2520m%2524%255E2%2524.%250AStarshot%2520lightsails%2520will%2520have%2520several%2520stringent%2520requirements%2520but%2520will%250Aultimately%2520be%2520driven%2520by%2520costs%2520to%2520build%2520at%2520scale.%2520Here%2520we%2520highlight%2520challenges%250Aand%2520possible%2520solutions%2520in%2520developing%2520lightsail%2520materials%2520-%2520showcasing%2520the%250Apotential%2520of%2520scaling%2520nanophotonics%2520for%2520cost-effective%2520next-generation%2520space%250Aexploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pentagonal%20Photonic%20Crystal%20Mirrors%3A%20Scalable%20Lightsails%20with%20Enhanced%0A%20%20Acceleration%20via%20Neural%20Topology%20Optimization&entry.906535625=L.%20Norder%20and%20S.%20Yin%20and%20M.%20J.%20de%20Jong%20and%20F.%20Stallone%20and%20H.%20Aydogmus%20and%20P.%20M.%20Sberna%20and%20M.%20A.%20Bessa%20and%20R.%20A.%20Norte&entry.1292438233=%20%20The%20Starshot%20Breakthrough%20Initiative%20aims%20to%20send%20one-gram%20microchip%20probes%0Ato%20Alpha%20Centauri%20within%2020%20years%2C%20using%20gram-scale%20lightsails%20propelled%20by%0Alaser-based%20radiation%20pressure%2C%20reaching%20velocities%20nearing%20a%20fifth%20of%20light%0Aspeed.%20This%20mission%20requires%20lightsail%20materials%20that%20challenge%20the%0Afundamentals%20of%20nanotechnology%2C%20requiring%20innovations%20in%20optics%2C%20material%0Ascience%20and%20structural%20engineering.%20Unlike%20the%20microchip%20payload%2C%20which%20must%20be%0Aminimized%20in%20every%20dimension%2C%20such%20lightsails%20need%20meter-scale%20dimensions%20with%0Ananoscale%20thickness%20and%20billions%20of%20nanoscale%20holes%20to%20enhance%20reflectivity%20and%0Areduce%20mass.%20Our%20study%20employs%20neural%20topology%20optimization%2C%20revealing%20a%20novel%0Apentagonal%20lattice-based%20photonic%20crystal%20%28PhC%29%20reflector.%20The%20optimized%0Adesigns%20shorten%20acceleration%20times%2C%20therefore%20lowering%20launch%20costs%0Asignificantly.%20Crucially%2C%20these%20designs%20also%20enable%20lightsail%20material%0Afabrication%20with%20orders-of-magnitude%20reduction%20in%20costs.%20We%20have%20fabricated%20a%0A60%20x%2060%20mm%24%5E2%24%2C%20200nm%20thick%2C%20single-layer%20reflector%20perforated%20with%20over%20a%0Abillion%20nanoscale%20features%3B%20the%20highest%20aspect-ratio%20nanophotonic%20element%20to%0Adate.%20We%20achieve%20this%20with%20nearly%209%2C000%20times%20cost%20reduction%20per%20m%24%5E2%24.%0AStarshot%20lightsails%20will%20have%20several%20stringent%20requirements%20but%20will%0Aultimately%20be%20driven%20by%20costs%20to%20build%20at%20scale.%20Here%20we%20highlight%20challenges%0Aand%20possible%20solutions%20in%20developing%20lightsail%20materials%20-%20showcasing%20the%0Apotential%20of%20scaling%20nanophotonics%20for%20cost-effective%20next-generation%20space%0Aexploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07896v1&entry.124074799=Read"},
{"title": "A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning\n  Geometry", "author": "Martin Lindstr\u00f6m and Borja Rodr\u00edguez-G\u00e1lvez and Ragnar Thobaben and Mikael Skoglund", "abstract": "  Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.\n", "link": "http://arxiv.org/abs/2407.07664v1", "date": "2024-07-10", "relevancy": 2.0785, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Coding-Theoretic%20Analysis%20of%20Hyperspherical%20Prototypical%20Learning%0A%20%20Geometry&body=Title%3A%20A%20Coding-Theoretic%20Analysis%20of%20Hyperspherical%20Prototypical%20Learning%0A%20%20Geometry%0AAuthor%3A%20Martin%20Lindstr%C3%B6m%20and%20Borja%20Rodr%C3%ADguez-G%C3%A1lvez%20and%20Ragnar%20Thobaben%20and%20Mikael%20Skoglund%0AAbstract%3A%20%20%20Hyperspherical%20Prototypical%20Learning%20%28HPL%29%20is%20a%20supervised%20approach%20to%0Arepresentation%20learning%20that%20designs%20class%20prototypes%20on%20the%20unit%20hypersphere.%0AThe%20prototypes%20bias%20the%20representations%20to%20class%20separation%20in%20a%20scale%0Ainvariant%20and%20known%20geometry.%20Previous%20approaches%20to%20HPL%20have%20either%20of%20the%0Afollowing%20shortcomings%3A%20%28i%29%20they%20follow%20an%20unprincipled%20optimisation%20procedure%3B%0Aor%20%28ii%29%20they%20are%20theoretically%20sound%2C%20but%20are%20constrained%20to%20only%20one%20possible%0Alatent%20dimension.%20In%20this%20paper%2C%20we%20address%20both%20shortcomings.%20To%20address%20%28i%29%2C%0Awe%20present%20a%20principled%20optimisation%20procedure%20whose%20solution%20we%20show%20is%0Aoptimal.%20To%20address%20%28ii%29%2C%20we%20construct%20well-separated%20prototypes%20in%20a%20wide%0Arange%20of%20dimensions%20using%20linear%20block%20codes.%20Additionally%2C%20we%20give%20a%20full%0Acharacterisation%20of%20the%20optimal%20prototype%20placement%20in%20terms%20of%20achievable%20and%0Aconverse%20bounds%2C%20showing%20that%20our%20proposed%20methods%20are%20near-optimal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Coding-Theoretic%2520Analysis%2520of%2520Hyperspherical%2520Prototypical%2520Learning%250A%2520%2520Geometry%26entry.906535625%3DMartin%2520Lindstr%25C3%25B6m%2520and%2520Borja%2520Rodr%25C3%25ADguez-G%25C3%25A1lvez%2520and%2520Ragnar%2520Thobaben%2520and%2520Mikael%2520Skoglund%26entry.1292438233%3D%2520%2520Hyperspherical%2520Prototypical%2520Learning%2520%2528HPL%2529%2520is%2520a%2520supervised%2520approach%2520to%250Arepresentation%2520learning%2520that%2520designs%2520class%2520prototypes%2520on%2520the%2520unit%2520hypersphere.%250AThe%2520prototypes%2520bias%2520the%2520representations%2520to%2520class%2520separation%2520in%2520a%2520scale%250Ainvariant%2520and%2520known%2520geometry.%2520Previous%2520approaches%2520to%2520HPL%2520have%2520either%2520of%2520the%250Afollowing%2520shortcomings%253A%2520%2528i%2529%2520they%2520follow%2520an%2520unprincipled%2520optimisation%2520procedure%253B%250Aor%2520%2528ii%2529%2520they%2520are%2520theoretically%2520sound%252C%2520but%2520are%2520constrained%2520to%2520only%2520one%2520possible%250Alatent%2520dimension.%2520In%2520this%2520paper%252C%2520we%2520address%2520both%2520shortcomings.%2520To%2520address%2520%2528i%2529%252C%250Awe%2520present%2520a%2520principled%2520optimisation%2520procedure%2520whose%2520solution%2520we%2520show%2520is%250Aoptimal.%2520To%2520address%2520%2528ii%2529%252C%2520we%2520construct%2520well-separated%2520prototypes%2520in%2520a%2520wide%250Arange%2520of%2520dimensions%2520using%2520linear%2520block%2520codes.%2520Additionally%252C%2520we%2520give%2520a%2520full%250Acharacterisation%2520of%2520the%2520optimal%2520prototype%2520placement%2520in%2520terms%2520of%2520achievable%2520and%250Aconverse%2520bounds%252C%2520showing%2520that%2520our%2520proposed%2520methods%2520are%2520near-optimal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Coding-Theoretic%20Analysis%20of%20Hyperspherical%20Prototypical%20Learning%0A%20%20Geometry&entry.906535625=Martin%20Lindstr%C3%B6m%20and%20Borja%20Rodr%C3%ADguez-G%C3%A1lvez%20and%20Ragnar%20Thobaben%20and%20Mikael%20Skoglund&entry.1292438233=%20%20Hyperspherical%20Prototypical%20Learning%20%28HPL%29%20is%20a%20supervised%20approach%20to%0Arepresentation%20learning%20that%20designs%20class%20prototypes%20on%20the%20unit%20hypersphere.%0AThe%20prototypes%20bias%20the%20representations%20to%20class%20separation%20in%20a%20scale%0Ainvariant%20and%20known%20geometry.%20Previous%20approaches%20to%20HPL%20have%20either%20of%20the%0Afollowing%20shortcomings%3A%20%28i%29%20they%20follow%20an%20unprincipled%20optimisation%20procedure%3B%0Aor%20%28ii%29%20they%20are%20theoretically%20sound%2C%20but%20are%20constrained%20to%20only%20one%20possible%0Alatent%20dimension.%20In%20this%20paper%2C%20we%20address%20both%20shortcomings.%20To%20address%20%28i%29%2C%0Awe%20present%20a%20principled%20optimisation%20procedure%20whose%20solution%20we%20show%20is%0Aoptimal.%20To%20address%20%28ii%29%2C%20we%20construct%20well-separated%20prototypes%20in%20a%20wide%0Arange%20of%20dimensions%20using%20linear%20block%20codes.%20Additionally%2C%20we%20give%20a%20full%0Acharacterisation%20of%20the%20optimal%20prototype%20placement%20in%20terms%20of%20achievable%20and%0Aconverse%20bounds%2C%20showing%20that%20our%20proposed%20methods%20are%20near-optimal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07664v1&entry.124074799=Read"},
{"title": "MSC-LIO: An MSCKF-Based LiDAR-Inertial Odometry with Same-Plane-Point\n  Tracking", "author": "Tisheng Zhang and Man Yuan and Linfu Wei and Hailiang Tang and Xiaoji Niu", "abstract": "  The multi-state constraint Kalman filter (MSCKF) has been proven to be more\nefficient than graph optimization for visual-based odometry while with similar\naccuracy. However, it has not yet been properly considered and studied for\nLiDAR-based odometry. In this paper, we propose a novel tightly coupled\nLiDAR-inertial odometry based on the MSCKF framework, named MSC-LIO. An\nefficient LiDAR same-plane-point (LSPP) tracking method, without explicit\nfeature extraction, is present for frame-to-frame data associations. The\ntracked LSPPs are employed to build an LSPP measurement model, which constructs\na multi-state constraint. Besides, we propose an effective point-velocity-based\nLiDAR-IMU time-delay (LITD) estimation method, which is derived from the\nproposed LSPP tracking method. Extensive experiments were conducted on both\npublic and private datasets. The results demonstrate that the proposed MSC-LIO\nyields higher accuracy and efficiency than the state-of-the-art methods. The\nablation experiment results indicate that the data-association efficiency is\nimproved by nearly 3 times using the LSPP tracking method. Besides, the\nproposed LITD estimation method can effectively and accurately estimate the\nLITD.\n", "link": "http://arxiv.org/abs/2407.07589v1", "date": "2024-07-10", "relevancy": 2.0775, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.547}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5205}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSC-LIO%3A%20An%20MSCKF-Based%20LiDAR-Inertial%20Odometry%20with%20Same-Plane-Point%0A%20%20Tracking&body=Title%3A%20MSC-LIO%3A%20An%20MSCKF-Based%20LiDAR-Inertial%20Odometry%20with%20Same-Plane-Point%0A%20%20Tracking%0AAuthor%3A%20Tisheng%20Zhang%20and%20Man%20Yuan%20and%20Linfu%20Wei%20and%20Hailiang%20Tang%20and%20Xiaoji%20Niu%0AAbstract%3A%20%20%20The%20multi-state%20constraint%20Kalman%20filter%20%28MSCKF%29%20has%20been%20proven%20to%20be%20more%0Aefficient%20than%20graph%20optimization%20for%20visual-based%20odometry%20while%20with%20similar%0Aaccuracy.%20However%2C%20it%20has%20not%20yet%20been%20properly%20considered%20and%20studied%20for%0ALiDAR-based%20odometry.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20tightly%20coupled%0ALiDAR-inertial%20odometry%20based%20on%20the%20MSCKF%20framework%2C%20named%20MSC-LIO.%20An%0Aefficient%20LiDAR%20same-plane-point%20%28LSPP%29%20tracking%20method%2C%20without%20explicit%0Afeature%20extraction%2C%20is%20present%20for%20frame-to-frame%20data%20associations.%20The%0Atracked%20LSPPs%20are%20employed%20to%20build%20an%20LSPP%20measurement%20model%2C%20which%20constructs%0Aa%20multi-state%20constraint.%20Besides%2C%20we%20propose%20an%20effective%20point-velocity-based%0ALiDAR-IMU%20time-delay%20%28LITD%29%20estimation%20method%2C%20which%20is%20derived%20from%20the%0Aproposed%20LSPP%20tracking%20method.%20Extensive%20experiments%20were%20conducted%20on%20both%0Apublic%20and%20private%20datasets.%20The%20results%20demonstrate%20that%20the%20proposed%20MSC-LIO%0Ayields%20higher%20accuracy%20and%20efficiency%20than%20the%20state-of-the-art%20methods.%20The%0Aablation%20experiment%20results%20indicate%20that%20the%20data-association%20efficiency%20is%0Aimproved%20by%20nearly%203%20times%20using%20the%20LSPP%20tracking%20method.%20Besides%2C%20the%0Aproposed%20LITD%20estimation%20method%20can%20effectively%20and%20accurately%20estimate%20the%0ALITD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSC-LIO%253A%2520An%2520MSCKF-Based%2520LiDAR-Inertial%2520Odometry%2520with%2520Same-Plane-Point%250A%2520%2520Tracking%26entry.906535625%3DTisheng%2520Zhang%2520and%2520Man%2520Yuan%2520and%2520Linfu%2520Wei%2520and%2520Hailiang%2520Tang%2520and%2520Xiaoji%2520Niu%26entry.1292438233%3D%2520%2520The%2520multi-state%2520constraint%2520Kalman%2520filter%2520%2528MSCKF%2529%2520has%2520been%2520proven%2520to%2520be%2520more%250Aefficient%2520than%2520graph%2520optimization%2520for%2520visual-based%2520odometry%2520while%2520with%2520similar%250Aaccuracy.%2520However%252C%2520it%2520has%2520not%2520yet%2520been%2520properly%2520considered%2520and%2520studied%2520for%250ALiDAR-based%2520odometry.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520tightly%2520coupled%250ALiDAR-inertial%2520odometry%2520based%2520on%2520the%2520MSCKF%2520framework%252C%2520named%2520MSC-LIO.%2520An%250Aefficient%2520LiDAR%2520same-plane-point%2520%2528LSPP%2529%2520tracking%2520method%252C%2520without%2520explicit%250Afeature%2520extraction%252C%2520is%2520present%2520for%2520frame-to-frame%2520data%2520associations.%2520The%250Atracked%2520LSPPs%2520are%2520employed%2520to%2520build%2520an%2520LSPP%2520measurement%2520model%252C%2520which%2520constructs%250Aa%2520multi-state%2520constraint.%2520Besides%252C%2520we%2520propose%2520an%2520effective%2520point-velocity-based%250ALiDAR-IMU%2520time-delay%2520%2528LITD%2529%2520estimation%2520method%252C%2520which%2520is%2520derived%2520from%2520the%250Aproposed%2520LSPP%2520tracking%2520method.%2520Extensive%2520experiments%2520were%2520conducted%2520on%2520both%250Apublic%2520and%2520private%2520datasets.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520MSC-LIO%250Ayields%2520higher%2520accuracy%2520and%2520efficiency%2520than%2520the%2520state-of-the-art%2520methods.%2520The%250Aablation%2520experiment%2520results%2520indicate%2520that%2520the%2520data-association%2520efficiency%2520is%250Aimproved%2520by%2520nearly%25203%2520times%2520using%2520the%2520LSPP%2520tracking%2520method.%2520Besides%252C%2520the%250Aproposed%2520LITD%2520estimation%2520method%2520can%2520effectively%2520and%2520accurately%2520estimate%2520the%250ALITD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSC-LIO%3A%20An%20MSCKF-Based%20LiDAR-Inertial%20Odometry%20with%20Same-Plane-Point%0A%20%20Tracking&entry.906535625=Tisheng%20Zhang%20and%20Man%20Yuan%20and%20Linfu%20Wei%20and%20Hailiang%20Tang%20and%20Xiaoji%20Niu&entry.1292438233=%20%20The%20multi-state%20constraint%20Kalman%20filter%20%28MSCKF%29%20has%20been%20proven%20to%20be%20more%0Aefficient%20than%20graph%20optimization%20for%20visual-based%20odometry%20while%20with%20similar%0Aaccuracy.%20However%2C%20it%20has%20not%20yet%20been%20properly%20considered%20and%20studied%20for%0ALiDAR-based%20odometry.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20tightly%20coupled%0ALiDAR-inertial%20odometry%20based%20on%20the%20MSCKF%20framework%2C%20named%20MSC-LIO.%20An%0Aefficient%20LiDAR%20same-plane-point%20%28LSPP%29%20tracking%20method%2C%20without%20explicit%0Afeature%20extraction%2C%20is%20present%20for%20frame-to-frame%20data%20associations.%20The%0Atracked%20LSPPs%20are%20employed%20to%20build%20an%20LSPP%20measurement%20model%2C%20which%20constructs%0Aa%20multi-state%20constraint.%20Besides%2C%20we%20propose%20an%20effective%20point-velocity-based%0ALiDAR-IMU%20time-delay%20%28LITD%29%20estimation%20method%2C%20which%20is%20derived%20from%20the%0Aproposed%20LSPP%20tracking%20method.%20Extensive%20experiments%20were%20conducted%20on%20both%0Apublic%20and%20private%20datasets.%20The%20results%20demonstrate%20that%20the%20proposed%20MSC-LIO%0Ayields%20higher%20accuracy%20and%20efficiency%20than%20the%20state-of-the-art%20methods.%20The%0Aablation%20experiment%20results%20indicate%20that%20the%20data-association%20efficiency%20is%0Aimproved%20by%20nearly%203%20times%20using%20the%20LSPP%20tracking%20method.%20Besides%2C%20the%0Aproposed%20LITD%20estimation%20method%20can%20effectively%20and%20accurately%20estimate%20the%0ALITD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07589v1&entry.124074799=Read"},
{"title": "Progressive Growing of Patch Size: Resource-Efficient Curriculum\n  Learning for Dense Prediction Tasks", "author": "Stefan M. Fischer and Lina Felsner and Richard Osuala and Johannes Kiechle and Daniel M. Lang and Jan C. Peeken and Julia A. Schnabel", "abstract": "  In this work, we introduce Progressive Growing of Patch Size, a\nresource-efficient implicit curriculum learning approach for dense prediction\ntasks. Our curriculum approach is defined by growing the patch size during\nmodel training, which gradually increases the task's difficulty. We integrated\nour curriculum into the nnU-Net framework and evaluated the methodology on all\n10 tasks of the Medical Segmentation Decathlon. With our approach, we are able\nto substantially reduce runtime, computational costs, and CO$_{2}$ emissions of\nnetwork training compared to classical constant patch size training. In our\nexperiments, the curriculum approach resulted in improved convergence. We are\nable to outperform standard nnU-Net training, which is trained with constant\npatch size, in terms of Dice Score on 7 out of 10 MSD tasks while only spending\nroughly 50\\% of the original training runtime. To the best of our knowledge,\nour Progressive Growing of Patch Size is the first successful employment of a\nsample-length curriculum in the form of patch size in the field of computer\nvision. Our code is publicly available at \\url{https://github.com}.\n", "link": "http://arxiv.org/abs/2407.07853v1", "date": "2024-07-10", "relevancy": 2.0761, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.533}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5249}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Growing%20of%20Patch%20Size%3A%20Resource-Efficient%20Curriculum%0A%20%20Learning%20for%20Dense%20Prediction%20Tasks&body=Title%3A%20Progressive%20Growing%20of%20Patch%20Size%3A%20Resource-Efficient%20Curriculum%0A%20%20Learning%20for%20Dense%20Prediction%20Tasks%0AAuthor%3A%20Stefan%20M.%20Fischer%20and%20Lina%20Felsner%20and%20Richard%20Osuala%20and%20Johannes%20Kiechle%20and%20Daniel%20M.%20Lang%20and%20Jan%20C.%20Peeken%20and%20Julia%20A.%20Schnabel%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20Progressive%20Growing%20of%20Patch%20Size%2C%20a%0Aresource-efficient%20implicit%20curriculum%20learning%20approach%20for%20dense%20prediction%0Atasks.%20Our%20curriculum%20approach%20is%20defined%20by%20growing%20the%20patch%20size%20during%0Amodel%20training%2C%20which%20gradually%20increases%20the%20task%27s%20difficulty.%20We%20integrated%0Aour%20curriculum%20into%20the%20nnU-Net%20framework%20and%20evaluated%20the%20methodology%20on%20all%0A10%20tasks%20of%20the%20Medical%20Segmentation%20Decathlon.%20With%20our%20approach%2C%20we%20are%20able%0Ato%20substantially%20reduce%20runtime%2C%20computational%20costs%2C%20and%20CO%24_%7B2%7D%24%20emissions%20of%0Anetwork%20training%20compared%20to%20classical%20constant%20patch%20size%20training.%20In%20our%0Aexperiments%2C%20the%20curriculum%20approach%20resulted%20in%20improved%20convergence.%20We%20are%0Aable%20to%20outperform%20standard%20nnU-Net%20training%2C%20which%20is%20trained%20with%20constant%0Apatch%20size%2C%20in%20terms%20of%20Dice%20Score%20on%207%20out%20of%2010%20MSD%20tasks%20while%20only%20spending%0Aroughly%2050%5C%25%20of%20the%20original%20training%20runtime.%20To%20the%20best%20of%20our%20knowledge%2C%0Aour%20Progressive%20Growing%20of%20Patch%20Size%20is%20the%20first%20successful%20employment%20of%20a%0Asample-length%20curriculum%20in%20the%20form%20of%20patch%20size%20in%20the%20field%20of%20computer%0Avision.%20Our%20code%20is%20publicly%20available%20at%20%5Curl%7Bhttps%3A//github.com%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Growing%2520of%2520Patch%2520Size%253A%2520Resource-Efficient%2520Curriculum%250A%2520%2520Learning%2520for%2520Dense%2520Prediction%2520Tasks%26entry.906535625%3DStefan%2520M.%2520Fischer%2520and%2520Lina%2520Felsner%2520and%2520Richard%2520Osuala%2520and%2520Johannes%2520Kiechle%2520and%2520Daniel%2520M.%2520Lang%2520and%2520Jan%2520C.%2520Peeken%2520and%2520Julia%2520A.%2520Schnabel%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Progressive%2520Growing%2520of%2520Patch%2520Size%252C%2520a%250Aresource-efficient%2520implicit%2520curriculum%2520learning%2520approach%2520for%2520dense%2520prediction%250Atasks.%2520Our%2520curriculum%2520approach%2520is%2520defined%2520by%2520growing%2520the%2520patch%2520size%2520during%250Amodel%2520training%252C%2520which%2520gradually%2520increases%2520the%2520task%2527s%2520difficulty.%2520We%2520integrated%250Aour%2520curriculum%2520into%2520the%2520nnU-Net%2520framework%2520and%2520evaluated%2520the%2520methodology%2520on%2520all%250A10%2520tasks%2520of%2520the%2520Medical%2520Segmentation%2520Decathlon.%2520With%2520our%2520approach%252C%2520we%2520are%2520able%250Ato%2520substantially%2520reduce%2520runtime%252C%2520computational%2520costs%252C%2520and%2520CO%2524_%257B2%257D%2524%2520emissions%2520of%250Anetwork%2520training%2520compared%2520to%2520classical%2520constant%2520patch%2520size%2520training.%2520In%2520our%250Aexperiments%252C%2520the%2520curriculum%2520approach%2520resulted%2520in%2520improved%2520convergence.%2520We%2520are%250Aable%2520to%2520outperform%2520standard%2520nnU-Net%2520training%252C%2520which%2520is%2520trained%2520with%2520constant%250Apatch%2520size%252C%2520in%2520terms%2520of%2520Dice%2520Score%2520on%25207%2520out%2520of%252010%2520MSD%2520tasks%2520while%2520only%2520spending%250Aroughly%252050%255C%2525%2520of%2520the%2520original%2520training%2520runtime.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250Aour%2520Progressive%2520Growing%2520of%2520Patch%2520Size%2520is%2520the%2520first%2520successful%2520employment%2520of%2520a%250Asample-length%2520curriculum%2520in%2520the%2520form%2520of%2520patch%2520size%2520in%2520the%2520field%2520of%2520computer%250Avision.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Growing%20of%20Patch%20Size%3A%20Resource-Efficient%20Curriculum%0A%20%20Learning%20for%20Dense%20Prediction%20Tasks&entry.906535625=Stefan%20M.%20Fischer%20and%20Lina%20Felsner%20and%20Richard%20Osuala%20and%20Johannes%20Kiechle%20and%20Daniel%20M.%20Lang%20and%20Jan%20C.%20Peeken%20and%20Julia%20A.%20Schnabel&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20Progressive%20Growing%20of%20Patch%20Size%2C%20a%0Aresource-efficient%20implicit%20curriculum%20learning%20approach%20for%20dense%20prediction%0Atasks.%20Our%20curriculum%20approach%20is%20defined%20by%20growing%20the%20patch%20size%20during%0Amodel%20training%2C%20which%20gradually%20increases%20the%20task%27s%20difficulty.%20We%20integrated%0Aour%20curriculum%20into%20the%20nnU-Net%20framework%20and%20evaluated%20the%20methodology%20on%20all%0A10%20tasks%20of%20the%20Medical%20Segmentation%20Decathlon.%20With%20our%20approach%2C%20we%20are%20able%0Ato%20substantially%20reduce%20runtime%2C%20computational%20costs%2C%20and%20CO%24_%7B2%7D%24%20emissions%20of%0Anetwork%20training%20compared%20to%20classical%20constant%20patch%20size%20training.%20In%20our%0Aexperiments%2C%20the%20curriculum%20approach%20resulted%20in%20improved%20convergence.%20We%20are%0Aable%20to%20outperform%20standard%20nnU-Net%20training%2C%20which%20is%20trained%20with%20constant%0Apatch%20size%2C%20in%20terms%20of%20Dice%20Score%20on%207%20out%20of%2010%20MSD%20tasks%20while%20only%20spending%0Aroughly%2050%5C%25%20of%20the%20original%20training%20runtime.%20To%20the%20best%20of%20our%20knowledge%2C%0Aour%20Progressive%20Growing%20of%20Patch%20Size%20is%20the%20first%20successful%20employment%20of%20a%0Asample-length%20curriculum%20in%20the%20form%20of%20patch%20size%20in%20the%20field%20of%20computer%0Avision.%20Our%20code%20is%20publicly%20available%20at%20%5Curl%7Bhttps%3A//github.com%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07853v1&entry.124074799=Read"},
{"title": "IRSAM: Advancing Segment Anything Model for Infrared Small Target\n  Detection", "author": "Mingjin Zhang and Yuchun Wang and Jie Guo and Yunsong Li and Xinbo Gao and Jing Zhang", "abstract": "  The recent Segment Anything Model (SAM) is a significant advancement in\nnatural image segmentation, exhibiting potent zero-shot performance suitable\nfor various downstream image segmentation tasks. However, directly utilizing\nthe pretrained SAM for Infrared Small Target Detection (IRSTD) task falls short\nin achieving satisfying performance due to a notable domain gap between natural\nand infrared images. Unlike a visible light camera, a thermal imager reveals an\nobject's temperature distribution by capturing infrared radiation. Small\ntargets often show a subtle temperature transition at the object's boundaries.\nTo address this issue, we propose the IRSAM model for IRSTD, which improves\nSAM's encoder-decoder architecture to learn better feature representation of\ninfrared small objects. Specifically, we design a Perona-Malik diffusion\n(PMD)-based block and incorporate it into multiple levels of SAM's encoder to\nhelp it capture essential structural features while suppressing noise.\nAdditionally, we devise a Granularity-Aware Decoder (GAD) to fuse the\nmulti-granularity feature from the encoder to capture structural information\nthat may be lost in long-distance modeling. Extensive experiments on the public\ndatasets, including NUAA-SIRST, NUDT-SIRST, and IRSTD-1K, validate the design\nchoice of IRSAM and its significant superiority over representative\nstate-of-the-art methods. The source code are available at:\ngithub.com/IPIC-Lab/IRSAM.\n", "link": "http://arxiv.org/abs/2407.07520v1", "date": "2024-07-10", "relevancy": 2.0701, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5271}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRSAM%3A%20Advancing%20Segment%20Anything%20Model%20for%20Infrared%20Small%20Target%0A%20%20Detection&body=Title%3A%20IRSAM%3A%20Advancing%20Segment%20Anything%20Model%20for%20Infrared%20Small%20Target%0A%20%20Detection%0AAuthor%3A%20Mingjin%20Zhang%20and%20Yuchun%20Wang%20and%20Jie%20Guo%20and%20Yunsong%20Li%20and%20Xinbo%20Gao%20and%20Jing%20Zhang%0AAbstract%3A%20%20%20The%20recent%20Segment%20Anything%20Model%20%28SAM%29%20is%20a%20significant%20advancement%20in%0Anatural%20image%20segmentation%2C%20exhibiting%20potent%20zero-shot%20performance%20suitable%0Afor%20various%20downstream%20image%20segmentation%20tasks.%20However%2C%20directly%20utilizing%0Athe%20pretrained%20SAM%20for%20Infrared%20Small%20Target%20Detection%20%28IRSTD%29%20task%20falls%20short%0Ain%20achieving%20satisfying%20performance%20due%20to%20a%20notable%20domain%20gap%20between%20natural%0Aand%20infrared%20images.%20Unlike%20a%20visible%20light%20camera%2C%20a%20thermal%20imager%20reveals%20an%0Aobject%27s%20temperature%20distribution%20by%20capturing%20infrared%20radiation.%20Small%0Atargets%20often%20show%20a%20subtle%20temperature%20transition%20at%20the%20object%27s%20boundaries.%0ATo%20address%20this%20issue%2C%20we%20propose%20the%20IRSAM%20model%20for%20IRSTD%2C%20which%20improves%0ASAM%27s%20encoder-decoder%20architecture%20to%20learn%20better%20feature%20representation%20of%0Ainfrared%20small%20objects.%20Specifically%2C%20we%20design%20a%20Perona-Malik%20diffusion%0A%28PMD%29-based%20block%20and%20incorporate%20it%20into%20multiple%20levels%20of%20SAM%27s%20encoder%20to%0Ahelp%20it%20capture%20essential%20structural%20features%20while%20suppressing%20noise.%0AAdditionally%2C%20we%20devise%20a%20Granularity-Aware%20Decoder%20%28GAD%29%20to%20fuse%20the%0Amulti-granularity%20feature%20from%20the%20encoder%20to%20capture%20structural%20information%0Athat%20may%20be%20lost%20in%20long-distance%20modeling.%20Extensive%20experiments%20on%20the%20public%0Adatasets%2C%20including%20NUAA-SIRST%2C%20NUDT-SIRST%2C%20and%20IRSTD-1K%2C%20validate%20the%20design%0Achoice%20of%20IRSAM%20and%20its%20significant%20superiority%20over%20representative%0Astate-of-the-art%20methods.%20The%20source%20code%20are%20available%20at%3A%0Agithub.com/IPIC-Lab/IRSAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRSAM%253A%2520Advancing%2520Segment%2520Anything%2520Model%2520for%2520Infrared%2520Small%2520Target%250A%2520%2520Detection%26entry.906535625%3DMingjin%2520Zhang%2520and%2520Yuchun%2520Wang%2520and%2520Jie%2520Guo%2520and%2520Yunsong%2520Li%2520and%2520Xinbo%2520Gao%2520and%2520Jing%2520Zhang%26entry.1292438233%3D%2520%2520The%2520recent%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520is%2520a%2520significant%2520advancement%2520in%250Anatural%2520image%2520segmentation%252C%2520exhibiting%2520potent%2520zero-shot%2520performance%2520suitable%250Afor%2520various%2520downstream%2520image%2520segmentation%2520tasks.%2520However%252C%2520directly%2520utilizing%250Athe%2520pretrained%2520SAM%2520for%2520Infrared%2520Small%2520Target%2520Detection%2520%2528IRSTD%2529%2520task%2520falls%2520short%250Ain%2520achieving%2520satisfying%2520performance%2520due%2520to%2520a%2520notable%2520domain%2520gap%2520between%2520natural%250Aand%2520infrared%2520images.%2520Unlike%2520a%2520visible%2520light%2520camera%252C%2520a%2520thermal%2520imager%2520reveals%2520an%250Aobject%2527s%2520temperature%2520distribution%2520by%2520capturing%2520infrared%2520radiation.%2520Small%250Atargets%2520often%2520show%2520a%2520subtle%2520temperature%2520transition%2520at%2520the%2520object%2527s%2520boundaries.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520the%2520IRSAM%2520model%2520for%2520IRSTD%252C%2520which%2520improves%250ASAM%2527s%2520encoder-decoder%2520architecture%2520to%2520learn%2520better%2520feature%2520representation%2520of%250Ainfrared%2520small%2520objects.%2520Specifically%252C%2520we%2520design%2520a%2520Perona-Malik%2520diffusion%250A%2528PMD%2529-based%2520block%2520and%2520incorporate%2520it%2520into%2520multiple%2520levels%2520of%2520SAM%2527s%2520encoder%2520to%250Ahelp%2520it%2520capture%2520essential%2520structural%2520features%2520while%2520suppressing%2520noise.%250AAdditionally%252C%2520we%2520devise%2520a%2520Granularity-Aware%2520Decoder%2520%2528GAD%2529%2520to%2520fuse%2520the%250Amulti-granularity%2520feature%2520from%2520the%2520encoder%2520to%2520capture%2520structural%2520information%250Athat%2520may%2520be%2520lost%2520in%2520long-distance%2520modeling.%2520Extensive%2520experiments%2520on%2520the%2520public%250Adatasets%252C%2520including%2520NUAA-SIRST%252C%2520NUDT-SIRST%252C%2520and%2520IRSTD-1K%252C%2520validate%2520the%2520design%250Achoice%2520of%2520IRSAM%2520and%2520its%2520significant%2520superiority%2520over%2520representative%250Astate-of-the-art%2520methods.%2520The%2520source%2520code%2520are%2520available%2520at%253A%250Agithub.com/IPIC-Lab/IRSAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRSAM%3A%20Advancing%20Segment%20Anything%20Model%20for%20Infrared%20Small%20Target%0A%20%20Detection&entry.906535625=Mingjin%20Zhang%20and%20Yuchun%20Wang%20and%20Jie%20Guo%20and%20Yunsong%20Li%20and%20Xinbo%20Gao%20and%20Jing%20Zhang&entry.1292438233=%20%20The%20recent%20Segment%20Anything%20Model%20%28SAM%29%20is%20a%20significant%20advancement%20in%0Anatural%20image%20segmentation%2C%20exhibiting%20potent%20zero-shot%20performance%20suitable%0Afor%20various%20downstream%20image%20segmentation%20tasks.%20However%2C%20directly%20utilizing%0Athe%20pretrained%20SAM%20for%20Infrared%20Small%20Target%20Detection%20%28IRSTD%29%20task%20falls%20short%0Ain%20achieving%20satisfying%20performance%20due%20to%20a%20notable%20domain%20gap%20between%20natural%0Aand%20infrared%20images.%20Unlike%20a%20visible%20light%20camera%2C%20a%20thermal%20imager%20reveals%20an%0Aobject%27s%20temperature%20distribution%20by%20capturing%20infrared%20radiation.%20Small%0Atargets%20often%20show%20a%20subtle%20temperature%20transition%20at%20the%20object%27s%20boundaries.%0ATo%20address%20this%20issue%2C%20we%20propose%20the%20IRSAM%20model%20for%20IRSTD%2C%20which%20improves%0ASAM%27s%20encoder-decoder%20architecture%20to%20learn%20better%20feature%20representation%20of%0Ainfrared%20small%20objects.%20Specifically%2C%20we%20design%20a%20Perona-Malik%20diffusion%0A%28PMD%29-based%20block%20and%20incorporate%20it%20into%20multiple%20levels%20of%20SAM%27s%20encoder%20to%0Ahelp%20it%20capture%20essential%20structural%20features%20while%20suppressing%20noise.%0AAdditionally%2C%20we%20devise%20a%20Granularity-Aware%20Decoder%20%28GAD%29%20to%20fuse%20the%0Amulti-granularity%20feature%20from%20the%20encoder%20to%20capture%20structural%20information%0Athat%20may%20be%20lost%20in%20long-distance%20modeling.%20Extensive%20experiments%20on%20the%20public%0Adatasets%2C%20including%20NUAA-SIRST%2C%20NUDT-SIRST%2C%20and%20IRSTD-1K%2C%20validate%20the%20design%0Achoice%20of%20IRSAM%20and%20its%20significant%20superiority%20over%20representative%0Astate-of-the-art%20methods.%20The%20source%20code%20are%20available%20at%3A%0Agithub.com/IPIC-Lab/IRSAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07520v1&entry.124074799=Read"},
{"title": "Continuous Control with Coarse-to-fine Reinforcement Learning", "author": "Younggyo Seo and Jafar Uru\u00e7 and Stephen James", "abstract": "  Despite recent advances in improving the sample-efficiency of reinforcement\nlearning (RL) algorithms, designing an RL algorithm that can be practically\ndeployed in real-world environments remains a challenge. In this paper, we\npresent Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL\nagents to zoom-into a continuous action space in a coarse-to-fine manner,\nenabling the use of stable, sample-efficient value-based RL algorithms for\nfine-grained continuous control tasks. Our key idea is to train agents that\noutput actions by iterating the procedure of (i) discretizing the continuous\naction space into multiple intervals and (ii) selecting the interval with the\nhighest Q-value to further discretize at the next level. We then introduce a\nconcrete, value-based algorithm within the CRL framework called Coarse-to-fine\nQ-Network (CQN). Our experiments demonstrate that CQN significantly outperforms\nRL and behavior cloning baselines on 20 sparsely-rewarded RLBench manipulation\ntasks with a modest number of environment interactions and expert\ndemonstrations. We also show that CQN robustly learns to solve real-world\nmanipulation tasks within a few minutes of online training.\n", "link": "http://arxiv.org/abs/2407.07787v1", "date": "2024-07-10", "relevancy": 2.0676, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5573}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5334}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Control%20with%20Coarse-to-fine%20Reinforcement%20Learning&body=Title%3A%20Continuous%20Control%20with%20Coarse-to-fine%20Reinforcement%20Learning%0AAuthor%3A%20Younggyo%20Seo%20and%20Jafar%20Uru%C3%A7%20and%20Stephen%20James%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20improving%20the%20sample-efficiency%20of%20reinforcement%0Alearning%20%28RL%29%20algorithms%2C%20designing%20an%20RL%20algorithm%20that%20can%20be%20practically%0Adeployed%20in%20real-world%20environments%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%0Apresent%20Coarse-to-fine%20Reinforcement%20Learning%20%28CRL%29%2C%20a%20framework%20that%20trains%20RL%0Aagents%20to%20zoom-into%20a%20continuous%20action%20space%20in%20a%20coarse-to-fine%20manner%2C%0Aenabling%20the%20use%20of%20stable%2C%20sample-efficient%20value-based%20RL%20algorithms%20for%0Afine-grained%20continuous%20control%20tasks.%20Our%20key%20idea%20is%20to%20train%20agents%20that%0Aoutput%20actions%20by%20iterating%20the%20procedure%20of%20%28i%29%20discretizing%20the%20continuous%0Aaction%20space%20into%20multiple%20intervals%20and%20%28ii%29%20selecting%20the%20interval%20with%20the%0Ahighest%20Q-value%20to%20further%20discretize%20at%20the%20next%20level.%20We%20then%20introduce%20a%0Aconcrete%2C%20value-based%20algorithm%20within%20the%20CRL%20framework%20called%20Coarse-to-fine%0AQ-Network%20%28CQN%29.%20Our%20experiments%20demonstrate%20that%20CQN%20significantly%20outperforms%0ARL%20and%20behavior%20cloning%20baselines%20on%2020%20sparsely-rewarded%20RLBench%20manipulation%0Atasks%20with%20a%20modest%20number%20of%20environment%20interactions%20and%20expert%0Ademonstrations.%20We%20also%20show%20that%20CQN%20robustly%20learns%20to%20solve%20real-world%0Amanipulation%20tasks%20within%20a%20few%20minutes%20of%20online%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Control%2520with%2520Coarse-to-fine%2520Reinforcement%2520Learning%26entry.906535625%3DYounggyo%2520Seo%2520and%2520Jafar%2520Uru%25C3%25A7%2520and%2520Stephen%2520James%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520improving%2520the%2520sample-efficiency%2520of%2520reinforcement%250Alearning%2520%2528RL%2529%2520algorithms%252C%2520designing%2520an%2520RL%2520algorithm%2520that%2520can%2520be%2520practically%250Adeployed%2520in%2520real-world%2520environments%2520remains%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520Coarse-to-fine%2520Reinforcement%2520Learning%2520%2528CRL%2529%252C%2520a%2520framework%2520that%2520trains%2520RL%250Aagents%2520to%2520zoom-into%2520a%2520continuous%2520action%2520space%2520in%2520a%2520coarse-to-fine%2520manner%252C%250Aenabling%2520the%2520use%2520of%2520stable%252C%2520sample-efficient%2520value-based%2520RL%2520algorithms%2520for%250Afine-grained%2520continuous%2520control%2520tasks.%2520Our%2520key%2520idea%2520is%2520to%2520train%2520agents%2520that%250Aoutput%2520actions%2520by%2520iterating%2520the%2520procedure%2520of%2520%2528i%2529%2520discretizing%2520the%2520continuous%250Aaction%2520space%2520into%2520multiple%2520intervals%2520and%2520%2528ii%2529%2520selecting%2520the%2520interval%2520with%2520the%250Ahighest%2520Q-value%2520to%2520further%2520discretize%2520at%2520the%2520next%2520level.%2520We%2520then%2520introduce%2520a%250Aconcrete%252C%2520value-based%2520algorithm%2520within%2520the%2520CRL%2520framework%2520called%2520Coarse-to-fine%250AQ-Network%2520%2528CQN%2529.%2520Our%2520experiments%2520demonstrate%2520that%2520CQN%2520significantly%2520outperforms%250ARL%2520and%2520behavior%2520cloning%2520baselines%2520on%252020%2520sparsely-rewarded%2520RLBench%2520manipulation%250Atasks%2520with%2520a%2520modest%2520number%2520of%2520environment%2520interactions%2520and%2520expert%250Ademonstrations.%2520We%2520also%2520show%2520that%2520CQN%2520robustly%2520learns%2520to%2520solve%2520real-world%250Amanipulation%2520tasks%2520within%2520a%2520few%2520minutes%2520of%2520online%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Control%20with%20Coarse-to-fine%20Reinforcement%20Learning&entry.906535625=Younggyo%20Seo%20and%20Jafar%20Uru%C3%A7%20and%20Stephen%20James&entry.1292438233=%20%20Despite%20recent%20advances%20in%20improving%20the%20sample-efficiency%20of%20reinforcement%0Alearning%20%28RL%29%20algorithms%2C%20designing%20an%20RL%20algorithm%20that%20can%20be%20practically%0Adeployed%20in%20real-world%20environments%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%0Apresent%20Coarse-to-fine%20Reinforcement%20Learning%20%28CRL%29%2C%20a%20framework%20that%20trains%20RL%0Aagents%20to%20zoom-into%20a%20continuous%20action%20space%20in%20a%20coarse-to-fine%20manner%2C%0Aenabling%20the%20use%20of%20stable%2C%20sample-efficient%20value-based%20RL%20algorithms%20for%0Afine-grained%20continuous%20control%20tasks.%20Our%20key%20idea%20is%20to%20train%20agents%20that%0Aoutput%20actions%20by%20iterating%20the%20procedure%20of%20%28i%29%20discretizing%20the%20continuous%0Aaction%20space%20into%20multiple%20intervals%20and%20%28ii%29%20selecting%20the%20interval%20with%20the%0Ahighest%20Q-value%20to%20further%20discretize%20at%20the%20next%20level.%20We%20then%20introduce%20a%0Aconcrete%2C%20value-based%20algorithm%20within%20the%20CRL%20framework%20called%20Coarse-to-fine%0AQ-Network%20%28CQN%29.%20Our%20experiments%20demonstrate%20that%20CQN%20significantly%20outperforms%0ARL%20and%20behavior%20cloning%20baselines%20on%2020%20sparsely-rewarded%20RLBench%20manipulation%0Atasks%20with%20a%20modest%20number%20of%20environment%20interactions%20and%20expert%0Ademonstrations.%20We%20also%20show%20that%20CQN%20robustly%20learns%20to%20solve%20real-world%0Amanipulation%20tasks%20within%20a%20few%20minutes%20of%20online%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07787v1&entry.124074799=Read"},
{"title": "Multivector Neurons: Better and Faster O(n)-Equivariant Clifford Graph\n  Neural Networks", "author": "Cong Liu and David Ruhe and Patrick Forr\u00e9", "abstract": "  Most current deep learning models equivariant to $O(n)$ or $SO(n)$ either\nconsider mostly scalar information such as distances and angles or have a very\nhigh computational complexity. In this work, we test a few novel message\npassing graph neural networks (GNNs) based on Clifford multivectors, structured\nsimilarly to other prevalent equivariant models in geometric deep learning. Our\napproach leverages efficient invariant scalar features while simultaneously\nperforming expressive learning on multivector representations, particularly\nthrough the use of the equivariant geometric product operator. By integrating\nthese elements, our methods outperform established efficient baseline models on\nan N-Body simulation task and protein denoising task while maintaining a high\nefficiency. In particular, we push the state-of-the-art error on the N-body\ndataset to 0.0035 (averaged over 3 runs); an 8% improvement over recent\nmethods. Our implementation is available on Github.\n", "link": "http://arxiv.org/abs/2406.04052v2", "date": "2024-07-10", "relevancy": 2.0587, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5247}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5199}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multivector%20Neurons%3A%20Better%20and%20Faster%20O%28n%29-Equivariant%20Clifford%20Graph%0A%20%20Neural%20Networks&body=Title%3A%20Multivector%20Neurons%3A%20Better%20and%20Faster%20O%28n%29-Equivariant%20Clifford%20Graph%0A%20%20Neural%20Networks%0AAuthor%3A%20Cong%20Liu%20and%20David%20Ruhe%20and%20Patrick%20Forr%C3%A9%0AAbstract%3A%20%20%20Most%20current%20deep%20learning%20models%20equivariant%20to%20%24O%28n%29%24%20or%20%24SO%28n%29%24%20either%0Aconsider%20mostly%20scalar%20information%20such%20as%20distances%20and%20angles%20or%20have%20a%20very%0Ahigh%20computational%20complexity.%20In%20this%20work%2C%20we%20test%20a%20few%20novel%20message%0Apassing%20graph%20neural%20networks%20%28GNNs%29%20based%20on%20Clifford%20multivectors%2C%20structured%0Asimilarly%20to%20other%20prevalent%20equivariant%20models%20in%20geometric%20deep%20learning.%20Our%0Aapproach%20leverages%20efficient%20invariant%20scalar%20features%20while%20simultaneously%0Aperforming%20expressive%20learning%20on%20multivector%20representations%2C%20particularly%0Athrough%20the%20use%20of%20the%20equivariant%20geometric%20product%20operator.%20By%20integrating%0Athese%20elements%2C%20our%20methods%20outperform%20established%20efficient%20baseline%20models%20on%0Aan%20N-Body%20simulation%20task%20and%20protein%20denoising%20task%20while%20maintaining%20a%20high%0Aefficiency.%20In%20particular%2C%20we%20push%20the%20state-of-the-art%20error%20on%20the%20N-body%0Adataset%20to%200.0035%20%28averaged%20over%203%20runs%29%3B%20an%208%25%20improvement%20over%20recent%0Amethods.%20Our%20implementation%20is%20available%20on%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultivector%2520Neurons%253A%2520Better%2520and%2520Faster%2520O%2528n%2529-Equivariant%2520Clifford%2520Graph%250A%2520%2520Neural%2520Networks%26entry.906535625%3DCong%2520Liu%2520and%2520David%2520Ruhe%2520and%2520Patrick%2520Forr%25C3%25A9%26entry.1292438233%3D%2520%2520Most%2520current%2520deep%2520learning%2520models%2520equivariant%2520to%2520%2524O%2528n%2529%2524%2520or%2520%2524SO%2528n%2529%2524%2520either%250Aconsider%2520mostly%2520scalar%2520information%2520such%2520as%2520distances%2520and%2520angles%2520or%2520have%2520a%2520very%250Ahigh%2520computational%2520complexity.%2520In%2520this%2520work%252C%2520we%2520test%2520a%2520few%2520novel%2520message%250Apassing%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520based%2520on%2520Clifford%2520multivectors%252C%2520structured%250Asimilarly%2520to%2520other%2520prevalent%2520equivariant%2520models%2520in%2520geometric%2520deep%2520learning.%2520Our%250Aapproach%2520leverages%2520efficient%2520invariant%2520scalar%2520features%2520while%2520simultaneously%250Aperforming%2520expressive%2520learning%2520on%2520multivector%2520representations%252C%2520particularly%250Athrough%2520the%2520use%2520of%2520the%2520equivariant%2520geometric%2520product%2520operator.%2520By%2520integrating%250Athese%2520elements%252C%2520our%2520methods%2520outperform%2520established%2520efficient%2520baseline%2520models%2520on%250Aan%2520N-Body%2520simulation%2520task%2520and%2520protein%2520denoising%2520task%2520while%2520maintaining%2520a%2520high%250Aefficiency.%2520In%2520particular%252C%2520we%2520push%2520the%2520state-of-the-art%2520error%2520on%2520the%2520N-body%250Adataset%2520to%25200.0035%2520%2528averaged%2520over%25203%2520runs%2529%253B%2520an%25208%2525%2520improvement%2520over%2520recent%250Amethods.%2520Our%2520implementation%2520is%2520available%2520on%2520Github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multivector%20Neurons%3A%20Better%20and%20Faster%20O%28n%29-Equivariant%20Clifford%20Graph%0A%20%20Neural%20Networks&entry.906535625=Cong%20Liu%20and%20David%20Ruhe%20and%20Patrick%20Forr%C3%A9&entry.1292438233=%20%20Most%20current%20deep%20learning%20models%20equivariant%20to%20%24O%28n%29%24%20or%20%24SO%28n%29%24%20either%0Aconsider%20mostly%20scalar%20information%20such%20as%20distances%20and%20angles%20or%20have%20a%20very%0Ahigh%20computational%20complexity.%20In%20this%20work%2C%20we%20test%20a%20few%20novel%20message%0Apassing%20graph%20neural%20networks%20%28GNNs%29%20based%20on%20Clifford%20multivectors%2C%20structured%0Asimilarly%20to%20other%20prevalent%20equivariant%20models%20in%20geometric%20deep%20learning.%20Our%0Aapproach%20leverages%20efficient%20invariant%20scalar%20features%20while%20simultaneously%0Aperforming%20expressive%20learning%20on%20multivector%20representations%2C%20particularly%0Athrough%20the%20use%20of%20the%20equivariant%20geometric%20product%20operator.%20By%20integrating%0Athese%20elements%2C%20our%20methods%20outperform%20established%20efficient%20baseline%20models%20on%0Aan%20N-Body%20simulation%20task%20and%20protein%20denoising%20task%20while%20maintaining%20a%20high%0Aefficiency.%20In%20particular%2C%20we%20push%20the%20state-of-the-art%20error%20on%20the%20N-body%0Adataset%20to%200.0035%20%28averaged%20over%203%20runs%29%3B%20an%208%25%20improvement%20over%20recent%0Amethods.%20Our%20implementation%20is%20available%20on%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04052v2&entry.124074799=Read"},
{"title": "Learning treatment effects while treating those in need", "author": "Bryan Wilder and Pim Welle", "abstract": "  Many social programs attempt to allocate scarce resources to people with the\ngreatest need. Indeed, public services increasingly use algorithmic risk\nassessments motivated by this goal. However, targeting the highest-need\nrecipients often conflicts with attempting to evaluate the causal effect of the\nprogram as a whole, as the best evaluations would be obtained by randomizing\nthe allocation. We propose a framework to design randomized allocation rules\nwhich optimally balance targeting high-need individuals with learning treatment\neffects, presenting policymakers with a Pareto frontier between the two goals.\nWe give sample complexity guarantees for the policy learning problem and\nprovide a computationally efficient strategy to implement it. We then apply our\nframework to data from human services in Allegheny County, Pennsylvania.\nOptimized policies can substantially mitigate the tradeoff between learning and\ntargeting. For example, it is often possible to obtain 90% of the optimal\nutility in targeting high-need individuals while ensuring that the average\ntreatment effect can be estimated with less than 2 times the samples that a\nrandomized controlled trial would require. Mechanisms for targeting public\nservices often focus on measuring need as accurately as possible. However, our\nresults suggest that algorithmic systems in public services can be most\nimpactful if they incorporate program evaluation as an explicit goal alongside\ntargeting.\n", "link": "http://arxiv.org/abs/2407.07596v1", "date": "2024-07-10", "relevancy": 2.0586, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4121}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4116}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20treatment%20effects%20while%20treating%20those%20in%20need&body=Title%3A%20Learning%20treatment%20effects%20while%20treating%20those%20in%20need%0AAuthor%3A%20Bryan%20Wilder%20and%20Pim%20Welle%0AAbstract%3A%20%20%20Many%20social%20programs%20attempt%20to%20allocate%20scarce%20resources%20to%20people%20with%20the%0Agreatest%20need.%20Indeed%2C%20public%20services%20increasingly%20use%20algorithmic%20risk%0Aassessments%20motivated%20by%20this%20goal.%20However%2C%20targeting%20the%20highest-need%0Arecipients%20often%20conflicts%20with%20attempting%20to%20evaluate%20the%20causal%20effect%20of%20the%0Aprogram%20as%20a%20whole%2C%20as%20the%20best%20evaluations%20would%20be%20obtained%20by%20randomizing%0Athe%20allocation.%20We%20propose%20a%20framework%20to%20design%20randomized%20allocation%20rules%0Awhich%20optimally%20balance%20targeting%20high-need%20individuals%20with%20learning%20treatment%0Aeffects%2C%20presenting%20policymakers%20with%20a%20Pareto%20frontier%20between%20the%20two%20goals.%0AWe%20give%20sample%20complexity%20guarantees%20for%20the%20policy%20learning%20problem%20and%0Aprovide%20a%20computationally%20efficient%20strategy%20to%20implement%20it.%20We%20then%20apply%20our%0Aframework%20to%20data%20from%20human%20services%20in%20Allegheny%20County%2C%20Pennsylvania.%0AOptimized%20policies%20can%20substantially%20mitigate%20the%20tradeoff%20between%20learning%20and%0Atargeting.%20For%20example%2C%20it%20is%20often%20possible%20to%20obtain%2090%25%20of%20the%20optimal%0Autility%20in%20targeting%20high-need%20individuals%20while%20ensuring%20that%20the%20average%0Atreatment%20effect%20can%20be%20estimated%20with%20less%20than%202%20times%20the%20samples%20that%20a%0Arandomized%20controlled%20trial%20would%20require.%20Mechanisms%20for%20targeting%20public%0Aservices%20often%20focus%20on%20measuring%20need%20as%20accurately%20as%20possible.%20However%2C%20our%0Aresults%20suggest%20that%20algorithmic%20systems%20in%20public%20services%20can%20be%20most%0Aimpactful%20if%20they%20incorporate%20program%20evaluation%20as%20an%20explicit%20goal%20alongside%0Atargeting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520treatment%2520effects%2520while%2520treating%2520those%2520in%2520need%26entry.906535625%3DBryan%2520Wilder%2520and%2520Pim%2520Welle%26entry.1292438233%3D%2520%2520Many%2520social%2520programs%2520attempt%2520to%2520allocate%2520scarce%2520resources%2520to%2520people%2520with%2520the%250Agreatest%2520need.%2520Indeed%252C%2520public%2520services%2520increasingly%2520use%2520algorithmic%2520risk%250Aassessments%2520motivated%2520by%2520this%2520goal.%2520However%252C%2520targeting%2520the%2520highest-need%250Arecipients%2520often%2520conflicts%2520with%2520attempting%2520to%2520evaluate%2520the%2520causal%2520effect%2520of%2520the%250Aprogram%2520as%2520a%2520whole%252C%2520as%2520the%2520best%2520evaluations%2520would%2520be%2520obtained%2520by%2520randomizing%250Athe%2520allocation.%2520We%2520propose%2520a%2520framework%2520to%2520design%2520randomized%2520allocation%2520rules%250Awhich%2520optimally%2520balance%2520targeting%2520high-need%2520individuals%2520with%2520learning%2520treatment%250Aeffects%252C%2520presenting%2520policymakers%2520with%2520a%2520Pareto%2520frontier%2520between%2520the%2520two%2520goals.%250AWe%2520give%2520sample%2520complexity%2520guarantees%2520for%2520the%2520policy%2520learning%2520problem%2520and%250Aprovide%2520a%2520computationally%2520efficient%2520strategy%2520to%2520implement%2520it.%2520We%2520then%2520apply%2520our%250Aframework%2520to%2520data%2520from%2520human%2520services%2520in%2520Allegheny%2520County%252C%2520Pennsylvania.%250AOptimized%2520policies%2520can%2520substantially%2520mitigate%2520the%2520tradeoff%2520between%2520learning%2520and%250Atargeting.%2520For%2520example%252C%2520it%2520is%2520often%2520possible%2520to%2520obtain%252090%2525%2520of%2520the%2520optimal%250Autility%2520in%2520targeting%2520high-need%2520individuals%2520while%2520ensuring%2520that%2520the%2520average%250Atreatment%2520effect%2520can%2520be%2520estimated%2520with%2520less%2520than%25202%2520times%2520the%2520samples%2520that%2520a%250Arandomized%2520controlled%2520trial%2520would%2520require.%2520Mechanisms%2520for%2520targeting%2520public%250Aservices%2520often%2520focus%2520on%2520measuring%2520need%2520as%2520accurately%2520as%2520possible.%2520However%252C%2520our%250Aresults%2520suggest%2520that%2520algorithmic%2520systems%2520in%2520public%2520services%2520can%2520be%2520most%250Aimpactful%2520if%2520they%2520incorporate%2520program%2520evaluation%2520as%2520an%2520explicit%2520goal%2520alongside%250Atargeting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20treatment%20effects%20while%20treating%20those%20in%20need&entry.906535625=Bryan%20Wilder%20and%20Pim%20Welle&entry.1292438233=%20%20Many%20social%20programs%20attempt%20to%20allocate%20scarce%20resources%20to%20people%20with%20the%0Agreatest%20need.%20Indeed%2C%20public%20services%20increasingly%20use%20algorithmic%20risk%0Aassessments%20motivated%20by%20this%20goal.%20However%2C%20targeting%20the%20highest-need%0Arecipients%20often%20conflicts%20with%20attempting%20to%20evaluate%20the%20causal%20effect%20of%20the%0Aprogram%20as%20a%20whole%2C%20as%20the%20best%20evaluations%20would%20be%20obtained%20by%20randomizing%0Athe%20allocation.%20We%20propose%20a%20framework%20to%20design%20randomized%20allocation%20rules%0Awhich%20optimally%20balance%20targeting%20high-need%20individuals%20with%20learning%20treatment%0Aeffects%2C%20presenting%20policymakers%20with%20a%20Pareto%20frontier%20between%20the%20two%20goals.%0AWe%20give%20sample%20complexity%20guarantees%20for%20the%20policy%20learning%20problem%20and%0Aprovide%20a%20computationally%20efficient%20strategy%20to%20implement%20it.%20We%20then%20apply%20our%0Aframework%20to%20data%20from%20human%20services%20in%20Allegheny%20County%2C%20Pennsylvania.%0AOptimized%20policies%20can%20substantially%20mitigate%20the%20tradeoff%20between%20learning%20and%0Atargeting.%20For%20example%2C%20it%20is%20often%20possible%20to%20obtain%2090%25%20of%20the%20optimal%0Autility%20in%20targeting%20high-need%20individuals%20while%20ensuring%20that%20the%20average%0Atreatment%20effect%20can%20be%20estimated%20with%20less%20than%202%20times%20the%20samples%20that%20a%0Arandomized%20controlled%20trial%20would%20require.%20Mechanisms%20for%20targeting%20public%0Aservices%20often%20focus%20on%20measuring%20need%20as%20accurately%20as%20possible.%20However%2C%20our%0Aresults%20suggest%20that%20algorithmic%20systems%20in%20public%20services%20can%20be%20most%0Aimpactful%20if%20they%20incorporate%20program%20evaluation%20as%20an%20explicit%20goal%20alongside%0Atargeting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07596v1&entry.124074799=Read"},
{"title": "CAPformer: Compression-Aware Pre-trained Transformer for Low-Light Image\n  Enhancement", "author": "Wei Wang and Zhi Jin", "abstract": "  Low-Light Image Enhancement (LLIE) has advanced with the surge in phone\nphotography demand, yet many existing methods neglect compression, a crucial\nconcern for resource-constrained phone photography. Most LLIE methods overlook\nthis, hindering their effectiveness. In this study, we investigate the effects\nof JPEG compression on low-light images and reveal substantial information loss\ncaused by JPEG due to widespread low pixel values in dark areas. Hence, we\npropose the Compression-Aware Pre-trained Transformer (CAPformer), employing a\nnovel pre-training strategy to learn lossless information from uncompressed\nlow-light images. Additionally, the proposed Brightness-Guided Self-Attention\n(BGSA) mechanism enhances rational information gathering. Experiments\ndemonstrate the superiority of our approach in mitigating compression effects\non LLIE, showcasing its potential for improving LLIE in resource-constrained\nscenarios.\n", "link": "http://arxiv.org/abs/2407.07056v2", "date": "2024-07-10", "relevancy": 2.0565, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5193}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5167}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAPformer%3A%20Compression-Aware%20Pre-trained%20Transformer%20for%20Low-Light%20Image%0A%20%20Enhancement&body=Title%3A%20CAPformer%3A%20Compression-Aware%20Pre-trained%20Transformer%20for%20Low-Light%20Image%0A%20%20Enhancement%0AAuthor%3A%20Wei%20Wang%20and%20Zhi%20Jin%0AAbstract%3A%20%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20has%20advanced%20with%20the%20surge%20in%20phone%0Aphotography%20demand%2C%20yet%20many%20existing%20methods%20neglect%20compression%2C%20a%20crucial%0Aconcern%20for%20resource-constrained%20phone%20photography.%20Most%20LLIE%20methods%20overlook%0Athis%2C%20hindering%20their%20effectiveness.%20In%20this%20study%2C%20we%20investigate%20the%20effects%0Aof%20JPEG%20compression%20on%20low-light%20images%20and%20reveal%20substantial%20information%20loss%0Acaused%20by%20JPEG%20due%20to%20widespread%20low%20pixel%20values%20in%20dark%20areas.%20Hence%2C%20we%0Apropose%20the%20Compression-Aware%20Pre-trained%20Transformer%20%28CAPformer%29%2C%20employing%20a%0Anovel%20pre-training%20strategy%20to%20learn%20lossless%20information%20from%20uncompressed%0Alow-light%20images.%20Additionally%2C%20the%20proposed%20Brightness-Guided%20Self-Attention%0A%28BGSA%29%20mechanism%20enhances%20rational%20information%20gathering.%20Experiments%0Ademonstrate%20the%20superiority%20of%20our%20approach%20in%20mitigating%20compression%20effects%0Aon%20LLIE%2C%20showcasing%20its%20potential%20for%20improving%20LLIE%20in%20resource-constrained%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07056v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAPformer%253A%2520Compression-Aware%2520Pre-trained%2520Transformer%2520for%2520Low-Light%2520Image%250A%2520%2520Enhancement%26entry.906535625%3DWei%2520Wang%2520and%2520Zhi%2520Jin%26entry.1292438233%3D%2520%2520Low-Light%2520Image%2520Enhancement%2520%2528LLIE%2529%2520has%2520advanced%2520with%2520the%2520surge%2520in%2520phone%250Aphotography%2520demand%252C%2520yet%2520many%2520existing%2520methods%2520neglect%2520compression%252C%2520a%2520crucial%250Aconcern%2520for%2520resource-constrained%2520phone%2520photography.%2520Most%2520LLIE%2520methods%2520overlook%250Athis%252C%2520hindering%2520their%2520effectiveness.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520effects%250Aof%2520JPEG%2520compression%2520on%2520low-light%2520images%2520and%2520reveal%2520substantial%2520information%2520loss%250Acaused%2520by%2520JPEG%2520due%2520to%2520widespread%2520low%2520pixel%2520values%2520in%2520dark%2520areas.%2520Hence%252C%2520we%250Apropose%2520the%2520Compression-Aware%2520Pre-trained%2520Transformer%2520%2528CAPformer%2529%252C%2520employing%2520a%250Anovel%2520pre-training%2520strategy%2520to%2520learn%2520lossless%2520information%2520from%2520uncompressed%250Alow-light%2520images.%2520Additionally%252C%2520the%2520proposed%2520Brightness-Guided%2520Self-Attention%250A%2528BGSA%2529%2520mechanism%2520enhances%2520rational%2520information%2520gathering.%2520Experiments%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520in%2520mitigating%2520compression%2520effects%250Aon%2520LLIE%252C%2520showcasing%2520its%2520potential%2520for%2520improving%2520LLIE%2520in%2520resource-constrained%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07056v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPformer%3A%20Compression-Aware%20Pre-trained%20Transformer%20for%20Low-Light%20Image%0A%20%20Enhancement&entry.906535625=Wei%20Wang%20and%20Zhi%20Jin&entry.1292438233=%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20has%20advanced%20with%20the%20surge%20in%20phone%0Aphotography%20demand%2C%20yet%20many%20existing%20methods%20neglect%20compression%2C%20a%20crucial%0Aconcern%20for%20resource-constrained%20phone%20photography.%20Most%20LLIE%20methods%20overlook%0Athis%2C%20hindering%20their%20effectiveness.%20In%20this%20study%2C%20we%20investigate%20the%20effects%0Aof%20JPEG%20compression%20on%20low-light%20images%20and%20reveal%20substantial%20information%20loss%0Acaused%20by%20JPEG%20due%20to%20widespread%20low%20pixel%20values%20in%20dark%20areas.%20Hence%2C%20we%0Apropose%20the%20Compression-Aware%20Pre-trained%20Transformer%20%28CAPformer%29%2C%20employing%20a%0Anovel%20pre-training%20strategy%20to%20learn%20lossless%20information%20from%20uncompressed%0Alow-light%20images.%20Additionally%2C%20the%20proposed%20Brightness-Guided%20Self-Attention%0A%28BGSA%29%20mechanism%20enhances%20rational%20information%20gathering.%20Experiments%0Ademonstrate%20the%20superiority%20of%20our%20approach%20in%20mitigating%20compression%20effects%0Aon%20LLIE%2C%20showcasing%20its%20potential%20for%20improving%20LLIE%20in%20resource-constrained%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07056v2&entry.124074799=Read"},
{"title": "Tuning Vision-Language Models with Candidate Labels by Prompt Alignment", "author": "Zhifang Zhang and Beibei Li", "abstract": "  Vision-language models (VLMs) can learn high-quality representations from a\nlarge-scale training dataset of image-text pairs. Prompt learning is a popular\napproach to fine-tuning VLM to adapt them to downstream tasks. Despite the\nsatisfying performance, a major limitation of prompt learning is the demand for\nlabelled data. In real-world scenarios, we may only obtain candidate labels\n(where the true label is included) instead of the true labels due to data\nprivacy or sensitivity issues. In this paper, we provide the first study on\nprompt learning with candidate labels for VLMs. We empirically demonstrate that\nprompt learning is more advantageous than other fine-tuning methods, for\nhandling candidate labels. Nonetheless, its performance drops when the label\nambiguity increases. In order to improve its robustness, we propose a simple\nyet effective framework that better leverages the prior knowledge of VLMs to\nguide the learning process with candidate labels. Specifically, our framework\ndisambiguates candidate labels by aligning the model output with the mixed\nclass posterior jointly predicted by both the learnable and the handcrafted\nprompt. Besides, our framework can be equipped with various off-the-shelf\ntraining objectives for learning with candidate labels to further improve their\nperformance. Extensive experiments demonstrate the effectiveness of our\nproposed framework.\n", "link": "http://arxiv.org/abs/2407.07638v1", "date": "2024-07-10", "relevancy": 2.0551, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5436}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5108}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tuning%20Vision-Language%20Models%20with%20Candidate%20Labels%20by%20Prompt%20Alignment&body=Title%3A%20Tuning%20Vision-Language%20Models%20with%20Candidate%20Labels%20by%20Prompt%20Alignment%0AAuthor%3A%20Zhifang%20Zhang%20and%20Beibei%20Li%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20can%20learn%20high-quality%20representations%20from%20a%0Alarge-scale%20training%20dataset%20of%20image-text%20pairs.%20Prompt%20learning%20is%20a%20popular%0Aapproach%20to%20fine-tuning%20VLM%20to%20adapt%20them%20to%20downstream%20tasks.%20Despite%20the%0Asatisfying%20performance%2C%20a%20major%20limitation%20of%20prompt%20learning%20is%20the%20demand%20for%0Alabelled%20data.%20In%20real-world%20scenarios%2C%20we%20may%20only%20obtain%20candidate%20labels%0A%28where%20the%20true%20label%20is%20included%29%20instead%20of%20the%20true%20labels%20due%20to%20data%0Aprivacy%20or%20sensitivity%20issues.%20In%20this%20paper%2C%20we%20provide%20the%20first%20study%20on%0Aprompt%20learning%20with%20candidate%20labels%20for%20VLMs.%20We%20empirically%20demonstrate%20that%0Aprompt%20learning%20is%20more%20advantageous%20than%20other%20fine-tuning%20methods%2C%20for%0Ahandling%20candidate%20labels.%20Nonetheless%2C%20its%20performance%20drops%20when%20the%20label%0Aambiguity%20increases.%20In%20order%20to%20improve%20its%20robustness%2C%20we%20propose%20a%20simple%0Ayet%20effective%20framework%20that%20better%20leverages%20the%20prior%20knowledge%20of%20VLMs%20to%0Aguide%20the%20learning%20process%20with%20candidate%20labels.%20Specifically%2C%20our%20framework%0Adisambiguates%20candidate%20labels%20by%20aligning%20the%20model%20output%20with%20the%20mixed%0Aclass%20posterior%20jointly%20predicted%20by%20both%20the%20learnable%20and%20the%20handcrafted%0Aprompt.%20Besides%2C%20our%20framework%20can%20be%20equipped%20with%20various%20off-the-shelf%0Atraining%20objectives%20for%20learning%20with%20candidate%20labels%20to%20further%20improve%20their%0Aperformance.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuning%2520Vision-Language%2520Models%2520with%2520Candidate%2520Labels%2520by%2520Prompt%2520Alignment%26entry.906535625%3DZhifang%2520Zhang%2520and%2520Beibei%2520Li%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520can%2520learn%2520high-quality%2520representations%2520from%2520a%250Alarge-scale%2520training%2520dataset%2520of%2520image-text%2520pairs.%2520Prompt%2520learning%2520is%2520a%2520popular%250Aapproach%2520to%2520fine-tuning%2520VLM%2520to%2520adapt%2520them%2520to%2520downstream%2520tasks.%2520Despite%2520the%250Asatisfying%2520performance%252C%2520a%2520major%2520limitation%2520of%2520prompt%2520learning%2520is%2520the%2520demand%2520for%250Alabelled%2520data.%2520In%2520real-world%2520scenarios%252C%2520we%2520may%2520only%2520obtain%2520candidate%2520labels%250A%2528where%2520the%2520true%2520label%2520is%2520included%2529%2520instead%2520of%2520the%2520true%2520labels%2520due%2520to%2520data%250Aprivacy%2520or%2520sensitivity%2520issues.%2520In%2520this%2520paper%252C%2520we%2520provide%2520the%2520first%2520study%2520on%250Aprompt%2520learning%2520with%2520candidate%2520labels%2520for%2520VLMs.%2520We%2520empirically%2520demonstrate%2520that%250Aprompt%2520learning%2520is%2520more%2520advantageous%2520than%2520other%2520fine-tuning%2520methods%252C%2520for%250Ahandling%2520candidate%2520labels.%2520Nonetheless%252C%2520its%2520performance%2520drops%2520when%2520the%2520label%250Aambiguity%2520increases.%2520In%2520order%2520to%2520improve%2520its%2520robustness%252C%2520we%2520propose%2520a%2520simple%250Ayet%2520effective%2520framework%2520that%2520better%2520leverages%2520the%2520prior%2520knowledge%2520of%2520VLMs%2520to%250Aguide%2520the%2520learning%2520process%2520with%2520candidate%2520labels.%2520Specifically%252C%2520our%2520framework%250Adisambiguates%2520candidate%2520labels%2520by%2520aligning%2520the%2520model%2520output%2520with%2520the%2520mixed%250Aclass%2520posterior%2520jointly%2520predicted%2520by%2520both%2520the%2520learnable%2520and%2520the%2520handcrafted%250Aprompt.%2520Besides%252C%2520our%2520framework%2520can%2520be%2520equipped%2520with%2520various%2520off-the-shelf%250Atraining%2520objectives%2520for%2520learning%2520with%2520candidate%2520labels%2520to%2520further%2520improve%2520their%250Aperformance.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning%20Vision-Language%20Models%20with%20Candidate%20Labels%20by%20Prompt%20Alignment&entry.906535625=Zhifang%20Zhang%20and%20Beibei%20Li&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20can%20learn%20high-quality%20representations%20from%20a%0Alarge-scale%20training%20dataset%20of%20image-text%20pairs.%20Prompt%20learning%20is%20a%20popular%0Aapproach%20to%20fine-tuning%20VLM%20to%20adapt%20them%20to%20downstream%20tasks.%20Despite%20the%0Asatisfying%20performance%2C%20a%20major%20limitation%20of%20prompt%20learning%20is%20the%20demand%20for%0Alabelled%20data.%20In%20real-world%20scenarios%2C%20we%20may%20only%20obtain%20candidate%20labels%0A%28where%20the%20true%20label%20is%20included%29%20instead%20of%20the%20true%20labels%20due%20to%20data%0Aprivacy%20or%20sensitivity%20issues.%20In%20this%20paper%2C%20we%20provide%20the%20first%20study%20on%0Aprompt%20learning%20with%20candidate%20labels%20for%20VLMs.%20We%20empirically%20demonstrate%20that%0Aprompt%20learning%20is%20more%20advantageous%20than%20other%20fine-tuning%20methods%2C%20for%0Ahandling%20candidate%20labels.%20Nonetheless%2C%20its%20performance%20drops%20when%20the%20label%0Aambiguity%20increases.%20In%20order%20to%20improve%20its%20robustness%2C%20we%20propose%20a%20simple%0Ayet%20effective%20framework%20that%20better%20leverages%20the%20prior%20knowledge%20of%20VLMs%20to%0Aguide%20the%20learning%20process%20with%20candidate%20labels.%20Specifically%2C%20our%20framework%0Adisambiguates%20candidate%20labels%20by%20aligning%20the%20model%20output%20with%20the%20mixed%0Aclass%20posterior%20jointly%20predicted%20by%20both%20the%20learnable%20and%20the%20handcrafted%0Aprompt.%20Besides%2C%20our%20framework%20can%20be%20equipped%20with%20various%20off-the-shelf%0Atraining%20objectives%20for%20learning%20with%20candidate%20labels%20to%20further%20improve%20their%0Aperformance.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07638v1&entry.124074799=Read"},
{"title": "SHERL: Synthesizing High Accuracy and Efficient Memory for\n  Resource-Limited Transfer Learning", "author": "Haiwen Diao and Bo Wan and Xu Jia and Yunzhi Zhuge and Ying Zhang and Huchuan Lu and Long Chen", "abstract": "  Parameter-efficient transfer learning (PETL) has emerged as a flourishing\nresearch field for adapting large pre-trained models to downstream tasks,\ngreatly reducing trainable parameters while grappling with memory challenges\nduring fine-tuning. To address it, memory-efficient series (METL) avoid\nbackpropagating gradients through the large backbone. However, they compromise\nby exclusively relying on frozen intermediate outputs and limiting the\nexhaustive exploration of prior knowledge from pre-trained models. Moreover,\nthe dependency and redundancy between cross-layer features are frequently\noverlooked, thereby submerging more discriminative representations and causing\nan inherent performance gap (vs. conventional PETL methods). Hence, we propose\nan innovative METL strategy called SHERL for resource-limited scenarios to\ndecouple the entire adaptation into two successive and complementary processes.\nIn the early route, intermediate outputs are consolidated via an\nanti-redundancy operation, enhancing their compatibility for subsequent\ninteractions; thereby in the late route, utilizing minimal late pre-trained\nlayers could alleviate the peak demand on memory overhead and regulate these\nfairly flexible features into more adaptive and powerful representations for\nnew domains. Extensive ablations on vision-and-language and language-only tasks\nshow that SHERL combines the strengths of both parameter and memory-efficient\ntechniques, performing on-par or better across diverse architectures with lower\nmemory during fine-tuning. Our code is publicly available at:\nhttps://github.com/Paranioar/SHERL.\n", "link": "http://arxiv.org/abs/2407.07523v1", "date": "2024-07-10", "relevancy": 2.0443, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5082}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHERL%3A%20Synthesizing%20High%20Accuracy%20and%20Efficient%20Memory%20for%0A%20%20Resource-Limited%20Transfer%20Learning&body=Title%3A%20SHERL%3A%20Synthesizing%20High%20Accuracy%20and%20Efficient%20Memory%20for%0A%20%20Resource-Limited%20Transfer%20Learning%0AAuthor%3A%20Haiwen%20Diao%20and%20Bo%20Wan%20and%20Xu%20Jia%20and%20Yunzhi%20Zhuge%20and%20Ying%20Zhang%20and%20Huchuan%20Lu%20and%20Long%20Chen%0AAbstract%3A%20%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20has%20emerged%20as%20a%20flourishing%0Aresearch%20field%20for%20adapting%20large%20pre-trained%20models%20to%20downstream%20tasks%2C%0Agreatly%20reducing%20trainable%20parameters%20while%20grappling%20with%20memory%20challenges%0Aduring%20fine-tuning.%20To%20address%20it%2C%20memory-efficient%20series%20%28METL%29%20avoid%0Abackpropagating%20gradients%20through%20the%20large%20backbone.%20However%2C%20they%20compromise%0Aby%20exclusively%20relying%20on%20frozen%20intermediate%20outputs%20and%20limiting%20the%0Aexhaustive%20exploration%20of%20prior%20knowledge%20from%20pre-trained%20models.%20Moreover%2C%0Athe%20dependency%20and%20redundancy%20between%20cross-layer%20features%20are%20frequently%0Aoverlooked%2C%20thereby%20submerging%20more%20discriminative%20representations%20and%20causing%0Aan%20inherent%20performance%20gap%20%28vs.%20conventional%20PETL%20methods%29.%20Hence%2C%20we%20propose%0Aan%20innovative%20METL%20strategy%20called%20SHERL%20for%20resource-limited%20scenarios%20to%0Adecouple%20the%20entire%20adaptation%20into%20two%20successive%20and%20complementary%20processes.%0AIn%20the%20early%20route%2C%20intermediate%20outputs%20are%20consolidated%20via%20an%0Aanti-redundancy%20operation%2C%20enhancing%20their%20compatibility%20for%20subsequent%0Ainteractions%3B%20thereby%20in%20the%20late%20route%2C%20utilizing%20minimal%20late%20pre-trained%0Alayers%20could%20alleviate%20the%20peak%20demand%20on%20memory%20overhead%20and%20regulate%20these%0Afairly%20flexible%20features%20into%20more%20adaptive%20and%20powerful%20representations%20for%0Anew%20domains.%20Extensive%20ablations%20on%20vision-and-language%20and%20language-only%20tasks%0Ashow%20that%20SHERL%20combines%20the%20strengths%20of%20both%20parameter%20and%20memory-efficient%0Atechniques%2C%20performing%20on-par%20or%20better%20across%20diverse%20architectures%20with%20lower%0Amemory%20during%20fine-tuning.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Paranioar/SHERL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHERL%253A%2520Synthesizing%2520High%2520Accuracy%2520and%2520Efficient%2520Memory%2520for%250A%2520%2520Resource-Limited%2520Transfer%2520Learning%26entry.906535625%3DHaiwen%2520Diao%2520and%2520Bo%2520Wan%2520and%2520Xu%2520Jia%2520and%2520Yunzhi%2520Zhuge%2520and%2520Ying%2520Zhang%2520and%2520Huchuan%2520Lu%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Parameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520has%2520emerged%2520as%2520a%2520flourishing%250Aresearch%2520field%2520for%2520adapting%2520large%2520pre-trained%2520models%2520to%2520downstream%2520tasks%252C%250Agreatly%2520reducing%2520trainable%2520parameters%2520while%2520grappling%2520with%2520memory%2520challenges%250Aduring%2520fine-tuning.%2520To%2520address%2520it%252C%2520memory-efficient%2520series%2520%2528METL%2529%2520avoid%250Abackpropagating%2520gradients%2520through%2520the%2520large%2520backbone.%2520However%252C%2520they%2520compromise%250Aby%2520exclusively%2520relying%2520on%2520frozen%2520intermediate%2520outputs%2520and%2520limiting%2520the%250Aexhaustive%2520exploration%2520of%2520prior%2520knowledge%2520from%2520pre-trained%2520models.%2520Moreover%252C%250Athe%2520dependency%2520and%2520redundancy%2520between%2520cross-layer%2520features%2520are%2520frequently%250Aoverlooked%252C%2520thereby%2520submerging%2520more%2520discriminative%2520representations%2520and%2520causing%250Aan%2520inherent%2520performance%2520gap%2520%2528vs.%2520conventional%2520PETL%2520methods%2529.%2520Hence%252C%2520we%2520propose%250Aan%2520innovative%2520METL%2520strategy%2520called%2520SHERL%2520for%2520resource-limited%2520scenarios%2520to%250Adecouple%2520the%2520entire%2520adaptation%2520into%2520two%2520successive%2520and%2520complementary%2520processes.%250AIn%2520the%2520early%2520route%252C%2520intermediate%2520outputs%2520are%2520consolidated%2520via%2520an%250Aanti-redundancy%2520operation%252C%2520enhancing%2520their%2520compatibility%2520for%2520subsequent%250Ainteractions%253B%2520thereby%2520in%2520the%2520late%2520route%252C%2520utilizing%2520minimal%2520late%2520pre-trained%250Alayers%2520could%2520alleviate%2520the%2520peak%2520demand%2520on%2520memory%2520overhead%2520and%2520regulate%2520these%250Afairly%2520flexible%2520features%2520into%2520more%2520adaptive%2520and%2520powerful%2520representations%2520for%250Anew%2520domains.%2520Extensive%2520ablations%2520on%2520vision-and-language%2520and%2520language-only%2520tasks%250Ashow%2520that%2520SHERL%2520combines%2520the%2520strengths%2520of%2520both%2520parameter%2520and%2520memory-efficient%250Atechniques%252C%2520performing%2520on-par%2520or%2520better%2520across%2520diverse%2520architectures%2520with%2520lower%250Amemory%2520during%2520fine-tuning.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/Paranioar/SHERL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHERL%3A%20Synthesizing%20High%20Accuracy%20and%20Efficient%20Memory%20for%0A%20%20Resource-Limited%20Transfer%20Learning&entry.906535625=Haiwen%20Diao%20and%20Bo%20Wan%20and%20Xu%20Jia%20and%20Yunzhi%20Zhuge%20and%20Ying%20Zhang%20and%20Huchuan%20Lu%20and%20Long%20Chen&entry.1292438233=%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20has%20emerged%20as%20a%20flourishing%0Aresearch%20field%20for%20adapting%20large%20pre-trained%20models%20to%20downstream%20tasks%2C%0Agreatly%20reducing%20trainable%20parameters%20while%20grappling%20with%20memory%20challenges%0Aduring%20fine-tuning.%20To%20address%20it%2C%20memory-efficient%20series%20%28METL%29%20avoid%0Abackpropagating%20gradients%20through%20the%20large%20backbone.%20However%2C%20they%20compromise%0Aby%20exclusively%20relying%20on%20frozen%20intermediate%20outputs%20and%20limiting%20the%0Aexhaustive%20exploration%20of%20prior%20knowledge%20from%20pre-trained%20models.%20Moreover%2C%0Athe%20dependency%20and%20redundancy%20between%20cross-layer%20features%20are%20frequently%0Aoverlooked%2C%20thereby%20submerging%20more%20discriminative%20representations%20and%20causing%0Aan%20inherent%20performance%20gap%20%28vs.%20conventional%20PETL%20methods%29.%20Hence%2C%20we%20propose%0Aan%20innovative%20METL%20strategy%20called%20SHERL%20for%20resource-limited%20scenarios%20to%0Adecouple%20the%20entire%20adaptation%20into%20two%20successive%20and%20complementary%20processes.%0AIn%20the%20early%20route%2C%20intermediate%20outputs%20are%20consolidated%20via%20an%0Aanti-redundancy%20operation%2C%20enhancing%20their%20compatibility%20for%20subsequent%0Ainteractions%3B%20thereby%20in%20the%20late%20route%2C%20utilizing%20minimal%20late%20pre-trained%0Alayers%20could%20alleviate%20the%20peak%20demand%20on%20memory%20overhead%20and%20regulate%20these%0Afairly%20flexible%20features%20into%20more%20adaptive%20and%20powerful%20representations%20for%0Anew%20domains.%20Extensive%20ablations%20on%20vision-and-language%20and%20language-only%20tasks%0Ashow%20that%20SHERL%20combines%20the%20strengths%20of%20both%20parameter%20and%20memory-efficient%0Atechniques%2C%20performing%20on-par%20or%20better%20across%20diverse%20architectures%20with%20lower%0Amemory%20during%20fine-tuning.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Paranioar/SHERL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07523v1&entry.124074799=Read"},
{"title": "ROSA: Random Subspace Adaptation for Efficient Fine-Tuning", "author": "Marawan Gamal Abdel Hameed and Aristides Milios and Siva Reddy and Guillaume Rabusseau", "abstract": "  Model training requires significantly more memory, compared with inference.\nParameter efficient fine-tuning (PEFT) methods provide a means of adapting\nlarge models to downstream tasks using less memory. However, existing methods\nsuch as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce\nlatency overhead at inference time or achieve subpar downstream performance\ncompared with full fine-tuning. In this work we propose Random Subspace\nAdaptation (ROSA), a method that outperforms previous PEFT methods by a\nsignificant margin, while maintaining a zero latency overhead during inference\ntime. In contrast to previous methods, ROSA is able to adapt subspaces of\narbitrarily large dimension, better approximating full-finetuning. We\ndemonstrate both theoretically and experimentally that this makes ROSA strictly\nmore expressive than LoRA, without consuming additional memory during runtime.\nAs PEFT methods are especially useful in the natural language processing\ndomain, where models operate on scales that make full fine-tuning very\nexpensive, we evaluate ROSA in two common NLP scenarios: natural language\ngeneration (NLG) and natural language understanding (NLU) with GPT-2 and\nRoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms\nLoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our\ncode is available at https://github.com/rosa-paper/rosa\n", "link": "http://arxiv.org/abs/2407.07802v1", "date": "2024-07-10", "relevancy": 2.0364, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5099}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5087}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROSA%3A%20Random%20Subspace%20Adaptation%20for%20Efficient%20Fine-Tuning&body=Title%3A%20ROSA%3A%20Random%20Subspace%20Adaptation%20for%20Efficient%20Fine-Tuning%0AAuthor%3A%20Marawan%20Gamal%20Abdel%20Hameed%20and%20Aristides%20Milios%20and%20Siva%20Reddy%20and%20Guillaume%20Rabusseau%0AAbstract%3A%20%20%20Model%20training%20requires%20significantly%20more%20memory%2C%20compared%20with%20inference.%0AParameter%20efficient%20fine-tuning%20%28PEFT%29%20methods%20provide%20a%20means%20of%20adapting%0Alarge%20models%20to%20downstream%20tasks%20using%20less%20memory.%20However%2C%20existing%20methods%0Asuch%20as%20adapters%2C%20prompt%20tuning%20or%20low-rank%20adaptation%20%28LoRA%29%20either%20introduce%0Alatency%20overhead%20at%20inference%20time%20or%20achieve%20subpar%20downstream%20performance%0Acompared%20with%20full%20fine-tuning.%20In%20this%20work%20we%20propose%20Random%20Subspace%0AAdaptation%20%28ROSA%29%2C%20a%20method%20that%20outperforms%20previous%20PEFT%20methods%20by%20a%0Asignificant%20margin%2C%20while%20maintaining%20a%20zero%20latency%20overhead%20during%20inference%0Atime.%20In%20contrast%20to%20previous%20methods%2C%20ROSA%20is%20able%20to%20adapt%20subspaces%20of%0Aarbitrarily%20large%20dimension%2C%20better%20approximating%20full-finetuning.%20We%0Ademonstrate%20both%20theoretically%20and%20experimentally%20that%20this%20makes%20ROSA%20strictly%0Amore%20expressive%20than%20LoRA%2C%20without%20consuming%20additional%20memory%20during%20runtime.%0AAs%20PEFT%20methods%20are%20especially%20useful%20in%20the%20natural%20language%20processing%0Adomain%2C%20where%20models%20operate%20on%20scales%20that%20make%20full%20fine-tuning%20very%0Aexpensive%2C%20we%20evaluate%20ROSA%20in%20two%20common%20NLP%20scenarios%3A%20natural%20language%0Ageneration%20%28NLG%29%20and%20natural%20language%20understanding%20%28NLU%29%20with%20GPT-2%20and%0ARoBERTa%2C%20respectively.%20We%20show%20that%20on%20almost%20every%20GLUE%20task%20ROSA%20outperforms%0ALoRA%20by%20a%20significant%20margin%2C%20while%20also%20outperforming%20LoRA%20on%20NLG%20tasks.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/rosa-paper/rosa%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROSA%253A%2520Random%2520Subspace%2520Adaptation%2520for%2520Efficient%2520Fine-Tuning%26entry.906535625%3DMarawan%2520Gamal%2520Abdel%2520Hameed%2520and%2520Aristides%2520Milios%2520and%2520Siva%2520Reddy%2520and%2520Guillaume%2520Rabusseau%26entry.1292438233%3D%2520%2520Model%2520training%2520requires%2520significantly%2520more%2520memory%252C%2520compared%2520with%2520inference.%250AParameter%2520efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%2520provide%2520a%2520means%2520of%2520adapting%250Alarge%2520models%2520to%2520downstream%2520tasks%2520using%2520less%2520memory.%2520However%252C%2520existing%2520methods%250Asuch%2520as%2520adapters%252C%2520prompt%2520tuning%2520or%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520either%2520introduce%250Alatency%2520overhead%2520at%2520inference%2520time%2520or%2520achieve%2520subpar%2520downstream%2520performance%250Acompared%2520with%2520full%2520fine-tuning.%2520In%2520this%2520work%2520we%2520propose%2520Random%2520Subspace%250AAdaptation%2520%2528ROSA%2529%252C%2520a%2520method%2520that%2520outperforms%2520previous%2520PEFT%2520methods%2520by%2520a%250Asignificant%2520margin%252C%2520while%2520maintaining%2520a%2520zero%2520latency%2520overhead%2520during%2520inference%250Atime.%2520In%2520contrast%2520to%2520previous%2520methods%252C%2520ROSA%2520is%2520able%2520to%2520adapt%2520subspaces%2520of%250Aarbitrarily%2520large%2520dimension%252C%2520better%2520approximating%2520full-finetuning.%2520We%250Ademonstrate%2520both%2520theoretically%2520and%2520experimentally%2520that%2520this%2520makes%2520ROSA%2520strictly%250Amore%2520expressive%2520than%2520LoRA%252C%2520without%2520consuming%2520additional%2520memory%2520during%2520runtime.%250AAs%2520PEFT%2520methods%2520are%2520especially%2520useful%2520in%2520the%2520natural%2520language%2520processing%250Adomain%252C%2520where%2520models%2520operate%2520on%2520scales%2520that%2520make%2520full%2520fine-tuning%2520very%250Aexpensive%252C%2520we%2520evaluate%2520ROSA%2520in%2520two%2520common%2520NLP%2520scenarios%253A%2520natural%2520language%250Ageneration%2520%2528NLG%2529%2520and%2520natural%2520language%2520understanding%2520%2528NLU%2529%2520with%2520GPT-2%2520and%250ARoBERTa%252C%2520respectively.%2520We%2520show%2520that%2520on%2520almost%2520every%2520GLUE%2520task%2520ROSA%2520outperforms%250ALoRA%2520by%2520a%2520significant%2520margin%252C%2520while%2520also%2520outperforming%2520LoRA%2520on%2520NLG%2520tasks.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/rosa-paper/rosa%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROSA%3A%20Random%20Subspace%20Adaptation%20for%20Efficient%20Fine-Tuning&entry.906535625=Marawan%20Gamal%20Abdel%20Hameed%20and%20Aristides%20Milios%20and%20Siva%20Reddy%20and%20Guillaume%20Rabusseau&entry.1292438233=%20%20Model%20training%20requires%20significantly%20more%20memory%2C%20compared%20with%20inference.%0AParameter%20efficient%20fine-tuning%20%28PEFT%29%20methods%20provide%20a%20means%20of%20adapting%0Alarge%20models%20to%20downstream%20tasks%20using%20less%20memory.%20However%2C%20existing%20methods%0Asuch%20as%20adapters%2C%20prompt%20tuning%20or%20low-rank%20adaptation%20%28LoRA%29%20either%20introduce%0Alatency%20overhead%20at%20inference%20time%20or%20achieve%20subpar%20downstream%20performance%0Acompared%20with%20full%20fine-tuning.%20In%20this%20work%20we%20propose%20Random%20Subspace%0AAdaptation%20%28ROSA%29%2C%20a%20method%20that%20outperforms%20previous%20PEFT%20methods%20by%20a%0Asignificant%20margin%2C%20while%20maintaining%20a%20zero%20latency%20overhead%20during%20inference%0Atime.%20In%20contrast%20to%20previous%20methods%2C%20ROSA%20is%20able%20to%20adapt%20subspaces%20of%0Aarbitrarily%20large%20dimension%2C%20better%20approximating%20full-finetuning.%20We%0Ademonstrate%20both%20theoretically%20and%20experimentally%20that%20this%20makes%20ROSA%20strictly%0Amore%20expressive%20than%20LoRA%2C%20without%20consuming%20additional%20memory%20during%20runtime.%0AAs%20PEFT%20methods%20are%20especially%20useful%20in%20the%20natural%20language%20processing%0Adomain%2C%20where%20models%20operate%20on%20scales%20that%20make%20full%20fine-tuning%20very%0Aexpensive%2C%20we%20evaluate%20ROSA%20in%20two%20common%20NLP%20scenarios%3A%20natural%20language%0Ageneration%20%28NLG%29%20and%20natural%20language%20understanding%20%28NLU%29%20with%20GPT-2%20and%0ARoBERTa%2C%20respectively.%20We%20show%20that%20on%20almost%20every%20GLUE%20task%20ROSA%20outperforms%0ALoRA%20by%20a%20significant%20margin%2C%20while%20also%20outperforming%20LoRA%20on%20NLG%20tasks.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/rosa-paper/rosa%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07802v1&entry.124074799=Read"},
{"title": "Vanilla Feedforward Neural Networks as a Discretization of Dynamical\n  Systems", "author": "Yifei Duan and Li'ang Li and Guanghua Ji and Yongqiang Cai", "abstract": "  Deep learning has made significant applications in the field of data science\nand natural science. Some studies have linked deep neural networks to dynamic\nsystems, but the network structure is restricted to the residual network. It is\nknown that residual networks can be regarded as a numerical discretization of\ndynamic systems. In this paper, we back to the classical network structure and\nprove that the vanilla feedforward networks could also be a numerical\ndiscretization of dynamic systems, where the width of the network is equal to\nthe dimension of the input and output. Our proof is based on the properties of\nthe leaky-ReLU function and the numerical technique of splitting method to\nsolve differential equations. Our results could provide a new perspective for\nunderstanding the approximation properties of feedforward neural networks.\n", "link": "http://arxiv.org/abs/2209.10909v2", "date": "2024-07-10", "relevancy": 2.0195, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5309}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4968}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vanilla%20Feedforward%20Neural%20Networks%20as%20a%20Discretization%20of%20Dynamical%0A%20%20Systems&body=Title%3A%20Vanilla%20Feedforward%20Neural%20Networks%20as%20a%20Discretization%20of%20Dynamical%0A%20%20Systems%0AAuthor%3A%20Yifei%20Duan%20and%20Li%27ang%20Li%20and%20Guanghua%20Ji%20and%20Yongqiang%20Cai%0AAbstract%3A%20%20%20Deep%20learning%20has%20made%20significant%20applications%20in%20the%20field%20of%20data%20science%0Aand%20natural%20science.%20Some%20studies%20have%20linked%20deep%20neural%20networks%20to%20dynamic%0Asystems%2C%20but%20the%20network%20structure%20is%20restricted%20to%20the%20residual%20network.%20It%20is%0Aknown%20that%20residual%20networks%20can%20be%20regarded%20as%20a%20numerical%20discretization%20of%0Adynamic%20systems.%20In%20this%20paper%2C%20we%20back%20to%20the%20classical%20network%20structure%20and%0Aprove%20that%20the%20vanilla%20feedforward%20networks%20could%20also%20be%20a%20numerical%0Adiscretization%20of%20dynamic%20systems%2C%20where%20the%20width%20of%20the%20network%20is%20equal%20to%0Athe%20dimension%20of%20the%20input%20and%20output.%20Our%20proof%20is%20based%20on%20the%20properties%20of%0Athe%20leaky-ReLU%20function%20and%20the%20numerical%20technique%20of%20splitting%20method%20to%0Asolve%20differential%20equations.%20Our%20results%20could%20provide%20a%20new%20perspective%20for%0Aunderstanding%20the%20approximation%20properties%20of%20feedforward%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.10909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVanilla%2520Feedforward%2520Neural%2520Networks%2520as%2520a%2520Discretization%2520of%2520Dynamical%250A%2520%2520Systems%26entry.906535625%3DYifei%2520Duan%2520and%2520Li%2527ang%2520Li%2520and%2520Guanghua%2520Ji%2520and%2520Yongqiang%2520Cai%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520made%2520significant%2520applications%2520in%2520the%2520field%2520of%2520data%2520science%250Aand%2520natural%2520science.%2520Some%2520studies%2520have%2520linked%2520deep%2520neural%2520networks%2520to%2520dynamic%250Asystems%252C%2520but%2520the%2520network%2520structure%2520is%2520restricted%2520to%2520the%2520residual%2520network.%2520It%2520is%250Aknown%2520that%2520residual%2520networks%2520can%2520be%2520regarded%2520as%2520a%2520numerical%2520discretization%2520of%250Adynamic%2520systems.%2520In%2520this%2520paper%252C%2520we%2520back%2520to%2520the%2520classical%2520network%2520structure%2520and%250Aprove%2520that%2520the%2520vanilla%2520feedforward%2520networks%2520could%2520also%2520be%2520a%2520numerical%250Adiscretization%2520of%2520dynamic%2520systems%252C%2520where%2520the%2520width%2520of%2520the%2520network%2520is%2520equal%2520to%250Athe%2520dimension%2520of%2520the%2520input%2520and%2520output.%2520Our%2520proof%2520is%2520based%2520on%2520the%2520properties%2520of%250Athe%2520leaky-ReLU%2520function%2520and%2520the%2520numerical%2520technique%2520of%2520splitting%2520method%2520to%250Asolve%2520differential%2520equations.%2520Our%2520results%2520could%2520provide%2520a%2520new%2520perspective%2520for%250Aunderstanding%2520the%2520approximation%2520properties%2520of%2520feedforward%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.10909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vanilla%20Feedforward%20Neural%20Networks%20as%20a%20Discretization%20of%20Dynamical%0A%20%20Systems&entry.906535625=Yifei%20Duan%20and%20Li%27ang%20Li%20and%20Guanghua%20Ji%20and%20Yongqiang%20Cai&entry.1292438233=%20%20Deep%20learning%20has%20made%20significant%20applications%20in%20the%20field%20of%20data%20science%0Aand%20natural%20science.%20Some%20studies%20have%20linked%20deep%20neural%20networks%20to%20dynamic%0Asystems%2C%20but%20the%20network%20structure%20is%20restricted%20to%20the%20residual%20network.%20It%20is%0Aknown%20that%20residual%20networks%20can%20be%20regarded%20as%20a%20numerical%20discretization%20of%0Adynamic%20systems.%20In%20this%20paper%2C%20we%20back%20to%20the%20classical%20network%20structure%20and%0Aprove%20that%20the%20vanilla%20feedforward%20networks%20could%20also%20be%20a%20numerical%0Adiscretization%20of%20dynamic%20systems%2C%20where%20the%20width%20of%20the%20network%20is%20equal%20to%0Athe%20dimension%20of%20the%20input%20and%20output.%20Our%20proof%20is%20based%20on%20the%20properties%20of%0Athe%20leaky-ReLU%20function%20and%20the%20numerical%20technique%20of%20splitting%20method%20to%0Asolve%20differential%20equations.%20Our%20results%20could%20provide%20a%20new%20perspective%20for%0Aunderstanding%20the%20approximation%20properties%20of%20feedforward%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.10909v2&entry.124074799=Read"},
{"title": "Deep learning in a bilateral brain with hemispheric specialization", "author": "Chandramouli Rajagopalan and David Rawlinson and Elkhonon Goldberg and Gideon Kowadlo", "abstract": "  The brains of all bilaterally symmetric animals on Earth are divided into\nleft and right hemispheres. The anatomy and functionality of the hemispheres\nhave a large degree of overlap, but there are asymmetries, and they specialise\nin possesses different attributes. Other authors have used computational models\nto mimic hemispheric asymmetries with a focus on reproducing human data on\nsemantic and visual processing tasks. We took a different approach and aimed to\nunderstand how dual hemispheres in a bilateral architecture interact to perform\nwell in a given task. We propose a bilateral artificial neural network that\nimitates lateralisation observed in nature: that the left hemisphere\nspecialises in local features and the right in global features. We used\ndifferent training objectives to achieve the desired specialisation and tested\nit on an image classification task with two different CNN backbones: ResNet and\nVGG. Our analysis found that the hemispheres represent complementary features\nthat are exploited by a network head that implements a type of weighted\nattention. The bilateral architecture outperformed a range of baselines of\nsimilar representational capacity that do not exploit differential\nspecialisation, with the exception of a conventional ensemble of unilateral\nnetworks trained on dual training objectives for local and global features. The\nresults demonstrate the efficacy of bilateralism, contribute to the discussion\nof bilateralism in biological brains, and the principle may serve as an\ninductive bias for new AI systems.\n", "link": "http://arxiv.org/abs/2209.06862v9", "date": "2024-07-10", "relevancy": 2.0194, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5116}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5081}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning%20in%20a%20bilateral%20brain%20with%20hemispheric%20specialization&body=Title%3A%20Deep%20learning%20in%20a%20bilateral%20brain%20with%20hemispheric%20specialization%0AAuthor%3A%20Chandramouli%20Rajagopalan%20and%20David%20Rawlinson%20and%20Elkhonon%20Goldberg%20and%20Gideon%20Kowadlo%0AAbstract%3A%20%20%20The%20brains%20of%20all%20bilaterally%20symmetric%20animals%20on%20Earth%20are%20divided%20into%0Aleft%20and%20right%20hemispheres.%20The%20anatomy%20and%20functionality%20of%20the%20hemispheres%0Ahave%20a%20large%20degree%20of%20overlap%2C%20but%20there%20are%20asymmetries%2C%20and%20they%20specialise%0Ain%20possesses%20different%20attributes.%20Other%20authors%20have%20used%20computational%20models%0Ato%20mimic%20hemispheric%20asymmetries%20with%20a%20focus%20on%20reproducing%20human%20data%20on%0Asemantic%20and%20visual%20processing%20tasks.%20We%20took%20a%20different%20approach%20and%20aimed%20to%0Aunderstand%20how%20dual%20hemispheres%20in%20a%20bilateral%20architecture%20interact%20to%20perform%0Awell%20in%20a%20given%20task.%20We%20propose%20a%20bilateral%20artificial%20neural%20network%20that%0Aimitates%20lateralisation%20observed%20in%20nature%3A%20that%20the%20left%20hemisphere%0Aspecialises%20in%20local%20features%20and%20the%20right%20in%20global%20features.%20We%20used%0Adifferent%20training%20objectives%20to%20achieve%20the%20desired%20specialisation%20and%20tested%0Ait%20on%20an%20image%20classification%20task%20with%20two%20different%20CNN%20backbones%3A%20ResNet%20and%0AVGG.%20Our%20analysis%20found%20that%20the%20hemispheres%20represent%20complementary%20features%0Athat%20are%20exploited%20by%20a%20network%20head%20that%20implements%20a%20type%20of%20weighted%0Aattention.%20The%20bilateral%20architecture%20outperformed%20a%20range%20of%20baselines%20of%0Asimilar%20representational%20capacity%20that%20do%20not%20exploit%20differential%0Aspecialisation%2C%20with%20the%20exception%20of%20a%20conventional%20ensemble%20of%20unilateral%0Anetworks%20trained%20on%20dual%20training%20objectives%20for%20local%20and%20global%20features.%20The%0Aresults%20demonstrate%20the%20efficacy%20of%20bilateralism%2C%20contribute%20to%20the%20discussion%0Aof%20bilateralism%20in%20biological%20brains%2C%20and%20the%20principle%20may%20serve%20as%20an%0Ainductive%20bias%20for%20new%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.06862v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning%2520in%2520a%2520bilateral%2520brain%2520with%2520hemispheric%2520specialization%26entry.906535625%3DChandramouli%2520Rajagopalan%2520and%2520David%2520Rawlinson%2520and%2520Elkhonon%2520Goldberg%2520and%2520Gideon%2520Kowadlo%26entry.1292438233%3D%2520%2520The%2520brains%2520of%2520all%2520bilaterally%2520symmetric%2520animals%2520on%2520Earth%2520are%2520divided%2520into%250Aleft%2520and%2520right%2520hemispheres.%2520The%2520anatomy%2520and%2520functionality%2520of%2520the%2520hemispheres%250Ahave%2520a%2520large%2520degree%2520of%2520overlap%252C%2520but%2520there%2520are%2520asymmetries%252C%2520and%2520they%2520specialise%250Ain%2520possesses%2520different%2520attributes.%2520Other%2520authors%2520have%2520used%2520computational%2520models%250Ato%2520mimic%2520hemispheric%2520asymmetries%2520with%2520a%2520focus%2520on%2520reproducing%2520human%2520data%2520on%250Asemantic%2520and%2520visual%2520processing%2520tasks.%2520We%2520took%2520a%2520different%2520approach%2520and%2520aimed%2520to%250Aunderstand%2520how%2520dual%2520hemispheres%2520in%2520a%2520bilateral%2520architecture%2520interact%2520to%2520perform%250Awell%2520in%2520a%2520given%2520task.%2520We%2520propose%2520a%2520bilateral%2520artificial%2520neural%2520network%2520that%250Aimitates%2520lateralisation%2520observed%2520in%2520nature%253A%2520that%2520the%2520left%2520hemisphere%250Aspecialises%2520in%2520local%2520features%2520and%2520the%2520right%2520in%2520global%2520features.%2520We%2520used%250Adifferent%2520training%2520objectives%2520to%2520achieve%2520the%2520desired%2520specialisation%2520and%2520tested%250Ait%2520on%2520an%2520image%2520classification%2520task%2520with%2520two%2520different%2520CNN%2520backbones%253A%2520ResNet%2520and%250AVGG.%2520Our%2520analysis%2520found%2520that%2520the%2520hemispheres%2520represent%2520complementary%2520features%250Athat%2520are%2520exploited%2520by%2520a%2520network%2520head%2520that%2520implements%2520a%2520type%2520of%2520weighted%250Aattention.%2520The%2520bilateral%2520architecture%2520outperformed%2520a%2520range%2520of%2520baselines%2520of%250Asimilar%2520representational%2520capacity%2520that%2520do%2520not%2520exploit%2520differential%250Aspecialisation%252C%2520with%2520the%2520exception%2520of%2520a%2520conventional%2520ensemble%2520of%2520unilateral%250Anetworks%2520trained%2520on%2520dual%2520training%2520objectives%2520for%2520local%2520and%2520global%2520features.%2520The%250Aresults%2520demonstrate%2520the%2520efficacy%2520of%2520bilateralism%252C%2520contribute%2520to%2520the%2520discussion%250Aof%2520bilateralism%2520in%2520biological%2520brains%252C%2520and%2520the%2520principle%2520may%2520serve%2520as%2520an%250Ainductive%2520bias%2520for%2520new%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.06862v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning%20in%20a%20bilateral%20brain%20with%20hemispheric%20specialization&entry.906535625=Chandramouli%20Rajagopalan%20and%20David%20Rawlinson%20and%20Elkhonon%20Goldberg%20and%20Gideon%20Kowadlo&entry.1292438233=%20%20The%20brains%20of%20all%20bilaterally%20symmetric%20animals%20on%20Earth%20are%20divided%20into%0Aleft%20and%20right%20hemispheres.%20The%20anatomy%20and%20functionality%20of%20the%20hemispheres%0Ahave%20a%20large%20degree%20of%20overlap%2C%20but%20there%20are%20asymmetries%2C%20and%20they%20specialise%0Ain%20possesses%20different%20attributes.%20Other%20authors%20have%20used%20computational%20models%0Ato%20mimic%20hemispheric%20asymmetries%20with%20a%20focus%20on%20reproducing%20human%20data%20on%0Asemantic%20and%20visual%20processing%20tasks.%20We%20took%20a%20different%20approach%20and%20aimed%20to%0Aunderstand%20how%20dual%20hemispheres%20in%20a%20bilateral%20architecture%20interact%20to%20perform%0Awell%20in%20a%20given%20task.%20We%20propose%20a%20bilateral%20artificial%20neural%20network%20that%0Aimitates%20lateralisation%20observed%20in%20nature%3A%20that%20the%20left%20hemisphere%0Aspecialises%20in%20local%20features%20and%20the%20right%20in%20global%20features.%20We%20used%0Adifferent%20training%20objectives%20to%20achieve%20the%20desired%20specialisation%20and%20tested%0Ait%20on%20an%20image%20classification%20task%20with%20two%20different%20CNN%20backbones%3A%20ResNet%20and%0AVGG.%20Our%20analysis%20found%20that%20the%20hemispheres%20represent%20complementary%20features%0Athat%20are%20exploited%20by%20a%20network%20head%20that%20implements%20a%20type%20of%20weighted%0Aattention.%20The%20bilateral%20architecture%20outperformed%20a%20range%20of%20baselines%20of%0Asimilar%20representational%20capacity%20that%20do%20not%20exploit%20differential%0Aspecialisation%2C%20with%20the%20exception%20of%20a%20conventional%20ensemble%20of%20unilateral%0Anetworks%20trained%20on%20dual%20training%20objectives%20for%20local%20and%20global%20features.%20The%0Aresults%20demonstrate%20the%20efficacy%20of%20bilateralism%2C%20contribute%20to%20the%20discussion%0Aof%20bilateralism%20in%20biological%20brains%2C%20and%20the%20principle%20may%20serve%20as%20an%0Ainductive%20bias%20for%20new%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.06862v9&entry.124074799=Read"},
{"title": "The Misclassification Likelihood Matrix: Some Classes Are More Likely To\n  Be Misclassified Than Others", "author": "Daniel Sikar and Artur Garcez and Robin Bloomfield and Tillman Weyde and Kaleem Peeroo and Naman Singh and Maeve Hutchinson and Mirela Reljan-Delaney", "abstract": "  This study introduces the Misclassification Likelihood Matrix (MLM) as a\nnovel tool for quantifying the reliability of neural network predictions under\ndistribution shifts. The MLM is obtained by leveraging softmax outputs and\nclustering techniques to measure the distances between the predictions of a\ntrained neural network and class centroids. By analyzing these distances, the\nMLM provides a comprehensive view of the model's misclassification tendencies,\nenabling decision-makers to identify the most common and critical sources of\nerrors. The MLM allows for the prioritization of model improvements and the\nestablishment of decision thresholds based on acceptable risk levels. The\napproach is evaluated on the MNIST dataset using a Convolutional Neural Network\n(CNN) and a perturbed version of the dataset to simulate distribution shifts.\nThe results demonstrate the effectiveness of the MLM in assessing the\nreliability of predictions and highlight its potential in enhancing the\ninterpretability and risk mitigation capabilities of neural networks. The\nimplications of this work extend beyond image classification, with ongoing\napplications in autonomous systems, such as self-driving cars, to improve the\nsafety and reliability of decision-making in complex, real-world environments.\n", "link": "http://arxiv.org/abs/2407.07818v1", "date": "2024-07-10", "relevancy": 2.0094, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4968}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Misclassification%20Likelihood%20Matrix%3A%20Some%20Classes%20Are%20More%20Likely%20To%0A%20%20Be%20Misclassified%20Than%20Others&body=Title%3A%20The%20Misclassification%20Likelihood%20Matrix%3A%20Some%20Classes%20Are%20More%20Likely%20To%0A%20%20Be%20Misclassified%20Than%20Others%0AAuthor%3A%20Daniel%20Sikar%20and%20Artur%20Garcez%20and%20Robin%20Bloomfield%20and%20Tillman%20Weyde%20and%20Kaleem%20Peeroo%20and%20Naman%20Singh%20and%20Maeve%20Hutchinson%20and%20Mirela%20Reljan-Delaney%0AAbstract%3A%20%20%20This%20study%20introduces%20the%20Misclassification%20Likelihood%20Matrix%20%28MLM%29%20as%20a%0Anovel%20tool%20for%20quantifying%20the%20reliability%20of%20neural%20network%20predictions%20under%0Adistribution%20shifts.%20The%20MLM%20is%20obtained%20by%20leveraging%20softmax%20outputs%20and%0Aclustering%20techniques%20to%20measure%20the%20distances%20between%20the%20predictions%20of%20a%0Atrained%20neural%20network%20and%20class%20centroids.%20By%20analyzing%20these%20distances%2C%20the%0AMLM%20provides%20a%20comprehensive%20view%20of%20the%20model%27s%20misclassification%20tendencies%2C%0Aenabling%20decision-makers%20to%20identify%20the%20most%20common%20and%20critical%20sources%20of%0Aerrors.%20The%20MLM%20allows%20for%20the%20prioritization%20of%20model%20improvements%20and%20the%0Aestablishment%20of%20decision%20thresholds%20based%20on%20acceptable%20risk%20levels.%20The%0Aapproach%20is%20evaluated%20on%20the%20MNIST%20dataset%20using%20a%20Convolutional%20Neural%20Network%0A%28CNN%29%20and%20a%20perturbed%20version%20of%20the%20dataset%20to%20simulate%20distribution%20shifts.%0AThe%20results%20demonstrate%20the%20effectiveness%20of%20the%20MLM%20in%20assessing%20the%0Areliability%20of%20predictions%20and%20highlight%20its%20potential%20in%20enhancing%20the%0Ainterpretability%20and%20risk%20mitigation%20capabilities%20of%20neural%20networks.%20The%0Aimplications%20of%20this%20work%20extend%20beyond%20image%20classification%2C%20with%20ongoing%0Aapplications%20in%20autonomous%20systems%2C%20such%20as%20self-driving%20cars%2C%20to%20improve%20the%0Asafety%20and%20reliability%20of%20decision-making%20in%20complex%2C%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Misclassification%2520Likelihood%2520Matrix%253A%2520Some%2520Classes%2520Are%2520More%2520Likely%2520To%250A%2520%2520Be%2520Misclassified%2520Than%2520Others%26entry.906535625%3DDaniel%2520Sikar%2520and%2520Artur%2520Garcez%2520and%2520Robin%2520Bloomfield%2520and%2520Tillman%2520Weyde%2520and%2520Kaleem%2520Peeroo%2520and%2520Naman%2520Singh%2520and%2520Maeve%2520Hutchinson%2520and%2520Mirela%2520Reljan-Delaney%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520the%2520Misclassification%2520Likelihood%2520Matrix%2520%2528MLM%2529%2520as%2520a%250Anovel%2520tool%2520for%2520quantifying%2520the%2520reliability%2520of%2520neural%2520network%2520predictions%2520under%250Adistribution%2520shifts.%2520The%2520MLM%2520is%2520obtained%2520by%2520leveraging%2520softmax%2520outputs%2520and%250Aclustering%2520techniques%2520to%2520measure%2520the%2520distances%2520between%2520the%2520predictions%2520of%2520a%250Atrained%2520neural%2520network%2520and%2520class%2520centroids.%2520By%2520analyzing%2520these%2520distances%252C%2520the%250AMLM%2520provides%2520a%2520comprehensive%2520view%2520of%2520the%2520model%2527s%2520misclassification%2520tendencies%252C%250Aenabling%2520decision-makers%2520to%2520identify%2520the%2520most%2520common%2520and%2520critical%2520sources%2520of%250Aerrors.%2520The%2520MLM%2520allows%2520for%2520the%2520prioritization%2520of%2520model%2520improvements%2520and%2520the%250Aestablishment%2520of%2520decision%2520thresholds%2520based%2520on%2520acceptable%2520risk%2520levels.%2520The%250Aapproach%2520is%2520evaluated%2520on%2520the%2520MNIST%2520dataset%2520using%2520a%2520Convolutional%2520Neural%2520Network%250A%2528CNN%2529%2520and%2520a%2520perturbed%2520version%2520of%2520the%2520dataset%2520to%2520simulate%2520distribution%2520shifts.%250AThe%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520MLM%2520in%2520assessing%2520the%250Areliability%2520of%2520predictions%2520and%2520highlight%2520its%2520potential%2520in%2520enhancing%2520the%250Ainterpretability%2520and%2520risk%2520mitigation%2520capabilities%2520of%2520neural%2520networks.%2520The%250Aimplications%2520of%2520this%2520work%2520extend%2520beyond%2520image%2520classification%252C%2520with%2520ongoing%250Aapplications%2520in%2520autonomous%2520systems%252C%2520such%2520as%2520self-driving%2520cars%252C%2520to%2520improve%2520the%250Asafety%2520and%2520reliability%2520of%2520decision-making%2520in%2520complex%252C%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Misclassification%20Likelihood%20Matrix%3A%20Some%20Classes%20Are%20More%20Likely%20To%0A%20%20Be%20Misclassified%20Than%20Others&entry.906535625=Daniel%20Sikar%20and%20Artur%20Garcez%20and%20Robin%20Bloomfield%20and%20Tillman%20Weyde%20and%20Kaleem%20Peeroo%20and%20Naman%20Singh%20and%20Maeve%20Hutchinson%20and%20Mirela%20Reljan-Delaney&entry.1292438233=%20%20This%20study%20introduces%20the%20Misclassification%20Likelihood%20Matrix%20%28MLM%29%20as%20a%0Anovel%20tool%20for%20quantifying%20the%20reliability%20of%20neural%20network%20predictions%20under%0Adistribution%20shifts.%20The%20MLM%20is%20obtained%20by%20leveraging%20softmax%20outputs%20and%0Aclustering%20techniques%20to%20measure%20the%20distances%20between%20the%20predictions%20of%20a%0Atrained%20neural%20network%20and%20class%20centroids.%20By%20analyzing%20these%20distances%2C%20the%0AMLM%20provides%20a%20comprehensive%20view%20of%20the%20model%27s%20misclassification%20tendencies%2C%0Aenabling%20decision-makers%20to%20identify%20the%20most%20common%20and%20critical%20sources%20of%0Aerrors.%20The%20MLM%20allows%20for%20the%20prioritization%20of%20model%20improvements%20and%20the%0Aestablishment%20of%20decision%20thresholds%20based%20on%20acceptable%20risk%20levels.%20The%0Aapproach%20is%20evaluated%20on%20the%20MNIST%20dataset%20using%20a%20Convolutional%20Neural%20Network%0A%28CNN%29%20and%20a%20perturbed%20version%20of%20the%20dataset%20to%20simulate%20distribution%20shifts.%0AThe%20results%20demonstrate%20the%20effectiveness%20of%20the%20MLM%20in%20assessing%20the%0Areliability%20of%20predictions%20and%20highlight%20its%20potential%20in%20enhancing%20the%0Ainterpretability%20and%20risk%20mitigation%20capabilities%20of%20neural%20networks.%20The%0Aimplications%20of%20this%20work%20extend%20beyond%20image%20classification%2C%20with%20ongoing%0Aapplications%20in%20autonomous%20systems%2C%20such%20as%20self-driving%20cars%2C%20to%20improve%20the%0Asafety%20and%20reliability%20of%20decision-making%20in%20complex%2C%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07818v1&entry.124074799=Read"},
{"title": "LSM: A Comprehensive Metric for Assessing the Safety of Lane Detection\n  Systems in Autonomous Driving", "author": "J\u00f6rg Gamerdinger and Sven Teufel and Stephan Amann and Georg Volk and Oliver Bringmann", "abstract": "  Comprehensive perception of the vehicle's environment and correct\ninterpretation of the environment are crucial for the safe operation of\nautonomous vehicles. The perception of surrounding objects is the main\ncomponent for further tasks such as trajectory planning. However, safe\ntrajectory planning requires not only object detection, but also the detection\nof drivable areas and lane corridors. While first approaches consider an\nadvanced safety evaluation of object detection, the evaluation of lane\ndetection still lacks sufficient safety metrics. Similar to the safety metrics\nfor object detection, additional factors such as the semantics of the scene\nwith road type and road width, the detection range as well as the potential\ncauses of missing detections, incorporated by vehicle speed, should be\nconsidered for the evaluation of lane detection. Therefore, we propose the Lane\nSafety Metric (LSM), which takes these factors into account and allows to\nevaluate the safety of lane detection systems by determining an easily\ninterpretable safety score. We evaluate our offline safety metric on various\nvirtual scenarios using different lane detection approaches and compare it with\nstate-of-the-art performance metrics.\n", "link": "http://arxiv.org/abs/2407.07740v1", "date": "2024-07-10", "relevancy": 2.006, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5258}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LSM%3A%20A%20Comprehensive%20Metric%20for%20Assessing%20the%20Safety%20of%20Lane%20Detection%0A%20%20Systems%20in%20Autonomous%20Driving&body=Title%3A%20LSM%3A%20A%20Comprehensive%20Metric%20for%20Assessing%20the%20Safety%20of%20Lane%20Detection%0A%20%20Systems%20in%20Autonomous%20Driving%0AAuthor%3A%20J%C3%B6rg%20Gamerdinger%20and%20Sven%20Teufel%20and%20Stephan%20Amann%20and%20Georg%20Volk%20and%20Oliver%20Bringmann%0AAbstract%3A%20%20%20Comprehensive%20perception%20of%20the%20vehicle%27s%20environment%20and%20correct%0Ainterpretation%20of%20the%20environment%20are%20crucial%20for%20the%20safe%20operation%20of%0Aautonomous%20vehicles.%20The%20perception%20of%20surrounding%20objects%20is%20the%20main%0Acomponent%20for%20further%20tasks%20such%20as%20trajectory%20planning.%20However%2C%20safe%0Atrajectory%20planning%20requires%20not%20only%20object%20detection%2C%20but%20also%20the%20detection%0Aof%20drivable%20areas%20and%20lane%20corridors.%20While%20first%20approaches%20consider%20an%0Aadvanced%20safety%20evaluation%20of%20object%20detection%2C%20the%20evaluation%20of%20lane%0Adetection%20still%20lacks%20sufficient%20safety%20metrics.%20Similar%20to%20the%20safety%20metrics%0Afor%20object%20detection%2C%20additional%20factors%20such%20as%20the%20semantics%20of%20the%20scene%0Awith%20road%20type%20and%20road%20width%2C%20the%20detection%20range%20as%20well%20as%20the%20potential%0Acauses%20of%20missing%20detections%2C%20incorporated%20by%20vehicle%20speed%2C%20should%20be%0Aconsidered%20for%20the%20evaluation%20of%20lane%20detection.%20Therefore%2C%20we%20propose%20the%20Lane%0ASafety%20Metric%20%28LSM%29%2C%20which%20takes%20these%20factors%20into%20account%20and%20allows%20to%0Aevaluate%20the%20safety%20of%20lane%20detection%20systems%20by%20determining%20an%20easily%0Ainterpretable%20safety%20score.%20We%20evaluate%20our%20offline%20safety%20metric%20on%20various%0Avirtual%20scenarios%20using%20different%20lane%20detection%20approaches%20and%20compare%20it%20with%0Astate-of-the-art%20performance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLSM%253A%2520A%2520Comprehensive%2520Metric%2520for%2520Assessing%2520the%2520Safety%2520of%2520Lane%2520Detection%250A%2520%2520Systems%2520in%2520Autonomous%2520Driving%26entry.906535625%3DJ%25C3%25B6rg%2520Gamerdinger%2520and%2520Sven%2520Teufel%2520and%2520Stephan%2520Amann%2520and%2520Georg%2520Volk%2520and%2520Oliver%2520Bringmann%26entry.1292438233%3D%2520%2520Comprehensive%2520perception%2520of%2520the%2520vehicle%2527s%2520environment%2520and%2520correct%250Ainterpretation%2520of%2520the%2520environment%2520are%2520crucial%2520for%2520the%2520safe%2520operation%2520of%250Aautonomous%2520vehicles.%2520The%2520perception%2520of%2520surrounding%2520objects%2520is%2520the%2520main%250Acomponent%2520for%2520further%2520tasks%2520such%2520as%2520trajectory%2520planning.%2520However%252C%2520safe%250Atrajectory%2520planning%2520requires%2520not%2520only%2520object%2520detection%252C%2520but%2520also%2520the%2520detection%250Aof%2520drivable%2520areas%2520and%2520lane%2520corridors.%2520While%2520first%2520approaches%2520consider%2520an%250Aadvanced%2520safety%2520evaluation%2520of%2520object%2520detection%252C%2520the%2520evaluation%2520of%2520lane%250Adetection%2520still%2520lacks%2520sufficient%2520safety%2520metrics.%2520Similar%2520to%2520the%2520safety%2520metrics%250Afor%2520object%2520detection%252C%2520additional%2520factors%2520such%2520as%2520the%2520semantics%2520of%2520the%2520scene%250Awith%2520road%2520type%2520and%2520road%2520width%252C%2520the%2520detection%2520range%2520as%2520well%2520as%2520the%2520potential%250Acauses%2520of%2520missing%2520detections%252C%2520incorporated%2520by%2520vehicle%2520speed%252C%2520should%2520be%250Aconsidered%2520for%2520the%2520evaluation%2520of%2520lane%2520detection.%2520Therefore%252C%2520we%2520propose%2520the%2520Lane%250ASafety%2520Metric%2520%2528LSM%2529%252C%2520which%2520takes%2520these%2520factors%2520into%2520account%2520and%2520allows%2520to%250Aevaluate%2520the%2520safety%2520of%2520lane%2520detection%2520systems%2520by%2520determining%2520an%2520easily%250Ainterpretable%2520safety%2520score.%2520We%2520evaluate%2520our%2520offline%2520safety%2520metric%2520on%2520various%250Avirtual%2520scenarios%2520using%2520different%2520lane%2520detection%2520approaches%2520and%2520compare%2520it%2520with%250Astate-of-the-art%2520performance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSM%3A%20A%20Comprehensive%20Metric%20for%20Assessing%20the%20Safety%20of%20Lane%20Detection%0A%20%20Systems%20in%20Autonomous%20Driving&entry.906535625=J%C3%B6rg%20Gamerdinger%20and%20Sven%20Teufel%20and%20Stephan%20Amann%20and%20Georg%20Volk%20and%20Oliver%20Bringmann&entry.1292438233=%20%20Comprehensive%20perception%20of%20the%20vehicle%27s%20environment%20and%20correct%0Ainterpretation%20of%20the%20environment%20are%20crucial%20for%20the%20safe%20operation%20of%0Aautonomous%20vehicles.%20The%20perception%20of%20surrounding%20objects%20is%20the%20main%0Acomponent%20for%20further%20tasks%20such%20as%20trajectory%20planning.%20However%2C%20safe%0Atrajectory%20planning%20requires%20not%20only%20object%20detection%2C%20but%20also%20the%20detection%0Aof%20drivable%20areas%20and%20lane%20corridors.%20While%20first%20approaches%20consider%20an%0Aadvanced%20safety%20evaluation%20of%20object%20detection%2C%20the%20evaluation%20of%20lane%0Adetection%20still%20lacks%20sufficient%20safety%20metrics.%20Similar%20to%20the%20safety%20metrics%0Afor%20object%20detection%2C%20additional%20factors%20such%20as%20the%20semantics%20of%20the%20scene%0Awith%20road%20type%20and%20road%20width%2C%20the%20detection%20range%20as%20well%20as%20the%20potential%0Acauses%20of%20missing%20detections%2C%20incorporated%20by%20vehicle%20speed%2C%20should%20be%0Aconsidered%20for%20the%20evaluation%20of%20lane%20detection.%20Therefore%2C%20we%20propose%20the%20Lane%0ASafety%20Metric%20%28LSM%29%2C%20which%20takes%20these%20factors%20into%20account%20and%20allows%20to%0Aevaluate%20the%20safety%20of%20lane%20detection%20systems%20by%20determining%20an%20easily%0Ainterpretable%20safety%20score.%20We%20evaluate%20our%20offline%20safety%20metric%20on%20various%0Avirtual%20scenarios%20using%20different%20lane%20detection%20approaches%20and%20compare%20it%20with%0Astate-of-the-art%20performance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07740v1&entry.124074799=Read"},
{"title": "KaiRacters: Character-level-based Writer Retrieval for Greek Papyri", "author": "Marco Peer and Robert Sablatnig and Olga Serbaeva and Isabelle Marthot-Santaniello", "abstract": "  This paper presents a character-based approach for enhancing writer retrieval\nperformance in the context of Greek papyri. Our contribution lies in\nintroducing character-level annotations for frequently used characters, in our\ncase the trigram kai and four additional letters (epsilon, kappa, mu, omega),\nin Greek texts. We use a state-of-the-art writer retrieval approach based on\nNetVLAD and compare a character-level-based feature aggregation method against\nthe current default baseline of using small patches located at SIFT keypoint\nlocations for building the page descriptors. We demonstrate that by using only\nabout 15 characters per page, we are able to boost the performance up to 4% mAP\n(a relative improvement of 11%) on the GRK-120 dataset. Additionally, our\nqualitative analysis offers insights into the similarity scores of SIFT patches\nand specific characters. We publish the dataset with character-level\nannotations, including a quality label and our binarized images for further\nresearch.\n", "link": "http://arxiv.org/abs/2407.07536v1", "date": "2024-07-10", "relevancy": 2.0054, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4175}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3963}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KaiRacters%3A%20Character-level-based%20Writer%20Retrieval%20for%20Greek%20Papyri&body=Title%3A%20KaiRacters%3A%20Character-level-based%20Writer%20Retrieval%20for%20Greek%20Papyri%0AAuthor%3A%20Marco%20Peer%20and%20Robert%20Sablatnig%20and%20Olga%20Serbaeva%20and%20Isabelle%20Marthot-Santaniello%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20character-based%20approach%20for%20enhancing%20writer%20retrieval%0Aperformance%20in%20the%20context%20of%20Greek%20papyri.%20Our%20contribution%20lies%20in%0Aintroducing%20character-level%20annotations%20for%20frequently%20used%20characters%2C%20in%20our%0Acase%20the%20trigram%20kai%20and%20four%20additional%20letters%20%28epsilon%2C%20kappa%2C%20mu%2C%20omega%29%2C%0Ain%20Greek%20texts.%20We%20use%20a%20state-of-the-art%20writer%20retrieval%20approach%20based%20on%0ANetVLAD%20and%20compare%20a%20character-level-based%20feature%20aggregation%20method%20against%0Athe%20current%20default%20baseline%20of%20using%20small%20patches%20located%20at%20SIFT%20keypoint%0Alocations%20for%20building%20the%20page%20descriptors.%20We%20demonstrate%20that%20by%20using%20only%0Aabout%2015%20characters%20per%20page%2C%20we%20are%20able%20to%20boost%20the%20performance%20up%20to%204%25%20mAP%0A%28a%20relative%20improvement%20of%2011%25%29%20on%20the%20GRK-120%20dataset.%20Additionally%2C%20our%0Aqualitative%20analysis%20offers%20insights%20into%20the%20similarity%20scores%20of%20SIFT%20patches%0Aand%20specific%20characters.%20We%20publish%20the%20dataset%20with%20character-level%0Aannotations%2C%20including%20a%20quality%20label%20and%20our%20binarized%20images%20for%20further%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKaiRacters%253A%2520Character-level-based%2520Writer%2520Retrieval%2520for%2520Greek%2520Papyri%26entry.906535625%3DMarco%2520Peer%2520and%2520Robert%2520Sablatnig%2520and%2520Olga%2520Serbaeva%2520and%2520Isabelle%2520Marthot-Santaniello%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520character-based%2520approach%2520for%2520enhancing%2520writer%2520retrieval%250Aperformance%2520in%2520the%2520context%2520of%2520Greek%2520papyri.%2520Our%2520contribution%2520lies%2520in%250Aintroducing%2520character-level%2520annotations%2520for%2520frequently%2520used%2520characters%252C%2520in%2520our%250Acase%2520the%2520trigram%2520kai%2520and%2520four%2520additional%2520letters%2520%2528epsilon%252C%2520kappa%252C%2520mu%252C%2520omega%2529%252C%250Ain%2520Greek%2520texts.%2520We%2520use%2520a%2520state-of-the-art%2520writer%2520retrieval%2520approach%2520based%2520on%250ANetVLAD%2520and%2520compare%2520a%2520character-level-based%2520feature%2520aggregation%2520method%2520against%250Athe%2520current%2520default%2520baseline%2520of%2520using%2520small%2520patches%2520located%2520at%2520SIFT%2520keypoint%250Alocations%2520for%2520building%2520the%2520page%2520descriptors.%2520We%2520demonstrate%2520that%2520by%2520using%2520only%250Aabout%252015%2520characters%2520per%2520page%252C%2520we%2520are%2520able%2520to%2520boost%2520the%2520performance%2520up%2520to%25204%2525%2520mAP%250A%2528a%2520relative%2520improvement%2520of%252011%2525%2529%2520on%2520the%2520GRK-120%2520dataset.%2520Additionally%252C%2520our%250Aqualitative%2520analysis%2520offers%2520insights%2520into%2520the%2520similarity%2520scores%2520of%2520SIFT%2520patches%250Aand%2520specific%2520characters.%2520We%2520publish%2520the%2520dataset%2520with%2520character-level%250Aannotations%252C%2520including%2520a%2520quality%2520label%2520and%2520our%2520binarized%2520images%2520for%2520further%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KaiRacters%3A%20Character-level-based%20Writer%20Retrieval%20for%20Greek%20Papyri&entry.906535625=Marco%20Peer%20and%20Robert%20Sablatnig%20and%20Olga%20Serbaeva%20and%20Isabelle%20Marthot-Santaniello&entry.1292438233=%20%20This%20paper%20presents%20a%20character-based%20approach%20for%20enhancing%20writer%20retrieval%0Aperformance%20in%20the%20context%20of%20Greek%20papyri.%20Our%20contribution%20lies%20in%0Aintroducing%20character-level%20annotations%20for%20frequently%20used%20characters%2C%20in%20our%0Acase%20the%20trigram%20kai%20and%20four%20additional%20letters%20%28epsilon%2C%20kappa%2C%20mu%2C%20omega%29%2C%0Ain%20Greek%20texts.%20We%20use%20a%20state-of-the-art%20writer%20retrieval%20approach%20based%20on%0ANetVLAD%20and%20compare%20a%20character-level-based%20feature%20aggregation%20method%20against%0Athe%20current%20default%20baseline%20of%20using%20small%20patches%20located%20at%20SIFT%20keypoint%0Alocations%20for%20building%20the%20page%20descriptors.%20We%20demonstrate%20that%20by%20using%20only%0Aabout%2015%20characters%20per%20page%2C%20we%20are%20able%20to%20boost%20the%20performance%20up%20to%204%25%20mAP%0A%28a%20relative%20improvement%20of%2011%25%29%20on%20the%20GRK-120%20dataset.%20Additionally%2C%20our%0Aqualitative%20analysis%20offers%20insights%20into%20the%20similarity%20scores%20of%20SIFT%20patches%0Aand%20specific%20characters.%20We%20publish%20the%20dataset%20with%20character-level%0Aannotations%2C%20including%20a%20quality%20label%20and%20our%20binarized%20images%20for%20further%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07536v1&entry.124074799=Read"},
{"title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model\n  Series", "author": "Ge Zhang and Scott Qu and Jiaheng Liu and Chenchen Zhang and Chenghua Lin and Chou Leuang Yu and Danny Pan and Esther Cheng and Jie Liu and Qunshu Lin and Raven Yuan and Tuney Zheng and Wei Pang and Xinrun Du and Yiming Liang and Yinghao Ma and Yizhi Li and Ziyang Ma and Bill Lin and Emmanouil Benetos and Huan Yang and Junting Zhou and Kaijing Ma and Minghao Liu and Morry Niu and Noah Wang and Quehry Que and Ruibo Liu and Sine Liu and Shawn Guo and Soren Gao and Wangchunshu Zhou and Xinyue Zhang and Yizhi Zhou and Yubo Wang and Yuelin Bai and Yuhan Zhang and Yuxiang Zhang and Zenith Wang and Zhenzhu Yang and Zijian Zhao and Jiajun Zhang and Wanli Ouyang and Wenhao Huang and Wenhu Chen", "abstract": "  Large Language Models (LLMs) have made great strides in recent years to\nachieve unprecedented performance across different tasks. However, due to\ncommercial interest, the most competitive models like GPT, Gemini, and Claude\nhave been gated behind proprietary interfaces without disclosing the training\ndetails. Recently, many institutions have open-sourced several strong LLMs like\nLLaMA-3, comparable to existing closed-source LLMs. However, only the model's\nweights are provided with most details (e.g., intermediate checkpoints,\npre-training corpus, and training code, etc.) being undisclosed. To improve the\ntransparency of LLMs, the research community has formed to open-source truly\nopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training\ncorpus and training code) are being provided. These models have greatly\nadvanced the scientific study of these large models including their strengths,\nweaknesses, biases and risks. However, we observe that the existing truly open\nLLMs on reasoning, knowledge, and coding tasks are still inferior to existing\nstate-of-the-art LLMs with similar model sizes. To this end, we open-source\nMAP-Neo, a highly capable and transparent bilingual language model with 7B\nparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the\nfirst fully open-sourced bilingual LLM with comparable performance compared to\nexisting state-of-the-art LLMs. Moreover, we open-source all details to\nreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning\npipeline, checkpoints, and well-optimized training/evaluation framework are\nprovided. Finally, we hope our MAP-Neo will enhance and strengthen the open\nresearch community and inspire more innovations and creativities to facilitate\nthe further improvements of LLMs.\n", "link": "http://arxiv.org/abs/2405.19327v4", "date": "2024-07-10", "relevancy": 2.0034, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5054}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAP-Neo%3A%20Highly%20Capable%20and%20Transparent%20Bilingual%20Large%20Language%20Model%0A%20%20Series&body=Title%3A%20MAP-Neo%3A%20Highly%20Capable%20and%20Transparent%20Bilingual%20Large%20Language%20Model%0A%20%20Series%0AAuthor%3A%20Ge%20Zhang%20and%20Scott%20Qu%20and%20Jiaheng%20Liu%20and%20Chenchen%20Zhang%20and%20Chenghua%20Lin%20and%20Chou%20Leuang%20Yu%20and%20Danny%20Pan%20and%20Esther%20Cheng%20and%20Jie%20Liu%20and%20Qunshu%20Lin%20and%20Raven%20Yuan%20and%20Tuney%20Zheng%20and%20Wei%20Pang%20and%20Xinrun%20Du%20and%20Yiming%20Liang%20and%20Yinghao%20Ma%20and%20Yizhi%20Li%20and%20Ziyang%20Ma%20and%20Bill%20Lin%20and%20Emmanouil%20Benetos%20and%20Huan%20Yang%20and%20Junting%20Zhou%20and%20Kaijing%20Ma%20and%20Minghao%20Liu%20and%20Morry%20Niu%20and%20Noah%20Wang%20and%20Quehry%20Que%20and%20Ruibo%20Liu%20and%20Sine%20Liu%20and%20Shawn%20Guo%20and%20Soren%20Gao%20and%20Wangchunshu%20Zhou%20and%20Xinyue%20Zhang%20and%20Yizhi%20Zhou%20and%20Yubo%20Wang%20and%20Yuelin%20Bai%20and%20Yuhan%20Zhang%20and%20Yuxiang%20Zhang%20and%20Zenith%20Wang%20and%20Zhenzhu%20Yang%20and%20Zijian%20Zhao%20and%20Jiajun%20Zhang%20and%20Wanli%20Ouyang%20and%20Wenhao%20Huang%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20great%20strides%20in%20recent%20years%20to%0Aachieve%20unprecedented%20performance%20across%20different%20tasks.%20However%2C%20due%20to%0Acommercial%20interest%2C%20the%20most%20competitive%20models%20like%20GPT%2C%20Gemini%2C%20and%20Claude%0Ahave%20been%20gated%20behind%20proprietary%20interfaces%20without%20disclosing%20the%20training%0Adetails.%20Recently%2C%20many%20institutions%20have%20open-sourced%20several%20strong%20LLMs%20like%0ALLaMA-3%2C%20comparable%20to%20existing%20closed-source%20LLMs.%20However%2C%20only%20the%20model%27s%0Aweights%20are%20provided%20with%20most%20details%20%28e.g.%2C%20intermediate%20checkpoints%2C%0Apre-training%20corpus%2C%20and%20training%20code%2C%20etc.%29%20being%20undisclosed.%20To%20improve%20the%0Atransparency%20of%20LLMs%2C%20the%20research%20community%20has%20formed%20to%20open-source%20truly%0Aopen%20LLMs%20%28e.g.%2C%20Pythia%2C%20Amber%2C%20OLMo%29%2C%20where%20more%20details%20%28e.g.%2C%20pre-training%0Acorpus%20and%20training%20code%29%20are%20being%20provided.%20These%20models%20have%20greatly%0Aadvanced%20the%20scientific%20study%20of%20these%20large%20models%20including%20their%20strengths%2C%0Aweaknesses%2C%20biases%20and%20risks.%20However%2C%20we%20observe%20that%20the%20existing%20truly%20open%0ALLMs%20on%20reasoning%2C%20knowledge%2C%20and%20coding%20tasks%20are%20still%20inferior%20to%20existing%0Astate-of-the-art%20LLMs%20with%20similar%20model%20sizes.%20To%20this%20end%2C%20we%20open-source%0AMAP-Neo%2C%20a%20highly%20capable%20and%20transparent%20bilingual%20language%20model%20with%207B%0Aparameters%20trained%20from%20scratch%20on%204.5T%20high-quality%20tokens.%20Our%20MAP-Neo%20is%20the%0Afirst%20fully%20open-sourced%20bilingual%20LLM%20with%20comparable%20performance%20compared%20to%0Aexisting%20state-of-the-art%20LLMs.%20Moreover%2C%20we%20open-source%20all%20details%20to%0Areproduce%20our%20MAP-Neo%2C%20where%20the%20cleaned%20pre-training%20corpus%2C%20data%20cleaning%0Apipeline%2C%20checkpoints%2C%20and%20well-optimized%20training/evaluation%20framework%20are%0Aprovided.%20Finally%2C%20we%20hope%20our%20MAP-Neo%20will%20enhance%20and%20strengthen%20the%20open%0Aresearch%20community%20and%20inspire%20more%20innovations%20and%20creativities%20to%20facilitate%0Athe%20further%20improvements%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19327v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAP-Neo%253A%2520Highly%2520Capable%2520and%2520Transparent%2520Bilingual%2520Large%2520Language%2520Model%250A%2520%2520Series%26entry.906535625%3DGe%2520Zhang%2520and%2520Scott%2520Qu%2520and%2520Jiaheng%2520Liu%2520and%2520Chenchen%2520Zhang%2520and%2520Chenghua%2520Lin%2520and%2520Chou%2520Leuang%2520Yu%2520and%2520Danny%2520Pan%2520and%2520Esther%2520Cheng%2520and%2520Jie%2520Liu%2520and%2520Qunshu%2520Lin%2520and%2520Raven%2520Yuan%2520and%2520Tuney%2520Zheng%2520and%2520Wei%2520Pang%2520and%2520Xinrun%2520Du%2520and%2520Yiming%2520Liang%2520and%2520Yinghao%2520Ma%2520and%2520Yizhi%2520Li%2520and%2520Ziyang%2520Ma%2520and%2520Bill%2520Lin%2520and%2520Emmanouil%2520Benetos%2520and%2520Huan%2520Yang%2520and%2520Junting%2520Zhou%2520and%2520Kaijing%2520Ma%2520and%2520Minghao%2520Liu%2520and%2520Morry%2520Niu%2520and%2520Noah%2520Wang%2520and%2520Quehry%2520Que%2520and%2520Ruibo%2520Liu%2520and%2520Sine%2520Liu%2520and%2520Shawn%2520Guo%2520and%2520Soren%2520Gao%2520and%2520Wangchunshu%2520Zhou%2520and%2520Xinyue%2520Zhang%2520and%2520Yizhi%2520Zhou%2520and%2520Yubo%2520Wang%2520and%2520Yuelin%2520Bai%2520and%2520Yuhan%2520Zhang%2520and%2520Yuxiang%2520Zhang%2520and%2520Zenith%2520Wang%2520and%2520Zhenzhu%2520Yang%2520and%2520Zijian%2520Zhao%2520and%2520Jiajun%2520Zhang%2520and%2520Wanli%2520Ouyang%2520and%2520Wenhao%2520Huang%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520great%2520strides%2520in%2520recent%2520years%2520to%250Aachieve%2520unprecedented%2520performance%2520across%2520different%2520tasks.%2520However%252C%2520due%2520to%250Acommercial%2520interest%252C%2520the%2520most%2520competitive%2520models%2520like%2520GPT%252C%2520Gemini%252C%2520and%2520Claude%250Ahave%2520been%2520gated%2520behind%2520proprietary%2520interfaces%2520without%2520disclosing%2520the%2520training%250Adetails.%2520Recently%252C%2520many%2520institutions%2520have%2520open-sourced%2520several%2520strong%2520LLMs%2520like%250ALLaMA-3%252C%2520comparable%2520to%2520existing%2520closed-source%2520LLMs.%2520However%252C%2520only%2520the%2520model%2527s%250Aweights%2520are%2520provided%2520with%2520most%2520details%2520%2528e.g.%252C%2520intermediate%2520checkpoints%252C%250Apre-training%2520corpus%252C%2520and%2520training%2520code%252C%2520etc.%2529%2520being%2520undisclosed.%2520To%2520improve%2520the%250Atransparency%2520of%2520LLMs%252C%2520the%2520research%2520community%2520has%2520formed%2520to%2520open-source%2520truly%250Aopen%2520LLMs%2520%2528e.g.%252C%2520Pythia%252C%2520Amber%252C%2520OLMo%2529%252C%2520where%2520more%2520details%2520%2528e.g.%252C%2520pre-training%250Acorpus%2520and%2520training%2520code%2529%2520are%2520being%2520provided.%2520These%2520models%2520have%2520greatly%250Aadvanced%2520the%2520scientific%2520study%2520of%2520these%2520large%2520models%2520including%2520their%2520strengths%252C%250Aweaknesses%252C%2520biases%2520and%2520risks.%2520However%252C%2520we%2520observe%2520that%2520the%2520existing%2520truly%2520open%250ALLMs%2520on%2520reasoning%252C%2520knowledge%252C%2520and%2520coding%2520tasks%2520are%2520still%2520inferior%2520to%2520existing%250Astate-of-the-art%2520LLMs%2520with%2520similar%2520model%2520sizes.%2520To%2520this%2520end%252C%2520we%2520open-source%250AMAP-Neo%252C%2520a%2520highly%2520capable%2520and%2520transparent%2520bilingual%2520language%2520model%2520with%25207B%250Aparameters%2520trained%2520from%2520scratch%2520on%25204.5T%2520high-quality%2520tokens.%2520Our%2520MAP-Neo%2520is%2520the%250Afirst%2520fully%2520open-sourced%2520bilingual%2520LLM%2520with%2520comparable%2520performance%2520compared%2520to%250Aexisting%2520state-of-the-art%2520LLMs.%2520Moreover%252C%2520we%2520open-source%2520all%2520details%2520to%250Areproduce%2520our%2520MAP-Neo%252C%2520where%2520the%2520cleaned%2520pre-training%2520corpus%252C%2520data%2520cleaning%250Apipeline%252C%2520checkpoints%252C%2520and%2520well-optimized%2520training/evaluation%2520framework%2520are%250Aprovided.%2520Finally%252C%2520we%2520hope%2520our%2520MAP-Neo%2520will%2520enhance%2520and%2520strengthen%2520the%2520open%250Aresearch%2520community%2520and%2520inspire%2520more%2520innovations%2520and%2520creativities%2520to%2520facilitate%250Athe%2520further%2520improvements%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19327v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAP-Neo%3A%20Highly%20Capable%20and%20Transparent%20Bilingual%20Large%20Language%20Model%0A%20%20Series&entry.906535625=Ge%20Zhang%20and%20Scott%20Qu%20and%20Jiaheng%20Liu%20and%20Chenchen%20Zhang%20and%20Chenghua%20Lin%20and%20Chou%20Leuang%20Yu%20and%20Danny%20Pan%20and%20Esther%20Cheng%20and%20Jie%20Liu%20and%20Qunshu%20Lin%20and%20Raven%20Yuan%20and%20Tuney%20Zheng%20and%20Wei%20Pang%20and%20Xinrun%20Du%20and%20Yiming%20Liang%20and%20Yinghao%20Ma%20and%20Yizhi%20Li%20and%20Ziyang%20Ma%20and%20Bill%20Lin%20and%20Emmanouil%20Benetos%20and%20Huan%20Yang%20and%20Junting%20Zhou%20and%20Kaijing%20Ma%20and%20Minghao%20Liu%20and%20Morry%20Niu%20and%20Noah%20Wang%20and%20Quehry%20Que%20and%20Ruibo%20Liu%20and%20Sine%20Liu%20and%20Shawn%20Guo%20and%20Soren%20Gao%20and%20Wangchunshu%20Zhou%20and%20Xinyue%20Zhang%20and%20Yizhi%20Zhou%20and%20Yubo%20Wang%20and%20Yuelin%20Bai%20and%20Yuhan%20Zhang%20and%20Yuxiang%20Zhang%20and%20Zenith%20Wang%20and%20Zhenzhu%20Yang%20and%20Zijian%20Zhao%20and%20Jiajun%20Zhang%20and%20Wanli%20Ouyang%20and%20Wenhao%20Huang%20and%20Wenhu%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20great%20strides%20in%20recent%20years%20to%0Aachieve%20unprecedented%20performance%20across%20different%20tasks.%20However%2C%20due%20to%0Acommercial%20interest%2C%20the%20most%20competitive%20models%20like%20GPT%2C%20Gemini%2C%20and%20Claude%0Ahave%20been%20gated%20behind%20proprietary%20interfaces%20without%20disclosing%20the%20training%0Adetails.%20Recently%2C%20many%20institutions%20have%20open-sourced%20several%20strong%20LLMs%20like%0ALLaMA-3%2C%20comparable%20to%20existing%20closed-source%20LLMs.%20However%2C%20only%20the%20model%27s%0Aweights%20are%20provided%20with%20most%20details%20%28e.g.%2C%20intermediate%20checkpoints%2C%0Apre-training%20corpus%2C%20and%20training%20code%2C%20etc.%29%20being%20undisclosed.%20To%20improve%20the%0Atransparency%20of%20LLMs%2C%20the%20research%20community%20has%20formed%20to%20open-source%20truly%0Aopen%20LLMs%20%28e.g.%2C%20Pythia%2C%20Amber%2C%20OLMo%29%2C%20where%20more%20details%20%28e.g.%2C%20pre-training%0Acorpus%20and%20training%20code%29%20are%20being%20provided.%20These%20models%20have%20greatly%0Aadvanced%20the%20scientific%20study%20of%20these%20large%20models%20including%20their%20strengths%2C%0Aweaknesses%2C%20biases%20and%20risks.%20However%2C%20we%20observe%20that%20the%20existing%20truly%20open%0ALLMs%20on%20reasoning%2C%20knowledge%2C%20and%20coding%20tasks%20are%20still%20inferior%20to%20existing%0Astate-of-the-art%20LLMs%20with%20similar%20model%20sizes.%20To%20this%20end%2C%20we%20open-source%0AMAP-Neo%2C%20a%20highly%20capable%20and%20transparent%20bilingual%20language%20model%20with%207B%0Aparameters%20trained%20from%20scratch%20on%204.5T%20high-quality%20tokens.%20Our%20MAP-Neo%20is%20the%0Afirst%20fully%20open-sourced%20bilingual%20LLM%20with%20comparable%20performance%20compared%20to%0Aexisting%20state-of-the-art%20LLMs.%20Moreover%2C%20we%20open-source%20all%20details%20to%0Areproduce%20our%20MAP-Neo%2C%20where%20the%20cleaned%20pre-training%20corpus%2C%20data%20cleaning%0Apipeline%2C%20checkpoints%2C%20and%20well-optimized%20training/evaluation%20framework%20are%0Aprovided.%20Finally%2C%20we%20hope%20our%20MAP-Neo%20will%20enhance%20and%20strengthen%20the%20open%0Aresearch%20community%20and%20inspire%20more%20innovations%20and%20creativities%20to%20facilitate%0Athe%20further%20improvements%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19327v4&entry.124074799=Read"},
{"title": "The Hybrid Extended Bicycle: A Simple Model for High Dynamic Vehicle\n  Trajectory Planning", "author": "Agapius Bou Ghosn and Philip Polack and Arnaud de La Fortelle", "abstract": "  While highly automated driving relies most of the time on a smooth driving\nassumption, the possibility of a vehicle performing harsh maneuvers with high\ndynamic driving to face unexpected events is very likely. The modeling of the\nbehavior of the vehicle in these events is crucial to proper planning and\ncontrolling; the used model should present accurate and computationally\nefficient properties to ensure consistency with the dynamics of the vehicle and\nto be employed in real-time systems. In this article, we propose an LSTM-based\nhybrid extended bicycle model able to present an accurate description of the\nstate of the vehicle for both normal and aggressive situations. The introduced\nmodel is used in a Model Predictive Path Integral (MPPI) plan and control\nframework for performing trajectories in high-dynamic scenarios. The proposed\nmodel and framework prove their ability to plan feasible trajectories ensuring\nan accurate vehicle behavior even at the limits of handling.\n", "link": "http://arxiv.org/abs/2306.04857v2", "date": "2024-07-10", "relevancy": 1.9926, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5698}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4953}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Hybrid%20Extended%20Bicycle%3A%20A%20Simple%20Model%20for%20High%20Dynamic%20Vehicle%0A%20%20Trajectory%20Planning&body=Title%3A%20The%20Hybrid%20Extended%20Bicycle%3A%20A%20Simple%20Model%20for%20High%20Dynamic%20Vehicle%0A%20%20Trajectory%20Planning%0AAuthor%3A%20Agapius%20Bou%20Ghosn%20and%20Philip%20Polack%20and%20Arnaud%20de%20La%20Fortelle%0AAbstract%3A%20%20%20While%20highly%20automated%20driving%20relies%20most%20of%20the%20time%20on%20a%20smooth%20driving%0Aassumption%2C%20the%20possibility%20of%20a%20vehicle%20performing%20harsh%20maneuvers%20with%20high%0Adynamic%20driving%20to%20face%20unexpected%20events%20is%20very%20likely.%20The%20modeling%20of%20the%0Abehavior%20of%20the%20vehicle%20in%20these%20events%20is%20crucial%20to%20proper%20planning%20and%0Acontrolling%3B%20the%20used%20model%20should%20present%20accurate%20and%20computationally%0Aefficient%20properties%20to%20ensure%20consistency%20with%20the%20dynamics%20of%20the%20vehicle%20and%0Ato%20be%20employed%20in%20real-time%20systems.%20In%20this%20article%2C%20we%20propose%20an%20LSTM-based%0Ahybrid%20extended%20bicycle%20model%20able%20to%20present%20an%20accurate%20description%20of%20the%0Astate%20of%20the%20vehicle%20for%20both%20normal%20and%20aggressive%20situations.%20The%20introduced%0Amodel%20is%20used%20in%20a%20Model%20Predictive%20Path%20Integral%20%28MPPI%29%20plan%20and%20control%0Aframework%20for%20performing%20trajectories%20in%20high-dynamic%20scenarios.%20The%20proposed%0Amodel%20and%20framework%20prove%20their%20ability%20to%20plan%20feasible%20trajectories%20ensuring%0Aan%20accurate%20vehicle%20behavior%20even%20at%20the%20limits%20of%20handling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04857v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Hybrid%2520Extended%2520Bicycle%253A%2520A%2520Simple%2520Model%2520for%2520High%2520Dynamic%2520Vehicle%250A%2520%2520Trajectory%2520Planning%26entry.906535625%3DAgapius%2520Bou%2520Ghosn%2520and%2520Philip%2520Polack%2520and%2520Arnaud%2520de%2520La%2520Fortelle%26entry.1292438233%3D%2520%2520While%2520highly%2520automated%2520driving%2520relies%2520most%2520of%2520the%2520time%2520on%2520a%2520smooth%2520driving%250Aassumption%252C%2520the%2520possibility%2520of%2520a%2520vehicle%2520performing%2520harsh%2520maneuvers%2520with%2520high%250Adynamic%2520driving%2520to%2520face%2520unexpected%2520events%2520is%2520very%2520likely.%2520The%2520modeling%2520of%2520the%250Abehavior%2520of%2520the%2520vehicle%2520in%2520these%2520events%2520is%2520crucial%2520to%2520proper%2520planning%2520and%250Acontrolling%253B%2520the%2520used%2520model%2520should%2520present%2520accurate%2520and%2520computationally%250Aefficient%2520properties%2520to%2520ensure%2520consistency%2520with%2520the%2520dynamics%2520of%2520the%2520vehicle%2520and%250Ato%2520be%2520employed%2520in%2520real-time%2520systems.%2520In%2520this%2520article%252C%2520we%2520propose%2520an%2520LSTM-based%250Ahybrid%2520extended%2520bicycle%2520model%2520able%2520to%2520present%2520an%2520accurate%2520description%2520of%2520the%250Astate%2520of%2520the%2520vehicle%2520for%2520both%2520normal%2520and%2520aggressive%2520situations.%2520The%2520introduced%250Amodel%2520is%2520used%2520in%2520a%2520Model%2520Predictive%2520Path%2520Integral%2520%2528MPPI%2529%2520plan%2520and%2520control%250Aframework%2520for%2520performing%2520trajectories%2520in%2520high-dynamic%2520scenarios.%2520The%2520proposed%250Amodel%2520and%2520framework%2520prove%2520their%2520ability%2520to%2520plan%2520feasible%2520trajectories%2520ensuring%250Aan%2520accurate%2520vehicle%2520behavior%2520even%2520at%2520the%2520limits%2520of%2520handling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.04857v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Hybrid%20Extended%20Bicycle%3A%20A%20Simple%20Model%20for%20High%20Dynamic%20Vehicle%0A%20%20Trajectory%20Planning&entry.906535625=Agapius%20Bou%20Ghosn%20and%20Philip%20Polack%20and%20Arnaud%20de%20La%20Fortelle&entry.1292438233=%20%20While%20highly%20automated%20driving%20relies%20most%20of%20the%20time%20on%20a%20smooth%20driving%0Aassumption%2C%20the%20possibility%20of%20a%20vehicle%20performing%20harsh%20maneuvers%20with%20high%0Adynamic%20driving%20to%20face%20unexpected%20events%20is%20very%20likely.%20The%20modeling%20of%20the%0Abehavior%20of%20the%20vehicle%20in%20these%20events%20is%20crucial%20to%20proper%20planning%20and%0Acontrolling%3B%20the%20used%20model%20should%20present%20accurate%20and%20computationally%0Aefficient%20properties%20to%20ensure%20consistency%20with%20the%20dynamics%20of%20the%20vehicle%20and%0Ato%20be%20employed%20in%20real-time%20systems.%20In%20this%20article%2C%20we%20propose%20an%20LSTM-based%0Ahybrid%20extended%20bicycle%20model%20able%20to%20present%20an%20accurate%20description%20of%20the%0Astate%20of%20the%20vehicle%20for%20both%20normal%20and%20aggressive%20situations.%20The%20introduced%0Amodel%20is%20used%20in%20a%20Model%20Predictive%20Path%20Integral%20%28MPPI%29%20plan%20and%20control%0Aframework%20for%20performing%20trajectories%20in%20high-dynamic%20scenarios.%20The%20proposed%0Amodel%20and%20framework%20prove%20their%20ability%20to%20plan%20feasible%20trajectories%20ensuring%0Aan%20accurate%20vehicle%20behavior%20even%20at%20the%20limits%20of%20handling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04857v2&entry.124074799=Read"},
{"title": "Machine Unlearning for Medical Imaging", "author": "Reza Nasirigerdeh and Nader Razmi and Julia A. Schnabel and Daniel Rueckert and Georgios Kaissis", "abstract": "  Machine unlearning is the process of removing the impact of a particular set\nof training samples from a pretrained model. It aims to fulfill the \"right to\nbe forgotten\", which grants the individuals such as patients the right to\nreconsider their contribution in models including medical imaging models. In\nthis study, we evaluate the effectiveness (performance) and computational\nefficiency of different unlearning algorithms in medical imaging domain. Our\nevaluations demonstrate that the considered unlearning algorithms perform well\non the retain set (samples whose influence on the model is allowed to be\nretained) and forget set (samples whose contribution to the model should be\neliminated), and show no bias against male or female samples. They, however,\nadversely impact the generalization of the model, especially for larger forget\nset sizes. Moreover, they might be biased against easy or hard samples, and\nneed additional computational overhead for hyper-parameter tuning. In\nconclusion, machine unlearning seems promising for medical imaging, but the\nexisting unlearning algorithms still needs further improvements to become more\npractical for medical applications.\n", "link": "http://arxiv.org/abs/2407.07539v1", "date": "2024-07-10", "relevancy": 1.9753, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5419}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Unlearning%20for%20Medical%20Imaging&body=Title%3A%20Machine%20Unlearning%20for%20Medical%20Imaging%0AAuthor%3A%20Reza%20Nasirigerdeh%20and%20Nader%20Razmi%20and%20Julia%20A.%20Schnabel%20and%20Daniel%20Rueckert%20and%20Georgios%20Kaissis%0AAbstract%3A%20%20%20Machine%20unlearning%20is%20the%20process%20of%20removing%20the%20impact%20of%20a%20particular%20set%0Aof%20training%20samples%20from%20a%20pretrained%20model.%20It%20aims%20to%20fulfill%20the%20%22right%20to%0Abe%20forgotten%22%2C%20which%20grants%20the%20individuals%20such%20as%20patients%20the%20right%20to%0Areconsider%20their%20contribution%20in%20models%20including%20medical%20imaging%20models.%20In%0Athis%20study%2C%20we%20evaluate%20the%20effectiveness%20%28performance%29%20and%20computational%0Aefficiency%20of%20different%20unlearning%20algorithms%20in%20medical%20imaging%20domain.%20Our%0Aevaluations%20demonstrate%20that%20the%20considered%20unlearning%20algorithms%20perform%20well%0Aon%20the%20retain%20set%20%28samples%20whose%20influence%20on%20the%20model%20is%20allowed%20to%20be%0Aretained%29%20and%20forget%20set%20%28samples%20whose%20contribution%20to%20the%20model%20should%20be%0Aeliminated%29%2C%20and%20show%20no%20bias%20against%20male%20or%20female%20samples.%20They%2C%20however%2C%0Aadversely%20impact%20the%20generalization%20of%20the%20model%2C%20especially%20for%20larger%20forget%0Aset%20sizes.%20Moreover%2C%20they%20might%20be%20biased%20against%20easy%20or%20hard%20samples%2C%20and%0Aneed%20additional%20computational%20overhead%20for%20hyper-parameter%20tuning.%20In%0Aconclusion%2C%20machine%20unlearning%20seems%20promising%20for%20medical%20imaging%2C%20but%20the%0Aexisting%20unlearning%20algorithms%20still%20needs%20further%20improvements%20to%20become%20more%0Apractical%20for%20medical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Unlearning%2520for%2520Medical%2520Imaging%26entry.906535625%3DReza%2520Nasirigerdeh%2520and%2520Nader%2520Razmi%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Daniel%2520Rueckert%2520and%2520Georgios%2520Kaissis%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520is%2520the%2520process%2520of%2520removing%2520the%2520impact%2520of%2520a%2520particular%2520set%250Aof%2520training%2520samples%2520from%2520a%2520pretrained%2520model.%2520It%2520aims%2520to%2520fulfill%2520the%2520%2522right%2520to%250Abe%2520forgotten%2522%252C%2520which%2520grants%2520the%2520individuals%2520such%2520as%2520patients%2520the%2520right%2520to%250Areconsider%2520their%2520contribution%2520in%2520models%2520including%2520medical%2520imaging%2520models.%2520In%250Athis%2520study%252C%2520we%2520evaluate%2520the%2520effectiveness%2520%2528performance%2529%2520and%2520computational%250Aefficiency%2520of%2520different%2520unlearning%2520algorithms%2520in%2520medical%2520imaging%2520domain.%2520Our%250Aevaluations%2520demonstrate%2520that%2520the%2520considered%2520unlearning%2520algorithms%2520perform%2520well%250Aon%2520the%2520retain%2520set%2520%2528samples%2520whose%2520influence%2520on%2520the%2520model%2520is%2520allowed%2520to%2520be%250Aretained%2529%2520and%2520forget%2520set%2520%2528samples%2520whose%2520contribution%2520to%2520the%2520model%2520should%2520be%250Aeliminated%2529%252C%2520and%2520show%2520no%2520bias%2520against%2520male%2520or%2520female%2520samples.%2520They%252C%2520however%252C%250Aadversely%2520impact%2520the%2520generalization%2520of%2520the%2520model%252C%2520especially%2520for%2520larger%2520forget%250Aset%2520sizes.%2520Moreover%252C%2520they%2520might%2520be%2520biased%2520against%2520easy%2520or%2520hard%2520samples%252C%2520and%250Aneed%2520additional%2520computational%2520overhead%2520for%2520hyper-parameter%2520tuning.%2520In%250Aconclusion%252C%2520machine%2520unlearning%2520seems%2520promising%2520for%2520medical%2520imaging%252C%2520but%2520the%250Aexisting%2520unlearning%2520algorithms%2520still%2520needs%2520further%2520improvements%2520to%2520become%2520more%250Apractical%2520for%2520medical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Unlearning%20for%20Medical%20Imaging&entry.906535625=Reza%20Nasirigerdeh%20and%20Nader%20Razmi%20and%20Julia%20A.%20Schnabel%20and%20Daniel%20Rueckert%20and%20Georgios%20Kaissis&entry.1292438233=%20%20Machine%20unlearning%20is%20the%20process%20of%20removing%20the%20impact%20of%20a%20particular%20set%0Aof%20training%20samples%20from%20a%20pretrained%20model.%20It%20aims%20to%20fulfill%20the%20%22right%20to%0Abe%20forgotten%22%2C%20which%20grants%20the%20individuals%20such%20as%20patients%20the%20right%20to%0Areconsider%20their%20contribution%20in%20models%20including%20medical%20imaging%20models.%20In%0Athis%20study%2C%20we%20evaluate%20the%20effectiveness%20%28performance%29%20and%20computational%0Aefficiency%20of%20different%20unlearning%20algorithms%20in%20medical%20imaging%20domain.%20Our%0Aevaluations%20demonstrate%20that%20the%20considered%20unlearning%20algorithms%20perform%20well%0Aon%20the%20retain%20set%20%28samples%20whose%20influence%20on%20the%20model%20is%20allowed%20to%20be%0Aretained%29%20and%20forget%20set%20%28samples%20whose%20contribution%20to%20the%20model%20should%20be%0Aeliminated%29%2C%20and%20show%20no%20bias%20against%20male%20or%20female%20samples.%20They%2C%20however%2C%0Aadversely%20impact%20the%20generalization%20of%20the%20model%2C%20especially%20for%20larger%20forget%0Aset%20sizes.%20Moreover%2C%20they%20might%20be%20biased%20against%20easy%20or%20hard%20samples%2C%20and%0Aneed%20additional%20computational%20overhead%20for%20hyper-parameter%20tuning.%20In%0Aconclusion%2C%20machine%20unlearning%20seems%20promising%20for%20medical%20imaging%2C%20but%20the%0Aexisting%20unlearning%20algorithms%20still%20needs%20further%20improvements%20to%20become%20more%0Apractical%20for%20medical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07539v1&entry.124074799=Read"},
{"title": "Benchmarking Embedding Aggregation Methods in Computational Pathology: A\n  Clinical Data Perspective", "author": "Shengjia Chen and Gabriele Campanella and Abdulkadir Elmas and Aryeh Stock and Jennifer Zeng and Alexandros D. Polydorides and Adam J. Schoenfeld and Kuan-lin Huang and Jane Houldsworth and Chad Vanderbilt and Thomas J. Fuchs", "abstract": "  Recent advances in artificial intelligence (AI), in particular\nself-supervised learning of foundation models (FMs), are revolutionizing\nmedical imaging and computational pathology (CPath). A constant challenge in\nthe analysis of digital Whole Slide Images (WSIs) is the problem of aggregating\ntens of thousands of tile-level image embeddings to a slide-level\nrepresentation. Due to the prevalent use of datasets created for genomic\nresearch, such as TCGA, for method development, the performance of these\ntechniques on diagnostic slides from clinical practice has been inadequately\nexplored. This study conducts a thorough benchmarking analysis of ten\nslide-level aggregation techniques across nine clinically relevant tasks,\nincluding diagnostic assessment, biomarker classification, and outcome\nprediction. The results yield following key insights: (1) Embeddings derived\nfrom domain-specific (histological images) FMs outperform those from generic\nImageNet-based models across aggregation methods. (2) Spatial-aware aggregators\nenhance the performance significantly when using ImageNet pre-trained models\nbut not when using FMs. (3) No single model excels in all tasks and\nspatially-aware models do not show general superiority as it would be expected.\nThese findings underscore the need for more adaptable and universally\napplicable aggregation techniques, guiding future research towards tools that\nbetter meet the evolving needs of clinical-AI in pathology. The code used in\nthis work is available at\n\\url{https://github.com/fuchs-lab-public/CPath_SABenchmark}.\n", "link": "http://arxiv.org/abs/2407.07841v1", "date": "2024-07-10", "relevancy": 1.9745, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4983}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4907}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Embedding%20Aggregation%20Methods%20in%20Computational%20Pathology%3A%20A%0A%20%20Clinical%20Data%20Perspective&body=Title%3A%20Benchmarking%20Embedding%20Aggregation%20Methods%20in%20Computational%20Pathology%3A%20A%0A%20%20Clinical%20Data%20Perspective%0AAuthor%3A%20Shengjia%20Chen%20and%20Gabriele%20Campanella%20and%20Abdulkadir%20Elmas%20and%20Aryeh%20Stock%20and%20Jennifer%20Zeng%20and%20Alexandros%20D.%20Polydorides%20and%20Adam%20J.%20Schoenfeld%20and%20Kuan-lin%20Huang%20and%20Jane%20Houldsworth%20and%20Chad%20Vanderbilt%20and%20Thomas%20J.%20Fuchs%0AAbstract%3A%20%20%20Recent%20advances%20in%20artificial%20intelligence%20%28AI%29%2C%20in%20particular%0Aself-supervised%20learning%20of%20foundation%20models%20%28FMs%29%2C%20are%20revolutionizing%0Amedical%20imaging%20and%20computational%20pathology%20%28CPath%29.%20A%20constant%20challenge%20in%0Athe%20analysis%20of%20digital%20Whole%20Slide%20Images%20%28WSIs%29%20is%20the%20problem%20of%20aggregating%0Atens%20of%20thousands%20of%20tile-level%20image%20embeddings%20to%20a%20slide-level%0Arepresentation.%20Due%20to%20the%20prevalent%20use%20of%20datasets%20created%20for%20genomic%0Aresearch%2C%20such%20as%20TCGA%2C%20for%20method%20development%2C%20the%20performance%20of%20these%0Atechniques%20on%20diagnostic%20slides%20from%20clinical%20practice%20has%20been%20inadequately%0Aexplored.%20This%20study%20conducts%20a%20thorough%20benchmarking%20analysis%20of%20ten%0Aslide-level%20aggregation%20techniques%20across%20nine%20clinically%20relevant%20tasks%2C%0Aincluding%20diagnostic%20assessment%2C%20biomarker%20classification%2C%20and%20outcome%0Aprediction.%20The%20results%20yield%20following%20key%20insights%3A%20%281%29%20Embeddings%20derived%0Afrom%20domain-specific%20%28histological%20images%29%20FMs%20outperform%20those%20from%20generic%0AImageNet-based%20models%20across%20aggregation%20methods.%20%282%29%20Spatial-aware%20aggregators%0Aenhance%20the%20performance%20significantly%20when%20using%20ImageNet%20pre-trained%20models%0Abut%20not%20when%20using%20FMs.%20%283%29%20No%20single%20model%20excels%20in%20all%20tasks%20and%0Aspatially-aware%20models%20do%20not%20show%20general%20superiority%20as%20it%20would%20be%20expected.%0AThese%20findings%20underscore%20the%20need%20for%20more%20adaptable%20and%20universally%0Aapplicable%20aggregation%20techniques%2C%20guiding%20future%20research%20towards%20tools%20that%0Abetter%20meet%20the%20evolving%20needs%20of%20clinical-AI%20in%20pathology.%20The%20code%20used%20in%0Athis%20work%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/fuchs-lab-public/CPath_SABenchmark%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Embedding%2520Aggregation%2520Methods%2520in%2520Computational%2520Pathology%253A%2520A%250A%2520%2520Clinical%2520Data%2520Perspective%26entry.906535625%3DShengjia%2520Chen%2520and%2520Gabriele%2520Campanella%2520and%2520Abdulkadir%2520Elmas%2520and%2520Aryeh%2520Stock%2520and%2520Jennifer%2520Zeng%2520and%2520Alexandros%2520D.%2520Polydorides%2520and%2520Adam%2520J.%2520Schoenfeld%2520and%2520Kuan-lin%2520Huang%2520and%2520Jane%2520Houldsworth%2520and%2520Chad%2520Vanderbilt%2520and%2520Thomas%2520J.%2520Fuchs%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520in%2520particular%250Aself-supervised%2520learning%2520of%2520foundation%2520models%2520%2528FMs%2529%252C%2520are%2520revolutionizing%250Amedical%2520imaging%2520and%2520computational%2520pathology%2520%2528CPath%2529.%2520A%2520constant%2520challenge%2520in%250Athe%2520analysis%2520of%2520digital%2520Whole%2520Slide%2520Images%2520%2528WSIs%2529%2520is%2520the%2520problem%2520of%2520aggregating%250Atens%2520of%2520thousands%2520of%2520tile-level%2520image%2520embeddings%2520to%2520a%2520slide-level%250Arepresentation.%2520Due%2520to%2520the%2520prevalent%2520use%2520of%2520datasets%2520created%2520for%2520genomic%250Aresearch%252C%2520such%2520as%2520TCGA%252C%2520for%2520method%2520development%252C%2520the%2520performance%2520of%2520these%250Atechniques%2520on%2520diagnostic%2520slides%2520from%2520clinical%2520practice%2520has%2520been%2520inadequately%250Aexplored.%2520This%2520study%2520conducts%2520a%2520thorough%2520benchmarking%2520analysis%2520of%2520ten%250Aslide-level%2520aggregation%2520techniques%2520across%2520nine%2520clinically%2520relevant%2520tasks%252C%250Aincluding%2520diagnostic%2520assessment%252C%2520biomarker%2520classification%252C%2520and%2520outcome%250Aprediction.%2520The%2520results%2520yield%2520following%2520key%2520insights%253A%2520%25281%2529%2520Embeddings%2520derived%250Afrom%2520domain-specific%2520%2528histological%2520images%2529%2520FMs%2520outperform%2520those%2520from%2520generic%250AImageNet-based%2520models%2520across%2520aggregation%2520methods.%2520%25282%2529%2520Spatial-aware%2520aggregators%250Aenhance%2520the%2520performance%2520significantly%2520when%2520using%2520ImageNet%2520pre-trained%2520models%250Abut%2520not%2520when%2520using%2520FMs.%2520%25283%2529%2520No%2520single%2520model%2520excels%2520in%2520all%2520tasks%2520and%250Aspatially-aware%2520models%2520do%2520not%2520show%2520general%2520superiority%2520as%2520it%2520would%2520be%2520expected.%250AThese%2520findings%2520underscore%2520the%2520need%2520for%2520more%2520adaptable%2520and%2520universally%250Aapplicable%2520aggregation%2520techniques%252C%2520guiding%2520future%2520research%2520towards%2520tools%2520that%250Abetter%2520meet%2520the%2520evolving%2520needs%2520of%2520clinical-AI%2520in%2520pathology.%2520The%2520code%2520used%2520in%250Athis%2520work%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/fuchs-lab-public/CPath_SABenchmark%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Embedding%20Aggregation%20Methods%20in%20Computational%20Pathology%3A%20A%0A%20%20Clinical%20Data%20Perspective&entry.906535625=Shengjia%20Chen%20and%20Gabriele%20Campanella%20and%20Abdulkadir%20Elmas%20and%20Aryeh%20Stock%20and%20Jennifer%20Zeng%20and%20Alexandros%20D.%20Polydorides%20and%20Adam%20J.%20Schoenfeld%20and%20Kuan-lin%20Huang%20and%20Jane%20Houldsworth%20and%20Chad%20Vanderbilt%20and%20Thomas%20J.%20Fuchs&entry.1292438233=%20%20Recent%20advances%20in%20artificial%20intelligence%20%28AI%29%2C%20in%20particular%0Aself-supervised%20learning%20of%20foundation%20models%20%28FMs%29%2C%20are%20revolutionizing%0Amedical%20imaging%20and%20computational%20pathology%20%28CPath%29.%20A%20constant%20challenge%20in%0Athe%20analysis%20of%20digital%20Whole%20Slide%20Images%20%28WSIs%29%20is%20the%20problem%20of%20aggregating%0Atens%20of%20thousands%20of%20tile-level%20image%20embeddings%20to%20a%20slide-level%0Arepresentation.%20Due%20to%20the%20prevalent%20use%20of%20datasets%20created%20for%20genomic%0Aresearch%2C%20such%20as%20TCGA%2C%20for%20method%20development%2C%20the%20performance%20of%20these%0Atechniques%20on%20diagnostic%20slides%20from%20clinical%20practice%20has%20been%20inadequately%0Aexplored.%20This%20study%20conducts%20a%20thorough%20benchmarking%20analysis%20of%20ten%0Aslide-level%20aggregation%20techniques%20across%20nine%20clinically%20relevant%20tasks%2C%0Aincluding%20diagnostic%20assessment%2C%20biomarker%20classification%2C%20and%20outcome%0Aprediction.%20The%20results%20yield%20following%20key%20insights%3A%20%281%29%20Embeddings%20derived%0Afrom%20domain-specific%20%28histological%20images%29%20FMs%20outperform%20those%20from%20generic%0AImageNet-based%20models%20across%20aggregation%20methods.%20%282%29%20Spatial-aware%20aggregators%0Aenhance%20the%20performance%20significantly%20when%20using%20ImageNet%20pre-trained%20models%0Abut%20not%20when%20using%20FMs.%20%283%29%20No%20single%20model%20excels%20in%20all%20tasks%20and%0Aspatially-aware%20models%20do%20not%20show%20general%20superiority%20as%20it%20would%20be%20expected.%0AThese%20findings%20underscore%20the%20need%20for%20more%20adaptable%20and%20universally%0Aapplicable%20aggregation%20techniques%2C%20guiding%20future%20research%20towards%20tools%20that%0Abetter%20meet%20the%20evolving%20needs%20of%20clinical-AI%20in%20pathology.%20The%20code%20used%20in%0Athis%20work%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/fuchs-lab-public/CPath_SABenchmark%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07841v1&entry.124074799=Read"},
{"title": "Reinforcement Learning of Adaptive Acquisition Policies for Inverse\n  Problems", "author": "Gianluigi Silvestri and Fabio Valerio Massoli and Tribhuvanesh Orekondy and Afshin Abdi and Arash Behboodi", "abstract": "  A promising way to mitigate the expensive process of obtaining a\nhigh-dimensional signal is to acquire a limited number of low-dimensional\nmeasurements and solve an under-determined inverse problem by utilizing the\nstructural prior about the signal. In this paper, we focus on adaptive\nacquisition schemes to save further the number of measurements. To this end, we\npropose a reinforcement learning-based approach that sequentially collects\nmeasurements to better recover the underlying signal by acquiring fewer\nmeasurements. Our approach applies to general inverse problems with continuous\naction spaces and jointly learns the recovery algorithm. Using insights\nobtained from theoretical analysis, we also provide a probabilistic design for\nour methods using variational formulation. We evaluate our approach on multiple\ndatasets and with two measurement spaces (Gaussian, Radon). Our results confirm\nthe benefits of adaptive strategies in low-acquisition horizon settings.\n", "link": "http://arxiv.org/abs/2407.07794v1", "date": "2024-07-10", "relevancy": 1.9735, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20of%20Adaptive%20Acquisition%20Policies%20for%20Inverse%0A%20%20Problems&body=Title%3A%20Reinforcement%20Learning%20of%20Adaptive%20Acquisition%20Policies%20for%20Inverse%0A%20%20Problems%0AAuthor%3A%20Gianluigi%20Silvestri%20and%20Fabio%20Valerio%20Massoli%20and%20Tribhuvanesh%20Orekondy%20and%20Afshin%20Abdi%20and%20Arash%20Behboodi%0AAbstract%3A%20%20%20A%20promising%20way%20to%20mitigate%20the%20expensive%20process%20of%20obtaining%20a%0Ahigh-dimensional%20signal%20is%20to%20acquire%20a%20limited%20number%20of%20low-dimensional%0Ameasurements%20and%20solve%20an%20under-determined%20inverse%20problem%20by%20utilizing%20the%0Astructural%20prior%20about%20the%20signal.%20In%20this%20paper%2C%20we%20focus%20on%20adaptive%0Aacquisition%20schemes%20to%20save%20further%20the%20number%20of%20measurements.%20To%20this%20end%2C%20we%0Apropose%20a%20reinforcement%20learning-based%20approach%20that%20sequentially%20collects%0Ameasurements%20to%20better%20recover%20the%20underlying%20signal%20by%20acquiring%20fewer%0Ameasurements.%20Our%20approach%20applies%20to%20general%20inverse%20problems%20with%20continuous%0Aaction%20spaces%20and%20jointly%20learns%20the%20recovery%20algorithm.%20Using%20insights%0Aobtained%20from%20theoretical%20analysis%2C%20we%20also%20provide%20a%20probabilistic%20design%20for%0Aour%20methods%20using%20variational%20formulation.%20We%20evaluate%20our%20approach%20on%20multiple%0Adatasets%20and%20with%20two%20measurement%20spaces%20%28Gaussian%2C%20Radon%29.%20Our%20results%20confirm%0Athe%20benefits%20of%20adaptive%20strategies%20in%20low-acquisition%20horizon%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520of%2520Adaptive%2520Acquisition%2520Policies%2520for%2520Inverse%250A%2520%2520Problems%26entry.906535625%3DGianluigi%2520Silvestri%2520and%2520Fabio%2520Valerio%2520Massoli%2520and%2520Tribhuvanesh%2520Orekondy%2520and%2520Afshin%2520Abdi%2520and%2520Arash%2520Behboodi%26entry.1292438233%3D%2520%2520A%2520promising%2520way%2520to%2520mitigate%2520the%2520expensive%2520process%2520of%2520obtaining%2520a%250Ahigh-dimensional%2520signal%2520is%2520to%2520acquire%2520a%2520limited%2520number%2520of%2520low-dimensional%250Ameasurements%2520and%2520solve%2520an%2520under-determined%2520inverse%2520problem%2520by%2520utilizing%2520the%250Astructural%2520prior%2520about%2520the%2520signal.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520adaptive%250Aacquisition%2520schemes%2520to%2520save%2520further%2520the%2520number%2520of%2520measurements.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520reinforcement%2520learning-based%2520approach%2520that%2520sequentially%2520collects%250Ameasurements%2520to%2520better%2520recover%2520the%2520underlying%2520signal%2520by%2520acquiring%2520fewer%250Ameasurements.%2520Our%2520approach%2520applies%2520to%2520general%2520inverse%2520problems%2520with%2520continuous%250Aaction%2520spaces%2520and%2520jointly%2520learns%2520the%2520recovery%2520algorithm.%2520Using%2520insights%250Aobtained%2520from%2520theoretical%2520analysis%252C%2520we%2520also%2520provide%2520a%2520probabilistic%2520design%2520for%250Aour%2520methods%2520using%2520variational%2520formulation.%2520We%2520evaluate%2520our%2520approach%2520on%2520multiple%250Adatasets%2520and%2520with%2520two%2520measurement%2520spaces%2520%2528Gaussian%252C%2520Radon%2529.%2520Our%2520results%2520confirm%250Athe%2520benefits%2520of%2520adaptive%2520strategies%2520in%2520low-acquisition%2520horizon%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20of%20Adaptive%20Acquisition%20Policies%20for%20Inverse%0A%20%20Problems&entry.906535625=Gianluigi%20Silvestri%20and%20Fabio%20Valerio%20Massoli%20and%20Tribhuvanesh%20Orekondy%20and%20Afshin%20Abdi%20and%20Arash%20Behboodi&entry.1292438233=%20%20A%20promising%20way%20to%20mitigate%20the%20expensive%20process%20of%20obtaining%20a%0Ahigh-dimensional%20signal%20is%20to%20acquire%20a%20limited%20number%20of%20low-dimensional%0Ameasurements%20and%20solve%20an%20under-determined%20inverse%20problem%20by%20utilizing%20the%0Astructural%20prior%20about%20the%20signal.%20In%20this%20paper%2C%20we%20focus%20on%20adaptive%0Aacquisition%20schemes%20to%20save%20further%20the%20number%20of%20measurements.%20To%20this%20end%2C%20we%0Apropose%20a%20reinforcement%20learning-based%20approach%20that%20sequentially%20collects%0Ameasurements%20to%20better%20recover%20the%20underlying%20signal%20by%20acquiring%20fewer%0Ameasurements.%20Our%20approach%20applies%20to%20general%20inverse%20problems%20with%20continuous%0Aaction%20spaces%20and%20jointly%20learns%20the%20recovery%20algorithm.%20Using%20insights%0Aobtained%20from%20theoretical%20analysis%2C%20we%20also%20provide%20a%20probabilistic%20design%20for%0Aour%20methods%20using%20variational%20formulation.%20We%20evaluate%20our%20approach%20on%20multiple%0Adatasets%20and%20with%20two%20measurement%20spaces%20%28Gaussian%2C%20Radon%29.%20Our%20results%20confirm%0Athe%20benefits%20of%20adaptive%20strategies%20in%20low-acquisition%20horizon%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07794v1&entry.124074799=Read"},
{"title": "Training on the Test Task Confounds Evaluation and Emergence", "author": "Ricardo Dominguez-Olmedo and Florian E. Dorner and Moritz Hardt", "abstract": "  We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of techniques to\ninclude task-relevant data in the pretraining stage of a language model. We\ndemonstrate that training on the test task confounds both relative model\nevaluations and claims about emergent capabilities. We argue that the seeming\nsuperiority of one model family over another may be explained by a different\ndegree of training on the test task. To this end, we propose an effective\nmethod to adjust for training on the test task by fine-tuning each model under\ncomparison on the same task-relevant data before evaluation. We then show that\ninstances of emergent behavior largely vanish once we adjust for training on\nthe test task. This also applies to reported instances of emergent behavior\nthat cannot be explained by the choice of evaluation metric. Our work promotes\na new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities.\n", "link": "http://arxiv.org/abs/2407.07890v1", "date": "2024-07-10", "relevancy": 1.9694, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5019}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20on%20the%20Test%20Task%20Confounds%20Evaluation%20and%20Emergence&body=Title%3A%20Training%20on%20the%20Test%20Task%20Confounds%20Evaluation%20and%20Emergence%0AAuthor%3A%20Ricardo%20Dominguez-Olmedo%20and%20Florian%20E.%20Dorner%20and%20Moritz%20Hardt%0AAbstract%3A%20%20%20We%20study%20a%20fundamental%20problem%20in%20the%20evaluation%20of%20large%20language%20models%0Athat%20we%20call%20training%20on%20the%20test%20task.%20Unlike%20wrongful%20practices%20like%20training%0Aon%20the%20test%20data%2C%20leakage%2C%20or%20data%20contamination%2C%20training%20on%20the%20test%20task%20is%0Anot%20a%20malpractice.%20Rather%2C%20the%20term%20describes%20a%20growing%20set%20of%20techniques%20to%0Ainclude%20task-relevant%20data%20in%20the%20pretraining%20stage%20of%20a%20language%20model.%20We%0Ademonstrate%20that%20training%20on%20the%20test%20task%20confounds%20both%20relative%20model%0Aevaluations%20and%20claims%20about%20emergent%20capabilities.%20We%20argue%20that%20the%20seeming%0Asuperiority%20of%20one%20model%20family%20over%20another%20may%20be%20explained%20by%20a%20different%0Adegree%20of%20training%20on%20the%20test%20task.%20To%20this%20end%2C%20we%20propose%20an%20effective%0Amethod%20to%20adjust%20for%20training%20on%20the%20test%20task%20by%20fine-tuning%20each%20model%20under%0Acomparison%20on%20the%20same%20task-relevant%20data%20before%20evaluation.%20We%20then%20show%20that%0Ainstances%20of%20emergent%20behavior%20largely%20vanish%20once%20we%20adjust%20for%20training%20on%0Athe%20test%20task.%20This%20also%20applies%20to%20reported%20instances%20of%20emergent%20behavior%0Athat%20cannot%20be%20explained%20by%20the%20choice%20of%20evaluation%20metric.%20Our%20work%20promotes%0Aa%20new%20perspective%20on%20the%20evaluation%20of%20large%20language%20models%20with%20broad%0Aimplications%20for%20benchmarking%20and%20the%20study%20of%20emergent%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520on%2520the%2520Test%2520Task%2520Confounds%2520Evaluation%2520and%2520Emergence%26entry.906535625%3DRicardo%2520Dominguez-Olmedo%2520and%2520Florian%2520E.%2520Dorner%2520and%2520Moritz%2520Hardt%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520fundamental%2520problem%2520in%2520the%2520evaluation%2520of%2520large%2520language%2520models%250Athat%2520we%2520call%2520training%2520on%2520the%2520test%2520task.%2520Unlike%2520wrongful%2520practices%2520like%2520training%250Aon%2520the%2520test%2520data%252C%2520leakage%252C%2520or%2520data%2520contamination%252C%2520training%2520on%2520the%2520test%2520task%2520is%250Anot%2520a%2520malpractice.%2520Rather%252C%2520the%2520term%2520describes%2520a%2520growing%2520set%2520of%2520techniques%2520to%250Ainclude%2520task-relevant%2520data%2520in%2520the%2520pretraining%2520stage%2520of%2520a%2520language%2520model.%2520We%250Ademonstrate%2520that%2520training%2520on%2520the%2520test%2520task%2520confounds%2520both%2520relative%2520model%250Aevaluations%2520and%2520claims%2520about%2520emergent%2520capabilities.%2520We%2520argue%2520that%2520the%2520seeming%250Asuperiority%2520of%2520one%2520model%2520family%2520over%2520another%2520may%2520be%2520explained%2520by%2520a%2520different%250Adegree%2520of%2520training%2520on%2520the%2520test%2520task.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520effective%250Amethod%2520to%2520adjust%2520for%2520training%2520on%2520the%2520test%2520task%2520by%2520fine-tuning%2520each%2520model%2520under%250Acomparison%2520on%2520the%2520same%2520task-relevant%2520data%2520before%2520evaluation.%2520We%2520then%2520show%2520that%250Ainstances%2520of%2520emergent%2520behavior%2520largely%2520vanish%2520once%2520we%2520adjust%2520for%2520training%2520on%250Athe%2520test%2520task.%2520This%2520also%2520applies%2520to%2520reported%2520instances%2520of%2520emergent%2520behavior%250Athat%2520cannot%2520be%2520explained%2520by%2520the%2520choice%2520of%2520evaluation%2520metric.%2520Our%2520work%2520promotes%250Aa%2520new%2520perspective%2520on%2520the%2520evaluation%2520of%2520large%2520language%2520models%2520with%2520broad%250Aimplications%2520for%2520benchmarking%2520and%2520the%2520study%2520of%2520emergent%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20on%20the%20Test%20Task%20Confounds%20Evaluation%20and%20Emergence&entry.906535625=Ricardo%20Dominguez-Olmedo%20and%20Florian%20E.%20Dorner%20and%20Moritz%20Hardt&entry.1292438233=%20%20We%20study%20a%20fundamental%20problem%20in%20the%20evaluation%20of%20large%20language%20models%0Athat%20we%20call%20training%20on%20the%20test%20task.%20Unlike%20wrongful%20practices%20like%20training%0Aon%20the%20test%20data%2C%20leakage%2C%20or%20data%20contamination%2C%20training%20on%20the%20test%20task%20is%0Anot%20a%20malpractice.%20Rather%2C%20the%20term%20describes%20a%20growing%20set%20of%20techniques%20to%0Ainclude%20task-relevant%20data%20in%20the%20pretraining%20stage%20of%20a%20language%20model.%20We%0Ademonstrate%20that%20training%20on%20the%20test%20task%20confounds%20both%20relative%20model%0Aevaluations%20and%20claims%20about%20emergent%20capabilities.%20We%20argue%20that%20the%20seeming%0Asuperiority%20of%20one%20model%20family%20over%20another%20may%20be%20explained%20by%20a%20different%0Adegree%20of%20training%20on%20the%20test%20task.%20To%20this%20end%2C%20we%20propose%20an%20effective%0Amethod%20to%20adjust%20for%20training%20on%20the%20test%20task%20by%20fine-tuning%20each%20model%20under%0Acomparison%20on%20the%20same%20task-relevant%20data%20before%20evaluation.%20We%20then%20show%20that%0Ainstances%20of%20emergent%20behavior%20largely%20vanish%20once%20we%20adjust%20for%20training%20on%0Athe%20test%20task.%20This%20also%20applies%20to%20reported%20instances%20of%20emergent%20behavior%0Athat%20cannot%20be%20explained%20by%20the%20choice%20of%20evaluation%20metric.%20Our%20work%20promotes%0Aa%20new%20perspective%20on%20the%20evaluation%20of%20large%20language%20models%20with%20broad%0Aimplications%20for%20benchmarking%20and%20the%20study%20of%20emergent%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07890v1&entry.124074799=Read"},
{"title": "Generative AI for RF Sensing in IoT systems", "author": "Li Wang and Chao Zhang and Qiyang Zhao and Hang Zou and Samson Lasaulce and Giuseppe Valenzise and Zhuo He and Merouane Debbah", "abstract": "  The development of wireless sensing technologies, using signals such as\nWi-Fi, infrared, and RF to gather environmental data, has significantly\nadvanced within Internet of Things (IoT) systems. Among these, Radio Frequency\n(RF) sensing stands out for its cost-effective and non-intrusive monitoring of\nhuman activities and environmental changes. However, traditional RF sensing\nmethods face significant challenges, including noise, interference, incomplete\ndata, and high deployment costs, which limit their effectiveness and\nscalability. This paper investigates the potential of Generative AI (GenAI) to\novercome these limitations within the IoT ecosystem. We provide a comprehensive\nreview of state-of-the-art GenAI techniques, focusing on their application to\nRF sensing problems. By generating high-quality synthetic data, enhancing\nsignal quality, and integrating multi-modal data, GenAI offers robust solutions\nfor RF environment reconstruction, localization, and imaging. Additionally,\nGenAI's ability to generalize enables IoT devices to adapt to new environments\nand unseen tasks, improving their efficiency and performance. The main\ncontributions of this article include a detailed analysis of the challenges in\nRF sensing, the presentation of innovative GenAI-based solutions, and the\nproposal of a unified framework for diverse RF sensing tasks. Through case\nstudies, we demonstrate the effectiveness of integrating GenAI models, leading\nto advanced, scalable, and intelligent IoT systems.\n", "link": "http://arxiv.org/abs/2407.07506v1", "date": "2024-07-10", "relevancy": 1.9683, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5348}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4632}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20for%20RF%20Sensing%20in%20IoT%20systems&body=Title%3A%20Generative%20AI%20for%20RF%20Sensing%20in%20IoT%20systems%0AAuthor%3A%20Li%20Wang%20and%20Chao%20Zhang%20and%20Qiyang%20Zhao%20and%20Hang%20Zou%20and%20Samson%20Lasaulce%20and%20Giuseppe%20Valenzise%20and%20Zhuo%20He%20and%20Merouane%20Debbah%0AAbstract%3A%20%20%20The%20development%20of%20wireless%20sensing%20technologies%2C%20using%20signals%20such%20as%0AWi-Fi%2C%20infrared%2C%20and%20RF%20to%20gather%20environmental%20data%2C%20has%20significantly%0Aadvanced%20within%20Internet%20of%20Things%20%28IoT%29%20systems.%20Among%20these%2C%20Radio%20Frequency%0A%28RF%29%20sensing%20stands%20out%20for%20its%20cost-effective%20and%20non-intrusive%20monitoring%20of%0Ahuman%20activities%20and%20environmental%20changes.%20However%2C%20traditional%20RF%20sensing%0Amethods%20face%20significant%20challenges%2C%20including%20noise%2C%20interference%2C%20incomplete%0Adata%2C%20and%20high%20deployment%20costs%2C%20which%20limit%20their%20effectiveness%20and%0Ascalability.%20This%20paper%20investigates%20the%20potential%20of%20Generative%20AI%20%28GenAI%29%20to%0Aovercome%20these%20limitations%20within%20the%20IoT%20ecosystem.%20We%20provide%20a%20comprehensive%0Areview%20of%20state-of-the-art%20GenAI%20techniques%2C%20focusing%20on%20their%20application%20to%0ARF%20sensing%20problems.%20By%20generating%20high-quality%20synthetic%20data%2C%20enhancing%0Asignal%20quality%2C%20and%20integrating%20multi-modal%20data%2C%20GenAI%20offers%20robust%20solutions%0Afor%20RF%20environment%20reconstruction%2C%20localization%2C%20and%20imaging.%20Additionally%2C%0AGenAI%27s%20ability%20to%20generalize%20enables%20IoT%20devices%20to%20adapt%20to%20new%20environments%0Aand%20unseen%20tasks%2C%20improving%20their%20efficiency%20and%20performance.%20The%20main%0Acontributions%20of%20this%20article%20include%20a%20detailed%20analysis%20of%20the%20challenges%20in%0ARF%20sensing%2C%20the%20presentation%20of%20innovative%20GenAI-based%20solutions%2C%20and%20the%0Aproposal%20of%20a%20unified%20framework%20for%20diverse%20RF%20sensing%20tasks.%20Through%20case%0Astudies%2C%20we%20demonstrate%20the%20effectiveness%20of%20integrating%20GenAI%20models%2C%20leading%0Ato%20advanced%2C%20scalable%2C%20and%20intelligent%20IoT%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520for%2520RF%2520Sensing%2520in%2520IoT%2520systems%26entry.906535625%3DLi%2520Wang%2520and%2520Chao%2520Zhang%2520and%2520Qiyang%2520Zhao%2520and%2520Hang%2520Zou%2520and%2520Samson%2520Lasaulce%2520and%2520Giuseppe%2520Valenzise%2520and%2520Zhuo%2520He%2520and%2520Merouane%2520Debbah%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520wireless%2520sensing%2520technologies%252C%2520using%2520signals%2520such%2520as%250AWi-Fi%252C%2520infrared%252C%2520and%2520RF%2520to%2520gather%2520environmental%2520data%252C%2520has%2520significantly%250Aadvanced%2520within%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520systems.%2520Among%2520these%252C%2520Radio%2520Frequency%250A%2528RF%2529%2520sensing%2520stands%2520out%2520for%2520its%2520cost-effective%2520and%2520non-intrusive%2520monitoring%2520of%250Ahuman%2520activities%2520and%2520environmental%2520changes.%2520However%252C%2520traditional%2520RF%2520sensing%250Amethods%2520face%2520significant%2520challenges%252C%2520including%2520noise%252C%2520interference%252C%2520incomplete%250Adata%252C%2520and%2520high%2520deployment%2520costs%252C%2520which%2520limit%2520their%2520effectiveness%2520and%250Ascalability.%2520This%2520paper%2520investigates%2520the%2520potential%2520of%2520Generative%2520AI%2520%2528GenAI%2529%2520to%250Aovercome%2520these%2520limitations%2520within%2520the%2520IoT%2520ecosystem.%2520We%2520provide%2520a%2520comprehensive%250Areview%2520of%2520state-of-the-art%2520GenAI%2520techniques%252C%2520focusing%2520on%2520their%2520application%2520to%250ARF%2520sensing%2520problems.%2520By%2520generating%2520high-quality%2520synthetic%2520data%252C%2520enhancing%250Asignal%2520quality%252C%2520and%2520integrating%2520multi-modal%2520data%252C%2520GenAI%2520offers%2520robust%2520solutions%250Afor%2520RF%2520environment%2520reconstruction%252C%2520localization%252C%2520and%2520imaging.%2520Additionally%252C%250AGenAI%2527s%2520ability%2520to%2520generalize%2520enables%2520IoT%2520devices%2520to%2520adapt%2520to%2520new%2520environments%250Aand%2520unseen%2520tasks%252C%2520improving%2520their%2520efficiency%2520and%2520performance.%2520The%2520main%250Acontributions%2520of%2520this%2520article%2520include%2520a%2520detailed%2520analysis%2520of%2520the%2520challenges%2520in%250ARF%2520sensing%252C%2520the%2520presentation%2520of%2520innovative%2520GenAI-based%2520solutions%252C%2520and%2520the%250Aproposal%2520of%2520a%2520unified%2520framework%2520for%2520diverse%2520RF%2520sensing%2520tasks.%2520Through%2520case%250Astudies%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520integrating%2520GenAI%2520models%252C%2520leading%250Ato%2520advanced%252C%2520scalable%252C%2520and%2520intelligent%2520IoT%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20for%20RF%20Sensing%20in%20IoT%20systems&entry.906535625=Li%20Wang%20and%20Chao%20Zhang%20and%20Qiyang%20Zhao%20and%20Hang%20Zou%20and%20Samson%20Lasaulce%20and%20Giuseppe%20Valenzise%20and%20Zhuo%20He%20and%20Merouane%20Debbah&entry.1292438233=%20%20The%20development%20of%20wireless%20sensing%20technologies%2C%20using%20signals%20such%20as%0AWi-Fi%2C%20infrared%2C%20and%20RF%20to%20gather%20environmental%20data%2C%20has%20significantly%0Aadvanced%20within%20Internet%20of%20Things%20%28IoT%29%20systems.%20Among%20these%2C%20Radio%20Frequency%0A%28RF%29%20sensing%20stands%20out%20for%20its%20cost-effective%20and%20non-intrusive%20monitoring%20of%0Ahuman%20activities%20and%20environmental%20changes.%20However%2C%20traditional%20RF%20sensing%0Amethods%20face%20significant%20challenges%2C%20including%20noise%2C%20interference%2C%20incomplete%0Adata%2C%20and%20high%20deployment%20costs%2C%20which%20limit%20their%20effectiveness%20and%0Ascalability.%20This%20paper%20investigates%20the%20potential%20of%20Generative%20AI%20%28GenAI%29%20to%0Aovercome%20these%20limitations%20within%20the%20IoT%20ecosystem.%20We%20provide%20a%20comprehensive%0Areview%20of%20state-of-the-art%20GenAI%20techniques%2C%20focusing%20on%20their%20application%20to%0ARF%20sensing%20problems.%20By%20generating%20high-quality%20synthetic%20data%2C%20enhancing%0Asignal%20quality%2C%20and%20integrating%20multi-modal%20data%2C%20GenAI%20offers%20robust%20solutions%0Afor%20RF%20environment%20reconstruction%2C%20localization%2C%20and%20imaging.%20Additionally%2C%0AGenAI%27s%20ability%20to%20generalize%20enables%20IoT%20devices%20to%20adapt%20to%20new%20environments%0Aand%20unseen%20tasks%2C%20improving%20their%20efficiency%20and%20performance.%20The%20main%0Acontributions%20of%20this%20article%20include%20a%20detailed%20analysis%20of%20the%20challenges%20in%0ARF%20sensing%2C%20the%20presentation%20of%20innovative%20GenAI-based%20solutions%2C%20and%20the%0Aproposal%20of%20a%20unified%20framework%20for%20diverse%20RF%20sensing%20tasks.%20Through%20case%0Astudies%2C%20we%20demonstrate%20the%20effectiveness%20of%20integrating%20GenAI%20models%2C%20leading%0Ato%20advanced%2C%20scalable%2C%20and%20intelligent%20IoT%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07506v1&entry.124074799=Read"},
{"title": "Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model", "author": "Qi Song and Ziyuan Luo and Ka Chun Cheung and Simon See and Renjie Wan", "abstract": "  Neural Radiance Fields (NeRFs) have become a key method for 3D scene\nrepresentation. With the rising prominence and influence of NeRF, safeguarding\nits intellectual property has become increasingly important. In this paper, we\npropose \\textbf{NeRFProtector}, which adopts a plug-and-play strategy to\nprotect NeRF's copyright during its creation. NeRFProtector utilizes a\npre-trained watermarking base model, enabling NeRF creators to embed binary\nmessages directly while creating their NeRF. Our plug-and-play property ensures\nNeRF creators can flexibly choose NeRF variants without excessive\nmodifications. Leveraging our newly designed progressive distillation, we\ndemonstrate performance on par with several leading-edge neural rendering\nmethods. Our project is available at:\n\\url{https://qsong2001.github.io/NeRFProtector}.\n", "link": "http://arxiv.org/abs/2407.07735v1", "date": "2024-07-10", "relevancy": 1.9583, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5183}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4874}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protecting%20NeRFs%27%20Copyright%20via%20Plug-And-Play%20Watermarking%20Base%20Model&body=Title%3A%20Protecting%20NeRFs%27%20Copyright%20via%20Plug-And-Play%20Watermarking%20Base%20Model%0AAuthor%3A%20Qi%20Song%20and%20Ziyuan%20Luo%20and%20Ka%20Chun%20Cheung%20and%20Simon%20See%20and%20Renjie%20Wan%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20become%20a%20key%20method%20for%203D%20scene%0Arepresentation.%20With%20the%20rising%20prominence%20and%20influence%20of%20NeRF%2C%20safeguarding%0Aits%20intellectual%20property%20has%20become%20increasingly%20important.%20In%20this%20paper%2C%20we%0Apropose%20%5Ctextbf%7BNeRFProtector%7D%2C%20which%20adopts%20a%20plug-and-play%20strategy%20to%0Aprotect%20NeRF%27s%20copyright%20during%20its%20creation.%20NeRFProtector%20utilizes%20a%0Apre-trained%20watermarking%20base%20model%2C%20enabling%20NeRF%20creators%20to%20embed%20binary%0Amessages%20directly%20while%20creating%20their%20NeRF.%20Our%20plug-and-play%20property%20ensures%0ANeRF%20creators%20can%20flexibly%20choose%20NeRF%20variants%20without%20excessive%0Amodifications.%20Leveraging%20our%20newly%20designed%20progressive%20distillation%2C%20we%0Ademonstrate%20performance%20on%20par%20with%20several%20leading-edge%20neural%20rendering%0Amethods.%20Our%20project%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//qsong2001.github.io/NeRFProtector%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtecting%2520NeRFs%2527%2520Copyright%2520via%2520Plug-And-Play%2520Watermarking%2520Base%2520Model%26entry.906535625%3DQi%2520Song%2520and%2520Ziyuan%2520Luo%2520and%2520Ka%2520Chun%2520Cheung%2520and%2520Simon%2520See%2520and%2520Renjie%2520Wan%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520become%2520a%2520key%2520method%2520for%25203D%2520scene%250Arepresentation.%2520With%2520the%2520rising%2520prominence%2520and%2520influence%2520of%2520NeRF%252C%2520safeguarding%250Aits%2520intellectual%2520property%2520has%2520become%2520increasingly%2520important.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520%255Ctextbf%257BNeRFProtector%257D%252C%2520which%2520adopts%2520a%2520plug-and-play%2520strategy%2520to%250Aprotect%2520NeRF%2527s%2520copyright%2520during%2520its%2520creation.%2520NeRFProtector%2520utilizes%2520a%250Apre-trained%2520watermarking%2520base%2520model%252C%2520enabling%2520NeRF%2520creators%2520to%2520embed%2520binary%250Amessages%2520directly%2520while%2520creating%2520their%2520NeRF.%2520Our%2520plug-and-play%2520property%2520ensures%250ANeRF%2520creators%2520can%2520flexibly%2520choose%2520NeRF%2520variants%2520without%2520excessive%250Amodifications.%2520Leveraging%2520our%2520newly%2520designed%2520progressive%2520distillation%252C%2520we%250Ademonstrate%2520performance%2520on%2520par%2520with%2520several%2520leading-edge%2520neural%2520rendering%250Amethods.%2520Our%2520project%2520is%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//qsong2001.github.io/NeRFProtector%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protecting%20NeRFs%27%20Copyright%20via%20Plug-And-Play%20Watermarking%20Base%20Model&entry.906535625=Qi%20Song%20and%20Ziyuan%20Luo%20and%20Ka%20Chun%20Cheung%20and%20Simon%20See%20and%20Renjie%20Wan&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20become%20a%20key%20method%20for%203D%20scene%0Arepresentation.%20With%20the%20rising%20prominence%20and%20influence%20of%20NeRF%2C%20safeguarding%0Aits%20intellectual%20property%20has%20become%20increasingly%20important.%20In%20this%20paper%2C%20we%0Apropose%20%5Ctextbf%7BNeRFProtector%7D%2C%20which%20adopts%20a%20plug-and-play%20strategy%20to%0Aprotect%20NeRF%27s%20copyright%20during%20its%20creation.%20NeRFProtector%20utilizes%20a%0Apre-trained%20watermarking%20base%20model%2C%20enabling%20NeRF%20creators%20to%20embed%20binary%0Amessages%20directly%20while%20creating%20their%20NeRF.%20Our%20plug-and-play%20property%20ensures%0ANeRF%20creators%20can%20flexibly%20choose%20NeRF%20variants%20without%20excessive%0Amodifications.%20Leveraging%20our%20newly%20designed%20progressive%20distillation%2C%20we%0Ademonstrate%20performance%20on%20par%20with%20several%20leading-edge%20neural%20rendering%0Amethods.%20Our%20project%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//qsong2001.github.io/NeRFProtector%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07735v1&entry.124074799=Read"},
{"title": "Evaluating the Adversarial Robustness of Retrieval-Based In-Context\n  Learning for Large Language Models", "author": "Simon Chi Lok Yu and Jie He and Pasquale Minervini and Jeff Z. Pan", "abstract": "  With the emergence of large language models, such as LLaMA and OpenAI GPT-3,\nIn-Context Learning (ICL) gained significant attention due to its effectiveness\nand efficiency. However, ICL is very sensitive to the choice, order, and\nverbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented\nICL methods try to address this problem by leveraging retrievers to extract\nsemantically related examples as demonstrations. While this approach yields\nmore accurate results, its robustness against various types of adversarial\nattacks, including perturbations on test samples, demonstrations, and retrieved\ndata, remains under-explored. Our study reveals that retrieval-augmented models\ncan enhance robustness against test sample attacks, outperforming vanilla ICL\nwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit\noverconfidence in the demonstrations, leading to a 2% increase in ASR for\ndemonstration attacks. Adversarial training can help improve the robustness of\nICL methods to adversarial attacks; however, such a training scheme can be too\ncostly in the context of LLMs. As an alternative, we introduce an effective\ntraining-free adversarial defence method, DARD, which enriches the example pool\nwith those attacked samples. We show that DARD yields improvements in\nperformance and robustness, achieving a 15% reduction in ASR over the\nbaselines. Code and data are released to encourage further research:\nhttps://github.com/simonucl/adv-retreival-icl\n", "link": "http://arxiv.org/abs/2405.15984v2", "date": "2024-07-10", "relevancy": 1.9509, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4922}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Adversarial%20Robustness%20of%20Retrieval-Based%20In-Context%0A%20%20Learning%20for%20Large%20Language%20Models&body=Title%3A%20Evaluating%20the%20Adversarial%20Robustness%20of%20Retrieval-Based%20In-Context%0A%20%20Learning%20for%20Large%20Language%20Models%0AAuthor%3A%20Simon%20Chi%20Lok%20Yu%20and%20Jie%20He%20and%20Pasquale%20Minervini%20and%20Jeff%20Z.%20Pan%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20large%20language%20models%2C%20such%20as%20LLaMA%20and%20OpenAI%20GPT-3%2C%0AIn-Context%20Learning%20%28ICL%29%20gained%20significant%20attention%20due%20to%20its%20effectiveness%0Aand%20efficiency.%20However%2C%20ICL%20is%20very%20sensitive%20to%20the%20choice%2C%20order%2C%20and%0Averbaliser%20used%20to%20encode%20the%20demonstrations%20in%20the%20prompt.%20Retrieval-Augmented%0AICL%20methods%20try%20to%20address%20this%20problem%20by%20leveraging%20retrievers%20to%20extract%0Asemantically%20related%20examples%20as%20demonstrations.%20While%20this%20approach%20yields%0Amore%20accurate%20results%2C%20its%20robustness%20against%20various%20types%20of%20adversarial%0Aattacks%2C%20including%20perturbations%20on%20test%20samples%2C%20demonstrations%2C%20and%20retrieved%0Adata%2C%20remains%20under-explored.%20Our%20study%20reveals%20that%20retrieval-augmented%20models%0Acan%20enhance%20robustness%20against%20test%20sample%20attacks%2C%20outperforming%20vanilla%20ICL%0Awith%20a%204.87%25%20reduction%20in%20Attack%20Success%20Rate%20%28ASR%29%3B%20however%2C%20they%20exhibit%0Aoverconfidence%20in%20the%20demonstrations%2C%20leading%20to%20a%202%25%20increase%20in%20ASR%20for%0Ademonstration%20attacks.%20Adversarial%20training%20can%20help%20improve%20the%20robustness%20of%0AICL%20methods%20to%20adversarial%20attacks%3B%20however%2C%20such%20a%20training%20scheme%20can%20be%20too%0Acostly%20in%20the%20context%20of%20LLMs.%20As%20an%20alternative%2C%20we%20introduce%20an%20effective%0Atraining-free%20adversarial%20defence%20method%2C%20DARD%2C%20which%20enriches%20the%20example%20pool%0Awith%20those%20attacked%20samples.%20We%20show%20that%20DARD%20yields%20improvements%20in%0Aperformance%20and%20robustness%2C%20achieving%20a%2015%25%20reduction%20in%20ASR%20over%20the%0Abaselines.%20Code%20and%20data%20are%20released%20to%20encourage%20further%20research%3A%0Ahttps%3A//github.com/simonucl/adv-retreival-icl%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15984v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Adversarial%2520Robustness%2520of%2520Retrieval-Based%2520In-Context%250A%2520%2520Learning%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DSimon%2520Chi%2520Lok%2520Yu%2520and%2520Jie%2520He%2520and%2520Pasquale%2520Minervini%2520and%2520Jeff%2520Z.%2520Pan%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520large%2520language%2520models%252C%2520such%2520as%2520LLaMA%2520and%2520OpenAI%2520GPT-3%252C%250AIn-Context%2520Learning%2520%2528ICL%2529%2520gained%2520significant%2520attention%2520due%2520to%2520its%2520effectiveness%250Aand%2520efficiency.%2520However%252C%2520ICL%2520is%2520very%2520sensitive%2520to%2520the%2520choice%252C%2520order%252C%2520and%250Averbaliser%2520used%2520to%2520encode%2520the%2520demonstrations%2520in%2520the%2520prompt.%2520Retrieval-Augmented%250AICL%2520methods%2520try%2520to%2520address%2520this%2520problem%2520by%2520leveraging%2520retrievers%2520to%2520extract%250Asemantically%2520related%2520examples%2520as%2520demonstrations.%2520While%2520this%2520approach%2520yields%250Amore%2520accurate%2520results%252C%2520its%2520robustness%2520against%2520various%2520types%2520of%2520adversarial%250Aattacks%252C%2520including%2520perturbations%2520on%2520test%2520samples%252C%2520demonstrations%252C%2520and%2520retrieved%250Adata%252C%2520remains%2520under-explored.%2520Our%2520study%2520reveals%2520that%2520retrieval-augmented%2520models%250Acan%2520enhance%2520robustness%2520against%2520test%2520sample%2520attacks%252C%2520outperforming%2520vanilla%2520ICL%250Awith%2520a%25204.87%2525%2520reduction%2520in%2520Attack%2520Success%2520Rate%2520%2528ASR%2529%253B%2520however%252C%2520they%2520exhibit%250Aoverconfidence%2520in%2520the%2520demonstrations%252C%2520leading%2520to%2520a%25202%2525%2520increase%2520in%2520ASR%2520for%250Ademonstration%2520attacks.%2520Adversarial%2520training%2520can%2520help%2520improve%2520the%2520robustness%2520of%250AICL%2520methods%2520to%2520adversarial%2520attacks%253B%2520however%252C%2520such%2520a%2520training%2520scheme%2520can%2520be%2520too%250Acostly%2520in%2520the%2520context%2520of%2520LLMs.%2520As%2520an%2520alternative%252C%2520we%2520introduce%2520an%2520effective%250Atraining-free%2520adversarial%2520defence%2520method%252C%2520DARD%252C%2520which%2520enriches%2520the%2520example%2520pool%250Awith%2520those%2520attacked%2520samples.%2520We%2520show%2520that%2520DARD%2520yields%2520improvements%2520in%250Aperformance%2520and%2520robustness%252C%2520achieving%2520a%252015%2525%2520reduction%2520in%2520ASR%2520over%2520the%250Abaselines.%2520Code%2520and%2520data%2520are%2520released%2520to%2520encourage%2520further%2520research%253A%250Ahttps%253A//github.com/simonucl/adv-retreival-icl%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15984v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Adversarial%20Robustness%20of%20Retrieval-Based%20In-Context%0A%20%20Learning%20for%20Large%20Language%20Models&entry.906535625=Simon%20Chi%20Lok%20Yu%20and%20Jie%20He%20and%20Pasquale%20Minervini%20and%20Jeff%20Z.%20Pan&entry.1292438233=%20%20With%20the%20emergence%20of%20large%20language%20models%2C%20such%20as%20LLaMA%20and%20OpenAI%20GPT-3%2C%0AIn-Context%20Learning%20%28ICL%29%20gained%20significant%20attention%20due%20to%20its%20effectiveness%0Aand%20efficiency.%20However%2C%20ICL%20is%20very%20sensitive%20to%20the%20choice%2C%20order%2C%20and%0Averbaliser%20used%20to%20encode%20the%20demonstrations%20in%20the%20prompt.%20Retrieval-Augmented%0AICL%20methods%20try%20to%20address%20this%20problem%20by%20leveraging%20retrievers%20to%20extract%0Asemantically%20related%20examples%20as%20demonstrations.%20While%20this%20approach%20yields%0Amore%20accurate%20results%2C%20its%20robustness%20against%20various%20types%20of%20adversarial%0Aattacks%2C%20including%20perturbations%20on%20test%20samples%2C%20demonstrations%2C%20and%20retrieved%0Adata%2C%20remains%20under-explored.%20Our%20study%20reveals%20that%20retrieval-augmented%20models%0Acan%20enhance%20robustness%20against%20test%20sample%20attacks%2C%20outperforming%20vanilla%20ICL%0Awith%20a%204.87%25%20reduction%20in%20Attack%20Success%20Rate%20%28ASR%29%3B%20however%2C%20they%20exhibit%0Aoverconfidence%20in%20the%20demonstrations%2C%20leading%20to%20a%202%25%20increase%20in%20ASR%20for%0Ademonstration%20attacks.%20Adversarial%20training%20can%20help%20improve%20the%20robustness%20of%0AICL%20methods%20to%20adversarial%20attacks%3B%20however%2C%20such%20a%20training%20scheme%20can%20be%20too%0Acostly%20in%20the%20context%20of%20LLMs.%20As%20an%20alternative%2C%20we%20introduce%20an%20effective%0Atraining-free%20adversarial%20defence%20method%2C%20DARD%2C%20which%20enriches%20the%20example%20pool%0Awith%20those%20attacked%20samples.%20We%20show%20that%20DARD%20yields%20improvements%20in%0Aperformance%20and%20robustness%2C%20achieving%20a%2015%25%20reduction%20in%20ASR%20over%20the%0Abaselines.%20Code%20and%20data%20are%20released%20to%20encourage%20further%20research%3A%0Ahttps%3A//github.com/simonucl/adv-retreival-icl%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15984v2&entry.124074799=Read"},
{"title": "Targeted Augmented Data for Audio Deepfake Detection", "author": "Marcella Astrid and Enjie Ghorbel and Djamila Aouada", "abstract": "  The availability of highly convincing audio deepfake generators highlights\nthe need for designing robust audio deepfake detectors. Existing works often\nrely solely on real and fake data available in the training set, which may lead\nto overfitting, thereby reducing the robustness to unseen manipulations. To\nenhance the generalization capabilities of audio deepfake detectors, we propose\na novel augmentation method for generating audio pseudo-fakes targeting the\ndecision boundary of the model. Inspired by adversarial attacks, we perturb\noriginal real data to synthesize pseudo-fakes with ambiguous prediction\nprobabilities. Comprehensive experiments on two well-known architectures\ndemonstrate that the proposed augmentation contributes to improving the\ngeneralization capabilities of these architectures.\n", "link": "http://arxiv.org/abs/2407.07598v1", "date": "2024-07-10", "relevancy": 1.9467, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4917}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4905}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Targeted%20Augmented%20Data%20for%20Audio%20Deepfake%20Detection&body=Title%3A%20Targeted%20Augmented%20Data%20for%20Audio%20Deepfake%20Detection%0AAuthor%3A%20Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20The%20availability%20of%20highly%20convincing%20audio%20deepfake%20generators%20highlights%0Athe%20need%20for%20designing%20robust%20audio%20deepfake%20detectors.%20Existing%20works%20often%0Arely%20solely%20on%20real%20and%20fake%20data%20available%20in%20the%20training%20set%2C%20which%20may%20lead%0Ato%20overfitting%2C%20thereby%20reducing%20the%20robustness%20to%20unseen%20manipulations.%20To%0Aenhance%20the%20generalization%20capabilities%20of%20audio%20deepfake%20detectors%2C%20we%20propose%0Aa%20novel%20augmentation%20method%20for%20generating%20audio%20pseudo-fakes%20targeting%20the%0Adecision%20boundary%20of%20the%20model.%20Inspired%20by%20adversarial%20attacks%2C%20we%20perturb%0Aoriginal%20real%20data%20to%20synthesize%20pseudo-fakes%20with%20ambiguous%20prediction%0Aprobabilities.%20Comprehensive%20experiments%20on%20two%20well-known%20architectures%0Ademonstrate%20that%20the%20proposed%20augmentation%20contributes%20to%20improving%20the%0Ageneralization%20capabilities%20of%20these%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTargeted%2520Augmented%2520Data%2520for%2520Audio%2520Deepfake%2520Detection%26entry.906535625%3DMarcella%2520Astrid%2520and%2520Enjie%2520Ghorbel%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520The%2520availability%2520of%2520highly%2520convincing%2520audio%2520deepfake%2520generators%2520highlights%250Athe%2520need%2520for%2520designing%2520robust%2520audio%2520deepfake%2520detectors.%2520Existing%2520works%2520often%250Arely%2520solely%2520on%2520real%2520and%2520fake%2520data%2520available%2520in%2520the%2520training%2520set%252C%2520which%2520may%2520lead%250Ato%2520overfitting%252C%2520thereby%2520reducing%2520the%2520robustness%2520to%2520unseen%2520manipulations.%2520To%250Aenhance%2520the%2520generalization%2520capabilities%2520of%2520audio%2520deepfake%2520detectors%252C%2520we%2520propose%250Aa%2520novel%2520augmentation%2520method%2520for%2520generating%2520audio%2520pseudo-fakes%2520targeting%2520the%250Adecision%2520boundary%2520of%2520the%2520model.%2520Inspired%2520by%2520adversarial%2520attacks%252C%2520we%2520perturb%250Aoriginal%2520real%2520data%2520to%2520synthesize%2520pseudo-fakes%2520with%2520ambiguous%2520prediction%250Aprobabilities.%2520Comprehensive%2520experiments%2520on%2520two%2520well-known%2520architectures%250Ademonstrate%2520that%2520the%2520proposed%2520augmentation%2520contributes%2520to%2520improving%2520the%250Ageneralization%2520capabilities%2520of%2520these%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Targeted%20Augmented%20Data%20for%20Audio%20Deepfake%20Detection&entry.906535625=Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada&entry.1292438233=%20%20The%20availability%20of%20highly%20convincing%20audio%20deepfake%20generators%20highlights%0Athe%20need%20for%20designing%20robust%20audio%20deepfake%20detectors.%20Existing%20works%20often%0Arely%20solely%20on%20real%20and%20fake%20data%20available%20in%20the%20training%20set%2C%20which%20may%20lead%0Ato%20overfitting%2C%20thereby%20reducing%20the%20robustness%20to%20unseen%20manipulations.%20To%0Aenhance%20the%20generalization%20capabilities%20of%20audio%20deepfake%20detectors%2C%20we%20propose%0Aa%20novel%20augmentation%20method%20for%20generating%20audio%20pseudo-fakes%20targeting%20the%0Adecision%20boundary%20of%20the%20model.%20Inspired%20by%20adversarial%20attacks%2C%20we%20perturb%0Aoriginal%20real%20data%20to%20synthesize%20pseudo-fakes%20with%20ambiguous%20prediction%0Aprobabilities.%20Comprehensive%20experiments%20on%20two%20well-known%20architectures%0Ademonstrate%20that%20the%20proposed%20augmentation%20contributes%20to%20improving%20the%0Ageneralization%20capabilities%20of%20these%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07598v1&entry.124074799=Read"},
{"title": "SecureReg: Combining NLP and MLP for Enhanced Detection of Malicious\n  Domain Name Registrations", "author": "Furkan \u00c7olhak and Mert \u0130lhan Ecevit and Hasan Da\u011f and Reiner Creutzburg", "abstract": "  The escalating landscape of cyber threats, characterized by the registration\nof thousands of new domains daily for large-scale Internet attacks such as\nspam, phishing, and drive-by downloads, underscores the imperative for\ninnovative detection methodologies. This paper introduces a cutting-edge\napproach for identifying suspicious domains at the onset of the registration\nprocess. The accompanying data pipeline generates crucial features by comparing\nnew domains to registered domains, emphasizing the crucial similarity score.\nThe proposed system analyzes semantic and numerical attributes by leveraging a\nnovel combination of Natural Language Processing (NLP) techniques, including a\npretrained CANINE model and Multilayer Perceptron (MLP) models, providing a\nrobust solution for early threat detection. This integrated Pretrained NLP\n(CANINE) + MLP model showcases the outstanding performance, surpassing both\nindividual pretrained NLP models and standalone MLP models. With an F1 score of\n84.86\\% and an accuracy of 84.95\\% on the SecureReg dataset, it effectively\ndetects malicious domain registrations. The findings demonstrate the\neffectiveness of the integrated approach and contribute to the ongoing efforts\nto develop proactive strategies to mitigate the risks associated with illicit\nonline activities through the early identification of suspicious domain\nregistrations.\n", "link": "http://arxiv.org/abs/2401.03196v3", "date": "2024-07-10", "relevancy": 1.3445, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4549}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4454}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SecureReg%3A%20Combining%20NLP%20and%20MLP%20for%20Enhanced%20Detection%20of%20Malicious%0A%20%20Domain%20Name%20Registrations&body=Title%3A%20SecureReg%3A%20Combining%20NLP%20and%20MLP%20for%20Enhanced%20Detection%20of%20Malicious%0A%20%20Domain%20Name%20Registrations%0AAuthor%3A%20Furkan%20%C3%87olhak%20and%20Mert%20%C4%B0lhan%20Ecevit%20and%20Hasan%20Da%C4%9F%20and%20Reiner%20Creutzburg%0AAbstract%3A%20%20%20The%20escalating%20landscape%20of%20cyber%20threats%2C%20characterized%20by%20the%20registration%0Aof%20thousands%20of%20new%20domains%20daily%20for%20large-scale%20Internet%20attacks%20such%20as%0Aspam%2C%20phishing%2C%20and%20drive-by%20downloads%2C%20underscores%20the%20imperative%20for%0Ainnovative%20detection%20methodologies.%20This%20paper%20introduces%20a%20cutting-edge%0Aapproach%20for%20identifying%20suspicious%20domains%20at%20the%20onset%20of%20the%20registration%0Aprocess.%20The%20accompanying%20data%20pipeline%20generates%20crucial%20features%20by%20comparing%0Anew%20domains%20to%20registered%20domains%2C%20emphasizing%20the%20crucial%20similarity%20score.%0AThe%20proposed%20system%20analyzes%20semantic%20and%20numerical%20attributes%20by%20leveraging%20a%0Anovel%20combination%20of%20Natural%20Language%20Processing%20%28NLP%29%20techniques%2C%20including%20a%0Apretrained%20CANINE%20model%20and%20Multilayer%20Perceptron%20%28MLP%29%20models%2C%20providing%20a%0Arobust%20solution%20for%20early%20threat%20detection.%20This%20integrated%20Pretrained%20NLP%0A%28CANINE%29%20%2B%20MLP%20model%20showcases%20the%20outstanding%20performance%2C%20surpassing%20both%0Aindividual%20pretrained%20NLP%20models%20and%20standalone%20MLP%20models.%20With%20an%20F1%20score%20of%0A84.86%5C%25%20and%20an%20accuracy%20of%2084.95%5C%25%20on%20the%20SecureReg%20dataset%2C%20it%20effectively%0Adetects%20malicious%20domain%20registrations.%20The%20findings%20demonstrate%20the%0Aeffectiveness%20of%20the%20integrated%20approach%20and%20contribute%20to%20the%20ongoing%20efforts%0Ato%20develop%20proactive%20strategies%20to%20mitigate%20the%20risks%20associated%20with%20illicit%0Aonline%20activities%20through%20the%20early%20identification%20of%20suspicious%20domain%0Aregistrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03196v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecureReg%253A%2520Combining%2520NLP%2520and%2520MLP%2520for%2520Enhanced%2520Detection%2520of%2520Malicious%250A%2520%2520Domain%2520Name%2520Registrations%26entry.906535625%3DFurkan%2520%25C3%2587olhak%2520and%2520Mert%2520%25C4%25B0lhan%2520Ecevit%2520and%2520Hasan%2520Da%25C4%259F%2520and%2520Reiner%2520Creutzburg%26entry.1292438233%3D%2520%2520The%2520escalating%2520landscape%2520of%2520cyber%2520threats%252C%2520characterized%2520by%2520the%2520registration%250Aof%2520thousands%2520of%2520new%2520domains%2520daily%2520for%2520large-scale%2520Internet%2520attacks%2520such%2520as%250Aspam%252C%2520phishing%252C%2520and%2520drive-by%2520downloads%252C%2520underscores%2520the%2520imperative%2520for%250Ainnovative%2520detection%2520methodologies.%2520This%2520paper%2520introduces%2520a%2520cutting-edge%250Aapproach%2520for%2520identifying%2520suspicious%2520domains%2520at%2520the%2520onset%2520of%2520the%2520registration%250Aprocess.%2520The%2520accompanying%2520data%2520pipeline%2520generates%2520crucial%2520features%2520by%2520comparing%250Anew%2520domains%2520to%2520registered%2520domains%252C%2520emphasizing%2520the%2520crucial%2520similarity%2520score.%250AThe%2520proposed%2520system%2520analyzes%2520semantic%2520and%2520numerical%2520attributes%2520by%2520leveraging%2520a%250Anovel%2520combination%2520of%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520techniques%252C%2520including%2520a%250Apretrained%2520CANINE%2520model%2520and%2520Multilayer%2520Perceptron%2520%2528MLP%2529%2520models%252C%2520providing%2520a%250Arobust%2520solution%2520for%2520early%2520threat%2520detection.%2520This%2520integrated%2520Pretrained%2520NLP%250A%2528CANINE%2529%2520%252B%2520MLP%2520model%2520showcases%2520the%2520outstanding%2520performance%252C%2520surpassing%2520both%250Aindividual%2520pretrained%2520NLP%2520models%2520and%2520standalone%2520MLP%2520models.%2520With%2520an%2520F1%2520score%2520of%250A84.86%255C%2525%2520and%2520an%2520accuracy%2520of%252084.95%255C%2525%2520on%2520the%2520SecureReg%2520dataset%252C%2520it%2520effectively%250Adetects%2520malicious%2520domain%2520registrations.%2520The%2520findings%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520integrated%2520approach%2520and%2520contribute%2520to%2520the%2520ongoing%2520efforts%250Ato%2520develop%2520proactive%2520strategies%2520to%2520mitigate%2520the%2520risks%2520associated%2520with%2520illicit%250Aonline%2520activities%2520through%2520the%2520early%2520identification%2520of%2520suspicious%2520domain%250Aregistrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03196v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SecureReg%3A%20Combining%20NLP%20and%20MLP%20for%20Enhanced%20Detection%20of%20Malicious%0A%20%20Domain%20Name%20Registrations&entry.906535625=Furkan%20%C3%87olhak%20and%20Mert%20%C4%B0lhan%20Ecevit%20and%20Hasan%20Da%C4%9F%20and%20Reiner%20Creutzburg&entry.1292438233=%20%20The%20escalating%20landscape%20of%20cyber%20threats%2C%20characterized%20by%20the%20registration%0Aof%20thousands%20of%20new%20domains%20daily%20for%20large-scale%20Internet%20attacks%20such%20as%0Aspam%2C%20phishing%2C%20and%20drive-by%20downloads%2C%20underscores%20the%20imperative%20for%0Ainnovative%20detection%20methodologies.%20This%20paper%20introduces%20a%20cutting-edge%0Aapproach%20for%20identifying%20suspicious%20domains%20at%20the%20onset%20of%20the%20registration%0Aprocess.%20The%20accompanying%20data%20pipeline%20generates%20crucial%20features%20by%20comparing%0Anew%20domains%20to%20registered%20domains%2C%20emphasizing%20the%20crucial%20similarity%20score.%0AThe%20proposed%20system%20analyzes%20semantic%20and%20numerical%20attributes%20by%20leveraging%20a%0Anovel%20combination%20of%20Natural%20Language%20Processing%20%28NLP%29%20techniques%2C%20including%20a%0Apretrained%20CANINE%20model%20and%20Multilayer%20Perceptron%20%28MLP%29%20models%2C%20providing%20a%0Arobust%20solution%20for%20early%20threat%20detection.%20This%20integrated%20Pretrained%20NLP%0A%28CANINE%29%20%2B%20MLP%20model%20showcases%20the%20outstanding%20performance%2C%20surpassing%20both%0Aindividual%20pretrained%20NLP%20models%20and%20standalone%20MLP%20models.%20With%20an%20F1%20score%20of%0A84.86%5C%25%20and%20an%20accuracy%20of%2084.95%5C%25%20on%20the%20SecureReg%20dataset%2C%20it%20effectively%0Adetects%20malicious%20domain%20registrations.%20The%20findings%20demonstrate%20the%0Aeffectiveness%20of%20the%20integrated%20approach%20and%20contribute%20to%20the%20ongoing%20efforts%0Ato%20develop%20proactive%20strategies%20to%20mitigate%20the%20risks%20associated%20with%20illicit%0Aonline%20activities%20through%20the%20early%20identification%20of%20suspicious%20domain%0Aregistrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03196v3&entry.124074799=Read"},
{"title": "Feasibility Study on Active Learning of Smart Surrogates for Scientific\n  Simulations", "author": "Pradeep Bajracharya and Javier Quetzalc\u00f3atl Toledo-Mar\u00edn and Geoffrey Fox and Shantenu Jha and Linwei Wang", "abstract": "  High-performance scientific simulations, important for comprehension of\ncomplex systems, encounter computational challenges especially when exploring\nextensive parameter spaces. There has been an increasing interest in developing\ndeep neural networks (DNNs) as surrogate models capable of accelerating the\nsimulations. However, existing approaches for training these DNN surrogates\nrely on extensive simulation data which are heuristically selected and\ngenerated with expensive computation -- a challenge under-explored in the\nliterature. In this paper, we investigate the potential of incorporating active\nlearning into DNN surrogate training. This allows intelligent and objective\nselection of training simulations, reducing the need to generate extensive\nsimulation data as well as the dependency of the performance of DNN surrogates\non pre-defined training simulations. In the problem context of constructing DNN\nsurrogates for diffusion equations with sources, we examine the efficacy of\ndiversity- and uncertainty-based strategies for selecting training simulations,\nconsidering two different DNN architecture. The results set the groundwork for\ndeveloping the high-performance computing infrastructure for Smart Surrogates\nthat supports on-the-fly generation of simulation data steered by active\nlearning strategies to potentially improve the efficiency of scientific\nsimulations.\n", "link": "http://arxiv.org/abs/2407.07674v1", "date": "2024-07-10", "relevancy": 1.0046, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5181}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feasibility%20Study%20on%20Active%20Learning%20of%20Smart%20Surrogates%20for%20Scientific%0A%20%20Simulations&body=Title%3A%20Feasibility%20Study%20on%20Active%20Learning%20of%20Smart%20Surrogates%20for%20Scientific%0A%20%20Simulations%0AAuthor%3A%20Pradeep%20Bajracharya%20and%20Javier%20Quetzalc%C3%B3atl%20Toledo-Mar%C3%ADn%20and%20Geoffrey%20Fox%20and%20Shantenu%20Jha%20and%20Linwei%20Wang%0AAbstract%3A%20%20%20High-performance%20scientific%20simulations%2C%20important%20for%20comprehension%20of%0Acomplex%20systems%2C%20encounter%20computational%20challenges%20especially%20when%20exploring%0Aextensive%20parameter%20spaces.%20There%20has%20been%20an%20increasing%20interest%20in%20developing%0Adeep%20neural%20networks%20%28DNNs%29%20as%20surrogate%20models%20capable%20of%20accelerating%20the%0Asimulations.%20However%2C%20existing%20approaches%20for%20training%20these%20DNN%20surrogates%0Arely%20on%20extensive%20simulation%20data%20which%20are%20heuristically%20selected%20and%0Agenerated%20with%20expensive%20computation%20--%20a%20challenge%20under-explored%20in%20the%0Aliterature.%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20incorporating%20active%0Alearning%20into%20DNN%20surrogate%20training.%20This%20allows%20intelligent%20and%20objective%0Aselection%20of%20training%20simulations%2C%20reducing%20the%20need%20to%20generate%20extensive%0Asimulation%20data%20as%20well%20as%20the%20dependency%20of%20the%20performance%20of%20DNN%20surrogates%0Aon%20pre-defined%20training%20simulations.%20In%20the%20problem%20context%20of%20constructing%20DNN%0Asurrogates%20for%20diffusion%20equations%20with%20sources%2C%20we%20examine%20the%20efficacy%20of%0Adiversity-%20and%20uncertainty-based%20strategies%20for%20selecting%20training%20simulations%2C%0Aconsidering%20two%20different%20DNN%20architecture.%20The%20results%20set%20the%20groundwork%20for%0Adeveloping%20the%20high-performance%20computing%20infrastructure%20for%20Smart%20Surrogates%0Athat%20supports%20on-the-fly%20generation%20of%20simulation%20data%20steered%20by%20active%0Alearning%20strategies%20to%20potentially%20improve%20the%20efficiency%20of%20scientific%0Asimulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeasibility%2520Study%2520on%2520Active%2520Learning%2520of%2520Smart%2520Surrogates%2520for%2520Scientific%250A%2520%2520Simulations%26entry.906535625%3DPradeep%2520Bajracharya%2520and%2520Javier%2520Quetzalc%25C3%25B3atl%2520Toledo-Mar%25C3%25ADn%2520and%2520Geoffrey%2520Fox%2520and%2520Shantenu%2520Jha%2520and%2520Linwei%2520Wang%26entry.1292438233%3D%2520%2520High-performance%2520scientific%2520simulations%252C%2520important%2520for%2520comprehension%2520of%250Acomplex%2520systems%252C%2520encounter%2520computational%2520challenges%2520especially%2520when%2520exploring%250Aextensive%2520parameter%2520spaces.%2520There%2520has%2520been%2520an%2520increasing%2520interest%2520in%2520developing%250Adeep%2520neural%2520networks%2520%2528DNNs%2529%2520as%2520surrogate%2520models%2520capable%2520of%2520accelerating%2520the%250Asimulations.%2520However%252C%2520existing%2520approaches%2520for%2520training%2520these%2520DNN%2520surrogates%250Arely%2520on%2520extensive%2520simulation%2520data%2520which%2520are%2520heuristically%2520selected%2520and%250Agenerated%2520with%2520expensive%2520computation%2520--%2520a%2520challenge%2520under-explored%2520in%2520the%250Aliterature.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520potential%2520of%2520incorporating%2520active%250Alearning%2520into%2520DNN%2520surrogate%2520training.%2520This%2520allows%2520intelligent%2520and%2520objective%250Aselection%2520of%2520training%2520simulations%252C%2520reducing%2520the%2520need%2520to%2520generate%2520extensive%250Asimulation%2520data%2520as%2520well%2520as%2520the%2520dependency%2520of%2520the%2520performance%2520of%2520DNN%2520surrogates%250Aon%2520pre-defined%2520training%2520simulations.%2520In%2520the%2520problem%2520context%2520of%2520constructing%2520DNN%250Asurrogates%2520for%2520diffusion%2520equations%2520with%2520sources%252C%2520we%2520examine%2520the%2520efficacy%2520of%250Adiversity-%2520and%2520uncertainty-based%2520strategies%2520for%2520selecting%2520training%2520simulations%252C%250Aconsidering%2520two%2520different%2520DNN%2520architecture.%2520The%2520results%2520set%2520the%2520groundwork%2520for%250Adeveloping%2520the%2520high-performance%2520computing%2520infrastructure%2520for%2520Smart%2520Surrogates%250Athat%2520supports%2520on-the-fly%2520generation%2520of%2520simulation%2520data%2520steered%2520by%2520active%250Alearning%2520strategies%2520to%2520potentially%2520improve%2520the%2520efficiency%2520of%2520scientific%250Asimulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feasibility%20Study%20on%20Active%20Learning%20of%20Smart%20Surrogates%20for%20Scientific%0A%20%20Simulations&entry.906535625=Pradeep%20Bajracharya%20and%20Javier%20Quetzalc%C3%B3atl%20Toledo-Mar%C3%ADn%20and%20Geoffrey%20Fox%20and%20Shantenu%20Jha%20and%20Linwei%20Wang&entry.1292438233=%20%20High-performance%20scientific%20simulations%2C%20important%20for%20comprehension%20of%0Acomplex%20systems%2C%20encounter%20computational%20challenges%20especially%20when%20exploring%0Aextensive%20parameter%20spaces.%20There%20has%20been%20an%20increasing%20interest%20in%20developing%0Adeep%20neural%20networks%20%28DNNs%29%20as%20surrogate%20models%20capable%20of%20accelerating%20the%0Asimulations.%20However%2C%20existing%20approaches%20for%20training%20these%20DNN%20surrogates%0Arely%20on%20extensive%20simulation%20data%20which%20are%20heuristically%20selected%20and%0Agenerated%20with%20expensive%20computation%20--%20a%20challenge%20under-explored%20in%20the%0Aliterature.%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20incorporating%20active%0Alearning%20into%20DNN%20surrogate%20training.%20This%20allows%20intelligent%20and%20objective%0Aselection%20of%20training%20simulations%2C%20reducing%20the%20need%20to%20generate%20extensive%0Asimulation%20data%20as%20well%20as%20the%20dependency%20of%20the%20performance%20of%20DNN%20surrogates%0Aon%20pre-defined%20training%20simulations.%20In%20the%20problem%20context%20of%20constructing%20DNN%0Asurrogates%20for%20diffusion%20equations%20with%20sources%2C%20we%20examine%20the%20efficacy%20of%0Adiversity-%20and%20uncertainty-based%20strategies%20for%20selecting%20training%20simulations%2C%0Aconsidering%20two%20different%20DNN%20architecture.%20The%20results%20set%20the%20groundwork%20for%0Adeveloping%20the%20high-performance%20computing%20infrastructure%20for%20Smart%20Surrogates%0Athat%20supports%20on-the-fly%20generation%20of%20simulation%20data%20steered%20by%20active%0Alearning%20strategies%20to%20potentially%20improve%20the%20efficiency%20of%20scientific%0Asimulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07674v1&entry.124074799=Read"},
{"title": "Early Explorations of Lightweight Models for Wound Segmentation on\n  Mobile Devices", "author": "Vanessa Borst and Timo Dittus and Konstantin M\u00fcller and Samuel Kounev", "abstract": "  The aging population poses numerous challenges to healthcare, including the\nincrease in chronic wounds in the elderly. The current approach to wound\nassessment by therapists based on photographic documentation is subjective,\nhighlighting the need for computer-aided wound recognition from smartphone\nphotos. This offers objective and convenient therapy monitoring, while being\naccessible to patients from their home at any time. However, despite research\nin mobile image segmentation, there is a lack of focus on mobile wound\nsegmentation. To address this gap, we conduct initial research on three\nlightweight architectures to investigate their suitability for smartphone-based\nwound segmentation. Using public datasets and UNet as a baseline, our results\nare promising, with both ENet and TopFormer, as well as the larger UNeXt\nvariant, showing comparable performance to UNet. Furthermore, we deploy the\nmodels into a smartphone app for visual assessment of live segmentation, where\nresults demonstrate the effectiveness of TopFormer in distinguishing wounds\nfrom wound-coloured objects. While our study highlights the potential of\ntransformer models for mobile wound segmentation, future work should aim to\nfurther improve the mask contours.\n", "link": "http://arxiv.org/abs/2407.07605v1", "date": "2024-07-10", "relevancy": 0.9858, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5031}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4909}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Explorations%20of%20Lightweight%20Models%20for%20Wound%20Segmentation%20on%0A%20%20Mobile%20Devices&body=Title%3A%20Early%20Explorations%20of%20Lightweight%20Models%20for%20Wound%20Segmentation%20on%0A%20%20Mobile%20Devices%0AAuthor%3A%20Vanessa%20Borst%20and%20Timo%20Dittus%20and%20Konstantin%20M%C3%BCller%20and%20Samuel%20Kounev%0AAbstract%3A%20%20%20The%20aging%20population%20poses%20numerous%20challenges%20to%20healthcare%2C%20including%20the%0Aincrease%20in%20chronic%20wounds%20in%20the%20elderly.%20The%20current%20approach%20to%20wound%0Aassessment%20by%20therapists%20based%20on%20photographic%20documentation%20is%20subjective%2C%0Ahighlighting%20the%20need%20for%20computer-aided%20wound%20recognition%20from%20smartphone%0Aphotos.%20This%20offers%20objective%20and%20convenient%20therapy%20monitoring%2C%20while%20being%0Aaccessible%20to%20patients%20from%20their%20home%20at%20any%20time.%20However%2C%20despite%20research%0Ain%20mobile%20image%20segmentation%2C%20there%20is%20a%20lack%20of%20focus%20on%20mobile%20wound%0Asegmentation.%20To%20address%20this%20gap%2C%20we%20conduct%20initial%20research%20on%20three%0Alightweight%20architectures%20to%20investigate%20their%20suitability%20for%20smartphone-based%0Awound%20segmentation.%20Using%20public%20datasets%20and%20UNet%20as%20a%20baseline%2C%20our%20results%0Aare%20promising%2C%20with%20both%20ENet%20and%20TopFormer%2C%20as%20well%20as%20the%20larger%20UNeXt%0Avariant%2C%20showing%20comparable%20performance%20to%20UNet.%20Furthermore%2C%20we%20deploy%20the%0Amodels%20into%20a%20smartphone%20app%20for%20visual%20assessment%20of%20live%20segmentation%2C%20where%0Aresults%20demonstrate%20the%20effectiveness%20of%20TopFormer%20in%20distinguishing%20wounds%0Afrom%20wound-coloured%20objects.%20While%20our%20study%20highlights%20the%20potential%20of%0Atransformer%20models%20for%20mobile%20wound%20segmentation%2C%20future%20work%20should%20aim%20to%0Afurther%20improve%20the%20mask%20contours.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Explorations%2520of%2520Lightweight%2520Models%2520for%2520Wound%2520Segmentation%2520on%250A%2520%2520Mobile%2520Devices%26entry.906535625%3DVanessa%2520Borst%2520and%2520Timo%2520Dittus%2520and%2520Konstantin%2520M%25C3%25BCller%2520and%2520Samuel%2520Kounev%26entry.1292438233%3D%2520%2520The%2520aging%2520population%2520poses%2520numerous%2520challenges%2520to%2520healthcare%252C%2520including%2520the%250Aincrease%2520in%2520chronic%2520wounds%2520in%2520the%2520elderly.%2520The%2520current%2520approach%2520to%2520wound%250Aassessment%2520by%2520therapists%2520based%2520on%2520photographic%2520documentation%2520is%2520subjective%252C%250Ahighlighting%2520the%2520need%2520for%2520computer-aided%2520wound%2520recognition%2520from%2520smartphone%250Aphotos.%2520This%2520offers%2520objective%2520and%2520convenient%2520therapy%2520monitoring%252C%2520while%2520being%250Aaccessible%2520to%2520patients%2520from%2520their%2520home%2520at%2520any%2520time.%2520However%252C%2520despite%2520research%250Ain%2520mobile%2520image%2520segmentation%252C%2520there%2520is%2520a%2520lack%2520of%2520focus%2520on%2520mobile%2520wound%250Asegmentation.%2520To%2520address%2520this%2520gap%252C%2520we%2520conduct%2520initial%2520research%2520on%2520three%250Alightweight%2520architectures%2520to%2520investigate%2520their%2520suitability%2520for%2520smartphone-based%250Awound%2520segmentation.%2520Using%2520public%2520datasets%2520and%2520UNet%2520as%2520a%2520baseline%252C%2520our%2520results%250Aare%2520promising%252C%2520with%2520both%2520ENet%2520and%2520TopFormer%252C%2520as%2520well%2520as%2520the%2520larger%2520UNeXt%250Avariant%252C%2520showing%2520comparable%2520performance%2520to%2520UNet.%2520Furthermore%252C%2520we%2520deploy%2520the%250Amodels%2520into%2520a%2520smartphone%2520app%2520for%2520visual%2520assessment%2520of%2520live%2520segmentation%252C%2520where%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520TopFormer%2520in%2520distinguishing%2520wounds%250Afrom%2520wound-coloured%2520objects.%2520While%2520our%2520study%2520highlights%2520the%2520potential%2520of%250Atransformer%2520models%2520for%2520mobile%2520wound%2520segmentation%252C%2520future%2520work%2520should%2520aim%2520to%250Afurther%2520improve%2520the%2520mask%2520contours.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Explorations%20of%20Lightweight%20Models%20for%20Wound%20Segmentation%20on%0A%20%20Mobile%20Devices&entry.906535625=Vanessa%20Borst%20and%20Timo%20Dittus%20and%20Konstantin%20M%C3%BCller%20and%20Samuel%20Kounev&entry.1292438233=%20%20The%20aging%20population%20poses%20numerous%20challenges%20to%20healthcare%2C%20including%20the%0Aincrease%20in%20chronic%20wounds%20in%20the%20elderly.%20The%20current%20approach%20to%20wound%0Aassessment%20by%20therapists%20based%20on%20photographic%20documentation%20is%20subjective%2C%0Ahighlighting%20the%20need%20for%20computer-aided%20wound%20recognition%20from%20smartphone%0Aphotos.%20This%20offers%20objective%20and%20convenient%20therapy%20monitoring%2C%20while%20being%0Aaccessible%20to%20patients%20from%20their%20home%20at%20any%20time.%20However%2C%20despite%20research%0Ain%20mobile%20image%20segmentation%2C%20there%20is%20a%20lack%20of%20focus%20on%20mobile%20wound%0Asegmentation.%20To%20address%20this%20gap%2C%20we%20conduct%20initial%20research%20on%20three%0Alightweight%20architectures%20to%20investigate%20their%20suitability%20for%20smartphone-based%0Awound%20segmentation.%20Using%20public%20datasets%20and%20UNet%20as%20a%20baseline%2C%20our%20results%0Aare%20promising%2C%20with%20both%20ENet%20and%20TopFormer%2C%20as%20well%20as%20the%20larger%20UNeXt%0Avariant%2C%20showing%20comparable%20performance%20to%20UNet.%20Furthermore%2C%20we%20deploy%20the%0Amodels%20into%20a%20smartphone%20app%20for%20visual%20assessment%20of%20live%20segmentation%2C%20where%0Aresults%20demonstrate%20the%20effectiveness%20of%20TopFormer%20in%20distinguishing%20wounds%0Afrom%20wound-coloured%20objects.%20While%20our%20study%20highlights%20the%20potential%20of%0Atransformer%20models%20for%20mobile%20wound%20segmentation%2C%20future%20work%20should%20aim%20to%0Afurther%20improve%20the%20mask%20contours.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07605v1&entry.124074799=Read"},
{"title": "MLRS-PDS: A Meta-learning recommendation of dynamic ensemble selection\n  pipelines", "author": "Hesam Jalalian and Rafael M. O. Cruz", "abstract": "  Dynamic Selection (DS), where base classifiers are chosen from a classifier's\npool for each new instance at test time, has shown to be highly effective in\npattern recognition. However, instability and redundancy in the classifier\npools can impede computational efficiency and accuracy in dynamic ensemble\nselection. This paper introduces a meta-learning recommendation system (MLRS)\nto recommend the optimal pool generation scheme for DES methods tailored to\nindividual datasets. The system employs a meta-model built from dataset\nmeta-features to predict the most suitable pool generation scheme and DES\nmethod for a given dataset. Through an extensive experimental study\nencompassing 288 datasets, we demonstrate that this meta-learning\nrecommendation system outperforms traditional fixed pool or DES method\nselection strategies, highlighting the efficacy of a meta-learning approach in\nrefining DES method selection. The source code, datasets, and supplementary\nresults can be found in this project's GitHub repository:\nhttps://github.com/Menelau/MLRS-PDS.\n", "link": "http://arxiv.org/abs/2407.07528v1", "date": "2024-07-10", "relevancy": 1.4892, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4912}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLRS-PDS%3A%20A%20Meta-learning%20recommendation%20of%20dynamic%20ensemble%20selection%0A%20%20pipelines&body=Title%3A%20MLRS-PDS%3A%20A%20Meta-learning%20recommendation%20of%20dynamic%20ensemble%20selection%0A%20%20pipelines%0AAuthor%3A%20Hesam%20Jalalian%20and%20Rafael%20M.%20O.%20Cruz%0AAbstract%3A%20%20%20Dynamic%20Selection%20%28DS%29%2C%20where%20base%20classifiers%20are%20chosen%20from%20a%20classifier%27s%0Apool%20for%20each%20new%20instance%20at%20test%20time%2C%20has%20shown%20to%20be%20highly%20effective%20in%0Apattern%20recognition.%20However%2C%20instability%20and%20redundancy%20in%20the%20classifier%0Apools%20can%20impede%20computational%20efficiency%20and%20accuracy%20in%20dynamic%20ensemble%0Aselection.%20This%20paper%20introduces%20a%20meta-learning%20recommendation%20system%20%28MLRS%29%0Ato%20recommend%20the%20optimal%20pool%20generation%20scheme%20for%20DES%20methods%20tailored%20to%0Aindividual%20datasets.%20The%20system%20employs%20a%20meta-model%20built%20from%20dataset%0Ameta-features%20to%20predict%20the%20most%20suitable%20pool%20generation%20scheme%20and%20DES%0Amethod%20for%20a%20given%20dataset.%20Through%20an%20extensive%20experimental%20study%0Aencompassing%20288%20datasets%2C%20we%20demonstrate%20that%20this%20meta-learning%0Arecommendation%20system%20outperforms%20traditional%20fixed%20pool%20or%20DES%20method%0Aselection%20strategies%2C%20highlighting%20the%20efficacy%20of%20a%20meta-learning%20approach%20in%0Arefining%20DES%20method%20selection.%20The%20source%20code%2C%20datasets%2C%20and%20supplementary%0Aresults%20can%20be%20found%20in%20this%20project%27s%20GitHub%20repository%3A%0Ahttps%3A//github.com/Menelau/MLRS-PDS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLRS-PDS%253A%2520A%2520Meta-learning%2520recommendation%2520of%2520dynamic%2520ensemble%2520selection%250A%2520%2520pipelines%26entry.906535625%3DHesam%2520Jalalian%2520and%2520Rafael%2520M.%2520O.%2520Cruz%26entry.1292438233%3D%2520%2520Dynamic%2520Selection%2520%2528DS%2529%252C%2520where%2520base%2520classifiers%2520are%2520chosen%2520from%2520a%2520classifier%2527s%250Apool%2520for%2520each%2520new%2520instance%2520at%2520test%2520time%252C%2520has%2520shown%2520to%2520be%2520highly%2520effective%2520in%250Apattern%2520recognition.%2520However%252C%2520instability%2520and%2520redundancy%2520in%2520the%2520classifier%250Apools%2520can%2520impede%2520computational%2520efficiency%2520and%2520accuracy%2520in%2520dynamic%2520ensemble%250Aselection.%2520This%2520paper%2520introduces%2520a%2520meta-learning%2520recommendation%2520system%2520%2528MLRS%2529%250Ato%2520recommend%2520the%2520optimal%2520pool%2520generation%2520scheme%2520for%2520DES%2520methods%2520tailored%2520to%250Aindividual%2520datasets.%2520The%2520system%2520employs%2520a%2520meta-model%2520built%2520from%2520dataset%250Ameta-features%2520to%2520predict%2520the%2520most%2520suitable%2520pool%2520generation%2520scheme%2520and%2520DES%250Amethod%2520for%2520a%2520given%2520dataset.%2520Through%2520an%2520extensive%2520experimental%2520study%250Aencompassing%2520288%2520datasets%252C%2520we%2520demonstrate%2520that%2520this%2520meta-learning%250Arecommendation%2520system%2520outperforms%2520traditional%2520fixed%2520pool%2520or%2520DES%2520method%250Aselection%2520strategies%252C%2520highlighting%2520the%2520efficacy%2520of%2520a%2520meta-learning%2520approach%2520in%250Arefining%2520DES%2520method%2520selection.%2520The%2520source%2520code%252C%2520datasets%252C%2520and%2520supplementary%250Aresults%2520can%2520be%2520found%2520in%2520this%2520project%2527s%2520GitHub%2520repository%253A%250Ahttps%253A//github.com/Menelau/MLRS-PDS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLRS-PDS%3A%20A%20Meta-learning%20recommendation%20of%20dynamic%20ensemble%20selection%0A%20%20pipelines&entry.906535625=Hesam%20Jalalian%20and%20Rafael%20M.%20O.%20Cruz&entry.1292438233=%20%20Dynamic%20Selection%20%28DS%29%2C%20where%20base%20classifiers%20are%20chosen%20from%20a%20classifier%27s%0Apool%20for%20each%20new%20instance%20at%20test%20time%2C%20has%20shown%20to%20be%20highly%20effective%20in%0Apattern%20recognition.%20However%2C%20instability%20and%20redundancy%20in%20the%20classifier%0Apools%20can%20impede%20computational%20efficiency%20and%20accuracy%20in%20dynamic%20ensemble%0Aselection.%20This%20paper%20introduces%20a%20meta-learning%20recommendation%20system%20%28MLRS%29%0Ato%20recommend%20the%20optimal%20pool%20generation%20scheme%20for%20DES%20methods%20tailored%20to%0Aindividual%20datasets.%20The%20system%20employs%20a%20meta-model%20built%20from%20dataset%0Ameta-features%20to%20predict%20the%20most%20suitable%20pool%20generation%20scheme%20and%20DES%0Amethod%20for%20a%20given%20dataset.%20Through%20an%20extensive%20experimental%20study%0Aencompassing%20288%20datasets%2C%20we%20demonstrate%20that%20this%20meta-learning%0Arecommendation%20system%20outperforms%20traditional%20fixed%20pool%20or%20DES%20method%0Aselection%20strategies%2C%20highlighting%20the%20efficacy%20of%20a%20meta-learning%20approach%20in%0Arefining%20DES%20method%20selection.%20The%20source%20code%2C%20datasets%2C%20and%20supplementary%0Aresults%20can%20be%20found%20in%20this%20project%27s%20GitHub%20repository%3A%0Ahttps%3A//github.com/Menelau/MLRS-PDS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07528v1&entry.124074799=Read"},
{"title": "Fair Column Subset Selection", "author": "Antonis Matakos and Bruno Ordozgoiti and Suhas Thejaswi", "abstract": "  The problem of column subset selection asks for a subset of columns from an\ninput matrix such that the matrix can be reconstructed as accurately as\npossible within the span of the selected columns. A natural extension is to\nconsider a setting where the matrix rows are partitioned into two groups, and\nthe goal is to choose a subset of columns that minimizes the maximum\nreconstruction error of both groups, relative to their respective best rank-k\napproximation. Extending the known results of column subset selection to this\nfair setting is not straightforward: in certain scenarios it is unavoidable to\nchoose columns separately for each group, resulting in double the expected\ncolumn count. We propose a deterministic leverage-score sampling strategy for\nthe fair setting and show that sampling a column subset of minimum size becomes\nNP-hard in the presence of two groups. Despite these negative results, we give\nan approximation algorithm that guarantees a solution within 1.5 times the\noptimal solution size. We also present practical heuristic algorithms based on\nrank-revealing QR factorization. Finally, we validate our methods through an\nextensive set of experiments using real-world data.\n", "link": "http://arxiv.org/abs/2306.04489v3", "date": "2024-07-10", "relevancy": 1.9128, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4046}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.375}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Column%20Subset%20Selection&body=Title%3A%20Fair%20Column%20Subset%20Selection%0AAuthor%3A%20Antonis%20Matakos%20and%20Bruno%20Ordozgoiti%20and%20Suhas%20Thejaswi%0AAbstract%3A%20%20%20The%20problem%20of%20column%20subset%20selection%20asks%20for%20a%20subset%20of%20columns%20from%20an%0Ainput%20matrix%20such%20that%20the%20matrix%20can%20be%20reconstructed%20as%20accurately%20as%0Apossible%20within%20the%20span%20of%20the%20selected%20columns.%20A%20natural%20extension%20is%20to%0Aconsider%20a%20setting%20where%20the%20matrix%20rows%20are%20partitioned%20into%20two%20groups%2C%20and%0Athe%20goal%20is%20to%20choose%20a%20subset%20of%20columns%20that%20minimizes%20the%20maximum%0Areconstruction%20error%20of%20both%20groups%2C%20relative%20to%20their%20respective%20best%20rank-k%0Aapproximation.%20Extending%20the%20known%20results%20of%20column%20subset%20selection%20to%20this%0Afair%20setting%20is%20not%20straightforward%3A%20in%20certain%20scenarios%20it%20is%20unavoidable%20to%0Achoose%20columns%20separately%20for%20each%20group%2C%20resulting%20in%20double%20the%20expected%0Acolumn%20count.%20We%20propose%20a%20deterministic%20leverage-score%20sampling%20strategy%20for%0Athe%20fair%20setting%20and%20show%20that%20sampling%20a%20column%20subset%20of%20minimum%20size%20becomes%0ANP-hard%20in%20the%20presence%20of%20two%20groups.%20Despite%20these%20negative%20results%2C%20we%20give%0Aan%20approximation%20algorithm%20that%20guarantees%20a%20solution%20within%201.5%20times%20the%0Aoptimal%20solution%20size.%20We%20also%20present%20practical%20heuristic%20algorithms%20based%20on%0Arank-revealing%20QR%20factorization.%20Finally%2C%20we%20validate%20our%20methods%20through%20an%0Aextensive%20set%20of%20experiments%20using%20real-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04489v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Column%2520Subset%2520Selection%26entry.906535625%3DAntonis%2520Matakos%2520and%2520Bruno%2520Ordozgoiti%2520and%2520Suhas%2520Thejaswi%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520column%2520subset%2520selection%2520asks%2520for%2520a%2520subset%2520of%2520columns%2520from%2520an%250Ainput%2520matrix%2520such%2520that%2520the%2520matrix%2520can%2520be%2520reconstructed%2520as%2520accurately%2520as%250Apossible%2520within%2520the%2520span%2520of%2520the%2520selected%2520columns.%2520A%2520natural%2520extension%2520is%2520to%250Aconsider%2520a%2520setting%2520where%2520the%2520matrix%2520rows%2520are%2520partitioned%2520into%2520two%2520groups%252C%2520and%250Athe%2520goal%2520is%2520to%2520choose%2520a%2520subset%2520of%2520columns%2520that%2520minimizes%2520the%2520maximum%250Areconstruction%2520error%2520of%2520both%2520groups%252C%2520relative%2520to%2520their%2520respective%2520best%2520rank-k%250Aapproximation.%2520Extending%2520the%2520known%2520results%2520of%2520column%2520subset%2520selection%2520to%2520this%250Afair%2520setting%2520is%2520not%2520straightforward%253A%2520in%2520certain%2520scenarios%2520it%2520is%2520unavoidable%2520to%250Achoose%2520columns%2520separately%2520for%2520each%2520group%252C%2520resulting%2520in%2520double%2520the%2520expected%250Acolumn%2520count.%2520We%2520propose%2520a%2520deterministic%2520leverage-score%2520sampling%2520strategy%2520for%250Athe%2520fair%2520setting%2520and%2520show%2520that%2520sampling%2520a%2520column%2520subset%2520of%2520minimum%2520size%2520becomes%250ANP-hard%2520in%2520the%2520presence%2520of%2520two%2520groups.%2520Despite%2520these%2520negative%2520results%252C%2520we%2520give%250Aan%2520approximation%2520algorithm%2520that%2520guarantees%2520a%2520solution%2520within%25201.5%2520times%2520the%250Aoptimal%2520solution%2520size.%2520We%2520also%2520present%2520practical%2520heuristic%2520algorithms%2520based%2520on%250Arank-revealing%2520QR%2520factorization.%2520Finally%252C%2520we%2520validate%2520our%2520methods%2520through%2520an%250Aextensive%2520set%2520of%2520experiments%2520using%2520real-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.04489v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Column%20Subset%20Selection&entry.906535625=Antonis%20Matakos%20and%20Bruno%20Ordozgoiti%20and%20Suhas%20Thejaswi&entry.1292438233=%20%20The%20problem%20of%20column%20subset%20selection%20asks%20for%20a%20subset%20of%20columns%20from%20an%0Ainput%20matrix%20such%20that%20the%20matrix%20can%20be%20reconstructed%20as%20accurately%20as%0Apossible%20within%20the%20span%20of%20the%20selected%20columns.%20A%20natural%20extension%20is%20to%0Aconsider%20a%20setting%20where%20the%20matrix%20rows%20are%20partitioned%20into%20two%20groups%2C%20and%0Athe%20goal%20is%20to%20choose%20a%20subset%20of%20columns%20that%20minimizes%20the%20maximum%0Areconstruction%20error%20of%20both%20groups%2C%20relative%20to%20their%20respective%20best%20rank-k%0Aapproximation.%20Extending%20the%20known%20results%20of%20column%20subset%20selection%20to%20this%0Afair%20setting%20is%20not%20straightforward%3A%20in%20certain%20scenarios%20it%20is%20unavoidable%20to%0Achoose%20columns%20separately%20for%20each%20group%2C%20resulting%20in%20double%20the%20expected%0Acolumn%20count.%20We%20propose%20a%20deterministic%20leverage-score%20sampling%20strategy%20for%0Athe%20fair%20setting%20and%20show%20that%20sampling%20a%20column%20subset%20of%20minimum%20size%20becomes%0ANP-hard%20in%20the%20presence%20of%20two%20groups.%20Despite%20these%20negative%20results%2C%20we%20give%0Aan%20approximation%20algorithm%20that%20guarantees%20a%20solution%20within%201.5%20times%20the%0Aoptimal%20solution%20size.%20We%20also%20present%20practical%20heuristic%20algorithms%20based%20on%0Arank-revealing%20QR%20factorization.%20Finally%2C%20we%20validate%20our%20methods%20through%20an%0Aextensive%20set%20of%20experiments%20using%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04489v3&entry.124074799=Read"},
{"title": "Learning Spatial-Semantic Features for Robust Video Object Segmentation", "author": "Xin Li and Deshui Miao and Zhenyu He and Yaowei Wang and Huchuan Lu and Ming-Hsuan Yang", "abstract": "  Tracking and segmenting multiple similar objects with complex or separate\nparts in long-term videos is inherently challenging due to the ambiguity of\ntarget parts and identity confusion caused by occlusion, background clutter,\nand long-term variations. In this paper, we propose a robust video object\nsegmentation framework equipped with spatial-semantic features and\ndiscriminative object queries to address the above issues. Specifically, we\nconstruct a spatial-semantic network comprising a semantic embedding block and\nspatial dependencies modeling block to associate the pretrained ViT features\nwith global semantic features and local spatial features, providing a\ncomprehensive target representation. In addition, we develop a masked\ncross-attention module to generate object queries that focus on the most\ndiscriminative parts of target objects during query propagation, alleviating\nnoise accumulation and ensuring effective long-term query propagation. The\nexperimental results show that the proposed method set a new state-of-the-art\nperformance on multiple datasets, including the DAVIS2017 test (89.1%),\nYoutubeVOS 2019 (88.5%), MOSE (75.1%), LVOS test (73.0%), and LVOS val (75.1%),\nwhich demonstrate the effectiveness and generalization capacity of the proposed\nmethod. We will make all source code and trained models publicly available.\n", "link": "http://arxiv.org/abs/2407.07760v1", "date": "2024-07-10", "relevancy": 1.7168, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5771}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Spatial-Semantic%20Features%20for%20Robust%20Video%20Object%20Segmentation&body=Title%3A%20Learning%20Spatial-Semantic%20Features%20for%20Robust%20Video%20Object%20Segmentation%0AAuthor%3A%20Xin%20Li%20and%20Deshui%20Miao%20and%20Zhenyu%20He%20and%20Yaowei%20Wang%20and%20Huchuan%20Lu%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Tracking%20and%20segmenting%20multiple%20similar%20objects%20with%20complex%20or%20separate%0Aparts%20in%20long-term%20videos%20is%20inherently%20challenging%20due%20to%20the%20ambiguity%20of%0Atarget%20parts%20and%20identity%20confusion%20caused%20by%20occlusion%2C%20background%20clutter%2C%0Aand%20long-term%20variations.%20In%20this%20paper%2C%20we%20propose%20a%20robust%20video%20object%0Asegmentation%20framework%20equipped%20with%20spatial-semantic%20features%20and%0Adiscriminative%20object%20queries%20to%20address%20the%20above%20issues.%20Specifically%2C%20we%0Aconstruct%20a%20spatial-semantic%20network%20comprising%20a%20semantic%20embedding%20block%20and%0Aspatial%20dependencies%20modeling%20block%20to%20associate%20the%20pretrained%20ViT%20features%0Awith%20global%20semantic%20features%20and%20local%20spatial%20features%2C%20providing%20a%0Acomprehensive%20target%20representation.%20In%20addition%2C%20we%20develop%20a%20masked%0Across-attention%20module%20to%20generate%20object%20queries%20that%20focus%20on%20the%20most%0Adiscriminative%20parts%20of%20target%20objects%20during%20query%20propagation%2C%20alleviating%0Anoise%20accumulation%20and%20ensuring%20effective%20long-term%20query%20propagation.%20The%0Aexperimental%20results%20show%20that%20the%20proposed%20method%20set%20a%20new%20state-of-the-art%0Aperformance%20on%20multiple%20datasets%2C%20including%20the%20DAVIS2017%20test%20%2889.1%25%29%2C%0AYoutubeVOS%202019%20%2888.5%25%29%2C%20MOSE%20%2875.1%25%29%2C%20LVOS%20test%20%2873.0%25%29%2C%20and%20LVOS%20val%20%2875.1%25%29%2C%0Awhich%20demonstrate%20the%20effectiveness%20and%20generalization%20capacity%20of%20the%20proposed%0Amethod.%20We%20will%20make%20all%20source%20code%20and%20trained%20models%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Spatial-Semantic%2520Features%2520for%2520Robust%2520Video%2520Object%2520Segmentation%26entry.906535625%3DXin%2520Li%2520and%2520Deshui%2520Miao%2520and%2520Zhenyu%2520He%2520and%2520Yaowei%2520Wang%2520and%2520Huchuan%2520Lu%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Tracking%2520and%2520segmenting%2520multiple%2520similar%2520objects%2520with%2520complex%2520or%2520separate%250Aparts%2520in%2520long-term%2520videos%2520is%2520inherently%2520challenging%2520due%2520to%2520the%2520ambiguity%2520of%250Atarget%2520parts%2520and%2520identity%2520confusion%2520caused%2520by%2520occlusion%252C%2520background%2520clutter%252C%250Aand%2520long-term%2520variations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520robust%2520video%2520object%250Asegmentation%2520framework%2520equipped%2520with%2520spatial-semantic%2520features%2520and%250Adiscriminative%2520object%2520queries%2520to%2520address%2520the%2520above%2520issues.%2520Specifically%252C%2520we%250Aconstruct%2520a%2520spatial-semantic%2520network%2520comprising%2520a%2520semantic%2520embedding%2520block%2520and%250Aspatial%2520dependencies%2520modeling%2520block%2520to%2520associate%2520the%2520pretrained%2520ViT%2520features%250Awith%2520global%2520semantic%2520features%2520and%2520local%2520spatial%2520features%252C%2520providing%2520a%250Acomprehensive%2520target%2520representation.%2520In%2520addition%252C%2520we%2520develop%2520a%2520masked%250Across-attention%2520module%2520to%2520generate%2520object%2520queries%2520that%2520focus%2520on%2520the%2520most%250Adiscriminative%2520parts%2520of%2520target%2520objects%2520during%2520query%2520propagation%252C%2520alleviating%250Anoise%2520accumulation%2520and%2520ensuring%2520effective%2520long-term%2520query%2520propagation.%2520The%250Aexperimental%2520results%2520show%2520that%2520the%2520proposed%2520method%2520set%2520a%2520new%2520state-of-the-art%250Aperformance%2520on%2520multiple%2520datasets%252C%2520including%2520the%2520DAVIS2017%2520test%2520%252889.1%2525%2529%252C%250AYoutubeVOS%25202019%2520%252888.5%2525%2529%252C%2520MOSE%2520%252875.1%2525%2529%252C%2520LVOS%2520test%2520%252873.0%2525%2529%252C%2520and%2520LVOS%2520val%2520%252875.1%2525%2529%252C%250Awhich%2520demonstrate%2520the%2520effectiveness%2520and%2520generalization%2520capacity%2520of%2520the%2520proposed%250Amethod.%2520We%2520will%2520make%2520all%2520source%2520code%2520and%2520trained%2520models%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Spatial-Semantic%20Features%20for%20Robust%20Video%20Object%20Segmentation&entry.906535625=Xin%20Li%20and%20Deshui%20Miao%20and%20Zhenyu%20He%20and%20Yaowei%20Wang%20and%20Huchuan%20Lu%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Tracking%20and%20segmenting%20multiple%20similar%20objects%20with%20complex%20or%20separate%0Aparts%20in%20long-term%20videos%20is%20inherently%20challenging%20due%20to%20the%20ambiguity%20of%0Atarget%20parts%20and%20identity%20confusion%20caused%20by%20occlusion%2C%20background%20clutter%2C%0Aand%20long-term%20variations.%20In%20this%20paper%2C%20we%20propose%20a%20robust%20video%20object%0Asegmentation%20framework%20equipped%20with%20spatial-semantic%20features%20and%0Adiscriminative%20object%20queries%20to%20address%20the%20above%20issues.%20Specifically%2C%20we%0Aconstruct%20a%20spatial-semantic%20network%20comprising%20a%20semantic%20embedding%20block%20and%0Aspatial%20dependencies%20modeling%20block%20to%20associate%20the%20pretrained%20ViT%20features%0Awith%20global%20semantic%20features%20and%20local%20spatial%20features%2C%20providing%20a%0Acomprehensive%20target%20representation.%20In%20addition%2C%20we%20develop%20a%20masked%0Across-attention%20module%20to%20generate%20object%20queries%20that%20focus%20on%20the%20most%0Adiscriminative%20parts%20of%20target%20objects%20during%20query%20propagation%2C%20alleviating%0Anoise%20accumulation%20and%20ensuring%20effective%20long-term%20query%20propagation.%20The%0Aexperimental%20results%20show%20that%20the%20proposed%20method%20set%20a%20new%20state-of-the-art%0Aperformance%20on%20multiple%20datasets%2C%20including%20the%20DAVIS2017%20test%20%2889.1%25%29%2C%0AYoutubeVOS%202019%20%2888.5%25%29%2C%20MOSE%20%2875.1%25%29%2C%20LVOS%20test%20%2873.0%25%29%2C%20and%20LVOS%20val%20%2875.1%25%29%2C%0Awhich%20demonstrate%20the%20effectiveness%20and%20generalization%20capacity%20of%20the%20proposed%0Amethod.%20We%20will%20make%20all%20source%20code%20and%20trained%20models%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07760v1&entry.124074799=Read"},
{"title": "Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs\n  and Topological Graphs", "author": "Hao-Tien Lewis Chiang and Zhuo Xu and Zipeng Fu and Mithun George Jacob and Tingnan Zhang and Tsang-Wei Edward Lee and Wenhao Yu and Connor Schenck and David Rendleman and Dhruv Shah and Fei Xia and Jasmine Hsu and Jonathan Hoech and Pete Florence and Sean Kirmani and Sumeet Singh and Vikas Sindhwani and Carolina Parada and Chelsea Finn and Peng Xu and Sergey Levine and Jie Tan", "abstract": "  An elusive goal in navigation research is to build an intelligent agent that\ncan understand multimodal instructions including natural language and image,\nand perform useful navigation. To achieve this, we study a widely useful\ncategory of navigation tasks we call Multimodal Instruction Navigation with\ndemonstration Tours (MINT), in which the environment prior is provided through\na previously recorded demonstration video. Recent advances in Vision Language\nModels (VLMs) have shown a promising path in achieving this goal as it\ndemonstrates capabilities in perceiving and reasoning about multimodal inputs.\nHowever, VLMs are typically trained to predict textual output and it is an open\nresearch question about how to best utilize them in navigation. To solve MINT,\nwe present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation\npolicy that combines the environment understanding and common sense reasoning\npower of long-context VLMs and a robust low-level navigation policy based on\ntopological graphs. The high-level policy consists of a long-context VLM that\ntakes the demonstration tour video and the multimodal user instruction as input\nto find the goal frame in the tour video. Next, a low-level policy uses the\ngoal frame and an offline constructed topological graph to generate robot\nactions at every timestep. We evaluated Mobility VLA in a 836m^2 real world\nenvironment and show that Mobility VLA has a high end-to-end success rates on\npreviously unsolved multimodal instructions such as \"Where should I return\nthis?\" while holding a plastic bin.\n", "link": "http://arxiv.org/abs/2407.07775v1", "date": "2024-07-10", "relevancy": 1.7974, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6096}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mobility%20VLA%3A%20Multimodal%20Instruction%20Navigation%20with%20Long-Context%20VLMs%0A%20%20and%20Topological%20Graphs&body=Title%3A%20Mobility%20VLA%3A%20Multimodal%20Instruction%20Navigation%20with%20Long-Context%20VLMs%0A%20%20and%20Topological%20Graphs%0AAuthor%3A%20Hao-Tien%20Lewis%20Chiang%20and%20Zhuo%20Xu%20and%20Zipeng%20Fu%20and%20Mithun%20George%20Jacob%20and%20Tingnan%20Zhang%20and%20Tsang-Wei%20Edward%20Lee%20and%20Wenhao%20Yu%20and%20Connor%20Schenck%20and%20David%20Rendleman%20and%20Dhruv%20Shah%20and%20Fei%20Xia%20and%20Jasmine%20Hsu%20and%20Jonathan%20Hoech%20and%20Pete%20Florence%20and%20Sean%20Kirmani%20and%20Sumeet%20Singh%20and%20Vikas%20Sindhwani%20and%20Carolina%20Parada%20and%20Chelsea%20Finn%20and%20Peng%20Xu%20and%20Sergey%20Levine%20and%20Jie%20Tan%0AAbstract%3A%20%20%20An%20elusive%20goal%20in%20navigation%20research%20is%20to%20build%20an%20intelligent%20agent%20that%0Acan%20understand%20multimodal%20instructions%20including%20natural%20language%20and%20image%2C%0Aand%20perform%20useful%20navigation.%20To%20achieve%20this%2C%20we%20study%20a%20widely%20useful%0Acategory%20of%20navigation%20tasks%20we%20call%20Multimodal%20Instruction%20Navigation%20with%0Ademonstration%20Tours%20%28MINT%29%2C%20in%20which%20the%20environment%20prior%20is%20provided%20through%0Aa%20previously%20recorded%20demonstration%20video.%20Recent%20advances%20in%20Vision%20Language%0AModels%20%28VLMs%29%20have%20shown%20a%20promising%20path%20in%20achieving%20this%20goal%20as%20it%0Ademonstrates%20capabilities%20in%20perceiving%20and%20reasoning%20about%20multimodal%20inputs.%0AHowever%2C%20VLMs%20are%20typically%20trained%20to%20predict%20textual%20output%20and%20it%20is%20an%20open%0Aresearch%20question%20about%20how%20to%20best%20utilize%20them%20in%20navigation.%20To%20solve%20MINT%2C%0Awe%20present%20Mobility%20VLA%2C%20a%20hierarchical%20Vision-Language-Action%20%28VLA%29%20navigation%0Apolicy%20that%20combines%20the%20environment%20understanding%20and%20common%20sense%20reasoning%0Apower%20of%20long-context%20VLMs%20and%20a%20robust%20low-level%20navigation%20policy%20based%20on%0Atopological%20graphs.%20The%20high-level%20policy%20consists%20of%20a%20long-context%20VLM%20that%0Atakes%20the%20demonstration%20tour%20video%20and%20the%20multimodal%20user%20instruction%20as%20input%0Ato%20find%20the%20goal%20frame%20in%20the%20tour%20video.%20Next%2C%20a%20low-level%20policy%20uses%20the%0Agoal%20frame%20and%20an%20offline%20constructed%20topological%20graph%20to%20generate%20robot%0Aactions%20at%20every%20timestep.%20We%20evaluated%20Mobility%20VLA%20in%20a%20836m%5E2%20real%20world%0Aenvironment%20and%20show%20that%20Mobility%20VLA%20has%20a%20high%20end-to-end%20success%20rates%20on%0Apreviously%20unsolved%20multimodal%20instructions%20such%20as%20%22Where%20should%20I%20return%0Athis%3F%22%20while%20holding%20a%20plastic%20bin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMobility%2520VLA%253A%2520Multimodal%2520Instruction%2520Navigation%2520with%2520Long-Context%2520VLMs%250A%2520%2520and%2520Topological%2520Graphs%26entry.906535625%3DHao-Tien%2520Lewis%2520Chiang%2520and%2520Zhuo%2520Xu%2520and%2520Zipeng%2520Fu%2520and%2520Mithun%2520George%2520Jacob%2520and%2520Tingnan%2520Zhang%2520and%2520Tsang-Wei%2520Edward%2520Lee%2520and%2520Wenhao%2520Yu%2520and%2520Connor%2520Schenck%2520and%2520David%2520Rendleman%2520and%2520Dhruv%2520Shah%2520and%2520Fei%2520Xia%2520and%2520Jasmine%2520Hsu%2520and%2520Jonathan%2520Hoech%2520and%2520Pete%2520Florence%2520and%2520Sean%2520Kirmani%2520and%2520Sumeet%2520Singh%2520and%2520Vikas%2520Sindhwani%2520and%2520Carolina%2520Parada%2520and%2520Chelsea%2520Finn%2520and%2520Peng%2520Xu%2520and%2520Sergey%2520Levine%2520and%2520Jie%2520Tan%26entry.1292438233%3D%2520%2520An%2520elusive%2520goal%2520in%2520navigation%2520research%2520is%2520to%2520build%2520an%2520intelligent%2520agent%2520that%250Acan%2520understand%2520multimodal%2520instructions%2520including%2520natural%2520language%2520and%2520image%252C%250Aand%2520perform%2520useful%2520navigation.%2520To%2520achieve%2520this%252C%2520we%2520study%2520a%2520widely%2520useful%250Acategory%2520of%2520navigation%2520tasks%2520we%2520call%2520Multimodal%2520Instruction%2520Navigation%2520with%250Ademonstration%2520Tours%2520%2528MINT%2529%252C%2520in%2520which%2520the%2520environment%2520prior%2520is%2520provided%2520through%250Aa%2520previously%2520recorded%2520demonstration%2520video.%2520Recent%2520advances%2520in%2520Vision%2520Language%250AModels%2520%2528VLMs%2529%2520have%2520shown%2520a%2520promising%2520path%2520in%2520achieving%2520this%2520goal%2520as%2520it%250Ademonstrates%2520capabilities%2520in%2520perceiving%2520and%2520reasoning%2520about%2520multimodal%2520inputs.%250AHowever%252C%2520VLMs%2520are%2520typically%2520trained%2520to%2520predict%2520textual%2520output%2520and%2520it%2520is%2520an%2520open%250Aresearch%2520question%2520about%2520how%2520to%2520best%2520utilize%2520them%2520in%2520navigation.%2520To%2520solve%2520MINT%252C%250Awe%2520present%2520Mobility%2520VLA%252C%2520a%2520hierarchical%2520Vision-Language-Action%2520%2528VLA%2529%2520navigation%250Apolicy%2520that%2520combines%2520the%2520environment%2520understanding%2520and%2520common%2520sense%2520reasoning%250Apower%2520of%2520long-context%2520VLMs%2520and%2520a%2520robust%2520low-level%2520navigation%2520policy%2520based%2520on%250Atopological%2520graphs.%2520The%2520high-level%2520policy%2520consists%2520of%2520a%2520long-context%2520VLM%2520that%250Atakes%2520the%2520demonstration%2520tour%2520video%2520and%2520the%2520multimodal%2520user%2520instruction%2520as%2520input%250Ato%2520find%2520the%2520goal%2520frame%2520in%2520the%2520tour%2520video.%2520Next%252C%2520a%2520low-level%2520policy%2520uses%2520the%250Agoal%2520frame%2520and%2520an%2520offline%2520constructed%2520topological%2520graph%2520to%2520generate%2520robot%250Aactions%2520at%2520every%2520timestep.%2520We%2520evaluated%2520Mobility%2520VLA%2520in%2520a%2520836m%255E2%2520real%2520world%250Aenvironment%2520and%2520show%2520that%2520Mobility%2520VLA%2520has%2520a%2520high%2520end-to-end%2520success%2520rates%2520on%250Apreviously%2520unsolved%2520multimodal%2520instructions%2520such%2520as%2520%2522Where%2520should%2520I%2520return%250Athis%253F%2522%2520while%2520holding%2520a%2520plastic%2520bin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mobility%20VLA%3A%20Multimodal%20Instruction%20Navigation%20with%20Long-Context%20VLMs%0A%20%20and%20Topological%20Graphs&entry.906535625=Hao-Tien%20Lewis%20Chiang%20and%20Zhuo%20Xu%20and%20Zipeng%20Fu%20and%20Mithun%20George%20Jacob%20and%20Tingnan%20Zhang%20and%20Tsang-Wei%20Edward%20Lee%20and%20Wenhao%20Yu%20and%20Connor%20Schenck%20and%20David%20Rendleman%20and%20Dhruv%20Shah%20and%20Fei%20Xia%20and%20Jasmine%20Hsu%20and%20Jonathan%20Hoech%20and%20Pete%20Florence%20and%20Sean%20Kirmani%20and%20Sumeet%20Singh%20and%20Vikas%20Sindhwani%20and%20Carolina%20Parada%20and%20Chelsea%20Finn%20and%20Peng%20Xu%20and%20Sergey%20Levine%20and%20Jie%20Tan&entry.1292438233=%20%20An%20elusive%20goal%20in%20navigation%20research%20is%20to%20build%20an%20intelligent%20agent%20that%0Acan%20understand%20multimodal%20instructions%20including%20natural%20language%20and%20image%2C%0Aand%20perform%20useful%20navigation.%20To%20achieve%20this%2C%20we%20study%20a%20widely%20useful%0Acategory%20of%20navigation%20tasks%20we%20call%20Multimodal%20Instruction%20Navigation%20with%0Ademonstration%20Tours%20%28MINT%29%2C%20in%20which%20the%20environment%20prior%20is%20provided%20through%0Aa%20previously%20recorded%20demonstration%20video.%20Recent%20advances%20in%20Vision%20Language%0AModels%20%28VLMs%29%20have%20shown%20a%20promising%20path%20in%20achieving%20this%20goal%20as%20it%0Ademonstrates%20capabilities%20in%20perceiving%20and%20reasoning%20about%20multimodal%20inputs.%0AHowever%2C%20VLMs%20are%20typically%20trained%20to%20predict%20textual%20output%20and%20it%20is%20an%20open%0Aresearch%20question%20about%20how%20to%20best%20utilize%20them%20in%20navigation.%20To%20solve%20MINT%2C%0Awe%20present%20Mobility%20VLA%2C%20a%20hierarchical%20Vision-Language-Action%20%28VLA%29%20navigation%0Apolicy%20that%20combines%20the%20environment%20understanding%20and%20common%20sense%20reasoning%0Apower%20of%20long-context%20VLMs%20and%20a%20robust%20low-level%20navigation%20policy%20based%20on%0Atopological%20graphs.%20The%20high-level%20policy%20consists%20of%20a%20long-context%20VLM%20that%0Atakes%20the%20demonstration%20tour%20video%20and%20the%20multimodal%20user%20instruction%20as%20input%0Ato%20find%20the%20goal%20frame%20in%20the%20tour%20video.%20Next%2C%20a%20low-level%20policy%20uses%20the%0Agoal%20frame%20and%20an%20offline%20constructed%20topological%20graph%20to%20generate%20robot%0Aactions%20at%20every%20timestep.%20We%20evaluated%20Mobility%20VLA%20in%20a%20836m%5E2%20real%20world%0Aenvironment%20and%20show%20that%20Mobility%20VLA%20has%20a%20high%20end-to-end%20success%20rates%20on%0Apreviously%20unsolved%20multimodal%20instructions%20such%20as%20%22Where%20should%20I%20return%0Athis%3F%22%20while%20holding%20a%20plastic%20bin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07775v1&entry.124074799=Read"},
{"title": "Toto: Time Series Optimized Transformer for Observability", "author": "Ben Cohen and Emaad Khwaja and Kan Wang and Charles Masson and Elise Ram\u00e9 and Youssef Doubli and Othmane Abou-Amal", "abstract": "  This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics. Toto was\ntrained on a dataset of one trillion time series data points, the largest among\nall currently published time series foundation models. Alongside publicly\navailable time series datasets, 75% of the data used to train Toto consists of\nfully anonymous numerical metric data points from the Datadog platform. In our\nexperiments, Toto outperforms existing time series foundation models on\nobservability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.\n", "link": "http://arxiv.org/abs/2407.07874v1", "date": "2024-07-10", "relevancy": 1.3594, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4701}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4632}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toto%3A%20Time%20Series%20Optimized%20Transformer%20for%20Observability&body=Title%3A%20Toto%3A%20Time%20Series%20Optimized%20Transformer%20for%20Observability%0AAuthor%3A%20Ben%20Cohen%20and%20Emaad%20Khwaja%20and%20Kan%20Wang%20and%20Charles%20Masson%20and%20Elise%20Ram%C3%A9%20and%20Youssef%20Doubli%20and%20Othmane%20Abou-Amal%0AAbstract%3A%20%20%20This%20technical%20report%20describes%20the%20Time%20Series%20Optimized%20Transformer%20for%0AObservability%20%28Toto%29%2C%20a%20new%20state%20of%20the%20art%20foundation%20model%20for%20time%20series%0Aforecasting%20developed%20by%20Datadog.%20In%20addition%20to%20advancing%20the%20state%20of%20the%20art%0Aon%20generalized%20time%20series%20benchmarks%20in%20domains%20such%20as%20electricity%20and%0Aweather%2C%20this%20model%20is%20the%20first%20general-purpose%20time%20series%20forecasting%0Afoundation%20model%20to%20be%20specifically%20tuned%20for%20observability%20metrics.%20Toto%20was%0Atrained%20on%20a%20dataset%20of%20one%20trillion%20time%20series%20data%20points%2C%20the%20largest%20among%0Aall%20currently%20published%20time%20series%20foundation%20models.%20Alongside%20publicly%0Aavailable%20time%20series%20datasets%2C%2075%25%20of%20the%20data%20used%20to%20train%20Toto%20consists%20of%0Afully%20anonymous%20numerical%20metric%20data%20points%20from%20the%20Datadog%20platform.%20In%20our%0Aexperiments%2C%20Toto%20outperforms%20existing%20time%20series%20foundation%20models%20on%0Aobservability%20data.%20It%20does%20this%20while%20also%20excelling%20at%20general-purpose%0Aforecasting%20tasks%2C%20achieving%20state-of-the-art%20zero-shot%20performance%20on%20multiple%0Aopen%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToto%253A%2520Time%2520Series%2520Optimized%2520Transformer%2520for%2520Observability%26entry.906535625%3DBen%2520Cohen%2520and%2520Emaad%2520Khwaja%2520and%2520Kan%2520Wang%2520and%2520Charles%2520Masson%2520and%2520Elise%2520Ram%25C3%25A9%2520and%2520Youssef%2520Doubli%2520and%2520Othmane%2520Abou-Amal%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520describes%2520the%2520Time%2520Series%2520Optimized%2520Transformer%2520for%250AObservability%2520%2528Toto%2529%252C%2520a%2520new%2520state%2520of%2520the%2520art%2520foundation%2520model%2520for%2520time%2520series%250Aforecasting%2520developed%2520by%2520Datadog.%2520In%2520addition%2520to%2520advancing%2520the%2520state%2520of%2520the%2520art%250Aon%2520generalized%2520time%2520series%2520benchmarks%2520in%2520domains%2520such%2520as%2520electricity%2520and%250Aweather%252C%2520this%2520model%2520is%2520the%2520first%2520general-purpose%2520time%2520series%2520forecasting%250Afoundation%2520model%2520to%2520be%2520specifically%2520tuned%2520for%2520observability%2520metrics.%2520Toto%2520was%250Atrained%2520on%2520a%2520dataset%2520of%2520one%2520trillion%2520time%2520series%2520data%2520points%252C%2520the%2520largest%2520among%250Aall%2520currently%2520published%2520time%2520series%2520foundation%2520models.%2520Alongside%2520publicly%250Aavailable%2520time%2520series%2520datasets%252C%252075%2525%2520of%2520the%2520data%2520used%2520to%2520train%2520Toto%2520consists%2520of%250Afully%2520anonymous%2520numerical%2520metric%2520data%2520points%2520from%2520the%2520Datadog%2520platform.%2520In%2520our%250Aexperiments%252C%2520Toto%2520outperforms%2520existing%2520time%2520series%2520foundation%2520models%2520on%250Aobservability%2520data.%2520It%2520does%2520this%2520while%2520also%2520excelling%2520at%2520general-purpose%250Aforecasting%2520tasks%252C%2520achieving%2520state-of-the-art%2520zero-shot%2520performance%2520on%2520multiple%250Aopen%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toto%3A%20Time%20Series%20Optimized%20Transformer%20for%20Observability&entry.906535625=Ben%20Cohen%20and%20Emaad%20Khwaja%20and%20Kan%20Wang%20and%20Charles%20Masson%20and%20Elise%20Ram%C3%A9%20and%20Youssef%20Doubli%20and%20Othmane%20Abou-Amal&entry.1292438233=%20%20This%20technical%20report%20describes%20the%20Time%20Series%20Optimized%20Transformer%20for%0AObservability%20%28Toto%29%2C%20a%20new%20state%20of%20the%20art%20foundation%20model%20for%20time%20series%0Aforecasting%20developed%20by%20Datadog.%20In%20addition%20to%20advancing%20the%20state%20of%20the%20art%0Aon%20generalized%20time%20series%20benchmarks%20in%20domains%20such%20as%20electricity%20and%0Aweather%2C%20this%20model%20is%20the%20first%20general-purpose%20time%20series%20forecasting%0Afoundation%20model%20to%20be%20specifically%20tuned%20for%20observability%20metrics.%20Toto%20was%0Atrained%20on%20a%20dataset%20of%20one%20trillion%20time%20series%20data%20points%2C%20the%20largest%20among%0Aall%20currently%20published%20time%20series%20foundation%20models.%20Alongside%20publicly%0Aavailable%20time%20series%20datasets%2C%2075%25%20of%20the%20data%20used%20to%20train%20Toto%20consists%20of%0Afully%20anonymous%20numerical%20metric%20data%20points%20from%20the%20Datadog%20platform.%20In%20our%0Aexperiments%2C%20Toto%20outperforms%20existing%20time%20series%20foundation%20models%20on%0Aobservability%20data.%20It%20does%20this%20while%20also%20excelling%20at%20general-purpose%0Aforecasting%20tasks%2C%20achieving%20state-of-the-art%20zero-shot%20performance%20on%20multiple%0Aopen%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07874v1&entry.124074799=Read"},
{"title": "Probabilistic Routing for Graph-Based Approximate Nearest Neighbor\n  Search", "author": "Kejing Lu and Chuan Xiao and Yoshiharu Ishikawa", "abstract": "  Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a\npivotal challenge in the field of machine learning. In recent years,\ngraph-based methods have emerged as the superior approach to ANNS, establishing\na new state of the art. Although various optimizations for graph-based ANNS\nhave been introduced, they predominantly rely on heuristic methods that lack\nformal theoretical backing. This paper aims to enhance routing within\ngraph-based ANNS by introducing a method that offers a probabilistic guarantee\nwhen exploring a node's neighbors in the graph. We formulate the problem as\nprobabilistic routing and develop two baseline strategies by incorporating\nlocality-sensitive techniques. Subsequently, we introduce PEOs, a novel\napproach that efficiently identifies which neighbors in the graph should be\nconsidered for exact distance calculation, thus significantly improving\nefficiency in practice. Our experiments demonstrate that equipping PEOs can\nincrease throughput on commonly utilized graph indexes (HNSW and NSSG) by a\nfactor of 1.6 to 2.5, and its efficiency consistently outperforms the\nleading-edge routing technique by 1.1 to 1.4 times.\n", "link": "http://arxiv.org/abs/2402.11354v2", "date": "2024-07-10", "relevancy": 1.376, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4692}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4638}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Routing%20for%20Graph-Based%20Approximate%20Nearest%20Neighbor%0A%20%20Search&body=Title%3A%20Probabilistic%20Routing%20for%20Graph-Based%20Approximate%20Nearest%20Neighbor%0A%20%20Search%0AAuthor%3A%20Kejing%20Lu%20and%20Chuan%20Xiao%20and%20Yoshiharu%20Ishikawa%0AAbstract%3A%20%20%20Approximate%20nearest%20neighbor%20search%20%28ANNS%29%20in%20high-dimensional%20spaces%20is%20a%0Apivotal%20challenge%20in%20the%20field%20of%20machine%20learning.%20In%20recent%20years%2C%0Agraph-based%20methods%20have%20emerged%20as%20the%20superior%20approach%20to%20ANNS%2C%20establishing%0Aa%20new%20state%20of%20the%20art.%20Although%20various%20optimizations%20for%20graph-based%20ANNS%0Ahave%20been%20introduced%2C%20they%20predominantly%20rely%20on%20heuristic%20methods%20that%20lack%0Aformal%20theoretical%20backing.%20This%20paper%20aims%20to%20enhance%20routing%20within%0Agraph-based%20ANNS%20by%20introducing%20a%20method%20that%20offers%20a%20probabilistic%20guarantee%0Awhen%20exploring%20a%20node%27s%20neighbors%20in%20the%20graph.%20We%20formulate%20the%20problem%20as%0Aprobabilistic%20routing%20and%20develop%20two%20baseline%20strategies%20by%20incorporating%0Alocality-sensitive%20techniques.%20Subsequently%2C%20we%20introduce%20PEOs%2C%20a%20novel%0Aapproach%20that%20efficiently%20identifies%20which%20neighbors%20in%20the%20graph%20should%20be%0Aconsidered%20for%20exact%20distance%20calculation%2C%20thus%20significantly%20improving%0Aefficiency%20in%20practice.%20Our%20experiments%20demonstrate%20that%20equipping%20PEOs%20can%0Aincrease%20throughput%20on%20commonly%20utilized%20graph%20indexes%20%28HNSW%20and%20NSSG%29%20by%20a%0Afactor%20of%201.6%20to%202.5%2C%20and%20its%20efficiency%20consistently%20outperforms%20the%0Aleading-edge%20routing%20technique%20by%201.1%20to%201.4%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11354v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Routing%2520for%2520Graph-Based%2520Approximate%2520Nearest%2520Neighbor%250A%2520%2520Search%26entry.906535625%3DKejing%2520Lu%2520and%2520Chuan%2520Xiao%2520and%2520Yoshiharu%2520Ishikawa%26entry.1292438233%3D%2520%2520Approximate%2520nearest%2520neighbor%2520search%2520%2528ANNS%2529%2520in%2520high-dimensional%2520spaces%2520is%2520a%250Apivotal%2520challenge%2520in%2520the%2520field%2520of%2520machine%2520learning.%2520In%2520recent%2520years%252C%250Agraph-based%2520methods%2520have%2520emerged%2520as%2520the%2520superior%2520approach%2520to%2520ANNS%252C%2520establishing%250Aa%2520new%2520state%2520of%2520the%2520art.%2520Although%2520various%2520optimizations%2520for%2520graph-based%2520ANNS%250Ahave%2520been%2520introduced%252C%2520they%2520predominantly%2520rely%2520on%2520heuristic%2520methods%2520that%2520lack%250Aformal%2520theoretical%2520backing.%2520This%2520paper%2520aims%2520to%2520enhance%2520routing%2520within%250Agraph-based%2520ANNS%2520by%2520introducing%2520a%2520method%2520that%2520offers%2520a%2520probabilistic%2520guarantee%250Awhen%2520exploring%2520a%2520node%2527s%2520neighbors%2520in%2520the%2520graph.%2520We%2520formulate%2520the%2520problem%2520as%250Aprobabilistic%2520routing%2520and%2520develop%2520two%2520baseline%2520strategies%2520by%2520incorporating%250Alocality-sensitive%2520techniques.%2520Subsequently%252C%2520we%2520introduce%2520PEOs%252C%2520a%2520novel%250Aapproach%2520that%2520efficiently%2520identifies%2520which%2520neighbors%2520in%2520the%2520graph%2520should%2520be%250Aconsidered%2520for%2520exact%2520distance%2520calculation%252C%2520thus%2520significantly%2520improving%250Aefficiency%2520in%2520practice.%2520Our%2520experiments%2520demonstrate%2520that%2520equipping%2520PEOs%2520can%250Aincrease%2520throughput%2520on%2520commonly%2520utilized%2520graph%2520indexes%2520%2528HNSW%2520and%2520NSSG%2529%2520by%2520a%250Afactor%2520of%25201.6%2520to%25202.5%252C%2520and%2520its%2520efficiency%2520consistently%2520outperforms%2520the%250Aleading-edge%2520routing%2520technique%2520by%25201.1%2520to%25201.4%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11354v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Routing%20for%20Graph-Based%20Approximate%20Nearest%20Neighbor%0A%20%20Search&entry.906535625=Kejing%20Lu%20and%20Chuan%20Xiao%20and%20Yoshiharu%20Ishikawa&entry.1292438233=%20%20Approximate%20nearest%20neighbor%20search%20%28ANNS%29%20in%20high-dimensional%20spaces%20is%20a%0Apivotal%20challenge%20in%20the%20field%20of%20machine%20learning.%20In%20recent%20years%2C%0Agraph-based%20methods%20have%20emerged%20as%20the%20superior%20approach%20to%20ANNS%2C%20establishing%0Aa%20new%20state%20of%20the%20art.%20Although%20various%20optimizations%20for%20graph-based%20ANNS%0Ahave%20been%20introduced%2C%20they%20predominantly%20rely%20on%20heuristic%20methods%20that%20lack%0Aformal%20theoretical%20backing.%20This%20paper%20aims%20to%20enhance%20routing%20within%0Agraph-based%20ANNS%20by%20introducing%20a%20method%20that%20offers%20a%20probabilistic%20guarantee%0Awhen%20exploring%20a%20node%27s%20neighbors%20in%20the%20graph.%20We%20formulate%20the%20problem%20as%0Aprobabilistic%20routing%20and%20develop%20two%20baseline%20strategies%20by%20incorporating%0Alocality-sensitive%20techniques.%20Subsequently%2C%20we%20introduce%20PEOs%2C%20a%20novel%0Aapproach%20that%20efficiently%20identifies%20which%20neighbors%20in%20the%20graph%20should%20be%0Aconsidered%20for%20exact%20distance%20calculation%2C%20thus%20significantly%20improving%0Aefficiency%20in%20practice.%20Our%20experiments%20demonstrate%20that%20equipping%20PEOs%20can%0Aincrease%20throughput%20on%20commonly%20utilized%20graph%20indexes%20%28HNSW%20and%20NSSG%29%20by%20a%0Afactor%20of%201.6%20to%202.5%2C%20and%20its%20efficiency%20consistently%20outperforms%20the%0Aleading-edge%20routing%20technique%20by%201.1%20to%201.4%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11354v2&entry.124074799=Read"},
{"title": "CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs", "author": "Jingzhe Shi and Jialuo Li and Qinwei Ma and Zaiwen Yang and Huan Ma and Lei Li", "abstract": "  Businesses and software platforms are increasingly turning to Large Language\nModels (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance\nwith file access or as reasoning agents for customer service. However, current\nLLM-based customer service models have limited integration with customer\nprofiles and lack the operational capabilities necessary for effective service.\nMoreover, existing API integrations emphasize diversity over the precision and\nerror avoidance essential in real-world customer service scenarios. To address\nthese issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile\nin existing System), designed to: (1) efficiently utilize existing databases or\nsystems for accessing user information or interacting with these systems\nfollowing existing guidelines; (2) provide accurate and reasonable responses or\ncarry out required operations in the system while avoiding harmful operations;\nand (3) leverage a combination of small and large LLMs to achieve satisfying\nperformance at a reasonable inference cost. We introduce a practical dataset,\nthe CPHOS-dataset, which includes a database, guiding files, and QA pairs\ncollected from CPHOS, an online platform that facilitates the organization of\nsimulated Physics Olympiads for high school teachers and students. We have\nconducted extensive experiments to validate the performance of our proposed\nCHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how\nLLMs can enhance or serve as alternatives to human customer service. Code for\nour proposed architecture and dataset can be found at\n{https://github.com/JingzheShi/CHOPS}.\n", "link": "http://arxiv.org/abs/2404.01343v3", "date": "2024-07-10", "relevancy": 1.4265, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4904}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHOPS%3A%20CHat%20with%20custOmer%20Profile%20Systems%20for%20Customer%20Service%20with%20LLMs&body=Title%3A%20CHOPS%3A%20CHat%20with%20custOmer%20Profile%20Systems%20for%20Customer%20Service%20with%20LLMs%0AAuthor%3A%20Jingzhe%20Shi%20and%20Jialuo%20Li%20and%20Qinwei%20Ma%20and%20Zaiwen%20Yang%20and%20Huan%20Ma%20and%20Lei%20Li%0AAbstract%3A%20%20%20Businesses%20and%20software%20platforms%20are%20increasingly%20turning%20to%20Large%20Language%0AModels%20%28LLMs%29%20such%20as%20GPT-3.5%2C%20GPT-4%2C%20GLM-3%2C%20and%20LLaMa-2%20for%20chat%20assistance%0Awith%20file%20access%20or%20as%20reasoning%20agents%20for%20customer%20service.%20However%2C%20current%0ALLM-based%20customer%20service%20models%20have%20limited%20integration%20with%20customer%0Aprofiles%20and%20lack%20the%20operational%20capabilities%20necessary%20for%20effective%20service.%0AMoreover%2C%20existing%20API%20integrations%20emphasize%20diversity%20over%20the%20precision%20and%0Aerror%20avoidance%20essential%20in%20real-world%20customer%20service%20scenarios.%20To%20address%0Athese%20issues%2C%20we%20propose%20an%20LLM%20agent%20named%20CHOPS%20%28CHat%20with%20custOmer%20Profile%0Ain%20existing%20System%29%2C%20designed%20to%3A%20%281%29%20efficiently%20utilize%20existing%20databases%20or%0Asystems%20for%20accessing%20user%20information%20or%20interacting%20with%20these%20systems%0Afollowing%20existing%20guidelines%3B%20%282%29%20provide%20accurate%20and%20reasonable%20responses%20or%0Acarry%20out%20required%20operations%20in%20the%20system%20while%20avoiding%20harmful%20operations%3B%0Aand%20%283%29%20leverage%20a%20combination%20of%20small%20and%20large%20LLMs%20to%20achieve%20satisfying%0Aperformance%20at%20a%20reasonable%20inference%20cost.%20We%20introduce%20a%20practical%20dataset%2C%0Athe%20CPHOS-dataset%2C%20which%20includes%20a%20database%2C%20guiding%20files%2C%20and%20QA%20pairs%0Acollected%20from%20CPHOS%2C%20an%20online%20platform%20that%20facilitates%20the%20organization%20of%0Asimulated%20Physics%20Olympiads%20for%20high%20school%20teachers%20and%20students.%20We%20have%0Aconducted%20extensive%20experiments%20to%20validate%20the%20performance%20of%20our%20proposed%0ACHOPS%20architecture%20using%20the%20CPHOS-dataset%2C%20with%20the%20aim%20of%20demonstrating%20how%0ALLMs%20can%20enhance%20or%20serve%20as%20alternatives%20to%20human%20customer%20service.%20Code%20for%0Aour%20proposed%20architecture%20and%20dataset%20can%20be%20found%20at%0A%7Bhttps%3A//github.com/JingzheShi/CHOPS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01343v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHOPS%253A%2520CHat%2520with%2520custOmer%2520Profile%2520Systems%2520for%2520Customer%2520Service%2520with%2520LLMs%26entry.906535625%3DJingzhe%2520Shi%2520and%2520Jialuo%2520Li%2520and%2520Qinwei%2520Ma%2520and%2520Zaiwen%2520Yang%2520and%2520Huan%2520Ma%2520and%2520Lei%2520Li%26entry.1292438233%3D%2520%2520Businesses%2520and%2520software%2520platforms%2520are%2520increasingly%2520turning%2520to%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520such%2520as%2520GPT-3.5%252C%2520GPT-4%252C%2520GLM-3%252C%2520and%2520LLaMa-2%2520for%2520chat%2520assistance%250Awith%2520file%2520access%2520or%2520as%2520reasoning%2520agents%2520for%2520customer%2520service.%2520However%252C%2520current%250ALLM-based%2520customer%2520service%2520models%2520have%2520limited%2520integration%2520with%2520customer%250Aprofiles%2520and%2520lack%2520the%2520operational%2520capabilities%2520necessary%2520for%2520effective%2520service.%250AMoreover%252C%2520existing%2520API%2520integrations%2520emphasize%2520diversity%2520over%2520the%2520precision%2520and%250Aerror%2520avoidance%2520essential%2520in%2520real-world%2520customer%2520service%2520scenarios.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520an%2520LLM%2520agent%2520named%2520CHOPS%2520%2528CHat%2520with%2520custOmer%2520Profile%250Ain%2520existing%2520System%2529%252C%2520designed%2520to%253A%2520%25281%2529%2520efficiently%2520utilize%2520existing%2520databases%2520or%250Asystems%2520for%2520accessing%2520user%2520information%2520or%2520interacting%2520with%2520these%2520systems%250Afollowing%2520existing%2520guidelines%253B%2520%25282%2529%2520provide%2520accurate%2520and%2520reasonable%2520responses%2520or%250Acarry%2520out%2520required%2520operations%2520in%2520the%2520system%2520while%2520avoiding%2520harmful%2520operations%253B%250Aand%2520%25283%2529%2520leverage%2520a%2520combination%2520of%2520small%2520and%2520large%2520LLMs%2520to%2520achieve%2520satisfying%250Aperformance%2520at%2520a%2520reasonable%2520inference%2520cost.%2520We%2520introduce%2520a%2520practical%2520dataset%252C%250Athe%2520CPHOS-dataset%252C%2520which%2520includes%2520a%2520database%252C%2520guiding%2520files%252C%2520and%2520QA%2520pairs%250Acollected%2520from%2520CPHOS%252C%2520an%2520online%2520platform%2520that%2520facilitates%2520the%2520organization%2520of%250Asimulated%2520Physics%2520Olympiads%2520for%2520high%2520school%2520teachers%2520and%2520students.%2520We%2520have%250Aconducted%2520extensive%2520experiments%2520to%2520validate%2520the%2520performance%2520of%2520our%2520proposed%250ACHOPS%2520architecture%2520using%2520the%2520CPHOS-dataset%252C%2520with%2520the%2520aim%2520of%2520demonstrating%2520how%250ALLMs%2520can%2520enhance%2520or%2520serve%2520as%2520alternatives%2520to%2520human%2520customer%2520service.%2520Code%2520for%250Aour%2520proposed%2520architecture%2520and%2520dataset%2520can%2520be%2520found%2520at%250A%257Bhttps%253A//github.com/JingzheShi/CHOPS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01343v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHOPS%3A%20CHat%20with%20custOmer%20Profile%20Systems%20for%20Customer%20Service%20with%20LLMs&entry.906535625=Jingzhe%20Shi%20and%20Jialuo%20Li%20and%20Qinwei%20Ma%20and%20Zaiwen%20Yang%20and%20Huan%20Ma%20and%20Lei%20Li&entry.1292438233=%20%20Businesses%20and%20software%20platforms%20are%20increasingly%20turning%20to%20Large%20Language%0AModels%20%28LLMs%29%20such%20as%20GPT-3.5%2C%20GPT-4%2C%20GLM-3%2C%20and%20LLaMa-2%20for%20chat%20assistance%0Awith%20file%20access%20or%20as%20reasoning%20agents%20for%20customer%20service.%20However%2C%20current%0ALLM-based%20customer%20service%20models%20have%20limited%20integration%20with%20customer%0Aprofiles%20and%20lack%20the%20operational%20capabilities%20necessary%20for%20effective%20service.%0AMoreover%2C%20existing%20API%20integrations%20emphasize%20diversity%20over%20the%20precision%20and%0Aerror%20avoidance%20essential%20in%20real-world%20customer%20service%20scenarios.%20To%20address%0Athese%20issues%2C%20we%20propose%20an%20LLM%20agent%20named%20CHOPS%20%28CHat%20with%20custOmer%20Profile%0Ain%20existing%20System%29%2C%20designed%20to%3A%20%281%29%20efficiently%20utilize%20existing%20databases%20or%0Asystems%20for%20accessing%20user%20information%20or%20interacting%20with%20these%20systems%0Afollowing%20existing%20guidelines%3B%20%282%29%20provide%20accurate%20and%20reasonable%20responses%20or%0Acarry%20out%20required%20operations%20in%20the%20system%20while%20avoiding%20harmful%20operations%3B%0Aand%20%283%29%20leverage%20a%20combination%20of%20small%20and%20large%20LLMs%20to%20achieve%20satisfying%0Aperformance%20at%20a%20reasonable%20inference%20cost.%20We%20introduce%20a%20practical%20dataset%2C%0Athe%20CPHOS-dataset%2C%20which%20includes%20a%20database%2C%20guiding%20files%2C%20and%20QA%20pairs%0Acollected%20from%20CPHOS%2C%20an%20online%20platform%20that%20facilitates%20the%20organization%20of%0Asimulated%20Physics%20Olympiads%20for%20high%20school%20teachers%20and%20students.%20We%20have%0Aconducted%20extensive%20experiments%20to%20validate%20the%20performance%20of%20our%20proposed%0ACHOPS%20architecture%20using%20the%20CPHOS-dataset%2C%20with%20the%20aim%20of%20demonstrating%20how%0ALLMs%20can%20enhance%20or%20serve%20as%20alternatives%20to%20human%20customer%20service.%20Code%20for%0Aour%20proposed%20architecture%20and%20dataset%20can%20be%20found%20at%0A%7Bhttps%3A//github.com/JingzheShi/CHOPS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01343v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


