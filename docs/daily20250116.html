<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250115.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "3VL: Using Trees to Improve Vision-Language Models' Interpretability", "author": "Nir Yellinek and Leonid Karlinsky and Raja Giryes", "abstract": "  Vision-Language models (VLMs) have proven to be effective at aligning image\nand text representations, producing superior zero-shot results when transferred\nto many downstream tasks. However, these representations suffer from some key\nshortcomings in understanding Compositional Language Concepts (CLC), such as\nrecognizing objects' attributes, states, and relations between different\nobjects. Moreover, VLMs typically have poor interpretability, making it\nchallenging to debug and mitigate compositional-understanding failures. In this\nwork, we introduce the architecture and training technique of Tree-augmented\nVision-Language (3VL) model accompanied by our proposed Anchor inference method\nand Differential Relevance (DiRe) interpretability tool. By expanding the text\nof an arbitrary image-text pair into a hierarchical tree structure using\nlanguage analysis tools, 3VL allows the induction of this structure into the\nvisual representation learned by the model, enhancing its interpretability and\ncompositional reasoning. Additionally, we show how Anchor, a simple technique\nfor text unification, can be used to filter nuisance factors while increasing\nCLC understanding performance, e.g., on the fundamental VL-Checklist benchmark.\nWe also show how DiRe, which performs a differential comparison between VLM\nrelevancy maps, enables us to generate compelling visualizations of the reasons\nfor a model's success or failure. Our code is available at:\nhttps://github.com/niryellinek/3VL.\n", "link": "http://arxiv.org/abs/2312.17345v2", "date": "2025-01-15", "relevancy": 3.1494, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6455}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203VL%3A%20Using%20Trees%20to%20Improve%20Vision-Language%20Models%27%20Interpretability&body=Title%3A%203VL%3A%20Using%20Trees%20to%20Improve%20Vision-Language%20Models%27%20Interpretability%0AAuthor%3A%20Nir%20Yellinek%20and%20Leonid%20Karlinsky%20and%20Raja%20Giryes%0AAbstract%3A%20%20%20Vision-Language%20models%20%28VLMs%29%20have%20proven%20to%20be%20effective%20at%20aligning%20image%0Aand%20text%20representations%2C%20producing%20superior%20zero-shot%20results%20when%20transferred%0Ato%20many%20downstream%20tasks.%20However%2C%20these%20representations%20suffer%20from%20some%20key%0Ashortcomings%20in%20understanding%20Compositional%20Language%20Concepts%20%28CLC%29%2C%20such%20as%0Arecognizing%20objects%27%20attributes%2C%20states%2C%20and%20relations%20between%20different%0Aobjects.%20Moreover%2C%20VLMs%20typically%20have%20poor%20interpretability%2C%20making%20it%0Achallenging%20to%20debug%20and%20mitigate%20compositional-understanding%20failures.%20In%20this%0Awork%2C%20we%20introduce%20the%20architecture%20and%20training%20technique%20of%20Tree-augmented%0AVision-Language%20%283VL%29%20model%20accompanied%20by%20our%20proposed%20Anchor%20inference%20method%0Aand%20Differential%20Relevance%20%28DiRe%29%20interpretability%20tool.%20By%20expanding%20the%20text%0Aof%20an%20arbitrary%20image-text%20pair%20into%20a%20hierarchical%20tree%20structure%20using%0Alanguage%20analysis%20tools%2C%203VL%20allows%20the%20induction%20of%20this%20structure%20into%20the%0Avisual%20representation%20learned%20by%20the%20model%2C%20enhancing%20its%20interpretability%20and%0Acompositional%20reasoning.%20Additionally%2C%20we%20show%20how%20Anchor%2C%20a%20simple%20technique%0Afor%20text%20unification%2C%20can%20be%20used%20to%20filter%20nuisance%20factors%20while%20increasing%0ACLC%20understanding%20performance%2C%20e.g.%2C%20on%20the%20fundamental%20VL-Checklist%20benchmark.%0AWe%20also%20show%20how%20DiRe%2C%20which%20performs%20a%20differential%20comparison%20between%20VLM%0Arelevancy%20maps%2C%20enables%20us%20to%20generate%20compelling%20visualizations%20of%20the%20reasons%0Afor%20a%20model%27s%20success%20or%20failure.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/niryellinek/3VL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3VL%253A%2520Using%2520Trees%2520to%2520Improve%2520Vision-Language%2520Models%2527%2520Interpretability%26entry.906535625%3DNir%2520Yellinek%2520and%2520Leonid%2520Karlinsky%2520and%2520Raja%2520Giryes%26entry.1292438233%3D%2520%2520Vision-Language%2520models%2520%2528VLMs%2529%2520have%2520proven%2520to%2520be%2520effective%2520at%2520aligning%2520image%250Aand%2520text%2520representations%252C%2520producing%2520superior%2520zero-shot%2520results%2520when%2520transferred%250Ato%2520many%2520downstream%2520tasks.%2520However%252C%2520these%2520representations%2520suffer%2520from%2520some%2520key%250Ashortcomings%2520in%2520understanding%2520Compositional%2520Language%2520Concepts%2520%2528CLC%2529%252C%2520such%2520as%250Arecognizing%2520objects%2527%2520attributes%252C%2520states%252C%2520and%2520relations%2520between%2520different%250Aobjects.%2520Moreover%252C%2520VLMs%2520typically%2520have%2520poor%2520interpretability%252C%2520making%2520it%250Achallenging%2520to%2520debug%2520and%2520mitigate%2520compositional-understanding%2520failures.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520the%2520architecture%2520and%2520training%2520technique%2520of%2520Tree-augmented%250AVision-Language%2520%25283VL%2529%2520model%2520accompanied%2520by%2520our%2520proposed%2520Anchor%2520inference%2520method%250Aand%2520Differential%2520Relevance%2520%2528DiRe%2529%2520interpretability%2520tool.%2520By%2520expanding%2520the%2520text%250Aof%2520an%2520arbitrary%2520image-text%2520pair%2520into%2520a%2520hierarchical%2520tree%2520structure%2520using%250Alanguage%2520analysis%2520tools%252C%25203VL%2520allows%2520the%2520induction%2520of%2520this%2520structure%2520into%2520the%250Avisual%2520representation%2520learned%2520by%2520the%2520model%252C%2520enhancing%2520its%2520interpretability%2520and%250Acompositional%2520reasoning.%2520Additionally%252C%2520we%2520show%2520how%2520Anchor%252C%2520a%2520simple%2520technique%250Afor%2520text%2520unification%252C%2520can%2520be%2520used%2520to%2520filter%2520nuisance%2520factors%2520while%2520increasing%250ACLC%2520understanding%2520performance%252C%2520e.g.%252C%2520on%2520the%2520fundamental%2520VL-Checklist%2520benchmark.%250AWe%2520also%2520show%2520how%2520DiRe%252C%2520which%2520performs%2520a%2520differential%2520comparison%2520between%2520VLM%250Arelevancy%2520maps%252C%2520enables%2520us%2520to%2520generate%2520compelling%2520visualizations%2520of%2520the%2520reasons%250Afor%2520a%2520model%2527s%2520success%2520or%2520failure.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/niryellinek/3VL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.17345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3VL%3A%20Using%20Trees%20to%20Improve%20Vision-Language%20Models%27%20Interpretability&entry.906535625=Nir%20Yellinek%20and%20Leonid%20Karlinsky%20and%20Raja%20Giryes&entry.1292438233=%20%20Vision-Language%20models%20%28VLMs%29%20have%20proven%20to%20be%20effective%20at%20aligning%20image%0Aand%20text%20representations%2C%20producing%20superior%20zero-shot%20results%20when%20transferred%0Ato%20many%20downstream%20tasks.%20However%2C%20these%20representations%20suffer%20from%20some%20key%0Ashortcomings%20in%20understanding%20Compositional%20Language%20Concepts%20%28CLC%29%2C%20such%20as%0Arecognizing%20objects%27%20attributes%2C%20states%2C%20and%20relations%20between%20different%0Aobjects.%20Moreover%2C%20VLMs%20typically%20have%20poor%20interpretability%2C%20making%20it%0Achallenging%20to%20debug%20and%20mitigate%20compositional-understanding%20failures.%20In%20this%0Awork%2C%20we%20introduce%20the%20architecture%20and%20training%20technique%20of%20Tree-augmented%0AVision-Language%20%283VL%29%20model%20accompanied%20by%20our%20proposed%20Anchor%20inference%20method%0Aand%20Differential%20Relevance%20%28DiRe%29%20interpretability%20tool.%20By%20expanding%20the%20text%0Aof%20an%20arbitrary%20image-text%20pair%20into%20a%20hierarchical%20tree%20structure%20using%0Alanguage%20analysis%20tools%2C%203VL%20allows%20the%20induction%20of%20this%20structure%20into%20the%0Avisual%20representation%20learned%20by%20the%20model%2C%20enhancing%20its%20interpretability%20and%0Acompositional%20reasoning.%20Additionally%2C%20we%20show%20how%20Anchor%2C%20a%20simple%20technique%0Afor%20text%20unification%2C%20can%20be%20used%20to%20filter%20nuisance%20factors%20while%20increasing%0ACLC%20understanding%20performance%2C%20e.g.%2C%20on%20the%20fundamental%20VL-Checklist%20benchmark.%0AWe%20also%20show%20how%20DiRe%2C%20which%20performs%20a%20differential%20comparison%20between%20VLM%0Arelevancy%20maps%2C%20enables%20us%20to%20generate%20compelling%20visualizations%20of%20the%20reasons%0Afor%20a%20model%27s%20success%20or%20failure.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/niryellinek/3VL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17345v2&entry.124074799=Read"},
{"title": "A Foundation Language-Image Model of the Retina (FLAIR): Encoding Expert\n  Knowledge in Text Supervision", "author": "Julio Silva-Rodr\u00edguez and Hadi Chakor and Riadh Kobbi and Jose Dolz and Ismail Ben Ayed", "abstract": "  Foundation vision-language models are currently transforming computer vision,\nand are on the rise in medical imaging fueled by their very promising\ngeneralization capabilities. However, the initial attempts to transfer this new\nparadigm to medical imaging have shown less impressive performances than those\nobserved in other domains, due to the significant domain shift and the complex,\nexpert domain knowledge inherent to medical-imaging tasks. Motivated by the\nneed for domain-expert foundation models, we present FLAIR, a pre-trained\nvision-language model for universal retinal fundus image understanding. To this\nend, we compiled 38 open-access, mostly categorical fundus imaging datasets\nfrom various sources, with up to 101 different target conditions and 288,307\nimages. We integrate the expert's domain knowledge in the form of descriptive\ntextual prompts, during both pre-training and zero-shot inference, enhancing\nthe less-informative categorical supervision of the data. Such a textual\nexpert's knowledge, which we compiled from the relevant clinical literature and\ncommunity standards, describes the fine-grained features of the pathologies as\nwell as the hierarchies and dependencies between them. We report comprehensive\nevaluations, which illustrate the benefit of integrating expert knowledge and\nthe strong generalization capabilities of FLAIR under difficult scenarios with\ndomain shifts or unseen categories. When adapted with a lightweight linear\nprobe, FLAIR outperforms fully-trained, dataset-focused models, more so in the\nfew-shot regimes. Interestingly, FLAIR outperforms by a wide margin\nlarger-scale generalist image-language models and retina domain-specific\nself-supervised networks, which emphasizes the potential of embedding experts'\ndomain knowledge and the limitations of generalist models in medical imaging.\n", "link": "http://arxiv.org/abs/2308.07898v2", "date": "2025-01-15", "relevancy": 3.0233, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6316}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Foundation%20Language-Image%20Model%20of%20the%20Retina%20%28FLAIR%29%3A%20Encoding%20Expert%0A%20%20Knowledge%20in%20Text%20Supervision&body=Title%3A%20A%20Foundation%20Language-Image%20Model%20of%20the%20Retina%20%28FLAIR%29%3A%20Encoding%20Expert%0A%20%20Knowledge%20in%20Text%20Supervision%0AAuthor%3A%20Julio%20Silva-Rodr%C3%ADguez%20and%20Hadi%20Chakor%20and%20Riadh%20Kobbi%20and%20Jose%20Dolz%20and%20Ismail%20Ben%20Ayed%0AAbstract%3A%20%20%20Foundation%20vision-language%20models%20are%20currently%20transforming%20computer%20vision%2C%0Aand%20are%20on%20the%20rise%20in%20medical%20imaging%20fueled%20by%20their%20very%20promising%0Ageneralization%20capabilities.%20However%2C%20the%20initial%20attempts%20to%20transfer%20this%20new%0Aparadigm%20to%20medical%20imaging%20have%20shown%20less%20impressive%20performances%20than%20those%0Aobserved%20in%20other%20domains%2C%20due%20to%20the%20significant%20domain%20shift%20and%20the%20complex%2C%0Aexpert%20domain%20knowledge%20inherent%20to%20medical-imaging%20tasks.%20Motivated%20by%20the%0Aneed%20for%20domain-expert%20foundation%20models%2C%20we%20present%20FLAIR%2C%20a%20pre-trained%0Avision-language%20model%20for%20universal%20retinal%20fundus%20image%20understanding.%20To%20this%0Aend%2C%20we%20compiled%2038%20open-access%2C%20mostly%20categorical%20fundus%20imaging%20datasets%0Afrom%20various%20sources%2C%20with%20up%20to%20101%20different%20target%20conditions%20and%20288%2C307%0Aimages.%20We%20integrate%20the%20expert%27s%20domain%20knowledge%20in%20the%20form%20of%20descriptive%0Atextual%20prompts%2C%20during%20both%20pre-training%20and%20zero-shot%20inference%2C%20enhancing%0Athe%20less-informative%20categorical%20supervision%20of%20the%20data.%20Such%20a%20textual%0Aexpert%27s%20knowledge%2C%20which%20we%20compiled%20from%20the%20relevant%20clinical%20literature%20and%0Acommunity%20standards%2C%20describes%20the%20fine-grained%20features%20of%20the%20pathologies%20as%0Awell%20as%20the%20hierarchies%20and%20dependencies%20between%20them.%20We%20report%20comprehensive%0Aevaluations%2C%20which%20illustrate%20the%20benefit%20of%20integrating%20expert%20knowledge%20and%0Athe%20strong%20generalization%20capabilities%20of%20FLAIR%20under%20difficult%20scenarios%20with%0Adomain%20shifts%20or%20unseen%20categories.%20When%20adapted%20with%20a%20lightweight%20linear%0Aprobe%2C%20FLAIR%20outperforms%20fully-trained%2C%20dataset-focused%20models%2C%20more%20so%20in%20the%0Afew-shot%20regimes.%20Interestingly%2C%20FLAIR%20outperforms%20by%20a%20wide%20margin%0Alarger-scale%20generalist%20image-language%20models%20and%20retina%20domain-specific%0Aself-supervised%20networks%2C%20which%20emphasizes%20the%20potential%20of%20embedding%20experts%27%0Adomain%20knowledge%20and%20the%20limitations%20of%20generalist%20models%20in%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.07898v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Foundation%2520Language-Image%2520Model%2520of%2520the%2520Retina%2520%2528FLAIR%2529%253A%2520Encoding%2520Expert%250A%2520%2520Knowledge%2520in%2520Text%2520Supervision%26entry.906535625%3DJulio%2520Silva-Rodr%25C3%25ADguez%2520and%2520Hadi%2520Chakor%2520and%2520Riadh%2520Kobbi%2520and%2520Jose%2520Dolz%2520and%2520Ismail%2520Ben%2520Ayed%26entry.1292438233%3D%2520%2520Foundation%2520vision-language%2520models%2520are%2520currently%2520transforming%2520computer%2520vision%252C%250Aand%2520are%2520on%2520the%2520rise%2520in%2520medical%2520imaging%2520fueled%2520by%2520their%2520very%2520promising%250Ageneralization%2520capabilities.%2520However%252C%2520the%2520initial%2520attempts%2520to%2520transfer%2520this%2520new%250Aparadigm%2520to%2520medical%2520imaging%2520have%2520shown%2520less%2520impressive%2520performances%2520than%2520those%250Aobserved%2520in%2520other%2520domains%252C%2520due%2520to%2520the%2520significant%2520domain%2520shift%2520and%2520the%2520complex%252C%250Aexpert%2520domain%2520knowledge%2520inherent%2520to%2520medical-imaging%2520tasks.%2520Motivated%2520by%2520the%250Aneed%2520for%2520domain-expert%2520foundation%2520models%252C%2520we%2520present%2520FLAIR%252C%2520a%2520pre-trained%250Avision-language%2520model%2520for%2520universal%2520retinal%2520fundus%2520image%2520understanding.%2520To%2520this%250Aend%252C%2520we%2520compiled%252038%2520open-access%252C%2520mostly%2520categorical%2520fundus%2520imaging%2520datasets%250Afrom%2520various%2520sources%252C%2520with%2520up%2520to%2520101%2520different%2520target%2520conditions%2520and%2520288%252C307%250Aimages.%2520We%2520integrate%2520the%2520expert%2527s%2520domain%2520knowledge%2520in%2520the%2520form%2520of%2520descriptive%250Atextual%2520prompts%252C%2520during%2520both%2520pre-training%2520and%2520zero-shot%2520inference%252C%2520enhancing%250Athe%2520less-informative%2520categorical%2520supervision%2520of%2520the%2520data.%2520Such%2520a%2520textual%250Aexpert%2527s%2520knowledge%252C%2520which%2520we%2520compiled%2520from%2520the%2520relevant%2520clinical%2520literature%2520and%250Acommunity%2520standards%252C%2520describes%2520the%2520fine-grained%2520features%2520of%2520the%2520pathologies%2520as%250Awell%2520as%2520the%2520hierarchies%2520and%2520dependencies%2520between%2520them.%2520We%2520report%2520comprehensive%250Aevaluations%252C%2520which%2520illustrate%2520the%2520benefit%2520of%2520integrating%2520expert%2520knowledge%2520and%250Athe%2520strong%2520generalization%2520capabilities%2520of%2520FLAIR%2520under%2520difficult%2520scenarios%2520with%250Adomain%2520shifts%2520or%2520unseen%2520categories.%2520When%2520adapted%2520with%2520a%2520lightweight%2520linear%250Aprobe%252C%2520FLAIR%2520outperforms%2520fully-trained%252C%2520dataset-focused%2520models%252C%2520more%2520so%2520in%2520the%250Afew-shot%2520regimes.%2520Interestingly%252C%2520FLAIR%2520outperforms%2520by%2520a%2520wide%2520margin%250Alarger-scale%2520generalist%2520image-language%2520models%2520and%2520retina%2520domain-specific%250Aself-supervised%2520networks%252C%2520which%2520emphasizes%2520the%2520potential%2520of%2520embedding%2520experts%2527%250Adomain%2520knowledge%2520and%2520the%2520limitations%2520of%2520generalist%2520models%2520in%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.07898v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Foundation%20Language-Image%20Model%20of%20the%20Retina%20%28FLAIR%29%3A%20Encoding%20Expert%0A%20%20Knowledge%20in%20Text%20Supervision&entry.906535625=Julio%20Silva-Rodr%C3%ADguez%20and%20Hadi%20Chakor%20and%20Riadh%20Kobbi%20and%20Jose%20Dolz%20and%20Ismail%20Ben%20Ayed&entry.1292438233=%20%20Foundation%20vision-language%20models%20are%20currently%20transforming%20computer%20vision%2C%0Aand%20are%20on%20the%20rise%20in%20medical%20imaging%20fueled%20by%20their%20very%20promising%0Ageneralization%20capabilities.%20However%2C%20the%20initial%20attempts%20to%20transfer%20this%20new%0Aparadigm%20to%20medical%20imaging%20have%20shown%20less%20impressive%20performances%20than%20those%0Aobserved%20in%20other%20domains%2C%20due%20to%20the%20significant%20domain%20shift%20and%20the%20complex%2C%0Aexpert%20domain%20knowledge%20inherent%20to%20medical-imaging%20tasks.%20Motivated%20by%20the%0Aneed%20for%20domain-expert%20foundation%20models%2C%20we%20present%20FLAIR%2C%20a%20pre-trained%0Avision-language%20model%20for%20universal%20retinal%20fundus%20image%20understanding.%20To%20this%0Aend%2C%20we%20compiled%2038%20open-access%2C%20mostly%20categorical%20fundus%20imaging%20datasets%0Afrom%20various%20sources%2C%20with%20up%20to%20101%20different%20target%20conditions%20and%20288%2C307%0Aimages.%20We%20integrate%20the%20expert%27s%20domain%20knowledge%20in%20the%20form%20of%20descriptive%0Atextual%20prompts%2C%20during%20both%20pre-training%20and%20zero-shot%20inference%2C%20enhancing%0Athe%20less-informative%20categorical%20supervision%20of%20the%20data.%20Such%20a%20textual%0Aexpert%27s%20knowledge%2C%20which%20we%20compiled%20from%20the%20relevant%20clinical%20literature%20and%0Acommunity%20standards%2C%20describes%20the%20fine-grained%20features%20of%20the%20pathologies%20as%0Awell%20as%20the%20hierarchies%20and%20dependencies%20between%20them.%20We%20report%20comprehensive%0Aevaluations%2C%20which%20illustrate%20the%20benefit%20of%20integrating%20expert%20knowledge%20and%0Athe%20strong%20generalization%20capabilities%20of%20FLAIR%20under%20difficult%20scenarios%20with%0Adomain%20shifts%20or%20unseen%20categories.%20When%20adapted%20with%20a%20lightweight%20linear%0Aprobe%2C%20FLAIR%20outperforms%20fully-trained%2C%20dataset-focused%20models%2C%20more%20so%20in%20the%0Afew-shot%20regimes.%20Interestingly%2C%20FLAIR%20outperforms%20by%20a%20wide%20margin%0Alarger-scale%20generalist%20image-language%20models%20and%20retina%20domain-specific%0Aself-supervised%20networks%2C%20which%20emphasizes%20the%20potential%20of%20embedding%20experts%27%0Adomain%20knowledge%20and%20the%20limitations%20of%20generalist%20models%20in%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.07898v2&entry.124074799=Read"},
{"title": "Generative Planning with 3D-vision Language Pre-training for End-to-End\n  Autonomous Driving", "author": "Tengpeng Li and Hanli Wang and Xianfei Li and Wenlong Liao and Tao He and Pai Peng", "abstract": "  Autonomous driving is a challenging task that requires perceiving and\nunderstanding the surrounding environment for safe trajectory planning. While\nexisting vision-based end-to-end models have achieved promising results, these\nmethods are still facing the challenges of vision understanding, decision\nreasoning and scene generalization. To solve these issues, a generative\nplanning with 3D-vision language pre-training model named GPVL is proposed for\nend-to-end autonomous driving. The proposed paradigm has two significant\naspects. On one hand, a 3D-vision language pre-training module is designed to\nbridge the gap between visual perception and linguistic understanding in the\nbird's eye view. On the other hand, a cross-modal language model is introduced\nto generate holistic driving decisions and fine-grained trajectories with\nperception and navigation information in an auto-regressive manner. Experiments\non the challenging nuScenes dataset demonstrate that the proposed scheme\nachieves excellent performances compared with state-of-the-art methods.\nBesides, the proposed GPVL presents strong generalization ability and real-time\npotential when handling high-level commands in various scenarios. It is\nbelieved that the effective, robust and efficient performance of GPVL is\ncrucial for the practical application of future autonomous driving systems.\nCode is available at https://github.com/ltp1995/GPVL\n", "link": "http://arxiv.org/abs/2501.08861v1", "date": "2025-01-15", "relevancy": 3.0197, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6237}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Planning%20with%203D-vision%20Language%20Pre-training%20for%20End-to-End%0A%20%20Autonomous%20Driving&body=Title%3A%20Generative%20Planning%20with%203D-vision%20Language%20Pre-training%20for%20End-to-End%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Tengpeng%20Li%20and%20Hanli%20Wang%20and%20Xianfei%20Li%20and%20Wenlong%20Liao%20and%20Tao%20He%20and%20Pai%20Peng%0AAbstract%3A%20%20%20Autonomous%20driving%20is%20a%20challenging%20task%20that%20requires%20perceiving%20and%0Aunderstanding%20the%20surrounding%20environment%20for%20safe%20trajectory%20planning.%20While%0Aexisting%20vision-based%20end-to-end%20models%20have%20achieved%20promising%20results%2C%20these%0Amethods%20are%20still%20facing%20the%20challenges%20of%20vision%20understanding%2C%20decision%0Areasoning%20and%20scene%20generalization.%20To%20solve%20these%20issues%2C%20a%20generative%0Aplanning%20with%203D-vision%20language%20pre-training%20model%20named%20GPVL%20is%20proposed%20for%0Aend-to-end%20autonomous%20driving.%20The%20proposed%20paradigm%20has%20two%20significant%0Aaspects.%20On%20one%20hand%2C%20a%203D-vision%20language%20pre-training%20module%20is%20designed%20to%0Abridge%20the%20gap%20between%20visual%20perception%20and%20linguistic%20understanding%20in%20the%0Abird%27s%20eye%20view.%20On%20the%20other%20hand%2C%20a%20cross-modal%20language%20model%20is%20introduced%0Ato%20generate%20holistic%20driving%20decisions%20and%20fine-grained%20trajectories%20with%0Aperception%20and%20navigation%20information%20in%20an%20auto-regressive%20manner.%20Experiments%0Aon%20the%20challenging%20nuScenes%20dataset%20demonstrate%20that%20the%20proposed%20scheme%0Aachieves%20excellent%20performances%20compared%20with%20state-of-the-art%20methods.%0ABesides%2C%20the%20proposed%20GPVL%20presents%20strong%20generalization%20ability%20and%20real-time%0Apotential%20when%20handling%20high-level%20commands%20in%20various%20scenarios.%20It%20is%0Abelieved%20that%20the%20effective%2C%20robust%20and%20efficient%20performance%20of%20GPVL%20is%0Acrucial%20for%20the%20practical%20application%20of%20future%20autonomous%20driving%20systems.%0ACode%20is%20available%20at%20https%3A//github.com/ltp1995/GPVL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Planning%2520with%25203D-vision%2520Language%2520Pre-training%2520for%2520End-to-End%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DTengpeng%2520Li%2520and%2520Hanli%2520Wang%2520and%2520Xianfei%2520Li%2520and%2520Wenlong%2520Liao%2520and%2520Tao%2520He%2520and%2520Pai%2520Peng%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520is%2520a%2520challenging%2520task%2520that%2520requires%2520perceiving%2520and%250Aunderstanding%2520the%2520surrounding%2520environment%2520for%2520safe%2520trajectory%2520planning.%2520While%250Aexisting%2520vision-based%2520end-to-end%2520models%2520have%2520achieved%2520promising%2520results%252C%2520these%250Amethods%2520are%2520still%2520facing%2520the%2520challenges%2520of%2520vision%2520understanding%252C%2520decision%250Areasoning%2520and%2520scene%2520generalization.%2520To%2520solve%2520these%2520issues%252C%2520a%2520generative%250Aplanning%2520with%25203D-vision%2520language%2520pre-training%2520model%2520named%2520GPVL%2520is%2520proposed%2520for%250Aend-to-end%2520autonomous%2520driving.%2520The%2520proposed%2520paradigm%2520has%2520two%2520significant%250Aaspects.%2520On%2520one%2520hand%252C%2520a%25203D-vision%2520language%2520pre-training%2520module%2520is%2520designed%2520to%250Abridge%2520the%2520gap%2520between%2520visual%2520perception%2520and%2520linguistic%2520understanding%2520in%2520the%250Abird%2527s%2520eye%2520view.%2520On%2520the%2520other%2520hand%252C%2520a%2520cross-modal%2520language%2520model%2520is%2520introduced%250Ato%2520generate%2520holistic%2520driving%2520decisions%2520and%2520fine-grained%2520trajectories%2520with%250Aperception%2520and%2520navigation%2520information%2520in%2520an%2520auto-regressive%2520manner.%2520Experiments%250Aon%2520the%2520challenging%2520nuScenes%2520dataset%2520demonstrate%2520that%2520the%2520proposed%2520scheme%250Aachieves%2520excellent%2520performances%2520compared%2520with%2520state-of-the-art%2520methods.%250ABesides%252C%2520the%2520proposed%2520GPVL%2520presents%2520strong%2520generalization%2520ability%2520and%2520real-time%250Apotential%2520when%2520handling%2520high-level%2520commands%2520in%2520various%2520scenarios.%2520It%2520is%250Abelieved%2520that%2520the%2520effective%252C%2520robust%2520and%2520efficient%2520performance%2520of%2520GPVL%2520is%250Acrucial%2520for%2520the%2520practical%2520application%2520of%2520future%2520autonomous%2520driving%2520systems.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/ltp1995/GPVL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Planning%20with%203D-vision%20Language%20Pre-training%20for%20End-to-End%0A%20%20Autonomous%20Driving&entry.906535625=Tengpeng%20Li%20and%20Hanli%20Wang%20and%20Xianfei%20Li%20and%20Wenlong%20Liao%20and%20Tao%20He%20and%20Pai%20Peng&entry.1292438233=%20%20Autonomous%20driving%20is%20a%20challenging%20task%20that%20requires%20perceiving%20and%0Aunderstanding%20the%20surrounding%20environment%20for%20safe%20trajectory%20planning.%20While%0Aexisting%20vision-based%20end-to-end%20models%20have%20achieved%20promising%20results%2C%20these%0Amethods%20are%20still%20facing%20the%20challenges%20of%20vision%20understanding%2C%20decision%0Areasoning%20and%20scene%20generalization.%20To%20solve%20these%20issues%2C%20a%20generative%0Aplanning%20with%203D-vision%20language%20pre-training%20model%20named%20GPVL%20is%20proposed%20for%0Aend-to-end%20autonomous%20driving.%20The%20proposed%20paradigm%20has%20two%20significant%0Aaspects.%20On%20one%20hand%2C%20a%203D-vision%20language%20pre-training%20module%20is%20designed%20to%0Abridge%20the%20gap%20between%20visual%20perception%20and%20linguistic%20understanding%20in%20the%0Abird%27s%20eye%20view.%20On%20the%20other%20hand%2C%20a%20cross-modal%20language%20model%20is%20introduced%0Ato%20generate%20holistic%20driving%20decisions%20and%20fine-grained%20trajectories%20with%0Aperception%20and%20navigation%20information%20in%20an%20auto-regressive%20manner.%20Experiments%0Aon%20the%20challenging%20nuScenes%20dataset%20demonstrate%20that%20the%20proposed%20scheme%0Aachieves%20excellent%20performances%20compared%20with%20state-of-the-art%20methods.%0ABesides%2C%20the%20proposed%20GPVL%20presents%20strong%20generalization%20ability%20and%20real-time%0Apotential%20when%20handling%20high-level%20commands%20in%20various%20scenarios.%20It%20is%0Abelieved%20that%20the%20effective%2C%20robust%20and%20efficient%20performance%20of%20GPVL%20is%0Acrucial%20for%20the%20practical%20application%20of%20future%20autonomous%20driving%20systems.%0ACode%20is%20available%20at%20https%3A//github.com/ltp1995/GPVL%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08861v1&entry.124074799=Read"},
{"title": "CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes\n  with Gaussian Representation", "author": "Qi Ma and Runyi Yang and Bin Ren and Ender Konukoglu and Luc Van Gool and Danda Pani Paudel", "abstract": "  Localizing text descriptions in large-scale 3D scenes is inherently an\nambiguous task. This nonetheless arises while describing general concepts, e.g.\nall traffic lights in a city.\n  To facilitate reasoning based on such concepts, text localization in the form\nof distribution is required. In this paper, we generate the distribution of the\ncamera poses conditioned upon the textual description.\n  To facilitate such generation, we propose a diffusion-based architecture that\nconditionally diffuses the noisy 6DoF camera poses to their plausible\nlocations.\n  The conditional signals are derived from the text descriptions, using the\npre-trained text encoders. The connection between text descriptions and pose\ndistribution is established through pretrained Vision-Language-Model, i.e.\nCLIP. Furthermore, we demonstrate that the candidate poses for the distribution\ncan be further refined by rendering potential poses using 3D Gaussian\nsplatting, guiding incorrectly posed samples towards locations that better\nalign with the textual description, through visual reasoning.\n  We demonstrate the effectiveness of our method by comparing it with both\nstandard retrieval methods and learning-based approaches. Our proposed method\nconsistently outperforms these baselines across all five large-scale datasets.\nOur source code and dataset will be made publicly available.\n", "link": "http://arxiv.org/abs/2501.08982v1", "date": "2025-01-15", "relevancy": 3.0192, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6094}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6083}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CityLoc%3A%206%20DoF%20Localization%20of%20Text%20Descriptions%20in%20Large-Scale%20Scenes%0A%20%20with%20Gaussian%20Representation&body=Title%3A%20CityLoc%3A%206%20DoF%20Localization%20of%20Text%20Descriptions%20in%20Large-Scale%20Scenes%0A%20%20with%20Gaussian%20Representation%0AAuthor%3A%20Qi%20Ma%20and%20Runyi%20Yang%20and%20Bin%20Ren%20and%20Ender%20Konukoglu%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel%0AAbstract%3A%20%20%20Localizing%20text%20descriptions%20in%20large-scale%203D%20scenes%20is%20inherently%20an%0Aambiguous%20task.%20This%20nonetheless%20arises%20while%20describing%20general%20concepts%2C%20e.g.%0Aall%20traffic%20lights%20in%20a%20city.%0A%20%20To%20facilitate%20reasoning%20based%20on%20such%20concepts%2C%20text%20localization%20in%20the%20form%0Aof%20distribution%20is%20required.%20In%20this%20paper%2C%20we%20generate%20the%20distribution%20of%20the%0Acamera%20poses%20conditioned%20upon%20the%20textual%20description.%0A%20%20To%20facilitate%20such%20generation%2C%20we%20propose%20a%20diffusion-based%20architecture%20that%0Aconditionally%20diffuses%20the%20noisy%206DoF%20camera%20poses%20to%20their%20plausible%0Alocations.%0A%20%20The%20conditional%20signals%20are%20derived%20from%20the%20text%20descriptions%2C%20using%20the%0Apre-trained%20text%20encoders.%20The%20connection%20between%20text%20descriptions%20and%20pose%0Adistribution%20is%20established%20through%20pretrained%20Vision-Language-Model%2C%20i.e.%0ACLIP.%20Furthermore%2C%20we%20demonstrate%20that%20the%20candidate%20poses%20for%20the%20distribution%0Acan%20be%20further%20refined%20by%20rendering%20potential%20poses%20using%203D%20Gaussian%0Asplatting%2C%20guiding%20incorrectly%20posed%20samples%20towards%20locations%20that%20better%0Aalign%20with%20the%20textual%20description%2C%20through%20visual%20reasoning.%0A%20%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20by%20comparing%20it%20with%20both%0Astandard%20retrieval%20methods%20and%20learning-based%20approaches.%20Our%20proposed%20method%0Aconsistently%20outperforms%20these%20baselines%20across%20all%20five%20large-scale%20datasets.%0AOur%20source%20code%20and%20dataset%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCityLoc%253A%25206%2520DoF%2520Localization%2520of%2520Text%2520Descriptions%2520in%2520Large-Scale%2520Scenes%250A%2520%2520with%2520Gaussian%2520Representation%26entry.906535625%3DQi%2520Ma%2520and%2520Runyi%2520Yang%2520and%2520Bin%2520Ren%2520and%2520Ender%2520Konukoglu%2520and%2520Luc%2520Van%2520Gool%2520and%2520Danda%2520Pani%2520Paudel%26entry.1292438233%3D%2520%2520Localizing%2520text%2520descriptions%2520in%2520large-scale%25203D%2520scenes%2520is%2520inherently%2520an%250Aambiguous%2520task.%2520This%2520nonetheless%2520arises%2520while%2520describing%2520general%2520concepts%252C%2520e.g.%250Aall%2520traffic%2520lights%2520in%2520a%2520city.%250A%2520%2520To%2520facilitate%2520reasoning%2520based%2520on%2520such%2520concepts%252C%2520text%2520localization%2520in%2520the%2520form%250Aof%2520distribution%2520is%2520required.%2520In%2520this%2520paper%252C%2520we%2520generate%2520the%2520distribution%2520of%2520the%250Acamera%2520poses%2520conditioned%2520upon%2520the%2520textual%2520description.%250A%2520%2520To%2520facilitate%2520such%2520generation%252C%2520we%2520propose%2520a%2520diffusion-based%2520architecture%2520that%250Aconditionally%2520diffuses%2520the%2520noisy%25206DoF%2520camera%2520poses%2520to%2520their%2520plausible%250Alocations.%250A%2520%2520The%2520conditional%2520signals%2520are%2520derived%2520from%2520the%2520text%2520descriptions%252C%2520using%2520the%250Apre-trained%2520text%2520encoders.%2520The%2520connection%2520between%2520text%2520descriptions%2520and%2520pose%250Adistribution%2520is%2520established%2520through%2520pretrained%2520Vision-Language-Model%252C%2520i.e.%250ACLIP.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520the%2520candidate%2520poses%2520for%2520the%2520distribution%250Acan%2520be%2520further%2520refined%2520by%2520rendering%2520potential%2520poses%2520using%25203D%2520Gaussian%250Asplatting%252C%2520guiding%2520incorrectly%2520posed%2520samples%2520towards%2520locations%2520that%2520better%250Aalign%2520with%2520the%2520textual%2520description%252C%2520through%2520visual%2520reasoning.%250A%2520%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520by%2520comparing%2520it%2520with%2520both%250Astandard%2520retrieval%2520methods%2520and%2520learning-based%2520approaches.%2520Our%2520proposed%2520method%250Aconsistently%2520outperforms%2520these%2520baselines%2520across%2520all%2520five%2520large-scale%2520datasets.%250AOur%2520source%2520code%2520and%2520dataset%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CityLoc%3A%206%20DoF%20Localization%20of%20Text%20Descriptions%20in%20Large-Scale%20Scenes%0A%20%20with%20Gaussian%20Representation&entry.906535625=Qi%20Ma%20and%20Runyi%20Yang%20and%20Bin%20Ren%20and%20Ender%20Konukoglu%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel&entry.1292438233=%20%20Localizing%20text%20descriptions%20in%20large-scale%203D%20scenes%20is%20inherently%20an%0Aambiguous%20task.%20This%20nonetheless%20arises%20while%20describing%20general%20concepts%2C%20e.g.%0Aall%20traffic%20lights%20in%20a%20city.%0A%20%20To%20facilitate%20reasoning%20based%20on%20such%20concepts%2C%20text%20localization%20in%20the%20form%0Aof%20distribution%20is%20required.%20In%20this%20paper%2C%20we%20generate%20the%20distribution%20of%20the%0Acamera%20poses%20conditioned%20upon%20the%20textual%20description.%0A%20%20To%20facilitate%20such%20generation%2C%20we%20propose%20a%20diffusion-based%20architecture%20that%0Aconditionally%20diffuses%20the%20noisy%206DoF%20camera%20poses%20to%20their%20plausible%0Alocations.%0A%20%20The%20conditional%20signals%20are%20derived%20from%20the%20text%20descriptions%2C%20using%20the%0Apre-trained%20text%20encoders.%20The%20connection%20between%20text%20descriptions%20and%20pose%0Adistribution%20is%20established%20through%20pretrained%20Vision-Language-Model%2C%20i.e.%0ACLIP.%20Furthermore%2C%20we%20demonstrate%20that%20the%20candidate%20poses%20for%20the%20distribution%0Acan%20be%20further%20refined%20by%20rendering%20potential%20poses%20using%203D%20Gaussian%0Asplatting%2C%20guiding%20incorrectly%20posed%20samples%20towards%20locations%20that%20better%0Aalign%20with%20the%20textual%20description%2C%20through%20visual%20reasoning.%0A%20%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20by%20comparing%20it%20with%20both%0Astandard%20retrieval%20methods%20and%20learning-based%20approaches.%20Our%20proposed%20method%0Aconsistently%20outperforms%20these%20baselines%20across%20all%20five%20large-scale%20datasets.%0AOur%20source%20code%20and%20dataset%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08982v1&entry.124074799=Read"},
{"title": "Vision Foundation Models for Computed Tomography", "author": "Suraj Pai and Ibrahim Hadzic and Dennis Bontempi and Keno Bressem and Benjamin H. Kann and Andriy Fedorov and Raymond H. Mak and Hugo J. W. L. Aerts", "abstract": "  Foundation models (FMs) have shown transformative potential in radiology by\nperforming diverse, complex tasks across imaging modalities. Here, we developed\nCT-FM, a large-scale 3D image-based pre-trained model designed explicitly for\nvarious radiological tasks. CT-FM was pre-trained using 148,000 computed\ntomography (CT) scans from the Imaging Data Commons through label-agnostic\ncontrastive learning. We evaluated CT-FM across four categories of tasks,\nnamely, whole-body and tumor segmentation, head CT triage, medical image\nretrieval, and semantic understanding, showing superior performance against\nstate-of-the-art models. Beyond quantitative success, CT-FM demonstrated the\nability to cluster regions anatomically and identify similar anatomical and\nstructural concepts across scans. Furthermore, it remained robust across\ntest-retest settings and indicated reasonable salient regions attached to its\nembeddings. This study demonstrates the value of large-scale medical imaging\nfoundation models and by open-sourcing the model weights, code, and data, aims\nto support more adaptable, reliable, and interpretable AI solutions in\nradiology.\n", "link": "http://arxiv.org/abs/2501.09001v1", "date": "2025-01-15", "relevancy": 2.8796, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Foundation%20Models%20for%20Computed%20Tomography&body=Title%3A%20Vision%20Foundation%20Models%20for%20Computed%20Tomography%0AAuthor%3A%20Suraj%20Pai%20and%20Ibrahim%20Hadzic%20and%20Dennis%20Bontempi%20and%20Keno%20Bressem%20and%20Benjamin%20H.%20Kann%20and%20Andriy%20Fedorov%20and%20Raymond%20H.%20Mak%20and%20Hugo%20J.%20W.%20L.%20Aerts%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20have%20shown%20transformative%20potential%20in%20radiology%20by%0Aperforming%20diverse%2C%20complex%20tasks%20across%20imaging%20modalities.%20Here%2C%20we%20developed%0ACT-FM%2C%20a%20large-scale%203D%20image-based%20pre-trained%20model%20designed%20explicitly%20for%0Avarious%20radiological%20tasks.%20CT-FM%20was%20pre-trained%20using%20148%2C000%20computed%0Atomography%20%28CT%29%20scans%20from%20the%20Imaging%20Data%20Commons%20through%20label-agnostic%0Acontrastive%20learning.%20We%20evaluated%20CT-FM%20across%20four%20categories%20of%20tasks%2C%0Anamely%2C%20whole-body%20and%20tumor%20segmentation%2C%20head%20CT%20triage%2C%20medical%20image%0Aretrieval%2C%20and%20semantic%20understanding%2C%20showing%20superior%20performance%20against%0Astate-of-the-art%20models.%20Beyond%20quantitative%20success%2C%20CT-FM%20demonstrated%20the%0Aability%20to%20cluster%20regions%20anatomically%20and%20identify%20similar%20anatomical%20and%0Astructural%20concepts%20across%20scans.%20Furthermore%2C%20it%20remained%20robust%20across%0Atest-retest%20settings%20and%20indicated%20reasonable%20salient%20regions%20attached%20to%20its%0Aembeddings.%20This%20study%20demonstrates%20the%20value%20of%20large-scale%20medical%20imaging%0Afoundation%20models%20and%20by%20open-sourcing%20the%20model%20weights%2C%20code%2C%20and%20data%2C%20aims%0Ato%20support%20more%20adaptable%2C%20reliable%2C%20and%20interpretable%20AI%20solutions%20in%0Aradiology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Foundation%2520Models%2520for%2520Computed%2520Tomography%26entry.906535625%3DSuraj%2520Pai%2520and%2520Ibrahim%2520Hadzic%2520and%2520Dennis%2520Bontempi%2520and%2520Keno%2520Bressem%2520and%2520Benjamin%2520H.%2520Kann%2520and%2520Andriy%2520Fedorov%2520and%2520Raymond%2520H.%2520Mak%2520and%2520Hugo%2520J.%2520W.%2520L.%2520Aerts%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520have%2520shown%2520transformative%2520potential%2520in%2520radiology%2520by%250Aperforming%2520diverse%252C%2520complex%2520tasks%2520across%2520imaging%2520modalities.%2520Here%252C%2520we%2520developed%250ACT-FM%252C%2520a%2520large-scale%25203D%2520image-based%2520pre-trained%2520model%2520designed%2520explicitly%2520for%250Avarious%2520radiological%2520tasks.%2520CT-FM%2520was%2520pre-trained%2520using%2520148%252C000%2520computed%250Atomography%2520%2528CT%2529%2520scans%2520from%2520the%2520Imaging%2520Data%2520Commons%2520through%2520label-agnostic%250Acontrastive%2520learning.%2520We%2520evaluated%2520CT-FM%2520across%2520four%2520categories%2520of%2520tasks%252C%250Anamely%252C%2520whole-body%2520and%2520tumor%2520segmentation%252C%2520head%2520CT%2520triage%252C%2520medical%2520image%250Aretrieval%252C%2520and%2520semantic%2520understanding%252C%2520showing%2520superior%2520performance%2520against%250Astate-of-the-art%2520models.%2520Beyond%2520quantitative%2520success%252C%2520CT-FM%2520demonstrated%2520the%250Aability%2520to%2520cluster%2520regions%2520anatomically%2520and%2520identify%2520similar%2520anatomical%2520and%250Astructural%2520concepts%2520across%2520scans.%2520Furthermore%252C%2520it%2520remained%2520robust%2520across%250Atest-retest%2520settings%2520and%2520indicated%2520reasonable%2520salient%2520regions%2520attached%2520to%2520its%250Aembeddings.%2520This%2520study%2520demonstrates%2520the%2520value%2520of%2520large-scale%2520medical%2520imaging%250Afoundation%2520models%2520and%2520by%2520open-sourcing%2520the%2520model%2520weights%252C%2520code%252C%2520and%2520data%252C%2520aims%250Ato%2520support%2520more%2520adaptable%252C%2520reliable%252C%2520and%2520interpretable%2520AI%2520solutions%2520in%250Aradiology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Foundation%20Models%20for%20Computed%20Tomography&entry.906535625=Suraj%20Pai%20and%20Ibrahim%20Hadzic%20and%20Dennis%20Bontempi%20and%20Keno%20Bressem%20and%20Benjamin%20H.%20Kann%20and%20Andriy%20Fedorov%20and%20Raymond%20H.%20Mak%20and%20Hugo%20J.%20W.%20L.%20Aerts&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20have%20shown%20transformative%20potential%20in%20radiology%20by%0Aperforming%20diverse%2C%20complex%20tasks%20across%20imaging%20modalities.%20Here%2C%20we%20developed%0ACT-FM%2C%20a%20large-scale%203D%20image-based%20pre-trained%20model%20designed%20explicitly%20for%0Avarious%20radiological%20tasks.%20CT-FM%20was%20pre-trained%20using%20148%2C000%20computed%0Atomography%20%28CT%29%20scans%20from%20the%20Imaging%20Data%20Commons%20through%20label-agnostic%0Acontrastive%20learning.%20We%20evaluated%20CT-FM%20across%20four%20categories%20of%20tasks%2C%0Anamely%2C%20whole-body%20and%20tumor%20segmentation%2C%20head%20CT%20triage%2C%20medical%20image%0Aretrieval%2C%20and%20semantic%20understanding%2C%20showing%20superior%20performance%20against%0Astate-of-the-art%20models.%20Beyond%20quantitative%20success%2C%20CT-FM%20demonstrated%20the%0Aability%20to%20cluster%20regions%20anatomically%20and%20identify%20similar%20anatomical%20and%0Astructural%20concepts%20across%20scans.%20Furthermore%2C%20it%20remained%20robust%20across%0Atest-retest%20settings%20and%20indicated%20reasonable%20salient%20regions%20attached%20to%20its%0Aembeddings.%20This%20study%20demonstrates%20the%20value%20of%20large-scale%20medical%20imaging%0Afoundation%20models%20and%20by%20open-sourcing%20the%20model%20weights%2C%20code%2C%20and%20data%2C%20aims%0Ato%20support%20more%20adaptable%2C%20reliable%2C%20and%20interpretable%20AI%20solutions%20in%0Aradiology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09001v1&entry.124074799=Read"},
{"title": "Incrementally Learning Multiple Diverse Data Domains via Multi-Source\n  Dynamic Expansion Model", "author": "Runqing Wu and Fei Ye and Qihe Liu and Guoxi Huang and Jinyu Guo and Rongyao Hu", "abstract": "  Continual Learning seeks to develop a model capable of incrementally\nassimilating new information while retaining prior knowledge. However, current\nresearch predominantly addresses a straightforward learning context, wherein\nall data samples originate from a singular data domain. This paper shifts focus\nto a more complex and realistic learning environment, characterized by data\nsamples sourced from multiple distinct domains. We tackle this intricate\nlearning challenge by introducing a novel methodology, termed the Multi-Source\nDynamic Expansion Model (MSDEM), which leverages various pre-trained models as\nbackbones and progressively establishes new experts based on them to adapt to\nemerging tasks. Additionally, we propose an innovative dynamic expandable\nattention mechanism designed to selectively harness knowledge from multiple\nbackbones, thereby accelerating the new task learning. Moreover, we introduce a\ndynamic graph weight router that strategically reuses all previously acquired\nparameters and representations for new task learning, maximizing the positive\nknowledge transfer effect, which further improves generalization performance.\nWe conduct a comprehensive series of experiments, and the empirical findings\nindicate that our proposed approach achieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2501.08878v1", "date": "2025-01-15", "relevancy": 2.8745, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5879}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incrementally%20Learning%20Multiple%20Diverse%20Data%20Domains%20via%20Multi-Source%0A%20%20Dynamic%20Expansion%20Model&body=Title%3A%20Incrementally%20Learning%20Multiple%20Diverse%20Data%20Domains%20via%20Multi-Source%0A%20%20Dynamic%20Expansion%20Model%0AAuthor%3A%20Runqing%20Wu%20and%20Fei%20Ye%20and%20Qihe%20Liu%20and%20Guoxi%20Huang%20and%20Jinyu%20Guo%20and%20Rongyao%20Hu%0AAbstract%3A%20%20%20Continual%20Learning%20seeks%20to%20develop%20a%20model%20capable%20of%20incrementally%0Aassimilating%20new%20information%20while%20retaining%20prior%20knowledge.%20However%2C%20current%0Aresearch%20predominantly%20addresses%20a%20straightforward%20learning%20context%2C%20wherein%0Aall%20data%20samples%20originate%20from%20a%20singular%20data%20domain.%20This%20paper%20shifts%20focus%0Ato%20a%20more%20complex%20and%20realistic%20learning%20environment%2C%20characterized%20by%20data%0Asamples%20sourced%20from%20multiple%20distinct%20domains.%20We%20tackle%20this%20intricate%0Alearning%20challenge%20by%20introducing%20a%20novel%20methodology%2C%20termed%20the%20Multi-Source%0ADynamic%20Expansion%20Model%20%28MSDEM%29%2C%20which%20leverages%20various%20pre-trained%20models%20as%0Abackbones%20and%20progressively%20establishes%20new%20experts%20based%20on%20them%20to%20adapt%20to%0Aemerging%20tasks.%20Additionally%2C%20we%20propose%20an%20innovative%20dynamic%20expandable%0Aattention%20mechanism%20designed%20to%20selectively%20harness%20knowledge%20from%20multiple%0Abackbones%2C%20thereby%20accelerating%20the%20new%20task%20learning.%20Moreover%2C%20we%20introduce%20a%0Adynamic%20graph%20weight%20router%20that%20strategically%20reuses%20all%20previously%20acquired%0Aparameters%20and%20representations%20for%20new%20task%20learning%2C%20maximizing%20the%20positive%0Aknowledge%20transfer%20effect%2C%20which%20further%20improves%20generalization%20performance.%0AWe%20conduct%20a%20comprehensive%20series%20of%20experiments%2C%20and%20the%20empirical%20findings%0Aindicate%20that%20our%20proposed%20approach%20achieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncrementally%2520Learning%2520Multiple%2520Diverse%2520Data%2520Domains%2520via%2520Multi-Source%250A%2520%2520Dynamic%2520Expansion%2520Model%26entry.906535625%3DRunqing%2520Wu%2520and%2520Fei%2520Ye%2520and%2520Qihe%2520Liu%2520and%2520Guoxi%2520Huang%2520and%2520Jinyu%2520Guo%2520and%2520Rongyao%2520Hu%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520seeks%2520to%2520develop%2520a%2520model%2520capable%2520of%2520incrementally%250Aassimilating%2520new%2520information%2520while%2520retaining%2520prior%2520knowledge.%2520However%252C%2520current%250Aresearch%2520predominantly%2520addresses%2520a%2520straightforward%2520learning%2520context%252C%2520wherein%250Aall%2520data%2520samples%2520originate%2520from%2520a%2520singular%2520data%2520domain.%2520This%2520paper%2520shifts%2520focus%250Ato%2520a%2520more%2520complex%2520and%2520realistic%2520learning%2520environment%252C%2520characterized%2520by%2520data%250Asamples%2520sourced%2520from%2520multiple%2520distinct%2520domains.%2520We%2520tackle%2520this%2520intricate%250Alearning%2520challenge%2520by%2520introducing%2520a%2520novel%2520methodology%252C%2520termed%2520the%2520Multi-Source%250ADynamic%2520Expansion%2520Model%2520%2528MSDEM%2529%252C%2520which%2520leverages%2520various%2520pre-trained%2520models%2520as%250Abackbones%2520and%2520progressively%2520establishes%2520new%2520experts%2520based%2520on%2520them%2520to%2520adapt%2520to%250Aemerging%2520tasks.%2520Additionally%252C%2520we%2520propose%2520an%2520innovative%2520dynamic%2520expandable%250Aattention%2520mechanism%2520designed%2520to%2520selectively%2520harness%2520knowledge%2520from%2520multiple%250Abackbones%252C%2520thereby%2520accelerating%2520the%2520new%2520task%2520learning.%2520Moreover%252C%2520we%2520introduce%2520a%250Adynamic%2520graph%2520weight%2520router%2520that%2520strategically%2520reuses%2520all%2520previously%2520acquired%250Aparameters%2520and%2520representations%2520for%2520new%2520task%2520learning%252C%2520maximizing%2520the%2520positive%250Aknowledge%2520transfer%2520effect%252C%2520which%2520further%2520improves%2520generalization%2520performance.%250AWe%2520conduct%2520a%2520comprehensive%2520series%2520of%2520experiments%252C%2520and%2520the%2520empirical%2520findings%250Aindicate%2520that%2520our%2520proposed%2520approach%2520achieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incrementally%20Learning%20Multiple%20Diverse%20Data%20Domains%20via%20Multi-Source%0A%20%20Dynamic%20Expansion%20Model&entry.906535625=Runqing%20Wu%20and%20Fei%20Ye%20and%20Qihe%20Liu%20and%20Guoxi%20Huang%20and%20Jinyu%20Guo%20and%20Rongyao%20Hu&entry.1292438233=%20%20Continual%20Learning%20seeks%20to%20develop%20a%20model%20capable%20of%20incrementally%0Aassimilating%20new%20information%20while%20retaining%20prior%20knowledge.%20However%2C%20current%0Aresearch%20predominantly%20addresses%20a%20straightforward%20learning%20context%2C%20wherein%0Aall%20data%20samples%20originate%20from%20a%20singular%20data%20domain.%20This%20paper%20shifts%20focus%0Ato%20a%20more%20complex%20and%20realistic%20learning%20environment%2C%20characterized%20by%20data%0Asamples%20sourced%20from%20multiple%20distinct%20domains.%20We%20tackle%20this%20intricate%0Alearning%20challenge%20by%20introducing%20a%20novel%20methodology%2C%20termed%20the%20Multi-Source%0ADynamic%20Expansion%20Model%20%28MSDEM%29%2C%20which%20leverages%20various%20pre-trained%20models%20as%0Abackbones%20and%20progressively%20establishes%20new%20experts%20based%20on%20them%20to%20adapt%20to%0Aemerging%20tasks.%20Additionally%2C%20we%20propose%20an%20innovative%20dynamic%20expandable%0Aattention%20mechanism%20designed%20to%20selectively%20harness%20knowledge%20from%20multiple%0Abackbones%2C%20thereby%20accelerating%20the%20new%20task%20learning.%20Moreover%2C%20we%20introduce%20a%0Adynamic%20graph%20weight%20router%20that%20strategically%20reuses%20all%20previously%20acquired%0Aparameters%20and%20representations%20for%20new%20task%20learning%2C%20maximizing%20the%20positive%0Aknowledge%20transfer%20effect%2C%20which%20further%20improves%20generalization%20performance.%0AWe%20conduct%20a%20comprehensive%20series%20of%20experiments%2C%20and%20the%20empirical%20findings%0Aindicate%20that%20our%20proposed%20approach%20achieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08878v1&entry.124074799=Read"},
{"title": "Self-supervised Transformation Learning for Equivariant Representations", "author": "Jaemyung Yu and Jaehyun Choi and Dong-Jae Lee and HyeongGwon Hong and Junmo Kim", "abstract": "  Unsupervised representation learning has significantly advanced various\nmachine learning tasks. In the computer vision domain, state-of-the-art\napproaches utilize transformations like random crop and color jitter to achieve\ninvariant representations, embedding semantically the same inputs despite\ntransformations. However, this can degrade performance in tasks requiring\nprecise features, such as localization or flower classification. To address\nthis, recent research incorporates equivariant representation learning, which\ncaptures transformation-sensitive information. However, current methods depend\non transformation labels and thus struggle with interdependency and complex\ntransformations. We propose Self-supervised Transformation Learning (STL),\nreplacing transformation labels with transformation representations derived\nfrom image pairs. The proposed method ensures transformation representation is\nimage-invariant and learns corresponding equivariant transformations, enhancing\nperformance without increased batch complexity. We demonstrate the approach's\neffectiveness across diverse classification and detection tasks, outperforming\nexisting methods in 7 out of 11 benchmarks and excelling in detection. By\nintegrating complex transformations like AugMix, unusable by prior equivariant\nmethods, this approach enhances performance across tasks, underscoring its\nadaptability and resilience. Additionally, its compatibility with various base\nmodels highlights its flexibility and broad applicability. The code is\navailable at https://github.com/jaemyung-u/stl.\n", "link": "http://arxiv.org/abs/2501.08712v1", "date": "2025-01-15", "relevancy": 2.8091, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5771}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5573}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Transformation%20Learning%20for%20Equivariant%20Representations&body=Title%3A%20Self-supervised%20Transformation%20Learning%20for%20Equivariant%20Representations%0AAuthor%3A%20Jaemyung%20Yu%20and%20Jaehyun%20Choi%20and%20Dong-Jae%20Lee%20and%20HyeongGwon%20Hong%20and%20Junmo%20Kim%0AAbstract%3A%20%20%20Unsupervised%20representation%20learning%20has%20significantly%20advanced%20various%0Amachine%20learning%20tasks.%20In%20the%20computer%20vision%20domain%2C%20state-of-the-art%0Aapproaches%20utilize%20transformations%20like%20random%20crop%20and%20color%20jitter%20to%20achieve%0Ainvariant%20representations%2C%20embedding%20semantically%20the%20same%20inputs%20despite%0Atransformations.%20However%2C%20this%20can%20degrade%20performance%20in%20tasks%20requiring%0Aprecise%20features%2C%20such%20as%20localization%20or%20flower%20classification.%20To%20address%0Athis%2C%20recent%20research%20incorporates%20equivariant%20representation%20learning%2C%20which%0Acaptures%20transformation-sensitive%20information.%20However%2C%20current%20methods%20depend%0Aon%20transformation%20labels%20and%20thus%20struggle%20with%20interdependency%20and%20complex%0Atransformations.%20We%20propose%20Self-supervised%20Transformation%20Learning%20%28STL%29%2C%0Areplacing%20transformation%20labels%20with%20transformation%20representations%20derived%0Afrom%20image%20pairs.%20The%20proposed%20method%20ensures%20transformation%20representation%20is%0Aimage-invariant%20and%20learns%20corresponding%20equivariant%20transformations%2C%20enhancing%0Aperformance%20without%20increased%20batch%20complexity.%20We%20demonstrate%20the%20approach%27s%0Aeffectiveness%20across%20diverse%20classification%20and%20detection%20tasks%2C%20outperforming%0Aexisting%20methods%20in%207%20out%20of%2011%20benchmarks%20and%20excelling%20in%20detection.%20By%0Aintegrating%20complex%20transformations%20like%20AugMix%2C%20unusable%20by%20prior%20equivariant%0Amethods%2C%20this%20approach%20enhances%20performance%20across%20tasks%2C%20underscoring%20its%0Aadaptability%20and%20resilience.%20Additionally%2C%20its%20compatibility%20with%20various%20base%0Amodels%20highlights%20its%20flexibility%20and%20broad%20applicability.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/jaemyung-u/stl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Transformation%2520Learning%2520for%2520Equivariant%2520Representations%26entry.906535625%3DJaemyung%2520Yu%2520and%2520Jaehyun%2520Choi%2520and%2520Dong-Jae%2520Lee%2520and%2520HyeongGwon%2520Hong%2520and%2520Junmo%2520Kim%26entry.1292438233%3D%2520%2520Unsupervised%2520representation%2520learning%2520has%2520significantly%2520advanced%2520various%250Amachine%2520learning%2520tasks.%2520In%2520the%2520computer%2520vision%2520domain%252C%2520state-of-the-art%250Aapproaches%2520utilize%2520transformations%2520like%2520random%2520crop%2520and%2520color%2520jitter%2520to%2520achieve%250Ainvariant%2520representations%252C%2520embedding%2520semantically%2520the%2520same%2520inputs%2520despite%250Atransformations.%2520However%252C%2520this%2520can%2520degrade%2520performance%2520in%2520tasks%2520requiring%250Aprecise%2520features%252C%2520such%2520as%2520localization%2520or%2520flower%2520classification.%2520To%2520address%250Athis%252C%2520recent%2520research%2520incorporates%2520equivariant%2520representation%2520learning%252C%2520which%250Acaptures%2520transformation-sensitive%2520information.%2520However%252C%2520current%2520methods%2520depend%250Aon%2520transformation%2520labels%2520and%2520thus%2520struggle%2520with%2520interdependency%2520and%2520complex%250Atransformations.%2520We%2520propose%2520Self-supervised%2520Transformation%2520Learning%2520%2528STL%2529%252C%250Areplacing%2520transformation%2520labels%2520with%2520transformation%2520representations%2520derived%250Afrom%2520image%2520pairs.%2520The%2520proposed%2520method%2520ensures%2520transformation%2520representation%2520is%250Aimage-invariant%2520and%2520learns%2520corresponding%2520equivariant%2520transformations%252C%2520enhancing%250Aperformance%2520without%2520increased%2520batch%2520complexity.%2520We%2520demonstrate%2520the%2520approach%2527s%250Aeffectiveness%2520across%2520diverse%2520classification%2520and%2520detection%2520tasks%252C%2520outperforming%250Aexisting%2520methods%2520in%25207%2520out%2520of%252011%2520benchmarks%2520and%2520excelling%2520in%2520detection.%2520By%250Aintegrating%2520complex%2520transformations%2520like%2520AugMix%252C%2520unusable%2520by%2520prior%2520equivariant%250Amethods%252C%2520this%2520approach%2520enhances%2520performance%2520across%2520tasks%252C%2520underscoring%2520its%250Aadaptability%2520and%2520resilience.%2520Additionally%252C%2520its%2520compatibility%2520with%2520various%2520base%250Amodels%2520highlights%2520its%2520flexibility%2520and%2520broad%2520applicability.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/jaemyung-u/stl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Transformation%20Learning%20for%20Equivariant%20Representations&entry.906535625=Jaemyung%20Yu%20and%20Jaehyun%20Choi%20and%20Dong-Jae%20Lee%20and%20HyeongGwon%20Hong%20and%20Junmo%20Kim&entry.1292438233=%20%20Unsupervised%20representation%20learning%20has%20significantly%20advanced%20various%0Amachine%20learning%20tasks.%20In%20the%20computer%20vision%20domain%2C%20state-of-the-art%0Aapproaches%20utilize%20transformations%20like%20random%20crop%20and%20color%20jitter%20to%20achieve%0Ainvariant%20representations%2C%20embedding%20semantically%20the%20same%20inputs%20despite%0Atransformations.%20However%2C%20this%20can%20degrade%20performance%20in%20tasks%20requiring%0Aprecise%20features%2C%20such%20as%20localization%20or%20flower%20classification.%20To%20address%0Athis%2C%20recent%20research%20incorporates%20equivariant%20representation%20learning%2C%20which%0Acaptures%20transformation-sensitive%20information.%20However%2C%20current%20methods%20depend%0Aon%20transformation%20labels%20and%20thus%20struggle%20with%20interdependency%20and%20complex%0Atransformations.%20We%20propose%20Self-supervised%20Transformation%20Learning%20%28STL%29%2C%0Areplacing%20transformation%20labels%20with%20transformation%20representations%20derived%0Afrom%20image%20pairs.%20The%20proposed%20method%20ensures%20transformation%20representation%20is%0Aimage-invariant%20and%20learns%20corresponding%20equivariant%20transformations%2C%20enhancing%0Aperformance%20without%20increased%20batch%20complexity.%20We%20demonstrate%20the%20approach%27s%0Aeffectiveness%20across%20diverse%20classification%20and%20detection%20tasks%2C%20outperforming%0Aexisting%20methods%20in%207%20out%20of%2011%20benchmarks%20and%20excelling%20in%20detection.%20By%0Aintegrating%20complex%20transformations%20like%20AugMix%2C%20unusable%20by%20prior%20equivariant%0Amethods%2C%20this%20approach%20enhances%20performance%20across%20tasks%2C%20underscoring%20its%0Aadaptability%20and%20resilience.%20Additionally%2C%20its%20compatibility%20with%20various%20base%0Amodels%20highlights%20its%20flexibility%20and%20broad%20applicability.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/jaemyung-u/stl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08712v1&entry.124074799=Read"},
{"title": "Multi-visual modality micro drone-based structural damage detection", "author": "Isaac Osei Agyemanga and Liaoyuan Zeng and Jianwen Chena and Isaac Adjei-Mensah and Daniel Acheampong", "abstract": "  Accurate detection and resilience of object detectors in structural damage\ndetection are important in ensuring the continuous use of civil infrastructure.\nHowever, achieving robustness in object detectors remains a persistent\nchallenge, impacting their ability to generalize effectively. This study\nproposes DetectorX, a robust framework for structural damage detection coupled\nwith a micro drone. DetectorX addresses the challenges of object detector\nrobustness by incorporating two innovative modules: a stem block and a spiral\npooling technique. The stem block introduces a dynamic visual modality by\nleveraging the outputs of two Deep Convolutional Neural Network (DCNN) models.\nThe framework employs the proposed event-based reward reinforcement learning to\nconstrain the actions of a parent and child DCNN model leading to a reward.\nThis results in the induction of two dynamic visual modalities alongside the\nRed, Green, and Blue (RGB) data. This enhancement significantly augments\nDetectorX's perception and adaptability in diverse environmental situations.\nFurther, a spiral pooling technique, an online image augmentation method,\nstrengthens the framework by increasing feature representations by\nconcatenating spiraled and average/max pooled features. In three extensive\nexperiments: (1) comparative and (2) robustness, which use the Pacific\nEarthquake Engineering Research Hub ImageNet dataset, and (3) field-experiment,\nDetectorX performed satisfactorily across varying metrics, including precision\n(0.88), recall (0.84), average precision (0.91), mean average precision (0.76),\nand mean average recall (0.73), compared to the competing detectors including\nYou Only Look Once X-medium (YOLOX-m) and others. The study's findings indicate\nthat DetectorX can provide satisfactory results and demonstrate resilience in\nchallenging environments.\n", "link": "http://arxiv.org/abs/2501.08807v1", "date": "2025-01-15", "relevancy": 2.7859, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-visual%20modality%20micro%20drone-based%20structural%20damage%20detection&body=Title%3A%20Multi-visual%20modality%20micro%20drone-based%20structural%20damage%20detection%0AAuthor%3A%20Isaac%20Osei%20Agyemanga%20and%20Liaoyuan%20Zeng%20and%20Jianwen%20Chena%20and%20Isaac%20Adjei-Mensah%20and%20Daniel%20Acheampong%0AAbstract%3A%20%20%20Accurate%20detection%20and%20resilience%20of%20object%20detectors%20in%20structural%20damage%0Adetection%20are%20important%20in%20ensuring%20the%20continuous%20use%20of%20civil%20infrastructure.%0AHowever%2C%20achieving%20robustness%20in%20object%20detectors%20remains%20a%20persistent%0Achallenge%2C%20impacting%20their%20ability%20to%20generalize%20effectively.%20This%20study%0Aproposes%20DetectorX%2C%20a%20robust%20framework%20for%20structural%20damage%20detection%20coupled%0Awith%20a%20micro%20drone.%20DetectorX%20addresses%20the%20challenges%20of%20object%20detector%0Arobustness%20by%20incorporating%20two%20innovative%20modules%3A%20a%20stem%20block%20and%20a%20spiral%0Apooling%20technique.%20The%20stem%20block%20introduces%20a%20dynamic%20visual%20modality%20by%0Aleveraging%20the%20outputs%20of%20two%20Deep%20Convolutional%20Neural%20Network%20%28DCNN%29%20models.%0AThe%20framework%20employs%20the%20proposed%20event-based%20reward%20reinforcement%20learning%20to%0Aconstrain%20the%20actions%20of%20a%20parent%20and%20child%20DCNN%20model%20leading%20to%20a%20reward.%0AThis%20results%20in%20the%20induction%20of%20two%20dynamic%20visual%20modalities%20alongside%20the%0ARed%2C%20Green%2C%20and%20Blue%20%28RGB%29%20data.%20This%20enhancement%20significantly%20augments%0ADetectorX%27s%20perception%20and%20adaptability%20in%20diverse%20environmental%20situations.%0AFurther%2C%20a%20spiral%20pooling%20technique%2C%20an%20online%20image%20augmentation%20method%2C%0Astrengthens%20the%20framework%20by%20increasing%20feature%20representations%20by%0Aconcatenating%20spiraled%20and%20average/max%20pooled%20features.%20In%20three%20extensive%0Aexperiments%3A%20%281%29%20comparative%20and%20%282%29%20robustness%2C%20which%20use%20the%20Pacific%0AEarthquake%20Engineering%20Research%20Hub%20ImageNet%20dataset%2C%20and%20%283%29%20field-experiment%2C%0ADetectorX%20performed%20satisfactorily%20across%20varying%20metrics%2C%20including%20precision%0A%280.88%29%2C%20recall%20%280.84%29%2C%20average%20precision%20%280.91%29%2C%20mean%20average%20precision%20%280.76%29%2C%0Aand%20mean%20average%20recall%20%280.73%29%2C%20compared%20to%20the%20competing%20detectors%20including%0AYou%20Only%20Look%20Once%20X-medium%20%28YOLOX-m%29%20and%20others.%20The%20study%27s%20findings%20indicate%0Athat%20DetectorX%20can%20provide%20satisfactory%20results%20and%20demonstrate%20resilience%20in%0Achallenging%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-visual%2520modality%2520micro%2520drone-based%2520structural%2520damage%2520detection%26entry.906535625%3DIsaac%2520Osei%2520Agyemanga%2520and%2520Liaoyuan%2520Zeng%2520and%2520Jianwen%2520Chena%2520and%2520Isaac%2520Adjei-Mensah%2520and%2520Daniel%2520Acheampong%26entry.1292438233%3D%2520%2520Accurate%2520detection%2520and%2520resilience%2520of%2520object%2520detectors%2520in%2520structural%2520damage%250Adetection%2520are%2520important%2520in%2520ensuring%2520the%2520continuous%2520use%2520of%2520civil%2520infrastructure.%250AHowever%252C%2520achieving%2520robustness%2520in%2520object%2520detectors%2520remains%2520a%2520persistent%250Achallenge%252C%2520impacting%2520their%2520ability%2520to%2520generalize%2520effectively.%2520This%2520study%250Aproposes%2520DetectorX%252C%2520a%2520robust%2520framework%2520for%2520structural%2520damage%2520detection%2520coupled%250Awith%2520a%2520micro%2520drone.%2520DetectorX%2520addresses%2520the%2520challenges%2520of%2520object%2520detector%250Arobustness%2520by%2520incorporating%2520two%2520innovative%2520modules%253A%2520a%2520stem%2520block%2520and%2520a%2520spiral%250Apooling%2520technique.%2520The%2520stem%2520block%2520introduces%2520a%2520dynamic%2520visual%2520modality%2520by%250Aleveraging%2520the%2520outputs%2520of%2520two%2520Deep%2520Convolutional%2520Neural%2520Network%2520%2528DCNN%2529%2520models.%250AThe%2520framework%2520employs%2520the%2520proposed%2520event-based%2520reward%2520reinforcement%2520learning%2520to%250Aconstrain%2520the%2520actions%2520of%2520a%2520parent%2520and%2520child%2520DCNN%2520model%2520leading%2520to%2520a%2520reward.%250AThis%2520results%2520in%2520the%2520induction%2520of%2520two%2520dynamic%2520visual%2520modalities%2520alongside%2520the%250ARed%252C%2520Green%252C%2520and%2520Blue%2520%2528RGB%2529%2520data.%2520This%2520enhancement%2520significantly%2520augments%250ADetectorX%2527s%2520perception%2520and%2520adaptability%2520in%2520diverse%2520environmental%2520situations.%250AFurther%252C%2520a%2520spiral%2520pooling%2520technique%252C%2520an%2520online%2520image%2520augmentation%2520method%252C%250Astrengthens%2520the%2520framework%2520by%2520increasing%2520feature%2520representations%2520by%250Aconcatenating%2520spiraled%2520and%2520average/max%2520pooled%2520features.%2520In%2520three%2520extensive%250Aexperiments%253A%2520%25281%2529%2520comparative%2520and%2520%25282%2529%2520robustness%252C%2520which%2520use%2520the%2520Pacific%250AEarthquake%2520Engineering%2520Research%2520Hub%2520ImageNet%2520dataset%252C%2520and%2520%25283%2529%2520field-experiment%252C%250ADetectorX%2520performed%2520satisfactorily%2520across%2520varying%2520metrics%252C%2520including%2520precision%250A%25280.88%2529%252C%2520recall%2520%25280.84%2529%252C%2520average%2520precision%2520%25280.91%2529%252C%2520mean%2520average%2520precision%2520%25280.76%2529%252C%250Aand%2520mean%2520average%2520recall%2520%25280.73%2529%252C%2520compared%2520to%2520the%2520competing%2520detectors%2520including%250AYou%2520Only%2520Look%2520Once%2520X-medium%2520%2528YOLOX-m%2529%2520and%2520others.%2520The%2520study%2527s%2520findings%2520indicate%250Athat%2520DetectorX%2520can%2520provide%2520satisfactory%2520results%2520and%2520demonstrate%2520resilience%2520in%250Achallenging%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-visual%20modality%20micro%20drone-based%20structural%20damage%20detection&entry.906535625=Isaac%20Osei%20Agyemanga%20and%20Liaoyuan%20Zeng%20and%20Jianwen%20Chena%20and%20Isaac%20Adjei-Mensah%20and%20Daniel%20Acheampong&entry.1292438233=%20%20Accurate%20detection%20and%20resilience%20of%20object%20detectors%20in%20structural%20damage%0Adetection%20are%20important%20in%20ensuring%20the%20continuous%20use%20of%20civil%20infrastructure.%0AHowever%2C%20achieving%20robustness%20in%20object%20detectors%20remains%20a%20persistent%0Achallenge%2C%20impacting%20their%20ability%20to%20generalize%20effectively.%20This%20study%0Aproposes%20DetectorX%2C%20a%20robust%20framework%20for%20structural%20damage%20detection%20coupled%0Awith%20a%20micro%20drone.%20DetectorX%20addresses%20the%20challenges%20of%20object%20detector%0Arobustness%20by%20incorporating%20two%20innovative%20modules%3A%20a%20stem%20block%20and%20a%20spiral%0Apooling%20technique.%20The%20stem%20block%20introduces%20a%20dynamic%20visual%20modality%20by%0Aleveraging%20the%20outputs%20of%20two%20Deep%20Convolutional%20Neural%20Network%20%28DCNN%29%20models.%0AThe%20framework%20employs%20the%20proposed%20event-based%20reward%20reinforcement%20learning%20to%0Aconstrain%20the%20actions%20of%20a%20parent%20and%20child%20DCNN%20model%20leading%20to%20a%20reward.%0AThis%20results%20in%20the%20induction%20of%20two%20dynamic%20visual%20modalities%20alongside%20the%0ARed%2C%20Green%2C%20and%20Blue%20%28RGB%29%20data.%20This%20enhancement%20significantly%20augments%0ADetectorX%27s%20perception%20and%20adaptability%20in%20diverse%20environmental%20situations.%0AFurther%2C%20a%20spiral%20pooling%20technique%2C%20an%20online%20image%20augmentation%20method%2C%0Astrengthens%20the%20framework%20by%20increasing%20feature%20representations%20by%0Aconcatenating%20spiraled%20and%20average/max%20pooled%20features.%20In%20three%20extensive%0Aexperiments%3A%20%281%29%20comparative%20and%20%282%29%20robustness%2C%20which%20use%20the%20Pacific%0AEarthquake%20Engineering%20Research%20Hub%20ImageNet%20dataset%2C%20and%20%283%29%20field-experiment%2C%0ADetectorX%20performed%20satisfactorily%20across%20varying%20metrics%2C%20including%20precision%0A%280.88%29%2C%20recall%20%280.84%29%2C%20average%20precision%20%280.91%29%2C%20mean%20average%20precision%20%280.76%29%2C%0Aand%20mean%20average%20recall%20%280.73%29%2C%20compared%20to%20the%20competing%20detectors%20including%0AYou%20Only%20Look%20Once%20X-medium%20%28YOLOX-m%29%20and%20others.%20The%20study%27s%20findings%20indicate%0Athat%20DetectorX%20can%20provide%20satisfactory%20results%20and%20demonstrate%20resilience%20in%0Achallenging%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08807v1&entry.124074799=Read"},
{"title": "Human Pose-Constrained UV Map Estimation", "author": "Matej Suchanek and Miroslav Purkrabek and Jiri Matas", "abstract": "  UV map estimation is used in computer vision for detailed analysis of human\nposture or activity. Previous methods assign pixels to body model vertices by\ncomparing pixel descriptors independently, without enforcing global coherence\nor plausibility in the UV map. We propose Pose-Constrained Continuous Surface\nEmbeddings (PC-CSE), which integrates estimated 2D human pose into the\npixel-to-vertex assignment process. The pose provides global anatomical\nconstraints, ensuring that UV maps remain coherent while preserving local\nprecision. Evaluation on DensePose COCO demonstrates consistent improvement,\nregardless of the chosen 2D human pose model. Whole-body poses offer better\nconstraints by incorporating additional details about the hands and feet.\nConditioning UV maps with human pose reduces invalid mappings and enhances\nanatomical plausibility. In addition, we highlight inconsistencies in the\nground-truth annotations.\n", "link": "http://arxiv.org/abs/2501.08815v1", "date": "2025-01-15", "relevancy": 2.7716, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5567}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5566}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Pose-Constrained%20UV%20Map%20Estimation&body=Title%3A%20Human%20Pose-Constrained%20UV%20Map%20Estimation%0AAuthor%3A%20Matej%20Suchanek%20and%20Miroslav%20Purkrabek%20and%20Jiri%20Matas%0AAbstract%3A%20%20%20UV%20map%20estimation%20is%20used%20in%20computer%20vision%20for%20detailed%20analysis%20of%20human%0Aposture%20or%20activity.%20Previous%20methods%20assign%20pixels%20to%20body%20model%20vertices%20by%0Acomparing%20pixel%20descriptors%20independently%2C%20without%20enforcing%20global%20coherence%0Aor%20plausibility%20in%20the%20UV%20map.%20We%20propose%20Pose-Constrained%20Continuous%20Surface%0AEmbeddings%20%28PC-CSE%29%2C%20which%20integrates%20estimated%202D%20human%20pose%20into%20the%0Apixel-to-vertex%20assignment%20process.%20The%20pose%20provides%20global%20anatomical%0Aconstraints%2C%20ensuring%20that%20UV%20maps%20remain%20coherent%20while%20preserving%20local%0Aprecision.%20Evaluation%20on%20DensePose%20COCO%20demonstrates%20consistent%20improvement%2C%0Aregardless%20of%20the%20chosen%202D%20human%20pose%20model.%20Whole-body%20poses%20offer%20better%0Aconstraints%20by%20incorporating%20additional%20details%20about%20the%20hands%20and%20feet.%0AConditioning%20UV%20maps%20with%20human%20pose%20reduces%20invalid%20mappings%20and%20enhances%0Aanatomical%20plausibility.%20In%20addition%2C%20we%20highlight%20inconsistencies%20in%20the%0Aground-truth%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Pose-Constrained%2520UV%2520Map%2520Estimation%26entry.906535625%3DMatej%2520Suchanek%2520and%2520Miroslav%2520Purkrabek%2520and%2520Jiri%2520Matas%26entry.1292438233%3D%2520%2520UV%2520map%2520estimation%2520is%2520used%2520in%2520computer%2520vision%2520for%2520detailed%2520analysis%2520of%2520human%250Aposture%2520or%2520activity.%2520Previous%2520methods%2520assign%2520pixels%2520to%2520body%2520model%2520vertices%2520by%250Acomparing%2520pixel%2520descriptors%2520independently%252C%2520without%2520enforcing%2520global%2520coherence%250Aor%2520plausibility%2520in%2520the%2520UV%2520map.%2520We%2520propose%2520Pose-Constrained%2520Continuous%2520Surface%250AEmbeddings%2520%2528PC-CSE%2529%252C%2520which%2520integrates%2520estimated%25202D%2520human%2520pose%2520into%2520the%250Apixel-to-vertex%2520assignment%2520process.%2520The%2520pose%2520provides%2520global%2520anatomical%250Aconstraints%252C%2520ensuring%2520that%2520UV%2520maps%2520remain%2520coherent%2520while%2520preserving%2520local%250Aprecision.%2520Evaluation%2520on%2520DensePose%2520COCO%2520demonstrates%2520consistent%2520improvement%252C%250Aregardless%2520of%2520the%2520chosen%25202D%2520human%2520pose%2520model.%2520Whole-body%2520poses%2520offer%2520better%250Aconstraints%2520by%2520incorporating%2520additional%2520details%2520about%2520the%2520hands%2520and%2520feet.%250AConditioning%2520UV%2520maps%2520with%2520human%2520pose%2520reduces%2520invalid%2520mappings%2520and%2520enhances%250Aanatomical%2520plausibility.%2520In%2520addition%252C%2520we%2520highlight%2520inconsistencies%2520in%2520the%250Aground-truth%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Pose-Constrained%20UV%20Map%20Estimation&entry.906535625=Matej%20Suchanek%20and%20Miroslav%20Purkrabek%20and%20Jiri%20Matas&entry.1292438233=%20%20UV%20map%20estimation%20is%20used%20in%20computer%20vision%20for%20detailed%20analysis%20of%20human%0Aposture%20or%20activity.%20Previous%20methods%20assign%20pixels%20to%20body%20model%20vertices%20by%0Acomparing%20pixel%20descriptors%20independently%2C%20without%20enforcing%20global%20coherence%0Aor%20plausibility%20in%20the%20UV%20map.%20We%20propose%20Pose-Constrained%20Continuous%20Surface%0AEmbeddings%20%28PC-CSE%29%2C%20which%20integrates%20estimated%202D%20human%20pose%20into%20the%0Apixel-to-vertex%20assignment%20process.%20The%20pose%20provides%20global%20anatomical%0Aconstraints%2C%20ensuring%20that%20UV%20maps%20remain%20coherent%20while%20preserving%20local%0Aprecision.%20Evaluation%20on%20DensePose%20COCO%20demonstrates%20consistent%20improvement%2C%0Aregardless%20of%20the%20chosen%202D%20human%20pose%20model.%20Whole-body%20poses%20offer%20better%0Aconstraints%20by%20incorporating%20additional%20details%20about%20the%20hands%20and%20feet.%0AConditioning%20UV%20maps%20with%20human%20pose%20reduces%20invalid%20mappings%20and%20enhances%0Aanatomical%20plausibility.%20In%20addition%2C%20we%20highlight%20inconsistencies%20in%20the%0Aground-truth%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08815v1&entry.124074799=Read"},
{"title": "Ouroboros-Diffusion: Exploring Consistent Content Generation in\n  Tuning-free Long Video Diffusion", "author": "Jingyuan Chen and Fuchen Long and Jie An and Zhaofan Qiu and Ting Yao and Jiebo Luo and Tao Mei", "abstract": "  The first-in-first-out (FIFO) video diffusion, built on a pre-trained\ntext-to-video model, has recently emerged as an effective approach for\ntuning-free long video generation. This technique maintains a queue of video\nframes with progressively increasing noise, continuously producing clean frames\nat the queue's head while Gaussian noise is enqueued at the tail. However,\nFIFO-Diffusion often struggles to keep long-range temporal consistency in the\ngenerated videos due to the lack of correspondence modeling across frames. In\nthis paper, we propose Ouroboros-Diffusion, a novel video denoising framework\ndesigned to enhance structural and content (subject) consistency, enabling the\ngeneration of consistent videos of arbitrary length. Specifically, we introduce\na new latent sampling technique at the queue tail to improve structural\nconsistency, ensuring perceptually smooth transitions among frames. To enhance\nsubject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA)\nmechanism, which aligns subjects across frames within short segments to achieve\nbetter visual coherence. Furthermore, we introduce self-recurrent guidance.\nThis technique leverages information from all previous cleaner frames at the\nfront of the queue to guide the denoising of noisier frames at the end,\nfostering rich and contextual global information interaction. Extensive\nexperiments of long video generation on the VBench benchmark demonstrate the\nsuperiority of our Ouroboros-Diffusion, particularly in terms of subject\nconsistency, motion smoothness, and temporal consistency.\n", "link": "http://arxiv.org/abs/2501.09019v1", "date": "2025-01-15", "relevancy": 2.6603, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6995}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6406}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ouroboros-Diffusion%3A%20Exploring%20Consistent%20Content%20Generation%20in%0A%20%20Tuning-free%20Long%20Video%20Diffusion&body=Title%3A%20Ouroboros-Diffusion%3A%20Exploring%20Consistent%20Content%20Generation%20in%0A%20%20Tuning-free%20Long%20Video%20Diffusion%0AAuthor%3A%20Jingyuan%20Chen%20and%20Fuchen%20Long%20and%20Jie%20An%20and%20Zhaofan%20Qiu%20and%20Ting%20Yao%20and%20Jiebo%20Luo%20and%20Tao%20Mei%0AAbstract%3A%20%20%20The%20first-in-first-out%20%28FIFO%29%20video%20diffusion%2C%20built%20on%20a%20pre-trained%0Atext-to-video%20model%2C%20has%20recently%20emerged%20as%20an%20effective%20approach%20for%0Atuning-free%20long%20video%20generation.%20This%20technique%20maintains%20a%20queue%20of%20video%0Aframes%20with%20progressively%20increasing%20noise%2C%20continuously%20producing%20clean%20frames%0Aat%20the%20queue%27s%20head%20while%20Gaussian%20noise%20is%20enqueued%20at%20the%20tail.%20However%2C%0AFIFO-Diffusion%20often%20struggles%20to%20keep%20long-range%20temporal%20consistency%20in%20the%0Agenerated%20videos%20due%20to%20the%20lack%20of%20correspondence%20modeling%20across%20frames.%20In%0Athis%20paper%2C%20we%20propose%20Ouroboros-Diffusion%2C%20a%20novel%20video%20denoising%20framework%0Adesigned%20to%20enhance%20structural%20and%20content%20%28subject%29%20consistency%2C%20enabling%20the%0Ageneration%20of%20consistent%20videos%20of%20arbitrary%20length.%20Specifically%2C%20we%20introduce%0Aa%20new%20latent%20sampling%20technique%20at%20the%20queue%20tail%20to%20improve%20structural%0Aconsistency%2C%20ensuring%20perceptually%20smooth%20transitions%20among%20frames.%20To%20enhance%0Asubject%20consistency%2C%20we%20devise%20a%20Subject-Aware%20Cross-Frame%20Attention%20%28SACFA%29%0Amechanism%2C%20which%20aligns%20subjects%20across%20frames%20within%20short%20segments%20to%20achieve%0Abetter%20visual%20coherence.%20Furthermore%2C%20we%20introduce%20self-recurrent%20guidance.%0AThis%20technique%20leverages%20information%20from%20all%20previous%20cleaner%20frames%20at%20the%0Afront%20of%20the%20queue%20to%20guide%20the%20denoising%20of%20noisier%20frames%20at%20the%20end%2C%0Afostering%20rich%20and%20contextual%20global%20information%20interaction.%20Extensive%0Aexperiments%20of%20long%20video%20generation%20on%20the%20VBench%20benchmark%20demonstrate%20the%0Asuperiority%20of%20our%20Ouroboros-Diffusion%2C%20particularly%20in%20terms%20of%20subject%0Aconsistency%2C%20motion%20smoothness%2C%20and%20temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOuroboros-Diffusion%253A%2520Exploring%2520Consistent%2520Content%2520Generation%2520in%250A%2520%2520Tuning-free%2520Long%2520Video%2520Diffusion%26entry.906535625%3DJingyuan%2520Chen%2520and%2520Fuchen%2520Long%2520and%2520Jie%2520An%2520and%2520Zhaofan%2520Qiu%2520and%2520Ting%2520Yao%2520and%2520Jiebo%2520Luo%2520and%2520Tao%2520Mei%26entry.1292438233%3D%2520%2520The%2520first-in-first-out%2520%2528FIFO%2529%2520video%2520diffusion%252C%2520built%2520on%2520a%2520pre-trained%250Atext-to-video%2520model%252C%2520has%2520recently%2520emerged%2520as%2520an%2520effective%2520approach%2520for%250Atuning-free%2520long%2520video%2520generation.%2520This%2520technique%2520maintains%2520a%2520queue%2520of%2520video%250Aframes%2520with%2520progressively%2520increasing%2520noise%252C%2520continuously%2520producing%2520clean%2520frames%250Aat%2520the%2520queue%2527s%2520head%2520while%2520Gaussian%2520noise%2520is%2520enqueued%2520at%2520the%2520tail.%2520However%252C%250AFIFO-Diffusion%2520often%2520struggles%2520to%2520keep%2520long-range%2520temporal%2520consistency%2520in%2520the%250Agenerated%2520videos%2520due%2520to%2520the%2520lack%2520of%2520correspondence%2520modeling%2520across%2520frames.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520Ouroboros-Diffusion%252C%2520a%2520novel%2520video%2520denoising%2520framework%250Adesigned%2520to%2520enhance%2520structural%2520and%2520content%2520%2528subject%2529%2520consistency%252C%2520enabling%2520the%250Ageneration%2520of%2520consistent%2520videos%2520of%2520arbitrary%2520length.%2520Specifically%252C%2520we%2520introduce%250Aa%2520new%2520latent%2520sampling%2520technique%2520at%2520the%2520queue%2520tail%2520to%2520improve%2520structural%250Aconsistency%252C%2520ensuring%2520perceptually%2520smooth%2520transitions%2520among%2520frames.%2520To%2520enhance%250Asubject%2520consistency%252C%2520we%2520devise%2520a%2520Subject-Aware%2520Cross-Frame%2520Attention%2520%2528SACFA%2529%250Amechanism%252C%2520which%2520aligns%2520subjects%2520across%2520frames%2520within%2520short%2520segments%2520to%2520achieve%250Abetter%2520visual%2520coherence.%2520Furthermore%252C%2520we%2520introduce%2520self-recurrent%2520guidance.%250AThis%2520technique%2520leverages%2520information%2520from%2520all%2520previous%2520cleaner%2520frames%2520at%2520the%250Afront%2520of%2520the%2520queue%2520to%2520guide%2520the%2520denoising%2520of%2520noisier%2520frames%2520at%2520the%2520end%252C%250Afostering%2520rich%2520and%2520contextual%2520global%2520information%2520interaction.%2520Extensive%250Aexperiments%2520of%2520long%2520video%2520generation%2520on%2520the%2520VBench%2520benchmark%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520Ouroboros-Diffusion%252C%2520particularly%2520in%2520terms%2520of%2520subject%250Aconsistency%252C%2520motion%2520smoothness%252C%2520and%2520temporal%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ouroboros-Diffusion%3A%20Exploring%20Consistent%20Content%20Generation%20in%0A%20%20Tuning-free%20Long%20Video%20Diffusion&entry.906535625=Jingyuan%20Chen%20and%20Fuchen%20Long%20and%20Jie%20An%20and%20Zhaofan%20Qiu%20and%20Ting%20Yao%20and%20Jiebo%20Luo%20and%20Tao%20Mei&entry.1292438233=%20%20The%20first-in-first-out%20%28FIFO%29%20video%20diffusion%2C%20built%20on%20a%20pre-trained%0Atext-to-video%20model%2C%20has%20recently%20emerged%20as%20an%20effective%20approach%20for%0Atuning-free%20long%20video%20generation.%20This%20technique%20maintains%20a%20queue%20of%20video%0Aframes%20with%20progressively%20increasing%20noise%2C%20continuously%20producing%20clean%20frames%0Aat%20the%20queue%27s%20head%20while%20Gaussian%20noise%20is%20enqueued%20at%20the%20tail.%20However%2C%0AFIFO-Diffusion%20often%20struggles%20to%20keep%20long-range%20temporal%20consistency%20in%20the%0Agenerated%20videos%20due%20to%20the%20lack%20of%20correspondence%20modeling%20across%20frames.%20In%0Athis%20paper%2C%20we%20propose%20Ouroboros-Diffusion%2C%20a%20novel%20video%20denoising%20framework%0Adesigned%20to%20enhance%20structural%20and%20content%20%28subject%29%20consistency%2C%20enabling%20the%0Ageneration%20of%20consistent%20videos%20of%20arbitrary%20length.%20Specifically%2C%20we%20introduce%0Aa%20new%20latent%20sampling%20technique%20at%20the%20queue%20tail%20to%20improve%20structural%0Aconsistency%2C%20ensuring%20perceptually%20smooth%20transitions%20among%20frames.%20To%20enhance%0Asubject%20consistency%2C%20we%20devise%20a%20Subject-Aware%20Cross-Frame%20Attention%20%28SACFA%29%0Amechanism%2C%20which%20aligns%20subjects%20across%20frames%20within%20short%20segments%20to%20achieve%0Abetter%20visual%20coherence.%20Furthermore%2C%20we%20introduce%20self-recurrent%20guidance.%0AThis%20technique%20leverages%20information%20from%20all%20previous%20cleaner%20frames%20at%20the%0Afront%20of%20the%20queue%20to%20guide%20the%20denoising%20of%20noisier%20frames%20at%20the%20end%2C%0Afostering%20rich%20and%20contextual%20global%20information%20interaction.%20Extensive%0Aexperiments%20of%20long%20video%20generation%20on%20the%20VBench%20benchmark%20demonstrate%20the%0Asuperiority%20of%20our%20Ouroboros-Diffusion%2C%20particularly%20in%20terms%20of%20subject%0Aconsistency%2C%20motion%20smoothness%2C%20and%20temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09019v1&entry.124074799=Read"},
{"title": "$\\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding\n  and Embedding", "author": "Tianru Zhang and Li Ju and Prashant Singh and Salman Toor", "abstract": "  Analyzing large-scale datasets, especially involving complex and\nhigh-dimensional data like images, is particularly challenging. While\nself-supervised learning (SSL) has proven effective for learning\nrepresentations from unlabelled data, it typically focuses on flat,\nnon-hierarchical structures, missing the multi-level relationships present in\nmany real-world datasets. Hierarchical clustering (HC) can uncover these\nrelationships by organizing data into a tree-like structure, but it often\nrelies on rigid similarity metrics that struggle to capture the complexity of\ndiverse data types. To address these we envision $\\texttt{InfoHier}$, a\nframework that combines SSL with HC to jointly learn robust latent\nrepresentations and hierarchical structures. This approach leverages SSL to\nprovide adaptive representations, enhancing HC's ability to capture complex\npatterns. Simultaneously, it integrates HC loss to refine SSL training,\nresulting in representations that are more attuned to the underlying\ninformation hierarchy. $\\texttt{InfoHier}$ has the potential to improve the\nexpressiveness and performance of both clustering and representation learning,\noffering significant benefits for data analysis, management, and information\nretrieval.\n", "link": "http://arxiv.org/abs/2501.08717v1", "date": "2025-01-15", "relevancy": 2.6283, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Ctexttt%7BInfoHier%7D%24%3A%20Hierarchical%20Information%20Extraction%20via%20Encoding%0A%20%20and%20Embedding&body=Title%3A%20%24%5Ctexttt%7BInfoHier%7D%24%3A%20Hierarchical%20Information%20Extraction%20via%20Encoding%0A%20%20and%20Embedding%0AAuthor%3A%20Tianru%20Zhang%20and%20Li%20Ju%20and%20Prashant%20Singh%20and%20Salman%20Toor%0AAbstract%3A%20%20%20Analyzing%20large-scale%20datasets%2C%20especially%20involving%20complex%20and%0Ahigh-dimensional%20data%20like%20images%2C%20is%20particularly%20challenging.%20While%0Aself-supervised%20learning%20%28SSL%29%20has%20proven%20effective%20for%20learning%0Arepresentations%20from%20unlabelled%20data%2C%20it%20typically%20focuses%20on%20flat%2C%0Anon-hierarchical%20structures%2C%20missing%20the%20multi-level%20relationships%20present%20in%0Amany%20real-world%20datasets.%20Hierarchical%20clustering%20%28HC%29%20can%20uncover%20these%0Arelationships%20by%20organizing%20data%20into%20a%20tree-like%20structure%2C%20but%20it%20often%0Arelies%20on%20rigid%20similarity%20metrics%20that%20struggle%20to%20capture%20the%20complexity%20of%0Adiverse%20data%20types.%20To%20address%20these%20we%20envision%20%24%5Ctexttt%7BInfoHier%7D%24%2C%20a%0Aframework%20that%20combines%20SSL%20with%20HC%20to%20jointly%20learn%20robust%20latent%0Arepresentations%20and%20hierarchical%20structures.%20This%20approach%20leverages%20SSL%20to%0Aprovide%20adaptive%20representations%2C%20enhancing%20HC%27s%20ability%20to%20capture%20complex%0Apatterns.%20Simultaneously%2C%20it%20integrates%20HC%20loss%20to%20refine%20SSL%20training%2C%0Aresulting%20in%20representations%20that%20are%20more%20attuned%20to%20the%20underlying%0Ainformation%20hierarchy.%20%24%5Ctexttt%7BInfoHier%7D%24%20has%20the%20potential%20to%20improve%20the%0Aexpressiveness%20and%20performance%20of%20both%20clustering%20and%20representation%20learning%2C%0Aoffering%20significant%20benefits%20for%20data%20analysis%2C%20management%2C%20and%20information%0Aretrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Ctexttt%257BInfoHier%257D%2524%253A%2520Hierarchical%2520Information%2520Extraction%2520via%2520Encoding%250A%2520%2520and%2520Embedding%26entry.906535625%3DTianru%2520Zhang%2520and%2520Li%2520Ju%2520and%2520Prashant%2520Singh%2520and%2520Salman%2520Toor%26entry.1292438233%3D%2520%2520Analyzing%2520large-scale%2520datasets%252C%2520especially%2520involving%2520complex%2520and%250Ahigh-dimensional%2520data%2520like%2520images%252C%2520is%2520particularly%2520challenging.%2520While%250Aself-supervised%2520learning%2520%2528SSL%2529%2520has%2520proven%2520effective%2520for%2520learning%250Arepresentations%2520from%2520unlabelled%2520data%252C%2520it%2520typically%2520focuses%2520on%2520flat%252C%250Anon-hierarchical%2520structures%252C%2520missing%2520the%2520multi-level%2520relationships%2520present%2520in%250Amany%2520real-world%2520datasets.%2520Hierarchical%2520clustering%2520%2528HC%2529%2520can%2520uncover%2520these%250Arelationships%2520by%2520organizing%2520data%2520into%2520a%2520tree-like%2520structure%252C%2520but%2520it%2520often%250Arelies%2520on%2520rigid%2520similarity%2520metrics%2520that%2520struggle%2520to%2520capture%2520the%2520complexity%2520of%250Adiverse%2520data%2520types.%2520To%2520address%2520these%2520we%2520envision%2520%2524%255Ctexttt%257BInfoHier%257D%2524%252C%2520a%250Aframework%2520that%2520combines%2520SSL%2520with%2520HC%2520to%2520jointly%2520learn%2520robust%2520latent%250Arepresentations%2520and%2520hierarchical%2520structures.%2520This%2520approach%2520leverages%2520SSL%2520to%250Aprovide%2520adaptive%2520representations%252C%2520enhancing%2520HC%2527s%2520ability%2520to%2520capture%2520complex%250Apatterns.%2520Simultaneously%252C%2520it%2520integrates%2520HC%2520loss%2520to%2520refine%2520SSL%2520training%252C%250Aresulting%2520in%2520representations%2520that%2520are%2520more%2520attuned%2520to%2520the%2520underlying%250Ainformation%2520hierarchy.%2520%2524%255Ctexttt%257BInfoHier%257D%2524%2520has%2520the%2520potential%2520to%2520improve%2520the%250Aexpressiveness%2520and%2520performance%2520of%2520both%2520clustering%2520and%2520representation%2520learning%252C%250Aoffering%2520significant%2520benefits%2520for%2520data%2520analysis%252C%2520management%252C%2520and%2520information%250Aretrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Ctexttt%7BInfoHier%7D%24%3A%20Hierarchical%20Information%20Extraction%20via%20Encoding%0A%20%20and%20Embedding&entry.906535625=Tianru%20Zhang%20and%20Li%20Ju%20and%20Prashant%20Singh%20and%20Salman%20Toor&entry.1292438233=%20%20Analyzing%20large-scale%20datasets%2C%20especially%20involving%20complex%20and%0Ahigh-dimensional%20data%20like%20images%2C%20is%20particularly%20challenging.%20While%0Aself-supervised%20learning%20%28SSL%29%20has%20proven%20effective%20for%20learning%0Arepresentations%20from%20unlabelled%20data%2C%20it%20typically%20focuses%20on%20flat%2C%0Anon-hierarchical%20structures%2C%20missing%20the%20multi-level%20relationships%20present%20in%0Amany%20real-world%20datasets.%20Hierarchical%20clustering%20%28HC%29%20can%20uncover%20these%0Arelationships%20by%20organizing%20data%20into%20a%20tree-like%20structure%2C%20but%20it%20often%0Arelies%20on%20rigid%20similarity%20metrics%20that%20struggle%20to%20capture%20the%20complexity%20of%0Adiverse%20data%20types.%20To%20address%20these%20we%20envision%20%24%5Ctexttt%7BInfoHier%7D%24%2C%20a%0Aframework%20that%20combines%20SSL%20with%20HC%20to%20jointly%20learn%20robust%20latent%0Arepresentations%20and%20hierarchical%20structures.%20This%20approach%20leverages%20SSL%20to%0Aprovide%20adaptive%20representations%2C%20enhancing%20HC%27s%20ability%20to%20capture%20complex%0Apatterns.%20Simultaneously%2C%20it%20integrates%20HC%20loss%20to%20refine%20SSL%20training%2C%0Aresulting%20in%20representations%20that%20are%20more%20attuned%20to%20the%20underlying%0Ainformation%20hierarchy.%20%24%5Ctexttt%7BInfoHier%7D%24%20has%20the%20potential%20to%20improve%20the%0Aexpressiveness%20and%20performance%20of%20both%20clustering%20and%20representation%20learning%2C%0Aoffering%20significant%20benefits%20for%20data%20analysis%2C%20management%2C%20and%20information%0Aretrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08717v1&entry.124074799=Read"},
{"title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents", "author": "Kuicai Dong and Yujing Chang and Xin Deik Goh and Dexun Li and Ruiming Tang and Yong Liu", "abstract": "  Multi-modal document retrieval is designed to identify and retrieve various\nforms of multi-modal content, such as figures, tables, charts, and layout\ninformation from extensive documents. Despite its significance, there is a\nnotable lack of a robust benchmark to effectively evaluate the performance of\nsystems in multi-modal document retrieval. To address this gap, this work\nintroduces a new benchmark, named as MMDocIR, encompassing two distinct tasks:\npage-level and layout-level retrieval. The former focuses on localizing the\nmost relevant pages within a long document, while the latter targets the\ndetection of specific layouts, offering a more fine-grained granularity than\nwhole-page analysis. A layout can refer to a variety of elements such as\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring expertly annotated labels for\n1,685 questions and bootstrapped labels for 173,843 questions, making it a\npivotal resource for advancing multi-modal document retrieval for both training\nand evaluation. Through rigorous experiments, we reveal that (i) visual\nretrievers significantly outperform their text counterparts, (ii) MMDocIR train\nset can effectively benefit the training process of multi-modal document\nretrieval and (iii) text retrievers leveraging on VLM-text perform much better\nthan those using OCR-text. These findings underscores the potential advantages\nof integrating visual elements for multi-modal document retrieval.\n", "link": "http://arxiv.org/abs/2501.08828v1", "date": "2025-01-15", "relevancy": 2.5722, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMDocIR%3A%20Benchmarking%20Multi-Modal%20Retrieval%20for%20Long%20Documents&body=Title%3A%20MMDocIR%3A%20Benchmarking%20Multi-Modal%20Retrieval%20for%20Long%20Documents%0AAuthor%3A%20Kuicai%20Dong%20and%20Yujing%20Chang%20and%20Xin%20Deik%20Goh%20and%20Dexun%20Li%20and%20Ruiming%20Tang%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Multi-modal%20document%20retrieval%20is%20designed%20to%20identify%20and%20retrieve%20various%0Aforms%20of%20multi-modal%20content%2C%20such%20as%20figures%2C%20tables%2C%20charts%2C%20and%20layout%0Ainformation%20from%20extensive%20documents.%20Despite%20its%20significance%2C%20there%20is%20a%0Anotable%20lack%20of%20a%20robust%20benchmark%20to%20effectively%20evaluate%20the%20performance%20of%0Asystems%20in%20multi-modal%20document%20retrieval.%20To%20address%20this%20gap%2C%20this%20work%0Aintroduces%20a%20new%20benchmark%2C%20named%20as%20MMDocIR%2C%20encompassing%20two%20distinct%20tasks%3A%0Apage-level%20and%20layout-level%20retrieval.%20The%20former%20focuses%20on%20localizing%20the%0Amost%20relevant%20pages%20within%20a%20long%20document%2C%20while%20the%20latter%20targets%20the%0Adetection%20of%20specific%20layouts%2C%20offering%20a%20more%20fine-grained%20granularity%20than%0Awhole-page%20analysis.%20A%20layout%20can%20refer%20to%20a%20variety%20of%20elements%20such%20as%0Atextual%20paragraphs%2C%20equations%2C%20figures%2C%20tables%2C%20or%20charts.%20The%20MMDocIR%0Abenchmark%20comprises%20a%20rich%20dataset%20featuring%20expertly%20annotated%20labels%20for%0A1%2C685%20questions%20and%20bootstrapped%20labels%20for%20173%2C843%20questions%2C%20making%20it%20a%0Apivotal%20resource%20for%20advancing%20multi-modal%20document%20retrieval%20for%20both%20training%0Aand%20evaluation.%20Through%20rigorous%20experiments%2C%20we%20reveal%20that%20%28i%29%20visual%0Aretrievers%20significantly%20outperform%20their%20text%20counterparts%2C%20%28ii%29%20MMDocIR%20train%0Aset%20can%20effectively%20benefit%20the%20training%20process%20of%20multi-modal%20document%0Aretrieval%20and%20%28iii%29%20text%20retrievers%20leveraging%20on%20VLM-text%20perform%20much%20better%0Athan%20those%20using%20OCR-text.%20These%20findings%20underscores%20the%20potential%20advantages%0Aof%20integrating%20visual%20elements%20for%20multi-modal%20document%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMDocIR%253A%2520Benchmarking%2520Multi-Modal%2520Retrieval%2520for%2520Long%2520Documents%26entry.906535625%3DKuicai%2520Dong%2520and%2520Yujing%2520Chang%2520and%2520Xin%2520Deik%2520Goh%2520and%2520Dexun%2520Li%2520and%2520Ruiming%2520Tang%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520Multi-modal%2520document%2520retrieval%2520is%2520designed%2520to%2520identify%2520and%2520retrieve%2520various%250Aforms%2520of%2520multi-modal%2520content%252C%2520such%2520as%2520figures%252C%2520tables%252C%2520charts%252C%2520and%2520layout%250Ainformation%2520from%2520extensive%2520documents.%2520Despite%2520its%2520significance%252C%2520there%2520is%2520a%250Anotable%2520lack%2520of%2520a%2520robust%2520benchmark%2520to%2520effectively%2520evaluate%2520the%2520performance%2520of%250Asystems%2520in%2520multi-modal%2520document%2520retrieval.%2520To%2520address%2520this%2520gap%252C%2520this%2520work%250Aintroduces%2520a%2520new%2520benchmark%252C%2520named%2520as%2520MMDocIR%252C%2520encompassing%2520two%2520distinct%2520tasks%253A%250Apage-level%2520and%2520layout-level%2520retrieval.%2520The%2520former%2520focuses%2520on%2520localizing%2520the%250Amost%2520relevant%2520pages%2520within%2520a%2520long%2520document%252C%2520while%2520the%2520latter%2520targets%2520the%250Adetection%2520of%2520specific%2520layouts%252C%2520offering%2520a%2520more%2520fine-grained%2520granularity%2520than%250Awhole-page%2520analysis.%2520A%2520layout%2520can%2520refer%2520to%2520a%2520variety%2520of%2520elements%2520such%2520as%250Atextual%2520paragraphs%252C%2520equations%252C%2520figures%252C%2520tables%252C%2520or%2520charts.%2520The%2520MMDocIR%250Abenchmark%2520comprises%2520a%2520rich%2520dataset%2520featuring%2520expertly%2520annotated%2520labels%2520for%250A1%252C685%2520questions%2520and%2520bootstrapped%2520labels%2520for%2520173%252C843%2520questions%252C%2520making%2520it%2520a%250Apivotal%2520resource%2520for%2520advancing%2520multi-modal%2520document%2520retrieval%2520for%2520both%2520training%250Aand%2520evaluation.%2520Through%2520rigorous%2520experiments%252C%2520we%2520reveal%2520that%2520%2528i%2529%2520visual%250Aretrievers%2520significantly%2520outperform%2520their%2520text%2520counterparts%252C%2520%2528ii%2529%2520MMDocIR%2520train%250Aset%2520can%2520effectively%2520benefit%2520the%2520training%2520process%2520of%2520multi-modal%2520document%250Aretrieval%2520and%2520%2528iii%2529%2520text%2520retrievers%2520leveraging%2520on%2520VLM-text%2520perform%2520much%2520better%250Athan%2520those%2520using%2520OCR-text.%2520These%2520findings%2520underscores%2520the%2520potential%2520advantages%250Aof%2520integrating%2520visual%2520elements%2520for%2520multi-modal%2520document%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMDocIR%3A%20Benchmarking%20Multi-Modal%20Retrieval%20for%20Long%20Documents&entry.906535625=Kuicai%20Dong%20and%20Yujing%20Chang%20and%20Xin%20Deik%20Goh%20and%20Dexun%20Li%20and%20Ruiming%20Tang%20and%20Yong%20Liu&entry.1292438233=%20%20Multi-modal%20document%20retrieval%20is%20designed%20to%20identify%20and%20retrieve%20various%0Aforms%20of%20multi-modal%20content%2C%20such%20as%20figures%2C%20tables%2C%20charts%2C%20and%20layout%0Ainformation%20from%20extensive%20documents.%20Despite%20its%20significance%2C%20there%20is%20a%0Anotable%20lack%20of%20a%20robust%20benchmark%20to%20effectively%20evaluate%20the%20performance%20of%0Asystems%20in%20multi-modal%20document%20retrieval.%20To%20address%20this%20gap%2C%20this%20work%0Aintroduces%20a%20new%20benchmark%2C%20named%20as%20MMDocIR%2C%20encompassing%20two%20distinct%20tasks%3A%0Apage-level%20and%20layout-level%20retrieval.%20The%20former%20focuses%20on%20localizing%20the%0Amost%20relevant%20pages%20within%20a%20long%20document%2C%20while%20the%20latter%20targets%20the%0Adetection%20of%20specific%20layouts%2C%20offering%20a%20more%20fine-grained%20granularity%20than%0Awhole-page%20analysis.%20A%20layout%20can%20refer%20to%20a%20variety%20of%20elements%20such%20as%0Atextual%20paragraphs%2C%20equations%2C%20figures%2C%20tables%2C%20or%20charts.%20The%20MMDocIR%0Abenchmark%20comprises%20a%20rich%20dataset%20featuring%20expertly%20annotated%20labels%20for%0A1%2C685%20questions%20and%20bootstrapped%20labels%20for%20173%2C843%20questions%2C%20making%20it%20a%0Apivotal%20resource%20for%20advancing%20multi-modal%20document%20retrieval%20for%20both%20training%0Aand%20evaluation.%20Through%20rigorous%20experiments%2C%20we%20reveal%20that%20%28i%29%20visual%0Aretrievers%20significantly%20outperform%20their%20text%20counterparts%2C%20%28ii%29%20MMDocIR%20train%0Aset%20can%20effectively%20benefit%20the%20training%20process%20of%20multi-modal%20document%0Aretrieval%20and%20%28iii%29%20text%20retrievers%20leveraging%20on%20VLM-text%20perform%20much%20better%0Athan%20those%20using%20OCR-text.%20These%20findings%20underscores%20the%20potential%20advantages%0Aof%20integrating%20visual%20elements%20for%20multi-modal%20document%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08828v1&entry.124074799=Read"},
{"title": "RepVideo: Rethinking Cross-Layer Representation for Video Generation", "author": "Chenyang Si and Weichen Fan and Zhengyao Lv and Ziqi Huang and Yu Qiao and Ziwei Liu", "abstract": "  Video generation has achieved remarkable progress with the introduction of\ndiffusion models, which have significantly improved the quality of generated\nvideos. However, recent research has primarily focused on scaling up model\ntraining, while offering limited insights into the direct impact of\nrepresentations on the video generation process. In this paper, we initially\ninvestigate the characteristics of features in intermediate layers, finding\nsubstantial variations in attention maps across different layers. These\nvariations lead to unstable semantic representations and contribute to\ncumulative differences between features, which ultimately reduce the similarity\nbetween adjacent frames and negatively affect temporal coherence. To address\nthis, we propose RepVideo, an enhanced representation framework for\ntext-to-video diffusion models. By accumulating features from neighboring\nlayers to form enriched representations, this approach captures more stable\nsemantic information. These enhanced representations are then used as inputs to\nthe attention mechanism, thereby improving semantic expressiveness while\nensuring feature consistency across adjacent frames. Extensive experiments\ndemonstrate that our RepVideo not only significantly enhances the ability to\ngenerate accurate spatial appearances, such as capturing complex spatial\nrelationships between multiple objects, but also improves temporal consistency\nin video generation.\n", "link": "http://arxiv.org/abs/2501.08994v1", "date": "2025-01-15", "relevancy": 2.5374, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6455}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6454}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepVideo%3A%20Rethinking%20Cross-Layer%20Representation%20for%20Video%20Generation&body=Title%3A%20RepVideo%3A%20Rethinking%20Cross-Layer%20Representation%20for%20Video%20Generation%0AAuthor%3A%20Chenyang%20Si%20and%20Weichen%20Fan%20and%20Zhengyao%20Lv%20and%20Ziqi%20Huang%20and%20Yu%20Qiao%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Video%20generation%20has%20achieved%20remarkable%20progress%20with%20the%20introduction%20of%0Adiffusion%20models%2C%20which%20have%20significantly%20improved%20the%20quality%20of%20generated%0Avideos.%20However%2C%20recent%20research%20has%20primarily%20focused%20on%20scaling%20up%20model%0Atraining%2C%20while%20offering%20limited%20insights%20into%20the%20direct%20impact%20of%0Arepresentations%20on%20the%20video%20generation%20process.%20In%20this%20paper%2C%20we%20initially%0Ainvestigate%20the%20characteristics%20of%20features%20in%20intermediate%20layers%2C%20finding%0Asubstantial%20variations%20in%20attention%20maps%20across%20different%20layers.%20These%0Avariations%20lead%20to%20unstable%20semantic%20representations%20and%20contribute%20to%0Acumulative%20differences%20between%20features%2C%20which%20ultimately%20reduce%20the%20similarity%0Abetween%20adjacent%20frames%20and%20negatively%20affect%20temporal%20coherence.%20To%20address%0Athis%2C%20we%20propose%20RepVideo%2C%20an%20enhanced%20representation%20framework%20for%0Atext-to-video%20diffusion%20models.%20By%20accumulating%20features%20from%20neighboring%0Alayers%20to%20form%20enriched%20representations%2C%20this%20approach%20captures%20more%20stable%0Asemantic%20information.%20These%20enhanced%20representations%20are%20then%20used%20as%20inputs%20to%0Athe%20attention%20mechanism%2C%20thereby%20improving%20semantic%20expressiveness%20while%0Aensuring%20feature%20consistency%20across%20adjacent%20frames.%20Extensive%20experiments%0Ademonstrate%20that%20our%20RepVideo%20not%20only%20significantly%20enhances%20the%20ability%20to%0Agenerate%20accurate%20spatial%20appearances%2C%20such%20as%20capturing%20complex%20spatial%0Arelationships%20between%20multiple%20objects%2C%20but%20also%20improves%20temporal%20consistency%0Ain%20video%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepVideo%253A%2520Rethinking%2520Cross-Layer%2520Representation%2520for%2520Video%2520Generation%26entry.906535625%3DChenyang%2520Si%2520and%2520Weichen%2520Fan%2520and%2520Zhengyao%2520Lv%2520and%2520Ziqi%2520Huang%2520and%2520Yu%2520Qiao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Video%2520generation%2520has%2520achieved%2520remarkable%2520progress%2520with%2520the%2520introduction%2520of%250Adiffusion%2520models%252C%2520which%2520have%2520significantly%2520improved%2520the%2520quality%2520of%2520generated%250Avideos.%2520However%252C%2520recent%2520research%2520has%2520primarily%2520focused%2520on%2520scaling%2520up%2520model%250Atraining%252C%2520while%2520offering%2520limited%2520insights%2520into%2520the%2520direct%2520impact%2520of%250Arepresentations%2520on%2520the%2520video%2520generation%2520process.%2520In%2520this%2520paper%252C%2520we%2520initially%250Ainvestigate%2520the%2520characteristics%2520of%2520features%2520in%2520intermediate%2520layers%252C%2520finding%250Asubstantial%2520variations%2520in%2520attention%2520maps%2520across%2520different%2520layers.%2520These%250Avariations%2520lead%2520to%2520unstable%2520semantic%2520representations%2520and%2520contribute%2520to%250Acumulative%2520differences%2520between%2520features%252C%2520which%2520ultimately%2520reduce%2520the%2520similarity%250Abetween%2520adjacent%2520frames%2520and%2520negatively%2520affect%2520temporal%2520coherence.%2520To%2520address%250Athis%252C%2520we%2520propose%2520RepVideo%252C%2520an%2520enhanced%2520representation%2520framework%2520for%250Atext-to-video%2520diffusion%2520models.%2520By%2520accumulating%2520features%2520from%2520neighboring%250Alayers%2520to%2520form%2520enriched%2520representations%252C%2520this%2520approach%2520captures%2520more%2520stable%250Asemantic%2520information.%2520These%2520enhanced%2520representations%2520are%2520then%2520used%2520as%2520inputs%2520to%250Athe%2520attention%2520mechanism%252C%2520thereby%2520improving%2520semantic%2520expressiveness%2520while%250Aensuring%2520feature%2520consistency%2520across%2520adjacent%2520frames.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520RepVideo%2520not%2520only%2520significantly%2520enhances%2520the%2520ability%2520to%250Agenerate%2520accurate%2520spatial%2520appearances%252C%2520such%2520as%2520capturing%2520complex%2520spatial%250Arelationships%2520between%2520multiple%2520objects%252C%2520but%2520also%2520improves%2520temporal%2520consistency%250Ain%2520video%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepVideo%3A%20Rethinking%20Cross-Layer%20Representation%20for%20Video%20Generation&entry.906535625=Chenyang%20Si%20and%20Weichen%20Fan%20and%20Zhengyao%20Lv%20and%20Ziqi%20Huang%20and%20Yu%20Qiao%20and%20Ziwei%20Liu&entry.1292438233=%20%20Video%20generation%20has%20achieved%20remarkable%20progress%20with%20the%20introduction%20of%0Adiffusion%20models%2C%20which%20have%20significantly%20improved%20the%20quality%20of%20generated%0Avideos.%20However%2C%20recent%20research%20has%20primarily%20focused%20on%20scaling%20up%20model%0Atraining%2C%20while%20offering%20limited%20insights%20into%20the%20direct%20impact%20of%0Arepresentations%20on%20the%20video%20generation%20process.%20In%20this%20paper%2C%20we%20initially%0Ainvestigate%20the%20characteristics%20of%20features%20in%20intermediate%20layers%2C%20finding%0Asubstantial%20variations%20in%20attention%20maps%20across%20different%20layers.%20These%0Avariations%20lead%20to%20unstable%20semantic%20representations%20and%20contribute%20to%0Acumulative%20differences%20between%20features%2C%20which%20ultimately%20reduce%20the%20similarity%0Abetween%20adjacent%20frames%20and%20negatively%20affect%20temporal%20coherence.%20To%20address%0Athis%2C%20we%20propose%20RepVideo%2C%20an%20enhanced%20representation%20framework%20for%0Atext-to-video%20diffusion%20models.%20By%20accumulating%20features%20from%20neighboring%0Alayers%20to%20form%20enriched%20representations%2C%20this%20approach%20captures%20more%20stable%0Asemantic%20information.%20These%20enhanced%20representations%20are%20then%20used%20as%20inputs%20to%0Athe%20attention%20mechanism%2C%20thereby%20improving%20semantic%20expressiveness%20while%0Aensuring%20feature%20consistency%20across%20adjacent%20frames.%20Extensive%20experiments%0Ademonstrate%20that%20our%20RepVideo%20not%20only%20significantly%20enhances%20the%20ability%20to%0Agenerate%20accurate%20spatial%20appearances%2C%20such%20as%20capturing%20complex%20spatial%0Arelationships%20between%20multiple%20objects%2C%20but%20also%20improves%20temporal%20consistency%0Ain%20video%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08994v1&entry.124074799=Read"},
{"title": "Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT\n  Scans: The C4R Study", "author": "Sneha N. Naik and Elsa D. Angelini and Eric A. Hoffman and Elizabeth C. Oelsner and R. Graham Barr and Benjamin M. Smith and Andrew F. Laine", "abstract": "  The ratio of airway tree lumen to lung size (ALR), assessed at full\ninspiration on high resolution full-lung computed tomography (CT), is a major\nrisk factor for chronic obstructive pulmonary disease (COPD). There is growing\ninterest to infer ALR from cardiac CT images, which are widely available in\nepidemiological cohorts, to investigate the relationship of ALR to severe\nCOVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously,\ncardiac scans included approximately 2/3 of the total lung volume with 5-6x\ngreater slice thickness than high-resolution (HR) full-lung (FL) CT. In this\nstudy, we present a novel attention-based Multi-view Swin Transformer to infer\nFL ALR values from segmented cardiac CT scans. For the supervised training we\nexploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of\nAtherosclerosis (MESA). Our network significantly outperforms a proxy direct\nALR inference on segmented cardiac CT scans and achieves accuracy and\nreproducibility comparable with a scan-rescan reproducibility of the FL ALR\nground-truth.\n", "link": "http://arxiv.org/abs/2501.08902v1", "date": "2025-01-15", "relevancy": 2.4807, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5007}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Transformers%20for%20Airway-To-Lung%20Ratio%20Inference%20on%20Cardiac%20CT%0A%20%20Scans%3A%20The%20C4R%20Study&body=Title%3A%20Multi-View%20Transformers%20for%20Airway-To-Lung%20Ratio%20Inference%20on%20Cardiac%20CT%0A%20%20Scans%3A%20The%20C4R%20Study%0AAuthor%3A%20Sneha%20N.%20Naik%20and%20Elsa%20D.%20Angelini%20and%20Eric%20A.%20Hoffman%20and%20Elizabeth%20C.%20Oelsner%20and%20R.%20Graham%20Barr%20and%20Benjamin%20M.%20Smith%20and%20Andrew%20F.%20Laine%0AAbstract%3A%20%20%20The%20ratio%20of%20airway%20tree%20lumen%20to%20lung%20size%20%28ALR%29%2C%20assessed%20at%20full%0Ainspiration%20on%20high%20resolution%20full-lung%20computed%20tomography%20%28CT%29%2C%20is%20a%20major%0Arisk%20factor%20for%20chronic%20obstructive%20pulmonary%20disease%20%28COPD%29.%20There%20is%20growing%0Ainterest%20to%20infer%20ALR%20from%20cardiac%20CT%20images%2C%20which%20are%20widely%20available%20in%0Aepidemiological%20cohorts%2C%20to%20investigate%20the%20relationship%20of%20ALR%20to%20severe%0ACOVID-19%20and%20post-acute%20sequelae%20of%20SARS-CoV-2%20infection%20%28PASC%29.%20Previously%2C%0Acardiac%20scans%20included%20approximately%202/3%20of%20the%20total%20lung%20volume%20with%205-6x%0Agreater%20slice%20thickness%20than%20high-resolution%20%28HR%29%20full-lung%20%28FL%29%20CT.%20In%20this%0Astudy%2C%20we%20present%20a%20novel%20attention-based%20Multi-view%20Swin%20Transformer%20to%20infer%0AFL%20ALR%20values%20from%20segmented%20cardiac%20CT%20scans.%20For%20the%20supervised%20training%20we%0Aexploit%20paired%20full-lung%20and%20cardiac%20CTs%20acquired%20in%20the%20Multi-Ethnic%20Study%20of%0AAtherosclerosis%20%28MESA%29.%20Our%20network%20significantly%20outperforms%20a%20proxy%20direct%0AALR%20inference%20on%20segmented%20cardiac%20CT%20scans%20and%20achieves%20accuracy%20and%0Areproducibility%20comparable%20with%20a%20scan-rescan%20reproducibility%20of%20the%20FL%20ALR%0Aground-truth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Transformers%2520for%2520Airway-To-Lung%2520Ratio%2520Inference%2520on%2520Cardiac%2520CT%250A%2520%2520Scans%253A%2520The%2520C4R%2520Study%26entry.906535625%3DSneha%2520N.%2520Naik%2520and%2520Elsa%2520D.%2520Angelini%2520and%2520Eric%2520A.%2520Hoffman%2520and%2520Elizabeth%2520C.%2520Oelsner%2520and%2520R.%2520Graham%2520Barr%2520and%2520Benjamin%2520M.%2520Smith%2520and%2520Andrew%2520F.%2520Laine%26entry.1292438233%3D%2520%2520The%2520ratio%2520of%2520airway%2520tree%2520lumen%2520to%2520lung%2520size%2520%2528ALR%2529%252C%2520assessed%2520at%2520full%250Ainspiration%2520on%2520high%2520resolution%2520full-lung%2520computed%2520tomography%2520%2528CT%2529%252C%2520is%2520a%2520major%250Arisk%2520factor%2520for%2520chronic%2520obstructive%2520pulmonary%2520disease%2520%2528COPD%2529.%2520There%2520is%2520growing%250Ainterest%2520to%2520infer%2520ALR%2520from%2520cardiac%2520CT%2520images%252C%2520which%2520are%2520widely%2520available%2520in%250Aepidemiological%2520cohorts%252C%2520to%2520investigate%2520the%2520relationship%2520of%2520ALR%2520to%2520severe%250ACOVID-19%2520and%2520post-acute%2520sequelae%2520of%2520SARS-CoV-2%2520infection%2520%2528PASC%2529.%2520Previously%252C%250Acardiac%2520scans%2520included%2520approximately%25202/3%2520of%2520the%2520total%2520lung%2520volume%2520with%25205-6x%250Agreater%2520slice%2520thickness%2520than%2520high-resolution%2520%2528HR%2529%2520full-lung%2520%2528FL%2529%2520CT.%2520In%2520this%250Astudy%252C%2520we%2520present%2520a%2520novel%2520attention-based%2520Multi-view%2520Swin%2520Transformer%2520to%2520infer%250AFL%2520ALR%2520values%2520from%2520segmented%2520cardiac%2520CT%2520scans.%2520For%2520the%2520supervised%2520training%2520we%250Aexploit%2520paired%2520full-lung%2520and%2520cardiac%2520CTs%2520acquired%2520in%2520the%2520Multi-Ethnic%2520Study%2520of%250AAtherosclerosis%2520%2528MESA%2529.%2520Our%2520network%2520significantly%2520outperforms%2520a%2520proxy%2520direct%250AALR%2520inference%2520on%2520segmented%2520cardiac%2520CT%2520scans%2520and%2520achieves%2520accuracy%2520and%250Areproducibility%2520comparable%2520with%2520a%2520scan-rescan%2520reproducibility%2520of%2520the%2520FL%2520ALR%250Aground-truth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Transformers%20for%20Airway-To-Lung%20Ratio%20Inference%20on%20Cardiac%20CT%0A%20%20Scans%3A%20The%20C4R%20Study&entry.906535625=Sneha%20N.%20Naik%20and%20Elsa%20D.%20Angelini%20and%20Eric%20A.%20Hoffman%20and%20Elizabeth%20C.%20Oelsner%20and%20R.%20Graham%20Barr%20and%20Benjamin%20M.%20Smith%20and%20Andrew%20F.%20Laine&entry.1292438233=%20%20The%20ratio%20of%20airway%20tree%20lumen%20to%20lung%20size%20%28ALR%29%2C%20assessed%20at%20full%0Ainspiration%20on%20high%20resolution%20full-lung%20computed%20tomography%20%28CT%29%2C%20is%20a%20major%0Arisk%20factor%20for%20chronic%20obstructive%20pulmonary%20disease%20%28COPD%29.%20There%20is%20growing%0Ainterest%20to%20infer%20ALR%20from%20cardiac%20CT%20images%2C%20which%20are%20widely%20available%20in%0Aepidemiological%20cohorts%2C%20to%20investigate%20the%20relationship%20of%20ALR%20to%20severe%0ACOVID-19%20and%20post-acute%20sequelae%20of%20SARS-CoV-2%20infection%20%28PASC%29.%20Previously%2C%0Acardiac%20scans%20included%20approximately%202/3%20of%20the%20total%20lung%20volume%20with%205-6x%0Agreater%20slice%20thickness%20than%20high-resolution%20%28HR%29%20full-lung%20%28FL%29%20CT.%20In%20this%0Astudy%2C%20we%20present%20a%20novel%20attention-based%20Multi-view%20Swin%20Transformer%20to%20infer%0AFL%20ALR%20values%20from%20segmented%20cardiac%20CT%20scans.%20For%20the%20supervised%20training%20we%0Aexploit%20paired%20full-lung%20and%20cardiac%20CTs%20acquired%20in%20the%20Multi-Ethnic%20Study%20of%0AAtherosclerosis%20%28MESA%29.%20Our%20network%20significantly%20outperforms%20a%20proxy%20direct%0AALR%20inference%20on%20segmented%20cardiac%20CT%20scans%20and%20achieves%20accuracy%20and%0Areproducibility%20comparable%20with%20a%20scan-rescan%20reproducibility%20of%20the%20FL%20ALR%0Aground-truth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08902v1&entry.124074799=Read"},
{"title": "Analyzing the Ethical Logic of Six Large Language Models", "author": "W. Russell Neuman and Chad Coleman and Manan Shah", "abstract": "  This study examines the ethical reasoning of six prominent generative large\nlanguage models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude\n3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these\nmodels articulate and apply ethical logic, particularly in response to moral\ndilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from\ntraditional alignment studies, the study adopts an explainability-transparency\nframework, prompting models to explain their ethical reasoning. This approach\nis analyzed through three established ethical typologies: the\nconsequentialist-deontological analytic, Moral Foundations Theory, and the\nKohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit\nlargely convergent ethical logic, marked by a rationalist, consequentialist\nemphasis, with decisions often prioritizing harm minimization and fairness.\nDespite similarities in pre-training and model architecture, a mixture of\nnuanced and significant differences in ethical reasoning emerge across models,\nreflecting variations in fine-tuning and post-training processes. The models\nconsistently display erudition, caution, and self-awareness, presenting ethical\nreasoning akin to a graduate-level discourse in moral philosophy. In striking\nuniformity these systems all describe their ethical reasoning as more\nsophisticated than what is characteristic of typical human moral logic.\n", "link": "http://arxiv.org/abs/2501.08951v1", "date": "2025-01-15", "relevancy": 2.463, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20the%20Ethical%20Logic%20of%20Six%20Large%20Language%20Models&body=Title%3A%20Analyzing%20the%20Ethical%20Logic%20of%20Six%20Large%20Language%20Models%0AAuthor%3A%20W.%20Russell%20Neuman%20and%20Chad%20Coleman%20and%20Manan%20Shah%0AAbstract%3A%20%20%20This%20study%20examines%20the%20ethical%20reasoning%20of%20six%20prominent%20generative%20large%0Alanguage%20models%3A%20OpenAI%20GPT-4o%2C%20Meta%20LLaMA%203.1%2C%20Perplexity%2C%20Anthropic%20Claude%0A3.5%20Sonnet%2C%20Google%20Gemini%2C%20and%20Mistral%207B.%20The%20research%20explores%20how%20these%0Amodels%20articulate%20and%20apply%20ethical%20logic%2C%20particularly%20in%20response%20to%20moral%0Adilemmas%20such%20as%20the%20Trolley%20Problem%2C%20and%20Heinz%20Dilemma.%20Departing%20from%0Atraditional%20alignment%20studies%2C%20the%20study%20adopts%20an%20explainability-transparency%0Aframework%2C%20prompting%20models%20to%20explain%20their%20ethical%20reasoning.%20This%20approach%0Ais%20analyzed%20through%20three%20established%20ethical%20typologies%3A%20the%0Aconsequentialist-deontological%20analytic%2C%20Moral%20Foundations%20Theory%2C%20and%20the%0AKohlberg%20Stages%20of%20Moral%20Development%20Model.%20Findings%20reveal%20that%20LLMs%20exhibit%0Alargely%20convergent%20ethical%20logic%2C%20marked%20by%20a%20rationalist%2C%20consequentialist%0Aemphasis%2C%20with%20decisions%20often%20prioritizing%20harm%20minimization%20and%20fairness.%0ADespite%20similarities%20in%20pre-training%20and%20model%20architecture%2C%20a%20mixture%20of%0Anuanced%20and%20significant%20differences%20in%20ethical%20reasoning%20emerge%20across%20models%2C%0Areflecting%20variations%20in%20fine-tuning%20and%20post-training%20processes.%20The%20models%0Aconsistently%20display%20erudition%2C%20caution%2C%20and%20self-awareness%2C%20presenting%20ethical%0Areasoning%20akin%20to%20a%20graduate-level%20discourse%20in%20moral%20philosophy.%20In%20striking%0Auniformity%20these%20systems%20all%20describe%20their%20ethical%20reasoning%20as%20more%0Asophisticated%20than%20what%20is%20characteristic%20of%20typical%20human%20moral%20logic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520the%2520Ethical%2520Logic%2520of%2520Six%2520Large%2520Language%2520Models%26entry.906535625%3DW.%2520Russell%2520Neuman%2520and%2520Chad%2520Coleman%2520and%2520Manan%2520Shah%26entry.1292438233%3D%2520%2520This%2520study%2520examines%2520the%2520ethical%2520reasoning%2520of%2520six%2520prominent%2520generative%2520large%250Alanguage%2520models%253A%2520OpenAI%2520GPT-4o%252C%2520Meta%2520LLaMA%25203.1%252C%2520Perplexity%252C%2520Anthropic%2520Claude%250A3.5%2520Sonnet%252C%2520Google%2520Gemini%252C%2520and%2520Mistral%25207B.%2520The%2520research%2520explores%2520how%2520these%250Amodels%2520articulate%2520and%2520apply%2520ethical%2520logic%252C%2520particularly%2520in%2520response%2520to%2520moral%250Adilemmas%2520such%2520as%2520the%2520Trolley%2520Problem%252C%2520and%2520Heinz%2520Dilemma.%2520Departing%2520from%250Atraditional%2520alignment%2520studies%252C%2520the%2520study%2520adopts%2520an%2520explainability-transparency%250Aframework%252C%2520prompting%2520models%2520to%2520explain%2520their%2520ethical%2520reasoning.%2520This%2520approach%250Ais%2520analyzed%2520through%2520three%2520established%2520ethical%2520typologies%253A%2520the%250Aconsequentialist-deontological%2520analytic%252C%2520Moral%2520Foundations%2520Theory%252C%2520and%2520the%250AKohlberg%2520Stages%2520of%2520Moral%2520Development%2520Model.%2520Findings%2520reveal%2520that%2520LLMs%2520exhibit%250Alargely%2520convergent%2520ethical%2520logic%252C%2520marked%2520by%2520a%2520rationalist%252C%2520consequentialist%250Aemphasis%252C%2520with%2520decisions%2520often%2520prioritizing%2520harm%2520minimization%2520and%2520fairness.%250ADespite%2520similarities%2520in%2520pre-training%2520and%2520model%2520architecture%252C%2520a%2520mixture%2520of%250Anuanced%2520and%2520significant%2520differences%2520in%2520ethical%2520reasoning%2520emerge%2520across%2520models%252C%250Areflecting%2520variations%2520in%2520fine-tuning%2520and%2520post-training%2520processes.%2520The%2520models%250Aconsistently%2520display%2520erudition%252C%2520caution%252C%2520and%2520self-awareness%252C%2520presenting%2520ethical%250Areasoning%2520akin%2520to%2520a%2520graduate-level%2520discourse%2520in%2520moral%2520philosophy.%2520In%2520striking%250Auniformity%2520these%2520systems%2520all%2520describe%2520their%2520ethical%2520reasoning%2520as%2520more%250Asophisticated%2520than%2520what%2520is%2520characteristic%2520of%2520typical%2520human%2520moral%2520logic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20the%20Ethical%20Logic%20of%20Six%20Large%20Language%20Models&entry.906535625=W.%20Russell%20Neuman%20and%20Chad%20Coleman%20and%20Manan%20Shah&entry.1292438233=%20%20This%20study%20examines%20the%20ethical%20reasoning%20of%20six%20prominent%20generative%20large%0Alanguage%20models%3A%20OpenAI%20GPT-4o%2C%20Meta%20LLaMA%203.1%2C%20Perplexity%2C%20Anthropic%20Claude%0A3.5%20Sonnet%2C%20Google%20Gemini%2C%20and%20Mistral%207B.%20The%20research%20explores%20how%20these%0Amodels%20articulate%20and%20apply%20ethical%20logic%2C%20particularly%20in%20response%20to%20moral%0Adilemmas%20such%20as%20the%20Trolley%20Problem%2C%20and%20Heinz%20Dilemma.%20Departing%20from%0Atraditional%20alignment%20studies%2C%20the%20study%20adopts%20an%20explainability-transparency%0Aframework%2C%20prompting%20models%20to%20explain%20their%20ethical%20reasoning.%20This%20approach%0Ais%20analyzed%20through%20three%20established%20ethical%20typologies%3A%20the%0Aconsequentialist-deontological%20analytic%2C%20Moral%20Foundations%20Theory%2C%20and%20the%0AKohlberg%20Stages%20of%20Moral%20Development%20Model.%20Findings%20reveal%20that%20LLMs%20exhibit%0Alargely%20convergent%20ethical%20logic%2C%20marked%20by%20a%20rationalist%2C%20consequentialist%0Aemphasis%2C%20with%20decisions%20often%20prioritizing%20harm%20minimization%20and%20fairness.%0ADespite%20similarities%20in%20pre-training%20and%20model%20architecture%2C%20a%20mixture%20of%0Anuanced%20and%20significant%20differences%20in%20ethical%20reasoning%20emerge%20across%20models%2C%0Areflecting%20variations%20in%20fine-tuning%20and%20post-training%20processes.%20The%20models%0Aconsistently%20display%20erudition%2C%20caution%2C%20and%20self-awareness%2C%20presenting%20ethical%0Areasoning%20akin%20to%20a%20graduate-level%20discourse%20in%20moral%20philosophy.%20In%20striking%0Auniformity%20these%20systems%20all%20describe%20their%20ethical%20reasoning%20as%20more%0Asophisticated%20than%20what%20is%20characteristic%20of%20typical%20human%20moral%20logic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08951v1&entry.124074799=Read"},
{"title": "Exploring ChatGPT for Face Presentation Attack Detection in Zero and\n  Few-Shot in-Context Learning", "author": "Alain Komaty and Hatef Otroshi Shahreza and Anjith George and Sebastien Marcel", "abstract": "  This study highlights the potential of ChatGPT (specifically GPT-4o) as a\ncompetitive alternative for Face Presentation Attack Detection (PAD),\noutperforming several PAD models, including commercial solutions, in specific\nscenarios. Our results show that GPT-4o demonstrates high consistency,\nparticularly in few-shot in-context learning, where its performance improves as\nmore examples are provided (reference data). We also observe that detailed\nprompts enable the model to provide scores reliably, a behavior not observed\nwith concise prompts. Additionally, explanation-seeking prompts slightly\nenhance the model's performance by improving its interpretability. Remarkably,\nthe model exhibits emergent reasoning capabilities, correctly predicting the\nattack type (print or replay) with high accuracy in few-shot scenarios, despite\nnot being explicitly instructed to classify attack types. Despite these\nstrengths, GPT-4o faces challenges in zero-shot tasks, where its performance is\nlimited compared to specialized PAD systems. Experiments were conducted on a\nsubset of the SOTERIA dataset, ensuring compliance with data privacy\nregulations by using only data from consenting individuals. These findings\nunderscore GPT-4o's promise in PAD applications, laying the groundwork for\nfuture research to address broader data privacy concerns and improve\ncross-dataset generalization. Code available here:\nhttps://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad\n", "link": "http://arxiv.org/abs/2501.08799v1", "date": "2025-01-15", "relevancy": 2.4415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4811}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20ChatGPT%20for%20Face%20Presentation%20Attack%20Detection%20in%20Zero%20and%0A%20%20Few-Shot%20in-Context%20Learning&body=Title%3A%20Exploring%20ChatGPT%20for%20Face%20Presentation%20Attack%20Detection%20in%20Zero%20and%0A%20%20Few-Shot%20in-Context%20Learning%0AAuthor%3A%20Alain%20Komaty%20and%20Hatef%20Otroshi%20Shahreza%20and%20Anjith%20George%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20This%20study%20highlights%20the%20potential%20of%20ChatGPT%20%28specifically%20GPT-4o%29%20as%20a%0Acompetitive%20alternative%20for%20Face%20Presentation%20Attack%20Detection%20%28PAD%29%2C%0Aoutperforming%20several%20PAD%20models%2C%20including%20commercial%20solutions%2C%20in%20specific%0Ascenarios.%20Our%20results%20show%20that%20GPT-4o%20demonstrates%20high%20consistency%2C%0Aparticularly%20in%20few-shot%20in-context%20learning%2C%20where%20its%20performance%20improves%20as%0Amore%20examples%20are%20provided%20%28reference%20data%29.%20We%20also%20observe%20that%20detailed%0Aprompts%20enable%20the%20model%20to%20provide%20scores%20reliably%2C%20a%20behavior%20not%20observed%0Awith%20concise%20prompts.%20Additionally%2C%20explanation-seeking%20prompts%20slightly%0Aenhance%20the%20model%27s%20performance%20by%20improving%20its%20interpretability.%20Remarkably%2C%0Athe%20model%20exhibits%20emergent%20reasoning%20capabilities%2C%20correctly%20predicting%20the%0Aattack%20type%20%28print%20or%20replay%29%20with%20high%20accuracy%20in%20few-shot%20scenarios%2C%20despite%0Anot%20being%20explicitly%20instructed%20to%20classify%20attack%20types.%20Despite%20these%0Astrengths%2C%20GPT-4o%20faces%20challenges%20in%20zero-shot%20tasks%2C%20where%20its%20performance%20is%0Alimited%20compared%20to%20specialized%20PAD%20systems.%20Experiments%20were%20conducted%20on%20a%0Asubset%20of%20the%20SOTERIA%20dataset%2C%20ensuring%20compliance%20with%20data%20privacy%0Aregulations%20by%20using%20only%20data%20from%20consenting%20individuals.%20These%20findings%0Aunderscore%20GPT-4o%27s%20promise%20in%20PAD%20applications%2C%20laying%20the%20groundwork%20for%0Afuture%20research%20to%20address%20broader%20data%20privacy%20concerns%20and%20improve%0Across-dataset%20generalization.%20Code%20available%20here%3A%0Ahttps%3A//gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520ChatGPT%2520for%2520Face%2520Presentation%2520Attack%2520Detection%2520in%2520Zero%2520and%250A%2520%2520Few-Shot%2520in-Context%2520Learning%26entry.906535625%3DAlain%2520Komaty%2520and%2520Hatef%2520Otroshi%2520Shahreza%2520and%2520Anjith%2520George%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520This%2520study%2520highlights%2520the%2520potential%2520of%2520ChatGPT%2520%2528specifically%2520GPT-4o%2529%2520as%2520a%250Acompetitive%2520alternative%2520for%2520Face%2520Presentation%2520Attack%2520Detection%2520%2528PAD%2529%252C%250Aoutperforming%2520several%2520PAD%2520models%252C%2520including%2520commercial%2520solutions%252C%2520in%2520specific%250Ascenarios.%2520Our%2520results%2520show%2520that%2520GPT-4o%2520demonstrates%2520high%2520consistency%252C%250Aparticularly%2520in%2520few-shot%2520in-context%2520learning%252C%2520where%2520its%2520performance%2520improves%2520as%250Amore%2520examples%2520are%2520provided%2520%2528reference%2520data%2529.%2520We%2520also%2520observe%2520that%2520detailed%250Aprompts%2520enable%2520the%2520model%2520to%2520provide%2520scores%2520reliably%252C%2520a%2520behavior%2520not%2520observed%250Awith%2520concise%2520prompts.%2520Additionally%252C%2520explanation-seeking%2520prompts%2520slightly%250Aenhance%2520the%2520model%2527s%2520performance%2520by%2520improving%2520its%2520interpretability.%2520Remarkably%252C%250Athe%2520model%2520exhibits%2520emergent%2520reasoning%2520capabilities%252C%2520correctly%2520predicting%2520the%250Aattack%2520type%2520%2528print%2520or%2520replay%2529%2520with%2520high%2520accuracy%2520in%2520few-shot%2520scenarios%252C%2520despite%250Anot%2520being%2520explicitly%2520instructed%2520to%2520classify%2520attack%2520types.%2520Despite%2520these%250Astrengths%252C%2520GPT-4o%2520faces%2520challenges%2520in%2520zero-shot%2520tasks%252C%2520where%2520its%2520performance%2520is%250Alimited%2520compared%2520to%2520specialized%2520PAD%2520systems.%2520Experiments%2520were%2520conducted%2520on%2520a%250Asubset%2520of%2520the%2520SOTERIA%2520dataset%252C%2520ensuring%2520compliance%2520with%2520data%2520privacy%250Aregulations%2520by%2520using%2520only%2520data%2520from%2520consenting%2520individuals.%2520These%2520findings%250Aunderscore%2520GPT-4o%2527s%2520promise%2520in%2520PAD%2520applications%252C%2520laying%2520the%2520groundwork%2520for%250Afuture%2520research%2520to%2520address%2520broader%2520data%2520privacy%2520concerns%2520and%2520improve%250Across-dataset%2520generalization.%2520Code%2520available%2520here%253A%250Ahttps%253A//gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20ChatGPT%20for%20Face%20Presentation%20Attack%20Detection%20in%20Zero%20and%0A%20%20Few-Shot%20in-Context%20Learning&entry.906535625=Alain%20Komaty%20and%20Hatef%20Otroshi%20Shahreza%20and%20Anjith%20George%20and%20Sebastien%20Marcel&entry.1292438233=%20%20This%20study%20highlights%20the%20potential%20of%20ChatGPT%20%28specifically%20GPT-4o%29%20as%20a%0Acompetitive%20alternative%20for%20Face%20Presentation%20Attack%20Detection%20%28PAD%29%2C%0Aoutperforming%20several%20PAD%20models%2C%20including%20commercial%20solutions%2C%20in%20specific%0Ascenarios.%20Our%20results%20show%20that%20GPT-4o%20demonstrates%20high%20consistency%2C%0Aparticularly%20in%20few-shot%20in-context%20learning%2C%20where%20its%20performance%20improves%20as%0Amore%20examples%20are%20provided%20%28reference%20data%29.%20We%20also%20observe%20that%20detailed%0Aprompts%20enable%20the%20model%20to%20provide%20scores%20reliably%2C%20a%20behavior%20not%20observed%0Awith%20concise%20prompts.%20Additionally%2C%20explanation-seeking%20prompts%20slightly%0Aenhance%20the%20model%27s%20performance%20by%20improving%20its%20interpretability.%20Remarkably%2C%0Athe%20model%20exhibits%20emergent%20reasoning%20capabilities%2C%20correctly%20predicting%20the%0Aattack%20type%20%28print%20or%20replay%29%20with%20high%20accuracy%20in%20few-shot%20scenarios%2C%20despite%0Anot%20being%20explicitly%20instructed%20to%20classify%20attack%20types.%20Despite%20these%0Astrengths%2C%20GPT-4o%20faces%20challenges%20in%20zero-shot%20tasks%2C%20where%20its%20performance%20is%0Alimited%20compared%20to%20specialized%20PAD%20systems.%20Experiments%20were%20conducted%20on%20a%0Asubset%20of%20the%20SOTERIA%20dataset%2C%20ensuring%20compliance%20with%20data%20privacy%0Aregulations%20by%20using%20only%20data%20from%20consenting%20individuals.%20These%20findings%0Aunderscore%20GPT-4o%27s%20promise%20in%20PAD%20applications%2C%20laying%20the%20groundwork%20for%0Afuture%20research%20to%20address%20broader%20data%20privacy%20concerns%20and%20improve%0Across-dataset%20generalization.%20Code%20available%20here%3A%0Ahttps%3A//gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08799v1&entry.124074799=Read"},
{"title": "Resource-Constrained Federated Continual Learning: What Does Matter?", "author": "Yichen Li and Yuying Wang and Jiahua Dong and Haozhao Wang and Yining Qi and Rui Zhang and Ruixuan Li", "abstract": "  Federated Continual Learning (FCL) aims to enable sequentially\nprivacy-preserving model training on streams of incoming data that vary in edge\ndevices by preserving previous knowledge while adapting to new data. Current\nFCL literature focuses on restricted data privacy and access to previously seen\ndata while imposing no constraints on the training overhead. This is\nunreasonable for FCL applications in real-world scenarios, where edge devices\nare primarily constrained by resources such as storage, computational budget,\nand label rate. We revisit this problem with a large-scale benchmark and\nanalyze the performance of state-of-the-art FCL approaches under different\nresource-constrained settings. Various typical FCL techniques and six datasets\nin two incremental learning scenarios (Class-IL and Domain-IL) are involved in\nour experiments. Through extensive experiments amounting to a total of over\n1,000+ GPU hours, we find that, under limited resource-constrained settings,\nexisting FCL approaches, with no exception, fail to achieve the expected\nperformance. Our conclusions are consistent in the sensitivity analysis. This\nsuggests that most existing FCL methods are particularly too resource-dependent\nfor real-world deployment. Moreover, we study the performance of typical FCL\ntechniques with resource constraints and shed light on future research\ndirections in FCL.\n", "link": "http://arxiv.org/abs/2501.08737v1", "date": "2025-01-15", "relevancy": 2.4118, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resource-Constrained%20Federated%20Continual%20Learning%3A%20What%20Does%20Matter%3F&body=Title%3A%20Resource-Constrained%20Federated%20Continual%20Learning%3A%20What%20Does%20Matter%3F%0AAuthor%3A%20Yichen%20Li%20and%20Yuying%20Wang%20and%20Jiahua%20Dong%20and%20Haozhao%20Wang%20and%20Yining%20Qi%20and%20Rui%20Zhang%20and%20Ruixuan%20Li%0AAbstract%3A%20%20%20Federated%20Continual%20Learning%20%28FCL%29%20aims%20to%20enable%20sequentially%0Aprivacy-preserving%20model%20training%20on%20streams%20of%20incoming%20data%20that%20vary%20in%20edge%0Adevices%20by%20preserving%20previous%20knowledge%20while%20adapting%20to%20new%20data.%20Current%0AFCL%20literature%20focuses%20on%20restricted%20data%20privacy%20and%20access%20to%20previously%20seen%0Adata%20while%20imposing%20no%20constraints%20on%20the%20training%20overhead.%20This%20is%0Aunreasonable%20for%20FCL%20applications%20in%20real-world%20scenarios%2C%20where%20edge%20devices%0Aare%20primarily%20constrained%20by%20resources%20such%20as%20storage%2C%20computational%20budget%2C%0Aand%20label%20rate.%20We%20revisit%20this%20problem%20with%20a%20large-scale%20benchmark%20and%0Aanalyze%20the%20performance%20of%20state-of-the-art%20FCL%20approaches%20under%20different%0Aresource-constrained%20settings.%20Various%20typical%20FCL%20techniques%20and%20six%20datasets%0Ain%20two%20incremental%20learning%20scenarios%20%28Class-IL%20and%20Domain-IL%29%20are%20involved%20in%0Aour%20experiments.%20Through%20extensive%20experiments%20amounting%20to%20a%20total%20of%20over%0A1%2C000%2B%20GPU%20hours%2C%20we%20find%20that%2C%20under%20limited%20resource-constrained%20settings%2C%0Aexisting%20FCL%20approaches%2C%20with%20no%20exception%2C%20fail%20to%20achieve%20the%20expected%0Aperformance.%20Our%20conclusions%20are%20consistent%20in%20the%20sensitivity%20analysis.%20This%0Asuggests%20that%20most%20existing%20FCL%20methods%20are%20particularly%20too%20resource-dependent%0Afor%20real-world%20deployment.%20Moreover%2C%20we%20study%20the%20performance%20of%20typical%20FCL%0Atechniques%20with%20resource%20constraints%20and%20shed%20light%20on%20future%20research%0Adirections%20in%20FCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResource-Constrained%2520Federated%2520Continual%2520Learning%253A%2520What%2520Does%2520Matter%253F%26entry.906535625%3DYichen%2520Li%2520and%2520Yuying%2520Wang%2520and%2520Jiahua%2520Dong%2520and%2520Haozhao%2520Wang%2520and%2520Yining%2520Qi%2520and%2520Rui%2520Zhang%2520and%2520Ruixuan%2520Li%26entry.1292438233%3D%2520%2520Federated%2520Continual%2520Learning%2520%2528FCL%2529%2520aims%2520to%2520enable%2520sequentially%250Aprivacy-preserving%2520model%2520training%2520on%2520streams%2520of%2520incoming%2520data%2520that%2520vary%2520in%2520edge%250Adevices%2520by%2520preserving%2520previous%2520knowledge%2520while%2520adapting%2520to%2520new%2520data.%2520Current%250AFCL%2520literature%2520focuses%2520on%2520restricted%2520data%2520privacy%2520and%2520access%2520to%2520previously%2520seen%250Adata%2520while%2520imposing%2520no%2520constraints%2520on%2520the%2520training%2520overhead.%2520This%2520is%250Aunreasonable%2520for%2520FCL%2520applications%2520in%2520real-world%2520scenarios%252C%2520where%2520edge%2520devices%250Aare%2520primarily%2520constrained%2520by%2520resources%2520such%2520as%2520storage%252C%2520computational%2520budget%252C%250Aand%2520label%2520rate.%2520We%2520revisit%2520this%2520problem%2520with%2520a%2520large-scale%2520benchmark%2520and%250Aanalyze%2520the%2520performance%2520of%2520state-of-the-art%2520FCL%2520approaches%2520under%2520different%250Aresource-constrained%2520settings.%2520Various%2520typical%2520FCL%2520techniques%2520and%2520six%2520datasets%250Ain%2520two%2520incremental%2520learning%2520scenarios%2520%2528Class-IL%2520and%2520Domain-IL%2529%2520are%2520involved%2520in%250Aour%2520experiments.%2520Through%2520extensive%2520experiments%2520amounting%2520to%2520a%2520total%2520of%2520over%250A1%252C000%252B%2520GPU%2520hours%252C%2520we%2520find%2520that%252C%2520under%2520limited%2520resource-constrained%2520settings%252C%250Aexisting%2520FCL%2520approaches%252C%2520with%2520no%2520exception%252C%2520fail%2520to%2520achieve%2520the%2520expected%250Aperformance.%2520Our%2520conclusions%2520are%2520consistent%2520in%2520the%2520sensitivity%2520analysis.%2520This%250Asuggests%2520that%2520most%2520existing%2520FCL%2520methods%2520are%2520particularly%2520too%2520resource-dependent%250Afor%2520real-world%2520deployment.%2520Moreover%252C%2520we%2520study%2520the%2520performance%2520of%2520typical%2520FCL%250Atechniques%2520with%2520resource%2520constraints%2520and%2520shed%2520light%2520on%2520future%2520research%250Adirections%2520in%2520FCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource-Constrained%20Federated%20Continual%20Learning%3A%20What%20Does%20Matter%3F&entry.906535625=Yichen%20Li%20and%20Yuying%20Wang%20and%20Jiahua%20Dong%20and%20Haozhao%20Wang%20and%20Yining%20Qi%20and%20Rui%20Zhang%20and%20Ruixuan%20Li&entry.1292438233=%20%20Federated%20Continual%20Learning%20%28FCL%29%20aims%20to%20enable%20sequentially%0Aprivacy-preserving%20model%20training%20on%20streams%20of%20incoming%20data%20that%20vary%20in%20edge%0Adevices%20by%20preserving%20previous%20knowledge%20while%20adapting%20to%20new%20data.%20Current%0AFCL%20literature%20focuses%20on%20restricted%20data%20privacy%20and%20access%20to%20previously%20seen%0Adata%20while%20imposing%20no%20constraints%20on%20the%20training%20overhead.%20This%20is%0Aunreasonable%20for%20FCL%20applications%20in%20real-world%20scenarios%2C%20where%20edge%20devices%0Aare%20primarily%20constrained%20by%20resources%20such%20as%20storage%2C%20computational%20budget%2C%0Aand%20label%20rate.%20We%20revisit%20this%20problem%20with%20a%20large-scale%20benchmark%20and%0Aanalyze%20the%20performance%20of%20state-of-the-art%20FCL%20approaches%20under%20different%0Aresource-constrained%20settings.%20Various%20typical%20FCL%20techniques%20and%20six%20datasets%0Ain%20two%20incremental%20learning%20scenarios%20%28Class-IL%20and%20Domain-IL%29%20are%20involved%20in%0Aour%20experiments.%20Through%20extensive%20experiments%20amounting%20to%20a%20total%20of%20over%0A1%2C000%2B%20GPU%20hours%2C%20we%20find%20that%2C%20under%20limited%20resource-constrained%20settings%2C%0Aexisting%20FCL%20approaches%2C%20with%20no%20exception%2C%20fail%20to%20achieve%20the%20expected%0Aperformance.%20Our%20conclusions%20are%20consistent%20in%20the%20sensitivity%20analysis.%20This%0Asuggests%20that%20most%20existing%20FCL%20methods%20are%20particularly%20too%20resource-dependent%0Afor%20real-world%20deployment.%20Moreover%2C%20we%20study%20the%20performance%20of%20typical%20FCL%0Atechniques%20with%20resource%20constraints%20and%20shed%20light%20on%20future%20research%0Adirections%20in%20FCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08737v1&entry.124074799=Read"},
{"title": "SLC$^2$-SLAM: Semantic-guided Loop Closure with Shared Latent Code for\n  NeRF SLAM", "author": "Yuhang Ming and Di Ma and Weichen Dai and Han Yang and Rui Fan and Guofeng Zhang and Wanzeng Kong", "abstract": "  Targeting the notorious cumulative drift errors in NeRF SLAM, we propose a\nSemantic-guided Loop Closure with Shared Latent Code, dubbed SLC$^2$-SLAM.\nEspecially, we argue that latent codes stored in many NeRF SLAM systems are not\nfully exploited, as they are only used for better reconstruction. In this\npaper, we propose a simple yet effective way to detect potential loops using\nthe same latent codes as local features. To further improve the loop detection\nperformance, we use the semantic information, which are also decoded from the\nsame latent codes to guide the aggregation of local features. Finally, with the\npotential loops detected, we close them with a graph optimization followed by\nbundle adjustment to refine both the estimated poses and the reconstructed\nscene. To evaluate the performance of our SLC$^2$-SLAM, we conduct extensive\nexperiments on Replica and ScanNet datasets. Our proposed semantic-guided loop\nclosure significantly outperforms the pre-trained NetVLAD and ORB combined with\nBag-of-Words, which are used in all the other NeRF SLAM with loop closure. As a\nresult, our SLC$^2$-SLAM also demonstrated better tracking and reconstruction\nperformance, especially in larger scenes with more loops, like ScanNet.\n", "link": "http://arxiv.org/abs/2501.08880v1", "date": "2025-01-15", "relevancy": 2.384, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6166}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5943}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLC%24%5E2%24-SLAM%3A%20Semantic-guided%20Loop%20Closure%20with%20Shared%20Latent%20Code%20for%0A%20%20NeRF%20SLAM&body=Title%3A%20SLC%24%5E2%24-SLAM%3A%20Semantic-guided%20Loop%20Closure%20with%20Shared%20Latent%20Code%20for%0A%20%20NeRF%20SLAM%0AAuthor%3A%20Yuhang%20Ming%20and%20Di%20Ma%20and%20Weichen%20Dai%20and%20Han%20Yang%20and%20Rui%20Fan%20and%20Guofeng%20Zhang%20and%20Wanzeng%20Kong%0AAbstract%3A%20%20%20Targeting%20the%20notorious%20cumulative%20drift%20errors%20in%20NeRF%20SLAM%2C%20we%20propose%20a%0ASemantic-guided%20Loop%20Closure%20with%20Shared%20Latent%20Code%2C%20dubbed%20SLC%24%5E2%24-SLAM.%0AEspecially%2C%20we%20argue%20that%20latent%20codes%20stored%20in%20many%20NeRF%20SLAM%20systems%20are%20not%0Afully%20exploited%2C%20as%20they%20are%20only%20used%20for%20better%20reconstruction.%20In%20this%0Apaper%2C%20we%20propose%20a%20simple%20yet%20effective%20way%20to%20detect%20potential%20loops%20using%0Athe%20same%20latent%20codes%20as%20local%20features.%20To%20further%20improve%20the%20loop%20detection%0Aperformance%2C%20we%20use%20the%20semantic%20information%2C%20which%20are%20also%20decoded%20from%20the%0Asame%20latent%20codes%20to%20guide%20the%20aggregation%20of%20local%20features.%20Finally%2C%20with%20the%0Apotential%20loops%20detected%2C%20we%20close%20them%20with%20a%20graph%20optimization%20followed%20by%0Abundle%20adjustment%20to%20refine%20both%20the%20estimated%20poses%20and%20the%20reconstructed%0Ascene.%20To%20evaluate%20the%20performance%20of%20our%20SLC%24%5E2%24-SLAM%2C%20we%20conduct%20extensive%0Aexperiments%20on%20Replica%20and%20ScanNet%20datasets.%20Our%20proposed%20semantic-guided%20loop%0Aclosure%20significantly%20outperforms%20the%20pre-trained%20NetVLAD%20and%20ORB%20combined%20with%0ABag-of-Words%2C%20which%20are%20used%20in%20all%20the%20other%20NeRF%20SLAM%20with%20loop%20closure.%20As%20a%0Aresult%2C%20our%20SLC%24%5E2%24-SLAM%20also%20demonstrated%20better%20tracking%20and%20reconstruction%0Aperformance%2C%20especially%20in%20larger%20scenes%20with%20more%20loops%2C%20like%20ScanNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLC%2524%255E2%2524-SLAM%253A%2520Semantic-guided%2520Loop%2520Closure%2520with%2520Shared%2520Latent%2520Code%2520for%250A%2520%2520NeRF%2520SLAM%26entry.906535625%3DYuhang%2520Ming%2520and%2520Di%2520Ma%2520and%2520Weichen%2520Dai%2520and%2520Han%2520Yang%2520and%2520Rui%2520Fan%2520and%2520Guofeng%2520Zhang%2520and%2520Wanzeng%2520Kong%26entry.1292438233%3D%2520%2520Targeting%2520the%2520notorious%2520cumulative%2520drift%2520errors%2520in%2520NeRF%2520SLAM%252C%2520we%2520propose%2520a%250ASemantic-guided%2520Loop%2520Closure%2520with%2520Shared%2520Latent%2520Code%252C%2520dubbed%2520SLC%2524%255E2%2524-SLAM.%250AEspecially%252C%2520we%2520argue%2520that%2520latent%2520codes%2520stored%2520in%2520many%2520NeRF%2520SLAM%2520systems%2520are%2520not%250Afully%2520exploited%252C%2520as%2520they%2520are%2520only%2520used%2520for%2520better%2520reconstruction.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520way%2520to%2520detect%2520potential%2520loops%2520using%250Athe%2520same%2520latent%2520codes%2520as%2520local%2520features.%2520To%2520further%2520improve%2520the%2520loop%2520detection%250Aperformance%252C%2520we%2520use%2520the%2520semantic%2520information%252C%2520which%2520are%2520also%2520decoded%2520from%2520the%250Asame%2520latent%2520codes%2520to%2520guide%2520the%2520aggregation%2520of%2520local%2520features.%2520Finally%252C%2520with%2520the%250Apotential%2520loops%2520detected%252C%2520we%2520close%2520them%2520with%2520a%2520graph%2520optimization%2520followed%2520by%250Abundle%2520adjustment%2520to%2520refine%2520both%2520the%2520estimated%2520poses%2520and%2520the%2520reconstructed%250Ascene.%2520To%2520evaluate%2520the%2520performance%2520of%2520our%2520SLC%2524%255E2%2524-SLAM%252C%2520we%2520conduct%2520extensive%250Aexperiments%2520on%2520Replica%2520and%2520ScanNet%2520datasets.%2520Our%2520proposed%2520semantic-guided%2520loop%250Aclosure%2520significantly%2520outperforms%2520the%2520pre-trained%2520NetVLAD%2520and%2520ORB%2520combined%2520with%250ABag-of-Words%252C%2520which%2520are%2520used%2520in%2520all%2520the%2520other%2520NeRF%2520SLAM%2520with%2520loop%2520closure.%2520As%2520a%250Aresult%252C%2520our%2520SLC%2524%255E2%2524-SLAM%2520also%2520demonstrated%2520better%2520tracking%2520and%2520reconstruction%250Aperformance%252C%2520especially%2520in%2520larger%2520scenes%2520with%2520more%2520loops%252C%2520like%2520ScanNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLC%24%5E2%24-SLAM%3A%20Semantic-guided%20Loop%20Closure%20with%20Shared%20Latent%20Code%20for%0A%20%20NeRF%20SLAM&entry.906535625=Yuhang%20Ming%20and%20Di%20Ma%20and%20Weichen%20Dai%20and%20Han%20Yang%20and%20Rui%20Fan%20and%20Guofeng%20Zhang%20and%20Wanzeng%20Kong&entry.1292438233=%20%20Targeting%20the%20notorious%20cumulative%20drift%20errors%20in%20NeRF%20SLAM%2C%20we%20propose%20a%0ASemantic-guided%20Loop%20Closure%20with%20Shared%20Latent%20Code%2C%20dubbed%20SLC%24%5E2%24-SLAM.%0AEspecially%2C%20we%20argue%20that%20latent%20codes%20stored%20in%20many%20NeRF%20SLAM%20systems%20are%20not%0Afully%20exploited%2C%20as%20they%20are%20only%20used%20for%20better%20reconstruction.%20In%20this%0Apaper%2C%20we%20propose%20a%20simple%20yet%20effective%20way%20to%20detect%20potential%20loops%20using%0Athe%20same%20latent%20codes%20as%20local%20features.%20To%20further%20improve%20the%20loop%20detection%0Aperformance%2C%20we%20use%20the%20semantic%20information%2C%20which%20are%20also%20decoded%20from%20the%0Asame%20latent%20codes%20to%20guide%20the%20aggregation%20of%20local%20features.%20Finally%2C%20with%20the%0Apotential%20loops%20detected%2C%20we%20close%20them%20with%20a%20graph%20optimization%20followed%20by%0Abundle%20adjustment%20to%20refine%20both%20the%20estimated%20poses%20and%20the%20reconstructed%0Ascene.%20To%20evaluate%20the%20performance%20of%20our%20SLC%24%5E2%24-SLAM%2C%20we%20conduct%20extensive%0Aexperiments%20on%20Replica%20and%20ScanNet%20datasets.%20Our%20proposed%20semantic-guided%20loop%0Aclosure%20significantly%20outperforms%20the%20pre-trained%20NetVLAD%20and%20ORB%20combined%20with%0ABag-of-Words%2C%20which%20are%20used%20in%20all%20the%20other%20NeRF%20SLAM%20with%20loop%20closure.%20As%20a%0Aresult%2C%20our%20SLC%24%5E2%24-SLAM%20also%20demonstrated%20better%20tracking%20and%20reconstruction%0Aperformance%2C%20especially%20in%20larger%20scenes%20with%20more%20loops%2C%20like%20ScanNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08880v1&entry.124074799=Read"},
{"title": "CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities", "author": "Haozhe Xie and Zhaoxi Chen and Fangzhou Hong and Ziwei Liu", "abstract": "  3D scene generation has garnered growing attention in recent years and has\nmade significant progress. Generating 4D cities is more challenging than 3D\nscenes due to the presence of structurally complex, visually diverse objects\nlike buildings and vehicles, and heightened human sensitivity to distortions in\nurban environments. To tackle these issues, we propose CityDreamer4D, a\ncompositional generative model specifically tailored for generating unbounded\n4D cities. Our main insights are 1) 4D city generation should separate dynamic\nobjects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2)\nall objects in the 4D scene should be composed of different types of neural\nfields for buildings, vehicles, and background stuff. Specifically, we propose\nTraffic Scenario Generator and Unbounded Layout Generator to produce dynamic\ntraffic scenarios and static city layouts using a highly compact BEV\nrepresentation. Objects in 4D cities are generated by combining stuff-oriented\nand instance-oriented neural fields for background stuff, buildings, and\nvehicles. To suit the distinct characteristics of background stuff and\ninstances, the neural fields employ customized generative hash grids and\nperiodic positional embeddings as scene parameterizations. Furthermore, we\noffer a comprehensive suite of datasets for city generation, including OSM,\nGoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world\ncity layouts, while the Google Earth and CityTopia datasets deliver\nlarge-scale, high-quality city imagery complete with 3D instance annotations.\nLeveraging its compositional design, CityDreamer4D supports a range of\ndownstream applications, such as instance editing, city stylization, and urban\nsimulation, while delivering state-of-the-art performance in generating\nrealistic 4D cities.\n", "link": "http://arxiv.org/abs/2501.08983v1", "date": "2025-01-15", "relevancy": 2.3572, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6101}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5851}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CityDreamer4D%3A%20Compositional%20Generative%20Model%20of%20Unbounded%204D%20Cities&body=Title%3A%20CityDreamer4D%3A%20Compositional%20Generative%20Model%20of%20Unbounded%204D%20Cities%0AAuthor%3A%20Haozhe%20Xie%20and%20Zhaoxi%20Chen%20and%20Fangzhou%20Hong%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%203D%20scene%20generation%20has%20garnered%20growing%20attention%20in%20recent%20years%20and%20has%0Amade%20significant%20progress.%20Generating%204D%20cities%20is%20more%20challenging%20than%203D%0Ascenes%20due%20to%20the%20presence%20of%20structurally%20complex%2C%20visually%20diverse%20objects%0Alike%20buildings%20and%20vehicles%2C%20and%20heightened%20human%20sensitivity%20to%20distortions%20in%0Aurban%20environments.%20To%20tackle%20these%20issues%2C%20we%20propose%20CityDreamer4D%2C%20a%0Acompositional%20generative%20model%20specifically%20tailored%20for%20generating%20unbounded%0A4D%20cities.%20Our%20main%20insights%20are%201%29%204D%20city%20generation%20should%20separate%20dynamic%0Aobjects%20%28e.g.%2C%20vehicles%29%20from%20static%20scenes%20%28e.g.%2C%20buildings%20and%20roads%29%2C%20and%202%29%0Aall%20objects%20in%20the%204D%20scene%20should%20be%20composed%20of%20different%20types%20of%20neural%0Afields%20for%20buildings%2C%20vehicles%2C%20and%20background%20stuff.%20Specifically%2C%20we%20propose%0ATraffic%20Scenario%20Generator%20and%20Unbounded%20Layout%20Generator%20to%20produce%20dynamic%0Atraffic%20scenarios%20and%20static%20city%20layouts%20using%20a%20highly%20compact%20BEV%0Arepresentation.%20Objects%20in%204D%20cities%20are%20generated%20by%20combining%20stuff-oriented%0Aand%20instance-oriented%20neural%20fields%20for%20background%20stuff%2C%20buildings%2C%20and%0Avehicles.%20To%20suit%20the%20distinct%20characteristics%20of%20background%20stuff%20and%0Ainstances%2C%20the%20neural%20fields%20employ%20customized%20generative%20hash%20grids%20and%0Aperiodic%20positional%20embeddings%20as%20scene%20parameterizations.%20Furthermore%2C%20we%0Aoffer%20a%20comprehensive%20suite%20of%20datasets%20for%20city%20generation%2C%20including%20OSM%2C%0AGoogleEarth%2C%20and%20CityTopia.%20The%20OSM%20dataset%20provides%20a%20variety%20of%20real-world%0Acity%20layouts%2C%20while%20the%20Google%20Earth%20and%20CityTopia%20datasets%20deliver%0Alarge-scale%2C%20high-quality%20city%20imagery%20complete%20with%203D%20instance%20annotations.%0ALeveraging%20its%20compositional%20design%2C%20CityDreamer4D%20supports%20a%20range%20of%0Adownstream%20applications%2C%20such%20as%20instance%20editing%2C%20city%20stylization%2C%20and%20urban%0Asimulation%2C%20while%20delivering%20state-of-the-art%20performance%20in%20generating%0Arealistic%204D%20cities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCityDreamer4D%253A%2520Compositional%2520Generative%2520Model%2520of%2520Unbounded%25204D%2520Cities%26entry.906535625%3DHaozhe%2520Xie%2520and%2520Zhaoxi%2520Chen%2520and%2520Fangzhou%2520Hong%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%25203D%2520scene%2520generation%2520has%2520garnered%2520growing%2520attention%2520in%2520recent%2520years%2520and%2520has%250Amade%2520significant%2520progress.%2520Generating%25204D%2520cities%2520is%2520more%2520challenging%2520than%25203D%250Ascenes%2520due%2520to%2520the%2520presence%2520of%2520structurally%2520complex%252C%2520visually%2520diverse%2520objects%250Alike%2520buildings%2520and%2520vehicles%252C%2520and%2520heightened%2520human%2520sensitivity%2520to%2520distortions%2520in%250Aurban%2520environments.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520CityDreamer4D%252C%2520a%250Acompositional%2520generative%2520model%2520specifically%2520tailored%2520for%2520generating%2520unbounded%250A4D%2520cities.%2520Our%2520main%2520insights%2520are%25201%2529%25204D%2520city%2520generation%2520should%2520separate%2520dynamic%250Aobjects%2520%2528e.g.%252C%2520vehicles%2529%2520from%2520static%2520scenes%2520%2528e.g.%252C%2520buildings%2520and%2520roads%2529%252C%2520and%25202%2529%250Aall%2520objects%2520in%2520the%25204D%2520scene%2520should%2520be%2520composed%2520of%2520different%2520types%2520of%2520neural%250Afields%2520for%2520buildings%252C%2520vehicles%252C%2520and%2520background%2520stuff.%2520Specifically%252C%2520we%2520propose%250ATraffic%2520Scenario%2520Generator%2520and%2520Unbounded%2520Layout%2520Generator%2520to%2520produce%2520dynamic%250Atraffic%2520scenarios%2520and%2520static%2520city%2520layouts%2520using%2520a%2520highly%2520compact%2520BEV%250Arepresentation.%2520Objects%2520in%25204D%2520cities%2520are%2520generated%2520by%2520combining%2520stuff-oriented%250Aand%2520instance-oriented%2520neural%2520fields%2520for%2520background%2520stuff%252C%2520buildings%252C%2520and%250Avehicles.%2520To%2520suit%2520the%2520distinct%2520characteristics%2520of%2520background%2520stuff%2520and%250Ainstances%252C%2520the%2520neural%2520fields%2520employ%2520customized%2520generative%2520hash%2520grids%2520and%250Aperiodic%2520positional%2520embeddings%2520as%2520scene%2520parameterizations.%2520Furthermore%252C%2520we%250Aoffer%2520a%2520comprehensive%2520suite%2520of%2520datasets%2520for%2520city%2520generation%252C%2520including%2520OSM%252C%250AGoogleEarth%252C%2520and%2520CityTopia.%2520The%2520OSM%2520dataset%2520provides%2520a%2520variety%2520of%2520real-world%250Acity%2520layouts%252C%2520while%2520the%2520Google%2520Earth%2520and%2520CityTopia%2520datasets%2520deliver%250Alarge-scale%252C%2520high-quality%2520city%2520imagery%2520complete%2520with%25203D%2520instance%2520annotations.%250ALeveraging%2520its%2520compositional%2520design%252C%2520CityDreamer4D%2520supports%2520a%2520range%2520of%250Adownstream%2520applications%252C%2520such%2520as%2520instance%2520editing%252C%2520city%2520stylization%252C%2520and%2520urban%250Asimulation%252C%2520while%2520delivering%2520state-of-the-art%2520performance%2520in%2520generating%250Arealistic%25204D%2520cities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CityDreamer4D%3A%20Compositional%20Generative%20Model%20of%20Unbounded%204D%20Cities&entry.906535625=Haozhe%20Xie%20and%20Zhaoxi%20Chen%20and%20Fangzhou%20Hong%20and%20Ziwei%20Liu&entry.1292438233=%20%203D%20scene%20generation%20has%20garnered%20growing%20attention%20in%20recent%20years%20and%20has%0Amade%20significant%20progress.%20Generating%204D%20cities%20is%20more%20challenging%20than%203D%0Ascenes%20due%20to%20the%20presence%20of%20structurally%20complex%2C%20visually%20diverse%20objects%0Alike%20buildings%20and%20vehicles%2C%20and%20heightened%20human%20sensitivity%20to%20distortions%20in%0Aurban%20environments.%20To%20tackle%20these%20issues%2C%20we%20propose%20CityDreamer4D%2C%20a%0Acompositional%20generative%20model%20specifically%20tailored%20for%20generating%20unbounded%0A4D%20cities.%20Our%20main%20insights%20are%201%29%204D%20city%20generation%20should%20separate%20dynamic%0Aobjects%20%28e.g.%2C%20vehicles%29%20from%20static%20scenes%20%28e.g.%2C%20buildings%20and%20roads%29%2C%20and%202%29%0Aall%20objects%20in%20the%204D%20scene%20should%20be%20composed%20of%20different%20types%20of%20neural%0Afields%20for%20buildings%2C%20vehicles%2C%20and%20background%20stuff.%20Specifically%2C%20we%20propose%0ATraffic%20Scenario%20Generator%20and%20Unbounded%20Layout%20Generator%20to%20produce%20dynamic%0Atraffic%20scenarios%20and%20static%20city%20layouts%20using%20a%20highly%20compact%20BEV%0Arepresentation.%20Objects%20in%204D%20cities%20are%20generated%20by%20combining%20stuff-oriented%0Aand%20instance-oriented%20neural%20fields%20for%20background%20stuff%2C%20buildings%2C%20and%0Avehicles.%20To%20suit%20the%20distinct%20characteristics%20of%20background%20stuff%20and%0Ainstances%2C%20the%20neural%20fields%20employ%20customized%20generative%20hash%20grids%20and%0Aperiodic%20positional%20embeddings%20as%20scene%20parameterizations.%20Furthermore%2C%20we%0Aoffer%20a%20comprehensive%20suite%20of%20datasets%20for%20city%20generation%2C%20including%20OSM%2C%0AGoogleEarth%2C%20and%20CityTopia.%20The%20OSM%20dataset%20provides%20a%20variety%20of%20real-world%0Acity%20layouts%2C%20while%20the%20Google%20Earth%20and%20CityTopia%20datasets%20deliver%0Alarge-scale%2C%20high-quality%20city%20imagery%20complete%20with%203D%20instance%20annotations.%0ALeveraging%20its%20compositional%20design%2C%20CityDreamer4D%20supports%20a%20range%20of%0Adownstream%20applications%2C%20such%20as%20instance%20editing%2C%20city%20stylization%2C%20and%20urban%0Asimulation%2C%20while%20delivering%20state-of-the-art%20performance%20in%20generating%0Arealistic%204D%20cities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08983v1&entry.124074799=Read"},
{"title": "Enhanced Multi-Scale Cross-Attention for Person Image Generation", "author": "Hao Tang and Ling Shao and Nicu Sebe and Luc Van Gool", "abstract": "  In this paper, we propose a novel cross-attention-based generative\nadversarial network (GAN) for the challenging person image generation task.\nCross-attention is a novel and intuitive multi-modal fusion method in which an\nattention/correlation matrix is calculated between two feature maps of\ndifferent modalities. Specifically, we propose the novel XingGAN (or\nCrossingGAN), which consists of two generation branches that capture the\nperson's appearance and shape, respectively. Moreover, we propose two novel\ncross-attention blocks to effectively transfer and update the person's shape\nand appearance embeddings for mutual improvement. This has not been considered\nby any other existing GAN-based image generation work. To further learn the\nlong-range correlations between different person poses at different scales and\nsub-regions, we propose two novel multi-scale cross-attention blocks. To tackle\nthe issue of independent correlation computations within the cross-attention\nmechanism leading to noisy and ambiguous attention weights, which hinder\nperformance improvements, we propose a module called enhanced attention (EA).\nLastly, we introduce a novel densely connected co-attention module to fuse\nappearance and shape features at different stages effectively. Extensive\nexperiments on two public datasets demonstrate that the proposed method\noutperforms current GAN-based methods and performs on par with diffusion-based\nmethods. However, our method is significantly faster than diffusion-based\nmethods in both training and inference.\n", "link": "http://arxiv.org/abs/2501.08900v1", "date": "2025-01-15", "relevancy": 2.347, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5965}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5903}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Multi-Scale%20Cross-Attention%20for%20Person%20Image%20Generation&body=Title%3A%20Enhanced%20Multi-Scale%20Cross-Attention%20for%20Person%20Image%20Generation%0AAuthor%3A%20Hao%20Tang%20and%20Ling%20Shao%20and%20Nicu%20Sebe%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20cross-attention-based%20generative%0Aadversarial%20network%20%28GAN%29%20for%20the%20challenging%20person%20image%20generation%20task.%0ACross-attention%20is%20a%20novel%20and%20intuitive%20multi-modal%20fusion%20method%20in%20which%20an%0Aattention/correlation%20matrix%20is%20calculated%20between%20two%20feature%20maps%20of%0Adifferent%20modalities.%20Specifically%2C%20we%20propose%20the%20novel%20XingGAN%20%28or%0ACrossingGAN%29%2C%20which%20consists%20of%20two%20generation%20branches%20that%20capture%20the%0Aperson%27s%20appearance%20and%20shape%2C%20respectively.%20Moreover%2C%20we%20propose%20two%20novel%0Across-attention%20blocks%20to%20effectively%20transfer%20and%20update%20the%20person%27s%20shape%0Aand%20appearance%20embeddings%20for%20mutual%20improvement.%20This%20has%20not%20been%20considered%0Aby%20any%20other%20existing%20GAN-based%20image%20generation%20work.%20To%20further%20learn%20the%0Along-range%20correlations%20between%20different%20person%20poses%20at%20different%20scales%20and%0Asub-regions%2C%20we%20propose%20two%20novel%20multi-scale%20cross-attention%20blocks.%20To%20tackle%0Athe%20issue%20of%20independent%20correlation%20computations%20within%20the%20cross-attention%0Amechanism%20leading%20to%20noisy%20and%20ambiguous%20attention%20weights%2C%20which%20hinder%0Aperformance%20improvements%2C%20we%20propose%20a%20module%20called%20enhanced%20attention%20%28EA%29.%0ALastly%2C%20we%20introduce%20a%20novel%20densely%20connected%20co-attention%20module%20to%20fuse%0Aappearance%20and%20shape%20features%20at%20different%20stages%20effectively.%20Extensive%0Aexperiments%20on%20two%20public%20datasets%20demonstrate%20that%20the%20proposed%20method%0Aoutperforms%20current%20GAN-based%20methods%20and%20performs%20on%20par%20with%20diffusion-based%0Amethods.%20However%2C%20our%20method%20is%20significantly%20faster%20than%20diffusion-based%0Amethods%20in%20both%20training%20and%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Multi-Scale%2520Cross-Attention%2520for%2520Person%2520Image%2520Generation%26entry.906535625%3DHao%2520Tang%2520and%2520Ling%2520Shao%2520and%2520Nicu%2520Sebe%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520cross-attention-based%2520generative%250Aadversarial%2520network%2520%2528GAN%2529%2520for%2520the%2520challenging%2520person%2520image%2520generation%2520task.%250ACross-attention%2520is%2520a%2520novel%2520and%2520intuitive%2520multi-modal%2520fusion%2520method%2520in%2520which%2520an%250Aattention/correlation%2520matrix%2520is%2520calculated%2520between%2520two%2520feature%2520maps%2520of%250Adifferent%2520modalities.%2520Specifically%252C%2520we%2520propose%2520the%2520novel%2520XingGAN%2520%2528or%250ACrossingGAN%2529%252C%2520which%2520consists%2520of%2520two%2520generation%2520branches%2520that%2520capture%2520the%250Aperson%2527s%2520appearance%2520and%2520shape%252C%2520respectively.%2520Moreover%252C%2520we%2520propose%2520two%2520novel%250Across-attention%2520blocks%2520to%2520effectively%2520transfer%2520and%2520update%2520the%2520person%2527s%2520shape%250Aand%2520appearance%2520embeddings%2520for%2520mutual%2520improvement.%2520This%2520has%2520not%2520been%2520considered%250Aby%2520any%2520other%2520existing%2520GAN-based%2520image%2520generation%2520work.%2520To%2520further%2520learn%2520the%250Along-range%2520correlations%2520between%2520different%2520person%2520poses%2520at%2520different%2520scales%2520and%250Asub-regions%252C%2520we%2520propose%2520two%2520novel%2520multi-scale%2520cross-attention%2520blocks.%2520To%2520tackle%250Athe%2520issue%2520of%2520independent%2520correlation%2520computations%2520within%2520the%2520cross-attention%250Amechanism%2520leading%2520to%2520noisy%2520and%2520ambiguous%2520attention%2520weights%252C%2520which%2520hinder%250Aperformance%2520improvements%252C%2520we%2520propose%2520a%2520module%2520called%2520enhanced%2520attention%2520%2528EA%2529.%250ALastly%252C%2520we%2520introduce%2520a%2520novel%2520densely%2520connected%2520co-attention%2520module%2520to%2520fuse%250Aappearance%2520and%2520shape%2520features%2520at%2520different%2520stages%2520effectively.%2520Extensive%250Aexperiments%2520on%2520two%2520public%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aoutperforms%2520current%2520GAN-based%2520methods%2520and%2520performs%2520on%2520par%2520with%2520diffusion-based%250Amethods.%2520However%252C%2520our%2520method%2520is%2520significantly%2520faster%2520than%2520diffusion-based%250Amethods%2520in%2520both%2520training%2520and%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Multi-Scale%20Cross-Attention%20for%20Person%20Image%20Generation&entry.906535625=Hao%20Tang%20and%20Ling%20Shao%20and%20Nicu%20Sebe%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20cross-attention-based%20generative%0Aadversarial%20network%20%28GAN%29%20for%20the%20challenging%20person%20image%20generation%20task.%0ACross-attention%20is%20a%20novel%20and%20intuitive%20multi-modal%20fusion%20method%20in%20which%20an%0Aattention/correlation%20matrix%20is%20calculated%20between%20two%20feature%20maps%20of%0Adifferent%20modalities.%20Specifically%2C%20we%20propose%20the%20novel%20XingGAN%20%28or%0ACrossingGAN%29%2C%20which%20consists%20of%20two%20generation%20branches%20that%20capture%20the%0Aperson%27s%20appearance%20and%20shape%2C%20respectively.%20Moreover%2C%20we%20propose%20two%20novel%0Across-attention%20blocks%20to%20effectively%20transfer%20and%20update%20the%20person%27s%20shape%0Aand%20appearance%20embeddings%20for%20mutual%20improvement.%20This%20has%20not%20been%20considered%0Aby%20any%20other%20existing%20GAN-based%20image%20generation%20work.%20To%20further%20learn%20the%0Along-range%20correlations%20between%20different%20person%20poses%20at%20different%20scales%20and%0Asub-regions%2C%20we%20propose%20two%20novel%20multi-scale%20cross-attention%20blocks.%20To%20tackle%0Athe%20issue%20of%20independent%20correlation%20computations%20within%20the%20cross-attention%0Amechanism%20leading%20to%20noisy%20and%20ambiguous%20attention%20weights%2C%20which%20hinder%0Aperformance%20improvements%2C%20we%20propose%20a%20module%20called%20enhanced%20attention%20%28EA%29.%0ALastly%2C%20we%20introduce%20a%20novel%20densely%20connected%20co-attention%20module%20to%20fuse%0Aappearance%20and%20shape%20features%20at%20different%20stages%20effectively.%20Extensive%0Aexperiments%20on%20two%20public%20datasets%20demonstrate%20that%20the%20proposed%20method%0Aoutperforms%20current%20GAN-based%20methods%20and%20performs%20on%20par%20with%20diffusion-based%0Amethods.%20However%2C%20our%20method%20is%20significantly%20faster%20than%20diffusion-based%0Amethods%20in%20both%20training%20and%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08900v1&entry.124074799=Read"},
{"title": "Modeling Melt Pool Features and Spatter Using Symbolic Regression and\n  Machine Learning", "author": "Olabode T. Ajenifujah and Amir Barati Farimani", "abstract": "  Additive manufacturing (AM) is a rapidly evolving technology that has\nattracted applications across a wide range of fields due to its ability to\nfabricate complex geometries. However, one of the key challenges in AM is\nachieving consistent print quality. This inconsistency is often attributed to\nuncontrolled melt pool dynamics, partly caused by spatter which can lead to\ndefects. Therefore, capturing and controlling the evolution of the melt pool is\ncrucial for enhancing process stability and part quality. In this study, we\ndeveloped a framework to support decision-making in AM operations, facilitating\nquality control and minimizing defects via machine learning (ML) and polynomial\nsymbolic regression models. We implemented experimentally validated\ncomputational tools as a cost-effective approach to collect large datasets from\nlaser powder bed fusion (LPBF) processes. For a dataset consisting of 281\nprocess conditions, parameters such as melt pool dimensions (length, width,\ndepth), melt pool geometry (area, volume), and volume indicated as spatter were\nextracted. Using machine learning (ML) and polynomial symbolic regression\nmodels, a high R2 of over 95 % was achieved in predicting the melt pool\ndimensions and geometry features for both the training and testing datasets,\nwith either process conditions (power and velocity) or melt pool dimensions as\nthe model inputs. In the case of volume indicated as spatter, R2 improved after\nlogarithmic transforming the model inputs, which was either the process\nconditions or the melt pool dimensions. Among the investigated ML models, the\nExtraTree model achieved the highest R2 values of 96.7 % and 87.5 %.\n", "link": "http://arxiv.org/abs/2501.08922v1", "date": "2025-01-15", "relevancy": 2.3408, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4799}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4673}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Melt%20Pool%20Features%20and%20Spatter%20Using%20Symbolic%20Regression%20and%0A%20%20Machine%20Learning&body=Title%3A%20Modeling%20Melt%20Pool%20Features%20and%20Spatter%20Using%20Symbolic%20Regression%20and%0A%20%20Machine%20Learning%0AAuthor%3A%20Olabode%20T.%20Ajenifujah%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20%20%20Additive%20manufacturing%20%28AM%29%20is%20a%20rapidly%20evolving%20technology%20that%20has%0Aattracted%20applications%20across%20a%20wide%20range%20of%20fields%20due%20to%20its%20ability%20to%0Afabricate%20complex%20geometries.%20However%2C%20one%20of%20the%20key%20challenges%20in%20AM%20is%0Aachieving%20consistent%20print%20quality.%20This%20inconsistency%20is%20often%20attributed%20to%0Auncontrolled%20melt%20pool%20dynamics%2C%20partly%20caused%20by%20spatter%20which%20can%20lead%20to%0Adefects.%20Therefore%2C%20capturing%20and%20controlling%20the%20evolution%20of%20the%20melt%20pool%20is%0Acrucial%20for%20enhancing%20process%20stability%20and%20part%20quality.%20In%20this%20study%2C%20we%0Adeveloped%20a%20framework%20to%20support%20decision-making%20in%20AM%20operations%2C%20facilitating%0Aquality%20control%20and%20minimizing%20defects%20via%20machine%20learning%20%28ML%29%20and%20polynomial%0Asymbolic%20regression%20models.%20We%20implemented%20experimentally%20validated%0Acomputational%20tools%20as%20a%20cost-effective%20approach%20to%20collect%20large%20datasets%20from%0Alaser%20powder%20bed%20fusion%20%28LPBF%29%20processes.%20For%20a%20dataset%20consisting%20of%20281%0Aprocess%20conditions%2C%20parameters%20such%20as%20melt%20pool%20dimensions%20%28length%2C%20width%2C%0Adepth%29%2C%20melt%20pool%20geometry%20%28area%2C%20volume%29%2C%20and%20volume%20indicated%20as%20spatter%20were%0Aextracted.%20Using%20machine%20learning%20%28ML%29%20and%20polynomial%20symbolic%20regression%0Amodels%2C%20a%20high%20R2%20of%20over%2095%20%25%20was%20achieved%20in%20predicting%20the%20melt%20pool%0Adimensions%20and%20geometry%20features%20for%20both%20the%20training%20and%20testing%20datasets%2C%0Awith%20either%20process%20conditions%20%28power%20and%20velocity%29%20or%20melt%20pool%20dimensions%20as%0Athe%20model%20inputs.%20In%20the%20case%20of%20volume%20indicated%20as%20spatter%2C%20R2%20improved%20after%0Alogarithmic%20transforming%20the%20model%20inputs%2C%20which%20was%20either%20the%20process%0Aconditions%20or%20the%20melt%20pool%20dimensions.%20Among%20the%20investigated%20ML%20models%2C%20the%0AExtraTree%20model%20achieved%20the%20highest%20R2%20values%20of%2096.7%20%25%20and%2087.5%20%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Melt%2520Pool%2520Features%2520and%2520Spatter%2520Using%2520Symbolic%2520Regression%2520and%250A%2520%2520Machine%2520Learning%26entry.906535625%3DOlabode%2520T.%2520Ajenifujah%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3D%2520%2520Additive%2520manufacturing%2520%2528AM%2529%2520is%2520a%2520rapidly%2520evolving%2520technology%2520that%2520has%250Aattracted%2520applications%2520across%2520a%2520wide%2520range%2520of%2520fields%2520due%2520to%2520its%2520ability%2520to%250Afabricate%2520complex%2520geometries.%2520However%252C%2520one%2520of%2520the%2520key%2520challenges%2520in%2520AM%2520is%250Aachieving%2520consistent%2520print%2520quality.%2520This%2520inconsistency%2520is%2520often%2520attributed%2520to%250Auncontrolled%2520melt%2520pool%2520dynamics%252C%2520partly%2520caused%2520by%2520spatter%2520which%2520can%2520lead%2520to%250Adefects.%2520Therefore%252C%2520capturing%2520and%2520controlling%2520the%2520evolution%2520of%2520the%2520melt%2520pool%2520is%250Acrucial%2520for%2520enhancing%2520process%2520stability%2520and%2520part%2520quality.%2520In%2520this%2520study%252C%2520we%250Adeveloped%2520a%2520framework%2520to%2520support%2520decision-making%2520in%2520AM%2520operations%252C%2520facilitating%250Aquality%2520control%2520and%2520minimizing%2520defects%2520via%2520machine%2520learning%2520%2528ML%2529%2520and%2520polynomial%250Asymbolic%2520regression%2520models.%2520We%2520implemented%2520experimentally%2520validated%250Acomputational%2520tools%2520as%2520a%2520cost-effective%2520approach%2520to%2520collect%2520large%2520datasets%2520from%250Alaser%2520powder%2520bed%2520fusion%2520%2528LPBF%2529%2520processes.%2520For%2520a%2520dataset%2520consisting%2520of%2520281%250Aprocess%2520conditions%252C%2520parameters%2520such%2520as%2520melt%2520pool%2520dimensions%2520%2528length%252C%2520width%252C%250Adepth%2529%252C%2520melt%2520pool%2520geometry%2520%2528area%252C%2520volume%2529%252C%2520and%2520volume%2520indicated%2520as%2520spatter%2520were%250Aextracted.%2520Using%2520machine%2520learning%2520%2528ML%2529%2520and%2520polynomial%2520symbolic%2520regression%250Amodels%252C%2520a%2520high%2520R2%2520of%2520over%252095%2520%2525%2520was%2520achieved%2520in%2520predicting%2520the%2520melt%2520pool%250Adimensions%2520and%2520geometry%2520features%2520for%2520both%2520the%2520training%2520and%2520testing%2520datasets%252C%250Awith%2520either%2520process%2520conditions%2520%2528power%2520and%2520velocity%2529%2520or%2520melt%2520pool%2520dimensions%2520as%250Athe%2520model%2520inputs.%2520In%2520the%2520case%2520of%2520volume%2520indicated%2520as%2520spatter%252C%2520R2%2520improved%2520after%250Alogarithmic%2520transforming%2520the%2520model%2520inputs%252C%2520which%2520was%2520either%2520the%2520process%250Aconditions%2520or%2520the%2520melt%2520pool%2520dimensions.%2520Among%2520the%2520investigated%2520ML%2520models%252C%2520the%250AExtraTree%2520model%2520achieved%2520the%2520highest%2520R2%2520values%2520of%252096.7%2520%2525%2520and%252087.5%2520%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Melt%20Pool%20Features%20and%20Spatter%20Using%20Symbolic%20Regression%20and%0A%20%20Machine%20Learning&entry.906535625=Olabode%20T.%20Ajenifujah%20and%20Amir%20Barati%20Farimani&entry.1292438233=%20%20Additive%20manufacturing%20%28AM%29%20is%20a%20rapidly%20evolving%20technology%20that%20has%0Aattracted%20applications%20across%20a%20wide%20range%20of%20fields%20due%20to%20its%20ability%20to%0Afabricate%20complex%20geometries.%20However%2C%20one%20of%20the%20key%20challenges%20in%20AM%20is%0Aachieving%20consistent%20print%20quality.%20This%20inconsistency%20is%20often%20attributed%20to%0Auncontrolled%20melt%20pool%20dynamics%2C%20partly%20caused%20by%20spatter%20which%20can%20lead%20to%0Adefects.%20Therefore%2C%20capturing%20and%20controlling%20the%20evolution%20of%20the%20melt%20pool%20is%0Acrucial%20for%20enhancing%20process%20stability%20and%20part%20quality.%20In%20this%20study%2C%20we%0Adeveloped%20a%20framework%20to%20support%20decision-making%20in%20AM%20operations%2C%20facilitating%0Aquality%20control%20and%20minimizing%20defects%20via%20machine%20learning%20%28ML%29%20and%20polynomial%0Asymbolic%20regression%20models.%20We%20implemented%20experimentally%20validated%0Acomputational%20tools%20as%20a%20cost-effective%20approach%20to%20collect%20large%20datasets%20from%0Alaser%20powder%20bed%20fusion%20%28LPBF%29%20processes.%20For%20a%20dataset%20consisting%20of%20281%0Aprocess%20conditions%2C%20parameters%20such%20as%20melt%20pool%20dimensions%20%28length%2C%20width%2C%0Adepth%29%2C%20melt%20pool%20geometry%20%28area%2C%20volume%29%2C%20and%20volume%20indicated%20as%20spatter%20were%0Aextracted.%20Using%20machine%20learning%20%28ML%29%20and%20polynomial%20symbolic%20regression%0Amodels%2C%20a%20high%20R2%20of%20over%2095%20%25%20was%20achieved%20in%20predicting%20the%20melt%20pool%0Adimensions%20and%20geometry%20features%20for%20both%20the%20training%20and%20testing%20datasets%2C%0Awith%20either%20process%20conditions%20%28power%20and%20velocity%29%20or%20melt%20pool%20dimensions%20as%0Athe%20model%20inputs.%20In%20the%20case%20of%20volume%20indicated%20as%20spatter%2C%20R2%20improved%20after%0Alogarithmic%20transforming%20the%20model%20inputs%2C%20which%20was%20either%20the%20process%0Aconditions%20or%20the%20melt%20pool%20dimensions.%20Among%20the%20investigated%20ML%20models%2C%20the%0AExtraTree%20model%20achieved%20the%20highest%20R2%20values%20of%2096.7%20%25%20and%2087.5%20%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08922v1&entry.124074799=Read"},
{"title": "IDEA: Image Description Enhanced CLIP-Adapter", "author": "Zhipeng Ye and Feng Jiang and Qiufeng Wang and Kaizhu Huang and Jiaqi Huang", "abstract": "  CLIP (Contrastive Language-Image Pre-training) has attained great success in\npattern recognition and computer vision. Transferring CLIP to downstream tasks\n(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.\nHowever, current studies primarily focus on either prompt learning for text or\nadapter tuning for vision, without fully exploiting the complementary\ninformation and correlations among image-text pairs. In this paper, we propose\nan Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to\nfew-shot image classification tasks. This method captures fine-grained features\nby leveraging both visual features and textual descriptions of images. IDEA is\na training-free method for CLIP, and it can be comparable to or even exceeds\nstate-of-the-art models on multiple tasks. Furthermore, we introduce\nTrainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable\ncomponents (i.e., a projector and a learnable latent space), further enhancing\nthe model's performance and achieving SOTA results on 11 datasets. As one\nimportant contribution, we employ the Llama model and design a comprehensive\npipeline to generate textual descriptions for images of 11 datasets, resulting\nin a total of 1,637,795 image-text pairs, named \"IMD-11\". Our code and data are\nreleased at https://github.com/FourierAI/IDEA.\n", "link": "http://arxiv.org/abs/2501.08816v1", "date": "2025-01-15", "relevancy": 2.3381, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6375}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5556}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDEA%3A%20Image%20Description%20Enhanced%20CLIP-Adapter&body=Title%3A%20IDEA%3A%20Image%20Description%20Enhanced%20CLIP-Adapter%0AAuthor%3A%20Zhipeng%20Ye%20and%20Feng%20Jiang%20and%20Qiufeng%20Wang%20and%20Kaizhu%20Huang%20and%20Jiaqi%20Huang%0AAbstract%3A%20%20%20CLIP%20%28Contrastive%20Language-Image%20Pre-training%29%20has%20attained%20great%20success%20in%0Apattern%20recognition%20and%20computer%20vision.%20Transferring%20CLIP%20to%20downstream%20tasks%0A%28e.g.%20zero-%20or%20few-shot%20classification%29%20is%20a%20hot%20topic%20in%20multimodal%20learning.%0AHowever%2C%20current%20studies%20primarily%20focus%20on%20either%20prompt%20learning%20for%20text%20or%0Aadapter%20tuning%20for%20vision%2C%20without%20fully%20exploiting%20the%20complementary%0Ainformation%20and%20correlations%20among%20image-text%20pairs.%20In%20this%20paper%2C%20we%20propose%0Aan%20Image%20Description%20Enhanced%20CLIP-Adapter%20%28IDEA%29%20method%20to%20adapt%20CLIP%20to%0Afew-shot%20image%20classification%20tasks.%20This%20method%20captures%20fine-grained%20features%0Aby%20leveraging%20both%20visual%20features%20and%20textual%20descriptions%20of%20images.%20IDEA%20is%0Aa%20training-free%20method%20for%20CLIP%2C%20and%20it%20can%20be%20comparable%20to%20or%20even%20exceeds%0Astate-of-the-art%20models%20on%20multiple%20tasks.%20Furthermore%2C%20we%20introduce%0ATrainable-IDEA%20%28T-IDEA%29%2C%20which%20extends%20IDEA%20by%20adding%20two%20lightweight%20learnable%0Acomponents%20%28i.e.%2C%20a%20projector%20and%20a%20learnable%20latent%20space%29%2C%20further%20enhancing%0Athe%20model%27s%20performance%20and%20achieving%20SOTA%20results%20on%2011%20datasets.%20As%20one%0Aimportant%20contribution%2C%20we%20employ%20the%20Llama%20model%20and%20design%20a%20comprehensive%0Apipeline%20to%20generate%20textual%20descriptions%20for%20images%20of%2011%20datasets%2C%20resulting%0Ain%20a%20total%20of%201%2C637%2C795%20image-text%20pairs%2C%20named%20%22IMD-11%22.%20Our%20code%20and%20data%20are%0Areleased%20at%20https%3A//github.com/FourierAI/IDEA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDEA%253A%2520Image%2520Description%2520Enhanced%2520CLIP-Adapter%26entry.906535625%3DZhipeng%2520Ye%2520and%2520Feng%2520Jiang%2520and%2520Qiufeng%2520Wang%2520and%2520Kaizhu%2520Huang%2520and%2520Jiaqi%2520Huang%26entry.1292438233%3D%2520%2520CLIP%2520%2528Contrastive%2520Language-Image%2520Pre-training%2529%2520has%2520attained%2520great%2520success%2520in%250Apattern%2520recognition%2520and%2520computer%2520vision.%2520Transferring%2520CLIP%2520to%2520downstream%2520tasks%250A%2528e.g.%2520zero-%2520or%2520few-shot%2520classification%2529%2520is%2520a%2520hot%2520topic%2520in%2520multimodal%2520learning.%250AHowever%252C%2520current%2520studies%2520primarily%2520focus%2520on%2520either%2520prompt%2520learning%2520for%2520text%2520or%250Aadapter%2520tuning%2520for%2520vision%252C%2520without%2520fully%2520exploiting%2520the%2520complementary%250Ainformation%2520and%2520correlations%2520among%2520image-text%2520pairs.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aan%2520Image%2520Description%2520Enhanced%2520CLIP-Adapter%2520%2528IDEA%2529%2520method%2520to%2520adapt%2520CLIP%2520to%250Afew-shot%2520image%2520classification%2520tasks.%2520This%2520method%2520captures%2520fine-grained%2520features%250Aby%2520leveraging%2520both%2520visual%2520features%2520and%2520textual%2520descriptions%2520of%2520images.%2520IDEA%2520is%250Aa%2520training-free%2520method%2520for%2520CLIP%252C%2520and%2520it%2520can%2520be%2520comparable%2520to%2520or%2520even%2520exceeds%250Astate-of-the-art%2520models%2520on%2520multiple%2520tasks.%2520Furthermore%252C%2520we%2520introduce%250ATrainable-IDEA%2520%2528T-IDEA%2529%252C%2520which%2520extends%2520IDEA%2520by%2520adding%2520two%2520lightweight%2520learnable%250Acomponents%2520%2528i.e.%252C%2520a%2520projector%2520and%2520a%2520learnable%2520latent%2520space%2529%252C%2520further%2520enhancing%250Athe%2520model%2527s%2520performance%2520and%2520achieving%2520SOTA%2520results%2520on%252011%2520datasets.%2520As%2520one%250Aimportant%2520contribution%252C%2520we%2520employ%2520the%2520Llama%2520model%2520and%2520design%2520a%2520comprehensive%250Apipeline%2520to%2520generate%2520textual%2520descriptions%2520for%2520images%2520of%252011%2520datasets%252C%2520resulting%250Ain%2520a%2520total%2520of%25201%252C637%252C795%2520image-text%2520pairs%252C%2520named%2520%2522IMD-11%2522.%2520Our%2520code%2520and%2520data%2520are%250Areleased%2520at%2520https%253A//github.com/FourierAI/IDEA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDEA%3A%20Image%20Description%20Enhanced%20CLIP-Adapter&entry.906535625=Zhipeng%20Ye%20and%20Feng%20Jiang%20and%20Qiufeng%20Wang%20and%20Kaizhu%20Huang%20and%20Jiaqi%20Huang&entry.1292438233=%20%20CLIP%20%28Contrastive%20Language-Image%20Pre-training%29%20has%20attained%20great%20success%20in%0Apattern%20recognition%20and%20computer%20vision.%20Transferring%20CLIP%20to%20downstream%20tasks%0A%28e.g.%20zero-%20or%20few-shot%20classification%29%20is%20a%20hot%20topic%20in%20multimodal%20learning.%0AHowever%2C%20current%20studies%20primarily%20focus%20on%20either%20prompt%20learning%20for%20text%20or%0Aadapter%20tuning%20for%20vision%2C%20without%20fully%20exploiting%20the%20complementary%0Ainformation%20and%20correlations%20among%20image-text%20pairs.%20In%20this%20paper%2C%20we%20propose%0Aan%20Image%20Description%20Enhanced%20CLIP-Adapter%20%28IDEA%29%20method%20to%20adapt%20CLIP%20to%0Afew-shot%20image%20classification%20tasks.%20This%20method%20captures%20fine-grained%20features%0Aby%20leveraging%20both%20visual%20features%20and%20textual%20descriptions%20of%20images.%20IDEA%20is%0Aa%20training-free%20method%20for%20CLIP%2C%20and%20it%20can%20be%20comparable%20to%20or%20even%20exceeds%0Astate-of-the-art%20models%20on%20multiple%20tasks.%20Furthermore%2C%20we%20introduce%0ATrainable-IDEA%20%28T-IDEA%29%2C%20which%20extends%20IDEA%20by%20adding%20two%20lightweight%20learnable%0Acomponents%20%28i.e.%2C%20a%20projector%20and%20a%20learnable%20latent%20space%29%2C%20further%20enhancing%0Athe%20model%27s%20performance%20and%20achieving%20SOTA%20results%20on%2011%20datasets.%20As%20one%0Aimportant%20contribution%2C%20we%20employ%20the%20Llama%20model%20and%20design%20a%20comprehensive%0Apipeline%20to%20generate%20textual%20descriptions%20for%20images%20of%2011%20datasets%2C%20resulting%0Ain%20a%20total%20of%201%2C637%2C795%20image-text%20pairs%2C%20named%20%22IMD-11%22.%20Our%20code%20and%20data%20are%0Areleased%20at%20https%3A//github.com/FourierAI/IDEA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08816v1&entry.124074799=Read"},
{"title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media", "author": "Wenlu Fan and Yuqi Zhu and Chenyang Wang and Bin Wang and Wentao Xu", "abstract": "  Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.\n", "link": "http://arxiv.org/abs/2501.08102v2", "date": "2025-01-15", "relevancy": 2.3293, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistency%20of%20Responses%20and%20Continuations%20Generated%20by%20Large%20Language%0A%20%20Models%20on%20Social%20Media&body=Title%3A%20Consistency%20of%20Responses%20and%20Continuations%20Generated%20by%20Large%20Language%0A%20%20Models%20on%20Social%20Media%0AAuthor%3A%20Wenlu%20Fan%20and%20Yuqi%20Zhu%20and%20Chenyang%20Wang%20and%20Bin%20Wang%20and%20Wentao%20Xu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20remarkable%20capabilities%20in%20text%0Ageneration%2C%20yet%20their%20emotional%20consistency%20and%20semantic%20coherence%20in%20social%0Amedia%20contexts%20remain%20insufficiently%20understood.%20This%20study%20investigates%20how%0ALLMs%20handle%20emotional%20content%20and%20maintain%20semantic%20relationships%20through%0Acontinuation%20and%20response%20tasks%20using%20two%20open-source%20models%3A%20Gemma%20and%20Llama.%0ABy%20analyzing%20climate%20change%20discussions%20from%20Twitter%20and%20Reddit%2C%20we%20examine%0Aemotional%20transitions%2C%20intensity%20patterns%2C%20and%20semantic%20similarity%20between%0Ahuman-authored%20and%20LLM-generated%20content.%20Our%20findings%20reveal%20that%20while%20both%0Amodels%20maintain%20high%20semantic%20coherence%2C%20they%20exhibit%20distinct%20emotional%0Apatterns%3A%20Gemma%20shows%20a%20tendency%20toward%20negative%20emotion%20amplification%2C%0Aparticularly%20anger%2C%20while%20maintaining%20certain%20positive%20emotions%20like%20optimism.%0ALlama%20demonstrates%20superior%20emotional%20preservation%20across%20a%20broader%20spectrum%20of%0Aaffects.%20Both%20models%20systematically%20generate%20responses%20with%20attenuated%0Aemotional%20intensity%20compared%20to%20human-authored%20content%20and%20show%20a%20bias%20toward%0Apositive%20emotions%20in%20response%20tasks.%20Additionally%2C%20both%20models%20maintain%20strong%0Asemantic%20similarity%20with%20original%20texts%2C%20though%20performance%20varies%20between%0Acontinuation%20and%20response%20tasks.%20These%20findings%20provide%20insights%20into%20LLMs%27%0Aemotional%20and%20semantic%20processing%20capabilities%2C%20with%20implications%20for%20their%0Adeployment%20in%20social%20media%20contexts%20and%20human-AI%20interaction%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08102v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistency%2520of%2520Responses%2520and%2520Continuations%2520Generated%2520by%2520Large%2520Language%250A%2520%2520Models%2520on%2520Social%2520Media%26entry.906535625%3DWenlu%2520Fan%2520and%2520Yuqi%2520Zhu%2520and%2520Chenyang%2520Wang%2520and%2520Bin%2520Wang%2520and%2520Wentao%2520Xu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520remarkable%2520capabilities%2520in%2520text%250Ageneration%252C%2520yet%2520their%2520emotional%2520consistency%2520and%2520semantic%2520coherence%2520in%2520social%250Amedia%2520contexts%2520remain%2520insufficiently%2520understood.%2520This%2520study%2520investigates%2520how%250ALLMs%2520handle%2520emotional%2520content%2520and%2520maintain%2520semantic%2520relationships%2520through%250Acontinuation%2520and%2520response%2520tasks%2520using%2520two%2520open-source%2520models%253A%2520Gemma%2520and%2520Llama.%250ABy%2520analyzing%2520climate%2520change%2520discussions%2520from%2520Twitter%2520and%2520Reddit%252C%2520we%2520examine%250Aemotional%2520transitions%252C%2520intensity%2520patterns%252C%2520and%2520semantic%2520similarity%2520between%250Ahuman-authored%2520and%2520LLM-generated%2520content.%2520Our%2520findings%2520reveal%2520that%2520while%2520both%250Amodels%2520maintain%2520high%2520semantic%2520coherence%252C%2520they%2520exhibit%2520distinct%2520emotional%250Apatterns%253A%2520Gemma%2520shows%2520a%2520tendency%2520toward%2520negative%2520emotion%2520amplification%252C%250Aparticularly%2520anger%252C%2520while%2520maintaining%2520certain%2520positive%2520emotions%2520like%2520optimism.%250ALlama%2520demonstrates%2520superior%2520emotional%2520preservation%2520across%2520a%2520broader%2520spectrum%2520of%250Aaffects.%2520Both%2520models%2520systematically%2520generate%2520responses%2520with%2520attenuated%250Aemotional%2520intensity%2520compared%2520to%2520human-authored%2520content%2520and%2520show%2520a%2520bias%2520toward%250Apositive%2520emotions%2520in%2520response%2520tasks.%2520Additionally%252C%2520both%2520models%2520maintain%2520strong%250Asemantic%2520similarity%2520with%2520original%2520texts%252C%2520though%2520performance%2520varies%2520between%250Acontinuation%2520and%2520response%2520tasks.%2520These%2520findings%2520provide%2520insights%2520into%2520LLMs%2527%250Aemotional%2520and%2520semantic%2520processing%2520capabilities%252C%2520with%2520implications%2520for%2520their%250Adeployment%2520in%2520social%2520media%2520contexts%2520and%2520human-AI%2520interaction%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08102v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistency%20of%20Responses%20and%20Continuations%20Generated%20by%20Large%20Language%0A%20%20Models%20on%20Social%20Media&entry.906535625=Wenlu%20Fan%20and%20Yuqi%20Zhu%20and%20Chenyang%20Wang%20and%20Bin%20Wang%20and%20Wentao%20Xu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20remarkable%20capabilities%20in%20text%0Ageneration%2C%20yet%20their%20emotional%20consistency%20and%20semantic%20coherence%20in%20social%0Amedia%20contexts%20remain%20insufficiently%20understood.%20This%20study%20investigates%20how%0ALLMs%20handle%20emotional%20content%20and%20maintain%20semantic%20relationships%20through%0Acontinuation%20and%20response%20tasks%20using%20two%20open-source%20models%3A%20Gemma%20and%20Llama.%0ABy%20analyzing%20climate%20change%20discussions%20from%20Twitter%20and%20Reddit%2C%20we%20examine%0Aemotional%20transitions%2C%20intensity%20patterns%2C%20and%20semantic%20similarity%20between%0Ahuman-authored%20and%20LLM-generated%20content.%20Our%20findings%20reveal%20that%20while%20both%0Amodels%20maintain%20high%20semantic%20coherence%2C%20they%20exhibit%20distinct%20emotional%0Apatterns%3A%20Gemma%20shows%20a%20tendency%20toward%20negative%20emotion%20amplification%2C%0Aparticularly%20anger%2C%20while%20maintaining%20certain%20positive%20emotions%20like%20optimism.%0ALlama%20demonstrates%20superior%20emotional%20preservation%20across%20a%20broader%20spectrum%20of%0Aaffects.%20Both%20models%20systematically%20generate%20responses%20with%20attenuated%0Aemotional%20intensity%20compared%20to%20human-authored%20content%20and%20show%20a%20bias%20toward%0Apositive%20emotions%20in%20response%20tasks.%20Additionally%2C%20both%20models%20maintain%20strong%0Asemantic%20similarity%20with%20original%20texts%2C%20though%20performance%20varies%20between%0Acontinuation%20and%20response%20tasks.%20These%20findings%20provide%20insights%20into%20LLMs%27%0Aemotional%20and%20semantic%20processing%20capabilities%2C%20with%20implications%20for%20their%0Adeployment%20in%20social%20media%20contexts%20and%20human-AI%20interaction%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08102v2&entry.124074799=Read"},
{"title": "A Two-Stage Pretraining-Finetuning Framework for Treatment Effect\n  Estimation with Unmeasured Confounding", "author": "Chuan Zhou and Yaxuan Li and Chunyuan Zheng and Haiteng Zhang and Min Zhang and Haoxuan Li and Mingming Gong", "abstract": "  Estimating the conditional average treatment effect (CATE) from observational\ndata plays a crucial role in areas such as e-commerce, healthcare, and\neconomics. Existing studies mainly rely on the strong ignorability assumption\nthat there are no unmeasured confounders, whose presence cannot be tested from\nobservational data and can invalidate any causal conclusion. In contrast, data\ncollected from randomized controlled trials (RCT) do not suffer from\nconfounding, but are usually limited by a small sample size. In this paper, we\npropose a two-stage pretraining-finetuning (TSPF) framework using both\nlarge-scale observational data and small-scale RCT data to estimate the CATE in\nthe presence of unmeasured confounding. In the first stage, a foundational\nrepresentation of covariates is trained to estimate counterfactual outcomes\nthrough large-scale observational data. In the second stage, we propose to\ntrain an augmented representation of the covariates, which is concatenated to\nthe foundational representation obtained in the first stage to adjust for the\nunmeasured confounding. To avoid overfitting caused by the small-scale RCT data\nin the second stage, we further propose a partial parameter initialization\napproach, rather than training a separate network. The superiority of our\napproach is validated on two public datasets with extensive experiments. The\ncode is available at https://github.com/zhouchuanCN/KDD25-TSPF.\n", "link": "http://arxiv.org/abs/2501.08888v1", "date": "2025-01-15", "relevancy": 2.3088, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.466}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4631}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Two-Stage%20Pretraining-Finetuning%20Framework%20for%20Treatment%20Effect%0A%20%20Estimation%20with%20Unmeasured%20Confounding&body=Title%3A%20A%20Two-Stage%20Pretraining-Finetuning%20Framework%20for%20Treatment%20Effect%0A%20%20Estimation%20with%20Unmeasured%20Confounding%0AAuthor%3A%20Chuan%20Zhou%20and%20Yaxuan%20Li%20and%20Chunyuan%20Zheng%20and%20Haiteng%20Zhang%20and%20Min%20Zhang%20and%20Haoxuan%20Li%20and%20Mingming%20Gong%0AAbstract%3A%20%20%20Estimating%20the%20conditional%20average%20treatment%20effect%20%28CATE%29%20from%20observational%0Adata%20plays%20a%20crucial%20role%20in%20areas%20such%20as%20e-commerce%2C%20healthcare%2C%20and%0Aeconomics.%20Existing%20studies%20mainly%20rely%20on%20the%20strong%20ignorability%20assumption%0Athat%20there%20are%20no%20unmeasured%20confounders%2C%20whose%20presence%20cannot%20be%20tested%20from%0Aobservational%20data%20and%20can%20invalidate%20any%20causal%20conclusion.%20In%20contrast%2C%20data%0Acollected%20from%20randomized%20controlled%20trials%20%28RCT%29%20do%20not%20suffer%20from%0Aconfounding%2C%20but%20are%20usually%20limited%20by%20a%20small%20sample%20size.%20In%20this%20paper%2C%20we%0Apropose%20a%20two-stage%20pretraining-finetuning%20%28TSPF%29%20framework%20using%20both%0Alarge-scale%20observational%20data%20and%20small-scale%20RCT%20data%20to%20estimate%20the%20CATE%20in%0Athe%20presence%20of%20unmeasured%20confounding.%20In%20the%20first%20stage%2C%20a%20foundational%0Arepresentation%20of%20covariates%20is%20trained%20to%20estimate%20counterfactual%20outcomes%0Athrough%20large-scale%20observational%20data.%20In%20the%20second%20stage%2C%20we%20propose%20to%0Atrain%20an%20augmented%20representation%20of%20the%20covariates%2C%20which%20is%20concatenated%20to%0Athe%20foundational%20representation%20obtained%20in%20the%20first%20stage%20to%20adjust%20for%20the%0Aunmeasured%20confounding.%20To%20avoid%20overfitting%20caused%20by%20the%20small-scale%20RCT%20data%0Ain%20the%20second%20stage%2C%20we%20further%20propose%20a%20partial%20parameter%20initialization%0Aapproach%2C%20rather%20than%20training%20a%20separate%20network.%20The%20superiority%20of%20our%0Aapproach%20is%20validated%20on%20two%20public%20datasets%20with%20extensive%20experiments.%20The%0Acode%20is%20available%20at%20https%3A//github.com/zhouchuanCN/KDD25-TSPF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Two-Stage%2520Pretraining-Finetuning%2520Framework%2520for%2520Treatment%2520Effect%250A%2520%2520Estimation%2520with%2520Unmeasured%2520Confounding%26entry.906535625%3DChuan%2520Zhou%2520and%2520Yaxuan%2520Li%2520and%2520Chunyuan%2520Zheng%2520and%2520Haiteng%2520Zhang%2520and%2520Min%2520Zhang%2520and%2520Haoxuan%2520Li%2520and%2520Mingming%2520Gong%26entry.1292438233%3D%2520%2520Estimating%2520the%2520conditional%2520average%2520treatment%2520effect%2520%2528CATE%2529%2520from%2520observational%250Adata%2520plays%2520a%2520crucial%2520role%2520in%2520areas%2520such%2520as%2520e-commerce%252C%2520healthcare%252C%2520and%250Aeconomics.%2520Existing%2520studies%2520mainly%2520rely%2520on%2520the%2520strong%2520ignorability%2520assumption%250Athat%2520there%2520are%2520no%2520unmeasured%2520confounders%252C%2520whose%2520presence%2520cannot%2520be%2520tested%2520from%250Aobservational%2520data%2520and%2520can%2520invalidate%2520any%2520causal%2520conclusion.%2520In%2520contrast%252C%2520data%250Acollected%2520from%2520randomized%2520controlled%2520trials%2520%2528RCT%2529%2520do%2520not%2520suffer%2520from%250Aconfounding%252C%2520but%2520are%2520usually%2520limited%2520by%2520a%2520small%2520sample%2520size.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520two-stage%2520pretraining-finetuning%2520%2528TSPF%2529%2520framework%2520using%2520both%250Alarge-scale%2520observational%2520data%2520and%2520small-scale%2520RCT%2520data%2520to%2520estimate%2520the%2520CATE%2520in%250Athe%2520presence%2520of%2520unmeasured%2520confounding.%2520In%2520the%2520first%2520stage%252C%2520a%2520foundational%250Arepresentation%2520of%2520covariates%2520is%2520trained%2520to%2520estimate%2520counterfactual%2520outcomes%250Athrough%2520large-scale%2520observational%2520data.%2520In%2520the%2520second%2520stage%252C%2520we%2520propose%2520to%250Atrain%2520an%2520augmented%2520representation%2520of%2520the%2520covariates%252C%2520which%2520is%2520concatenated%2520to%250Athe%2520foundational%2520representation%2520obtained%2520in%2520the%2520first%2520stage%2520to%2520adjust%2520for%2520the%250Aunmeasured%2520confounding.%2520To%2520avoid%2520overfitting%2520caused%2520by%2520the%2520small-scale%2520RCT%2520data%250Ain%2520the%2520second%2520stage%252C%2520we%2520further%2520propose%2520a%2520partial%2520parameter%2520initialization%250Aapproach%252C%2520rather%2520than%2520training%2520a%2520separate%2520network.%2520The%2520superiority%2520of%2520our%250Aapproach%2520is%2520validated%2520on%2520two%2520public%2520datasets%2520with%2520extensive%2520experiments.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/zhouchuanCN/KDD25-TSPF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Two-Stage%20Pretraining-Finetuning%20Framework%20for%20Treatment%20Effect%0A%20%20Estimation%20with%20Unmeasured%20Confounding&entry.906535625=Chuan%20Zhou%20and%20Yaxuan%20Li%20and%20Chunyuan%20Zheng%20and%20Haiteng%20Zhang%20and%20Min%20Zhang%20and%20Haoxuan%20Li%20and%20Mingming%20Gong&entry.1292438233=%20%20Estimating%20the%20conditional%20average%20treatment%20effect%20%28CATE%29%20from%20observational%0Adata%20plays%20a%20crucial%20role%20in%20areas%20such%20as%20e-commerce%2C%20healthcare%2C%20and%0Aeconomics.%20Existing%20studies%20mainly%20rely%20on%20the%20strong%20ignorability%20assumption%0Athat%20there%20are%20no%20unmeasured%20confounders%2C%20whose%20presence%20cannot%20be%20tested%20from%0Aobservational%20data%20and%20can%20invalidate%20any%20causal%20conclusion.%20In%20contrast%2C%20data%0Acollected%20from%20randomized%20controlled%20trials%20%28RCT%29%20do%20not%20suffer%20from%0Aconfounding%2C%20but%20are%20usually%20limited%20by%20a%20small%20sample%20size.%20In%20this%20paper%2C%20we%0Apropose%20a%20two-stage%20pretraining-finetuning%20%28TSPF%29%20framework%20using%20both%0Alarge-scale%20observational%20data%20and%20small-scale%20RCT%20data%20to%20estimate%20the%20CATE%20in%0Athe%20presence%20of%20unmeasured%20confounding.%20In%20the%20first%20stage%2C%20a%20foundational%0Arepresentation%20of%20covariates%20is%20trained%20to%20estimate%20counterfactual%20outcomes%0Athrough%20large-scale%20observational%20data.%20In%20the%20second%20stage%2C%20we%20propose%20to%0Atrain%20an%20augmented%20representation%20of%20the%20covariates%2C%20which%20is%20concatenated%20to%0Athe%20foundational%20representation%20obtained%20in%20the%20first%20stage%20to%20adjust%20for%20the%0Aunmeasured%20confounding.%20To%20avoid%20overfitting%20caused%20by%20the%20small-scale%20RCT%20data%0Ain%20the%20second%20stage%2C%20we%20further%20propose%20a%20partial%20parameter%20initialization%0Aapproach%2C%20rather%20than%20training%20a%20separate%20network.%20The%20superiority%20of%20our%0Aapproach%20is%20validated%20on%20two%20public%20datasets%20with%20extensive%20experiments.%20The%0Acode%20is%20available%20at%20https%3A//github.com/zhouchuanCN/KDD25-TSPF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08888v1&entry.124074799=Read"},
{"title": "Disentangling Exploration of Large Language Models by Optimal\n  Exploitation", "author": "Tim Grams and Patrick Betz and Christian Bartelt", "abstract": "  Exploration is a crucial skill for self-improvement and open-ended\nproblem-solving. However, it remains uncertain whether large language models\ncan effectively explore the state-space. Existing evaluations predominantly\nfocus on the trade-off between exploration and exploitation, often assessed in\nmulti-armed bandit problems. In contrast, this work isolates exploration as the\nsole objective, tasking the agent with delivering information that enhances\nfuture returns. For the evaluation, we propose to decompose missing rewards\ninto exploration and exploitation components by measuring the optimal\nachievable return for the states already explored. Our experiments with various\nLLMs reveal that most models struggle to sufficiently explore the state-space\nand that weak exploration is insufficient. We observe a positive correlation\nbetween model size and exploration performance, with larger models\ndemonstrating superior capabilities. Furthermore, we show that our\ndecomposition provides insights into differences in behaviors driven by agent\ninstructions during prompt engineering, offering a valuable tool for refining\nLLM performance in exploratory tasks.\n", "link": "http://arxiv.org/abs/2501.08925v1", "date": "2025-01-15", "relevancy": 2.2912, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20Exploration%20of%20Large%20Language%20Models%20by%20Optimal%0A%20%20Exploitation&body=Title%3A%20Disentangling%20Exploration%20of%20Large%20Language%20Models%20by%20Optimal%0A%20%20Exploitation%0AAuthor%3A%20Tim%20Grams%20and%20Patrick%20Betz%20and%20Christian%20Bartelt%0AAbstract%3A%20%20%20Exploration%20is%20a%20crucial%20skill%20for%20self-improvement%20and%20open-ended%0Aproblem-solving.%20However%2C%20it%20remains%20uncertain%20whether%20large%20language%20models%0Acan%20effectively%20explore%20the%20state-space.%20Existing%20evaluations%20predominantly%0Afocus%20on%20the%20trade-off%20between%20exploration%20and%20exploitation%2C%20often%20assessed%20in%0Amulti-armed%20bandit%20problems.%20In%20contrast%2C%20this%20work%20isolates%20exploration%20as%20the%0Asole%20objective%2C%20tasking%20the%20agent%20with%20delivering%20information%20that%20enhances%0Afuture%20returns.%20For%20the%20evaluation%2C%20we%20propose%20to%20decompose%20missing%20rewards%0Ainto%20exploration%20and%20exploitation%20components%20by%20measuring%20the%20optimal%0Aachievable%20return%20for%20the%20states%20already%20explored.%20Our%20experiments%20with%20various%0ALLMs%20reveal%20that%20most%20models%20struggle%20to%20sufficiently%20explore%20the%20state-space%0Aand%20that%20weak%20exploration%20is%20insufficient.%20We%20observe%20a%20positive%20correlation%0Abetween%20model%20size%20and%20exploration%20performance%2C%20with%20larger%20models%0Ademonstrating%20superior%20capabilities.%20Furthermore%2C%20we%20show%20that%20our%0Adecomposition%20provides%20insights%20into%20differences%20in%20behaviors%20driven%20by%20agent%0Ainstructions%20during%20prompt%20engineering%2C%20offering%20a%20valuable%20tool%20for%20refining%0ALLM%20performance%20in%20exploratory%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520Exploration%2520of%2520Large%2520Language%2520Models%2520by%2520Optimal%250A%2520%2520Exploitation%26entry.906535625%3DTim%2520Grams%2520and%2520Patrick%2520Betz%2520and%2520Christian%2520Bartelt%26entry.1292438233%3D%2520%2520Exploration%2520is%2520a%2520crucial%2520skill%2520for%2520self-improvement%2520and%2520open-ended%250Aproblem-solving.%2520However%252C%2520it%2520remains%2520uncertain%2520whether%2520large%2520language%2520models%250Acan%2520effectively%2520explore%2520the%2520state-space.%2520Existing%2520evaluations%2520predominantly%250Afocus%2520on%2520the%2520trade-off%2520between%2520exploration%2520and%2520exploitation%252C%2520often%2520assessed%2520in%250Amulti-armed%2520bandit%2520problems.%2520In%2520contrast%252C%2520this%2520work%2520isolates%2520exploration%2520as%2520the%250Asole%2520objective%252C%2520tasking%2520the%2520agent%2520with%2520delivering%2520information%2520that%2520enhances%250Afuture%2520returns.%2520For%2520the%2520evaluation%252C%2520we%2520propose%2520to%2520decompose%2520missing%2520rewards%250Ainto%2520exploration%2520and%2520exploitation%2520components%2520by%2520measuring%2520the%2520optimal%250Aachievable%2520return%2520for%2520the%2520states%2520already%2520explored.%2520Our%2520experiments%2520with%2520various%250ALLMs%2520reveal%2520that%2520most%2520models%2520struggle%2520to%2520sufficiently%2520explore%2520the%2520state-space%250Aand%2520that%2520weak%2520exploration%2520is%2520insufficient.%2520We%2520observe%2520a%2520positive%2520correlation%250Abetween%2520model%2520size%2520and%2520exploration%2520performance%252C%2520with%2520larger%2520models%250Ademonstrating%2520superior%2520capabilities.%2520Furthermore%252C%2520we%2520show%2520that%2520our%250Adecomposition%2520provides%2520insights%2520into%2520differences%2520in%2520behaviors%2520driven%2520by%2520agent%250Ainstructions%2520during%2520prompt%2520engineering%252C%2520offering%2520a%2520valuable%2520tool%2520for%2520refining%250ALLM%2520performance%2520in%2520exploratory%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20Exploration%20of%20Large%20Language%20Models%20by%20Optimal%0A%20%20Exploitation&entry.906535625=Tim%20Grams%20and%20Patrick%20Betz%20and%20Christian%20Bartelt&entry.1292438233=%20%20Exploration%20is%20a%20crucial%20skill%20for%20self-improvement%20and%20open-ended%0Aproblem-solving.%20However%2C%20it%20remains%20uncertain%20whether%20large%20language%20models%0Acan%20effectively%20explore%20the%20state-space.%20Existing%20evaluations%20predominantly%0Afocus%20on%20the%20trade-off%20between%20exploration%20and%20exploitation%2C%20often%20assessed%20in%0Amulti-armed%20bandit%20problems.%20In%20contrast%2C%20this%20work%20isolates%20exploration%20as%20the%0Asole%20objective%2C%20tasking%20the%20agent%20with%20delivering%20information%20that%20enhances%0Afuture%20returns.%20For%20the%20evaluation%2C%20we%20propose%20to%20decompose%20missing%20rewards%0Ainto%20exploration%20and%20exploitation%20components%20by%20measuring%20the%20optimal%0Aachievable%20return%20for%20the%20states%20already%20explored.%20Our%20experiments%20with%20various%0ALLMs%20reveal%20that%20most%20models%20struggle%20to%20sufficiently%20explore%20the%20state-space%0Aand%20that%20weak%20exploration%20is%20insufficient.%20We%20observe%20a%20positive%20correlation%0Abetween%20model%20size%20and%20exploration%20performance%2C%20with%20larger%20models%0Ademonstrating%20superior%20capabilities.%20Furthermore%2C%20we%20show%20that%20our%0Adecomposition%20provides%20insights%20into%20differences%20in%20behaviors%20driven%20by%20agent%0Ainstructions%20during%20prompt%20engineering%2C%20offering%20a%20valuable%20tool%20for%20refining%0ALLM%20performance%20in%20exploratory%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08925v1&entry.124074799=Read"},
{"title": "A Closer Look at Deep Learning Methods on Tabular Datasets", "author": "Han-Jia Ye and Si-Yang Liu and Hao-Run Cai and Qi-Le Zhou and De-Chuan Zhan", "abstract": "  Tabular data is prevalent across diverse domains in machine learning. While\nclassical methods like tree-based models have long been effective, Deep Neural\nNetwork (DNN)-based methods have recently demonstrated promising performance.\nHowever, the diverse characteristics of methods and the inherent heterogeneity\nof tabular datasets make understanding and interpreting tabular methods both\nchallenging and prone to unstable observations. In this paper, we conduct\nin-depth evaluations and comprehensive analyses of tabular methods, with a\nparticular focus on DNN-based models, using a benchmark of over 300 tabular\ndatasets spanning a wide range of task types, sizes, and domains. First, we\nperform an extensive comparison of 32 state-of-the-art deep and tree-based\nmethods, evaluating their average performance across multiple criteria.\nAlthough method ranks vary across datasets, we empirically find that\ntop-performing methods tend to concentrate within a small subset of tabular\nmodels, regardless of the criteria used. Next, we investigate whether the\ntraining dynamics of deep tabular models can be predicted based on dataset\nproperties. This approach not only offers insights into the behavior of deep\ntabular methods but also identifies a core set of \"meta-features\" that reflect\ndataset heterogeneity. The other subset includes datasets where method ranks\nare consistent with the overall benchmark, acting as a reliable probe for\nfurther tabular analysis.\n", "link": "http://arxiv.org/abs/2407.00956v3", "date": "2025-01-15", "relevancy": 2.2841, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Closer%20Look%20at%20Deep%20Learning%20Methods%20on%20Tabular%20Datasets&body=Title%3A%20A%20Closer%20Look%20at%20Deep%20Learning%20Methods%20on%20Tabular%20Datasets%0AAuthor%3A%20Han-Jia%20Ye%20and%20Si-Yang%20Liu%20and%20Hao-Run%20Cai%20and%20Qi-Le%20Zhou%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20Tabular%20data%20is%20prevalent%20across%20diverse%20domains%20in%20machine%20learning.%20While%0Aclassical%20methods%20like%20tree-based%20models%20have%20long%20been%20effective%2C%20Deep%20Neural%0ANetwork%20%28DNN%29-based%20methods%20have%20recently%20demonstrated%20promising%20performance.%0AHowever%2C%20the%20diverse%20characteristics%20of%20methods%20and%20the%20inherent%20heterogeneity%0Aof%20tabular%20datasets%20make%20understanding%20and%20interpreting%20tabular%20methods%20both%0Achallenging%20and%20prone%20to%20unstable%20observations.%20In%20this%20paper%2C%20we%20conduct%0Ain-depth%20evaluations%20and%20comprehensive%20analyses%20of%20tabular%20methods%2C%20with%20a%0Aparticular%20focus%20on%20DNN-based%20models%2C%20using%20a%20benchmark%20of%20over%20300%20tabular%0Adatasets%20spanning%20a%20wide%20range%20of%20task%20types%2C%20sizes%2C%20and%20domains.%20First%2C%20we%0Aperform%20an%20extensive%20comparison%20of%2032%20state-of-the-art%20deep%20and%20tree-based%0Amethods%2C%20evaluating%20their%20average%20performance%20across%20multiple%20criteria.%0AAlthough%20method%20ranks%20vary%20across%20datasets%2C%20we%20empirically%20find%20that%0Atop-performing%20methods%20tend%20to%20concentrate%20within%20a%20small%20subset%20of%20tabular%0Amodels%2C%20regardless%20of%20the%20criteria%20used.%20Next%2C%20we%20investigate%20whether%20the%0Atraining%20dynamics%20of%20deep%20tabular%20models%20can%20be%20predicted%20based%20on%20dataset%0Aproperties.%20This%20approach%20not%20only%20offers%20insights%20into%20the%20behavior%20of%20deep%0Atabular%20methods%20but%20also%20identifies%20a%20core%20set%20of%20%22meta-features%22%20that%20reflect%0Adataset%20heterogeneity.%20The%20other%20subset%20includes%20datasets%20where%20method%20ranks%0Aare%20consistent%20with%20the%20overall%20benchmark%2C%20acting%20as%20a%20reliable%20probe%20for%0Afurther%20tabular%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00956v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Closer%2520Look%2520at%2520Deep%2520Learning%2520Methods%2520on%2520Tabular%2520Datasets%26entry.906535625%3DHan-Jia%2520Ye%2520and%2520Si-Yang%2520Liu%2520and%2520Hao-Run%2520Cai%2520and%2520Qi-Le%2520Zhou%2520and%2520De-Chuan%2520Zhan%26entry.1292438233%3D%2520%2520Tabular%2520data%2520is%2520prevalent%2520across%2520diverse%2520domains%2520in%2520machine%2520learning.%2520While%250Aclassical%2520methods%2520like%2520tree-based%2520models%2520have%2520long%2520been%2520effective%252C%2520Deep%2520Neural%250ANetwork%2520%2528DNN%2529-based%2520methods%2520have%2520recently%2520demonstrated%2520promising%2520performance.%250AHowever%252C%2520the%2520diverse%2520characteristics%2520of%2520methods%2520and%2520the%2520inherent%2520heterogeneity%250Aof%2520tabular%2520datasets%2520make%2520understanding%2520and%2520interpreting%2520tabular%2520methods%2520both%250Achallenging%2520and%2520prone%2520to%2520unstable%2520observations.%2520In%2520this%2520paper%252C%2520we%2520conduct%250Ain-depth%2520evaluations%2520and%2520comprehensive%2520analyses%2520of%2520tabular%2520methods%252C%2520with%2520a%250Aparticular%2520focus%2520on%2520DNN-based%2520models%252C%2520using%2520a%2520benchmark%2520of%2520over%2520300%2520tabular%250Adatasets%2520spanning%2520a%2520wide%2520range%2520of%2520task%2520types%252C%2520sizes%252C%2520and%2520domains.%2520First%252C%2520we%250Aperform%2520an%2520extensive%2520comparison%2520of%252032%2520state-of-the-art%2520deep%2520and%2520tree-based%250Amethods%252C%2520evaluating%2520their%2520average%2520performance%2520across%2520multiple%2520criteria.%250AAlthough%2520method%2520ranks%2520vary%2520across%2520datasets%252C%2520we%2520empirically%2520find%2520that%250Atop-performing%2520methods%2520tend%2520to%2520concentrate%2520within%2520a%2520small%2520subset%2520of%2520tabular%250Amodels%252C%2520regardless%2520of%2520the%2520criteria%2520used.%2520Next%252C%2520we%2520investigate%2520whether%2520the%250Atraining%2520dynamics%2520of%2520deep%2520tabular%2520models%2520can%2520be%2520predicted%2520based%2520on%2520dataset%250Aproperties.%2520This%2520approach%2520not%2520only%2520offers%2520insights%2520into%2520the%2520behavior%2520of%2520deep%250Atabular%2520methods%2520but%2520also%2520identifies%2520a%2520core%2520set%2520of%2520%2522meta-features%2522%2520that%2520reflect%250Adataset%2520heterogeneity.%2520The%2520other%2520subset%2520includes%2520datasets%2520where%2520method%2520ranks%250Aare%2520consistent%2520with%2520the%2520overall%2520benchmark%252C%2520acting%2520as%2520a%2520reliable%2520probe%2520for%250Afurther%2520tabular%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00956v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Closer%20Look%20at%20Deep%20Learning%20Methods%20on%20Tabular%20Datasets&entry.906535625=Han-Jia%20Ye%20and%20Si-Yang%20Liu%20and%20Hao-Run%20Cai%20and%20Qi-Le%20Zhou%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20Tabular%20data%20is%20prevalent%20across%20diverse%20domains%20in%20machine%20learning.%20While%0Aclassical%20methods%20like%20tree-based%20models%20have%20long%20been%20effective%2C%20Deep%20Neural%0ANetwork%20%28DNN%29-based%20methods%20have%20recently%20demonstrated%20promising%20performance.%0AHowever%2C%20the%20diverse%20characteristics%20of%20methods%20and%20the%20inherent%20heterogeneity%0Aof%20tabular%20datasets%20make%20understanding%20and%20interpreting%20tabular%20methods%20both%0Achallenging%20and%20prone%20to%20unstable%20observations.%20In%20this%20paper%2C%20we%20conduct%0Ain-depth%20evaluations%20and%20comprehensive%20analyses%20of%20tabular%20methods%2C%20with%20a%0Aparticular%20focus%20on%20DNN-based%20models%2C%20using%20a%20benchmark%20of%20over%20300%20tabular%0Adatasets%20spanning%20a%20wide%20range%20of%20task%20types%2C%20sizes%2C%20and%20domains.%20First%2C%20we%0Aperform%20an%20extensive%20comparison%20of%2032%20state-of-the-art%20deep%20and%20tree-based%0Amethods%2C%20evaluating%20their%20average%20performance%20across%20multiple%20criteria.%0AAlthough%20method%20ranks%20vary%20across%20datasets%2C%20we%20empirically%20find%20that%0Atop-performing%20methods%20tend%20to%20concentrate%20within%20a%20small%20subset%20of%20tabular%0Amodels%2C%20regardless%20of%20the%20criteria%20used.%20Next%2C%20we%20investigate%20whether%20the%0Atraining%20dynamics%20of%20deep%20tabular%20models%20can%20be%20predicted%20based%20on%20dataset%0Aproperties.%20This%20approach%20not%20only%20offers%20insights%20into%20the%20behavior%20of%20deep%0Atabular%20methods%20but%20also%20identifies%20a%20core%20set%20of%20%22meta-features%22%20that%20reflect%0Adataset%20heterogeneity.%20The%20other%20subset%20includes%20datasets%20where%20method%20ranks%0Aare%20consistent%20with%20the%20overall%20benchmark%2C%20acting%20as%20a%20reliable%20probe%20for%0Afurther%20tabular%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00956v3&entry.124074799=Read"},
{"title": "Compression with Global Guidance: Towards Training-free High-Resolution\n  MLLMs Acceleration", "author": "Xuyang Liu and Ziming Wang and Yuhang Han and Yingyao Wang and Jiale Yuan and Jun Song and Bo Zheng and Linfeng Zhang and Siteng Huang and Honggang Chen", "abstract": "  Multimodal large language models (MLLMs) have attracted considerable\nattention due to their exceptional performance in visual content understanding\nand reasoning. However, their inference efficiency has been a notable concern,\nas the increasing length of multimodal contexts leads to quadratic complexity.\nToken compression techniques, which reduce the number of visual tokens, have\ndemonstrated their effectiveness in reducing computational costs. Yet, these\napproaches have struggled to keep pace with the rapid advancements in MLLMs,\nespecially the AnyRes strategy in the context of high-resolution image\nunderstanding. In this paper, we propose a novel token compression method,\nGlobalCom$^2$, tailored for high-resolution MLLMs that receive both the\nthumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the\nthumbnail as the \"commander\" of the entire token compression process, directing\nthe allocation of retention ratios and the specific compression for each crop.\nIn this way, redundant tokens are eliminated while important local details are\nadaptively preserved to the highest extent feasible. Empirical results across\n10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between\nperformance and efficiency, and consistently outperforms state-of-the-art token\ncompression methods with LLaVA-NeXT-7B/13B models. Our code is released at\nhttps://github.com/xuyang-liu16/GlobalCom2.\n", "link": "http://arxiv.org/abs/2501.05179v2", "date": "2025-01-15", "relevancy": 2.2669, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5732}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5653}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compression%20with%20Global%20Guidance%3A%20Towards%20Training-free%20High-Resolution%0A%20%20MLLMs%20Acceleration&body=Title%3A%20Compression%20with%20Global%20Guidance%3A%20Towards%20Training-free%20High-Resolution%0A%20%20MLLMs%20Acceleration%0AAuthor%3A%20Xuyang%20Liu%20and%20Ziming%20Wang%20and%20Yuhang%20Han%20and%20Yingyao%20Wang%20and%20Jiale%20Yuan%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Linfeng%20Zhang%20and%20Siteng%20Huang%20and%20Honggang%20Chen%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20attracted%20considerable%0Aattention%20due%20to%20their%20exceptional%20performance%20in%20visual%20content%20understanding%0Aand%20reasoning.%20However%2C%20their%20inference%20efficiency%20has%20been%20a%20notable%20concern%2C%0Aas%20the%20increasing%20length%20of%20multimodal%20contexts%20leads%20to%20quadratic%20complexity.%0AToken%20compression%20techniques%2C%20which%20reduce%20the%20number%20of%20visual%20tokens%2C%20have%0Ademonstrated%20their%20effectiveness%20in%20reducing%20computational%20costs.%20Yet%2C%20these%0Aapproaches%20have%20struggled%20to%20keep%20pace%20with%20the%20rapid%20advancements%20in%20MLLMs%2C%0Aespecially%20the%20AnyRes%20strategy%20in%20the%20context%20of%20high-resolution%20image%0Aunderstanding.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20token%20compression%20method%2C%0AGlobalCom%24%5E2%24%2C%20tailored%20for%20high-resolution%20MLLMs%20that%20receive%20both%20the%0Athumbnail%20and%20multiple%20crops.%20GlobalCom%24%5E2%24%20treats%20the%20tokens%20derived%20from%20the%0Athumbnail%20as%20the%20%22commander%22%20of%20the%20entire%20token%20compression%20process%2C%20directing%0Athe%20allocation%20of%20retention%20ratios%20and%20the%20specific%20compression%20for%20each%20crop.%0AIn%20this%20way%2C%20redundant%20tokens%20are%20eliminated%20while%20important%20local%20details%20are%0Aadaptively%20preserved%20to%20the%20highest%20extent%20feasible.%20Empirical%20results%20across%0A10%20benchmarks%20reveal%20that%20GlobalCom%24%5E2%24%20achieves%20an%20optimal%20balance%20between%0Aperformance%20and%20efficiency%2C%20and%20consistently%20outperforms%20state-of-the-art%20token%0Acompression%20methods%20with%20LLaVA-NeXT-7B/13B%20models.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/xuyang-liu16/GlobalCom2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompression%2520with%2520Global%2520Guidance%253A%2520Towards%2520Training-free%2520High-Resolution%250A%2520%2520MLLMs%2520Acceleration%26entry.906535625%3DXuyang%2520Liu%2520and%2520Ziming%2520Wang%2520and%2520Yuhang%2520Han%2520and%2520Yingyao%2520Wang%2520and%2520Jiale%2520Yuan%2520and%2520Jun%2520Song%2520and%2520Bo%2520Zheng%2520and%2520Linfeng%2520Zhang%2520and%2520Siteng%2520Huang%2520and%2520Honggang%2520Chen%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520attracted%2520considerable%250Aattention%2520due%2520to%2520their%2520exceptional%2520performance%2520in%2520visual%2520content%2520understanding%250Aand%2520reasoning.%2520However%252C%2520their%2520inference%2520efficiency%2520has%2520been%2520a%2520notable%2520concern%252C%250Aas%2520the%2520increasing%2520length%2520of%2520multimodal%2520contexts%2520leads%2520to%2520quadratic%2520complexity.%250AToken%2520compression%2520techniques%252C%2520which%2520reduce%2520the%2520number%2520of%2520visual%2520tokens%252C%2520have%250Ademonstrated%2520their%2520effectiveness%2520in%2520reducing%2520computational%2520costs.%2520Yet%252C%2520these%250Aapproaches%2520have%2520struggled%2520to%2520keep%2520pace%2520with%2520the%2520rapid%2520advancements%2520in%2520MLLMs%252C%250Aespecially%2520the%2520AnyRes%2520strategy%2520in%2520the%2520context%2520of%2520high-resolution%2520image%250Aunderstanding.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520token%2520compression%2520method%252C%250AGlobalCom%2524%255E2%2524%252C%2520tailored%2520for%2520high-resolution%2520MLLMs%2520that%2520receive%2520both%2520the%250Athumbnail%2520and%2520multiple%2520crops.%2520GlobalCom%2524%255E2%2524%2520treats%2520the%2520tokens%2520derived%2520from%2520the%250Athumbnail%2520as%2520the%2520%2522commander%2522%2520of%2520the%2520entire%2520token%2520compression%2520process%252C%2520directing%250Athe%2520allocation%2520of%2520retention%2520ratios%2520and%2520the%2520specific%2520compression%2520for%2520each%2520crop.%250AIn%2520this%2520way%252C%2520redundant%2520tokens%2520are%2520eliminated%2520while%2520important%2520local%2520details%2520are%250Aadaptively%2520preserved%2520to%2520the%2520highest%2520extent%2520feasible.%2520Empirical%2520results%2520across%250A10%2520benchmarks%2520reveal%2520that%2520GlobalCom%2524%255E2%2524%2520achieves%2520an%2520optimal%2520balance%2520between%250Aperformance%2520and%2520efficiency%252C%2520and%2520consistently%2520outperforms%2520state-of-the-art%2520token%250Acompression%2520methods%2520with%2520LLaVA-NeXT-7B/13B%2520models.%2520Our%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/xuyang-liu16/GlobalCom2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compression%20with%20Global%20Guidance%3A%20Towards%20Training-free%20High-Resolution%0A%20%20MLLMs%20Acceleration&entry.906535625=Xuyang%20Liu%20and%20Ziming%20Wang%20and%20Yuhang%20Han%20and%20Yingyao%20Wang%20and%20Jiale%20Yuan%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Linfeng%20Zhang%20and%20Siteng%20Huang%20and%20Honggang%20Chen&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20attracted%20considerable%0Aattention%20due%20to%20their%20exceptional%20performance%20in%20visual%20content%20understanding%0Aand%20reasoning.%20However%2C%20their%20inference%20efficiency%20has%20been%20a%20notable%20concern%2C%0Aas%20the%20increasing%20length%20of%20multimodal%20contexts%20leads%20to%20quadratic%20complexity.%0AToken%20compression%20techniques%2C%20which%20reduce%20the%20number%20of%20visual%20tokens%2C%20have%0Ademonstrated%20their%20effectiveness%20in%20reducing%20computational%20costs.%20Yet%2C%20these%0Aapproaches%20have%20struggled%20to%20keep%20pace%20with%20the%20rapid%20advancements%20in%20MLLMs%2C%0Aespecially%20the%20AnyRes%20strategy%20in%20the%20context%20of%20high-resolution%20image%0Aunderstanding.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20token%20compression%20method%2C%0AGlobalCom%24%5E2%24%2C%20tailored%20for%20high-resolution%20MLLMs%20that%20receive%20both%20the%0Athumbnail%20and%20multiple%20crops.%20GlobalCom%24%5E2%24%20treats%20the%20tokens%20derived%20from%20the%0Athumbnail%20as%20the%20%22commander%22%20of%20the%20entire%20token%20compression%20process%2C%20directing%0Athe%20allocation%20of%20retention%20ratios%20and%20the%20specific%20compression%20for%20each%20crop.%0AIn%20this%20way%2C%20redundant%20tokens%20are%20eliminated%20while%20important%20local%20details%20are%0Aadaptively%20preserved%20to%20the%20highest%20extent%20feasible.%20Empirical%20results%20across%0A10%20benchmarks%20reveal%20that%20GlobalCom%24%5E2%24%20achieves%20an%20optimal%20balance%20between%0Aperformance%20and%20efficiency%2C%20and%20consistently%20outperforms%20state-of-the-art%20token%0Acompression%20methods%20with%20LLaVA-NeXT-7B/13B%20models.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/xuyang-liu16/GlobalCom2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05179v2&entry.124074799=Read"},
{"title": "Mind the Error! Detection and Localization of Instruction Errors in\n  Vision-and-Language Navigation", "author": "Francesco Taioli and Stefano Rosa and Alberto Castellini and Lorenzo Natale and Alessio Del Bue and Alessandro Farinelli and Marco Cristani and Yiming Wang", "abstract": "  Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of\nthe most intuitive yet challenging embodied AI tasks. Agents are tasked to\nnavigate towards a target goal by executing a set of low-level actions,\nfollowing a series of natural language instructions. All VLN-CE methods in the\nliterature assume that language instructions are exact. However, in practice,\ninstructions given by humans can contain errors when describing a spatial\nenvironment due to inaccurate memory or confusion. Current VLN-CE benchmarks do\nnot address this scenario, making the state-of-the-art methods in VLN-CE\nfragile in the presence of erroneous instructions from human users. For the\nfirst time, we propose a novel benchmark dataset that introduces various types\nof instruction errors considering potential human causes. This benchmark\nprovides valuable insight into the robustness of VLN systems in continuous\nenvironments. We observe a noticeable performance drop (up to -25%) in Success\nRate when evaluating the state-of-the-art VLN-CE methods on our benchmark.\nMoreover, we formally define the task of Instruction Error Detection and\nLocalization, and establish an evaluation protocol on top of our benchmark\ndataset. We also propose an effective method, based on a cross-modal\ntransformer architecture, that achieves the best performance in error detection\nand localization, compared to baselines. Surprisingly, our proposed method has\nrevealed errors in the validation set of the two commonly used datasets for\nVLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in\nother tasks. Code and dataset available at\nhttps://intelligolabs.github.io/R2RIE-CE\n", "link": "http://arxiv.org/abs/2403.10700v2", "date": "2025-01-15", "relevancy": 2.2115, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Error%21%20Detection%20and%20Localization%20of%20Instruction%20Errors%20in%0A%20%20Vision-and-Language%20Navigation&body=Title%3A%20Mind%20the%20Error%21%20Detection%20and%20Localization%20of%20Instruction%20Errors%20in%0A%20%20Vision-and-Language%20Navigation%0AAuthor%3A%20Francesco%20Taioli%20and%20Stefano%20Rosa%20and%20Alberto%20Castellini%20and%20Lorenzo%20Natale%20and%20Alessio%20Del%20Bue%20and%20Alessandro%20Farinelli%20and%20Marco%20Cristani%20and%20Yiming%20Wang%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20in%20Continuous%20Environments%20%28VLN-CE%29%20is%20one%20of%0Athe%20most%20intuitive%20yet%20challenging%20embodied%20AI%20tasks.%20Agents%20are%20tasked%20to%0Anavigate%20towards%20a%20target%20goal%20by%20executing%20a%20set%20of%20low-level%20actions%2C%0Afollowing%20a%20series%20of%20natural%20language%20instructions.%20All%20VLN-CE%20methods%20in%20the%0Aliterature%20assume%20that%20language%20instructions%20are%20exact.%20However%2C%20in%20practice%2C%0Ainstructions%20given%20by%20humans%20can%20contain%20errors%20when%20describing%20a%20spatial%0Aenvironment%20due%20to%20inaccurate%20memory%20or%20confusion.%20Current%20VLN-CE%20benchmarks%20do%0Anot%20address%20this%20scenario%2C%20making%20the%20state-of-the-art%20methods%20in%20VLN-CE%0Afragile%20in%20the%20presence%20of%20erroneous%20instructions%20from%20human%20users.%20For%20the%0Afirst%20time%2C%20we%20propose%20a%20novel%20benchmark%20dataset%20that%20introduces%20various%20types%0Aof%20instruction%20errors%20considering%20potential%20human%20causes.%20This%20benchmark%0Aprovides%20valuable%20insight%20into%20the%20robustness%20of%20VLN%20systems%20in%20continuous%0Aenvironments.%20We%20observe%20a%20noticeable%20performance%20drop%20%28up%20to%20-25%25%29%20in%20Success%0ARate%20when%20evaluating%20the%20state-of-the-art%20VLN-CE%20methods%20on%20our%20benchmark.%0AMoreover%2C%20we%20formally%20define%20the%20task%20of%20Instruction%20Error%20Detection%20and%0ALocalization%2C%20and%20establish%20an%20evaluation%20protocol%20on%20top%20of%20our%20benchmark%0Adataset.%20We%20also%20propose%20an%20effective%20method%2C%20based%20on%20a%20cross-modal%0Atransformer%20architecture%2C%20that%20achieves%20the%20best%20performance%20in%20error%20detection%0Aand%20localization%2C%20compared%20to%20baselines.%20Surprisingly%2C%20our%20proposed%20method%20has%0Arevealed%20errors%20in%20the%20validation%20set%20of%20the%20two%20commonly%20used%20datasets%20for%0AVLN-CE%2C%20i.e.%2C%20R2R-CE%20and%20RxR-CE%2C%20demonstrating%20the%20utility%20of%20our%20technique%20in%0Aother%20tasks.%20Code%20and%20dataset%20available%20at%0Ahttps%3A//intelligolabs.github.io/R2RIE-CE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Error%2521%2520Detection%2520and%2520Localization%2520of%2520Instruction%2520Errors%2520in%250A%2520%2520Vision-and-Language%2520Navigation%26entry.906535625%3DFrancesco%2520Taioli%2520and%2520Stefano%2520Rosa%2520and%2520Alberto%2520Castellini%2520and%2520Lorenzo%2520Natale%2520and%2520Alessio%2520Del%2520Bue%2520and%2520Alessandro%2520Farinelli%2520and%2520Marco%2520Cristani%2520and%2520Yiming%2520Wang%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520in%2520Continuous%2520Environments%2520%2528VLN-CE%2529%2520is%2520one%2520of%250Athe%2520most%2520intuitive%2520yet%2520challenging%2520embodied%2520AI%2520tasks.%2520Agents%2520are%2520tasked%2520to%250Anavigate%2520towards%2520a%2520target%2520goal%2520by%2520executing%2520a%2520set%2520of%2520low-level%2520actions%252C%250Afollowing%2520a%2520series%2520of%2520natural%2520language%2520instructions.%2520All%2520VLN-CE%2520methods%2520in%2520the%250Aliterature%2520assume%2520that%2520language%2520instructions%2520are%2520exact.%2520However%252C%2520in%2520practice%252C%250Ainstructions%2520given%2520by%2520humans%2520can%2520contain%2520errors%2520when%2520describing%2520a%2520spatial%250Aenvironment%2520due%2520to%2520inaccurate%2520memory%2520or%2520confusion.%2520Current%2520VLN-CE%2520benchmarks%2520do%250Anot%2520address%2520this%2520scenario%252C%2520making%2520the%2520state-of-the-art%2520methods%2520in%2520VLN-CE%250Afragile%2520in%2520the%2520presence%2520of%2520erroneous%2520instructions%2520from%2520human%2520users.%2520For%2520the%250Afirst%2520time%252C%2520we%2520propose%2520a%2520novel%2520benchmark%2520dataset%2520that%2520introduces%2520various%2520types%250Aof%2520instruction%2520errors%2520considering%2520potential%2520human%2520causes.%2520This%2520benchmark%250Aprovides%2520valuable%2520insight%2520into%2520the%2520robustness%2520of%2520VLN%2520systems%2520in%2520continuous%250Aenvironments.%2520We%2520observe%2520a%2520noticeable%2520performance%2520drop%2520%2528up%2520to%2520-25%2525%2529%2520in%2520Success%250ARate%2520when%2520evaluating%2520the%2520state-of-the-art%2520VLN-CE%2520methods%2520on%2520our%2520benchmark.%250AMoreover%252C%2520we%2520formally%2520define%2520the%2520task%2520of%2520Instruction%2520Error%2520Detection%2520and%250ALocalization%252C%2520and%2520establish%2520an%2520evaluation%2520protocol%2520on%2520top%2520of%2520our%2520benchmark%250Adataset.%2520We%2520also%2520propose%2520an%2520effective%2520method%252C%2520based%2520on%2520a%2520cross-modal%250Atransformer%2520architecture%252C%2520that%2520achieves%2520the%2520best%2520performance%2520in%2520error%2520detection%250Aand%2520localization%252C%2520compared%2520to%2520baselines.%2520Surprisingly%252C%2520our%2520proposed%2520method%2520has%250Arevealed%2520errors%2520in%2520the%2520validation%2520set%2520of%2520the%2520two%2520commonly%2520used%2520datasets%2520for%250AVLN-CE%252C%2520i.e.%252C%2520R2R-CE%2520and%2520RxR-CE%252C%2520demonstrating%2520the%2520utility%2520of%2520our%2520technique%2520in%250Aother%2520tasks.%2520Code%2520and%2520dataset%2520available%2520at%250Ahttps%253A//intelligolabs.github.io/R2RIE-CE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Error%21%20Detection%20and%20Localization%20of%20Instruction%20Errors%20in%0A%20%20Vision-and-Language%20Navigation&entry.906535625=Francesco%20Taioli%20and%20Stefano%20Rosa%20and%20Alberto%20Castellini%20and%20Lorenzo%20Natale%20and%20Alessio%20Del%20Bue%20and%20Alessandro%20Farinelli%20and%20Marco%20Cristani%20and%20Yiming%20Wang&entry.1292438233=%20%20Vision-and-Language%20Navigation%20in%20Continuous%20Environments%20%28VLN-CE%29%20is%20one%20of%0Athe%20most%20intuitive%20yet%20challenging%20embodied%20AI%20tasks.%20Agents%20are%20tasked%20to%0Anavigate%20towards%20a%20target%20goal%20by%20executing%20a%20set%20of%20low-level%20actions%2C%0Afollowing%20a%20series%20of%20natural%20language%20instructions.%20All%20VLN-CE%20methods%20in%20the%0Aliterature%20assume%20that%20language%20instructions%20are%20exact.%20However%2C%20in%20practice%2C%0Ainstructions%20given%20by%20humans%20can%20contain%20errors%20when%20describing%20a%20spatial%0Aenvironment%20due%20to%20inaccurate%20memory%20or%20confusion.%20Current%20VLN-CE%20benchmarks%20do%0Anot%20address%20this%20scenario%2C%20making%20the%20state-of-the-art%20methods%20in%20VLN-CE%0Afragile%20in%20the%20presence%20of%20erroneous%20instructions%20from%20human%20users.%20For%20the%0Afirst%20time%2C%20we%20propose%20a%20novel%20benchmark%20dataset%20that%20introduces%20various%20types%0Aof%20instruction%20errors%20considering%20potential%20human%20causes.%20This%20benchmark%0Aprovides%20valuable%20insight%20into%20the%20robustness%20of%20VLN%20systems%20in%20continuous%0Aenvironments.%20We%20observe%20a%20noticeable%20performance%20drop%20%28up%20to%20-25%25%29%20in%20Success%0ARate%20when%20evaluating%20the%20state-of-the-art%20VLN-CE%20methods%20on%20our%20benchmark.%0AMoreover%2C%20we%20formally%20define%20the%20task%20of%20Instruction%20Error%20Detection%20and%0ALocalization%2C%20and%20establish%20an%20evaluation%20protocol%20on%20top%20of%20our%20benchmark%0Adataset.%20We%20also%20propose%20an%20effective%20method%2C%20based%20on%20a%20cross-modal%0Atransformer%20architecture%2C%20that%20achieves%20the%20best%20performance%20in%20error%20detection%0Aand%20localization%2C%20compared%20to%20baselines.%20Surprisingly%2C%20our%20proposed%20method%20has%0Arevealed%20errors%20in%20the%20validation%20set%20of%20the%20two%20commonly%20used%20datasets%20for%0AVLN-CE%2C%20i.e.%2C%20R2R-CE%20and%20RxR-CE%2C%20demonstrating%20the%20utility%20of%20our%20technique%20in%0Aother%20tasks.%20Code%20and%20dataset%20available%20at%0Ahttps%3A//intelligolabs.github.io/R2RIE-CE%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10700v2&entry.124074799=Read"},
{"title": "Industrial Anomaly Detection and Localization Using Weakly-Supervised\n  Residual Transformers", "author": "Hanxi Li and Jingqi Wu and Deyin Liu and Lin Wu and Hao Chen and Mingwen Wang and Chunhua Shen", "abstract": "  Recent advancements in industrial anomaly detection (AD) have demonstrated\nthat incorporating a small number of anomalous samples during training can\nsignificantly enhance accuracy. However, this improvement often comes at the\ncost of extensive annotation efforts, which are impractical for many real-world\napplications. In this paper, we introduce a novel framework, Weak}ly-supervised\nRESidual Transformer (WeakREST), designed to achieve high anomaly detection\naccuracy while minimizing the reliance on manual annotations. First, we\nreformulate the pixel-wise anomaly localization task into a block-wise\nclassification problem. Second, we introduce a residual-based feature\nrepresentation called Positional Fast Anomaly Residuals (PosFAR) which captures\nanomalous patterns more effectively. To leverage this feature, we adapt the\nSwin Transformer for enhanced anomaly detection and localization. Additionally,\nwe propose a weak annotation approach, utilizing bounding boxes and image tags\nto define anomalous regions. This approach establishes a semi-supervised\nlearning context that reduces the dependency on precise pixel-level labels. To\nfurther improve the learning process, we develop a novel ResMixMatch algorithm,\ncapable of handling the interplay between weak labels and residual-based\nrepresentations.\n  On the benchmark dataset MVTec-AD, our method achieves an Average Precision\n(AP) of $83.0\\%$, surpassing the previous best result of $82.7\\%$ in the\nunsupervised setting. In the supervised AD setting, WeakREST attains an AP of\n$87.6\\%$, outperforming the previous best of $86.0\\%$. Notably, even when using\nweaker annotations such as bounding boxes, WeakREST exceeds the performance of\nleading methods relying on pixel-wise supervision, achieving an AP of $87.1\\%$\ncompared to the prior best of $86.0\\%$ on MVTec-AD.\n", "link": "http://arxiv.org/abs/2306.03492v6", "date": "2025-01-15", "relevancy": 2.2114, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5858}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5529}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Industrial%20Anomaly%20Detection%20and%20Localization%20Using%20Weakly-Supervised%0A%20%20Residual%20Transformers&body=Title%3A%20Industrial%20Anomaly%20Detection%20and%20Localization%20Using%20Weakly-Supervised%0A%20%20Residual%20Transformers%0AAuthor%3A%20Hanxi%20Li%20and%20Jingqi%20Wu%20and%20Deyin%20Liu%20and%20Lin%20Wu%20and%20Hao%20Chen%20and%20Mingwen%20Wang%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20industrial%20anomaly%20detection%20%28AD%29%20have%20demonstrated%0Athat%20incorporating%20a%20small%20number%20of%20anomalous%20samples%20during%20training%20can%0Asignificantly%20enhance%20accuracy.%20However%2C%20this%20improvement%20often%20comes%20at%20the%0Acost%20of%20extensive%20annotation%20efforts%2C%20which%20are%20impractical%20for%20many%20real-world%0Aapplications.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%2C%20Weak%7Dly-supervised%0ARESidual%20Transformer%20%28WeakREST%29%2C%20designed%20to%20achieve%20high%20anomaly%20detection%0Aaccuracy%20while%20minimizing%20the%20reliance%20on%20manual%20annotations.%20First%2C%20we%0Areformulate%20the%20pixel-wise%20anomaly%20localization%20task%20into%20a%20block-wise%0Aclassification%20problem.%20Second%2C%20we%20introduce%20a%20residual-based%20feature%0Arepresentation%20called%20Positional%20Fast%20Anomaly%20Residuals%20%28PosFAR%29%20which%20captures%0Aanomalous%20patterns%20more%20effectively.%20To%20leverage%20this%20feature%2C%20we%20adapt%20the%0ASwin%20Transformer%20for%20enhanced%20anomaly%20detection%20and%20localization.%20Additionally%2C%0Awe%20propose%20a%20weak%20annotation%20approach%2C%20utilizing%20bounding%20boxes%20and%20image%20tags%0Ato%20define%20anomalous%20regions.%20This%20approach%20establishes%20a%20semi-supervised%0Alearning%20context%20that%20reduces%20the%20dependency%20on%20precise%20pixel-level%20labels.%20To%0Afurther%20improve%20the%20learning%20process%2C%20we%20develop%20a%20novel%20ResMixMatch%20algorithm%2C%0Acapable%20of%20handling%20the%20interplay%20between%20weak%20labels%20and%20residual-based%0Arepresentations.%0A%20%20On%20the%20benchmark%20dataset%20MVTec-AD%2C%20our%20method%20achieves%20an%20Average%20Precision%0A%28AP%29%20of%20%2483.0%5C%25%24%2C%20surpassing%20the%20previous%20best%20result%20of%20%2482.7%5C%25%24%20in%20the%0Aunsupervised%20setting.%20In%20the%20supervised%20AD%20setting%2C%20WeakREST%20attains%20an%20AP%20of%0A%2487.6%5C%25%24%2C%20outperforming%20the%20previous%20best%20of%20%2486.0%5C%25%24.%20Notably%2C%20even%20when%20using%0Aweaker%20annotations%20such%20as%20bounding%20boxes%2C%20WeakREST%20exceeds%20the%20performance%20of%0Aleading%20methods%20relying%20on%20pixel-wise%20supervision%2C%20achieving%20an%20AP%20of%20%2487.1%5C%25%24%0Acompared%20to%20the%20prior%20best%20of%20%2486.0%5C%25%24%20on%20MVTec-AD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.03492v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndustrial%2520Anomaly%2520Detection%2520and%2520Localization%2520Using%2520Weakly-Supervised%250A%2520%2520Residual%2520Transformers%26entry.906535625%3DHanxi%2520Li%2520and%2520Jingqi%2520Wu%2520and%2520Deyin%2520Liu%2520and%2520Lin%2520Wu%2520and%2520Hao%2520Chen%2520and%2520Mingwen%2520Wang%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520industrial%2520anomaly%2520detection%2520%2528AD%2529%2520have%2520demonstrated%250Athat%2520incorporating%2520a%2520small%2520number%2520of%2520anomalous%2520samples%2520during%2520training%2520can%250Asignificantly%2520enhance%2520accuracy.%2520However%252C%2520this%2520improvement%2520often%2520comes%2520at%2520the%250Acost%2520of%2520extensive%2520annotation%2520efforts%252C%2520which%2520are%2520impractical%2520for%2520many%2520real-world%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520framework%252C%2520Weak%257Dly-supervised%250ARESidual%2520Transformer%2520%2528WeakREST%2529%252C%2520designed%2520to%2520achieve%2520high%2520anomaly%2520detection%250Aaccuracy%2520while%2520minimizing%2520the%2520reliance%2520on%2520manual%2520annotations.%2520First%252C%2520we%250Areformulate%2520the%2520pixel-wise%2520anomaly%2520localization%2520task%2520into%2520a%2520block-wise%250Aclassification%2520problem.%2520Second%252C%2520we%2520introduce%2520a%2520residual-based%2520feature%250Arepresentation%2520called%2520Positional%2520Fast%2520Anomaly%2520Residuals%2520%2528PosFAR%2529%2520which%2520captures%250Aanomalous%2520patterns%2520more%2520effectively.%2520To%2520leverage%2520this%2520feature%252C%2520we%2520adapt%2520the%250ASwin%2520Transformer%2520for%2520enhanced%2520anomaly%2520detection%2520and%2520localization.%2520Additionally%252C%250Awe%2520propose%2520a%2520weak%2520annotation%2520approach%252C%2520utilizing%2520bounding%2520boxes%2520and%2520image%2520tags%250Ato%2520define%2520anomalous%2520regions.%2520This%2520approach%2520establishes%2520a%2520semi-supervised%250Alearning%2520context%2520that%2520reduces%2520the%2520dependency%2520on%2520precise%2520pixel-level%2520labels.%2520To%250Afurther%2520improve%2520the%2520learning%2520process%252C%2520we%2520develop%2520a%2520novel%2520ResMixMatch%2520algorithm%252C%250Acapable%2520of%2520handling%2520the%2520interplay%2520between%2520weak%2520labels%2520and%2520residual-based%250Arepresentations.%250A%2520%2520On%2520the%2520benchmark%2520dataset%2520MVTec-AD%252C%2520our%2520method%2520achieves%2520an%2520Average%2520Precision%250A%2528AP%2529%2520of%2520%252483.0%255C%2525%2524%252C%2520surpassing%2520the%2520previous%2520best%2520result%2520of%2520%252482.7%255C%2525%2524%2520in%2520the%250Aunsupervised%2520setting.%2520In%2520the%2520supervised%2520AD%2520setting%252C%2520WeakREST%2520attains%2520an%2520AP%2520of%250A%252487.6%255C%2525%2524%252C%2520outperforming%2520the%2520previous%2520best%2520of%2520%252486.0%255C%2525%2524.%2520Notably%252C%2520even%2520when%2520using%250Aweaker%2520annotations%2520such%2520as%2520bounding%2520boxes%252C%2520WeakREST%2520exceeds%2520the%2520performance%2520of%250Aleading%2520methods%2520relying%2520on%2520pixel-wise%2520supervision%252C%2520achieving%2520an%2520AP%2520of%2520%252487.1%255C%2525%2524%250Acompared%2520to%2520the%2520prior%2520best%2520of%2520%252486.0%255C%2525%2524%2520on%2520MVTec-AD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.03492v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Industrial%20Anomaly%20Detection%20and%20Localization%20Using%20Weakly-Supervised%0A%20%20Residual%20Transformers&entry.906535625=Hanxi%20Li%20and%20Jingqi%20Wu%20and%20Deyin%20Liu%20and%20Lin%20Wu%20and%20Hao%20Chen%20and%20Mingwen%20Wang%20and%20Chunhua%20Shen&entry.1292438233=%20%20Recent%20advancements%20in%20industrial%20anomaly%20detection%20%28AD%29%20have%20demonstrated%0Athat%20incorporating%20a%20small%20number%20of%20anomalous%20samples%20during%20training%20can%0Asignificantly%20enhance%20accuracy.%20However%2C%20this%20improvement%20often%20comes%20at%20the%0Acost%20of%20extensive%20annotation%20efforts%2C%20which%20are%20impractical%20for%20many%20real-world%0Aapplications.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%2C%20Weak%7Dly-supervised%0ARESidual%20Transformer%20%28WeakREST%29%2C%20designed%20to%20achieve%20high%20anomaly%20detection%0Aaccuracy%20while%20minimizing%20the%20reliance%20on%20manual%20annotations.%20First%2C%20we%0Areformulate%20the%20pixel-wise%20anomaly%20localization%20task%20into%20a%20block-wise%0Aclassification%20problem.%20Second%2C%20we%20introduce%20a%20residual-based%20feature%0Arepresentation%20called%20Positional%20Fast%20Anomaly%20Residuals%20%28PosFAR%29%20which%20captures%0Aanomalous%20patterns%20more%20effectively.%20To%20leverage%20this%20feature%2C%20we%20adapt%20the%0ASwin%20Transformer%20for%20enhanced%20anomaly%20detection%20and%20localization.%20Additionally%2C%0Awe%20propose%20a%20weak%20annotation%20approach%2C%20utilizing%20bounding%20boxes%20and%20image%20tags%0Ato%20define%20anomalous%20regions.%20This%20approach%20establishes%20a%20semi-supervised%0Alearning%20context%20that%20reduces%20the%20dependency%20on%20precise%20pixel-level%20labels.%20To%0Afurther%20improve%20the%20learning%20process%2C%20we%20develop%20a%20novel%20ResMixMatch%20algorithm%2C%0Acapable%20of%20handling%20the%20interplay%20between%20weak%20labels%20and%20residual-based%0Arepresentations.%0A%20%20On%20the%20benchmark%20dataset%20MVTec-AD%2C%20our%20method%20achieves%20an%20Average%20Precision%0A%28AP%29%20of%20%2483.0%5C%25%24%2C%20surpassing%20the%20previous%20best%20result%20of%20%2482.7%5C%25%24%20in%20the%0Aunsupervised%20setting.%20In%20the%20supervised%20AD%20setting%2C%20WeakREST%20attains%20an%20AP%20of%0A%2487.6%5C%25%24%2C%20outperforming%20the%20previous%20best%20of%20%2486.0%5C%25%24.%20Notably%2C%20even%20when%20using%0Aweaker%20annotations%20such%20as%20bounding%20boxes%2C%20WeakREST%20exceeds%20the%20performance%20of%0Aleading%20methods%20relying%20on%20pixel-wise%20supervision%2C%20achieving%20an%20AP%20of%20%2487.1%5C%25%24%0Acompared%20to%20the%20prior%20best%20of%20%2486.0%5C%25%24%20on%20MVTec-AD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.03492v6&entry.124074799=Read"},
{"title": "Exploring Task-Level Optimal Prompts for Visual In-Context Learning", "author": "Yan Zhu and Huan Ma and Changqing Zhang", "abstract": "  With the development of Vision Foundation Models (VFMs) in recent years,\nVisual In-Context Learning (VICL) has become a better choice compared to\nmodifying models in most scenarios. Different from retraining or fine-tuning\nmodel, VICL does not require modifications to the model's weights or\narchitecture, and only needs a prompt with demonstrations to teach VFM how to\nsolve tasks. Currently, significant computational cost for finding optimal\nprompts for every test sample hinders the deployment of VICL, as determining\nwhich demonstrations to use for constructing prompts is very costly. In this\npaper, however, we find a counterintuitive phenomenon that most test samples\nactually achieve optimal performance under the same prompts, and searching for\nsample-level prompts only costs more time but results in completely identical\nprompts. Therefore, we propose task-level prompting to reduce the cost of\nsearching for prompts during the inference stage and introduce two time-saving\nyet effective task-level prompt search strategies. Extensive experimental\nresults show that our proposed method can identify near-optimal prompts and\nreach the best VICL performance with a minimal cost that prior work has never\nachieved.\n", "link": "http://arxiv.org/abs/2501.08841v1", "date": "2025-01-15", "relevancy": 2.158, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Task-Level%20Optimal%20Prompts%20for%20Visual%20In-Context%20Learning&body=Title%3A%20Exploring%20Task-Level%20Optimal%20Prompts%20for%20Visual%20In-Context%20Learning%0AAuthor%3A%20Yan%20Zhu%20and%20Huan%20Ma%20and%20Changqing%20Zhang%0AAbstract%3A%20%20%20With%20the%20development%20of%20Vision%20Foundation%20Models%20%28VFMs%29%20in%20recent%20years%2C%0AVisual%20In-Context%20Learning%20%28VICL%29%20has%20become%20a%20better%20choice%20compared%20to%0Amodifying%20models%20in%20most%20scenarios.%20Different%20from%20retraining%20or%20fine-tuning%0Amodel%2C%20VICL%20does%20not%20require%20modifications%20to%20the%20model%27s%20weights%20or%0Aarchitecture%2C%20and%20only%20needs%20a%20prompt%20with%20demonstrations%20to%20teach%20VFM%20how%20to%0Asolve%20tasks.%20Currently%2C%20significant%20computational%20cost%20for%20finding%20optimal%0Aprompts%20for%20every%20test%20sample%20hinders%20the%20deployment%20of%20VICL%2C%20as%20determining%0Awhich%20demonstrations%20to%20use%20for%20constructing%20prompts%20is%20very%20costly.%20In%20this%0Apaper%2C%20however%2C%20we%20find%20a%20counterintuitive%20phenomenon%20that%20most%20test%20samples%0Aactually%20achieve%20optimal%20performance%20under%20the%20same%20prompts%2C%20and%20searching%20for%0Asample-level%20prompts%20only%20costs%20more%20time%20but%20results%20in%20completely%20identical%0Aprompts.%20Therefore%2C%20we%20propose%20task-level%20prompting%20to%20reduce%20the%20cost%20of%0Asearching%20for%20prompts%20during%20the%20inference%20stage%20and%20introduce%20two%20time-saving%0Ayet%20effective%20task-level%20prompt%20search%20strategies.%20Extensive%20experimental%0Aresults%20show%20that%20our%20proposed%20method%20can%20identify%20near-optimal%20prompts%20and%0Areach%20the%20best%20VICL%20performance%20with%20a%20minimal%20cost%20that%20prior%20work%20has%20never%0Aachieved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Task-Level%2520Optimal%2520Prompts%2520for%2520Visual%2520In-Context%2520Learning%26entry.906535625%3DYan%2520Zhu%2520and%2520Huan%2520Ma%2520and%2520Changqing%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%2520in%2520recent%2520years%252C%250AVisual%2520In-Context%2520Learning%2520%2528VICL%2529%2520has%2520become%2520a%2520better%2520choice%2520compared%2520to%250Amodifying%2520models%2520in%2520most%2520scenarios.%2520Different%2520from%2520retraining%2520or%2520fine-tuning%250Amodel%252C%2520VICL%2520does%2520not%2520require%2520modifications%2520to%2520the%2520model%2527s%2520weights%2520or%250Aarchitecture%252C%2520and%2520only%2520needs%2520a%2520prompt%2520with%2520demonstrations%2520to%2520teach%2520VFM%2520how%2520to%250Asolve%2520tasks.%2520Currently%252C%2520significant%2520computational%2520cost%2520for%2520finding%2520optimal%250Aprompts%2520for%2520every%2520test%2520sample%2520hinders%2520the%2520deployment%2520of%2520VICL%252C%2520as%2520determining%250Awhich%2520demonstrations%2520to%2520use%2520for%2520constructing%2520prompts%2520is%2520very%2520costly.%2520In%2520this%250Apaper%252C%2520however%252C%2520we%2520find%2520a%2520counterintuitive%2520phenomenon%2520that%2520most%2520test%2520samples%250Aactually%2520achieve%2520optimal%2520performance%2520under%2520the%2520same%2520prompts%252C%2520and%2520searching%2520for%250Asample-level%2520prompts%2520only%2520costs%2520more%2520time%2520but%2520results%2520in%2520completely%2520identical%250Aprompts.%2520Therefore%252C%2520we%2520propose%2520task-level%2520prompting%2520to%2520reduce%2520the%2520cost%2520of%250Asearching%2520for%2520prompts%2520during%2520the%2520inference%2520stage%2520and%2520introduce%2520two%2520time-saving%250Ayet%2520effective%2520task-level%2520prompt%2520search%2520strategies.%2520Extensive%2520experimental%250Aresults%2520show%2520that%2520our%2520proposed%2520method%2520can%2520identify%2520near-optimal%2520prompts%2520and%250Areach%2520the%2520best%2520VICL%2520performance%2520with%2520a%2520minimal%2520cost%2520that%2520prior%2520work%2520has%2520never%250Aachieved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Task-Level%20Optimal%20Prompts%20for%20Visual%20In-Context%20Learning&entry.906535625=Yan%20Zhu%20and%20Huan%20Ma%20and%20Changqing%20Zhang&entry.1292438233=%20%20With%20the%20development%20of%20Vision%20Foundation%20Models%20%28VFMs%29%20in%20recent%20years%2C%0AVisual%20In-Context%20Learning%20%28VICL%29%20has%20become%20a%20better%20choice%20compared%20to%0Amodifying%20models%20in%20most%20scenarios.%20Different%20from%20retraining%20or%20fine-tuning%0Amodel%2C%20VICL%20does%20not%20require%20modifications%20to%20the%20model%27s%20weights%20or%0Aarchitecture%2C%20and%20only%20needs%20a%20prompt%20with%20demonstrations%20to%20teach%20VFM%20how%20to%0Asolve%20tasks.%20Currently%2C%20significant%20computational%20cost%20for%20finding%20optimal%0Aprompts%20for%20every%20test%20sample%20hinders%20the%20deployment%20of%20VICL%2C%20as%20determining%0Awhich%20demonstrations%20to%20use%20for%20constructing%20prompts%20is%20very%20costly.%20In%20this%0Apaper%2C%20however%2C%20we%20find%20a%20counterintuitive%20phenomenon%20that%20most%20test%20samples%0Aactually%20achieve%20optimal%20performance%20under%20the%20same%20prompts%2C%20and%20searching%20for%0Asample-level%20prompts%20only%20costs%20more%20time%20but%20results%20in%20completely%20identical%0Aprompts.%20Therefore%2C%20we%20propose%20task-level%20prompting%20to%20reduce%20the%20cost%20of%0Asearching%20for%20prompts%20during%20the%20inference%20stage%20and%20introduce%20two%20time-saving%0Ayet%20effective%20task-level%20prompt%20search%20strategies.%20Extensive%20experimental%0Aresults%20show%20that%20our%20proposed%20method%20can%20identify%20near-optimal%20prompts%20and%0Areach%20the%20best%20VICL%20performance%20with%20a%20minimal%20cost%20that%20prior%20work%20has%20never%0Aachieved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08841v1&entry.124074799=Read"},
{"title": "Click-Calib: A Robust Extrinsic Calibration Method for Surround-View\n  Systems", "author": "Lihao Wang", "abstract": "  Surround-View System (SVS) is an essential component in Advanced Driver\nAssistance System (ADAS) and requires precise calibrations. However,\nconventional offline extrinsic calibration methods are cumbersome and\ntime-consuming as they rely heavily on physical patterns. Additionally, these\nmethods primarily focus on short-range areas surrounding the vehicle, resulting\nin lower calibration quality in more distant zones. To address these\nlimitations, we propose Click-Calib, a pattern-free approach for offline SVS\nextrinsic calibration. Without requiring any special setup, the user only needs\nto click a few keypoints on the ground in natural scenes. Unlike other offline\ncalibration approaches, Click-Calib optimizes camera poses over a wide range by\nminimizing reprojection distance errors of keypoints, thereby achieving\naccurate calibrations at both short and long distances. Furthermore,\nClick-Calib supports both single-frame and multiple-frame modes, with the\nlatter offering even better results. Evaluations on our in-house dataset and\nthe public WoodScape dataset demonstrate its superior accuracy and robustness\ncompared to baseline methods. Code is available at\nhttps://github.com/lwangvaleo/click_calib.\n", "link": "http://arxiv.org/abs/2501.01557v2", "date": "2025-01-15", "relevancy": 2.1539, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5588}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5328}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Click-Calib%3A%20A%20Robust%20Extrinsic%20Calibration%20Method%20for%20Surround-View%0A%20%20Systems&body=Title%3A%20Click-Calib%3A%20A%20Robust%20Extrinsic%20Calibration%20Method%20for%20Surround-View%0A%20%20Systems%0AAuthor%3A%20Lihao%20Wang%0AAbstract%3A%20%20%20Surround-View%20System%20%28SVS%29%20is%20an%20essential%20component%20in%20Advanced%20Driver%0AAssistance%20System%20%28ADAS%29%20and%20requires%20precise%20calibrations.%20However%2C%0Aconventional%20offline%20extrinsic%20calibration%20methods%20are%20cumbersome%20and%0Atime-consuming%20as%20they%20rely%20heavily%20on%20physical%20patterns.%20Additionally%2C%20these%0Amethods%20primarily%20focus%20on%20short-range%20areas%20surrounding%20the%20vehicle%2C%20resulting%0Ain%20lower%20calibration%20quality%20in%20more%20distant%20zones.%20To%20address%20these%0Alimitations%2C%20we%20propose%20Click-Calib%2C%20a%20pattern-free%20approach%20for%20offline%20SVS%0Aextrinsic%20calibration.%20Without%20requiring%20any%20special%20setup%2C%20the%20user%20only%20needs%0Ato%20click%20a%20few%20keypoints%20on%20the%20ground%20in%20natural%20scenes.%20Unlike%20other%20offline%0Acalibration%20approaches%2C%20Click-Calib%20optimizes%20camera%20poses%20over%20a%20wide%20range%20by%0Aminimizing%20reprojection%20distance%20errors%20of%20keypoints%2C%20thereby%20achieving%0Aaccurate%20calibrations%20at%20both%20short%20and%20long%20distances.%20Furthermore%2C%0AClick-Calib%20supports%20both%20single-frame%20and%20multiple-frame%20modes%2C%20with%20the%0Alatter%20offering%20even%20better%20results.%20Evaluations%20on%20our%20in-house%20dataset%20and%0Athe%20public%20WoodScape%20dataset%20demonstrate%20its%20superior%20accuracy%20and%20robustness%0Acompared%20to%20baseline%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/lwangvaleo/click_calib.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClick-Calib%253A%2520A%2520Robust%2520Extrinsic%2520Calibration%2520Method%2520for%2520Surround-View%250A%2520%2520Systems%26entry.906535625%3DLihao%2520Wang%26entry.1292438233%3D%2520%2520Surround-View%2520System%2520%2528SVS%2529%2520is%2520an%2520essential%2520component%2520in%2520Advanced%2520Driver%250AAssistance%2520System%2520%2528ADAS%2529%2520and%2520requires%2520precise%2520calibrations.%2520However%252C%250Aconventional%2520offline%2520extrinsic%2520calibration%2520methods%2520are%2520cumbersome%2520and%250Atime-consuming%2520as%2520they%2520rely%2520heavily%2520on%2520physical%2520patterns.%2520Additionally%252C%2520these%250Amethods%2520primarily%2520focus%2520on%2520short-range%2520areas%2520surrounding%2520the%2520vehicle%252C%2520resulting%250Ain%2520lower%2520calibration%2520quality%2520in%2520more%2520distant%2520zones.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520Click-Calib%252C%2520a%2520pattern-free%2520approach%2520for%2520offline%2520SVS%250Aextrinsic%2520calibration.%2520Without%2520requiring%2520any%2520special%2520setup%252C%2520the%2520user%2520only%2520needs%250Ato%2520click%2520a%2520few%2520keypoints%2520on%2520the%2520ground%2520in%2520natural%2520scenes.%2520Unlike%2520other%2520offline%250Acalibration%2520approaches%252C%2520Click-Calib%2520optimizes%2520camera%2520poses%2520over%2520a%2520wide%2520range%2520by%250Aminimizing%2520reprojection%2520distance%2520errors%2520of%2520keypoints%252C%2520thereby%2520achieving%250Aaccurate%2520calibrations%2520at%2520both%2520short%2520and%2520long%2520distances.%2520Furthermore%252C%250AClick-Calib%2520supports%2520both%2520single-frame%2520and%2520multiple-frame%2520modes%252C%2520with%2520the%250Alatter%2520offering%2520even%2520better%2520results.%2520Evaluations%2520on%2520our%2520in-house%2520dataset%2520and%250Athe%2520public%2520WoodScape%2520dataset%2520demonstrate%2520its%2520superior%2520accuracy%2520and%2520robustness%250Acompared%2520to%2520baseline%2520methods.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/lwangvaleo/click_calib.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Click-Calib%3A%20A%20Robust%20Extrinsic%20Calibration%20Method%20for%20Surround-View%0A%20%20Systems&entry.906535625=Lihao%20Wang&entry.1292438233=%20%20Surround-View%20System%20%28SVS%29%20is%20an%20essential%20component%20in%20Advanced%20Driver%0AAssistance%20System%20%28ADAS%29%20and%20requires%20precise%20calibrations.%20However%2C%0Aconventional%20offline%20extrinsic%20calibration%20methods%20are%20cumbersome%20and%0Atime-consuming%20as%20they%20rely%20heavily%20on%20physical%20patterns.%20Additionally%2C%20these%0Amethods%20primarily%20focus%20on%20short-range%20areas%20surrounding%20the%20vehicle%2C%20resulting%0Ain%20lower%20calibration%20quality%20in%20more%20distant%20zones.%20To%20address%20these%0Alimitations%2C%20we%20propose%20Click-Calib%2C%20a%20pattern-free%20approach%20for%20offline%20SVS%0Aextrinsic%20calibration.%20Without%20requiring%20any%20special%20setup%2C%20the%20user%20only%20needs%0Ato%20click%20a%20few%20keypoints%20on%20the%20ground%20in%20natural%20scenes.%20Unlike%20other%20offline%0Acalibration%20approaches%2C%20Click-Calib%20optimizes%20camera%20poses%20over%20a%20wide%20range%20by%0Aminimizing%20reprojection%20distance%20errors%20of%20keypoints%2C%20thereby%20achieving%0Aaccurate%20calibrations%20at%20both%20short%20and%20long%20distances.%20Furthermore%2C%0AClick-Calib%20supports%20both%20single-frame%20and%20multiple-frame%20modes%2C%20with%20the%0Alatter%20offering%20even%20better%20results.%20Evaluations%20on%20our%20in-house%20dataset%20and%0Athe%20public%20WoodScape%20dataset%20demonstrate%20its%20superior%20accuracy%20and%20robustness%0Acompared%20to%20baseline%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/lwangvaleo/click_calib.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01557v2&entry.124074799=Read"},
{"title": "The Surprising Ineffectiveness of Pre-Trained Visual Representations for\n  Model-Based Reinforcement Learning", "author": "Moritz Schneider and Robert Krug and Narunas Vaskevicius and Luigi Palmieri and Joschka Boedecker", "abstract": "  Visual Reinforcement Learning (RL) methods often require extensive amounts of\ndata. As opposed to model-free RL, model-based RL (MBRL) offers a potential\nsolution with efficient data utilization through planning. Additionally, RL\nlacks generalization capabilities for real-world tasks. Prior work has shown\nthat incorporating pre-trained visual representations (PVRs) enhances sample\nefficiency and generalization. While PVRs have been extensively studied in the\ncontext of model-free RL, their potential in MBRL remains largely unexplored.\nIn this paper, we benchmark a set of PVRs on challenging control tasks in a\nmodel-based RL setting. We investigate the data efficiency, generalization\ncapabilities, and the impact of different properties of PVRs on the performance\nof model-based agents. Our results, perhaps surprisingly, reveal that for MBRL\ncurrent PVRs are not more sample efficient than learning representations from\nscratch, and that they do not generalize better to out-of-distribution (OOD)\nsettings. To explain this, we analyze the quality of the trained dynamics\nmodel. Furthermore, we show that data diversity and network architecture are\nthe most important contributors to OOD generalization performance.\n", "link": "http://arxiv.org/abs/2411.10175v2", "date": "2025-01-15", "relevancy": 2.1484, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Surprising%20Ineffectiveness%20of%20Pre-Trained%20Visual%20Representations%20for%0A%20%20Model-Based%20Reinforcement%20Learning&body=Title%3A%20The%20Surprising%20Ineffectiveness%20of%20Pre-Trained%20Visual%20Representations%20for%0A%20%20Model-Based%20Reinforcement%20Learning%0AAuthor%3A%20Moritz%20Schneider%20and%20Robert%20Krug%20and%20Narunas%20Vaskevicius%20and%20Luigi%20Palmieri%20and%20Joschka%20Boedecker%0AAbstract%3A%20%20%20Visual%20Reinforcement%20Learning%20%28RL%29%20methods%20often%20require%20extensive%20amounts%20of%0Adata.%20As%20opposed%20to%20model-free%20RL%2C%20model-based%20RL%20%28MBRL%29%20offers%20a%20potential%0Asolution%20with%20efficient%20data%20utilization%20through%20planning.%20Additionally%2C%20RL%0Alacks%20generalization%20capabilities%20for%20real-world%20tasks.%20Prior%20work%20has%20shown%0Athat%20incorporating%20pre-trained%20visual%20representations%20%28PVRs%29%20enhances%20sample%0Aefficiency%20and%20generalization.%20While%20PVRs%20have%20been%20extensively%20studied%20in%20the%0Acontext%20of%20model-free%20RL%2C%20their%20potential%20in%20MBRL%20remains%20largely%20unexplored.%0AIn%20this%20paper%2C%20we%20benchmark%20a%20set%20of%20PVRs%20on%20challenging%20control%20tasks%20in%20a%0Amodel-based%20RL%20setting.%20We%20investigate%20the%20data%20efficiency%2C%20generalization%0Acapabilities%2C%20and%20the%20impact%20of%20different%20properties%20of%20PVRs%20on%20the%20performance%0Aof%20model-based%20agents.%20Our%20results%2C%20perhaps%20surprisingly%2C%20reveal%20that%20for%20MBRL%0Acurrent%20PVRs%20are%20not%20more%20sample%20efficient%20than%20learning%20representations%20from%0Ascratch%2C%20and%20that%20they%20do%20not%20generalize%20better%20to%20out-of-distribution%20%28OOD%29%0Asettings.%20To%20explain%20this%2C%20we%20analyze%20the%20quality%20of%20the%20trained%20dynamics%0Amodel.%20Furthermore%2C%20we%20show%20that%20data%20diversity%20and%20network%20architecture%20are%0Athe%20most%20important%20contributors%20to%20OOD%20generalization%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10175v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Surprising%2520Ineffectiveness%2520of%2520Pre-Trained%2520Visual%2520Representations%2520for%250A%2520%2520Model-Based%2520Reinforcement%2520Learning%26entry.906535625%3DMoritz%2520Schneider%2520and%2520Robert%2520Krug%2520and%2520Narunas%2520Vaskevicius%2520and%2520Luigi%2520Palmieri%2520and%2520Joschka%2520Boedecker%26entry.1292438233%3D%2520%2520Visual%2520Reinforcement%2520Learning%2520%2528RL%2529%2520methods%2520often%2520require%2520extensive%2520amounts%2520of%250Adata.%2520As%2520opposed%2520to%2520model-free%2520RL%252C%2520model-based%2520RL%2520%2528MBRL%2529%2520offers%2520a%2520potential%250Asolution%2520with%2520efficient%2520data%2520utilization%2520through%2520planning.%2520Additionally%252C%2520RL%250Alacks%2520generalization%2520capabilities%2520for%2520real-world%2520tasks.%2520Prior%2520work%2520has%2520shown%250Athat%2520incorporating%2520pre-trained%2520visual%2520representations%2520%2528PVRs%2529%2520enhances%2520sample%250Aefficiency%2520and%2520generalization.%2520While%2520PVRs%2520have%2520been%2520extensively%2520studied%2520in%2520the%250Acontext%2520of%2520model-free%2520RL%252C%2520their%2520potential%2520in%2520MBRL%2520remains%2520largely%2520unexplored.%250AIn%2520this%2520paper%252C%2520we%2520benchmark%2520a%2520set%2520of%2520PVRs%2520on%2520challenging%2520control%2520tasks%2520in%2520a%250Amodel-based%2520RL%2520setting.%2520We%2520investigate%2520the%2520data%2520efficiency%252C%2520generalization%250Acapabilities%252C%2520and%2520the%2520impact%2520of%2520different%2520properties%2520of%2520PVRs%2520on%2520the%2520performance%250Aof%2520model-based%2520agents.%2520Our%2520results%252C%2520perhaps%2520surprisingly%252C%2520reveal%2520that%2520for%2520MBRL%250Acurrent%2520PVRs%2520are%2520not%2520more%2520sample%2520efficient%2520than%2520learning%2520representations%2520from%250Ascratch%252C%2520and%2520that%2520they%2520do%2520not%2520generalize%2520better%2520to%2520out-of-distribution%2520%2528OOD%2529%250Asettings.%2520To%2520explain%2520this%252C%2520we%2520analyze%2520the%2520quality%2520of%2520the%2520trained%2520dynamics%250Amodel.%2520Furthermore%252C%2520we%2520show%2520that%2520data%2520diversity%2520and%2520network%2520architecture%2520are%250Athe%2520most%2520important%2520contributors%2520to%2520OOD%2520generalization%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10175v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Surprising%20Ineffectiveness%20of%20Pre-Trained%20Visual%20Representations%20for%0A%20%20Model-Based%20Reinforcement%20Learning&entry.906535625=Moritz%20Schneider%20and%20Robert%20Krug%20and%20Narunas%20Vaskevicius%20and%20Luigi%20Palmieri%20and%20Joschka%20Boedecker&entry.1292438233=%20%20Visual%20Reinforcement%20Learning%20%28RL%29%20methods%20often%20require%20extensive%20amounts%20of%0Adata.%20As%20opposed%20to%20model-free%20RL%2C%20model-based%20RL%20%28MBRL%29%20offers%20a%20potential%0Asolution%20with%20efficient%20data%20utilization%20through%20planning.%20Additionally%2C%20RL%0Alacks%20generalization%20capabilities%20for%20real-world%20tasks.%20Prior%20work%20has%20shown%0Athat%20incorporating%20pre-trained%20visual%20representations%20%28PVRs%29%20enhances%20sample%0Aefficiency%20and%20generalization.%20While%20PVRs%20have%20been%20extensively%20studied%20in%20the%0Acontext%20of%20model-free%20RL%2C%20their%20potential%20in%20MBRL%20remains%20largely%20unexplored.%0AIn%20this%20paper%2C%20we%20benchmark%20a%20set%20of%20PVRs%20on%20challenging%20control%20tasks%20in%20a%0Amodel-based%20RL%20setting.%20We%20investigate%20the%20data%20efficiency%2C%20generalization%0Acapabilities%2C%20and%20the%20impact%20of%20different%20properties%20of%20PVRs%20on%20the%20performance%0Aof%20model-based%20agents.%20Our%20results%2C%20perhaps%20surprisingly%2C%20reveal%20that%20for%20MBRL%0Acurrent%20PVRs%20are%20not%20more%20sample%20efficient%20than%20learning%20representations%20from%0Ascratch%2C%20and%20that%20they%20do%20not%20generalize%20better%20to%20out-of-distribution%20%28OOD%29%0Asettings.%20To%20explain%20this%2C%20we%20analyze%20the%20quality%20of%20the%20trained%20dynamics%0Amodel.%20Furthermore%2C%20we%20show%20that%20data%20diversity%20and%20network%20architecture%20are%0Athe%20most%20important%20contributors%20to%20OOD%20generalization%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10175v2&entry.124074799=Read"},
{"title": "Reward-Driven Automated Curriculum Learning for Interaction-Aware\n  Self-Driving at Unsignalized Intersections", "author": "Zengqi Peng and Xiao Zhou and Lei Zheng and Yubin Wang and Jun Ma", "abstract": "  In this work, we present a reward-driven automated curriculum reinforcement\nlearning approach for interaction-aware self-driving at unsignalized\nintersections, taking into account the uncertainties associated with\nsurrounding vehicles (SVs). These uncertainties encompass the uncertainty of\nSVs' driving intention and also the quantity of SVs. To deal with this problem,\nthe curriculum set is specifically designed to accommodate a progressively\nincreasing number of SVs. By implementing an automated curriculum selection\nmechanism, the importance weights are rationally allocated across various\ncurricula, thereby facilitating improved sample efficiency and training\noutcomes. Furthermore, the reward function is meticulously designed to guide\nthe agent towards effective policy exploration. Thus the proposed framework\ncould proactively address the above uncertainties at unsignalized intersections\nby employing the automated curriculum learning technique that progressively\nincreases task difficulty, and this ensures safe self-driving through effective\ninteraction with SVs. Comparative experiments are conducted in $Highway\\_Env$,\nand the results indicate that our approach achieves the highest task success\nrate, attains strong robustness to initialization parameters of the curriculum\nselection module, and exhibits superior adaptability to diverse situational\nconfigurations at unsignalized intersections. Furthermore, the effectiveness of\nthe proposed method is validated using the high-fidelity CARLA simulator.\n", "link": "http://arxiv.org/abs/2403.13674v2", "date": "2025-01-15", "relevancy": 2.1301, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5353}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5328}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward-Driven%20Automated%20Curriculum%20Learning%20for%20Interaction-Aware%0A%20%20Self-Driving%20at%20Unsignalized%20Intersections&body=Title%3A%20Reward-Driven%20Automated%20Curriculum%20Learning%20for%20Interaction-Aware%0A%20%20Self-Driving%20at%20Unsignalized%20Intersections%0AAuthor%3A%20Zengqi%20Peng%20and%20Xiao%20Zhou%20and%20Lei%20Zheng%20and%20Yubin%20Wang%20and%20Jun%20Ma%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20reward-driven%20automated%20curriculum%20reinforcement%0Alearning%20approach%20for%20interaction-aware%20self-driving%20at%20unsignalized%0Aintersections%2C%20taking%20into%20account%20the%20uncertainties%20associated%20with%0Asurrounding%20vehicles%20%28SVs%29.%20These%20uncertainties%20encompass%20the%20uncertainty%20of%0ASVs%27%20driving%20intention%20and%20also%20the%20quantity%20of%20SVs.%20To%20deal%20with%20this%20problem%2C%0Athe%20curriculum%20set%20is%20specifically%20designed%20to%20accommodate%20a%20progressively%0Aincreasing%20number%20of%20SVs.%20By%20implementing%20an%20automated%20curriculum%20selection%0Amechanism%2C%20the%20importance%20weights%20are%20rationally%20allocated%20across%20various%0Acurricula%2C%20thereby%20facilitating%20improved%20sample%20efficiency%20and%20training%0Aoutcomes.%20Furthermore%2C%20the%20reward%20function%20is%20meticulously%20designed%20to%20guide%0Athe%20agent%20towards%20effective%20policy%20exploration.%20Thus%20the%20proposed%20framework%0Acould%20proactively%20address%20the%20above%20uncertainties%20at%20unsignalized%20intersections%0Aby%20employing%20the%20automated%20curriculum%20learning%20technique%20that%20progressively%0Aincreases%20task%20difficulty%2C%20and%20this%20ensures%20safe%20self-driving%20through%20effective%0Ainteraction%20with%20SVs.%20Comparative%20experiments%20are%20conducted%20in%20%24Highway%5C_Env%24%2C%0Aand%20the%20results%20indicate%20that%20our%20approach%20achieves%20the%20highest%20task%20success%0Arate%2C%20attains%20strong%20robustness%20to%20initialization%20parameters%20of%20the%20curriculum%0Aselection%20module%2C%20and%20exhibits%20superior%20adaptability%20to%20diverse%20situational%0Aconfigurations%20at%20unsignalized%20intersections.%20Furthermore%2C%20the%20effectiveness%20of%0Athe%20proposed%20method%20is%20validated%20using%20the%20high-fidelity%20CARLA%20simulator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13674v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward-Driven%2520Automated%2520Curriculum%2520Learning%2520for%2520Interaction-Aware%250A%2520%2520Self-Driving%2520at%2520Unsignalized%2520Intersections%26entry.906535625%3DZengqi%2520Peng%2520and%2520Xiao%2520Zhou%2520and%2520Lei%2520Zheng%2520and%2520Yubin%2520Wang%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520reward-driven%2520automated%2520curriculum%2520reinforcement%250Alearning%2520approach%2520for%2520interaction-aware%2520self-driving%2520at%2520unsignalized%250Aintersections%252C%2520taking%2520into%2520account%2520the%2520uncertainties%2520associated%2520with%250Asurrounding%2520vehicles%2520%2528SVs%2529.%2520These%2520uncertainties%2520encompass%2520the%2520uncertainty%2520of%250ASVs%2527%2520driving%2520intention%2520and%2520also%2520the%2520quantity%2520of%2520SVs.%2520To%2520deal%2520with%2520this%2520problem%252C%250Athe%2520curriculum%2520set%2520is%2520specifically%2520designed%2520to%2520accommodate%2520a%2520progressively%250Aincreasing%2520number%2520of%2520SVs.%2520By%2520implementing%2520an%2520automated%2520curriculum%2520selection%250Amechanism%252C%2520the%2520importance%2520weights%2520are%2520rationally%2520allocated%2520across%2520various%250Acurricula%252C%2520thereby%2520facilitating%2520improved%2520sample%2520efficiency%2520and%2520training%250Aoutcomes.%2520Furthermore%252C%2520the%2520reward%2520function%2520is%2520meticulously%2520designed%2520to%2520guide%250Athe%2520agent%2520towards%2520effective%2520policy%2520exploration.%2520Thus%2520the%2520proposed%2520framework%250Acould%2520proactively%2520address%2520the%2520above%2520uncertainties%2520at%2520unsignalized%2520intersections%250Aby%2520employing%2520the%2520automated%2520curriculum%2520learning%2520technique%2520that%2520progressively%250Aincreases%2520task%2520difficulty%252C%2520and%2520this%2520ensures%2520safe%2520self-driving%2520through%2520effective%250Ainteraction%2520with%2520SVs.%2520Comparative%2520experiments%2520are%2520conducted%2520in%2520%2524Highway%255C_Env%2524%252C%250Aand%2520the%2520results%2520indicate%2520that%2520our%2520approach%2520achieves%2520the%2520highest%2520task%2520success%250Arate%252C%2520attains%2520strong%2520robustness%2520to%2520initialization%2520parameters%2520of%2520the%2520curriculum%250Aselection%2520module%252C%2520and%2520exhibits%2520superior%2520adaptability%2520to%2520diverse%2520situational%250Aconfigurations%2520at%2520unsignalized%2520intersections.%2520Furthermore%252C%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520method%2520is%2520validated%2520using%2520the%2520high-fidelity%2520CARLA%2520simulator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13674v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward-Driven%20Automated%20Curriculum%20Learning%20for%20Interaction-Aware%0A%20%20Self-Driving%20at%20Unsignalized%20Intersections&entry.906535625=Zengqi%20Peng%20and%20Xiao%20Zhou%20and%20Lei%20Zheng%20and%20Yubin%20Wang%20and%20Jun%20Ma&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20reward-driven%20automated%20curriculum%20reinforcement%0Alearning%20approach%20for%20interaction-aware%20self-driving%20at%20unsignalized%0Aintersections%2C%20taking%20into%20account%20the%20uncertainties%20associated%20with%0Asurrounding%20vehicles%20%28SVs%29.%20These%20uncertainties%20encompass%20the%20uncertainty%20of%0ASVs%27%20driving%20intention%20and%20also%20the%20quantity%20of%20SVs.%20To%20deal%20with%20this%20problem%2C%0Athe%20curriculum%20set%20is%20specifically%20designed%20to%20accommodate%20a%20progressively%0Aincreasing%20number%20of%20SVs.%20By%20implementing%20an%20automated%20curriculum%20selection%0Amechanism%2C%20the%20importance%20weights%20are%20rationally%20allocated%20across%20various%0Acurricula%2C%20thereby%20facilitating%20improved%20sample%20efficiency%20and%20training%0Aoutcomes.%20Furthermore%2C%20the%20reward%20function%20is%20meticulously%20designed%20to%20guide%0Athe%20agent%20towards%20effective%20policy%20exploration.%20Thus%20the%20proposed%20framework%0Acould%20proactively%20address%20the%20above%20uncertainties%20at%20unsignalized%20intersections%0Aby%20employing%20the%20automated%20curriculum%20learning%20technique%20that%20progressively%0Aincreases%20task%20difficulty%2C%20and%20this%20ensures%20safe%20self-driving%20through%20effective%0Ainteraction%20with%20SVs.%20Comparative%20experiments%20are%20conducted%20in%20%24Highway%5C_Env%24%2C%0Aand%20the%20results%20indicate%20that%20our%20approach%20achieves%20the%20highest%20task%20success%0Arate%2C%20attains%20strong%20robustness%20to%20initialization%20parameters%20of%20the%20curriculum%0Aselection%20module%2C%20and%20exhibits%20superior%20adaptability%20to%20diverse%20situational%0Aconfigurations%20at%20unsignalized%20intersections.%20Furthermore%2C%20the%20effectiveness%20of%0Athe%20proposed%20method%20is%20validated%20using%20the%20high-fidelity%20CARLA%20simulator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13674v2&entry.124074799=Read"},
{"title": "MGF: Mixed Gaussian Flow for Diverse Trajectory Prediction", "author": "Jiahe Chen and Jinkun Cao and Dahua Lin and Kris Kitani and Jiangmiao Pang", "abstract": "  To predict future trajectories, the normalizing flow with a standard Gaussian\nprior suffers from weak diversity. The ineffectiveness comes from the conflict\nbetween the fact of asymmetric and multi-modal distribution of likely outcomes\nand symmetric and single-modal original distribution and supervision losses.\nInstead, we propose constructing a mixed Gaussian prior for a normalizing flow\nmodel for trajectory prediction. The prior is constructed by analyzing the\ntrajectory patterns in the training samples without requiring extra annotations\nwhile showing better expressiveness and being multi-modal and asymmetric.\nBesides diversity, it also provides better controllability for probabilistic\ntrajectory generation. We name our method Mixed Gaussian Flow (MGF). It\nachieves state-of-the-art performance in the evaluation of both trajectory\nalignment and diversity on the popular UCY/ETH and SDD datasets. Code is\navailable at https://github.com/mulplue/MGF.\n", "link": "http://arxiv.org/abs/2402.12238v2", "date": "2025-01-15", "relevancy": 2.1286, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5363}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.536}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGF%3A%20Mixed%20Gaussian%20Flow%20for%20Diverse%20Trajectory%20Prediction&body=Title%3A%20MGF%3A%20Mixed%20Gaussian%20Flow%20for%20Diverse%20Trajectory%20Prediction%0AAuthor%3A%20Jiahe%20Chen%20and%20Jinkun%20Cao%20and%20Dahua%20Lin%20and%20Kris%20Kitani%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20To%20predict%20future%20trajectories%2C%20the%20normalizing%20flow%20with%20a%20standard%20Gaussian%0Aprior%20suffers%20from%20weak%20diversity.%20The%20ineffectiveness%20comes%20from%20the%20conflict%0Abetween%20the%20fact%20of%20asymmetric%20and%20multi-modal%20distribution%20of%20likely%20outcomes%0Aand%20symmetric%20and%20single-modal%20original%20distribution%20and%20supervision%20losses.%0AInstead%2C%20we%20propose%20constructing%20a%20mixed%20Gaussian%20prior%20for%20a%20normalizing%20flow%0Amodel%20for%20trajectory%20prediction.%20The%20prior%20is%20constructed%20by%20analyzing%20the%0Atrajectory%20patterns%20in%20the%20training%20samples%20without%20requiring%20extra%20annotations%0Awhile%20showing%20better%20expressiveness%20and%20being%20multi-modal%20and%20asymmetric.%0ABesides%20diversity%2C%20it%20also%20provides%20better%20controllability%20for%20probabilistic%0Atrajectory%20generation.%20We%20name%20our%20method%20Mixed%20Gaussian%20Flow%20%28MGF%29.%20It%0Aachieves%20state-of-the-art%20performance%20in%20the%20evaluation%20of%20both%20trajectory%0Aalignment%20and%20diversity%20on%20the%20popular%20UCY/ETH%20and%20SDD%20datasets.%20Code%20is%0Aavailable%20at%20https%3A//github.com/mulplue/MGF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGF%253A%2520Mixed%2520Gaussian%2520Flow%2520for%2520Diverse%2520Trajectory%2520Prediction%26entry.906535625%3DJiahe%2520Chen%2520and%2520Jinkun%2520Cao%2520and%2520Dahua%2520Lin%2520and%2520Kris%2520Kitani%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520To%2520predict%2520future%2520trajectories%252C%2520the%2520normalizing%2520flow%2520with%2520a%2520standard%2520Gaussian%250Aprior%2520suffers%2520from%2520weak%2520diversity.%2520The%2520ineffectiveness%2520comes%2520from%2520the%2520conflict%250Abetween%2520the%2520fact%2520of%2520asymmetric%2520and%2520multi-modal%2520distribution%2520of%2520likely%2520outcomes%250Aand%2520symmetric%2520and%2520single-modal%2520original%2520distribution%2520and%2520supervision%2520losses.%250AInstead%252C%2520we%2520propose%2520constructing%2520a%2520mixed%2520Gaussian%2520prior%2520for%2520a%2520normalizing%2520flow%250Amodel%2520for%2520trajectory%2520prediction.%2520The%2520prior%2520is%2520constructed%2520by%2520analyzing%2520the%250Atrajectory%2520patterns%2520in%2520the%2520training%2520samples%2520without%2520requiring%2520extra%2520annotations%250Awhile%2520showing%2520better%2520expressiveness%2520and%2520being%2520multi-modal%2520and%2520asymmetric.%250ABesides%2520diversity%252C%2520it%2520also%2520provides%2520better%2520controllability%2520for%2520probabilistic%250Atrajectory%2520generation.%2520We%2520name%2520our%2520method%2520Mixed%2520Gaussian%2520Flow%2520%2528MGF%2529.%2520It%250Aachieves%2520state-of-the-art%2520performance%2520in%2520the%2520evaluation%2520of%2520both%2520trajectory%250Aalignment%2520and%2520diversity%2520on%2520the%2520popular%2520UCY/ETH%2520and%2520SDD%2520datasets.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/mulplue/MGF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGF%3A%20Mixed%20Gaussian%20Flow%20for%20Diverse%20Trajectory%20Prediction&entry.906535625=Jiahe%20Chen%20and%20Jinkun%20Cao%20and%20Dahua%20Lin%20and%20Kris%20Kitani%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20To%20predict%20future%20trajectories%2C%20the%20normalizing%20flow%20with%20a%20standard%20Gaussian%0Aprior%20suffers%20from%20weak%20diversity.%20The%20ineffectiveness%20comes%20from%20the%20conflict%0Abetween%20the%20fact%20of%20asymmetric%20and%20multi-modal%20distribution%20of%20likely%20outcomes%0Aand%20symmetric%20and%20single-modal%20original%20distribution%20and%20supervision%20losses.%0AInstead%2C%20we%20propose%20constructing%20a%20mixed%20Gaussian%20prior%20for%20a%20normalizing%20flow%0Amodel%20for%20trajectory%20prediction.%20The%20prior%20is%20constructed%20by%20analyzing%20the%0Atrajectory%20patterns%20in%20the%20training%20samples%20without%20requiring%20extra%20annotations%0Awhile%20showing%20better%20expressiveness%20and%20being%20multi-modal%20and%20asymmetric.%0ABesides%20diversity%2C%20it%20also%20provides%20better%20controllability%20for%20probabilistic%0Atrajectory%20generation.%20We%20name%20our%20method%20Mixed%20Gaussian%20Flow%20%28MGF%29.%20It%0Aachieves%20state-of-the-art%20performance%20in%20the%20evaluation%20of%20both%20trajectory%0Aalignment%20and%20diversity%20on%20the%20popular%20UCY/ETH%20and%20SDD%20datasets.%20Code%20is%0Aavailable%20at%20https%3A//github.com/mulplue/MGF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12238v2&entry.124074799=Read"},
{"title": "Admitting Ignorance Helps the Video Question Answering Models to Answer", "author": "Haopeng Li and Tom Drummond and Mingming Gong and Mohammed Bennamoun and Qiuhong Ke", "abstract": "  Significant progress has been made in the field of video question answering\n(VideoQA) thanks to deep learning and large-scale pretraining. Despite the\npresence of sophisticated model structures and powerful video-text foundation\nmodels, most existing methods focus solely on maximizing the correlation\nbetween answers and video-question pairs during training. We argue that these\nmodels often establish shortcuts, resulting in spurious correlations between\nquestions and answers, especially when the alignment between video and text\ndata is suboptimal. To address these spurious correlations, we propose a novel\ntraining framework in which the model is compelled to acknowledge its ignorance\nwhen presented with an intervened question, rather than making guesses solely\nbased on superficial question-answer correlations. We introduce methodologies\nfor intervening in questions, utilizing techniques such as displacement and\nperturbation, and design frameworks for the model to admit its lack of\nknowledge in both multi-choice VideoQA and open-ended settings. In practice, we\nintegrate a state-of-the-art model into our framework to validate its\neffectiveness. The results clearly demonstrate that our framework can\nsignificantly enhance the performance of VideoQA models with minimal structural\nmodifications.\n", "link": "http://arxiv.org/abs/2501.08771v1", "date": "2025-01-15", "relevancy": 2.1249, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5494}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5305}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Admitting%20Ignorance%20Helps%20the%20Video%20Question%20Answering%20Models%20to%20Answer&body=Title%3A%20Admitting%20Ignorance%20Helps%20the%20Video%20Question%20Answering%20Models%20to%20Answer%0AAuthor%3A%20Haopeng%20Li%20and%20Tom%20Drummond%20and%20Mingming%20Gong%20and%20Mohammed%20Bennamoun%20and%20Qiuhong%20Ke%0AAbstract%3A%20%20%20Significant%20progress%20has%20been%20made%20in%20the%20field%20of%20video%20question%20answering%0A%28VideoQA%29%20thanks%20to%20deep%20learning%20and%20large-scale%20pretraining.%20Despite%20the%0Apresence%20of%20sophisticated%20model%20structures%20and%20powerful%20video-text%20foundation%0Amodels%2C%20most%20existing%20methods%20focus%20solely%20on%20maximizing%20the%20correlation%0Abetween%20answers%20and%20video-question%20pairs%20during%20training.%20We%20argue%20that%20these%0Amodels%20often%20establish%20shortcuts%2C%20resulting%20in%20spurious%20correlations%20between%0Aquestions%20and%20answers%2C%20especially%20when%20the%20alignment%20between%20video%20and%20text%0Adata%20is%20suboptimal.%20To%20address%20these%20spurious%20correlations%2C%20we%20propose%20a%20novel%0Atraining%20framework%20in%20which%20the%20model%20is%20compelled%20to%20acknowledge%20its%20ignorance%0Awhen%20presented%20with%20an%20intervened%20question%2C%20rather%20than%20making%20guesses%20solely%0Abased%20on%20superficial%20question-answer%20correlations.%20We%20introduce%20methodologies%0Afor%20intervening%20in%20questions%2C%20utilizing%20techniques%20such%20as%20displacement%20and%0Aperturbation%2C%20and%20design%20frameworks%20for%20the%20model%20to%20admit%20its%20lack%20of%0Aknowledge%20in%20both%20multi-choice%20VideoQA%20and%20open-ended%20settings.%20In%20practice%2C%20we%0Aintegrate%20a%20state-of-the-art%20model%20into%20our%20framework%20to%20validate%20its%0Aeffectiveness.%20The%20results%20clearly%20demonstrate%20that%20our%20framework%20can%0Asignificantly%20enhance%20the%20performance%20of%20VideoQA%20models%20with%20minimal%20structural%0Amodifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdmitting%2520Ignorance%2520Helps%2520the%2520Video%2520Question%2520Answering%2520Models%2520to%2520Answer%26entry.906535625%3DHaopeng%2520Li%2520and%2520Tom%2520Drummond%2520and%2520Mingming%2520Gong%2520and%2520Mohammed%2520Bennamoun%2520and%2520Qiuhong%2520Ke%26entry.1292438233%3D%2520%2520Significant%2520progress%2520has%2520been%2520made%2520in%2520the%2520field%2520of%2520video%2520question%2520answering%250A%2528VideoQA%2529%2520thanks%2520to%2520deep%2520learning%2520and%2520large-scale%2520pretraining.%2520Despite%2520the%250Apresence%2520of%2520sophisticated%2520model%2520structures%2520and%2520powerful%2520video-text%2520foundation%250Amodels%252C%2520most%2520existing%2520methods%2520focus%2520solely%2520on%2520maximizing%2520the%2520correlation%250Abetween%2520answers%2520and%2520video-question%2520pairs%2520during%2520training.%2520We%2520argue%2520that%2520these%250Amodels%2520often%2520establish%2520shortcuts%252C%2520resulting%2520in%2520spurious%2520correlations%2520between%250Aquestions%2520and%2520answers%252C%2520especially%2520when%2520the%2520alignment%2520between%2520video%2520and%2520text%250Adata%2520is%2520suboptimal.%2520To%2520address%2520these%2520spurious%2520correlations%252C%2520we%2520propose%2520a%2520novel%250Atraining%2520framework%2520in%2520which%2520the%2520model%2520is%2520compelled%2520to%2520acknowledge%2520its%2520ignorance%250Awhen%2520presented%2520with%2520an%2520intervened%2520question%252C%2520rather%2520than%2520making%2520guesses%2520solely%250Abased%2520on%2520superficial%2520question-answer%2520correlations.%2520We%2520introduce%2520methodologies%250Afor%2520intervening%2520in%2520questions%252C%2520utilizing%2520techniques%2520such%2520as%2520displacement%2520and%250Aperturbation%252C%2520and%2520design%2520frameworks%2520for%2520the%2520model%2520to%2520admit%2520its%2520lack%2520of%250Aknowledge%2520in%2520both%2520multi-choice%2520VideoQA%2520and%2520open-ended%2520settings.%2520In%2520practice%252C%2520we%250Aintegrate%2520a%2520state-of-the-art%2520model%2520into%2520our%2520framework%2520to%2520validate%2520its%250Aeffectiveness.%2520The%2520results%2520clearly%2520demonstrate%2520that%2520our%2520framework%2520can%250Asignificantly%2520enhance%2520the%2520performance%2520of%2520VideoQA%2520models%2520with%2520minimal%2520structural%250Amodifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Admitting%20Ignorance%20Helps%20the%20Video%20Question%20Answering%20Models%20to%20Answer&entry.906535625=Haopeng%20Li%20and%20Tom%20Drummond%20and%20Mingming%20Gong%20and%20Mohammed%20Bennamoun%20and%20Qiuhong%20Ke&entry.1292438233=%20%20Significant%20progress%20has%20been%20made%20in%20the%20field%20of%20video%20question%20answering%0A%28VideoQA%29%20thanks%20to%20deep%20learning%20and%20large-scale%20pretraining.%20Despite%20the%0Apresence%20of%20sophisticated%20model%20structures%20and%20powerful%20video-text%20foundation%0Amodels%2C%20most%20existing%20methods%20focus%20solely%20on%20maximizing%20the%20correlation%0Abetween%20answers%20and%20video-question%20pairs%20during%20training.%20We%20argue%20that%20these%0Amodels%20often%20establish%20shortcuts%2C%20resulting%20in%20spurious%20correlations%20between%0Aquestions%20and%20answers%2C%20especially%20when%20the%20alignment%20between%20video%20and%20text%0Adata%20is%20suboptimal.%20To%20address%20these%20spurious%20correlations%2C%20we%20propose%20a%20novel%0Atraining%20framework%20in%20which%20the%20model%20is%20compelled%20to%20acknowledge%20its%20ignorance%0Awhen%20presented%20with%20an%20intervened%20question%2C%20rather%20than%20making%20guesses%20solely%0Abased%20on%20superficial%20question-answer%20correlations.%20We%20introduce%20methodologies%0Afor%20intervening%20in%20questions%2C%20utilizing%20techniques%20such%20as%20displacement%20and%0Aperturbation%2C%20and%20design%20frameworks%20for%20the%20model%20to%20admit%20its%20lack%20of%0Aknowledge%20in%20both%20multi-choice%20VideoQA%20and%20open-ended%20settings.%20In%20practice%2C%20we%0Aintegrate%20a%20state-of-the-art%20model%20into%20our%20framework%20to%20validate%20its%0Aeffectiveness.%20The%20results%20clearly%20demonstrate%20that%20our%20framework%20can%0Asignificantly%20enhance%20the%20performance%20of%20VideoQA%20models%20with%20minimal%20structural%0Amodifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08771v1&entry.124074799=Read"},
{"title": "Few-Shot Learner Generalizes Across AI-Generated Image Detection", "author": "Shiyu Wu and Jing Liu and Jing Li and Yequan Wang", "abstract": "  Current fake image detectors trained on large synthetic image datasets\nperform satisfactorily on limited studied generative models. However, they\nsuffer a notable performance decline over unseen models. Besides, collecting\nadequate training data from online generative models is often expensive or\ninfeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a\nnovel AI-generated image detector which learns a specialized metric space to\neffectively distinguish unseen fake images by utilizing very few samples.\nExperiments show FSD achieves state-of-the-art performance by $+7.4\\%$ average\nACC on GenImage dataset. More importantly, our method is better capable of\ncapturing the intra-category common features in unseen images without further\ntraining.\n", "link": "http://arxiv.org/abs/2501.08763v1", "date": "2025-01-15", "relevancy": 2.1183, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5486}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5205}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Learner%20Generalizes%20Across%20AI-Generated%20Image%20Detection&body=Title%3A%20Few-Shot%20Learner%20Generalizes%20Across%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Shiyu%20Wu%20and%20Jing%20Liu%20and%20Jing%20Li%20and%20Yequan%20Wang%0AAbstract%3A%20%20%20Current%20fake%20image%20detectors%20trained%20on%20large%20synthetic%20image%20datasets%0Aperform%20satisfactorily%20on%20limited%20studied%20generative%20models.%20However%2C%20they%0Asuffer%20a%20notable%20performance%20decline%20over%20unseen%20models.%20Besides%2C%20collecting%0Aadequate%20training%20data%20from%20online%20generative%20models%20is%20often%20expensive%20or%0Ainfeasible.%20To%20overcome%20these%20issues%2C%20we%20propose%20Few-Shot%20Detector%20%28FSD%29%2C%20a%0Anovel%20AI-generated%20image%20detector%20which%20learns%20a%20specialized%20metric%20space%20to%0Aeffectively%20distinguish%20unseen%20fake%20images%20by%20utilizing%20very%20few%20samples.%0AExperiments%20show%20FSD%20achieves%20state-of-the-art%20performance%20by%20%24%2B7.4%5C%25%24%20average%0AACC%20on%20GenImage%20dataset.%20More%20importantly%2C%20our%20method%20is%20better%20capable%20of%0Acapturing%20the%20intra-category%20common%20features%20in%20unseen%20images%20without%20further%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Learner%2520Generalizes%2520Across%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DShiyu%2520Wu%2520and%2520Jing%2520Liu%2520and%2520Jing%2520Li%2520and%2520Yequan%2520Wang%26entry.1292438233%3D%2520%2520Current%2520fake%2520image%2520detectors%2520trained%2520on%2520large%2520synthetic%2520image%2520datasets%250Aperform%2520satisfactorily%2520on%2520limited%2520studied%2520generative%2520models.%2520However%252C%2520they%250Asuffer%2520a%2520notable%2520performance%2520decline%2520over%2520unseen%2520models.%2520Besides%252C%2520collecting%250Aadequate%2520training%2520data%2520from%2520online%2520generative%2520models%2520is%2520often%2520expensive%2520or%250Ainfeasible.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520Few-Shot%2520Detector%2520%2528FSD%2529%252C%2520a%250Anovel%2520AI-generated%2520image%2520detector%2520which%2520learns%2520a%2520specialized%2520metric%2520space%2520to%250Aeffectively%2520distinguish%2520unseen%2520fake%2520images%2520by%2520utilizing%2520very%2520few%2520samples.%250AExperiments%2520show%2520FSD%2520achieves%2520state-of-the-art%2520performance%2520by%2520%2524%252B7.4%255C%2525%2524%2520average%250AACC%2520on%2520GenImage%2520dataset.%2520More%2520importantly%252C%2520our%2520method%2520is%2520better%2520capable%2520of%250Acapturing%2520the%2520intra-category%2520common%2520features%2520in%2520unseen%2520images%2520without%2520further%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Learner%20Generalizes%20Across%20AI-Generated%20Image%20Detection&entry.906535625=Shiyu%20Wu%20and%20Jing%20Liu%20and%20Jing%20Li%20and%20Yequan%20Wang&entry.1292438233=%20%20Current%20fake%20image%20detectors%20trained%20on%20large%20synthetic%20image%20datasets%0Aperform%20satisfactorily%20on%20limited%20studied%20generative%20models.%20However%2C%20they%0Asuffer%20a%20notable%20performance%20decline%20over%20unseen%20models.%20Besides%2C%20collecting%0Aadequate%20training%20data%20from%20online%20generative%20models%20is%20often%20expensive%20or%0Ainfeasible.%20To%20overcome%20these%20issues%2C%20we%20propose%20Few-Shot%20Detector%20%28FSD%29%2C%20a%0Anovel%20AI-generated%20image%20detector%20which%20learns%20a%20specialized%20metric%20space%20to%0Aeffectively%20distinguish%20unseen%20fake%20images%20by%20utilizing%20very%20few%20samples.%0AExperiments%20show%20FSD%20achieves%20state-of-the-art%20performance%20by%20%24%2B7.4%5C%25%24%20average%0AACC%20on%20GenImage%20dataset.%20More%20importantly%2C%20our%20method%20is%20better%20capable%20of%0Acapturing%20the%20intra-category%20common%20features%20in%20unseen%20images%20without%20further%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08763v1&entry.124074799=Read"},
{"title": "FADE: Towards Fairness-aware Augmentation for Domain Generalization via\n  Classifier-Guided Score-based Diffusion Models", "author": "Yujie Lin and Dong Li and Chen Zhao and Minglai Shao and Guihong Wan", "abstract": "  Fairness-aware domain generalization (FairDG) has emerged as a critical\nchallenge for deploying trustworthy AI systems, particularly in scenarios\ninvolving distribution shifts. Traditional methods for addressing fairness have\nfailed in domain generalization due to their lack of consideration for\ndistribution shifts. Although disentanglement has been used to tackle FairDG,\nit is limited by its strong assumptions. To overcome these limitations, we\npropose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as\na novel approach to effectively address the FairDG issue. Specifically, we\nfirst pre-train a score-based diffusion model (SDM) and two classifiers to\nequip the model with strong generalization capabilities across different\ndomains. Then, we guide the SDM using these pre-trained classifiers to\neffectively eliminate sensitive information from the generated data. Finally,\nthe generated fair data is used to train downstream classifiers, ensuring\nrobust performance under new data distributions. Extensive experiments on three\nreal-world datasets demonstrate that FADE not only enhances fairness but also\nimproves accuracy in the presence of distribution shifts. Additionally, FADE\noutperforms existing methods in achieving the best accuracy-fairness\ntrade-offs.\n", "link": "http://arxiv.org/abs/2406.09495v3", "date": "2025-01-15", "relevancy": 2.1053, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5395}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5274}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FADE%3A%20Towards%20Fairness-aware%20Augmentation%20for%20Domain%20Generalization%20via%0A%20%20Classifier-Guided%20Score-based%20Diffusion%20Models&body=Title%3A%20FADE%3A%20Towards%20Fairness-aware%20Augmentation%20for%20Domain%20Generalization%20via%0A%20%20Classifier-Guided%20Score-based%20Diffusion%20Models%0AAuthor%3A%20Yujie%20Lin%20and%20Dong%20Li%20and%20Chen%20Zhao%20and%20Minglai%20Shao%20and%20Guihong%20Wan%0AAbstract%3A%20%20%20Fairness-aware%20domain%20generalization%20%28FairDG%29%20has%20emerged%20as%20a%20critical%0Achallenge%20for%20deploying%20trustworthy%20AI%20systems%2C%20particularly%20in%20scenarios%0Ainvolving%20distribution%20shifts.%20Traditional%20methods%20for%20addressing%20fairness%20have%0Afailed%20in%20domain%20generalization%20due%20to%20their%20lack%20of%20consideration%20for%0Adistribution%20shifts.%20Although%20disentanglement%20has%20been%20used%20to%20tackle%20FairDG%2C%0Ait%20is%20limited%20by%20its%20strong%20assumptions.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20Fairness-aware%20Classifier-Guided%20Score-based%20Diffusion%20Models%20%28FADE%29%20as%0Aa%20novel%20approach%20to%20effectively%20address%20the%20FairDG%20issue.%20Specifically%2C%20we%0Afirst%20pre-train%20a%20score-based%20diffusion%20model%20%28SDM%29%20and%20two%20classifiers%20to%0Aequip%20the%20model%20with%20strong%20generalization%20capabilities%20across%20different%0Adomains.%20Then%2C%20we%20guide%20the%20SDM%20using%20these%20pre-trained%20classifiers%20to%0Aeffectively%20eliminate%20sensitive%20information%20from%20the%20generated%20data.%20Finally%2C%0Athe%20generated%20fair%20data%20is%20used%20to%20train%20downstream%20classifiers%2C%20ensuring%0Arobust%20performance%20under%20new%20data%20distributions.%20Extensive%20experiments%20on%20three%0Areal-world%20datasets%20demonstrate%20that%20FADE%20not%20only%20enhances%20fairness%20but%20also%0Aimproves%20accuracy%20in%20the%20presence%20of%20distribution%20shifts.%20Additionally%2C%20FADE%0Aoutperforms%20existing%20methods%20in%20achieving%20the%20best%20accuracy-fairness%0Atrade-offs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09495v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFADE%253A%2520Towards%2520Fairness-aware%2520Augmentation%2520for%2520Domain%2520Generalization%2520via%250A%2520%2520Classifier-Guided%2520Score-based%2520Diffusion%2520Models%26entry.906535625%3DYujie%2520Lin%2520and%2520Dong%2520Li%2520and%2520Chen%2520Zhao%2520and%2520Minglai%2520Shao%2520and%2520Guihong%2520Wan%26entry.1292438233%3D%2520%2520Fairness-aware%2520domain%2520generalization%2520%2528FairDG%2529%2520has%2520emerged%2520as%2520a%2520critical%250Achallenge%2520for%2520deploying%2520trustworthy%2520AI%2520systems%252C%2520particularly%2520in%2520scenarios%250Ainvolving%2520distribution%2520shifts.%2520Traditional%2520methods%2520for%2520addressing%2520fairness%2520have%250Afailed%2520in%2520domain%2520generalization%2520due%2520to%2520their%2520lack%2520of%2520consideration%2520for%250Adistribution%2520shifts.%2520Although%2520disentanglement%2520has%2520been%2520used%2520to%2520tackle%2520FairDG%252C%250Ait%2520is%2520limited%2520by%2520its%2520strong%2520assumptions.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520Fairness-aware%2520Classifier-Guided%2520Score-based%2520Diffusion%2520Models%2520%2528FADE%2529%2520as%250Aa%2520novel%2520approach%2520to%2520effectively%2520address%2520the%2520FairDG%2520issue.%2520Specifically%252C%2520we%250Afirst%2520pre-train%2520a%2520score-based%2520diffusion%2520model%2520%2528SDM%2529%2520and%2520two%2520classifiers%2520to%250Aequip%2520the%2520model%2520with%2520strong%2520generalization%2520capabilities%2520across%2520different%250Adomains.%2520Then%252C%2520we%2520guide%2520the%2520SDM%2520using%2520these%2520pre-trained%2520classifiers%2520to%250Aeffectively%2520eliminate%2520sensitive%2520information%2520from%2520the%2520generated%2520data.%2520Finally%252C%250Athe%2520generated%2520fair%2520data%2520is%2520used%2520to%2520train%2520downstream%2520classifiers%252C%2520ensuring%250Arobust%2520performance%2520under%2520new%2520data%2520distributions.%2520Extensive%2520experiments%2520on%2520three%250Areal-world%2520datasets%2520demonstrate%2520that%2520FADE%2520not%2520only%2520enhances%2520fairness%2520but%2520also%250Aimproves%2520accuracy%2520in%2520the%2520presence%2520of%2520distribution%2520shifts.%2520Additionally%252C%2520FADE%250Aoutperforms%2520existing%2520methods%2520in%2520achieving%2520the%2520best%2520accuracy-fairness%250Atrade-offs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09495v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FADE%3A%20Towards%20Fairness-aware%20Augmentation%20for%20Domain%20Generalization%20via%0A%20%20Classifier-Guided%20Score-based%20Diffusion%20Models&entry.906535625=Yujie%20Lin%20and%20Dong%20Li%20and%20Chen%20Zhao%20and%20Minglai%20Shao%20and%20Guihong%20Wan&entry.1292438233=%20%20Fairness-aware%20domain%20generalization%20%28FairDG%29%20has%20emerged%20as%20a%20critical%0Achallenge%20for%20deploying%20trustworthy%20AI%20systems%2C%20particularly%20in%20scenarios%0Ainvolving%20distribution%20shifts.%20Traditional%20methods%20for%20addressing%20fairness%20have%0Afailed%20in%20domain%20generalization%20due%20to%20their%20lack%20of%20consideration%20for%0Adistribution%20shifts.%20Although%20disentanglement%20has%20been%20used%20to%20tackle%20FairDG%2C%0Ait%20is%20limited%20by%20its%20strong%20assumptions.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20Fairness-aware%20Classifier-Guided%20Score-based%20Diffusion%20Models%20%28FADE%29%20as%0Aa%20novel%20approach%20to%20effectively%20address%20the%20FairDG%20issue.%20Specifically%2C%20we%0Afirst%20pre-train%20a%20score-based%20diffusion%20model%20%28SDM%29%20and%20two%20classifiers%20to%0Aequip%20the%20model%20with%20strong%20generalization%20capabilities%20across%20different%0Adomains.%20Then%2C%20we%20guide%20the%20SDM%20using%20these%20pre-trained%20classifiers%20to%0Aeffectively%20eliminate%20sensitive%20information%20from%20the%20generated%20data.%20Finally%2C%0Athe%20generated%20fair%20data%20is%20used%20to%20train%20downstream%20classifiers%2C%20ensuring%0Arobust%20performance%20under%20new%20data%20distributions.%20Extensive%20experiments%20on%20three%0Areal-world%20datasets%20demonstrate%20that%20FADE%20not%20only%20enhances%20fairness%20but%20also%0Aimproves%20accuracy%20in%20the%20presence%20of%20distribution%20shifts.%20Additionally%2C%20FADE%0Aoutperforms%20existing%20methods%20in%20achieving%20the%20best%20accuracy-fairness%0Atrade-offs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09495v3&entry.124074799=Read"},
{"title": "A Reinforcement Learning Approach to Quiet and Safe UAM Traffic\n  Management", "author": "Surya Murthy and John-Paul Clarke and Ufuk Topcu and Zhenyu Gao", "abstract": "  Urban air mobility (UAM) is a transformative system that operates various\nsmall aerial vehicles in urban environments to reshape urban transportation.\nHowever, integrating UAM into existing urban environments presents a variety of\ncomplex challenges. Recent analyses of UAM's operational constraints highlight\naircraft noise and system safety as key hurdles to UAM system implementation.\nFuture UAM air traffic management schemes must ensure that the system is both\nquiet and safe. We propose a multi-agent reinforcement learning approach to\nmanage UAM traffic, aiming at both vertical separation assurance and noise\nmitigation. Through extensive training, the reinforcement learning agent learns\nto balance the two primary objectives by employing altitude adjustments in a\nmulti-layer UAM network. The results reveal the tradeoffs among noise impact,\ntraffic congestion, and separation. Overall, our findings demonstrate the\npotential of reinforcement learning in mitigating UAM's noise impact while\nmaintaining safe separation using altitude adjustments\n", "link": "http://arxiv.org/abs/2501.08941v1", "date": "2025-01-15", "relevancy": 2.1004, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5368}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5316}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Reinforcement%20Learning%20Approach%20to%20Quiet%20and%20Safe%20UAM%20Traffic%0A%20%20Management&body=Title%3A%20A%20Reinforcement%20Learning%20Approach%20to%20Quiet%20and%20Safe%20UAM%20Traffic%0A%20%20Management%0AAuthor%3A%20Surya%20Murthy%20and%20John-Paul%20Clarke%20and%20Ufuk%20Topcu%20and%20Zhenyu%20Gao%0AAbstract%3A%20%20%20Urban%20air%20mobility%20%28UAM%29%20is%20a%20transformative%20system%20that%20operates%20various%0Asmall%20aerial%20vehicles%20in%20urban%20environments%20to%20reshape%20urban%20transportation.%0AHowever%2C%20integrating%20UAM%20into%20existing%20urban%20environments%20presents%20a%20variety%20of%0Acomplex%20challenges.%20Recent%20analyses%20of%20UAM%27s%20operational%20constraints%20highlight%0Aaircraft%20noise%20and%20system%20safety%20as%20key%20hurdles%20to%20UAM%20system%20implementation.%0AFuture%20UAM%20air%20traffic%20management%20schemes%20must%20ensure%20that%20the%20system%20is%20both%0Aquiet%20and%20safe.%20We%20propose%20a%20multi-agent%20reinforcement%20learning%20approach%20to%0Amanage%20UAM%20traffic%2C%20aiming%20at%20both%20vertical%20separation%20assurance%20and%20noise%0Amitigation.%20Through%20extensive%20training%2C%20the%20reinforcement%20learning%20agent%20learns%0Ato%20balance%20the%20two%20primary%20objectives%20by%20employing%20altitude%20adjustments%20in%20a%0Amulti-layer%20UAM%20network.%20The%20results%20reveal%20the%20tradeoffs%20among%20noise%20impact%2C%0Atraffic%20congestion%2C%20and%20separation.%20Overall%2C%20our%20findings%20demonstrate%20the%0Apotential%20of%20reinforcement%20learning%20in%20mitigating%20UAM%27s%20noise%20impact%20while%0Amaintaining%20safe%20separation%20using%20altitude%20adjustments%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Reinforcement%2520Learning%2520Approach%2520to%2520Quiet%2520and%2520Safe%2520UAM%2520Traffic%250A%2520%2520Management%26entry.906535625%3DSurya%2520Murthy%2520and%2520John-Paul%2520Clarke%2520and%2520Ufuk%2520Topcu%2520and%2520Zhenyu%2520Gao%26entry.1292438233%3D%2520%2520Urban%2520air%2520mobility%2520%2528UAM%2529%2520is%2520a%2520transformative%2520system%2520that%2520operates%2520various%250Asmall%2520aerial%2520vehicles%2520in%2520urban%2520environments%2520to%2520reshape%2520urban%2520transportation.%250AHowever%252C%2520integrating%2520UAM%2520into%2520existing%2520urban%2520environments%2520presents%2520a%2520variety%2520of%250Acomplex%2520challenges.%2520Recent%2520analyses%2520of%2520UAM%2527s%2520operational%2520constraints%2520highlight%250Aaircraft%2520noise%2520and%2520system%2520safety%2520as%2520key%2520hurdles%2520to%2520UAM%2520system%2520implementation.%250AFuture%2520UAM%2520air%2520traffic%2520management%2520schemes%2520must%2520ensure%2520that%2520the%2520system%2520is%2520both%250Aquiet%2520and%2520safe.%2520We%2520propose%2520a%2520multi-agent%2520reinforcement%2520learning%2520approach%2520to%250Amanage%2520UAM%2520traffic%252C%2520aiming%2520at%2520both%2520vertical%2520separation%2520assurance%2520and%2520noise%250Amitigation.%2520Through%2520extensive%2520training%252C%2520the%2520reinforcement%2520learning%2520agent%2520learns%250Ato%2520balance%2520the%2520two%2520primary%2520objectives%2520by%2520employing%2520altitude%2520adjustments%2520in%2520a%250Amulti-layer%2520UAM%2520network.%2520The%2520results%2520reveal%2520the%2520tradeoffs%2520among%2520noise%2520impact%252C%250Atraffic%2520congestion%252C%2520and%2520separation.%2520Overall%252C%2520our%2520findings%2520demonstrate%2520the%250Apotential%2520of%2520reinforcement%2520learning%2520in%2520mitigating%2520UAM%2527s%2520noise%2520impact%2520while%250Amaintaining%2520safe%2520separation%2520using%2520altitude%2520adjustments%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Reinforcement%20Learning%20Approach%20to%20Quiet%20and%20Safe%20UAM%20Traffic%0A%20%20Management&entry.906535625=Surya%20Murthy%20and%20John-Paul%20Clarke%20and%20Ufuk%20Topcu%20and%20Zhenyu%20Gao&entry.1292438233=%20%20Urban%20air%20mobility%20%28UAM%29%20is%20a%20transformative%20system%20that%20operates%20various%0Asmall%20aerial%20vehicles%20in%20urban%20environments%20to%20reshape%20urban%20transportation.%0AHowever%2C%20integrating%20UAM%20into%20existing%20urban%20environments%20presents%20a%20variety%20of%0Acomplex%20challenges.%20Recent%20analyses%20of%20UAM%27s%20operational%20constraints%20highlight%0Aaircraft%20noise%20and%20system%20safety%20as%20key%20hurdles%20to%20UAM%20system%20implementation.%0AFuture%20UAM%20air%20traffic%20management%20schemes%20must%20ensure%20that%20the%20system%20is%20both%0Aquiet%20and%20safe.%20We%20propose%20a%20multi-agent%20reinforcement%20learning%20approach%20to%0Amanage%20UAM%20traffic%2C%20aiming%20at%20both%20vertical%20separation%20assurance%20and%20noise%0Amitigation.%20Through%20extensive%20training%2C%20the%20reinforcement%20learning%20agent%20learns%0Ato%20balance%20the%20two%20primary%20objectives%20by%20employing%20altitude%20adjustments%20in%20a%0Amulti-layer%20UAM%20network.%20The%20results%20reveal%20the%20tradeoffs%20among%20noise%20impact%2C%0Atraffic%20congestion%2C%20and%20separation.%20Overall%2C%20our%20findings%20demonstrate%20the%0Apotential%20of%20reinforcement%20learning%20in%20mitigating%20UAM%27s%20noise%20impact%20while%0Amaintaining%20safe%20separation%20using%20altitude%20adjustments%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08941v1&entry.124074799=Read"},
{"title": "Structural damage detection via hierarchical damage information with\n  volumetric assessment", "author": "Isaac Osei Agyemang and Isaac Adjei-Mensah and Daniel Acheampong and Gordon Owusu Boateng and Adu Asare Baffour", "abstract": "  Structural health monitoring (SHM) is essential for ensuring the safety and\nlongevity of infrastructure, but complex image environments, noisy labels, and\nreliance on manual damage assessments often hinder its effectiveness. This\nstudy introduces the Guided Detection Network (Guided-DetNet), a framework\ndesigned to address these challenges. Guided-DetNet is characterized by a\nGenerative Attention Module (GAM), Hierarchical Elimination Algorithm (HEA),\nand Volumetric Contour Visual Assessment (VCVA). GAM leverages cross-horizontal\nand cross-vertical patch merging and cross-foreground-background feature fusion\nto generate varied features to mitigate complex image environments. HEA\naddresses noisy labeling using hierarchical relationships among classes to\nrefine instances given an image by eliminating unlikely class instances. VCVA\nassesses the severity of detected damages via volumetric representation and\nquantification leveraging the Dirac delta distribution. A comprehensive\nquantitative study and two robustness tests were conducted using the PEER Hub\ndataset, and a drone-based application, which involved a field experiment, was\nconducted to substantiate Guided-DetNet's promising performances. In triple\nclassification tasks, the framework achieved 96% accuracy, surpassing\nstate-of-the-art classifiers by up to 3%. In dual detection tasks, it\noutperformed competitive detectors with a precision of 94% and a mean average\nprecision (mAP) of 79% while maintaining a frame rate of 57.04fps, suitable for\nreal-time applications. Additionally, robustness tests demonstrated resilience\nunder adverse conditions, with precision scores ranging from 79% to 91%.\nGuided-DetNet is established as a robust and efficient framework for SHM,\noffering advancements in automation and precision, with the potential for\nwidespread application in drone-based infrastructure inspections.\n", "link": "http://arxiv.org/abs/2407.19694v2", "date": "2025-01-15", "relevancy": 2.0894, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.525}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5219}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20damage%20detection%20via%20hierarchical%20damage%20information%20with%0A%20%20volumetric%20assessment&body=Title%3A%20Structural%20damage%20detection%20via%20hierarchical%20damage%20information%20with%0A%20%20volumetric%20assessment%0AAuthor%3A%20Isaac%20Osei%20Agyemang%20and%20Isaac%20Adjei-Mensah%20and%20Daniel%20Acheampong%20and%20Gordon%20Owusu%20Boateng%20and%20Adu%20Asare%20Baffour%0AAbstract%3A%20%20%20Structural%20health%20monitoring%20%28SHM%29%20is%20essential%20for%20ensuring%20the%20safety%20and%0Alongevity%20of%20infrastructure%2C%20but%20complex%20image%20environments%2C%20noisy%20labels%2C%20and%0Areliance%20on%20manual%20damage%20assessments%20often%20hinder%20its%20effectiveness.%20This%0Astudy%20introduces%20the%20Guided%20Detection%20Network%20%28Guided-DetNet%29%2C%20a%20framework%0Adesigned%20to%20address%20these%20challenges.%20Guided-DetNet%20is%20characterized%20by%20a%0AGenerative%20Attention%20Module%20%28GAM%29%2C%20Hierarchical%20Elimination%20Algorithm%20%28HEA%29%2C%0Aand%20Volumetric%20Contour%20Visual%20Assessment%20%28VCVA%29.%20GAM%20leverages%20cross-horizontal%0Aand%20cross-vertical%20patch%20merging%20and%20cross-foreground-background%20feature%20fusion%0Ato%20generate%20varied%20features%20to%20mitigate%20complex%20image%20environments.%20HEA%0Aaddresses%20noisy%20labeling%20using%20hierarchical%20relationships%20among%20classes%20to%0Arefine%20instances%20given%20an%20image%20by%20eliminating%20unlikely%20class%20instances.%20VCVA%0Aassesses%20the%20severity%20of%20detected%20damages%20via%20volumetric%20representation%20and%0Aquantification%20leveraging%20the%20Dirac%20delta%20distribution.%20A%20comprehensive%0Aquantitative%20study%20and%20two%20robustness%20tests%20were%20conducted%20using%20the%20PEER%20Hub%0Adataset%2C%20and%20a%20drone-based%20application%2C%20which%20involved%20a%20field%20experiment%2C%20was%0Aconducted%20to%20substantiate%20Guided-DetNet%27s%20promising%20performances.%20In%20triple%0Aclassification%20tasks%2C%20the%20framework%20achieved%2096%25%20accuracy%2C%20surpassing%0Astate-of-the-art%20classifiers%20by%20up%20to%203%25.%20In%20dual%20detection%20tasks%2C%20it%0Aoutperformed%20competitive%20detectors%20with%20a%20precision%20of%2094%25%20and%20a%20mean%20average%0Aprecision%20%28mAP%29%20of%2079%25%20while%20maintaining%20a%20frame%20rate%20of%2057.04fps%2C%20suitable%20for%0Areal-time%20applications.%20Additionally%2C%20robustness%20tests%20demonstrated%20resilience%0Aunder%20adverse%20conditions%2C%20with%20precision%20scores%20ranging%20from%2079%25%20to%2091%25.%0AGuided-DetNet%20is%20established%20as%20a%20robust%20and%20efficient%20framework%20for%20SHM%2C%0Aoffering%20advancements%20in%20automation%20and%20precision%2C%20with%20the%20potential%20for%0Awidespread%20application%20in%20drone-based%20infrastructure%20inspections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19694v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520damage%2520detection%2520via%2520hierarchical%2520damage%2520information%2520with%250A%2520%2520volumetric%2520assessment%26entry.906535625%3DIsaac%2520Osei%2520Agyemang%2520and%2520Isaac%2520Adjei-Mensah%2520and%2520Daniel%2520Acheampong%2520and%2520Gordon%2520Owusu%2520Boateng%2520and%2520Adu%2520Asare%2520Baffour%26entry.1292438233%3D%2520%2520Structural%2520health%2520monitoring%2520%2528SHM%2529%2520is%2520essential%2520for%2520ensuring%2520the%2520safety%2520and%250Alongevity%2520of%2520infrastructure%252C%2520but%2520complex%2520image%2520environments%252C%2520noisy%2520labels%252C%2520and%250Areliance%2520on%2520manual%2520damage%2520assessments%2520often%2520hinder%2520its%2520effectiveness.%2520This%250Astudy%2520introduces%2520the%2520Guided%2520Detection%2520Network%2520%2528Guided-DetNet%2529%252C%2520a%2520framework%250Adesigned%2520to%2520address%2520these%2520challenges.%2520Guided-DetNet%2520is%2520characterized%2520by%2520a%250AGenerative%2520Attention%2520Module%2520%2528GAM%2529%252C%2520Hierarchical%2520Elimination%2520Algorithm%2520%2528HEA%2529%252C%250Aand%2520Volumetric%2520Contour%2520Visual%2520Assessment%2520%2528VCVA%2529.%2520GAM%2520leverages%2520cross-horizontal%250Aand%2520cross-vertical%2520patch%2520merging%2520and%2520cross-foreground-background%2520feature%2520fusion%250Ato%2520generate%2520varied%2520features%2520to%2520mitigate%2520complex%2520image%2520environments.%2520HEA%250Aaddresses%2520noisy%2520labeling%2520using%2520hierarchical%2520relationships%2520among%2520classes%2520to%250Arefine%2520instances%2520given%2520an%2520image%2520by%2520eliminating%2520unlikely%2520class%2520instances.%2520VCVA%250Aassesses%2520the%2520severity%2520of%2520detected%2520damages%2520via%2520volumetric%2520representation%2520and%250Aquantification%2520leveraging%2520the%2520Dirac%2520delta%2520distribution.%2520A%2520comprehensive%250Aquantitative%2520study%2520and%2520two%2520robustness%2520tests%2520were%2520conducted%2520using%2520the%2520PEER%2520Hub%250Adataset%252C%2520and%2520a%2520drone-based%2520application%252C%2520which%2520involved%2520a%2520field%2520experiment%252C%2520was%250Aconducted%2520to%2520substantiate%2520Guided-DetNet%2527s%2520promising%2520performances.%2520In%2520triple%250Aclassification%2520tasks%252C%2520the%2520framework%2520achieved%252096%2525%2520accuracy%252C%2520surpassing%250Astate-of-the-art%2520classifiers%2520by%2520up%2520to%25203%2525.%2520In%2520dual%2520detection%2520tasks%252C%2520it%250Aoutperformed%2520competitive%2520detectors%2520with%2520a%2520precision%2520of%252094%2525%2520and%2520a%2520mean%2520average%250Aprecision%2520%2528mAP%2529%2520of%252079%2525%2520while%2520maintaining%2520a%2520frame%2520rate%2520of%252057.04fps%252C%2520suitable%2520for%250Areal-time%2520applications.%2520Additionally%252C%2520robustness%2520tests%2520demonstrated%2520resilience%250Aunder%2520adverse%2520conditions%252C%2520with%2520precision%2520scores%2520ranging%2520from%252079%2525%2520to%252091%2525.%250AGuided-DetNet%2520is%2520established%2520as%2520a%2520robust%2520and%2520efficient%2520framework%2520for%2520SHM%252C%250Aoffering%2520advancements%2520in%2520automation%2520and%2520precision%252C%2520with%2520the%2520potential%2520for%250Awidespread%2520application%2520in%2520drone-based%2520infrastructure%2520inspections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19694v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20damage%20detection%20via%20hierarchical%20damage%20information%20with%0A%20%20volumetric%20assessment&entry.906535625=Isaac%20Osei%20Agyemang%20and%20Isaac%20Adjei-Mensah%20and%20Daniel%20Acheampong%20and%20Gordon%20Owusu%20Boateng%20and%20Adu%20Asare%20Baffour&entry.1292438233=%20%20Structural%20health%20monitoring%20%28SHM%29%20is%20essential%20for%20ensuring%20the%20safety%20and%0Alongevity%20of%20infrastructure%2C%20but%20complex%20image%20environments%2C%20noisy%20labels%2C%20and%0Areliance%20on%20manual%20damage%20assessments%20often%20hinder%20its%20effectiveness.%20This%0Astudy%20introduces%20the%20Guided%20Detection%20Network%20%28Guided-DetNet%29%2C%20a%20framework%0Adesigned%20to%20address%20these%20challenges.%20Guided-DetNet%20is%20characterized%20by%20a%0AGenerative%20Attention%20Module%20%28GAM%29%2C%20Hierarchical%20Elimination%20Algorithm%20%28HEA%29%2C%0Aand%20Volumetric%20Contour%20Visual%20Assessment%20%28VCVA%29.%20GAM%20leverages%20cross-horizontal%0Aand%20cross-vertical%20patch%20merging%20and%20cross-foreground-background%20feature%20fusion%0Ato%20generate%20varied%20features%20to%20mitigate%20complex%20image%20environments.%20HEA%0Aaddresses%20noisy%20labeling%20using%20hierarchical%20relationships%20among%20classes%20to%0Arefine%20instances%20given%20an%20image%20by%20eliminating%20unlikely%20class%20instances.%20VCVA%0Aassesses%20the%20severity%20of%20detected%20damages%20via%20volumetric%20representation%20and%0Aquantification%20leveraging%20the%20Dirac%20delta%20distribution.%20A%20comprehensive%0Aquantitative%20study%20and%20two%20robustness%20tests%20were%20conducted%20using%20the%20PEER%20Hub%0Adataset%2C%20and%20a%20drone-based%20application%2C%20which%20involved%20a%20field%20experiment%2C%20was%0Aconducted%20to%20substantiate%20Guided-DetNet%27s%20promising%20performances.%20In%20triple%0Aclassification%20tasks%2C%20the%20framework%20achieved%2096%25%20accuracy%2C%20surpassing%0Astate-of-the-art%20classifiers%20by%20up%20to%203%25.%20In%20dual%20detection%20tasks%2C%20it%0Aoutperformed%20competitive%20detectors%20with%20a%20precision%20of%2094%25%20and%20a%20mean%20average%0Aprecision%20%28mAP%29%20of%2079%25%20while%20maintaining%20a%20frame%20rate%20of%2057.04fps%2C%20suitable%20for%0Areal-time%20applications.%20Additionally%2C%20robustness%20tests%20demonstrated%20resilience%0Aunder%20adverse%20conditions%2C%20with%20precision%20scores%20ranging%20from%2079%25%20to%2091%25.%0AGuided-DetNet%20is%20established%20as%20a%20robust%20and%20efficient%20framework%20for%20SHM%2C%0Aoffering%20advancements%20in%20automation%20and%20precision%2C%20with%20the%20potential%20for%0Awidespread%20application%20in%20drone-based%20infrastructure%20inspections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19694v2&entry.124074799=Read"},
{"title": "Unseen Horizons: Unveiling the Real Capability of LLM Code Generation\n  Beyond the Familiar", "author": "Yuanliang Zhang and Yifan Xie and Shanshan Li and Ke Liu and Chong Wang and Zhouyang Jia and Xiangbing Huang and Jie Song and Chaopeng Luo and Zhizheng Zheng and Rulin Xu and Yitong Liu and Si Zheng and Xiangke Liao", "abstract": "  Recently, large language models (LLMs) have shown strong potential in code\ngeneration tasks. However, there are still gaps before they can be fully\napplied in actual software development processes. Accurately assessing the code\ngeneration capabilities of large language models has become an important basis\nfor evaluating and improving the models. Some existing works have constructed\ndatasets to evaluate the capabilities of these models. However, the current\nevaluation process may encounter the illusion of \"Specialist in Familiarity\",\nprimarily due to three gaps: the exposure of target code, case timeliness, and\ndependency availability. The fundamental reason for these gaps is that the code\nin current datasets may have been extensively exposed and exercised during the\ntraining phase, and due to the continuous training and development of LLM,\ntheir timeliness has been severely compromised. The key to solve the problem is\nto, as much as possible, evaluate the LLMs using code that they have not\nencountered before. Thus, the fundamental idea in this paper is to draw on the\nconcept of code obfuscation, changing code at different levels while ensuring\nthe functionality and output. To this end, we build a code-obfuscation based\nbenchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world\nprojects, including function description and code. Then we use three-level\nstrategy (symbol, structure and semantic) to obfuscate descriptions, code and\ncontext dependencies. We evaluate four LLMs on OBFU- SEVAL and compared the\neffectiveness of different obfuscation strategy. We use official test suites of\nthese projects to evaluate the generated code. The results show that after\nobfuscation, the average decrease ratio of test pass rate can up to 62.5%.\n", "link": "http://arxiv.org/abs/2412.08109v2", "date": "2025-01-15", "relevancy": 2.0875, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unseen%20Horizons%3A%20Unveiling%20the%20Real%20Capability%20of%20LLM%20Code%20Generation%0A%20%20Beyond%20the%20Familiar&body=Title%3A%20Unseen%20Horizons%3A%20Unveiling%20the%20Real%20Capability%20of%20LLM%20Code%20Generation%0A%20%20Beyond%20the%20Familiar%0AAuthor%3A%20Yuanliang%20Zhang%20and%20Yifan%20Xie%20and%20Shanshan%20Li%20and%20Ke%20Liu%20and%20Chong%20Wang%20and%20Zhouyang%20Jia%20and%20Xiangbing%20Huang%20and%20Jie%20Song%20and%20Chaopeng%20Luo%20and%20Zhizheng%20Zheng%20and%20Rulin%20Xu%20and%20Yitong%20Liu%20and%20Si%20Zheng%20and%20Xiangke%20Liao%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20shown%20strong%20potential%20in%20code%0Ageneration%20tasks.%20However%2C%20there%20are%20still%20gaps%20before%20they%20can%20be%20fully%0Aapplied%20in%20actual%20software%20development%20processes.%20Accurately%20assessing%20the%20code%0Ageneration%20capabilities%20of%20large%20language%20models%20has%20become%20an%20important%20basis%0Afor%20evaluating%20and%20improving%20the%20models.%20Some%20existing%20works%20have%20constructed%0Adatasets%20to%20evaluate%20the%20capabilities%20of%20these%20models.%20However%2C%20the%20current%0Aevaluation%20process%20may%20encounter%20the%20illusion%20of%20%22Specialist%20in%20Familiarity%22%2C%0Aprimarily%20due%20to%20three%20gaps%3A%20the%20exposure%20of%20target%20code%2C%20case%20timeliness%2C%20and%0Adependency%20availability.%20The%20fundamental%20reason%20for%20these%20gaps%20is%20that%20the%20code%0Ain%20current%20datasets%20may%20have%20been%20extensively%20exposed%20and%20exercised%20during%20the%0Atraining%20phase%2C%20and%20due%20to%20the%20continuous%20training%20and%20development%20of%20LLM%2C%0Atheir%20timeliness%20has%20been%20severely%20compromised.%20The%20key%20to%20solve%20the%20problem%20is%0Ato%2C%20as%20much%20as%20possible%2C%20evaluate%20the%20LLMs%20using%20code%20that%20they%20have%20not%0Aencountered%20before.%20Thus%2C%20the%20fundamental%20idea%20in%20this%20paper%20is%20to%20draw%20on%20the%0Aconcept%20of%20code%20obfuscation%2C%20changing%20code%20at%20different%20levels%20while%20ensuring%0Athe%20functionality%20and%20output.%20To%20this%20end%2C%20we%20build%20a%20code-obfuscation%20based%0Abenchmark%20OBFUSEVAL.%20We%20first%20collect%201%2C354%20raw%20cases%20from%20five%20real-world%0Aprojects%2C%20including%20function%20description%20and%20code.%20Then%20we%20use%20three-level%0Astrategy%20%28symbol%2C%20structure%20and%20semantic%29%20to%20obfuscate%20descriptions%2C%20code%20and%0Acontext%20dependencies.%20We%20evaluate%20four%20LLMs%20on%20OBFU-%20SEVAL%20and%20compared%20the%0Aeffectiveness%20of%20different%20obfuscation%20strategy.%20We%20use%20official%20test%20suites%20of%0Athese%20projects%20to%20evaluate%20the%20generated%20code.%20The%20results%20show%20that%20after%0Aobfuscation%2C%20the%20average%20decrease%20ratio%20of%20test%20pass%20rate%20can%20up%20to%2062.5%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08109v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnseen%2520Horizons%253A%2520Unveiling%2520the%2520Real%2520Capability%2520of%2520LLM%2520Code%2520Generation%250A%2520%2520Beyond%2520the%2520Familiar%26entry.906535625%3DYuanliang%2520Zhang%2520and%2520Yifan%2520Xie%2520and%2520Shanshan%2520Li%2520and%2520Ke%2520Liu%2520and%2520Chong%2520Wang%2520and%2520Zhouyang%2520Jia%2520and%2520Xiangbing%2520Huang%2520and%2520Jie%2520Song%2520and%2520Chaopeng%2520Luo%2520and%2520Zhizheng%2520Zheng%2520and%2520Rulin%2520Xu%2520and%2520Yitong%2520Liu%2520and%2520Si%2520Zheng%2520and%2520Xiangke%2520Liao%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520strong%2520potential%2520in%2520code%250Ageneration%2520tasks.%2520However%252C%2520there%2520are%2520still%2520gaps%2520before%2520they%2520can%2520be%2520fully%250Aapplied%2520in%2520actual%2520software%2520development%2520processes.%2520Accurately%2520assessing%2520the%2520code%250Ageneration%2520capabilities%2520of%2520large%2520language%2520models%2520has%2520become%2520an%2520important%2520basis%250Afor%2520evaluating%2520and%2520improving%2520the%2520models.%2520Some%2520existing%2520works%2520have%2520constructed%250Adatasets%2520to%2520evaluate%2520the%2520capabilities%2520of%2520these%2520models.%2520However%252C%2520the%2520current%250Aevaluation%2520process%2520may%2520encounter%2520the%2520illusion%2520of%2520%2522Specialist%2520in%2520Familiarity%2522%252C%250Aprimarily%2520due%2520to%2520three%2520gaps%253A%2520the%2520exposure%2520of%2520target%2520code%252C%2520case%2520timeliness%252C%2520and%250Adependency%2520availability.%2520The%2520fundamental%2520reason%2520for%2520these%2520gaps%2520is%2520that%2520the%2520code%250Ain%2520current%2520datasets%2520may%2520have%2520been%2520extensively%2520exposed%2520and%2520exercised%2520during%2520the%250Atraining%2520phase%252C%2520and%2520due%2520to%2520the%2520continuous%2520training%2520and%2520development%2520of%2520LLM%252C%250Atheir%2520timeliness%2520has%2520been%2520severely%2520compromised.%2520The%2520key%2520to%2520solve%2520the%2520problem%2520is%250Ato%252C%2520as%2520much%2520as%2520possible%252C%2520evaluate%2520the%2520LLMs%2520using%2520code%2520that%2520they%2520have%2520not%250Aencountered%2520before.%2520Thus%252C%2520the%2520fundamental%2520idea%2520in%2520this%2520paper%2520is%2520to%2520draw%2520on%2520the%250Aconcept%2520of%2520code%2520obfuscation%252C%2520changing%2520code%2520at%2520different%2520levels%2520while%2520ensuring%250Athe%2520functionality%2520and%2520output.%2520To%2520this%2520end%252C%2520we%2520build%2520a%2520code-obfuscation%2520based%250Abenchmark%2520OBFUSEVAL.%2520We%2520first%2520collect%25201%252C354%2520raw%2520cases%2520from%2520five%2520real-world%250Aprojects%252C%2520including%2520function%2520description%2520and%2520code.%2520Then%2520we%2520use%2520three-level%250Astrategy%2520%2528symbol%252C%2520structure%2520and%2520semantic%2529%2520to%2520obfuscate%2520descriptions%252C%2520code%2520and%250Acontext%2520dependencies.%2520We%2520evaluate%2520four%2520LLMs%2520on%2520OBFU-%2520SEVAL%2520and%2520compared%2520the%250Aeffectiveness%2520of%2520different%2520obfuscation%2520strategy.%2520We%2520use%2520official%2520test%2520suites%2520of%250Athese%2520projects%2520to%2520evaluate%2520the%2520generated%2520code.%2520The%2520results%2520show%2520that%2520after%250Aobfuscation%252C%2520the%2520average%2520decrease%2520ratio%2520of%2520test%2520pass%2520rate%2520can%2520up%2520to%252062.5%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08109v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unseen%20Horizons%3A%20Unveiling%20the%20Real%20Capability%20of%20LLM%20Code%20Generation%0A%20%20Beyond%20the%20Familiar&entry.906535625=Yuanliang%20Zhang%20and%20Yifan%20Xie%20and%20Shanshan%20Li%20and%20Ke%20Liu%20and%20Chong%20Wang%20and%20Zhouyang%20Jia%20and%20Xiangbing%20Huang%20and%20Jie%20Song%20and%20Chaopeng%20Luo%20and%20Zhizheng%20Zheng%20and%20Rulin%20Xu%20and%20Yitong%20Liu%20and%20Si%20Zheng%20and%20Xiangke%20Liao&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20shown%20strong%20potential%20in%20code%0Ageneration%20tasks.%20However%2C%20there%20are%20still%20gaps%20before%20they%20can%20be%20fully%0Aapplied%20in%20actual%20software%20development%20processes.%20Accurately%20assessing%20the%20code%0Ageneration%20capabilities%20of%20large%20language%20models%20has%20become%20an%20important%20basis%0Afor%20evaluating%20and%20improving%20the%20models.%20Some%20existing%20works%20have%20constructed%0Adatasets%20to%20evaluate%20the%20capabilities%20of%20these%20models.%20However%2C%20the%20current%0Aevaluation%20process%20may%20encounter%20the%20illusion%20of%20%22Specialist%20in%20Familiarity%22%2C%0Aprimarily%20due%20to%20three%20gaps%3A%20the%20exposure%20of%20target%20code%2C%20case%20timeliness%2C%20and%0Adependency%20availability.%20The%20fundamental%20reason%20for%20these%20gaps%20is%20that%20the%20code%0Ain%20current%20datasets%20may%20have%20been%20extensively%20exposed%20and%20exercised%20during%20the%0Atraining%20phase%2C%20and%20due%20to%20the%20continuous%20training%20and%20development%20of%20LLM%2C%0Atheir%20timeliness%20has%20been%20severely%20compromised.%20The%20key%20to%20solve%20the%20problem%20is%0Ato%2C%20as%20much%20as%20possible%2C%20evaluate%20the%20LLMs%20using%20code%20that%20they%20have%20not%0Aencountered%20before.%20Thus%2C%20the%20fundamental%20idea%20in%20this%20paper%20is%20to%20draw%20on%20the%0Aconcept%20of%20code%20obfuscation%2C%20changing%20code%20at%20different%20levels%20while%20ensuring%0Athe%20functionality%20and%20output.%20To%20this%20end%2C%20we%20build%20a%20code-obfuscation%20based%0Abenchmark%20OBFUSEVAL.%20We%20first%20collect%201%2C354%20raw%20cases%20from%20five%20real-world%0Aprojects%2C%20including%20function%20description%20and%20code.%20Then%20we%20use%20three-level%0Astrategy%20%28symbol%2C%20structure%20and%20semantic%29%20to%20obfuscate%20descriptions%2C%20code%20and%0Acontext%20dependencies.%20We%20evaluate%20four%20LLMs%20on%20OBFU-%20SEVAL%20and%20compared%20the%0Aeffectiveness%20of%20different%20obfuscation%20strategy.%20We%20use%20official%20test%20suites%20of%0Athese%20projects%20to%20evaluate%20the%20generated%20code.%20The%20results%20show%20that%20after%0Aobfuscation%2C%20the%20average%20decrease%20ratio%20of%20test%20pass%20rate%20can%20up%20to%2062.5%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08109v2&entry.124074799=Read"},
{"title": "PACE: Marrying generalization in PArameter-efficient fine-tuning with\n  Consistency rEgularization", "author": "Yao Ni and Shan Zhang and Piotr Koniusz", "abstract": "  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained\ntransformers to downstream tasks. However, the optimization of tasks\nperformance often comes at the cost of generalizability in fine-tuned models.\nTo address this issue, we theoretically connect smaller weight gradient norms\nduring training and larger datasets to the improvements in model\ngeneralization. Motivated by this connection, we propose reducing gradient\nnorms for enhanced generalization and aligning fine-tuned model with the\npre-trained counterpart to retain knowledge from large-scale pre-training data.\nYet, naive alignment does not guarantee gradient reduction and can potentially\ncause gradient explosion, complicating efforts to manage gradients. To address\nsuch an issue, we propose PACE, marrying generalization of PArameter-efficient\nfine-tuning with Consistency rEgularization. We perturb features learned from\nthe adapter with the multiplicative noise and ensure the fine-tuned model\nremains consistent for same sample under different perturbations. Theoretical\nanalysis shows that PACE not only implicitly regularizes gradients for enhanced\ngeneralization, but also implicitly aligns the fine-tuned and pre-trained\nmodels to retain knowledge. Experimental evidence supports our theories. PACE\nsurpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC,\nfew-shot learning, domain adaptation) showcasing its potential for\nresource-efficient fine-tuning. It also improves LoRA in text classification\n(GLUE) and mathematical reasoning (GSM-8K). The code is available at\nhttps://github.com/MaxwellYaoNi/PACE\n", "link": "http://arxiv.org/abs/2409.17137v4", "date": "2025-01-15", "relevancy": 2.0811, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5229}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PACE%3A%20Marrying%20generalization%20in%20PArameter-efficient%20fine-tuning%20with%0A%20%20Consistency%20rEgularization&body=Title%3A%20PACE%3A%20Marrying%20generalization%20in%20PArameter-efficient%20fine-tuning%20with%0A%20%20Consistency%20rEgularization%0AAuthor%3A%20Yao%20Ni%20and%20Shan%20Zhang%20and%20Piotr%20Koniusz%0AAbstract%3A%20%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20effectively%20adapts%20pre-trained%0Atransformers%20to%20downstream%20tasks.%20However%2C%20the%20optimization%20of%20tasks%0Aperformance%20often%20comes%20at%20the%20cost%20of%20generalizability%20in%20fine-tuned%20models.%0ATo%20address%20this%20issue%2C%20we%20theoretically%20connect%20smaller%20weight%20gradient%20norms%0Aduring%20training%20and%20larger%20datasets%20to%20the%20improvements%20in%20model%0Ageneralization.%20Motivated%20by%20this%20connection%2C%20we%20propose%20reducing%20gradient%0Anorms%20for%20enhanced%20generalization%20and%20aligning%20fine-tuned%20model%20with%20the%0Apre-trained%20counterpart%20to%20retain%20knowledge%20from%20large-scale%20pre-training%20data.%0AYet%2C%20naive%20alignment%20does%20not%20guarantee%20gradient%20reduction%20and%20can%20potentially%0Acause%20gradient%20explosion%2C%20complicating%20efforts%20to%20manage%20gradients.%20To%20address%0Asuch%20an%20issue%2C%20we%20propose%20PACE%2C%20marrying%20generalization%20of%20PArameter-efficient%0Afine-tuning%20with%20Consistency%20rEgularization.%20We%20perturb%20features%20learned%20from%0Athe%20adapter%20with%20the%20multiplicative%20noise%20and%20ensure%20the%20fine-tuned%20model%0Aremains%20consistent%20for%20same%20sample%20under%20different%20perturbations.%20Theoretical%0Aanalysis%20shows%20that%20PACE%20not%20only%20implicitly%20regularizes%20gradients%20for%20enhanced%0Ageneralization%2C%20but%20also%20implicitly%20aligns%20the%20fine-tuned%20and%20pre-trained%0Amodels%20to%20retain%20knowledge.%20Experimental%20evidence%20supports%20our%20theories.%20PACE%0Asurpasses%20existing%20PEFT%20methods%20in%20visual%20adaptation%20tasks%20%28VTAB-1k%2C%20FGVC%2C%0Afew-shot%20learning%2C%20domain%20adaptation%29%20showcasing%20its%20potential%20for%0Aresource-efficient%20fine-tuning.%20It%20also%20improves%20LoRA%20in%20text%20classification%0A%28GLUE%29%20and%20mathematical%20reasoning%20%28GSM-8K%29.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/MaxwellYaoNi/PACE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17137v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPACE%253A%2520Marrying%2520generalization%2520in%2520PArameter-efficient%2520fine-tuning%2520with%250A%2520%2520Consistency%2520rEgularization%26entry.906535625%3DYao%2520Ni%2520and%2520Shan%2520Zhang%2520and%2520Piotr%2520Koniusz%26entry.1292438233%3D%2520%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520effectively%2520adapts%2520pre-trained%250Atransformers%2520to%2520downstream%2520tasks.%2520However%252C%2520the%2520optimization%2520of%2520tasks%250Aperformance%2520often%2520comes%2520at%2520the%2520cost%2520of%2520generalizability%2520in%2520fine-tuned%2520models.%250ATo%2520address%2520this%2520issue%252C%2520we%2520theoretically%2520connect%2520smaller%2520weight%2520gradient%2520norms%250Aduring%2520training%2520and%2520larger%2520datasets%2520to%2520the%2520improvements%2520in%2520model%250Ageneralization.%2520Motivated%2520by%2520this%2520connection%252C%2520we%2520propose%2520reducing%2520gradient%250Anorms%2520for%2520enhanced%2520generalization%2520and%2520aligning%2520fine-tuned%2520model%2520with%2520the%250Apre-trained%2520counterpart%2520to%2520retain%2520knowledge%2520from%2520large-scale%2520pre-training%2520data.%250AYet%252C%2520naive%2520alignment%2520does%2520not%2520guarantee%2520gradient%2520reduction%2520and%2520can%2520potentially%250Acause%2520gradient%2520explosion%252C%2520complicating%2520efforts%2520to%2520manage%2520gradients.%2520To%2520address%250Asuch%2520an%2520issue%252C%2520we%2520propose%2520PACE%252C%2520marrying%2520generalization%2520of%2520PArameter-efficient%250Afine-tuning%2520with%2520Consistency%2520rEgularization.%2520We%2520perturb%2520features%2520learned%2520from%250Athe%2520adapter%2520with%2520the%2520multiplicative%2520noise%2520and%2520ensure%2520the%2520fine-tuned%2520model%250Aremains%2520consistent%2520for%2520same%2520sample%2520under%2520different%2520perturbations.%2520Theoretical%250Aanalysis%2520shows%2520that%2520PACE%2520not%2520only%2520implicitly%2520regularizes%2520gradients%2520for%2520enhanced%250Ageneralization%252C%2520but%2520also%2520implicitly%2520aligns%2520the%2520fine-tuned%2520and%2520pre-trained%250Amodels%2520to%2520retain%2520knowledge.%2520Experimental%2520evidence%2520supports%2520our%2520theories.%2520PACE%250Asurpasses%2520existing%2520PEFT%2520methods%2520in%2520visual%2520adaptation%2520tasks%2520%2528VTAB-1k%252C%2520FGVC%252C%250Afew-shot%2520learning%252C%2520domain%2520adaptation%2529%2520showcasing%2520its%2520potential%2520for%250Aresource-efficient%2520fine-tuning.%2520It%2520also%2520improves%2520LoRA%2520in%2520text%2520classification%250A%2528GLUE%2529%2520and%2520mathematical%2520reasoning%2520%2528GSM-8K%2529.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/MaxwellYaoNi/PACE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17137v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACE%3A%20Marrying%20generalization%20in%20PArameter-efficient%20fine-tuning%20with%0A%20%20Consistency%20rEgularization&entry.906535625=Yao%20Ni%20and%20Shan%20Zhang%20and%20Piotr%20Koniusz&entry.1292438233=%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20effectively%20adapts%20pre-trained%0Atransformers%20to%20downstream%20tasks.%20However%2C%20the%20optimization%20of%20tasks%0Aperformance%20often%20comes%20at%20the%20cost%20of%20generalizability%20in%20fine-tuned%20models.%0ATo%20address%20this%20issue%2C%20we%20theoretically%20connect%20smaller%20weight%20gradient%20norms%0Aduring%20training%20and%20larger%20datasets%20to%20the%20improvements%20in%20model%0Ageneralization.%20Motivated%20by%20this%20connection%2C%20we%20propose%20reducing%20gradient%0Anorms%20for%20enhanced%20generalization%20and%20aligning%20fine-tuned%20model%20with%20the%0Apre-trained%20counterpart%20to%20retain%20knowledge%20from%20large-scale%20pre-training%20data.%0AYet%2C%20naive%20alignment%20does%20not%20guarantee%20gradient%20reduction%20and%20can%20potentially%0Acause%20gradient%20explosion%2C%20complicating%20efforts%20to%20manage%20gradients.%20To%20address%0Asuch%20an%20issue%2C%20we%20propose%20PACE%2C%20marrying%20generalization%20of%20PArameter-efficient%0Afine-tuning%20with%20Consistency%20rEgularization.%20We%20perturb%20features%20learned%20from%0Athe%20adapter%20with%20the%20multiplicative%20noise%20and%20ensure%20the%20fine-tuned%20model%0Aremains%20consistent%20for%20same%20sample%20under%20different%20perturbations.%20Theoretical%0Aanalysis%20shows%20that%20PACE%20not%20only%20implicitly%20regularizes%20gradients%20for%20enhanced%0Ageneralization%2C%20but%20also%20implicitly%20aligns%20the%20fine-tuned%20and%20pre-trained%0Amodels%20to%20retain%20knowledge.%20Experimental%20evidence%20supports%20our%20theories.%20PACE%0Asurpasses%20existing%20PEFT%20methods%20in%20visual%20adaptation%20tasks%20%28VTAB-1k%2C%20FGVC%2C%0Afew-shot%20learning%2C%20domain%20adaptation%29%20showcasing%20its%20potential%20for%0Aresource-efficient%20fine-tuning.%20It%20also%20improves%20LoRA%20in%20text%20classification%0A%28GLUE%29%20and%20mathematical%20reasoning%20%28GSM-8K%29.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/MaxwellYaoNi/PACE%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17137v4&entry.124074799=Read"},
{"title": "Disentangled Interleaving Variational Encoding", "author": "Noelle Y. L. Wong and Eng Yeow Cheu and Zhonglin Chiam", "abstract": "  Conflicting objectives present a considerable challenge in interleaving\nmulti-task learning, necessitating the need for meticulous design and balance\nto ensure effective learning of a representative latent data space across all\ntasks without mutual negative impact. Drawing inspiration from the concept of\nmarginal and conditional probability distributions in probability theory, we\ndesign a principled and well-founded approach to disentangle the original input\ninto marginal and conditional probability distributions in the latent space of\na variational autoencoder. Our proposed model, Deep Disentangled Interleaving\nVariational Encoding (DeepDIVE) learns disentangled features from the original\ninput to form clusters in the embedding space and unifies these features via\nthe cross-attention mechanism in the fusion stage. We theoretically prove that\ncombining the objectives for reconstruction and forecasting fully captures the\nlower bound and mathematically derive a loss function for disentanglement using\nNa\\\"ive Bayes. Under the assumption that the prior is a mixture of log-concave\ndistributions, we also establish that the Kullback-Leibler divergence between\nthe prior and the posterior is upper bounded by a function minimized by the\nminimizer of the cross entropy loss, informing our adoption of radial basis\nfunctions (RBF) and cross entropy with interleaving training for DeepDIVE to\nprovide a justified basis for convergence. Experiments on two public datasets\nshow that DeepDIVE disentangles the original input and yields forecast\naccuracies better than the original VAE and comparable to existing\nstate-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2501.08710v1", "date": "2025-01-15", "relevancy": 2.0783, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Interleaving%20Variational%20Encoding&body=Title%3A%20Disentangled%20Interleaving%20Variational%20Encoding%0AAuthor%3A%20Noelle%20Y.%20L.%20Wong%20and%20Eng%20Yeow%20Cheu%20and%20Zhonglin%20Chiam%0AAbstract%3A%20%20%20Conflicting%20objectives%20present%20a%20considerable%20challenge%20in%20interleaving%0Amulti-task%20learning%2C%20necessitating%20the%20need%20for%20meticulous%20design%20and%20balance%0Ato%20ensure%20effective%20learning%20of%20a%20representative%20latent%20data%20space%20across%20all%0Atasks%20without%20mutual%20negative%20impact.%20Drawing%20inspiration%20from%20the%20concept%20of%0Amarginal%20and%20conditional%20probability%20distributions%20in%20probability%20theory%2C%20we%0Adesign%20a%20principled%20and%20well-founded%20approach%20to%20disentangle%20the%20original%20input%0Ainto%20marginal%20and%20conditional%20probability%20distributions%20in%20the%20latent%20space%20of%0Aa%20variational%20autoencoder.%20Our%20proposed%20model%2C%20Deep%20Disentangled%20Interleaving%0AVariational%20Encoding%20%28DeepDIVE%29%20learns%20disentangled%20features%20from%20the%20original%0Ainput%20to%20form%20clusters%20in%20the%20embedding%20space%20and%20unifies%20these%20features%20via%0Athe%20cross-attention%20mechanism%20in%20the%20fusion%20stage.%20We%20theoretically%20prove%20that%0Acombining%20the%20objectives%20for%20reconstruction%20and%20forecasting%20fully%20captures%20the%0Alower%20bound%20and%20mathematically%20derive%20a%20loss%20function%20for%20disentanglement%20using%0ANa%5C%22ive%20Bayes.%20Under%20the%20assumption%20that%20the%20prior%20is%20a%20mixture%20of%20log-concave%0Adistributions%2C%20we%20also%20establish%20that%20the%20Kullback-Leibler%20divergence%20between%0Athe%20prior%20and%20the%20posterior%20is%20upper%20bounded%20by%20a%20function%20minimized%20by%20the%0Aminimizer%20of%20the%20cross%20entropy%20loss%2C%20informing%20our%20adoption%20of%20radial%20basis%0Afunctions%20%28RBF%29%20and%20cross%20entropy%20with%20interleaving%20training%20for%20DeepDIVE%20to%0Aprovide%20a%20justified%20basis%20for%20convergence.%20Experiments%20on%20two%20public%20datasets%0Ashow%20that%20DeepDIVE%20disentangles%20the%20original%20input%20and%20yields%20forecast%0Aaccuracies%20better%20than%20the%20original%20VAE%20and%20comparable%20to%20existing%0Astate-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Interleaving%2520Variational%2520Encoding%26entry.906535625%3DNoelle%2520Y.%2520L.%2520Wong%2520and%2520Eng%2520Yeow%2520Cheu%2520and%2520Zhonglin%2520Chiam%26entry.1292438233%3D%2520%2520Conflicting%2520objectives%2520present%2520a%2520considerable%2520challenge%2520in%2520interleaving%250Amulti-task%2520learning%252C%2520necessitating%2520the%2520need%2520for%2520meticulous%2520design%2520and%2520balance%250Ato%2520ensure%2520effective%2520learning%2520of%2520a%2520representative%2520latent%2520data%2520space%2520across%2520all%250Atasks%2520without%2520mutual%2520negative%2520impact.%2520Drawing%2520inspiration%2520from%2520the%2520concept%2520of%250Amarginal%2520and%2520conditional%2520probability%2520distributions%2520in%2520probability%2520theory%252C%2520we%250Adesign%2520a%2520principled%2520and%2520well-founded%2520approach%2520to%2520disentangle%2520the%2520original%2520input%250Ainto%2520marginal%2520and%2520conditional%2520probability%2520distributions%2520in%2520the%2520latent%2520space%2520of%250Aa%2520variational%2520autoencoder.%2520Our%2520proposed%2520model%252C%2520Deep%2520Disentangled%2520Interleaving%250AVariational%2520Encoding%2520%2528DeepDIVE%2529%2520learns%2520disentangled%2520features%2520from%2520the%2520original%250Ainput%2520to%2520form%2520clusters%2520in%2520the%2520embedding%2520space%2520and%2520unifies%2520these%2520features%2520via%250Athe%2520cross-attention%2520mechanism%2520in%2520the%2520fusion%2520stage.%2520We%2520theoretically%2520prove%2520that%250Acombining%2520the%2520objectives%2520for%2520reconstruction%2520and%2520forecasting%2520fully%2520captures%2520the%250Alower%2520bound%2520and%2520mathematically%2520derive%2520a%2520loss%2520function%2520for%2520disentanglement%2520using%250ANa%255C%2522ive%2520Bayes.%2520Under%2520the%2520assumption%2520that%2520the%2520prior%2520is%2520a%2520mixture%2520of%2520log-concave%250Adistributions%252C%2520we%2520also%2520establish%2520that%2520the%2520Kullback-Leibler%2520divergence%2520between%250Athe%2520prior%2520and%2520the%2520posterior%2520is%2520upper%2520bounded%2520by%2520a%2520function%2520minimized%2520by%2520the%250Aminimizer%2520of%2520the%2520cross%2520entropy%2520loss%252C%2520informing%2520our%2520adoption%2520of%2520radial%2520basis%250Afunctions%2520%2528RBF%2529%2520and%2520cross%2520entropy%2520with%2520interleaving%2520training%2520for%2520DeepDIVE%2520to%250Aprovide%2520a%2520justified%2520basis%2520for%2520convergence.%2520Experiments%2520on%2520two%2520public%2520datasets%250Ashow%2520that%2520DeepDIVE%2520disentangles%2520the%2520original%2520input%2520and%2520yields%2520forecast%250Aaccuracies%2520better%2520than%2520the%2520original%2520VAE%2520and%2520comparable%2520to%2520existing%250Astate-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Interleaving%20Variational%20Encoding&entry.906535625=Noelle%20Y.%20L.%20Wong%20and%20Eng%20Yeow%20Cheu%20and%20Zhonglin%20Chiam&entry.1292438233=%20%20Conflicting%20objectives%20present%20a%20considerable%20challenge%20in%20interleaving%0Amulti-task%20learning%2C%20necessitating%20the%20need%20for%20meticulous%20design%20and%20balance%0Ato%20ensure%20effective%20learning%20of%20a%20representative%20latent%20data%20space%20across%20all%0Atasks%20without%20mutual%20negative%20impact.%20Drawing%20inspiration%20from%20the%20concept%20of%0Amarginal%20and%20conditional%20probability%20distributions%20in%20probability%20theory%2C%20we%0Adesign%20a%20principled%20and%20well-founded%20approach%20to%20disentangle%20the%20original%20input%0Ainto%20marginal%20and%20conditional%20probability%20distributions%20in%20the%20latent%20space%20of%0Aa%20variational%20autoencoder.%20Our%20proposed%20model%2C%20Deep%20Disentangled%20Interleaving%0AVariational%20Encoding%20%28DeepDIVE%29%20learns%20disentangled%20features%20from%20the%20original%0Ainput%20to%20form%20clusters%20in%20the%20embedding%20space%20and%20unifies%20these%20features%20via%0Athe%20cross-attention%20mechanism%20in%20the%20fusion%20stage.%20We%20theoretically%20prove%20that%0Acombining%20the%20objectives%20for%20reconstruction%20and%20forecasting%20fully%20captures%20the%0Alower%20bound%20and%20mathematically%20derive%20a%20loss%20function%20for%20disentanglement%20using%0ANa%5C%22ive%20Bayes.%20Under%20the%20assumption%20that%20the%20prior%20is%20a%20mixture%20of%20log-concave%0Adistributions%2C%20we%20also%20establish%20that%20the%20Kullback-Leibler%20divergence%20between%0Athe%20prior%20and%20the%20posterior%20is%20upper%20bounded%20by%20a%20function%20minimized%20by%20the%0Aminimizer%20of%20the%20cross%20entropy%20loss%2C%20informing%20our%20adoption%20of%20radial%20basis%0Afunctions%20%28RBF%29%20and%20cross%20entropy%20with%20interleaving%20training%20for%20DeepDIVE%20to%0Aprovide%20a%20justified%20basis%20for%20convergence.%20Experiments%20on%20two%20public%20datasets%0Ashow%20that%20DeepDIVE%20disentangles%20the%20original%20input%20and%20yields%20forecast%0Aaccuracies%20better%20than%20the%20original%20VAE%20and%20comparable%20to%20existing%0Astate-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08710v1&entry.124074799=Read"},
{"title": "CGCOD: Class-Guided Camouflaged Object Detection", "author": "Chenxi Zhang and Qing Zhang and Jiayun Wu and Youwei Pang", "abstract": "  Camouflaged Object Detection (COD) aims to identify objects that blend\nseamlessly into their surroundings. The inherent visual complexity of\ncamouflaged objects, including their low contrast with the background, diverse\ntextures, and subtle appearance variations, often obscures semantic cues,\nmaking accurate segmentation highly challenging. Existing methods primarily\nrely on visual features, which are insufficient to handle the variability and\nintricacy of camouflaged objects, leading to unstable object perception and\nambiguous segmentation results. To tackle these limitations, we introduce a\nnovel task, class-guided camouflaged object detection (CGCOD), which extends\ntraditional COD task by incorporating object-specific class knowledge to\nenhance detection robustness and accuracy. To facilitate this task, we present\na new dataset, CamoClass, comprising real-world camouflaged objects with class\nannotations. Furthermore, we propose a multi-stage framework, CGNet, which\nincorporates a plug-and-play class prompt generator and a simple yet effective\nclass-guided detector. This establishes a new paradigm for COD, bridging the\ngap between contextual understanding and class-guided detection. Extensive\nexperimental results demonstrate the effectiveness of our flexible framework in\nimproving the performance of proposed and existing detectors by leveraging\nclass-level textual information.\n", "link": "http://arxiv.org/abs/2412.18977v2", "date": "2025-01-15", "relevancy": 2.0604, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5298}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5127}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CGCOD%3A%20Class-Guided%20Camouflaged%20Object%20Detection&body=Title%3A%20CGCOD%3A%20Class-Guided%20Camouflaged%20Object%20Detection%0AAuthor%3A%20Chenxi%20Zhang%20and%20Qing%20Zhang%20and%20Jiayun%20Wu%20and%20Youwei%20Pang%0AAbstract%3A%20%20%20Camouflaged%20Object%20Detection%20%28COD%29%20aims%20to%20identify%20objects%20that%20blend%0Aseamlessly%20into%20their%20surroundings.%20The%20inherent%20visual%20complexity%20of%0Acamouflaged%20objects%2C%20including%20their%20low%20contrast%20with%20the%20background%2C%20diverse%0Atextures%2C%20and%20subtle%20appearance%20variations%2C%20often%20obscures%20semantic%20cues%2C%0Amaking%20accurate%20segmentation%20highly%20challenging.%20Existing%20methods%20primarily%0Arely%20on%20visual%20features%2C%20which%20are%20insufficient%20to%20handle%20the%20variability%20and%0Aintricacy%20of%20camouflaged%20objects%2C%20leading%20to%20unstable%20object%20perception%20and%0Aambiguous%20segmentation%20results.%20To%20tackle%20these%20limitations%2C%20we%20introduce%20a%0Anovel%20task%2C%20class-guided%20camouflaged%20object%20detection%20%28CGCOD%29%2C%20which%20extends%0Atraditional%20COD%20task%20by%20incorporating%20object-specific%20class%20knowledge%20to%0Aenhance%20detection%20robustness%20and%20accuracy.%20To%20facilitate%20this%20task%2C%20we%20present%0Aa%20new%20dataset%2C%20CamoClass%2C%20comprising%20real-world%20camouflaged%20objects%20with%20class%0Aannotations.%20Furthermore%2C%20we%20propose%20a%20multi-stage%20framework%2C%20CGNet%2C%20which%0Aincorporates%20a%20plug-and-play%20class%20prompt%20generator%20and%20a%20simple%20yet%20effective%0Aclass-guided%20detector.%20This%20establishes%20a%20new%20paradigm%20for%20COD%2C%20bridging%20the%0Agap%20between%20contextual%20understanding%20and%20class-guided%20detection.%20Extensive%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20flexible%20framework%20in%0Aimproving%20the%20performance%20of%20proposed%20and%20existing%20detectors%20by%20leveraging%0Aclass-level%20textual%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCGCOD%253A%2520Class-Guided%2520Camouflaged%2520Object%2520Detection%26entry.906535625%3DChenxi%2520Zhang%2520and%2520Qing%2520Zhang%2520and%2520Jiayun%2520Wu%2520and%2520Youwei%2520Pang%26entry.1292438233%3D%2520%2520Camouflaged%2520Object%2520Detection%2520%2528COD%2529%2520aims%2520to%2520identify%2520objects%2520that%2520blend%250Aseamlessly%2520into%2520their%2520surroundings.%2520The%2520inherent%2520visual%2520complexity%2520of%250Acamouflaged%2520objects%252C%2520including%2520their%2520low%2520contrast%2520with%2520the%2520background%252C%2520diverse%250Atextures%252C%2520and%2520subtle%2520appearance%2520variations%252C%2520often%2520obscures%2520semantic%2520cues%252C%250Amaking%2520accurate%2520segmentation%2520highly%2520challenging.%2520Existing%2520methods%2520primarily%250Arely%2520on%2520visual%2520features%252C%2520which%2520are%2520insufficient%2520to%2520handle%2520the%2520variability%2520and%250Aintricacy%2520of%2520camouflaged%2520objects%252C%2520leading%2520to%2520unstable%2520object%2520perception%2520and%250Aambiguous%2520segmentation%2520results.%2520To%2520tackle%2520these%2520limitations%252C%2520we%2520introduce%2520a%250Anovel%2520task%252C%2520class-guided%2520camouflaged%2520object%2520detection%2520%2528CGCOD%2529%252C%2520which%2520extends%250Atraditional%2520COD%2520task%2520by%2520incorporating%2520object-specific%2520class%2520knowledge%2520to%250Aenhance%2520detection%2520robustness%2520and%2520accuracy.%2520To%2520facilitate%2520this%2520task%252C%2520we%2520present%250Aa%2520new%2520dataset%252C%2520CamoClass%252C%2520comprising%2520real-world%2520camouflaged%2520objects%2520with%2520class%250Aannotations.%2520Furthermore%252C%2520we%2520propose%2520a%2520multi-stage%2520framework%252C%2520CGNet%252C%2520which%250Aincorporates%2520a%2520plug-and-play%2520class%2520prompt%2520generator%2520and%2520a%2520simple%2520yet%2520effective%250Aclass-guided%2520detector.%2520This%2520establishes%2520a%2520new%2520paradigm%2520for%2520COD%252C%2520bridging%2520the%250Agap%2520between%2520contextual%2520understanding%2520and%2520class-guided%2520detection.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520flexible%2520framework%2520in%250Aimproving%2520the%2520performance%2520of%2520proposed%2520and%2520existing%2520detectors%2520by%2520leveraging%250Aclass-level%2520textual%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CGCOD%3A%20Class-Guided%20Camouflaged%20Object%20Detection&entry.906535625=Chenxi%20Zhang%20and%20Qing%20Zhang%20and%20Jiayun%20Wu%20and%20Youwei%20Pang&entry.1292438233=%20%20Camouflaged%20Object%20Detection%20%28COD%29%20aims%20to%20identify%20objects%20that%20blend%0Aseamlessly%20into%20their%20surroundings.%20The%20inherent%20visual%20complexity%20of%0Acamouflaged%20objects%2C%20including%20their%20low%20contrast%20with%20the%20background%2C%20diverse%0Atextures%2C%20and%20subtle%20appearance%20variations%2C%20often%20obscures%20semantic%20cues%2C%0Amaking%20accurate%20segmentation%20highly%20challenging.%20Existing%20methods%20primarily%0Arely%20on%20visual%20features%2C%20which%20are%20insufficient%20to%20handle%20the%20variability%20and%0Aintricacy%20of%20camouflaged%20objects%2C%20leading%20to%20unstable%20object%20perception%20and%0Aambiguous%20segmentation%20results.%20To%20tackle%20these%20limitations%2C%20we%20introduce%20a%0Anovel%20task%2C%20class-guided%20camouflaged%20object%20detection%20%28CGCOD%29%2C%20which%20extends%0Atraditional%20COD%20task%20by%20incorporating%20object-specific%20class%20knowledge%20to%0Aenhance%20detection%20robustness%20and%20accuracy.%20To%20facilitate%20this%20task%2C%20we%20present%0Aa%20new%20dataset%2C%20CamoClass%2C%20comprising%20real-world%20camouflaged%20objects%20with%20class%0Aannotations.%20Furthermore%2C%20we%20propose%20a%20multi-stage%20framework%2C%20CGNet%2C%20which%0Aincorporates%20a%20plug-and-play%20class%20prompt%20generator%20and%20a%20simple%20yet%20effective%0Aclass-guided%20detector.%20This%20establishes%20a%20new%20paradigm%20for%20COD%2C%20bridging%20the%0Agap%20between%20contextual%20understanding%20and%20class-guided%20detection.%20Extensive%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20flexible%20framework%20in%0Aimproving%20the%20performance%20of%20proposed%20and%20existing%20detectors%20by%20leveraging%0Aclass-level%20textual%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18977v2&entry.124074799=Read"},
{"title": "TextSleuth: Towards Explainable Tampered Text Detection", "author": "Chenfan Qu and Jian Liu and Haoxing Chen and Baihan Yu and Jingjing Liu and Weiqiang Wang and Lianwen Jin", "abstract": "  Recently, tampered text detection has attracted increasing attention due to\nits essential role in information security. Although existing methods can\ndetect the tampered text region, the interpretation of such detection remains\nunclear, making the prediction unreliable. To address this problem, we propose\nto explain the basis of tampered text detection with natural language via large\nmultimodal models. To fill the data gap for this task, we propose a\nlarge-scale, comprehensive dataset, ETTD, which contains both pixel-level\nannotations for tampered text region and natural language annotations\ndescribing the anomaly of the tampered text. Multiple methods are employed to\nimprove the quality of the proposed data. For example, elaborate queries are\nintroduced to generate high-quality anomaly descriptions with GPT4o. A fused\nmask prompt is proposed to reduce confusion when querying GPT4o to generate\nanomaly descriptions. To automatically filter out low-quality annotations, we\nalso propose to prompt GPT4o to recognize tampered texts before describing the\nanomaly, and to filter out the responses with low OCR accuracy. To further\nimprove explainable tampered text detection, we propose a simple yet effective\nmodel called TextSleuth, which achieves improved fine-grained perception and\ncross-domain generalization by focusing on the suspected region, with a\ntwo-stage analysis paradigm and an auxiliary grounding prompt. Extensive\nexperiments on both the ETTD dataset and the public dataset have verified the\neffectiveness of the proposed methods. In-depth analysis is also provided to\ninspire further research. Our dataset and code will be open-source.\n", "link": "http://arxiv.org/abs/2412.14816v3", "date": "2025-01-15", "relevancy": 2.0526, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5424}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextSleuth%3A%20Towards%20Explainable%20Tampered%20Text%20Detection&body=Title%3A%20TextSleuth%3A%20Towards%20Explainable%20Tampered%20Text%20Detection%0AAuthor%3A%20Chenfan%20Qu%20and%20Jian%20Liu%20and%20Haoxing%20Chen%20and%20Baihan%20Yu%20and%20Jingjing%20Liu%20and%20Weiqiang%20Wang%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Recently%2C%20tampered%20text%20detection%20has%20attracted%20increasing%20attention%20due%20to%0Aits%20essential%20role%20in%20information%20security.%20Although%20existing%20methods%20can%0Adetect%20the%20tampered%20text%20region%2C%20the%20interpretation%20of%20such%20detection%20remains%0Aunclear%2C%20making%20the%20prediction%20unreliable.%20To%20address%20this%20problem%2C%20we%20propose%0Ato%20explain%20the%20basis%20of%20tampered%20text%20detection%20with%20natural%20language%20via%20large%0Amultimodal%20models.%20To%20fill%20the%20data%20gap%20for%20this%20task%2C%20we%20propose%20a%0Alarge-scale%2C%20comprehensive%20dataset%2C%20ETTD%2C%20which%20contains%20both%20pixel-level%0Aannotations%20for%20tampered%20text%20region%20and%20natural%20language%20annotations%0Adescribing%20the%20anomaly%20of%20the%20tampered%20text.%20Multiple%20methods%20are%20employed%20to%0Aimprove%20the%20quality%20of%20the%20proposed%20data.%20For%20example%2C%20elaborate%20queries%20are%0Aintroduced%20to%20generate%20high-quality%20anomaly%20descriptions%20with%20GPT4o.%20A%20fused%0Amask%20prompt%20is%20proposed%20to%20reduce%20confusion%20when%20querying%20GPT4o%20to%20generate%0Aanomaly%20descriptions.%20To%20automatically%20filter%20out%20low-quality%20annotations%2C%20we%0Aalso%20propose%20to%20prompt%20GPT4o%20to%20recognize%20tampered%20texts%20before%20describing%20the%0Aanomaly%2C%20and%20to%20filter%20out%20the%20responses%20with%20low%20OCR%20accuracy.%20To%20further%0Aimprove%20explainable%20tampered%20text%20detection%2C%20we%20propose%20a%20simple%20yet%20effective%0Amodel%20called%20TextSleuth%2C%20which%20achieves%20improved%20fine-grained%20perception%20and%0Across-domain%20generalization%20by%20focusing%20on%20the%20suspected%20region%2C%20with%20a%0Atwo-stage%20analysis%20paradigm%20and%20an%20auxiliary%20grounding%20prompt.%20Extensive%0Aexperiments%20on%20both%20the%20ETTD%20dataset%20and%20the%20public%20dataset%20have%20verified%20the%0Aeffectiveness%20of%20the%20proposed%20methods.%20In-depth%20analysis%20is%20also%20provided%20to%0Ainspire%20further%20research.%20Our%20dataset%20and%20code%20will%20be%20open-source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14816v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextSleuth%253A%2520Towards%2520Explainable%2520Tampered%2520Text%2520Detection%26entry.906535625%3DChenfan%2520Qu%2520and%2520Jian%2520Liu%2520and%2520Haoxing%2520Chen%2520and%2520Baihan%2520Yu%2520and%2520Jingjing%2520Liu%2520and%2520Weiqiang%2520Wang%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Recently%252C%2520tampered%2520text%2520detection%2520has%2520attracted%2520increasing%2520attention%2520due%2520to%250Aits%2520essential%2520role%2520in%2520information%2520security.%2520Although%2520existing%2520methods%2520can%250Adetect%2520the%2520tampered%2520text%2520region%252C%2520the%2520interpretation%2520of%2520such%2520detection%2520remains%250Aunclear%252C%2520making%2520the%2520prediction%2520unreliable.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%250Ato%2520explain%2520the%2520basis%2520of%2520tampered%2520text%2520detection%2520with%2520natural%2520language%2520via%2520large%250Amultimodal%2520models.%2520To%2520fill%2520the%2520data%2520gap%2520for%2520this%2520task%252C%2520we%2520propose%2520a%250Alarge-scale%252C%2520comprehensive%2520dataset%252C%2520ETTD%252C%2520which%2520contains%2520both%2520pixel-level%250Aannotations%2520for%2520tampered%2520text%2520region%2520and%2520natural%2520language%2520annotations%250Adescribing%2520the%2520anomaly%2520of%2520the%2520tampered%2520text.%2520Multiple%2520methods%2520are%2520employed%2520to%250Aimprove%2520the%2520quality%2520of%2520the%2520proposed%2520data.%2520For%2520example%252C%2520elaborate%2520queries%2520are%250Aintroduced%2520to%2520generate%2520high-quality%2520anomaly%2520descriptions%2520with%2520GPT4o.%2520A%2520fused%250Amask%2520prompt%2520is%2520proposed%2520to%2520reduce%2520confusion%2520when%2520querying%2520GPT4o%2520to%2520generate%250Aanomaly%2520descriptions.%2520To%2520automatically%2520filter%2520out%2520low-quality%2520annotations%252C%2520we%250Aalso%2520propose%2520to%2520prompt%2520GPT4o%2520to%2520recognize%2520tampered%2520texts%2520before%2520describing%2520the%250Aanomaly%252C%2520and%2520to%2520filter%2520out%2520the%2520responses%2520with%2520low%2520OCR%2520accuracy.%2520To%2520further%250Aimprove%2520explainable%2520tampered%2520text%2520detection%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250Amodel%2520called%2520TextSleuth%252C%2520which%2520achieves%2520improved%2520fine-grained%2520perception%2520and%250Across-domain%2520generalization%2520by%2520focusing%2520on%2520the%2520suspected%2520region%252C%2520with%2520a%250Atwo-stage%2520analysis%2520paradigm%2520and%2520an%2520auxiliary%2520grounding%2520prompt.%2520Extensive%250Aexperiments%2520on%2520both%2520the%2520ETTD%2520dataset%2520and%2520the%2520public%2520dataset%2520have%2520verified%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520methods.%2520In-depth%2520analysis%2520is%2520also%2520provided%2520to%250Ainspire%2520further%2520research.%2520Our%2520dataset%2520and%2520code%2520will%2520be%2520open-source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14816v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextSleuth%3A%20Towards%20Explainable%20Tampered%20Text%20Detection&entry.906535625=Chenfan%20Qu%20and%20Jian%20Liu%20and%20Haoxing%20Chen%20and%20Baihan%20Yu%20and%20Jingjing%20Liu%20and%20Weiqiang%20Wang%20and%20Lianwen%20Jin&entry.1292438233=%20%20Recently%2C%20tampered%20text%20detection%20has%20attracted%20increasing%20attention%20due%20to%0Aits%20essential%20role%20in%20information%20security.%20Although%20existing%20methods%20can%0Adetect%20the%20tampered%20text%20region%2C%20the%20interpretation%20of%20such%20detection%20remains%0Aunclear%2C%20making%20the%20prediction%20unreliable.%20To%20address%20this%20problem%2C%20we%20propose%0Ato%20explain%20the%20basis%20of%20tampered%20text%20detection%20with%20natural%20language%20via%20large%0Amultimodal%20models.%20To%20fill%20the%20data%20gap%20for%20this%20task%2C%20we%20propose%20a%0Alarge-scale%2C%20comprehensive%20dataset%2C%20ETTD%2C%20which%20contains%20both%20pixel-level%0Aannotations%20for%20tampered%20text%20region%20and%20natural%20language%20annotations%0Adescribing%20the%20anomaly%20of%20the%20tampered%20text.%20Multiple%20methods%20are%20employed%20to%0Aimprove%20the%20quality%20of%20the%20proposed%20data.%20For%20example%2C%20elaborate%20queries%20are%0Aintroduced%20to%20generate%20high-quality%20anomaly%20descriptions%20with%20GPT4o.%20A%20fused%0Amask%20prompt%20is%20proposed%20to%20reduce%20confusion%20when%20querying%20GPT4o%20to%20generate%0Aanomaly%20descriptions.%20To%20automatically%20filter%20out%20low-quality%20annotations%2C%20we%0Aalso%20propose%20to%20prompt%20GPT4o%20to%20recognize%20tampered%20texts%20before%20describing%20the%0Aanomaly%2C%20and%20to%20filter%20out%20the%20responses%20with%20low%20OCR%20accuracy.%20To%20further%0Aimprove%20explainable%20tampered%20text%20detection%2C%20we%20propose%20a%20simple%20yet%20effective%0Amodel%20called%20TextSleuth%2C%20which%20achieves%20improved%20fine-grained%20perception%20and%0Across-domain%20generalization%20by%20focusing%20on%20the%20suspected%20region%2C%20with%20a%0Atwo-stage%20analysis%20paradigm%20and%20an%20auxiliary%20grounding%20prompt.%20Extensive%0Aexperiments%20on%20both%20the%20ETTD%20dataset%20and%20the%20public%20dataset%20have%20verified%20the%0Aeffectiveness%20of%20the%20proposed%20methods.%20In-depth%20analysis%20is%20also%20provided%20to%0Ainspire%20further%20research.%20Our%20dataset%20and%20code%20will%20be%20open-source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14816v3&entry.124074799=Read"},
{"title": "Taming the Long Tail in Human Mobility Prediction", "author": "Xiaohang Xu and Renhe Jiang and Chuang Yang and Zipei Fan and Kaoru Sezaki", "abstract": "  With the popularity of location-based services, human mobility prediction\nplays a key role in enhancing personalized navigation, optimizing\nrecommendation systems, and facilitating urban mobility and planning. This\ninvolves predicting a user's next POI (point-of-interest) visit using their\npast visit history. However, the uneven distribution of visitations over time\nand space, namely the long-tail problem in spatial distribution, makes it\ndifficult for AI models to predict those POIs that are less visited by humans.\nIn light of this issue, we propose the Long-Tail Adjusted Next POI Prediction\n(LoTNext) framework for mobility prediction, combining a Long-Tailed Graph\nAdjustment module to reduce the impact of the long-tailed nodes in the user-POI\ninteraction graph and a novel Long-Tailed Loss Adjustment module to adjust loss\nby logit score and sample weight adjustment strategy. Also, we employ the\nauxiliary prediction task to enhance generalization and accuracy. Our\nexperiments with two real-world trajectory datasets demonstrate that LoTNext\nsignificantly surpasses existing state-of-the-art works.\n", "link": "http://arxiv.org/abs/2410.14970v4", "date": "2025-01-15", "relevancy": 2.0412, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5314}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5065}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20the%20Long%20Tail%20in%20Human%20Mobility%20Prediction&body=Title%3A%20Taming%20the%20Long%20Tail%20in%20Human%20Mobility%20Prediction%0AAuthor%3A%20Xiaohang%20Xu%20and%20Renhe%20Jiang%20and%20Chuang%20Yang%20and%20Zipei%20Fan%20and%20Kaoru%20Sezaki%0AAbstract%3A%20%20%20With%20the%20popularity%20of%20location-based%20services%2C%20human%20mobility%20prediction%0Aplays%20a%20key%20role%20in%20enhancing%20personalized%20navigation%2C%20optimizing%0Arecommendation%20systems%2C%20and%20facilitating%20urban%20mobility%20and%20planning.%20This%0Ainvolves%20predicting%20a%20user%27s%20next%20POI%20%28point-of-interest%29%20visit%20using%20their%0Apast%20visit%20history.%20However%2C%20the%20uneven%20distribution%20of%20visitations%20over%20time%0Aand%20space%2C%20namely%20the%20long-tail%20problem%20in%20spatial%20distribution%2C%20makes%20it%0Adifficult%20for%20AI%20models%20to%20predict%20those%20POIs%20that%20are%20less%20visited%20by%20humans.%0AIn%20light%20of%20this%20issue%2C%20we%20propose%20the%20Long-Tail%20Adjusted%20Next%20POI%20Prediction%0A%28LoTNext%29%20framework%20for%20mobility%20prediction%2C%20combining%20a%20Long-Tailed%20Graph%0AAdjustment%20module%20to%20reduce%20the%20impact%20of%20the%20long-tailed%20nodes%20in%20the%20user-POI%0Ainteraction%20graph%20and%20a%20novel%20Long-Tailed%20Loss%20Adjustment%20module%20to%20adjust%20loss%0Aby%20logit%20score%20and%20sample%20weight%20adjustment%20strategy.%20Also%2C%20we%20employ%20the%0Aauxiliary%20prediction%20task%20to%20enhance%20generalization%20and%20accuracy.%20Our%0Aexperiments%20with%20two%20real-world%20trajectory%20datasets%20demonstrate%20that%20LoTNext%0Asignificantly%20surpasses%20existing%20state-of-the-art%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14970v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520the%2520Long%2520Tail%2520in%2520Human%2520Mobility%2520Prediction%26entry.906535625%3DXiaohang%2520Xu%2520and%2520Renhe%2520Jiang%2520and%2520Chuang%2520Yang%2520and%2520Zipei%2520Fan%2520and%2520Kaoru%2520Sezaki%26entry.1292438233%3D%2520%2520With%2520the%2520popularity%2520of%2520location-based%2520services%252C%2520human%2520mobility%2520prediction%250Aplays%2520a%2520key%2520role%2520in%2520enhancing%2520personalized%2520navigation%252C%2520optimizing%250Arecommendation%2520systems%252C%2520and%2520facilitating%2520urban%2520mobility%2520and%2520planning.%2520This%250Ainvolves%2520predicting%2520a%2520user%2527s%2520next%2520POI%2520%2528point-of-interest%2529%2520visit%2520using%2520their%250Apast%2520visit%2520history.%2520However%252C%2520the%2520uneven%2520distribution%2520of%2520visitations%2520over%2520time%250Aand%2520space%252C%2520namely%2520the%2520long-tail%2520problem%2520in%2520spatial%2520distribution%252C%2520makes%2520it%250Adifficult%2520for%2520AI%2520models%2520to%2520predict%2520those%2520POIs%2520that%2520are%2520less%2520visited%2520by%2520humans.%250AIn%2520light%2520of%2520this%2520issue%252C%2520we%2520propose%2520the%2520Long-Tail%2520Adjusted%2520Next%2520POI%2520Prediction%250A%2528LoTNext%2529%2520framework%2520for%2520mobility%2520prediction%252C%2520combining%2520a%2520Long-Tailed%2520Graph%250AAdjustment%2520module%2520to%2520reduce%2520the%2520impact%2520of%2520the%2520long-tailed%2520nodes%2520in%2520the%2520user-POI%250Ainteraction%2520graph%2520and%2520a%2520novel%2520Long-Tailed%2520Loss%2520Adjustment%2520module%2520to%2520adjust%2520loss%250Aby%2520logit%2520score%2520and%2520sample%2520weight%2520adjustment%2520strategy.%2520Also%252C%2520we%2520employ%2520the%250Aauxiliary%2520prediction%2520task%2520to%2520enhance%2520generalization%2520and%2520accuracy.%2520Our%250Aexperiments%2520with%2520two%2520real-world%2520trajectory%2520datasets%2520demonstrate%2520that%2520LoTNext%250Asignificantly%2520surpasses%2520existing%2520state-of-the-art%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14970v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20the%20Long%20Tail%20in%20Human%20Mobility%20Prediction&entry.906535625=Xiaohang%20Xu%20and%20Renhe%20Jiang%20and%20Chuang%20Yang%20and%20Zipei%20Fan%20and%20Kaoru%20Sezaki&entry.1292438233=%20%20With%20the%20popularity%20of%20location-based%20services%2C%20human%20mobility%20prediction%0Aplays%20a%20key%20role%20in%20enhancing%20personalized%20navigation%2C%20optimizing%0Arecommendation%20systems%2C%20and%20facilitating%20urban%20mobility%20and%20planning.%20This%0Ainvolves%20predicting%20a%20user%27s%20next%20POI%20%28point-of-interest%29%20visit%20using%20their%0Apast%20visit%20history.%20However%2C%20the%20uneven%20distribution%20of%20visitations%20over%20time%0Aand%20space%2C%20namely%20the%20long-tail%20problem%20in%20spatial%20distribution%2C%20makes%20it%0Adifficult%20for%20AI%20models%20to%20predict%20those%20POIs%20that%20are%20less%20visited%20by%20humans.%0AIn%20light%20of%20this%20issue%2C%20we%20propose%20the%20Long-Tail%20Adjusted%20Next%20POI%20Prediction%0A%28LoTNext%29%20framework%20for%20mobility%20prediction%2C%20combining%20a%20Long-Tailed%20Graph%0AAdjustment%20module%20to%20reduce%20the%20impact%20of%20the%20long-tailed%20nodes%20in%20the%20user-POI%0Ainteraction%20graph%20and%20a%20novel%20Long-Tailed%20Loss%20Adjustment%20module%20to%20adjust%20loss%0Aby%20logit%20score%20and%20sample%20weight%20adjustment%20strategy.%20Also%2C%20we%20employ%20the%0Aauxiliary%20prediction%20task%20to%20enhance%20generalization%20and%20accuracy.%20Our%0Aexperiments%20with%20two%20real-world%20trajectory%20datasets%20demonstrate%20that%20LoTNext%0Asignificantly%20surpasses%20existing%20state-of-the-art%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14970v4&entry.124074799=Read"},
{"title": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning", "author": "Carlos G\u00fcemes-Palau and Miquel Ferriol-Galm\u00e9s and Jordi Paillisse-Vilanova and Albert L\u00f3pez-Bresc\u00f3 and Pere Barlet-Ros and Albert Cabellos-Aparicio", "abstract": "  Network simulation is pivotal in network modeling, assisting with tasks\nranging from capacity planning to performance estimation. Traditional\napproaches such as Discrete Event Simulation (DES) face limitations in terms of\ncomputational cost and accuracy. This paper introduces RouteNet-Gauss, a novel\nintegration of a testbed network with a Machine Learning (ML) model to address\nthese challenges. By using the testbed as a hardware accelerator,\nRouteNet-Gauss generates training datasets rapidly and simulates network\nscenarios with high fidelity to real-world conditions. Experimental results\nshow that RouteNet-Gauss significantly reduces prediction errors by up to 95%\nand achieves a 488x speedup in inference time compared to state-of-the-art\nDES-based methods. RouteNet-Gauss's modular architecture is dynamically\nconstructed based on the specific characteristics of the network scenario, such\nas topology and routing. This enables it to understand and generalize to\ndifferent network configurations beyond those seen during training, including\nnetworks up to 10x larger. Additionally, it supports Temporal Aggregated\nPerformance Estimation (TAPE), providing configurable temporal granularity and\nmaintaining high accuracy in flow performance metrics. This approach shows\npromise in improving both simulation efficiency and accuracy, offering a\nvaluable tool for network operators.\n", "link": "http://arxiv.org/abs/2501.08848v1", "date": "2025-01-15", "relevancy": 2.036, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5171}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5122}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RouteNet-Gauss%3A%20Hardware-Enhanced%20Network%20Modeling%20with%20Machine%20Learning&body=Title%3A%20RouteNet-Gauss%3A%20Hardware-Enhanced%20Network%20Modeling%20with%20Machine%20Learning%0AAuthor%3A%20Carlos%20G%C3%BCemes-Palau%20and%20Miquel%20Ferriol-Galm%C3%A9s%20and%20Jordi%20Paillisse-Vilanova%20and%20Albert%20L%C3%B3pez-Bresc%C3%B3%20and%20Pere%20Barlet-Ros%20and%20Albert%20Cabellos-Aparicio%0AAbstract%3A%20%20%20Network%20simulation%20is%20pivotal%20in%20network%20modeling%2C%20assisting%20with%20tasks%0Aranging%20from%20capacity%20planning%20to%20performance%20estimation.%20Traditional%0Aapproaches%20such%20as%20Discrete%20Event%20Simulation%20%28DES%29%20face%20limitations%20in%20terms%20of%0Acomputational%20cost%20and%20accuracy.%20This%20paper%20introduces%20RouteNet-Gauss%2C%20a%20novel%0Aintegration%20of%20a%20testbed%20network%20with%20a%20Machine%20Learning%20%28ML%29%20model%20to%20address%0Athese%20challenges.%20By%20using%20the%20testbed%20as%20a%20hardware%20accelerator%2C%0ARouteNet-Gauss%20generates%20training%20datasets%20rapidly%20and%20simulates%20network%0Ascenarios%20with%20high%20fidelity%20to%20real-world%20conditions.%20Experimental%20results%0Ashow%20that%20RouteNet-Gauss%20significantly%20reduces%20prediction%20errors%20by%20up%20to%2095%25%0Aand%20achieves%20a%20488x%20speedup%20in%20inference%20time%20compared%20to%20state-of-the-art%0ADES-based%20methods.%20RouteNet-Gauss%27s%20modular%20architecture%20is%20dynamically%0Aconstructed%20based%20on%20the%20specific%20characteristics%20of%20the%20network%20scenario%2C%20such%0Aas%20topology%20and%20routing.%20This%20enables%20it%20to%20understand%20and%20generalize%20to%0Adifferent%20network%20configurations%20beyond%20those%20seen%20during%20training%2C%20including%0Anetworks%20up%20to%2010x%20larger.%20Additionally%2C%20it%20supports%20Temporal%20Aggregated%0APerformance%20Estimation%20%28TAPE%29%2C%20providing%20configurable%20temporal%20granularity%20and%0Amaintaining%20high%20accuracy%20in%20flow%20performance%20metrics.%20This%20approach%20shows%0Apromise%20in%20improving%20both%20simulation%20efficiency%20and%20accuracy%2C%20offering%20a%0Avaluable%20tool%20for%20network%20operators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRouteNet-Gauss%253A%2520Hardware-Enhanced%2520Network%2520Modeling%2520with%2520Machine%2520Learning%26entry.906535625%3DCarlos%2520G%25C3%25BCemes-Palau%2520and%2520Miquel%2520Ferriol-Galm%25C3%25A9s%2520and%2520Jordi%2520Paillisse-Vilanova%2520and%2520Albert%2520L%25C3%25B3pez-Bresc%25C3%25B3%2520and%2520Pere%2520Barlet-Ros%2520and%2520Albert%2520Cabellos-Aparicio%26entry.1292438233%3D%2520%2520Network%2520simulation%2520is%2520pivotal%2520in%2520network%2520modeling%252C%2520assisting%2520with%2520tasks%250Aranging%2520from%2520capacity%2520planning%2520to%2520performance%2520estimation.%2520Traditional%250Aapproaches%2520such%2520as%2520Discrete%2520Event%2520Simulation%2520%2528DES%2529%2520face%2520limitations%2520in%2520terms%2520of%250Acomputational%2520cost%2520and%2520accuracy.%2520This%2520paper%2520introduces%2520RouteNet-Gauss%252C%2520a%2520novel%250Aintegration%2520of%2520a%2520testbed%2520network%2520with%2520a%2520Machine%2520Learning%2520%2528ML%2529%2520model%2520to%2520address%250Athese%2520challenges.%2520By%2520using%2520the%2520testbed%2520as%2520a%2520hardware%2520accelerator%252C%250ARouteNet-Gauss%2520generates%2520training%2520datasets%2520rapidly%2520and%2520simulates%2520network%250Ascenarios%2520with%2520high%2520fidelity%2520to%2520real-world%2520conditions.%2520Experimental%2520results%250Ashow%2520that%2520RouteNet-Gauss%2520significantly%2520reduces%2520prediction%2520errors%2520by%2520up%2520to%252095%2525%250Aand%2520achieves%2520a%2520488x%2520speedup%2520in%2520inference%2520time%2520compared%2520to%2520state-of-the-art%250ADES-based%2520methods.%2520RouteNet-Gauss%2527s%2520modular%2520architecture%2520is%2520dynamically%250Aconstructed%2520based%2520on%2520the%2520specific%2520characteristics%2520of%2520the%2520network%2520scenario%252C%2520such%250Aas%2520topology%2520and%2520routing.%2520This%2520enables%2520it%2520to%2520understand%2520and%2520generalize%2520to%250Adifferent%2520network%2520configurations%2520beyond%2520those%2520seen%2520during%2520training%252C%2520including%250Anetworks%2520up%2520to%252010x%2520larger.%2520Additionally%252C%2520it%2520supports%2520Temporal%2520Aggregated%250APerformance%2520Estimation%2520%2528TAPE%2529%252C%2520providing%2520configurable%2520temporal%2520granularity%2520and%250Amaintaining%2520high%2520accuracy%2520in%2520flow%2520performance%2520metrics.%2520This%2520approach%2520shows%250Apromise%2520in%2520improving%2520both%2520simulation%2520efficiency%2520and%2520accuracy%252C%2520offering%2520a%250Avaluable%2520tool%2520for%2520network%2520operators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RouteNet-Gauss%3A%20Hardware-Enhanced%20Network%20Modeling%20with%20Machine%20Learning&entry.906535625=Carlos%20G%C3%BCemes-Palau%20and%20Miquel%20Ferriol-Galm%C3%A9s%20and%20Jordi%20Paillisse-Vilanova%20and%20Albert%20L%C3%B3pez-Bresc%C3%B3%20and%20Pere%20Barlet-Ros%20and%20Albert%20Cabellos-Aparicio&entry.1292438233=%20%20Network%20simulation%20is%20pivotal%20in%20network%20modeling%2C%20assisting%20with%20tasks%0Aranging%20from%20capacity%20planning%20to%20performance%20estimation.%20Traditional%0Aapproaches%20such%20as%20Discrete%20Event%20Simulation%20%28DES%29%20face%20limitations%20in%20terms%20of%0Acomputational%20cost%20and%20accuracy.%20This%20paper%20introduces%20RouteNet-Gauss%2C%20a%20novel%0Aintegration%20of%20a%20testbed%20network%20with%20a%20Machine%20Learning%20%28ML%29%20model%20to%20address%0Athese%20challenges.%20By%20using%20the%20testbed%20as%20a%20hardware%20accelerator%2C%0ARouteNet-Gauss%20generates%20training%20datasets%20rapidly%20and%20simulates%20network%0Ascenarios%20with%20high%20fidelity%20to%20real-world%20conditions.%20Experimental%20results%0Ashow%20that%20RouteNet-Gauss%20significantly%20reduces%20prediction%20errors%20by%20up%20to%2095%25%0Aand%20achieves%20a%20488x%20speedup%20in%20inference%20time%20compared%20to%20state-of-the-art%0ADES-based%20methods.%20RouteNet-Gauss%27s%20modular%20architecture%20is%20dynamically%0Aconstructed%20based%20on%20the%20specific%20characteristics%20of%20the%20network%20scenario%2C%20such%0Aas%20topology%20and%20routing.%20This%20enables%20it%20to%20understand%20and%20generalize%20to%0Adifferent%20network%20configurations%20beyond%20those%20seen%20during%20training%2C%20including%0Anetworks%20up%20to%2010x%20larger.%20Additionally%2C%20it%20supports%20Temporal%20Aggregated%0APerformance%20Estimation%20%28TAPE%29%2C%20providing%20configurable%20temporal%20granularity%20and%0Amaintaining%20high%20accuracy%20in%20flow%20performance%20metrics.%20This%20approach%20shows%0Apromise%20in%20improving%20both%20simulation%20efficiency%20and%20accuracy%2C%20offering%20a%0Avaluable%20tool%20for%20network%20operators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08848v1&entry.124074799=Read"},
{"title": "VECT-GAN: A variationally encoded generative model for overcoming data\n  scarcity in pharmaceutical science", "author": "Youssef Abdalla and Marrisa Taub and Eleanor Hilton and Priya Akkaraju and Alexander Milanovic and Mine Orlu and Abdul W. Basit and Michael T Cook and Tapabrata Chakraborty and David Shorthouse", "abstract": "  Data scarcity in pharmaceutical research has led to reliance on\nlabour-intensive trial and error approaches for development rather than data\ndriven methods. While Machine Learning offers a solution, existing datasets are\noften small and noisy, limiting their utility. To address this, we developed a\nVariationally Encoded Conditional Tabular Generative Adversarial Network (VECT\nGAN), a novel generative model specifically designed for augmenting small,\nnoisy datasets. We introduce a pipeline where data is augmented before\nregression model development and demonstrate that this consistently and\nsignificantly improves performance over other state of the art tabular\ngenerative models. We apply this pipeline across six pharmaceutical datasets,\nand highlight its real-world applicability by developing novel polymers with\nmedically desirable mucoadhesive properties, which we made and experimentally\ncharacterised. Additionally, we pre-train the model on the ChEMBL database of\ndrug-like molecules, leveraging knowledge distillation to enhance its\ngeneralisability, making it readily available for use on pharmaceutical\ndatasets containing small molecules, which is an extremely common\npharmaceutical task. We demonstrate the power of synthetic data for\nregularising small tabular datasets, highlighting its potential to become\nstandard practice in pharmaceutical model development, and make our method,\nincluding VECT GAN pretrained on ChEMBL available as a pip package.\n", "link": "http://arxiv.org/abs/2501.08995v1", "date": "2025-01-15", "relevancy": 2.0327, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5238}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5066}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VECT-GAN%3A%20A%20variationally%20encoded%20generative%20model%20for%20overcoming%20data%0A%20%20scarcity%20in%20pharmaceutical%20science&body=Title%3A%20VECT-GAN%3A%20A%20variationally%20encoded%20generative%20model%20for%20overcoming%20data%0A%20%20scarcity%20in%20pharmaceutical%20science%0AAuthor%3A%20Youssef%20Abdalla%20and%20Marrisa%20Taub%20and%20Eleanor%20Hilton%20and%20Priya%20Akkaraju%20and%20Alexander%20Milanovic%20and%20Mine%20Orlu%20and%20Abdul%20W.%20Basit%20and%20Michael%20T%20Cook%20and%20Tapabrata%20Chakraborty%20and%20David%20Shorthouse%0AAbstract%3A%20%20%20Data%20scarcity%20in%20pharmaceutical%20research%20has%20led%20to%20reliance%20on%0Alabour-intensive%20trial%20and%20error%20approaches%20for%20development%20rather%20than%20data%0Adriven%20methods.%20While%20Machine%20Learning%20offers%20a%20solution%2C%20existing%20datasets%20are%0Aoften%20small%20and%20noisy%2C%20limiting%20their%20utility.%20To%20address%20this%2C%20we%20developed%20a%0AVariationally%20Encoded%20Conditional%20Tabular%20Generative%20Adversarial%20Network%20%28VECT%0AGAN%29%2C%20a%20novel%20generative%20model%20specifically%20designed%20for%20augmenting%20small%2C%0Anoisy%20datasets.%20We%20introduce%20a%20pipeline%20where%20data%20is%20augmented%20before%0Aregression%20model%20development%20and%20demonstrate%20that%20this%20consistently%20and%0Asignificantly%20improves%20performance%20over%20other%20state%20of%20the%20art%20tabular%0Agenerative%20models.%20We%20apply%20this%20pipeline%20across%20six%20pharmaceutical%20datasets%2C%0Aand%20highlight%20its%20real-world%20applicability%20by%20developing%20novel%20polymers%20with%0Amedically%20desirable%20mucoadhesive%20properties%2C%20which%20we%20made%20and%20experimentally%0Acharacterised.%20Additionally%2C%20we%20pre-train%20the%20model%20on%20the%20ChEMBL%20database%20of%0Adrug-like%20molecules%2C%20leveraging%20knowledge%20distillation%20to%20enhance%20its%0Ageneralisability%2C%20making%20it%20readily%20available%20for%20use%20on%20pharmaceutical%0Adatasets%20containing%20small%20molecules%2C%20which%20is%20an%20extremely%20common%0Apharmaceutical%20task.%20We%20demonstrate%20the%20power%20of%20synthetic%20data%20for%0Aregularising%20small%20tabular%20datasets%2C%20highlighting%20its%20potential%20to%20become%0Astandard%20practice%20in%20pharmaceutical%20model%20development%2C%20and%20make%20our%20method%2C%0Aincluding%20VECT%20GAN%20pretrained%20on%20ChEMBL%20available%20as%20a%20pip%20package.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVECT-GAN%253A%2520A%2520variationally%2520encoded%2520generative%2520model%2520for%2520overcoming%2520data%250A%2520%2520scarcity%2520in%2520pharmaceutical%2520science%26entry.906535625%3DYoussef%2520Abdalla%2520and%2520Marrisa%2520Taub%2520and%2520Eleanor%2520Hilton%2520and%2520Priya%2520Akkaraju%2520and%2520Alexander%2520Milanovic%2520and%2520Mine%2520Orlu%2520and%2520Abdul%2520W.%2520Basit%2520and%2520Michael%2520T%2520Cook%2520and%2520Tapabrata%2520Chakraborty%2520and%2520David%2520Shorthouse%26entry.1292438233%3D%2520%2520Data%2520scarcity%2520in%2520pharmaceutical%2520research%2520has%2520led%2520to%2520reliance%2520on%250Alabour-intensive%2520trial%2520and%2520error%2520approaches%2520for%2520development%2520rather%2520than%2520data%250Adriven%2520methods.%2520While%2520Machine%2520Learning%2520offers%2520a%2520solution%252C%2520existing%2520datasets%2520are%250Aoften%2520small%2520and%2520noisy%252C%2520limiting%2520their%2520utility.%2520To%2520address%2520this%252C%2520we%2520developed%2520a%250AVariationally%2520Encoded%2520Conditional%2520Tabular%2520Generative%2520Adversarial%2520Network%2520%2528VECT%250AGAN%2529%252C%2520a%2520novel%2520generative%2520model%2520specifically%2520designed%2520for%2520augmenting%2520small%252C%250Anoisy%2520datasets.%2520We%2520introduce%2520a%2520pipeline%2520where%2520data%2520is%2520augmented%2520before%250Aregression%2520model%2520development%2520and%2520demonstrate%2520that%2520this%2520consistently%2520and%250Asignificantly%2520improves%2520performance%2520over%2520other%2520state%2520of%2520the%2520art%2520tabular%250Agenerative%2520models.%2520We%2520apply%2520this%2520pipeline%2520across%2520six%2520pharmaceutical%2520datasets%252C%250Aand%2520highlight%2520its%2520real-world%2520applicability%2520by%2520developing%2520novel%2520polymers%2520with%250Amedically%2520desirable%2520mucoadhesive%2520properties%252C%2520which%2520we%2520made%2520and%2520experimentally%250Acharacterised.%2520Additionally%252C%2520we%2520pre-train%2520the%2520model%2520on%2520the%2520ChEMBL%2520database%2520of%250Adrug-like%2520molecules%252C%2520leveraging%2520knowledge%2520distillation%2520to%2520enhance%2520its%250Ageneralisability%252C%2520making%2520it%2520readily%2520available%2520for%2520use%2520on%2520pharmaceutical%250Adatasets%2520containing%2520small%2520molecules%252C%2520which%2520is%2520an%2520extremely%2520common%250Apharmaceutical%2520task.%2520We%2520demonstrate%2520the%2520power%2520of%2520synthetic%2520data%2520for%250Aregularising%2520small%2520tabular%2520datasets%252C%2520highlighting%2520its%2520potential%2520to%2520become%250Astandard%2520practice%2520in%2520pharmaceutical%2520model%2520development%252C%2520and%2520make%2520our%2520method%252C%250Aincluding%2520VECT%2520GAN%2520pretrained%2520on%2520ChEMBL%2520available%2520as%2520a%2520pip%2520package.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VECT-GAN%3A%20A%20variationally%20encoded%20generative%20model%20for%20overcoming%20data%0A%20%20scarcity%20in%20pharmaceutical%20science&entry.906535625=Youssef%20Abdalla%20and%20Marrisa%20Taub%20and%20Eleanor%20Hilton%20and%20Priya%20Akkaraju%20and%20Alexander%20Milanovic%20and%20Mine%20Orlu%20and%20Abdul%20W.%20Basit%20and%20Michael%20T%20Cook%20and%20Tapabrata%20Chakraborty%20and%20David%20Shorthouse&entry.1292438233=%20%20Data%20scarcity%20in%20pharmaceutical%20research%20has%20led%20to%20reliance%20on%0Alabour-intensive%20trial%20and%20error%20approaches%20for%20development%20rather%20than%20data%0Adriven%20methods.%20While%20Machine%20Learning%20offers%20a%20solution%2C%20existing%20datasets%20are%0Aoften%20small%20and%20noisy%2C%20limiting%20their%20utility.%20To%20address%20this%2C%20we%20developed%20a%0AVariationally%20Encoded%20Conditional%20Tabular%20Generative%20Adversarial%20Network%20%28VECT%0AGAN%29%2C%20a%20novel%20generative%20model%20specifically%20designed%20for%20augmenting%20small%2C%0Anoisy%20datasets.%20We%20introduce%20a%20pipeline%20where%20data%20is%20augmented%20before%0Aregression%20model%20development%20and%20demonstrate%20that%20this%20consistently%20and%0Asignificantly%20improves%20performance%20over%20other%20state%20of%20the%20art%20tabular%0Agenerative%20models.%20We%20apply%20this%20pipeline%20across%20six%20pharmaceutical%20datasets%2C%0Aand%20highlight%20its%20real-world%20applicability%20by%20developing%20novel%20polymers%20with%0Amedically%20desirable%20mucoadhesive%20properties%2C%20which%20we%20made%20and%20experimentally%0Acharacterised.%20Additionally%2C%20we%20pre-train%20the%20model%20on%20the%20ChEMBL%20database%20of%0Adrug-like%20molecules%2C%20leveraging%20knowledge%20distillation%20to%20enhance%20its%0Ageneralisability%2C%20making%20it%20readily%20available%20for%20use%20on%20pharmaceutical%0Adatasets%20containing%20small%20molecules%2C%20which%20is%20an%20extremely%20common%0Apharmaceutical%20task.%20We%20demonstrate%20the%20power%20of%20synthetic%20data%20for%0Aregularising%20small%20tabular%20datasets%2C%20highlighting%20its%20potential%20to%20become%0Astandard%20practice%20in%20pharmaceutical%20model%20development%2C%20and%20make%20our%20method%2C%0Aincluding%20VECT%20GAN%20pretrained%20on%20ChEMBL%20available%20as%20a%20pip%20package.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08995v1&entry.124074799=Read"},
{"title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation\n  Framework", "author": "Sida Tian and Can Zhang and Wei Yuan and Wei Tan and Wenjie Zhu", "abstract": "  In recent years, remarkable advancements in artificial intelligence-generated\ncontent (AIGC) have been achieved in the fields of image synthesis and text\ngeneration, generating content comparable to that produced by humans. However,\nthe quality of AI-generated music has not yet reached this standard, primarily\ndue to the challenge of effectively controlling musical emotions and ensuring\nhigh-quality outputs. This paper presents a generalized symbolic music\ngeneration framework, XMusic, which supports flexible prompts (i.e., images,\nvideos, texts, tags, and humming) to generate emotionally controllable and\nhigh-quality symbolic music. XMusic consists of two core components, XProjector\nand XComposer. XProjector parses the prompts of various modalities into\nsymbolic music elements (i.e., emotions, genres, rhythms and notes) within the\nprojection space to generate matching music. XComposer contains a Generator and\na Selector. The Generator generates emotionally controllable and melodious\nmusic based on our innovative symbolic music representation, whereas the\nSelector identifies high-quality symbolic music by constructing a multi-task\nlearning scheme involving quality assessment, emotion recognition, and genre\nrecognition tasks. In addition, we build XMIDI, a large-scale symbolic music\ndataset that contains 108,023 MIDI files annotated with precise emotion and\ngenre labels. Objective and subjective evaluations show that XMusic\nsignificantly outperforms the current state-of-the-art methods with impressive\nmusic quality. Our XMusic has been awarded as one of the nine Highlights of\nCollectibles at WAIC 2023. The project homepage of XMusic is\nhttps://xmusic-project.github.io.\n", "link": "http://arxiv.org/abs/2501.08809v1", "date": "2025-01-15", "relevancy": 2.02, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5209}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4997}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XMusic%3A%20Towards%20a%20Generalized%20and%20Controllable%20Symbolic%20Music%20Generation%0A%20%20Framework&body=Title%3A%20XMusic%3A%20Towards%20a%20Generalized%20and%20Controllable%20Symbolic%20Music%20Generation%0A%20%20Framework%0AAuthor%3A%20Sida%20Tian%20and%20Can%20Zhang%20and%20Wei%20Yuan%20and%20Wei%20Tan%20and%20Wenjie%20Zhu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20remarkable%20advancements%20in%20artificial%20intelligence-generated%0Acontent%20%28AIGC%29%20have%20been%20achieved%20in%20the%20fields%20of%20image%20synthesis%20and%20text%0Ageneration%2C%20generating%20content%20comparable%20to%20that%20produced%20by%20humans.%20However%2C%0Athe%20quality%20of%20AI-generated%20music%20has%20not%20yet%20reached%20this%20standard%2C%20primarily%0Adue%20to%20the%20challenge%20of%20effectively%20controlling%20musical%20emotions%20and%20ensuring%0Ahigh-quality%20outputs.%20This%20paper%20presents%20a%20generalized%20symbolic%20music%0Ageneration%20framework%2C%20XMusic%2C%20which%20supports%20flexible%20prompts%20%28i.e.%2C%20images%2C%0Avideos%2C%20texts%2C%20tags%2C%20and%20humming%29%20to%20generate%20emotionally%20controllable%20and%0Ahigh-quality%20symbolic%20music.%20XMusic%20consists%20of%20two%20core%20components%2C%20XProjector%0Aand%20XComposer.%20XProjector%20parses%20the%20prompts%20of%20various%20modalities%20into%0Asymbolic%20music%20elements%20%28i.e.%2C%20emotions%2C%20genres%2C%20rhythms%20and%20notes%29%20within%20the%0Aprojection%20space%20to%20generate%20matching%20music.%20XComposer%20contains%20a%20Generator%20and%0Aa%20Selector.%20The%20Generator%20generates%20emotionally%20controllable%20and%20melodious%0Amusic%20based%20on%20our%20innovative%20symbolic%20music%20representation%2C%20whereas%20the%0ASelector%20identifies%20high-quality%20symbolic%20music%20by%20constructing%20a%20multi-task%0Alearning%20scheme%20involving%20quality%20assessment%2C%20emotion%20recognition%2C%20and%20genre%0Arecognition%20tasks.%20In%20addition%2C%20we%20build%20XMIDI%2C%20a%20large-scale%20symbolic%20music%0Adataset%20that%20contains%20108%2C023%20MIDI%20files%20annotated%20with%20precise%20emotion%20and%0Agenre%20labels.%20Objective%20and%20subjective%20evaluations%20show%20that%20XMusic%0Asignificantly%20outperforms%20the%20current%20state-of-the-art%20methods%20with%20impressive%0Amusic%20quality.%20Our%20XMusic%20has%20been%20awarded%20as%20one%20of%20the%20nine%20Highlights%20of%0ACollectibles%20at%20WAIC%202023.%20The%20project%20homepage%20of%20XMusic%20is%0Ahttps%3A//xmusic-project.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXMusic%253A%2520Towards%2520a%2520Generalized%2520and%2520Controllable%2520Symbolic%2520Music%2520Generation%250A%2520%2520Framework%26entry.906535625%3DSida%2520Tian%2520and%2520Can%2520Zhang%2520and%2520Wei%2520Yuan%2520and%2520Wei%2520Tan%2520and%2520Wenjie%2520Zhu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520remarkable%2520advancements%2520in%2520artificial%2520intelligence-generated%250Acontent%2520%2528AIGC%2529%2520have%2520been%2520achieved%2520in%2520the%2520fields%2520of%2520image%2520synthesis%2520and%2520text%250Ageneration%252C%2520generating%2520content%2520comparable%2520to%2520that%2520produced%2520by%2520humans.%2520However%252C%250Athe%2520quality%2520of%2520AI-generated%2520music%2520has%2520not%2520yet%2520reached%2520this%2520standard%252C%2520primarily%250Adue%2520to%2520the%2520challenge%2520of%2520effectively%2520controlling%2520musical%2520emotions%2520and%2520ensuring%250Ahigh-quality%2520outputs.%2520This%2520paper%2520presents%2520a%2520generalized%2520symbolic%2520music%250Ageneration%2520framework%252C%2520XMusic%252C%2520which%2520supports%2520flexible%2520prompts%2520%2528i.e.%252C%2520images%252C%250Avideos%252C%2520texts%252C%2520tags%252C%2520and%2520humming%2529%2520to%2520generate%2520emotionally%2520controllable%2520and%250Ahigh-quality%2520symbolic%2520music.%2520XMusic%2520consists%2520of%2520two%2520core%2520components%252C%2520XProjector%250Aand%2520XComposer.%2520XProjector%2520parses%2520the%2520prompts%2520of%2520various%2520modalities%2520into%250Asymbolic%2520music%2520elements%2520%2528i.e.%252C%2520emotions%252C%2520genres%252C%2520rhythms%2520and%2520notes%2529%2520within%2520the%250Aprojection%2520space%2520to%2520generate%2520matching%2520music.%2520XComposer%2520contains%2520a%2520Generator%2520and%250Aa%2520Selector.%2520The%2520Generator%2520generates%2520emotionally%2520controllable%2520and%2520melodious%250Amusic%2520based%2520on%2520our%2520innovative%2520symbolic%2520music%2520representation%252C%2520whereas%2520the%250ASelector%2520identifies%2520high-quality%2520symbolic%2520music%2520by%2520constructing%2520a%2520multi-task%250Alearning%2520scheme%2520involving%2520quality%2520assessment%252C%2520emotion%2520recognition%252C%2520and%2520genre%250Arecognition%2520tasks.%2520In%2520addition%252C%2520we%2520build%2520XMIDI%252C%2520a%2520large-scale%2520symbolic%2520music%250Adataset%2520that%2520contains%2520108%252C023%2520MIDI%2520files%2520annotated%2520with%2520precise%2520emotion%2520and%250Agenre%2520labels.%2520Objective%2520and%2520subjective%2520evaluations%2520show%2520that%2520XMusic%250Asignificantly%2520outperforms%2520the%2520current%2520state-of-the-art%2520methods%2520with%2520impressive%250Amusic%2520quality.%2520Our%2520XMusic%2520has%2520been%2520awarded%2520as%2520one%2520of%2520the%2520nine%2520Highlights%2520of%250ACollectibles%2520at%2520WAIC%25202023.%2520The%2520project%2520homepage%2520of%2520XMusic%2520is%250Ahttps%253A//xmusic-project.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XMusic%3A%20Towards%20a%20Generalized%20and%20Controllable%20Symbolic%20Music%20Generation%0A%20%20Framework&entry.906535625=Sida%20Tian%20and%20Can%20Zhang%20and%20Wei%20Yuan%20and%20Wei%20Tan%20and%20Wenjie%20Zhu&entry.1292438233=%20%20In%20recent%20years%2C%20remarkable%20advancements%20in%20artificial%20intelligence-generated%0Acontent%20%28AIGC%29%20have%20been%20achieved%20in%20the%20fields%20of%20image%20synthesis%20and%20text%0Ageneration%2C%20generating%20content%20comparable%20to%20that%20produced%20by%20humans.%20However%2C%0Athe%20quality%20of%20AI-generated%20music%20has%20not%20yet%20reached%20this%20standard%2C%20primarily%0Adue%20to%20the%20challenge%20of%20effectively%20controlling%20musical%20emotions%20and%20ensuring%0Ahigh-quality%20outputs.%20This%20paper%20presents%20a%20generalized%20symbolic%20music%0Ageneration%20framework%2C%20XMusic%2C%20which%20supports%20flexible%20prompts%20%28i.e.%2C%20images%2C%0Avideos%2C%20texts%2C%20tags%2C%20and%20humming%29%20to%20generate%20emotionally%20controllable%20and%0Ahigh-quality%20symbolic%20music.%20XMusic%20consists%20of%20two%20core%20components%2C%20XProjector%0Aand%20XComposer.%20XProjector%20parses%20the%20prompts%20of%20various%20modalities%20into%0Asymbolic%20music%20elements%20%28i.e.%2C%20emotions%2C%20genres%2C%20rhythms%20and%20notes%29%20within%20the%0Aprojection%20space%20to%20generate%20matching%20music.%20XComposer%20contains%20a%20Generator%20and%0Aa%20Selector.%20The%20Generator%20generates%20emotionally%20controllable%20and%20melodious%0Amusic%20based%20on%20our%20innovative%20symbolic%20music%20representation%2C%20whereas%20the%0ASelector%20identifies%20high-quality%20symbolic%20music%20by%20constructing%20a%20multi-task%0Alearning%20scheme%20involving%20quality%20assessment%2C%20emotion%20recognition%2C%20and%20genre%0Arecognition%20tasks.%20In%20addition%2C%20we%20build%20XMIDI%2C%20a%20large-scale%20symbolic%20music%0Adataset%20that%20contains%20108%2C023%20MIDI%20files%20annotated%20with%20precise%20emotion%20and%0Agenre%20labels.%20Objective%20and%20subjective%20evaluations%20show%20that%20XMusic%0Asignificantly%20outperforms%20the%20current%20state-of-the-art%20methods%20with%20impressive%0Amusic%20quality.%20Our%20XMusic%20has%20been%20awarded%20as%20one%20of%20the%20nine%20Highlights%20of%0ACollectibles%20at%20WAIC%202023.%20The%20project%20homepage%20of%20XMusic%20is%0Ahttps%3A//xmusic-project.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08809v1&entry.124074799=Read"},
{"title": "CrystalGRW: Generative Modeling of Crystal Structures with Targeted\n  Properties via Geodesic Random Walks", "author": "Krit Tangsongcharoen and Teerachote Pakornchote and Chayanon Atthapak and Natthaphon Choomphon-anomakhun and Annop Ektarawong and Bj\u00f6rn Alling and Christopher Sutton and Thiti Bovornratanaraks and Thiparat Chotibut", "abstract": "  Determining whether a candidate crystalline material is thermodynamically\nstable depends on identifying its true ground-state structure, a central\nchallenge in computational materials science. We introduce CrystalGRW, a\ndiffusion-based generative model on Riemannian manifolds that proposes novel\ncrystal configurations and can predict stable phases validated by density\nfunctional theory. The crystal properties, such as fractional coordinates,\natomic types, and lattice matrices, are represented on suitable Riemannian\nmanifolds, ensuring that new predictions generated through the diffusion\nprocess preserve the periodicity of crystal structures. We incorporate an\nequivariant graph neural network to also account for rotational and\ntranslational symmetries during the generation process. CrystalGRW demonstrates\nthe ability to generate realistic crystal structures that are close to their\nground states with accuracy comparable to existing models, while also enabling\nconditional control, such as specifying a desired crystallographic point group.\nThese features help accelerate materials discovery and inverse design by\noffering stable, symmetry-consistent crystal candidates for experimental\nvalidation.\n", "link": "http://arxiv.org/abs/2501.08998v1", "date": "2025-01-15", "relevancy": 2.0134, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5358}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4883}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrystalGRW%3A%20Generative%20Modeling%20of%20Crystal%20Structures%20with%20Targeted%0A%20%20Properties%20via%20Geodesic%20Random%20Walks&body=Title%3A%20CrystalGRW%3A%20Generative%20Modeling%20of%20Crystal%20Structures%20with%20Targeted%0A%20%20Properties%20via%20Geodesic%20Random%20Walks%0AAuthor%3A%20Krit%20Tangsongcharoen%20and%20Teerachote%20Pakornchote%20and%20Chayanon%20Atthapak%20and%20Natthaphon%20Choomphon-anomakhun%20and%20Annop%20Ektarawong%20and%20Bj%C3%B6rn%20Alling%20and%20Christopher%20Sutton%20and%20Thiti%20Bovornratanaraks%20and%20Thiparat%20Chotibut%0AAbstract%3A%20%20%20Determining%20whether%20a%20candidate%20crystalline%20material%20is%20thermodynamically%0Astable%20depends%20on%20identifying%20its%20true%20ground-state%20structure%2C%20a%20central%0Achallenge%20in%20computational%20materials%20science.%20We%20introduce%20CrystalGRW%2C%20a%0Adiffusion-based%20generative%20model%20on%20Riemannian%20manifolds%20that%20proposes%20novel%0Acrystal%20configurations%20and%20can%20predict%20stable%20phases%20validated%20by%20density%0Afunctional%20theory.%20The%20crystal%20properties%2C%20such%20as%20fractional%20coordinates%2C%0Aatomic%20types%2C%20and%20lattice%20matrices%2C%20are%20represented%20on%20suitable%20Riemannian%0Amanifolds%2C%20ensuring%20that%20new%20predictions%20generated%20through%20the%20diffusion%0Aprocess%20preserve%20the%20periodicity%20of%20crystal%20structures.%20We%20incorporate%20an%0Aequivariant%20graph%20neural%20network%20to%20also%20account%20for%20rotational%20and%0Atranslational%20symmetries%20during%20the%20generation%20process.%20CrystalGRW%20demonstrates%0Athe%20ability%20to%20generate%20realistic%20crystal%20structures%20that%20are%20close%20to%20their%0Aground%20states%20with%20accuracy%20comparable%20to%20existing%20models%2C%20while%20also%20enabling%0Aconditional%20control%2C%20such%20as%20specifying%20a%20desired%20crystallographic%20point%20group.%0AThese%20features%20help%20accelerate%20materials%20discovery%20and%20inverse%20design%20by%0Aoffering%20stable%2C%20symmetry-consistent%20crystal%20candidates%20for%20experimental%0Avalidation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrystalGRW%253A%2520Generative%2520Modeling%2520of%2520Crystal%2520Structures%2520with%2520Targeted%250A%2520%2520Properties%2520via%2520Geodesic%2520Random%2520Walks%26entry.906535625%3DKrit%2520Tangsongcharoen%2520and%2520Teerachote%2520Pakornchote%2520and%2520Chayanon%2520Atthapak%2520and%2520Natthaphon%2520Choomphon-anomakhun%2520and%2520Annop%2520Ektarawong%2520and%2520Bj%25C3%25B6rn%2520Alling%2520and%2520Christopher%2520Sutton%2520and%2520Thiti%2520Bovornratanaraks%2520and%2520Thiparat%2520Chotibut%26entry.1292438233%3D%2520%2520Determining%2520whether%2520a%2520candidate%2520crystalline%2520material%2520is%2520thermodynamically%250Astable%2520depends%2520on%2520identifying%2520its%2520true%2520ground-state%2520structure%252C%2520a%2520central%250Achallenge%2520in%2520computational%2520materials%2520science.%2520We%2520introduce%2520CrystalGRW%252C%2520a%250Adiffusion-based%2520generative%2520model%2520on%2520Riemannian%2520manifolds%2520that%2520proposes%2520novel%250Acrystal%2520configurations%2520and%2520can%2520predict%2520stable%2520phases%2520validated%2520by%2520density%250Afunctional%2520theory.%2520The%2520crystal%2520properties%252C%2520such%2520as%2520fractional%2520coordinates%252C%250Aatomic%2520types%252C%2520and%2520lattice%2520matrices%252C%2520are%2520represented%2520on%2520suitable%2520Riemannian%250Amanifolds%252C%2520ensuring%2520that%2520new%2520predictions%2520generated%2520through%2520the%2520diffusion%250Aprocess%2520preserve%2520the%2520periodicity%2520of%2520crystal%2520structures.%2520We%2520incorporate%2520an%250Aequivariant%2520graph%2520neural%2520network%2520to%2520also%2520account%2520for%2520rotational%2520and%250Atranslational%2520symmetries%2520during%2520the%2520generation%2520process.%2520CrystalGRW%2520demonstrates%250Athe%2520ability%2520to%2520generate%2520realistic%2520crystal%2520structures%2520that%2520are%2520close%2520to%2520their%250Aground%2520states%2520with%2520accuracy%2520comparable%2520to%2520existing%2520models%252C%2520while%2520also%2520enabling%250Aconditional%2520control%252C%2520such%2520as%2520specifying%2520a%2520desired%2520crystallographic%2520point%2520group.%250AThese%2520features%2520help%2520accelerate%2520materials%2520discovery%2520and%2520inverse%2520design%2520by%250Aoffering%2520stable%252C%2520symmetry-consistent%2520crystal%2520candidates%2520for%2520experimental%250Avalidation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrystalGRW%3A%20Generative%20Modeling%20of%20Crystal%20Structures%20with%20Targeted%0A%20%20Properties%20via%20Geodesic%20Random%20Walks&entry.906535625=Krit%20Tangsongcharoen%20and%20Teerachote%20Pakornchote%20and%20Chayanon%20Atthapak%20and%20Natthaphon%20Choomphon-anomakhun%20and%20Annop%20Ektarawong%20and%20Bj%C3%B6rn%20Alling%20and%20Christopher%20Sutton%20and%20Thiti%20Bovornratanaraks%20and%20Thiparat%20Chotibut&entry.1292438233=%20%20Determining%20whether%20a%20candidate%20crystalline%20material%20is%20thermodynamically%0Astable%20depends%20on%20identifying%20its%20true%20ground-state%20structure%2C%20a%20central%0Achallenge%20in%20computational%20materials%20science.%20We%20introduce%20CrystalGRW%2C%20a%0Adiffusion-based%20generative%20model%20on%20Riemannian%20manifolds%20that%20proposes%20novel%0Acrystal%20configurations%20and%20can%20predict%20stable%20phases%20validated%20by%20density%0Afunctional%20theory.%20The%20crystal%20properties%2C%20such%20as%20fractional%20coordinates%2C%0Aatomic%20types%2C%20and%20lattice%20matrices%2C%20are%20represented%20on%20suitable%20Riemannian%0Amanifolds%2C%20ensuring%20that%20new%20predictions%20generated%20through%20the%20diffusion%0Aprocess%20preserve%20the%20periodicity%20of%20crystal%20structures.%20We%20incorporate%20an%0Aequivariant%20graph%20neural%20network%20to%20also%20account%20for%20rotational%20and%0Atranslational%20symmetries%20during%20the%20generation%20process.%20CrystalGRW%20demonstrates%0Athe%20ability%20to%20generate%20realistic%20crystal%20structures%20that%20are%20close%20to%20their%0Aground%20states%20with%20accuracy%20comparable%20to%20existing%20models%2C%20while%20also%20enabling%0Aconditional%20control%2C%20such%20as%20specifying%20a%20desired%20crystallographic%20point%20group.%0AThese%20features%20help%20accelerate%20materials%20discovery%20and%20inverse%20design%20by%0Aoffering%20stable%2C%20symmetry-consistent%20crystal%20candidates%20for%20experimental%0Avalidation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08998v1&entry.124074799=Read"},
{"title": "RoHan: Robust Hand Detection in Operation Room", "author": "Roi Papo and Sapir Gershov and Tom Friedman and Itay Or and Gil Bolotin and Shlomi Laufer", "abstract": "  Hand-specific localization has garnered significant interest within the\ncomputer vision community. Although there are numerous datasets with hand\nannotations from various angles and settings, domain transfer techniques\nfrequently struggle in surgical environments. This is mainly due to the limited\navailability of gloved hand instances and the unique challenges of operating\nrooms (ORs). Thus, hand-detection models tailored to OR settings require\nextensive training and expensive annotation processes. To overcome these\nchallenges, we present \"RoHan\" - a novel approach for robust hand detection in\nthe OR, leveraging advanced semi-supervised domain adaptation techniques to\ntackle the challenges of varying recording conditions, diverse glove colors,\nand occlusions common in surgical settings. Our methodology encompasses two\nmain stages: (1) data augmentation strategy that utilizes \"Artificial Gloves,\"\na method for augmenting publicly available hand datasets with synthetic images\nof hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that\nimproves detection performance in real-world OR settings through iterative\nprediction refinement and efficient frame filtering. We evaluate our method\nusing two datasets: simulated enterotomy repair and saphenous vein graft\nharvesting. \"RoHan\" substantially reduces the need for extensive labeling and\nmodel training, paving the way for the practical implementation of hand\ndetection technologies in medical settings.\n", "link": "http://arxiv.org/abs/2501.08115v2", "date": "2025-01-15", "relevancy": 2.0083, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5181}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5032}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoHan%3A%20Robust%20Hand%20Detection%20in%20Operation%20Room&body=Title%3A%20RoHan%3A%20Robust%20Hand%20Detection%20in%20Operation%20Room%0AAuthor%3A%20Roi%20Papo%20and%20Sapir%20Gershov%20and%20Tom%20Friedman%20and%20Itay%20Or%20and%20Gil%20Bolotin%20and%20Shlomi%20Laufer%0AAbstract%3A%20%20%20Hand-specific%20localization%20has%20garnered%20significant%20interest%20within%20the%0Acomputer%20vision%20community.%20Although%20there%20are%20numerous%20datasets%20with%20hand%0Aannotations%20from%20various%20angles%20and%20settings%2C%20domain%20transfer%20techniques%0Afrequently%20struggle%20in%20surgical%20environments.%20This%20is%20mainly%20due%20to%20the%20limited%0Aavailability%20of%20gloved%20hand%20instances%20and%20the%20unique%20challenges%20of%20operating%0Arooms%20%28ORs%29.%20Thus%2C%20hand-detection%20models%20tailored%20to%20OR%20settings%20require%0Aextensive%20training%20and%20expensive%20annotation%20processes.%20To%20overcome%20these%0Achallenges%2C%20we%20present%20%22RoHan%22%20-%20a%20novel%20approach%20for%20robust%20hand%20detection%20in%0Athe%20OR%2C%20leveraging%20advanced%20semi-supervised%20domain%20adaptation%20techniques%20to%0Atackle%20the%20challenges%20of%20varying%20recording%20conditions%2C%20diverse%20glove%20colors%2C%0Aand%20occlusions%20common%20in%20surgical%20settings.%20Our%20methodology%20encompasses%20two%0Amain%20stages%3A%20%281%29%20data%20augmentation%20strategy%20that%20utilizes%20%22Artificial%20Gloves%2C%22%0Aa%20method%20for%20augmenting%20publicly%20available%20hand%20datasets%20with%20synthetic%20images%0Aof%20hands-wearing%20gloves%3B%20%282%29%20semi-supervised%20domain%20adaptation%20pipeline%20that%0Aimproves%20detection%20performance%20in%20real-world%20OR%20settings%20through%20iterative%0Aprediction%20refinement%20and%20efficient%20frame%20filtering.%20We%20evaluate%20our%20method%0Ausing%20two%20datasets%3A%20simulated%20enterotomy%20repair%20and%20saphenous%20vein%20graft%0Aharvesting.%20%22RoHan%22%20substantially%20reduces%20the%20need%20for%20extensive%20labeling%20and%0Amodel%20training%2C%20paving%20the%20way%20for%20the%20practical%20implementation%20of%20hand%0Adetection%20technologies%20in%20medical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoHan%253A%2520Robust%2520Hand%2520Detection%2520in%2520Operation%2520Room%26entry.906535625%3DRoi%2520Papo%2520and%2520Sapir%2520Gershov%2520and%2520Tom%2520Friedman%2520and%2520Itay%2520Or%2520and%2520Gil%2520Bolotin%2520and%2520Shlomi%2520Laufer%26entry.1292438233%3D%2520%2520Hand-specific%2520localization%2520has%2520garnered%2520significant%2520interest%2520within%2520the%250Acomputer%2520vision%2520community.%2520Although%2520there%2520are%2520numerous%2520datasets%2520with%2520hand%250Aannotations%2520from%2520various%2520angles%2520and%2520settings%252C%2520domain%2520transfer%2520techniques%250Afrequently%2520struggle%2520in%2520surgical%2520environments.%2520This%2520is%2520mainly%2520due%2520to%2520the%2520limited%250Aavailability%2520of%2520gloved%2520hand%2520instances%2520and%2520the%2520unique%2520challenges%2520of%2520operating%250Arooms%2520%2528ORs%2529.%2520Thus%252C%2520hand-detection%2520models%2520tailored%2520to%2520OR%2520settings%2520require%250Aextensive%2520training%2520and%2520expensive%2520annotation%2520processes.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520present%2520%2522RoHan%2522%2520-%2520a%2520novel%2520approach%2520for%2520robust%2520hand%2520detection%2520in%250Athe%2520OR%252C%2520leveraging%2520advanced%2520semi-supervised%2520domain%2520adaptation%2520techniques%2520to%250Atackle%2520the%2520challenges%2520of%2520varying%2520recording%2520conditions%252C%2520diverse%2520glove%2520colors%252C%250Aand%2520occlusions%2520common%2520in%2520surgical%2520settings.%2520Our%2520methodology%2520encompasses%2520two%250Amain%2520stages%253A%2520%25281%2529%2520data%2520augmentation%2520strategy%2520that%2520utilizes%2520%2522Artificial%2520Gloves%252C%2522%250Aa%2520method%2520for%2520augmenting%2520publicly%2520available%2520hand%2520datasets%2520with%2520synthetic%2520images%250Aof%2520hands-wearing%2520gloves%253B%2520%25282%2529%2520semi-supervised%2520domain%2520adaptation%2520pipeline%2520that%250Aimproves%2520detection%2520performance%2520in%2520real-world%2520OR%2520settings%2520through%2520iterative%250Aprediction%2520refinement%2520and%2520efficient%2520frame%2520filtering.%2520We%2520evaluate%2520our%2520method%250Ausing%2520two%2520datasets%253A%2520simulated%2520enterotomy%2520repair%2520and%2520saphenous%2520vein%2520graft%250Aharvesting.%2520%2522RoHan%2522%2520substantially%2520reduces%2520the%2520need%2520for%2520extensive%2520labeling%2520and%250Amodel%2520training%252C%2520paving%2520the%2520way%2520for%2520the%2520practical%2520implementation%2520of%2520hand%250Adetection%2520technologies%2520in%2520medical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoHan%3A%20Robust%20Hand%20Detection%20in%20Operation%20Room&entry.906535625=Roi%20Papo%20and%20Sapir%20Gershov%20and%20Tom%20Friedman%20and%20Itay%20Or%20and%20Gil%20Bolotin%20and%20Shlomi%20Laufer&entry.1292438233=%20%20Hand-specific%20localization%20has%20garnered%20significant%20interest%20within%20the%0Acomputer%20vision%20community.%20Although%20there%20are%20numerous%20datasets%20with%20hand%0Aannotations%20from%20various%20angles%20and%20settings%2C%20domain%20transfer%20techniques%0Afrequently%20struggle%20in%20surgical%20environments.%20This%20is%20mainly%20due%20to%20the%20limited%0Aavailability%20of%20gloved%20hand%20instances%20and%20the%20unique%20challenges%20of%20operating%0Arooms%20%28ORs%29.%20Thus%2C%20hand-detection%20models%20tailored%20to%20OR%20settings%20require%0Aextensive%20training%20and%20expensive%20annotation%20processes.%20To%20overcome%20these%0Achallenges%2C%20we%20present%20%22RoHan%22%20-%20a%20novel%20approach%20for%20robust%20hand%20detection%20in%0Athe%20OR%2C%20leveraging%20advanced%20semi-supervised%20domain%20adaptation%20techniques%20to%0Atackle%20the%20challenges%20of%20varying%20recording%20conditions%2C%20diverse%20glove%20colors%2C%0Aand%20occlusions%20common%20in%20surgical%20settings.%20Our%20methodology%20encompasses%20two%0Amain%20stages%3A%20%281%29%20data%20augmentation%20strategy%20that%20utilizes%20%22Artificial%20Gloves%2C%22%0Aa%20method%20for%20augmenting%20publicly%20available%20hand%20datasets%20with%20synthetic%20images%0Aof%20hands-wearing%20gloves%3B%20%282%29%20semi-supervised%20domain%20adaptation%20pipeline%20that%0Aimproves%20detection%20performance%20in%20real-world%20OR%20settings%20through%20iterative%0Aprediction%20refinement%20and%20efficient%20frame%20filtering.%20We%20evaluate%20our%20method%0Ausing%20two%20datasets%3A%20simulated%20enterotomy%20repair%20and%20saphenous%20vein%20graft%0Aharvesting.%20%22RoHan%22%20substantially%20reduces%20the%20need%20for%20extensive%20labeling%20and%0Amodel%20training%2C%20paving%20the%20way%20for%20the%20practical%20implementation%20of%20hand%0Adetection%20technologies%20in%20medical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08115v2&entry.124074799=Read"},
{"title": "Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular\n  Data", "author": "David Holzm\u00fcller and L\u00e9o Grinsztajn and Ingo Steinwart", "abstract": "  For classification and regression on tabular data, the dominance of\ngradient-boosted decision trees (GBDTs) has recently been challenged by often\nmuch slower deep learning methods with extensive hyperparameter tuning. We\naddress this discrepancy by introducing (a) RealMLP, an improved multilayer\nperceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and\nRealMLP. We tune RealMLP and the default parameters on a meta-train benchmark\nwith 118 datasets and compare them to hyperparameter-optimized versions on a\ndisjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly\nbenchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large\ntabular datasets (1K--500K samples) show that RealMLP offers a favorable\ntime-accuracy tradeoff compared to other neural baselines and is competitive\nwith GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and\nGBDTs with improved default parameters can achieve excellent results without\nhyperparameter tuning. Finally, we demonstrate that some of RealMLP's\nimprovements can also considerably improve the performance of TabR with default\nparameters.\n", "link": "http://arxiv.org/abs/2407.04491v3", "date": "2025-01-15", "relevancy": 2.0025, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5457}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4711}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20by%20Default%3A%20Strong%20Pre-Tuned%20MLPs%20and%20Boosted%20Trees%20on%20Tabular%0A%20%20Data&body=Title%3A%20Better%20by%20Default%3A%20Strong%20Pre-Tuned%20MLPs%20and%20Boosted%20Trees%20on%20Tabular%0A%20%20Data%0AAuthor%3A%20David%20Holzm%C3%BCller%20and%20L%C3%A9o%20Grinsztajn%20and%20Ingo%20Steinwart%0AAbstract%3A%20%20%20For%20classification%20and%20regression%20on%20tabular%20data%2C%20the%20dominance%20of%0Agradient-boosted%20decision%20trees%20%28GBDTs%29%20has%20recently%20been%20challenged%20by%20often%0Amuch%20slower%20deep%20learning%20methods%20with%20extensive%20hyperparameter%20tuning.%20We%0Aaddress%20this%20discrepancy%20by%20introducing%20%28a%29%20RealMLP%2C%20an%20improved%20multilayer%0Aperceptron%20%28MLP%29%2C%20and%20%28b%29%20strong%20meta-tuned%20default%20parameters%20for%20GBDTs%20and%0ARealMLP.%20We%20tune%20RealMLP%20and%20the%20default%20parameters%20on%20a%20meta-train%20benchmark%0Awith%20118%20datasets%20and%20compare%20them%20to%20hyperparameter-optimized%20versions%20on%20a%0Adisjoint%20meta-test%20benchmark%20with%2090%20datasets%2C%20as%20well%20as%20the%20GBDT-friendly%0Abenchmark%20by%20Grinsztajn%20et%20al.%20%282022%29.%20Our%20benchmark%20results%20on%20medium-to-large%0Atabular%20datasets%20%281K--500K%20samples%29%20show%20that%20RealMLP%20offers%20a%20favorable%0Atime-accuracy%20tradeoff%20compared%20to%20other%20neural%20baselines%20and%20is%20competitive%0Awith%20GBDTs%20in%20terms%20of%20benchmark%20scores.%20Moreover%2C%20a%20combination%20of%20RealMLP%20and%0AGBDTs%20with%20improved%20default%20parameters%20can%20achieve%20excellent%20results%20without%0Ahyperparameter%20tuning.%20Finally%2C%20we%20demonstrate%20that%20some%20of%20RealMLP%27s%0Aimprovements%20can%20also%20considerably%20improve%20the%20performance%20of%20TabR%20with%20default%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04491v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520by%2520Default%253A%2520Strong%2520Pre-Tuned%2520MLPs%2520and%2520Boosted%2520Trees%2520on%2520Tabular%250A%2520%2520Data%26entry.906535625%3DDavid%2520Holzm%25C3%25BCller%2520and%2520L%25C3%25A9o%2520Grinsztajn%2520and%2520Ingo%2520Steinwart%26entry.1292438233%3D%2520%2520For%2520classification%2520and%2520regression%2520on%2520tabular%2520data%252C%2520the%2520dominance%2520of%250Agradient-boosted%2520decision%2520trees%2520%2528GBDTs%2529%2520has%2520recently%2520been%2520challenged%2520by%2520often%250Amuch%2520slower%2520deep%2520learning%2520methods%2520with%2520extensive%2520hyperparameter%2520tuning.%2520We%250Aaddress%2520this%2520discrepancy%2520by%2520introducing%2520%2528a%2529%2520RealMLP%252C%2520an%2520improved%2520multilayer%250Aperceptron%2520%2528MLP%2529%252C%2520and%2520%2528b%2529%2520strong%2520meta-tuned%2520default%2520parameters%2520for%2520GBDTs%2520and%250ARealMLP.%2520We%2520tune%2520RealMLP%2520and%2520the%2520default%2520parameters%2520on%2520a%2520meta-train%2520benchmark%250Awith%2520118%2520datasets%2520and%2520compare%2520them%2520to%2520hyperparameter-optimized%2520versions%2520on%2520a%250Adisjoint%2520meta-test%2520benchmark%2520with%252090%2520datasets%252C%2520as%2520well%2520as%2520the%2520GBDT-friendly%250Abenchmark%2520by%2520Grinsztajn%2520et%2520al.%2520%25282022%2529.%2520Our%2520benchmark%2520results%2520on%2520medium-to-large%250Atabular%2520datasets%2520%25281K--500K%2520samples%2529%2520show%2520that%2520RealMLP%2520offers%2520a%2520favorable%250Atime-accuracy%2520tradeoff%2520compared%2520to%2520other%2520neural%2520baselines%2520and%2520is%2520competitive%250Awith%2520GBDTs%2520in%2520terms%2520of%2520benchmark%2520scores.%2520Moreover%252C%2520a%2520combination%2520of%2520RealMLP%2520and%250AGBDTs%2520with%2520improved%2520default%2520parameters%2520can%2520achieve%2520excellent%2520results%2520without%250Ahyperparameter%2520tuning.%2520Finally%252C%2520we%2520demonstrate%2520that%2520some%2520of%2520RealMLP%2527s%250Aimprovements%2520can%2520also%2520considerably%2520improve%2520the%2520performance%2520of%2520TabR%2520with%2520default%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04491v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20by%20Default%3A%20Strong%20Pre-Tuned%20MLPs%20and%20Boosted%20Trees%20on%20Tabular%0A%20%20Data&entry.906535625=David%20Holzm%C3%BCller%20and%20L%C3%A9o%20Grinsztajn%20and%20Ingo%20Steinwart&entry.1292438233=%20%20For%20classification%20and%20regression%20on%20tabular%20data%2C%20the%20dominance%20of%0Agradient-boosted%20decision%20trees%20%28GBDTs%29%20has%20recently%20been%20challenged%20by%20often%0Amuch%20slower%20deep%20learning%20methods%20with%20extensive%20hyperparameter%20tuning.%20We%0Aaddress%20this%20discrepancy%20by%20introducing%20%28a%29%20RealMLP%2C%20an%20improved%20multilayer%0Aperceptron%20%28MLP%29%2C%20and%20%28b%29%20strong%20meta-tuned%20default%20parameters%20for%20GBDTs%20and%0ARealMLP.%20We%20tune%20RealMLP%20and%20the%20default%20parameters%20on%20a%20meta-train%20benchmark%0Awith%20118%20datasets%20and%20compare%20them%20to%20hyperparameter-optimized%20versions%20on%20a%0Adisjoint%20meta-test%20benchmark%20with%2090%20datasets%2C%20as%20well%20as%20the%20GBDT-friendly%0Abenchmark%20by%20Grinsztajn%20et%20al.%20%282022%29.%20Our%20benchmark%20results%20on%20medium-to-large%0Atabular%20datasets%20%281K--500K%20samples%29%20show%20that%20RealMLP%20offers%20a%20favorable%0Atime-accuracy%20tradeoff%20compared%20to%20other%20neural%20baselines%20and%20is%20competitive%0Awith%20GBDTs%20in%20terms%20of%20benchmark%20scores.%20Moreover%2C%20a%20combination%20of%20RealMLP%20and%0AGBDTs%20with%20improved%20default%20parameters%20can%20achieve%20excellent%20results%20without%0Ahyperparameter%20tuning.%20Finally%2C%20we%20demonstrate%20that%20some%20of%20RealMLP%27s%0Aimprovements%20can%20also%20considerably%20improve%20the%20performance%20of%20TabR%20with%20default%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04491v3&entry.124074799=Read"},
{"title": "Networked Agents in the Dark: Team Value Learning under Partial\n  Observability", "author": "Guilherme S. Varela and Alberto Sardinha and Francisco S. Melo", "abstract": "  We propose a novel cooperative multi-agent reinforcement learning (MARL)\napproach for networked agents. In contrast to previous methods that rely on\ncomplete state information or joint observations, our agents must learn how to\nreach shared objectives under partial observability. During training, they\ncollect individual rewards and approximate a team value function through local\ncommunication, resulting in cooperative behavior. To describe our problem, we\nintroduce the networked dynamic partially observable Markov game framework,\nwhere agents communicate over a switching topology communication network. Our\ndistributed method, DNA-MARL, uses a consensus mechanism for local\ncommunication and gradient descent for local computation. DNA-MARL increases\nthe range of the possible applications of networked agents, being well-suited\nfor real world domains that impose privacy and where the messages may not reach\ntheir recipients. We evaluate DNA-MARL across benchmark MARL scenarios. Our\nresults highlight the superior performance of DNA-MARL over previous methods.\n", "link": "http://arxiv.org/abs/2501.08778v1", "date": "2025-01-15", "relevancy": 1.9872, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5296}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5094}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Networked%20Agents%20in%20the%20Dark%3A%20Team%20Value%20Learning%20under%20Partial%0A%20%20Observability&body=Title%3A%20Networked%20Agents%20in%20the%20Dark%3A%20Team%20Value%20Learning%20under%20Partial%0A%20%20Observability%0AAuthor%3A%20Guilherme%20S.%20Varela%20and%20Alberto%20Sardinha%20and%20Francisco%20S.%20Melo%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20cooperative%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Aapproach%20for%20networked%20agents.%20In%20contrast%20to%20previous%20methods%20that%20rely%20on%0Acomplete%20state%20information%20or%20joint%20observations%2C%20our%20agents%20must%20learn%20how%20to%0Areach%20shared%20objectives%20under%20partial%20observability.%20During%20training%2C%20they%0Acollect%20individual%20rewards%20and%20approximate%20a%20team%20value%20function%20through%20local%0Acommunication%2C%20resulting%20in%20cooperative%20behavior.%20To%20describe%20our%20problem%2C%20we%0Aintroduce%20the%20networked%20dynamic%20partially%20observable%20Markov%20game%20framework%2C%0Awhere%20agents%20communicate%20over%20a%20switching%20topology%20communication%20network.%20Our%0Adistributed%20method%2C%20DNA-MARL%2C%20uses%20a%20consensus%20mechanism%20for%20local%0Acommunication%20and%20gradient%20descent%20for%20local%20computation.%20DNA-MARL%20increases%0Athe%20range%20of%20the%20possible%20applications%20of%20networked%20agents%2C%20being%20well-suited%0Afor%20real%20world%20domains%20that%20impose%20privacy%20and%20where%20the%20messages%20may%20not%20reach%0Atheir%20recipients.%20We%20evaluate%20DNA-MARL%20across%20benchmark%20MARL%20scenarios.%20Our%0Aresults%20highlight%20the%20superior%20performance%20of%20DNA-MARL%20over%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetworked%2520Agents%2520in%2520the%2520Dark%253A%2520Team%2520Value%2520Learning%2520under%2520Partial%250A%2520%2520Observability%26entry.906535625%3DGuilherme%2520S.%2520Varela%2520and%2520Alberto%2520Sardinha%2520and%2520Francisco%2520S.%2520Melo%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520cooperative%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%250Aapproach%2520for%2520networked%2520agents.%2520In%2520contrast%2520to%2520previous%2520methods%2520that%2520rely%2520on%250Acomplete%2520state%2520information%2520or%2520joint%2520observations%252C%2520our%2520agents%2520must%2520learn%2520how%2520to%250Areach%2520shared%2520objectives%2520under%2520partial%2520observability.%2520During%2520training%252C%2520they%250Acollect%2520individual%2520rewards%2520and%2520approximate%2520a%2520team%2520value%2520function%2520through%2520local%250Acommunication%252C%2520resulting%2520in%2520cooperative%2520behavior.%2520To%2520describe%2520our%2520problem%252C%2520we%250Aintroduce%2520the%2520networked%2520dynamic%2520partially%2520observable%2520Markov%2520game%2520framework%252C%250Awhere%2520agents%2520communicate%2520over%2520a%2520switching%2520topology%2520communication%2520network.%2520Our%250Adistributed%2520method%252C%2520DNA-MARL%252C%2520uses%2520a%2520consensus%2520mechanism%2520for%2520local%250Acommunication%2520and%2520gradient%2520descent%2520for%2520local%2520computation.%2520DNA-MARL%2520increases%250Athe%2520range%2520of%2520the%2520possible%2520applications%2520of%2520networked%2520agents%252C%2520being%2520well-suited%250Afor%2520real%2520world%2520domains%2520that%2520impose%2520privacy%2520and%2520where%2520the%2520messages%2520may%2520not%2520reach%250Atheir%2520recipients.%2520We%2520evaluate%2520DNA-MARL%2520across%2520benchmark%2520MARL%2520scenarios.%2520Our%250Aresults%2520highlight%2520the%2520superior%2520performance%2520of%2520DNA-MARL%2520over%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Networked%20Agents%20in%20the%20Dark%3A%20Team%20Value%20Learning%20under%20Partial%0A%20%20Observability&entry.906535625=Guilherme%20S.%20Varela%20and%20Alberto%20Sardinha%20and%20Francisco%20S.%20Melo&entry.1292438233=%20%20We%20propose%20a%20novel%20cooperative%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Aapproach%20for%20networked%20agents.%20In%20contrast%20to%20previous%20methods%20that%20rely%20on%0Acomplete%20state%20information%20or%20joint%20observations%2C%20our%20agents%20must%20learn%20how%20to%0Areach%20shared%20objectives%20under%20partial%20observability.%20During%20training%2C%20they%0Acollect%20individual%20rewards%20and%20approximate%20a%20team%20value%20function%20through%20local%0Acommunication%2C%20resulting%20in%20cooperative%20behavior.%20To%20describe%20our%20problem%2C%20we%0Aintroduce%20the%20networked%20dynamic%20partially%20observable%20Markov%20game%20framework%2C%0Awhere%20agents%20communicate%20over%20a%20switching%20topology%20communication%20network.%20Our%0Adistributed%20method%2C%20DNA-MARL%2C%20uses%20a%20consensus%20mechanism%20for%20local%0Acommunication%20and%20gradient%20descent%20for%20local%20computation.%20DNA-MARL%20increases%0Athe%20range%20of%20the%20possible%20applications%20of%20networked%20agents%2C%20being%20well-suited%0Afor%20real%20world%20domains%20that%20impose%20privacy%20and%20where%20the%20messages%20may%20not%20reach%0Atheir%20recipients.%20We%20evaluate%20DNA-MARL%20across%20benchmark%20MARL%20scenarios.%20Our%0Aresults%20highlight%20the%20superior%20performance%20of%20DNA-MARL%20over%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08778v1&entry.124074799=Read"},
{"title": "Constrained Latent Action Policies for Model-Based Offline Reinforcement\n  Learning", "author": "Marvin Alles and Philip Becker-Ehmck and Patrick van der Smagt and Maximilian Karl", "abstract": "  In offline reinforcement learning, a policy is learned using a static dataset\nin the absence of costly feedback from the environment. In contrast to the\nonline setting, only using static datasets poses additional challenges, such as\npolicies generating out-of-distribution samples. Model-based offline\nreinforcement learning methods try to overcome these by learning a model of the\nunderlying dynamics of the environment and using it to guide policy search. It\nis beneficial but, with limited datasets, errors in the model and the issue of\nvalue overestimation among out-of-distribution states can worsen performance.\nCurrent model-based methods apply some notion of conservatism to the Bellman\nupdate, often implemented using uncertainty estimation derived from model\nensembles. In this paper, we propose Constrained Latent Action Policies (C-LAP)\nwhich learns a generative model of the joint distribution of observations and\nactions. We cast policy learning as a constrained objective to always stay\nwithin the support of the latent action distribution, and use the generative\ncapabilities of the model to impose an implicit constraint on the generated\nactions. Thereby eliminating the need to use additional uncertainty penalties\non the Bellman update and significantly decreasing the number of gradient steps\nrequired to learn a policy. We empirically evaluate C-LAP on the D4RL and\nV-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art\nmethods, especially outperforming on datasets with visual observations.\n", "link": "http://arxiv.org/abs/2411.04562v2", "date": "2025-01-15", "relevancy": 1.9856, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5239}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4915}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Latent%20Action%20Policies%20for%20Model-Based%20Offline%20Reinforcement%0A%20%20Learning&body=Title%3A%20Constrained%20Latent%20Action%20Policies%20for%20Model-Based%20Offline%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Marvin%20Alles%20and%20Philip%20Becker-Ehmck%20and%20Patrick%20van%20der%20Smagt%20and%20Maximilian%20Karl%0AAbstract%3A%20%20%20In%20offline%20reinforcement%20learning%2C%20a%20policy%20is%20learned%20using%20a%20static%20dataset%0Ain%20the%20absence%20of%20costly%20feedback%20from%20the%20environment.%20In%20contrast%20to%20the%0Aonline%20setting%2C%20only%20using%20static%20datasets%20poses%20additional%20challenges%2C%20such%20as%0Apolicies%20generating%20out-of-distribution%20samples.%20Model-based%20offline%0Areinforcement%20learning%20methods%20try%20to%20overcome%20these%20by%20learning%20a%20model%20of%20the%0Aunderlying%20dynamics%20of%20the%20environment%20and%20using%20it%20to%20guide%20policy%20search.%20It%0Ais%20beneficial%20but%2C%20with%20limited%20datasets%2C%20errors%20in%20the%20model%20and%20the%20issue%20of%0Avalue%20overestimation%20among%20out-of-distribution%20states%20can%20worsen%20performance.%0ACurrent%20model-based%20methods%20apply%20some%20notion%20of%20conservatism%20to%20the%20Bellman%0Aupdate%2C%20often%20implemented%20using%20uncertainty%20estimation%20derived%20from%20model%0Aensembles.%20In%20this%20paper%2C%20we%20propose%20Constrained%20Latent%20Action%20Policies%20%28C-LAP%29%0Awhich%20learns%20a%20generative%20model%20of%20the%20joint%20distribution%20of%20observations%20and%0Aactions.%20We%20cast%20policy%20learning%20as%20a%20constrained%20objective%20to%20always%20stay%0Awithin%20the%20support%20of%20the%20latent%20action%20distribution%2C%20and%20use%20the%20generative%0Acapabilities%20of%20the%20model%20to%20impose%20an%20implicit%20constraint%20on%20the%20generated%0Aactions.%20Thereby%20eliminating%20the%20need%20to%20use%20additional%20uncertainty%20penalties%0Aon%20the%20Bellman%20update%20and%20significantly%20decreasing%20the%20number%20of%20gradient%20steps%0Arequired%20to%20learn%20a%20policy.%20We%20empirically%20evaluate%20C-LAP%20on%20the%20D4RL%20and%0AV-D4RL%20benchmark%2C%20and%20show%20that%20C-LAP%20is%20competitive%20to%20state-of-the-art%0Amethods%2C%20especially%20outperforming%20on%20datasets%20with%20visual%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Latent%2520Action%2520Policies%2520for%2520Model-Based%2520Offline%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DMarvin%2520Alles%2520and%2520Philip%2520Becker-Ehmck%2520and%2520Patrick%2520van%2520der%2520Smagt%2520and%2520Maximilian%2520Karl%26entry.1292438233%3D%2520%2520In%2520offline%2520reinforcement%2520learning%252C%2520a%2520policy%2520is%2520learned%2520using%2520a%2520static%2520dataset%250Ain%2520the%2520absence%2520of%2520costly%2520feedback%2520from%2520the%2520environment.%2520In%2520contrast%2520to%2520the%250Aonline%2520setting%252C%2520only%2520using%2520static%2520datasets%2520poses%2520additional%2520challenges%252C%2520such%2520as%250Apolicies%2520generating%2520out-of-distribution%2520samples.%2520Model-based%2520offline%250Areinforcement%2520learning%2520methods%2520try%2520to%2520overcome%2520these%2520by%2520learning%2520a%2520model%2520of%2520the%250Aunderlying%2520dynamics%2520of%2520the%2520environment%2520and%2520using%2520it%2520to%2520guide%2520policy%2520search.%2520It%250Ais%2520beneficial%2520but%252C%2520with%2520limited%2520datasets%252C%2520errors%2520in%2520the%2520model%2520and%2520the%2520issue%2520of%250Avalue%2520overestimation%2520among%2520out-of-distribution%2520states%2520can%2520worsen%2520performance.%250ACurrent%2520model-based%2520methods%2520apply%2520some%2520notion%2520of%2520conservatism%2520to%2520the%2520Bellman%250Aupdate%252C%2520often%2520implemented%2520using%2520uncertainty%2520estimation%2520derived%2520from%2520model%250Aensembles.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Constrained%2520Latent%2520Action%2520Policies%2520%2528C-LAP%2529%250Awhich%2520learns%2520a%2520generative%2520model%2520of%2520the%2520joint%2520distribution%2520of%2520observations%2520and%250Aactions.%2520We%2520cast%2520policy%2520learning%2520as%2520a%2520constrained%2520objective%2520to%2520always%2520stay%250Awithin%2520the%2520support%2520of%2520the%2520latent%2520action%2520distribution%252C%2520and%2520use%2520the%2520generative%250Acapabilities%2520of%2520the%2520model%2520to%2520impose%2520an%2520implicit%2520constraint%2520on%2520the%2520generated%250Aactions.%2520Thereby%2520eliminating%2520the%2520need%2520to%2520use%2520additional%2520uncertainty%2520penalties%250Aon%2520the%2520Bellman%2520update%2520and%2520significantly%2520decreasing%2520the%2520number%2520of%2520gradient%2520steps%250Arequired%2520to%2520learn%2520a%2520policy.%2520We%2520empirically%2520evaluate%2520C-LAP%2520on%2520the%2520D4RL%2520and%250AV-D4RL%2520benchmark%252C%2520and%2520show%2520that%2520C-LAP%2520is%2520competitive%2520to%2520state-of-the-art%250Amethods%252C%2520especially%2520outperforming%2520on%2520datasets%2520with%2520visual%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Latent%20Action%20Policies%20for%20Model-Based%20Offline%20Reinforcement%0A%20%20Learning&entry.906535625=Marvin%20Alles%20and%20Philip%20Becker-Ehmck%20and%20Patrick%20van%20der%20Smagt%20and%20Maximilian%20Karl&entry.1292438233=%20%20In%20offline%20reinforcement%20learning%2C%20a%20policy%20is%20learned%20using%20a%20static%20dataset%0Ain%20the%20absence%20of%20costly%20feedback%20from%20the%20environment.%20In%20contrast%20to%20the%0Aonline%20setting%2C%20only%20using%20static%20datasets%20poses%20additional%20challenges%2C%20such%20as%0Apolicies%20generating%20out-of-distribution%20samples.%20Model-based%20offline%0Areinforcement%20learning%20methods%20try%20to%20overcome%20these%20by%20learning%20a%20model%20of%20the%0Aunderlying%20dynamics%20of%20the%20environment%20and%20using%20it%20to%20guide%20policy%20search.%20It%0Ais%20beneficial%20but%2C%20with%20limited%20datasets%2C%20errors%20in%20the%20model%20and%20the%20issue%20of%0Avalue%20overestimation%20among%20out-of-distribution%20states%20can%20worsen%20performance.%0ACurrent%20model-based%20methods%20apply%20some%20notion%20of%20conservatism%20to%20the%20Bellman%0Aupdate%2C%20often%20implemented%20using%20uncertainty%20estimation%20derived%20from%20model%0Aensembles.%20In%20this%20paper%2C%20we%20propose%20Constrained%20Latent%20Action%20Policies%20%28C-LAP%29%0Awhich%20learns%20a%20generative%20model%20of%20the%20joint%20distribution%20of%20observations%20and%0Aactions.%20We%20cast%20policy%20learning%20as%20a%20constrained%20objective%20to%20always%20stay%0Awithin%20the%20support%20of%20the%20latent%20action%20distribution%2C%20and%20use%20the%20generative%0Acapabilities%20of%20the%20model%20to%20impose%20an%20implicit%20constraint%20on%20the%20generated%0Aactions.%20Thereby%20eliminating%20the%20need%20to%20use%20additional%20uncertainty%20penalties%0Aon%20the%20Bellman%20update%20and%20significantly%20decreasing%20the%20number%20of%20gradient%20steps%0Arequired%20to%20learn%20a%20policy.%20We%20empirically%20evaluate%20C-LAP%20on%20the%20D4RL%20and%0AV-D4RL%20benchmark%2C%20and%20show%20that%20C-LAP%20is%20competitive%20to%20state-of-the-art%0Amethods%2C%20especially%20outperforming%20on%20datasets%20with%20visual%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04562v2&entry.124074799=Read"},
{"title": "MADiff: Text-Guided Fashion Image Editing with Mask Prediction and\n  Attention-Enhanced Diffusion", "author": "Zechao Zhan and Dehong Gao and Jinxia Zhang and Jiale Huang and Yang Hu and Xin Wang", "abstract": "  Text-guided image editing model has achieved great success in general domain.\nHowever, directly applying these models to the fashion domain may encounter two\nissues: (1) Inaccurate localization of editing region; (2) Weak editing\nmagnitude. To address these issues, the MADiff model is proposed. Specifically,\nto more accurately identify editing region, the MaskNet is proposed, in which\nthe foreground region, densepose and mask prompts from large language model are\nfed into a lightweight UNet to predict the mask for editing region. To\nstrengthen the editing magnitude, the Attention-Enhanced Diffusion Model is\nproposed, where the noise map, attention map, and the mask from MaskNet are fed\ninto the proposed Attention Processor to produce a refined noise map. By\nintegrating the refined noise map into the diffusion model, the edited image\ncan better align with the target prompt. Given the absence of benchmarks in\nfashion image editing, we constructed a dataset named Fashion-E, comprising\n28390 image-text pairs in the training set, and 2639 image-text pairs for four\ntypes of fashion tasks in the evaluation set. Extensive experiments on\nFashion-E demonstrate that our proposed method can accurately predict the mask\nof editing region and significantly enhance editing magnitude in fashion image\nediting compared to the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2412.20062v2", "date": "2025-01-15", "relevancy": 1.9853, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7731}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6465}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MADiff%3A%20Text-Guided%20Fashion%20Image%20Editing%20with%20Mask%20Prediction%20and%0A%20%20Attention-Enhanced%20Diffusion&body=Title%3A%20MADiff%3A%20Text-Guided%20Fashion%20Image%20Editing%20with%20Mask%20Prediction%20and%0A%20%20Attention-Enhanced%20Diffusion%0AAuthor%3A%20Zechao%20Zhan%20and%20Dehong%20Gao%20and%20Jinxia%20Zhang%20and%20Jiale%20Huang%20and%20Yang%20Hu%20and%20Xin%20Wang%0AAbstract%3A%20%20%20Text-guided%20image%20editing%20model%20has%20achieved%20great%20success%20in%20general%20domain.%0AHowever%2C%20directly%20applying%20these%20models%20to%20the%20fashion%20domain%20may%20encounter%20two%0Aissues%3A%20%281%29%20Inaccurate%20localization%20of%20editing%20region%3B%20%282%29%20Weak%20editing%0Amagnitude.%20To%20address%20these%20issues%2C%20the%20MADiff%20model%20is%20proposed.%20Specifically%2C%0Ato%20more%20accurately%20identify%20editing%20region%2C%20the%20MaskNet%20is%20proposed%2C%20in%20which%0Athe%20foreground%20region%2C%20densepose%20and%20mask%20prompts%20from%20large%20language%20model%20are%0Afed%20into%20a%20lightweight%20UNet%20to%20predict%20the%20mask%20for%20editing%20region.%20To%0Astrengthen%20the%20editing%20magnitude%2C%20the%20Attention-Enhanced%20Diffusion%20Model%20is%0Aproposed%2C%20where%20the%20noise%20map%2C%20attention%20map%2C%20and%20the%20mask%20from%20MaskNet%20are%20fed%0Ainto%20the%20proposed%20Attention%20Processor%20to%20produce%20a%20refined%20noise%20map.%20By%0Aintegrating%20the%20refined%20noise%20map%20into%20the%20diffusion%20model%2C%20the%20edited%20image%0Acan%20better%20align%20with%20the%20target%20prompt.%20Given%20the%20absence%20of%20benchmarks%20in%0Afashion%20image%20editing%2C%20we%20constructed%20a%20dataset%20named%20Fashion-E%2C%20comprising%0A28390%20image-text%20pairs%20in%20the%20training%20set%2C%20and%202639%20image-text%20pairs%20for%20four%0Atypes%20of%20fashion%20tasks%20in%20the%20evaluation%20set.%20Extensive%20experiments%20on%0AFashion-E%20demonstrate%20that%20our%20proposed%20method%20can%20accurately%20predict%20the%20mask%0Aof%20editing%20region%20and%20significantly%20enhance%20editing%20magnitude%20in%20fashion%20image%0Aediting%20compared%20to%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20062v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMADiff%253A%2520Text-Guided%2520Fashion%2520Image%2520Editing%2520with%2520Mask%2520Prediction%2520and%250A%2520%2520Attention-Enhanced%2520Diffusion%26entry.906535625%3DZechao%2520Zhan%2520and%2520Dehong%2520Gao%2520and%2520Jinxia%2520Zhang%2520and%2520Jiale%2520Huang%2520and%2520Yang%2520Hu%2520and%2520Xin%2520Wang%26entry.1292438233%3D%2520%2520Text-guided%2520image%2520editing%2520model%2520has%2520achieved%2520great%2520success%2520in%2520general%2520domain.%250AHowever%252C%2520directly%2520applying%2520these%2520models%2520to%2520the%2520fashion%2520domain%2520may%2520encounter%2520two%250Aissues%253A%2520%25281%2529%2520Inaccurate%2520localization%2520of%2520editing%2520region%253B%2520%25282%2529%2520Weak%2520editing%250Amagnitude.%2520To%2520address%2520these%2520issues%252C%2520the%2520MADiff%2520model%2520is%2520proposed.%2520Specifically%252C%250Ato%2520more%2520accurately%2520identify%2520editing%2520region%252C%2520the%2520MaskNet%2520is%2520proposed%252C%2520in%2520which%250Athe%2520foreground%2520region%252C%2520densepose%2520and%2520mask%2520prompts%2520from%2520large%2520language%2520model%2520are%250Afed%2520into%2520a%2520lightweight%2520UNet%2520to%2520predict%2520the%2520mask%2520for%2520editing%2520region.%2520To%250Astrengthen%2520the%2520editing%2520magnitude%252C%2520the%2520Attention-Enhanced%2520Diffusion%2520Model%2520is%250Aproposed%252C%2520where%2520the%2520noise%2520map%252C%2520attention%2520map%252C%2520and%2520the%2520mask%2520from%2520MaskNet%2520are%2520fed%250Ainto%2520the%2520proposed%2520Attention%2520Processor%2520to%2520produce%2520a%2520refined%2520noise%2520map.%2520By%250Aintegrating%2520the%2520refined%2520noise%2520map%2520into%2520the%2520diffusion%2520model%252C%2520the%2520edited%2520image%250Acan%2520better%2520align%2520with%2520the%2520target%2520prompt.%2520Given%2520the%2520absence%2520of%2520benchmarks%2520in%250Afashion%2520image%2520editing%252C%2520we%2520constructed%2520a%2520dataset%2520named%2520Fashion-E%252C%2520comprising%250A28390%2520image-text%2520pairs%2520in%2520the%2520training%2520set%252C%2520and%25202639%2520image-text%2520pairs%2520for%2520four%250Atypes%2520of%2520fashion%2520tasks%2520in%2520the%2520evaluation%2520set.%2520Extensive%2520experiments%2520on%250AFashion-E%2520demonstrate%2520that%2520our%2520proposed%2520method%2520can%2520accurately%2520predict%2520the%2520mask%250Aof%2520editing%2520region%2520and%2520significantly%2520enhance%2520editing%2520magnitude%2520in%2520fashion%2520image%250Aediting%2520compared%2520to%2520the%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20062v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MADiff%3A%20Text-Guided%20Fashion%20Image%20Editing%20with%20Mask%20Prediction%20and%0A%20%20Attention-Enhanced%20Diffusion&entry.906535625=Zechao%20Zhan%20and%20Dehong%20Gao%20and%20Jinxia%20Zhang%20and%20Jiale%20Huang%20and%20Yang%20Hu%20and%20Xin%20Wang&entry.1292438233=%20%20Text-guided%20image%20editing%20model%20has%20achieved%20great%20success%20in%20general%20domain.%0AHowever%2C%20directly%20applying%20these%20models%20to%20the%20fashion%20domain%20may%20encounter%20two%0Aissues%3A%20%281%29%20Inaccurate%20localization%20of%20editing%20region%3B%20%282%29%20Weak%20editing%0Amagnitude.%20To%20address%20these%20issues%2C%20the%20MADiff%20model%20is%20proposed.%20Specifically%2C%0Ato%20more%20accurately%20identify%20editing%20region%2C%20the%20MaskNet%20is%20proposed%2C%20in%20which%0Athe%20foreground%20region%2C%20densepose%20and%20mask%20prompts%20from%20large%20language%20model%20are%0Afed%20into%20a%20lightweight%20UNet%20to%20predict%20the%20mask%20for%20editing%20region.%20To%0Astrengthen%20the%20editing%20magnitude%2C%20the%20Attention-Enhanced%20Diffusion%20Model%20is%0Aproposed%2C%20where%20the%20noise%20map%2C%20attention%20map%2C%20and%20the%20mask%20from%20MaskNet%20are%20fed%0Ainto%20the%20proposed%20Attention%20Processor%20to%20produce%20a%20refined%20noise%20map.%20By%0Aintegrating%20the%20refined%20noise%20map%20into%20the%20diffusion%20model%2C%20the%20edited%20image%0Acan%20better%20align%20with%20the%20target%20prompt.%20Given%20the%20absence%20of%20benchmarks%20in%0Afashion%20image%20editing%2C%20we%20constructed%20a%20dataset%20named%20Fashion-E%2C%20comprising%0A28390%20image-text%20pairs%20in%20the%20training%20set%2C%20and%202639%20image-text%20pairs%20for%20four%0Atypes%20of%20fashion%20tasks%20in%20the%20evaluation%20set.%20Extensive%20experiments%20on%0AFashion-E%20demonstrate%20that%20our%20proposed%20method%20can%20accurately%20predict%20the%20mask%0Aof%20editing%20region%20and%20significantly%20enhance%20editing%20magnitude%20in%20fashion%20image%0Aediting%20compared%20to%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20062v2&entry.124074799=Read"},
{"title": "Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex\n  and Professional Sports", "author": "Haopeng Li and Andong Deng and Jun Liu and Hossein Rahmani and Yulan Guo and Bernt Schiele and Mohammed Bennamoun and Qiuhong Ke", "abstract": "  Reasoning over sports videos for question answering is an important task with\nnumerous applications, such as player training and information retrieval.\nHowever, this task has not been explored due to the lack of relevant datasets\nand the challenging nature it presents. Most datasets for video question\nanswering (VideoQA) focus mainly on general and coarse-grained understanding of\ndaily-life videos, which is not applicable to sports scenarios requiring\nprofessional action understanding and fine-grained motion analysis. In this\npaper, we introduce the first dataset, named Sports-QA, specifically designed\nfor the sports VideoQA task. The Sports-QA dataset includes various types of\nquestions, such as descriptions, chronologies, causalities, and counterfactual\nconditions, covering multiple sports. Furthermore, to address the\ncharacteristics of the sports VideoQA task, we propose a new Auto-Focus\nTransformer (AFT) capable of automatically focusing on particular scales of\ntemporal information for question answering. We conduct extensive experiments\non Sports-QA, including baseline studies and the evaluation of different\nmethods. The results demonstrate that our AFT achieves state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2401.01505v4", "date": "2025-01-15", "relevancy": 1.9827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sports-QA%3A%20A%20Large-Scale%20Video%20Question%20Answering%20Benchmark%20for%20Complex%0A%20%20and%20Professional%20Sports&body=Title%3A%20Sports-QA%3A%20A%20Large-Scale%20Video%20Question%20Answering%20Benchmark%20for%20Complex%0A%20%20and%20Professional%20Sports%0AAuthor%3A%20Haopeng%20Li%20and%20Andong%20Deng%20and%20Jun%20Liu%20and%20Hossein%20Rahmani%20and%20Yulan%20Guo%20and%20Bernt%20Schiele%20and%20Mohammed%20Bennamoun%20and%20Qiuhong%20Ke%0AAbstract%3A%20%20%20Reasoning%20over%20sports%20videos%20for%20question%20answering%20is%20an%20important%20task%20with%0Anumerous%20applications%2C%20such%20as%20player%20training%20and%20information%20retrieval.%0AHowever%2C%20this%20task%20has%20not%20been%20explored%20due%20to%20the%20lack%20of%20relevant%20datasets%0Aand%20the%20challenging%20nature%20it%20presents.%20Most%20datasets%20for%20video%20question%0Aanswering%20%28VideoQA%29%20focus%20mainly%20on%20general%20and%20coarse-grained%20understanding%20of%0Adaily-life%20videos%2C%20which%20is%20not%20applicable%20to%20sports%20scenarios%20requiring%0Aprofessional%20action%20understanding%20and%20fine-grained%20motion%20analysis.%20In%20this%0Apaper%2C%20we%20introduce%20the%20first%20dataset%2C%20named%20Sports-QA%2C%20specifically%20designed%0Afor%20the%20sports%20VideoQA%20task.%20The%20Sports-QA%20dataset%20includes%20various%20types%20of%0Aquestions%2C%20such%20as%20descriptions%2C%20chronologies%2C%20causalities%2C%20and%20counterfactual%0Aconditions%2C%20covering%20multiple%20sports.%20Furthermore%2C%20to%20address%20the%0Acharacteristics%20of%20the%20sports%20VideoQA%20task%2C%20we%20propose%20a%20new%20Auto-Focus%0ATransformer%20%28AFT%29%20capable%20of%20automatically%20focusing%20on%20particular%20scales%20of%0Atemporal%20information%20for%20question%20answering.%20We%20conduct%20extensive%20experiments%0Aon%20Sports-QA%2C%20including%20baseline%20studies%20and%20the%20evaluation%20of%20different%0Amethods.%20The%20results%20demonstrate%20that%20our%20AFT%20achieves%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01505v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSports-QA%253A%2520A%2520Large-Scale%2520Video%2520Question%2520Answering%2520Benchmark%2520for%2520Complex%250A%2520%2520and%2520Professional%2520Sports%26entry.906535625%3DHaopeng%2520Li%2520and%2520Andong%2520Deng%2520and%2520Jun%2520Liu%2520and%2520Hossein%2520Rahmani%2520and%2520Yulan%2520Guo%2520and%2520Bernt%2520Schiele%2520and%2520Mohammed%2520Bennamoun%2520and%2520Qiuhong%2520Ke%26entry.1292438233%3D%2520%2520Reasoning%2520over%2520sports%2520videos%2520for%2520question%2520answering%2520is%2520an%2520important%2520task%2520with%250Anumerous%2520applications%252C%2520such%2520as%2520player%2520training%2520and%2520information%2520retrieval.%250AHowever%252C%2520this%2520task%2520has%2520not%2520been%2520explored%2520due%2520to%2520the%2520lack%2520of%2520relevant%2520datasets%250Aand%2520the%2520challenging%2520nature%2520it%2520presents.%2520Most%2520datasets%2520for%2520video%2520question%250Aanswering%2520%2528VideoQA%2529%2520focus%2520mainly%2520on%2520general%2520and%2520coarse-grained%2520understanding%2520of%250Adaily-life%2520videos%252C%2520which%2520is%2520not%2520applicable%2520to%2520sports%2520scenarios%2520requiring%250Aprofessional%2520action%2520understanding%2520and%2520fine-grained%2520motion%2520analysis.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520the%2520first%2520dataset%252C%2520named%2520Sports-QA%252C%2520specifically%2520designed%250Afor%2520the%2520sports%2520VideoQA%2520task.%2520The%2520Sports-QA%2520dataset%2520includes%2520various%2520types%2520of%250Aquestions%252C%2520such%2520as%2520descriptions%252C%2520chronologies%252C%2520causalities%252C%2520and%2520counterfactual%250Aconditions%252C%2520covering%2520multiple%2520sports.%2520Furthermore%252C%2520to%2520address%2520the%250Acharacteristics%2520of%2520the%2520sports%2520VideoQA%2520task%252C%2520we%2520propose%2520a%2520new%2520Auto-Focus%250ATransformer%2520%2528AFT%2529%2520capable%2520of%2520automatically%2520focusing%2520on%2520particular%2520scales%2520of%250Atemporal%2520information%2520for%2520question%2520answering.%2520We%2520conduct%2520extensive%2520experiments%250Aon%2520Sports-QA%252C%2520including%2520baseline%2520studies%2520and%2520the%2520evaluation%2520of%2520different%250Amethods.%2520The%2520results%2520demonstrate%2520that%2520our%2520AFT%2520achieves%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01505v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sports-QA%3A%20A%20Large-Scale%20Video%20Question%20Answering%20Benchmark%20for%20Complex%0A%20%20and%20Professional%20Sports&entry.906535625=Haopeng%20Li%20and%20Andong%20Deng%20and%20Jun%20Liu%20and%20Hossein%20Rahmani%20and%20Yulan%20Guo%20and%20Bernt%20Schiele%20and%20Mohammed%20Bennamoun%20and%20Qiuhong%20Ke&entry.1292438233=%20%20Reasoning%20over%20sports%20videos%20for%20question%20answering%20is%20an%20important%20task%20with%0Anumerous%20applications%2C%20such%20as%20player%20training%20and%20information%20retrieval.%0AHowever%2C%20this%20task%20has%20not%20been%20explored%20due%20to%20the%20lack%20of%20relevant%20datasets%0Aand%20the%20challenging%20nature%20it%20presents.%20Most%20datasets%20for%20video%20question%0Aanswering%20%28VideoQA%29%20focus%20mainly%20on%20general%20and%20coarse-grained%20understanding%20of%0Adaily-life%20videos%2C%20which%20is%20not%20applicable%20to%20sports%20scenarios%20requiring%0Aprofessional%20action%20understanding%20and%20fine-grained%20motion%20analysis.%20In%20this%0Apaper%2C%20we%20introduce%20the%20first%20dataset%2C%20named%20Sports-QA%2C%20specifically%20designed%0Afor%20the%20sports%20VideoQA%20task.%20The%20Sports-QA%20dataset%20includes%20various%20types%20of%0Aquestions%2C%20such%20as%20descriptions%2C%20chronologies%2C%20causalities%2C%20and%20counterfactual%0Aconditions%2C%20covering%20multiple%20sports.%20Furthermore%2C%20to%20address%20the%0Acharacteristics%20of%20the%20sports%20VideoQA%20task%2C%20we%20propose%20a%20new%20Auto-Focus%0ATransformer%20%28AFT%29%20capable%20of%20automatically%20focusing%20on%20particular%20scales%20of%0Atemporal%20information%20for%20question%20answering.%20We%20conduct%20extensive%20experiments%0Aon%20Sports-QA%2C%20including%20baseline%20studies%20and%20the%20evaluation%20of%20different%0Amethods.%20The%20results%20demonstrate%20that%20our%20AFT%20achieves%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01505v4&entry.124074799=Read"},
{"title": "Sparse Low-Ranked Self-Attention Transformer for Remaining Useful\n  Lifetime Prediction of Optical Fiber Amplifiers", "author": "Dominic Schneider and Lutz Rapp", "abstract": "  Optical fiber amplifiers are key elements in present optical networks.\nFailures of these components result in high financial loss of income of the\nnetwork operator as the communication traffic over an affected link is\ninterrupted. Applying Remaining useful lifetime (RUL) prediction in the context\nof Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming\nsystem failures at an early stage, so that network outages can be minimized\nthrough planning of targeted maintenance actions, ensures reliability and\nsafety. Optical fiber amplifier are complex systems, that work under various\noperating conditions, which makes correct forecasting a difficult task.\nIncreased monitoring capabilities of systems results in datasets that\nfacilitate the application of data-driven RUL prediction methods. Deep learning\nmodels in particular have shown good performance, but generalization based on\ncomparatively small datasets for RUL prediction is difficult. In this paper, we\npropose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL\nprediction method. SLAT is based on an encoder-decoder architecture, wherein\ntwo parallel working encoders extract features for sensors and time steps. By\nutilizing the self-attention mechanism, long-term dependencies can be learned\nfrom long sequences. The implementation of sparsity in the attention matrix and\na low-rank parametrization reduce overfitting and increase generalization.\nExperimental application to optical fiber amplifiers exemplified on EDFA, as\nwell as a reference dataset from turbofan engines, shows that SLAT outperforms\nthe state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2409.14378v3", "date": "2025-01-15", "relevancy": 1.9753, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5086}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4919}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Low-Ranked%20Self-Attention%20Transformer%20for%20Remaining%20Useful%0A%20%20Lifetime%20Prediction%20of%20Optical%20Fiber%20Amplifiers&body=Title%3A%20Sparse%20Low-Ranked%20Self-Attention%20Transformer%20for%20Remaining%20Useful%0A%20%20Lifetime%20Prediction%20of%20Optical%20Fiber%20Amplifiers%0AAuthor%3A%20Dominic%20Schneider%20and%20Lutz%20Rapp%0AAbstract%3A%20%20%20Optical%20fiber%20amplifiers%20are%20key%20elements%20in%20present%20optical%20networks.%0AFailures%20of%20these%20components%20result%20in%20high%20financial%20loss%20of%20income%20of%20the%0Anetwork%20operator%20as%20the%20communication%20traffic%20over%20an%20affected%20link%20is%0Ainterrupted.%20Applying%20Remaining%20useful%20lifetime%20%28RUL%29%20prediction%20in%20the%20context%0Aof%20Predictive%20Maintenance%20%28PdM%29%20to%20optical%20fiber%20amplifiers%20to%20predict%20upcoming%0Asystem%20failures%20at%20an%20early%20stage%2C%20so%20that%20network%20outages%20can%20be%20minimized%0Athrough%20planning%20of%20targeted%20maintenance%20actions%2C%20ensures%20reliability%20and%0Asafety.%20Optical%20fiber%20amplifier%20are%20complex%20systems%2C%20that%20work%20under%20various%0Aoperating%20conditions%2C%20which%20makes%20correct%20forecasting%20a%20difficult%20task.%0AIncreased%20monitoring%20capabilities%20of%20systems%20results%20in%20datasets%20that%0Afacilitate%20the%20application%20of%20data-driven%20RUL%20prediction%20methods.%20Deep%20learning%0Amodels%20in%20particular%20have%20shown%20good%20performance%2C%20but%20generalization%20based%20on%0Acomparatively%20small%20datasets%20for%20RUL%20prediction%20is%20difficult.%20In%20this%20paper%2C%20we%0Apropose%20Sparse%20Low-ranked%20self-Attention%20Transformer%20%28SLAT%29%20as%20a%20novel%20RUL%0Aprediction%20method.%20SLAT%20is%20based%20on%20an%20encoder-decoder%20architecture%2C%20wherein%0Atwo%20parallel%20working%20encoders%20extract%20features%20for%20sensors%20and%20time%20steps.%20By%0Autilizing%20the%20self-attention%20mechanism%2C%20long-term%20dependencies%20can%20be%20learned%0Afrom%20long%20sequences.%20The%20implementation%20of%20sparsity%20in%20the%20attention%20matrix%20and%0Aa%20low-rank%20parametrization%20reduce%20overfitting%20and%20increase%20generalization.%0AExperimental%20application%20to%20optical%20fiber%20amplifiers%20exemplified%20on%20EDFA%2C%20as%0Awell%20as%20a%20reference%20dataset%20from%20turbofan%20engines%2C%20shows%20that%20SLAT%20outperforms%0Athe%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14378v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Low-Ranked%2520Self-Attention%2520Transformer%2520for%2520Remaining%2520Useful%250A%2520%2520Lifetime%2520Prediction%2520of%2520Optical%2520Fiber%2520Amplifiers%26entry.906535625%3DDominic%2520Schneider%2520and%2520Lutz%2520Rapp%26entry.1292438233%3D%2520%2520Optical%2520fiber%2520amplifiers%2520are%2520key%2520elements%2520in%2520present%2520optical%2520networks.%250AFailures%2520of%2520these%2520components%2520result%2520in%2520high%2520financial%2520loss%2520of%2520income%2520of%2520the%250Anetwork%2520operator%2520as%2520the%2520communication%2520traffic%2520over%2520an%2520affected%2520link%2520is%250Ainterrupted.%2520Applying%2520Remaining%2520useful%2520lifetime%2520%2528RUL%2529%2520prediction%2520in%2520the%2520context%250Aof%2520Predictive%2520Maintenance%2520%2528PdM%2529%2520to%2520optical%2520fiber%2520amplifiers%2520to%2520predict%2520upcoming%250Asystem%2520failures%2520at%2520an%2520early%2520stage%252C%2520so%2520that%2520network%2520outages%2520can%2520be%2520minimized%250Athrough%2520planning%2520of%2520targeted%2520maintenance%2520actions%252C%2520ensures%2520reliability%2520and%250Asafety.%2520Optical%2520fiber%2520amplifier%2520are%2520complex%2520systems%252C%2520that%2520work%2520under%2520various%250Aoperating%2520conditions%252C%2520which%2520makes%2520correct%2520forecasting%2520a%2520difficult%2520task.%250AIncreased%2520monitoring%2520capabilities%2520of%2520systems%2520results%2520in%2520datasets%2520that%250Afacilitate%2520the%2520application%2520of%2520data-driven%2520RUL%2520prediction%2520methods.%2520Deep%2520learning%250Amodels%2520in%2520particular%2520have%2520shown%2520good%2520performance%252C%2520but%2520generalization%2520based%2520on%250Acomparatively%2520small%2520datasets%2520for%2520RUL%2520prediction%2520is%2520difficult.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Sparse%2520Low-ranked%2520self-Attention%2520Transformer%2520%2528SLAT%2529%2520as%2520a%2520novel%2520RUL%250Aprediction%2520method.%2520SLAT%2520is%2520based%2520on%2520an%2520encoder-decoder%2520architecture%252C%2520wherein%250Atwo%2520parallel%2520working%2520encoders%2520extract%2520features%2520for%2520sensors%2520and%2520time%2520steps.%2520By%250Autilizing%2520the%2520self-attention%2520mechanism%252C%2520long-term%2520dependencies%2520can%2520be%2520learned%250Afrom%2520long%2520sequences.%2520The%2520implementation%2520of%2520sparsity%2520in%2520the%2520attention%2520matrix%2520and%250Aa%2520low-rank%2520parametrization%2520reduce%2520overfitting%2520and%2520increase%2520generalization.%250AExperimental%2520application%2520to%2520optical%2520fiber%2520amplifiers%2520exemplified%2520on%2520EDFA%252C%2520as%250Awell%2520as%2520a%2520reference%2520dataset%2520from%2520turbofan%2520engines%252C%2520shows%2520that%2520SLAT%2520outperforms%250Athe%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14378v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Low-Ranked%20Self-Attention%20Transformer%20for%20Remaining%20Useful%0A%20%20Lifetime%20Prediction%20of%20Optical%20Fiber%20Amplifiers&entry.906535625=Dominic%20Schneider%20and%20Lutz%20Rapp&entry.1292438233=%20%20Optical%20fiber%20amplifiers%20are%20key%20elements%20in%20present%20optical%20networks.%0AFailures%20of%20these%20components%20result%20in%20high%20financial%20loss%20of%20income%20of%20the%0Anetwork%20operator%20as%20the%20communication%20traffic%20over%20an%20affected%20link%20is%0Ainterrupted.%20Applying%20Remaining%20useful%20lifetime%20%28RUL%29%20prediction%20in%20the%20context%0Aof%20Predictive%20Maintenance%20%28PdM%29%20to%20optical%20fiber%20amplifiers%20to%20predict%20upcoming%0Asystem%20failures%20at%20an%20early%20stage%2C%20so%20that%20network%20outages%20can%20be%20minimized%0Athrough%20planning%20of%20targeted%20maintenance%20actions%2C%20ensures%20reliability%20and%0Asafety.%20Optical%20fiber%20amplifier%20are%20complex%20systems%2C%20that%20work%20under%20various%0Aoperating%20conditions%2C%20which%20makes%20correct%20forecasting%20a%20difficult%20task.%0AIncreased%20monitoring%20capabilities%20of%20systems%20results%20in%20datasets%20that%0Afacilitate%20the%20application%20of%20data-driven%20RUL%20prediction%20methods.%20Deep%20learning%0Amodels%20in%20particular%20have%20shown%20good%20performance%2C%20but%20generalization%20based%20on%0Acomparatively%20small%20datasets%20for%20RUL%20prediction%20is%20difficult.%20In%20this%20paper%2C%20we%0Apropose%20Sparse%20Low-ranked%20self-Attention%20Transformer%20%28SLAT%29%20as%20a%20novel%20RUL%0Aprediction%20method.%20SLAT%20is%20based%20on%20an%20encoder-decoder%20architecture%2C%20wherein%0Atwo%20parallel%20working%20encoders%20extract%20features%20for%20sensors%20and%20time%20steps.%20By%0Autilizing%20the%20self-attention%20mechanism%2C%20long-term%20dependencies%20can%20be%20learned%0Afrom%20long%20sequences.%20The%20implementation%20of%20sparsity%20in%20the%20attention%20matrix%20and%0Aa%20low-rank%20parametrization%20reduce%20overfitting%20and%20increase%20generalization.%0AExperimental%20application%20to%20optical%20fiber%20amplifiers%20exemplified%20on%20EDFA%2C%20as%0Awell%20as%20a%20reference%20dataset%20from%20turbofan%20engines%2C%20shows%20that%20SLAT%20outperforms%0Athe%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14378v3&entry.124074799=Read"},
{"title": "Let Network Decide What to Learn: Symbolic Music Understanding Model\n  Based on Large-scale Adversarial Pre-training", "author": "Zijian Zhao", "abstract": "  As a crucial aspect of Music Information Retrieval (MIR), Symbolic Music\nUnderstanding (SMU) has garnered significant attention for its potential to\nassist both musicians and enthusiasts in learning and creating music. Recently,\npre-trained language models have been widely adopted in SMU due to the\nsubstantial similarities between symbolic music and natural language, as well\nas the ability of these models to leverage limited music data effectively.\nHowever, some studies have shown the common pre-trained methods like Mask\nLanguage Model (MLM) may introduce bias issues like racism discrimination in\nNatural Language Process (NLP) and affects the performance of downstream tasks,\nwhich also happens in SMU. This bias often arises when masked tokens cannot be\ninferred from their context, forcing the model to overfit the training set\ninstead of generalizing. To address this challenge, we propose\nAdversarial-MidiBERT for SMU, which adaptively determines what to mask during\nMLM via a masker network, rather than employing random masking. By avoiding the\nmasking of tokens that are difficult to infer from context, our model is better\nequipped to capture contextual structures and relationships, rather than merely\nconforming to the training data distribution. We evaluate our method across\nfour SMU tasks, and our approach demonstrates excellent performance in all\ncases. The code for our model is publicly available at\nhttps://github.com/RS2002/Adversarial-MidiBERT.\n", "link": "http://arxiv.org/abs/2407.08306v2", "date": "2025-01-15", "relevancy": 1.9696, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4872}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%20Network%20Decide%20What%20to%20Learn%3A%20Symbolic%20Music%20Understanding%20Model%0A%20%20Based%20on%20Large-scale%20Adversarial%20Pre-training&body=Title%3A%20Let%20Network%20Decide%20What%20to%20Learn%3A%20Symbolic%20Music%20Understanding%20Model%0A%20%20Based%20on%20Large-scale%20Adversarial%20Pre-training%0AAuthor%3A%20Zijian%20Zhao%0AAbstract%3A%20%20%20As%20a%20crucial%20aspect%20of%20Music%20Information%20Retrieval%20%28MIR%29%2C%20Symbolic%20Music%0AUnderstanding%20%28SMU%29%20has%20garnered%20significant%20attention%20for%20its%20potential%20to%0Aassist%20both%20musicians%20and%20enthusiasts%20in%20learning%20and%20creating%20music.%20Recently%2C%0Apre-trained%20language%20models%20have%20been%20widely%20adopted%20in%20SMU%20due%20to%20the%0Asubstantial%20similarities%20between%20symbolic%20music%20and%20natural%20language%2C%20as%20well%0Aas%20the%20ability%20of%20these%20models%20to%20leverage%20limited%20music%20data%20effectively.%0AHowever%2C%20some%20studies%20have%20shown%20the%20common%20pre-trained%20methods%20like%20Mask%0ALanguage%20Model%20%28MLM%29%20may%20introduce%20bias%20issues%20like%20racism%20discrimination%20in%0ANatural%20Language%20Process%20%28NLP%29%20and%20affects%20the%20performance%20of%20downstream%20tasks%2C%0Awhich%20also%20happens%20in%20SMU.%20This%20bias%20often%20arises%20when%20masked%20tokens%20cannot%20be%0Ainferred%20from%20their%20context%2C%20forcing%20the%20model%20to%20overfit%20the%20training%20set%0Ainstead%20of%20generalizing.%20To%20address%20this%20challenge%2C%20we%20propose%0AAdversarial-MidiBERT%20for%20SMU%2C%20which%20adaptively%20determines%20what%20to%20mask%20during%0AMLM%20via%20a%20masker%20network%2C%20rather%20than%20employing%20random%20masking.%20By%20avoiding%20the%0Amasking%20of%20tokens%20that%20are%20difficult%20to%20infer%20from%20context%2C%20our%20model%20is%20better%0Aequipped%20to%20capture%20contextual%20structures%20and%20relationships%2C%20rather%20than%20merely%0Aconforming%20to%20the%20training%20data%20distribution.%20We%20evaluate%20our%20method%20across%0Afour%20SMU%20tasks%2C%20and%20our%20approach%20demonstrates%20excellent%20performance%20in%20all%0Acases.%20The%20code%20for%20our%20model%20is%20publicly%20available%20at%0Ahttps%3A//github.com/RS2002/Adversarial-MidiBERT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2520Network%2520Decide%2520What%2520to%2520Learn%253A%2520Symbolic%2520Music%2520Understanding%2520Model%250A%2520%2520Based%2520on%2520Large-scale%2520Adversarial%2520Pre-training%26entry.906535625%3DZijian%2520Zhao%26entry.1292438233%3D%2520%2520As%2520a%2520crucial%2520aspect%2520of%2520Music%2520Information%2520Retrieval%2520%2528MIR%2529%252C%2520Symbolic%2520Music%250AUnderstanding%2520%2528SMU%2529%2520has%2520garnered%2520significant%2520attention%2520for%2520its%2520potential%2520to%250Aassist%2520both%2520musicians%2520and%2520enthusiasts%2520in%2520learning%2520and%2520creating%2520music.%2520Recently%252C%250Apre-trained%2520language%2520models%2520have%2520been%2520widely%2520adopted%2520in%2520SMU%2520due%2520to%2520the%250Asubstantial%2520similarities%2520between%2520symbolic%2520music%2520and%2520natural%2520language%252C%2520as%2520well%250Aas%2520the%2520ability%2520of%2520these%2520models%2520to%2520leverage%2520limited%2520music%2520data%2520effectively.%250AHowever%252C%2520some%2520studies%2520have%2520shown%2520the%2520common%2520pre-trained%2520methods%2520like%2520Mask%250ALanguage%2520Model%2520%2528MLM%2529%2520may%2520introduce%2520bias%2520issues%2520like%2520racism%2520discrimination%2520in%250ANatural%2520Language%2520Process%2520%2528NLP%2529%2520and%2520affects%2520the%2520performance%2520of%2520downstream%2520tasks%252C%250Awhich%2520also%2520happens%2520in%2520SMU.%2520This%2520bias%2520often%2520arises%2520when%2520masked%2520tokens%2520cannot%2520be%250Ainferred%2520from%2520their%2520context%252C%2520forcing%2520the%2520model%2520to%2520overfit%2520the%2520training%2520set%250Ainstead%2520of%2520generalizing.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250AAdversarial-MidiBERT%2520for%2520SMU%252C%2520which%2520adaptively%2520determines%2520what%2520to%2520mask%2520during%250AMLM%2520via%2520a%2520masker%2520network%252C%2520rather%2520than%2520employing%2520random%2520masking.%2520By%2520avoiding%2520the%250Amasking%2520of%2520tokens%2520that%2520are%2520difficult%2520to%2520infer%2520from%2520context%252C%2520our%2520model%2520is%2520better%250Aequipped%2520to%2520capture%2520contextual%2520structures%2520and%2520relationships%252C%2520rather%2520than%2520merely%250Aconforming%2520to%2520the%2520training%2520data%2520distribution.%2520We%2520evaluate%2520our%2520method%2520across%250Afour%2520SMU%2520tasks%252C%2520and%2520our%2520approach%2520demonstrates%2520excellent%2520performance%2520in%2520all%250Acases.%2520The%2520code%2520for%2520our%2520model%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/RS2002/Adversarial-MidiBERT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%20Network%20Decide%20What%20to%20Learn%3A%20Symbolic%20Music%20Understanding%20Model%0A%20%20Based%20on%20Large-scale%20Adversarial%20Pre-training&entry.906535625=Zijian%20Zhao&entry.1292438233=%20%20As%20a%20crucial%20aspect%20of%20Music%20Information%20Retrieval%20%28MIR%29%2C%20Symbolic%20Music%0AUnderstanding%20%28SMU%29%20has%20garnered%20significant%20attention%20for%20its%20potential%20to%0Aassist%20both%20musicians%20and%20enthusiasts%20in%20learning%20and%20creating%20music.%20Recently%2C%0Apre-trained%20language%20models%20have%20been%20widely%20adopted%20in%20SMU%20due%20to%20the%0Asubstantial%20similarities%20between%20symbolic%20music%20and%20natural%20language%2C%20as%20well%0Aas%20the%20ability%20of%20these%20models%20to%20leverage%20limited%20music%20data%20effectively.%0AHowever%2C%20some%20studies%20have%20shown%20the%20common%20pre-trained%20methods%20like%20Mask%0ALanguage%20Model%20%28MLM%29%20may%20introduce%20bias%20issues%20like%20racism%20discrimination%20in%0ANatural%20Language%20Process%20%28NLP%29%20and%20affects%20the%20performance%20of%20downstream%20tasks%2C%0Awhich%20also%20happens%20in%20SMU.%20This%20bias%20often%20arises%20when%20masked%20tokens%20cannot%20be%0Ainferred%20from%20their%20context%2C%20forcing%20the%20model%20to%20overfit%20the%20training%20set%0Ainstead%20of%20generalizing.%20To%20address%20this%20challenge%2C%20we%20propose%0AAdversarial-MidiBERT%20for%20SMU%2C%20which%20adaptively%20determines%20what%20to%20mask%20during%0AMLM%20via%20a%20masker%20network%2C%20rather%20than%20employing%20random%20masking.%20By%20avoiding%20the%0Amasking%20of%20tokens%20that%20are%20difficult%20to%20infer%20from%20context%2C%20our%20model%20is%20better%0Aequipped%20to%20capture%20contextual%20structures%20and%20relationships%2C%20rather%20than%20merely%0Aconforming%20to%20the%20training%20data%20distribution.%20We%20evaluate%20our%20method%20across%0Afour%20SMU%20tasks%2C%20and%20our%20approach%20demonstrates%20excellent%20performance%20in%20all%0Acases.%20The%20code%20for%20our%20model%20is%20publicly%20available%20at%0Ahttps%3A//github.com/RS2002/Adversarial-MidiBERT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08306v2&entry.124074799=Read"},
{"title": "MambaLRP: Explaining Selective State Space Sequence Models", "author": "Farnoush Rezaei Jafari and Gr\u00e9goire Montavon and Klaus-Robert M\u00fcller and Oliver Eberle", "abstract": "  Recent sequence modeling approaches using selective state space sequence\nmodels, referred to as Mamba models, have seen a surge of interest. These\nmodels allow efficient processing of long sequences in linear time and are\nrapidly being adopted in a wide range of applications such as language\nmodeling, demonstrating promising performance. To foster their reliable use in\nreal-world scenarios, it is crucial to augment their transparency. Our work\nbridges this critical gap by bringing explainability, particularly Layer-wise\nRelevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of\nrelevance conservation, we identify specific components in the Mamba\narchitecture, which cause unfaithful explanations. To remedy this issue, we\npropose MambaLRP, a novel algorithm within the LRP framework, which ensures a\nmore stable and reliable relevance propagation through these components. Our\nproposed method is theoretically sound and excels in achieving state-of-the-art\nexplanation performance across a diverse range of models and datasets.\nMoreover, MambaLRP facilitates a deeper inspection of Mamba architectures,\nuncovering various biases and evaluating their significance. It also enables\nthe analysis of previous speculations regarding the long-range capabilities of\nMamba models.\n", "link": "http://arxiv.org/abs/2406.07592v3", "date": "2025-01-15", "relevancy": 1.9564, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaLRP%3A%20Explaining%20Selective%20State%20Space%20Sequence%20Models&body=Title%3A%20MambaLRP%3A%20Explaining%20Selective%20State%20Space%20Sequence%20Models%0AAuthor%3A%20Farnoush%20Rezaei%20Jafari%20and%20Gr%C3%A9goire%20Montavon%20and%20Klaus-Robert%20M%C3%BCller%20and%20Oliver%20Eberle%0AAbstract%3A%20%20%20Recent%20sequence%20modeling%20approaches%20using%20selective%20state%20space%20sequence%0Amodels%2C%20referred%20to%20as%20Mamba%20models%2C%20have%20seen%20a%20surge%20of%20interest.%20These%0Amodels%20allow%20efficient%20processing%20of%20long%20sequences%20in%20linear%20time%20and%20are%0Arapidly%20being%20adopted%20in%20a%20wide%20range%20of%20applications%20such%20as%20language%0Amodeling%2C%20demonstrating%20promising%20performance.%20To%20foster%20their%20reliable%20use%20in%0Areal-world%20scenarios%2C%20it%20is%20crucial%20to%20augment%20their%20transparency.%20Our%20work%0Abridges%20this%20critical%20gap%20by%20bringing%20explainability%2C%20particularly%20Layer-wise%0ARelevance%20Propagation%20%28LRP%29%2C%20to%20the%20Mamba%20architecture.%20Guided%20by%20the%20axiom%20of%0Arelevance%20conservation%2C%20we%20identify%20specific%20components%20in%20the%20Mamba%0Aarchitecture%2C%20which%20cause%20unfaithful%20explanations.%20To%20remedy%20this%20issue%2C%20we%0Apropose%20MambaLRP%2C%20a%20novel%20algorithm%20within%20the%20LRP%20framework%2C%20which%20ensures%20a%0Amore%20stable%20and%20reliable%20relevance%20propagation%20through%20these%20components.%20Our%0Aproposed%20method%20is%20theoretically%20sound%20and%20excels%20in%20achieving%20state-of-the-art%0Aexplanation%20performance%20across%20a%20diverse%20range%20of%20models%20and%20datasets.%0AMoreover%2C%20MambaLRP%20facilitates%20a%20deeper%20inspection%20of%20Mamba%20architectures%2C%0Auncovering%20various%20biases%20and%20evaluating%20their%20significance.%20It%20also%20enables%0Athe%20analysis%20of%20previous%20speculations%20regarding%20the%20long-range%20capabilities%20of%0AMamba%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07592v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaLRP%253A%2520Explaining%2520Selective%2520State%2520Space%2520Sequence%2520Models%26entry.906535625%3DFarnoush%2520Rezaei%2520Jafari%2520and%2520Gr%25C3%25A9goire%2520Montavon%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Oliver%2520Eberle%26entry.1292438233%3D%2520%2520Recent%2520sequence%2520modeling%2520approaches%2520using%2520selective%2520state%2520space%2520sequence%250Amodels%252C%2520referred%2520to%2520as%2520Mamba%2520models%252C%2520have%2520seen%2520a%2520surge%2520of%2520interest.%2520These%250Amodels%2520allow%2520efficient%2520processing%2520of%2520long%2520sequences%2520in%2520linear%2520time%2520and%2520are%250Arapidly%2520being%2520adopted%2520in%2520a%2520wide%2520range%2520of%2520applications%2520such%2520as%2520language%250Amodeling%252C%2520demonstrating%2520promising%2520performance.%2520To%2520foster%2520their%2520reliable%2520use%2520in%250Areal-world%2520scenarios%252C%2520it%2520is%2520crucial%2520to%2520augment%2520their%2520transparency.%2520Our%2520work%250Abridges%2520this%2520critical%2520gap%2520by%2520bringing%2520explainability%252C%2520particularly%2520Layer-wise%250ARelevance%2520Propagation%2520%2528LRP%2529%252C%2520to%2520the%2520Mamba%2520architecture.%2520Guided%2520by%2520the%2520axiom%2520of%250Arelevance%2520conservation%252C%2520we%2520identify%2520specific%2520components%2520in%2520the%2520Mamba%250Aarchitecture%252C%2520which%2520cause%2520unfaithful%2520explanations.%2520To%2520remedy%2520this%2520issue%252C%2520we%250Apropose%2520MambaLRP%252C%2520a%2520novel%2520algorithm%2520within%2520the%2520LRP%2520framework%252C%2520which%2520ensures%2520a%250Amore%2520stable%2520and%2520reliable%2520relevance%2520propagation%2520through%2520these%2520components.%2520Our%250Aproposed%2520method%2520is%2520theoretically%2520sound%2520and%2520excels%2520in%2520achieving%2520state-of-the-art%250Aexplanation%2520performance%2520across%2520a%2520diverse%2520range%2520of%2520models%2520and%2520datasets.%250AMoreover%252C%2520MambaLRP%2520facilitates%2520a%2520deeper%2520inspection%2520of%2520Mamba%2520architectures%252C%250Auncovering%2520various%2520biases%2520and%2520evaluating%2520their%2520significance.%2520It%2520also%2520enables%250Athe%2520analysis%2520of%2520previous%2520speculations%2520regarding%2520the%2520long-range%2520capabilities%2520of%250AMamba%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07592v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaLRP%3A%20Explaining%20Selective%20State%20Space%20Sequence%20Models&entry.906535625=Farnoush%20Rezaei%20Jafari%20and%20Gr%C3%A9goire%20Montavon%20and%20Klaus-Robert%20M%C3%BCller%20and%20Oliver%20Eberle&entry.1292438233=%20%20Recent%20sequence%20modeling%20approaches%20using%20selective%20state%20space%20sequence%0Amodels%2C%20referred%20to%20as%20Mamba%20models%2C%20have%20seen%20a%20surge%20of%20interest.%20These%0Amodels%20allow%20efficient%20processing%20of%20long%20sequences%20in%20linear%20time%20and%20are%0Arapidly%20being%20adopted%20in%20a%20wide%20range%20of%20applications%20such%20as%20language%0Amodeling%2C%20demonstrating%20promising%20performance.%20To%20foster%20their%20reliable%20use%20in%0Areal-world%20scenarios%2C%20it%20is%20crucial%20to%20augment%20their%20transparency.%20Our%20work%0Abridges%20this%20critical%20gap%20by%20bringing%20explainability%2C%20particularly%20Layer-wise%0ARelevance%20Propagation%20%28LRP%29%2C%20to%20the%20Mamba%20architecture.%20Guided%20by%20the%20axiom%20of%0Arelevance%20conservation%2C%20we%20identify%20specific%20components%20in%20the%20Mamba%0Aarchitecture%2C%20which%20cause%20unfaithful%20explanations.%20To%20remedy%20this%20issue%2C%20we%0Apropose%20MambaLRP%2C%20a%20novel%20algorithm%20within%20the%20LRP%20framework%2C%20which%20ensures%20a%0Amore%20stable%20and%20reliable%20relevance%20propagation%20through%20these%20components.%20Our%0Aproposed%20method%20is%20theoretically%20sound%20and%20excels%20in%20achieving%20state-of-the-art%0Aexplanation%20performance%20across%20a%20diverse%20range%20of%20models%20and%20datasets.%0AMoreover%2C%20MambaLRP%20facilitates%20a%20deeper%20inspection%20of%20Mamba%20architectures%2C%0Auncovering%20various%20biases%20and%20evaluating%20their%20significance.%20It%20also%20enables%0Athe%20analysis%20of%20previous%20speculations%20regarding%20the%20long-range%20capabilities%20of%0AMamba%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07592v3&entry.124074799=Read"},
{"title": "Extended convexity and smoothness and their applications in deep\n  learning", "author": "Binchuan Qi and Wei Gong and Li Li", "abstract": "  This paper introduces an optimization framework aimed at providing a\ntheoretical foundation for a class of composite optimization problems,\nparticularly those encountered in deep learning. In this framework, we\nintroduce $\\mathcal{H}(\\phi)$-convexity and $\\mathcal{H}(\\Phi)$-smoothness to\ngeneralize the existing concepts of Lipschitz smoothness and strong convexity.\nFurthermore, we analyze and establish the convergence of both gradient descent\nand stochastic gradient descent methods for objective functions that are\n$\\mathcal{H}(\\Phi)$-smooth. We prove that the optimal convergence rates of\nthese methods depend solely on the homogeneous degree of $\\Phi$. Based on these\nfindings, we construct two types of non-convex and non-smooth optimization\nproblems: deterministic composite and stochastic composite optimization\nproblems, which encompass the majority of optimization problems in deep\nlearning. To address these problems, we develop the gradient structure control\nalgorithm and prove that it can locate approximate global optima. This marks a\nsignificant departure from traditional non-convex analysis framework, which\ntypically settle for stationary points. Therefore, with the introduction of\n$\\mathcal{H}(\\phi)$-convexity and $\\mathcal{H}(\\Phi)$-smoothness, along with\nthe GSC algorithm, the non-convex optimization mechanisms in deep learning can\nbe theoretically explained and supported. Finally, the effectiveness of the\nproposed framework is substantiated through empirical experimentation.\n", "link": "http://arxiv.org/abs/2410.05807v2", "date": "2025-01-15", "relevancy": 1.9534, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5137}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4876}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extended%20convexity%20and%20smoothness%20and%20their%20applications%20in%20deep%0A%20%20learning&body=Title%3A%20Extended%20convexity%20and%20smoothness%20and%20their%20applications%20in%20deep%0A%20%20learning%0AAuthor%3A%20Binchuan%20Qi%20and%20Wei%20Gong%20and%20Li%20Li%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20optimization%20framework%20aimed%20at%20providing%20a%0Atheoretical%20foundation%20for%20a%20class%20of%20composite%20optimization%20problems%2C%0Aparticularly%20those%20encountered%20in%20deep%20learning.%20In%20this%20framework%2C%20we%0Aintroduce%20%24%5Cmathcal%7BH%7D%28%5Cphi%29%24-convexity%20and%20%24%5Cmathcal%7BH%7D%28%5CPhi%29%24-smoothness%20to%0Ageneralize%20the%20existing%20concepts%20of%20Lipschitz%20smoothness%20and%20strong%20convexity.%0AFurthermore%2C%20we%20analyze%20and%20establish%20the%20convergence%20of%20both%20gradient%20descent%0Aand%20stochastic%20gradient%20descent%20methods%20for%20objective%20functions%20that%20are%0A%24%5Cmathcal%7BH%7D%28%5CPhi%29%24-smooth.%20We%20prove%20that%20the%20optimal%20convergence%20rates%20of%0Athese%20methods%20depend%20solely%20on%20the%20homogeneous%20degree%20of%20%24%5CPhi%24.%20Based%20on%20these%0Afindings%2C%20we%20construct%20two%20types%20of%20non-convex%20and%20non-smooth%20optimization%0Aproblems%3A%20deterministic%20composite%20and%20stochastic%20composite%20optimization%0Aproblems%2C%20which%20encompass%20the%20majority%20of%20optimization%20problems%20in%20deep%0Alearning.%20To%20address%20these%20problems%2C%20we%20develop%20the%20gradient%20structure%20control%0Aalgorithm%20and%20prove%20that%20it%20can%20locate%20approximate%20global%20optima.%20This%20marks%20a%0Asignificant%20departure%20from%20traditional%20non-convex%20analysis%20framework%2C%20which%0Atypically%20settle%20for%20stationary%20points.%20Therefore%2C%20with%20the%20introduction%20of%0A%24%5Cmathcal%7BH%7D%28%5Cphi%29%24-convexity%20and%20%24%5Cmathcal%7BH%7D%28%5CPhi%29%24-smoothness%2C%20along%20with%0Athe%20GSC%20algorithm%2C%20the%20non-convex%20optimization%20mechanisms%20in%20deep%20learning%20can%0Abe%20theoretically%20explained%20and%20supported.%20Finally%2C%20the%20effectiveness%20of%20the%0Aproposed%20framework%20is%20substantiated%20through%20empirical%20experimentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtended%2520convexity%2520and%2520smoothness%2520and%2520their%2520applications%2520in%2520deep%250A%2520%2520learning%26entry.906535625%3DBinchuan%2520Qi%2520and%2520Wei%2520Gong%2520and%2520Li%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520optimization%2520framework%2520aimed%2520at%2520providing%2520a%250Atheoretical%2520foundation%2520for%2520a%2520class%2520of%2520composite%2520optimization%2520problems%252C%250Aparticularly%2520those%2520encountered%2520in%2520deep%2520learning.%2520In%2520this%2520framework%252C%2520we%250Aintroduce%2520%2524%255Cmathcal%257BH%257D%2528%255Cphi%2529%2524-convexity%2520and%2520%2524%255Cmathcal%257BH%257D%2528%255CPhi%2529%2524-smoothness%2520to%250Ageneralize%2520the%2520existing%2520concepts%2520of%2520Lipschitz%2520smoothness%2520and%2520strong%2520convexity.%250AFurthermore%252C%2520we%2520analyze%2520and%2520establish%2520the%2520convergence%2520of%2520both%2520gradient%2520descent%250Aand%2520stochastic%2520gradient%2520descent%2520methods%2520for%2520objective%2520functions%2520that%2520are%250A%2524%255Cmathcal%257BH%257D%2528%255CPhi%2529%2524-smooth.%2520We%2520prove%2520that%2520the%2520optimal%2520convergence%2520rates%2520of%250Athese%2520methods%2520depend%2520solely%2520on%2520the%2520homogeneous%2520degree%2520of%2520%2524%255CPhi%2524.%2520Based%2520on%2520these%250Afindings%252C%2520we%2520construct%2520two%2520types%2520of%2520non-convex%2520and%2520non-smooth%2520optimization%250Aproblems%253A%2520deterministic%2520composite%2520and%2520stochastic%2520composite%2520optimization%250Aproblems%252C%2520which%2520encompass%2520the%2520majority%2520of%2520optimization%2520problems%2520in%2520deep%250Alearning.%2520To%2520address%2520these%2520problems%252C%2520we%2520develop%2520the%2520gradient%2520structure%2520control%250Aalgorithm%2520and%2520prove%2520that%2520it%2520can%2520locate%2520approximate%2520global%2520optima.%2520This%2520marks%2520a%250Asignificant%2520departure%2520from%2520traditional%2520non-convex%2520analysis%2520framework%252C%2520which%250Atypically%2520settle%2520for%2520stationary%2520points.%2520Therefore%252C%2520with%2520the%2520introduction%2520of%250A%2524%255Cmathcal%257BH%257D%2528%255Cphi%2529%2524-convexity%2520and%2520%2524%255Cmathcal%257BH%257D%2528%255CPhi%2529%2524-smoothness%252C%2520along%2520with%250Athe%2520GSC%2520algorithm%252C%2520the%2520non-convex%2520optimization%2520mechanisms%2520in%2520deep%2520learning%2520can%250Abe%2520theoretically%2520explained%2520and%2520supported.%2520Finally%252C%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520framework%2520is%2520substantiated%2520through%2520empirical%2520experimentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extended%20convexity%20and%20smoothness%20and%20their%20applications%20in%20deep%0A%20%20learning&entry.906535625=Binchuan%20Qi%20and%20Wei%20Gong%20and%20Li%20Li&entry.1292438233=%20%20This%20paper%20introduces%20an%20optimization%20framework%20aimed%20at%20providing%20a%0Atheoretical%20foundation%20for%20a%20class%20of%20composite%20optimization%20problems%2C%0Aparticularly%20those%20encountered%20in%20deep%20learning.%20In%20this%20framework%2C%20we%0Aintroduce%20%24%5Cmathcal%7BH%7D%28%5Cphi%29%24-convexity%20and%20%24%5Cmathcal%7BH%7D%28%5CPhi%29%24-smoothness%20to%0Ageneralize%20the%20existing%20concepts%20of%20Lipschitz%20smoothness%20and%20strong%20convexity.%0AFurthermore%2C%20we%20analyze%20and%20establish%20the%20convergence%20of%20both%20gradient%20descent%0Aand%20stochastic%20gradient%20descent%20methods%20for%20objective%20functions%20that%20are%0A%24%5Cmathcal%7BH%7D%28%5CPhi%29%24-smooth.%20We%20prove%20that%20the%20optimal%20convergence%20rates%20of%0Athese%20methods%20depend%20solely%20on%20the%20homogeneous%20degree%20of%20%24%5CPhi%24.%20Based%20on%20these%0Afindings%2C%20we%20construct%20two%20types%20of%20non-convex%20and%20non-smooth%20optimization%0Aproblems%3A%20deterministic%20composite%20and%20stochastic%20composite%20optimization%0Aproblems%2C%20which%20encompass%20the%20majority%20of%20optimization%20problems%20in%20deep%0Alearning.%20To%20address%20these%20problems%2C%20we%20develop%20the%20gradient%20structure%20control%0Aalgorithm%20and%20prove%20that%20it%20can%20locate%20approximate%20global%20optima.%20This%20marks%20a%0Asignificant%20departure%20from%20traditional%20non-convex%20analysis%20framework%2C%20which%0Atypically%20settle%20for%20stationary%20points.%20Therefore%2C%20with%20the%20introduction%20of%0A%24%5Cmathcal%7BH%7D%28%5Cphi%29%24-convexity%20and%20%24%5Cmathcal%7BH%7D%28%5CPhi%29%24-smoothness%2C%20along%20with%0Athe%20GSC%20algorithm%2C%20the%20non-convex%20optimization%20mechanisms%20in%20deep%20learning%20can%0Abe%20theoretically%20explained%20and%20supported.%20Finally%2C%20the%20effectiveness%20of%20the%0Aproposed%20framework%20is%20substantiated%20through%20empirical%20experimentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05807v2&entry.124074799=Read"},
{"title": "Applying the maximum entropy principle to neural networks enhances\n  multi-species distribution models", "author": "Maxime Ryckewaert and Diego Marcos and Christophe Botella and Maximilien Servajean and Pierre Bonnet and Alexis Joly", "abstract": "  The rapid expansion of citizen science initiatives has led to a significant\ngrowth of biodiversity databases, and particularly presence-only (PO)\nobservations. PO data are invaluable for understanding species distributions\nand their dynamics, but their use in a Species Distribution Model (SDM) is\ncurtailed by sampling biases and the lack of information on absences. Poisson\npoint processes are widely used for SDMs, with Maxent being one of the most\npopular methods. Maxent maximises the entropy of a probability distribution\nacross sites as a function of predefined transformations of variables, called\nfeatures. In contrast, neural networks and deep learning have emerged as a\npromising technique for automatic feature extraction from complex input\nvariables. Arbitrarily complex transformations of input variables can be\nlearned from the data efficiently through backpropagation and stochastic\ngradient descent (SGD). In this paper, we propose DeepMaxent, which harnesses\nneural networks to automatically learn shared features among species, using the\nmaximum entropy principle. To do so, it employs a normalised Poisson loss where\nfor each species, presence probabilities across sites are modelled by a neural\nnetwork. We evaluate DeepMaxent on a benchmark dataset known for its spatial\nsampling biases, using PO data for calibration and presence-absence (PA) data\nfor validation across six regions with different biological groups and\ncovariates. Our results indicate that DeepMaxent performs better than Maxent\nand other leading SDMs across all regions and taxonomic groups. The method\nperforms particularly well in regions of uneven sampling, demonstrating\nsubstantial potential to increase SDM performances. In particular, our approach\nyields more accurate predictions than traditional single-species models, which\nopens up new possibilities for methodological enhancement.\n", "link": "http://arxiv.org/abs/2412.19217v2", "date": "2025-01-15", "relevancy": 1.9495, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4818}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applying%20the%20maximum%20entropy%20principle%20to%20neural%20networks%20enhances%0A%20%20multi-species%20distribution%20models&body=Title%3A%20Applying%20the%20maximum%20entropy%20principle%20to%20neural%20networks%20enhances%0A%20%20multi-species%20distribution%20models%0AAuthor%3A%20Maxime%20Ryckewaert%20and%20Diego%20Marcos%20and%20Christophe%20Botella%20and%20Maximilien%20Servajean%20and%20Pierre%20Bonnet%20and%20Alexis%20Joly%0AAbstract%3A%20%20%20The%20rapid%20expansion%20of%20citizen%20science%20initiatives%20has%20led%20to%20a%20significant%0Agrowth%20of%20biodiversity%20databases%2C%20and%20particularly%20presence-only%20%28PO%29%0Aobservations.%20PO%20data%20are%20invaluable%20for%20understanding%20species%20distributions%0Aand%20their%20dynamics%2C%20but%20their%20use%20in%20a%20Species%20Distribution%20Model%20%28SDM%29%20is%0Acurtailed%20by%20sampling%20biases%20and%20the%20lack%20of%20information%20on%20absences.%20Poisson%0Apoint%20processes%20are%20widely%20used%20for%20SDMs%2C%20with%20Maxent%20being%20one%20of%20the%20most%0Apopular%20methods.%20Maxent%20maximises%20the%20entropy%20of%20a%20probability%20distribution%0Aacross%20sites%20as%20a%20function%20of%20predefined%20transformations%20of%20variables%2C%20called%0Afeatures.%20In%20contrast%2C%20neural%20networks%20and%20deep%20learning%20have%20emerged%20as%20a%0Apromising%20technique%20for%20automatic%20feature%20extraction%20from%20complex%20input%0Avariables.%20Arbitrarily%20complex%20transformations%20of%20input%20variables%20can%20be%0Alearned%20from%20the%20data%20efficiently%20through%20backpropagation%20and%20stochastic%0Agradient%20descent%20%28SGD%29.%20In%20this%20paper%2C%20we%20propose%20DeepMaxent%2C%20which%20harnesses%0Aneural%20networks%20to%20automatically%20learn%20shared%20features%20among%20species%2C%20using%20the%0Amaximum%20entropy%20principle.%20To%20do%20so%2C%20it%20employs%20a%20normalised%20Poisson%20loss%20where%0Afor%20each%20species%2C%20presence%20probabilities%20across%20sites%20are%20modelled%20by%20a%20neural%0Anetwork.%20We%20evaluate%20DeepMaxent%20on%20a%20benchmark%20dataset%20known%20for%20its%20spatial%0Asampling%20biases%2C%20using%20PO%20data%20for%20calibration%20and%20presence-absence%20%28PA%29%20data%0Afor%20validation%20across%20six%20regions%20with%20different%20biological%20groups%20and%0Acovariates.%20Our%20results%20indicate%20that%20DeepMaxent%20performs%20better%20than%20Maxent%0Aand%20other%20leading%20SDMs%20across%20all%20regions%20and%20taxonomic%20groups.%20The%20method%0Aperforms%20particularly%20well%20in%20regions%20of%20uneven%20sampling%2C%20demonstrating%0Asubstantial%20potential%20to%20increase%20SDM%20performances.%20In%20particular%2C%20our%20approach%0Ayields%20more%20accurate%20predictions%20than%20traditional%20single-species%20models%2C%20which%0Aopens%20up%20new%20possibilities%20for%20methodological%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19217v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplying%2520the%2520maximum%2520entropy%2520principle%2520to%2520neural%2520networks%2520enhances%250A%2520%2520multi-species%2520distribution%2520models%26entry.906535625%3DMaxime%2520Ryckewaert%2520and%2520Diego%2520Marcos%2520and%2520Christophe%2520Botella%2520and%2520Maximilien%2520Servajean%2520and%2520Pierre%2520Bonnet%2520and%2520Alexis%2520Joly%26entry.1292438233%3D%2520%2520The%2520rapid%2520expansion%2520of%2520citizen%2520science%2520initiatives%2520has%2520led%2520to%2520a%2520significant%250Agrowth%2520of%2520biodiversity%2520databases%252C%2520and%2520particularly%2520presence-only%2520%2528PO%2529%250Aobservations.%2520PO%2520data%2520are%2520invaluable%2520for%2520understanding%2520species%2520distributions%250Aand%2520their%2520dynamics%252C%2520but%2520their%2520use%2520in%2520a%2520Species%2520Distribution%2520Model%2520%2528SDM%2529%2520is%250Acurtailed%2520by%2520sampling%2520biases%2520and%2520the%2520lack%2520of%2520information%2520on%2520absences.%2520Poisson%250Apoint%2520processes%2520are%2520widely%2520used%2520for%2520SDMs%252C%2520with%2520Maxent%2520being%2520one%2520of%2520the%2520most%250Apopular%2520methods.%2520Maxent%2520maximises%2520the%2520entropy%2520of%2520a%2520probability%2520distribution%250Aacross%2520sites%2520as%2520a%2520function%2520of%2520predefined%2520transformations%2520of%2520variables%252C%2520called%250Afeatures.%2520In%2520contrast%252C%2520neural%2520networks%2520and%2520deep%2520learning%2520have%2520emerged%2520as%2520a%250Apromising%2520technique%2520for%2520automatic%2520feature%2520extraction%2520from%2520complex%2520input%250Avariables.%2520Arbitrarily%2520complex%2520transformations%2520of%2520input%2520variables%2520can%2520be%250Alearned%2520from%2520the%2520data%2520efficiently%2520through%2520backpropagation%2520and%2520stochastic%250Agradient%2520descent%2520%2528SGD%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DeepMaxent%252C%2520which%2520harnesses%250Aneural%2520networks%2520to%2520automatically%2520learn%2520shared%2520features%2520among%2520species%252C%2520using%2520the%250Amaximum%2520entropy%2520principle.%2520To%2520do%2520so%252C%2520it%2520employs%2520a%2520normalised%2520Poisson%2520loss%2520where%250Afor%2520each%2520species%252C%2520presence%2520probabilities%2520across%2520sites%2520are%2520modelled%2520by%2520a%2520neural%250Anetwork.%2520We%2520evaluate%2520DeepMaxent%2520on%2520a%2520benchmark%2520dataset%2520known%2520for%2520its%2520spatial%250Asampling%2520biases%252C%2520using%2520PO%2520data%2520for%2520calibration%2520and%2520presence-absence%2520%2528PA%2529%2520data%250Afor%2520validation%2520across%2520six%2520regions%2520with%2520different%2520biological%2520groups%2520and%250Acovariates.%2520Our%2520results%2520indicate%2520that%2520DeepMaxent%2520performs%2520better%2520than%2520Maxent%250Aand%2520other%2520leading%2520SDMs%2520across%2520all%2520regions%2520and%2520taxonomic%2520groups.%2520The%2520method%250Aperforms%2520particularly%2520well%2520in%2520regions%2520of%2520uneven%2520sampling%252C%2520demonstrating%250Asubstantial%2520potential%2520to%2520increase%2520SDM%2520performances.%2520In%2520particular%252C%2520our%2520approach%250Ayields%2520more%2520accurate%2520predictions%2520than%2520traditional%2520single-species%2520models%252C%2520which%250Aopens%2520up%2520new%2520possibilities%2520for%2520methodological%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19217v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applying%20the%20maximum%20entropy%20principle%20to%20neural%20networks%20enhances%0A%20%20multi-species%20distribution%20models&entry.906535625=Maxime%20Ryckewaert%20and%20Diego%20Marcos%20and%20Christophe%20Botella%20and%20Maximilien%20Servajean%20and%20Pierre%20Bonnet%20and%20Alexis%20Joly&entry.1292438233=%20%20The%20rapid%20expansion%20of%20citizen%20science%20initiatives%20has%20led%20to%20a%20significant%0Agrowth%20of%20biodiversity%20databases%2C%20and%20particularly%20presence-only%20%28PO%29%0Aobservations.%20PO%20data%20are%20invaluable%20for%20understanding%20species%20distributions%0Aand%20their%20dynamics%2C%20but%20their%20use%20in%20a%20Species%20Distribution%20Model%20%28SDM%29%20is%0Acurtailed%20by%20sampling%20biases%20and%20the%20lack%20of%20information%20on%20absences.%20Poisson%0Apoint%20processes%20are%20widely%20used%20for%20SDMs%2C%20with%20Maxent%20being%20one%20of%20the%20most%0Apopular%20methods.%20Maxent%20maximises%20the%20entropy%20of%20a%20probability%20distribution%0Aacross%20sites%20as%20a%20function%20of%20predefined%20transformations%20of%20variables%2C%20called%0Afeatures.%20In%20contrast%2C%20neural%20networks%20and%20deep%20learning%20have%20emerged%20as%20a%0Apromising%20technique%20for%20automatic%20feature%20extraction%20from%20complex%20input%0Avariables.%20Arbitrarily%20complex%20transformations%20of%20input%20variables%20can%20be%0Alearned%20from%20the%20data%20efficiently%20through%20backpropagation%20and%20stochastic%0Agradient%20descent%20%28SGD%29.%20In%20this%20paper%2C%20we%20propose%20DeepMaxent%2C%20which%20harnesses%0Aneural%20networks%20to%20automatically%20learn%20shared%20features%20among%20species%2C%20using%20the%0Amaximum%20entropy%20principle.%20To%20do%20so%2C%20it%20employs%20a%20normalised%20Poisson%20loss%20where%0Afor%20each%20species%2C%20presence%20probabilities%20across%20sites%20are%20modelled%20by%20a%20neural%0Anetwork.%20We%20evaluate%20DeepMaxent%20on%20a%20benchmark%20dataset%20known%20for%20its%20spatial%0Asampling%20biases%2C%20using%20PO%20data%20for%20calibration%20and%20presence-absence%20%28PA%29%20data%0Afor%20validation%20across%20six%20regions%20with%20different%20biological%20groups%20and%0Acovariates.%20Our%20results%20indicate%20that%20DeepMaxent%20performs%20better%20than%20Maxent%0Aand%20other%20leading%20SDMs%20across%20all%20regions%20and%20taxonomic%20groups.%20The%20method%0Aperforms%20particularly%20well%20in%20regions%20of%20uneven%20sampling%2C%20demonstrating%0Asubstantial%20potential%20to%20increase%20SDM%20performances.%20In%20particular%2C%20our%20approach%0Ayields%20more%20accurate%20predictions%20than%20traditional%20single-species%20models%2C%20which%0Aopens%20up%20new%20possibilities%20for%20methodological%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19217v2&entry.124074799=Read"},
{"title": "A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection", "author": "Konstantin Garov and Kamalika Chaudhuri", "abstract": "  Machine learning algorithms often encounter different or\n\"out-of-distribution\" (OOD) data at deployment time, and OOD detection is\nfrequently employed to detect these examples. While it works reasonably well in\npractice, existing theoretical results on OOD detection are highly pessimistic.\nIn this work, we take a closer look at this problem, and make a distinction\nbetween uniform and non-uniform learnability, following PAC learning theory. We\ncharacterize under what conditions OOD detection is uniformly and non-uniformly\nlearnable, and we show that in several cases, non-uniform learnability turns a\nnumber of negative results into positive. In all cases where OOD detection is\nlearnable, we provide concrete learning algorithms and a sample-complexity\nanalysis.\n", "link": "http://arxiv.org/abs/2501.08821v1", "date": "2025-01-15", "relevancy": 1.935, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5003}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4863}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Closer%20Look%20at%20the%20Learnability%20of%20Out-of-Distribution%20%28OOD%29%20Detection&body=Title%3A%20A%20Closer%20Look%20at%20the%20Learnability%20of%20Out-of-Distribution%20%28OOD%29%20Detection%0AAuthor%3A%20Konstantin%20Garov%20and%20Kamalika%20Chaudhuri%0AAbstract%3A%20%20%20Machine%20learning%20algorithms%20often%20encounter%20different%20or%0A%22out-of-distribution%22%20%28OOD%29%20data%20at%20deployment%20time%2C%20and%20OOD%20detection%20is%0Afrequently%20employed%20to%20detect%20these%20examples.%20While%20it%20works%20reasonably%20well%20in%0Apractice%2C%20existing%20theoretical%20results%20on%20OOD%20detection%20are%20highly%20pessimistic.%0AIn%20this%20work%2C%20we%20take%20a%20closer%20look%20at%20this%20problem%2C%20and%20make%20a%20distinction%0Abetween%20uniform%20and%20non-uniform%20learnability%2C%20following%20PAC%20learning%20theory.%20We%0Acharacterize%20under%20what%20conditions%20OOD%20detection%20is%20uniformly%20and%20non-uniformly%0Alearnable%2C%20and%20we%20show%20that%20in%20several%20cases%2C%20non-uniform%20learnability%20turns%20a%0Anumber%20of%20negative%20results%20into%20positive.%20In%20all%20cases%20where%20OOD%20detection%20is%0Alearnable%2C%20we%20provide%20concrete%20learning%20algorithms%20and%20a%20sample-complexity%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Closer%2520Look%2520at%2520the%2520Learnability%2520of%2520Out-of-Distribution%2520%2528OOD%2529%2520Detection%26entry.906535625%3DKonstantin%2520Garov%2520and%2520Kamalika%2520Chaudhuri%26entry.1292438233%3D%2520%2520Machine%2520learning%2520algorithms%2520often%2520encounter%2520different%2520or%250A%2522out-of-distribution%2522%2520%2528OOD%2529%2520data%2520at%2520deployment%2520time%252C%2520and%2520OOD%2520detection%2520is%250Afrequently%2520employed%2520to%2520detect%2520these%2520examples.%2520While%2520it%2520works%2520reasonably%2520well%2520in%250Apractice%252C%2520existing%2520theoretical%2520results%2520on%2520OOD%2520detection%2520are%2520highly%2520pessimistic.%250AIn%2520this%2520work%252C%2520we%2520take%2520a%2520closer%2520look%2520at%2520this%2520problem%252C%2520and%2520make%2520a%2520distinction%250Abetween%2520uniform%2520and%2520non-uniform%2520learnability%252C%2520following%2520PAC%2520learning%2520theory.%2520We%250Acharacterize%2520under%2520what%2520conditions%2520OOD%2520detection%2520is%2520uniformly%2520and%2520non-uniformly%250Alearnable%252C%2520and%2520we%2520show%2520that%2520in%2520several%2520cases%252C%2520non-uniform%2520learnability%2520turns%2520a%250Anumber%2520of%2520negative%2520results%2520into%2520positive.%2520In%2520all%2520cases%2520where%2520OOD%2520detection%2520is%250Alearnable%252C%2520we%2520provide%2520concrete%2520learning%2520algorithms%2520and%2520a%2520sample-complexity%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Closer%20Look%20at%20the%20Learnability%20of%20Out-of-Distribution%20%28OOD%29%20Detection&entry.906535625=Konstantin%20Garov%20and%20Kamalika%20Chaudhuri&entry.1292438233=%20%20Machine%20learning%20algorithms%20often%20encounter%20different%20or%0A%22out-of-distribution%22%20%28OOD%29%20data%20at%20deployment%20time%2C%20and%20OOD%20detection%20is%0Afrequently%20employed%20to%20detect%20these%20examples.%20While%20it%20works%20reasonably%20well%20in%0Apractice%2C%20existing%20theoretical%20results%20on%20OOD%20detection%20are%20highly%20pessimistic.%0AIn%20this%20work%2C%20we%20take%20a%20closer%20look%20at%20this%20problem%2C%20and%20make%20a%20distinction%0Abetween%20uniform%20and%20non-uniform%20learnability%2C%20following%20PAC%20learning%20theory.%20We%0Acharacterize%20under%20what%20conditions%20OOD%20detection%20is%20uniformly%20and%20non-uniformly%0Alearnable%2C%20and%20we%20show%20that%20in%20several%20cases%2C%20non-uniform%20learnability%20turns%20a%0Anumber%20of%20negative%20results%20into%20positive.%20In%20all%20cases%20where%20OOD%20detection%20is%0Alearnable%2C%20we%20provide%20concrete%20learning%20algorithms%20and%20a%20sample-complexity%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08821v1&entry.124074799=Read"},
{"title": "Increasing Batch Size Improves Convergence of Stochastic Gradient\n  Descent with Momentum", "author": "Keisuke Kamo and Hideaki Iiduka", "abstract": "  Stochastic gradient descent with momentum (SGDM), which is defined by adding\na momentum term to SGD, has been well studied in both theory and practice.\nTheoretically investigated results showed that the settings of the learning\nrate and momentum weight affect the convergence of SGDM. Meanwhile, practical\nresults showed that the setting of batch size strongly depends on the\nperformance of SGDM. In this paper, we focus on mini-batch SGDM with constant\nlearning rate and constant momentum weight, which is frequently used to train\ndeep neural networks in practice. The contribution of this paper is showing\ntheoretically that using a constant batch size does not always minimize the\nexpectation of the full gradient norm of the empirical loss in training a deep\nneural network, whereas using an increasing batch size definitely minimizes it,\nthat is, increasing batch size improves convergence of mini-batch SGDM. We also\nprovide numerical results supporting our analyses, indicating specifically that\nmini-batch SGDM with an increasing batch size converges to stationary points\nfaster than with a constant batch size. Python implementations of the\noptimizers used in the numerical experiments are available at\nhttps://anonymous.4open.science/r/momentum-increasing-batch-size-888C/.\n", "link": "http://arxiv.org/abs/2501.08883v1", "date": "2025-01-15", "relevancy": 1.9193, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5005}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4653}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Increasing%20Batch%20Size%20Improves%20Convergence%20of%20Stochastic%20Gradient%0A%20%20Descent%20with%20Momentum&body=Title%3A%20Increasing%20Batch%20Size%20Improves%20Convergence%20of%20Stochastic%20Gradient%0A%20%20Descent%20with%20Momentum%0AAuthor%3A%20Keisuke%20Kamo%20and%20Hideaki%20Iiduka%0AAbstract%3A%20%20%20Stochastic%20gradient%20descent%20with%20momentum%20%28SGDM%29%2C%20which%20is%20defined%20by%20adding%0Aa%20momentum%20term%20to%20SGD%2C%20has%20been%20well%20studied%20in%20both%20theory%20and%20practice.%0ATheoretically%20investigated%20results%20showed%20that%20the%20settings%20of%20the%20learning%0Arate%20and%20momentum%20weight%20affect%20the%20convergence%20of%20SGDM.%20Meanwhile%2C%20practical%0Aresults%20showed%20that%20the%20setting%20of%20batch%20size%20strongly%20depends%20on%20the%0Aperformance%20of%20SGDM.%20In%20this%20paper%2C%20we%20focus%20on%20mini-batch%20SGDM%20with%20constant%0Alearning%20rate%20and%20constant%20momentum%20weight%2C%20which%20is%20frequently%20used%20to%20train%0Adeep%20neural%20networks%20in%20practice.%20The%20contribution%20of%20this%20paper%20is%20showing%0Atheoretically%20that%20using%20a%20constant%20batch%20size%20does%20not%20always%20minimize%20the%0Aexpectation%20of%20the%20full%20gradient%20norm%20of%20the%20empirical%20loss%20in%20training%20a%20deep%0Aneural%20network%2C%20whereas%20using%20an%20increasing%20batch%20size%20definitely%20minimizes%20it%2C%0Athat%20is%2C%20increasing%20batch%20size%20improves%20convergence%20of%20mini-batch%20SGDM.%20We%20also%0Aprovide%20numerical%20results%20supporting%20our%20analyses%2C%20indicating%20specifically%20that%0Amini-batch%20SGDM%20with%20an%20increasing%20batch%20size%20converges%20to%20stationary%20points%0Afaster%20than%20with%20a%20constant%20batch%20size.%20Python%20implementations%20of%20the%0Aoptimizers%20used%20in%20the%20numerical%20experiments%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/momentum-increasing-batch-size-888C/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncreasing%2520Batch%2520Size%2520Improves%2520Convergence%2520of%2520Stochastic%2520Gradient%250A%2520%2520Descent%2520with%2520Momentum%26entry.906535625%3DKeisuke%2520Kamo%2520and%2520Hideaki%2520Iiduka%26entry.1292438233%3D%2520%2520Stochastic%2520gradient%2520descent%2520with%2520momentum%2520%2528SGDM%2529%252C%2520which%2520is%2520defined%2520by%2520adding%250Aa%2520momentum%2520term%2520to%2520SGD%252C%2520has%2520been%2520well%2520studied%2520in%2520both%2520theory%2520and%2520practice.%250ATheoretically%2520investigated%2520results%2520showed%2520that%2520the%2520settings%2520of%2520the%2520learning%250Arate%2520and%2520momentum%2520weight%2520affect%2520the%2520convergence%2520of%2520SGDM.%2520Meanwhile%252C%2520practical%250Aresults%2520showed%2520that%2520the%2520setting%2520of%2520batch%2520size%2520strongly%2520depends%2520on%2520the%250Aperformance%2520of%2520SGDM.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520mini-batch%2520SGDM%2520with%2520constant%250Alearning%2520rate%2520and%2520constant%2520momentum%2520weight%252C%2520which%2520is%2520frequently%2520used%2520to%2520train%250Adeep%2520neural%2520networks%2520in%2520practice.%2520The%2520contribution%2520of%2520this%2520paper%2520is%2520showing%250Atheoretically%2520that%2520using%2520a%2520constant%2520batch%2520size%2520does%2520not%2520always%2520minimize%2520the%250Aexpectation%2520of%2520the%2520full%2520gradient%2520norm%2520of%2520the%2520empirical%2520loss%2520in%2520training%2520a%2520deep%250Aneural%2520network%252C%2520whereas%2520using%2520an%2520increasing%2520batch%2520size%2520definitely%2520minimizes%2520it%252C%250Athat%2520is%252C%2520increasing%2520batch%2520size%2520improves%2520convergence%2520of%2520mini-batch%2520SGDM.%2520We%2520also%250Aprovide%2520numerical%2520results%2520supporting%2520our%2520analyses%252C%2520indicating%2520specifically%2520that%250Amini-batch%2520SGDM%2520with%2520an%2520increasing%2520batch%2520size%2520converges%2520to%2520stationary%2520points%250Afaster%2520than%2520with%2520a%2520constant%2520batch%2520size.%2520Python%2520implementations%2520of%2520the%250Aoptimizers%2520used%2520in%2520the%2520numerical%2520experiments%2520are%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/momentum-increasing-batch-size-888C/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Increasing%20Batch%20Size%20Improves%20Convergence%20of%20Stochastic%20Gradient%0A%20%20Descent%20with%20Momentum&entry.906535625=Keisuke%20Kamo%20and%20Hideaki%20Iiduka&entry.1292438233=%20%20Stochastic%20gradient%20descent%20with%20momentum%20%28SGDM%29%2C%20which%20is%20defined%20by%20adding%0Aa%20momentum%20term%20to%20SGD%2C%20has%20been%20well%20studied%20in%20both%20theory%20and%20practice.%0ATheoretically%20investigated%20results%20showed%20that%20the%20settings%20of%20the%20learning%0Arate%20and%20momentum%20weight%20affect%20the%20convergence%20of%20SGDM.%20Meanwhile%2C%20practical%0Aresults%20showed%20that%20the%20setting%20of%20batch%20size%20strongly%20depends%20on%20the%0Aperformance%20of%20SGDM.%20In%20this%20paper%2C%20we%20focus%20on%20mini-batch%20SGDM%20with%20constant%0Alearning%20rate%20and%20constant%20momentum%20weight%2C%20which%20is%20frequently%20used%20to%20train%0Adeep%20neural%20networks%20in%20practice.%20The%20contribution%20of%20this%20paper%20is%20showing%0Atheoretically%20that%20using%20a%20constant%20batch%20size%20does%20not%20always%20minimize%20the%0Aexpectation%20of%20the%20full%20gradient%20norm%20of%20the%20empirical%20loss%20in%20training%20a%20deep%0Aneural%20network%2C%20whereas%20using%20an%20increasing%20batch%20size%20definitely%20minimizes%20it%2C%0Athat%20is%2C%20increasing%20batch%20size%20improves%20convergence%20of%20mini-batch%20SGDM.%20We%20also%0Aprovide%20numerical%20results%20supporting%20our%20analyses%2C%20indicating%20specifically%20that%0Amini-batch%20SGDM%20with%20an%20increasing%20batch%20size%20converges%20to%20stationary%20points%0Afaster%20than%20with%20a%20constant%20batch%20size.%20Python%20implementations%20of%20the%0Aoptimizers%20used%20in%20the%20numerical%20experiments%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/momentum-increasing-batch-size-888C/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08883v1&entry.124074799=Read"},
{"title": "ARMOR: Shielding Unlearnable Examples against Data Augmentation", "author": "Xueluan Gong and Yuji Wang and Yanjiao Chen and Haocheng Dong and Yiming Li and Mengyuan Sun and Shuaike Li and Qian Wang and Chen Chen", "abstract": "  Private data, when published online, may be collected by unauthorized parties\nto train deep neural networks (DNNs). To protect privacy, defensive noises can\nbe added to original samples to degrade their learnability by DNNs. Recently,\nunlearnable examples are proposed to minimize the training loss such that the\nmodel learns almost nothing. However, raw data are often pre-processed before\nbeing used for training, which may restore the private information of protected\ndata. In this paper, we reveal the data privacy violation induced by data\naugmentation, a commonly used data pre-processing technique to improve model\ngeneralization capability, which is the first of its kind as far as we are\nconcerned. We demonstrate that data augmentation can significantly raise the\naccuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To\naddress this issue, we propose a defense framework, dubbed ARMOR, to protect\ndata privacy from potential breaches of data augmentation. To overcome the\ndifficulty of having no access to the model training process, we design a\nnon-local module-assisted surrogate model that better captures the effect of\ndata augmentation. In addition, we design a surrogate augmentation selection\nstrategy that maximizes distribution alignment between augmented and\nnon-augmented samples, to choose the optimal augmentation strategy for each\nclass. We also use a dynamic step size adjustment algorithm to enhance the\ndefensive noise generation process. Extensive experiments are conducted on 4\ndatasets and 5 data augmentation methods to verify the performance of ARMOR.\nComparisons with 6 state-of-the-art defense methods have demonstrated that\nARMOR can preserve the unlearnability of protected private data under data\naugmentation. ARMOR reduces the test accuracy of the model trained on augmented\nprotected samples by as much as 60% more than baselines.\n", "link": "http://arxiv.org/abs/2501.08862v1", "date": "2025-01-15", "relevancy": 1.9181, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4998}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4946}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARMOR%3A%20Shielding%20Unlearnable%20Examples%20against%20Data%20Augmentation&body=Title%3A%20ARMOR%3A%20Shielding%20Unlearnable%20Examples%20against%20Data%20Augmentation%0AAuthor%3A%20Xueluan%20Gong%20and%20Yuji%20Wang%20and%20Yanjiao%20Chen%20and%20Haocheng%20Dong%20and%20Yiming%20Li%20and%20Mengyuan%20Sun%20and%20Shuaike%20Li%20and%20Qian%20Wang%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Private%20data%2C%20when%20published%20online%2C%20may%20be%20collected%20by%20unauthorized%20parties%0Ato%20train%20deep%20neural%20networks%20%28DNNs%29.%20To%20protect%20privacy%2C%20defensive%20noises%20can%0Abe%20added%20to%20original%20samples%20to%20degrade%20their%20learnability%20by%20DNNs.%20Recently%2C%0Aunlearnable%20examples%20are%20proposed%20to%20minimize%20the%20training%20loss%20such%20that%20the%0Amodel%20learns%20almost%20nothing.%20However%2C%20raw%20data%20are%20often%20pre-processed%20before%0Abeing%20used%20for%20training%2C%20which%20may%20restore%20the%20private%20information%20of%20protected%0Adata.%20In%20this%20paper%2C%20we%20reveal%20the%20data%20privacy%20violation%20induced%20by%20data%0Aaugmentation%2C%20a%20commonly%20used%20data%20pre-processing%20technique%20to%20improve%20model%0Ageneralization%20capability%2C%20which%20is%20the%20first%20of%20its%20kind%20as%20far%20as%20we%20are%0Aconcerned.%20We%20demonstrate%20that%20data%20augmentation%20can%20significantly%20raise%20the%0Aaccuracy%20of%20the%20model%20trained%20on%20unlearnable%20examples%20from%2021.3%25%20to%2066.1%25.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20defense%20framework%2C%20dubbed%20ARMOR%2C%20to%20protect%0Adata%20privacy%20from%20potential%20breaches%20of%20data%20augmentation.%20To%20overcome%20the%0Adifficulty%20of%20having%20no%20access%20to%20the%20model%20training%20process%2C%20we%20design%20a%0Anon-local%20module-assisted%20surrogate%20model%20that%20better%20captures%20the%20effect%20of%0Adata%20augmentation.%20In%20addition%2C%20we%20design%20a%20surrogate%20augmentation%20selection%0Astrategy%20that%20maximizes%20distribution%20alignment%20between%20augmented%20and%0Anon-augmented%20samples%2C%20to%20choose%20the%20optimal%20augmentation%20strategy%20for%20each%0Aclass.%20We%20also%20use%20a%20dynamic%20step%20size%20adjustment%20algorithm%20to%20enhance%20the%0Adefensive%20noise%20generation%20process.%20Extensive%20experiments%20are%20conducted%20on%204%0Adatasets%20and%205%20data%20augmentation%20methods%20to%20verify%20the%20performance%20of%20ARMOR.%0AComparisons%20with%206%20state-of-the-art%20defense%20methods%20have%20demonstrated%20that%0AARMOR%20can%20preserve%20the%20unlearnability%20of%20protected%20private%20data%20under%20data%0Aaugmentation.%20ARMOR%20reduces%20the%20test%20accuracy%20of%20the%20model%20trained%20on%20augmented%0Aprotected%20samples%20by%20as%20much%20as%2060%25%20more%20than%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARMOR%253A%2520Shielding%2520Unlearnable%2520Examples%2520against%2520Data%2520Augmentation%26entry.906535625%3DXueluan%2520Gong%2520and%2520Yuji%2520Wang%2520and%2520Yanjiao%2520Chen%2520and%2520Haocheng%2520Dong%2520and%2520Yiming%2520Li%2520and%2520Mengyuan%2520Sun%2520and%2520Shuaike%2520Li%2520and%2520Qian%2520Wang%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Private%2520data%252C%2520when%2520published%2520online%252C%2520may%2520be%2520collected%2520by%2520unauthorized%2520parties%250Ato%2520train%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520To%2520protect%2520privacy%252C%2520defensive%2520noises%2520can%250Abe%2520added%2520to%2520original%2520samples%2520to%2520degrade%2520their%2520learnability%2520by%2520DNNs.%2520Recently%252C%250Aunlearnable%2520examples%2520are%2520proposed%2520to%2520minimize%2520the%2520training%2520loss%2520such%2520that%2520the%250Amodel%2520learns%2520almost%2520nothing.%2520However%252C%2520raw%2520data%2520are%2520often%2520pre-processed%2520before%250Abeing%2520used%2520for%2520training%252C%2520which%2520may%2520restore%2520the%2520private%2520information%2520of%2520protected%250Adata.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520the%2520data%2520privacy%2520violation%2520induced%2520by%2520data%250Aaugmentation%252C%2520a%2520commonly%2520used%2520data%2520pre-processing%2520technique%2520to%2520improve%2520model%250Ageneralization%2520capability%252C%2520which%2520is%2520the%2520first%2520of%2520its%2520kind%2520as%2520far%2520as%2520we%2520are%250Aconcerned.%2520We%2520demonstrate%2520that%2520data%2520augmentation%2520can%2520significantly%2520raise%2520the%250Aaccuracy%2520of%2520the%2520model%2520trained%2520on%2520unlearnable%2520examples%2520from%252021.3%2525%2520to%252066.1%2525.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520defense%2520framework%252C%2520dubbed%2520ARMOR%252C%2520to%2520protect%250Adata%2520privacy%2520from%2520potential%2520breaches%2520of%2520data%2520augmentation.%2520To%2520overcome%2520the%250Adifficulty%2520of%2520having%2520no%2520access%2520to%2520the%2520model%2520training%2520process%252C%2520we%2520design%2520a%250Anon-local%2520module-assisted%2520surrogate%2520model%2520that%2520better%2520captures%2520the%2520effect%2520of%250Adata%2520augmentation.%2520In%2520addition%252C%2520we%2520design%2520a%2520surrogate%2520augmentation%2520selection%250Astrategy%2520that%2520maximizes%2520distribution%2520alignment%2520between%2520augmented%2520and%250Anon-augmented%2520samples%252C%2520to%2520choose%2520the%2520optimal%2520augmentation%2520strategy%2520for%2520each%250Aclass.%2520We%2520also%2520use%2520a%2520dynamic%2520step%2520size%2520adjustment%2520algorithm%2520to%2520enhance%2520the%250Adefensive%2520noise%2520generation%2520process.%2520Extensive%2520experiments%2520are%2520conducted%2520on%25204%250Adatasets%2520and%25205%2520data%2520augmentation%2520methods%2520to%2520verify%2520the%2520performance%2520of%2520ARMOR.%250AComparisons%2520with%25206%2520state-of-the-art%2520defense%2520methods%2520have%2520demonstrated%2520that%250AARMOR%2520can%2520preserve%2520the%2520unlearnability%2520of%2520protected%2520private%2520data%2520under%2520data%250Aaugmentation.%2520ARMOR%2520reduces%2520the%2520test%2520accuracy%2520of%2520the%2520model%2520trained%2520on%2520augmented%250Aprotected%2520samples%2520by%2520as%2520much%2520as%252060%2525%2520more%2520than%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARMOR%3A%20Shielding%20Unlearnable%20Examples%20against%20Data%20Augmentation&entry.906535625=Xueluan%20Gong%20and%20Yuji%20Wang%20and%20Yanjiao%20Chen%20and%20Haocheng%20Dong%20and%20Yiming%20Li%20and%20Mengyuan%20Sun%20and%20Shuaike%20Li%20and%20Qian%20Wang%20and%20Chen%20Chen&entry.1292438233=%20%20Private%20data%2C%20when%20published%20online%2C%20may%20be%20collected%20by%20unauthorized%20parties%0Ato%20train%20deep%20neural%20networks%20%28DNNs%29.%20To%20protect%20privacy%2C%20defensive%20noises%20can%0Abe%20added%20to%20original%20samples%20to%20degrade%20their%20learnability%20by%20DNNs.%20Recently%2C%0Aunlearnable%20examples%20are%20proposed%20to%20minimize%20the%20training%20loss%20such%20that%20the%0Amodel%20learns%20almost%20nothing.%20However%2C%20raw%20data%20are%20often%20pre-processed%20before%0Abeing%20used%20for%20training%2C%20which%20may%20restore%20the%20private%20information%20of%20protected%0Adata.%20In%20this%20paper%2C%20we%20reveal%20the%20data%20privacy%20violation%20induced%20by%20data%0Aaugmentation%2C%20a%20commonly%20used%20data%20pre-processing%20technique%20to%20improve%20model%0Ageneralization%20capability%2C%20which%20is%20the%20first%20of%20its%20kind%20as%20far%20as%20we%20are%0Aconcerned.%20We%20demonstrate%20that%20data%20augmentation%20can%20significantly%20raise%20the%0Aaccuracy%20of%20the%20model%20trained%20on%20unlearnable%20examples%20from%2021.3%25%20to%2066.1%25.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20defense%20framework%2C%20dubbed%20ARMOR%2C%20to%20protect%0Adata%20privacy%20from%20potential%20breaches%20of%20data%20augmentation.%20To%20overcome%20the%0Adifficulty%20of%20having%20no%20access%20to%20the%20model%20training%20process%2C%20we%20design%20a%0Anon-local%20module-assisted%20surrogate%20model%20that%20better%20captures%20the%20effect%20of%0Adata%20augmentation.%20In%20addition%2C%20we%20design%20a%20surrogate%20augmentation%20selection%0Astrategy%20that%20maximizes%20distribution%20alignment%20between%20augmented%20and%0Anon-augmented%20samples%2C%20to%20choose%20the%20optimal%20augmentation%20strategy%20for%20each%0Aclass.%20We%20also%20use%20a%20dynamic%20step%20size%20adjustment%20algorithm%20to%20enhance%20the%0Adefensive%20noise%20generation%20process.%20Extensive%20experiments%20are%20conducted%20on%204%0Adatasets%20and%205%20data%20augmentation%20methods%20to%20verify%20the%20performance%20of%20ARMOR.%0AComparisons%20with%206%20state-of-the-art%20defense%20methods%20have%20demonstrated%20that%0AARMOR%20can%20preserve%20the%20unlearnability%20of%20protected%20private%20data%20under%20data%0Aaugmentation.%20ARMOR%20reduces%20the%20test%20accuracy%20of%20the%20model%20trained%20on%20augmented%0Aprotected%20samples%20by%20as%20much%20as%2060%25%20more%20than%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08862v1&entry.124074799=Read"},
{"title": "GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text\n  Detection Challenge", "author": "Liam Dugan and Andrew Zhu and Firoj Alam and Preslav Nakov and Marianna Apidianaki and Chris Callison-Burch", "abstract": "  Recently there have been many shared tasks targeting the detection of\ngenerated text from Large Language Models (LLMs). However, these shared tasks\ntend to focus either on cases where text is limited to one particular domain or\ncases where text can be from many domains, some of which may not be seen during\ntest time. In this shared task, using the newly released RAID benchmark, we aim\nto answer whether or not models can detect generated text from a large, yet\nfixed, number of domains and LLMs, all of which are seen during training. Over\nthe course of three months, our task was attempted by 9 teams with 23 detector\nsubmissions. We find that multiple participants were able to obtain accuracies\nof over 99% on machine-generated text from RAID while maintaining a 5% False\nPositive Rate -- suggesting that detectors are able to robustly detect text\nfrom many domains and models simultaneously. We discuss potential\ninterpretations of this result and provide directions for future research.\n", "link": "http://arxiv.org/abs/2501.08913v1", "date": "2025-01-15", "relevancy": 1.9094, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.494}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4765}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenAI%20Content%20Detection%20Task%203%3A%20Cross-Domain%20Machine-Generated%20Text%0A%20%20Detection%20Challenge&body=Title%3A%20GenAI%20Content%20Detection%20Task%203%3A%20Cross-Domain%20Machine-Generated%20Text%0A%20%20Detection%20Challenge%0AAuthor%3A%20Liam%20Dugan%20and%20Andrew%20Zhu%20and%20Firoj%20Alam%20and%20Preslav%20Nakov%20and%20Marianna%20Apidianaki%20and%20Chris%20Callison-Burch%0AAbstract%3A%20%20%20Recently%20there%20have%20been%20many%20shared%20tasks%20targeting%20the%20detection%20of%0Agenerated%20text%20from%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20these%20shared%20tasks%0Atend%20to%20focus%20either%20on%20cases%20where%20text%20is%20limited%20to%20one%20particular%20domain%20or%0Acases%20where%20text%20can%20be%20from%20many%20domains%2C%20some%20of%20which%20may%20not%20be%20seen%20during%0Atest%20time.%20In%20this%20shared%20task%2C%20using%20the%20newly%20released%20RAID%20benchmark%2C%20we%20aim%0Ato%20answer%20whether%20or%20not%20models%20can%20detect%20generated%20text%20from%20a%20large%2C%20yet%0Afixed%2C%20number%20of%20domains%20and%20LLMs%2C%20all%20of%20which%20are%20seen%20during%20training.%20Over%0Athe%20course%20of%20three%20months%2C%20our%20task%20was%20attempted%20by%209%20teams%20with%2023%20detector%0Asubmissions.%20We%20find%20that%20multiple%20participants%20were%20able%20to%20obtain%20accuracies%0Aof%20over%2099%25%20on%20machine-generated%20text%20from%20RAID%20while%20maintaining%20a%205%25%20False%0APositive%20Rate%20--%20suggesting%20that%20detectors%20are%20able%20to%20robustly%20detect%20text%0Afrom%20many%20domains%20and%20models%20simultaneously.%20We%20discuss%20potential%0Ainterpretations%20of%20this%20result%20and%20provide%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenAI%2520Content%2520Detection%2520Task%25203%253A%2520Cross-Domain%2520Machine-Generated%2520Text%250A%2520%2520Detection%2520Challenge%26entry.906535625%3DLiam%2520Dugan%2520and%2520Andrew%2520Zhu%2520and%2520Firoj%2520Alam%2520and%2520Preslav%2520Nakov%2520and%2520Marianna%2520Apidianaki%2520and%2520Chris%2520Callison-Burch%26entry.1292438233%3D%2520%2520Recently%2520there%2520have%2520been%2520many%2520shared%2520tasks%2520targeting%2520the%2520detection%2520of%250Agenerated%2520text%2520from%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520However%252C%2520these%2520shared%2520tasks%250Atend%2520to%2520focus%2520either%2520on%2520cases%2520where%2520text%2520is%2520limited%2520to%2520one%2520particular%2520domain%2520or%250Acases%2520where%2520text%2520can%2520be%2520from%2520many%2520domains%252C%2520some%2520of%2520which%2520may%2520not%2520be%2520seen%2520during%250Atest%2520time.%2520In%2520this%2520shared%2520task%252C%2520using%2520the%2520newly%2520released%2520RAID%2520benchmark%252C%2520we%2520aim%250Ato%2520answer%2520whether%2520or%2520not%2520models%2520can%2520detect%2520generated%2520text%2520from%2520a%2520large%252C%2520yet%250Afixed%252C%2520number%2520of%2520domains%2520and%2520LLMs%252C%2520all%2520of%2520which%2520are%2520seen%2520during%2520training.%2520Over%250Athe%2520course%2520of%2520three%2520months%252C%2520our%2520task%2520was%2520attempted%2520by%25209%2520teams%2520with%252023%2520detector%250Asubmissions.%2520We%2520find%2520that%2520multiple%2520participants%2520were%2520able%2520to%2520obtain%2520accuracies%250Aof%2520over%252099%2525%2520on%2520machine-generated%2520text%2520from%2520RAID%2520while%2520maintaining%2520a%25205%2525%2520False%250APositive%2520Rate%2520--%2520suggesting%2520that%2520detectors%2520are%2520able%2520to%2520robustly%2520detect%2520text%250Afrom%2520many%2520domains%2520and%2520models%2520simultaneously.%2520We%2520discuss%2520potential%250Ainterpretations%2520of%2520this%2520result%2520and%2520provide%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenAI%20Content%20Detection%20Task%203%3A%20Cross-Domain%20Machine-Generated%20Text%0A%20%20Detection%20Challenge&entry.906535625=Liam%20Dugan%20and%20Andrew%20Zhu%20and%20Firoj%20Alam%20and%20Preslav%20Nakov%20and%20Marianna%20Apidianaki%20and%20Chris%20Callison-Burch&entry.1292438233=%20%20Recently%20there%20have%20been%20many%20shared%20tasks%20targeting%20the%20detection%20of%0Agenerated%20text%20from%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20these%20shared%20tasks%0Atend%20to%20focus%20either%20on%20cases%20where%20text%20is%20limited%20to%20one%20particular%20domain%20or%0Acases%20where%20text%20can%20be%20from%20many%20domains%2C%20some%20of%20which%20may%20not%20be%20seen%20during%0Atest%20time.%20In%20this%20shared%20task%2C%20using%20the%20newly%20released%20RAID%20benchmark%2C%20we%20aim%0Ato%20answer%20whether%20or%20not%20models%20can%20detect%20generated%20text%20from%20a%20large%2C%20yet%0Afixed%2C%20number%20of%20domains%20and%20LLMs%2C%20all%20of%20which%20are%20seen%20during%20training.%20Over%0Athe%20course%20of%20three%20months%2C%20our%20task%20was%20attempted%20by%209%20teams%20with%2023%20detector%0Asubmissions.%20We%20find%20that%20multiple%20participants%20were%20able%20to%20obtain%20accuracies%0Aof%20over%2099%25%20on%20machine-generated%20text%20from%20RAID%20while%20maintaining%20a%205%25%20False%0APositive%20Rate%20--%20suggesting%20that%20detectors%20are%20able%20to%20robustly%20detect%20text%0Afrom%20many%20domains%20and%20models%20simultaneously.%20We%20discuss%20potential%0Ainterpretations%20of%20this%20result%20and%20provide%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08913v1&entry.124074799=Read"},
{"title": "SAIF: A Comprehensive Framework for Evaluating the Risks of Generative\n  AI in the Public Sector", "author": "Kyeongryul Lee and Heehyeon Kim and Joyce Jiyoung Whang", "abstract": "  The rapid adoption of generative AI in the public sector, encompassing\ndiverse applications ranging from automated public assistance to welfare\nservices and immigration processes, highlights its transformative potential\nwhile underscoring the pressing need for thorough risk assessments. Despite its\ngrowing presence, evaluations of risks associated with AI-driven systems in the\npublic sector remain insufficiently explored. Building upon an established\ntaxonomy of AI risks derived from diverse government policies and corporate\nguidelines, we investigate the critical risks posed by generative AI in the\npublic sector while extending the scope to account for its multimodal\ncapabilities. In addition, we propose a Systematic dAta generatIon Framework\nfor evaluating the risks of generative AI (SAIF). SAIF involves four key\nstages: breaking down risks, designing scenarios, applying jailbreak methods,\nand exploring prompt types. It ensures the systematic and consistent generation\nof prompt data, facilitating a comprehensive evaluation while providing a solid\nfoundation for mitigating the risks. Furthermore, SAIF is designed to\naccommodate emerging jailbreak methods and evolving prompt types, thereby\nenabling effective responses to unforeseen risk scenarios. We believe that this\nstudy can play a crucial role in fostering the safe and responsible integration\nof generative AI into the public sector.\n", "link": "http://arxiv.org/abs/2501.08814v1", "date": "2025-01-15", "relevancy": 1.9084, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4915}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4701}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAIF%3A%20A%20Comprehensive%20Framework%20for%20Evaluating%20the%20Risks%20of%20Generative%0A%20%20AI%20in%20the%20Public%20Sector&body=Title%3A%20SAIF%3A%20A%20Comprehensive%20Framework%20for%20Evaluating%20the%20Risks%20of%20Generative%0A%20%20AI%20in%20the%20Public%20Sector%0AAuthor%3A%20Kyeongryul%20Lee%20and%20Heehyeon%20Kim%20and%20Joyce%20Jiyoung%20Whang%0AAbstract%3A%20%20%20The%20rapid%20adoption%20of%20generative%20AI%20in%20the%20public%20sector%2C%20encompassing%0Adiverse%20applications%20ranging%20from%20automated%20public%20assistance%20to%20welfare%0Aservices%20and%20immigration%20processes%2C%20highlights%20its%20transformative%20potential%0Awhile%20underscoring%20the%20pressing%20need%20for%20thorough%20risk%20assessments.%20Despite%20its%0Agrowing%20presence%2C%20evaluations%20of%20risks%20associated%20with%20AI-driven%20systems%20in%20the%0Apublic%20sector%20remain%20insufficiently%20explored.%20Building%20upon%20an%20established%0Ataxonomy%20of%20AI%20risks%20derived%20from%20diverse%20government%20policies%20and%20corporate%0Aguidelines%2C%20we%20investigate%20the%20critical%20risks%20posed%20by%20generative%20AI%20in%20the%0Apublic%20sector%20while%20extending%20the%20scope%20to%20account%20for%20its%20multimodal%0Acapabilities.%20In%20addition%2C%20we%20propose%20a%20Systematic%20dAta%20generatIon%20Framework%0Afor%20evaluating%20the%20risks%20of%20generative%20AI%20%28SAIF%29.%20SAIF%20involves%20four%20key%0Astages%3A%20breaking%20down%20risks%2C%20designing%20scenarios%2C%20applying%20jailbreak%20methods%2C%0Aand%20exploring%20prompt%20types.%20It%20ensures%20the%20systematic%20and%20consistent%20generation%0Aof%20prompt%20data%2C%20facilitating%20a%20comprehensive%20evaluation%20while%20providing%20a%20solid%0Afoundation%20for%20mitigating%20the%20risks.%20Furthermore%2C%20SAIF%20is%20designed%20to%0Aaccommodate%20emerging%20jailbreak%20methods%20and%20evolving%20prompt%20types%2C%20thereby%0Aenabling%20effective%20responses%20to%20unforeseen%20risk%20scenarios.%20We%20believe%20that%20this%0Astudy%20can%20play%20a%20crucial%20role%20in%20fostering%20the%20safe%20and%20responsible%20integration%0Aof%20generative%20AI%20into%20the%20public%20sector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAIF%253A%2520A%2520Comprehensive%2520Framework%2520for%2520Evaluating%2520the%2520Risks%2520of%2520Generative%250A%2520%2520AI%2520in%2520the%2520Public%2520Sector%26entry.906535625%3DKyeongryul%2520Lee%2520and%2520Heehyeon%2520Kim%2520and%2520Joyce%2520Jiyoung%2520Whang%26entry.1292438233%3D%2520%2520The%2520rapid%2520adoption%2520of%2520generative%2520AI%2520in%2520the%2520public%2520sector%252C%2520encompassing%250Adiverse%2520applications%2520ranging%2520from%2520automated%2520public%2520assistance%2520to%2520welfare%250Aservices%2520and%2520immigration%2520processes%252C%2520highlights%2520its%2520transformative%2520potential%250Awhile%2520underscoring%2520the%2520pressing%2520need%2520for%2520thorough%2520risk%2520assessments.%2520Despite%2520its%250Agrowing%2520presence%252C%2520evaluations%2520of%2520risks%2520associated%2520with%2520AI-driven%2520systems%2520in%2520the%250Apublic%2520sector%2520remain%2520insufficiently%2520explored.%2520Building%2520upon%2520an%2520established%250Ataxonomy%2520of%2520AI%2520risks%2520derived%2520from%2520diverse%2520government%2520policies%2520and%2520corporate%250Aguidelines%252C%2520we%2520investigate%2520the%2520critical%2520risks%2520posed%2520by%2520generative%2520AI%2520in%2520the%250Apublic%2520sector%2520while%2520extending%2520the%2520scope%2520to%2520account%2520for%2520its%2520multimodal%250Acapabilities.%2520In%2520addition%252C%2520we%2520propose%2520a%2520Systematic%2520dAta%2520generatIon%2520Framework%250Afor%2520evaluating%2520the%2520risks%2520of%2520generative%2520AI%2520%2528SAIF%2529.%2520SAIF%2520involves%2520four%2520key%250Astages%253A%2520breaking%2520down%2520risks%252C%2520designing%2520scenarios%252C%2520applying%2520jailbreak%2520methods%252C%250Aand%2520exploring%2520prompt%2520types.%2520It%2520ensures%2520the%2520systematic%2520and%2520consistent%2520generation%250Aof%2520prompt%2520data%252C%2520facilitating%2520a%2520comprehensive%2520evaluation%2520while%2520providing%2520a%2520solid%250Afoundation%2520for%2520mitigating%2520the%2520risks.%2520Furthermore%252C%2520SAIF%2520is%2520designed%2520to%250Aaccommodate%2520emerging%2520jailbreak%2520methods%2520and%2520evolving%2520prompt%2520types%252C%2520thereby%250Aenabling%2520effective%2520responses%2520to%2520unforeseen%2520risk%2520scenarios.%2520We%2520believe%2520that%2520this%250Astudy%2520can%2520play%2520a%2520crucial%2520role%2520in%2520fostering%2520the%2520safe%2520and%2520responsible%2520integration%250Aof%2520generative%2520AI%2520into%2520the%2520public%2520sector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAIF%3A%20A%20Comprehensive%20Framework%20for%20Evaluating%20the%20Risks%20of%20Generative%0A%20%20AI%20in%20the%20Public%20Sector&entry.906535625=Kyeongryul%20Lee%20and%20Heehyeon%20Kim%20and%20Joyce%20Jiyoung%20Whang&entry.1292438233=%20%20The%20rapid%20adoption%20of%20generative%20AI%20in%20the%20public%20sector%2C%20encompassing%0Adiverse%20applications%20ranging%20from%20automated%20public%20assistance%20to%20welfare%0Aservices%20and%20immigration%20processes%2C%20highlights%20its%20transformative%20potential%0Awhile%20underscoring%20the%20pressing%20need%20for%20thorough%20risk%20assessments.%20Despite%20its%0Agrowing%20presence%2C%20evaluations%20of%20risks%20associated%20with%20AI-driven%20systems%20in%20the%0Apublic%20sector%20remain%20insufficiently%20explored.%20Building%20upon%20an%20established%0Ataxonomy%20of%20AI%20risks%20derived%20from%20diverse%20government%20policies%20and%20corporate%0Aguidelines%2C%20we%20investigate%20the%20critical%20risks%20posed%20by%20generative%20AI%20in%20the%0Apublic%20sector%20while%20extending%20the%20scope%20to%20account%20for%20its%20multimodal%0Acapabilities.%20In%20addition%2C%20we%20propose%20a%20Systematic%20dAta%20generatIon%20Framework%0Afor%20evaluating%20the%20risks%20of%20generative%20AI%20%28SAIF%29.%20SAIF%20involves%20four%20key%0Astages%3A%20breaking%20down%20risks%2C%20designing%20scenarios%2C%20applying%20jailbreak%20methods%2C%0Aand%20exploring%20prompt%20types.%20It%20ensures%20the%20systematic%20and%20consistent%20generation%0Aof%20prompt%20data%2C%20facilitating%20a%20comprehensive%20evaluation%20while%20providing%20a%20solid%0Afoundation%20for%20mitigating%20the%20risks.%20Furthermore%2C%20SAIF%20is%20designed%20to%0Aaccommodate%20emerging%20jailbreak%20methods%20and%20evolving%20prompt%20types%2C%20thereby%0Aenabling%20effective%20responses%20to%20unforeseen%20risk%20scenarios.%20We%20believe%20that%20this%0Astudy%20can%20play%20a%20crucial%20role%20in%20fostering%20the%20safe%20and%20responsible%20integration%0Aof%20generative%20AI%20into%20the%20public%20sector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08814v1&entry.124074799=Read"},
{"title": "Projection Implicit Q-Learning with Support Constraint for Offline\n  Reinforcement Learning", "author": "Xinchen Han and Hossam Afifi and Michel Marot", "abstract": "  Offline Reinforcement Learning (RL) faces a critical challenge of\nextrapolation errors caused by out-of-distribution (OOD) actions. Implicit\nQ-Learning (IQL) algorithm employs expectile regression to achieve in-sample\nlearning, effectively mitigating the risks associated with OOD actions.\nHowever, the fixed hyperparameter in policy evaluation and density-based policy\nimprovement method limit its overall efficiency. In this paper, we propose\nProj-IQL, a projective IQL algorithm enhanced with the support constraint. In\nthe policy evaluation phase, Proj-IQL generalizes the one-step approach to a\nmulti-step approach through vector projection, while maintaining in-sample\nlearning and expectile regression framework. In the policy improvement phase,\nProj-IQL introduces support constraint that is more aligned with the policy\nevaluation approach. Furthermore, we theoretically demonstrate that Proj-IQL\nguarantees monotonic policy improvement and enjoys a progressively more\nrigorous criterion for superior actions. Empirical results demonstrate the\nProj-IQL achieves state-of-the-art performance on D4RL benchmarks, especially\nin challenging navigation domains.\n", "link": "http://arxiv.org/abs/2501.08907v1", "date": "2025-01-15", "relevancy": 1.9042, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4979}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4849}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Projection%20Implicit%20Q-Learning%20with%20Support%20Constraint%20for%20Offline%0A%20%20Reinforcement%20Learning&body=Title%3A%20Projection%20Implicit%20Q-Learning%20with%20Support%20Constraint%20for%20Offline%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Xinchen%20Han%20and%20Hossam%20Afifi%20and%20Michel%20Marot%0AAbstract%3A%20%20%20Offline%20Reinforcement%20Learning%20%28RL%29%20faces%20a%20critical%20challenge%20of%0Aextrapolation%20errors%20caused%20by%20out-of-distribution%20%28OOD%29%20actions.%20Implicit%0AQ-Learning%20%28IQL%29%20algorithm%20employs%20expectile%20regression%20to%20achieve%20in-sample%0Alearning%2C%20effectively%20mitigating%20the%20risks%20associated%20with%20OOD%20actions.%0AHowever%2C%20the%20fixed%20hyperparameter%20in%20policy%20evaluation%20and%20density-based%20policy%0Aimprovement%20method%20limit%20its%20overall%20efficiency.%20In%20this%20paper%2C%20we%20propose%0AProj-IQL%2C%20a%20projective%20IQL%20algorithm%20enhanced%20with%20the%20support%20constraint.%20In%0Athe%20policy%20evaluation%20phase%2C%20Proj-IQL%20generalizes%20the%20one-step%20approach%20to%20a%0Amulti-step%20approach%20through%20vector%20projection%2C%20while%20maintaining%20in-sample%0Alearning%20and%20expectile%20regression%20framework.%20In%20the%20policy%20improvement%20phase%2C%0AProj-IQL%20introduces%20support%20constraint%20that%20is%20more%20aligned%20with%20the%20policy%0Aevaluation%20approach.%20Furthermore%2C%20we%20theoretically%20demonstrate%20that%20Proj-IQL%0Aguarantees%20monotonic%20policy%20improvement%20and%20enjoys%20a%20progressively%20more%0Arigorous%20criterion%20for%20superior%20actions.%20Empirical%20results%20demonstrate%20the%0AProj-IQL%20achieves%20state-of-the-art%20performance%20on%20D4RL%20benchmarks%2C%20especially%0Ain%20challenging%20navigation%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProjection%2520Implicit%2520Q-Learning%2520with%2520Support%2520Constraint%2520for%2520Offline%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DXinchen%2520Han%2520and%2520Hossam%2520Afifi%2520and%2520Michel%2520Marot%26entry.1292438233%3D%2520%2520Offline%2520Reinforcement%2520Learning%2520%2528RL%2529%2520faces%2520a%2520critical%2520challenge%2520of%250Aextrapolation%2520errors%2520caused%2520by%2520out-of-distribution%2520%2528OOD%2529%2520actions.%2520Implicit%250AQ-Learning%2520%2528IQL%2529%2520algorithm%2520employs%2520expectile%2520regression%2520to%2520achieve%2520in-sample%250Alearning%252C%2520effectively%2520mitigating%2520the%2520risks%2520associated%2520with%2520OOD%2520actions.%250AHowever%252C%2520the%2520fixed%2520hyperparameter%2520in%2520policy%2520evaluation%2520and%2520density-based%2520policy%250Aimprovement%2520method%2520limit%2520its%2520overall%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520propose%250AProj-IQL%252C%2520a%2520projective%2520IQL%2520algorithm%2520enhanced%2520with%2520the%2520support%2520constraint.%2520In%250Athe%2520policy%2520evaluation%2520phase%252C%2520Proj-IQL%2520generalizes%2520the%2520one-step%2520approach%2520to%2520a%250Amulti-step%2520approach%2520through%2520vector%2520projection%252C%2520while%2520maintaining%2520in-sample%250Alearning%2520and%2520expectile%2520regression%2520framework.%2520In%2520the%2520policy%2520improvement%2520phase%252C%250AProj-IQL%2520introduces%2520support%2520constraint%2520that%2520is%2520more%2520aligned%2520with%2520the%2520policy%250Aevaluation%2520approach.%2520Furthermore%252C%2520we%2520theoretically%2520demonstrate%2520that%2520Proj-IQL%250Aguarantees%2520monotonic%2520policy%2520improvement%2520and%2520enjoys%2520a%2520progressively%2520more%250Arigorous%2520criterion%2520for%2520superior%2520actions.%2520Empirical%2520results%2520demonstrate%2520the%250AProj-IQL%2520achieves%2520state-of-the-art%2520performance%2520on%2520D4RL%2520benchmarks%252C%2520especially%250Ain%2520challenging%2520navigation%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Projection%20Implicit%20Q-Learning%20with%20Support%20Constraint%20for%20Offline%0A%20%20Reinforcement%20Learning&entry.906535625=Xinchen%20Han%20and%20Hossam%20Afifi%20and%20Michel%20Marot&entry.1292438233=%20%20Offline%20Reinforcement%20Learning%20%28RL%29%20faces%20a%20critical%20challenge%20of%0Aextrapolation%20errors%20caused%20by%20out-of-distribution%20%28OOD%29%20actions.%20Implicit%0AQ-Learning%20%28IQL%29%20algorithm%20employs%20expectile%20regression%20to%20achieve%20in-sample%0Alearning%2C%20effectively%20mitigating%20the%20risks%20associated%20with%20OOD%20actions.%0AHowever%2C%20the%20fixed%20hyperparameter%20in%20policy%20evaluation%20and%20density-based%20policy%0Aimprovement%20method%20limit%20its%20overall%20efficiency.%20In%20this%20paper%2C%20we%20propose%0AProj-IQL%2C%20a%20projective%20IQL%20algorithm%20enhanced%20with%20the%20support%20constraint.%20In%0Athe%20policy%20evaluation%20phase%2C%20Proj-IQL%20generalizes%20the%20one-step%20approach%20to%20a%0Amulti-step%20approach%20through%20vector%20projection%2C%20while%20maintaining%20in-sample%0Alearning%20and%20expectile%20regression%20framework.%20In%20the%20policy%20improvement%20phase%2C%0AProj-IQL%20introduces%20support%20constraint%20that%20is%20more%20aligned%20with%20the%20policy%0Aevaluation%20approach.%20Furthermore%2C%20we%20theoretically%20demonstrate%20that%20Proj-IQL%0Aguarantees%20monotonic%20policy%20improvement%20and%20enjoys%20a%20progressively%20more%0Arigorous%20criterion%20for%20superior%20actions.%20Empirical%20results%20demonstrate%20the%0AProj-IQL%20achieves%20state-of-the-art%20performance%20on%20D4RL%20benchmarks%2C%20especially%0Ain%20challenging%20navigation%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08907v1&entry.124074799=Read"},
{"title": "Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model\n  Deforestation: An Exemplification from the Amazon Rainforest", "author": "Karthik R. and Ramamoorthy A", "abstract": "  Intelligent automation supports us against cyclones, droughts, and seismic\nevents with recent technology advancements. Algorithmic learning has advanced\nfields like neuroscience, genetics, and human-computer interaction. Time-series\ndata boosts progress. Challenges persist in adopting these approaches in\ntraditional fields. Neural networks face comprehension and bias issues. AI's\nexpansion across scientific areas is due to adaptable descriptors and\ncombinatorial argumentation. This article focuses on modeling Forest loss using\nthe VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest\ncover, demonstrated on Amazon Rainforest data against other forecasters like\nLong Short-Term Memory, N-BEATS, RCN.\n", "link": "http://arxiv.org/abs/2308.06471v2", "date": "2025-01-15", "relevancy": 1.8933, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4694}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Volterra%20Accentuated%20Non-Linear%20Dynamical%20Admittance%20%28VANYA%29%20to%20model%0A%20%20Deforestation%3A%20An%20Exemplification%20from%20the%20Amazon%20Rainforest&body=Title%3A%20Volterra%20Accentuated%20Non-Linear%20Dynamical%20Admittance%20%28VANYA%29%20to%20model%0A%20%20Deforestation%3A%20An%20Exemplification%20from%20the%20Amazon%20Rainforest%0AAuthor%3A%20Karthik%20R.%20and%20Ramamoorthy%20A%0AAbstract%3A%20%20%20Intelligent%20automation%20supports%20us%20against%20cyclones%2C%20droughts%2C%20and%20seismic%0Aevents%20with%20recent%20technology%20advancements.%20Algorithmic%20learning%20has%20advanced%0Afields%20like%20neuroscience%2C%20genetics%2C%20and%20human-computer%20interaction.%20Time-series%0Adata%20boosts%20progress.%20Challenges%20persist%20in%20adopting%20these%20approaches%20in%0Atraditional%20fields.%20Neural%20networks%20face%20comprehension%20and%20bias%20issues.%20AI%27s%0Aexpansion%20across%20scientific%20areas%20is%20due%20to%20adaptable%20descriptors%20and%0Acombinatorial%20argumentation.%20This%20article%20focuses%20on%20modeling%20Forest%20loss%20using%0Athe%20VANYA%20Model%2C%20incorporating%20Prey%20Predator%20Dynamics.%20VANYA%20predicts%20forest%0Acover%2C%20demonstrated%20on%20Amazon%20Rainforest%20data%20against%20other%20forecasters%20like%0ALong%20Short-Term%20Memory%2C%20N-BEATS%2C%20RCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.06471v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolterra%2520Accentuated%2520Non-Linear%2520Dynamical%2520Admittance%2520%2528VANYA%2529%2520to%2520model%250A%2520%2520Deforestation%253A%2520An%2520Exemplification%2520from%2520the%2520Amazon%2520Rainforest%26entry.906535625%3DKarthik%2520R.%2520and%2520Ramamoorthy%2520A%26entry.1292438233%3D%2520%2520Intelligent%2520automation%2520supports%2520us%2520against%2520cyclones%252C%2520droughts%252C%2520and%2520seismic%250Aevents%2520with%2520recent%2520technology%2520advancements.%2520Algorithmic%2520learning%2520has%2520advanced%250Afields%2520like%2520neuroscience%252C%2520genetics%252C%2520and%2520human-computer%2520interaction.%2520Time-series%250Adata%2520boosts%2520progress.%2520Challenges%2520persist%2520in%2520adopting%2520these%2520approaches%2520in%250Atraditional%2520fields.%2520Neural%2520networks%2520face%2520comprehension%2520and%2520bias%2520issues.%2520AI%2527s%250Aexpansion%2520across%2520scientific%2520areas%2520is%2520due%2520to%2520adaptable%2520descriptors%2520and%250Acombinatorial%2520argumentation.%2520This%2520article%2520focuses%2520on%2520modeling%2520Forest%2520loss%2520using%250Athe%2520VANYA%2520Model%252C%2520incorporating%2520Prey%2520Predator%2520Dynamics.%2520VANYA%2520predicts%2520forest%250Acover%252C%2520demonstrated%2520on%2520Amazon%2520Rainforest%2520data%2520against%2520other%2520forecasters%2520like%250ALong%2520Short-Term%2520Memory%252C%2520N-BEATS%252C%2520RCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.06471v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Volterra%20Accentuated%20Non-Linear%20Dynamical%20Admittance%20%28VANYA%29%20to%20model%0A%20%20Deforestation%3A%20An%20Exemplification%20from%20the%20Amazon%20Rainforest&entry.906535625=Karthik%20R.%20and%20Ramamoorthy%20A&entry.1292438233=%20%20Intelligent%20automation%20supports%20us%20against%20cyclones%2C%20droughts%2C%20and%20seismic%0Aevents%20with%20recent%20technology%20advancements.%20Algorithmic%20learning%20has%20advanced%0Afields%20like%20neuroscience%2C%20genetics%2C%20and%20human-computer%20interaction.%20Time-series%0Adata%20boosts%20progress.%20Challenges%20persist%20in%20adopting%20these%20approaches%20in%0Atraditional%20fields.%20Neural%20networks%20face%20comprehension%20and%20bias%20issues.%20AI%27s%0Aexpansion%20across%20scientific%20areas%20is%20due%20to%20adaptable%20descriptors%20and%0Acombinatorial%20argumentation.%20This%20article%20focuses%20on%20modeling%20Forest%20loss%20using%0Athe%20VANYA%20Model%2C%20incorporating%20Prey%20Predator%20Dynamics.%20VANYA%20predicts%20forest%0Acover%2C%20demonstrated%20on%20Amazon%20Rainforest%20data%20against%20other%20forecasters%20like%0ALong%20Short-Term%20Memory%2C%20N-BEATS%2C%20RCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.06471v2&entry.124074799=Read"},
{"title": "Empowering Agricultural Insights: RiceLeafBD - A Novel Dataset and\n  Optimal Model Selection for Rice Leaf Disease Diagnosis through Transfer\n  Learning Technique", "author": "Sadia Afrin Rimi and Md. Jalal Uddin Chowdhury and Rifat Abdullah and Iftekhar Ahmed and Mahrima Akter Mim and Mohammad Shoaib Rahman", "abstract": "  The number of people living in this agricultural nation of ours, which is\nsurrounded by lush greenery, is growing on a daily basis. As a result of this,\nthe level of arable land is decreasing, as well as residential houses and\nindustrial factories. The food crisis is becoming the main threat for us in the\nupcoming days. Because on the one hand, the population is increasing, and on\nthe other hand, the amount of food crop production is decreasing due to the\nattack of diseases. Rice is one of the most significant cultivated crops since\nit provides food for more than half of the world's population. Bangladesh is\ndependent on rice (Oryza sativa) as a vital crop for its agriculture, but it\nfaces a significant problem as a result of the ongoing decline in rice yield\nbrought on by common diseases. Early disease detection is the main difficulty\nin rice crop cultivation. In this paper, we proposed our own dataset, which was\ncollected from the Bangladesh field, and also applied deep learning and\ntransfer learning models for the evaluation of the datasets. We elaborately\nexplain our dataset and also give direction for further research work to serve\nsociety using this dataset. We applied a light CNN model and pre-trained\nInceptionNet-V2, EfficientNet-V2, and MobileNet-V2 models, which achieved 91.5%\nperformance for the EfficientNet-V2 model of this work. The results obtained\nassaulted other models and even exceeded approaches that are considered to be\npart of the state of the art. It has been demonstrated by this study that it is\npossible to precisely and effectively identify diseases that affect rice leaves\nusing this unbiased datasets. After analysis of the performance of different\nmodels, the proposed datasets are significant for the society for research work\nto provide solutions for decreasing rice leaf disease.\n", "link": "http://arxiv.org/abs/2501.08912v1", "date": "2025-01-15", "relevancy": 1.8628, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4698}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4685}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Agricultural%20Insights%3A%20RiceLeafBD%20-%20A%20Novel%20Dataset%20and%0A%20%20Optimal%20Model%20Selection%20for%20Rice%20Leaf%20Disease%20Diagnosis%20through%20Transfer%0A%20%20Learning%20Technique&body=Title%3A%20Empowering%20Agricultural%20Insights%3A%20RiceLeafBD%20-%20A%20Novel%20Dataset%20and%0A%20%20Optimal%20Model%20Selection%20for%20Rice%20Leaf%20Disease%20Diagnosis%20through%20Transfer%0A%20%20Learning%20Technique%0AAuthor%3A%20Sadia%20Afrin%20Rimi%20and%20Md.%20Jalal%20Uddin%20Chowdhury%20and%20Rifat%20Abdullah%20and%20Iftekhar%20Ahmed%20and%20Mahrima%20Akter%20Mim%20and%20Mohammad%20Shoaib%20Rahman%0AAbstract%3A%20%20%20The%20number%20of%20people%20living%20in%20this%20agricultural%20nation%20of%20ours%2C%20which%20is%0Asurrounded%20by%20lush%20greenery%2C%20is%20growing%20on%20a%20daily%20basis.%20As%20a%20result%20of%20this%2C%0Athe%20level%20of%20arable%20land%20is%20decreasing%2C%20as%20well%20as%20residential%20houses%20and%0Aindustrial%20factories.%20The%20food%20crisis%20is%20becoming%20the%20main%20threat%20for%20us%20in%20the%0Aupcoming%20days.%20Because%20on%20the%20one%20hand%2C%20the%20population%20is%20increasing%2C%20and%20on%0Athe%20other%20hand%2C%20the%20amount%20of%20food%20crop%20production%20is%20decreasing%20due%20to%20the%0Aattack%20of%20diseases.%20Rice%20is%20one%20of%20the%20most%20significant%20cultivated%20crops%20since%0Ait%20provides%20food%20for%20more%20than%20half%20of%20the%20world%27s%20population.%20Bangladesh%20is%0Adependent%20on%20rice%20%28Oryza%20sativa%29%20as%20a%20vital%20crop%20for%20its%20agriculture%2C%20but%20it%0Afaces%20a%20significant%20problem%20as%20a%20result%20of%20the%20ongoing%20decline%20in%20rice%20yield%0Abrought%20on%20by%20common%20diseases.%20Early%20disease%20detection%20is%20the%20main%20difficulty%0Ain%20rice%20crop%20cultivation.%20In%20this%20paper%2C%20we%20proposed%20our%20own%20dataset%2C%20which%20was%0Acollected%20from%20the%20Bangladesh%20field%2C%20and%20also%20applied%20deep%20learning%20and%0Atransfer%20learning%20models%20for%20the%20evaluation%20of%20the%20datasets.%20We%20elaborately%0Aexplain%20our%20dataset%20and%20also%20give%20direction%20for%20further%20research%20work%20to%20serve%0Asociety%20using%20this%20dataset.%20We%20applied%20a%20light%20CNN%20model%20and%20pre-trained%0AInceptionNet-V2%2C%20EfficientNet-V2%2C%20and%20MobileNet-V2%20models%2C%20which%20achieved%2091.5%25%0Aperformance%20for%20the%20EfficientNet-V2%20model%20of%20this%20work.%20The%20results%20obtained%0Aassaulted%20other%20models%20and%20even%20exceeded%20approaches%20that%20are%20considered%20to%20be%0Apart%20of%20the%20state%20of%20the%20art.%20It%20has%20been%20demonstrated%20by%20this%20study%20that%20it%20is%0Apossible%20to%20precisely%20and%20effectively%20identify%20diseases%20that%20affect%20rice%20leaves%0Ausing%20this%20unbiased%20datasets.%20After%20analysis%20of%20the%20performance%20of%20different%0Amodels%2C%20the%20proposed%20datasets%20are%20significant%20for%20the%20society%20for%20research%20work%0Ato%20provide%20solutions%20for%20decreasing%20rice%20leaf%20disease.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Agricultural%2520Insights%253A%2520RiceLeafBD%2520-%2520A%2520Novel%2520Dataset%2520and%250A%2520%2520Optimal%2520Model%2520Selection%2520for%2520Rice%2520Leaf%2520Disease%2520Diagnosis%2520through%2520Transfer%250A%2520%2520Learning%2520Technique%26entry.906535625%3DSadia%2520Afrin%2520Rimi%2520and%2520Md.%2520Jalal%2520Uddin%2520Chowdhury%2520and%2520Rifat%2520Abdullah%2520and%2520Iftekhar%2520Ahmed%2520and%2520Mahrima%2520Akter%2520Mim%2520and%2520Mohammad%2520Shoaib%2520Rahman%26entry.1292438233%3D%2520%2520The%2520number%2520of%2520people%2520living%2520in%2520this%2520agricultural%2520nation%2520of%2520ours%252C%2520which%2520is%250Asurrounded%2520by%2520lush%2520greenery%252C%2520is%2520growing%2520on%2520a%2520daily%2520basis.%2520As%2520a%2520result%2520of%2520this%252C%250Athe%2520level%2520of%2520arable%2520land%2520is%2520decreasing%252C%2520as%2520well%2520as%2520residential%2520houses%2520and%250Aindustrial%2520factories.%2520The%2520food%2520crisis%2520is%2520becoming%2520the%2520main%2520threat%2520for%2520us%2520in%2520the%250Aupcoming%2520days.%2520Because%2520on%2520the%2520one%2520hand%252C%2520the%2520population%2520is%2520increasing%252C%2520and%2520on%250Athe%2520other%2520hand%252C%2520the%2520amount%2520of%2520food%2520crop%2520production%2520is%2520decreasing%2520due%2520to%2520the%250Aattack%2520of%2520diseases.%2520Rice%2520is%2520one%2520of%2520the%2520most%2520significant%2520cultivated%2520crops%2520since%250Ait%2520provides%2520food%2520for%2520more%2520than%2520half%2520of%2520the%2520world%2527s%2520population.%2520Bangladesh%2520is%250Adependent%2520on%2520rice%2520%2528Oryza%2520sativa%2529%2520as%2520a%2520vital%2520crop%2520for%2520its%2520agriculture%252C%2520but%2520it%250Afaces%2520a%2520significant%2520problem%2520as%2520a%2520result%2520of%2520the%2520ongoing%2520decline%2520in%2520rice%2520yield%250Abrought%2520on%2520by%2520common%2520diseases.%2520Early%2520disease%2520detection%2520is%2520the%2520main%2520difficulty%250Ain%2520rice%2520crop%2520cultivation.%2520In%2520this%2520paper%252C%2520we%2520proposed%2520our%2520own%2520dataset%252C%2520which%2520was%250Acollected%2520from%2520the%2520Bangladesh%2520field%252C%2520and%2520also%2520applied%2520deep%2520learning%2520and%250Atransfer%2520learning%2520models%2520for%2520the%2520evaluation%2520of%2520the%2520datasets.%2520We%2520elaborately%250Aexplain%2520our%2520dataset%2520and%2520also%2520give%2520direction%2520for%2520further%2520research%2520work%2520to%2520serve%250Asociety%2520using%2520this%2520dataset.%2520We%2520applied%2520a%2520light%2520CNN%2520model%2520and%2520pre-trained%250AInceptionNet-V2%252C%2520EfficientNet-V2%252C%2520and%2520MobileNet-V2%2520models%252C%2520which%2520achieved%252091.5%2525%250Aperformance%2520for%2520the%2520EfficientNet-V2%2520model%2520of%2520this%2520work.%2520The%2520results%2520obtained%250Aassaulted%2520other%2520models%2520and%2520even%2520exceeded%2520approaches%2520that%2520are%2520considered%2520to%2520be%250Apart%2520of%2520the%2520state%2520of%2520the%2520art.%2520It%2520has%2520been%2520demonstrated%2520by%2520this%2520study%2520that%2520it%2520is%250Apossible%2520to%2520precisely%2520and%2520effectively%2520identify%2520diseases%2520that%2520affect%2520rice%2520leaves%250Ausing%2520this%2520unbiased%2520datasets.%2520After%2520analysis%2520of%2520the%2520performance%2520of%2520different%250Amodels%252C%2520the%2520proposed%2520datasets%2520are%2520significant%2520for%2520the%2520society%2520for%2520research%2520work%250Ato%2520provide%2520solutions%2520for%2520decreasing%2520rice%2520leaf%2520disease.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Agricultural%20Insights%3A%20RiceLeafBD%20-%20A%20Novel%20Dataset%20and%0A%20%20Optimal%20Model%20Selection%20for%20Rice%20Leaf%20Disease%20Diagnosis%20through%20Transfer%0A%20%20Learning%20Technique&entry.906535625=Sadia%20Afrin%20Rimi%20and%20Md.%20Jalal%20Uddin%20Chowdhury%20and%20Rifat%20Abdullah%20and%20Iftekhar%20Ahmed%20and%20Mahrima%20Akter%20Mim%20and%20Mohammad%20Shoaib%20Rahman&entry.1292438233=%20%20The%20number%20of%20people%20living%20in%20this%20agricultural%20nation%20of%20ours%2C%20which%20is%0Asurrounded%20by%20lush%20greenery%2C%20is%20growing%20on%20a%20daily%20basis.%20As%20a%20result%20of%20this%2C%0Athe%20level%20of%20arable%20land%20is%20decreasing%2C%20as%20well%20as%20residential%20houses%20and%0Aindustrial%20factories.%20The%20food%20crisis%20is%20becoming%20the%20main%20threat%20for%20us%20in%20the%0Aupcoming%20days.%20Because%20on%20the%20one%20hand%2C%20the%20population%20is%20increasing%2C%20and%20on%0Athe%20other%20hand%2C%20the%20amount%20of%20food%20crop%20production%20is%20decreasing%20due%20to%20the%0Aattack%20of%20diseases.%20Rice%20is%20one%20of%20the%20most%20significant%20cultivated%20crops%20since%0Ait%20provides%20food%20for%20more%20than%20half%20of%20the%20world%27s%20population.%20Bangladesh%20is%0Adependent%20on%20rice%20%28Oryza%20sativa%29%20as%20a%20vital%20crop%20for%20its%20agriculture%2C%20but%20it%0Afaces%20a%20significant%20problem%20as%20a%20result%20of%20the%20ongoing%20decline%20in%20rice%20yield%0Abrought%20on%20by%20common%20diseases.%20Early%20disease%20detection%20is%20the%20main%20difficulty%0Ain%20rice%20crop%20cultivation.%20In%20this%20paper%2C%20we%20proposed%20our%20own%20dataset%2C%20which%20was%0Acollected%20from%20the%20Bangladesh%20field%2C%20and%20also%20applied%20deep%20learning%20and%0Atransfer%20learning%20models%20for%20the%20evaluation%20of%20the%20datasets.%20We%20elaborately%0Aexplain%20our%20dataset%20and%20also%20give%20direction%20for%20further%20research%20work%20to%20serve%0Asociety%20using%20this%20dataset.%20We%20applied%20a%20light%20CNN%20model%20and%20pre-trained%0AInceptionNet-V2%2C%20EfficientNet-V2%2C%20and%20MobileNet-V2%20models%2C%20which%20achieved%2091.5%25%0Aperformance%20for%20the%20EfficientNet-V2%20model%20of%20this%20work.%20The%20results%20obtained%0Aassaulted%20other%20models%20and%20even%20exceeded%20approaches%20that%20are%20considered%20to%20be%0Apart%20of%20the%20state%20of%20the%20art.%20It%20has%20been%20demonstrated%20by%20this%20study%20that%20it%20is%0Apossible%20to%20precisely%20and%20effectively%20identify%20diseases%20that%20affect%20rice%20leaves%0Ausing%20this%20unbiased%20datasets.%20After%20analysis%20of%20the%20performance%20of%20different%0Amodels%2C%20the%20proposed%20datasets%20are%20significant%20for%20the%20society%20for%20research%20work%0Ato%20provide%20solutions%20for%20decreasing%20rice%20leaf%20disease.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08912v1&entry.124074799=Read"},
{"title": "Integrating Multi-Physics Simulations and Machine Learning to Define the\n  Spatter Mechanism and Process Window in Laser Powder Bed Fusion", "author": "Olabode T. Ajenifujah and Francis Ogoke and Florian Wirth and Jack Beuth and Amir Barati Farimani", "abstract": "  Laser powder bed fusion (LPBF) has shown promise for wide range of\napplications due to its ability to fabricate freeform geometries and generate a\ncontrolled microstructure. However, components generated by LPBF still possess\nsub-optimal mechanical properties due to the defects that are created during\nlaser-material interactions. In this work, we investigate mechanism of spatter\nformation, using a high-fidelity modelling tool that was built to simulate the\nmulti-physics phenomena in LPBF. The modelling tool have the capability to\ncapture the 3D resolution of the meltpool and the spatter behavior. To\nunderstand spatter behavior and formation, we reveal its properties at ejection\nand evaluate its variation from the meltpool, the source where it is formed.\nThe dataset of the spatter and the meltpool collected consist of 50 % spatter\nand 50 % melt pool samples, with features that include position components,\nvelocity components, velocity magnitude, temperature, density and pressure. The\nrelationship between the spatter and the meltpool were evaluated via\ncorrelation analysis and machine learning (ML) algorithms for classification\ntasks. Upon screening different ML algorithms on the dataset, a high accuracy\nwas observed for all the ML models, with ExtraTrees having the highest at 96 %\nand KNN having the lowest at 94 %.\n", "link": "http://arxiv.org/abs/2405.07823v2", "date": "2025-01-15", "relevancy": 1.8577, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4734}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4677}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Multi-Physics%20Simulations%20and%20Machine%20Learning%20to%20Define%20the%0A%20%20Spatter%20Mechanism%20and%20Process%20Window%20in%20Laser%20Powder%20Bed%20Fusion&body=Title%3A%20Integrating%20Multi-Physics%20Simulations%20and%20Machine%20Learning%20to%20Define%20the%0A%20%20Spatter%20Mechanism%20and%20Process%20Window%20in%20Laser%20Powder%20Bed%20Fusion%0AAuthor%3A%20Olabode%20T.%20Ajenifujah%20and%20Francis%20Ogoke%20and%20Florian%20Wirth%20and%20Jack%20Beuth%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20%20%20Laser%20powder%20bed%20fusion%20%28LPBF%29%20has%20shown%20promise%20for%20wide%20range%20of%0Aapplications%20due%20to%20its%20ability%20to%20fabricate%20freeform%20geometries%20and%20generate%20a%0Acontrolled%20microstructure.%20However%2C%20components%20generated%20by%20LPBF%20still%20possess%0Asub-optimal%20mechanical%20properties%20due%20to%20the%20defects%20that%20are%20created%20during%0Alaser-material%20interactions.%20In%20this%20work%2C%20we%20investigate%20mechanism%20of%20spatter%0Aformation%2C%20using%20a%20high-fidelity%20modelling%20tool%20that%20was%20built%20to%20simulate%20the%0Amulti-physics%20phenomena%20in%20LPBF.%20The%20modelling%20tool%20have%20the%20capability%20to%0Acapture%20the%203D%20resolution%20of%20the%20meltpool%20and%20the%20spatter%20behavior.%20To%0Aunderstand%20spatter%20behavior%20and%20formation%2C%20we%20reveal%20its%20properties%20at%20ejection%0Aand%20evaluate%20its%20variation%20from%20the%20meltpool%2C%20the%20source%20where%20it%20is%20formed.%0AThe%20dataset%20of%20the%20spatter%20and%20the%20meltpool%20collected%20consist%20of%2050%20%25%20spatter%0Aand%2050%20%25%20melt%20pool%20samples%2C%20with%20features%20that%20include%20position%20components%2C%0Avelocity%20components%2C%20velocity%20magnitude%2C%20temperature%2C%20density%20and%20pressure.%20The%0Arelationship%20between%20the%20spatter%20and%20the%20meltpool%20were%20evaluated%20via%0Acorrelation%20analysis%20and%20machine%20learning%20%28ML%29%20algorithms%20for%20classification%0Atasks.%20Upon%20screening%20different%20ML%20algorithms%20on%20the%20dataset%2C%20a%20high%20accuracy%0Awas%20observed%20for%20all%20the%20ML%20models%2C%20with%20ExtraTrees%20having%20the%20highest%20at%2096%20%25%0Aand%20KNN%20having%20the%20lowest%20at%2094%20%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Multi-Physics%2520Simulations%2520and%2520Machine%2520Learning%2520to%2520Define%2520the%250A%2520%2520Spatter%2520Mechanism%2520and%2520Process%2520Window%2520in%2520Laser%2520Powder%2520Bed%2520Fusion%26entry.906535625%3DOlabode%2520T.%2520Ajenifujah%2520and%2520Francis%2520Ogoke%2520and%2520Florian%2520Wirth%2520and%2520Jack%2520Beuth%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3D%2520%2520Laser%2520powder%2520bed%2520fusion%2520%2528LPBF%2529%2520has%2520shown%2520promise%2520for%2520wide%2520range%2520of%250Aapplications%2520due%2520to%2520its%2520ability%2520to%2520fabricate%2520freeform%2520geometries%2520and%2520generate%2520a%250Acontrolled%2520microstructure.%2520However%252C%2520components%2520generated%2520by%2520LPBF%2520still%2520possess%250Asub-optimal%2520mechanical%2520properties%2520due%2520to%2520the%2520defects%2520that%2520are%2520created%2520during%250Alaser-material%2520interactions.%2520In%2520this%2520work%252C%2520we%2520investigate%2520mechanism%2520of%2520spatter%250Aformation%252C%2520using%2520a%2520high-fidelity%2520modelling%2520tool%2520that%2520was%2520built%2520to%2520simulate%2520the%250Amulti-physics%2520phenomena%2520in%2520LPBF.%2520The%2520modelling%2520tool%2520have%2520the%2520capability%2520to%250Acapture%2520the%25203D%2520resolution%2520of%2520the%2520meltpool%2520and%2520the%2520spatter%2520behavior.%2520To%250Aunderstand%2520spatter%2520behavior%2520and%2520formation%252C%2520we%2520reveal%2520its%2520properties%2520at%2520ejection%250Aand%2520evaluate%2520its%2520variation%2520from%2520the%2520meltpool%252C%2520the%2520source%2520where%2520it%2520is%2520formed.%250AThe%2520dataset%2520of%2520the%2520spatter%2520and%2520the%2520meltpool%2520collected%2520consist%2520of%252050%2520%2525%2520spatter%250Aand%252050%2520%2525%2520melt%2520pool%2520samples%252C%2520with%2520features%2520that%2520include%2520position%2520components%252C%250Avelocity%2520components%252C%2520velocity%2520magnitude%252C%2520temperature%252C%2520density%2520and%2520pressure.%2520The%250Arelationship%2520between%2520the%2520spatter%2520and%2520the%2520meltpool%2520were%2520evaluated%2520via%250Acorrelation%2520analysis%2520and%2520machine%2520learning%2520%2528ML%2529%2520algorithms%2520for%2520classification%250Atasks.%2520Upon%2520screening%2520different%2520ML%2520algorithms%2520on%2520the%2520dataset%252C%2520a%2520high%2520accuracy%250Awas%2520observed%2520for%2520all%2520the%2520ML%2520models%252C%2520with%2520ExtraTrees%2520having%2520the%2520highest%2520at%252096%2520%2525%250Aand%2520KNN%2520having%2520the%2520lowest%2520at%252094%2520%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Multi-Physics%20Simulations%20and%20Machine%20Learning%20to%20Define%20the%0A%20%20Spatter%20Mechanism%20and%20Process%20Window%20in%20Laser%20Powder%20Bed%20Fusion&entry.906535625=Olabode%20T.%20Ajenifujah%20and%20Francis%20Ogoke%20and%20Florian%20Wirth%20and%20Jack%20Beuth%20and%20Amir%20Barati%20Farimani&entry.1292438233=%20%20Laser%20powder%20bed%20fusion%20%28LPBF%29%20has%20shown%20promise%20for%20wide%20range%20of%0Aapplications%20due%20to%20its%20ability%20to%20fabricate%20freeform%20geometries%20and%20generate%20a%0Acontrolled%20microstructure.%20However%2C%20components%20generated%20by%20LPBF%20still%20possess%0Asub-optimal%20mechanical%20properties%20due%20to%20the%20defects%20that%20are%20created%20during%0Alaser-material%20interactions.%20In%20this%20work%2C%20we%20investigate%20mechanism%20of%20spatter%0Aformation%2C%20using%20a%20high-fidelity%20modelling%20tool%20that%20was%20built%20to%20simulate%20the%0Amulti-physics%20phenomena%20in%20LPBF.%20The%20modelling%20tool%20have%20the%20capability%20to%0Acapture%20the%203D%20resolution%20of%20the%20meltpool%20and%20the%20spatter%20behavior.%20To%0Aunderstand%20spatter%20behavior%20and%20formation%2C%20we%20reveal%20its%20properties%20at%20ejection%0Aand%20evaluate%20its%20variation%20from%20the%20meltpool%2C%20the%20source%20where%20it%20is%20formed.%0AThe%20dataset%20of%20the%20spatter%20and%20the%20meltpool%20collected%20consist%20of%2050%20%25%20spatter%0Aand%2050%20%25%20melt%20pool%20samples%2C%20with%20features%20that%20include%20position%20components%2C%0Avelocity%20components%2C%20velocity%20magnitude%2C%20temperature%2C%20density%20and%20pressure.%20The%0Arelationship%20between%20the%20spatter%20and%20the%20meltpool%20were%20evaluated%20via%0Acorrelation%20analysis%20and%20machine%20learning%20%28ML%29%20algorithms%20for%20classification%0Atasks.%20Upon%20screening%20different%20ML%20algorithms%20on%20the%20dataset%2C%20a%20high%20accuracy%0Awas%20observed%20for%20all%20the%20ML%20models%2C%20with%20ExtraTrees%20having%20the%20highest%20at%2096%20%25%0Aand%20KNN%20having%20the%20lowest%20at%2094%20%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07823v2&entry.124074799=Read"},
{"title": "A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly\n  Detection Approaches for Multivariate Time Series", "author": "Lucas Correia and Jan-Christoph Goos and Thomas B\u00e4ck and Anna V. Kononova", "abstract": "  Benchmarking anomaly detection approaches for multivariate time series is\nchallenging due to the lack of high-quality datasets. Current publicly\navailable datasets are too small, not diverse and feature trivial anomalies,\nwhich hinders measurable progress in this research area. We propose a solution:\na diverse, extensive, and non-trivial dataset generated via state-of-the-art\nsimulation tools that reflects realistic behaviour of an automotive powertrain,\nincluding its multivariate, dynamic and variable-state properties. To cater for\nboth unsupervised and semi-supervised anomaly detection settings, as well as\ntime series generation and forecasting, we make different versions of the\ndataset available, where training and test subsets are offered in contaminated\nand clean versions, depending on the task. We also provide baseline results\nfrom a small selection of approaches based on deterministic and variational\nautoencoders, as well as a non-parametric approach. As expected, the baseline\nexperimentation shows that the approaches trained on the semi-supervised\nversion of the dataset outperform their unsupervised counterparts, highlighting\na need for approaches more robust to contaminated training data.\n", "link": "http://arxiv.org/abs/2411.13951v3", "date": "2025-01-15", "relevancy": 1.8449, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4626}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4621}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Discrete-sequence%20Dataset%20for%20Evaluating%20Online%20Unsupervised%20Anomaly%0A%20%20Detection%20Approaches%20for%20Multivariate%20Time%20Series&body=Title%3A%20A%20Discrete-sequence%20Dataset%20for%20Evaluating%20Online%20Unsupervised%20Anomaly%0A%20%20Detection%20Approaches%20for%20Multivariate%20Time%20Series%0AAuthor%3A%20Lucas%20Correia%20and%20Jan-Christoph%20Goos%20and%20Thomas%20B%C3%A4ck%20and%20Anna%20V.%20Kononova%0AAbstract%3A%20%20%20Benchmarking%20anomaly%20detection%20approaches%20for%20multivariate%20time%20series%20is%0Achallenging%20due%20to%20the%20lack%20of%20high-quality%20datasets.%20Current%20publicly%0Aavailable%20datasets%20are%20too%20small%2C%20not%20diverse%20and%20feature%20trivial%20anomalies%2C%0Awhich%20hinders%20measurable%20progress%20in%20this%20research%20area.%20We%20propose%20a%20solution%3A%0Aa%20diverse%2C%20extensive%2C%20and%20non-trivial%20dataset%20generated%20via%20state-of-the-art%0Asimulation%20tools%20that%20reflects%20realistic%20behaviour%20of%20an%20automotive%20powertrain%2C%0Aincluding%20its%20multivariate%2C%20dynamic%20and%20variable-state%20properties.%20To%20cater%20for%0Aboth%20unsupervised%20and%20semi-supervised%20anomaly%20detection%20settings%2C%20as%20well%20as%0Atime%20series%20generation%20and%20forecasting%2C%20we%20make%20different%20versions%20of%20the%0Adataset%20available%2C%20where%20training%20and%20test%20subsets%20are%20offered%20in%20contaminated%0Aand%20clean%20versions%2C%20depending%20on%20the%20task.%20We%20also%20provide%20baseline%20results%0Afrom%20a%20small%20selection%20of%20approaches%20based%20on%20deterministic%20and%20variational%0Aautoencoders%2C%20as%20well%20as%20a%20non-parametric%20approach.%20As%20expected%2C%20the%20baseline%0Aexperimentation%20shows%20that%20the%20approaches%20trained%20on%20the%20semi-supervised%0Aversion%20of%20the%20dataset%20outperform%20their%20unsupervised%20counterparts%2C%20highlighting%0Aa%20need%20for%20approaches%20more%20robust%20to%20contaminated%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13951v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Discrete-sequence%2520Dataset%2520for%2520Evaluating%2520Online%2520Unsupervised%2520Anomaly%250A%2520%2520Detection%2520Approaches%2520for%2520Multivariate%2520Time%2520Series%26entry.906535625%3DLucas%2520Correia%2520and%2520Jan-Christoph%2520Goos%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Anna%2520V.%2520Kononova%26entry.1292438233%3D%2520%2520Benchmarking%2520anomaly%2520detection%2520approaches%2520for%2520multivariate%2520time%2520series%2520is%250Achallenging%2520due%2520to%2520the%2520lack%2520of%2520high-quality%2520datasets.%2520Current%2520publicly%250Aavailable%2520datasets%2520are%2520too%2520small%252C%2520not%2520diverse%2520and%2520feature%2520trivial%2520anomalies%252C%250Awhich%2520hinders%2520measurable%2520progress%2520in%2520this%2520research%2520area.%2520We%2520propose%2520a%2520solution%253A%250Aa%2520diverse%252C%2520extensive%252C%2520and%2520non-trivial%2520dataset%2520generated%2520via%2520state-of-the-art%250Asimulation%2520tools%2520that%2520reflects%2520realistic%2520behaviour%2520of%2520an%2520automotive%2520powertrain%252C%250Aincluding%2520its%2520multivariate%252C%2520dynamic%2520and%2520variable-state%2520properties.%2520To%2520cater%2520for%250Aboth%2520unsupervised%2520and%2520semi-supervised%2520anomaly%2520detection%2520settings%252C%2520as%2520well%2520as%250Atime%2520series%2520generation%2520and%2520forecasting%252C%2520we%2520make%2520different%2520versions%2520of%2520the%250Adataset%2520available%252C%2520where%2520training%2520and%2520test%2520subsets%2520are%2520offered%2520in%2520contaminated%250Aand%2520clean%2520versions%252C%2520depending%2520on%2520the%2520task.%2520We%2520also%2520provide%2520baseline%2520results%250Afrom%2520a%2520small%2520selection%2520of%2520approaches%2520based%2520on%2520deterministic%2520and%2520variational%250Aautoencoders%252C%2520as%2520well%2520as%2520a%2520non-parametric%2520approach.%2520As%2520expected%252C%2520the%2520baseline%250Aexperimentation%2520shows%2520that%2520the%2520approaches%2520trained%2520on%2520the%2520semi-supervised%250Aversion%2520of%2520the%2520dataset%2520outperform%2520their%2520unsupervised%2520counterparts%252C%2520highlighting%250Aa%2520need%2520for%2520approaches%2520more%2520robust%2520to%2520contaminated%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13951v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Discrete-sequence%20Dataset%20for%20Evaluating%20Online%20Unsupervised%20Anomaly%0A%20%20Detection%20Approaches%20for%20Multivariate%20Time%20Series&entry.906535625=Lucas%20Correia%20and%20Jan-Christoph%20Goos%20and%20Thomas%20B%C3%A4ck%20and%20Anna%20V.%20Kononova&entry.1292438233=%20%20Benchmarking%20anomaly%20detection%20approaches%20for%20multivariate%20time%20series%20is%0Achallenging%20due%20to%20the%20lack%20of%20high-quality%20datasets.%20Current%20publicly%0Aavailable%20datasets%20are%20too%20small%2C%20not%20diverse%20and%20feature%20trivial%20anomalies%2C%0Awhich%20hinders%20measurable%20progress%20in%20this%20research%20area.%20We%20propose%20a%20solution%3A%0Aa%20diverse%2C%20extensive%2C%20and%20non-trivial%20dataset%20generated%20via%20state-of-the-art%0Asimulation%20tools%20that%20reflects%20realistic%20behaviour%20of%20an%20automotive%20powertrain%2C%0Aincluding%20its%20multivariate%2C%20dynamic%20and%20variable-state%20properties.%20To%20cater%20for%0Aboth%20unsupervised%20and%20semi-supervised%20anomaly%20detection%20settings%2C%20as%20well%20as%0Atime%20series%20generation%20and%20forecasting%2C%20we%20make%20different%20versions%20of%20the%0Adataset%20available%2C%20where%20training%20and%20test%20subsets%20are%20offered%20in%20contaminated%0Aand%20clean%20versions%2C%20depending%20on%20the%20task.%20We%20also%20provide%20baseline%20results%0Afrom%20a%20small%20selection%20of%20approaches%20based%20on%20deterministic%20and%20variational%0Aautoencoders%2C%20as%20well%20as%20a%20non-parametric%20approach.%20As%20expected%2C%20the%20baseline%0Aexperimentation%20shows%20that%20the%20approaches%20trained%20on%20the%20semi-supervised%0Aversion%20of%20the%20dataset%20outperform%20their%20unsupervised%20counterparts%2C%20highlighting%0Aa%20need%20for%20approaches%20more%20robust%20to%20contaminated%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13951v3&entry.124074799=Read"},
{"title": "Delay Sensitive Hierarchical Federated Learning with Stochastic Local\n  Updates", "author": "Abdulmoneam Ali and Ahmed Arafa", "abstract": "  The impact of local averaging on the performance of federated learning (FL)\nsystems is studied in the presence of communication delay between the clients\nand the parameter server. To minimize the effect of delay, clients are assigned\ninto different groups, each having its own local parameter server (LPS) that\naggregates its clients' models. The groups' models are then aggregated at a\nglobal parameter server (GPS) that only communicates with the LPSs. Such\nsetting is known as hierarchical FL (HFL). Unlike most works in the literature,\nthe number of local and global communication rounds in our work is randomly\ndetermined by the (different) delays experienced by each group of clients.\nSpecifically, the number of local averaging rounds is tied to a wall-clock time\nperiod coined the sync time $S$, after which the LPSs synchronize their models\nby sharing them with the GPS. Such sync time $S$ is then reapplied until a\nglobal wall-clock time is exhausted.\n  First, an upper bound on the deviation between the updated model at each LPS\nwith respect to that available at the GPS is derived. This is then used as a\ntool to derive the convergence analysis of our proposed delay-sensitive HFL\nalgorithm, first at each LPS individually, and then at the GPS. Our theoretical\nconvergence bound showcases the effects of the whole system's parameters,\nincluding the number of groups, the number of clients per group, and the value\nof $S$. Our results show that the value of $S$ should be carefully chosen,\nespecially since it implicitly governs how the delay statistics affect the\nperformance of HFL in situations where training time is restricted.\n", "link": "http://arxiv.org/abs/2302.04851v2", "date": "2025-01-15", "relevancy": 1.8327, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4741}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4617}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Delay%20Sensitive%20Hierarchical%20Federated%20Learning%20with%20Stochastic%20Local%0A%20%20Updates&body=Title%3A%20Delay%20Sensitive%20Hierarchical%20Federated%20Learning%20with%20Stochastic%20Local%0A%20%20Updates%0AAuthor%3A%20Abdulmoneam%20Ali%20and%20Ahmed%20Arafa%0AAbstract%3A%20%20%20The%20impact%20of%20local%20averaging%20on%20the%20performance%20of%20federated%20learning%20%28FL%29%0Asystems%20is%20studied%20in%20the%20presence%20of%20communication%20delay%20between%20the%20clients%0Aand%20the%20parameter%20server.%20To%20minimize%20the%20effect%20of%20delay%2C%20clients%20are%20assigned%0Ainto%20different%20groups%2C%20each%20having%20its%20own%20local%20parameter%20server%20%28LPS%29%20that%0Aaggregates%20its%20clients%27%20models.%20The%20groups%27%20models%20are%20then%20aggregated%20at%20a%0Aglobal%20parameter%20server%20%28GPS%29%20that%20only%20communicates%20with%20the%20LPSs.%20Such%0Asetting%20is%20known%20as%20hierarchical%20FL%20%28HFL%29.%20Unlike%20most%20works%20in%20the%20literature%2C%0Athe%20number%20of%20local%20and%20global%20communication%20rounds%20in%20our%20work%20is%20randomly%0Adetermined%20by%20the%20%28different%29%20delays%20experienced%20by%20each%20group%20of%20clients.%0ASpecifically%2C%20the%20number%20of%20local%20averaging%20rounds%20is%20tied%20to%20a%20wall-clock%20time%0Aperiod%20coined%20the%20sync%20time%20%24S%24%2C%20after%20which%20the%20LPSs%20synchronize%20their%20models%0Aby%20sharing%20them%20with%20the%20GPS.%20Such%20sync%20time%20%24S%24%20is%20then%20reapplied%20until%20a%0Aglobal%20wall-clock%20time%20is%20exhausted.%0A%20%20First%2C%20an%20upper%20bound%20on%20the%20deviation%20between%20the%20updated%20model%20at%20each%20LPS%0Awith%20respect%20to%20that%20available%20at%20the%20GPS%20is%20derived.%20This%20is%20then%20used%20as%20a%0Atool%20to%20derive%20the%20convergence%20analysis%20of%20our%20proposed%20delay-sensitive%20HFL%0Aalgorithm%2C%20first%20at%20each%20LPS%20individually%2C%20and%20then%20at%20the%20GPS.%20Our%20theoretical%0Aconvergence%20bound%20showcases%20the%20effects%20of%20the%20whole%20system%27s%20parameters%2C%0Aincluding%20the%20number%20of%20groups%2C%20the%20number%20of%20clients%20per%20group%2C%20and%20the%20value%0Aof%20%24S%24.%20Our%20results%20show%20that%20the%20value%20of%20%24S%24%20should%20be%20carefully%20chosen%2C%0Aespecially%20since%20it%20implicitly%20governs%20how%20the%20delay%20statistics%20affect%20the%0Aperformance%20of%20HFL%20in%20situations%20where%20training%20time%20is%20restricted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.04851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelay%2520Sensitive%2520Hierarchical%2520Federated%2520Learning%2520with%2520Stochastic%2520Local%250A%2520%2520Updates%26entry.906535625%3DAbdulmoneam%2520Ali%2520and%2520Ahmed%2520Arafa%26entry.1292438233%3D%2520%2520The%2520impact%2520of%2520local%2520averaging%2520on%2520the%2520performance%2520of%2520federated%2520learning%2520%2528FL%2529%250Asystems%2520is%2520studied%2520in%2520the%2520presence%2520of%2520communication%2520delay%2520between%2520the%2520clients%250Aand%2520the%2520parameter%2520server.%2520To%2520minimize%2520the%2520effect%2520of%2520delay%252C%2520clients%2520are%2520assigned%250Ainto%2520different%2520groups%252C%2520each%2520having%2520its%2520own%2520local%2520parameter%2520server%2520%2528LPS%2529%2520that%250Aaggregates%2520its%2520clients%2527%2520models.%2520The%2520groups%2527%2520models%2520are%2520then%2520aggregated%2520at%2520a%250Aglobal%2520parameter%2520server%2520%2528GPS%2529%2520that%2520only%2520communicates%2520with%2520the%2520LPSs.%2520Such%250Asetting%2520is%2520known%2520as%2520hierarchical%2520FL%2520%2528HFL%2529.%2520Unlike%2520most%2520works%2520in%2520the%2520literature%252C%250Athe%2520number%2520of%2520local%2520and%2520global%2520communication%2520rounds%2520in%2520our%2520work%2520is%2520randomly%250Adetermined%2520by%2520the%2520%2528different%2529%2520delays%2520experienced%2520by%2520each%2520group%2520of%2520clients.%250ASpecifically%252C%2520the%2520number%2520of%2520local%2520averaging%2520rounds%2520is%2520tied%2520to%2520a%2520wall-clock%2520time%250Aperiod%2520coined%2520the%2520sync%2520time%2520%2524S%2524%252C%2520after%2520which%2520the%2520LPSs%2520synchronize%2520their%2520models%250Aby%2520sharing%2520them%2520with%2520the%2520GPS.%2520Such%2520sync%2520time%2520%2524S%2524%2520is%2520then%2520reapplied%2520until%2520a%250Aglobal%2520wall-clock%2520time%2520is%2520exhausted.%250A%2520%2520First%252C%2520an%2520upper%2520bound%2520on%2520the%2520deviation%2520between%2520the%2520updated%2520model%2520at%2520each%2520LPS%250Awith%2520respect%2520to%2520that%2520available%2520at%2520the%2520GPS%2520is%2520derived.%2520This%2520is%2520then%2520used%2520as%2520a%250Atool%2520to%2520derive%2520the%2520convergence%2520analysis%2520of%2520our%2520proposed%2520delay-sensitive%2520HFL%250Aalgorithm%252C%2520first%2520at%2520each%2520LPS%2520individually%252C%2520and%2520then%2520at%2520the%2520GPS.%2520Our%2520theoretical%250Aconvergence%2520bound%2520showcases%2520the%2520effects%2520of%2520the%2520whole%2520system%2527s%2520parameters%252C%250Aincluding%2520the%2520number%2520of%2520groups%252C%2520the%2520number%2520of%2520clients%2520per%2520group%252C%2520and%2520the%2520value%250Aof%2520%2524S%2524.%2520Our%2520results%2520show%2520that%2520the%2520value%2520of%2520%2524S%2524%2520should%2520be%2520carefully%2520chosen%252C%250Aespecially%2520since%2520it%2520implicitly%2520governs%2520how%2520the%2520delay%2520statistics%2520affect%2520the%250Aperformance%2520of%2520HFL%2520in%2520situations%2520where%2520training%2520time%2520is%2520restricted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.04851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delay%20Sensitive%20Hierarchical%20Federated%20Learning%20with%20Stochastic%20Local%0A%20%20Updates&entry.906535625=Abdulmoneam%20Ali%20and%20Ahmed%20Arafa&entry.1292438233=%20%20The%20impact%20of%20local%20averaging%20on%20the%20performance%20of%20federated%20learning%20%28FL%29%0Asystems%20is%20studied%20in%20the%20presence%20of%20communication%20delay%20between%20the%20clients%0Aand%20the%20parameter%20server.%20To%20minimize%20the%20effect%20of%20delay%2C%20clients%20are%20assigned%0Ainto%20different%20groups%2C%20each%20having%20its%20own%20local%20parameter%20server%20%28LPS%29%20that%0Aaggregates%20its%20clients%27%20models.%20The%20groups%27%20models%20are%20then%20aggregated%20at%20a%0Aglobal%20parameter%20server%20%28GPS%29%20that%20only%20communicates%20with%20the%20LPSs.%20Such%0Asetting%20is%20known%20as%20hierarchical%20FL%20%28HFL%29.%20Unlike%20most%20works%20in%20the%20literature%2C%0Athe%20number%20of%20local%20and%20global%20communication%20rounds%20in%20our%20work%20is%20randomly%0Adetermined%20by%20the%20%28different%29%20delays%20experienced%20by%20each%20group%20of%20clients.%0ASpecifically%2C%20the%20number%20of%20local%20averaging%20rounds%20is%20tied%20to%20a%20wall-clock%20time%0Aperiod%20coined%20the%20sync%20time%20%24S%24%2C%20after%20which%20the%20LPSs%20synchronize%20their%20models%0Aby%20sharing%20them%20with%20the%20GPS.%20Such%20sync%20time%20%24S%24%20is%20then%20reapplied%20until%20a%0Aglobal%20wall-clock%20time%20is%20exhausted.%0A%20%20First%2C%20an%20upper%20bound%20on%20the%20deviation%20between%20the%20updated%20model%20at%20each%20LPS%0Awith%20respect%20to%20that%20available%20at%20the%20GPS%20is%20derived.%20This%20is%20then%20used%20as%20a%0Atool%20to%20derive%20the%20convergence%20analysis%20of%20our%20proposed%20delay-sensitive%20HFL%0Aalgorithm%2C%20first%20at%20each%20LPS%20individually%2C%20and%20then%20at%20the%20GPS.%20Our%20theoretical%0Aconvergence%20bound%20showcases%20the%20effects%20of%20the%20whole%20system%27s%20parameters%2C%0Aincluding%20the%20number%20of%20groups%2C%20the%20number%20of%20clients%20per%20group%2C%20and%20the%20value%0Aof%20%24S%24.%20Our%20results%20show%20that%20the%20value%20of%20%24S%24%20should%20be%20carefully%20chosen%2C%0Aespecially%20since%20it%20implicitly%20governs%20how%20the%20delay%20statistics%20affect%20the%0Aperformance%20of%20HFL%20in%20situations%20where%20training%20time%20is%20restricted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.04851v2&entry.124074799=Read"},
{"title": "DeblurDiNAT: A Compact Model with Exceptional Generalization and Visual\n  Fidelity on Unseen Domains", "author": "Hanzhou Liu and Binghan Li and Chengkai Liu and Mi Lu", "abstract": "  Recent deblurring networks have effectively restored clear images from the\nblurred ones. However, they often struggle with generalization to unknown\ndomains. Moreover, these models typically focus on distortion metrics such as\nPSNR and SSIM, neglecting the critical aspect of metrics aligned with human\nperception. To address these limitations, we propose DeblurDiNAT, a deblurring\nTransformer based on Dilated Neighborhood Attention. First, DeblurDiNAT employs\nan alternating dilation factor paradigm to capture both local and global\nblurred patterns, enhancing generalization and perceptual clarity. Second, a\nlocal cross-channel learner aids the Transformer block to understand the\nshort-range relationships between adjacent channels. Additionally, we present a\nlinear feed-forward network with a simple while effective design. Finally, a\ndual-stage feature fusion module is introduced as an alternative to the\nexisting approach, which efficiently process multi-scale visual information\nacross network levels. Compared to state-of-the-art models, our compact\nDeblurDiNAT demonstrates superior generalization capabilities and achieves\nremarkable performance in perceptual metrics, while maintaining a favorable\nmodel size.\n", "link": "http://arxiv.org/abs/2403.13163v5", "date": "2025-01-15", "relevancy": 1.8214, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.64}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6209}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeblurDiNAT%3A%20A%20Compact%20Model%20with%20Exceptional%20Generalization%20and%20Visual%0A%20%20Fidelity%20on%20Unseen%20Domains&body=Title%3A%20DeblurDiNAT%3A%20A%20Compact%20Model%20with%20Exceptional%20Generalization%20and%20Visual%0A%20%20Fidelity%20on%20Unseen%20Domains%0AAuthor%3A%20Hanzhou%20Liu%20and%20Binghan%20Li%20and%20Chengkai%20Liu%20and%20Mi%20Lu%0AAbstract%3A%20%20%20Recent%20deblurring%20networks%20have%20effectively%20restored%20clear%20images%20from%20the%0Ablurred%20ones.%20However%2C%20they%20often%20struggle%20with%20generalization%20to%20unknown%0Adomains.%20Moreover%2C%20these%20models%20typically%20focus%20on%20distortion%20metrics%20such%20as%0APSNR%20and%20SSIM%2C%20neglecting%20the%20critical%20aspect%20of%20metrics%20aligned%20with%20human%0Aperception.%20To%20address%20these%20limitations%2C%20we%20propose%20DeblurDiNAT%2C%20a%20deblurring%0ATransformer%20based%20on%20Dilated%20Neighborhood%20Attention.%20First%2C%20DeblurDiNAT%20employs%0Aan%20alternating%20dilation%20factor%20paradigm%20to%20capture%20both%20local%20and%20global%0Ablurred%20patterns%2C%20enhancing%20generalization%20and%20perceptual%20clarity.%20Second%2C%20a%0Alocal%20cross-channel%20learner%20aids%20the%20Transformer%20block%20to%20understand%20the%0Ashort-range%20relationships%20between%20adjacent%20channels.%20Additionally%2C%20we%20present%20a%0Alinear%20feed-forward%20network%20with%20a%20simple%20while%20effective%20design.%20Finally%2C%20a%0Adual-stage%20feature%20fusion%20module%20is%20introduced%20as%20an%20alternative%20to%20the%0Aexisting%20approach%2C%20which%20efficiently%20process%20multi-scale%20visual%20information%0Aacross%20network%20levels.%20Compared%20to%20state-of-the-art%20models%2C%20our%20compact%0ADeblurDiNAT%20demonstrates%20superior%20generalization%20capabilities%20and%20achieves%0Aremarkable%20performance%20in%20perceptual%20metrics%2C%20while%20maintaining%20a%20favorable%0Amodel%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13163v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeblurDiNAT%253A%2520A%2520Compact%2520Model%2520with%2520Exceptional%2520Generalization%2520and%2520Visual%250A%2520%2520Fidelity%2520on%2520Unseen%2520Domains%26entry.906535625%3DHanzhou%2520Liu%2520and%2520Binghan%2520Li%2520and%2520Chengkai%2520Liu%2520and%2520Mi%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520deblurring%2520networks%2520have%2520effectively%2520restored%2520clear%2520images%2520from%2520the%250Ablurred%2520ones.%2520However%252C%2520they%2520often%2520struggle%2520with%2520generalization%2520to%2520unknown%250Adomains.%2520Moreover%252C%2520these%2520models%2520typically%2520focus%2520on%2520distortion%2520metrics%2520such%2520as%250APSNR%2520and%2520SSIM%252C%2520neglecting%2520the%2520critical%2520aspect%2520of%2520metrics%2520aligned%2520with%2520human%250Aperception.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520DeblurDiNAT%252C%2520a%2520deblurring%250ATransformer%2520based%2520on%2520Dilated%2520Neighborhood%2520Attention.%2520First%252C%2520DeblurDiNAT%2520employs%250Aan%2520alternating%2520dilation%2520factor%2520paradigm%2520to%2520capture%2520both%2520local%2520and%2520global%250Ablurred%2520patterns%252C%2520enhancing%2520generalization%2520and%2520perceptual%2520clarity.%2520Second%252C%2520a%250Alocal%2520cross-channel%2520learner%2520aids%2520the%2520Transformer%2520block%2520to%2520understand%2520the%250Ashort-range%2520relationships%2520between%2520adjacent%2520channels.%2520Additionally%252C%2520we%2520present%2520a%250Alinear%2520feed-forward%2520network%2520with%2520a%2520simple%2520while%2520effective%2520design.%2520Finally%252C%2520a%250Adual-stage%2520feature%2520fusion%2520module%2520is%2520introduced%2520as%2520an%2520alternative%2520to%2520the%250Aexisting%2520approach%252C%2520which%2520efficiently%2520process%2520multi-scale%2520visual%2520information%250Aacross%2520network%2520levels.%2520Compared%2520to%2520state-of-the-art%2520models%252C%2520our%2520compact%250ADeblurDiNAT%2520demonstrates%2520superior%2520generalization%2520capabilities%2520and%2520achieves%250Aremarkable%2520performance%2520in%2520perceptual%2520metrics%252C%2520while%2520maintaining%2520a%2520favorable%250Amodel%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13163v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeblurDiNAT%3A%20A%20Compact%20Model%20with%20Exceptional%20Generalization%20and%20Visual%0A%20%20Fidelity%20on%20Unseen%20Domains&entry.906535625=Hanzhou%20Liu%20and%20Binghan%20Li%20and%20Chengkai%20Liu%20and%20Mi%20Lu&entry.1292438233=%20%20Recent%20deblurring%20networks%20have%20effectively%20restored%20clear%20images%20from%20the%0Ablurred%20ones.%20However%2C%20they%20often%20struggle%20with%20generalization%20to%20unknown%0Adomains.%20Moreover%2C%20these%20models%20typically%20focus%20on%20distortion%20metrics%20such%20as%0APSNR%20and%20SSIM%2C%20neglecting%20the%20critical%20aspect%20of%20metrics%20aligned%20with%20human%0Aperception.%20To%20address%20these%20limitations%2C%20we%20propose%20DeblurDiNAT%2C%20a%20deblurring%0ATransformer%20based%20on%20Dilated%20Neighborhood%20Attention.%20First%2C%20DeblurDiNAT%20employs%0Aan%20alternating%20dilation%20factor%20paradigm%20to%20capture%20both%20local%20and%20global%0Ablurred%20patterns%2C%20enhancing%20generalization%20and%20perceptual%20clarity.%20Second%2C%20a%0Alocal%20cross-channel%20learner%20aids%20the%20Transformer%20block%20to%20understand%20the%0Ashort-range%20relationships%20between%20adjacent%20channels.%20Additionally%2C%20we%20present%20a%0Alinear%20feed-forward%20network%20with%20a%20simple%20while%20effective%20design.%20Finally%2C%20a%0Adual-stage%20feature%20fusion%20module%20is%20introduced%20as%20an%20alternative%20to%20the%0Aexisting%20approach%2C%20which%20efficiently%20process%20multi-scale%20visual%20information%0Aacross%20network%20levels.%20Compared%20to%20state-of-the-art%20models%2C%20our%20compact%0ADeblurDiNAT%20demonstrates%20superior%20generalization%20capabilities%20and%20achieves%0Aremarkable%20performance%20in%20perceptual%20metrics%2C%20while%20maintaining%20a%20favorable%0Amodel%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13163v5&entry.124074799=Read"},
{"title": "Mask-guided cross-image attention for zero-shot in-silico\n  histopathologic image generation with a diffusion model", "author": "Dominik Winter and Nicolas Triltsch and Marco Rosati and Anatoliy Shumilov and Ziya Kokaragac and Yuri Popov and Thomas Padel and Laura Sebastian Monasor and Ross Hill and Markus Schick and Nicolas Brieu", "abstract": "  Creating in-silico data with generative AI promises a cost-effective\nalternative to staining, imaging, and annotating whole slide images in\ncomputational pathology. Diffusion models are the state-of-the-art solution for\ngenerating in-silico images, offering unparalleled fidelity and realism. Using\nappearance transfer diffusion models allows for zero-shot image generation,\nfacilitating fast application and making model training unnecessary. However\ncurrent appearance transfer diffusion models are designed for natural images,\nwhere the main task is to transfer the foreground object from an origin to a\ntarget domain, while the background is of insignificant importance. In\ncomputational pathology, specifically in oncology, it is however not\nstraightforward to define which objects in an image should be classified as\nforeground and background, as all objects in an image may be of critical\nimportance for the detailed understanding the tumor micro-environment. We\ncontribute to the applicability of appearance transfer diffusion models to\nimmunohistochemistry-stained images by modifying the appearance transfer\nguidance to alternate between class-specific AdaIN feature statistics matchings\nusing existing segmentation masks. The performance of the proposed method is\ndemonstrated on the downstream task of supervised epithelium segmentation,\nshowing that the number of manual annotations required for model training can\nbe reduced by 75%, outperforming the baseline approach. Additionally, we\nconsulted with a certified pathologist to investigate future improvements. We\nanticipate this work to inspire the application of zero-shot diffusion models\nin computational pathology, providing an efficient method to generate in-silico\nimages with unmatched fidelity and realism, which prove meaningful for\ndownstream tasks, such as training existing deep learning models or finetuning\nfoundation models.\n", "link": "http://arxiv.org/abs/2407.11664v3", "date": "2025-01-15", "relevancy": 1.8105, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6218}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5984}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask-guided%20cross-image%20attention%20for%20zero-shot%20in-silico%0A%20%20histopathologic%20image%20generation%20with%20a%20diffusion%20model&body=Title%3A%20Mask-guided%20cross-image%20attention%20for%20zero-shot%20in-silico%0A%20%20histopathologic%20image%20generation%20with%20a%20diffusion%20model%0AAuthor%3A%20Dominik%20Winter%20and%20Nicolas%20Triltsch%20and%20Marco%20Rosati%20and%20Anatoliy%20Shumilov%20and%20Ziya%20Kokaragac%20and%20Yuri%20Popov%20and%20Thomas%20Padel%20and%20Laura%20Sebastian%20Monasor%20and%20Ross%20Hill%20and%20Markus%20Schick%20and%20Nicolas%20Brieu%0AAbstract%3A%20%20%20Creating%20in-silico%20data%20with%20generative%20AI%20promises%20a%20cost-effective%0Aalternative%20to%20staining%2C%20imaging%2C%20and%20annotating%20whole%20slide%20images%20in%0Acomputational%20pathology.%20Diffusion%20models%20are%20the%20state-of-the-art%20solution%20for%0Agenerating%20in-silico%20images%2C%20offering%20unparalleled%20fidelity%20and%20realism.%20Using%0Aappearance%20transfer%20diffusion%20models%20allows%20for%20zero-shot%20image%20generation%2C%0Afacilitating%20fast%20application%20and%20making%20model%20training%20unnecessary.%20However%0Acurrent%20appearance%20transfer%20diffusion%20models%20are%20designed%20for%20natural%20images%2C%0Awhere%20the%20main%20task%20is%20to%20transfer%20the%20foreground%20object%20from%20an%20origin%20to%20a%0Atarget%20domain%2C%20while%20the%20background%20is%20of%20insignificant%20importance.%20In%0Acomputational%20pathology%2C%20specifically%20in%20oncology%2C%20it%20is%20however%20not%0Astraightforward%20to%20define%20which%20objects%20in%20an%20image%20should%20be%20classified%20as%0Aforeground%20and%20background%2C%20as%20all%20objects%20in%20an%20image%20may%20be%20of%20critical%0Aimportance%20for%20the%20detailed%20understanding%20the%20tumor%20micro-environment.%20We%0Acontribute%20to%20the%20applicability%20of%20appearance%20transfer%20diffusion%20models%20to%0Aimmunohistochemistry-stained%20images%20by%20modifying%20the%20appearance%20transfer%0Aguidance%20to%20alternate%20between%20class-specific%20AdaIN%20feature%20statistics%20matchings%0Ausing%20existing%20segmentation%20masks.%20The%20performance%20of%20the%20proposed%20method%20is%0Ademonstrated%20on%20the%20downstream%20task%20of%20supervised%20epithelium%20segmentation%2C%0Ashowing%20that%20the%20number%20of%20manual%20annotations%20required%20for%20model%20training%20can%0Abe%20reduced%20by%2075%25%2C%20outperforming%20the%20baseline%20approach.%20Additionally%2C%20we%0Aconsulted%20with%20a%20certified%20pathologist%20to%20investigate%20future%20improvements.%20We%0Aanticipate%20this%20work%20to%20inspire%20the%20application%20of%20zero-shot%20diffusion%20models%0Ain%20computational%20pathology%2C%20providing%20an%20efficient%20method%20to%20generate%20in-silico%0Aimages%20with%20unmatched%20fidelity%20and%20realism%2C%20which%20prove%20meaningful%20for%0Adownstream%20tasks%2C%20such%20as%20training%20existing%20deep%20learning%20models%20or%20finetuning%0Afoundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11664v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask-guided%2520cross-image%2520attention%2520for%2520zero-shot%2520in-silico%250A%2520%2520histopathologic%2520image%2520generation%2520with%2520a%2520diffusion%2520model%26entry.906535625%3DDominik%2520Winter%2520and%2520Nicolas%2520Triltsch%2520and%2520Marco%2520Rosati%2520and%2520Anatoliy%2520Shumilov%2520and%2520Ziya%2520Kokaragac%2520and%2520Yuri%2520Popov%2520and%2520Thomas%2520Padel%2520and%2520Laura%2520Sebastian%2520Monasor%2520and%2520Ross%2520Hill%2520and%2520Markus%2520Schick%2520and%2520Nicolas%2520Brieu%26entry.1292438233%3D%2520%2520Creating%2520in-silico%2520data%2520with%2520generative%2520AI%2520promises%2520a%2520cost-effective%250Aalternative%2520to%2520staining%252C%2520imaging%252C%2520and%2520annotating%2520whole%2520slide%2520images%2520in%250Acomputational%2520pathology.%2520Diffusion%2520models%2520are%2520the%2520state-of-the-art%2520solution%2520for%250Agenerating%2520in-silico%2520images%252C%2520offering%2520unparalleled%2520fidelity%2520and%2520realism.%2520Using%250Aappearance%2520transfer%2520diffusion%2520models%2520allows%2520for%2520zero-shot%2520image%2520generation%252C%250Afacilitating%2520fast%2520application%2520and%2520making%2520model%2520training%2520unnecessary.%2520However%250Acurrent%2520appearance%2520transfer%2520diffusion%2520models%2520are%2520designed%2520for%2520natural%2520images%252C%250Awhere%2520the%2520main%2520task%2520is%2520to%2520transfer%2520the%2520foreground%2520object%2520from%2520an%2520origin%2520to%2520a%250Atarget%2520domain%252C%2520while%2520the%2520background%2520is%2520of%2520insignificant%2520importance.%2520In%250Acomputational%2520pathology%252C%2520specifically%2520in%2520oncology%252C%2520it%2520is%2520however%2520not%250Astraightforward%2520to%2520define%2520which%2520objects%2520in%2520an%2520image%2520should%2520be%2520classified%2520as%250Aforeground%2520and%2520background%252C%2520as%2520all%2520objects%2520in%2520an%2520image%2520may%2520be%2520of%2520critical%250Aimportance%2520for%2520the%2520detailed%2520understanding%2520the%2520tumor%2520micro-environment.%2520We%250Acontribute%2520to%2520the%2520applicability%2520of%2520appearance%2520transfer%2520diffusion%2520models%2520to%250Aimmunohistochemistry-stained%2520images%2520by%2520modifying%2520the%2520appearance%2520transfer%250Aguidance%2520to%2520alternate%2520between%2520class-specific%2520AdaIN%2520feature%2520statistics%2520matchings%250Ausing%2520existing%2520segmentation%2520masks.%2520The%2520performance%2520of%2520the%2520proposed%2520method%2520is%250Ademonstrated%2520on%2520the%2520downstream%2520task%2520of%2520supervised%2520epithelium%2520segmentation%252C%250Ashowing%2520that%2520the%2520number%2520of%2520manual%2520annotations%2520required%2520for%2520model%2520training%2520can%250Abe%2520reduced%2520by%252075%2525%252C%2520outperforming%2520the%2520baseline%2520approach.%2520Additionally%252C%2520we%250Aconsulted%2520with%2520a%2520certified%2520pathologist%2520to%2520investigate%2520future%2520improvements.%2520We%250Aanticipate%2520this%2520work%2520to%2520inspire%2520the%2520application%2520of%2520zero-shot%2520diffusion%2520models%250Ain%2520computational%2520pathology%252C%2520providing%2520an%2520efficient%2520method%2520to%2520generate%2520in-silico%250Aimages%2520with%2520unmatched%2520fidelity%2520and%2520realism%252C%2520which%2520prove%2520meaningful%2520for%250Adownstream%2520tasks%252C%2520such%2520as%2520training%2520existing%2520deep%2520learning%2520models%2520or%2520finetuning%250Afoundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11664v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask-guided%20cross-image%20attention%20for%20zero-shot%20in-silico%0A%20%20histopathologic%20image%20generation%20with%20a%20diffusion%20model&entry.906535625=Dominik%20Winter%20and%20Nicolas%20Triltsch%20and%20Marco%20Rosati%20and%20Anatoliy%20Shumilov%20and%20Ziya%20Kokaragac%20and%20Yuri%20Popov%20and%20Thomas%20Padel%20and%20Laura%20Sebastian%20Monasor%20and%20Ross%20Hill%20and%20Markus%20Schick%20and%20Nicolas%20Brieu&entry.1292438233=%20%20Creating%20in-silico%20data%20with%20generative%20AI%20promises%20a%20cost-effective%0Aalternative%20to%20staining%2C%20imaging%2C%20and%20annotating%20whole%20slide%20images%20in%0Acomputational%20pathology.%20Diffusion%20models%20are%20the%20state-of-the-art%20solution%20for%0Agenerating%20in-silico%20images%2C%20offering%20unparalleled%20fidelity%20and%20realism.%20Using%0Aappearance%20transfer%20diffusion%20models%20allows%20for%20zero-shot%20image%20generation%2C%0Afacilitating%20fast%20application%20and%20making%20model%20training%20unnecessary.%20However%0Acurrent%20appearance%20transfer%20diffusion%20models%20are%20designed%20for%20natural%20images%2C%0Awhere%20the%20main%20task%20is%20to%20transfer%20the%20foreground%20object%20from%20an%20origin%20to%20a%0Atarget%20domain%2C%20while%20the%20background%20is%20of%20insignificant%20importance.%20In%0Acomputational%20pathology%2C%20specifically%20in%20oncology%2C%20it%20is%20however%20not%0Astraightforward%20to%20define%20which%20objects%20in%20an%20image%20should%20be%20classified%20as%0Aforeground%20and%20background%2C%20as%20all%20objects%20in%20an%20image%20may%20be%20of%20critical%0Aimportance%20for%20the%20detailed%20understanding%20the%20tumor%20micro-environment.%20We%0Acontribute%20to%20the%20applicability%20of%20appearance%20transfer%20diffusion%20models%20to%0Aimmunohistochemistry-stained%20images%20by%20modifying%20the%20appearance%20transfer%0Aguidance%20to%20alternate%20between%20class-specific%20AdaIN%20feature%20statistics%20matchings%0Ausing%20existing%20segmentation%20masks.%20The%20performance%20of%20the%20proposed%20method%20is%0Ademonstrated%20on%20the%20downstream%20task%20of%20supervised%20epithelium%20segmentation%2C%0Ashowing%20that%20the%20number%20of%20manual%20annotations%20required%20for%20model%20training%20can%0Abe%20reduced%20by%2075%25%2C%20outperforming%20the%20baseline%20approach.%20Additionally%2C%20we%0Aconsulted%20with%20a%20certified%20pathologist%20to%20investigate%20future%20improvements.%20We%0Aanticipate%20this%20work%20to%20inspire%20the%20application%20of%20zero-shot%20diffusion%20models%0Ain%20computational%20pathology%2C%20providing%20an%20efficient%20method%20to%20generate%20in-silico%0Aimages%20with%20unmatched%20fidelity%20and%20realism%2C%20which%20prove%20meaningful%20for%0Adownstream%20tasks%2C%20such%20as%20training%20existing%20deep%20learning%20models%20or%20finetuning%0Afoundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11664v3&entry.124074799=Read"},
{"title": "Personality Modeling for Persuasion of Misinformation using AI Agent", "author": "Qianmin Lou and Wentao Xu", "abstract": "  The proliferation of misinformation on social media platforms has highlighted\nthe need to understand how individual personality traits influence\nsusceptibility to and propagation of misinformation. This study employs an\ninnovative agent-based modeling approach to investigate the relationship\nbetween personality traits and misinformation dynamics. Using six AI agents\nembodying different dimensions of the Big Five personality traits\n(Extraversion, Agreeableness, and Neuroticism), we simulated interactions\nacross six diverse misinformation topics. The experiment, implemented through\nthe AgentScope framework using the GLM-4-Flash model, generated 90 unique\ninteractions, revealing complex patterns in how personality combinations affect\npersuasion and resistance to misinformation. Our findings demonstrate that\nanalytical and critical personality traits enhance effectiveness in\nevidence-based discussions, while non-aggressive persuasion strategies show\nunexpected success in misinformation correction. Notably, agents with critical\ntraits achieved a 59.4% success rate in HIV-related misinformation discussions,\nwhile those employing non-aggressive approaches maintained consistent\npersuasion rates above 40% across different personality combinations. The study\nalso revealed a non-transitive pattern in persuasion effectiveness, challenging\nconventional assumptions about personality-based influence. These results\nprovide crucial insights for developing personality-aware interventions in\ndigital environments and suggest that effective misinformation countermeasures\nshould prioritize emotional connection and trust-building over confrontational\napproaches. The findings contribute to both theoretical understanding of\npersonality-misinformation dynamics and practical strategies for combating\nmisinformation in social media contexts.\n", "link": "http://arxiv.org/abs/2501.08985v1", "date": "2025-01-15", "relevancy": 1.7918, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4617}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4488}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personality%20Modeling%20for%20Persuasion%20of%20Misinformation%20using%20AI%20Agent&body=Title%3A%20Personality%20Modeling%20for%20Persuasion%20of%20Misinformation%20using%20AI%20Agent%0AAuthor%3A%20Qianmin%20Lou%20and%20Wentao%20Xu%0AAbstract%3A%20%20%20The%20proliferation%20of%20misinformation%20on%20social%20media%20platforms%20has%20highlighted%0Athe%20need%20to%20understand%20how%20individual%20personality%20traits%20influence%0Asusceptibility%20to%20and%20propagation%20of%20misinformation.%20This%20study%20employs%20an%0Ainnovative%20agent-based%20modeling%20approach%20to%20investigate%20the%20relationship%0Abetween%20personality%20traits%20and%20misinformation%20dynamics.%20Using%20six%20AI%20agents%0Aembodying%20different%20dimensions%20of%20the%20Big%20Five%20personality%20traits%0A%28Extraversion%2C%20Agreeableness%2C%20and%20Neuroticism%29%2C%20we%20simulated%20interactions%0Aacross%20six%20diverse%20misinformation%20topics.%20The%20experiment%2C%20implemented%20through%0Athe%20AgentScope%20framework%20using%20the%20GLM-4-Flash%20model%2C%20generated%2090%20unique%0Ainteractions%2C%20revealing%20complex%20patterns%20in%20how%20personality%20combinations%20affect%0Apersuasion%20and%20resistance%20to%20misinformation.%20Our%20findings%20demonstrate%20that%0Aanalytical%20and%20critical%20personality%20traits%20enhance%20effectiveness%20in%0Aevidence-based%20discussions%2C%20while%20non-aggressive%20persuasion%20strategies%20show%0Aunexpected%20success%20in%20misinformation%20correction.%20Notably%2C%20agents%20with%20critical%0Atraits%20achieved%20a%2059.4%25%20success%20rate%20in%20HIV-related%20misinformation%20discussions%2C%0Awhile%20those%20employing%20non-aggressive%20approaches%20maintained%20consistent%0Apersuasion%20rates%20above%2040%25%20across%20different%20personality%20combinations.%20The%20study%0Aalso%20revealed%20a%20non-transitive%20pattern%20in%20persuasion%20effectiveness%2C%20challenging%0Aconventional%20assumptions%20about%20personality-based%20influence.%20These%20results%0Aprovide%20crucial%20insights%20for%20developing%20personality-aware%20interventions%20in%0Adigital%20environments%20and%20suggest%20that%20effective%20misinformation%20countermeasures%0Ashould%20prioritize%20emotional%20connection%20and%20trust-building%20over%20confrontational%0Aapproaches.%20The%20findings%20contribute%20to%20both%20theoretical%20understanding%20of%0Apersonality-misinformation%20dynamics%20and%20practical%20strategies%20for%20combating%0Amisinformation%20in%20social%20media%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonality%2520Modeling%2520for%2520Persuasion%2520of%2520Misinformation%2520using%2520AI%2520Agent%26entry.906535625%3DQianmin%2520Lou%2520and%2520Wentao%2520Xu%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520misinformation%2520on%2520social%2520media%2520platforms%2520has%2520highlighted%250Athe%2520need%2520to%2520understand%2520how%2520individual%2520personality%2520traits%2520influence%250Asusceptibility%2520to%2520and%2520propagation%2520of%2520misinformation.%2520This%2520study%2520employs%2520an%250Ainnovative%2520agent-based%2520modeling%2520approach%2520to%2520investigate%2520the%2520relationship%250Abetween%2520personality%2520traits%2520and%2520misinformation%2520dynamics.%2520Using%2520six%2520AI%2520agents%250Aembodying%2520different%2520dimensions%2520of%2520the%2520Big%2520Five%2520personality%2520traits%250A%2528Extraversion%252C%2520Agreeableness%252C%2520and%2520Neuroticism%2529%252C%2520we%2520simulated%2520interactions%250Aacross%2520six%2520diverse%2520misinformation%2520topics.%2520The%2520experiment%252C%2520implemented%2520through%250Athe%2520AgentScope%2520framework%2520using%2520the%2520GLM-4-Flash%2520model%252C%2520generated%252090%2520unique%250Ainteractions%252C%2520revealing%2520complex%2520patterns%2520in%2520how%2520personality%2520combinations%2520affect%250Apersuasion%2520and%2520resistance%2520to%2520misinformation.%2520Our%2520findings%2520demonstrate%2520that%250Aanalytical%2520and%2520critical%2520personality%2520traits%2520enhance%2520effectiveness%2520in%250Aevidence-based%2520discussions%252C%2520while%2520non-aggressive%2520persuasion%2520strategies%2520show%250Aunexpected%2520success%2520in%2520misinformation%2520correction.%2520Notably%252C%2520agents%2520with%2520critical%250Atraits%2520achieved%2520a%252059.4%2525%2520success%2520rate%2520in%2520HIV-related%2520misinformation%2520discussions%252C%250Awhile%2520those%2520employing%2520non-aggressive%2520approaches%2520maintained%2520consistent%250Apersuasion%2520rates%2520above%252040%2525%2520across%2520different%2520personality%2520combinations.%2520The%2520study%250Aalso%2520revealed%2520a%2520non-transitive%2520pattern%2520in%2520persuasion%2520effectiveness%252C%2520challenging%250Aconventional%2520assumptions%2520about%2520personality-based%2520influence.%2520These%2520results%250Aprovide%2520crucial%2520insights%2520for%2520developing%2520personality-aware%2520interventions%2520in%250Adigital%2520environments%2520and%2520suggest%2520that%2520effective%2520misinformation%2520countermeasures%250Ashould%2520prioritize%2520emotional%2520connection%2520and%2520trust-building%2520over%2520confrontational%250Aapproaches.%2520The%2520findings%2520contribute%2520to%2520both%2520theoretical%2520understanding%2520of%250Apersonality-misinformation%2520dynamics%2520and%2520practical%2520strategies%2520for%2520combating%250Amisinformation%2520in%2520social%2520media%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personality%20Modeling%20for%20Persuasion%20of%20Misinformation%20using%20AI%20Agent&entry.906535625=Qianmin%20Lou%20and%20Wentao%20Xu&entry.1292438233=%20%20The%20proliferation%20of%20misinformation%20on%20social%20media%20platforms%20has%20highlighted%0Athe%20need%20to%20understand%20how%20individual%20personality%20traits%20influence%0Asusceptibility%20to%20and%20propagation%20of%20misinformation.%20This%20study%20employs%20an%0Ainnovative%20agent-based%20modeling%20approach%20to%20investigate%20the%20relationship%0Abetween%20personality%20traits%20and%20misinformation%20dynamics.%20Using%20six%20AI%20agents%0Aembodying%20different%20dimensions%20of%20the%20Big%20Five%20personality%20traits%0A%28Extraversion%2C%20Agreeableness%2C%20and%20Neuroticism%29%2C%20we%20simulated%20interactions%0Aacross%20six%20diverse%20misinformation%20topics.%20The%20experiment%2C%20implemented%20through%0Athe%20AgentScope%20framework%20using%20the%20GLM-4-Flash%20model%2C%20generated%2090%20unique%0Ainteractions%2C%20revealing%20complex%20patterns%20in%20how%20personality%20combinations%20affect%0Apersuasion%20and%20resistance%20to%20misinformation.%20Our%20findings%20demonstrate%20that%0Aanalytical%20and%20critical%20personality%20traits%20enhance%20effectiveness%20in%0Aevidence-based%20discussions%2C%20while%20non-aggressive%20persuasion%20strategies%20show%0Aunexpected%20success%20in%20misinformation%20correction.%20Notably%2C%20agents%20with%20critical%0Atraits%20achieved%20a%2059.4%25%20success%20rate%20in%20HIV-related%20misinformation%20discussions%2C%0Awhile%20those%20employing%20non-aggressive%20approaches%20maintained%20consistent%0Apersuasion%20rates%20above%2040%25%20across%20different%20personality%20combinations.%20The%20study%0Aalso%20revealed%20a%20non-transitive%20pattern%20in%20persuasion%20effectiveness%2C%20challenging%0Aconventional%20assumptions%20about%20personality-based%20influence.%20These%20results%0Aprovide%20crucial%20insights%20for%20developing%20personality-aware%20interventions%20in%0Adigital%20environments%20and%20suggest%20that%20effective%20misinformation%20countermeasures%0Ashould%20prioritize%20emotional%20connection%20and%20trust-building%20over%20confrontational%0Aapproaches.%20The%20findings%20contribute%20to%20both%20theoretical%20understanding%20of%0Apersonality-misinformation%20dynamics%20and%20practical%20strategies%20for%20combating%0Amisinformation%20in%20social%20media%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08985v1&entry.124074799=Read"},
{"title": "T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video\n  Generation", "author": "Kaiyue Sun and Kaiyi Huang and Xian Liu and Yue Wu and Zihan Xu and Zhenguo Li and Xihui Liu", "abstract": "  Text-to-video (T2V) generative models have advanced significantly, yet their\nability to compose different objects, attributes, actions, and motions into a\nvideo remains unexplored. Previous text-to-video benchmarks also neglect this\nimportant ability for evaluation. In this work, we conduct the first systematic\nstudy on compositional text-to-video generation. We propose T2V-CompBench, the\nfirst benchmark tailored for compositional text-to-video generation.\nT2V-CompBench encompasses diverse aspects of compositionality, including\nconsistent attribute binding, dynamic attribute binding, spatial relationships,\nmotion binding, action binding, object interactions, and generative numeracy.\nWe further carefully design evaluation metrics of multimodal large language\nmodel (MLLM)-based, detection-based, and tracking-based metrics, which can\nbetter reflect the compositional text-to-video generation quality of seven\nproposed categories with 1400 text prompts. The effectiveness of the proposed\nmetrics is verified by correlation with human evaluations. We also benchmark\nvarious text-to-video generative models and conduct in-depth analysis across\ndifferent models and various compositional categories. We find that\ncompositional text-to-video generation is highly challenging for current\nmodels, and we hope our attempt could shed light on future research in this\ndirection.\n", "link": "http://arxiv.org/abs/2407.14505v2", "date": "2025-01-15", "relevancy": 1.7887, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6083}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5947}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2V-CompBench%3A%20A%20Comprehensive%20Benchmark%20for%20Compositional%20Text-to-video%0A%20%20Generation&body=Title%3A%20T2V-CompBench%3A%20A%20Comprehensive%20Benchmark%20for%20Compositional%20Text-to-video%0A%20%20Generation%0AAuthor%3A%20Kaiyue%20Sun%20and%20Kaiyi%20Huang%20and%20Xian%20Liu%20and%20Yue%20Wu%20and%20Zihan%20Xu%20and%20Zhenguo%20Li%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Text-to-video%20%28T2V%29%20generative%20models%20have%20advanced%20significantly%2C%20yet%20their%0Aability%20to%20compose%20different%20objects%2C%20attributes%2C%20actions%2C%20and%20motions%20into%20a%0Avideo%20remains%20unexplored.%20Previous%20text-to-video%20benchmarks%20also%20neglect%20this%0Aimportant%20ability%20for%20evaluation.%20In%20this%20work%2C%20we%20conduct%20the%20first%20systematic%0Astudy%20on%20compositional%20text-to-video%20generation.%20We%20propose%20T2V-CompBench%2C%20the%0Afirst%20benchmark%20tailored%20for%20compositional%20text-to-video%20generation.%0AT2V-CompBench%20encompasses%20diverse%20aspects%20of%20compositionality%2C%20including%0Aconsistent%20attribute%20binding%2C%20dynamic%20attribute%20binding%2C%20spatial%20relationships%2C%0Amotion%20binding%2C%20action%20binding%2C%20object%20interactions%2C%20and%20generative%20numeracy.%0AWe%20further%20carefully%20design%20evaluation%20metrics%20of%20multimodal%20large%20language%0Amodel%20%28MLLM%29-based%2C%20detection-based%2C%20and%20tracking-based%20metrics%2C%20which%20can%0Abetter%20reflect%20the%20compositional%20text-to-video%20generation%20quality%20of%20seven%0Aproposed%20categories%20with%201400%20text%20prompts.%20The%20effectiveness%20of%20the%20proposed%0Ametrics%20is%20verified%20by%20correlation%20with%20human%20evaluations.%20We%20also%20benchmark%0Avarious%20text-to-video%20generative%20models%20and%20conduct%20in-depth%20analysis%20across%0Adifferent%20models%20and%20various%20compositional%20categories.%20We%20find%20that%0Acompositional%20text-to-video%20generation%20is%20highly%20challenging%20for%20current%0Amodels%2C%20and%20we%20hope%20our%20attempt%20could%20shed%20light%20on%20future%20research%20in%20this%0Adirection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14505v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2V-CompBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Compositional%2520Text-to-video%250A%2520%2520Generation%26entry.906535625%3DKaiyue%2520Sun%2520and%2520Kaiyi%2520Huang%2520and%2520Xian%2520Liu%2520and%2520Yue%2520Wu%2520and%2520Zihan%2520Xu%2520and%2520Zhenguo%2520Li%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Text-to-video%2520%2528T2V%2529%2520generative%2520models%2520have%2520advanced%2520significantly%252C%2520yet%2520their%250Aability%2520to%2520compose%2520different%2520objects%252C%2520attributes%252C%2520actions%252C%2520and%2520motions%2520into%2520a%250Avideo%2520remains%2520unexplored.%2520Previous%2520text-to-video%2520benchmarks%2520also%2520neglect%2520this%250Aimportant%2520ability%2520for%2520evaluation.%2520In%2520this%2520work%252C%2520we%2520conduct%2520the%2520first%2520systematic%250Astudy%2520on%2520compositional%2520text-to-video%2520generation.%2520We%2520propose%2520T2V-CompBench%252C%2520the%250Afirst%2520benchmark%2520tailored%2520for%2520compositional%2520text-to-video%2520generation.%250AT2V-CompBench%2520encompasses%2520diverse%2520aspects%2520of%2520compositionality%252C%2520including%250Aconsistent%2520attribute%2520binding%252C%2520dynamic%2520attribute%2520binding%252C%2520spatial%2520relationships%252C%250Amotion%2520binding%252C%2520action%2520binding%252C%2520object%2520interactions%252C%2520and%2520generative%2520numeracy.%250AWe%2520further%2520carefully%2520design%2520evaluation%2520metrics%2520of%2520multimodal%2520large%2520language%250Amodel%2520%2528MLLM%2529-based%252C%2520detection-based%252C%2520and%2520tracking-based%2520metrics%252C%2520which%2520can%250Abetter%2520reflect%2520the%2520compositional%2520text-to-video%2520generation%2520quality%2520of%2520seven%250Aproposed%2520categories%2520with%25201400%2520text%2520prompts.%2520The%2520effectiveness%2520of%2520the%2520proposed%250Ametrics%2520is%2520verified%2520by%2520correlation%2520with%2520human%2520evaluations.%2520We%2520also%2520benchmark%250Avarious%2520text-to-video%2520generative%2520models%2520and%2520conduct%2520in-depth%2520analysis%2520across%250Adifferent%2520models%2520and%2520various%2520compositional%2520categories.%2520We%2520find%2520that%250Acompositional%2520text-to-video%2520generation%2520is%2520highly%2520challenging%2520for%2520current%250Amodels%252C%2520and%2520we%2520hope%2520our%2520attempt%2520could%2520shed%2520light%2520on%2520future%2520research%2520in%2520this%250Adirection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14505v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2V-CompBench%3A%20A%20Comprehensive%20Benchmark%20for%20Compositional%20Text-to-video%0A%20%20Generation&entry.906535625=Kaiyue%20Sun%20and%20Kaiyi%20Huang%20and%20Xian%20Liu%20and%20Yue%20Wu%20and%20Zihan%20Xu%20and%20Zhenguo%20Li%20and%20Xihui%20Liu&entry.1292438233=%20%20Text-to-video%20%28T2V%29%20generative%20models%20have%20advanced%20significantly%2C%20yet%20their%0Aability%20to%20compose%20different%20objects%2C%20attributes%2C%20actions%2C%20and%20motions%20into%20a%0Avideo%20remains%20unexplored.%20Previous%20text-to-video%20benchmarks%20also%20neglect%20this%0Aimportant%20ability%20for%20evaluation.%20In%20this%20work%2C%20we%20conduct%20the%20first%20systematic%0Astudy%20on%20compositional%20text-to-video%20generation.%20We%20propose%20T2V-CompBench%2C%20the%0Afirst%20benchmark%20tailored%20for%20compositional%20text-to-video%20generation.%0AT2V-CompBench%20encompasses%20diverse%20aspects%20of%20compositionality%2C%20including%0Aconsistent%20attribute%20binding%2C%20dynamic%20attribute%20binding%2C%20spatial%20relationships%2C%0Amotion%20binding%2C%20action%20binding%2C%20object%20interactions%2C%20and%20generative%20numeracy.%0AWe%20further%20carefully%20design%20evaluation%20metrics%20of%20multimodal%20large%20language%0Amodel%20%28MLLM%29-based%2C%20detection-based%2C%20and%20tracking-based%20metrics%2C%20which%20can%0Abetter%20reflect%20the%20compositional%20text-to-video%20generation%20quality%20of%20seven%0Aproposed%20categories%20with%201400%20text%20prompts.%20The%20effectiveness%20of%20the%20proposed%0Ametrics%20is%20verified%20by%20correlation%20with%20human%20evaluations.%20We%20also%20benchmark%0Avarious%20text-to-video%20generative%20models%20and%20conduct%20in-depth%20analysis%20across%0Adifferent%20models%20and%20various%20compositional%20categories.%20We%20find%20that%0Acompositional%20text-to-video%20generation%20is%20highly%20challenging%20for%20current%0Amodels%2C%20and%20we%20hope%20our%20attempt%20could%20shed%20light%20on%20future%20research%20in%20this%0Adirection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14505v2&entry.124074799=Read"},
{"title": "Leveraging LLM Agents for Translating Network Configurations", "author": "Yunze Wei and Xiaohui Xie and Yiwei Zuo and Tianshuo Hu and Xinyi Chen and Kaiwen Chi and Yong Cui", "abstract": "  Configuration translation is a critical and frequent task in network\noperations. When a network device is damaged or outdated, administrators need\nto replace it to maintain service continuity. The replacement devices may\noriginate from different vendors, necessitating configuration translation to\nensure seamless network operation. However, translating configurations manually\nis a labor-intensive and error-prone process. In this paper, we propose an\nintent-based framework for translating network configuration with Large\nLanguage Model (LLM) Agents. The core of our approach is an Intent-based\nRetrieval Augmented Generation (IRAG) module that systematically splits a\nconfiguration file into fragments, extracts intents, and generates accurate\ntranslations. We also design a two-stage verification method to validate the\nsyntax and semantics correctness of the translated configurations. We implement\nand evaluate the proposed method on real-world network configurations.\nExperimental results show that our method achieves 97.74% syntax correctness,\noutperforming state-of-the-art methods in translation accuracy.\n", "link": "http://arxiv.org/abs/2501.08760v1", "date": "2025-01-15", "relevancy": 1.7862, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4514}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20LLM%20Agents%20for%20Translating%20Network%20Configurations&body=Title%3A%20Leveraging%20LLM%20Agents%20for%20Translating%20Network%20Configurations%0AAuthor%3A%20Yunze%20Wei%20and%20Xiaohui%20Xie%20and%20Yiwei%20Zuo%20and%20Tianshuo%20Hu%20and%20Xinyi%20Chen%20and%20Kaiwen%20Chi%20and%20Yong%20Cui%0AAbstract%3A%20%20%20Configuration%20translation%20is%20a%20critical%20and%20frequent%20task%20in%20network%0Aoperations.%20When%20a%20network%20device%20is%20damaged%20or%20outdated%2C%20administrators%20need%0Ato%20replace%20it%20to%20maintain%20service%20continuity.%20The%20replacement%20devices%20may%0Aoriginate%20from%20different%20vendors%2C%20necessitating%20configuration%20translation%20to%0Aensure%20seamless%20network%20operation.%20However%2C%20translating%20configurations%20manually%0Ais%20a%20labor-intensive%20and%20error-prone%20process.%20In%20this%20paper%2C%20we%20propose%20an%0Aintent-based%20framework%20for%20translating%20network%20configuration%20with%20Large%0ALanguage%20Model%20%28LLM%29%20Agents.%20The%20core%20of%20our%20approach%20is%20an%20Intent-based%0ARetrieval%20Augmented%20Generation%20%28IRAG%29%20module%20that%20systematically%20splits%20a%0Aconfiguration%20file%20into%20fragments%2C%20extracts%20intents%2C%20and%20generates%20accurate%0Atranslations.%20We%20also%20design%20a%20two-stage%20verification%20method%20to%20validate%20the%0Asyntax%20and%20semantics%20correctness%20of%20the%20translated%20configurations.%20We%20implement%0Aand%20evaluate%20the%20proposed%20method%20on%20real-world%20network%20configurations.%0AExperimental%20results%20show%20that%20our%20method%20achieves%2097.74%25%20syntax%20correctness%2C%0Aoutperforming%20state-of-the-art%20methods%20in%20translation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520LLM%2520Agents%2520for%2520Translating%2520Network%2520Configurations%26entry.906535625%3DYunze%2520Wei%2520and%2520Xiaohui%2520Xie%2520and%2520Yiwei%2520Zuo%2520and%2520Tianshuo%2520Hu%2520and%2520Xinyi%2520Chen%2520and%2520Kaiwen%2520Chi%2520and%2520Yong%2520Cui%26entry.1292438233%3D%2520%2520Configuration%2520translation%2520is%2520a%2520critical%2520and%2520frequent%2520task%2520in%2520network%250Aoperations.%2520When%2520a%2520network%2520device%2520is%2520damaged%2520or%2520outdated%252C%2520administrators%2520need%250Ato%2520replace%2520it%2520to%2520maintain%2520service%2520continuity.%2520The%2520replacement%2520devices%2520may%250Aoriginate%2520from%2520different%2520vendors%252C%2520necessitating%2520configuration%2520translation%2520to%250Aensure%2520seamless%2520network%2520operation.%2520However%252C%2520translating%2520configurations%2520manually%250Ais%2520a%2520labor-intensive%2520and%2520error-prone%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aintent-based%2520framework%2520for%2520translating%2520network%2520configuration%2520with%2520Large%250ALanguage%2520Model%2520%2528LLM%2529%2520Agents.%2520The%2520core%2520of%2520our%2520approach%2520is%2520an%2520Intent-based%250ARetrieval%2520Augmented%2520Generation%2520%2528IRAG%2529%2520module%2520that%2520systematically%2520splits%2520a%250Aconfiguration%2520file%2520into%2520fragments%252C%2520extracts%2520intents%252C%2520and%2520generates%2520accurate%250Atranslations.%2520We%2520also%2520design%2520a%2520two-stage%2520verification%2520method%2520to%2520validate%2520the%250Asyntax%2520and%2520semantics%2520correctness%2520of%2520the%2520translated%2520configurations.%2520We%2520implement%250Aand%2520evaluate%2520the%2520proposed%2520method%2520on%2520real-world%2520network%2520configurations.%250AExperimental%2520results%2520show%2520that%2520our%2520method%2520achieves%252097.74%2525%2520syntax%2520correctness%252C%250Aoutperforming%2520state-of-the-art%2520methods%2520in%2520translation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20LLM%20Agents%20for%20Translating%20Network%20Configurations&entry.906535625=Yunze%20Wei%20and%20Xiaohui%20Xie%20and%20Yiwei%20Zuo%20and%20Tianshuo%20Hu%20and%20Xinyi%20Chen%20and%20Kaiwen%20Chi%20and%20Yong%20Cui&entry.1292438233=%20%20Configuration%20translation%20is%20a%20critical%20and%20frequent%20task%20in%20network%0Aoperations.%20When%20a%20network%20device%20is%20damaged%20or%20outdated%2C%20administrators%20need%0Ato%20replace%20it%20to%20maintain%20service%20continuity.%20The%20replacement%20devices%20may%0Aoriginate%20from%20different%20vendors%2C%20necessitating%20configuration%20translation%20to%0Aensure%20seamless%20network%20operation.%20However%2C%20translating%20configurations%20manually%0Ais%20a%20labor-intensive%20and%20error-prone%20process.%20In%20this%20paper%2C%20we%20propose%20an%0Aintent-based%20framework%20for%20translating%20network%20configuration%20with%20Large%0ALanguage%20Model%20%28LLM%29%20Agents.%20The%20core%20of%20our%20approach%20is%20an%20Intent-based%0ARetrieval%20Augmented%20Generation%20%28IRAG%29%20module%20that%20systematically%20splits%20a%0Aconfiguration%20file%20into%20fragments%2C%20extracts%20intents%2C%20and%20generates%20accurate%0Atranslations.%20We%20also%20design%20a%20two-stage%20verification%20method%20to%20validate%20the%0Asyntax%20and%20semantics%20correctness%20of%20the%20translated%20configurations.%20We%20implement%0Aand%20evaluate%20the%20proposed%20method%20on%20real-world%20network%20configurations.%0AExperimental%20results%20show%20that%20our%20method%20achieves%2097.74%25%20syntax%20correctness%2C%0Aoutperforming%20state-of-the-art%20methods%20in%20translation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08760v1&entry.124074799=Read"},
{"title": "Identifying Spurious Correlations using Counterfactual Alignment", "author": "Joseph Paul Cohen and Louis Blankemeier and Akshay Chaudhari", "abstract": "  Models driven by spurious correlations often yield poor generalization\nperformance. We propose the counterfactual (CF) alignment method to detect and\nquantify spurious correlations of black box classifiers. Our methodology is\nbased on counterfactual images generated with respect to one classifier being\ninput into other classifiers to see if they also induce changes in the outputs\nof these classifiers. The relationship between these responses can be\nquantified and used to identify specific instances where a spurious correlation\nexists. This is validated by observing intuitive trends in face-attribute and\nwaterbird classifiers, as well as by fabricating spurious correlations and\ndetecting their presence, both visually and quantitatively. Furthermore,\nutilizing the CF alignment method, we demonstrate that we can evaluate robust\noptimization methods (GroupDRO, JTT, and FLAC) by detecting a reduction in\nspurious correlations.\n", "link": "http://arxiv.org/abs/2312.02186v3", "date": "2025-01-15", "relevancy": 1.784, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4498}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4488}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Spurious%20Correlations%20using%20Counterfactual%20Alignment&body=Title%3A%20Identifying%20Spurious%20Correlations%20using%20Counterfactual%20Alignment%0AAuthor%3A%20Joseph%20Paul%20Cohen%20and%20Louis%20Blankemeier%20and%20Akshay%20Chaudhari%0AAbstract%3A%20%20%20Models%20driven%20by%20spurious%20correlations%20often%20yield%20poor%20generalization%0Aperformance.%20We%20propose%20the%20counterfactual%20%28CF%29%20alignment%20method%20to%20detect%20and%0Aquantify%20spurious%20correlations%20of%20black%20box%20classifiers.%20Our%20methodology%20is%0Abased%20on%20counterfactual%20images%20generated%20with%20respect%20to%20one%20classifier%20being%0Ainput%20into%20other%20classifiers%20to%20see%20if%20they%20also%20induce%20changes%20in%20the%20outputs%0Aof%20these%20classifiers.%20The%20relationship%20between%20these%20responses%20can%20be%0Aquantified%20and%20used%20to%20identify%20specific%20instances%20where%20a%20spurious%20correlation%0Aexists.%20This%20is%20validated%20by%20observing%20intuitive%20trends%20in%20face-attribute%20and%0Awaterbird%20classifiers%2C%20as%20well%20as%20by%20fabricating%20spurious%20correlations%20and%0Adetecting%20their%20presence%2C%20both%20visually%20and%20quantitatively.%20Furthermore%2C%0Autilizing%20the%20CF%20alignment%20method%2C%20we%20demonstrate%20that%20we%20can%20evaluate%20robust%0Aoptimization%20methods%20%28GroupDRO%2C%20JTT%2C%20and%20FLAC%29%20by%20detecting%20a%20reduction%20in%0Aspurious%20correlations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02186v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Spurious%2520Correlations%2520using%2520Counterfactual%2520Alignment%26entry.906535625%3DJoseph%2520Paul%2520Cohen%2520and%2520Louis%2520Blankemeier%2520and%2520Akshay%2520Chaudhari%26entry.1292438233%3D%2520%2520Models%2520driven%2520by%2520spurious%2520correlations%2520often%2520yield%2520poor%2520generalization%250Aperformance.%2520We%2520propose%2520the%2520counterfactual%2520%2528CF%2529%2520alignment%2520method%2520to%2520detect%2520and%250Aquantify%2520spurious%2520correlations%2520of%2520black%2520box%2520classifiers.%2520Our%2520methodology%2520is%250Abased%2520on%2520counterfactual%2520images%2520generated%2520with%2520respect%2520to%2520one%2520classifier%2520being%250Ainput%2520into%2520other%2520classifiers%2520to%2520see%2520if%2520they%2520also%2520induce%2520changes%2520in%2520the%2520outputs%250Aof%2520these%2520classifiers.%2520The%2520relationship%2520between%2520these%2520responses%2520can%2520be%250Aquantified%2520and%2520used%2520to%2520identify%2520specific%2520instances%2520where%2520a%2520spurious%2520correlation%250Aexists.%2520This%2520is%2520validated%2520by%2520observing%2520intuitive%2520trends%2520in%2520face-attribute%2520and%250Awaterbird%2520classifiers%252C%2520as%2520well%2520as%2520by%2520fabricating%2520spurious%2520correlations%2520and%250Adetecting%2520their%2520presence%252C%2520both%2520visually%2520and%2520quantitatively.%2520Furthermore%252C%250Autilizing%2520the%2520CF%2520alignment%2520method%252C%2520we%2520demonstrate%2520that%2520we%2520can%2520evaluate%2520robust%250Aoptimization%2520methods%2520%2528GroupDRO%252C%2520JTT%252C%2520and%2520FLAC%2529%2520by%2520detecting%2520a%2520reduction%2520in%250Aspurious%2520correlations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02186v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Spurious%20Correlations%20using%20Counterfactual%20Alignment&entry.906535625=Joseph%20Paul%20Cohen%20and%20Louis%20Blankemeier%20and%20Akshay%20Chaudhari&entry.1292438233=%20%20Models%20driven%20by%20spurious%20correlations%20often%20yield%20poor%20generalization%0Aperformance.%20We%20propose%20the%20counterfactual%20%28CF%29%20alignment%20method%20to%20detect%20and%0Aquantify%20spurious%20correlations%20of%20black%20box%20classifiers.%20Our%20methodology%20is%0Abased%20on%20counterfactual%20images%20generated%20with%20respect%20to%20one%20classifier%20being%0Ainput%20into%20other%20classifiers%20to%20see%20if%20they%20also%20induce%20changes%20in%20the%20outputs%0Aof%20these%20classifiers.%20The%20relationship%20between%20these%20responses%20can%20be%0Aquantified%20and%20used%20to%20identify%20specific%20instances%20where%20a%20spurious%20correlation%0Aexists.%20This%20is%20validated%20by%20observing%20intuitive%20trends%20in%20face-attribute%20and%0Awaterbird%20classifiers%2C%20as%20well%20as%20by%20fabricating%20spurious%20correlations%20and%0Adetecting%20their%20presence%2C%20both%20visually%20and%20quantitatively.%20Furthermore%2C%0Autilizing%20the%20CF%20alignment%20method%2C%20we%20demonstrate%20that%20we%20can%20evaluate%20robust%0Aoptimization%20methods%20%28GroupDRO%2C%20JTT%2C%20and%20FLAC%29%20by%20detecting%20a%20reduction%20in%0Aspurious%20correlations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02186v3&entry.124074799=Read"},
{"title": "Automatic tuning of communication protocols for vehicular ad hoc\n  networks using metaheuristics", "author": "Jos\u00e9 Garc\u00eda-Nieto and Jamal Toutouh and Enrique Alba", "abstract": "  The emerging field of vehicular ad hoc networks (VANETs) deals with a set of\ncommunicating vehicles which are able to spontaneously interconnect without any\npre-existing infrastructure. In such kind of networks, it is crucial to make an\noptimal configuration of the communication protocols previously to the final\nnetwork deployment. This way, a human designer can obtain an optimal QoS of the\nnetwork beforehand. The problem we consider in this work lies in configuring\nthe File Transfer protocol Configuration (FTC) with the aim of optimizing the\ntransmission time, the number of lost packets, and the amount of data\ntransferred in realistic VANET scenarios. We face the FTC with five\nrepresentative state-of-the-art optimization techniques and compare their\nperformance. These algorithms are: Particle Swarm Optimization (PSO),\nDifferential Evolution (DE), Genetic Algorithm (GA), Evolutionary Strategy\n(ES), and Simulated Annealing (SA). For our tests, two typical environment\ninstances of VANETs for Urban and Highway scenarios have been defined. The\nexperiments using ns- 2 (a well-known realistic VANET simulator) reveal that\nPSO outperforms all the compared algorithms for both studied VANET instances.\n", "link": "http://arxiv.org/abs/2501.08847v1", "date": "2025-01-15", "relevancy": 1.7729, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4458}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20tuning%20of%20communication%20protocols%20for%20vehicular%20ad%20hoc%0A%20%20networks%20using%20metaheuristics&body=Title%3A%20Automatic%20tuning%20of%20communication%20protocols%20for%20vehicular%20ad%20hoc%0A%20%20networks%20using%20metaheuristics%0AAuthor%3A%20Jos%C3%A9%20Garc%C3%ADa-Nieto%20and%20Jamal%20Toutouh%20and%20Enrique%20Alba%0AAbstract%3A%20%20%20The%20emerging%20field%20of%20vehicular%20ad%20hoc%20networks%20%28VANETs%29%20deals%20with%20a%20set%20of%0Acommunicating%20vehicles%20which%20are%20able%20to%20spontaneously%20interconnect%20without%20any%0Apre-existing%20infrastructure.%20In%20such%20kind%20of%20networks%2C%20it%20is%20crucial%20to%20make%20an%0Aoptimal%20configuration%20of%20the%20communication%20protocols%20previously%20to%20the%20final%0Anetwork%20deployment.%20This%20way%2C%20a%20human%20designer%20can%20obtain%20an%20optimal%20QoS%20of%20the%0Anetwork%20beforehand.%20The%20problem%20we%20consider%20in%20this%20work%20lies%20in%20configuring%0Athe%20File%20Transfer%20protocol%20Configuration%20%28FTC%29%20with%20the%20aim%20of%20optimizing%20the%0Atransmission%20time%2C%20the%20number%20of%20lost%20packets%2C%20and%20the%20amount%20of%20data%0Atransferred%20in%20realistic%20VANET%20scenarios.%20We%20face%20the%20FTC%20with%20five%0Arepresentative%20state-of-the-art%20optimization%20techniques%20and%20compare%20their%0Aperformance.%20These%20algorithms%20are%3A%20Particle%20Swarm%20Optimization%20%28PSO%29%2C%0ADifferential%20Evolution%20%28DE%29%2C%20Genetic%20Algorithm%20%28GA%29%2C%20Evolutionary%20Strategy%0A%28ES%29%2C%20and%20Simulated%20Annealing%20%28SA%29.%20For%20our%20tests%2C%20two%20typical%20environment%0Ainstances%20of%20VANETs%20for%20Urban%20and%20Highway%20scenarios%20have%20been%20defined.%20The%0Aexperiments%20using%20ns-%202%20%28a%20well-known%20realistic%20VANET%20simulator%29%20reveal%20that%0APSO%20outperforms%20all%20the%20compared%20algorithms%20for%20both%20studied%20VANET%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520tuning%2520of%2520communication%2520protocols%2520for%2520vehicular%2520ad%2520hoc%250A%2520%2520networks%2520using%2520metaheuristics%26entry.906535625%3DJos%25C3%25A9%2520Garc%25C3%25ADa-Nieto%2520and%2520Jamal%2520Toutouh%2520and%2520Enrique%2520Alba%26entry.1292438233%3D%2520%2520The%2520emerging%2520field%2520of%2520vehicular%2520ad%2520hoc%2520networks%2520%2528VANETs%2529%2520deals%2520with%2520a%2520set%2520of%250Acommunicating%2520vehicles%2520which%2520are%2520able%2520to%2520spontaneously%2520interconnect%2520without%2520any%250Apre-existing%2520infrastructure.%2520In%2520such%2520kind%2520of%2520networks%252C%2520it%2520is%2520crucial%2520to%2520make%2520an%250Aoptimal%2520configuration%2520of%2520the%2520communication%2520protocols%2520previously%2520to%2520the%2520final%250Anetwork%2520deployment.%2520This%2520way%252C%2520a%2520human%2520designer%2520can%2520obtain%2520an%2520optimal%2520QoS%2520of%2520the%250Anetwork%2520beforehand.%2520The%2520problem%2520we%2520consider%2520in%2520this%2520work%2520lies%2520in%2520configuring%250Athe%2520File%2520Transfer%2520protocol%2520Configuration%2520%2528FTC%2529%2520with%2520the%2520aim%2520of%2520optimizing%2520the%250Atransmission%2520time%252C%2520the%2520number%2520of%2520lost%2520packets%252C%2520and%2520the%2520amount%2520of%2520data%250Atransferred%2520in%2520realistic%2520VANET%2520scenarios.%2520We%2520face%2520the%2520FTC%2520with%2520five%250Arepresentative%2520state-of-the-art%2520optimization%2520techniques%2520and%2520compare%2520their%250Aperformance.%2520These%2520algorithms%2520are%253A%2520Particle%2520Swarm%2520Optimization%2520%2528PSO%2529%252C%250ADifferential%2520Evolution%2520%2528DE%2529%252C%2520Genetic%2520Algorithm%2520%2528GA%2529%252C%2520Evolutionary%2520Strategy%250A%2528ES%2529%252C%2520and%2520Simulated%2520Annealing%2520%2528SA%2529.%2520For%2520our%2520tests%252C%2520two%2520typical%2520environment%250Ainstances%2520of%2520VANETs%2520for%2520Urban%2520and%2520Highway%2520scenarios%2520have%2520been%2520defined.%2520The%250Aexperiments%2520using%2520ns-%25202%2520%2528a%2520well-known%2520realistic%2520VANET%2520simulator%2529%2520reveal%2520that%250APSO%2520outperforms%2520all%2520the%2520compared%2520algorithms%2520for%2520both%2520studied%2520VANET%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20tuning%20of%20communication%20protocols%20for%20vehicular%20ad%20hoc%0A%20%20networks%20using%20metaheuristics&entry.906535625=Jos%C3%A9%20Garc%C3%ADa-Nieto%20and%20Jamal%20Toutouh%20and%20Enrique%20Alba&entry.1292438233=%20%20The%20emerging%20field%20of%20vehicular%20ad%20hoc%20networks%20%28VANETs%29%20deals%20with%20a%20set%20of%0Acommunicating%20vehicles%20which%20are%20able%20to%20spontaneously%20interconnect%20without%20any%0Apre-existing%20infrastructure.%20In%20such%20kind%20of%20networks%2C%20it%20is%20crucial%20to%20make%20an%0Aoptimal%20configuration%20of%20the%20communication%20protocols%20previously%20to%20the%20final%0Anetwork%20deployment.%20This%20way%2C%20a%20human%20designer%20can%20obtain%20an%20optimal%20QoS%20of%20the%0Anetwork%20beforehand.%20The%20problem%20we%20consider%20in%20this%20work%20lies%20in%20configuring%0Athe%20File%20Transfer%20protocol%20Configuration%20%28FTC%29%20with%20the%20aim%20of%20optimizing%20the%0Atransmission%20time%2C%20the%20number%20of%20lost%20packets%2C%20and%20the%20amount%20of%20data%0Atransferred%20in%20realistic%20VANET%20scenarios.%20We%20face%20the%20FTC%20with%20five%0Arepresentative%20state-of-the-art%20optimization%20techniques%20and%20compare%20their%0Aperformance.%20These%20algorithms%20are%3A%20Particle%20Swarm%20Optimization%20%28PSO%29%2C%0ADifferential%20Evolution%20%28DE%29%2C%20Genetic%20Algorithm%20%28GA%29%2C%20Evolutionary%20Strategy%0A%28ES%29%2C%20and%20Simulated%20Annealing%20%28SA%29.%20For%20our%20tests%2C%20two%20typical%20environment%0Ainstances%20of%20VANETs%20for%20Urban%20and%20Highway%20scenarios%20have%20been%20defined.%20The%0Aexperiments%20using%20ns-%202%20%28a%20well-known%20realistic%20VANET%20simulator%29%20reveal%20that%0APSO%20outperforms%20all%20the%20compared%20algorithms%20for%20both%20studied%20VANET%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08847v1&entry.124074799=Read"},
{"title": "A design of Convolutional Neural Network model for the Diagnosis of the\n  COVID-19", "author": "Xinyuan Song", "abstract": "  With the spread of COVID-19 around the globe over the past year, the usage of\nartificial intelligence (AI) algorithms and image processing methods to analyze\nthe X-ray images of patients' chest with COVID-19 has become essential. The\nCOVID-19 virus recognition in the lung area of a patient is one of the basic\nand essential needs of clicical centers and hospitals. Most research in this\nfield has been devoted to papers on the basis of deep learning methods\nutilizing CNNs (Convolutional Neural Network), which mainly deal with the\nscreening of sick and healthy people.In this study, a new structure of a\n19-layer CNN has been recommended for accurately recognition of the COVID-19\nfrom the X-ray pictures of chest. The offered CNN is developed to serve as a\nprecise diagnosis system for a three class (viral pneumonia, Normal, COVID) and\na four classclassification (Lung opacity, Normal, COVID-19, and pneumonia). A\ncomparison is conducted among the outcomes of the offered procedure and some\npopular pretrained networks, including Inception, Alexnet, ResNet50,\nSqueezenet, and VGG19 and based on Specificity, Accuracy, Precision,\nSensitivity, Confusion Matrix, and F1-score. The experimental results of the\noffered CNN method specify its dominance over the existing published\nprocedures. This method can be a useful tool for clinicians in deciding\nproperly about COVID-19.\n", "link": "http://arxiv.org/abs/2311.06394v5", "date": "2025-01-15", "relevancy": 1.7699, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4517}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.444}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20design%20of%20Convolutional%20Neural%20Network%20model%20for%20the%20Diagnosis%20of%20the%0A%20%20COVID-19&body=Title%3A%20A%20design%20of%20Convolutional%20Neural%20Network%20model%20for%20the%20Diagnosis%20of%20the%0A%20%20COVID-19%0AAuthor%3A%20Xinyuan%20Song%0AAbstract%3A%20%20%20With%20the%20spread%20of%20COVID-19%20around%20the%20globe%20over%20the%20past%20year%2C%20the%20usage%20of%0Aartificial%20intelligence%20%28AI%29%20algorithms%20and%20image%20processing%20methods%20to%20analyze%0Athe%20X-ray%20images%20of%20patients%27%20chest%20with%20COVID-19%20has%20become%20essential.%20The%0ACOVID-19%20virus%20recognition%20in%20the%20lung%20area%20of%20a%20patient%20is%20one%20of%20the%20basic%0Aand%20essential%20needs%20of%20clicical%20centers%20and%20hospitals.%20Most%20research%20in%20this%0Afield%20has%20been%20devoted%20to%20papers%20on%20the%20basis%20of%20deep%20learning%20methods%0Autilizing%20CNNs%20%28Convolutional%20Neural%20Network%29%2C%20which%20mainly%20deal%20with%20the%0Ascreening%20of%20sick%20and%20healthy%20people.In%20this%20study%2C%20a%20new%20structure%20of%20a%0A19-layer%20CNN%20has%20been%20recommended%20for%20accurately%20recognition%20of%20the%20COVID-19%0Afrom%20the%20X-ray%20pictures%20of%20chest.%20The%20offered%20CNN%20is%20developed%20to%20serve%20as%20a%0Aprecise%20diagnosis%20system%20for%20a%20three%20class%20%28viral%20pneumonia%2C%20Normal%2C%20COVID%29%20and%0Aa%20four%20classclassification%20%28Lung%20opacity%2C%20Normal%2C%20COVID-19%2C%20and%20pneumonia%29.%20A%0Acomparison%20is%20conducted%20among%20the%20outcomes%20of%20the%20offered%20procedure%20and%20some%0Apopular%20pretrained%20networks%2C%20including%20Inception%2C%20Alexnet%2C%20ResNet50%2C%0ASqueezenet%2C%20and%20VGG19%20and%20based%20on%20Specificity%2C%20Accuracy%2C%20Precision%2C%0ASensitivity%2C%20Confusion%20Matrix%2C%20and%20F1-score.%20The%20experimental%20results%20of%20the%0Aoffered%20CNN%20method%20specify%20its%20dominance%20over%20the%20existing%20published%0Aprocedures.%20This%20method%20can%20be%20a%20useful%20tool%20for%20clinicians%20in%20deciding%0Aproperly%20about%20COVID-19.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06394v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520design%2520of%2520Convolutional%2520Neural%2520Network%2520model%2520for%2520the%2520Diagnosis%2520of%2520the%250A%2520%2520COVID-19%26entry.906535625%3DXinyuan%2520Song%26entry.1292438233%3D%2520%2520With%2520the%2520spread%2520of%2520COVID-19%2520around%2520the%2520globe%2520over%2520the%2520past%2520year%252C%2520the%2520usage%2520of%250Aartificial%2520intelligence%2520%2528AI%2529%2520algorithms%2520and%2520image%2520processing%2520methods%2520to%2520analyze%250Athe%2520X-ray%2520images%2520of%2520patients%2527%2520chest%2520with%2520COVID-19%2520has%2520become%2520essential.%2520The%250ACOVID-19%2520virus%2520recognition%2520in%2520the%2520lung%2520area%2520of%2520a%2520patient%2520is%2520one%2520of%2520the%2520basic%250Aand%2520essential%2520needs%2520of%2520clicical%2520centers%2520and%2520hospitals.%2520Most%2520research%2520in%2520this%250Afield%2520has%2520been%2520devoted%2520to%2520papers%2520on%2520the%2520basis%2520of%2520deep%2520learning%2520methods%250Autilizing%2520CNNs%2520%2528Convolutional%2520Neural%2520Network%2529%252C%2520which%2520mainly%2520deal%2520with%2520the%250Ascreening%2520of%2520sick%2520and%2520healthy%2520people.In%2520this%2520study%252C%2520a%2520new%2520structure%2520of%2520a%250A19-layer%2520CNN%2520has%2520been%2520recommended%2520for%2520accurately%2520recognition%2520of%2520the%2520COVID-19%250Afrom%2520the%2520X-ray%2520pictures%2520of%2520chest.%2520The%2520offered%2520CNN%2520is%2520developed%2520to%2520serve%2520as%2520a%250Aprecise%2520diagnosis%2520system%2520for%2520a%2520three%2520class%2520%2528viral%2520pneumonia%252C%2520Normal%252C%2520COVID%2529%2520and%250Aa%2520four%2520classclassification%2520%2528Lung%2520opacity%252C%2520Normal%252C%2520COVID-19%252C%2520and%2520pneumonia%2529.%2520A%250Acomparison%2520is%2520conducted%2520among%2520the%2520outcomes%2520of%2520the%2520offered%2520procedure%2520and%2520some%250Apopular%2520pretrained%2520networks%252C%2520including%2520Inception%252C%2520Alexnet%252C%2520ResNet50%252C%250ASqueezenet%252C%2520and%2520VGG19%2520and%2520based%2520on%2520Specificity%252C%2520Accuracy%252C%2520Precision%252C%250ASensitivity%252C%2520Confusion%2520Matrix%252C%2520and%2520F1-score.%2520The%2520experimental%2520results%2520of%2520the%250Aoffered%2520CNN%2520method%2520specify%2520its%2520dominance%2520over%2520the%2520existing%2520published%250Aprocedures.%2520This%2520method%2520can%2520be%2520a%2520useful%2520tool%2520for%2520clinicians%2520in%2520deciding%250Aproperly%2520about%2520COVID-19.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.06394v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20design%20of%20Convolutional%20Neural%20Network%20model%20for%20the%20Diagnosis%20of%20the%0A%20%20COVID-19&entry.906535625=Xinyuan%20Song&entry.1292438233=%20%20With%20the%20spread%20of%20COVID-19%20around%20the%20globe%20over%20the%20past%20year%2C%20the%20usage%20of%0Aartificial%20intelligence%20%28AI%29%20algorithms%20and%20image%20processing%20methods%20to%20analyze%0Athe%20X-ray%20images%20of%20patients%27%20chest%20with%20COVID-19%20has%20become%20essential.%20The%0ACOVID-19%20virus%20recognition%20in%20the%20lung%20area%20of%20a%20patient%20is%20one%20of%20the%20basic%0Aand%20essential%20needs%20of%20clicical%20centers%20and%20hospitals.%20Most%20research%20in%20this%0Afield%20has%20been%20devoted%20to%20papers%20on%20the%20basis%20of%20deep%20learning%20methods%0Autilizing%20CNNs%20%28Convolutional%20Neural%20Network%29%2C%20which%20mainly%20deal%20with%20the%0Ascreening%20of%20sick%20and%20healthy%20people.In%20this%20study%2C%20a%20new%20structure%20of%20a%0A19-layer%20CNN%20has%20been%20recommended%20for%20accurately%20recognition%20of%20the%20COVID-19%0Afrom%20the%20X-ray%20pictures%20of%20chest.%20The%20offered%20CNN%20is%20developed%20to%20serve%20as%20a%0Aprecise%20diagnosis%20system%20for%20a%20three%20class%20%28viral%20pneumonia%2C%20Normal%2C%20COVID%29%20and%0Aa%20four%20classclassification%20%28Lung%20opacity%2C%20Normal%2C%20COVID-19%2C%20and%20pneumonia%29.%20A%0Acomparison%20is%20conducted%20among%20the%20outcomes%20of%20the%20offered%20procedure%20and%20some%0Apopular%20pretrained%20networks%2C%20including%20Inception%2C%20Alexnet%2C%20ResNet50%2C%0ASqueezenet%2C%20and%20VGG19%20and%20based%20on%20Specificity%2C%20Accuracy%2C%20Precision%2C%0ASensitivity%2C%20Confusion%20Matrix%2C%20and%20F1-score.%20The%20experimental%20results%20of%20the%0Aoffered%20CNN%20method%20specify%20its%20dominance%20over%20the%20existing%20published%0Aprocedures.%20This%20method%20can%20be%20a%20useful%20tool%20for%20clinicians%20in%20deciding%0Aproperly%20about%20COVID-19.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06394v5&entry.124074799=Read"},
{"title": "Training-Aware Risk Control for Intensity Modulated Radiation Therapies\n  Quality Assurance with Conformal Prediction", "author": "Kevin He and David Adam and Sarah Han-Oh and Anqi Liu", "abstract": "  Measurement quality assurance (QA) practices play a key role in the safe use\nof Intensity Modulated Radiation Therapies (IMRT) for cancer treatment. These\npractices have reduced measurement-based IMRT QA failure below 1%. However,\nthese practices are time and labor intensive which can lead to delays in\npatient care. In this study, we examine how conformal prediction methodologies\ncan be used to robustly triage plans. We propose a new training-aware conformal\nrisk control method by combining the benefit of conformal risk control and\nconformal training. We incorporate the decision making thresholds based on the\ngamma passing rate, along with the risk functions used in clinical evaluation,\ninto the design of the risk control framework. Our method achieves high\nsensitivity and specificity and significantly reduces the number of plans\nneeding measurement without generating a huge confidence interval. Our results\ndemonstrate the validity and applicability of conformal prediction methods for\nimproving efficiency and reducing the workload of the IMRT QA process.\n", "link": "http://arxiv.org/abs/2501.08963v1", "date": "2025-01-15", "relevancy": 1.7691, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4633}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4526}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Aware%20Risk%20Control%20for%20Intensity%20Modulated%20Radiation%20Therapies%0A%20%20Quality%20Assurance%20with%20Conformal%20Prediction&body=Title%3A%20Training-Aware%20Risk%20Control%20for%20Intensity%20Modulated%20Radiation%20Therapies%0A%20%20Quality%20Assurance%20with%20Conformal%20Prediction%0AAuthor%3A%20Kevin%20He%20and%20David%20Adam%20and%20Sarah%20Han-Oh%20and%20Anqi%20Liu%0AAbstract%3A%20%20%20Measurement%20quality%20assurance%20%28QA%29%20practices%20play%20a%20key%20role%20in%20the%20safe%20use%0Aof%20Intensity%20Modulated%20Radiation%20Therapies%20%28IMRT%29%20for%20cancer%20treatment.%20These%0Apractices%20have%20reduced%20measurement-based%20IMRT%20QA%20failure%20below%201%25.%20However%2C%0Athese%20practices%20are%20time%20and%20labor%20intensive%20which%20can%20lead%20to%20delays%20in%0Apatient%20care.%20In%20this%20study%2C%20we%20examine%20how%20conformal%20prediction%20methodologies%0Acan%20be%20used%20to%20robustly%20triage%20plans.%20We%20propose%20a%20new%20training-aware%20conformal%0Arisk%20control%20method%20by%20combining%20the%20benefit%20of%20conformal%20risk%20control%20and%0Aconformal%20training.%20We%20incorporate%20the%20decision%20making%20thresholds%20based%20on%20the%0Agamma%20passing%20rate%2C%20along%20with%20the%20risk%20functions%20used%20in%20clinical%20evaluation%2C%0Ainto%20the%20design%20of%20the%20risk%20control%20framework.%20Our%20method%20achieves%20high%0Asensitivity%20and%20specificity%20and%20significantly%20reduces%20the%20number%20of%20plans%0Aneeding%20measurement%20without%20generating%20a%20huge%20confidence%20interval.%20Our%20results%0Ademonstrate%20the%20validity%20and%20applicability%20of%20conformal%20prediction%20methods%20for%0Aimproving%20efficiency%20and%20reducing%20the%20workload%20of%20the%20IMRT%20QA%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Aware%2520Risk%2520Control%2520for%2520Intensity%2520Modulated%2520Radiation%2520Therapies%250A%2520%2520Quality%2520Assurance%2520with%2520Conformal%2520Prediction%26entry.906535625%3DKevin%2520He%2520and%2520David%2520Adam%2520and%2520Sarah%2520Han-Oh%2520and%2520Anqi%2520Liu%26entry.1292438233%3D%2520%2520Measurement%2520quality%2520assurance%2520%2528QA%2529%2520practices%2520play%2520a%2520key%2520role%2520in%2520the%2520safe%2520use%250Aof%2520Intensity%2520Modulated%2520Radiation%2520Therapies%2520%2528IMRT%2529%2520for%2520cancer%2520treatment.%2520These%250Apractices%2520have%2520reduced%2520measurement-based%2520IMRT%2520QA%2520failure%2520below%25201%2525.%2520However%252C%250Athese%2520practices%2520are%2520time%2520and%2520labor%2520intensive%2520which%2520can%2520lead%2520to%2520delays%2520in%250Apatient%2520care.%2520In%2520this%2520study%252C%2520we%2520examine%2520how%2520conformal%2520prediction%2520methodologies%250Acan%2520be%2520used%2520to%2520robustly%2520triage%2520plans.%2520We%2520propose%2520a%2520new%2520training-aware%2520conformal%250Arisk%2520control%2520method%2520by%2520combining%2520the%2520benefit%2520of%2520conformal%2520risk%2520control%2520and%250Aconformal%2520training.%2520We%2520incorporate%2520the%2520decision%2520making%2520thresholds%2520based%2520on%2520the%250Agamma%2520passing%2520rate%252C%2520along%2520with%2520the%2520risk%2520functions%2520used%2520in%2520clinical%2520evaluation%252C%250Ainto%2520the%2520design%2520of%2520the%2520risk%2520control%2520framework.%2520Our%2520method%2520achieves%2520high%250Asensitivity%2520and%2520specificity%2520and%2520significantly%2520reduces%2520the%2520number%2520of%2520plans%250Aneeding%2520measurement%2520without%2520generating%2520a%2520huge%2520confidence%2520interval.%2520Our%2520results%250Ademonstrate%2520the%2520validity%2520and%2520applicability%2520of%2520conformal%2520prediction%2520methods%2520for%250Aimproving%2520efficiency%2520and%2520reducing%2520the%2520workload%2520of%2520the%2520IMRT%2520QA%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Aware%20Risk%20Control%20for%20Intensity%20Modulated%20Radiation%20Therapies%0A%20%20Quality%20Assurance%20with%20Conformal%20Prediction&entry.906535625=Kevin%20He%20and%20David%20Adam%20and%20Sarah%20Han-Oh%20and%20Anqi%20Liu&entry.1292438233=%20%20Measurement%20quality%20assurance%20%28QA%29%20practices%20play%20a%20key%20role%20in%20the%20safe%20use%0Aof%20Intensity%20Modulated%20Radiation%20Therapies%20%28IMRT%29%20for%20cancer%20treatment.%20These%0Apractices%20have%20reduced%20measurement-based%20IMRT%20QA%20failure%20below%201%25.%20However%2C%0Athese%20practices%20are%20time%20and%20labor%20intensive%20which%20can%20lead%20to%20delays%20in%0Apatient%20care.%20In%20this%20study%2C%20we%20examine%20how%20conformal%20prediction%20methodologies%0Acan%20be%20used%20to%20robustly%20triage%20plans.%20We%20propose%20a%20new%20training-aware%20conformal%0Arisk%20control%20method%20by%20combining%20the%20benefit%20of%20conformal%20risk%20control%20and%0Aconformal%20training.%20We%20incorporate%20the%20decision%20making%20thresholds%20based%20on%20the%0Agamma%20passing%20rate%2C%20along%20with%20the%20risk%20functions%20used%20in%20clinical%20evaluation%2C%0Ainto%20the%20design%20of%20the%20risk%20control%20framework.%20Our%20method%20achieves%20high%0Asensitivity%20and%20specificity%20and%20significantly%20reduces%20the%20number%20of%20plans%0Aneeding%20measurement%20without%20generating%20a%20huge%20confidence%20interval.%20Our%20results%0Ademonstrate%20the%20validity%20and%20applicability%20of%20conformal%20prediction%20methods%20for%0Aimproving%20efficiency%20and%20reducing%20the%20workload%20of%20the%20IMRT%20QA%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08963v1&entry.124074799=Read"},
{"title": "SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and\n  Segmentation Mask Generation", "author": "Aditya Bhat and Rupak Bose and Chinedu Innocent Nwoye and Nicolas Padoy", "abstract": "  Acquiring and annotating surgical data is often resource-intensive, ethical\nconstraining, and requiring significant expert involvement. While generative AI\nmodels like text-to-image can alleviate data scarcity, incorporating spatial\nannotations, such as segmentation masks, is crucial for precision-driven\nsurgical applications, simulation, and education. This study introduces both a\nnovel task and method, SimGen, for Simultaneous Image and Mask Generation.\nSimGen is a diffusion model based on the DDPM framework and Residual U-Net,\ndesigned to jointly generate high-fidelity surgical images and their\ncorresponding segmentation masks. The model leverages cross-correlation priors\nto capture dependencies between continuous image and discrete mask\ndistributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed to\nenhance class separability and uniformity in the RGB space of the masks. SimGen\ndelivers high-fidelity images and accurate segmentation masks, outperforming\nbaselines across six public datasets assessed on image and semantic inception\ndistance metrics. Ablation study shows that the CFL improves mask quality and\nspatial separation. Downstream experiments suggest generated image-mask pairs\nare usable if regulations limit human data release for research. This work\noffers a cost-effective solution for generating paired surgical images and\ncomplex labels, advancing surgical AI development by reducing the need for\nexpensive manual annotations.\n", "link": "http://arxiv.org/abs/2501.09008v1", "date": "2025-01-15", "relevancy": 1.7634, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6307}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5775}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimGen%3A%20A%20Diffusion-Based%20Framework%20for%20Simultaneous%20Surgical%20Image%20and%0A%20%20Segmentation%20Mask%20Generation&body=Title%3A%20SimGen%3A%20A%20Diffusion-Based%20Framework%20for%20Simultaneous%20Surgical%20Image%20and%0A%20%20Segmentation%20Mask%20Generation%0AAuthor%3A%20Aditya%20Bhat%20and%20Rupak%20Bose%20and%20Chinedu%20Innocent%20Nwoye%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Acquiring%20and%20annotating%20surgical%20data%20is%20often%20resource-intensive%2C%20ethical%0Aconstraining%2C%20and%20requiring%20significant%20expert%20involvement.%20While%20generative%20AI%0Amodels%20like%20text-to-image%20can%20alleviate%20data%20scarcity%2C%20incorporating%20spatial%0Aannotations%2C%20such%20as%20segmentation%20masks%2C%20is%20crucial%20for%20precision-driven%0Asurgical%20applications%2C%20simulation%2C%20and%20education.%20This%20study%20introduces%20both%20a%0Anovel%20task%20and%20method%2C%20SimGen%2C%20for%20Simultaneous%20Image%20and%20Mask%20Generation.%0ASimGen%20is%20a%20diffusion%20model%20based%20on%20the%20DDPM%20framework%20and%20Residual%20U-Net%2C%0Adesigned%20to%20jointly%20generate%20high-fidelity%20surgical%20images%20and%20their%0Acorresponding%20segmentation%20masks.%20The%20model%20leverages%20cross-correlation%20priors%0Ato%20capture%20dependencies%20between%20continuous%20image%20and%20discrete%20mask%0Adistributions.%20Additionally%2C%20a%20Canonical%20Fibonacci%20Lattice%20%28CFL%29%20is%20employed%20to%0Aenhance%20class%20separability%20and%20uniformity%20in%20the%20RGB%20space%20of%20the%20masks.%20SimGen%0Adelivers%20high-fidelity%20images%20and%20accurate%20segmentation%20masks%2C%20outperforming%0Abaselines%20across%20six%20public%20datasets%20assessed%20on%20image%20and%20semantic%20inception%0Adistance%20metrics.%20Ablation%20study%20shows%20that%20the%20CFL%20improves%20mask%20quality%20and%0Aspatial%20separation.%20Downstream%20experiments%20suggest%20generated%20image-mask%20pairs%0Aare%20usable%20if%20regulations%20limit%20human%20data%20release%20for%20research.%20This%20work%0Aoffers%20a%20cost-effective%20solution%20for%20generating%20paired%20surgical%20images%20and%0Acomplex%20labels%2C%20advancing%20surgical%20AI%20development%20by%20reducing%20the%20need%20for%0Aexpensive%20manual%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimGen%253A%2520A%2520Diffusion-Based%2520Framework%2520for%2520Simultaneous%2520Surgical%2520Image%2520and%250A%2520%2520Segmentation%2520Mask%2520Generation%26entry.906535625%3DAditya%2520Bhat%2520and%2520Rupak%2520Bose%2520and%2520Chinedu%2520Innocent%2520Nwoye%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Acquiring%2520and%2520annotating%2520surgical%2520data%2520is%2520often%2520resource-intensive%252C%2520ethical%250Aconstraining%252C%2520and%2520requiring%2520significant%2520expert%2520involvement.%2520While%2520generative%2520AI%250Amodels%2520like%2520text-to-image%2520can%2520alleviate%2520data%2520scarcity%252C%2520incorporating%2520spatial%250Aannotations%252C%2520such%2520as%2520segmentation%2520masks%252C%2520is%2520crucial%2520for%2520precision-driven%250Asurgical%2520applications%252C%2520simulation%252C%2520and%2520education.%2520This%2520study%2520introduces%2520both%2520a%250Anovel%2520task%2520and%2520method%252C%2520SimGen%252C%2520for%2520Simultaneous%2520Image%2520and%2520Mask%2520Generation.%250ASimGen%2520is%2520a%2520diffusion%2520model%2520based%2520on%2520the%2520DDPM%2520framework%2520and%2520Residual%2520U-Net%252C%250Adesigned%2520to%2520jointly%2520generate%2520high-fidelity%2520surgical%2520images%2520and%2520their%250Acorresponding%2520segmentation%2520masks.%2520The%2520model%2520leverages%2520cross-correlation%2520priors%250Ato%2520capture%2520dependencies%2520between%2520continuous%2520image%2520and%2520discrete%2520mask%250Adistributions.%2520Additionally%252C%2520a%2520Canonical%2520Fibonacci%2520Lattice%2520%2528CFL%2529%2520is%2520employed%2520to%250Aenhance%2520class%2520separability%2520and%2520uniformity%2520in%2520the%2520RGB%2520space%2520of%2520the%2520masks.%2520SimGen%250Adelivers%2520high-fidelity%2520images%2520and%2520accurate%2520segmentation%2520masks%252C%2520outperforming%250Abaselines%2520across%2520six%2520public%2520datasets%2520assessed%2520on%2520image%2520and%2520semantic%2520inception%250Adistance%2520metrics.%2520Ablation%2520study%2520shows%2520that%2520the%2520CFL%2520improves%2520mask%2520quality%2520and%250Aspatial%2520separation.%2520Downstream%2520experiments%2520suggest%2520generated%2520image-mask%2520pairs%250Aare%2520usable%2520if%2520regulations%2520limit%2520human%2520data%2520release%2520for%2520research.%2520This%2520work%250Aoffers%2520a%2520cost-effective%2520solution%2520for%2520generating%2520paired%2520surgical%2520images%2520and%250Acomplex%2520labels%252C%2520advancing%2520surgical%2520AI%2520development%2520by%2520reducing%2520the%2520need%2520for%250Aexpensive%2520manual%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimGen%3A%20A%20Diffusion-Based%20Framework%20for%20Simultaneous%20Surgical%20Image%20and%0A%20%20Segmentation%20Mask%20Generation&entry.906535625=Aditya%20Bhat%20and%20Rupak%20Bose%20and%20Chinedu%20Innocent%20Nwoye%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Acquiring%20and%20annotating%20surgical%20data%20is%20often%20resource-intensive%2C%20ethical%0Aconstraining%2C%20and%20requiring%20significant%20expert%20involvement.%20While%20generative%20AI%0Amodels%20like%20text-to-image%20can%20alleviate%20data%20scarcity%2C%20incorporating%20spatial%0Aannotations%2C%20such%20as%20segmentation%20masks%2C%20is%20crucial%20for%20precision-driven%0Asurgical%20applications%2C%20simulation%2C%20and%20education.%20This%20study%20introduces%20both%20a%0Anovel%20task%20and%20method%2C%20SimGen%2C%20for%20Simultaneous%20Image%20and%20Mask%20Generation.%0ASimGen%20is%20a%20diffusion%20model%20based%20on%20the%20DDPM%20framework%20and%20Residual%20U-Net%2C%0Adesigned%20to%20jointly%20generate%20high-fidelity%20surgical%20images%20and%20their%0Acorresponding%20segmentation%20masks.%20The%20model%20leverages%20cross-correlation%20priors%0Ato%20capture%20dependencies%20between%20continuous%20image%20and%20discrete%20mask%0Adistributions.%20Additionally%2C%20a%20Canonical%20Fibonacci%20Lattice%20%28CFL%29%20is%20employed%20to%0Aenhance%20class%20separability%20and%20uniformity%20in%20the%20RGB%20space%20of%20the%20masks.%20SimGen%0Adelivers%20high-fidelity%20images%20and%20accurate%20segmentation%20masks%2C%20outperforming%0Abaselines%20across%20six%20public%20datasets%20assessed%20on%20image%20and%20semantic%20inception%0Adistance%20metrics.%20Ablation%20study%20shows%20that%20the%20CFL%20improves%20mask%20quality%20and%0Aspatial%20separation.%20Downstream%20experiments%20suggest%20generated%20image-mask%20pairs%0Aare%20usable%20if%20regulations%20limit%20human%20data%20release%20for%20research.%20This%20work%0Aoffers%20a%20cost-effective%20solution%20for%20generating%20paired%20surgical%20images%20and%0Acomplex%20labels%2C%20advancing%20surgical%20AI%20development%20by%20reducing%20the%20need%20for%0Aexpensive%20manual%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09008v1&entry.124074799=Read"},
{"title": "Supervised Kernel Thinning", "author": "Albert Gong and Kyuseong Choi and Raaz Dwivedi", "abstract": "  The kernel thinning algorithm of Dwivedi & Mackey (2024) provides a\nbetter-than-i.i.d. compression of a generic set of points. By generating\nhigh-fidelity coresets of size significantly smaller than the input points, KT\nis known to speed up unsupervised tasks like Monte Carlo integration,\nuncertainty quantification, and non-parametric hypothesis testing, with minimal\nloss in statistical accuracy. In this work, we generalize the KT algorithm to\nspeed up supervised learning problems involving kernel methods. Specifically,\nwe combine two classical algorithms--Nadaraya-Watson (NW) regression or kernel\nsmoothing, and kernel ridge regression (KRR)--with KT to provide a quadratic\nspeed-up in both training and inference times. We show how distribution\ncompression with KT in each setting reduces to constructing an appropriate\nkernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators.\nWe prove that KT-based regression estimators enjoy significantly superior\ncomputational efficiency over the full-data estimators and improved statistical\nefficiency over i.i.d. subsampling of the training data. En route, we also\nprovide a novel multiplicative error guarantee for compressing with KT. We\nvalidate our design choices with both simulations and real data experiments.\n", "link": "http://arxiv.org/abs/2410.13749v2", "date": "2025-01-15", "relevancy": 1.7596, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4706}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4416}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Kernel%20Thinning&body=Title%3A%20Supervised%20Kernel%20Thinning%0AAuthor%3A%20Albert%20Gong%20and%20Kyuseong%20Choi%20and%20Raaz%20Dwivedi%0AAbstract%3A%20%20%20The%20kernel%20thinning%20algorithm%20of%20Dwivedi%20%26%20Mackey%20%282024%29%20provides%20a%0Abetter-than-i.i.d.%20compression%20of%20a%20generic%20set%20of%20points.%20By%20generating%0Ahigh-fidelity%20coresets%20of%20size%20significantly%20smaller%20than%20the%20input%20points%2C%20KT%0Ais%20known%20to%20speed%20up%20unsupervised%20tasks%20like%20Monte%20Carlo%20integration%2C%0Auncertainty%20quantification%2C%20and%20non-parametric%20hypothesis%20testing%2C%20with%20minimal%0Aloss%20in%20statistical%20accuracy.%20In%20this%20work%2C%20we%20generalize%20the%20KT%20algorithm%20to%0Aspeed%20up%20supervised%20learning%20problems%20involving%20kernel%20methods.%20Specifically%2C%0Awe%20combine%20two%20classical%20algorithms--Nadaraya-Watson%20%28NW%29%20regression%20or%20kernel%0Asmoothing%2C%20and%20kernel%20ridge%20regression%20%28KRR%29--with%20KT%20to%20provide%20a%20quadratic%0Aspeed-up%20in%20both%20training%20and%20inference%20times.%20We%20show%20how%20distribution%0Acompression%20with%20KT%20in%20each%20setting%20reduces%20to%20constructing%20an%20appropriate%0Akernel%2C%20and%20introduce%20the%20Kernel-Thinned%20NW%20and%20Kernel-Thinned%20KRR%20estimators.%0AWe%20prove%20that%20KT-based%20regression%20estimators%20enjoy%20significantly%20superior%0Acomputational%20efficiency%20over%20the%20full-data%20estimators%20and%20improved%20statistical%0Aefficiency%20over%20i.i.d.%20subsampling%20of%20the%20training%20data.%20En%20route%2C%20we%20also%0Aprovide%20a%20novel%20multiplicative%20error%20guarantee%20for%20compressing%20with%20KT.%20We%0Avalidate%20our%20design%20choices%20with%20both%20simulations%20and%20real%20data%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13749v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Kernel%2520Thinning%26entry.906535625%3DAlbert%2520Gong%2520and%2520Kyuseong%2520Choi%2520and%2520Raaz%2520Dwivedi%26entry.1292438233%3D%2520%2520The%2520kernel%2520thinning%2520algorithm%2520of%2520Dwivedi%2520%2526%2520Mackey%2520%25282024%2529%2520provides%2520a%250Abetter-than-i.i.d.%2520compression%2520of%2520a%2520generic%2520set%2520of%2520points.%2520By%2520generating%250Ahigh-fidelity%2520coresets%2520of%2520size%2520significantly%2520smaller%2520than%2520the%2520input%2520points%252C%2520KT%250Ais%2520known%2520to%2520speed%2520up%2520unsupervised%2520tasks%2520like%2520Monte%2520Carlo%2520integration%252C%250Auncertainty%2520quantification%252C%2520and%2520non-parametric%2520hypothesis%2520testing%252C%2520with%2520minimal%250Aloss%2520in%2520statistical%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520generalize%2520the%2520KT%2520algorithm%2520to%250Aspeed%2520up%2520supervised%2520learning%2520problems%2520involving%2520kernel%2520methods.%2520Specifically%252C%250Awe%2520combine%2520two%2520classical%2520algorithms--Nadaraya-Watson%2520%2528NW%2529%2520regression%2520or%2520kernel%250Asmoothing%252C%2520and%2520kernel%2520ridge%2520regression%2520%2528KRR%2529--with%2520KT%2520to%2520provide%2520a%2520quadratic%250Aspeed-up%2520in%2520both%2520training%2520and%2520inference%2520times.%2520We%2520show%2520how%2520distribution%250Acompression%2520with%2520KT%2520in%2520each%2520setting%2520reduces%2520to%2520constructing%2520an%2520appropriate%250Akernel%252C%2520and%2520introduce%2520the%2520Kernel-Thinned%2520NW%2520and%2520Kernel-Thinned%2520KRR%2520estimators.%250AWe%2520prove%2520that%2520KT-based%2520regression%2520estimators%2520enjoy%2520significantly%2520superior%250Acomputational%2520efficiency%2520over%2520the%2520full-data%2520estimators%2520and%2520improved%2520statistical%250Aefficiency%2520over%2520i.i.d.%2520subsampling%2520of%2520the%2520training%2520data.%2520En%2520route%252C%2520we%2520also%250Aprovide%2520a%2520novel%2520multiplicative%2520error%2520guarantee%2520for%2520compressing%2520with%2520KT.%2520We%250Avalidate%2520our%2520design%2520choices%2520with%2520both%2520simulations%2520and%2520real%2520data%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13749v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Kernel%20Thinning&entry.906535625=Albert%20Gong%20and%20Kyuseong%20Choi%20and%20Raaz%20Dwivedi&entry.1292438233=%20%20The%20kernel%20thinning%20algorithm%20of%20Dwivedi%20%26%20Mackey%20%282024%29%20provides%20a%0Abetter-than-i.i.d.%20compression%20of%20a%20generic%20set%20of%20points.%20By%20generating%0Ahigh-fidelity%20coresets%20of%20size%20significantly%20smaller%20than%20the%20input%20points%2C%20KT%0Ais%20known%20to%20speed%20up%20unsupervised%20tasks%20like%20Monte%20Carlo%20integration%2C%0Auncertainty%20quantification%2C%20and%20non-parametric%20hypothesis%20testing%2C%20with%20minimal%0Aloss%20in%20statistical%20accuracy.%20In%20this%20work%2C%20we%20generalize%20the%20KT%20algorithm%20to%0Aspeed%20up%20supervised%20learning%20problems%20involving%20kernel%20methods.%20Specifically%2C%0Awe%20combine%20two%20classical%20algorithms--Nadaraya-Watson%20%28NW%29%20regression%20or%20kernel%0Asmoothing%2C%20and%20kernel%20ridge%20regression%20%28KRR%29--with%20KT%20to%20provide%20a%20quadratic%0Aspeed-up%20in%20both%20training%20and%20inference%20times.%20We%20show%20how%20distribution%0Acompression%20with%20KT%20in%20each%20setting%20reduces%20to%20constructing%20an%20appropriate%0Akernel%2C%20and%20introduce%20the%20Kernel-Thinned%20NW%20and%20Kernel-Thinned%20KRR%20estimators.%0AWe%20prove%20that%20KT-based%20regression%20estimators%20enjoy%20significantly%20superior%0Acomputational%20efficiency%20over%20the%20full-data%20estimators%20and%20improved%20statistical%0Aefficiency%20over%20i.i.d.%20subsampling%20of%20the%20training%20data.%20En%20route%2C%20we%20also%0Aprovide%20a%20novel%20multiplicative%20error%20guarantee%20for%20compressing%20with%20KT.%20We%0Avalidate%20our%20design%20choices%20with%20both%20simulations%20and%20real%20data%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13749v2&entry.124074799=Read"},
{"title": "A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models", "author": "Raghav Singhal and Zachary Horvitz and Ryan Teehan and Mengye Ren and Zhou Yu and Kathleen McKeown and Rajesh Ranganath", "abstract": "  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n", "link": "http://arxiv.org/abs/2501.06848v2", "date": "2025-01-15", "relevancy": 1.7588, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6231}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5947}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20General%20Framework%20for%20Inference-time%20Scaling%20and%20Steering%20of%20Diffusion%0A%20%20Models&body=Title%3A%20A%20General%20Framework%20for%20Inference-time%20Scaling%20and%20Steering%20of%20Diffusion%0A%20%20Models%0AAuthor%3A%20Raghav%20Singhal%20and%20Zachary%20Horvitz%20and%20Ryan%20Teehan%20and%20Mengye%20Ren%20and%20Zhou%20Yu%20and%20Kathleen%20McKeown%20and%20Rajesh%20Ranganath%0AAbstract%3A%20%20%20Diffusion%20models%20produce%20impressive%20results%20in%20modalities%20ranging%20from%20images%0Aand%20video%20to%20protein%20design%20and%20text.%20However%2C%20generating%20samples%20with%0Auser-specified%20properties%20remains%20a%20challenge.%20Recent%20research%20proposes%0Afine-tuning%20models%20to%20maximize%20rewards%20that%20capture%20desired%20properties%2C%20but%0Athese%20methods%20require%20expensive%20training%20and%20are%20prone%20to%20mode%20collapse.%20In%0Athis%20work%2C%20we%20propose%20Feynman%20Kac%20%28FK%29%20steering%2C%20an%20inference-time%20framework%0Afor%20steering%20diffusion%20models%20with%20reward%20functions.%20FK%20steering%20works%20by%0Asampling%20a%20system%20of%20multiple%20interacting%20diffusion%20processes%2C%20called%0Aparticles%2C%20and%20resampling%20particles%20at%20intermediate%20steps%20based%20on%20scores%0Acomputed%20using%20functions%20called%20potentials.%20Potentials%20are%20defined%20using%0Arewards%20for%20intermediate%20states%20and%20are%20selected%20such%20that%20a%20high%20value%0Aindicates%20that%20the%20particle%20will%20yield%20a%20high-reward%20sample.%20We%20explore%20various%0Achoices%20of%20potentials%2C%20intermediate%20rewards%2C%20and%20samplers.%20We%20evaluate%20FK%0Asteering%20on%20text-to-image%20and%20text%20diffusion%20models.%20For%20steering%20text-to-image%0Amodels%20with%20a%20human%20preference%20reward%2C%20we%20find%20that%20FK%20steering%20a%200.8B%0Aparameter%20model%20outperforms%20a%202.6B%20parameter%20fine-tuned%20model%20on%20prompt%0Afidelity%2C%20with%20faster%20sampling%20and%20no%20training.%20For%20steering%20text%20diffusion%0Amodels%20with%20rewards%20for%20text%20quality%20and%20specific%20text%20attributes%2C%20we%20find%20that%0AFK%20steering%20generates%20lower%20perplexity%2C%20more%20linguistically%20acceptable%20outputs%0Aand%20enables%20gradient-free%20control%20of%20attributes%20like%20toxicity.%20Our%20results%0Ademonstrate%20that%20inference-time%20scaling%20and%20steering%20of%20diffusion%20models%2C%20even%0Awith%20off-the-shelf%20rewards%2C%20can%20provide%20significant%20sample%20quality%20gains%20and%0Acontrollability%20benefits.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zacharyhorvitz/Fk-Diffusion-Steering%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06848v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520General%2520Framework%2520for%2520Inference-time%2520Scaling%2520and%2520Steering%2520of%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DRaghav%2520Singhal%2520and%2520Zachary%2520Horvitz%2520and%2520Ryan%2520Teehan%2520and%2520Mengye%2520Ren%2520and%2520Zhou%2520Yu%2520and%2520Kathleen%2520McKeown%2520and%2520Rajesh%2520Ranganath%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520produce%2520impressive%2520results%2520in%2520modalities%2520ranging%2520from%2520images%250Aand%2520video%2520to%2520protein%2520design%2520and%2520text.%2520However%252C%2520generating%2520samples%2520with%250Auser-specified%2520properties%2520remains%2520a%2520challenge.%2520Recent%2520research%2520proposes%250Afine-tuning%2520models%2520to%2520maximize%2520rewards%2520that%2520capture%2520desired%2520properties%252C%2520but%250Athese%2520methods%2520require%2520expensive%2520training%2520and%2520are%2520prone%2520to%2520mode%2520collapse.%2520In%250Athis%2520work%252C%2520we%2520propose%2520Feynman%2520Kac%2520%2528FK%2529%2520steering%252C%2520an%2520inference-time%2520framework%250Afor%2520steering%2520diffusion%2520models%2520with%2520reward%2520functions.%2520FK%2520steering%2520works%2520by%250Asampling%2520a%2520system%2520of%2520multiple%2520interacting%2520diffusion%2520processes%252C%2520called%250Aparticles%252C%2520and%2520resampling%2520particles%2520at%2520intermediate%2520steps%2520based%2520on%2520scores%250Acomputed%2520using%2520functions%2520called%2520potentials.%2520Potentials%2520are%2520defined%2520using%250Arewards%2520for%2520intermediate%2520states%2520and%2520are%2520selected%2520such%2520that%2520a%2520high%2520value%250Aindicates%2520that%2520the%2520particle%2520will%2520yield%2520a%2520high-reward%2520sample.%2520We%2520explore%2520various%250Achoices%2520of%2520potentials%252C%2520intermediate%2520rewards%252C%2520and%2520samplers.%2520We%2520evaluate%2520FK%250Asteering%2520on%2520text-to-image%2520and%2520text%2520diffusion%2520models.%2520For%2520steering%2520text-to-image%250Amodels%2520with%2520a%2520human%2520preference%2520reward%252C%2520we%2520find%2520that%2520FK%2520steering%2520a%25200.8B%250Aparameter%2520model%2520outperforms%2520a%25202.6B%2520parameter%2520fine-tuned%2520model%2520on%2520prompt%250Afidelity%252C%2520with%2520faster%2520sampling%2520and%2520no%2520training.%2520For%2520steering%2520text%2520diffusion%250Amodels%2520with%2520rewards%2520for%2520text%2520quality%2520and%2520specific%2520text%2520attributes%252C%2520we%2520find%2520that%250AFK%2520steering%2520generates%2520lower%2520perplexity%252C%2520more%2520linguistically%2520acceptable%2520outputs%250Aand%2520enables%2520gradient-free%2520control%2520of%2520attributes%2520like%2520toxicity.%2520Our%2520results%250Ademonstrate%2520that%2520inference-time%2520scaling%2520and%2520steering%2520of%2520diffusion%2520models%252C%2520even%250Awith%2520off-the-shelf%2520rewards%252C%2520can%2520provide%2520significant%2520sample%2520quality%2520gains%2520and%250Acontrollability%2520benefits.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/zacharyhorvitz/Fk-Diffusion-Steering%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06848v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Framework%20for%20Inference-time%20Scaling%20and%20Steering%20of%20Diffusion%0A%20%20Models&entry.906535625=Raghav%20Singhal%20and%20Zachary%20Horvitz%20and%20Ryan%20Teehan%20and%20Mengye%20Ren%20and%20Zhou%20Yu%20and%20Kathleen%20McKeown%20and%20Rajesh%20Ranganath&entry.1292438233=%20%20Diffusion%20models%20produce%20impressive%20results%20in%20modalities%20ranging%20from%20images%0Aand%20video%20to%20protein%20design%20and%20text.%20However%2C%20generating%20samples%20with%0Auser-specified%20properties%20remains%20a%20challenge.%20Recent%20research%20proposes%0Afine-tuning%20models%20to%20maximize%20rewards%20that%20capture%20desired%20properties%2C%20but%0Athese%20methods%20require%20expensive%20training%20and%20are%20prone%20to%20mode%20collapse.%20In%0Athis%20work%2C%20we%20propose%20Feynman%20Kac%20%28FK%29%20steering%2C%20an%20inference-time%20framework%0Afor%20steering%20diffusion%20models%20with%20reward%20functions.%20FK%20steering%20works%20by%0Asampling%20a%20system%20of%20multiple%20interacting%20diffusion%20processes%2C%20called%0Aparticles%2C%20and%20resampling%20particles%20at%20intermediate%20steps%20based%20on%20scores%0Acomputed%20using%20functions%20called%20potentials.%20Potentials%20are%20defined%20using%0Arewards%20for%20intermediate%20states%20and%20are%20selected%20such%20that%20a%20high%20value%0Aindicates%20that%20the%20particle%20will%20yield%20a%20high-reward%20sample.%20We%20explore%20various%0Achoices%20of%20potentials%2C%20intermediate%20rewards%2C%20and%20samplers.%20We%20evaluate%20FK%0Asteering%20on%20text-to-image%20and%20text%20diffusion%20models.%20For%20steering%20text-to-image%0Amodels%20with%20a%20human%20preference%20reward%2C%20we%20find%20that%20FK%20steering%20a%200.8B%0Aparameter%20model%20outperforms%20a%202.6B%20parameter%20fine-tuned%20model%20on%20prompt%0Afidelity%2C%20with%20faster%20sampling%20and%20no%20training.%20For%20steering%20text%20diffusion%0Amodels%20with%20rewards%20for%20text%20quality%20and%20specific%20text%20attributes%2C%20we%20find%20that%0AFK%20steering%20generates%20lower%20perplexity%2C%20more%20linguistically%20acceptable%20outputs%0Aand%20enables%20gradient-free%20control%20of%20attributes%20like%20toxicity.%20Our%20results%0Ademonstrate%20that%20inference-time%20scaling%20and%20steering%20of%20diffusion%20models%2C%20even%0Awith%20off-the-shelf%20rewards%2C%20can%20provide%20significant%20sample%20quality%20gains%20and%0Acontrollability%20benefits.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zacharyhorvitz/Fk-Diffusion-Steering%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06848v2&entry.124074799=Read"},
{"title": "Visual WetlandBirds Dataset: Bird Species Identification and Behavior\n  Recognition in Videos", "author": "Javier Rodriguez-Juan and David Ortiz-Perez and Manuel Benavent-Lledo and David Mulero-P\u00e9rez and Pablo Ruiz-Ponce and Adrian Orihuela-Torres and Jose Garcia-Rodriguez and Esther Sebasti\u00e1n-Gonz\u00e1lez", "abstract": "  The current biodiversity loss crisis makes animal monitoring a relevant field\nof study. In light of this, data collected through monitoring can provide\nessential insights, and information for decision-making aimed at preserving\nglobal biodiversity. Despite the importance of such data, there is a notable\nscarcity of datasets featuring videos of birds, and none of the existing\ndatasets offer detailed annotations of bird behaviors in video format. In\nresponse to this gap, our study introduces the first fine-grained video dataset\nspecifically designed for bird behavior detection and species classification.\nThis dataset addresses the need for comprehensive bird video datasets and\nprovides detailed data on bird actions, facilitating the development of deep\nlearning models to recognize these, similar to the advancements made in human\naction recognition. The proposed dataset comprises 178 videos recorded in\nSpanish wetlands, capturing 13 different bird species performing 7 distinct\nbehavior classes. In addition, we also present baseline results using state of\nthe art models on two tasks: bird behavior recognition and species\nclassification.\n", "link": "http://arxiv.org/abs/2501.08931v1", "date": "2025-01-15", "relevancy": 1.7447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4371}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20WetlandBirds%20Dataset%3A%20Bird%20Species%20Identification%20and%20Behavior%0A%20%20Recognition%20in%20Videos&body=Title%3A%20Visual%20WetlandBirds%20Dataset%3A%20Bird%20Species%20Identification%20and%20Behavior%0A%20%20Recognition%20in%20Videos%0AAuthor%3A%20Javier%20Rodriguez-Juan%20and%20David%20Ortiz-Perez%20and%20Manuel%20Benavent-Lledo%20and%20David%20Mulero-P%C3%A9rez%20and%20Pablo%20Ruiz-Ponce%20and%20Adrian%20Orihuela-Torres%20and%20Jose%20Garcia-Rodriguez%20and%20Esther%20Sebasti%C3%A1n-Gonz%C3%A1lez%0AAbstract%3A%20%20%20The%20current%20biodiversity%20loss%20crisis%20makes%20animal%20monitoring%20a%20relevant%20field%0Aof%20study.%20In%20light%20of%20this%2C%20data%20collected%20through%20monitoring%20can%20provide%0Aessential%20insights%2C%20and%20information%20for%20decision-making%20aimed%20at%20preserving%0Aglobal%20biodiversity.%20Despite%20the%20importance%20of%20such%20data%2C%20there%20is%20a%20notable%0Ascarcity%20of%20datasets%20featuring%20videos%20of%20birds%2C%20and%20none%20of%20the%20existing%0Adatasets%20offer%20detailed%20annotations%20of%20bird%20behaviors%20in%20video%20format.%20In%0Aresponse%20to%20this%20gap%2C%20our%20study%20introduces%20the%20first%20fine-grained%20video%20dataset%0Aspecifically%20designed%20for%20bird%20behavior%20detection%20and%20species%20classification.%0AThis%20dataset%20addresses%20the%20need%20for%20comprehensive%20bird%20video%20datasets%20and%0Aprovides%20detailed%20data%20on%20bird%20actions%2C%20facilitating%20the%20development%20of%20deep%0Alearning%20models%20to%20recognize%20these%2C%20similar%20to%20the%20advancements%20made%20in%20human%0Aaction%20recognition.%20The%20proposed%20dataset%20comprises%20178%20videos%20recorded%20in%0ASpanish%20wetlands%2C%20capturing%2013%20different%20bird%20species%20performing%207%20distinct%0Abehavior%20classes.%20In%20addition%2C%20we%20also%20present%20baseline%20results%20using%20state%20of%0Athe%20art%20models%20on%20two%20tasks%3A%20bird%20behavior%20recognition%20and%20species%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520WetlandBirds%2520Dataset%253A%2520Bird%2520Species%2520Identification%2520and%2520Behavior%250A%2520%2520Recognition%2520in%2520Videos%26entry.906535625%3DJavier%2520Rodriguez-Juan%2520and%2520David%2520Ortiz-Perez%2520and%2520Manuel%2520Benavent-Lledo%2520and%2520David%2520Mulero-P%25C3%25A9rez%2520and%2520Pablo%2520Ruiz-Ponce%2520and%2520Adrian%2520Orihuela-Torres%2520and%2520Jose%2520Garcia-Rodriguez%2520and%2520Esther%2520Sebasti%25C3%25A1n-Gonz%25C3%25A1lez%26entry.1292438233%3D%2520%2520The%2520current%2520biodiversity%2520loss%2520crisis%2520makes%2520animal%2520monitoring%2520a%2520relevant%2520field%250Aof%2520study.%2520In%2520light%2520of%2520this%252C%2520data%2520collected%2520through%2520monitoring%2520can%2520provide%250Aessential%2520insights%252C%2520and%2520information%2520for%2520decision-making%2520aimed%2520at%2520preserving%250Aglobal%2520biodiversity.%2520Despite%2520the%2520importance%2520of%2520such%2520data%252C%2520there%2520is%2520a%2520notable%250Ascarcity%2520of%2520datasets%2520featuring%2520videos%2520of%2520birds%252C%2520and%2520none%2520of%2520the%2520existing%250Adatasets%2520offer%2520detailed%2520annotations%2520of%2520bird%2520behaviors%2520in%2520video%2520format.%2520In%250Aresponse%2520to%2520this%2520gap%252C%2520our%2520study%2520introduces%2520the%2520first%2520fine-grained%2520video%2520dataset%250Aspecifically%2520designed%2520for%2520bird%2520behavior%2520detection%2520and%2520species%2520classification.%250AThis%2520dataset%2520addresses%2520the%2520need%2520for%2520comprehensive%2520bird%2520video%2520datasets%2520and%250Aprovides%2520detailed%2520data%2520on%2520bird%2520actions%252C%2520facilitating%2520the%2520development%2520of%2520deep%250Alearning%2520models%2520to%2520recognize%2520these%252C%2520similar%2520to%2520the%2520advancements%2520made%2520in%2520human%250Aaction%2520recognition.%2520The%2520proposed%2520dataset%2520comprises%2520178%2520videos%2520recorded%2520in%250ASpanish%2520wetlands%252C%2520capturing%252013%2520different%2520bird%2520species%2520performing%25207%2520distinct%250Abehavior%2520classes.%2520In%2520addition%252C%2520we%2520also%2520present%2520baseline%2520results%2520using%2520state%2520of%250Athe%2520art%2520models%2520on%2520two%2520tasks%253A%2520bird%2520behavior%2520recognition%2520and%2520species%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20WetlandBirds%20Dataset%3A%20Bird%20Species%20Identification%20and%20Behavior%0A%20%20Recognition%20in%20Videos&entry.906535625=Javier%20Rodriguez-Juan%20and%20David%20Ortiz-Perez%20and%20Manuel%20Benavent-Lledo%20and%20David%20Mulero-P%C3%A9rez%20and%20Pablo%20Ruiz-Ponce%20and%20Adrian%20Orihuela-Torres%20and%20Jose%20Garcia-Rodriguez%20and%20Esther%20Sebasti%C3%A1n-Gonz%C3%A1lez&entry.1292438233=%20%20The%20current%20biodiversity%20loss%20crisis%20makes%20animal%20monitoring%20a%20relevant%20field%0Aof%20study.%20In%20light%20of%20this%2C%20data%20collected%20through%20monitoring%20can%20provide%0Aessential%20insights%2C%20and%20information%20for%20decision-making%20aimed%20at%20preserving%0Aglobal%20biodiversity.%20Despite%20the%20importance%20of%20such%20data%2C%20there%20is%20a%20notable%0Ascarcity%20of%20datasets%20featuring%20videos%20of%20birds%2C%20and%20none%20of%20the%20existing%0Adatasets%20offer%20detailed%20annotations%20of%20bird%20behaviors%20in%20video%20format.%20In%0Aresponse%20to%20this%20gap%2C%20our%20study%20introduces%20the%20first%20fine-grained%20video%20dataset%0Aspecifically%20designed%20for%20bird%20behavior%20detection%20and%20species%20classification.%0AThis%20dataset%20addresses%20the%20need%20for%20comprehensive%20bird%20video%20datasets%20and%0Aprovides%20detailed%20data%20on%20bird%20actions%2C%20facilitating%20the%20development%20of%20deep%0Alearning%20models%20to%20recognize%20these%2C%20similar%20to%20the%20advancements%20made%20in%20human%0Aaction%20recognition.%20The%20proposed%20dataset%20comprises%20178%20videos%20recorded%20in%0ASpanish%20wetlands%2C%20capturing%2013%20different%20bird%20species%20performing%207%20distinct%0Abehavior%20classes.%20In%20addition%2C%20we%20also%20present%20baseline%20results%20using%20state%20of%0Athe%20art%20models%20on%20two%20tasks%3A%20bird%20behavior%20recognition%20and%20species%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08931v1&entry.124074799=Read"},
{"title": "Improved Compression Bounds for Scenario Decision Making", "author": "Guillaume O. Berger and Rapha\u00ebl M. Jungers", "abstract": "  Scenario decision making offers a flexible way of making decision in an\nuncertain environment while obtaining probabilistic guarantees on the risk of\nfailure of the decision. The idea of this approach is to draw samples of the\nuncertainty and make a decision based on the samples, called \"scenarios\". The\nprobabilistic guarantees take the form of a bound on the probability of\nsampling a set of scenarios that will lead to a decision whose risk of failure\nis above a given maximum tolerance. This bound can be expressed as a function\nof the number of sampled scenarios, the maximum tolerated risk, and some\nintrinsic property of the problem called the \"compression size\". Several such\nbounds have been proposed in the literature under various assumptions on the\nproblem. We propose new bounds that improve upon the existing ones without\nrequiring stronger assumptions on the problem.\n", "link": "http://arxiv.org/abs/2501.08884v1", "date": "2025-01-15", "relevancy": 1.7347, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Compression%20Bounds%20for%20Scenario%20Decision%20Making&body=Title%3A%20Improved%20Compression%20Bounds%20for%20Scenario%20Decision%20Making%0AAuthor%3A%20Guillaume%20O.%20Berger%20and%20Rapha%C3%ABl%20M.%20Jungers%0AAbstract%3A%20%20%20Scenario%20decision%20making%20offers%20a%20flexible%20way%20of%20making%20decision%20in%20an%0Auncertain%20environment%20while%20obtaining%20probabilistic%20guarantees%20on%20the%20risk%20of%0Afailure%20of%20the%20decision.%20The%20idea%20of%20this%20approach%20is%20to%20draw%20samples%20of%20the%0Auncertainty%20and%20make%20a%20decision%20based%20on%20the%20samples%2C%20called%20%22scenarios%22.%20The%0Aprobabilistic%20guarantees%20take%20the%20form%20of%20a%20bound%20on%20the%20probability%20of%0Asampling%20a%20set%20of%20scenarios%20that%20will%20lead%20to%20a%20decision%20whose%20risk%20of%20failure%0Ais%20above%20a%20given%20maximum%20tolerance.%20This%20bound%20can%20be%20expressed%20as%20a%20function%0Aof%20the%20number%20of%20sampled%20scenarios%2C%20the%20maximum%20tolerated%20risk%2C%20and%20some%0Aintrinsic%20property%20of%20the%20problem%20called%20the%20%22compression%20size%22.%20Several%20such%0Abounds%20have%20been%20proposed%20in%20the%20literature%20under%20various%20assumptions%20on%20the%0Aproblem.%20We%20propose%20new%20bounds%20that%20improve%20upon%20the%20existing%20ones%20without%0Arequiring%20stronger%20assumptions%20on%20the%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Compression%2520Bounds%2520for%2520Scenario%2520Decision%2520Making%26entry.906535625%3DGuillaume%2520O.%2520Berger%2520and%2520Rapha%25C3%25ABl%2520M.%2520Jungers%26entry.1292438233%3D%2520%2520Scenario%2520decision%2520making%2520offers%2520a%2520flexible%2520way%2520of%2520making%2520decision%2520in%2520an%250Auncertain%2520environment%2520while%2520obtaining%2520probabilistic%2520guarantees%2520on%2520the%2520risk%2520of%250Afailure%2520of%2520the%2520decision.%2520The%2520idea%2520of%2520this%2520approach%2520is%2520to%2520draw%2520samples%2520of%2520the%250Auncertainty%2520and%2520make%2520a%2520decision%2520based%2520on%2520the%2520samples%252C%2520called%2520%2522scenarios%2522.%2520The%250Aprobabilistic%2520guarantees%2520take%2520the%2520form%2520of%2520a%2520bound%2520on%2520the%2520probability%2520of%250Asampling%2520a%2520set%2520of%2520scenarios%2520that%2520will%2520lead%2520to%2520a%2520decision%2520whose%2520risk%2520of%2520failure%250Ais%2520above%2520a%2520given%2520maximum%2520tolerance.%2520This%2520bound%2520can%2520be%2520expressed%2520as%2520a%2520function%250Aof%2520the%2520number%2520of%2520sampled%2520scenarios%252C%2520the%2520maximum%2520tolerated%2520risk%252C%2520and%2520some%250Aintrinsic%2520property%2520of%2520the%2520problem%2520called%2520the%2520%2522compression%2520size%2522.%2520Several%2520such%250Abounds%2520have%2520been%2520proposed%2520in%2520the%2520literature%2520under%2520various%2520assumptions%2520on%2520the%250Aproblem.%2520We%2520propose%2520new%2520bounds%2520that%2520improve%2520upon%2520the%2520existing%2520ones%2520without%250Arequiring%2520stronger%2520assumptions%2520on%2520the%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Compression%20Bounds%20for%20Scenario%20Decision%20Making&entry.906535625=Guillaume%20O.%20Berger%20and%20Rapha%C3%ABl%20M.%20Jungers&entry.1292438233=%20%20Scenario%20decision%20making%20offers%20a%20flexible%20way%20of%20making%20decision%20in%20an%0Auncertain%20environment%20while%20obtaining%20probabilistic%20guarantees%20on%20the%20risk%20of%0Afailure%20of%20the%20decision.%20The%20idea%20of%20this%20approach%20is%20to%20draw%20samples%20of%20the%0Auncertainty%20and%20make%20a%20decision%20based%20on%20the%20samples%2C%20called%20%22scenarios%22.%20The%0Aprobabilistic%20guarantees%20take%20the%20form%20of%20a%20bound%20on%20the%20probability%20of%0Asampling%20a%20set%20of%20scenarios%20that%20will%20lead%20to%20a%20decision%20whose%20risk%20of%20failure%0Ais%20above%20a%20given%20maximum%20tolerance.%20This%20bound%20can%20be%20expressed%20as%20a%20function%0Aof%20the%20number%20of%20sampled%20scenarios%2C%20the%20maximum%20tolerated%20risk%2C%20and%20some%0Aintrinsic%20property%20of%20the%20problem%20called%20the%20%22compression%20size%22.%20Several%20such%0Abounds%20have%20been%20proposed%20in%20the%20literature%20under%20various%20assumptions%20on%20the%0Aproblem.%20We%20propose%20new%20bounds%20that%20improve%20upon%20the%20existing%20ones%20without%0Arequiring%20stronger%20assumptions%20on%20the%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08884v1&entry.124074799=Read"},
{"title": "Computing Approximated Fixpoints via Dampened Mann Iteration", "author": "Paolo Baldan and Sebastian Gurke and Barbara K\u00f6nig and Tommaso Padoan and Florian Wittbold", "abstract": "  Fixpoints are ubiquitous in computer science and when dealing with\nquantitative semantics and verification one is commonly led to consider least\nfixpoints of (higher-dimensional) functions over the nonnegative reals. We show\nhow to approximate the least fixpoint of such functions, focusing on the case\nin which they are not known precisely, but represented by a sequence of\napproximating functions that converge to them. We concentrate on monotone and\nnon-expansive functions, for which uniqueness of fixpoints is not guaranteed\nand standard fixpoint iteration schemes might get stuck at a fixpoint that is\nnot the least. Our main contribution is the identification of an iteration\nscheme, a variation of Mann iteration with a dampening factor, which, under\nsuitable conditions, is shown to guarantee convergence to the least fixpoint of\nthe function of interest. We then argue that these results are relevant in the\ncontext of model-based reinforcement learning for Markov decision processes\n(MDPs), showing that the proposed iteration scheme instantiates to MDPs and\nallows us to derive convergence to the optimal expected return. More generally,\nwe show that our results can be used to iterate to the least fixpoint almost\nsurely for systems where the function of interest can be approximated with\ngiven probabilistic error bounds, as it happens for probabilistic systems, such\nas simple stochastic games, that can be explored via sampling.\n", "link": "http://arxiv.org/abs/2501.08950v1", "date": "2025-01-15", "relevancy": 1.7208, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4462}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4227}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computing%20Approximated%20Fixpoints%20via%20Dampened%20Mann%20Iteration&body=Title%3A%20Computing%20Approximated%20Fixpoints%20via%20Dampened%20Mann%20Iteration%0AAuthor%3A%20Paolo%20Baldan%20and%20Sebastian%20Gurke%20and%20Barbara%20K%C3%B6nig%20and%20Tommaso%20Padoan%20and%20Florian%20Wittbold%0AAbstract%3A%20%20%20Fixpoints%20are%20ubiquitous%20in%20computer%20science%20and%20when%20dealing%20with%0Aquantitative%20semantics%20and%20verification%20one%20is%20commonly%20led%20to%20consider%20least%0Afixpoints%20of%20%28higher-dimensional%29%20functions%20over%20the%20nonnegative%20reals.%20We%20show%0Ahow%20to%20approximate%20the%20least%20fixpoint%20of%20such%20functions%2C%20focusing%20on%20the%20case%0Ain%20which%20they%20are%20not%20known%20precisely%2C%20but%20represented%20by%20a%20sequence%20of%0Aapproximating%20functions%20that%20converge%20to%20them.%20We%20concentrate%20on%20monotone%20and%0Anon-expansive%20functions%2C%20for%20which%20uniqueness%20of%20fixpoints%20is%20not%20guaranteed%0Aand%20standard%20fixpoint%20iteration%20schemes%20might%20get%20stuck%20at%20a%20fixpoint%20that%20is%0Anot%20the%20least.%20Our%20main%20contribution%20is%20the%20identification%20of%20an%20iteration%0Ascheme%2C%20a%20variation%20of%20Mann%20iteration%20with%20a%20dampening%20factor%2C%20which%2C%20under%0Asuitable%20conditions%2C%20is%20shown%20to%20guarantee%20convergence%20to%20the%20least%20fixpoint%20of%0Athe%20function%20of%20interest.%20We%20then%20argue%20that%20these%20results%20are%20relevant%20in%20the%0Acontext%20of%20model-based%20reinforcement%20learning%20for%20Markov%20decision%20processes%0A%28MDPs%29%2C%20showing%20that%20the%20proposed%20iteration%20scheme%20instantiates%20to%20MDPs%20and%0Aallows%20us%20to%20derive%20convergence%20to%20the%20optimal%20expected%20return.%20More%20generally%2C%0Awe%20show%20that%20our%20results%20can%20be%20used%20to%20iterate%20to%20the%20least%20fixpoint%20almost%0Asurely%20for%20systems%20where%20the%20function%20of%20interest%20can%20be%20approximated%20with%0Agiven%20probabilistic%20error%20bounds%2C%20as%20it%20happens%20for%20probabilistic%20systems%2C%20such%0Aas%20simple%20stochastic%20games%2C%20that%20can%20be%20explored%20via%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputing%2520Approximated%2520Fixpoints%2520via%2520Dampened%2520Mann%2520Iteration%26entry.906535625%3DPaolo%2520Baldan%2520and%2520Sebastian%2520Gurke%2520and%2520Barbara%2520K%25C3%25B6nig%2520and%2520Tommaso%2520Padoan%2520and%2520Florian%2520Wittbold%26entry.1292438233%3D%2520%2520Fixpoints%2520are%2520ubiquitous%2520in%2520computer%2520science%2520and%2520when%2520dealing%2520with%250Aquantitative%2520semantics%2520and%2520verification%2520one%2520is%2520commonly%2520led%2520to%2520consider%2520least%250Afixpoints%2520of%2520%2528higher-dimensional%2529%2520functions%2520over%2520the%2520nonnegative%2520reals.%2520We%2520show%250Ahow%2520to%2520approximate%2520the%2520least%2520fixpoint%2520of%2520such%2520functions%252C%2520focusing%2520on%2520the%2520case%250Ain%2520which%2520they%2520are%2520not%2520known%2520precisely%252C%2520but%2520represented%2520by%2520a%2520sequence%2520of%250Aapproximating%2520functions%2520that%2520converge%2520to%2520them.%2520We%2520concentrate%2520on%2520monotone%2520and%250Anon-expansive%2520functions%252C%2520for%2520which%2520uniqueness%2520of%2520fixpoints%2520is%2520not%2520guaranteed%250Aand%2520standard%2520fixpoint%2520iteration%2520schemes%2520might%2520get%2520stuck%2520at%2520a%2520fixpoint%2520that%2520is%250Anot%2520the%2520least.%2520Our%2520main%2520contribution%2520is%2520the%2520identification%2520of%2520an%2520iteration%250Ascheme%252C%2520a%2520variation%2520of%2520Mann%2520iteration%2520with%2520a%2520dampening%2520factor%252C%2520which%252C%2520under%250Asuitable%2520conditions%252C%2520is%2520shown%2520to%2520guarantee%2520convergence%2520to%2520the%2520least%2520fixpoint%2520of%250Athe%2520function%2520of%2520interest.%2520We%2520then%2520argue%2520that%2520these%2520results%2520are%2520relevant%2520in%2520the%250Acontext%2520of%2520model-based%2520reinforcement%2520learning%2520for%2520Markov%2520decision%2520processes%250A%2528MDPs%2529%252C%2520showing%2520that%2520the%2520proposed%2520iteration%2520scheme%2520instantiates%2520to%2520MDPs%2520and%250Aallows%2520us%2520to%2520derive%2520convergence%2520to%2520the%2520optimal%2520expected%2520return.%2520More%2520generally%252C%250Awe%2520show%2520that%2520our%2520results%2520can%2520be%2520used%2520to%2520iterate%2520to%2520the%2520least%2520fixpoint%2520almost%250Asurely%2520for%2520systems%2520where%2520the%2520function%2520of%2520interest%2520can%2520be%2520approximated%2520with%250Agiven%2520probabilistic%2520error%2520bounds%252C%2520as%2520it%2520happens%2520for%2520probabilistic%2520systems%252C%2520such%250Aas%2520simple%2520stochastic%2520games%252C%2520that%2520can%2520be%2520explored%2520via%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computing%20Approximated%20Fixpoints%20via%20Dampened%20Mann%20Iteration&entry.906535625=Paolo%20Baldan%20and%20Sebastian%20Gurke%20and%20Barbara%20K%C3%B6nig%20and%20Tommaso%20Padoan%20and%20Florian%20Wittbold&entry.1292438233=%20%20Fixpoints%20are%20ubiquitous%20in%20computer%20science%20and%20when%20dealing%20with%0Aquantitative%20semantics%20and%20verification%20one%20is%20commonly%20led%20to%20consider%20least%0Afixpoints%20of%20%28higher-dimensional%29%20functions%20over%20the%20nonnegative%20reals.%20We%20show%0Ahow%20to%20approximate%20the%20least%20fixpoint%20of%20such%20functions%2C%20focusing%20on%20the%20case%0Ain%20which%20they%20are%20not%20known%20precisely%2C%20but%20represented%20by%20a%20sequence%20of%0Aapproximating%20functions%20that%20converge%20to%20them.%20We%20concentrate%20on%20monotone%20and%0Anon-expansive%20functions%2C%20for%20which%20uniqueness%20of%20fixpoints%20is%20not%20guaranteed%0Aand%20standard%20fixpoint%20iteration%20schemes%20might%20get%20stuck%20at%20a%20fixpoint%20that%20is%0Anot%20the%20least.%20Our%20main%20contribution%20is%20the%20identification%20of%20an%20iteration%0Ascheme%2C%20a%20variation%20of%20Mann%20iteration%20with%20a%20dampening%20factor%2C%20which%2C%20under%0Asuitable%20conditions%2C%20is%20shown%20to%20guarantee%20convergence%20to%20the%20least%20fixpoint%20of%0Athe%20function%20of%20interest.%20We%20then%20argue%20that%20these%20results%20are%20relevant%20in%20the%0Acontext%20of%20model-based%20reinforcement%20learning%20for%20Markov%20decision%20processes%0A%28MDPs%29%2C%20showing%20that%20the%20proposed%20iteration%20scheme%20instantiates%20to%20MDPs%20and%0Aallows%20us%20to%20derive%20convergence%20to%20the%20optimal%20expected%20return.%20More%20generally%2C%0Awe%20show%20that%20our%20results%20can%20be%20used%20to%20iterate%20to%20the%20least%20fixpoint%20almost%0Asurely%20for%20systems%20where%20the%20function%20of%20interest%20can%20be%20approximated%20with%0Agiven%20probabilistic%20error%20bounds%2C%20as%20it%20happens%20for%20probabilistic%20systems%2C%20such%0Aas%20simple%20stochastic%20games%2C%20that%20can%20be%20explored%20via%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08950v1&entry.124074799=Read"},
{"title": "Transformed Low-rank Adaptation via Tensor Decomposition and Its\n  Applications to Text-to-image Models", "author": "Zerui Tao and Yuhta Takida and Naoki Murata and Qibin Zhao and Yuki Mitsufuji", "abstract": "  Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an\nincreasingly popular technique with many applications. Among the various PEFT\nmethods, Low-Rank Adaptation (LoRA) and its variants have gained significant\nattention due to their effectiveness, enabling users to fine-tune models with\nlimited computational resources. However, the approximation gap between the\nlow-rank assumption and desired fine-tuning weights prevents the simultaneous\nacquisition of ultra-parameter-efficiency and better performance. To reduce\nthis gap and further improve the power of LoRA, we propose a new PEFT method\nthat combines two classes of adaptations, namely, transform and residual\nadaptations. In specific, we first apply a full-rank and dense transform to the\npre-trained weight. This learnable transform is expected to align the\npre-trained weight as closely as possible to the desired weight, thereby\nreducing the rank of the residual weight. Then, the residual part can be\neffectively approximated by more compact and parameter-efficient structures,\nwith a smaller approximation error. To achieve ultra-parameter-efficiency in\npractice, we design highly flexible and effective tensor decompositions for\nboth the transform and residual adaptations. Additionally, popular PEFT methods\nsuch as DoRA can be summarized under this transform plus residual adaptation\nscheme. Experiments are conducted on fine-tuning Stable Diffusion models in\nsubject-driven and controllable generation. The results manifest that our\nmethod can achieve better performances and parameter efficiency compared to\nLoRA and several baselines.\n", "link": "http://arxiv.org/abs/2501.08727v1", "date": "2025-01-15", "relevancy": 1.7175, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6094}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5705}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformed%20Low-rank%20Adaptation%20via%20Tensor%20Decomposition%20and%20Its%0A%20%20Applications%20to%20Text-to-image%20Models&body=Title%3A%20Transformed%20Low-rank%20Adaptation%20via%20Tensor%20Decomposition%20and%20Its%0A%20%20Applications%20to%20Text-to-image%20Models%0AAuthor%3A%20Zerui%20Tao%20and%20Yuhta%20Takida%20and%20Naoki%20Murata%20and%20Qibin%20Zhao%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20of%20text-to-image%20models%20has%20become%20an%0Aincreasingly%20popular%20technique%20with%20many%20applications.%20Among%20the%20various%20PEFT%0Amethods%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20and%20its%20variants%20have%20gained%20significant%0Aattention%20due%20to%20their%20effectiveness%2C%20enabling%20users%20to%20fine-tune%20models%20with%0Alimited%20computational%20resources.%20However%2C%20the%20approximation%20gap%20between%20the%0Alow-rank%20assumption%20and%20desired%20fine-tuning%20weights%20prevents%20the%20simultaneous%0Aacquisition%20of%20ultra-parameter-efficiency%20and%20better%20performance.%20To%20reduce%0Athis%20gap%20and%20further%20improve%20the%20power%20of%20LoRA%2C%20we%20propose%20a%20new%20PEFT%20method%0Athat%20combines%20two%20classes%20of%20adaptations%2C%20namely%2C%20transform%20and%20residual%0Aadaptations.%20In%20specific%2C%20we%20first%20apply%20a%20full-rank%20and%20dense%20transform%20to%20the%0Apre-trained%20weight.%20This%20learnable%20transform%20is%20expected%20to%20align%20the%0Apre-trained%20weight%20as%20closely%20as%20possible%20to%20the%20desired%20weight%2C%20thereby%0Areducing%20the%20rank%20of%20the%20residual%20weight.%20Then%2C%20the%20residual%20part%20can%20be%0Aeffectively%20approximated%20by%20more%20compact%20and%20parameter-efficient%20structures%2C%0Awith%20a%20smaller%20approximation%20error.%20To%20achieve%20ultra-parameter-efficiency%20in%0Apractice%2C%20we%20design%20highly%20flexible%20and%20effective%20tensor%20decompositions%20for%0Aboth%20the%20transform%20and%20residual%20adaptations.%20Additionally%2C%20popular%20PEFT%20methods%0Asuch%20as%20DoRA%20can%20be%20summarized%20under%20this%20transform%20plus%20residual%20adaptation%0Ascheme.%20Experiments%20are%20conducted%20on%20fine-tuning%20Stable%20Diffusion%20models%20in%0Asubject-driven%20and%20controllable%20generation.%20The%20results%20manifest%20that%20our%0Amethod%20can%20achieve%20better%20performances%20and%20parameter%20efficiency%20compared%20to%0ALoRA%20and%20several%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformed%2520Low-rank%2520Adaptation%2520via%2520Tensor%2520Decomposition%2520and%2520Its%250A%2520%2520Applications%2520to%2520Text-to-image%2520Models%26entry.906535625%3DZerui%2520Tao%2520and%2520Yuhta%2520Takida%2520and%2520Naoki%2520Murata%2520and%2520Qibin%2520Zhao%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520of%2520text-to-image%2520models%2520has%2520become%2520an%250Aincreasingly%2520popular%2520technique%2520with%2520many%2520applications.%2520Among%2520the%2520various%2520PEFT%250Amethods%252C%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520and%2520its%2520variants%2520have%2520gained%2520significant%250Aattention%2520due%2520to%2520their%2520effectiveness%252C%2520enabling%2520users%2520to%2520fine-tune%2520models%2520with%250Alimited%2520computational%2520resources.%2520However%252C%2520the%2520approximation%2520gap%2520between%2520the%250Alow-rank%2520assumption%2520and%2520desired%2520fine-tuning%2520weights%2520prevents%2520the%2520simultaneous%250Aacquisition%2520of%2520ultra-parameter-efficiency%2520and%2520better%2520performance.%2520To%2520reduce%250Athis%2520gap%2520and%2520further%2520improve%2520the%2520power%2520of%2520LoRA%252C%2520we%2520propose%2520a%2520new%2520PEFT%2520method%250Athat%2520combines%2520two%2520classes%2520of%2520adaptations%252C%2520namely%252C%2520transform%2520and%2520residual%250Aadaptations.%2520In%2520specific%252C%2520we%2520first%2520apply%2520a%2520full-rank%2520and%2520dense%2520transform%2520to%2520the%250Apre-trained%2520weight.%2520This%2520learnable%2520transform%2520is%2520expected%2520to%2520align%2520the%250Apre-trained%2520weight%2520as%2520closely%2520as%2520possible%2520to%2520the%2520desired%2520weight%252C%2520thereby%250Areducing%2520the%2520rank%2520of%2520the%2520residual%2520weight.%2520Then%252C%2520the%2520residual%2520part%2520can%2520be%250Aeffectively%2520approximated%2520by%2520more%2520compact%2520and%2520parameter-efficient%2520structures%252C%250Awith%2520a%2520smaller%2520approximation%2520error.%2520To%2520achieve%2520ultra-parameter-efficiency%2520in%250Apractice%252C%2520we%2520design%2520highly%2520flexible%2520and%2520effective%2520tensor%2520decompositions%2520for%250Aboth%2520the%2520transform%2520and%2520residual%2520adaptations.%2520Additionally%252C%2520popular%2520PEFT%2520methods%250Asuch%2520as%2520DoRA%2520can%2520be%2520summarized%2520under%2520this%2520transform%2520plus%2520residual%2520adaptation%250Ascheme.%2520Experiments%2520are%2520conducted%2520on%2520fine-tuning%2520Stable%2520Diffusion%2520models%2520in%250Asubject-driven%2520and%2520controllable%2520generation.%2520The%2520results%2520manifest%2520that%2520our%250Amethod%2520can%2520achieve%2520better%2520performances%2520and%2520parameter%2520efficiency%2520compared%2520to%250ALoRA%2520and%2520several%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformed%20Low-rank%20Adaptation%20via%20Tensor%20Decomposition%20and%20Its%0A%20%20Applications%20to%20Text-to-image%20Models&entry.906535625=Zerui%20Tao%20and%20Yuhta%20Takida%20and%20Naoki%20Murata%20and%20Qibin%20Zhao%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20of%20text-to-image%20models%20has%20become%20an%0Aincreasingly%20popular%20technique%20with%20many%20applications.%20Among%20the%20various%20PEFT%0Amethods%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20and%20its%20variants%20have%20gained%20significant%0Aattention%20due%20to%20their%20effectiveness%2C%20enabling%20users%20to%20fine-tune%20models%20with%0Alimited%20computational%20resources.%20However%2C%20the%20approximation%20gap%20between%20the%0Alow-rank%20assumption%20and%20desired%20fine-tuning%20weights%20prevents%20the%20simultaneous%0Aacquisition%20of%20ultra-parameter-efficiency%20and%20better%20performance.%20To%20reduce%0Athis%20gap%20and%20further%20improve%20the%20power%20of%20LoRA%2C%20we%20propose%20a%20new%20PEFT%20method%0Athat%20combines%20two%20classes%20of%20adaptations%2C%20namely%2C%20transform%20and%20residual%0Aadaptations.%20In%20specific%2C%20we%20first%20apply%20a%20full-rank%20and%20dense%20transform%20to%20the%0Apre-trained%20weight.%20This%20learnable%20transform%20is%20expected%20to%20align%20the%0Apre-trained%20weight%20as%20closely%20as%20possible%20to%20the%20desired%20weight%2C%20thereby%0Areducing%20the%20rank%20of%20the%20residual%20weight.%20Then%2C%20the%20residual%20part%20can%20be%0Aeffectively%20approximated%20by%20more%20compact%20and%20parameter-efficient%20structures%2C%0Awith%20a%20smaller%20approximation%20error.%20To%20achieve%20ultra-parameter-efficiency%20in%0Apractice%2C%20we%20design%20highly%20flexible%20and%20effective%20tensor%20decompositions%20for%0Aboth%20the%20transform%20and%20residual%20adaptations.%20Additionally%2C%20popular%20PEFT%20methods%0Asuch%20as%20DoRA%20can%20be%20summarized%20under%20this%20transform%20plus%20residual%20adaptation%0Ascheme.%20Experiments%20are%20conducted%20on%20fine-tuning%20Stable%20Diffusion%20models%20in%0Asubject-driven%20and%20controllable%20generation.%20The%20results%20manifest%20that%20our%0Amethod%20can%20achieve%20better%20performances%20and%20parameter%20efficiency%20compared%20to%0ALoRA%20and%20several%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08727v1&entry.124074799=Read"},
{"title": "Learning Joint Denoising, Demosaicing, and Compression from the Raw\n  Natural Image Noise Dataset", "author": "Benoit Brummer and Christophe De Vleeschouwer", "abstract": "  This paper introduces the Raw Natural Image Noise Dataset (RawNIND), a\ndiverse collection of paired raw images designed to support the development of\ndenoising models that generalize across sensors, image development workflows,\nand styles. Two denoising methods are proposed: one operates directly on raw\nBayer data, leveraging computational efficiency, while the other processes\nlinear RGB images for improved generalization to different sensors, with both\npreserving flexibility for subsequent development. Both methods outperform\ntraditional approaches which rely on developed images. Additionally, the\nintegration of denoising and compression at the raw data level significantly\nenhances rate-distortion performance and computational efficiency. These\nfindings suggest a paradigm shift toward raw data workflows for efficient and\nflexible image processing.\n", "link": "http://arxiv.org/abs/2501.08924v1", "date": "2025-01-15", "relevancy": 1.7159, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5846}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5586}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Joint%20Denoising%2C%20Demosaicing%2C%20and%20Compression%20from%20the%20Raw%0A%20%20Natural%20Image%20Noise%20Dataset&body=Title%3A%20Learning%20Joint%20Denoising%2C%20Demosaicing%2C%20and%20Compression%20from%20the%20Raw%0A%20%20Natural%20Image%20Noise%20Dataset%0AAuthor%3A%20Benoit%20Brummer%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20Raw%20Natural%20Image%20Noise%20Dataset%20%28RawNIND%29%2C%20a%0Adiverse%20collection%20of%20paired%20raw%20images%20designed%20to%20support%20the%20development%20of%0Adenoising%20models%20that%20generalize%20across%20sensors%2C%20image%20development%20workflows%2C%0Aand%20styles.%20Two%20denoising%20methods%20are%20proposed%3A%20one%20operates%20directly%20on%20raw%0ABayer%20data%2C%20leveraging%20computational%20efficiency%2C%20while%20the%20other%20processes%0Alinear%20RGB%20images%20for%20improved%20generalization%20to%20different%20sensors%2C%20with%20both%0Apreserving%20flexibility%20for%20subsequent%20development.%20Both%20methods%20outperform%0Atraditional%20approaches%20which%20rely%20on%20developed%20images.%20Additionally%2C%20the%0Aintegration%20of%20denoising%20and%20compression%20at%20the%20raw%20data%20level%20significantly%0Aenhances%20rate-distortion%20performance%20and%20computational%20efficiency.%20These%0Afindings%20suggest%20a%20paradigm%20shift%20toward%20raw%20data%20workflows%20for%20efficient%20and%0Aflexible%20image%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Joint%2520Denoising%252C%2520Demosaicing%252C%2520and%2520Compression%2520from%2520the%2520Raw%250A%2520%2520Natural%2520Image%2520Noise%2520Dataset%26entry.906535625%3DBenoit%2520Brummer%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520Raw%2520Natural%2520Image%2520Noise%2520Dataset%2520%2528RawNIND%2529%252C%2520a%250Adiverse%2520collection%2520of%2520paired%2520raw%2520images%2520designed%2520to%2520support%2520the%2520development%2520of%250Adenoising%2520models%2520that%2520generalize%2520across%2520sensors%252C%2520image%2520development%2520workflows%252C%250Aand%2520styles.%2520Two%2520denoising%2520methods%2520are%2520proposed%253A%2520one%2520operates%2520directly%2520on%2520raw%250ABayer%2520data%252C%2520leveraging%2520computational%2520efficiency%252C%2520while%2520the%2520other%2520processes%250Alinear%2520RGB%2520images%2520for%2520improved%2520generalization%2520to%2520different%2520sensors%252C%2520with%2520both%250Apreserving%2520flexibility%2520for%2520subsequent%2520development.%2520Both%2520methods%2520outperform%250Atraditional%2520approaches%2520which%2520rely%2520on%2520developed%2520images.%2520Additionally%252C%2520the%250Aintegration%2520of%2520denoising%2520and%2520compression%2520at%2520the%2520raw%2520data%2520level%2520significantly%250Aenhances%2520rate-distortion%2520performance%2520and%2520computational%2520efficiency.%2520These%250Afindings%2520suggest%2520a%2520paradigm%2520shift%2520toward%2520raw%2520data%2520workflows%2520for%2520efficient%2520and%250Aflexible%2520image%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Joint%20Denoising%2C%20Demosaicing%2C%20and%20Compression%20from%20the%20Raw%0A%20%20Natural%20Image%20Noise%20Dataset&entry.906535625=Benoit%20Brummer%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20This%20paper%20introduces%20the%20Raw%20Natural%20Image%20Noise%20Dataset%20%28RawNIND%29%2C%20a%0Adiverse%20collection%20of%20paired%20raw%20images%20designed%20to%20support%20the%20development%20of%0Adenoising%20models%20that%20generalize%20across%20sensors%2C%20image%20development%20workflows%2C%0Aand%20styles.%20Two%20denoising%20methods%20are%20proposed%3A%20one%20operates%20directly%20on%20raw%0ABayer%20data%2C%20leveraging%20computational%20efficiency%2C%20while%20the%20other%20processes%0Alinear%20RGB%20images%20for%20improved%20generalization%20to%20different%20sensors%2C%20with%20both%0Apreserving%20flexibility%20for%20subsequent%20development.%20Both%20methods%20outperform%0Atraditional%20approaches%20which%20rely%20on%20developed%20images.%20Additionally%2C%20the%0Aintegration%20of%20denoising%20and%20compression%20at%20the%20raw%20data%20level%20significantly%0Aenhances%20rate-distortion%20performance%20and%20computational%20efficiency.%20These%0Afindings%20suggest%20a%20paradigm%20shift%20toward%20raw%20data%20workflows%20for%20efficient%20and%0Aflexible%20image%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08924v1&entry.124074799=Read"},
{"title": "Evaluation of Artificial Intelligence Methods for Lead Time Prediction\n  in Non-Cycled Areas of Automotive Production", "author": "Cornelius Hake and Jonas Weigele and Frederik Reichert and Christian Friedrich", "abstract": "  The present study examines the effectiveness of applying Artificial\nIntelligence methods in an automotive production environment to predict unknown\nlead times in a non-cycle-controlled production area. Data structures are\nanalyzed to identify contextual features and then preprocessed using one-hot\nencoding. Methods selection focuses on supervised machine learning techniques.\nIn supervised learning methods, regression and classification methods are\nevaluated. Continuous regression based on target size distribution is not\nfeasible. Classification methods analysis shows that Ensemble Learning and\nSupport Vector Machines are the most suitable. Preliminary study results\nindicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost\nyield the best results. After further testing and extensive hyperparameter\noptimization, the final method choice is the LightGBM algorithm. Depending on\nfeature availability and prediction interval granularity, relative prediction\naccuracies of up to 90% can be achieved. Further tests highlight the importance\nof periodic retraining of AI models to accurately represent complex production\nprocesses using the database. The research demonstrates that AI methods can be\neffectively applied to highly variable production data, adding business value\nby providing an additional metric for various control tasks while outperforming\ncurrent non AI-based systems.\n", "link": "http://arxiv.org/abs/2501.07317v3", "date": "2025-01-15", "relevancy": 1.7148, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.46}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4234}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Artificial%20Intelligence%20Methods%20for%20Lead%20Time%20Prediction%0A%20%20in%20Non-Cycled%20Areas%20of%20Automotive%20Production&body=Title%3A%20Evaluation%20of%20Artificial%20Intelligence%20Methods%20for%20Lead%20Time%20Prediction%0A%20%20in%20Non-Cycled%20Areas%20of%20Automotive%20Production%0AAuthor%3A%20Cornelius%20Hake%20and%20Jonas%20Weigele%20and%20Frederik%20Reichert%20and%20Christian%20Friedrich%0AAbstract%3A%20%20%20The%20present%20study%20examines%20the%20effectiveness%20of%20applying%20Artificial%0AIntelligence%20methods%20in%20an%20automotive%20production%20environment%20to%20predict%20unknown%0Alead%20times%20in%20a%20non-cycle-controlled%20production%20area.%20Data%20structures%20are%0Aanalyzed%20to%20identify%20contextual%20features%20and%20then%20preprocessed%20using%20one-hot%0Aencoding.%20Methods%20selection%20focuses%20on%20supervised%20machine%20learning%20techniques.%0AIn%20supervised%20learning%20methods%2C%20regression%20and%20classification%20methods%20are%0Aevaluated.%20Continuous%20regression%20based%20on%20target%20size%20distribution%20is%20not%0Afeasible.%20Classification%20methods%20analysis%20shows%20that%20Ensemble%20Learning%20and%0ASupport%20Vector%20Machines%20are%20the%20most%20suitable.%20Preliminary%20study%20results%0Aindicate%20that%20gradient%20boosting%20algorithms%20LightGBM%2C%20XGBoost%2C%20and%20CatBoost%0Ayield%20the%20best%20results.%20After%20further%20testing%20and%20extensive%20hyperparameter%0Aoptimization%2C%20the%20final%20method%20choice%20is%20the%20LightGBM%20algorithm.%20Depending%20on%0Afeature%20availability%20and%20prediction%20interval%20granularity%2C%20relative%20prediction%0Aaccuracies%20of%20up%20to%2090%25%20can%20be%20achieved.%20Further%20tests%20highlight%20the%20importance%0Aof%20periodic%20retraining%20of%20AI%20models%20to%20accurately%20represent%20complex%20production%0Aprocesses%20using%20the%20database.%20The%20research%20demonstrates%20that%20AI%20methods%20can%20be%0Aeffectively%20applied%20to%20highly%20variable%20production%20data%2C%20adding%20business%20value%0Aby%20providing%20an%20additional%20metric%20for%20various%20control%20tasks%20while%20outperforming%0Acurrent%20non%20AI-based%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07317v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Artificial%2520Intelligence%2520Methods%2520for%2520Lead%2520Time%2520Prediction%250A%2520%2520in%2520Non-Cycled%2520Areas%2520of%2520Automotive%2520Production%26entry.906535625%3DCornelius%2520Hake%2520and%2520Jonas%2520Weigele%2520and%2520Frederik%2520Reichert%2520and%2520Christian%2520Friedrich%26entry.1292438233%3D%2520%2520The%2520present%2520study%2520examines%2520the%2520effectiveness%2520of%2520applying%2520Artificial%250AIntelligence%2520methods%2520in%2520an%2520automotive%2520production%2520environment%2520to%2520predict%2520unknown%250Alead%2520times%2520in%2520a%2520non-cycle-controlled%2520production%2520area.%2520Data%2520structures%2520are%250Aanalyzed%2520to%2520identify%2520contextual%2520features%2520and%2520then%2520preprocessed%2520using%2520one-hot%250Aencoding.%2520Methods%2520selection%2520focuses%2520on%2520supervised%2520machine%2520learning%2520techniques.%250AIn%2520supervised%2520learning%2520methods%252C%2520regression%2520and%2520classification%2520methods%2520are%250Aevaluated.%2520Continuous%2520regression%2520based%2520on%2520target%2520size%2520distribution%2520is%2520not%250Afeasible.%2520Classification%2520methods%2520analysis%2520shows%2520that%2520Ensemble%2520Learning%2520and%250ASupport%2520Vector%2520Machines%2520are%2520the%2520most%2520suitable.%2520Preliminary%2520study%2520results%250Aindicate%2520that%2520gradient%2520boosting%2520algorithms%2520LightGBM%252C%2520XGBoost%252C%2520and%2520CatBoost%250Ayield%2520the%2520best%2520results.%2520After%2520further%2520testing%2520and%2520extensive%2520hyperparameter%250Aoptimization%252C%2520the%2520final%2520method%2520choice%2520is%2520the%2520LightGBM%2520algorithm.%2520Depending%2520on%250Afeature%2520availability%2520and%2520prediction%2520interval%2520granularity%252C%2520relative%2520prediction%250Aaccuracies%2520of%2520up%2520to%252090%2525%2520can%2520be%2520achieved.%2520Further%2520tests%2520highlight%2520the%2520importance%250Aof%2520periodic%2520retraining%2520of%2520AI%2520models%2520to%2520accurately%2520represent%2520complex%2520production%250Aprocesses%2520using%2520the%2520database.%2520The%2520research%2520demonstrates%2520that%2520AI%2520methods%2520can%2520be%250Aeffectively%2520applied%2520to%2520highly%2520variable%2520production%2520data%252C%2520adding%2520business%2520value%250Aby%2520providing%2520an%2520additional%2520metric%2520for%2520various%2520control%2520tasks%2520while%2520outperforming%250Acurrent%2520non%2520AI-based%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07317v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Artificial%20Intelligence%20Methods%20for%20Lead%20Time%20Prediction%0A%20%20in%20Non-Cycled%20Areas%20of%20Automotive%20Production&entry.906535625=Cornelius%20Hake%20and%20Jonas%20Weigele%20and%20Frederik%20Reichert%20and%20Christian%20Friedrich&entry.1292438233=%20%20The%20present%20study%20examines%20the%20effectiveness%20of%20applying%20Artificial%0AIntelligence%20methods%20in%20an%20automotive%20production%20environment%20to%20predict%20unknown%0Alead%20times%20in%20a%20non-cycle-controlled%20production%20area.%20Data%20structures%20are%0Aanalyzed%20to%20identify%20contextual%20features%20and%20then%20preprocessed%20using%20one-hot%0Aencoding.%20Methods%20selection%20focuses%20on%20supervised%20machine%20learning%20techniques.%0AIn%20supervised%20learning%20methods%2C%20regression%20and%20classification%20methods%20are%0Aevaluated.%20Continuous%20regression%20based%20on%20target%20size%20distribution%20is%20not%0Afeasible.%20Classification%20methods%20analysis%20shows%20that%20Ensemble%20Learning%20and%0ASupport%20Vector%20Machines%20are%20the%20most%20suitable.%20Preliminary%20study%20results%0Aindicate%20that%20gradient%20boosting%20algorithms%20LightGBM%2C%20XGBoost%2C%20and%20CatBoost%0Ayield%20the%20best%20results.%20After%20further%20testing%20and%20extensive%20hyperparameter%0Aoptimization%2C%20the%20final%20method%20choice%20is%20the%20LightGBM%20algorithm.%20Depending%20on%0Afeature%20availability%20and%20prediction%20interval%20granularity%2C%20relative%20prediction%0Aaccuracies%20of%20up%20to%2090%25%20can%20be%20achieved.%20Further%20tests%20highlight%20the%20importance%0Aof%20periodic%20retraining%20of%20AI%20models%20to%20accurately%20represent%20complex%20production%0Aprocesses%20using%20the%20database.%20The%20research%20demonstrates%20that%20AI%20methods%20can%20be%0Aeffectively%20applied%20to%20highly%20variable%20production%20data%2C%20adding%20business%20value%0Aby%20providing%20an%20additional%20metric%20for%20various%20control%20tasks%20while%20outperforming%0Acurrent%20non%20AI-based%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07317v3&entry.124074799=Read"},
{"title": "Nesterov Acceleration for Ensemble Kalman Inversion and Variants", "author": "Sydney Vernon and Eviatar Bach and Oliver R. A. Dunbar", "abstract": "  Ensemble Kalman inversion (EKI) is a derivative-free, particle-based\noptimization method for solving inverse problems. It can be shown that EKI\napproximates a gradient flow, which allows the application of methods for\naccelerating gradient descent. Here, we show that Nesterov acceleration is\neffective in speeding up the reduction of the EKI cost function on a variety of\ninverse problems. We also implement Nesterov acceleration for two EKI variants,\nunscented Kalman inversion and ensemble transform Kalman inversion. Our\nspecific implementation takes the form of a particle-level nudge that is\ndemonstrably simple to couple in a black-box fashion with any existing EKI\nvariant algorithms, comes with no additional computational expense, and with no\nadditional tuning hyperparameters. This work shows a pathway for future\nresearch to translate advances in gradient-based optimization into advances in\ngradient-free Kalman optimization.\n", "link": "http://arxiv.org/abs/2501.08779v1", "date": "2025-01-15", "relevancy": 1.7116, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4589}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4071}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nesterov%20Acceleration%20for%20Ensemble%20Kalman%20Inversion%20and%20Variants&body=Title%3A%20Nesterov%20Acceleration%20for%20Ensemble%20Kalman%20Inversion%20and%20Variants%0AAuthor%3A%20Sydney%20Vernon%20and%20Eviatar%20Bach%20and%20Oliver%20R.%20A.%20Dunbar%0AAbstract%3A%20%20%20Ensemble%20Kalman%20inversion%20%28EKI%29%20is%20a%20derivative-free%2C%20particle-based%0Aoptimization%20method%20for%20solving%20inverse%20problems.%20It%20can%20be%20shown%20that%20EKI%0Aapproximates%20a%20gradient%20flow%2C%20which%20allows%20the%20application%20of%20methods%20for%0Aaccelerating%20gradient%20descent.%20Here%2C%20we%20show%20that%20Nesterov%20acceleration%20is%0Aeffective%20in%20speeding%20up%20the%20reduction%20of%20the%20EKI%20cost%20function%20on%20a%20variety%20of%0Ainverse%20problems.%20We%20also%20implement%20Nesterov%20acceleration%20for%20two%20EKI%20variants%2C%0Aunscented%20Kalman%20inversion%20and%20ensemble%20transform%20Kalman%20inversion.%20Our%0Aspecific%20implementation%20takes%20the%20form%20of%20a%20particle-level%20nudge%20that%20is%0Ademonstrably%20simple%20to%20couple%20in%20a%20black-box%20fashion%20with%20any%20existing%20EKI%0Avariant%20algorithms%2C%20comes%20with%20no%20additional%20computational%20expense%2C%20and%20with%20no%0Aadditional%20tuning%20hyperparameters.%20This%20work%20shows%20a%20pathway%20for%20future%0Aresearch%20to%20translate%20advances%20in%20gradient-based%20optimization%20into%20advances%20in%0Agradient-free%20Kalman%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNesterov%2520Acceleration%2520for%2520Ensemble%2520Kalman%2520Inversion%2520and%2520Variants%26entry.906535625%3DSydney%2520Vernon%2520and%2520Eviatar%2520Bach%2520and%2520Oliver%2520R.%2520A.%2520Dunbar%26entry.1292438233%3D%2520%2520Ensemble%2520Kalman%2520inversion%2520%2528EKI%2529%2520is%2520a%2520derivative-free%252C%2520particle-based%250Aoptimization%2520method%2520for%2520solving%2520inverse%2520problems.%2520It%2520can%2520be%2520shown%2520that%2520EKI%250Aapproximates%2520a%2520gradient%2520flow%252C%2520which%2520allows%2520the%2520application%2520of%2520methods%2520for%250Aaccelerating%2520gradient%2520descent.%2520Here%252C%2520we%2520show%2520that%2520Nesterov%2520acceleration%2520is%250Aeffective%2520in%2520speeding%2520up%2520the%2520reduction%2520of%2520the%2520EKI%2520cost%2520function%2520on%2520a%2520variety%2520of%250Ainverse%2520problems.%2520We%2520also%2520implement%2520Nesterov%2520acceleration%2520for%2520two%2520EKI%2520variants%252C%250Aunscented%2520Kalman%2520inversion%2520and%2520ensemble%2520transform%2520Kalman%2520inversion.%2520Our%250Aspecific%2520implementation%2520takes%2520the%2520form%2520of%2520a%2520particle-level%2520nudge%2520that%2520is%250Ademonstrably%2520simple%2520to%2520couple%2520in%2520a%2520black-box%2520fashion%2520with%2520any%2520existing%2520EKI%250Avariant%2520algorithms%252C%2520comes%2520with%2520no%2520additional%2520computational%2520expense%252C%2520and%2520with%2520no%250Aadditional%2520tuning%2520hyperparameters.%2520This%2520work%2520shows%2520a%2520pathway%2520for%2520future%250Aresearch%2520to%2520translate%2520advances%2520in%2520gradient-based%2520optimization%2520into%2520advances%2520in%250Agradient-free%2520Kalman%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nesterov%20Acceleration%20for%20Ensemble%20Kalman%20Inversion%20and%20Variants&entry.906535625=Sydney%20Vernon%20and%20Eviatar%20Bach%20and%20Oliver%20R.%20A.%20Dunbar&entry.1292438233=%20%20Ensemble%20Kalman%20inversion%20%28EKI%29%20is%20a%20derivative-free%2C%20particle-based%0Aoptimization%20method%20for%20solving%20inverse%20problems.%20It%20can%20be%20shown%20that%20EKI%0Aapproximates%20a%20gradient%20flow%2C%20which%20allows%20the%20application%20of%20methods%20for%0Aaccelerating%20gradient%20descent.%20Here%2C%20we%20show%20that%20Nesterov%20acceleration%20is%0Aeffective%20in%20speeding%20up%20the%20reduction%20of%20the%20EKI%20cost%20function%20on%20a%20variety%20of%0Ainverse%20problems.%20We%20also%20implement%20Nesterov%20acceleration%20for%20two%20EKI%20variants%2C%0Aunscented%20Kalman%20inversion%20and%20ensemble%20transform%20Kalman%20inversion.%20Our%0Aspecific%20implementation%20takes%20the%20form%20of%20a%20particle-level%20nudge%20that%20is%0Ademonstrably%20simple%20to%20couple%20in%20a%20black-box%20fashion%20with%20any%20existing%20EKI%0Avariant%20algorithms%2C%20comes%20with%20no%20additional%20computational%20expense%2C%20and%20with%20no%0Aadditional%20tuning%20hyperparameters.%20This%20work%20shows%20a%20pathway%20for%20future%0Aresearch%20to%20translate%20advances%20in%20gradient-based%20optimization%20into%20advances%20in%0Agradient-free%20Kalman%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08779v1&entry.124074799=Read"},
{"title": "Deep learning for temporal super-resolution 4D Flow MRI", "author": "Pia Callmer and Mia Bonini and Edward Ferdian and David Nordsletten and Daniel Giese and Alistair A. Young and Alexander Fyrdahl and David Marlevi", "abstract": "  4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive technique\nfor volumetric, time-resolved blood flow quantification. However, apparent\ntrade-offs between acquisition time, image noise, and resolution limit clinical\napplicability. In particular, in regions of highly transient flow, coarse\ntemporal resolution can hinder accurate capture of physiologically relevant\nflow variations. To overcome these issues, post-processing techniques using\ndeep learning have shown promising results to enhance resolution post-scan\nusing so-called super-resolution networks. However, while super-resolution has\nbeen focusing on spatial upsampling, temporal super-resolution remains largely\nunexplored. The aim of this study was therefore to implement and evaluate a\nresidual network for temporal super-resolution 4D Flow MRI. To achieve this, an\nexisting spatial network (4DFlowNet) was re-designed for temporal upsampling,\nadapting input dimensions, and optimizing internal layer structures. Training\nand testing were performed using synthetic 4D Flow MRI data originating from\npatient-specific in-silico models, as well as using in-vivo datasets. Overall,\nexcellent performance was achieved with input velocities effectively denoised\nand temporally upsampled, with a mean absolute error (MAE) of 1.0 cm/s in an\nunseen in-silico setting, outperforming deterministic alternatives (linear\ninterpolation MAE = 2.3 cm/s, sinc interpolation MAE = 2.6 cm/s). Further, the\nnetwork synthesized high-resolution temporal information from unseen\nlow-resolution in-vivo data, with strong correlation observed at peak flow\nframes. As such, our results highlight the potential of utilizing data-driven\nneural networks for temporal super-resolution 4D Flow MRI, enabling\nhigh-frame-rate flow quantification without extending acquisition times beyond\nclinically acceptable limits.\n", "link": "http://arxiv.org/abs/2501.08780v1", "date": "2025-01-15", "relevancy": 1.5702, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6069}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5292}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning%20for%20temporal%20super-resolution%204D%20Flow%20MRI&body=Title%3A%20Deep%20learning%20for%20temporal%20super-resolution%204D%20Flow%20MRI%0AAuthor%3A%20Pia%20Callmer%20and%20Mia%20Bonini%20and%20Edward%20Ferdian%20and%20David%20Nordsletten%20and%20Daniel%20Giese%20and%20Alistair%20A.%20Young%20and%20Alexander%20Fyrdahl%20and%20David%20Marlevi%0AAbstract%3A%20%20%204D%20Flow%20Magnetic%20Resonance%20Imaging%20%284D%20Flow%20MRI%29%20is%20a%20non-invasive%20technique%0Afor%20volumetric%2C%20time-resolved%20blood%20flow%20quantification.%20However%2C%20apparent%0Atrade-offs%20between%20acquisition%20time%2C%20image%20noise%2C%20and%20resolution%20limit%20clinical%0Aapplicability.%20In%20particular%2C%20in%20regions%20of%20highly%20transient%20flow%2C%20coarse%0Atemporal%20resolution%20can%20hinder%20accurate%20capture%20of%20physiologically%20relevant%0Aflow%20variations.%20To%20overcome%20these%20issues%2C%20post-processing%20techniques%20using%0Adeep%20learning%20have%20shown%20promising%20results%20to%20enhance%20resolution%20post-scan%0Ausing%20so-called%20super-resolution%20networks.%20However%2C%20while%20super-resolution%20has%0Abeen%20focusing%20on%20spatial%20upsampling%2C%20temporal%20super-resolution%20remains%20largely%0Aunexplored.%20The%20aim%20of%20this%20study%20was%20therefore%20to%20implement%20and%20evaluate%20a%0Aresidual%20network%20for%20temporal%20super-resolution%204D%20Flow%20MRI.%20To%20achieve%20this%2C%20an%0Aexisting%20spatial%20network%20%284DFlowNet%29%20was%20re-designed%20for%20temporal%20upsampling%2C%0Aadapting%20input%20dimensions%2C%20and%20optimizing%20internal%20layer%20structures.%20Training%0Aand%20testing%20were%20performed%20using%20synthetic%204D%20Flow%20MRI%20data%20originating%20from%0Apatient-specific%20in-silico%20models%2C%20as%20well%20as%20using%20in-vivo%20datasets.%20Overall%2C%0Aexcellent%20performance%20was%20achieved%20with%20input%20velocities%20effectively%20denoised%0Aand%20temporally%20upsampled%2C%20with%20a%20mean%20absolute%20error%20%28MAE%29%20of%201.0%20cm/s%20in%20an%0Aunseen%20in-silico%20setting%2C%20outperforming%20deterministic%20alternatives%20%28linear%0Ainterpolation%20MAE%20%3D%202.3%20cm/s%2C%20sinc%20interpolation%20MAE%20%3D%202.6%20cm/s%29.%20Further%2C%20the%0Anetwork%20synthesized%20high-resolution%20temporal%20information%20from%20unseen%0Alow-resolution%20in-vivo%20data%2C%20with%20strong%20correlation%20observed%20at%20peak%20flow%0Aframes.%20As%20such%2C%20our%20results%20highlight%20the%20potential%20of%20utilizing%20data-driven%0Aneural%20networks%20for%20temporal%20super-resolution%204D%20Flow%20MRI%2C%20enabling%0Ahigh-frame-rate%20flow%20quantification%20without%20extending%20acquisition%20times%20beyond%0Aclinically%20acceptable%20limits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning%2520for%2520temporal%2520super-resolution%25204D%2520Flow%2520MRI%26entry.906535625%3DPia%2520Callmer%2520and%2520Mia%2520Bonini%2520and%2520Edward%2520Ferdian%2520and%2520David%2520Nordsletten%2520and%2520Daniel%2520Giese%2520and%2520Alistair%2520A.%2520Young%2520and%2520Alexander%2520Fyrdahl%2520and%2520David%2520Marlevi%26entry.1292438233%3D%2520%25204D%2520Flow%2520Magnetic%2520Resonance%2520Imaging%2520%25284D%2520Flow%2520MRI%2529%2520is%2520a%2520non-invasive%2520technique%250Afor%2520volumetric%252C%2520time-resolved%2520blood%2520flow%2520quantification.%2520However%252C%2520apparent%250Atrade-offs%2520between%2520acquisition%2520time%252C%2520image%2520noise%252C%2520and%2520resolution%2520limit%2520clinical%250Aapplicability.%2520In%2520particular%252C%2520in%2520regions%2520of%2520highly%2520transient%2520flow%252C%2520coarse%250Atemporal%2520resolution%2520can%2520hinder%2520accurate%2520capture%2520of%2520physiologically%2520relevant%250Aflow%2520variations.%2520To%2520overcome%2520these%2520issues%252C%2520post-processing%2520techniques%2520using%250Adeep%2520learning%2520have%2520shown%2520promising%2520results%2520to%2520enhance%2520resolution%2520post-scan%250Ausing%2520so-called%2520super-resolution%2520networks.%2520However%252C%2520while%2520super-resolution%2520has%250Abeen%2520focusing%2520on%2520spatial%2520upsampling%252C%2520temporal%2520super-resolution%2520remains%2520largely%250Aunexplored.%2520The%2520aim%2520of%2520this%2520study%2520was%2520therefore%2520to%2520implement%2520and%2520evaluate%2520a%250Aresidual%2520network%2520for%2520temporal%2520super-resolution%25204D%2520Flow%2520MRI.%2520To%2520achieve%2520this%252C%2520an%250Aexisting%2520spatial%2520network%2520%25284DFlowNet%2529%2520was%2520re-designed%2520for%2520temporal%2520upsampling%252C%250Aadapting%2520input%2520dimensions%252C%2520and%2520optimizing%2520internal%2520layer%2520structures.%2520Training%250Aand%2520testing%2520were%2520performed%2520using%2520synthetic%25204D%2520Flow%2520MRI%2520data%2520originating%2520from%250Apatient-specific%2520in-silico%2520models%252C%2520as%2520well%2520as%2520using%2520in-vivo%2520datasets.%2520Overall%252C%250Aexcellent%2520performance%2520was%2520achieved%2520with%2520input%2520velocities%2520effectively%2520denoised%250Aand%2520temporally%2520upsampled%252C%2520with%2520a%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25201.0%2520cm/s%2520in%2520an%250Aunseen%2520in-silico%2520setting%252C%2520outperforming%2520deterministic%2520alternatives%2520%2528linear%250Ainterpolation%2520MAE%2520%253D%25202.3%2520cm/s%252C%2520sinc%2520interpolation%2520MAE%2520%253D%25202.6%2520cm/s%2529.%2520Further%252C%2520the%250Anetwork%2520synthesized%2520high-resolution%2520temporal%2520information%2520from%2520unseen%250Alow-resolution%2520in-vivo%2520data%252C%2520with%2520strong%2520correlation%2520observed%2520at%2520peak%2520flow%250Aframes.%2520As%2520such%252C%2520our%2520results%2520highlight%2520the%2520potential%2520of%2520utilizing%2520data-driven%250Aneural%2520networks%2520for%2520temporal%2520super-resolution%25204D%2520Flow%2520MRI%252C%2520enabling%250Ahigh-frame-rate%2520flow%2520quantification%2520without%2520extending%2520acquisition%2520times%2520beyond%250Aclinically%2520acceptable%2520limits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning%20for%20temporal%20super-resolution%204D%20Flow%20MRI&entry.906535625=Pia%20Callmer%20and%20Mia%20Bonini%20and%20Edward%20Ferdian%20and%20David%20Nordsletten%20and%20Daniel%20Giese%20and%20Alistair%20A.%20Young%20and%20Alexander%20Fyrdahl%20and%20David%20Marlevi&entry.1292438233=%20%204D%20Flow%20Magnetic%20Resonance%20Imaging%20%284D%20Flow%20MRI%29%20is%20a%20non-invasive%20technique%0Afor%20volumetric%2C%20time-resolved%20blood%20flow%20quantification.%20However%2C%20apparent%0Atrade-offs%20between%20acquisition%20time%2C%20image%20noise%2C%20and%20resolution%20limit%20clinical%0Aapplicability.%20In%20particular%2C%20in%20regions%20of%20highly%20transient%20flow%2C%20coarse%0Atemporal%20resolution%20can%20hinder%20accurate%20capture%20of%20physiologically%20relevant%0Aflow%20variations.%20To%20overcome%20these%20issues%2C%20post-processing%20techniques%20using%0Adeep%20learning%20have%20shown%20promising%20results%20to%20enhance%20resolution%20post-scan%0Ausing%20so-called%20super-resolution%20networks.%20However%2C%20while%20super-resolution%20has%0Abeen%20focusing%20on%20spatial%20upsampling%2C%20temporal%20super-resolution%20remains%20largely%0Aunexplored.%20The%20aim%20of%20this%20study%20was%20therefore%20to%20implement%20and%20evaluate%20a%0Aresidual%20network%20for%20temporal%20super-resolution%204D%20Flow%20MRI.%20To%20achieve%20this%2C%20an%0Aexisting%20spatial%20network%20%284DFlowNet%29%20was%20re-designed%20for%20temporal%20upsampling%2C%0Aadapting%20input%20dimensions%2C%20and%20optimizing%20internal%20layer%20structures.%20Training%0Aand%20testing%20were%20performed%20using%20synthetic%204D%20Flow%20MRI%20data%20originating%20from%0Apatient-specific%20in-silico%20models%2C%20as%20well%20as%20using%20in-vivo%20datasets.%20Overall%2C%0Aexcellent%20performance%20was%20achieved%20with%20input%20velocities%20effectively%20denoised%0Aand%20temporally%20upsampled%2C%20with%20a%20mean%20absolute%20error%20%28MAE%29%20of%201.0%20cm/s%20in%20an%0Aunseen%20in-silico%20setting%2C%20outperforming%20deterministic%20alternatives%20%28linear%0Ainterpolation%20MAE%20%3D%202.3%20cm/s%2C%20sinc%20interpolation%20MAE%20%3D%202.6%20cm/s%29.%20Further%2C%20the%0Anetwork%20synthesized%20high-resolution%20temporal%20information%20from%20unseen%0Alow-resolution%20in-vivo%20data%2C%20with%20strong%20correlation%20observed%20at%20peak%20flow%0Aframes.%20As%20such%2C%20our%20results%20highlight%20the%20potential%20of%20utilizing%20data-driven%0Aneural%20networks%20for%20temporal%20super-resolution%204D%20Flow%20MRI%2C%20enabling%0Ahigh-frame-rate%20flow%20quantification%20without%20extending%20acquisition%20times%20beyond%0Aclinically%20acceptable%20limits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08780v1&entry.124074799=Read"},
{"title": "Integrated Push-and-Pull Update Model for Goal-Oriented Effective\n  Communication", "author": "Pouya Agheli and Nikolaos Pappas and Petar Popovski and Marios Kountouris", "abstract": "  This paper studies decision-making for goal-oriented effective communication.\nWe consider an end-to-end status update system where a sensing agent (SA)\nobserves a source, generates and transmits updates to an actuation agent (AA),\nwhile the AA takes actions to accomplish a goal at the endpoint. We integrate\nthe push- and pull-based update communication models to obtain a push-and-pull\nmodel, which allows the transmission controller at the SA to decide to push an\nupdate to the AA and the query controller at the AA to pull updates by raising\nqueries at specific time instances. To gauge effectiveness, we utilize a grade\nof effectiveness (GoE) metric incorporating updates' freshness, usefulness, and\ntimeliness of actions as qualitative attributes. We then derive effect-aware\npolicies to maximize the expected discounted sum of updates' effectiveness\nsubject to induced costs. The effect-aware policy at the SA considers the\npotential effectiveness of communicated updates at the endpoint, while at the\nAA, it accounts for the probabilistic evolution of the source and importance of\ngenerated updates. Our results show the proposed push-and-pull model\noutperforms models solely based on push- or pull-based updates both in terms of\nefficiency and effectiveness. Additionally, using effect-aware policies at both\nagents enhances effectiveness compared to periodic and/or probabilistic\neffect-agnostic policies at either or both agents.\n", "link": "http://arxiv.org/abs/2407.14092v2", "date": "2025-01-15", "relevancy": 1.3531, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4559}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.45}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrated%20Push-and-Pull%20Update%20Model%20for%20Goal-Oriented%20Effective%0A%20%20Communication&body=Title%3A%20Integrated%20Push-and-Pull%20Update%20Model%20for%20Goal-Oriented%20Effective%0A%20%20Communication%0AAuthor%3A%20Pouya%20Agheli%20and%20Nikolaos%20Pappas%20and%20Petar%20Popovski%20and%20Marios%20Kountouris%0AAbstract%3A%20%20%20This%20paper%20studies%20decision-making%20for%20goal-oriented%20effective%20communication.%0AWe%20consider%20an%20end-to-end%20status%20update%20system%20where%20a%20sensing%20agent%20%28SA%29%0Aobserves%20a%20source%2C%20generates%20and%20transmits%20updates%20to%20an%20actuation%20agent%20%28AA%29%2C%0Awhile%20the%20AA%20takes%20actions%20to%20accomplish%20a%20goal%20at%20the%20endpoint.%20We%20integrate%0Athe%20push-%20and%20pull-based%20update%20communication%20models%20to%20obtain%20a%20push-and-pull%0Amodel%2C%20which%20allows%20the%20transmission%20controller%20at%20the%20SA%20to%20decide%20to%20push%20an%0Aupdate%20to%20the%20AA%20and%20the%20query%20controller%20at%20the%20AA%20to%20pull%20updates%20by%20raising%0Aqueries%20at%20specific%20time%20instances.%20To%20gauge%20effectiveness%2C%20we%20utilize%20a%20grade%0Aof%20effectiveness%20%28GoE%29%20metric%20incorporating%20updates%27%20freshness%2C%20usefulness%2C%20and%0Atimeliness%20of%20actions%20as%20qualitative%20attributes.%20We%20then%20derive%20effect-aware%0Apolicies%20to%20maximize%20the%20expected%20discounted%20sum%20of%20updates%27%20effectiveness%0Asubject%20to%20induced%20costs.%20The%20effect-aware%20policy%20at%20the%20SA%20considers%20the%0Apotential%20effectiveness%20of%20communicated%20updates%20at%20the%20endpoint%2C%20while%20at%20the%0AAA%2C%20it%20accounts%20for%20the%20probabilistic%20evolution%20of%20the%20source%20and%20importance%20of%0Agenerated%20updates.%20Our%20results%20show%20the%20proposed%20push-and-pull%20model%0Aoutperforms%20models%20solely%20based%20on%20push-%20or%20pull-based%20updates%20both%20in%20terms%20of%0Aefficiency%20and%20effectiveness.%20Additionally%2C%20using%20effect-aware%20policies%20at%20both%0Aagents%20enhances%20effectiveness%20compared%20to%20periodic%20and/or%20probabilistic%0Aeffect-agnostic%20policies%20at%20either%20or%20both%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrated%2520Push-and-Pull%2520Update%2520Model%2520for%2520Goal-Oriented%2520Effective%250A%2520%2520Communication%26entry.906535625%3DPouya%2520Agheli%2520and%2520Nikolaos%2520Pappas%2520and%2520Petar%2520Popovski%2520and%2520Marios%2520Kountouris%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520decision-making%2520for%2520goal-oriented%2520effective%2520communication.%250AWe%2520consider%2520an%2520end-to-end%2520status%2520update%2520system%2520where%2520a%2520sensing%2520agent%2520%2528SA%2529%250Aobserves%2520a%2520source%252C%2520generates%2520and%2520transmits%2520updates%2520to%2520an%2520actuation%2520agent%2520%2528AA%2529%252C%250Awhile%2520the%2520AA%2520takes%2520actions%2520to%2520accomplish%2520a%2520goal%2520at%2520the%2520endpoint.%2520We%2520integrate%250Athe%2520push-%2520and%2520pull-based%2520update%2520communication%2520models%2520to%2520obtain%2520a%2520push-and-pull%250Amodel%252C%2520which%2520allows%2520the%2520transmission%2520controller%2520at%2520the%2520SA%2520to%2520decide%2520to%2520push%2520an%250Aupdate%2520to%2520the%2520AA%2520and%2520the%2520query%2520controller%2520at%2520the%2520AA%2520to%2520pull%2520updates%2520by%2520raising%250Aqueries%2520at%2520specific%2520time%2520instances.%2520To%2520gauge%2520effectiveness%252C%2520we%2520utilize%2520a%2520grade%250Aof%2520effectiveness%2520%2528GoE%2529%2520metric%2520incorporating%2520updates%2527%2520freshness%252C%2520usefulness%252C%2520and%250Atimeliness%2520of%2520actions%2520as%2520qualitative%2520attributes.%2520We%2520then%2520derive%2520effect-aware%250Apolicies%2520to%2520maximize%2520the%2520expected%2520discounted%2520sum%2520of%2520updates%2527%2520effectiveness%250Asubject%2520to%2520induced%2520costs.%2520The%2520effect-aware%2520policy%2520at%2520the%2520SA%2520considers%2520the%250Apotential%2520effectiveness%2520of%2520communicated%2520updates%2520at%2520the%2520endpoint%252C%2520while%2520at%2520the%250AAA%252C%2520it%2520accounts%2520for%2520the%2520probabilistic%2520evolution%2520of%2520the%2520source%2520and%2520importance%2520of%250Agenerated%2520updates.%2520Our%2520results%2520show%2520the%2520proposed%2520push-and-pull%2520model%250Aoutperforms%2520models%2520solely%2520based%2520on%2520push-%2520or%2520pull-based%2520updates%2520both%2520in%2520terms%2520of%250Aefficiency%2520and%2520effectiveness.%2520Additionally%252C%2520using%2520effect-aware%2520policies%2520at%2520both%250Aagents%2520enhances%2520effectiveness%2520compared%2520to%2520periodic%2520and/or%2520probabilistic%250Aeffect-agnostic%2520policies%2520at%2520either%2520or%2520both%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrated%20Push-and-Pull%20Update%20Model%20for%20Goal-Oriented%20Effective%0A%20%20Communication&entry.906535625=Pouya%20Agheli%20and%20Nikolaos%20Pappas%20and%20Petar%20Popovski%20and%20Marios%20Kountouris&entry.1292438233=%20%20This%20paper%20studies%20decision-making%20for%20goal-oriented%20effective%20communication.%0AWe%20consider%20an%20end-to-end%20status%20update%20system%20where%20a%20sensing%20agent%20%28SA%29%0Aobserves%20a%20source%2C%20generates%20and%20transmits%20updates%20to%20an%20actuation%20agent%20%28AA%29%2C%0Awhile%20the%20AA%20takes%20actions%20to%20accomplish%20a%20goal%20at%20the%20endpoint.%20We%20integrate%0Athe%20push-%20and%20pull-based%20update%20communication%20models%20to%20obtain%20a%20push-and-pull%0Amodel%2C%20which%20allows%20the%20transmission%20controller%20at%20the%20SA%20to%20decide%20to%20push%20an%0Aupdate%20to%20the%20AA%20and%20the%20query%20controller%20at%20the%20AA%20to%20pull%20updates%20by%20raising%0Aqueries%20at%20specific%20time%20instances.%20To%20gauge%20effectiveness%2C%20we%20utilize%20a%20grade%0Aof%20effectiveness%20%28GoE%29%20metric%20incorporating%20updates%27%20freshness%2C%20usefulness%2C%20and%0Atimeliness%20of%20actions%20as%20qualitative%20attributes.%20We%20then%20derive%20effect-aware%0Apolicies%20to%20maximize%20the%20expected%20discounted%20sum%20of%20updates%27%20effectiveness%0Asubject%20to%20induced%20costs.%20The%20effect-aware%20policy%20at%20the%20SA%20considers%20the%0Apotential%20effectiveness%20of%20communicated%20updates%20at%20the%20endpoint%2C%20while%20at%20the%0AAA%2C%20it%20accounts%20for%20the%20probabilistic%20evolution%20of%20the%20source%20and%20importance%20of%0Agenerated%20updates.%20Our%20results%20show%20the%20proposed%20push-and-pull%20model%0Aoutperforms%20models%20solely%20based%20on%20push-%20or%20pull-based%20updates%20both%20in%20terms%20of%0Aefficiency%20and%20effectiveness.%20Additionally%2C%20using%20effect-aware%20policies%20at%20both%0Aagents%20enhances%20effectiveness%20compared%20to%20periodic%20and/or%20probabilistic%0Aeffect-agnostic%20policies%20at%20either%20or%20both%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14092v2&entry.124074799=Read"},
{"title": "Feature-based One-For-All: A Universal Framework for Heterogeneous\n  Knowledge Distillation", "author": "Jhe-Hao Lin and Yi Yao and Chan-Feng Hsu and Hongxia Xie and Hong-Han Shuai and Wen-Huang Cheng", "abstract": "  Knowledge distillation (KD) involves transferring knowledge from a\npre-trained heavy teacher model to a lighter student model, thereby reducing\nthe inference cost while maintaining comparable effectiveness. Prior KD\ntechniques typically assume homogeneity between the teacher and student models.\nHowever, as technology advances, a wide variety of architectures have emerged,\nranging from initial Convolutional Neural Networks (CNNs) to Vision\nTransformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,\ndeveloping a universal KD framework compatible with any architecture has become\nan important research topic. In this paper, we introduce a feature-based\none-for-all (FOFA) KD framework to enable feature distillation across diverse\narchitecture. Our framework comprises two key components. First, we design\nprompt tuning blocks that incorporate student feedback, allowing teacher\nfeatures to adapt to the student model's learning process. Second, we propose\nregion-aware attention to mitigate the view mismatch problem between\nheterogeneous architecture. By leveraging these two modules, effective\ndistillation of intermediate features can be achieved across heterogeneous\narchitectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate\nthe superiority of the proposed method.\n", "link": "http://arxiv.org/abs/2501.08885v1", "date": "2025-01-15", "relevancy": 1.556, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5325}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5092}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-based%20One-For-All%3A%20A%20Universal%20Framework%20for%20Heterogeneous%0A%20%20Knowledge%20Distillation&body=Title%3A%20Feature-based%20One-For-All%3A%20A%20Universal%20Framework%20for%20Heterogeneous%0A%20%20Knowledge%20Distillation%0AAuthor%3A%20Jhe-Hao%20Lin%20and%20Yi%20Yao%20and%20Chan-Feng%20Hsu%20and%20Hongxia%20Xie%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20involves%20transferring%20knowledge%20from%20a%0Apre-trained%20heavy%20teacher%20model%20to%20a%20lighter%20student%20model%2C%20thereby%20reducing%0Athe%20inference%20cost%20while%20maintaining%20comparable%20effectiveness.%20Prior%20KD%0Atechniques%20typically%20assume%20homogeneity%20between%20the%20teacher%20and%20student%20models.%0AHowever%2C%20as%20technology%20advances%2C%20a%20wide%20variety%20of%20architectures%20have%20emerged%2C%0Aranging%20from%20initial%20Convolutional%20Neural%20Networks%20%28CNNs%29%20to%20Vision%0ATransformers%20%28ViTs%29%2C%20and%20Multi-Level%20Perceptrons%20%28MLPs%29.%20Consequently%2C%0Adeveloping%20a%20universal%20KD%20framework%20compatible%20with%20any%20architecture%20has%20become%0Aan%20important%20research%20topic.%20In%20this%20paper%2C%20we%20introduce%20a%20feature-based%0Aone-for-all%20%28FOFA%29%20KD%20framework%20to%20enable%20feature%20distillation%20across%20diverse%0Aarchitecture.%20Our%20framework%20comprises%20two%20key%20components.%20First%2C%20we%20design%0Aprompt%20tuning%20blocks%20that%20incorporate%20student%20feedback%2C%20allowing%20teacher%0Afeatures%20to%20adapt%20to%20the%20student%20model%27s%20learning%20process.%20Second%2C%20we%20propose%0Aregion-aware%20attention%20to%20mitigate%20the%20view%20mismatch%20problem%20between%0Aheterogeneous%20architecture.%20By%20leveraging%20these%20two%20modules%2C%20effective%0Adistillation%20of%20intermediate%20features%20can%20be%20achieved%20across%20heterogeneous%0Aarchitectures.%20Extensive%20experiments%20on%20CIFAR%2C%20ImageNet%2C%20and%20COCO%20demonstrate%0Athe%20superiority%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-based%2520One-For-All%253A%2520A%2520Universal%2520Framework%2520for%2520Heterogeneous%250A%2520%2520Knowledge%2520Distillation%26entry.906535625%3DJhe-Hao%2520Lin%2520and%2520Yi%2520Yao%2520and%2520Chan-Feng%2520Hsu%2520and%2520Hongxia%2520Xie%2520and%2520Hong-Han%2520Shuai%2520and%2520Wen-Huang%2520Cheng%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520involves%2520transferring%2520knowledge%2520from%2520a%250Apre-trained%2520heavy%2520teacher%2520model%2520to%2520a%2520lighter%2520student%2520model%252C%2520thereby%2520reducing%250Athe%2520inference%2520cost%2520while%2520maintaining%2520comparable%2520effectiveness.%2520Prior%2520KD%250Atechniques%2520typically%2520assume%2520homogeneity%2520between%2520the%2520teacher%2520and%2520student%2520models.%250AHowever%252C%2520as%2520technology%2520advances%252C%2520a%2520wide%2520variety%2520of%2520architectures%2520have%2520emerged%252C%250Aranging%2520from%2520initial%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520to%2520Vision%250ATransformers%2520%2528ViTs%2529%252C%2520and%2520Multi-Level%2520Perceptrons%2520%2528MLPs%2529.%2520Consequently%252C%250Adeveloping%2520a%2520universal%2520KD%2520framework%2520compatible%2520with%2520any%2520architecture%2520has%2520become%250Aan%2520important%2520research%2520topic.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520feature-based%250Aone-for-all%2520%2528FOFA%2529%2520KD%2520framework%2520to%2520enable%2520feature%2520distillation%2520across%2520diverse%250Aarchitecture.%2520Our%2520framework%2520comprises%2520two%2520key%2520components.%2520First%252C%2520we%2520design%250Aprompt%2520tuning%2520blocks%2520that%2520incorporate%2520student%2520feedback%252C%2520allowing%2520teacher%250Afeatures%2520to%2520adapt%2520to%2520the%2520student%2520model%2527s%2520learning%2520process.%2520Second%252C%2520we%2520propose%250Aregion-aware%2520attention%2520to%2520mitigate%2520the%2520view%2520mismatch%2520problem%2520between%250Aheterogeneous%2520architecture.%2520By%2520leveraging%2520these%2520two%2520modules%252C%2520effective%250Adistillation%2520of%2520intermediate%2520features%2520can%2520be%2520achieved%2520across%2520heterogeneous%250Aarchitectures.%2520Extensive%2520experiments%2520on%2520CIFAR%252C%2520ImageNet%252C%2520and%2520COCO%2520demonstrate%250Athe%2520superiority%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-based%20One-For-All%3A%20A%20Universal%20Framework%20for%20Heterogeneous%0A%20%20Knowledge%20Distillation&entry.906535625=Jhe-Hao%20Lin%20and%20Yi%20Yao%20and%20Chan-Feng%20Hsu%20and%20Hongxia%20Xie%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20involves%20transferring%20knowledge%20from%20a%0Apre-trained%20heavy%20teacher%20model%20to%20a%20lighter%20student%20model%2C%20thereby%20reducing%0Athe%20inference%20cost%20while%20maintaining%20comparable%20effectiveness.%20Prior%20KD%0Atechniques%20typically%20assume%20homogeneity%20between%20the%20teacher%20and%20student%20models.%0AHowever%2C%20as%20technology%20advances%2C%20a%20wide%20variety%20of%20architectures%20have%20emerged%2C%0Aranging%20from%20initial%20Convolutional%20Neural%20Networks%20%28CNNs%29%20to%20Vision%0ATransformers%20%28ViTs%29%2C%20and%20Multi-Level%20Perceptrons%20%28MLPs%29.%20Consequently%2C%0Adeveloping%20a%20universal%20KD%20framework%20compatible%20with%20any%20architecture%20has%20become%0Aan%20important%20research%20topic.%20In%20this%20paper%2C%20we%20introduce%20a%20feature-based%0Aone-for-all%20%28FOFA%29%20KD%20framework%20to%20enable%20feature%20distillation%20across%20diverse%0Aarchitecture.%20Our%20framework%20comprises%20two%20key%20components.%20First%2C%20we%20design%0Aprompt%20tuning%20blocks%20that%20incorporate%20student%20feedback%2C%20allowing%20teacher%0Afeatures%20to%20adapt%20to%20the%20student%20model%27s%20learning%20process.%20Second%2C%20we%20propose%0Aregion-aware%20attention%20to%20mitigate%20the%20view%20mismatch%20problem%20between%0Aheterogeneous%20architecture.%20By%20leveraging%20these%20two%20modules%2C%20effective%0Adistillation%20of%20intermediate%20features%20can%20be%20achieved%20across%20heterogeneous%0Aarchitectures.%20Extensive%20experiments%20on%20CIFAR%2C%20ImageNet%2C%20and%20COCO%20demonstrate%0Athe%20superiority%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08885v1&entry.124074799=Read"},
{"title": "Debiasing Synthetic Data Generated by Deep Generative Models", "author": "Alexander Decruyenaere and Heidelinde Dehaene and Paloma Rabaey and Christiaan Polet and Johan Decruyenaere and Thomas Demeester and Stijn Vansteelandt", "abstract": "  While synthetic data hold great promise for privacy protection, their\nstatistical analysis poses significant challenges that necessitate innovative\nsolutions. The use of deep generative models (DGMs) for synthetic data\ngeneration is known to induce considerable bias and imprecision into synthetic\ndata analyses, compromising their inferential utility as opposed to original\ndata analyses. This bias and uncertainty can be substantial enough to impede\nstatistical convergence rates, even in seemingly straightforward analyses like\nmean calculation. The standard errors of such estimators then exhibit slower\nshrinkage with sample size than the typical 1 over root-$n$ rate. This\ncomplicates fundamental calculations like p-values and confidence intervals,\nwith no straightforward remedy currently available. In response to these\nchallenges, we propose a new strategy that targets synthetic data created by\nDGMs for specific data analyses. Drawing insights from debiased and targeted\nmachine learning, our approach accounts for biases, enhances convergence rates,\nand facilitates the calculation of estimators with easily approximated large\nsample variances. We exemplify our proposal through a simulation study on toy\ndata and two case studies on real-world data, highlighting the importance of\ntailoring DGMs for targeted data analysis. This debiasing strategy contributes\nto advancing the reliability and applicability of synthetic data in statistical\ninference.\n", "link": "http://arxiv.org/abs/2411.04216v2", "date": "2025-01-15", "relevancy": 1.5406, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5382}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5074}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Debiasing%20Synthetic%20Data%20Generated%20by%20Deep%20Generative%20Models&body=Title%3A%20Debiasing%20Synthetic%20Data%20Generated%20by%20Deep%20Generative%20Models%0AAuthor%3A%20Alexander%20Decruyenaere%20and%20Heidelinde%20Dehaene%20and%20Paloma%20Rabaey%20and%20Christiaan%20Polet%20and%20Johan%20Decruyenaere%20and%20Thomas%20Demeester%20and%20Stijn%20Vansteelandt%0AAbstract%3A%20%20%20While%20synthetic%20data%20hold%20great%20promise%20for%20privacy%20protection%2C%20their%0Astatistical%20analysis%20poses%20significant%20challenges%20that%20necessitate%20innovative%0Asolutions.%20The%20use%20of%20deep%20generative%20models%20%28DGMs%29%20for%20synthetic%20data%0Ageneration%20is%20known%20to%20induce%20considerable%20bias%20and%20imprecision%20into%20synthetic%0Adata%20analyses%2C%20compromising%20their%20inferential%20utility%20as%20opposed%20to%20original%0Adata%20analyses.%20This%20bias%20and%20uncertainty%20can%20be%20substantial%20enough%20to%20impede%0Astatistical%20convergence%20rates%2C%20even%20in%20seemingly%20straightforward%20analyses%20like%0Amean%20calculation.%20The%20standard%20errors%20of%20such%20estimators%20then%20exhibit%20slower%0Ashrinkage%20with%20sample%20size%20than%20the%20typical%201%20over%20root-%24n%24%20rate.%20This%0Acomplicates%20fundamental%20calculations%20like%20p-values%20and%20confidence%20intervals%2C%0Awith%20no%20straightforward%20remedy%20currently%20available.%20In%20response%20to%20these%0Achallenges%2C%20we%20propose%20a%20new%20strategy%20that%20targets%20synthetic%20data%20created%20by%0ADGMs%20for%20specific%20data%20analyses.%20Drawing%20insights%20from%20debiased%20and%20targeted%0Amachine%20learning%2C%20our%20approach%20accounts%20for%20biases%2C%20enhances%20convergence%20rates%2C%0Aand%20facilitates%20the%20calculation%20of%20estimators%20with%20easily%20approximated%20large%0Asample%20variances.%20We%20exemplify%20our%20proposal%20through%20a%20simulation%20study%20on%20toy%0Adata%20and%20two%20case%20studies%20on%20real-world%20data%2C%20highlighting%20the%20importance%20of%0Atailoring%20DGMs%20for%20targeted%20data%20analysis.%20This%20debiasing%20strategy%20contributes%0Ato%20advancing%20the%20reliability%20and%20applicability%20of%20synthetic%20data%20in%20statistical%0Ainference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDebiasing%2520Synthetic%2520Data%2520Generated%2520by%2520Deep%2520Generative%2520Models%26entry.906535625%3DAlexander%2520Decruyenaere%2520and%2520Heidelinde%2520Dehaene%2520and%2520Paloma%2520Rabaey%2520and%2520Christiaan%2520Polet%2520and%2520Johan%2520Decruyenaere%2520and%2520Thomas%2520Demeester%2520and%2520Stijn%2520Vansteelandt%26entry.1292438233%3D%2520%2520While%2520synthetic%2520data%2520hold%2520great%2520promise%2520for%2520privacy%2520protection%252C%2520their%250Astatistical%2520analysis%2520poses%2520significant%2520challenges%2520that%2520necessitate%2520innovative%250Asolutions.%2520The%2520use%2520of%2520deep%2520generative%2520models%2520%2528DGMs%2529%2520for%2520synthetic%2520data%250Ageneration%2520is%2520known%2520to%2520induce%2520considerable%2520bias%2520and%2520imprecision%2520into%2520synthetic%250Adata%2520analyses%252C%2520compromising%2520their%2520inferential%2520utility%2520as%2520opposed%2520to%2520original%250Adata%2520analyses.%2520This%2520bias%2520and%2520uncertainty%2520can%2520be%2520substantial%2520enough%2520to%2520impede%250Astatistical%2520convergence%2520rates%252C%2520even%2520in%2520seemingly%2520straightforward%2520analyses%2520like%250Amean%2520calculation.%2520The%2520standard%2520errors%2520of%2520such%2520estimators%2520then%2520exhibit%2520slower%250Ashrinkage%2520with%2520sample%2520size%2520than%2520the%2520typical%25201%2520over%2520root-%2524n%2524%2520rate.%2520This%250Acomplicates%2520fundamental%2520calculations%2520like%2520p-values%2520and%2520confidence%2520intervals%252C%250Awith%2520no%2520straightforward%2520remedy%2520currently%2520available.%2520In%2520response%2520to%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520new%2520strategy%2520that%2520targets%2520synthetic%2520data%2520created%2520by%250ADGMs%2520for%2520specific%2520data%2520analyses.%2520Drawing%2520insights%2520from%2520debiased%2520and%2520targeted%250Amachine%2520learning%252C%2520our%2520approach%2520accounts%2520for%2520biases%252C%2520enhances%2520convergence%2520rates%252C%250Aand%2520facilitates%2520the%2520calculation%2520of%2520estimators%2520with%2520easily%2520approximated%2520large%250Asample%2520variances.%2520We%2520exemplify%2520our%2520proposal%2520through%2520a%2520simulation%2520study%2520on%2520toy%250Adata%2520and%2520two%2520case%2520studies%2520on%2520real-world%2520data%252C%2520highlighting%2520the%2520importance%2520of%250Atailoring%2520DGMs%2520for%2520targeted%2520data%2520analysis.%2520This%2520debiasing%2520strategy%2520contributes%250Ato%2520advancing%2520the%2520reliability%2520and%2520applicability%2520of%2520synthetic%2520data%2520in%2520statistical%250Ainference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debiasing%20Synthetic%20Data%20Generated%20by%20Deep%20Generative%20Models&entry.906535625=Alexander%20Decruyenaere%20and%20Heidelinde%20Dehaene%20and%20Paloma%20Rabaey%20and%20Christiaan%20Polet%20and%20Johan%20Decruyenaere%20and%20Thomas%20Demeester%20and%20Stijn%20Vansteelandt&entry.1292438233=%20%20While%20synthetic%20data%20hold%20great%20promise%20for%20privacy%20protection%2C%20their%0Astatistical%20analysis%20poses%20significant%20challenges%20that%20necessitate%20innovative%0Asolutions.%20The%20use%20of%20deep%20generative%20models%20%28DGMs%29%20for%20synthetic%20data%0Ageneration%20is%20known%20to%20induce%20considerable%20bias%20and%20imprecision%20into%20synthetic%0Adata%20analyses%2C%20compromising%20their%20inferential%20utility%20as%20opposed%20to%20original%0Adata%20analyses.%20This%20bias%20and%20uncertainty%20can%20be%20substantial%20enough%20to%20impede%0Astatistical%20convergence%20rates%2C%20even%20in%20seemingly%20straightforward%20analyses%20like%0Amean%20calculation.%20The%20standard%20errors%20of%20such%20estimators%20then%20exhibit%20slower%0Ashrinkage%20with%20sample%20size%20than%20the%20typical%201%20over%20root-%24n%24%20rate.%20This%0Acomplicates%20fundamental%20calculations%20like%20p-values%20and%20confidence%20intervals%2C%0Awith%20no%20straightforward%20remedy%20currently%20available.%20In%20response%20to%20these%0Achallenges%2C%20we%20propose%20a%20new%20strategy%20that%20targets%20synthetic%20data%20created%20by%0ADGMs%20for%20specific%20data%20analyses.%20Drawing%20insights%20from%20debiased%20and%20targeted%0Amachine%20learning%2C%20our%20approach%20accounts%20for%20biases%2C%20enhances%20convergence%20rates%2C%0Aand%20facilitates%20the%20calculation%20of%20estimators%20with%20easily%20approximated%20large%0Asample%20variances.%20We%20exemplify%20our%20proposal%20through%20a%20simulation%20study%20on%20toy%0Adata%20and%20two%20case%20studies%20on%20real-world%20data%2C%20highlighting%20the%20importance%20of%0Atailoring%20DGMs%20for%20targeted%20data%20analysis.%20This%20debiasing%20strategy%20contributes%0Ato%20advancing%20the%20reliability%20and%20applicability%20of%20synthetic%20data%20in%20statistical%0Ainference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04216v2&entry.124074799=Read"},
{"title": "Task Allocation in Mobile Robot Fleets: A review", "author": "Andr\u00e9s Meseguer Valenzuela and Francisco Blanes Noguera", "abstract": "  Mobile robot fleets are currently used in different scenarios such as medical\nenvironments or logistics. The management of these systems provides different\nchallenges that vary from the control of the movement of each robot to the\nallocation of tasks to be performed. Task Allocation (TA) problem is a key\ntopic for the proper management of mobile robot fleets to ensure the\nminimization of energy consumption and quantity of necessary robots. Solutions\non this aspect are essential to reach economic and environmental sustainability\nof robot fleets, mainly in industry applications such as warehouse logistics.\nThe minimization of energy consumption introduces TA problem as an optimization\nissue which has been treated in recent studies. This work focuses on the\nanalysis of current trends in solving TA of mobile robot fleets. Main TA\noptimization algorithms are presented, including novel methods based on\nArtificial Intelligence (AI). Additionally, this work showcases most important\nresults extracted from simulations, including frameworks utilized for the\ndevelopment of the simulations. Finally, some conclusions are obtained from the\nanalysis to target on gaps that must be treated in the future.\n", "link": "http://arxiv.org/abs/2501.08726v1", "date": "2025-01-15", "relevancy": 1.3558, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4622}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4492}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Allocation%20in%20Mobile%20Robot%20Fleets%3A%20A%20review&body=Title%3A%20Task%20Allocation%20in%20Mobile%20Robot%20Fleets%3A%20A%20review%0AAuthor%3A%20Andr%C3%A9s%20Meseguer%20Valenzuela%20and%20Francisco%20Blanes%20Noguera%0AAbstract%3A%20%20%20Mobile%20robot%20fleets%20are%20currently%20used%20in%20different%20scenarios%20such%20as%20medical%0Aenvironments%20or%20logistics.%20The%20management%20of%20these%20systems%20provides%20different%0Achallenges%20that%20vary%20from%20the%20control%20of%20the%20movement%20of%20each%20robot%20to%20the%0Aallocation%20of%20tasks%20to%20be%20performed.%20Task%20Allocation%20%28TA%29%20problem%20is%20a%20key%0Atopic%20for%20the%20proper%20management%20of%20mobile%20robot%20fleets%20to%20ensure%20the%0Aminimization%20of%20energy%20consumption%20and%20quantity%20of%20necessary%20robots.%20Solutions%0Aon%20this%20aspect%20are%20essential%20to%20reach%20economic%20and%20environmental%20sustainability%0Aof%20robot%20fleets%2C%20mainly%20in%20industry%20applications%20such%20as%20warehouse%20logistics.%0AThe%20minimization%20of%20energy%20consumption%20introduces%20TA%20problem%20as%20an%20optimization%0Aissue%20which%20has%20been%20treated%20in%20recent%20studies.%20This%20work%20focuses%20on%20the%0Aanalysis%20of%20current%20trends%20in%20solving%20TA%20of%20mobile%20robot%20fleets.%20Main%20TA%0Aoptimization%20algorithms%20are%20presented%2C%20including%20novel%20methods%20based%20on%0AArtificial%20Intelligence%20%28AI%29.%20Additionally%2C%20this%20work%20showcases%20most%20important%0Aresults%20extracted%20from%20simulations%2C%20including%20frameworks%20utilized%20for%20the%0Adevelopment%20of%20the%20simulations.%20Finally%2C%20some%20conclusions%20are%20obtained%20from%20the%0Aanalysis%20to%20target%20on%20gaps%20that%20must%20be%20treated%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Allocation%2520in%2520Mobile%2520Robot%2520Fleets%253A%2520A%2520review%26entry.906535625%3DAndr%25C3%25A9s%2520Meseguer%2520Valenzuela%2520and%2520Francisco%2520Blanes%2520Noguera%26entry.1292438233%3D%2520%2520Mobile%2520robot%2520fleets%2520are%2520currently%2520used%2520in%2520different%2520scenarios%2520such%2520as%2520medical%250Aenvironments%2520or%2520logistics.%2520The%2520management%2520of%2520these%2520systems%2520provides%2520different%250Achallenges%2520that%2520vary%2520from%2520the%2520control%2520of%2520the%2520movement%2520of%2520each%2520robot%2520to%2520the%250Aallocation%2520of%2520tasks%2520to%2520be%2520performed.%2520Task%2520Allocation%2520%2528TA%2529%2520problem%2520is%2520a%2520key%250Atopic%2520for%2520the%2520proper%2520management%2520of%2520mobile%2520robot%2520fleets%2520to%2520ensure%2520the%250Aminimization%2520of%2520energy%2520consumption%2520and%2520quantity%2520of%2520necessary%2520robots.%2520Solutions%250Aon%2520this%2520aspect%2520are%2520essential%2520to%2520reach%2520economic%2520and%2520environmental%2520sustainability%250Aof%2520robot%2520fleets%252C%2520mainly%2520in%2520industry%2520applications%2520such%2520as%2520warehouse%2520logistics.%250AThe%2520minimization%2520of%2520energy%2520consumption%2520introduces%2520TA%2520problem%2520as%2520an%2520optimization%250Aissue%2520which%2520has%2520been%2520treated%2520in%2520recent%2520studies.%2520This%2520work%2520focuses%2520on%2520the%250Aanalysis%2520of%2520current%2520trends%2520in%2520solving%2520TA%2520of%2520mobile%2520robot%2520fleets.%2520Main%2520TA%250Aoptimization%2520algorithms%2520are%2520presented%252C%2520including%2520novel%2520methods%2520based%2520on%250AArtificial%2520Intelligence%2520%2528AI%2529.%2520Additionally%252C%2520this%2520work%2520showcases%2520most%2520important%250Aresults%2520extracted%2520from%2520simulations%252C%2520including%2520frameworks%2520utilized%2520for%2520the%250Adevelopment%2520of%2520the%2520simulations.%2520Finally%252C%2520some%2520conclusions%2520are%2520obtained%2520from%2520the%250Aanalysis%2520to%2520target%2520on%2520gaps%2520that%2520must%2520be%2520treated%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Allocation%20in%20Mobile%20Robot%20Fleets%3A%20A%20review&entry.906535625=Andr%C3%A9s%20Meseguer%20Valenzuela%20and%20Francisco%20Blanes%20Noguera&entry.1292438233=%20%20Mobile%20robot%20fleets%20are%20currently%20used%20in%20different%20scenarios%20such%20as%20medical%0Aenvironments%20or%20logistics.%20The%20management%20of%20these%20systems%20provides%20different%0Achallenges%20that%20vary%20from%20the%20control%20of%20the%20movement%20of%20each%20robot%20to%20the%0Aallocation%20of%20tasks%20to%20be%20performed.%20Task%20Allocation%20%28TA%29%20problem%20is%20a%20key%0Atopic%20for%20the%20proper%20management%20of%20mobile%20robot%20fleets%20to%20ensure%20the%0Aminimization%20of%20energy%20consumption%20and%20quantity%20of%20necessary%20robots.%20Solutions%0Aon%20this%20aspect%20are%20essential%20to%20reach%20economic%20and%20environmental%20sustainability%0Aof%20robot%20fleets%2C%20mainly%20in%20industry%20applications%20such%20as%20warehouse%20logistics.%0AThe%20minimization%20of%20energy%20consumption%20introduces%20TA%20problem%20as%20an%20optimization%0Aissue%20which%20has%20been%20treated%20in%20recent%20studies.%20This%20work%20focuses%20on%20the%0Aanalysis%20of%20current%20trends%20in%20solving%20TA%20of%20mobile%20robot%20fleets.%20Main%20TA%0Aoptimization%20algorithms%20are%20presented%2C%20including%20novel%20methods%20based%20on%0AArtificial%20Intelligence%20%28AI%29.%20Additionally%2C%20this%20work%20showcases%20most%20important%0Aresults%20extracted%20from%20simulations%2C%20including%20frameworks%20utilized%20for%20the%0Adevelopment%20of%20the%20simulations.%20Finally%2C%20some%20conclusions%20are%20obtained%20from%20the%0Aanalysis%20to%20target%20on%20gaps%20that%20must%20be%20treated%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08726v1&entry.124074799=Read"},
{"title": "GRAPPA - A Hybrid Graph Neural Network for Predicting Pure Component\n  Vapor Pressures", "author": "Marco Hoffmann and Hans Hasse and Fabian Jirasek", "abstract": "  Although the pure component vapor pressure is one of the most important\nproperties for designing chemical processes, no broadly applicable,\nsufficiently accurate, and open-source prediction method has been available. To\novercome this, we have developed GRAPPA - a hybrid graph neural network for\npredicting vapor pressures of pure components. GRAPPA enables the prediction of\nthe vapor pressure curve of basically any organic molecule, requiring only the\nmolecular structure as input. The new model consists of three parts: A graph\nattention network for the message passing step, a pooling function that\ncaptures long-range interactions, and a prediction head that yields the\ncomponent-specific parameters of the Antoine equation, from which the vapor\npressure can readily and consistently be calculated for any temperature. We\nhave trained and evaluated GRAPPA on experimental vapor pressure data of almost\n25,000 pure components. We found excellent prediction accuracy for unseen\ncomponents, outperforming state-of-the-art group contribution methods and other\nmachine learning approaches in applicability and accuracy. The trained model\nand its code are fully disclosed, and GRAPPA is directly applicable via the\ninteractive website ml-prop.mv.rptu.de.\n", "link": "http://arxiv.org/abs/2501.08729v1", "date": "2025-01-15", "relevancy": 1.3474, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4498}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4491}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAPPA%20-%20A%20Hybrid%20Graph%20Neural%20Network%20for%20Predicting%20Pure%20Component%0A%20%20Vapor%20Pressures&body=Title%3A%20GRAPPA%20-%20A%20Hybrid%20Graph%20Neural%20Network%20for%20Predicting%20Pure%20Component%0A%20%20Vapor%20Pressures%0AAuthor%3A%20Marco%20Hoffmann%20and%20Hans%20Hasse%20and%20Fabian%20Jirasek%0AAbstract%3A%20%20%20Although%20the%20pure%20component%20vapor%20pressure%20is%20one%20of%20the%20most%20important%0Aproperties%20for%20designing%20chemical%20processes%2C%20no%20broadly%20applicable%2C%0Asufficiently%20accurate%2C%20and%20open-source%20prediction%20method%20has%20been%20available.%20To%0Aovercome%20this%2C%20we%20have%20developed%20GRAPPA%20-%20a%20hybrid%20graph%20neural%20network%20for%0Apredicting%20vapor%20pressures%20of%20pure%20components.%20GRAPPA%20enables%20the%20prediction%20of%0Athe%20vapor%20pressure%20curve%20of%20basically%20any%20organic%20molecule%2C%20requiring%20only%20the%0Amolecular%20structure%20as%20input.%20The%20new%20model%20consists%20of%20three%20parts%3A%20A%20graph%0Aattention%20network%20for%20the%20message%20passing%20step%2C%20a%20pooling%20function%20that%0Acaptures%20long-range%20interactions%2C%20and%20a%20prediction%20head%20that%20yields%20the%0Acomponent-specific%20parameters%20of%20the%20Antoine%20equation%2C%20from%20which%20the%20vapor%0Apressure%20can%20readily%20and%20consistently%20be%20calculated%20for%20any%20temperature.%20We%0Ahave%20trained%20and%20evaluated%20GRAPPA%20on%20experimental%20vapor%20pressure%20data%20of%20almost%0A25%2C000%20pure%20components.%20We%20found%20excellent%20prediction%20accuracy%20for%20unseen%0Acomponents%2C%20outperforming%20state-of-the-art%20group%20contribution%20methods%20and%20other%0Amachine%20learning%20approaches%20in%20applicability%20and%20accuracy.%20The%20trained%20model%0Aand%20its%20code%20are%20fully%20disclosed%2C%20and%20GRAPPA%20is%20directly%20applicable%20via%20the%0Ainteractive%20website%20ml-prop.mv.rptu.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAPPA%2520-%2520A%2520Hybrid%2520Graph%2520Neural%2520Network%2520for%2520Predicting%2520Pure%2520Component%250A%2520%2520Vapor%2520Pressures%26entry.906535625%3DMarco%2520Hoffmann%2520and%2520Hans%2520Hasse%2520and%2520Fabian%2520Jirasek%26entry.1292438233%3D%2520%2520Although%2520the%2520pure%2520component%2520vapor%2520pressure%2520is%2520one%2520of%2520the%2520most%2520important%250Aproperties%2520for%2520designing%2520chemical%2520processes%252C%2520no%2520broadly%2520applicable%252C%250Asufficiently%2520accurate%252C%2520and%2520open-source%2520prediction%2520method%2520has%2520been%2520available.%2520To%250Aovercome%2520this%252C%2520we%2520have%2520developed%2520GRAPPA%2520-%2520a%2520hybrid%2520graph%2520neural%2520network%2520for%250Apredicting%2520vapor%2520pressures%2520of%2520pure%2520components.%2520GRAPPA%2520enables%2520the%2520prediction%2520of%250Athe%2520vapor%2520pressure%2520curve%2520of%2520basically%2520any%2520organic%2520molecule%252C%2520requiring%2520only%2520the%250Amolecular%2520structure%2520as%2520input.%2520The%2520new%2520model%2520consists%2520of%2520three%2520parts%253A%2520A%2520graph%250Aattention%2520network%2520for%2520the%2520message%2520passing%2520step%252C%2520a%2520pooling%2520function%2520that%250Acaptures%2520long-range%2520interactions%252C%2520and%2520a%2520prediction%2520head%2520that%2520yields%2520the%250Acomponent-specific%2520parameters%2520of%2520the%2520Antoine%2520equation%252C%2520from%2520which%2520the%2520vapor%250Apressure%2520can%2520readily%2520and%2520consistently%2520be%2520calculated%2520for%2520any%2520temperature.%2520We%250Ahave%2520trained%2520and%2520evaluated%2520GRAPPA%2520on%2520experimental%2520vapor%2520pressure%2520data%2520of%2520almost%250A25%252C000%2520pure%2520components.%2520We%2520found%2520excellent%2520prediction%2520accuracy%2520for%2520unseen%250Acomponents%252C%2520outperforming%2520state-of-the-art%2520group%2520contribution%2520methods%2520and%2520other%250Amachine%2520learning%2520approaches%2520in%2520applicability%2520and%2520accuracy.%2520The%2520trained%2520model%250Aand%2520its%2520code%2520are%2520fully%2520disclosed%252C%2520and%2520GRAPPA%2520is%2520directly%2520applicable%2520via%2520the%250Ainteractive%2520website%2520ml-prop.mv.rptu.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAPPA%20-%20A%20Hybrid%20Graph%20Neural%20Network%20for%20Predicting%20Pure%20Component%0A%20%20Vapor%20Pressures&entry.906535625=Marco%20Hoffmann%20and%20Hans%20Hasse%20and%20Fabian%20Jirasek&entry.1292438233=%20%20Although%20the%20pure%20component%20vapor%20pressure%20is%20one%20of%20the%20most%20important%0Aproperties%20for%20designing%20chemical%20processes%2C%20no%20broadly%20applicable%2C%0Asufficiently%20accurate%2C%20and%20open-source%20prediction%20method%20has%20been%20available.%20To%0Aovercome%20this%2C%20we%20have%20developed%20GRAPPA%20-%20a%20hybrid%20graph%20neural%20network%20for%0Apredicting%20vapor%20pressures%20of%20pure%20components.%20GRAPPA%20enables%20the%20prediction%20of%0Athe%20vapor%20pressure%20curve%20of%20basically%20any%20organic%20molecule%2C%20requiring%20only%20the%0Amolecular%20structure%20as%20input.%20The%20new%20model%20consists%20of%20three%20parts%3A%20A%20graph%0Aattention%20network%20for%20the%20message%20passing%20step%2C%20a%20pooling%20function%20that%0Acaptures%20long-range%20interactions%2C%20and%20a%20prediction%20head%20that%20yields%20the%0Acomponent-specific%20parameters%20of%20the%20Antoine%20equation%2C%20from%20which%20the%20vapor%0Apressure%20can%20readily%20and%20consistently%20be%20calculated%20for%20any%20temperature.%20We%0Ahave%20trained%20and%20evaluated%20GRAPPA%20on%20experimental%20vapor%20pressure%20data%20of%20almost%0A25%2C000%20pure%20components.%20We%20found%20excellent%20prediction%20accuracy%20for%20unseen%0Acomponents%2C%20outperforming%20state-of-the-art%20group%20contribution%20methods%20and%20other%0Amachine%20learning%20approaches%20in%20applicability%20and%20accuracy.%20The%20trained%20model%0Aand%20its%20code%20are%20fully%20disclosed%2C%20and%20GRAPPA%20is%20directly%20applicable%20via%20the%0Ainteractive%20website%20ml-prop.mv.rptu.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08729v1&entry.124074799=Read"},
{"title": "Metric Space Magnitude for Evaluating the Diversity of Latent\n  Representations", "author": "Katharina Limbeck and Rayna Andreeva and Rik Sarkar and Bastian Rieck", "abstract": "  The magnitude of a metric space is a novel invariant that provides a measure\nof the 'effective size' of a space across multiple scales, while also capturing\nnumerous geometrical properties, such as curvature, density, or entropy. We\ndevelop a family of magnitude-based measures of the intrinsic diversity of\nlatent representations, formalising a novel notion of dissimilarity between\nmagnitude functions of finite metric spaces. Our measures are provably stable\nunder perturbations of the data, can be efficiently calculated, and enable a\nrigorous multi-scale characterisation and comparison of latent representations.\nWe show their utility and superior performance across different domains and\ntasks, including (i) the automated estimation of diversity, (ii) the detection\nof mode collapse, and (iii) the evaluation of generative models for text,\nimage, and graph data.\n", "link": "http://arxiv.org/abs/2311.16054v5", "date": "2025-01-15", "relevancy": 1.4149, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4705}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metric%20Space%20Magnitude%20for%20Evaluating%20the%20Diversity%20of%20Latent%0A%20%20Representations&body=Title%3A%20Metric%20Space%20Magnitude%20for%20Evaluating%20the%20Diversity%20of%20Latent%0A%20%20Representations%0AAuthor%3A%20Katharina%20Limbeck%20and%20Rayna%20Andreeva%20and%20Rik%20Sarkar%20and%20Bastian%20Rieck%0AAbstract%3A%20%20%20The%20magnitude%20of%20a%20metric%20space%20is%20a%20novel%20invariant%20that%20provides%20a%20measure%0Aof%20the%20%27effective%20size%27%20of%20a%20space%20across%20multiple%20scales%2C%20while%20also%20capturing%0Anumerous%20geometrical%20properties%2C%20such%20as%20curvature%2C%20density%2C%20or%20entropy.%20We%0Adevelop%20a%20family%20of%20magnitude-based%20measures%20of%20the%20intrinsic%20diversity%20of%0Alatent%20representations%2C%20formalising%20a%20novel%20notion%20of%20dissimilarity%20between%0Amagnitude%20functions%20of%20finite%20metric%20spaces.%20Our%20measures%20are%20provably%20stable%0Aunder%20perturbations%20of%20the%20data%2C%20can%20be%20efficiently%20calculated%2C%20and%20enable%20a%0Arigorous%20multi-scale%20characterisation%20and%20comparison%20of%20latent%20representations.%0AWe%20show%20their%20utility%20and%20superior%20performance%20across%20different%20domains%20and%0Atasks%2C%20including%20%28i%29%20the%20automated%20estimation%20of%20diversity%2C%20%28ii%29%20the%20detection%0Aof%20mode%20collapse%2C%20and%20%28iii%29%20the%20evaluation%20of%20generative%20models%20for%20text%2C%0Aimage%2C%20and%20graph%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16054v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetric%2520Space%2520Magnitude%2520for%2520Evaluating%2520the%2520Diversity%2520of%2520Latent%250A%2520%2520Representations%26entry.906535625%3DKatharina%2520Limbeck%2520and%2520Rayna%2520Andreeva%2520and%2520Rik%2520Sarkar%2520and%2520Bastian%2520Rieck%26entry.1292438233%3D%2520%2520The%2520magnitude%2520of%2520a%2520metric%2520space%2520is%2520a%2520novel%2520invariant%2520that%2520provides%2520a%2520measure%250Aof%2520the%2520%2527effective%2520size%2527%2520of%2520a%2520space%2520across%2520multiple%2520scales%252C%2520while%2520also%2520capturing%250Anumerous%2520geometrical%2520properties%252C%2520such%2520as%2520curvature%252C%2520density%252C%2520or%2520entropy.%2520We%250Adevelop%2520a%2520family%2520of%2520magnitude-based%2520measures%2520of%2520the%2520intrinsic%2520diversity%2520of%250Alatent%2520representations%252C%2520formalising%2520a%2520novel%2520notion%2520of%2520dissimilarity%2520between%250Amagnitude%2520functions%2520of%2520finite%2520metric%2520spaces.%2520Our%2520measures%2520are%2520provably%2520stable%250Aunder%2520perturbations%2520of%2520the%2520data%252C%2520can%2520be%2520efficiently%2520calculated%252C%2520and%2520enable%2520a%250Arigorous%2520multi-scale%2520characterisation%2520and%2520comparison%2520of%2520latent%2520representations.%250AWe%2520show%2520their%2520utility%2520and%2520superior%2520performance%2520across%2520different%2520domains%2520and%250Atasks%252C%2520including%2520%2528i%2529%2520the%2520automated%2520estimation%2520of%2520diversity%252C%2520%2528ii%2529%2520the%2520detection%250Aof%2520mode%2520collapse%252C%2520and%2520%2528iii%2529%2520the%2520evaluation%2520of%2520generative%2520models%2520for%2520text%252C%250Aimage%252C%2520and%2520graph%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16054v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metric%20Space%20Magnitude%20for%20Evaluating%20the%20Diversity%20of%20Latent%0A%20%20Representations&entry.906535625=Katharina%20Limbeck%20and%20Rayna%20Andreeva%20and%20Rik%20Sarkar%20and%20Bastian%20Rieck&entry.1292438233=%20%20The%20magnitude%20of%20a%20metric%20space%20is%20a%20novel%20invariant%20that%20provides%20a%20measure%0Aof%20the%20%27effective%20size%27%20of%20a%20space%20across%20multiple%20scales%2C%20while%20also%20capturing%0Anumerous%20geometrical%20properties%2C%20such%20as%20curvature%2C%20density%2C%20or%20entropy.%20We%0Adevelop%20a%20family%20of%20magnitude-based%20measures%20of%20the%20intrinsic%20diversity%20of%0Alatent%20representations%2C%20formalising%20a%20novel%20notion%20of%20dissimilarity%20between%0Amagnitude%20functions%20of%20finite%20metric%20spaces.%20Our%20measures%20are%20provably%20stable%0Aunder%20perturbations%20of%20the%20data%2C%20can%20be%20efficiently%20calculated%2C%20and%20enable%20a%0Arigorous%20multi-scale%20characterisation%20and%20comparison%20of%20latent%20representations.%0AWe%20show%20their%20utility%20and%20superior%20performance%20across%20different%20domains%20and%0Atasks%2C%20including%20%28i%29%20the%20automated%20estimation%20of%20diversity%2C%20%28ii%29%20the%20detection%0Aof%20mode%20collapse%2C%20and%20%28iii%29%20the%20evaluation%20of%20generative%20models%20for%20text%2C%0Aimage%2C%20and%20graph%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16054v5&entry.124074799=Read"},
{"title": "SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic\n  Emphasis", "author": "Xiangyue Zhang and Jianfang Li and Jiaxu Zhang and Ziqiang Dang and Jianqiang Ren and Liefeng Bo and Zhigang Tu", "abstract": "  A good co-speech motion generation cannot be achieved without a careful\nintegration of common rhythmic motion and rare yet essential semantic motion.\nIn this work, we propose SemTalk for holistic co-speech motion generation with\nframe-level semantic emphasis. Our key insight is to separately learn general\nmotions and sparse motions, and then adaptively fuse them. In particular,\nrhythmic consistency learning is explored to establish rhythm-related base\nmotion, ensuring a coherent foundation that synchronizes gestures with the\nspeech rhythm. Subsequently, textit{semantic emphasis learning is designed to\ngenerate semantic-aware sparse motion, focusing on frame-level semantic cues.\nFinally, to integrate sparse motion into the base motion and generate\nsemantic-emphasized co-speech gestures, we further leverage a learned semantic\nscore for adaptive synthesis. Qualitative and quantitative comparisons on two\npublic datasets demonstrate that our method outperforms the state-of-the-art,\ndelivering high-quality co-speech motion with enhanced semantic richness over a\nstable base motion.\n", "link": "http://arxiv.org/abs/2412.16563v2", "date": "2025-01-15", "relevancy": 1.6466, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5553}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5503}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemTalk%3A%20Holistic%20Co-speech%20Motion%20Generation%20with%20Frame-level%20Semantic%0A%20%20Emphasis&body=Title%3A%20SemTalk%3A%20Holistic%20Co-speech%20Motion%20Generation%20with%20Frame-level%20Semantic%0A%20%20Emphasis%0AAuthor%3A%20Xiangyue%20Zhang%20and%20Jianfang%20Li%20and%20Jiaxu%20Zhang%20and%20Ziqiang%20Dang%20and%20Jianqiang%20Ren%20and%20Liefeng%20Bo%20and%20Zhigang%20Tu%0AAbstract%3A%20%20%20A%20good%20co-speech%20motion%20generation%20cannot%20be%20achieved%20without%20a%20careful%0Aintegration%20of%20common%20rhythmic%20motion%20and%20rare%20yet%20essential%20semantic%20motion.%0AIn%20this%20work%2C%20we%20propose%20SemTalk%20for%20holistic%20co-speech%20motion%20generation%20with%0Aframe-level%20semantic%20emphasis.%20Our%20key%20insight%20is%20to%20separately%20learn%20general%0Amotions%20and%20sparse%20motions%2C%20and%20then%20adaptively%20fuse%20them.%20In%20particular%2C%0Arhythmic%20consistency%20learning%20is%20explored%20to%20establish%20rhythm-related%20base%0Amotion%2C%20ensuring%20a%20coherent%20foundation%20that%20synchronizes%20gestures%20with%20the%0Aspeech%20rhythm.%20Subsequently%2C%20textit%7Bsemantic%20emphasis%20learning%20is%20designed%20to%0Agenerate%20semantic-aware%20sparse%20motion%2C%20focusing%20on%20frame-level%20semantic%20cues.%0AFinally%2C%20to%20integrate%20sparse%20motion%20into%20the%20base%20motion%20and%20generate%0Asemantic-emphasized%20co-speech%20gestures%2C%20we%20further%20leverage%20a%20learned%20semantic%0Ascore%20for%20adaptive%20synthesis.%20Qualitative%20and%20quantitative%20comparisons%20on%20two%0Apublic%20datasets%20demonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%2C%0Adelivering%20high-quality%20co-speech%20motion%20with%20enhanced%20semantic%20richness%20over%20a%0Astable%20base%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16563v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemTalk%253A%2520Holistic%2520Co-speech%2520Motion%2520Generation%2520with%2520Frame-level%2520Semantic%250A%2520%2520Emphasis%26entry.906535625%3DXiangyue%2520Zhang%2520and%2520Jianfang%2520Li%2520and%2520Jiaxu%2520Zhang%2520and%2520Ziqiang%2520Dang%2520and%2520Jianqiang%2520Ren%2520and%2520Liefeng%2520Bo%2520and%2520Zhigang%2520Tu%26entry.1292438233%3D%2520%2520A%2520good%2520co-speech%2520motion%2520generation%2520cannot%2520be%2520achieved%2520without%2520a%2520careful%250Aintegration%2520of%2520common%2520rhythmic%2520motion%2520and%2520rare%2520yet%2520essential%2520semantic%2520motion.%250AIn%2520this%2520work%252C%2520we%2520propose%2520SemTalk%2520for%2520holistic%2520co-speech%2520motion%2520generation%2520with%250Aframe-level%2520semantic%2520emphasis.%2520Our%2520key%2520insight%2520is%2520to%2520separately%2520learn%2520general%250Amotions%2520and%2520sparse%2520motions%252C%2520and%2520then%2520adaptively%2520fuse%2520them.%2520In%2520particular%252C%250Arhythmic%2520consistency%2520learning%2520is%2520explored%2520to%2520establish%2520rhythm-related%2520base%250Amotion%252C%2520ensuring%2520a%2520coherent%2520foundation%2520that%2520synchronizes%2520gestures%2520with%2520the%250Aspeech%2520rhythm.%2520Subsequently%252C%2520textit%257Bsemantic%2520emphasis%2520learning%2520is%2520designed%2520to%250Agenerate%2520semantic-aware%2520sparse%2520motion%252C%2520focusing%2520on%2520frame-level%2520semantic%2520cues.%250AFinally%252C%2520to%2520integrate%2520sparse%2520motion%2520into%2520the%2520base%2520motion%2520and%2520generate%250Asemantic-emphasized%2520co-speech%2520gestures%252C%2520we%2520further%2520leverage%2520a%2520learned%2520semantic%250Ascore%2520for%2520adaptive%2520synthesis.%2520Qualitative%2520and%2520quantitative%2520comparisons%2520on%2520two%250Apublic%2520datasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%252C%250Adelivering%2520high-quality%2520co-speech%2520motion%2520with%2520enhanced%2520semantic%2520richness%2520over%2520a%250Astable%2520base%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16563v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemTalk%3A%20Holistic%20Co-speech%20Motion%20Generation%20with%20Frame-level%20Semantic%0A%20%20Emphasis&entry.906535625=Xiangyue%20Zhang%20and%20Jianfang%20Li%20and%20Jiaxu%20Zhang%20and%20Ziqiang%20Dang%20and%20Jianqiang%20Ren%20and%20Liefeng%20Bo%20and%20Zhigang%20Tu&entry.1292438233=%20%20A%20good%20co-speech%20motion%20generation%20cannot%20be%20achieved%20without%20a%20careful%0Aintegration%20of%20common%20rhythmic%20motion%20and%20rare%20yet%20essential%20semantic%20motion.%0AIn%20this%20work%2C%20we%20propose%20SemTalk%20for%20holistic%20co-speech%20motion%20generation%20with%0Aframe-level%20semantic%20emphasis.%20Our%20key%20insight%20is%20to%20separately%20learn%20general%0Amotions%20and%20sparse%20motions%2C%20and%20then%20adaptively%20fuse%20them.%20In%20particular%2C%0Arhythmic%20consistency%20learning%20is%20explored%20to%20establish%20rhythm-related%20base%0Amotion%2C%20ensuring%20a%20coherent%20foundation%20that%20synchronizes%20gestures%20with%20the%0Aspeech%20rhythm.%20Subsequently%2C%20textit%7Bsemantic%20emphasis%20learning%20is%20designed%20to%0Agenerate%20semantic-aware%20sparse%20motion%2C%20focusing%20on%20frame-level%20semantic%20cues.%0AFinally%2C%20to%20integrate%20sparse%20motion%20into%20the%20base%20motion%20and%20generate%0Asemantic-emphasized%20co-speech%20gestures%2C%20we%20further%20leverage%20a%20learned%20semantic%0Ascore%20for%20adaptive%20synthesis.%20Qualitative%20and%20quantitative%20comparisons%20on%20two%0Apublic%20datasets%20demonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%2C%0Adelivering%20high-quality%20co-speech%20motion%20with%20enhanced%20semantic%20richness%20over%20a%0Astable%20base%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16563v2&entry.124074799=Read"},
{"title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations", "author": "Chen Chen and Xueluan Gong and Ziyao Liu and Weifeng Jiang and Si Qi Goh and Kwok-Yan Lam", "abstract": "  AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation.\n", "link": "http://arxiv.org/abs/2408.12935v3", "date": "2025-01-15", "relevancy": 1.3282, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4672}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4371}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trustworthy%2C%20Responsible%2C%20and%20Safe%20AI%3A%20A%20Comprehensive%20Architectural%0A%20%20Framework%20for%20AI%20Safety%20with%20Challenges%20and%20Mitigations&body=Title%3A%20Trustworthy%2C%20Responsible%2C%20and%20Safe%20AI%3A%20A%20Comprehensive%20Architectural%0A%20%20Framework%20for%20AI%20Safety%20with%20Challenges%20and%20Mitigations%0AAuthor%3A%20Chen%20Chen%20and%20Xueluan%20Gong%20and%20Ziyao%20Liu%20and%20Weifeng%20Jiang%20and%20Si%20Qi%20Goh%20and%20Kwok-Yan%20Lam%0AAbstract%3A%20%20%20AI%20Safety%20is%20an%20emerging%20area%20of%20critical%20importance%20to%20the%20safe%20adoption%20and%0Adeployment%20of%20AI%20systems.%20With%20the%20rapid%20proliferation%20of%20AI%20and%20especially%0Awith%20the%20recent%20advancement%20of%20Generative%20AI%20%28or%20GAI%29%2C%20the%20technology%20ecosystem%0Abehind%20the%20design%2C%20development%2C%20adoption%2C%20and%20deployment%20of%20AI%20systems%20has%0Adrastically%20changed%2C%20broadening%20the%20scope%20of%20AI%20Safety%20to%20address%20impacts%20on%0Apublic%20safety%20and%20national%20security.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aarchitectural%20framework%20for%20understanding%20and%20analyzing%20AI%20Safety%3B%20defining%20its%0Acharacteristics%20from%20three%20perspectives%3A%20Trustworthy%20AI%2C%20Responsible%20AI%2C%20and%0ASafe%20AI.%20We%20provide%20an%20extensive%20review%20of%20current%20research%20and%20advancements%20in%0AAI%20safety%20from%20these%20perspectives%2C%20highlighting%20their%20key%20challenges%20and%0Amitigation%20approaches.%20Through%20examples%20from%20state-of-the-art%20technologies%2C%0Aparticularly%20Large%20Language%20Models%20%28LLMs%29%2C%20we%20present%20innovative%20mechanism%2C%0Amethodologies%2C%20and%20techniques%20for%20designing%20and%20testing%20AI%20safety.%20Our%20goal%20is%0Ato%20promote%20advancement%20in%20AI%20safety%20research%2C%20and%20ultimately%20enhance%20people%27s%0Atrust%20in%20digital%20transformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12935v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrustworthy%252C%2520Responsible%252C%2520and%2520Safe%2520AI%253A%2520A%2520Comprehensive%2520Architectural%250A%2520%2520Framework%2520for%2520AI%2520Safety%2520with%2520Challenges%2520and%2520Mitigations%26entry.906535625%3DChen%2520Chen%2520and%2520Xueluan%2520Gong%2520and%2520Ziyao%2520Liu%2520and%2520Weifeng%2520Jiang%2520and%2520Si%2520Qi%2520Goh%2520and%2520Kwok-Yan%2520Lam%26entry.1292438233%3D%2520%2520AI%2520Safety%2520is%2520an%2520emerging%2520area%2520of%2520critical%2520importance%2520to%2520the%2520safe%2520adoption%2520and%250Adeployment%2520of%2520AI%2520systems.%2520With%2520the%2520rapid%2520proliferation%2520of%2520AI%2520and%2520especially%250Awith%2520the%2520recent%2520advancement%2520of%2520Generative%2520AI%2520%2528or%2520GAI%2529%252C%2520the%2520technology%2520ecosystem%250Abehind%2520the%2520design%252C%2520development%252C%2520adoption%252C%2520and%2520deployment%2520of%2520AI%2520systems%2520has%250Adrastically%2520changed%252C%2520broadening%2520the%2520scope%2520of%2520AI%2520Safety%2520to%2520address%2520impacts%2520on%250Apublic%2520safety%2520and%2520national%2520security.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aarchitectural%2520framework%2520for%2520understanding%2520and%2520analyzing%2520AI%2520Safety%253B%2520defining%2520its%250Acharacteristics%2520from%2520three%2520perspectives%253A%2520Trustworthy%2520AI%252C%2520Responsible%2520AI%252C%2520and%250ASafe%2520AI.%2520We%2520provide%2520an%2520extensive%2520review%2520of%2520current%2520research%2520and%2520advancements%2520in%250AAI%2520safety%2520from%2520these%2520perspectives%252C%2520highlighting%2520their%2520key%2520challenges%2520and%250Amitigation%2520approaches.%2520Through%2520examples%2520from%2520state-of-the-art%2520technologies%252C%250Aparticularly%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520we%2520present%2520innovative%2520mechanism%252C%250Amethodologies%252C%2520and%2520techniques%2520for%2520designing%2520and%2520testing%2520AI%2520safety.%2520Our%2520goal%2520is%250Ato%2520promote%2520advancement%2520in%2520AI%2520safety%2520research%252C%2520and%2520ultimately%2520enhance%2520people%2527s%250Atrust%2520in%2520digital%2520transformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12935v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trustworthy%2C%20Responsible%2C%20and%20Safe%20AI%3A%20A%20Comprehensive%20Architectural%0A%20%20Framework%20for%20AI%20Safety%20with%20Challenges%20and%20Mitigations&entry.906535625=Chen%20Chen%20and%20Xueluan%20Gong%20and%20Ziyao%20Liu%20and%20Weifeng%20Jiang%20and%20Si%20Qi%20Goh%20and%20Kwok-Yan%20Lam&entry.1292438233=%20%20AI%20Safety%20is%20an%20emerging%20area%20of%20critical%20importance%20to%20the%20safe%20adoption%20and%0Adeployment%20of%20AI%20systems.%20With%20the%20rapid%20proliferation%20of%20AI%20and%20especially%0Awith%20the%20recent%20advancement%20of%20Generative%20AI%20%28or%20GAI%29%2C%20the%20technology%20ecosystem%0Abehind%20the%20design%2C%20development%2C%20adoption%2C%20and%20deployment%20of%20AI%20systems%20has%0Adrastically%20changed%2C%20broadening%20the%20scope%20of%20AI%20Safety%20to%20address%20impacts%20on%0Apublic%20safety%20and%20national%20security.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aarchitectural%20framework%20for%20understanding%20and%20analyzing%20AI%20Safety%3B%20defining%20its%0Acharacteristics%20from%20three%20perspectives%3A%20Trustworthy%20AI%2C%20Responsible%20AI%2C%20and%0ASafe%20AI.%20We%20provide%20an%20extensive%20review%20of%20current%20research%20and%20advancements%20in%0AAI%20safety%20from%20these%20perspectives%2C%20highlighting%20their%20key%20challenges%20and%0Amitigation%20approaches.%20Through%20examples%20from%20state-of-the-art%20technologies%2C%0Aparticularly%20Large%20Language%20Models%20%28LLMs%29%2C%20we%20present%20innovative%20mechanism%2C%0Amethodologies%2C%20and%20techniques%20for%20designing%20and%20testing%20AI%20safety.%20Our%20goal%20is%0Ato%20promote%20advancement%20in%20AI%20safety%20research%2C%20and%20ultimately%20enhance%20people%27s%0Atrust%20in%20digital%20transformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12935v3&entry.124074799=Read"},
{"title": "When No-Reference Image Quality Models Meet MAP Estimation in Diffusion\n  Latents", "author": "Weixia Zhang and Dingquan Li and Guangtao Zhai and Xiaokang Yang and Kede Ma", "abstract": "  Contemporary no-reference image quality assessment (NR-IQA) models can\neffectively quantify perceived image quality, often achieving strong\ncorrelations with human perceptual scores on standard IQA benchmarks. Yet,\nlimited efforts have been devoted to treating NR-IQA models as natural image\npriors for real-world image enhancement, and consequently comparing them from a\nperceptual optimization standpoint. In this work, we show -- for the first time\n-- that NR-IQA models can be plugged into the maximum a posteriori (MAP)\nestimation framework for image enhancement. This is achieved by performing\ngradient ascent in the diffusion latent space rather than in the raw pixel\ndomain, leveraging a pretrained differentiable and bijective diffusion process.\nLikely, different NR-IQA models lead to different enhanced outputs, which in\nturn provides a new computational means of comparing them. Unlike conventional\ncorrelation-based measures, our comparison method offers complementary insights\ninto the respective strengths and weaknesses of the competing NR-IQA models in\nperceptual optimization scenarios. Additionally, we aim to improve the\nbest-performing NR-IQA model in diffusion latent MAP estimation by\nincorporating the advantages of other top-performing methods. The resulting\nmodel delivers noticeably better results in enhancing real-world images\nafflicted by unknown and complex distortions, all preserving a high degree of\nimage fidelity.\n", "link": "http://arxiv.org/abs/2403.06406v2", "date": "2025-01-15", "relevancy": 1.2304, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6498}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6023}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20No-Reference%20Image%20Quality%20Models%20Meet%20MAP%20Estimation%20in%20Diffusion%0A%20%20Latents&body=Title%3A%20When%20No-Reference%20Image%20Quality%20Models%20Meet%20MAP%20Estimation%20in%20Diffusion%0A%20%20Latents%0AAuthor%3A%20Weixia%20Zhang%20and%20Dingquan%20Li%20and%20Guangtao%20Zhai%20and%20Xiaokang%20Yang%20and%20Kede%20Ma%0AAbstract%3A%20%20%20Contemporary%20no-reference%20image%20quality%20assessment%20%28NR-IQA%29%20models%20can%0Aeffectively%20quantify%20perceived%20image%20quality%2C%20often%20achieving%20strong%0Acorrelations%20with%20human%20perceptual%20scores%20on%20standard%20IQA%20benchmarks.%20Yet%2C%0Alimited%20efforts%20have%20been%20devoted%20to%20treating%20NR-IQA%20models%20as%20natural%20image%0Apriors%20for%20real-world%20image%20enhancement%2C%20and%20consequently%20comparing%20them%20from%20a%0Aperceptual%20optimization%20standpoint.%20In%20this%20work%2C%20we%20show%20--%20for%20the%20first%20time%0A--%20that%20NR-IQA%20models%20can%20be%20plugged%20into%20the%20maximum%20a%20posteriori%20%28MAP%29%0Aestimation%20framework%20for%20image%20enhancement.%20This%20is%20achieved%20by%20performing%0Agradient%20ascent%20in%20the%20diffusion%20latent%20space%20rather%20than%20in%20the%20raw%20pixel%0Adomain%2C%20leveraging%20a%20pretrained%20differentiable%20and%20bijective%20diffusion%20process.%0ALikely%2C%20different%20NR-IQA%20models%20lead%20to%20different%20enhanced%20outputs%2C%20which%20in%0Aturn%20provides%20a%20new%20computational%20means%20of%20comparing%20them.%20Unlike%20conventional%0Acorrelation-based%20measures%2C%20our%20comparison%20method%20offers%20complementary%20insights%0Ainto%20the%20respective%20strengths%20and%20weaknesses%20of%20the%20competing%20NR-IQA%20models%20in%0Aperceptual%20optimization%20scenarios.%20Additionally%2C%20we%20aim%20to%20improve%20the%0Abest-performing%20NR-IQA%20model%20in%20diffusion%20latent%20MAP%20estimation%20by%0Aincorporating%20the%20advantages%20of%20other%20top-performing%20methods.%20The%20resulting%0Amodel%20delivers%20noticeably%20better%20results%20in%20enhancing%20real-world%20images%0Aafflicted%20by%20unknown%20and%20complex%20distortions%2C%20all%20preserving%20a%20high%20degree%20of%0Aimage%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06406v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520No-Reference%2520Image%2520Quality%2520Models%2520Meet%2520MAP%2520Estimation%2520in%2520Diffusion%250A%2520%2520Latents%26entry.906535625%3DWeixia%2520Zhang%2520and%2520Dingquan%2520Li%2520and%2520Guangtao%2520Zhai%2520and%2520Xiaokang%2520Yang%2520and%2520Kede%2520Ma%26entry.1292438233%3D%2520%2520Contemporary%2520no-reference%2520image%2520quality%2520assessment%2520%2528NR-IQA%2529%2520models%2520can%250Aeffectively%2520quantify%2520perceived%2520image%2520quality%252C%2520often%2520achieving%2520strong%250Acorrelations%2520with%2520human%2520perceptual%2520scores%2520on%2520standard%2520IQA%2520benchmarks.%2520Yet%252C%250Alimited%2520efforts%2520have%2520been%2520devoted%2520to%2520treating%2520NR-IQA%2520models%2520as%2520natural%2520image%250Apriors%2520for%2520real-world%2520image%2520enhancement%252C%2520and%2520consequently%2520comparing%2520them%2520from%2520a%250Aperceptual%2520optimization%2520standpoint.%2520In%2520this%2520work%252C%2520we%2520show%2520--%2520for%2520the%2520first%2520time%250A--%2520that%2520NR-IQA%2520models%2520can%2520be%2520plugged%2520into%2520the%2520maximum%2520a%2520posteriori%2520%2528MAP%2529%250Aestimation%2520framework%2520for%2520image%2520enhancement.%2520This%2520is%2520achieved%2520by%2520performing%250Agradient%2520ascent%2520in%2520the%2520diffusion%2520latent%2520space%2520rather%2520than%2520in%2520the%2520raw%2520pixel%250Adomain%252C%2520leveraging%2520a%2520pretrained%2520differentiable%2520and%2520bijective%2520diffusion%2520process.%250ALikely%252C%2520different%2520NR-IQA%2520models%2520lead%2520to%2520different%2520enhanced%2520outputs%252C%2520which%2520in%250Aturn%2520provides%2520a%2520new%2520computational%2520means%2520of%2520comparing%2520them.%2520Unlike%2520conventional%250Acorrelation-based%2520measures%252C%2520our%2520comparison%2520method%2520offers%2520complementary%2520insights%250Ainto%2520the%2520respective%2520strengths%2520and%2520weaknesses%2520of%2520the%2520competing%2520NR-IQA%2520models%2520in%250Aperceptual%2520optimization%2520scenarios.%2520Additionally%252C%2520we%2520aim%2520to%2520improve%2520the%250Abest-performing%2520NR-IQA%2520model%2520in%2520diffusion%2520latent%2520MAP%2520estimation%2520by%250Aincorporating%2520the%2520advantages%2520of%2520other%2520top-performing%2520methods.%2520The%2520resulting%250Amodel%2520delivers%2520noticeably%2520better%2520results%2520in%2520enhancing%2520real-world%2520images%250Aafflicted%2520by%2520unknown%2520and%2520complex%2520distortions%252C%2520all%2520preserving%2520a%2520high%2520degree%2520of%250Aimage%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06406v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20No-Reference%20Image%20Quality%20Models%20Meet%20MAP%20Estimation%20in%20Diffusion%0A%20%20Latents&entry.906535625=Weixia%20Zhang%20and%20Dingquan%20Li%20and%20Guangtao%20Zhai%20and%20Xiaokang%20Yang%20and%20Kede%20Ma&entry.1292438233=%20%20Contemporary%20no-reference%20image%20quality%20assessment%20%28NR-IQA%29%20models%20can%0Aeffectively%20quantify%20perceived%20image%20quality%2C%20often%20achieving%20strong%0Acorrelations%20with%20human%20perceptual%20scores%20on%20standard%20IQA%20benchmarks.%20Yet%2C%0Alimited%20efforts%20have%20been%20devoted%20to%20treating%20NR-IQA%20models%20as%20natural%20image%0Apriors%20for%20real-world%20image%20enhancement%2C%20and%20consequently%20comparing%20them%20from%20a%0Aperceptual%20optimization%20standpoint.%20In%20this%20work%2C%20we%20show%20--%20for%20the%20first%20time%0A--%20that%20NR-IQA%20models%20can%20be%20plugged%20into%20the%20maximum%20a%20posteriori%20%28MAP%29%0Aestimation%20framework%20for%20image%20enhancement.%20This%20is%20achieved%20by%20performing%0Agradient%20ascent%20in%20the%20diffusion%20latent%20space%20rather%20than%20in%20the%20raw%20pixel%0Adomain%2C%20leveraging%20a%20pretrained%20differentiable%20and%20bijective%20diffusion%20process.%0ALikely%2C%20different%20NR-IQA%20models%20lead%20to%20different%20enhanced%20outputs%2C%20which%20in%0Aturn%20provides%20a%20new%20computational%20means%20of%20comparing%20them.%20Unlike%20conventional%0Acorrelation-based%20measures%2C%20our%20comparison%20method%20offers%20complementary%20insights%0Ainto%20the%20respective%20strengths%20and%20weaknesses%20of%20the%20competing%20NR-IQA%20models%20in%0Aperceptual%20optimization%20scenarios.%20Additionally%2C%20we%20aim%20to%20improve%20the%0Abest-performing%20NR-IQA%20model%20in%20diffusion%20latent%20MAP%20estimation%20by%0Aincorporating%20the%20advantages%20of%20other%20top-performing%20methods.%20The%20resulting%0Amodel%20delivers%20noticeably%20better%20results%20in%20enhancing%20real-world%20images%0Aafflicted%20by%20unknown%20and%20complex%20distortions%2C%20all%20preserving%20a%20high%20degree%20of%0Aimage%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06406v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


