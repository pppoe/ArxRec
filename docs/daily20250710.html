<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250709.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian\n  Splatting", "author": "Wijayathunga W. M. R. D. B", "abstract": "  We propose a novel framework that enhances non-rigid 3D model deformations by\nbridging mesh representations with 3D Gaussian splatting. While traditional\nGaussian splatting delivers fast, real-time radiance-field rendering, its\npost-editing capabilities and support for large-scale, non-rigid deformations\nremain limited. Our method addresses these challenges by embedding Gaussian\nkernels directly onto explicit mesh surfaces. This allows the mesh's inherent\ntopological and geometric priors to guide intuitive editing operations -- such\nas moving, scaling, and rotating individual 3D components -- and enables\ncomplex deformations like bending and stretching. This work paves the way for\nmore flexible 3D content-creation workflows in applications spanning virtual\nreality, character animation, and interactive design.\n", "link": "http://arxiv.org/abs/2507.07000v1", "date": "2025-07-09", "relevancy": 3.3879, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6797}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6775}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20non-Rigid%203D%20Model%20Deformations%20Using%20Mesh-based%20Gaussian%0A%20%20Splatting&body=Title%3A%20Enhancing%20non-Rigid%203D%20Model%20Deformations%20Using%20Mesh-based%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Wijayathunga%20W.%20M.%20R.%20D.%20B%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20framework%20that%20enhances%20non-rigid%203D%20model%20deformations%20by%0Abridging%20mesh%20representations%20with%203D%20Gaussian%20splatting.%20While%20traditional%0AGaussian%20splatting%20delivers%20fast%2C%20real-time%20radiance-field%20rendering%2C%20its%0Apost-editing%20capabilities%20and%20support%20for%20large-scale%2C%20non-rigid%20deformations%0Aremain%20limited.%20Our%20method%20addresses%20these%20challenges%20by%20embedding%20Gaussian%0Akernels%20directly%20onto%20explicit%20mesh%20surfaces.%20This%20allows%20the%20mesh%27s%20inherent%0Atopological%20and%20geometric%20priors%20to%20guide%20intuitive%20editing%20operations%20--%20such%0Aas%20moving%2C%20scaling%2C%20and%20rotating%20individual%203D%20components%20--%20and%20enables%0Acomplex%20deformations%20like%20bending%20and%20stretching.%20This%20work%20paves%20the%20way%20for%0Amore%20flexible%203D%20content-creation%20workflows%20in%20applications%20spanning%20virtual%0Areality%2C%20character%20animation%2C%20and%20interactive%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520non-Rigid%25203D%2520Model%2520Deformations%2520Using%2520Mesh-based%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DWijayathunga%2520W.%2520M.%2520R.%2520D.%2520B%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520framework%2520that%2520enhances%2520non-rigid%25203D%2520model%2520deformations%2520by%250Abridging%2520mesh%2520representations%2520with%25203D%2520Gaussian%2520splatting.%2520While%2520traditional%250AGaussian%2520splatting%2520delivers%2520fast%252C%2520real-time%2520radiance-field%2520rendering%252C%2520its%250Apost-editing%2520capabilities%2520and%2520support%2520for%2520large-scale%252C%2520non-rigid%2520deformations%250Aremain%2520limited.%2520Our%2520method%2520addresses%2520these%2520challenges%2520by%2520embedding%2520Gaussian%250Akernels%2520directly%2520onto%2520explicit%2520mesh%2520surfaces.%2520This%2520allows%2520the%2520mesh%2527s%2520inherent%250Atopological%2520and%2520geometric%2520priors%2520to%2520guide%2520intuitive%2520editing%2520operations%2520--%2520such%250Aas%2520moving%252C%2520scaling%252C%2520and%2520rotating%2520individual%25203D%2520components%2520--%2520and%2520enables%250Acomplex%2520deformations%2520like%2520bending%2520and%2520stretching.%2520This%2520work%2520paves%2520the%2520way%2520for%250Amore%2520flexible%25203D%2520content-creation%2520workflows%2520in%2520applications%2520spanning%2520virtual%250Areality%252C%2520character%2520animation%252C%2520and%2520interactive%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20non-Rigid%203D%20Model%20Deformations%20Using%20Mesh-based%20Gaussian%0A%20%20Splatting&entry.906535625=Wijayathunga%20W.%20M.%20R.%20D.%20B&entry.1292438233=%20%20We%20propose%20a%20novel%20framework%20that%20enhances%20non-rigid%203D%20model%20deformations%20by%0Abridging%20mesh%20representations%20with%203D%20Gaussian%20splatting.%20While%20traditional%0AGaussian%20splatting%20delivers%20fast%2C%20real-time%20radiance-field%20rendering%2C%20its%0Apost-editing%20capabilities%20and%20support%20for%20large-scale%2C%20non-rigid%20deformations%0Aremain%20limited.%20Our%20method%20addresses%20these%20challenges%20by%20embedding%20Gaussian%0Akernels%20directly%20onto%20explicit%20mesh%20surfaces.%20This%20allows%20the%20mesh%27s%20inherent%0Atopological%20and%20geometric%20priors%20to%20guide%20intuitive%20editing%20operations%20--%20such%0Aas%20moving%2C%20scaling%2C%20and%20rotating%20individual%203D%20components%20--%20and%20enables%0Acomplex%20deformations%20like%20bending%20and%20stretching.%20This%20work%20paves%20the%20way%20for%0Amore%20flexible%203D%20content-creation%20workflows%20in%20applications%20spanning%20virtual%0Areality%2C%20character%20animation%2C%20and%20interactive%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07000v1&entry.124074799=Read"},
{"title": "CULTURE3D: A Large-Scale and Diverse Dataset of Cultural Landmarks and\n  Terrains for Gaussian-Based Scene Rendering", "author": "Xinyi Zheng and Steve Zhang and Weizhe Lin and Aaron Zhang and Walterio W. Mayol-Cuevas and Yunze Liu and Junxiao Shen", "abstract": "  Current state-of-the-art 3D reconstruction models face limitations in\nbuilding extra-large scale outdoor scenes, primarily due to the lack of\nsufficiently large-scale and detailed datasets. In this paper, we present a\nextra-large fine-grained dataset with 10 billion points composed of 41,006\ndrone-captured high-resolution aerial images, covering 20 diverse and\nculturally significant scenes from worldwide locations such as Cambridge Uni\nmain buildings, the Pyramids, and the Forbidden City Palace. Compared to\nexisting datasets, ours offers significantly larger scale and higher detail,\nuniquely suited for fine-grained 3D applications. Each scene contains an\naccurate spatial layout and comprehensive structural information, supporting\ndetailed 3D reconstruction tasks. By reconstructing environments using these\ndetailed images, our dataset supports multiple applications, including outputs\nin the widely adopted COLMAP format, establishing a novel benchmark for\nevaluating state-of-the-art large-scale Gaussian Splatting methods.The\ndataset's flexibility encourages innovations and supports model plug-ins,\npaving the way for future 3D breakthroughs. All datasets and code will be\nopen-sourced for community use.\n", "link": "http://arxiv.org/abs/2501.06927v3", "date": "2025-07-09", "relevancy": 3.067, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6227}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6088}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CULTURE3D%3A%20A%20Large-Scale%20and%20Diverse%20Dataset%20of%20Cultural%20Landmarks%20and%0A%20%20Terrains%20for%20Gaussian-Based%20Scene%20Rendering&body=Title%3A%20CULTURE3D%3A%20A%20Large-Scale%20and%20Diverse%20Dataset%20of%20Cultural%20Landmarks%20and%0A%20%20Terrains%20for%20Gaussian-Based%20Scene%20Rendering%0AAuthor%3A%20Xinyi%20Zheng%20and%20Steve%20Zhang%20and%20Weizhe%20Lin%20and%20Aaron%20Zhang%20and%20Walterio%20W.%20Mayol-Cuevas%20and%20Yunze%20Liu%20and%20Junxiao%20Shen%0AAbstract%3A%20%20%20Current%20state-of-the-art%203D%20reconstruction%20models%20face%20limitations%20in%0Abuilding%20extra-large%20scale%20outdoor%20scenes%2C%20primarily%20due%20to%20the%20lack%20of%0Asufficiently%20large-scale%20and%20detailed%20datasets.%20In%20this%20paper%2C%20we%20present%20a%0Aextra-large%20fine-grained%20dataset%20with%2010%20billion%20points%20composed%20of%2041%2C006%0Adrone-captured%20high-resolution%20aerial%20images%2C%20covering%2020%20diverse%20and%0Aculturally%20significant%20scenes%20from%20worldwide%20locations%20such%20as%20Cambridge%20Uni%0Amain%20buildings%2C%20the%20Pyramids%2C%20and%20the%20Forbidden%20City%20Palace.%20Compared%20to%0Aexisting%20datasets%2C%20ours%20offers%20significantly%20larger%20scale%20and%20higher%20detail%2C%0Auniquely%20suited%20for%20fine-grained%203D%20applications.%20Each%20scene%20contains%20an%0Aaccurate%20spatial%20layout%20and%20comprehensive%20structural%20information%2C%20supporting%0Adetailed%203D%20reconstruction%20tasks.%20By%20reconstructing%20environments%20using%20these%0Adetailed%20images%2C%20our%20dataset%20supports%20multiple%20applications%2C%20including%20outputs%0Ain%20the%20widely%20adopted%20COLMAP%20format%2C%20establishing%20a%20novel%20benchmark%20for%0Aevaluating%20state-of-the-art%20large-scale%20Gaussian%20Splatting%20methods.The%0Adataset%27s%20flexibility%20encourages%20innovations%20and%20supports%20model%20plug-ins%2C%0Apaving%20the%20way%20for%20future%203D%20breakthroughs.%20All%20datasets%20and%20code%20will%20be%0Aopen-sourced%20for%20community%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06927v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCULTURE3D%253A%2520A%2520Large-Scale%2520and%2520Diverse%2520Dataset%2520of%2520Cultural%2520Landmarks%2520and%250A%2520%2520Terrains%2520for%2520Gaussian-Based%2520Scene%2520Rendering%26entry.906535625%3DXinyi%2520Zheng%2520and%2520Steve%2520Zhang%2520and%2520Weizhe%2520Lin%2520and%2520Aaron%2520Zhang%2520and%2520Walterio%2520W.%2520Mayol-Cuevas%2520and%2520Yunze%2520Liu%2520and%2520Junxiao%2520Shen%26entry.1292438233%3D%2520%2520Current%2520state-of-the-art%25203D%2520reconstruction%2520models%2520face%2520limitations%2520in%250Abuilding%2520extra-large%2520scale%2520outdoor%2520scenes%252C%2520primarily%2520due%2520to%2520the%2520lack%2520of%250Asufficiently%2520large-scale%2520and%2520detailed%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Aextra-large%2520fine-grained%2520dataset%2520with%252010%2520billion%2520points%2520composed%2520of%252041%252C006%250Adrone-captured%2520high-resolution%2520aerial%2520images%252C%2520covering%252020%2520diverse%2520and%250Aculturally%2520significant%2520scenes%2520from%2520worldwide%2520locations%2520such%2520as%2520Cambridge%2520Uni%250Amain%2520buildings%252C%2520the%2520Pyramids%252C%2520and%2520the%2520Forbidden%2520City%2520Palace.%2520Compared%2520to%250Aexisting%2520datasets%252C%2520ours%2520offers%2520significantly%2520larger%2520scale%2520and%2520higher%2520detail%252C%250Auniquely%2520suited%2520for%2520fine-grained%25203D%2520applications.%2520Each%2520scene%2520contains%2520an%250Aaccurate%2520spatial%2520layout%2520and%2520comprehensive%2520structural%2520information%252C%2520supporting%250Adetailed%25203D%2520reconstruction%2520tasks.%2520By%2520reconstructing%2520environments%2520using%2520these%250Adetailed%2520images%252C%2520our%2520dataset%2520supports%2520multiple%2520applications%252C%2520including%2520outputs%250Ain%2520the%2520widely%2520adopted%2520COLMAP%2520format%252C%2520establishing%2520a%2520novel%2520benchmark%2520for%250Aevaluating%2520state-of-the-art%2520large-scale%2520Gaussian%2520Splatting%2520methods.The%250Adataset%2527s%2520flexibility%2520encourages%2520innovations%2520and%2520supports%2520model%2520plug-ins%252C%250Apaving%2520the%2520way%2520for%2520future%25203D%2520breakthroughs.%2520All%2520datasets%2520and%2520code%2520will%2520be%250Aopen-sourced%2520for%2520community%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06927v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CULTURE3D%3A%20A%20Large-Scale%20and%20Diverse%20Dataset%20of%20Cultural%20Landmarks%20and%0A%20%20Terrains%20for%20Gaussian-Based%20Scene%20Rendering&entry.906535625=Xinyi%20Zheng%20and%20Steve%20Zhang%20and%20Weizhe%20Lin%20and%20Aaron%20Zhang%20and%20Walterio%20W.%20Mayol-Cuevas%20and%20Yunze%20Liu%20and%20Junxiao%20Shen&entry.1292438233=%20%20Current%20state-of-the-art%203D%20reconstruction%20models%20face%20limitations%20in%0Abuilding%20extra-large%20scale%20outdoor%20scenes%2C%20primarily%20due%20to%20the%20lack%20of%0Asufficiently%20large-scale%20and%20detailed%20datasets.%20In%20this%20paper%2C%20we%20present%20a%0Aextra-large%20fine-grained%20dataset%20with%2010%20billion%20points%20composed%20of%2041%2C006%0Adrone-captured%20high-resolution%20aerial%20images%2C%20covering%2020%20diverse%20and%0Aculturally%20significant%20scenes%20from%20worldwide%20locations%20such%20as%20Cambridge%20Uni%0Amain%20buildings%2C%20the%20Pyramids%2C%20and%20the%20Forbidden%20City%20Palace.%20Compared%20to%0Aexisting%20datasets%2C%20ours%20offers%20significantly%20larger%20scale%20and%20higher%20detail%2C%0Auniquely%20suited%20for%20fine-grained%203D%20applications.%20Each%20scene%20contains%20an%0Aaccurate%20spatial%20layout%20and%20comprehensive%20structural%20information%2C%20supporting%0Adetailed%203D%20reconstruction%20tasks.%20By%20reconstructing%20environments%20using%20these%0Adetailed%20images%2C%20our%20dataset%20supports%20multiple%20applications%2C%20including%20outputs%0Ain%20the%20widely%20adopted%20COLMAP%20format%2C%20establishing%20a%20novel%20benchmark%20for%0Aevaluating%20state-of-the-art%20large-scale%20Gaussian%20Splatting%20methods.The%0Adataset%27s%20flexibility%20encourages%20innovations%20and%20supports%20model%20plug-ins%2C%0Apaving%20the%20way%20for%20future%203D%20breakthroughs.%20All%20datasets%20and%20code%20will%20be%0Aopen-sourced%20for%20community%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06927v3&entry.124074799=Read"},
{"title": "RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose\n  Triangulation in a Millisecond", "author": "Daniel Bermuth and Alexander Poeppel and Wolfgang Reif", "abstract": "  The integration of multi-view imaging and pose estimation represents a\nsignificant advance in computer vision applications, offering new possibilities\nfor understanding human movement and interactions. This work presents a new\nalgorithm that improves multi-view multi-person pose estimation, focusing on\nfast triangulation speeds and good generalization capabilities.\n  The approach extends to whole-body pose estimation, capturing details from\nfacial expressions to finger movements across multiple individuals and\nviewpoints. Adaptability to different settings is demonstrated through strong\nperformance across unseen datasets and configurations. To support further\nprogress in this field, all of this work is publicly accessible.\n", "link": "http://arxiv.org/abs/2503.21692v3", "date": "2025-07-09", "relevancy": 2.9438, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6065}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5894}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RapidPoseTriangulation%3A%20Multi-view%20Multi-person%20Whole-body%20Human%20Pose%0A%20%20Triangulation%20in%20a%20Millisecond&body=Title%3A%20RapidPoseTriangulation%3A%20Multi-view%20Multi-person%20Whole-body%20Human%20Pose%0A%20%20Triangulation%20in%20a%20Millisecond%0AAuthor%3A%20Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif%0AAbstract%3A%20%20%20The%20integration%20of%20multi-view%20imaging%20and%20pose%20estimation%20represents%20a%0Asignificant%20advance%20in%20computer%20vision%20applications%2C%20offering%20new%20possibilities%0Afor%20understanding%20human%20movement%20and%20interactions.%20This%20work%20presents%20a%20new%0Aalgorithm%20that%20improves%20multi-view%20multi-person%20pose%20estimation%2C%20focusing%20on%0Afast%20triangulation%20speeds%20and%20good%20generalization%20capabilities.%0A%20%20The%20approach%20extends%20to%20whole-body%20pose%20estimation%2C%20capturing%20details%20from%0Afacial%20expressions%20to%20finger%20movements%20across%20multiple%20individuals%20and%0Aviewpoints.%20Adaptability%20to%20different%20settings%20is%20demonstrated%20through%20strong%0Aperformance%20across%20unseen%20datasets%20and%20configurations.%20To%20support%20further%0Aprogress%20in%20this%20field%2C%20all%20of%20this%20work%20is%20publicly%20accessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21692v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapidPoseTriangulation%253A%2520Multi-view%2520Multi-person%2520Whole-body%2520Human%2520Pose%250A%2520%2520Triangulation%2520in%2520a%2520Millisecond%26entry.906535625%3DDaniel%2520Bermuth%2520and%2520Alexander%2520Poeppel%2520and%2520Wolfgang%2520Reif%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520multi-view%2520imaging%2520and%2520pose%2520estimation%2520represents%2520a%250Asignificant%2520advance%2520in%2520computer%2520vision%2520applications%252C%2520offering%2520new%2520possibilities%250Afor%2520understanding%2520human%2520movement%2520and%2520interactions.%2520This%2520work%2520presents%2520a%2520new%250Aalgorithm%2520that%2520improves%2520multi-view%2520multi-person%2520pose%2520estimation%252C%2520focusing%2520on%250Afast%2520triangulation%2520speeds%2520and%2520good%2520generalization%2520capabilities.%250A%2520%2520The%2520approach%2520extends%2520to%2520whole-body%2520pose%2520estimation%252C%2520capturing%2520details%2520from%250Afacial%2520expressions%2520to%2520finger%2520movements%2520across%2520multiple%2520individuals%2520and%250Aviewpoints.%2520Adaptability%2520to%2520different%2520settings%2520is%2520demonstrated%2520through%2520strong%250Aperformance%2520across%2520unseen%2520datasets%2520and%2520configurations.%2520To%2520support%2520further%250Aprogress%2520in%2520this%2520field%252C%2520all%2520of%2520this%2520work%2520is%2520publicly%2520accessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21692v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RapidPoseTriangulation%3A%20Multi-view%20Multi-person%20Whole-body%20Human%20Pose%0A%20%20Triangulation%20in%20a%20Millisecond&entry.906535625=Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif&entry.1292438233=%20%20The%20integration%20of%20multi-view%20imaging%20and%20pose%20estimation%20represents%20a%0Asignificant%20advance%20in%20computer%20vision%20applications%2C%20offering%20new%20possibilities%0Afor%20understanding%20human%20movement%20and%20interactions.%20This%20work%20presents%20a%20new%0Aalgorithm%20that%20improves%20multi-view%20multi-person%20pose%20estimation%2C%20focusing%20on%0Afast%20triangulation%20speeds%20and%20good%20generalization%20capabilities.%0A%20%20The%20approach%20extends%20to%20whole-body%20pose%20estimation%2C%20capturing%20details%20from%0Afacial%20expressions%20to%20finger%20movements%20across%20multiple%20individuals%20and%0Aviewpoints.%20Adaptability%20to%20different%20settings%20is%20demonstrated%20through%20strong%0Aperformance%20across%20unseen%20datasets%20and%20configurations.%20To%20support%20further%0Aprogress%20in%20this%20field%2C%20all%20of%20this%20work%20is%20publicly%20accessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21692v3&entry.124074799=Read"},
{"title": "Democratizing High-Fidelity Co-Speech Gesture Video Generation", "author": "Xu Yang and Shaoli Huang and Shenbo Xie and Xuelin Chen and Yifei Liu and Changxing Ding", "abstract": "  Co-speech gesture video generation aims to synthesize realistic,\naudio-aligned videos of speakers, complete with synchronized facial expressions\nand body gestures. This task presents challenges due to the significant\none-to-many mapping between audio and visual content, further complicated by\nthe scarcity of large-scale public datasets and high computational demands. We\npropose a lightweight framework that utilizes 2D full-body skeletons as an\nefficient auxiliary condition to bridge audio signals with visual outputs. Our\napproach introduces a diffusion model conditioned on fine-grained audio\nsegments and a skeleton extracted from the speaker's reference image,\npredicting skeletal motions through skeleton-audio feature fusion to ensure\nstrict audio coordination and body shape consistency. The generated skeletons\nare then fed into an off-the-shelf human video generation model with the\nspeaker's reference image to synthesize high-fidelity videos. To democratize\nresearch, we present CSG-405-the first public dataset with 405 hours of\nhigh-resolution videos across 71 speech types, annotated with 2D skeletons and\ndiverse speaker demographics. Experiments show that our method exceeds\nstate-of-the-art approaches in visual quality and synchronization while\ngeneralizing across speakers and contexts.\n", "link": "http://arxiv.org/abs/2507.06812v1", "date": "2025-07-09", "relevancy": 2.9367, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6076}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5775}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Democratizing%20High-Fidelity%20Co-Speech%20Gesture%20Video%20Generation&body=Title%3A%20Democratizing%20High-Fidelity%20Co-Speech%20Gesture%20Video%20Generation%0AAuthor%3A%20Xu%20Yang%20and%20Shaoli%20Huang%20and%20Shenbo%20Xie%20and%20Xuelin%20Chen%20and%20Yifei%20Liu%20and%20Changxing%20Ding%0AAbstract%3A%20%20%20Co-speech%20gesture%20video%20generation%20aims%20to%20synthesize%20realistic%2C%0Aaudio-aligned%20videos%20of%20speakers%2C%20complete%20with%20synchronized%20facial%20expressions%0Aand%20body%20gestures.%20This%20task%20presents%20challenges%20due%20to%20the%20significant%0Aone-to-many%20mapping%20between%20audio%20and%20visual%20content%2C%20further%20complicated%20by%0Athe%20scarcity%20of%20large-scale%20public%20datasets%20and%20high%20computational%20demands.%20We%0Apropose%20a%20lightweight%20framework%20that%20utilizes%202D%20full-body%20skeletons%20as%20an%0Aefficient%20auxiliary%20condition%20to%20bridge%20audio%20signals%20with%20visual%20outputs.%20Our%0Aapproach%20introduces%20a%20diffusion%20model%20conditioned%20on%20fine-grained%20audio%0Asegments%20and%20a%20skeleton%20extracted%20from%20the%20speaker%27s%20reference%20image%2C%0Apredicting%20skeletal%20motions%20through%20skeleton-audio%20feature%20fusion%20to%20ensure%0Astrict%20audio%20coordination%20and%20body%20shape%20consistency.%20The%20generated%20skeletons%0Aare%20then%20fed%20into%20an%20off-the-shelf%20human%20video%20generation%20model%20with%20the%0Aspeaker%27s%20reference%20image%20to%20synthesize%20high-fidelity%20videos.%20To%20democratize%0Aresearch%2C%20we%20present%20CSG-405-the%20first%20public%20dataset%20with%20405%20hours%20of%0Ahigh-resolution%20videos%20across%2071%20speech%20types%2C%20annotated%20with%202D%20skeletons%20and%0Adiverse%20speaker%20demographics.%20Experiments%20show%20that%20our%20method%20exceeds%0Astate-of-the-art%20approaches%20in%20visual%20quality%20and%20synchronization%20while%0Ageneralizing%20across%20speakers%20and%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemocratizing%2520High-Fidelity%2520Co-Speech%2520Gesture%2520Video%2520Generation%26entry.906535625%3DXu%2520Yang%2520and%2520Shaoli%2520Huang%2520and%2520Shenbo%2520Xie%2520and%2520Xuelin%2520Chen%2520and%2520Yifei%2520Liu%2520and%2520Changxing%2520Ding%26entry.1292438233%3D%2520%2520Co-speech%2520gesture%2520video%2520generation%2520aims%2520to%2520synthesize%2520realistic%252C%250Aaudio-aligned%2520videos%2520of%2520speakers%252C%2520complete%2520with%2520synchronized%2520facial%2520expressions%250Aand%2520body%2520gestures.%2520This%2520task%2520presents%2520challenges%2520due%2520to%2520the%2520significant%250Aone-to-many%2520mapping%2520between%2520audio%2520and%2520visual%2520content%252C%2520further%2520complicated%2520by%250Athe%2520scarcity%2520of%2520large-scale%2520public%2520datasets%2520and%2520high%2520computational%2520demands.%2520We%250Apropose%2520a%2520lightweight%2520framework%2520that%2520utilizes%25202D%2520full-body%2520skeletons%2520as%2520an%250Aefficient%2520auxiliary%2520condition%2520to%2520bridge%2520audio%2520signals%2520with%2520visual%2520outputs.%2520Our%250Aapproach%2520introduces%2520a%2520diffusion%2520model%2520conditioned%2520on%2520fine-grained%2520audio%250Asegments%2520and%2520a%2520skeleton%2520extracted%2520from%2520the%2520speaker%2527s%2520reference%2520image%252C%250Apredicting%2520skeletal%2520motions%2520through%2520skeleton-audio%2520feature%2520fusion%2520to%2520ensure%250Astrict%2520audio%2520coordination%2520and%2520body%2520shape%2520consistency.%2520The%2520generated%2520skeletons%250Aare%2520then%2520fed%2520into%2520an%2520off-the-shelf%2520human%2520video%2520generation%2520model%2520with%2520the%250Aspeaker%2527s%2520reference%2520image%2520to%2520synthesize%2520high-fidelity%2520videos.%2520To%2520democratize%250Aresearch%252C%2520we%2520present%2520CSG-405-the%2520first%2520public%2520dataset%2520with%2520405%2520hours%2520of%250Ahigh-resolution%2520videos%2520across%252071%2520speech%2520types%252C%2520annotated%2520with%25202D%2520skeletons%2520and%250Adiverse%2520speaker%2520demographics.%2520Experiments%2520show%2520that%2520our%2520method%2520exceeds%250Astate-of-the-art%2520approaches%2520in%2520visual%2520quality%2520and%2520synchronization%2520while%250Ageneralizing%2520across%2520speakers%2520and%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Democratizing%20High-Fidelity%20Co-Speech%20Gesture%20Video%20Generation&entry.906535625=Xu%20Yang%20and%20Shaoli%20Huang%20and%20Shenbo%20Xie%20and%20Xuelin%20Chen%20and%20Yifei%20Liu%20and%20Changxing%20Ding&entry.1292438233=%20%20Co-speech%20gesture%20video%20generation%20aims%20to%20synthesize%20realistic%2C%0Aaudio-aligned%20videos%20of%20speakers%2C%20complete%20with%20synchronized%20facial%20expressions%0Aand%20body%20gestures.%20This%20task%20presents%20challenges%20due%20to%20the%20significant%0Aone-to-many%20mapping%20between%20audio%20and%20visual%20content%2C%20further%20complicated%20by%0Athe%20scarcity%20of%20large-scale%20public%20datasets%20and%20high%20computational%20demands.%20We%0Apropose%20a%20lightweight%20framework%20that%20utilizes%202D%20full-body%20skeletons%20as%20an%0Aefficient%20auxiliary%20condition%20to%20bridge%20audio%20signals%20with%20visual%20outputs.%20Our%0Aapproach%20introduces%20a%20diffusion%20model%20conditioned%20on%20fine-grained%20audio%0Asegments%20and%20a%20skeleton%20extracted%20from%20the%20speaker%27s%20reference%20image%2C%0Apredicting%20skeletal%20motions%20through%20skeleton-audio%20feature%20fusion%20to%20ensure%0Astrict%20audio%20coordination%20and%20body%20shape%20consistency.%20The%20generated%20skeletons%0Aare%20then%20fed%20into%20an%20off-the-shelf%20human%20video%20generation%20model%20with%20the%0Aspeaker%27s%20reference%20image%20to%20synthesize%20high-fidelity%20videos.%20To%20democratize%0Aresearch%2C%20we%20present%20CSG-405-the%20first%20public%20dataset%20with%20405%20hours%20of%0Ahigh-resolution%20videos%20across%2071%20speech%20types%2C%20annotated%20with%202D%20skeletons%20and%0Adiverse%20speaker%20demographics.%20Experiments%20show%20that%20our%20method%20exceeds%0Astate-of-the-art%20approaches%20in%20visual%20quality%20and%20synchronization%20while%0Ageneralizing%20across%20speakers%20and%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06812v1&entry.124074799=Read"},
{"title": "A Principled Framework for Multi-View Contrastive Learning", "author": "Panagiotis Koromilas and Efthymios Georgiou and Giorgos Bouritsas and Theodoros Giannakopoulos and Mihalis A. Nicolaou and Yannis Panagakis", "abstract": "  Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning\n(SSL), typically relies on pairs of data views generated through augmentation.\nWhile multiple augmentations per instance (more than two) improve\ngeneralization in supervised learning, current CL methods handle additional\nviews suboptimally by simply aggregating different pairwise objectives. This\napproach suffers from four critical limitations: (L1) it utilizes multiple\noptimization terms per data point resulting to conflicting objectives, (L2) it\nfails to model all interactions across views and data points, (L3) it inherits\nfundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL\nlosses, and (L4) it prevents fully realizing the benefits of increased view\nmultiplicity observed in supervised settings. We address these limitations\nthrough two novel loss functions: MV-InfoNCE, which extends InfoNCE to\nincorporate all possible view interactions simultaneously in one term per data\npoint, and MV-DHEL, which decouples alignment from uniformity across views\nwhile scaling interaction complexity with view multiplicity. Both approaches\nare theoretically grounded - we prove they asymptotically optimize for\nalignment of all views and uniformity, providing principled extensions to\nmulti-view contrastive learning. Our empirical results on ImageNet1K and three\nother datasets demonstrate that our methods consistently outperform existing\nmulti-view approaches and effectively scale with increasing view multiplicity.\nWe also apply our objectives to multimodal data and show that, in contrast to\nother contrastive objectives, they can scale beyond just two modalities. Most\nsignificantly, ablation studies reveal that MV-DHEL with five or more views\neffectively mitigates dimensionality collapse by fully utilizing the embedding\nspace, thereby delivering multi-view benefits observed in supervised learning.\n", "link": "http://arxiv.org/abs/2507.06979v1", "date": "2025-07-09", "relevancy": 2.8385, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5873}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Principled%20Framework%20for%20Multi-View%20Contrastive%20Learning&body=Title%3A%20A%20Principled%20Framework%20for%20Multi-View%20Contrastive%20Learning%0AAuthor%3A%20Panagiotis%20Koromilas%20and%20Efthymios%20Georgiou%20and%20Giorgos%20Bouritsas%20and%20Theodoros%20Giannakopoulos%20and%20Mihalis%20A.%20Nicolaou%20and%20Yannis%20Panagakis%0AAbstract%3A%20%20%20Contrastive%20Learning%20%28CL%29%2C%20a%20leading%20paradigm%20in%20Self-Supervised%20Learning%0A%28SSL%29%2C%20typically%20relies%20on%20pairs%20of%20data%20views%20generated%20through%20augmentation.%0AWhile%20multiple%20augmentations%20per%20instance%20%28more%20than%20two%29%20improve%0Ageneralization%20in%20supervised%20learning%2C%20current%20CL%20methods%20handle%20additional%0Aviews%20suboptimally%20by%20simply%20aggregating%20different%20pairwise%20objectives.%20This%0Aapproach%20suffers%20from%20four%20critical%20limitations%3A%20%28L1%29%20it%20utilizes%20multiple%0Aoptimization%20terms%20per%20data%20point%20resulting%20to%20conflicting%20objectives%2C%20%28L2%29%20it%0Afails%20to%20model%20all%20interactions%20across%20views%20and%20data%20points%2C%20%28L3%29%20it%20inherits%0Afundamental%20limitations%20%28e.g.%20alignment-uniformity%20coupling%29%20from%20pairwise%20CL%0Alosses%2C%20and%20%28L4%29%20it%20prevents%20fully%20realizing%20the%20benefits%20of%20increased%20view%0Amultiplicity%20observed%20in%20supervised%20settings.%20We%20address%20these%20limitations%0Athrough%20two%20novel%20loss%20functions%3A%20MV-InfoNCE%2C%20which%20extends%20InfoNCE%20to%0Aincorporate%20all%20possible%20view%20interactions%20simultaneously%20in%20one%20term%20per%20data%0Apoint%2C%20and%20MV-DHEL%2C%20which%20decouples%20alignment%20from%20uniformity%20across%20views%0Awhile%20scaling%20interaction%20complexity%20with%20view%20multiplicity.%20Both%20approaches%0Aare%20theoretically%20grounded%20-%20we%20prove%20they%20asymptotically%20optimize%20for%0Aalignment%20of%20all%20views%20and%20uniformity%2C%20providing%20principled%20extensions%20to%0Amulti-view%20contrastive%20learning.%20Our%20empirical%20results%20on%20ImageNet1K%20and%20three%0Aother%20datasets%20demonstrate%20that%20our%20methods%20consistently%20outperform%20existing%0Amulti-view%20approaches%20and%20effectively%20scale%20with%20increasing%20view%20multiplicity.%0AWe%20also%20apply%20our%20objectives%20to%20multimodal%20data%20and%20show%20that%2C%20in%20contrast%20to%0Aother%20contrastive%20objectives%2C%20they%20can%20scale%20beyond%20just%20two%20modalities.%20Most%0Asignificantly%2C%20ablation%20studies%20reveal%20that%20MV-DHEL%20with%20five%20or%20more%20views%0Aeffectively%20mitigates%20dimensionality%20collapse%20by%20fully%20utilizing%20the%20embedding%0Aspace%2C%20thereby%20delivering%20multi-view%20benefits%20observed%20in%20supervised%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Principled%2520Framework%2520for%2520Multi-View%2520Contrastive%2520Learning%26entry.906535625%3DPanagiotis%2520Koromilas%2520and%2520Efthymios%2520Georgiou%2520and%2520Giorgos%2520Bouritsas%2520and%2520Theodoros%2520Giannakopoulos%2520and%2520Mihalis%2520A.%2520Nicolaou%2520and%2520Yannis%2520Panagakis%26entry.1292438233%3D%2520%2520Contrastive%2520Learning%2520%2528CL%2529%252C%2520a%2520leading%2520paradigm%2520in%2520Self-Supervised%2520Learning%250A%2528SSL%2529%252C%2520typically%2520relies%2520on%2520pairs%2520of%2520data%2520views%2520generated%2520through%2520augmentation.%250AWhile%2520multiple%2520augmentations%2520per%2520instance%2520%2528more%2520than%2520two%2529%2520improve%250Ageneralization%2520in%2520supervised%2520learning%252C%2520current%2520CL%2520methods%2520handle%2520additional%250Aviews%2520suboptimally%2520by%2520simply%2520aggregating%2520different%2520pairwise%2520objectives.%2520This%250Aapproach%2520suffers%2520from%2520four%2520critical%2520limitations%253A%2520%2528L1%2529%2520it%2520utilizes%2520multiple%250Aoptimization%2520terms%2520per%2520data%2520point%2520resulting%2520to%2520conflicting%2520objectives%252C%2520%2528L2%2529%2520it%250Afails%2520to%2520model%2520all%2520interactions%2520across%2520views%2520and%2520data%2520points%252C%2520%2528L3%2529%2520it%2520inherits%250Afundamental%2520limitations%2520%2528e.g.%2520alignment-uniformity%2520coupling%2529%2520from%2520pairwise%2520CL%250Alosses%252C%2520and%2520%2528L4%2529%2520it%2520prevents%2520fully%2520realizing%2520the%2520benefits%2520of%2520increased%2520view%250Amultiplicity%2520observed%2520in%2520supervised%2520settings.%2520We%2520address%2520these%2520limitations%250Athrough%2520two%2520novel%2520loss%2520functions%253A%2520MV-InfoNCE%252C%2520which%2520extends%2520InfoNCE%2520to%250Aincorporate%2520all%2520possible%2520view%2520interactions%2520simultaneously%2520in%2520one%2520term%2520per%2520data%250Apoint%252C%2520and%2520MV-DHEL%252C%2520which%2520decouples%2520alignment%2520from%2520uniformity%2520across%2520views%250Awhile%2520scaling%2520interaction%2520complexity%2520with%2520view%2520multiplicity.%2520Both%2520approaches%250Aare%2520theoretically%2520grounded%2520-%2520we%2520prove%2520they%2520asymptotically%2520optimize%2520for%250Aalignment%2520of%2520all%2520views%2520and%2520uniformity%252C%2520providing%2520principled%2520extensions%2520to%250Amulti-view%2520contrastive%2520learning.%2520Our%2520empirical%2520results%2520on%2520ImageNet1K%2520and%2520three%250Aother%2520datasets%2520demonstrate%2520that%2520our%2520methods%2520consistently%2520outperform%2520existing%250Amulti-view%2520approaches%2520and%2520effectively%2520scale%2520with%2520increasing%2520view%2520multiplicity.%250AWe%2520also%2520apply%2520our%2520objectives%2520to%2520multimodal%2520data%2520and%2520show%2520that%252C%2520in%2520contrast%2520to%250Aother%2520contrastive%2520objectives%252C%2520they%2520can%2520scale%2520beyond%2520just%2520two%2520modalities.%2520Most%250Asignificantly%252C%2520ablation%2520studies%2520reveal%2520that%2520MV-DHEL%2520with%2520five%2520or%2520more%2520views%250Aeffectively%2520mitigates%2520dimensionality%2520collapse%2520by%2520fully%2520utilizing%2520the%2520embedding%250Aspace%252C%2520thereby%2520delivering%2520multi-view%2520benefits%2520observed%2520in%2520supervised%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Principled%20Framework%20for%20Multi-View%20Contrastive%20Learning&entry.906535625=Panagiotis%20Koromilas%20and%20Efthymios%20Georgiou%20and%20Giorgos%20Bouritsas%20and%20Theodoros%20Giannakopoulos%20and%20Mihalis%20A.%20Nicolaou%20and%20Yannis%20Panagakis&entry.1292438233=%20%20Contrastive%20Learning%20%28CL%29%2C%20a%20leading%20paradigm%20in%20Self-Supervised%20Learning%0A%28SSL%29%2C%20typically%20relies%20on%20pairs%20of%20data%20views%20generated%20through%20augmentation.%0AWhile%20multiple%20augmentations%20per%20instance%20%28more%20than%20two%29%20improve%0Ageneralization%20in%20supervised%20learning%2C%20current%20CL%20methods%20handle%20additional%0Aviews%20suboptimally%20by%20simply%20aggregating%20different%20pairwise%20objectives.%20This%0Aapproach%20suffers%20from%20four%20critical%20limitations%3A%20%28L1%29%20it%20utilizes%20multiple%0Aoptimization%20terms%20per%20data%20point%20resulting%20to%20conflicting%20objectives%2C%20%28L2%29%20it%0Afails%20to%20model%20all%20interactions%20across%20views%20and%20data%20points%2C%20%28L3%29%20it%20inherits%0Afundamental%20limitations%20%28e.g.%20alignment-uniformity%20coupling%29%20from%20pairwise%20CL%0Alosses%2C%20and%20%28L4%29%20it%20prevents%20fully%20realizing%20the%20benefits%20of%20increased%20view%0Amultiplicity%20observed%20in%20supervised%20settings.%20We%20address%20these%20limitations%0Athrough%20two%20novel%20loss%20functions%3A%20MV-InfoNCE%2C%20which%20extends%20InfoNCE%20to%0Aincorporate%20all%20possible%20view%20interactions%20simultaneously%20in%20one%20term%20per%20data%0Apoint%2C%20and%20MV-DHEL%2C%20which%20decouples%20alignment%20from%20uniformity%20across%20views%0Awhile%20scaling%20interaction%20complexity%20with%20view%20multiplicity.%20Both%20approaches%0Aare%20theoretically%20grounded%20-%20we%20prove%20they%20asymptotically%20optimize%20for%0Aalignment%20of%20all%20views%20and%20uniformity%2C%20providing%20principled%20extensions%20to%0Amulti-view%20contrastive%20learning.%20Our%20empirical%20results%20on%20ImageNet1K%20and%20three%0Aother%20datasets%20demonstrate%20that%20our%20methods%20consistently%20outperform%20existing%0Amulti-view%20approaches%20and%20effectively%20scale%20with%20increasing%20view%20multiplicity.%0AWe%20also%20apply%20our%20objectives%20to%20multimodal%20data%20and%20show%20that%2C%20in%20contrast%20to%0Aother%20contrastive%20objectives%2C%20they%20can%20scale%20beyond%20just%20two%20modalities.%20Most%0Asignificantly%2C%20ablation%20studies%20reveal%20that%20MV-DHEL%20with%20five%20or%20more%20views%0Aeffectively%20mitigates%20dimensionality%20collapse%20by%20fully%20utilizing%20the%20embedding%0Aspace%2C%20thereby%20delivering%20multi-view%20benefits%20observed%20in%20supervised%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06979v1&entry.124074799=Read"},
{"title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models", "author": "Liang Wang and Yu Rong and Tingyang Xu and Zhenyi Zhong and Zhiyuan Liu and Pengju Wang and Deli Zhao and Qiang Liu and Shu Wu and Liang Wang", "abstract": "  Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.\n", "link": "http://arxiv.org/abs/2507.06853v1", "date": "2025-07-09", "relevancy": 2.8008, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5626}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffSpectra%3A%20Molecular%20Structure%20Elucidation%20from%20Spectra%20using%0A%20%20Diffusion%20Models&body=Title%3A%20DiffSpectra%3A%20Molecular%20Structure%20Elucidation%20from%20Spectra%20using%0A%20%20Diffusion%20Models%0AAuthor%3A%20Liang%20Wang%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Zhenyi%20Zhong%20and%20Zhiyuan%20Liu%20and%20Pengju%20Wang%20and%20Deli%20Zhao%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang%0AAbstract%3A%20%20%20Molecular%20structure%20elucidation%20from%20spectra%20is%20a%20foundational%20problem%20in%0Achemistry%2C%20with%20profound%20implications%20for%20compound%20identification%2C%20synthesis%2C%0Aand%20drug%20development.%20Traditional%20methods%20rely%20heavily%20on%20expert%20interpretation%0Aand%20lack%20scalability.%20Pioneering%20machine%20learning%20methods%20have%20introduced%0Aretrieval-based%20strategies%2C%20but%20their%20reliance%20on%20finite%20libraries%20limits%0Ageneralization%20to%20novel%20molecules.%20Generative%20models%20offer%20a%20promising%0Aalternative%2C%20yet%20most%20adopt%20autoregressive%20SMILES-based%20architectures%20that%0Aoverlook%203D%20geometry%20and%20struggle%20to%20integrate%20diverse%20spectral%20modalities.%20In%0Athis%20work%2C%20we%20present%20DiffSpectra%2C%20a%20generative%20framework%20that%20directly%20infers%0Aboth%202D%20and%203D%20molecular%20structures%20from%20multi-modal%20spectral%20data%20using%0Adiffusion%20models.%20DiffSpectra%20formulates%20structure%20elucidation%20as%20a%20conditional%0Ageneration%20process.%20Its%20denoising%20network%20is%20parameterized%20by%20Diffusion%0AMolecule%20Transformer%2C%20an%20SE%283%29-equivariant%20architecture%20that%20integrates%0Atopological%20and%20geometric%20information.%20Conditioning%20is%20provided%20by%20SpecFormer%2C%0Aa%20transformer-based%20spectral%20encoder%20that%20captures%20intra-%20and%20inter-spectral%0Adependencies%20from%20multi-modal%20spectra.%20Extensive%20experiments%20demonstrate%20that%0ADiffSpectra%20achieves%20high%20accuracy%20in%20structure%20elucidation%2C%20recovering%20exact%0Astructures%20with%2016.01%25%20top-1%20accuracy%20and%2096.86%25%20top-20%20accuracy%20through%0Asampling.%20The%20model%20benefits%20significantly%20from%203D%20geometric%20modeling%2C%0ASpecFormer%20pre-training%2C%20and%20multi-modal%20conditioning.%20These%20results%20highlight%0Athe%20effectiveness%20of%20spectrum-conditioned%20diffusion%20modeling%20in%20addressing%20the%0Achallenge%20of%20molecular%20structure%20elucidation.%20To%20our%20knowledge%2C%20DiffSpectra%20is%0Athe%20first%20framework%20to%20unify%20multi-modal%20spectral%20reasoning%20and%20joint%202D/3D%0Agenerative%20modeling%20for%20de%20novo%20molecular%20structure%20elucidation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffSpectra%253A%2520Molecular%2520Structure%2520Elucidation%2520from%2520Spectra%2520using%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DLiang%2520Wang%2520and%2520Yu%2520Rong%2520and%2520Tingyang%2520Xu%2520and%2520Zhenyi%2520Zhong%2520and%2520Zhiyuan%2520Liu%2520and%2520Pengju%2520Wang%2520and%2520Deli%2520Zhao%2520and%2520Qiang%2520Liu%2520and%2520Shu%2520Wu%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520Molecular%2520structure%2520elucidation%2520from%2520spectra%2520is%2520a%2520foundational%2520problem%2520in%250Achemistry%252C%2520with%2520profound%2520implications%2520for%2520compound%2520identification%252C%2520synthesis%252C%250Aand%2520drug%2520development.%2520Traditional%2520methods%2520rely%2520heavily%2520on%2520expert%2520interpretation%250Aand%2520lack%2520scalability.%2520Pioneering%2520machine%2520learning%2520methods%2520have%2520introduced%250Aretrieval-based%2520strategies%252C%2520but%2520their%2520reliance%2520on%2520finite%2520libraries%2520limits%250Ageneralization%2520to%2520novel%2520molecules.%2520Generative%2520models%2520offer%2520a%2520promising%250Aalternative%252C%2520yet%2520most%2520adopt%2520autoregressive%2520SMILES-based%2520architectures%2520that%250Aoverlook%25203D%2520geometry%2520and%2520struggle%2520to%2520integrate%2520diverse%2520spectral%2520modalities.%2520In%250Athis%2520work%252C%2520we%2520present%2520DiffSpectra%252C%2520a%2520generative%2520framework%2520that%2520directly%2520infers%250Aboth%25202D%2520and%25203D%2520molecular%2520structures%2520from%2520multi-modal%2520spectral%2520data%2520using%250Adiffusion%2520models.%2520DiffSpectra%2520formulates%2520structure%2520elucidation%2520as%2520a%2520conditional%250Ageneration%2520process.%2520Its%2520denoising%2520network%2520is%2520parameterized%2520by%2520Diffusion%250AMolecule%2520Transformer%252C%2520an%2520SE%25283%2529-equivariant%2520architecture%2520that%2520integrates%250Atopological%2520and%2520geometric%2520information.%2520Conditioning%2520is%2520provided%2520by%2520SpecFormer%252C%250Aa%2520transformer-based%2520spectral%2520encoder%2520that%2520captures%2520intra-%2520and%2520inter-spectral%250Adependencies%2520from%2520multi-modal%2520spectra.%2520Extensive%2520experiments%2520demonstrate%2520that%250ADiffSpectra%2520achieves%2520high%2520accuracy%2520in%2520structure%2520elucidation%252C%2520recovering%2520exact%250Astructures%2520with%252016.01%2525%2520top-1%2520accuracy%2520and%252096.86%2525%2520top-20%2520accuracy%2520through%250Asampling.%2520The%2520model%2520benefits%2520significantly%2520from%25203D%2520geometric%2520modeling%252C%250ASpecFormer%2520pre-training%252C%2520and%2520multi-modal%2520conditioning.%2520These%2520results%2520highlight%250Athe%2520effectiveness%2520of%2520spectrum-conditioned%2520diffusion%2520modeling%2520in%2520addressing%2520the%250Achallenge%2520of%2520molecular%2520structure%2520elucidation.%2520To%2520our%2520knowledge%252C%2520DiffSpectra%2520is%250Athe%2520first%2520framework%2520to%2520unify%2520multi-modal%2520spectral%2520reasoning%2520and%2520joint%25202D/3D%250Agenerative%2520modeling%2520for%2520de%2520novo%2520molecular%2520structure%2520elucidation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffSpectra%3A%20Molecular%20Structure%20Elucidation%20from%20Spectra%20using%0A%20%20Diffusion%20Models&entry.906535625=Liang%20Wang%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Zhenyi%20Zhong%20and%20Zhiyuan%20Liu%20and%20Pengju%20Wang%20and%20Deli%20Zhao%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang&entry.1292438233=%20%20Molecular%20structure%20elucidation%20from%20spectra%20is%20a%20foundational%20problem%20in%0Achemistry%2C%20with%20profound%20implications%20for%20compound%20identification%2C%20synthesis%2C%0Aand%20drug%20development.%20Traditional%20methods%20rely%20heavily%20on%20expert%20interpretation%0Aand%20lack%20scalability.%20Pioneering%20machine%20learning%20methods%20have%20introduced%0Aretrieval-based%20strategies%2C%20but%20their%20reliance%20on%20finite%20libraries%20limits%0Ageneralization%20to%20novel%20molecules.%20Generative%20models%20offer%20a%20promising%0Aalternative%2C%20yet%20most%20adopt%20autoregressive%20SMILES-based%20architectures%20that%0Aoverlook%203D%20geometry%20and%20struggle%20to%20integrate%20diverse%20spectral%20modalities.%20In%0Athis%20work%2C%20we%20present%20DiffSpectra%2C%20a%20generative%20framework%20that%20directly%20infers%0Aboth%202D%20and%203D%20molecular%20structures%20from%20multi-modal%20spectral%20data%20using%0Adiffusion%20models.%20DiffSpectra%20formulates%20structure%20elucidation%20as%20a%20conditional%0Ageneration%20process.%20Its%20denoising%20network%20is%20parameterized%20by%20Diffusion%0AMolecule%20Transformer%2C%20an%20SE%283%29-equivariant%20architecture%20that%20integrates%0Atopological%20and%20geometric%20information.%20Conditioning%20is%20provided%20by%20SpecFormer%2C%0Aa%20transformer-based%20spectral%20encoder%20that%20captures%20intra-%20and%20inter-spectral%0Adependencies%20from%20multi-modal%20spectra.%20Extensive%20experiments%20demonstrate%20that%0ADiffSpectra%20achieves%20high%20accuracy%20in%20structure%20elucidation%2C%20recovering%20exact%0Astructures%20with%2016.01%25%20top-1%20accuracy%20and%2096.86%25%20top-20%20accuracy%20through%0Asampling.%20The%20model%20benefits%20significantly%20from%203D%20geometric%20modeling%2C%0ASpecFormer%20pre-training%2C%20and%20multi-modal%20conditioning.%20These%20results%20highlight%0Athe%20effectiveness%20of%20spectrum-conditioned%20diffusion%20modeling%20in%20addressing%20the%0Achallenge%20of%20molecular%20structure%20elucidation.%20To%20our%20knowledge%2C%20DiffSpectra%20is%0Athe%20first%20framework%20to%20unify%20multi-modal%20spectral%20reasoning%20and%20joint%202D/3D%0Agenerative%20modeling%20for%20de%20novo%20molecular%20structure%20elucidation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06853v1&entry.124074799=Read"},
{"title": "Adaptive Part Learning for Fine-Grained Generalized Category Discovery:\n  A Plug-and-Play Enhancement", "author": "Qiyuan Dai and Hanzhuo Huang and Yu Wu and Sibei Yang", "abstract": "  Generalized Category Discovery (GCD) aims to recognize unlabeled images from\nknown and novel classes by distinguishing novel classes from known ones, while\nalso transferring knowledge from another set of labeled images with known\nclasses. Existing GCD methods rely on self-supervised vision transformers such\nas DINO for representation learning. However, focusing solely on the global\nrepresentation of the DINO CLS token introduces an inherent trade-off between\ndiscriminability and generalization. In this paper, we introduce an adaptive\npart discovery and learning method, called APL, which generates consistent\nobject parts and their correspondences across different similar images using a\nset of shared learnable part queries and DINO part priors, without requiring\nany additional annotations. More importantly, we propose a novel all-min\ncontrastive loss to learn discriminative yet generalizable part representation,\nwhich adaptively highlights discriminative object parts to distinguish similar\ncategories for enhanced discriminability while simultaneously sharing other\nparts to facilitate knowledge transfer for improved generalization. Our APL can\neasily be incorporated into different GCD frameworks by replacing their CLS\ntoken feature with our part representations, showing significant enhancements\non fine-grained datasets.\n", "link": "http://arxiv.org/abs/2507.06928v1", "date": "2025-07-09", "relevancy": 2.7967, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5772}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5547}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Part%20Learning%20for%20Fine-Grained%20Generalized%20Category%20Discovery%3A%0A%20%20A%20Plug-and-Play%20Enhancement&body=Title%3A%20Adaptive%20Part%20Learning%20for%20Fine-Grained%20Generalized%20Category%20Discovery%3A%0A%20%20A%20Plug-and-Play%20Enhancement%0AAuthor%3A%20Qiyuan%20Dai%20and%20Hanzhuo%20Huang%20and%20Yu%20Wu%20and%20Sibei%20Yang%0AAbstract%3A%20%20%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20recognize%20unlabeled%20images%20from%0Aknown%20and%20novel%20classes%20by%20distinguishing%20novel%20classes%20from%20known%20ones%2C%20while%0Aalso%20transferring%20knowledge%20from%20another%20set%20of%20labeled%20images%20with%20known%0Aclasses.%20Existing%20GCD%20methods%20rely%20on%20self-supervised%20vision%20transformers%20such%0Aas%20DINO%20for%20representation%20learning.%20However%2C%20focusing%20solely%20on%20the%20global%0Arepresentation%20of%20the%20DINO%20CLS%20token%20introduces%20an%20inherent%20trade-off%20between%0Adiscriminability%20and%20generalization.%20In%20this%20paper%2C%20we%20introduce%20an%20adaptive%0Apart%20discovery%20and%20learning%20method%2C%20called%20APL%2C%20which%20generates%20consistent%0Aobject%20parts%20and%20their%20correspondences%20across%20different%20similar%20images%20using%20a%0Aset%20of%20shared%20learnable%20part%20queries%20and%20DINO%20part%20priors%2C%20without%20requiring%0Aany%20additional%20annotations.%20More%20importantly%2C%20we%20propose%20a%20novel%20all-min%0Acontrastive%20loss%20to%20learn%20discriminative%20yet%20generalizable%20part%20representation%2C%0Awhich%20adaptively%20highlights%20discriminative%20object%20parts%20to%20distinguish%20similar%0Acategories%20for%20enhanced%20discriminability%20while%20simultaneously%20sharing%20other%0Aparts%20to%20facilitate%20knowledge%20transfer%20for%20improved%20generalization.%20Our%20APL%20can%0Aeasily%20be%20incorporated%20into%20different%20GCD%20frameworks%20by%20replacing%20their%20CLS%0Atoken%20feature%20with%20our%20part%20representations%2C%20showing%20significant%20enhancements%0Aon%20fine-grained%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Part%2520Learning%2520for%2520Fine-Grained%2520Generalized%2520Category%2520Discovery%253A%250A%2520%2520A%2520Plug-and-Play%2520Enhancement%26entry.906535625%3DQiyuan%2520Dai%2520and%2520Hanzhuo%2520Huang%2520and%2520Yu%2520Wu%2520and%2520Sibei%2520Yang%26entry.1292438233%3D%2520%2520Generalized%2520Category%2520Discovery%2520%2528GCD%2529%2520aims%2520to%2520recognize%2520unlabeled%2520images%2520from%250Aknown%2520and%2520novel%2520classes%2520by%2520distinguishing%2520novel%2520classes%2520from%2520known%2520ones%252C%2520while%250Aalso%2520transferring%2520knowledge%2520from%2520another%2520set%2520of%2520labeled%2520images%2520with%2520known%250Aclasses.%2520Existing%2520GCD%2520methods%2520rely%2520on%2520self-supervised%2520vision%2520transformers%2520such%250Aas%2520DINO%2520for%2520representation%2520learning.%2520However%252C%2520focusing%2520solely%2520on%2520the%2520global%250Arepresentation%2520of%2520the%2520DINO%2520CLS%2520token%2520introduces%2520an%2520inherent%2520trade-off%2520between%250Adiscriminability%2520and%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520adaptive%250Apart%2520discovery%2520and%2520learning%2520method%252C%2520called%2520APL%252C%2520which%2520generates%2520consistent%250Aobject%2520parts%2520and%2520their%2520correspondences%2520across%2520different%2520similar%2520images%2520using%2520a%250Aset%2520of%2520shared%2520learnable%2520part%2520queries%2520and%2520DINO%2520part%2520priors%252C%2520without%2520requiring%250Aany%2520additional%2520annotations.%2520More%2520importantly%252C%2520we%2520propose%2520a%2520novel%2520all-min%250Acontrastive%2520loss%2520to%2520learn%2520discriminative%2520yet%2520generalizable%2520part%2520representation%252C%250Awhich%2520adaptively%2520highlights%2520discriminative%2520object%2520parts%2520to%2520distinguish%2520similar%250Acategories%2520for%2520enhanced%2520discriminability%2520while%2520simultaneously%2520sharing%2520other%250Aparts%2520to%2520facilitate%2520knowledge%2520transfer%2520for%2520improved%2520generalization.%2520Our%2520APL%2520can%250Aeasily%2520be%2520incorporated%2520into%2520different%2520GCD%2520frameworks%2520by%2520replacing%2520their%2520CLS%250Atoken%2520feature%2520with%2520our%2520part%2520representations%252C%2520showing%2520significant%2520enhancements%250Aon%2520fine-grained%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Part%20Learning%20for%20Fine-Grained%20Generalized%20Category%20Discovery%3A%0A%20%20A%20Plug-and-Play%20Enhancement&entry.906535625=Qiyuan%20Dai%20and%20Hanzhuo%20Huang%20and%20Yu%20Wu%20and%20Sibei%20Yang&entry.1292438233=%20%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20recognize%20unlabeled%20images%20from%0Aknown%20and%20novel%20classes%20by%20distinguishing%20novel%20classes%20from%20known%20ones%2C%20while%0Aalso%20transferring%20knowledge%20from%20another%20set%20of%20labeled%20images%20with%20known%0Aclasses.%20Existing%20GCD%20methods%20rely%20on%20self-supervised%20vision%20transformers%20such%0Aas%20DINO%20for%20representation%20learning.%20However%2C%20focusing%20solely%20on%20the%20global%0Arepresentation%20of%20the%20DINO%20CLS%20token%20introduces%20an%20inherent%20trade-off%20between%0Adiscriminability%20and%20generalization.%20In%20this%20paper%2C%20we%20introduce%20an%20adaptive%0Apart%20discovery%20and%20learning%20method%2C%20called%20APL%2C%20which%20generates%20consistent%0Aobject%20parts%20and%20their%20correspondences%20across%20different%20similar%20images%20using%20a%0Aset%20of%20shared%20learnable%20part%20queries%20and%20DINO%20part%20priors%2C%20without%20requiring%0Aany%20additional%20annotations.%20More%20importantly%2C%20we%20propose%20a%20novel%20all-min%0Acontrastive%20loss%20to%20learn%20discriminative%20yet%20generalizable%20part%20representation%2C%0Awhich%20adaptively%20highlights%20discriminative%20object%20parts%20to%20distinguish%20similar%0Acategories%20for%20enhanced%20discriminability%20while%20simultaneously%20sharing%20other%0Aparts%20to%20facilitate%20knowledge%20transfer%20for%20improved%20generalization.%20Our%20APL%20can%0Aeasily%20be%20incorporated%20into%20different%20GCD%20frameworks%20by%20replacing%20their%20CLS%0Atoken%20feature%20with%20our%20part%20representations%2C%20showing%20significant%20enhancements%0Aon%20fine-grained%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06928v1&entry.124074799=Read"},
{"title": "Modality-agnostic, patient-specific digital twins modeling temporally\n  varying digestive motion", "author": "Jorge Tapias Gomez and Nishant Nadkarni and Lando S. Bosma and Jue Jiang and Ergys D. Subashi and William P. Segars and James M. Balter and Mert R Sabuncu and Neelam Tyagi and Harini Veeraraghavan", "abstract": "  Objective: Clinical implementation of deformable image registration (DIR)\nrequires voxel-based spatial accuracy metrics such as manually identified\nlandmarks, which are challenging to implement for highly mobile\ngastrointestinal (GI) organs. To address this, patient-specific digital twins\n(DT) modeling temporally varying motion were created to assess the accuracy of\nDIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D\nsequences were generated from static 3D patient scans using published\nanalytical GI motion models through a semi-automated pipeline. Eleven datasets,\nincluding six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,\nand three contrast-enhanced CT scans. The motion amplitudes of the DTs were\nassessed against real patient stomach motion amplitudes extracted from\nindependent 4D MRI datasets. The generated DTs were then used to assess six\ndifferent DIR methods using target registration error, Dice similarity\ncoefficient, and the 95th percentile Hausdorff distance using summary metrics\nand voxel-level granular visualizations. Finally, for a subset of T2w MRI scans\nfrom patients treated with MR-guided radiation therapy, dose distributions were\nwarped and accumulated to assess dose warping errors, including evaluations of\nDIR performance in both low- and high-dose regions for patient-specific error\nestimation. Main results: Our proposed pipeline synthesized DTs modeling\nrealistic GI motion, achieving mean and maximum motion amplitudes and a mean\nlog Jacobian determinant within 0.8 mm and 0.01, respectively, similar to\npublished real-patient gastric motion data. It also enables the extraction of\ndetailed quantitative DIR performance metrics and rigorous validation of dose\nmapping accuracy. Significance: The pipeline enables rigorously testing DIR\ntools for dynamic, anatomically complex regions enabling granular spatial and\ndosimetric accuracies.\n", "link": "http://arxiv.org/abs/2507.01909v3", "date": "2025-07-09", "relevancy": 2.7788, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5778}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5483}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-agnostic%2C%20patient-specific%20digital%20twins%20modeling%20temporally%0A%20%20varying%20digestive%20motion&body=Title%3A%20Modality-agnostic%2C%20patient-specific%20digital%20twins%20modeling%20temporally%0A%20%20varying%20digestive%20motion%0AAuthor%3A%20Jorge%20Tapias%20Gomez%20and%20Nishant%20Nadkarni%20and%20Lando%20S.%20Bosma%20and%20Jue%20Jiang%20and%20Ergys%20D.%20Subashi%20and%20William%20P.%20Segars%20and%20James%20M.%20Balter%20and%20Mert%20R%20Sabuncu%20and%20Neelam%20Tyagi%20and%20Harini%20Veeraraghavan%0AAbstract%3A%20%20%20Objective%3A%20Clinical%20implementation%20of%20deformable%20image%20registration%20%28DIR%29%0Arequires%20voxel-based%20spatial%20accuracy%20metrics%20such%20as%20manually%20identified%0Alandmarks%2C%20which%20are%20challenging%20to%20implement%20for%20highly%20mobile%0Agastrointestinal%20%28GI%29%20organs.%20To%20address%20this%2C%20patient-specific%20digital%20twins%0A%28DT%29%20modeling%20temporally%20varying%20motion%20were%20created%20to%20assess%20the%20accuracy%20of%0ADIR%20methods.%20Approach%3A%2021%20motion%20phases%20simulating%20digestive%20GI%20motion%20as%204D%0Asequences%20were%20generated%20from%20static%203D%20patient%20scans%20using%20published%0Aanalytical%20GI%20motion%20models%20through%20a%20semi-automated%20pipeline.%20Eleven%20datasets%2C%0Aincluding%20six%20T2w%20FSE%20MRI%20%28T2w%20MRI%29%2C%20two%20T1w%204D%20golden-angle%20stack-of-stars%2C%0Aand%20three%20contrast-enhanced%20CT%20scans.%20The%20motion%20amplitudes%20of%20the%20DTs%20were%0Aassessed%20against%20real%20patient%20stomach%20motion%20amplitudes%20extracted%20from%0Aindependent%204D%20MRI%20datasets.%20The%20generated%20DTs%20were%20then%20used%20to%20assess%20six%0Adifferent%20DIR%20methods%20using%20target%20registration%20error%2C%20Dice%20similarity%0Acoefficient%2C%20and%20the%2095th%20percentile%20Hausdorff%20distance%20using%20summary%20metrics%0Aand%20voxel-level%20granular%20visualizations.%20Finally%2C%20for%20a%20subset%20of%20T2w%20MRI%20scans%0Afrom%20patients%20treated%20with%20MR-guided%20radiation%20therapy%2C%20dose%20distributions%20were%0Awarped%20and%20accumulated%20to%20assess%20dose%20warping%20errors%2C%20including%20evaluations%20of%0ADIR%20performance%20in%20both%20low-%20and%20high-dose%20regions%20for%20patient-specific%20error%0Aestimation.%20Main%20results%3A%20Our%20proposed%20pipeline%20synthesized%20DTs%20modeling%0Arealistic%20GI%20motion%2C%20achieving%20mean%20and%20maximum%20motion%20amplitudes%20and%20a%20mean%0Alog%20Jacobian%20determinant%20within%200.8%20mm%20and%200.01%2C%20respectively%2C%20similar%20to%0Apublished%20real-patient%20gastric%20motion%20data.%20It%20also%20enables%20the%20extraction%20of%0Adetailed%20quantitative%20DIR%20performance%20metrics%20and%20rigorous%20validation%20of%20dose%0Amapping%20accuracy.%20Significance%3A%20The%20pipeline%20enables%20rigorously%20testing%20DIR%0Atools%20for%20dynamic%2C%20anatomically%20complex%20regions%20enabling%20granular%20spatial%20and%0Adosimetric%20accuracies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01909v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-agnostic%252C%2520patient-specific%2520digital%2520twins%2520modeling%2520temporally%250A%2520%2520varying%2520digestive%2520motion%26entry.906535625%3DJorge%2520Tapias%2520Gomez%2520and%2520Nishant%2520Nadkarni%2520and%2520Lando%2520S.%2520Bosma%2520and%2520Jue%2520Jiang%2520and%2520Ergys%2520D.%2520Subashi%2520and%2520William%2520P.%2520Segars%2520and%2520James%2520M.%2520Balter%2520and%2520Mert%2520R%2520Sabuncu%2520and%2520Neelam%2520Tyagi%2520and%2520Harini%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Objective%253A%2520Clinical%2520implementation%2520of%2520deformable%2520image%2520registration%2520%2528DIR%2529%250Arequires%2520voxel-based%2520spatial%2520accuracy%2520metrics%2520such%2520as%2520manually%2520identified%250Alandmarks%252C%2520which%2520are%2520challenging%2520to%2520implement%2520for%2520highly%2520mobile%250Agastrointestinal%2520%2528GI%2529%2520organs.%2520To%2520address%2520this%252C%2520patient-specific%2520digital%2520twins%250A%2528DT%2529%2520modeling%2520temporally%2520varying%2520motion%2520were%2520created%2520to%2520assess%2520the%2520accuracy%2520of%250ADIR%2520methods.%2520Approach%253A%252021%2520motion%2520phases%2520simulating%2520digestive%2520GI%2520motion%2520as%25204D%250Asequences%2520were%2520generated%2520from%2520static%25203D%2520patient%2520scans%2520using%2520published%250Aanalytical%2520GI%2520motion%2520models%2520through%2520a%2520semi-automated%2520pipeline.%2520Eleven%2520datasets%252C%250Aincluding%2520six%2520T2w%2520FSE%2520MRI%2520%2528T2w%2520MRI%2529%252C%2520two%2520T1w%25204D%2520golden-angle%2520stack-of-stars%252C%250Aand%2520three%2520contrast-enhanced%2520CT%2520scans.%2520The%2520motion%2520amplitudes%2520of%2520the%2520DTs%2520were%250Aassessed%2520against%2520real%2520patient%2520stomach%2520motion%2520amplitudes%2520extracted%2520from%250Aindependent%25204D%2520MRI%2520datasets.%2520The%2520generated%2520DTs%2520were%2520then%2520used%2520to%2520assess%2520six%250Adifferent%2520DIR%2520methods%2520using%2520target%2520registration%2520error%252C%2520Dice%2520similarity%250Acoefficient%252C%2520and%2520the%252095th%2520percentile%2520Hausdorff%2520distance%2520using%2520summary%2520metrics%250Aand%2520voxel-level%2520granular%2520visualizations.%2520Finally%252C%2520for%2520a%2520subset%2520of%2520T2w%2520MRI%2520scans%250Afrom%2520patients%2520treated%2520with%2520MR-guided%2520radiation%2520therapy%252C%2520dose%2520distributions%2520were%250Awarped%2520and%2520accumulated%2520to%2520assess%2520dose%2520warping%2520errors%252C%2520including%2520evaluations%2520of%250ADIR%2520performance%2520in%2520both%2520low-%2520and%2520high-dose%2520regions%2520for%2520patient-specific%2520error%250Aestimation.%2520Main%2520results%253A%2520Our%2520proposed%2520pipeline%2520synthesized%2520DTs%2520modeling%250Arealistic%2520GI%2520motion%252C%2520achieving%2520mean%2520and%2520maximum%2520motion%2520amplitudes%2520and%2520a%2520mean%250Alog%2520Jacobian%2520determinant%2520within%25200.8%2520mm%2520and%25200.01%252C%2520respectively%252C%2520similar%2520to%250Apublished%2520real-patient%2520gastric%2520motion%2520data.%2520It%2520also%2520enables%2520the%2520extraction%2520of%250Adetailed%2520quantitative%2520DIR%2520performance%2520metrics%2520and%2520rigorous%2520validation%2520of%2520dose%250Amapping%2520accuracy.%2520Significance%253A%2520The%2520pipeline%2520enables%2520rigorously%2520testing%2520DIR%250Atools%2520for%2520dynamic%252C%2520anatomically%2520complex%2520regions%2520enabling%2520granular%2520spatial%2520and%250Adosimetric%2520accuracies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01909v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-agnostic%2C%20patient-specific%20digital%20twins%20modeling%20temporally%0A%20%20varying%20digestive%20motion&entry.906535625=Jorge%20Tapias%20Gomez%20and%20Nishant%20Nadkarni%20and%20Lando%20S.%20Bosma%20and%20Jue%20Jiang%20and%20Ergys%20D.%20Subashi%20and%20William%20P.%20Segars%20and%20James%20M.%20Balter%20and%20Mert%20R%20Sabuncu%20and%20Neelam%20Tyagi%20and%20Harini%20Veeraraghavan&entry.1292438233=%20%20Objective%3A%20Clinical%20implementation%20of%20deformable%20image%20registration%20%28DIR%29%0Arequires%20voxel-based%20spatial%20accuracy%20metrics%20such%20as%20manually%20identified%0Alandmarks%2C%20which%20are%20challenging%20to%20implement%20for%20highly%20mobile%0Agastrointestinal%20%28GI%29%20organs.%20To%20address%20this%2C%20patient-specific%20digital%20twins%0A%28DT%29%20modeling%20temporally%20varying%20motion%20were%20created%20to%20assess%20the%20accuracy%20of%0ADIR%20methods.%20Approach%3A%2021%20motion%20phases%20simulating%20digestive%20GI%20motion%20as%204D%0Asequences%20were%20generated%20from%20static%203D%20patient%20scans%20using%20published%0Aanalytical%20GI%20motion%20models%20through%20a%20semi-automated%20pipeline.%20Eleven%20datasets%2C%0Aincluding%20six%20T2w%20FSE%20MRI%20%28T2w%20MRI%29%2C%20two%20T1w%204D%20golden-angle%20stack-of-stars%2C%0Aand%20three%20contrast-enhanced%20CT%20scans.%20The%20motion%20amplitudes%20of%20the%20DTs%20were%0Aassessed%20against%20real%20patient%20stomach%20motion%20amplitudes%20extracted%20from%0Aindependent%204D%20MRI%20datasets.%20The%20generated%20DTs%20were%20then%20used%20to%20assess%20six%0Adifferent%20DIR%20methods%20using%20target%20registration%20error%2C%20Dice%20similarity%0Acoefficient%2C%20and%20the%2095th%20percentile%20Hausdorff%20distance%20using%20summary%20metrics%0Aand%20voxel-level%20granular%20visualizations.%20Finally%2C%20for%20a%20subset%20of%20T2w%20MRI%20scans%0Afrom%20patients%20treated%20with%20MR-guided%20radiation%20therapy%2C%20dose%20distributions%20were%0Awarped%20and%20accumulated%20to%20assess%20dose%20warping%20errors%2C%20including%20evaluations%20of%0ADIR%20performance%20in%20both%20low-%20and%20high-dose%20regions%20for%20patient-specific%20error%0Aestimation.%20Main%20results%3A%20Our%20proposed%20pipeline%20synthesized%20DTs%20modeling%0Arealistic%20GI%20motion%2C%20achieving%20mean%20and%20maximum%20motion%20amplitudes%20and%20a%20mean%0Alog%20Jacobian%20determinant%20within%200.8%20mm%20and%200.01%2C%20respectively%2C%20similar%20to%0Apublished%20real-patient%20gastric%20motion%20data.%20It%20also%20enables%20the%20extraction%20of%0Adetailed%20quantitative%20DIR%20performance%20metrics%20and%20rigorous%20validation%20of%20dose%0Amapping%20accuracy.%20Significance%3A%20The%20pipeline%20enables%20rigorously%20testing%20DIR%0Atools%20for%20dynamic%2C%20anatomically%20complex%20regions%20enabling%20granular%20spatial%20and%0Adosimetric%20accuracies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01909v3&entry.124074799=Read"},
{"title": "OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning", "author": "Zhaochen Su and Linjie Li and Mingyang Song and Yunzhuo Hao and Zhengyuan Yang and Jun Zhang and Guanjie Chen and Jiawei Gu and Juntao Li and Xiaoye Qu and Yu Cheng", "abstract": "  While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".\n", "link": "http://arxiv.org/abs/2505.08617v2", "date": "2025-07-09", "relevancy": 2.7761, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5637}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenThinkIMG%3A%20Learning%20to%20Think%20with%20Images%20via%20Visual%20Tool%0A%20%20Reinforcement%20Learning&body=Title%3A%20OpenThinkIMG%3A%20Learning%20to%20Think%20with%20Images%20via%20Visual%20Tool%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Zhaochen%20Su%20and%20Linjie%20Li%20and%20Mingyang%20Song%20and%20Yunzhuo%20Hao%20and%20Zhengyuan%20Yang%20and%20Jun%20Zhang%20and%20Guanjie%20Chen%20and%20Jiawei%20Gu%20and%20Juntao%20Li%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20While%20humans%20can%20flexibly%20leverage%20interactive%20visual%20cognition%20for%20complex%0Aproblem-solving%2C%20enabling%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20learn%0Asimilarly%20adaptive%20behaviors%20with%20visual%20tools%20remains%20challenging.%20A%0Asignificant%20hurdle%20is%20the%20current%20lack%20of%20standardized%20infrastructure%2C%20which%0Ahinders%20integrating%20diverse%20tools%2C%20generating%20rich%20interaction%20data%2C%20and%0Atraining%20robust%20agents%20effectively.%20To%20address%20these%20gaps%2C%20we%20introduce%0AOpenThinkIMG%2C%20the%20first%20open-source%2C%20comprehensive%20end-to-end%20framework%20for%0Atool-augmented%20LVLMs.%20It%20features%20standardized%20vision%20tool%20interfaces%2C%20scalable%0Atrajectory%20generation%20for%20policy%20initialization%2C%20and%20a%20flexible%20training%0Aenvironment.%20Furthermore%2C%20considering%20supervised%20fine-tuning%20%28SFT%29%20on%20static%0Ademonstrations%20offers%20limited%20policy%20generalization%20for%20dynamic%20tool%0Ainvocation%2C%20we%20propose%20a%20novel%20reinforcement%20learning%20%28RL%29%20framework%20V-ToolRL%0Ato%20train%20LVLMs%20to%20learn%20adaptive%20policies%20for%20invoking%20external%20vision%20tools.%0AV-ToolRL%20enables%20LVLMs%20to%20autonomously%20discover%20optimal%20tool-usage%20strategies%0Aby%20directly%20optimizing%20for%20task%20success%20using%20feedback%20from%20tool%20interactions.%0AWe%20empirically%20validate%20V-ToolRL%20on%20challenging%20chart%20reasoning%20tasks.%20Our%0ARL-trained%20agent%2C%20built%20upon%20a%20Qwen2-VL-2B%2C%20significantly%20outperforms%20its%0ASFT-initialized%20counterpart%20%28%2B28.83%20points%29%20and%20surpasses%20established%0Asupervised%20tool-learning%20baselines%20like%20Taco%20and%20CogCom%20by%20an%20average%20of%20%2B12.7%0Apoints.%20Notably%2C%20it%20also%20surpasses%20prominent%20closed-source%20models%20like%20GPT-4.1%0Aby%20%2B8.68%20accuracy%20points.%20We%20hope%20OpenThinkIMG%20can%20serve%20as%20a%20foundational%0Aframework%20for%20advancing%20dynamic%2C%20tool-augmented%20visual%20reasoning%2C%20helping%20the%0Acommunity%20develop%20AI%20agents%20that%20can%20genuinely%20%22think%20with%20images%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08617v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenThinkIMG%253A%2520Learning%2520to%2520Think%2520with%2520Images%2520via%2520Visual%2520Tool%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZhaochen%2520Su%2520and%2520Linjie%2520Li%2520and%2520Mingyang%2520Song%2520and%2520Yunzhuo%2520Hao%2520and%2520Zhengyuan%2520Yang%2520and%2520Jun%2520Zhang%2520and%2520Guanjie%2520Chen%2520and%2520Jiawei%2520Gu%2520and%2520Juntao%2520Li%2520and%2520Xiaoye%2520Qu%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520While%2520humans%2520can%2520flexibly%2520leverage%2520interactive%2520visual%2520cognition%2520for%2520complex%250Aproblem-solving%252C%2520enabling%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520learn%250Asimilarly%2520adaptive%2520behaviors%2520with%2520visual%2520tools%2520remains%2520challenging.%2520A%250Asignificant%2520hurdle%2520is%2520the%2520current%2520lack%2520of%2520standardized%2520infrastructure%252C%2520which%250Ahinders%2520integrating%2520diverse%2520tools%252C%2520generating%2520rich%2520interaction%2520data%252C%2520and%250Atraining%2520robust%2520agents%2520effectively.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%250AOpenThinkIMG%252C%2520the%2520first%2520open-source%252C%2520comprehensive%2520end-to-end%2520framework%2520for%250Atool-augmented%2520LVLMs.%2520It%2520features%2520standardized%2520vision%2520tool%2520interfaces%252C%2520scalable%250Atrajectory%2520generation%2520for%2520policy%2520initialization%252C%2520and%2520a%2520flexible%2520training%250Aenvironment.%2520Furthermore%252C%2520considering%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520on%2520static%250Ademonstrations%2520offers%2520limited%2520policy%2520generalization%2520for%2520dynamic%2520tool%250Ainvocation%252C%2520we%2520propose%2520a%2520novel%2520reinforcement%2520learning%2520%2528RL%2529%2520framework%2520V-ToolRL%250Ato%2520train%2520LVLMs%2520to%2520learn%2520adaptive%2520policies%2520for%2520invoking%2520external%2520vision%2520tools.%250AV-ToolRL%2520enables%2520LVLMs%2520to%2520autonomously%2520discover%2520optimal%2520tool-usage%2520strategies%250Aby%2520directly%2520optimizing%2520for%2520task%2520success%2520using%2520feedback%2520from%2520tool%2520interactions.%250AWe%2520empirically%2520validate%2520V-ToolRL%2520on%2520challenging%2520chart%2520reasoning%2520tasks.%2520Our%250ARL-trained%2520agent%252C%2520built%2520upon%2520a%2520Qwen2-VL-2B%252C%2520significantly%2520outperforms%2520its%250ASFT-initialized%2520counterpart%2520%2528%252B28.83%2520points%2529%2520and%2520surpasses%2520established%250Asupervised%2520tool-learning%2520baselines%2520like%2520Taco%2520and%2520CogCom%2520by%2520an%2520average%2520of%2520%252B12.7%250Apoints.%2520Notably%252C%2520it%2520also%2520surpasses%2520prominent%2520closed-source%2520models%2520like%2520GPT-4.1%250Aby%2520%252B8.68%2520accuracy%2520points.%2520We%2520hope%2520OpenThinkIMG%2520can%2520serve%2520as%2520a%2520foundational%250Aframework%2520for%2520advancing%2520dynamic%252C%2520tool-augmented%2520visual%2520reasoning%252C%2520helping%2520the%250Acommunity%2520develop%2520AI%2520agents%2520that%2520can%2520genuinely%2520%2522think%2520with%2520images%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08617v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenThinkIMG%3A%20Learning%20to%20Think%20with%20Images%20via%20Visual%20Tool%0A%20%20Reinforcement%20Learning&entry.906535625=Zhaochen%20Su%20and%20Linjie%20Li%20and%20Mingyang%20Song%20and%20Yunzhuo%20Hao%20and%20Zhengyuan%20Yang%20and%20Jun%20Zhang%20and%20Guanjie%20Chen%20and%20Jiawei%20Gu%20and%20Juntao%20Li%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng&entry.1292438233=%20%20While%20humans%20can%20flexibly%20leverage%20interactive%20visual%20cognition%20for%20complex%0Aproblem-solving%2C%20enabling%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20learn%0Asimilarly%20adaptive%20behaviors%20with%20visual%20tools%20remains%20challenging.%20A%0Asignificant%20hurdle%20is%20the%20current%20lack%20of%20standardized%20infrastructure%2C%20which%0Ahinders%20integrating%20diverse%20tools%2C%20generating%20rich%20interaction%20data%2C%20and%0Atraining%20robust%20agents%20effectively.%20To%20address%20these%20gaps%2C%20we%20introduce%0AOpenThinkIMG%2C%20the%20first%20open-source%2C%20comprehensive%20end-to-end%20framework%20for%0Atool-augmented%20LVLMs.%20It%20features%20standardized%20vision%20tool%20interfaces%2C%20scalable%0Atrajectory%20generation%20for%20policy%20initialization%2C%20and%20a%20flexible%20training%0Aenvironment.%20Furthermore%2C%20considering%20supervised%20fine-tuning%20%28SFT%29%20on%20static%0Ademonstrations%20offers%20limited%20policy%20generalization%20for%20dynamic%20tool%0Ainvocation%2C%20we%20propose%20a%20novel%20reinforcement%20learning%20%28RL%29%20framework%20V-ToolRL%0Ato%20train%20LVLMs%20to%20learn%20adaptive%20policies%20for%20invoking%20external%20vision%20tools.%0AV-ToolRL%20enables%20LVLMs%20to%20autonomously%20discover%20optimal%20tool-usage%20strategies%0Aby%20directly%20optimizing%20for%20task%20success%20using%20feedback%20from%20tool%20interactions.%0AWe%20empirically%20validate%20V-ToolRL%20on%20challenging%20chart%20reasoning%20tasks.%20Our%0ARL-trained%20agent%2C%20built%20upon%20a%20Qwen2-VL-2B%2C%20significantly%20outperforms%20its%0ASFT-initialized%20counterpart%20%28%2B28.83%20points%29%20and%20surpasses%20established%0Asupervised%20tool-learning%20baselines%20like%20Taco%20and%20CogCom%20by%20an%20average%20of%20%2B12.7%0Apoints.%20Notably%2C%20it%20also%20surpasses%20prominent%20closed-source%20models%20like%20GPT-4.1%0Aby%20%2B8.68%20accuracy%20points.%20We%20hope%20OpenThinkIMG%20can%20serve%20as%20a%20foundational%0Aframework%20for%20advancing%20dynamic%2C%20tool-augmented%20visual%20reasoning%2C%20helping%20the%0Acommunity%20develop%20AI%20agents%20that%20can%20genuinely%20%22think%20with%20images%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08617v2&entry.124074799=Read"},
{"title": "Know Your Attention Maps: Class-specific Token Masking for Weakly\n  Supervised Semantic Segmentation", "author": "Joelle Hanna and Damian Borth", "abstract": "  Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that\nhas been extensively studied in recent years. Traditional approaches often rely\non external modules like Class Activation Maps to highlight regions of interest\nand generate pseudo segmentation masks. In this work, we propose an end-to-end\nmethod that directly utilizes the attention maps learned by a Vision\nTransformer (ViT) for WSSS. We propose training a sparse ViT with multiple\n[CLS] tokens (one for each class), using a random masking strategy to promote\n[CLS] token - class assignment. At inference time, we aggregate the different\nself-attention maps of each [CLS] token corresponding to the predicted labels\nto generate pseudo segmentation masks. Our proposed approach enhances the\ninterpretability of self-attention maps and ensures accurate class assignments.\nExtensive experiments on two standard benchmarks and three specialized datasets\ndemonstrate that our method generates accurate pseudo-masks, outperforming\nrelated works. Those pseudo-masks can be used to train a segmentation model\nwhich achieves results comparable to fully-supervised models, significantly\nreducing the need for fine-grained labeled data.\n", "link": "http://arxiv.org/abs/2507.06848v1", "date": "2025-07-09", "relevancy": 2.7444, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5607}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5599}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Know%20Your%20Attention%20Maps%3A%20Class-specific%20Token%20Masking%20for%20Weakly%0A%20%20Supervised%20Semantic%20Segmentation&body=Title%3A%20Know%20Your%20Attention%20Maps%3A%20Class-specific%20Token%20Masking%20for%20Weakly%0A%20%20Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Joelle%20Hanna%20and%20Damian%20Borth%0AAbstract%3A%20%20%20Weakly%20Supervised%20Semantic%20Segmentation%20%28WSSS%29%20is%20a%20challenging%20problem%20that%0Ahas%20been%20extensively%20studied%20in%20recent%20years.%20Traditional%20approaches%20often%20rely%0Aon%20external%20modules%20like%20Class%20Activation%20Maps%20to%20highlight%20regions%20of%20interest%0Aand%20generate%20pseudo%20segmentation%20masks.%20In%20this%20work%2C%20we%20propose%20an%20end-to-end%0Amethod%20that%20directly%20utilizes%20the%20attention%20maps%20learned%20by%20a%20Vision%0ATransformer%20%28ViT%29%20for%20WSSS.%20We%20propose%20training%20a%20sparse%20ViT%20with%20multiple%0A%5BCLS%5D%20tokens%20%28one%20for%20each%20class%29%2C%20using%20a%20random%20masking%20strategy%20to%20promote%0A%5BCLS%5D%20token%20-%20class%20assignment.%20At%20inference%20time%2C%20we%20aggregate%20the%20different%0Aself-attention%20maps%20of%20each%20%5BCLS%5D%20token%20corresponding%20to%20the%20predicted%20labels%0Ato%20generate%20pseudo%20segmentation%20masks.%20Our%20proposed%20approach%20enhances%20the%0Ainterpretability%20of%20self-attention%20maps%20and%20ensures%20accurate%20class%20assignments.%0AExtensive%20experiments%20on%20two%20standard%20benchmarks%20and%20three%20specialized%20datasets%0Ademonstrate%20that%20our%20method%20generates%20accurate%20pseudo-masks%2C%20outperforming%0Arelated%20works.%20Those%20pseudo-masks%20can%20be%20used%20to%20train%20a%20segmentation%20model%0Awhich%20achieves%20results%20comparable%20to%20fully-supervised%20models%2C%20significantly%0Areducing%20the%20need%20for%20fine-grained%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnow%2520Your%2520Attention%2520Maps%253A%2520Class-specific%2520Token%2520Masking%2520for%2520Weakly%250A%2520%2520Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DJoelle%2520Hanna%2520and%2520Damian%2520Borth%26entry.1292438233%3D%2520%2520Weakly%2520Supervised%2520Semantic%2520Segmentation%2520%2528WSSS%2529%2520is%2520a%2520challenging%2520problem%2520that%250Ahas%2520been%2520extensively%2520studied%2520in%2520recent%2520years.%2520Traditional%2520approaches%2520often%2520rely%250Aon%2520external%2520modules%2520like%2520Class%2520Activation%2520Maps%2520to%2520highlight%2520regions%2520of%2520interest%250Aand%2520generate%2520pseudo%2520segmentation%2520masks.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520end-to-end%250Amethod%2520that%2520directly%2520utilizes%2520the%2520attention%2520maps%2520learned%2520by%2520a%2520Vision%250ATransformer%2520%2528ViT%2529%2520for%2520WSSS.%2520We%2520propose%2520training%2520a%2520sparse%2520ViT%2520with%2520multiple%250A%255BCLS%255D%2520tokens%2520%2528one%2520for%2520each%2520class%2529%252C%2520using%2520a%2520random%2520masking%2520strategy%2520to%2520promote%250A%255BCLS%255D%2520token%2520-%2520class%2520assignment.%2520At%2520inference%2520time%252C%2520we%2520aggregate%2520the%2520different%250Aself-attention%2520maps%2520of%2520each%2520%255BCLS%255D%2520token%2520corresponding%2520to%2520the%2520predicted%2520labels%250Ato%2520generate%2520pseudo%2520segmentation%2520masks.%2520Our%2520proposed%2520approach%2520enhances%2520the%250Ainterpretability%2520of%2520self-attention%2520maps%2520and%2520ensures%2520accurate%2520class%2520assignments.%250AExtensive%2520experiments%2520on%2520two%2520standard%2520benchmarks%2520and%2520three%2520specialized%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520generates%2520accurate%2520pseudo-masks%252C%2520outperforming%250Arelated%2520works.%2520Those%2520pseudo-masks%2520can%2520be%2520used%2520to%2520train%2520a%2520segmentation%2520model%250Awhich%2520achieves%2520results%2520comparable%2520to%2520fully-supervised%2520models%252C%2520significantly%250Areducing%2520the%2520need%2520for%2520fine-grained%2520labeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Know%20Your%20Attention%20Maps%3A%20Class-specific%20Token%20Masking%20for%20Weakly%0A%20%20Supervised%20Semantic%20Segmentation&entry.906535625=Joelle%20Hanna%20and%20Damian%20Borth&entry.1292438233=%20%20Weakly%20Supervised%20Semantic%20Segmentation%20%28WSSS%29%20is%20a%20challenging%20problem%20that%0Ahas%20been%20extensively%20studied%20in%20recent%20years.%20Traditional%20approaches%20often%20rely%0Aon%20external%20modules%20like%20Class%20Activation%20Maps%20to%20highlight%20regions%20of%20interest%0Aand%20generate%20pseudo%20segmentation%20masks.%20In%20this%20work%2C%20we%20propose%20an%20end-to-end%0Amethod%20that%20directly%20utilizes%20the%20attention%20maps%20learned%20by%20a%20Vision%0ATransformer%20%28ViT%29%20for%20WSSS.%20We%20propose%20training%20a%20sparse%20ViT%20with%20multiple%0A%5BCLS%5D%20tokens%20%28one%20for%20each%20class%29%2C%20using%20a%20random%20masking%20strategy%20to%20promote%0A%5BCLS%5D%20token%20-%20class%20assignment.%20At%20inference%20time%2C%20we%20aggregate%20the%20different%0Aself-attention%20maps%20of%20each%20%5BCLS%5D%20token%20corresponding%20to%20the%20predicted%20labels%0Ato%20generate%20pseudo%20segmentation%20masks.%20Our%20proposed%20approach%20enhances%20the%0Ainterpretability%20of%20self-attention%20maps%20and%20ensures%20accurate%20class%20assignments.%0AExtensive%20experiments%20on%20two%20standard%20benchmarks%20and%20three%20specialized%20datasets%0Ademonstrate%20that%20our%20method%20generates%20accurate%20pseudo-masks%2C%20outperforming%0Arelated%20works.%20Those%20pseudo-masks%20can%20be%20used%20to%20train%20a%20segmentation%20model%0Awhich%20achieves%20results%20comparable%20to%20fully-supervised%20models%2C%20significantly%0Areducing%20the%20need%20for%20fine-grained%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06848v1&entry.124074799=Read"},
{"title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning\n  in Multimodal LLMs", "author": "Yahan Yu and Yuyang Dong and Masafumi Oyamada", "abstract": "  Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility.\n", "link": "http://arxiv.org/abs/2507.06999v1", "date": "2025-07-09", "relevancy": 2.7367, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Deliberately%2C%20Acting%20Intuitively%3A%20Unlocking%20Test-Time%20Reasoning%0A%20%20in%20Multimodal%20LLMs&body=Title%3A%20Learning%20Deliberately%2C%20Acting%20Intuitively%3A%20Unlocking%20Test-Time%20Reasoning%0A%20%20in%20Multimodal%20LLMs%0AAuthor%3A%20Yahan%20Yu%20and%20Yuyang%20Dong%20and%20Masafumi%20Oyamada%0AAbstract%3A%20%20%20Reasoning%20is%20a%20key%20capability%20for%20large%20language%20models%20%28LLMs%29%2C%20particularly%0Awhen%20applied%20to%20complex%20tasks%20such%20as%20mathematical%20problem%20solving.%20However%2C%0Amultimodal%20reasoning%20research%20still%20requires%20further%20exploration%20of%20modality%0Aalignment%20and%20training%20costs.%20Many%20of%20these%20approaches%20rely%20on%20additional%20data%0Aannotation%20and%20relevant%20rule-based%20rewards%20to%20enhance%20the%20understanding%20and%0Areasoning%20ability%2C%20which%20significantly%20increases%20training%20costs%20and%20limits%0Ascalability.%20To%20address%20these%20challenges%2C%20we%20propose%20the%0ADeliberate-to-Intuitive%20reasoning%20framework%20%28D2I%29%20that%20improves%20the%0Aunderstanding%20and%20reasoning%20ability%20of%20multimodal%20LLMs%20%28MLLMs%29%20without%20extra%0Aannotations%20and%20complex%20rewards.%20Specifically%2C%20our%20method%20sets%20deliberate%0Areasoning%20strategies%20to%20enhance%20modality%20alignment%20only%20through%20the%20rule-based%0Aformat%20reward%20during%20training.%20While%20evaluating%2C%20the%20reasoning%20style%20shifts%20to%0Aintuitive%2C%20which%20removes%20deliberate%20reasoning%20strategies%20during%20training%20and%0Aimplicitly%20reflects%20the%20model%27s%20acquired%20abilities%20in%20the%20response.%20D2I%0Aoutperforms%20baselines%20across%20both%20in-domain%20and%20out-of-domain%20benchmarks.%20Our%0Afindings%20highlight%20the%20role%20of%20format%20reward%20in%20fostering%20transferable%0Areasoning%20skills%20in%20MLLMs%2C%20and%20inspire%20directions%20for%20decoupling%20training-time%0Areasoning%20depth%20from%20test-time%20response%20flexibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Deliberately%252C%2520Acting%2520Intuitively%253A%2520Unlocking%2520Test-Time%2520Reasoning%250A%2520%2520in%2520Multimodal%2520LLMs%26entry.906535625%3DYahan%2520Yu%2520and%2520Yuyang%2520Dong%2520and%2520Masafumi%2520Oyamada%26entry.1292438233%3D%2520%2520Reasoning%2520is%2520a%2520key%2520capability%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520particularly%250Awhen%2520applied%2520to%2520complex%2520tasks%2520such%2520as%2520mathematical%2520problem%2520solving.%2520However%252C%250Amultimodal%2520reasoning%2520research%2520still%2520requires%2520further%2520exploration%2520of%2520modality%250Aalignment%2520and%2520training%2520costs.%2520Many%2520of%2520these%2520approaches%2520rely%2520on%2520additional%2520data%250Aannotation%2520and%2520relevant%2520rule-based%2520rewards%2520to%2520enhance%2520the%2520understanding%2520and%250Areasoning%2520ability%252C%2520which%2520significantly%2520increases%2520training%2520costs%2520and%2520limits%250Ascalability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%250ADeliberate-to-Intuitive%2520reasoning%2520framework%2520%2528D2I%2529%2520that%2520improves%2520the%250Aunderstanding%2520and%2520reasoning%2520ability%2520of%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520without%2520extra%250Aannotations%2520and%2520complex%2520rewards.%2520Specifically%252C%2520our%2520method%2520sets%2520deliberate%250Areasoning%2520strategies%2520to%2520enhance%2520modality%2520alignment%2520only%2520through%2520the%2520rule-based%250Aformat%2520reward%2520during%2520training.%2520While%2520evaluating%252C%2520the%2520reasoning%2520style%2520shifts%2520to%250Aintuitive%252C%2520which%2520removes%2520deliberate%2520reasoning%2520strategies%2520during%2520training%2520and%250Aimplicitly%2520reflects%2520the%2520model%2527s%2520acquired%2520abilities%2520in%2520the%2520response.%2520D2I%250Aoutperforms%2520baselines%2520across%2520both%2520in-domain%2520and%2520out-of-domain%2520benchmarks.%2520Our%250Afindings%2520highlight%2520the%2520role%2520of%2520format%2520reward%2520in%2520fostering%2520transferable%250Areasoning%2520skills%2520in%2520MLLMs%252C%2520and%2520inspire%2520directions%2520for%2520decoupling%2520training-time%250Areasoning%2520depth%2520from%2520test-time%2520response%2520flexibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Deliberately%2C%20Acting%20Intuitively%3A%20Unlocking%20Test-Time%20Reasoning%0A%20%20in%20Multimodal%20LLMs&entry.906535625=Yahan%20Yu%20and%20Yuyang%20Dong%20and%20Masafumi%20Oyamada&entry.1292438233=%20%20Reasoning%20is%20a%20key%20capability%20for%20large%20language%20models%20%28LLMs%29%2C%20particularly%0Awhen%20applied%20to%20complex%20tasks%20such%20as%20mathematical%20problem%20solving.%20However%2C%0Amultimodal%20reasoning%20research%20still%20requires%20further%20exploration%20of%20modality%0Aalignment%20and%20training%20costs.%20Many%20of%20these%20approaches%20rely%20on%20additional%20data%0Aannotation%20and%20relevant%20rule-based%20rewards%20to%20enhance%20the%20understanding%20and%0Areasoning%20ability%2C%20which%20significantly%20increases%20training%20costs%20and%20limits%0Ascalability.%20To%20address%20these%20challenges%2C%20we%20propose%20the%0ADeliberate-to-Intuitive%20reasoning%20framework%20%28D2I%29%20that%20improves%20the%0Aunderstanding%20and%20reasoning%20ability%20of%20multimodal%20LLMs%20%28MLLMs%29%20without%20extra%0Aannotations%20and%20complex%20rewards.%20Specifically%2C%20our%20method%20sets%20deliberate%0Areasoning%20strategies%20to%20enhance%20modality%20alignment%20only%20through%20the%20rule-based%0Aformat%20reward%20during%20training.%20While%20evaluating%2C%20the%20reasoning%20style%20shifts%20to%0Aintuitive%2C%20which%20removes%20deliberate%20reasoning%20strategies%20during%20training%20and%0Aimplicitly%20reflects%20the%20model%27s%20acquired%20abilities%20in%20the%20response.%20D2I%0Aoutperforms%20baselines%20across%20both%20in-domain%20and%20out-of-domain%20benchmarks.%20Our%0Afindings%20highlight%20the%20role%20of%20format%20reward%20in%20fostering%20transferable%0Areasoning%20skills%20in%20MLLMs%2C%20and%20inspire%20directions%20for%20decoupling%20training-time%0Areasoning%20depth%20from%20test-time%20response%20flexibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06999v1&entry.124074799=Read"},
{"title": "Tissue Concepts v2: A Supervised Foundation Model For Whole Slide Images", "author": "Till Nicke and Daniela Schacherer and Jan Raphael Sch\u00e4fer and Natalia Artysh and Antje Prasse and Andr\u00e9 Homeyer and Andrea Schenk and Henning H\u00f6fener and Johannes Lotz", "abstract": "  Foundation models (FMs) are transforming the field of computational pathology\nby offering new approaches to analyzing histopathology images. Typically\nrelying on weeks of training on large databases, the creation of FMs is a\nresource-intensive process in many ways. In this paper, we introduce the\nextension of our supervised foundation model, Tissue Concepts, to whole slide\nimages, called Tissue Concepts v2 (TCv2), a supervised foundation model for\nwhole slide images to address the issue above. TCv2 uses supervised, end-to-end\nmultitask learning on slide-level labels. Training TCv2 uses a fraction of the\ntraining resources compared to self-supervised training. The presented model\nshows superior performance compared to SSL-trained models in cancer subtyping\nbenchmarks and is fully trained on freely available data. Furthermore, a shared\ntrained attention module provides an additional layer of explainability across\ndifferent tasks.\n", "link": "http://arxiv.org/abs/2507.05742v2", "date": "2025-07-09", "relevancy": 2.7153, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tissue%20Concepts%20v2%3A%20A%20Supervised%20Foundation%20Model%20For%20Whole%20Slide%20Images&body=Title%3A%20Tissue%20Concepts%20v2%3A%20A%20Supervised%20Foundation%20Model%20For%20Whole%20Slide%20Images%0AAuthor%3A%20Till%20Nicke%20and%20Daniela%20Schacherer%20and%20Jan%20Raphael%20Sch%C3%A4fer%20and%20Natalia%20Artysh%20and%20Antje%20Prasse%20and%20Andr%C3%A9%20Homeyer%20and%20Andrea%20Schenk%20and%20Henning%20H%C3%B6fener%20and%20Johannes%20Lotz%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20are%20transforming%20the%20field%20of%20computational%20pathology%0Aby%20offering%20new%20approaches%20to%20analyzing%20histopathology%20images.%20Typically%0Arelying%20on%20weeks%20of%20training%20on%20large%20databases%2C%20the%20creation%20of%20FMs%20is%20a%0Aresource-intensive%20process%20in%20many%20ways.%20In%20this%20paper%2C%20we%20introduce%20the%0Aextension%20of%20our%20supervised%20foundation%20model%2C%20Tissue%20Concepts%2C%20to%20whole%20slide%0Aimages%2C%20called%20Tissue%20Concepts%20v2%20%28TCv2%29%2C%20a%20supervised%20foundation%20model%20for%0Awhole%20slide%20images%20to%20address%20the%20issue%20above.%20TCv2%20uses%20supervised%2C%20end-to-end%0Amultitask%20learning%20on%20slide-level%20labels.%20Training%20TCv2%20uses%20a%20fraction%20of%20the%0Atraining%20resources%20compared%20to%20self-supervised%20training.%20The%20presented%20model%0Ashows%20superior%20performance%20compared%20to%20SSL-trained%20models%20in%20cancer%20subtyping%0Abenchmarks%20and%20is%20fully%20trained%20on%20freely%20available%20data.%20Furthermore%2C%20a%20shared%0Atrained%20attention%20module%20provides%20an%20additional%20layer%20of%20explainability%20across%0Adifferent%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05742v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTissue%2520Concepts%2520v2%253A%2520A%2520Supervised%2520Foundation%2520Model%2520For%2520Whole%2520Slide%2520Images%26entry.906535625%3DTill%2520Nicke%2520and%2520Daniela%2520Schacherer%2520and%2520Jan%2520Raphael%2520Sch%25C3%25A4fer%2520and%2520Natalia%2520Artysh%2520and%2520Antje%2520Prasse%2520and%2520Andr%25C3%25A9%2520Homeyer%2520and%2520Andrea%2520Schenk%2520and%2520Henning%2520H%25C3%25B6fener%2520and%2520Johannes%2520Lotz%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520are%2520transforming%2520the%2520field%2520of%2520computational%2520pathology%250Aby%2520offering%2520new%2520approaches%2520to%2520analyzing%2520histopathology%2520images.%2520Typically%250Arelying%2520on%2520weeks%2520of%2520training%2520on%2520large%2520databases%252C%2520the%2520creation%2520of%2520FMs%2520is%2520a%250Aresource-intensive%2520process%2520in%2520many%2520ways.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%250Aextension%2520of%2520our%2520supervised%2520foundation%2520model%252C%2520Tissue%2520Concepts%252C%2520to%2520whole%2520slide%250Aimages%252C%2520called%2520Tissue%2520Concepts%2520v2%2520%2528TCv2%2529%252C%2520a%2520supervised%2520foundation%2520model%2520for%250Awhole%2520slide%2520images%2520to%2520address%2520the%2520issue%2520above.%2520TCv2%2520uses%2520supervised%252C%2520end-to-end%250Amultitask%2520learning%2520on%2520slide-level%2520labels.%2520Training%2520TCv2%2520uses%2520a%2520fraction%2520of%2520the%250Atraining%2520resources%2520compared%2520to%2520self-supervised%2520training.%2520The%2520presented%2520model%250Ashows%2520superior%2520performance%2520compared%2520to%2520SSL-trained%2520models%2520in%2520cancer%2520subtyping%250Abenchmarks%2520and%2520is%2520fully%2520trained%2520on%2520freely%2520available%2520data.%2520Furthermore%252C%2520a%2520shared%250Atrained%2520attention%2520module%2520provides%2520an%2520additional%2520layer%2520of%2520explainability%2520across%250Adifferent%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05742v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tissue%20Concepts%20v2%3A%20A%20Supervised%20Foundation%20Model%20For%20Whole%20Slide%20Images&entry.906535625=Till%20Nicke%20and%20Daniela%20Schacherer%20and%20Jan%20Raphael%20Sch%C3%A4fer%20and%20Natalia%20Artysh%20and%20Antje%20Prasse%20and%20Andr%C3%A9%20Homeyer%20and%20Andrea%20Schenk%20and%20Henning%20H%C3%B6fener%20and%20Johannes%20Lotz&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20are%20transforming%20the%20field%20of%20computational%20pathology%0Aby%20offering%20new%20approaches%20to%20analyzing%20histopathology%20images.%20Typically%0Arelying%20on%20weeks%20of%20training%20on%20large%20databases%2C%20the%20creation%20of%20FMs%20is%20a%0Aresource-intensive%20process%20in%20many%20ways.%20In%20this%20paper%2C%20we%20introduce%20the%0Aextension%20of%20our%20supervised%20foundation%20model%2C%20Tissue%20Concepts%2C%20to%20whole%20slide%0Aimages%2C%20called%20Tissue%20Concepts%20v2%20%28TCv2%29%2C%20a%20supervised%20foundation%20model%20for%0Awhole%20slide%20images%20to%20address%20the%20issue%20above.%20TCv2%20uses%20supervised%2C%20end-to-end%0Amultitask%20learning%20on%20slide-level%20labels.%20Training%20TCv2%20uses%20a%20fraction%20of%20the%0Atraining%20resources%20compared%20to%20self-supervised%20training.%20The%20presented%20model%0Ashows%20superior%20performance%20compared%20to%20SSL-trained%20models%20in%20cancer%20subtyping%0Abenchmarks%20and%20is%20fully%20trained%20on%20freely%20available%20data.%20Furthermore%2C%20a%20shared%0Atrained%20attention%20module%20provides%20an%20additional%20layer%20of%20explainability%20across%0Adifferent%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05742v2&entry.124074799=Read"},
{"title": "Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise\n  Conditioning", "author": "Jihao Andreas Lin", "abstract": "  Gaussian processes are a powerful framework for uncertainty-aware function\napproximation and sequential decision-making. Unfortunately, their classical\nformulation does not scale gracefully to large amounts of data and modern\nhardware for massively-parallel computation, prompting many researchers to\ndevelop techniques which improve their scalability. This dissertation focuses\non the powerful combination of iterative methods and pathwise conditioning to\ndevelop methodological contributions which facilitate the use of Gaussian\nprocesses in modern large-scale settings. By combining these two techniques\nsynergistically, expensive computations are expressed as solutions to systems\nof linear equations and obtained by leveraging iterative linear system solvers.\nThis drastically reduces memory requirements, facilitating application to\nsignificantly larger amounts of data, and introduces matrix multiplication as\nthe main computational operation, which is ideal for modern hardware.\n", "link": "http://arxiv.org/abs/2507.06839v1", "date": "2025-07-09", "relevancy": 2.6626, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5409}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5405}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Gaussian%20Processes%3A%20Advances%20in%20Iterative%20Methods%20and%20Pathwise%0A%20%20Conditioning&body=Title%3A%20Scalable%20Gaussian%20Processes%3A%20Advances%20in%20Iterative%20Methods%20and%20Pathwise%0A%20%20Conditioning%0AAuthor%3A%20Jihao%20Andreas%20Lin%0AAbstract%3A%20%20%20Gaussian%20processes%20are%20a%20powerful%20framework%20for%20uncertainty-aware%20function%0Aapproximation%20and%20sequential%20decision-making.%20Unfortunately%2C%20their%20classical%0Aformulation%20does%20not%20scale%20gracefully%20to%20large%20amounts%20of%20data%20and%20modern%0Ahardware%20for%20massively-parallel%20computation%2C%20prompting%20many%20researchers%20to%0Adevelop%20techniques%20which%20improve%20their%20scalability.%20This%20dissertation%20focuses%0Aon%20the%20powerful%20combination%20of%20iterative%20methods%20and%20pathwise%20conditioning%20to%0Adevelop%20methodological%20contributions%20which%20facilitate%20the%20use%20of%20Gaussian%0Aprocesses%20in%20modern%20large-scale%20settings.%20By%20combining%20these%20two%20techniques%0Asynergistically%2C%20expensive%20computations%20are%20expressed%20as%20solutions%20to%20systems%0Aof%20linear%20equations%20and%20obtained%20by%20leveraging%20iterative%20linear%20system%20solvers.%0AThis%20drastically%20reduces%20memory%20requirements%2C%20facilitating%20application%20to%0Asignificantly%20larger%20amounts%20of%20data%2C%20and%20introduces%20matrix%20multiplication%20as%0Athe%20main%20computational%20operation%2C%20which%20is%20ideal%20for%20modern%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Gaussian%2520Processes%253A%2520Advances%2520in%2520Iterative%2520Methods%2520and%2520Pathwise%250A%2520%2520Conditioning%26entry.906535625%3DJihao%2520Andreas%2520Lin%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520are%2520a%2520powerful%2520framework%2520for%2520uncertainty-aware%2520function%250Aapproximation%2520and%2520sequential%2520decision-making.%2520Unfortunately%252C%2520their%2520classical%250Aformulation%2520does%2520not%2520scale%2520gracefully%2520to%2520large%2520amounts%2520of%2520data%2520and%2520modern%250Ahardware%2520for%2520massively-parallel%2520computation%252C%2520prompting%2520many%2520researchers%2520to%250Adevelop%2520techniques%2520which%2520improve%2520their%2520scalability.%2520This%2520dissertation%2520focuses%250Aon%2520the%2520powerful%2520combination%2520of%2520iterative%2520methods%2520and%2520pathwise%2520conditioning%2520to%250Adevelop%2520methodological%2520contributions%2520which%2520facilitate%2520the%2520use%2520of%2520Gaussian%250Aprocesses%2520in%2520modern%2520large-scale%2520settings.%2520By%2520combining%2520these%2520two%2520techniques%250Asynergistically%252C%2520expensive%2520computations%2520are%2520expressed%2520as%2520solutions%2520to%2520systems%250Aof%2520linear%2520equations%2520and%2520obtained%2520by%2520leveraging%2520iterative%2520linear%2520system%2520solvers.%250AThis%2520drastically%2520reduces%2520memory%2520requirements%252C%2520facilitating%2520application%2520to%250Asignificantly%2520larger%2520amounts%2520of%2520data%252C%2520and%2520introduces%2520matrix%2520multiplication%2520as%250Athe%2520main%2520computational%2520operation%252C%2520which%2520is%2520ideal%2520for%2520modern%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Gaussian%20Processes%3A%20Advances%20in%20Iterative%20Methods%20and%20Pathwise%0A%20%20Conditioning&entry.906535625=Jihao%20Andreas%20Lin&entry.1292438233=%20%20Gaussian%20processes%20are%20a%20powerful%20framework%20for%20uncertainty-aware%20function%0Aapproximation%20and%20sequential%20decision-making.%20Unfortunately%2C%20their%20classical%0Aformulation%20does%20not%20scale%20gracefully%20to%20large%20amounts%20of%20data%20and%20modern%0Ahardware%20for%20massively-parallel%20computation%2C%20prompting%20many%20researchers%20to%0Adevelop%20techniques%20which%20improve%20their%20scalability.%20This%20dissertation%20focuses%0Aon%20the%20powerful%20combination%20of%20iterative%20methods%20and%20pathwise%20conditioning%20to%0Adevelop%20methodological%20contributions%20which%20facilitate%20the%20use%20of%20Gaussian%0Aprocesses%20in%20modern%20large-scale%20settings.%20By%20combining%20these%20two%20techniques%0Asynergistically%2C%20expensive%20computations%20are%20expressed%20as%20solutions%20to%20systems%0Aof%20linear%20equations%20and%20obtained%20by%20leveraging%20iterative%20linear%20system%20solvers.%0AThis%20drastically%20reduces%20memory%20requirements%2C%20facilitating%20application%20to%0Asignificantly%20larger%20amounts%20of%20data%2C%20and%20introduces%20matrix%20multiplication%20as%0Athe%20main%20computational%20operation%2C%20which%20is%20ideal%20for%20modern%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06839v1&entry.124074799=Read"},
{"title": "SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction", "author": "Kaveh Moradkhani and R Jarrett Rushmore and Sylvain Bouix", "abstract": "  Accurate cortical surface reconstruction from magnetic resonance imaging\n(MRI) data is crucial for reliable neuroanatomical analyses. Current methods\nhave to contend with complex cortical geometries, strict topological\nrequirements, and often produce surfaces with overlaps, self-intersections, and\ntopological defects. To overcome these shortcomings, we introduce SimCortex, a\ndeep learning framework that simultaneously reconstructs all brain surfaces\n(left/right white-matter and pial) from T1-weighted(T1w) MRI volumes while\npreserving topological properties. Our method first segments the T1w image into\na nine-class tissue label map. From these segmentations, we generate\nsubject-specific, collision-free initial surface meshes. These surfaces serve\nas precise initializations for subsequent multiscale diffeomorphic\ndeformations. Employing stationary velocity fields (SVFs) integrated via\nscaling-and-squaring, our approach ensures smooth, topology-preserving\ntransformations with significantly reduced surface collisions and\nself-intersections. Evaluations on standard datasets demonstrate that SimCortex\ndramatically reduces surface overlaps and self-intersections, surpassing\ncurrent methods while maintaining state-of-the-art geometric accuracy.\n", "link": "http://arxiv.org/abs/2507.06955v1", "date": "2025-07-09", "relevancy": 2.6594, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimCortex%3A%20Collision-free%20Simultaneous%20Cortical%20Surfaces%20Reconstruction&body=Title%3A%20SimCortex%3A%20Collision-free%20Simultaneous%20Cortical%20Surfaces%20Reconstruction%0AAuthor%3A%20Kaveh%20Moradkhani%20and%20R%20Jarrett%20Rushmore%20and%20Sylvain%20Bouix%0AAbstract%3A%20%20%20Accurate%20cortical%20surface%20reconstruction%20from%20magnetic%20resonance%20imaging%0A%28MRI%29%20data%20is%20crucial%20for%20reliable%20neuroanatomical%20analyses.%20Current%20methods%0Ahave%20to%20contend%20with%20complex%20cortical%20geometries%2C%20strict%20topological%0Arequirements%2C%20and%20often%20produce%20surfaces%20with%20overlaps%2C%20self-intersections%2C%20and%0Atopological%20defects.%20To%20overcome%20these%20shortcomings%2C%20we%20introduce%20SimCortex%2C%20a%0Adeep%20learning%20framework%20that%20simultaneously%20reconstructs%20all%20brain%20surfaces%0A%28left/right%20white-matter%20and%20pial%29%20from%20T1-weighted%28T1w%29%20MRI%20volumes%20while%0Apreserving%20topological%20properties.%20Our%20method%20first%20segments%20the%20T1w%20image%20into%0Aa%20nine-class%20tissue%20label%20map.%20From%20these%20segmentations%2C%20we%20generate%0Asubject-specific%2C%20collision-free%20initial%20surface%20meshes.%20These%20surfaces%20serve%0Aas%20precise%20initializations%20for%20subsequent%20multiscale%20diffeomorphic%0Adeformations.%20Employing%20stationary%20velocity%20fields%20%28SVFs%29%20integrated%20via%0Ascaling-and-squaring%2C%20our%20approach%20ensures%20smooth%2C%20topology-preserving%0Atransformations%20with%20significantly%20reduced%20surface%20collisions%20and%0Aself-intersections.%20Evaluations%20on%20standard%20datasets%20demonstrate%20that%20SimCortex%0Adramatically%20reduces%20surface%20overlaps%20and%20self-intersections%2C%20surpassing%0Acurrent%20methods%20while%20maintaining%20state-of-the-art%20geometric%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimCortex%253A%2520Collision-free%2520Simultaneous%2520Cortical%2520Surfaces%2520Reconstruction%26entry.906535625%3DKaveh%2520Moradkhani%2520and%2520R%2520Jarrett%2520Rushmore%2520and%2520Sylvain%2520Bouix%26entry.1292438233%3D%2520%2520Accurate%2520cortical%2520surface%2520reconstruction%2520from%2520magnetic%2520resonance%2520imaging%250A%2528MRI%2529%2520data%2520is%2520crucial%2520for%2520reliable%2520neuroanatomical%2520analyses.%2520Current%2520methods%250Ahave%2520to%2520contend%2520with%2520complex%2520cortical%2520geometries%252C%2520strict%2520topological%250Arequirements%252C%2520and%2520often%2520produce%2520surfaces%2520with%2520overlaps%252C%2520self-intersections%252C%2520and%250Atopological%2520defects.%2520To%2520overcome%2520these%2520shortcomings%252C%2520we%2520introduce%2520SimCortex%252C%2520a%250Adeep%2520learning%2520framework%2520that%2520simultaneously%2520reconstructs%2520all%2520brain%2520surfaces%250A%2528left/right%2520white-matter%2520and%2520pial%2529%2520from%2520T1-weighted%2528T1w%2529%2520MRI%2520volumes%2520while%250Apreserving%2520topological%2520properties.%2520Our%2520method%2520first%2520segments%2520the%2520T1w%2520image%2520into%250Aa%2520nine-class%2520tissue%2520label%2520map.%2520From%2520these%2520segmentations%252C%2520we%2520generate%250Asubject-specific%252C%2520collision-free%2520initial%2520surface%2520meshes.%2520These%2520surfaces%2520serve%250Aas%2520precise%2520initializations%2520for%2520subsequent%2520multiscale%2520diffeomorphic%250Adeformations.%2520Employing%2520stationary%2520velocity%2520fields%2520%2528SVFs%2529%2520integrated%2520via%250Ascaling-and-squaring%252C%2520our%2520approach%2520ensures%2520smooth%252C%2520topology-preserving%250Atransformations%2520with%2520significantly%2520reduced%2520surface%2520collisions%2520and%250Aself-intersections.%2520Evaluations%2520on%2520standard%2520datasets%2520demonstrate%2520that%2520SimCortex%250Adramatically%2520reduces%2520surface%2520overlaps%2520and%2520self-intersections%252C%2520surpassing%250Acurrent%2520methods%2520while%2520maintaining%2520state-of-the-art%2520geometric%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimCortex%3A%20Collision-free%20Simultaneous%20Cortical%20Surfaces%20Reconstruction&entry.906535625=Kaveh%20Moradkhani%20and%20R%20Jarrett%20Rushmore%20and%20Sylvain%20Bouix&entry.1292438233=%20%20Accurate%20cortical%20surface%20reconstruction%20from%20magnetic%20resonance%20imaging%0A%28MRI%29%20data%20is%20crucial%20for%20reliable%20neuroanatomical%20analyses.%20Current%20methods%0Ahave%20to%20contend%20with%20complex%20cortical%20geometries%2C%20strict%20topological%0Arequirements%2C%20and%20often%20produce%20surfaces%20with%20overlaps%2C%20self-intersections%2C%20and%0Atopological%20defects.%20To%20overcome%20these%20shortcomings%2C%20we%20introduce%20SimCortex%2C%20a%0Adeep%20learning%20framework%20that%20simultaneously%20reconstructs%20all%20brain%20surfaces%0A%28left/right%20white-matter%20and%20pial%29%20from%20T1-weighted%28T1w%29%20MRI%20volumes%20while%0Apreserving%20topological%20properties.%20Our%20method%20first%20segments%20the%20T1w%20image%20into%0Aa%20nine-class%20tissue%20label%20map.%20From%20these%20segmentations%2C%20we%20generate%0Asubject-specific%2C%20collision-free%20initial%20surface%20meshes.%20These%20surfaces%20serve%0Aas%20precise%20initializations%20for%20subsequent%20multiscale%20diffeomorphic%0Adeformations.%20Employing%20stationary%20velocity%20fields%20%28SVFs%29%20integrated%20via%0Ascaling-and-squaring%2C%20our%20approach%20ensures%20smooth%2C%20topology-preserving%0Atransformations%20with%20significantly%20reduced%20surface%20collisions%20and%0Aself-intersections.%20Evaluations%20on%20standard%20datasets%20demonstrate%20that%20SimCortex%0Adramatically%20reduces%20surface%20overlaps%20and%20self-intersections%2C%20surpassing%0Acurrent%20methods%20while%20maintaining%20state-of-the-art%20geometric%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06955v1&entry.124074799=Read"},
{"title": "Physics-Grounded Motion Forecasting via Equation Discovery for\n  Trajectory-Guided Image-to-Video Generation", "author": "Tao Feng and Xianbing Zhao and Zhenhua Chen and Tien Tsin Wong and Hamid Rezatofighi and Gholamreza Haffari and Lizhen Qu", "abstract": "  Recent advances in diffusion-based and autoregressive video generation models\nhave achieved remarkable visual realism. However, these models typically lack\naccurate physical alignment, failing to replicate real-world dynamics in object\nmotion. This limitation arises primarily from their reliance on learned\nstatistical correlations rather than capturing mechanisms adhering to physical\nlaws. To address this issue, we introduce a novel framework that integrates\nsymbolic regression (SR) and trajectory-guided image-to-video (I2V) models for\nphysics-grounded video forecasting. Our approach extracts motion trajectories\nfrom input videos, uses a retrieval-based pre-training mechanism to enhance\nsymbolic regression, and discovers equations of motion to forecast physically\naccurate future trajectories. These trajectories then guide video generation\nwithout requiring fine-tuning of existing models. Evaluated on scenarios in\nClassical Mechanics, including spring-mass, pendulums, and projectile motions,\nour method successfully recovers ground-truth analytical equations and improves\nthe physical alignment of generated videos over baseline methods.\n", "link": "http://arxiv.org/abs/2507.06830v1", "date": "2025-07-09", "relevancy": 2.595, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7104}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6095}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Grounded%20Motion%20Forecasting%20via%20Equation%20Discovery%20for%0A%20%20Trajectory-Guided%20Image-to-Video%20Generation&body=Title%3A%20Physics-Grounded%20Motion%20Forecasting%20via%20Equation%20Discovery%20for%0A%20%20Trajectory-Guided%20Image-to-Video%20Generation%0AAuthor%3A%20Tao%20Feng%20and%20Xianbing%20Zhao%20and%20Zhenhua%20Chen%20and%20Tien%20Tsin%20Wong%20and%20Hamid%20Rezatofighi%20and%20Gholamreza%20Haffari%20and%20Lizhen%20Qu%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion-based%20and%20autoregressive%20video%20generation%20models%0Ahave%20achieved%20remarkable%20visual%20realism.%20However%2C%20these%20models%20typically%20lack%0Aaccurate%20physical%20alignment%2C%20failing%20to%20replicate%20real-world%20dynamics%20in%20object%0Amotion.%20This%20limitation%20arises%20primarily%20from%20their%20reliance%20on%20learned%0Astatistical%20correlations%20rather%20than%20capturing%20mechanisms%20adhering%20to%20physical%0Alaws.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20framework%20that%20integrates%0Asymbolic%20regression%20%28SR%29%20and%20trajectory-guided%20image-to-video%20%28I2V%29%20models%20for%0Aphysics-grounded%20video%20forecasting.%20Our%20approach%20extracts%20motion%20trajectories%0Afrom%20input%20videos%2C%20uses%20a%20retrieval-based%20pre-training%20mechanism%20to%20enhance%0Asymbolic%20regression%2C%20and%20discovers%20equations%20of%20motion%20to%20forecast%20physically%0Aaccurate%20future%20trajectories.%20These%20trajectories%20then%20guide%20video%20generation%0Awithout%20requiring%20fine-tuning%20of%20existing%20models.%20Evaluated%20on%20scenarios%20in%0AClassical%20Mechanics%2C%20including%20spring-mass%2C%20pendulums%2C%20and%20projectile%20motions%2C%0Aour%20method%20successfully%20recovers%20ground-truth%20analytical%20equations%20and%20improves%0Athe%20physical%20alignment%20of%20generated%20videos%20over%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Grounded%2520Motion%2520Forecasting%2520via%2520Equation%2520Discovery%2520for%250A%2520%2520Trajectory-Guided%2520Image-to-Video%2520Generation%26entry.906535625%3DTao%2520Feng%2520and%2520Xianbing%2520Zhao%2520and%2520Zhenhua%2520Chen%2520and%2520Tien%2520Tsin%2520Wong%2520and%2520Hamid%2520Rezatofighi%2520and%2520Gholamreza%2520Haffari%2520and%2520Lizhen%2520Qu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion-based%2520and%2520autoregressive%2520video%2520generation%2520models%250Ahave%2520achieved%2520remarkable%2520visual%2520realism.%2520However%252C%2520these%2520models%2520typically%2520lack%250Aaccurate%2520physical%2520alignment%252C%2520failing%2520to%2520replicate%2520real-world%2520dynamics%2520in%2520object%250Amotion.%2520This%2520limitation%2520arises%2520primarily%2520from%2520their%2520reliance%2520on%2520learned%250Astatistical%2520correlations%2520rather%2520than%2520capturing%2520mechanisms%2520adhering%2520to%2520physical%250Alaws.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%2520integrates%250Asymbolic%2520regression%2520%2528SR%2529%2520and%2520trajectory-guided%2520image-to-video%2520%2528I2V%2529%2520models%2520for%250Aphysics-grounded%2520video%2520forecasting.%2520Our%2520approach%2520extracts%2520motion%2520trajectories%250Afrom%2520input%2520videos%252C%2520uses%2520a%2520retrieval-based%2520pre-training%2520mechanism%2520to%2520enhance%250Asymbolic%2520regression%252C%2520and%2520discovers%2520equations%2520of%2520motion%2520to%2520forecast%2520physically%250Aaccurate%2520future%2520trajectories.%2520These%2520trajectories%2520then%2520guide%2520video%2520generation%250Awithout%2520requiring%2520fine-tuning%2520of%2520existing%2520models.%2520Evaluated%2520on%2520scenarios%2520in%250AClassical%2520Mechanics%252C%2520including%2520spring-mass%252C%2520pendulums%252C%2520and%2520projectile%2520motions%252C%250Aour%2520method%2520successfully%2520recovers%2520ground-truth%2520analytical%2520equations%2520and%2520improves%250Athe%2520physical%2520alignment%2520of%2520generated%2520videos%2520over%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Grounded%20Motion%20Forecasting%20via%20Equation%20Discovery%20for%0A%20%20Trajectory-Guided%20Image-to-Video%20Generation&entry.906535625=Tao%20Feng%20and%20Xianbing%20Zhao%20and%20Zhenhua%20Chen%20and%20Tien%20Tsin%20Wong%20and%20Hamid%20Rezatofighi%20and%20Gholamreza%20Haffari%20and%20Lizhen%20Qu&entry.1292438233=%20%20Recent%20advances%20in%20diffusion-based%20and%20autoregressive%20video%20generation%20models%0Ahave%20achieved%20remarkable%20visual%20realism.%20However%2C%20these%20models%20typically%20lack%0Aaccurate%20physical%20alignment%2C%20failing%20to%20replicate%20real-world%20dynamics%20in%20object%0Amotion.%20This%20limitation%20arises%20primarily%20from%20their%20reliance%20on%20learned%0Astatistical%20correlations%20rather%20than%20capturing%20mechanisms%20adhering%20to%20physical%0Alaws.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20framework%20that%20integrates%0Asymbolic%20regression%20%28SR%29%20and%20trajectory-guided%20image-to-video%20%28I2V%29%20models%20for%0Aphysics-grounded%20video%20forecasting.%20Our%20approach%20extracts%20motion%20trajectories%0Afrom%20input%20videos%2C%20uses%20a%20retrieval-based%20pre-training%20mechanism%20to%20enhance%0Asymbolic%20regression%2C%20and%20discovers%20equations%20of%20motion%20to%20forecast%20physically%0Aaccurate%20future%20trajectories.%20These%20trajectories%20then%20guide%20video%20generation%0Awithout%20requiring%20fine-tuning%20of%20existing%20models.%20Evaluated%20on%20scenarios%20in%0AClassical%20Mechanics%2C%20including%20spring-mass%2C%20pendulums%2C%20and%20projectile%20motions%2C%0Aour%20method%20successfully%20recovers%20ground-truth%20analytical%20equations%20and%20improves%0Athe%20physical%20alignment%20of%20generated%20videos%20over%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06830v1&entry.124074799=Read"},
{"title": "Addressing Imbalanced Domain-Incremental Learning through Dual-Balance\n  Collaborative Experts", "author": "Lan Li and Da-Wei Zhou and Han-Jia Ye and De-Chuan Zhan", "abstract": "  Domain-Incremental Learning (DIL) focuses on continual learning in\nnon-stationary environments, requiring models to adjust to evolving domains\nwhile preserving historical knowledge. DIL faces two critical challenges in the\ncontext of imbalanced data: intra-domain class imbalance and cross-domain class\ndistribution shifts. These challenges significantly hinder model performance,\nas intra-domain imbalance leads to underfitting of few-shot classes, while\ncross-domain shifts require maintaining well-learned many-shot classes and\ntransferring knowledge to improve few-shot class performance in old domains. To\novercome these challenges, we introduce the Dual-Balance Collaborative Experts\n(DCE) framework. DCE employs a frequency-aware expert group, where each expert\nis guided by specialized loss functions to learn features for specific\nfrequency groups, effectively addressing intra-domain class imbalance.\nSubsequently, a dynamic expert selector is learned by synthesizing\npseudo-features through balanced Gaussian sampling from historical class\nstatistics. This mechanism navigates the trade-off between preserving many-shot\nknowledge of previous domains and leveraging new data to improve few-shot class\nperformance in earlier tasks. Extensive experimental results on four benchmark\ndatasets demonstrate DCE's state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2507.07100v1", "date": "2025-07-09", "relevancy": 2.5743, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5385}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5063}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Imbalanced%20Domain-Incremental%20Learning%20through%20Dual-Balance%0A%20%20Collaborative%20Experts&body=Title%3A%20Addressing%20Imbalanced%20Domain-Incremental%20Learning%20through%20Dual-Balance%0A%20%20Collaborative%20Experts%0AAuthor%3A%20Lan%20Li%20and%20Da-Wei%20Zhou%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20Domain-Incremental%20Learning%20%28DIL%29%20focuses%20on%20continual%20learning%20in%0Anon-stationary%20environments%2C%20requiring%20models%20to%20adjust%20to%20evolving%20domains%0Awhile%20preserving%20historical%20knowledge.%20DIL%20faces%20two%20critical%20challenges%20in%20the%0Acontext%20of%20imbalanced%20data%3A%20intra-domain%20class%20imbalance%20and%20cross-domain%20class%0Adistribution%20shifts.%20These%20challenges%20significantly%20hinder%20model%20performance%2C%0Aas%20intra-domain%20imbalance%20leads%20to%20underfitting%20of%20few-shot%20classes%2C%20while%0Across-domain%20shifts%20require%20maintaining%20well-learned%20many-shot%20classes%20and%0Atransferring%20knowledge%20to%20improve%20few-shot%20class%20performance%20in%20old%20domains.%20To%0Aovercome%20these%20challenges%2C%20we%20introduce%20the%20Dual-Balance%20Collaborative%20Experts%0A%28DCE%29%20framework.%20DCE%20employs%20a%20frequency-aware%20expert%20group%2C%20where%20each%20expert%0Ais%20guided%20by%20specialized%20loss%20functions%20to%20learn%20features%20for%20specific%0Afrequency%20groups%2C%20effectively%20addressing%20intra-domain%20class%20imbalance.%0ASubsequently%2C%20a%20dynamic%20expert%20selector%20is%20learned%20by%20synthesizing%0Apseudo-features%20through%20balanced%20Gaussian%20sampling%20from%20historical%20class%0Astatistics.%20This%20mechanism%20navigates%20the%20trade-off%20between%20preserving%20many-shot%0Aknowledge%20of%20previous%20domains%20and%20leveraging%20new%20data%20to%20improve%20few-shot%20class%0Aperformance%20in%20earlier%20tasks.%20Extensive%20experimental%20results%20on%20four%20benchmark%0Adatasets%20demonstrate%20DCE%27s%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Imbalanced%2520Domain-Incremental%2520Learning%2520through%2520Dual-Balance%250A%2520%2520Collaborative%2520Experts%26entry.906535625%3DLan%2520Li%2520and%2520Da-Wei%2520Zhou%2520and%2520Han-Jia%2520Ye%2520and%2520De-Chuan%2520Zhan%26entry.1292438233%3D%2520%2520Domain-Incremental%2520Learning%2520%2528DIL%2529%2520focuses%2520on%2520continual%2520learning%2520in%250Anon-stationary%2520environments%252C%2520requiring%2520models%2520to%2520adjust%2520to%2520evolving%2520domains%250Awhile%2520preserving%2520historical%2520knowledge.%2520DIL%2520faces%2520two%2520critical%2520challenges%2520in%2520the%250Acontext%2520of%2520imbalanced%2520data%253A%2520intra-domain%2520class%2520imbalance%2520and%2520cross-domain%2520class%250Adistribution%2520shifts.%2520These%2520challenges%2520significantly%2520hinder%2520model%2520performance%252C%250Aas%2520intra-domain%2520imbalance%2520leads%2520to%2520underfitting%2520of%2520few-shot%2520classes%252C%2520while%250Across-domain%2520shifts%2520require%2520maintaining%2520well-learned%2520many-shot%2520classes%2520and%250Atransferring%2520knowledge%2520to%2520improve%2520few-shot%2520class%2520performance%2520in%2520old%2520domains.%2520To%250Aovercome%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520Dual-Balance%2520Collaborative%2520Experts%250A%2528DCE%2529%2520framework.%2520DCE%2520employs%2520a%2520frequency-aware%2520expert%2520group%252C%2520where%2520each%2520expert%250Ais%2520guided%2520by%2520specialized%2520loss%2520functions%2520to%2520learn%2520features%2520for%2520specific%250Afrequency%2520groups%252C%2520effectively%2520addressing%2520intra-domain%2520class%2520imbalance.%250ASubsequently%252C%2520a%2520dynamic%2520expert%2520selector%2520is%2520learned%2520by%2520synthesizing%250Apseudo-features%2520through%2520balanced%2520Gaussian%2520sampling%2520from%2520historical%2520class%250Astatistics.%2520This%2520mechanism%2520navigates%2520the%2520trade-off%2520between%2520preserving%2520many-shot%250Aknowledge%2520of%2520previous%2520domains%2520and%2520leveraging%2520new%2520data%2520to%2520improve%2520few-shot%2520class%250Aperformance%2520in%2520earlier%2520tasks.%2520Extensive%2520experimental%2520results%2520on%2520four%2520benchmark%250Adatasets%2520demonstrate%2520DCE%2527s%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Imbalanced%20Domain-Incremental%20Learning%20through%20Dual-Balance%0A%20%20Collaborative%20Experts&entry.906535625=Lan%20Li%20and%20Da-Wei%20Zhou%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20Domain-Incremental%20Learning%20%28DIL%29%20focuses%20on%20continual%20learning%20in%0Anon-stationary%20environments%2C%20requiring%20models%20to%20adjust%20to%20evolving%20domains%0Awhile%20preserving%20historical%20knowledge.%20DIL%20faces%20two%20critical%20challenges%20in%20the%0Acontext%20of%20imbalanced%20data%3A%20intra-domain%20class%20imbalance%20and%20cross-domain%20class%0Adistribution%20shifts.%20These%20challenges%20significantly%20hinder%20model%20performance%2C%0Aas%20intra-domain%20imbalance%20leads%20to%20underfitting%20of%20few-shot%20classes%2C%20while%0Across-domain%20shifts%20require%20maintaining%20well-learned%20many-shot%20classes%20and%0Atransferring%20knowledge%20to%20improve%20few-shot%20class%20performance%20in%20old%20domains.%20To%0Aovercome%20these%20challenges%2C%20we%20introduce%20the%20Dual-Balance%20Collaborative%20Experts%0A%28DCE%29%20framework.%20DCE%20employs%20a%20frequency-aware%20expert%20group%2C%20where%20each%20expert%0Ais%20guided%20by%20specialized%20loss%20functions%20to%20learn%20features%20for%20specific%0Afrequency%20groups%2C%20effectively%20addressing%20intra-domain%20class%20imbalance.%0ASubsequently%2C%20a%20dynamic%20expert%20selector%20is%20learned%20by%20synthesizing%0Apseudo-features%20through%20balanced%20Gaussian%20sampling%20from%20historical%20class%0Astatistics.%20This%20mechanism%20navigates%20the%20trade-off%20between%20preserving%20many-shot%0Aknowledge%20of%20previous%20domains%20and%20leveraging%20new%20data%20to%20improve%20few-shot%20class%0Aperformance%20in%20earlier%20tasks.%20Extensive%20experimental%20results%20on%20four%20benchmark%0Adatasets%20demonstrate%20DCE%27s%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07100v1&entry.124074799=Read"},
{"title": "GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision\n  Transformers for Whole Slide Image Classification and Captioning", "author": "S M Taslim Uddin Raju and Md. Milon Islam and Md Rezwanul Haque and Hamdi Altaheri and Fakhri Karray", "abstract": "  Microscopic assessment of histopathology images is vital for accurate cancer\ndiagnosis and treatment. Whole Slide Image (WSI) classification and captioning\nhave become crucial tasks in computer-aided pathology. However, microscopic WSI\nface challenges such as redundant patches and unknown patch positions due to\nsubjective pathologist captures. Moreover, generating automatic pathology\ncaptions remains a significant challenge. To address these issues, we introduce\na novel GNN-ViTCap framework for classification and caption generation from\nhistopathological microscopic images. First, a visual feature extractor\ngenerates patch embeddings. Redundant patches are then removed by dynamically\nclustering these embeddings using deep embedded clustering and selecting\nrepresentative patches via a scalar dot attention mechanism. We build a graph\nby connecting each node to its nearest neighbors in the similarity matrix and\napply a graph neural network to capture both local and global context. The\naggregated image embeddings are projected into the language model's input space\nthrough a linear layer and combined with caption tokens to fine-tune a large\nlanguage model. We validate our method on the BreakHis and PatchGastric\ndatasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for\nclassification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569\nfor captioning. Experimental results demonstrate that GNN-ViTCap outperforms\nstate of the art approaches, offering a reliable and efficient solution for\nmicroscopy based patient diagnosis.\n", "link": "http://arxiv.org/abs/2507.07006v1", "date": "2025-07-09", "relevancy": 2.5452, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5085}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNN-ViTCap%3A%20GNN-Enhanced%20Multiple%20Instance%20Learning%20with%20Vision%0A%20%20Transformers%20for%20Whole%20Slide%20Image%20Classification%20and%20Captioning&body=Title%3A%20GNN-ViTCap%3A%20GNN-Enhanced%20Multiple%20Instance%20Learning%20with%20Vision%0A%20%20Transformers%20for%20Whole%20Slide%20Image%20Classification%20and%20Captioning%0AAuthor%3A%20S%20M%20Taslim%20Uddin%20Raju%20and%20Md.%20Milon%20Islam%20and%20Md%20Rezwanul%20Haque%20and%20Hamdi%20Altaheri%20and%20Fakhri%20Karray%0AAbstract%3A%20%20%20Microscopic%20assessment%20of%20histopathology%20images%20is%20vital%20for%20accurate%20cancer%0Adiagnosis%20and%20treatment.%20Whole%20Slide%20Image%20%28WSI%29%20classification%20and%20captioning%0Ahave%20become%20crucial%20tasks%20in%20computer-aided%20pathology.%20However%2C%20microscopic%20WSI%0Aface%20challenges%20such%20as%20redundant%20patches%20and%20unknown%20patch%20positions%20due%20to%0Asubjective%20pathologist%20captures.%20Moreover%2C%20generating%20automatic%20pathology%0Acaptions%20remains%20a%20significant%20challenge.%20To%20address%20these%20issues%2C%20we%20introduce%0Aa%20novel%20GNN-ViTCap%20framework%20for%20classification%20and%20caption%20generation%20from%0Ahistopathological%20microscopic%20images.%20First%2C%20a%20visual%20feature%20extractor%0Agenerates%20patch%20embeddings.%20Redundant%20patches%20are%20then%20removed%20by%20dynamically%0Aclustering%20these%20embeddings%20using%20deep%20embedded%20clustering%20and%20selecting%0Arepresentative%20patches%20via%20a%20scalar%20dot%20attention%20mechanism.%20We%20build%20a%20graph%0Aby%20connecting%20each%20node%20to%20its%20nearest%20neighbors%20in%20the%20similarity%20matrix%20and%0Aapply%20a%20graph%20neural%20network%20to%20capture%20both%20local%20and%20global%20context.%20The%0Aaggregated%20image%20embeddings%20are%20projected%20into%20the%20language%20model%27s%20input%20space%0Athrough%20a%20linear%20layer%20and%20combined%20with%20caption%20tokens%20to%20fine-tune%20a%20large%0Alanguage%20model.%20We%20validate%20our%20method%20on%20the%20BreakHis%20and%20PatchGastric%0Adatasets.%20GNN-ViTCap%20achieves%20an%20F1%20score%20of%200.934%20and%20an%20AUC%20of%200.963%20for%0Aclassification%2C%20along%20with%20a%20BLEU-4%20score%20of%200.811%20and%20a%20METEOR%20score%20of%200.569%0Afor%20captioning.%20Experimental%20results%20demonstrate%20that%20GNN-ViTCap%20outperforms%0Astate%20of%20the%20art%20approaches%2C%20offering%20a%20reliable%20and%20efficient%20solution%20for%0Amicroscopy%20based%20patient%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNN-ViTCap%253A%2520GNN-Enhanced%2520Multiple%2520Instance%2520Learning%2520with%2520Vision%250A%2520%2520Transformers%2520for%2520Whole%2520Slide%2520Image%2520Classification%2520and%2520Captioning%26entry.906535625%3DS%2520M%2520Taslim%2520Uddin%2520Raju%2520and%2520Md.%2520Milon%2520Islam%2520and%2520Md%2520Rezwanul%2520Haque%2520and%2520Hamdi%2520Altaheri%2520and%2520Fakhri%2520Karray%26entry.1292438233%3D%2520%2520Microscopic%2520assessment%2520of%2520histopathology%2520images%2520is%2520vital%2520for%2520accurate%2520cancer%250Adiagnosis%2520and%2520treatment.%2520Whole%2520Slide%2520Image%2520%2528WSI%2529%2520classification%2520and%2520captioning%250Ahave%2520become%2520crucial%2520tasks%2520in%2520computer-aided%2520pathology.%2520However%252C%2520microscopic%2520WSI%250Aface%2520challenges%2520such%2520as%2520redundant%2520patches%2520and%2520unknown%2520patch%2520positions%2520due%2520to%250Asubjective%2520pathologist%2520captures.%2520Moreover%252C%2520generating%2520automatic%2520pathology%250Acaptions%2520remains%2520a%2520significant%2520challenge.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250Aa%2520novel%2520GNN-ViTCap%2520framework%2520for%2520classification%2520and%2520caption%2520generation%2520from%250Ahistopathological%2520microscopic%2520images.%2520First%252C%2520a%2520visual%2520feature%2520extractor%250Agenerates%2520patch%2520embeddings.%2520Redundant%2520patches%2520are%2520then%2520removed%2520by%2520dynamically%250Aclustering%2520these%2520embeddings%2520using%2520deep%2520embedded%2520clustering%2520and%2520selecting%250Arepresentative%2520patches%2520via%2520a%2520scalar%2520dot%2520attention%2520mechanism.%2520We%2520build%2520a%2520graph%250Aby%2520connecting%2520each%2520node%2520to%2520its%2520nearest%2520neighbors%2520in%2520the%2520similarity%2520matrix%2520and%250Aapply%2520a%2520graph%2520neural%2520network%2520to%2520capture%2520both%2520local%2520and%2520global%2520context.%2520The%250Aaggregated%2520image%2520embeddings%2520are%2520projected%2520into%2520the%2520language%2520model%2527s%2520input%2520space%250Athrough%2520a%2520linear%2520layer%2520and%2520combined%2520with%2520caption%2520tokens%2520to%2520fine-tune%2520a%2520large%250Alanguage%2520model.%2520We%2520validate%2520our%2520method%2520on%2520the%2520BreakHis%2520and%2520PatchGastric%250Adatasets.%2520GNN-ViTCap%2520achieves%2520an%2520F1%2520score%2520of%25200.934%2520and%2520an%2520AUC%2520of%25200.963%2520for%250Aclassification%252C%2520along%2520with%2520a%2520BLEU-4%2520score%2520of%25200.811%2520and%2520a%2520METEOR%2520score%2520of%25200.569%250Afor%2520captioning.%2520Experimental%2520results%2520demonstrate%2520that%2520GNN-ViTCap%2520outperforms%250Astate%2520of%2520the%2520art%2520approaches%252C%2520offering%2520a%2520reliable%2520and%2520efficient%2520solution%2520for%250Amicroscopy%2520based%2520patient%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNN-ViTCap%3A%20GNN-Enhanced%20Multiple%20Instance%20Learning%20with%20Vision%0A%20%20Transformers%20for%20Whole%20Slide%20Image%20Classification%20and%20Captioning&entry.906535625=S%20M%20Taslim%20Uddin%20Raju%20and%20Md.%20Milon%20Islam%20and%20Md%20Rezwanul%20Haque%20and%20Hamdi%20Altaheri%20and%20Fakhri%20Karray&entry.1292438233=%20%20Microscopic%20assessment%20of%20histopathology%20images%20is%20vital%20for%20accurate%20cancer%0Adiagnosis%20and%20treatment.%20Whole%20Slide%20Image%20%28WSI%29%20classification%20and%20captioning%0Ahave%20become%20crucial%20tasks%20in%20computer-aided%20pathology.%20However%2C%20microscopic%20WSI%0Aface%20challenges%20such%20as%20redundant%20patches%20and%20unknown%20patch%20positions%20due%20to%0Asubjective%20pathologist%20captures.%20Moreover%2C%20generating%20automatic%20pathology%0Acaptions%20remains%20a%20significant%20challenge.%20To%20address%20these%20issues%2C%20we%20introduce%0Aa%20novel%20GNN-ViTCap%20framework%20for%20classification%20and%20caption%20generation%20from%0Ahistopathological%20microscopic%20images.%20First%2C%20a%20visual%20feature%20extractor%0Agenerates%20patch%20embeddings.%20Redundant%20patches%20are%20then%20removed%20by%20dynamically%0Aclustering%20these%20embeddings%20using%20deep%20embedded%20clustering%20and%20selecting%0Arepresentative%20patches%20via%20a%20scalar%20dot%20attention%20mechanism.%20We%20build%20a%20graph%0Aby%20connecting%20each%20node%20to%20its%20nearest%20neighbors%20in%20the%20similarity%20matrix%20and%0Aapply%20a%20graph%20neural%20network%20to%20capture%20both%20local%20and%20global%20context.%20The%0Aaggregated%20image%20embeddings%20are%20projected%20into%20the%20language%20model%27s%20input%20space%0Athrough%20a%20linear%20layer%20and%20combined%20with%20caption%20tokens%20to%20fine-tune%20a%20large%0Alanguage%20model.%20We%20validate%20our%20method%20on%20the%20BreakHis%20and%20PatchGastric%0Adatasets.%20GNN-ViTCap%20achieves%20an%20F1%20score%20of%200.934%20and%20an%20AUC%20of%200.963%20for%0Aclassification%2C%20along%20with%20a%20BLEU-4%20score%20of%200.811%20and%20a%20METEOR%20score%20of%200.569%0Afor%20captioning.%20Experimental%20results%20demonstrate%20that%20GNN-ViTCap%20outperforms%0Astate%20of%20the%20art%20approaches%2C%20offering%20a%20reliable%20and%20efficient%20solution%20for%0Amicroscopy%20based%20patient%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07006v1&entry.124074799=Read"},
{"title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate\n  Speech Codecs", "author": "Yitian Gong and Luozhijie Jin and Ruifan Deng and Dong Zhang and Xin Zhang and Qinyuan Cheng and Zhaoye Fei and Shimin Li and Xipeng Qiu", "abstract": "  Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer.\n", "link": "http://arxiv.org/abs/2506.23325v2", "date": "2025-07-09", "relevancy": 2.5413, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XY-Tokenizer%3A%20Mitigating%20the%20Semantic-Acoustic%20Conflict%20in%20Low-Bitrate%0A%20%20Speech%20Codecs&body=Title%3A%20XY-Tokenizer%3A%20Mitigating%20the%20Semantic-Acoustic%20Conflict%20in%20Low-Bitrate%0A%20%20Speech%20Codecs%0AAuthor%3A%20Yitian%20Gong%20and%20Luozhijie%20Jin%20and%20Ruifan%20Deng%20and%20Dong%20Zhang%20and%20Xin%20Zhang%20and%20Qinyuan%20Cheng%20and%20Zhaoye%20Fei%20and%20Shimin%20Li%20and%20Xipeng%20Qiu%0AAbstract%3A%20%20%20Speech%20codecs%20serve%20as%20bridges%20between%20speech%20signals%20and%20large%20language%0Amodels.%20An%20ideal%20codec%20for%20speech%20language%20models%20should%20not%20only%20preserve%0Aacoustic%20information%20but%20also%20capture%20rich%20semantic%20information.%20However%2C%0Aexisting%20speech%20codecs%20struggle%20to%20balance%20high-quality%20audio%20reconstruction%0Awith%20ease%20of%20modeling%20by%20language%20models.%20In%20this%20study%2C%20we%20analyze%20the%0Alimitations%20of%20previous%20codecs%20in%20balancing%20semantic%20richness%20and%20acoustic%0Afidelity.%20We%20propose%20XY-Tokenizer%2C%20a%20novel%20codec%20that%20mitigates%20the%20conflict%0Abetween%20semantic%20and%20acoustic%20capabilities%20through%20multi-stage%2C%20multi-task%0Alearning.%20Experimental%20results%20demonstrate%20that%20XY-Tokenizer%20achieves%0Aperformance%20in%20both%20semantic%20and%20acoustic%20tasks%20comparable%20to%20that%20of%0Astate-of-the-art%20codecs%20operating%20at%20similar%20bitrates%2C%20even%20though%20those%0Aexisting%20codecs%20typically%20excel%20in%20only%20one%20aspect.%20Specifically%2C%20XY-Tokenizer%0Aachieves%20strong%20text%20alignment%2C%20surpassing%20distillation-based%20semantic%20modeling%0Amethods%20such%20as%20SpeechTokenizer%20and%20Mimi%2C%20while%20maintaining%20a%20speaker%0Asimilarity%20score%20of%200.83%20between%20reconstructed%20and%20original%20audio.%20The%0Areconstruction%20performance%20of%20XY-Tokenizer%20is%20comparable%20to%20that%20of%20BigCodec%2C%0Athe%20current%20state-of-the-art%20among%20acoustic-only%20codecs%2C%20which%20achieves%20a%0Aspeaker%20similarity%20score%20of%200.84%20at%20a%20similar%20bitrate.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/gyt1145028706/XY-Tokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXY-Tokenizer%253A%2520Mitigating%2520the%2520Semantic-Acoustic%2520Conflict%2520in%2520Low-Bitrate%250A%2520%2520Speech%2520Codecs%26entry.906535625%3DYitian%2520Gong%2520and%2520Luozhijie%2520Jin%2520and%2520Ruifan%2520Deng%2520and%2520Dong%2520Zhang%2520and%2520Xin%2520Zhang%2520and%2520Qinyuan%2520Cheng%2520and%2520Zhaoye%2520Fei%2520and%2520Shimin%2520Li%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3D%2520%2520Speech%2520codecs%2520serve%2520as%2520bridges%2520between%2520speech%2520signals%2520and%2520large%2520language%250Amodels.%2520An%2520ideal%2520codec%2520for%2520speech%2520language%2520models%2520should%2520not%2520only%2520preserve%250Aacoustic%2520information%2520but%2520also%2520capture%2520rich%2520semantic%2520information.%2520However%252C%250Aexisting%2520speech%2520codecs%2520struggle%2520to%2520balance%2520high-quality%2520audio%2520reconstruction%250Awith%2520ease%2520of%2520modeling%2520by%2520language%2520models.%2520In%2520this%2520study%252C%2520we%2520analyze%2520the%250Alimitations%2520of%2520previous%2520codecs%2520in%2520balancing%2520semantic%2520richness%2520and%2520acoustic%250Afidelity.%2520We%2520propose%2520XY-Tokenizer%252C%2520a%2520novel%2520codec%2520that%2520mitigates%2520the%2520conflict%250Abetween%2520semantic%2520and%2520acoustic%2520capabilities%2520through%2520multi-stage%252C%2520multi-task%250Alearning.%2520Experimental%2520results%2520demonstrate%2520that%2520XY-Tokenizer%2520achieves%250Aperformance%2520in%2520both%2520semantic%2520and%2520acoustic%2520tasks%2520comparable%2520to%2520that%2520of%250Astate-of-the-art%2520codecs%2520operating%2520at%2520similar%2520bitrates%252C%2520even%2520though%2520those%250Aexisting%2520codecs%2520typically%2520excel%2520in%2520only%2520one%2520aspect.%2520Specifically%252C%2520XY-Tokenizer%250Aachieves%2520strong%2520text%2520alignment%252C%2520surpassing%2520distillation-based%2520semantic%2520modeling%250Amethods%2520such%2520as%2520SpeechTokenizer%2520and%2520Mimi%252C%2520while%2520maintaining%2520a%2520speaker%250Asimilarity%2520score%2520of%25200.83%2520between%2520reconstructed%2520and%2520original%2520audio.%2520The%250Areconstruction%2520performance%2520of%2520XY-Tokenizer%2520is%2520comparable%2520to%2520that%2520of%2520BigCodec%252C%250Athe%2520current%2520state-of-the-art%2520among%2520acoustic-only%2520codecs%252C%2520which%2520achieves%2520a%250Aspeaker%2520similarity%2520score%2520of%25200.84%2520at%2520a%2520similar%2520bitrate.%2520Code%2520and%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/gyt1145028706/XY-Tokenizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XY-Tokenizer%3A%20Mitigating%20the%20Semantic-Acoustic%20Conflict%20in%20Low-Bitrate%0A%20%20Speech%20Codecs&entry.906535625=Yitian%20Gong%20and%20Luozhijie%20Jin%20and%20Ruifan%20Deng%20and%20Dong%20Zhang%20and%20Xin%20Zhang%20and%20Qinyuan%20Cheng%20and%20Zhaoye%20Fei%20and%20Shimin%20Li%20and%20Xipeng%20Qiu&entry.1292438233=%20%20Speech%20codecs%20serve%20as%20bridges%20between%20speech%20signals%20and%20large%20language%0Amodels.%20An%20ideal%20codec%20for%20speech%20language%20models%20should%20not%20only%20preserve%0Aacoustic%20information%20but%20also%20capture%20rich%20semantic%20information.%20However%2C%0Aexisting%20speech%20codecs%20struggle%20to%20balance%20high-quality%20audio%20reconstruction%0Awith%20ease%20of%20modeling%20by%20language%20models.%20In%20this%20study%2C%20we%20analyze%20the%0Alimitations%20of%20previous%20codecs%20in%20balancing%20semantic%20richness%20and%20acoustic%0Afidelity.%20We%20propose%20XY-Tokenizer%2C%20a%20novel%20codec%20that%20mitigates%20the%20conflict%0Abetween%20semantic%20and%20acoustic%20capabilities%20through%20multi-stage%2C%20multi-task%0Alearning.%20Experimental%20results%20demonstrate%20that%20XY-Tokenizer%20achieves%0Aperformance%20in%20both%20semantic%20and%20acoustic%20tasks%20comparable%20to%20that%20of%0Astate-of-the-art%20codecs%20operating%20at%20similar%20bitrates%2C%20even%20though%20those%0Aexisting%20codecs%20typically%20excel%20in%20only%20one%20aspect.%20Specifically%2C%20XY-Tokenizer%0Aachieves%20strong%20text%20alignment%2C%20surpassing%20distillation-based%20semantic%20modeling%0Amethods%20such%20as%20SpeechTokenizer%20and%20Mimi%2C%20while%20maintaining%20a%20speaker%0Asimilarity%20score%20of%200.83%20between%20reconstructed%20and%20original%20audio.%20The%0Areconstruction%20performance%20of%20XY-Tokenizer%20is%20comparable%20to%20that%20of%20BigCodec%2C%0Athe%20current%20state-of-the-art%20among%20acoustic-only%20codecs%2C%20which%20achieves%20a%0Aspeaker%20similarity%20score%20of%200.84%20at%20a%20similar%20bitrate.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/gyt1145028706/XY-Tokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23325v2&entry.124074799=Read"},
{"title": "Hallucinating 360\u00b0: Panoramic Street-View Generation via Local\n  Scenes Diffusion and Probabilistic Prompting", "author": "Fei Teng and Kai Luo and Sheng Wu and Siyu Li and Pujun Guo and Jiale Wei and Kunyu Peng and Jiaming Zhang and Kailun Yang", "abstract": "  Panoramic perception holds significant potential for autonomous driving,\nenabling vehicles to acquire a comprehensive 360{\\deg} surround view in a\nsingle shot. However, autonomous driving is a data-driven task. Complete\npanoramic data acquisition requires complex sampling systems and annotation\npipelines, which are time-consuming and labor-intensive. Although existing\nstreet view generation models have demonstrated strong data regeneration\ncapabilities, they can only learn from the fixed data distribution of existing\ndatasets and cannot achieve high-quality, controllable panoramic generation. In\nthis paper, we propose the first panoramic generation method Percep360 for\nautonomous driving. Percep360 enables coherent generation of panoramic data\nwith control signals based on the stitched panoramic data. Percep360 focuses on\ntwo key aspects: coherence and controllability. Specifically, to overcome the\ninherent information loss caused by the pinhole sampling process, we propose\nthe Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama\ngeneration as a spatially continuous diffusion process, bridging the gaps\nbetween different data distributions. Additionally, to achieve the controllable\ngeneration of panoramic images, we propose a Probabilistic Prompting Method\n(PPM). PPM dynamically selects the most relevant control cues, enabling\ncontrollable panoramic image generation. We evaluate the effectiveness of the\ngenerated images from three perspectives: image quality assessment (i.e.,\nno-reference and with reference), controllability, and their utility in\nreal-world Bird's Eye View (BEV) segmentation. Notably, the generated data\nconsistently outperforms the original stitched images in no-reference quality\nmetrics and enhances downstream perception models. The source code will be\npublicly available at https://github.com/Bryant-Teng/Percep360.\n", "link": "http://arxiv.org/abs/2507.06971v1", "date": "2025-07-09", "relevancy": 2.5331, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6576}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6283}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallucinating%20360%C2%B0%3A%20Panoramic%20Street-View%20Generation%20via%20Local%0A%20%20Scenes%20Diffusion%20and%20Probabilistic%20Prompting&body=Title%3A%20Hallucinating%20360%C2%B0%3A%20Panoramic%20Street-View%20Generation%20via%20Local%0A%20%20Scenes%20Diffusion%20and%20Probabilistic%20Prompting%0AAuthor%3A%20Fei%20Teng%20and%20Kai%20Luo%20and%20Sheng%20Wu%20and%20Siyu%20Li%20and%20Pujun%20Guo%20and%20Jiale%20Wei%20and%20Kunyu%20Peng%20and%20Jiaming%20Zhang%20and%20Kailun%20Yang%0AAbstract%3A%20%20%20Panoramic%20perception%20holds%20significant%20potential%20for%20autonomous%20driving%2C%0Aenabling%20vehicles%20to%20acquire%20a%20comprehensive%20360%7B%5Cdeg%7D%20surround%20view%20in%20a%0Asingle%20shot.%20However%2C%20autonomous%20driving%20is%20a%20data-driven%20task.%20Complete%0Apanoramic%20data%20acquisition%20requires%20complex%20sampling%20systems%20and%20annotation%0Apipelines%2C%20which%20are%20time-consuming%20and%20labor-intensive.%20Although%20existing%0Astreet%20view%20generation%20models%20have%20demonstrated%20strong%20data%20regeneration%0Acapabilities%2C%20they%20can%20only%20learn%20from%20the%20fixed%20data%20distribution%20of%20existing%0Adatasets%20and%20cannot%20achieve%20high-quality%2C%20controllable%20panoramic%20generation.%20In%0Athis%20paper%2C%20we%20propose%20the%20first%20panoramic%20generation%20method%20Percep360%20for%0Aautonomous%20driving.%20Percep360%20enables%20coherent%20generation%20of%20panoramic%20data%0Awith%20control%20signals%20based%20on%20the%20stitched%20panoramic%20data.%20Percep360%20focuses%20on%0Atwo%20key%20aspects%3A%20coherence%20and%20controllability.%20Specifically%2C%20to%20overcome%20the%0Ainherent%20information%20loss%20caused%20by%20the%20pinhole%20sampling%20process%2C%20we%20propose%0Athe%20Local%20Scenes%20Diffusion%20Method%20%28LSDM%29.%20LSDM%20reformulates%20the%20panorama%0Ageneration%20as%20a%20spatially%20continuous%20diffusion%20process%2C%20bridging%20the%20gaps%0Abetween%20different%20data%20distributions.%20Additionally%2C%20to%20achieve%20the%20controllable%0Ageneration%20of%20panoramic%20images%2C%20we%20propose%20a%20Probabilistic%20Prompting%20Method%0A%28PPM%29.%20PPM%20dynamically%20selects%20the%20most%20relevant%20control%20cues%2C%20enabling%0Acontrollable%20panoramic%20image%20generation.%20We%20evaluate%20the%20effectiveness%20of%20the%0Agenerated%20images%20from%20three%20perspectives%3A%20image%20quality%20assessment%20%28i.e.%2C%0Ano-reference%20and%20with%20reference%29%2C%20controllability%2C%20and%20their%20utility%20in%0Areal-world%20Bird%27s%20Eye%20View%20%28BEV%29%20segmentation.%20Notably%2C%20the%20generated%20data%0Aconsistently%20outperforms%20the%20original%20stitched%20images%20in%20no-reference%20quality%0Ametrics%20and%20enhances%20downstream%20perception%20models.%20The%20source%20code%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/Bryant-Teng/Percep360.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallucinating%2520360%25C2%25B0%253A%2520Panoramic%2520Street-View%2520Generation%2520via%2520Local%250A%2520%2520Scenes%2520Diffusion%2520and%2520Probabilistic%2520Prompting%26entry.906535625%3DFei%2520Teng%2520and%2520Kai%2520Luo%2520and%2520Sheng%2520Wu%2520and%2520Siyu%2520Li%2520and%2520Pujun%2520Guo%2520and%2520Jiale%2520Wei%2520and%2520Kunyu%2520Peng%2520and%2520Jiaming%2520Zhang%2520and%2520Kailun%2520Yang%26entry.1292438233%3D%2520%2520Panoramic%2520perception%2520holds%2520significant%2520potential%2520for%2520autonomous%2520driving%252C%250Aenabling%2520vehicles%2520to%2520acquire%2520a%2520comprehensive%2520360%257B%255Cdeg%257D%2520surround%2520view%2520in%2520a%250Asingle%2520shot.%2520However%252C%2520autonomous%2520driving%2520is%2520a%2520data-driven%2520task.%2520Complete%250Apanoramic%2520data%2520acquisition%2520requires%2520complex%2520sampling%2520systems%2520and%2520annotation%250Apipelines%252C%2520which%2520are%2520time-consuming%2520and%2520labor-intensive.%2520Although%2520existing%250Astreet%2520view%2520generation%2520models%2520have%2520demonstrated%2520strong%2520data%2520regeneration%250Acapabilities%252C%2520they%2520can%2520only%2520learn%2520from%2520the%2520fixed%2520data%2520distribution%2520of%2520existing%250Adatasets%2520and%2520cannot%2520achieve%2520high-quality%252C%2520controllable%2520panoramic%2520generation.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520the%2520first%2520panoramic%2520generation%2520method%2520Percep360%2520for%250Aautonomous%2520driving.%2520Percep360%2520enables%2520coherent%2520generation%2520of%2520panoramic%2520data%250Awith%2520control%2520signals%2520based%2520on%2520the%2520stitched%2520panoramic%2520data.%2520Percep360%2520focuses%2520on%250Atwo%2520key%2520aspects%253A%2520coherence%2520and%2520controllability.%2520Specifically%252C%2520to%2520overcome%2520the%250Ainherent%2520information%2520loss%2520caused%2520by%2520the%2520pinhole%2520sampling%2520process%252C%2520we%2520propose%250Athe%2520Local%2520Scenes%2520Diffusion%2520Method%2520%2528LSDM%2529.%2520LSDM%2520reformulates%2520the%2520panorama%250Ageneration%2520as%2520a%2520spatially%2520continuous%2520diffusion%2520process%252C%2520bridging%2520the%2520gaps%250Abetween%2520different%2520data%2520distributions.%2520Additionally%252C%2520to%2520achieve%2520the%2520controllable%250Ageneration%2520of%2520panoramic%2520images%252C%2520we%2520propose%2520a%2520Probabilistic%2520Prompting%2520Method%250A%2528PPM%2529.%2520PPM%2520dynamically%2520selects%2520the%2520most%2520relevant%2520control%2520cues%252C%2520enabling%250Acontrollable%2520panoramic%2520image%2520generation.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520the%250Agenerated%2520images%2520from%2520three%2520perspectives%253A%2520image%2520quality%2520assessment%2520%2528i.e.%252C%250Ano-reference%2520and%2520with%2520reference%2529%252C%2520controllability%252C%2520and%2520their%2520utility%2520in%250Areal-world%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520segmentation.%2520Notably%252C%2520the%2520generated%2520data%250Aconsistently%2520outperforms%2520the%2520original%2520stitched%2520images%2520in%2520no-reference%2520quality%250Ametrics%2520and%2520enhances%2520downstream%2520perception%2520models.%2520The%2520source%2520code%2520will%2520be%250Apublicly%2520available%2520at%2520https%253A//github.com/Bryant-Teng/Percep360.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallucinating%20360%C2%B0%3A%20Panoramic%20Street-View%20Generation%20via%20Local%0A%20%20Scenes%20Diffusion%20and%20Probabilistic%20Prompting&entry.906535625=Fei%20Teng%20and%20Kai%20Luo%20and%20Sheng%20Wu%20and%20Siyu%20Li%20and%20Pujun%20Guo%20and%20Jiale%20Wei%20and%20Kunyu%20Peng%20and%20Jiaming%20Zhang%20and%20Kailun%20Yang&entry.1292438233=%20%20Panoramic%20perception%20holds%20significant%20potential%20for%20autonomous%20driving%2C%0Aenabling%20vehicles%20to%20acquire%20a%20comprehensive%20360%7B%5Cdeg%7D%20surround%20view%20in%20a%0Asingle%20shot.%20However%2C%20autonomous%20driving%20is%20a%20data-driven%20task.%20Complete%0Apanoramic%20data%20acquisition%20requires%20complex%20sampling%20systems%20and%20annotation%0Apipelines%2C%20which%20are%20time-consuming%20and%20labor-intensive.%20Although%20existing%0Astreet%20view%20generation%20models%20have%20demonstrated%20strong%20data%20regeneration%0Acapabilities%2C%20they%20can%20only%20learn%20from%20the%20fixed%20data%20distribution%20of%20existing%0Adatasets%20and%20cannot%20achieve%20high-quality%2C%20controllable%20panoramic%20generation.%20In%0Athis%20paper%2C%20we%20propose%20the%20first%20panoramic%20generation%20method%20Percep360%20for%0Aautonomous%20driving.%20Percep360%20enables%20coherent%20generation%20of%20panoramic%20data%0Awith%20control%20signals%20based%20on%20the%20stitched%20panoramic%20data.%20Percep360%20focuses%20on%0Atwo%20key%20aspects%3A%20coherence%20and%20controllability.%20Specifically%2C%20to%20overcome%20the%0Ainherent%20information%20loss%20caused%20by%20the%20pinhole%20sampling%20process%2C%20we%20propose%0Athe%20Local%20Scenes%20Diffusion%20Method%20%28LSDM%29.%20LSDM%20reformulates%20the%20panorama%0Ageneration%20as%20a%20spatially%20continuous%20diffusion%20process%2C%20bridging%20the%20gaps%0Abetween%20different%20data%20distributions.%20Additionally%2C%20to%20achieve%20the%20controllable%0Ageneration%20of%20panoramic%20images%2C%20we%20propose%20a%20Probabilistic%20Prompting%20Method%0A%28PPM%29.%20PPM%20dynamically%20selects%20the%20most%20relevant%20control%20cues%2C%20enabling%0Acontrollable%20panoramic%20image%20generation.%20We%20evaluate%20the%20effectiveness%20of%20the%0Agenerated%20images%20from%20three%20perspectives%3A%20image%20quality%20assessment%20%28i.e.%2C%0Ano-reference%20and%20with%20reference%29%2C%20controllability%2C%20and%20their%20utility%20in%0Areal-world%20Bird%27s%20Eye%20View%20%28BEV%29%20segmentation.%20Notably%2C%20the%20generated%20data%0Aconsistently%20outperforms%20the%20original%20stitched%20images%20in%20no-reference%20quality%0Ametrics%20and%20enhances%20downstream%20perception%20models.%20The%20source%20code%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/Bryant-Teng/Percep360.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06971v1&entry.124074799=Read"},
{"title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models", "author": "Tiezheng Zhang and Yitong Li and Yu-cheng Chou and Jieneng Chen and Alan Yuille and Chen Wei and Junfei Xiao", "abstract": "  Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD.\n", "link": "http://arxiv.org/abs/2507.07104v1", "date": "2025-07-09", "relevancy": 2.4369, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6122}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language-Vision%20Auto-Encoder%3A%20Scalable%20Knowledge%20Distillation%0A%20%20from%20Diffusion%20Models&body=Title%3A%20Vision-Language-Vision%20Auto-Encoder%3A%20Scalable%20Knowledge%20Distillation%0A%20%20from%20Diffusion%20Models%0AAuthor%3A%20Tiezheng%20Zhang%20and%20Yitong%20Li%20and%20Yu-cheng%20Chou%20and%20Jieneng%20Chen%20and%20Alan%20Yuille%20and%20Chen%20Wei%20and%20Junfei%20Xiao%0AAbstract%3A%20%20%20Building%20state-of-the-art%20Vision-Language%20Models%20%28VLMs%29%20with%20strong%0Acaptioning%20capabilities%20typically%20necessitates%20training%20on%20billions%20of%0Ahigh-quality%20image-text%20pairs%2C%20requiring%20millions%20of%20GPU%20hours.%20This%20paper%0Aintroduces%20the%20Vision-Language-Vision%20%28VLV%29%20auto-encoder%20framework%2C%20which%0Astrategically%20leverages%20key%20pretrained%20components%3A%20a%20vision%20encoder%2C%20the%0Adecoder%20of%20a%20Text-to-Image%20%28T2I%29%20diffusion%20model%2C%20and%20subsequently%2C%20a%20Large%0ALanguage%20Model%20%28LLM%29.%20Specifically%2C%20we%20establish%20an%20information%20bottleneck%20by%0Aregularizing%20the%20language%20representation%20space%2C%20achieved%20through%20freezing%20the%0Apretrained%20T2I%20diffusion%20decoder.%20Our%20VLV%20pipeline%20effectively%20distills%0Aknowledge%20from%20the%20text-conditioned%20diffusion%20model%20using%20continuous%0Aembeddings%2C%20demonstrating%20comprehensive%20semantic%20understanding%20via%20high-quality%0Areconstructions.%20Furthermore%2C%20by%20fine-tuning%20a%20pretrained%20LLM%20to%20decode%20the%0Aintermediate%20language%20representations%20into%20detailed%20descriptions%2C%20we%20construct%0Aa%20state-of-the-art%20%28SoTA%29%20captioner%20comparable%20to%20leading%20models%20like%20GPT-4o%0Aand%20Gemini%202.0%20Flash.%20Our%20method%20demonstrates%20exceptional%20cost-efficiency%20and%0Asignificantly%20reduces%20data%20requirements%3B%20by%20primarily%20utilizing%20single-modal%0Aimages%20for%20training%20and%20maximizing%20the%20utility%20of%20existing%20pretrained%20models%0A%28image%20encoder%2C%20T2I%20diffusion%20model%2C%20and%20LLM%29%2C%20it%20circumvents%20the%20need%20for%0Amassive%20paired%20image-text%20datasets%2C%20keeping%20the%20total%20training%20expenditure%0Aunder%20%241%2C000%20USD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language-Vision%2520Auto-Encoder%253A%2520Scalable%2520Knowledge%2520Distillation%250A%2520%2520from%2520Diffusion%2520Models%26entry.906535625%3DTiezheng%2520Zhang%2520and%2520Yitong%2520Li%2520and%2520Yu-cheng%2520Chou%2520and%2520Jieneng%2520Chen%2520and%2520Alan%2520Yuille%2520and%2520Chen%2520Wei%2520and%2520Junfei%2520Xiao%26entry.1292438233%3D%2520%2520Building%2520state-of-the-art%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520with%2520strong%250Acaptioning%2520capabilities%2520typically%2520necessitates%2520training%2520on%2520billions%2520of%250Ahigh-quality%2520image-text%2520pairs%252C%2520requiring%2520millions%2520of%2520GPU%2520hours.%2520This%2520paper%250Aintroduces%2520the%2520Vision-Language-Vision%2520%2528VLV%2529%2520auto-encoder%2520framework%252C%2520which%250Astrategically%2520leverages%2520key%2520pretrained%2520components%253A%2520a%2520vision%2520encoder%252C%2520the%250Adecoder%2520of%2520a%2520Text-to-Image%2520%2528T2I%2529%2520diffusion%2520model%252C%2520and%2520subsequently%252C%2520a%2520Large%250ALanguage%2520Model%2520%2528LLM%2529.%2520Specifically%252C%2520we%2520establish%2520an%2520information%2520bottleneck%2520by%250Aregularizing%2520the%2520language%2520representation%2520space%252C%2520achieved%2520through%2520freezing%2520the%250Apretrained%2520T2I%2520diffusion%2520decoder.%2520Our%2520VLV%2520pipeline%2520effectively%2520distills%250Aknowledge%2520from%2520the%2520text-conditioned%2520diffusion%2520model%2520using%2520continuous%250Aembeddings%252C%2520demonstrating%2520comprehensive%2520semantic%2520understanding%2520via%2520high-quality%250Areconstructions.%2520Furthermore%252C%2520by%2520fine-tuning%2520a%2520pretrained%2520LLM%2520to%2520decode%2520the%250Aintermediate%2520language%2520representations%2520into%2520detailed%2520descriptions%252C%2520we%2520construct%250Aa%2520state-of-the-art%2520%2528SoTA%2529%2520captioner%2520comparable%2520to%2520leading%2520models%2520like%2520GPT-4o%250Aand%2520Gemini%25202.0%2520Flash.%2520Our%2520method%2520demonstrates%2520exceptional%2520cost-efficiency%2520and%250Asignificantly%2520reduces%2520data%2520requirements%253B%2520by%2520primarily%2520utilizing%2520single-modal%250Aimages%2520for%2520training%2520and%2520maximizing%2520the%2520utility%2520of%2520existing%2520pretrained%2520models%250A%2528image%2520encoder%252C%2520T2I%2520diffusion%2520model%252C%2520and%2520LLM%2529%252C%2520it%2520circumvents%2520the%2520need%2520for%250Amassive%2520paired%2520image-text%2520datasets%252C%2520keeping%2520the%2520total%2520training%2520expenditure%250Aunder%2520%25241%252C000%2520USD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language-Vision%20Auto-Encoder%3A%20Scalable%20Knowledge%20Distillation%0A%20%20from%20Diffusion%20Models&entry.906535625=Tiezheng%20Zhang%20and%20Yitong%20Li%20and%20Yu-cheng%20Chou%20and%20Jieneng%20Chen%20and%20Alan%20Yuille%20and%20Chen%20Wei%20and%20Junfei%20Xiao&entry.1292438233=%20%20Building%20state-of-the-art%20Vision-Language%20Models%20%28VLMs%29%20with%20strong%0Acaptioning%20capabilities%20typically%20necessitates%20training%20on%20billions%20of%0Ahigh-quality%20image-text%20pairs%2C%20requiring%20millions%20of%20GPU%20hours.%20This%20paper%0Aintroduces%20the%20Vision-Language-Vision%20%28VLV%29%20auto-encoder%20framework%2C%20which%0Astrategically%20leverages%20key%20pretrained%20components%3A%20a%20vision%20encoder%2C%20the%0Adecoder%20of%20a%20Text-to-Image%20%28T2I%29%20diffusion%20model%2C%20and%20subsequently%2C%20a%20Large%0ALanguage%20Model%20%28LLM%29.%20Specifically%2C%20we%20establish%20an%20information%20bottleneck%20by%0Aregularizing%20the%20language%20representation%20space%2C%20achieved%20through%20freezing%20the%0Apretrained%20T2I%20diffusion%20decoder.%20Our%20VLV%20pipeline%20effectively%20distills%0Aknowledge%20from%20the%20text-conditioned%20diffusion%20model%20using%20continuous%0Aembeddings%2C%20demonstrating%20comprehensive%20semantic%20understanding%20via%20high-quality%0Areconstructions.%20Furthermore%2C%20by%20fine-tuning%20a%20pretrained%20LLM%20to%20decode%20the%0Aintermediate%20language%20representations%20into%20detailed%20descriptions%2C%20we%20construct%0Aa%20state-of-the-art%20%28SoTA%29%20captioner%20comparable%20to%20leading%20models%20like%20GPT-4o%0Aand%20Gemini%202.0%20Flash.%20Our%20method%20demonstrates%20exceptional%20cost-efficiency%20and%0Asignificantly%20reduces%20data%20requirements%3B%20by%20primarily%20utilizing%20single-modal%0Aimages%20for%20training%20and%20maximizing%20the%20utility%20of%20existing%20pretrained%20models%0A%28image%20encoder%2C%20T2I%20diffusion%20model%2C%20and%20LLM%29%2C%20it%20circumvents%20the%20need%20for%0Amassive%20paired%20image-text%20datasets%2C%20keeping%20the%20total%20training%20expenditure%0Aunder%20%241%2C000%20USD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07104v1&entry.124074799=Read"},
{"title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data", "author": "Ke Fan and Shunlin Lu and Minyue Dai and Runyi Yu and Lixing Xiao and Zhiyang Dou and Junting Dong and Lizhuang Ma and Jingbo Wang", "abstract": "  Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.\n", "link": "http://arxiv.org/abs/2507.07095v1", "date": "2025-07-09", "relevancy": 2.4317, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6186}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6015}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Go%20to%20Zero%3A%20Towards%20Zero-shot%20Motion%20Generation%20with%20Million-scale%20Data&body=Title%3A%20Go%20to%20Zero%3A%20Towards%20Zero-shot%20Motion%20Generation%20with%20Million-scale%20Data%0AAuthor%3A%20Ke%20Fan%20and%20Shunlin%20Lu%20and%20Minyue%20Dai%20and%20Runyi%20Yu%20and%20Lixing%20Xiao%20and%20Zhiyang%20Dou%20and%20Junting%20Dong%20and%20Lizhuang%20Ma%20and%20Jingbo%20Wang%0AAbstract%3A%20%20%20Generating%20diverse%20and%20natural%20human%20motion%20sequences%20based%20on%20textual%0Adescriptions%20constitutes%20a%20fundamental%20and%20challenging%20research%20area%20within%20the%0Adomains%20of%20computer%20vision%2C%20graphics%2C%20and%20robotics.%20Despite%20significant%0Aadvancements%20in%20this%20field%2C%20current%20methodologies%20often%20face%20challenges%0Aregarding%20zero-shot%20generalization%20capabilities%2C%20largely%20attributable%20to%20the%0Alimited%20size%20of%20training%20datasets.%20Moreover%2C%20the%20lack%20of%20a%20comprehensive%0Aevaluation%20framework%20impedes%20the%20advancement%20of%20this%20task%20by%20failing%20to%0Aidentify%20directions%20for%20improvement.%20In%20this%20work%2C%20we%20aim%20to%20push%0Atext-to-motion%20into%20a%20new%20era%2C%20that%20is%2C%20to%20achieve%20the%20generalization%20ability%0Aof%20zero-shot.%20To%20this%20end%2C%20firstly%2C%20we%20develop%20an%20efficient%20annotation%20pipeline%0Aand%20introduce%20MotionMillion-the%20largest%20human%20motion%20dataset%20to%20date%2C%20featuring%0Aover%202%2C000%20hours%20and%202%20million%20high-quality%20motion%20sequences.%20Additionally%2C%20we%0Apropose%20MotionMillion-Eval%2C%20the%20most%20comprehensive%20benchmark%20for%20evaluating%0Azero-shot%20motion%20generation.%20Leveraging%20a%20scalable%20architecture%2C%20we%20scale%20our%0Amodel%20to%207B%20parameters%20and%20validate%20its%20performance%20on%20MotionMillion-Eval.%20Our%0Aresults%20demonstrate%20strong%20generalization%20to%20out-of-domain%20and%20complex%0Acompositional%20motions%2C%20marking%20a%20significant%20step%20toward%20zero-shot%20human%20motion%0Ageneration.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/VankouF/MotionMillion-Codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGo%2520to%2520Zero%253A%2520Towards%2520Zero-shot%2520Motion%2520Generation%2520with%2520Million-scale%2520Data%26entry.906535625%3DKe%2520Fan%2520and%2520Shunlin%2520Lu%2520and%2520Minyue%2520Dai%2520and%2520Runyi%2520Yu%2520and%2520Lixing%2520Xiao%2520and%2520Zhiyang%2520Dou%2520and%2520Junting%2520Dong%2520and%2520Lizhuang%2520Ma%2520and%2520Jingbo%2520Wang%26entry.1292438233%3D%2520%2520Generating%2520diverse%2520and%2520natural%2520human%2520motion%2520sequences%2520based%2520on%2520textual%250Adescriptions%2520constitutes%2520a%2520fundamental%2520and%2520challenging%2520research%2520area%2520within%2520the%250Adomains%2520of%2520computer%2520vision%252C%2520graphics%252C%2520and%2520robotics.%2520Despite%2520significant%250Aadvancements%2520in%2520this%2520field%252C%2520current%2520methodologies%2520often%2520face%2520challenges%250Aregarding%2520zero-shot%2520generalization%2520capabilities%252C%2520largely%2520attributable%2520to%2520the%250Alimited%2520size%2520of%2520training%2520datasets.%2520Moreover%252C%2520the%2520lack%2520of%2520a%2520comprehensive%250Aevaluation%2520framework%2520impedes%2520the%2520advancement%2520of%2520this%2520task%2520by%2520failing%2520to%250Aidentify%2520directions%2520for%2520improvement.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520push%250Atext-to-motion%2520into%2520a%2520new%2520era%252C%2520that%2520is%252C%2520to%2520achieve%2520the%2520generalization%2520ability%250Aof%2520zero-shot.%2520To%2520this%2520end%252C%2520firstly%252C%2520we%2520develop%2520an%2520efficient%2520annotation%2520pipeline%250Aand%2520introduce%2520MotionMillion-the%2520largest%2520human%2520motion%2520dataset%2520to%2520date%252C%2520featuring%250Aover%25202%252C000%2520hours%2520and%25202%2520million%2520high-quality%2520motion%2520sequences.%2520Additionally%252C%2520we%250Apropose%2520MotionMillion-Eval%252C%2520the%2520most%2520comprehensive%2520benchmark%2520for%2520evaluating%250Azero-shot%2520motion%2520generation.%2520Leveraging%2520a%2520scalable%2520architecture%252C%2520we%2520scale%2520our%250Amodel%2520to%25207B%2520parameters%2520and%2520validate%2520its%2520performance%2520on%2520MotionMillion-Eval.%2520Our%250Aresults%2520demonstrate%2520strong%2520generalization%2520to%2520out-of-domain%2520and%2520complex%250Acompositional%2520motions%252C%2520marking%2520a%2520significant%2520step%2520toward%2520zero-shot%2520human%2520motion%250Ageneration.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/VankouF/MotionMillion-Codes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Go%20to%20Zero%3A%20Towards%20Zero-shot%20Motion%20Generation%20with%20Million-scale%20Data&entry.906535625=Ke%20Fan%20and%20Shunlin%20Lu%20and%20Minyue%20Dai%20and%20Runyi%20Yu%20and%20Lixing%20Xiao%20and%20Zhiyang%20Dou%20and%20Junting%20Dong%20and%20Lizhuang%20Ma%20and%20Jingbo%20Wang&entry.1292438233=%20%20Generating%20diverse%20and%20natural%20human%20motion%20sequences%20based%20on%20textual%0Adescriptions%20constitutes%20a%20fundamental%20and%20challenging%20research%20area%20within%20the%0Adomains%20of%20computer%20vision%2C%20graphics%2C%20and%20robotics.%20Despite%20significant%0Aadvancements%20in%20this%20field%2C%20current%20methodologies%20often%20face%20challenges%0Aregarding%20zero-shot%20generalization%20capabilities%2C%20largely%20attributable%20to%20the%0Alimited%20size%20of%20training%20datasets.%20Moreover%2C%20the%20lack%20of%20a%20comprehensive%0Aevaluation%20framework%20impedes%20the%20advancement%20of%20this%20task%20by%20failing%20to%0Aidentify%20directions%20for%20improvement.%20In%20this%20work%2C%20we%20aim%20to%20push%0Atext-to-motion%20into%20a%20new%20era%2C%20that%20is%2C%20to%20achieve%20the%20generalization%20ability%0Aof%20zero-shot.%20To%20this%20end%2C%20firstly%2C%20we%20develop%20an%20efficient%20annotation%20pipeline%0Aand%20introduce%20MotionMillion-the%20largest%20human%20motion%20dataset%20to%20date%2C%20featuring%0Aover%202%2C000%20hours%20and%202%20million%20high-quality%20motion%20sequences.%20Additionally%2C%20we%0Apropose%20MotionMillion-Eval%2C%20the%20most%20comprehensive%20benchmark%20for%20evaluating%0Azero-shot%20motion%20generation.%20Leveraging%20a%20scalable%20architecture%2C%20we%20scale%20our%0Amodel%20to%207B%20parameters%20and%20validate%20its%20performance%20on%20MotionMillion-Eval.%20Our%0Aresults%20demonstrate%20strong%20generalization%20to%20out-of-domain%20and%20complex%0Acompositional%20motions%2C%20marking%20a%20significant%20step%20toward%20zero-shot%20human%20motion%0Ageneration.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/VankouF/MotionMillion-Codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07095v1&entry.124074799=Read"},
{"title": "Intrinsic Training Signals for Federated Learning Aggregation", "author": "Cosimo Fiorini and Matteo Mosconi and Pietro Buzzega and Riccardo Salami and Simone Calderara", "abstract": "  Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. While existing approaches\nfor aggregating client-specific classification heads and adapted backbone\nparameters require architectural modifications or loss function changes, our\nmethod uniquely leverages intrinsic training signals already available during\nstandard optimization. We present LIVAR (Layer Importance and VARiance-based\nmerging), which introduces: i) a variance-weighted classifier aggregation\nscheme using naturally emergent feature statistics, and ii) an\nexplainability-driven LoRA merging technique based on SHAP analysis of existing\nupdate parameter patterns. Without any architectural overhead, LIVAR achieves\nstate-of-the-art performance on multiple benchmarks while maintaining seamless\nintegration with existing FL methods. This work demonstrates that effective\nmodel merging can be achieved solely through existing training signals,\nestablishing a new paradigm for efficient federated model aggregation. The code\nwill be made publicly available upon acceptance.\n", "link": "http://arxiv.org/abs/2507.06813v1", "date": "2025-07-09", "relevancy": 2.4114, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4821}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Training%20Signals%20for%20Federated%20Learning%20Aggregation&body=Title%3A%20Intrinsic%20Training%20Signals%20for%20Federated%20Learning%20Aggregation%0AAuthor%3A%20Cosimo%20Fiorini%20and%20Matteo%20Mosconi%20and%20Pietro%20Buzzega%20and%20Riccardo%20Salami%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy.%20While%20existing%20approaches%0Afor%20aggregating%20client-specific%20classification%20heads%20and%20adapted%20backbone%0Aparameters%20require%20architectural%20modifications%20or%20loss%20function%20changes%2C%20our%0Amethod%20uniquely%20leverages%20intrinsic%20training%20signals%20already%20available%20during%0Astandard%20optimization.%20We%20present%20LIVAR%20%28Layer%20Importance%20and%20VARiance-based%0Amerging%29%2C%20which%20introduces%3A%20i%29%20a%20variance-weighted%20classifier%20aggregation%0Ascheme%20using%20naturally%20emergent%20feature%20statistics%2C%20and%20ii%29%20an%0Aexplainability-driven%20LoRA%20merging%20technique%20based%20on%20SHAP%20analysis%20of%20existing%0Aupdate%20parameter%20patterns.%20Without%20any%20architectural%20overhead%2C%20LIVAR%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%20while%20maintaining%20seamless%0Aintegration%20with%20existing%20FL%20methods.%20This%20work%20demonstrates%20that%20effective%0Amodel%20merging%20can%20be%20achieved%20solely%20through%20existing%20training%20signals%2C%0Aestablishing%20a%20new%20paradigm%20for%20efficient%20federated%20model%20aggregation.%20The%20code%0Awill%20be%20made%20publicly%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520Training%2520Signals%2520for%2520Federated%2520Learning%2520Aggregation%26entry.906535625%3DCosimo%2520Fiorini%2520and%2520Matteo%2520Mosconi%2520and%2520Pietro%2520Buzzega%2520and%2520Riccardo%2520Salami%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adistributed%2520clients%2520while%2520preserving%2520data%2520privacy.%2520While%2520existing%2520approaches%250Afor%2520aggregating%2520client-specific%2520classification%2520heads%2520and%2520adapted%2520backbone%250Aparameters%2520require%2520architectural%2520modifications%2520or%2520loss%2520function%2520changes%252C%2520our%250Amethod%2520uniquely%2520leverages%2520intrinsic%2520training%2520signals%2520already%2520available%2520during%250Astandard%2520optimization.%2520We%2520present%2520LIVAR%2520%2528Layer%2520Importance%2520and%2520VARiance-based%250Amerging%2529%252C%2520which%2520introduces%253A%2520i%2529%2520a%2520variance-weighted%2520classifier%2520aggregation%250Ascheme%2520using%2520naturally%2520emergent%2520feature%2520statistics%252C%2520and%2520ii%2529%2520an%250Aexplainability-driven%2520LoRA%2520merging%2520technique%2520based%2520on%2520SHAP%2520analysis%2520of%2520existing%250Aupdate%2520parameter%2520patterns.%2520Without%2520any%2520architectural%2520overhead%252C%2520LIVAR%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multiple%2520benchmarks%2520while%2520maintaining%2520seamless%250Aintegration%2520with%2520existing%2520FL%2520methods.%2520This%2520work%2520demonstrates%2520that%2520effective%250Amodel%2520merging%2520can%2520be%2520achieved%2520solely%2520through%2520existing%2520training%2520signals%252C%250Aestablishing%2520a%2520new%2520paradigm%2520for%2520efficient%2520federated%2520model%2520aggregation.%2520The%2520code%250Awill%2520be%2520made%2520publicly%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Training%20Signals%20for%20Federated%20Learning%20Aggregation&entry.906535625=Cosimo%20Fiorini%20and%20Matteo%20Mosconi%20and%20Pietro%20Buzzega%20and%20Riccardo%20Salami%20and%20Simone%20Calderara&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy.%20While%20existing%20approaches%0Afor%20aggregating%20client-specific%20classification%20heads%20and%20adapted%20backbone%0Aparameters%20require%20architectural%20modifications%20or%20loss%20function%20changes%2C%20our%0Amethod%20uniquely%20leverages%20intrinsic%20training%20signals%20already%20available%20during%0Astandard%20optimization.%20We%20present%20LIVAR%20%28Layer%20Importance%20and%20VARiance-based%0Amerging%29%2C%20which%20introduces%3A%20i%29%20a%20variance-weighted%20classifier%20aggregation%0Ascheme%20using%20naturally%20emergent%20feature%20statistics%2C%20and%20ii%29%20an%0Aexplainability-driven%20LoRA%20merging%20technique%20based%20on%20SHAP%20analysis%20of%20existing%0Aupdate%20parameter%20patterns.%20Without%20any%20architectural%20overhead%2C%20LIVAR%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%20while%20maintaining%20seamless%0Aintegration%20with%20existing%20FL%20methods.%20This%20work%20demonstrates%20that%20effective%0Amodel%20merging%20can%20be%20achieved%20solely%20through%20existing%20training%20signals%2C%0Aestablishing%20a%20new%20paradigm%20for%20efficient%20federated%20model%20aggregation.%20The%20code%0Awill%20be%20made%20publicly%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06813v1&entry.124074799=Read"},
{"title": "TokenShapley: Token Level Context Attribution with Shapley Value", "author": "Yingtai Xiao and Yuqing Zhu and Sirat Samyoun and Wanrong Zhang and Jiachen T. Wang and Jian Du", "abstract": "  Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy.\n", "link": "http://arxiv.org/abs/2507.05261v2", "date": "2025-07-09", "relevancy": 2.3759, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4842}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenShapley%3A%20Token%20Level%20Context%20Attribution%20with%20Shapley%20Value&body=Title%3A%20TokenShapley%3A%20Token%20Level%20Context%20Attribution%20with%20Shapley%20Value%0AAuthor%3A%20Yingtai%20Xiao%20and%20Yuqing%20Zhu%20and%20Sirat%20Samyoun%20and%20Wanrong%20Zhang%20and%20Jiachen%20T.%20Wang%20and%20Jian%20Du%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20strong%20capabilities%20in%20in-context%0Alearning%2C%20but%20verifying%20the%20correctness%20of%20their%20generated%20responses%20remains%20a%0Achallenge.%20Prior%20work%20has%20explored%20attribution%20at%20the%20sentence%20level%2C%20but%20these%0Amethods%20fall%20short%20when%20users%20seek%20attribution%20for%20specific%20keywords%20within%20the%0Aresponse%2C%20such%20as%20numbers%2C%20years%2C%20or%20names.%20To%20address%20this%20limitation%2C%20we%0Apropose%20TokenShapley%2C%20a%20novel%20token-level%20attribution%20method%20that%20combines%0AShapley%20value-based%20data%20attribution%20with%20KNN-based%20retrieval%20techniques%0Ainspired%20by%20recent%20advances%20in%20KNN-augmented%20LLMs.%20By%20leveraging%20a%20precomputed%0Adatastore%20for%20contextual%20retrieval%20and%20computing%20Shapley%20values%20to%20quantify%0Atoken%20importance%2C%20TokenShapley%20provides%20a%20fine-grained%20data%20attribution%0Aapproach.%20Extensive%20evaluations%20on%20four%20benchmarks%20show%20that%20TokenShapley%0Aoutperforms%20state-of-the-art%20baselines%20in%20token-level%20attribution%2C%20achieving%20an%0A11-23%25%20improvement%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05261v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenShapley%253A%2520Token%2520Level%2520Context%2520Attribution%2520with%2520Shapley%2520Value%26entry.906535625%3DYingtai%2520Xiao%2520and%2520Yuqing%2520Zhu%2520and%2520Sirat%2520Samyoun%2520and%2520Wanrong%2520Zhang%2520and%2520Jiachen%2520T.%2520Wang%2520and%2520Jian%2520Du%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520strong%2520capabilities%2520in%2520in-context%250Alearning%252C%2520but%2520verifying%2520the%2520correctness%2520of%2520their%2520generated%2520responses%2520remains%2520a%250Achallenge.%2520Prior%2520work%2520has%2520explored%2520attribution%2520at%2520the%2520sentence%2520level%252C%2520but%2520these%250Amethods%2520fall%2520short%2520when%2520users%2520seek%2520attribution%2520for%2520specific%2520keywords%2520within%2520the%250Aresponse%252C%2520such%2520as%2520numbers%252C%2520years%252C%2520or%2520names.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520TokenShapley%252C%2520a%2520novel%2520token-level%2520attribution%2520method%2520that%2520combines%250AShapley%2520value-based%2520data%2520attribution%2520with%2520KNN-based%2520retrieval%2520techniques%250Ainspired%2520by%2520recent%2520advances%2520in%2520KNN-augmented%2520LLMs.%2520By%2520leveraging%2520a%2520precomputed%250Adatastore%2520for%2520contextual%2520retrieval%2520and%2520computing%2520Shapley%2520values%2520to%2520quantify%250Atoken%2520importance%252C%2520TokenShapley%2520provides%2520a%2520fine-grained%2520data%2520attribution%250Aapproach.%2520Extensive%2520evaluations%2520on%2520four%2520benchmarks%2520show%2520that%2520TokenShapley%250Aoutperforms%2520state-of-the-art%2520baselines%2520in%2520token-level%2520attribution%252C%2520achieving%2520an%250A11-23%2525%2520improvement%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05261v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenShapley%3A%20Token%20Level%20Context%20Attribution%20with%20Shapley%20Value&entry.906535625=Yingtai%20Xiao%20and%20Yuqing%20Zhu%20and%20Sirat%20Samyoun%20and%20Wanrong%20Zhang%20and%20Jiachen%20T.%20Wang%20and%20Jian%20Du&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20strong%20capabilities%20in%20in-context%0Alearning%2C%20but%20verifying%20the%20correctness%20of%20their%20generated%20responses%20remains%20a%0Achallenge.%20Prior%20work%20has%20explored%20attribution%20at%20the%20sentence%20level%2C%20but%20these%0Amethods%20fall%20short%20when%20users%20seek%20attribution%20for%20specific%20keywords%20within%20the%0Aresponse%2C%20such%20as%20numbers%2C%20years%2C%20or%20names.%20To%20address%20this%20limitation%2C%20we%0Apropose%20TokenShapley%2C%20a%20novel%20token-level%20attribution%20method%20that%20combines%0AShapley%20value-based%20data%20attribution%20with%20KNN-based%20retrieval%20techniques%0Ainspired%20by%20recent%20advances%20in%20KNN-augmented%20LLMs.%20By%20leveraging%20a%20precomputed%0Adatastore%20for%20contextual%20retrieval%20and%20computing%20Shapley%20values%20to%20quantify%0Atoken%20importance%2C%20TokenShapley%20provides%20a%20fine-grained%20data%20attribution%0Aapproach.%20Extensive%20evaluations%20on%20four%20benchmarks%20show%20that%20TokenShapley%0Aoutperforms%20state-of-the-art%20baselines%20in%20token-level%20attribution%2C%20achieving%20an%0A11-23%25%20improvement%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05261v2&entry.124074799=Read"},
{"title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory", "author": "Nan Chen and Mengqi Huang and Yihao Meng and Zhendong Mao", "abstract": "  Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.\n", "link": "http://arxiv.org/abs/2507.01945v2", "date": "2025-07-09", "relevancy": 2.3709, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6096}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5915}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongAnimation%3A%20Long%20Animation%20Generation%20with%20Dynamic%20Global-Local%0A%20%20Memory&body=Title%3A%20LongAnimation%3A%20Long%20Animation%20Generation%20with%20Dynamic%20Global-Local%0A%20%20Memory%0AAuthor%3A%20Nan%20Chen%20and%20Mengqi%20Huang%20and%20Yihao%20Meng%20and%20Zhendong%20Mao%0AAbstract%3A%20%20%20Animation%20colorization%20is%20a%20crucial%20part%20of%20real%20animation%20industry%0Aproduction.%20Long%20animation%20colorization%20has%20high%20labor%20costs.%20Therefore%2C%0Aautomated%20long%20animation%20colorization%20based%20on%20the%20video%20generation%20model%20has%0Asignificant%20research%20value.%20Existing%20studies%20are%20limited%20to%20short-term%0Acolorization.%20These%20studies%20adopt%20a%20local%20paradigm%2C%20fusing%20overlapping%20features%0Ato%20achieve%20smooth%20transitions%20between%20local%20segments.%20However%2C%20the%20local%0Aparadigm%20neglects%20global%20information%2C%20failing%20to%20maintain%20long-term%20color%0Aconsistency.%20In%20this%20study%2C%20we%20argue%20that%20ideal%20long-term%20color%20consistency%20can%0Abe%20achieved%20through%20a%20dynamic%20global-local%20paradigm%2C%20i.e.%2C%20dynamically%0Aextracting%20global%20color-consistent%20features%20relevant%20to%20the%20current%20generation.%0ASpecifically%2C%20we%20propose%20LongAnimation%2C%20a%20novel%20framework%2C%20which%20mainly%0Aincludes%20a%20SketchDiT%2C%20a%20Dynamic%20Global-Local%20Memory%20%28DGLM%29%2C%20and%20a%20Color%0AConsistency%20Reward.%20The%20SketchDiT%20captures%20hybrid%20reference%20features%20to%20support%0Athe%20DGLM%20module.%20The%20DGLM%20module%20employs%20a%20long%20video%20understanding%20model%20to%0Adynamically%20compress%20global%20historical%20features%20and%20adaptively%20fuse%20them%20with%0Athe%20current%20generation%20features.%20To%20refine%20the%20color%20consistency%2C%20we%20introduce%0Aa%20Color%20Consistency%20Reward.%20During%20inference%2C%20we%20propose%20a%20color%20consistency%0Afusion%20to%20smooth%20the%20video%20segment%20transition.%20Extensive%20experiments%20on%20both%0Ashort-term%20%2814%20frames%29%20and%20long-term%20%28average%20500%20frames%29%20animations%20show%20the%0Aeffectiveness%20of%20LongAnimation%20in%20maintaining%20short-term%20and%20long-term%20color%0Aconsistency%20for%20open-domain%20animation%20colorization%20task.%20The%20code%20can%20be%20found%0Aat%20https%3A//cn-makers.github.io/long_animation_web/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongAnimation%253A%2520Long%2520Animation%2520Generation%2520with%2520Dynamic%2520Global-Local%250A%2520%2520Memory%26entry.906535625%3DNan%2520Chen%2520and%2520Mengqi%2520Huang%2520and%2520Yihao%2520Meng%2520and%2520Zhendong%2520Mao%26entry.1292438233%3D%2520%2520Animation%2520colorization%2520is%2520a%2520crucial%2520part%2520of%2520real%2520animation%2520industry%250Aproduction.%2520Long%2520animation%2520colorization%2520has%2520high%2520labor%2520costs.%2520Therefore%252C%250Aautomated%2520long%2520animation%2520colorization%2520based%2520on%2520the%2520video%2520generation%2520model%2520has%250Asignificant%2520research%2520value.%2520Existing%2520studies%2520are%2520limited%2520to%2520short-term%250Acolorization.%2520These%2520studies%2520adopt%2520a%2520local%2520paradigm%252C%2520fusing%2520overlapping%2520features%250Ato%2520achieve%2520smooth%2520transitions%2520between%2520local%2520segments.%2520However%252C%2520the%2520local%250Aparadigm%2520neglects%2520global%2520information%252C%2520failing%2520to%2520maintain%2520long-term%2520color%250Aconsistency.%2520In%2520this%2520study%252C%2520we%2520argue%2520that%2520ideal%2520long-term%2520color%2520consistency%2520can%250Abe%2520achieved%2520through%2520a%2520dynamic%2520global-local%2520paradigm%252C%2520i.e.%252C%2520dynamically%250Aextracting%2520global%2520color-consistent%2520features%2520relevant%2520to%2520the%2520current%2520generation.%250ASpecifically%252C%2520we%2520propose%2520LongAnimation%252C%2520a%2520novel%2520framework%252C%2520which%2520mainly%250Aincludes%2520a%2520SketchDiT%252C%2520a%2520Dynamic%2520Global-Local%2520Memory%2520%2528DGLM%2529%252C%2520and%2520a%2520Color%250AConsistency%2520Reward.%2520The%2520SketchDiT%2520captures%2520hybrid%2520reference%2520features%2520to%2520support%250Athe%2520DGLM%2520module.%2520The%2520DGLM%2520module%2520employs%2520a%2520long%2520video%2520understanding%2520model%2520to%250Adynamically%2520compress%2520global%2520historical%2520features%2520and%2520adaptively%2520fuse%2520them%2520with%250Athe%2520current%2520generation%2520features.%2520To%2520refine%2520the%2520color%2520consistency%252C%2520we%2520introduce%250Aa%2520Color%2520Consistency%2520Reward.%2520During%2520inference%252C%2520we%2520propose%2520a%2520color%2520consistency%250Afusion%2520to%2520smooth%2520the%2520video%2520segment%2520transition.%2520Extensive%2520experiments%2520on%2520both%250Ashort-term%2520%252814%2520frames%2529%2520and%2520long-term%2520%2528average%2520500%2520frames%2529%2520animations%2520show%2520the%250Aeffectiveness%2520of%2520LongAnimation%2520in%2520maintaining%2520short-term%2520and%2520long-term%2520color%250Aconsistency%2520for%2520open-domain%2520animation%2520colorization%2520task.%2520The%2520code%2520can%2520be%2520found%250Aat%2520https%253A//cn-makers.github.io/long_animation_web/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongAnimation%3A%20Long%20Animation%20Generation%20with%20Dynamic%20Global-Local%0A%20%20Memory&entry.906535625=Nan%20Chen%20and%20Mengqi%20Huang%20and%20Yihao%20Meng%20and%20Zhendong%20Mao&entry.1292438233=%20%20Animation%20colorization%20is%20a%20crucial%20part%20of%20real%20animation%20industry%0Aproduction.%20Long%20animation%20colorization%20has%20high%20labor%20costs.%20Therefore%2C%0Aautomated%20long%20animation%20colorization%20based%20on%20the%20video%20generation%20model%20has%0Asignificant%20research%20value.%20Existing%20studies%20are%20limited%20to%20short-term%0Acolorization.%20These%20studies%20adopt%20a%20local%20paradigm%2C%20fusing%20overlapping%20features%0Ato%20achieve%20smooth%20transitions%20between%20local%20segments.%20However%2C%20the%20local%0Aparadigm%20neglects%20global%20information%2C%20failing%20to%20maintain%20long-term%20color%0Aconsistency.%20In%20this%20study%2C%20we%20argue%20that%20ideal%20long-term%20color%20consistency%20can%0Abe%20achieved%20through%20a%20dynamic%20global-local%20paradigm%2C%20i.e.%2C%20dynamically%0Aextracting%20global%20color-consistent%20features%20relevant%20to%20the%20current%20generation.%0ASpecifically%2C%20we%20propose%20LongAnimation%2C%20a%20novel%20framework%2C%20which%20mainly%0Aincludes%20a%20SketchDiT%2C%20a%20Dynamic%20Global-Local%20Memory%20%28DGLM%29%2C%20and%20a%20Color%0AConsistency%20Reward.%20The%20SketchDiT%20captures%20hybrid%20reference%20features%20to%20support%0Athe%20DGLM%20module.%20The%20DGLM%20module%20employs%20a%20long%20video%20understanding%20model%20to%0Adynamically%20compress%20global%20historical%20features%20and%20adaptively%20fuse%20them%20with%0Athe%20current%20generation%20features.%20To%20refine%20the%20color%20consistency%2C%20we%20introduce%0Aa%20Color%20Consistency%20Reward.%20During%20inference%2C%20we%20propose%20a%20color%20consistency%0Afusion%20to%20smooth%20the%20video%20segment%20transition.%20Extensive%20experiments%20on%20both%0Ashort-term%20%2814%20frames%29%20and%20long-term%20%28average%20500%20frames%29%20animations%20show%20the%0Aeffectiveness%20of%20LongAnimation%20in%20maintaining%20short-term%20and%20long-term%20color%0Aconsistency%20for%20open-domain%20animation%20colorization%20task.%20The%20code%20can%20be%20found%0Aat%20https%3A//cn-makers.github.io/long_animation_web/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01945v2&entry.124074799=Read"},
{"title": "Adaptive collaboration for online personalized distributed learning with\n  heterogeneous clients", "author": "Constantin Philippenko and Batiste Le Bars and Kevin Scaman and Laurent Massouli\u00e9", "abstract": "  We study the problem of online personalized decentralized learning with $N$\nstatistically heterogeneous clients collaborating to accelerate local training.\nAn important challenge in this setting is to select relevant collaborators to\nreduce gradient variance while mitigating the introduced bias. To tackle this,\nwe introduce a gradient-based collaboration criterion, allowing each client to\ndynamically select peers with similar gradients during the optimization\nprocess. Our criterion is motivated by a refined and more general theoretical\nanalysis of the All-for-one algorithm, proved to be optimal in Even et al.\n(2022) for an oracle collaboration scheme. We derive excess loss upper-bounds\nfor smooth objective functions, being either strongly convex, non-convex, or\nsatisfying the Polyak-Lojasiewicz condition; our analysis reveals that the\nalgorithm acts as a variance reduction method where the speed-up depends on a\nsufficient variance. We put forward two collaboration methods instantiating the\nproposed general schema; and we show that one variant preserves the optimality\nof All-for-one. We validate our results with experiments on synthetic and real\ndatasets.\n", "link": "http://arxiv.org/abs/2507.06844v1", "date": "2025-07-09", "relevancy": 2.3655, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4946}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4663}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20collaboration%20for%20online%20personalized%20distributed%20learning%20with%0A%20%20heterogeneous%20clients&body=Title%3A%20Adaptive%20collaboration%20for%20online%20personalized%20distributed%20learning%20with%0A%20%20heterogeneous%20clients%0AAuthor%3A%20Constantin%20Philippenko%20and%20Batiste%20Le%20Bars%20and%20Kevin%20Scaman%20and%20Laurent%20Massouli%C3%A9%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20online%20personalized%20decentralized%20learning%20with%20%24N%24%0Astatistically%20heterogeneous%20clients%20collaborating%20to%20accelerate%20local%20training.%0AAn%20important%20challenge%20in%20this%20setting%20is%20to%20select%20relevant%20collaborators%20to%0Areduce%20gradient%20variance%20while%20mitigating%20the%20introduced%20bias.%20To%20tackle%20this%2C%0Awe%20introduce%20a%20gradient-based%20collaboration%20criterion%2C%20allowing%20each%20client%20to%0Adynamically%20select%20peers%20with%20similar%20gradients%20during%20the%20optimization%0Aprocess.%20Our%20criterion%20is%20motivated%20by%20a%20refined%20and%20more%20general%20theoretical%0Aanalysis%20of%20the%20All-for-one%20algorithm%2C%20proved%20to%20be%20optimal%20in%20Even%20et%20al.%0A%282022%29%20for%20an%20oracle%20collaboration%20scheme.%20We%20derive%20excess%20loss%20upper-bounds%0Afor%20smooth%20objective%20functions%2C%20being%20either%20strongly%20convex%2C%20non-convex%2C%20or%0Asatisfying%20the%20Polyak-Lojasiewicz%20condition%3B%20our%20analysis%20reveals%20that%20the%0Aalgorithm%20acts%20as%20a%20variance%20reduction%20method%20where%20the%20speed-up%20depends%20on%20a%0Asufficient%20variance.%20We%20put%20forward%20two%20collaboration%20methods%20instantiating%20the%0Aproposed%20general%20schema%3B%20and%20we%20show%20that%20one%20variant%20preserves%20the%20optimality%0Aof%20All-for-one.%20We%20validate%20our%20results%20with%20experiments%20on%20synthetic%20and%20real%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520collaboration%2520for%2520online%2520personalized%2520distributed%2520learning%2520with%250A%2520%2520heterogeneous%2520clients%26entry.906535625%3DConstantin%2520Philippenko%2520and%2520Batiste%2520Le%2520Bars%2520and%2520Kevin%2520Scaman%2520and%2520Laurent%2520Massouli%25C3%25A9%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520online%2520personalized%2520decentralized%2520learning%2520with%2520%2524N%2524%250Astatistically%2520heterogeneous%2520clients%2520collaborating%2520to%2520accelerate%2520local%2520training.%250AAn%2520important%2520challenge%2520in%2520this%2520setting%2520is%2520to%2520select%2520relevant%2520collaborators%2520to%250Areduce%2520gradient%2520variance%2520while%2520mitigating%2520the%2520introduced%2520bias.%2520To%2520tackle%2520this%252C%250Awe%2520introduce%2520a%2520gradient-based%2520collaboration%2520criterion%252C%2520allowing%2520each%2520client%2520to%250Adynamically%2520select%2520peers%2520with%2520similar%2520gradients%2520during%2520the%2520optimization%250Aprocess.%2520Our%2520criterion%2520is%2520motivated%2520by%2520a%2520refined%2520and%2520more%2520general%2520theoretical%250Aanalysis%2520of%2520the%2520All-for-one%2520algorithm%252C%2520proved%2520to%2520be%2520optimal%2520in%2520Even%2520et%2520al.%250A%25282022%2529%2520for%2520an%2520oracle%2520collaboration%2520scheme.%2520We%2520derive%2520excess%2520loss%2520upper-bounds%250Afor%2520smooth%2520objective%2520functions%252C%2520being%2520either%2520strongly%2520convex%252C%2520non-convex%252C%2520or%250Asatisfying%2520the%2520Polyak-Lojasiewicz%2520condition%253B%2520our%2520analysis%2520reveals%2520that%2520the%250Aalgorithm%2520acts%2520as%2520a%2520variance%2520reduction%2520method%2520where%2520the%2520speed-up%2520depends%2520on%2520a%250Asufficient%2520variance.%2520We%2520put%2520forward%2520two%2520collaboration%2520methods%2520instantiating%2520the%250Aproposed%2520general%2520schema%253B%2520and%2520we%2520show%2520that%2520one%2520variant%2520preserves%2520the%2520optimality%250Aof%2520All-for-one.%2520We%2520validate%2520our%2520results%2520with%2520experiments%2520on%2520synthetic%2520and%2520real%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20collaboration%20for%20online%20personalized%20distributed%20learning%20with%0A%20%20heterogeneous%20clients&entry.906535625=Constantin%20Philippenko%20and%20Batiste%20Le%20Bars%20and%20Kevin%20Scaman%20and%20Laurent%20Massouli%C3%A9&entry.1292438233=%20%20We%20study%20the%20problem%20of%20online%20personalized%20decentralized%20learning%20with%20%24N%24%0Astatistically%20heterogeneous%20clients%20collaborating%20to%20accelerate%20local%20training.%0AAn%20important%20challenge%20in%20this%20setting%20is%20to%20select%20relevant%20collaborators%20to%0Areduce%20gradient%20variance%20while%20mitigating%20the%20introduced%20bias.%20To%20tackle%20this%2C%0Awe%20introduce%20a%20gradient-based%20collaboration%20criterion%2C%20allowing%20each%20client%20to%0Adynamically%20select%20peers%20with%20similar%20gradients%20during%20the%20optimization%0Aprocess.%20Our%20criterion%20is%20motivated%20by%20a%20refined%20and%20more%20general%20theoretical%0Aanalysis%20of%20the%20All-for-one%20algorithm%2C%20proved%20to%20be%20optimal%20in%20Even%20et%20al.%0A%282022%29%20for%20an%20oracle%20collaboration%20scheme.%20We%20derive%20excess%20loss%20upper-bounds%0Afor%20smooth%20objective%20functions%2C%20being%20either%20strongly%20convex%2C%20non-convex%2C%20or%0Asatisfying%20the%20Polyak-Lojasiewicz%20condition%3B%20our%20analysis%20reveals%20that%20the%0Aalgorithm%20acts%20as%20a%20variance%20reduction%20method%20where%20the%20speed-up%20depends%20on%20a%0Asufficient%20variance.%20We%20put%20forward%20two%20collaboration%20methods%20instantiating%20the%0Aproposed%20general%20schema%3B%20and%20we%20show%20that%20one%20variant%20preserves%20the%20optimality%0Aof%20All-for-one.%20We%20validate%20our%20results%20with%20experiments%20on%20synthetic%20and%20real%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06844v1&entry.124074799=Read"},
{"title": "DICE: Data Influence Cascade in Decentralized Learning", "author": "Tongtian Zhu and Wenhao Li and Can Wang and Fengxiang He", "abstract": "  Decentralized learning offers a promising approach to crowdsource data\nconsumptions and computational workloads across geographically distributed\ncompute interconnected through peer-to-peer networks, accommodating the\nexponentially increasing demands. However, proper incentives are still in\nabsence, considerably discouraging participation. Our vision is that a fair\nincentive mechanism relies on fair attribution of contributions to\nparticipating nodes, which faces non-trivial challenges arising from the\nlocalized connections making influence ``cascade'' in a decentralized network.\nTo overcome this, we design the first method to estimate \\textbf{D}ata\n\\textbf{I}nfluence \\textbf{C}ascad\\textbf{E} (DICE) in a decentralized\nenvironment. Theoretically, the framework derives tractable approximations of\ninfluence cascade over arbitrary neighbor hops, suggesting the influence\ncascade is determined by an interplay of data, communication topology, and the\ncurvature of loss landscape. DICE also lays the foundations for applications\nincluding selecting suitable collaborators and identifying malicious behaviors.\nProject page is available at https://raiden-zhu.github.io/blog/2025/DICE/.\n", "link": "http://arxiv.org/abs/2507.06931v1", "date": "2025-07-09", "relevancy": 2.3507, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4869}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4733}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DICE%3A%20Data%20Influence%20Cascade%20in%20Decentralized%20Learning&body=Title%3A%20DICE%3A%20Data%20Influence%20Cascade%20in%20Decentralized%20Learning%0AAuthor%3A%20Tongtian%20Zhu%20and%20Wenhao%20Li%20and%20Can%20Wang%20and%20Fengxiang%20He%0AAbstract%3A%20%20%20Decentralized%20learning%20offers%20a%20promising%20approach%20to%20crowdsource%20data%0Aconsumptions%20and%20computational%20workloads%20across%20geographically%20distributed%0Acompute%20interconnected%20through%20peer-to-peer%20networks%2C%20accommodating%20the%0Aexponentially%20increasing%20demands.%20However%2C%20proper%20incentives%20are%20still%20in%0Aabsence%2C%20considerably%20discouraging%20participation.%20Our%20vision%20is%20that%20a%20fair%0Aincentive%20mechanism%20relies%20on%20fair%20attribution%20of%20contributions%20to%0Aparticipating%20nodes%2C%20which%20faces%20non-trivial%20challenges%20arising%20from%20the%0Alocalized%20connections%20making%20influence%20%60%60cascade%27%27%20in%20a%20decentralized%20network.%0ATo%20overcome%20this%2C%20we%20design%20the%20first%20method%20to%20estimate%20%5Ctextbf%7BD%7Data%0A%5Ctextbf%7BI%7Dnfluence%20%5Ctextbf%7BC%7Dascad%5Ctextbf%7BE%7D%20%28DICE%29%20in%20a%20decentralized%0Aenvironment.%20Theoretically%2C%20the%20framework%20derives%20tractable%20approximations%20of%0Ainfluence%20cascade%20over%20arbitrary%20neighbor%20hops%2C%20suggesting%20the%20influence%0Acascade%20is%20determined%20by%20an%20interplay%20of%20data%2C%20communication%20topology%2C%20and%20the%0Acurvature%20of%20loss%20landscape.%20DICE%20also%20lays%20the%20foundations%20for%20applications%0Aincluding%20selecting%20suitable%20collaborators%20and%20identifying%20malicious%20behaviors.%0AProject%20page%20is%20available%20at%20https%3A//raiden-zhu.github.io/blog/2025/DICE/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDICE%253A%2520Data%2520Influence%2520Cascade%2520in%2520Decentralized%2520Learning%26entry.906535625%3DTongtian%2520Zhu%2520and%2520Wenhao%2520Li%2520and%2520Can%2520Wang%2520and%2520Fengxiang%2520He%26entry.1292438233%3D%2520%2520Decentralized%2520learning%2520offers%2520a%2520promising%2520approach%2520to%2520crowdsource%2520data%250Aconsumptions%2520and%2520computational%2520workloads%2520across%2520geographically%2520distributed%250Acompute%2520interconnected%2520through%2520peer-to-peer%2520networks%252C%2520accommodating%2520the%250Aexponentially%2520increasing%2520demands.%2520However%252C%2520proper%2520incentives%2520are%2520still%2520in%250Aabsence%252C%2520considerably%2520discouraging%2520participation.%2520Our%2520vision%2520is%2520that%2520a%2520fair%250Aincentive%2520mechanism%2520relies%2520on%2520fair%2520attribution%2520of%2520contributions%2520to%250Aparticipating%2520nodes%252C%2520which%2520faces%2520non-trivial%2520challenges%2520arising%2520from%2520the%250Alocalized%2520connections%2520making%2520influence%2520%2560%2560cascade%2527%2527%2520in%2520a%2520decentralized%2520network.%250ATo%2520overcome%2520this%252C%2520we%2520design%2520the%2520first%2520method%2520to%2520estimate%2520%255Ctextbf%257BD%257Data%250A%255Ctextbf%257BI%257Dnfluence%2520%255Ctextbf%257BC%257Dascad%255Ctextbf%257BE%257D%2520%2528DICE%2529%2520in%2520a%2520decentralized%250Aenvironment.%2520Theoretically%252C%2520the%2520framework%2520derives%2520tractable%2520approximations%2520of%250Ainfluence%2520cascade%2520over%2520arbitrary%2520neighbor%2520hops%252C%2520suggesting%2520the%2520influence%250Acascade%2520is%2520determined%2520by%2520an%2520interplay%2520of%2520data%252C%2520communication%2520topology%252C%2520and%2520the%250Acurvature%2520of%2520loss%2520landscape.%2520DICE%2520also%2520lays%2520the%2520foundations%2520for%2520applications%250Aincluding%2520selecting%2520suitable%2520collaborators%2520and%2520identifying%2520malicious%2520behaviors.%250AProject%2520page%2520is%2520available%2520at%2520https%253A//raiden-zhu.github.io/blog/2025/DICE/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DICE%3A%20Data%20Influence%20Cascade%20in%20Decentralized%20Learning&entry.906535625=Tongtian%20Zhu%20and%20Wenhao%20Li%20and%20Can%20Wang%20and%20Fengxiang%20He&entry.1292438233=%20%20Decentralized%20learning%20offers%20a%20promising%20approach%20to%20crowdsource%20data%0Aconsumptions%20and%20computational%20workloads%20across%20geographically%20distributed%0Acompute%20interconnected%20through%20peer-to-peer%20networks%2C%20accommodating%20the%0Aexponentially%20increasing%20demands.%20However%2C%20proper%20incentives%20are%20still%20in%0Aabsence%2C%20considerably%20discouraging%20participation.%20Our%20vision%20is%20that%20a%20fair%0Aincentive%20mechanism%20relies%20on%20fair%20attribution%20of%20contributions%20to%0Aparticipating%20nodes%2C%20which%20faces%20non-trivial%20challenges%20arising%20from%20the%0Alocalized%20connections%20making%20influence%20%60%60cascade%27%27%20in%20a%20decentralized%20network.%0ATo%20overcome%20this%2C%20we%20design%20the%20first%20method%20to%20estimate%20%5Ctextbf%7BD%7Data%0A%5Ctextbf%7BI%7Dnfluence%20%5Ctextbf%7BC%7Dascad%5Ctextbf%7BE%7D%20%28DICE%29%20in%20a%20decentralized%0Aenvironment.%20Theoretically%2C%20the%20framework%20derives%20tractable%20approximations%20of%0Ainfluence%20cascade%20over%20arbitrary%20neighbor%20hops%2C%20suggesting%20the%20influence%0Acascade%20is%20determined%20by%20an%20interplay%20of%20data%2C%20communication%20topology%2C%20and%20the%0Acurvature%20of%20loss%20landscape.%20DICE%20also%20lays%20the%20foundations%20for%20applications%0Aincluding%20selecting%20suitable%20collaborators%20and%20identifying%20malicious%20behaviors.%0AProject%20page%20is%20available%20at%20https%3A//raiden-zhu.github.io/blog/2025/DICE/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06931v1&entry.124074799=Read"},
{"title": "Does Data Scaling Lead to Visual Compositional Generalization?", "author": "Arnas Uselis and Andrea Dittadi and Seong Joon Oh", "abstract": "  Compositional understanding is crucial for human intelligence, yet it remains\nunclear whether contemporary vision models exhibit it. The dominant machine\nlearning paradigm is built on the premise that scaling data and model sizes\nwill improve out-of-distribution performance, including compositional\ngeneralization. We test this premise through controlled experiments that\nsystematically vary data scale, concept diversity, and combination coverage. We\nfind that compositional generalization is driven by data diversity, not mere\ndata scale. Increased combinatorial coverage forces models to discover a\nlinearly factored representational structure, where concepts decompose into\nadditive components. We prove this structure is key to efficiency, enabling\nperfect generalization from few observed combinations. Evaluating pretrained\nmodels (DINO, CLIP), we find above-random yet imperfect performance, suggesting\npartial presence of this structure. Our work motivates stronger emphasis on\nconstructing diverse datasets for compositional generalization, and considering\nthe importance of representational structure that enables efficient\ncompositional learning. Code available at\nhttps://github.com/oshapio/visual-compositional-generalization.\n", "link": "http://arxiv.org/abs/2507.07102v1", "date": "2025-07-09", "relevancy": 2.344, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Data%20Scaling%20Lead%20to%20Visual%20Compositional%20Generalization%3F&body=Title%3A%20Does%20Data%20Scaling%20Lead%20to%20Visual%20Compositional%20Generalization%3F%0AAuthor%3A%20Arnas%20Uselis%20and%20Andrea%20Dittadi%20and%20Seong%20Joon%20Oh%0AAbstract%3A%20%20%20Compositional%20understanding%20is%20crucial%20for%20human%20intelligence%2C%20yet%20it%20remains%0Aunclear%20whether%20contemporary%20vision%20models%20exhibit%20it.%20The%20dominant%20machine%0Alearning%20paradigm%20is%20built%20on%20the%20premise%20that%20scaling%20data%20and%20model%20sizes%0Awill%20improve%20out-of-distribution%20performance%2C%20including%20compositional%0Ageneralization.%20We%20test%20this%20premise%20through%20controlled%20experiments%20that%0Asystematically%20vary%20data%20scale%2C%20concept%20diversity%2C%20and%20combination%20coverage.%20We%0Afind%20that%20compositional%20generalization%20is%20driven%20by%20data%20diversity%2C%20not%20mere%0Adata%20scale.%20Increased%20combinatorial%20coverage%20forces%20models%20to%20discover%20a%0Alinearly%20factored%20representational%20structure%2C%20where%20concepts%20decompose%20into%0Aadditive%20components.%20We%20prove%20this%20structure%20is%20key%20to%20efficiency%2C%20enabling%0Aperfect%20generalization%20from%20few%20observed%20combinations.%20Evaluating%20pretrained%0Amodels%20%28DINO%2C%20CLIP%29%2C%20we%20find%20above-random%20yet%20imperfect%20performance%2C%20suggesting%0Apartial%20presence%20of%20this%20structure.%20Our%20work%20motivates%20stronger%20emphasis%20on%0Aconstructing%20diverse%20datasets%20for%20compositional%20generalization%2C%20and%20considering%0Athe%20importance%20of%20representational%20structure%20that%20enables%20efficient%0Acompositional%20learning.%20Code%20available%20at%0Ahttps%3A//github.com/oshapio/visual-compositional-generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Data%2520Scaling%2520Lead%2520to%2520Visual%2520Compositional%2520Generalization%253F%26entry.906535625%3DArnas%2520Uselis%2520and%2520Andrea%2520Dittadi%2520and%2520Seong%2520Joon%2520Oh%26entry.1292438233%3D%2520%2520Compositional%2520understanding%2520is%2520crucial%2520for%2520human%2520intelligence%252C%2520yet%2520it%2520remains%250Aunclear%2520whether%2520contemporary%2520vision%2520models%2520exhibit%2520it.%2520The%2520dominant%2520machine%250Alearning%2520paradigm%2520is%2520built%2520on%2520the%2520premise%2520that%2520scaling%2520data%2520and%2520model%2520sizes%250Awill%2520improve%2520out-of-distribution%2520performance%252C%2520including%2520compositional%250Ageneralization.%2520We%2520test%2520this%2520premise%2520through%2520controlled%2520experiments%2520that%250Asystematically%2520vary%2520data%2520scale%252C%2520concept%2520diversity%252C%2520and%2520combination%2520coverage.%2520We%250Afind%2520that%2520compositional%2520generalization%2520is%2520driven%2520by%2520data%2520diversity%252C%2520not%2520mere%250Adata%2520scale.%2520Increased%2520combinatorial%2520coverage%2520forces%2520models%2520to%2520discover%2520a%250Alinearly%2520factored%2520representational%2520structure%252C%2520where%2520concepts%2520decompose%2520into%250Aadditive%2520components.%2520We%2520prove%2520this%2520structure%2520is%2520key%2520to%2520efficiency%252C%2520enabling%250Aperfect%2520generalization%2520from%2520few%2520observed%2520combinations.%2520Evaluating%2520pretrained%250Amodels%2520%2528DINO%252C%2520CLIP%2529%252C%2520we%2520find%2520above-random%2520yet%2520imperfect%2520performance%252C%2520suggesting%250Apartial%2520presence%2520of%2520this%2520structure.%2520Our%2520work%2520motivates%2520stronger%2520emphasis%2520on%250Aconstructing%2520diverse%2520datasets%2520for%2520compositional%2520generalization%252C%2520and%2520considering%250Athe%2520importance%2520of%2520representational%2520structure%2520that%2520enables%2520efficient%250Acompositional%2520learning.%2520Code%2520available%2520at%250Ahttps%253A//github.com/oshapio/visual-compositional-generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Data%20Scaling%20Lead%20to%20Visual%20Compositional%20Generalization%3F&entry.906535625=Arnas%20Uselis%20and%20Andrea%20Dittadi%20and%20Seong%20Joon%20Oh&entry.1292438233=%20%20Compositional%20understanding%20is%20crucial%20for%20human%20intelligence%2C%20yet%20it%20remains%0Aunclear%20whether%20contemporary%20vision%20models%20exhibit%20it.%20The%20dominant%20machine%0Alearning%20paradigm%20is%20built%20on%20the%20premise%20that%20scaling%20data%20and%20model%20sizes%0Awill%20improve%20out-of-distribution%20performance%2C%20including%20compositional%0Ageneralization.%20We%20test%20this%20premise%20through%20controlled%20experiments%20that%0Asystematically%20vary%20data%20scale%2C%20concept%20diversity%2C%20and%20combination%20coverage.%20We%0Afind%20that%20compositional%20generalization%20is%20driven%20by%20data%20diversity%2C%20not%20mere%0Adata%20scale.%20Increased%20combinatorial%20coverage%20forces%20models%20to%20discover%20a%0Alinearly%20factored%20representational%20structure%2C%20where%20concepts%20decompose%20into%0Aadditive%20components.%20We%20prove%20this%20structure%20is%20key%20to%20efficiency%2C%20enabling%0Aperfect%20generalization%20from%20few%20observed%20combinations.%20Evaluating%20pretrained%0Amodels%20%28DINO%2C%20CLIP%29%2C%20we%20find%20above-random%20yet%20imperfect%20performance%2C%20suggesting%0Apartial%20presence%20of%20this%20structure.%20Our%20work%20motivates%20stronger%20emphasis%20on%0Aconstructing%20diverse%20datasets%20for%20compositional%20generalization%2C%20and%20considering%0Athe%20importance%20of%20representational%20structure%20that%20enables%20efficient%0Acompositional%20learning.%20Code%20available%20at%0Ahttps%3A//github.com/oshapio/visual-compositional-generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07102v1&entry.124074799=Read"},
{"title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models", "author": "Keyon Vafa and Peter G. Chang and Ashesh Rambachan and Sendhil Mullainathan", "abstract": "  Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.\n", "link": "http://arxiv.org/abs/2507.06952v1", "date": "2025-07-09", "relevancy": 2.3324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6001}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Has%20a%20Foundation%20Model%20Found%3F%20Using%20Inductive%20Bias%20to%20Probe%20for%0A%20%20World%20Models&body=Title%3A%20What%20Has%20a%20Foundation%20Model%20Found%3F%20Using%20Inductive%20Bias%20to%20Probe%20for%0A%20%20World%20Models%0AAuthor%3A%20Keyon%20Vafa%20and%20Peter%20G.%20Chang%20and%20Ashesh%20Rambachan%20and%20Sendhil%20Mullainathan%0AAbstract%3A%20%20%20Foundation%20models%20are%20premised%20on%20the%20idea%20that%20sequence%20prediction%20can%0Auncover%20deeper%20domain%20understanding%2C%20much%20like%20how%20Kepler%27s%20predictions%20of%0Aplanetary%20motion%20later%20led%20to%20the%20discovery%20of%20Newtonian%20mechanics.%20However%2C%0Aevaluating%20whether%20these%20models%20truly%20capture%20deeper%20structure%20remains%20a%0Achallenge.%20We%20develop%20a%20technique%20for%20evaluating%20foundation%20models%20that%0Aexamines%20how%20they%20adapt%20to%20synthetic%20datasets%20generated%20from%20some%20postulated%0Aworld%20model.%20Our%20technique%20measures%20whether%20the%20foundation%20model%27s%20inductive%0Abias%20aligns%20with%20the%20world%20model%2C%20and%20so%20we%20refer%20to%20it%20as%20an%20inductive%20bias%0Aprobe.%20Across%20multiple%20domains%2C%20we%20find%20that%20foundation%20models%20can%20excel%20at%0Atheir%20training%20tasks%20yet%20fail%20to%20develop%20inductive%20biases%20towards%20the%0Aunderlying%20world%20model%20when%20adapted%20to%20new%20tasks.%20We%20particularly%20find%20that%0Afoundation%20models%20trained%20on%20orbital%20trajectories%20consistently%20fail%20to%20apply%0ANewtonian%20mechanics%20when%20adapted%20to%20new%20physics%20tasks.%20Further%20analysis%20reveals%0Athat%20these%20models%20behave%20as%20if%20they%20develop%20task-specific%20heuristics%20that%20fail%0Ato%20generalize.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Has%2520a%2520Foundation%2520Model%2520Found%253F%2520Using%2520Inductive%2520Bias%2520to%2520Probe%2520for%250A%2520%2520World%2520Models%26entry.906535625%3DKeyon%2520Vafa%2520and%2520Peter%2520G.%2520Chang%2520and%2520Ashesh%2520Rambachan%2520and%2520Sendhil%2520Mullainathan%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520premised%2520on%2520the%2520idea%2520that%2520sequence%2520prediction%2520can%250Auncover%2520deeper%2520domain%2520understanding%252C%2520much%2520like%2520how%2520Kepler%2527s%2520predictions%2520of%250Aplanetary%2520motion%2520later%2520led%2520to%2520the%2520discovery%2520of%2520Newtonian%2520mechanics.%2520However%252C%250Aevaluating%2520whether%2520these%2520models%2520truly%2520capture%2520deeper%2520structure%2520remains%2520a%250Achallenge.%2520We%2520develop%2520a%2520technique%2520for%2520evaluating%2520foundation%2520models%2520that%250Aexamines%2520how%2520they%2520adapt%2520to%2520synthetic%2520datasets%2520generated%2520from%2520some%2520postulated%250Aworld%2520model.%2520Our%2520technique%2520measures%2520whether%2520the%2520foundation%2520model%2527s%2520inductive%250Abias%2520aligns%2520with%2520the%2520world%2520model%252C%2520and%2520so%2520we%2520refer%2520to%2520it%2520as%2520an%2520inductive%2520bias%250Aprobe.%2520Across%2520multiple%2520domains%252C%2520we%2520find%2520that%2520foundation%2520models%2520can%2520excel%2520at%250Atheir%2520training%2520tasks%2520yet%2520fail%2520to%2520develop%2520inductive%2520biases%2520towards%2520the%250Aunderlying%2520world%2520model%2520when%2520adapted%2520to%2520new%2520tasks.%2520We%2520particularly%2520find%2520that%250Afoundation%2520models%2520trained%2520on%2520orbital%2520trajectories%2520consistently%2520fail%2520to%2520apply%250ANewtonian%2520mechanics%2520when%2520adapted%2520to%2520new%2520physics%2520tasks.%2520Further%2520analysis%2520reveals%250Athat%2520these%2520models%2520behave%2520as%2520if%2520they%2520develop%2520task-specific%2520heuristics%2520that%2520fail%250Ato%2520generalize.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Has%20a%20Foundation%20Model%20Found%3F%20Using%20Inductive%20Bias%20to%20Probe%20for%0A%20%20World%20Models&entry.906535625=Keyon%20Vafa%20and%20Peter%20G.%20Chang%20and%20Ashesh%20Rambachan%20and%20Sendhil%20Mullainathan&entry.1292438233=%20%20Foundation%20models%20are%20premised%20on%20the%20idea%20that%20sequence%20prediction%20can%0Auncover%20deeper%20domain%20understanding%2C%20much%20like%20how%20Kepler%27s%20predictions%20of%0Aplanetary%20motion%20later%20led%20to%20the%20discovery%20of%20Newtonian%20mechanics.%20However%2C%0Aevaluating%20whether%20these%20models%20truly%20capture%20deeper%20structure%20remains%20a%0Achallenge.%20We%20develop%20a%20technique%20for%20evaluating%20foundation%20models%20that%0Aexamines%20how%20they%20adapt%20to%20synthetic%20datasets%20generated%20from%20some%20postulated%0Aworld%20model.%20Our%20technique%20measures%20whether%20the%20foundation%20model%27s%20inductive%0Abias%20aligns%20with%20the%20world%20model%2C%20and%20so%20we%20refer%20to%20it%20as%20an%20inductive%20bias%0Aprobe.%20Across%20multiple%20domains%2C%20we%20find%20that%20foundation%20models%20can%20excel%20at%0Atheir%20training%20tasks%20yet%20fail%20to%20develop%20inductive%20biases%20towards%20the%0Aunderlying%20world%20model%20when%20adapted%20to%20new%20tasks.%20We%20particularly%20find%20that%0Afoundation%20models%20trained%20on%20orbital%20trajectories%20consistently%20fail%20to%20apply%0ANewtonian%20mechanics%20when%20adapted%20to%20new%20physics%20tasks.%20Further%20analysis%20reveals%0Athat%20these%20models%20behave%20as%20if%20they%20develop%20task-specific%20heuristics%20that%20fail%0Ato%20generalize.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06952v1&entry.124074799=Read"},
{"title": "Scaling 4D Representations", "author": "Jo\u00e3o Carreira and Dilara Gokay and Michael King and Chuhan Zhang and Ignacio Rocco and Aravindh Mahendran and Thomas Albert Keck and Joseph Heyward and Skanda Koppula and Etienne Pot and Goker Erdogan and Yana Hasson and Yi Yang and Klaus Greff and Guillaume Le Moing and Sjoerd van Steenkiste and Daniel Zoran and Drew A. Hudson and Pedro V\u00e9lez and Luisa Polan\u00eda and Luke Friedman and Chris Duvarney and Ross Goroshin and Kelsey Allen and Jacob Walker and Rishabh Kabra and Eric Aboussouan and Jennifer Sun and Thomas Kipf and Carl Doersch and Viorica P\u0103tr\u0103ucean and Dima Damen and Pauline Luc and Mehdi S. M. Sajjadi and Andrew Zisserman", "abstract": "  Scaling has not yet been convincingly demonstrated for pure self-supervised\nlearning from video. However, prior work has focused evaluations on\nsemantic-related tasks $\\unicode{x2013}$ action classification, ImageNet\nclassification, etc. In this paper we focus on evaluating self-supervised\nlearning on non-semantic vision tasks that are more spatial (3D) and temporal\n(+1D = 4D), such as camera pose estimation, point and object tracking, and\ndepth estimation. We show that by learning from very large video datasets,\nmasked auto-encoding (MAE) with transformer video models actually scales,\nconsistently improving performance on these 4D tasks, as model size increases\nfrom 20M all the way to the largest by far reported self-supervised video model\n$\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with\nmany recent image and video models demonstrates the benefits of scaling 4D\nrepresentations. Pretrained models are available at\nhttps://github.com/google-deepmind/representations4d .\n", "link": "http://arxiv.org/abs/2412.15212v2", "date": "2025-07-09", "relevancy": 2.3275, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5994}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%204D%20Representations&body=Title%3A%20Scaling%204D%20Representations%0AAuthor%3A%20Jo%C3%A3o%20Carreira%20and%20Dilara%20Gokay%20and%20Michael%20King%20and%20Chuhan%20Zhang%20and%20Ignacio%20Rocco%20and%20Aravindh%20Mahendran%20and%20Thomas%20Albert%20Keck%20and%20Joseph%20Heyward%20and%20Skanda%20Koppula%20and%20Etienne%20Pot%20and%20Goker%20Erdogan%20and%20Yana%20Hasson%20and%20Yi%20Yang%20and%20Klaus%20Greff%20and%20Guillaume%20Le%20Moing%20and%20Sjoerd%20van%20Steenkiste%20and%20Daniel%20Zoran%20and%20Drew%20A.%20Hudson%20and%20Pedro%20V%C3%A9lez%20and%20Luisa%20Polan%C3%ADa%20and%20Luke%20Friedman%20and%20Chris%20Duvarney%20and%20Ross%20Goroshin%20and%20Kelsey%20Allen%20and%20Jacob%20Walker%20and%20Rishabh%20Kabra%20and%20Eric%20Aboussouan%20and%20Jennifer%20Sun%20and%20Thomas%20Kipf%20and%20Carl%20Doersch%20and%20Viorica%20P%C4%83tr%C4%83ucean%20and%20Dima%20Damen%20and%20Pauline%20Luc%20and%20Mehdi%20S.%20M.%20Sajjadi%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Scaling%20has%20not%20yet%20been%20convincingly%20demonstrated%20for%20pure%20self-supervised%0Alearning%20from%20video.%20However%2C%20prior%20work%20has%20focused%20evaluations%20on%0Asemantic-related%20tasks%20%24%5Cunicode%7Bx2013%7D%24%20action%20classification%2C%20ImageNet%0Aclassification%2C%20etc.%20In%20this%20paper%20we%20focus%20on%20evaluating%20self-supervised%0Alearning%20on%20non-semantic%20vision%20tasks%20that%20are%20more%20spatial%20%283D%29%20and%20temporal%0A%28%2B1D%20%3D%204D%29%2C%20such%20as%20camera%20pose%20estimation%2C%20point%20and%20object%20tracking%2C%20and%0Adepth%20estimation.%20We%20show%20that%20by%20learning%20from%20very%20large%20video%20datasets%2C%0Amasked%20auto-encoding%20%28MAE%29%20with%20transformer%20video%20models%20actually%20scales%2C%0Aconsistently%20improving%20performance%20on%20these%204D%20tasks%2C%20as%20model%20size%20increases%0Afrom%2020M%20all%20the%20way%20to%20the%20largest%20by%20far%20reported%20self-supervised%20video%20model%0A%24%5Cunicode%7Bx2013%7D%24%2022B%20parameters.%20Rigorous%20apples-to-apples%20comparison%20with%0Amany%20recent%20image%20and%20video%20models%20demonstrates%20the%20benefits%20of%20scaling%204D%0Arepresentations.%20Pretrained%20models%20are%20available%20at%0Ahttps%3A//github.com/google-deepmind/representations4d%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15212v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%25204D%2520Representations%26entry.906535625%3DJo%25C3%25A3o%2520Carreira%2520and%2520Dilara%2520Gokay%2520and%2520Michael%2520King%2520and%2520Chuhan%2520Zhang%2520and%2520Ignacio%2520Rocco%2520and%2520Aravindh%2520Mahendran%2520and%2520Thomas%2520Albert%2520Keck%2520and%2520Joseph%2520Heyward%2520and%2520Skanda%2520Koppula%2520and%2520Etienne%2520Pot%2520and%2520Goker%2520Erdogan%2520and%2520Yana%2520Hasson%2520and%2520Yi%2520Yang%2520and%2520Klaus%2520Greff%2520and%2520Guillaume%2520Le%2520Moing%2520and%2520Sjoerd%2520van%2520Steenkiste%2520and%2520Daniel%2520Zoran%2520and%2520Drew%2520A.%2520Hudson%2520and%2520Pedro%2520V%25C3%25A9lez%2520and%2520Luisa%2520Polan%25C3%25ADa%2520and%2520Luke%2520Friedman%2520and%2520Chris%2520Duvarney%2520and%2520Ross%2520Goroshin%2520and%2520Kelsey%2520Allen%2520and%2520Jacob%2520Walker%2520and%2520Rishabh%2520Kabra%2520and%2520Eric%2520Aboussouan%2520and%2520Jennifer%2520Sun%2520and%2520Thomas%2520Kipf%2520and%2520Carl%2520Doersch%2520and%2520Viorica%2520P%25C4%2583tr%25C4%2583ucean%2520and%2520Dima%2520Damen%2520and%2520Pauline%2520Luc%2520and%2520Mehdi%2520S.%2520M.%2520Sajjadi%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520Scaling%2520has%2520not%2520yet%2520been%2520convincingly%2520demonstrated%2520for%2520pure%2520self-supervised%250Alearning%2520from%2520video.%2520However%252C%2520prior%2520work%2520has%2520focused%2520evaluations%2520on%250Asemantic-related%2520tasks%2520%2524%255Cunicode%257Bx2013%257D%2524%2520action%2520classification%252C%2520ImageNet%250Aclassification%252C%2520etc.%2520In%2520this%2520paper%2520we%2520focus%2520on%2520evaluating%2520self-supervised%250Alearning%2520on%2520non-semantic%2520vision%2520tasks%2520that%2520are%2520more%2520spatial%2520%25283D%2529%2520and%2520temporal%250A%2528%252B1D%2520%253D%25204D%2529%252C%2520such%2520as%2520camera%2520pose%2520estimation%252C%2520point%2520and%2520object%2520tracking%252C%2520and%250Adepth%2520estimation.%2520We%2520show%2520that%2520by%2520learning%2520from%2520very%2520large%2520video%2520datasets%252C%250Amasked%2520auto-encoding%2520%2528MAE%2529%2520with%2520transformer%2520video%2520models%2520actually%2520scales%252C%250Aconsistently%2520improving%2520performance%2520on%2520these%25204D%2520tasks%252C%2520as%2520model%2520size%2520increases%250Afrom%252020M%2520all%2520the%2520way%2520to%2520the%2520largest%2520by%2520far%2520reported%2520self-supervised%2520video%2520model%250A%2524%255Cunicode%257Bx2013%257D%2524%252022B%2520parameters.%2520Rigorous%2520apples-to-apples%2520comparison%2520with%250Amany%2520recent%2520image%2520and%2520video%2520models%2520demonstrates%2520the%2520benefits%2520of%2520scaling%25204D%250Arepresentations.%2520Pretrained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/google-deepmind/representations4d%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15212v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%204D%20Representations&entry.906535625=Jo%C3%A3o%20Carreira%20and%20Dilara%20Gokay%20and%20Michael%20King%20and%20Chuhan%20Zhang%20and%20Ignacio%20Rocco%20and%20Aravindh%20Mahendran%20and%20Thomas%20Albert%20Keck%20and%20Joseph%20Heyward%20and%20Skanda%20Koppula%20and%20Etienne%20Pot%20and%20Goker%20Erdogan%20and%20Yana%20Hasson%20and%20Yi%20Yang%20and%20Klaus%20Greff%20and%20Guillaume%20Le%20Moing%20and%20Sjoerd%20van%20Steenkiste%20and%20Daniel%20Zoran%20and%20Drew%20A.%20Hudson%20and%20Pedro%20V%C3%A9lez%20and%20Luisa%20Polan%C3%ADa%20and%20Luke%20Friedman%20and%20Chris%20Duvarney%20and%20Ross%20Goroshin%20and%20Kelsey%20Allen%20and%20Jacob%20Walker%20and%20Rishabh%20Kabra%20and%20Eric%20Aboussouan%20and%20Jennifer%20Sun%20and%20Thomas%20Kipf%20and%20Carl%20Doersch%20and%20Viorica%20P%C4%83tr%C4%83ucean%20and%20Dima%20Damen%20and%20Pauline%20Luc%20and%20Mehdi%20S.%20M.%20Sajjadi%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Scaling%20has%20not%20yet%20been%20convincingly%20demonstrated%20for%20pure%20self-supervised%0Alearning%20from%20video.%20However%2C%20prior%20work%20has%20focused%20evaluations%20on%0Asemantic-related%20tasks%20%24%5Cunicode%7Bx2013%7D%24%20action%20classification%2C%20ImageNet%0Aclassification%2C%20etc.%20In%20this%20paper%20we%20focus%20on%20evaluating%20self-supervised%0Alearning%20on%20non-semantic%20vision%20tasks%20that%20are%20more%20spatial%20%283D%29%20and%20temporal%0A%28%2B1D%20%3D%204D%29%2C%20such%20as%20camera%20pose%20estimation%2C%20point%20and%20object%20tracking%2C%20and%0Adepth%20estimation.%20We%20show%20that%20by%20learning%20from%20very%20large%20video%20datasets%2C%0Amasked%20auto-encoding%20%28MAE%29%20with%20transformer%20video%20models%20actually%20scales%2C%0Aconsistently%20improving%20performance%20on%20these%204D%20tasks%2C%20as%20model%20size%20increases%0Afrom%2020M%20all%20the%20way%20to%20the%20largest%20by%20far%20reported%20self-supervised%20video%20model%0A%24%5Cunicode%7Bx2013%7D%24%2022B%20parameters.%20Rigorous%20apples-to-apples%20comparison%20with%0Amany%20recent%20image%20and%20video%20models%20demonstrates%20the%20benefits%20of%20scaling%204D%0Arepresentations.%20Pretrained%20models%20are%20available%20at%0Ahttps%3A//github.com/google-deepmind/representations4d%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15212v2&entry.124074799=Read"},
{"title": "TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for\n  Image-to-Video Generation", "author": "Wenhao Wang and Yi Yang", "abstract": "  Video generation models are revolutionizing content creation, with\nimage-to-video models drawing increasing attention due to their enhanced\ncontrollability, visual consistency, and practical applications. However,\ndespite their popularity, these models rely on user-provided text and image\nprompts, and there is currently no dedicated dataset for studying these\nprompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of\nover 1.70 million unique user-provided Text and Image Prompts specifically for\nImage-to-Video generation. Additionally, we provide the corresponding generated\nvideos from five state-of-the-art image-to-video models. We begin by outlining\nthe time-consuming and costly process of curating this large-scale dataset.\nNext, we compare TIP-I2V to two popular prompt datasets, VidProM\n(text-to-video) and DiffusionDB (text-to-image), highlighting differences in\nboth basic and semantic information. This dataset enables advancements in\nimage-to-video research. For instance, to develop better models, researchers\ncan use the prompts in TIP-I2V to analyze user preferences and evaluate the\nmulti-dimensional performance of their trained models; and to enhance model\nsafety, they may focus on addressing the misinformation issue caused by\nimage-to-video models. The new research inspired by TIP-I2V and the differences\nwith existing datasets emphasize the importance of a specialized image-to-video\nprompt dataset. The project is available at https://tip-i2v.github.io.\n", "link": "http://arxiv.org/abs/2411.04709v2", "date": "2025-07-09", "relevancy": 2.3196, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6066}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5823}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIP-I2V%3A%20A%20Million-Scale%20Real%20Text%20and%20Image%20Prompt%20Dataset%20for%0A%20%20Image-to-Video%20Generation&body=Title%3A%20TIP-I2V%3A%20A%20Million-Scale%20Real%20Text%20and%20Image%20Prompt%20Dataset%20for%0A%20%20Image-to-Video%20Generation%0AAuthor%3A%20Wenhao%20Wang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Video%20generation%20models%20are%20revolutionizing%20content%20creation%2C%20with%0Aimage-to-video%20models%20drawing%20increasing%20attention%20due%20to%20their%20enhanced%0Acontrollability%2C%20visual%20consistency%2C%20and%20practical%20applications.%20However%2C%0Adespite%20their%20popularity%2C%20these%20models%20rely%20on%20user-provided%20text%20and%20image%0Aprompts%2C%20and%20there%20is%20currently%20no%20dedicated%20dataset%20for%20studying%20these%0Aprompts.%20In%20this%20paper%2C%20we%20introduce%20TIP-I2V%2C%20the%20first%20large-scale%20dataset%20of%0Aover%201.70%20million%20unique%20user-provided%20Text%20and%20Image%20Prompts%20specifically%20for%0AImage-to-Video%20generation.%20Additionally%2C%20we%20provide%20the%20corresponding%20generated%0Avideos%20from%20five%20state-of-the-art%20image-to-video%20models.%20We%20begin%20by%20outlining%0Athe%20time-consuming%20and%20costly%20process%20of%20curating%20this%20large-scale%20dataset.%0ANext%2C%20we%20compare%20TIP-I2V%20to%20two%20popular%20prompt%20datasets%2C%20VidProM%0A%28text-to-video%29%20and%20DiffusionDB%20%28text-to-image%29%2C%20highlighting%20differences%20in%0Aboth%20basic%20and%20semantic%20information.%20This%20dataset%20enables%20advancements%20in%0Aimage-to-video%20research.%20For%20instance%2C%20to%20develop%20better%20models%2C%20researchers%0Acan%20use%20the%20prompts%20in%20TIP-I2V%20to%20analyze%20user%20preferences%20and%20evaluate%20the%0Amulti-dimensional%20performance%20of%20their%20trained%20models%3B%20and%20to%20enhance%20model%0Asafety%2C%20they%20may%20focus%20on%20addressing%20the%20misinformation%20issue%20caused%20by%0Aimage-to-video%20models.%20The%20new%20research%20inspired%20by%20TIP-I2V%20and%20the%20differences%0Awith%20existing%20datasets%20emphasize%20the%20importance%20of%20a%20specialized%20image-to-video%0Aprompt%20dataset.%20The%20project%20is%20available%20at%20https%3A//tip-i2v.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04709v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIP-I2V%253A%2520A%2520Million-Scale%2520Real%2520Text%2520and%2520Image%2520Prompt%2520Dataset%2520for%250A%2520%2520Image-to-Video%2520Generation%26entry.906535625%3DWenhao%2520Wang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Video%2520generation%2520models%2520are%2520revolutionizing%2520content%2520creation%252C%2520with%250Aimage-to-video%2520models%2520drawing%2520increasing%2520attention%2520due%2520to%2520their%2520enhanced%250Acontrollability%252C%2520visual%2520consistency%252C%2520and%2520practical%2520applications.%2520However%252C%250Adespite%2520their%2520popularity%252C%2520these%2520models%2520rely%2520on%2520user-provided%2520text%2520and%2520image%250Aprompts%252C%2520and%2520there%2520is%2520currently%2520no%2520dedicated%2520dataset%2520for%2520studying%2520these%250Aprompts.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520TIP-I2V%252C%2520the%2520first%2520large-scale%2520dataset%2520of%250Aover%25201.70%2520million%2520unique%2520user-provided%2520Text%2520and%2520Image%2520Prompts%2520specifically%2520for%250AImage-to-Video%2520generation.%2520Additionally%252C%2520we%2520provide%2520the%2520corresponding%2520generated%250Avideos%2520from%2520five%2520state-of-the-art%2520image-to-video%2520models.%2520We%2520begin%2520by%2520outlining%250Athe%2520time-consuming%2520and%2520costly%2520process%2520of%2520curating%2520this%2520large-scale%2520dataset.%250ANext%252C%2520we%2520compare%2520TIP-I2V%2520to%2520two%2520popular%2520prompt%2520datasets%252C%2520VidProM%250A%2528text-to-video%2529%2520and%2520DiffusionDB%2520%2528text-to-image%2529%252C%2520highlighting%2520differences%2520in%250Aboth%2520basic%2520and%2520semantic%2520information.%2520This%2520dataset%2520enables%2520advancements%2520in%250Aimage-to-video%2520research.%2520For%2520instance%252C%2520to%2520develop%2520better%2520models%252C%2520researchers%250Acan%2520use%2520the%2520prompts%2520in%2520TIP-I2V%2520to%2520analyze%2520user%2520preferences%2520and%2520evaluate%2520the%250Amulti-dimensional%2520performance%2520of%2520their%2520trained%2520models%253B%2520and%2520to%2520enhance%2520model%250Asafety%252C%2520they%2520may%2520focus%2520on%2520addressing%2520the%2520misinformation%2520issue%2520caused%2520by%250Aimage-to-video%2520models.%2520The%2520new%2520research%2520inspired%2520by%2520TIP-I2V%2520and%2520the%2520differences%250Awith%2520existing%2520datasets%2520emphasize%2520the%2520importance%2520of%2520a%2520specialized%2520image-to-video%250Aprompt%2520dataset.%2520The%2520project%2520is%2520available%2520at%2520https%253A//tip-i2v.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04709v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIP-I2V%3A%20A%20Million-Scale%20Real%20Text%20and%20Image%20Prompt%20Dataset%20for%0A%20%20Image-to-Video%20Generation&entry.906535625=Wenhao%20Wang%20and%20Yi%20Yang&entry.1292438233=%20%20Video%20generation%20models%20are%20revolutionizing%20content%20creation%2C%20with%0Aimage-to-video%20models%20drawing%20increasing%20attention%20due%20to%20their%20enhanced%0Acontrollability%2C%20visual%20consistency%2C%20and%20practical%20applications.%20However%2C%0Adespite%20their%20popularity%2C%20these%20models%20rely%20on%20user-provided%20text%20and%20image%0Aprompts%2C%20and%20there%20is%20currently%20no%20dedicated%20dataset%20for%20studying%20these%0Aprompts.%20In%20this%20paper%2C%20we%20introduce%20TIP-I2V%2C%20the%20first%20large-scale%20dataset%20of%0Aover%201.70%20million%20unique%20user-provided%20Text%20and%20Image%20Prompts%20specifically%20for%0AImage-to-Video%20generation.%20Additionally%2C%20we%20provide%20the%20corresponding%20generated%0Avideos%20from%20five%20state-of-the-art%20image-to-video%20models.%20We%20begin%20by%20outlining%0Athe%20time-consuming%20and%20costly%20process%20of%20curating%20this%20large-scale%20dataset.%0ANext%2C%20we%20compare%20TIP-I2V%20to%20two%20popular%20prompt%20datasets%2C%20VidProM%0A%28text-to-video%29%20and%20DiffusionDB%20%28text-to-image%29%2C%20highlighting%20differences%20in%0Aboth%20basic%20and%20semantic%20information.%20This%20dataset%20enables%20advancements%20in%0Aimage-to-video%20research.%20For%20instance%2C%20to%20develop%20better%20models%2C%20researchers%0Acan%20use%20the%20prompts%20in%20TIP-I2V%20to%20analyze%20user%20preferences%20and%20evaluate%20the%0Amulti-dimensional%20performance%20of%20their%20trained%20models%3B%20and%20to%20enhance%20model%0Asafety%2C%20they%20may%20focus%20on%20addressing%20the%20misinformation%20issue%20caused%20by%0Aimage-to-video%20models.%20The%20new%20research%20inspired%20by%20TIP-I2V%20and%20the%20differences%0Awith%20existing%20datasets%20emphasize%20the%20importance%20of%20a%20specialized%20image-to-video%0Aprompt%20dataset.%20The%20project%20is%20available%20at%20https%3A//tip-i2v.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04709v2&entry.124074799=Read"},
{"title": "SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds", "author": "Matthias Zeller and Daniel Casado Herraez and Bengisu Ayan and Jens Behley and Michael Heidingsfeld and Cyrill Stachniss", "abstract": "  Semantic scene understanding, including the perception and classification of\nmoving agents, is essential to enabling safe and robust driving behaviours of\nautonomous vehicles. Cameras and LiDARs are commonly used for semantic scene\nunderstanding. However, both sensor modalities face limitations in adverse\nweather and usually do not provide motion information. Radar sensors overcome\nthese limitations and directly offer information about moving agents by\nmeasuring the Doppler velocity, but the measurements are comparably sparse and\nnoisy. In this paper, we address the problem of panoptic segmentation in sparse\nradar point clouds to enhance scene understanding. Our approach, called\nSemRaFiner, accounts for changing density in sparse radar point clouds and\noptimizes the feature extraction to improve accuracy. Furthermore, we propose\nan optimized training procedure to refine instance assignments by incorporating\na dedicated data augmentation. Our experiments suggest that our approach\noutperforms state-of-the-art methods for radar-based panoptic segmentation.\n", "link": "http://arxiv.org/abs/2507.06906v1", "date": "2025-07-09", "relevancy": 2.3131, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5973}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5699}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemRaFiner%3A%20Panoptic%20Segmentation%20in%20Sparse%20and%20Noisy%20Radar%20Point%20Clouds&body=Title%3A%20SemRaFiner%3A%20Panoptic%20Segmentation%20in%20Sparse%20and%20Noisy%20Radar%20Point%20Clouds%0AAuthor%3A%20Matthias%20Zeller%20and%20Daniel%20Casado%20Herraez%20and%20Bengisu%20Ayan%20and%20Jens%20Behley%20and%20Michael%20Heidingsfeld%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20Semantic%20scene%20understanding%2C%20including%20the%20perception%20and%20classification%20of%0Amoving%20agents%2C%20is%20essential%20to%20enabling%20safe%20and%20robust%20driving%20behaviours%20of%0Aautonomous%20vehicles.%20Cameras%20and%20LiDARs%20are%20commonly%20used%20for%20semantic%20scene%0Aunderstanding.%20However%2C%20both%20sensor%20modalities%20face%20limitations%20in%20adverse%0Aweather%20and%20usually%20do%20not%20provide%20motion%20information.%20Radar%20sensors%20overcome%0Athese%20limitations%20and%20directly%20offer%20information%20about%20moving%20agents%20by%0Ameasuring%20the%20Doppler%20velocity%2C%20but%20the%20measurements%20are%20comparably%20sparse%20and%0Anoisy.%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20panoptic%20segmentation%20in%20sparse%0Aradar%20point%20clouds%20to%20enhance%20scene%20understanding.%20Our%20approach%2C%20called%0ASemRaFiner%2C%20accounts%20for%20changing%20density%20in%20sparse%20radar%20point%20clouds%20and%0Aoptimizes%20the%20feature%20extraction%20to%20improve%20accuracy.%20Furthermore%2C%20we%20propose%0Aan%20optimized%20training%20procedure%20to%20refine%20instance%20assignments%20by%20incorporating%0Aa%20dedicated%20data%20augmentation.%20Our%20experiments%20suggest%20that%20our%20approach%0Aoutperforms%20state-of-the-art%20methods%20for%20radar-based%20panoptic%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemRaFiner%253A%2520Panoptic%2520Segmentation%2520in%2520Sparse%2520and%2520Noisy%2520Radar%2520Point%2520Clouds%26entry.906535625%3DMatthias%2520Zeller%2520and%2520Daniel%2520Casado%2520Herraez%2520and%2520Bengisu%2520Ayan%2520and%2520Jens%2520Behley%2520and%2520Michael%2520Heidingsfeld%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3D%2520%2520Semantic%2520scene%2520understanding%252C%2520including%2520the%2520perception%2520and%2520classification%2520of%250Amoving%2520agents%252C%2520is%2520essential%2520to%2520enabling%2520safe%2520and%2520robust%2520driving%2520behaviours%2520of%250Aautonomous%2520vehicles.%2520Cameras%2520and%2520LiDARs%2520are%2520commonly%2520used%2520for%2520semantic%2520scene%250Aunderstanding.%2520However%252C%2520both%2520sensor%2520modalities%2520face%2520limitations%2520in%2520adverse%250Aweather%2520and%2520usually%2520do%2520not%2520provide%2520motion%2520information.%2520Radar%2520sensors%2520overcome%250Athese%2520limitations%2520and%2520directly%2520offer%2520information%2520about%2520moving%2520agents%2520by%250Ameasuring%2520the%2520Doppler%2520velocity%252C%2520but%2520the%2520measurements%2520are%2520comparably%2520sparse%2520and%250Anoisy.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520problem%2520of%2520panoptic%2520segmentation%2520in%2520sparse%250Aradar%2520point%2520clouds%2520to%2520enhance%2520scene%2520understanding.%2520Our%2520approach%252C%2520called%250ASemRaFiner%252C%2520accounts%2520for%2520changing%2520density%2520in%2520sparse%2520radar%2520point%2520clouds%2520and%250Aoptimizes%2520the%2520feature%2520extraction%2520to%2520improve%2520accuracy.%2520Furthermore%252C%2520we%2520propose%250Aan%2520optimized%2520training%2520procedure%2520to%2520refine%2520instance%2520assignments%2520by%2520incorporating%250Aa%2520dedicated%2520data%2520augmentation.%2520Our%2520experiments%2520suggest%2520that%2520our%2520approach%250Aoutperforms%2520state-of-the-art%2520methods%2520for%2520radar-based%2520panoptic%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemRaFiner%3A%20Panoptic%20Segmentation%20in%20Sparse%20and%20Noisy%20Radar%20Point%20Clouds&entry.906535625=Matthias%20Zeller%20and%20Daniel%20Casado%20Herraez%20and%20Bengisu%20Ayan%20and%20Jens%20Behley%20and%20Michael%20Heidingsfeld%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20Semantic%20scene%20understanding%2C%20including%20the%20perception%20and%20classification%20of%0Amoving%20agents%2C%20is%20essential%20to%20enabling%20safe%20and%20robust%20driving%20behaviours%20of%0Aautonomous%20vehicles.%20Cameras%20and%20LiDARs%20are%20commonly%20used%20for%20semantic%20scene%0Aunderstanding.%20However%2C%20both%20sensor%20modalities%20face%20limitations%20in%20adverse%0Aweather%20and%20usually%20do%20not%20provide%20motion%20information.%20Radar%20sensors%20overcome%0Athese%20limitations%20and%20directly%20offer%20information%20about%20moving%20agents%20by%0Ameasuring%20the%20Doppler%20velocity%2C%20but%20the%20measurements%20are%20comparably%20sparse%20and%0Anoisy.%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20panoptic%20segmentation%20in%20sparse%0Aradar%20point%20clouds%20to%20enhance%20scene%20understanding.%20Our%20approach%2C%20called%0ASemRaFiner%2C%20accounts%20for%20changing%20density%20in%20sparse%20radar%20point%20clouds%20and%0Aoptimizes%20the%20feature%20extraction%20to%20improve%20accuracy.%20Furthermore%2C%20we%20propose%0Aan%20optimized%20training%20procedure%20to%20refine%20instance%20assignments%20by%20incorporating%0Aa%20dedicated%20data%20augmentation.%20Our%20experiments%20suggest%20that%20our%20approach%0Aoutperforms%20state-of-the-art%20methods%20for%20radar-based%20panoptic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06906v1&entry.124074799=Read"},
{"title": "ULC: A Unified and Fine-Grained Controller for Humanoid\n  Loco-Manipulation", "author": "Wandong Sun and Luying Feng and Baoshi Cao and Yang Liu and Yaochu Jin and Zongwu Xie", "abstract": "  Loco-Manipulation for humanoid robots aims to enable robots to integrate\nmobility with upper-body tracking capabilities. Most existing approaches adopt\nhierarchical architectures that decompose control into isolated upper-body\n(manipulation) and lower-body (locomotion) policies. While this decomposition\nreduces training complexity, it inherently limits coordination between\nsubsystems and contradicts the unified whole-body control exhibited by humans.\nWe demonstrate that a single unified policy can achieve a combination of\ntracking accuracy, large workspace, and robustness for humanoid\nloco-manipulation. We propose the Unified Loco-Manipulation Controller (ULC), a\nsingle-policy framework that simultaneously tracks root velocity, root height,\ntorso rotation, and dual-arm joint positions in an end-to-end manner, proving\nthe feasibility of unified control without sacrificing performance. We achieve\nthis unified control through key technologies: sequence skill acquisition for\nprogressive learning complexity, residual action modeling for fine-grained\ncontrol adjustments, command polynomial interpolation for smooth motion\ntransitions, random delay release for robustness to deploy variations, load\nrandomization for generalization to external disturbances, and\ncenter-of-gravity tracking for providing explicit policy gradients to maintain\nstability. We validate our method on the Unitree G1 humanoid robot with 3-DOF\n(degrees-of-freedom) waist. Compared with strong baselines, ULC shows better\ntracking performance to disentangled methods and demonstrating larger workspace\ncoverage. The unified dual-arm tracking enables precise manipulation under\nexternal loads while maintaining coordinated whole-body control for complex\nloco-manipulation tasks.\n", "link": "http://arxiv.org/abs/2507.06905v1", "date": "2025-07-09", "relevancy": 2.2973, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5879}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.567}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ULC%3A%20A%20Unified%20and%20Fine-Grained%20Controller%20for%20Humanoid%0A%20%20Loco-Manipulation&body=Title%3A%20ULC%3A%20A%20Unified%20and%20Fine-Grained%20Controller%20for%20Humanoid%0A%20%20Loco-Manipulation%0AAuthor%3A%20Wandong%20Sun%20and%20Luying%20Feng%20and%20Baoshi%20Cao%20and%20Yang%20Liu%20and%20Yaochu%20Jin%20and%20Zongwu%20Xie%0AAbstract%3A%20%20%20Loco-Manipulation%20for%20humanoid%20robots%20aims%20to%20enable%20robots%20to%20integrate%0Amobility%20with%20upper-body%20tracking%20capabilities.%20Most%20existing%20approaches%20adopt%0Ahierarchical%20architectures%20that%20decompose%20control%20into%20isolated%20upper-body%0A%28manipulation%29%20and%20lower-body%20%28locomotion%29%20policies.%20While%20this%20decomposition%0Areduces%20training%20complexity%2C%20it%20inherently%20limits%20coordination%20between%0Asubsystems%20and%20contradicts%20the%20unified%20whole-body%20control%20exhibited%20by%20humans.%0AWe%20demonstrate%20that%20a%20single%20unified%20policy%20can%20achieve%20a%20combination%20of%0Atracking%20accuracy%2C%20large%20workspace%2C%20and%20robustness%20for%20humanoid%0Aloco-manipulation.%20We%20propose%20the%20Unified%20Loco-Manipulation%20Controller%20%28ULC%29%2C%20a%0Asingle-policy%20framework%20that%20simultaneously%20tracks%20root%20velocity%2C%20root%20height%2C%0Atorso%20rotation%2C%20and%20dual-arm%20joint%20positions%20in%20an%20end-to-end%20manner%2C%20proving%0Athe%20feasibility%20of%20unified%20control%20without%20sacrificing%20performance.%20We%20achieve%0Athis%20unified%20control%20through%20key%20technologies%3A%20sequence%20skill%20acquisition%20for%0Aprogressive%20learning%20complexity%2C%20residual%20action%20modeling%20for%20fine-grained%0Acontrol%20adjustments%2C%20command%20polynomial%20interpolation%20for%20smooth%20motion%0Atransitions%2C%20random%20delay%20release%20for%20robustness%20to%20deploy%20variations%2C%20load%0Arandomization%20for%20generalization%20to%20external%20disturbances%2C%20and%0Acenter-of-gravity%20tracking%20for%20providing%20explicit%20policy%20gradients%20to%20maintain%0Astability.%20We%20validate%20our%20method%20on%20the%20Unitree%20G1%20humanoid%20robot%20with%203-DOF%0A%28degrees-of-freedom%29%20waist.%20Compared%20with%20strong%20baselines%2C%20ULC%20shows%20better%0Atracking%20performance%20to%20disentangled%20methods%20and%20demonstrating%20larger%20workspace%0Acoverage.%20The%20unified%20dual-arm%20tracking%20enables%20precise%20manipulation%20under%0Aexternal%20loads%20while%20maintaining%20coordinated%20whole-body%20control%20for%20complex%0Aloco-manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DULC%253A%2520A%2520Unified%2520and%2520Fine-Grained%2520Controller%2520for%2520Humanoid%250A%2520%2520Loco-Manipulation%26entry.906535625%3DWandong%2520Sun%2520and%2520Luying%2520Feng%2520and%2520Baoshi%2520Cao%2520and%2520Yang%2520Liu%2520and%2520Yaochu%2520Jin%2520and%2520Zongwu%2520Xie%26entry.1292438233%3D%2520%2520Loco-Manipulation%2520for%2520humanoid%2520robots%2520aims%2520to%2520enable%2520robots%2520to%2520integrate%250Amobility%2520with%2520upper-body%2520tracking%2520capabilities.%2520Most%2520existing%2520approaches%2520adopt%250Ahierarchical%2520architectures%2520that%2520decompose%2520control%2520into%2520isolated%2520upper-body%250A%2528manipulation%2529%2520and%2520lower-body%2520%2528locomotion%2529%2520policies.%2520While%2520this%2520decomposition%250Areduces%2520training%2520complexity%252C%2520it%2520inherently%2520limits%2520coordination%2520between%250Asubsystems%2520and%2520contradicts%2520the%2520unified%2520whole-body%2520control%2520exhibited%2520by%2520humans.%250AWe%2520demonstrate%2520that%2520a%2520single%2520unified%2520policy%2520can%2520achieve%2520a%2520combination%2520of%250Atracking%2520accuracy%252C%2520large%2520workspace%252C%2520and%2520robustness%2520for%2520humanoid%250Aloco-manipulation.%2520We%2520propose%2520the%2520Unified%2520Loco-Manipulation%2520Controller%2520%2528ULC%2529%252C%2520a%250Asingle-policy%2520framework%2520that%2520simultaneously%2520tracks%2520root%2520velocity%252C%2520root%2520height%252C%250Atorso%2520rotation%252C%2520and%2520dual-arm%2520joint%2520positions%2520in%2520an%2520end-to-end%2520manner%252C%2520proving%250Athe%2520feasibility%2520of%2520unified%2520control%2520without%2520sacrificing%2520performance.%2520We%2520achieve%250Athis%2520unified%2520control%2520through%2520key%2520technologies%253A%2520sequence%2520skill%2520acquisition%2520for%250Aprogressive%2520learning%2520complexity%252C%2520residual%2520action%2520modeling%2520for%2520fine-grained%250Acontrol%2520adjustments%252C%2520command%2520polynomial%2520interpolation%2520for%2520smooth%2520motion%250Atransitions%252C%2520random%2520delay%2520release%2520for%2520robustness%2520to%2520deploy%2520variations%252C%2520load%250Arandomization%2520for%2520generalization%2520to%2520external%2520disturbances%252C%2520and%250Acenter-of-gravity%2520tracking%2520for%2520providing%2520explicit%2520policy%2520gradients%2520to%2520maintain%250Astability.%2520We%2520validate%2520our%2520method%2520on%2520the%2520Unitree%2520G1%2520humanoid%2520robot%2520with%25203-DOF%250A%2528degrees-of-freedom%2529%2520waist.%2520Compared%2520with%2520strong%2520baselines%252C%2520ULC%2520shows%2520better%250Atracking%2520performance%2520to%2520disentangled%2520methods%2520and%2520demonstrating%2520larger%2520workspace%250Acoverage.%2520The%2520unified%2520dual-arm%2520tracking%2520enables%2520precise%2520manipulation%2520under%250Aexternal%2520loads%2520while%2520maintaining%2520coordinated%2520whole-body%2520control%2520for%2520complex%250Aloco-manipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ULC%3A%20A%20Unified%20and%20Fine-Grained%20Controller%20for%20Humanoid%0A%20%20Loco-Manipulation&entry.906535625=Wandong%20Sun%20and%20Luying%20Feng%20and%20Baoshi%20Cao%20and%20Yang%20Liu%20and%20Yaochu%20Jin%20and%20Zongwu%20Xie&entry.1292438233=%20%20Loco-Manipulation%20for%20humanoid%20robots%20aims%20to%20enable%20robots%20to%20integrate%0Amobility%20with%20upper-body%20tracking%20capabilities.%20Most%20existing%20approaches%20adopt%0Ahierarchical%20architectures%20that%20decompose%20control%20into%20isolated%20upper-body%0A%28manipulation%29%20and%20lower-body%20%28locomotion%29%20policies.%20While%20this%20decomposition%0Areduces%20training%20complexity%2C%20it%20inherently%20limits%20coordination%20between%0Asubsystems%20and%20contradicts%20the%20unified%20whole-body%20control%20exhibited%20by%20humans.%0AWe%20demonstrate%20that%20a%20single%20unified%20policy%20can%20achieve%20a%20combination%20of%0Atracking%20accuracy%2C%20large%20workspace%2C%20and%20robustness%20for%20humanoid%0Aloco-manipulation.%20We%20propose%20the%20Unified%20Loco-Manipulation%20Controller%20%28ULC%29%2C%20a%0Asingle-policy%20framework%20that%20simultaneously%20tracks%20root%20velocity%2C%20root%20height%2C%0Atorso%20rotation%2C%20and%20dual-arm%20joint%20positions%20in%20an%20end-to-end%20manner%2C%20proving%0Athe%20feasibility%20of%20unified%20control%20without%20sacrificing%20performance.%20We%20achieve%0Athis%20unified%20control%20through%20key%20technologies%3A%20sequence%20skill%20acquisition%20for%0Aprogressive%20learning%20complexity%2C%20residual%20action%20modeling%20for%20fine-grained%0Acontrol%20adjustments%2C%20command%20polynomial%20interpolation%20for%20smooth%20motion%0Atransitions%2C%20random%20delay%20release%20for%20robustness%20to%20deploy%20variations%2C%20load%0Arandomization%20for%20generalization%20to%20external%20disturbances%2C%20and%0Acenter-of-gravity%20tracking%20for%20providing%20explicit%20policy%20gradients%20to%20maintain%0Astability.%20We%20validate%20our%20method%20on%20the%20Unitree%20G1%20humanoid%20robot%20with%203-DOF%0A%28degrees-of-freedom%29%20waist.%20Compared%20with%20strong%20baselines%2C%20ULC%20shows%20better%0Atracking%20performance%20to%20disentangled%20methods%20and%20demonstrating%20larger%20workspace%0Acoverage.%20The%20unified%20dual-arm%20tracking%20enables%20precise%20manipulation%20under%0Aexternal%20loads%20while%20maintaining%20coordinated%20whole-body%20control%20for%20complex%0Aloco-manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06905v1&entry.124074799=Read"},
{"title": "Evaluating Attribute Confusion in Fashion Text-to-Image Generation", "author": "Ziyue Liu and Federico Girella and Yiming Wang and Davide Talon", "abstract": "  Despite the rapid advances in Text-to-Image (T2I) generation models, their\nevaluation remains challenging in domains like fashion, involving complex\ncompositional generation. Recent automated T2I evaluation methods leverage\npre-trained vision-language models to measure cross-modal alignment. However,\nour preliminary study reveals that they are still limited in assessing rich\nentity-attribute semantics, facing challenges in attribute confusion, i.e.,\nwhen attributes are correctly depicted but associated to the wrong entities. To\naddress this, we build on a Visual Question Answering (VQA) localization\nstrategy targeting one single entity at a time across both visual and textual\nmodalities. We propose a localized human evaluation protocol and introduce a\nnovel automatic metric, Localized VQAScore (L-VQAScore), that combines visual\nlocalization with VQA probing both correct (reflection) and miss-localized\n(leakage) attribute generation. On a newly curated dataset featuring\nchallenging compositional alignment scenarios, L-VQAScore outperforms\nstate-of-the-art T2I evaluation methods in terms of correlation with human\njudgments, demonstrating its strength in capturing fine-grained\nentity-attribute associations. We believe L-VQAScore can be a reliable and\nscalable alternative to subjective evaluations.\n", "link": "http://arxiv.org/abs/2507.07079v1", "date": "2025-07-09", "relevancy": 2.2807, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6089}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5659}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Attribute%20Confusion%20in%20Fashion%20Text-to-Image%20Generation&body=Title%3A%20Evaluating%20Attribute%20Confusion%20in%20Fashion%20Text-to-Image%20Generation%0AAuthor%3A%20Ziyue%20Liu%20and%20Federico%20Girella%20and%20Yiming%20Wang%20and%20Davide%20Talon%0AAbstract%3A%20%20%20Despite%20the%20rapid%20advances%20in%20Text-to-Image%20%28T2I%29%20generation%20models%2C%20their%0Aevaluation%20remains%20challenging%20in%20domains%20like%20fashion%2C%20involving%20complex%0Acompositional%20generation.%20Recent%20automated%20T2I%20evaluation%20methods%20leverage%0Apre-trained%20vision-language%20models%20to%20measure%20cross-modal%20alignment.%20However%2C%0Aour%20preliminary%20study%20reveals%20that%20they%20are%20still%20limited%20in%20assessing%20rich%0Aentity-attribute%20semantics%2C%20facing%20challenges%20in%20attribute%20confusion%2C%20i.e.%2C%0Awhen%20attributes%20are%20correctly%20depicted%20but%20associated%20to%20the%20wrong%20entities.%20To%0Aaddress%20this%2C%20we%20build%20on%20a%20Visual%20Question%20Answering%20%28VQA%29%20localization%0Astrategy%20targeting%20one%20single%20entity%20at%20a%20time%20across%20both%20visual%20and%20textual%0Amodalities.%20We%20propose%20a%20localized%20human%20evaluation%20protocol%20and%20introduce%20a%0Anovel%20automatic%20metric%2C%20Localized%20VQAScore%20%28L-VQAScore%29%2C%20that%20combines%20visual%0Alocalization%20with%20VQA%20probing%20both%20correct%20%28reflection%29%20and%20miss-localized%0A%28leakage%29%20attribute%20generation.%20On%20a%20newly%20curated%20dataset%20featuring%0Achallenging%20compositional%20alignment%20scenarios%2C%20L-VQAScore%20outperforms%0Astate-of-the-art%20T2I%20evaluation%20methods%20in%20terms%20of%20correlation%20with%20human%0Ajudgments%2C%20demonstrating%20its%20strength%20in%20capturing%20fine-grained%0Aentity-attribute%20associations.%20We%20believe%20L-VQAScore%20can%20be%20a%20reliable%20and%0Ascalable%20alternative%20to%20subjective%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Attribute%2520Confusion%2520in%2520Fashion%2520Text-to-Image%2520Generation%26entry.906535625%3DZiyue%2520Liu%2520and%2520Federico%2520Girella%2520and%2520Yiming%2520Wang%2520and%2520Davide%2520Talon%26entry.1292438233%3D%2520%2520Despite%2520the%2520rapid%2520advances%2520in%2520Text-to-Image%2520%2528T2I%2529%2520generation%2520models%252C%2520their%250Aevaluation%2520remains%2520challenging%2520in%2520domains%2520like%2520fashion%252C%2520involving%2520complex%250Acompositional%2520generation.%2520Recent%2520automated%2520T2I%2520evaluation%2520methods%2520leverage%250Apre-trained%2520vision-language%2520models%2520to%2520measure%2520cross-modal%2520alignment.%2520However%252C%250Aour%2520preliminary%2520study%2520reveals%2520that%2520they%2520are%2520still%2520limited%2520in%2520assessing%2520rich%250Aentity-attribute%2520semantics%252C%2520facing%2520challenges%2520in%2520attribute%2520confusion%252C%2520i.e.%252C%250Awhen%2520attributes%2520are%2520correctly%2520depicted%2520but%2520associated%2520to%2520the%2520wrong%2520entities.%2520To%250Aaddress%2520this%252C%2520we%2520build%2520on%2520a%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520localization%250Astrategy%2520targeting%2520one%2520single%2520entity%2520at%2520a%2520time%2520across%2520both%2520visual%2520and%2520textual%250Amodalities.%2520We%2520propose%2520a%2520localized%2520human%2520evaluation%2520protocol%2520and%2520introduce%2520a%250Anovel%2520automatic%2520metric%252C%2520Localized%2520VQAScore%2520%2528L-VQAScore%2529%252C%2520that%2520combines%2520visual%250Alocalization%2520with%2520VQA%2520probing%2520both%2520correct%2520%2528reflection%2529%2520and%2520miss-localized%250A%2528leakage%2529%2520attribute%2520generation.%2520On%2520a%2520newly%2520curated%2520dataset%2520featuring%250Achallenging%2520compositional%2520alignment%2520scenarios%252C%2520L-VQAScore%2520outperforms%250Astate-of-the-art%2520T2I%2520evaluation%2520methods%2520in%2520terms%2520of%2520correlation%2520with%2520human%250Ajudgments%252C%2520demonstrating%2520its%2520strength%2520in%2520capturing%2520fine-grained%250Aentity-attribute%2520associations.%2520We%2520believe%2520L-VQAScore%2520can%2520be%2520a%2520reliable%2520and%250Ascalable%2520alternative%2520to%2520subjective%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Attribute%20Confusion%20in%20Fashion%20Text-to-Image%20Generation&entry.906535625=Ziyue%20Liu%20and%20Federico%20Girella%20and%20Yiming%20Wang%20and%20Davide%20Talon&entry.1292438233=%20%20Despite%20the%20rapid%20advances%20in%20Text-to-Image%20%28T2I%29%20generation%20models%2C%20their%0Aevaluation%20remains%20challenging%20in%20domains%20like%20fashion%2C%20involving%20complex%0Acompositional%20generation.%20Recent%20automated%20T2I%20evaluation%20methods%20leverage%0Apre-trained%20vision-language%20models%20to%20measure%20cross-modal%20alignment.%20However%2C%0Aour%20preliminary%20study%20reveals%20that%20they%20are%20still%20limited%20in%20assessing%20rich%0Aentity-attribute%20semantics%2C%20facing%20challenges%20in%20attribute%20confusion%2C%20i.e.%2C%0Awhen%20attributes%20are%20correctly%20depicted%20but%20associated%20to%20the%20wrong%20entities.%20To%0Aaddress%20this%2C%20we%20build%20on%20a%20Visual%20Question%20Answering%20%28VQA%29%20localization%0Astrategy%20targeting%20one%20single%20entity%20at%20a%20time%20across%20both%20visual%20and%20textual%0Amodalities.%20We%20propose%20a%20localized%20human%20evaluation%20protocol%20and%20introduce%20a%0Anovel%20automatic%20metric%2C%20Localized%20VQAScore%20%28L-VQAScore%29%2C%20that%20combines%20visual%0Alocalization%20with%20VQA%20probing%20both%20correct%20%28reflection%29%20and%20miss-localized%0A%28leakage%29%20attribute%20generation.%20On%20a%20newly%20curated%20dataset%20featuring%0Achallenging%20compositional%20alignment%20scenarios%2C%20L-VQAScore%20outperforms%0Astate-of-the-art%20T2I%20evaluation%20methods%20in%20terms%20of%20correlation%20with%20human%0Ajudgments%2C%20demonstrating%20its%20strength%20in%20capturing%20fine-grained%0Aentity-attribute%20associations.%20We%20believe%20L-VQAScore%20can%20be%20a%20reliable%20and%0Ascalable%20alternative%20to%20subjective%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07079v1&entry.124074799=Read"},
{"title": "AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection", "author": "Lorenzo Pellegrini and Davide Cozzolino and Serafino Pandolfini and Davide Maltoni and Matteo Ferrara and Luisa Verdoliva and Marco Prati and Marco Ramilli", "abstract": "  The rapid advancement of generative AI has revolutionized image creation,\nenabling high-quality synthesis from text prompts while raising critical\nchallenges for media authenticity. We present Ai-GenBench, a novel benchmark\ndesigned to address the urgent need for robust detection of AI-generated images\nin real-world scenarios. Unlike existing solutions that evaluate models on\nstatic datasets, Ai-GenBench introduces a temporal evaluation framework where\ndetection methods are incrementally trained on synthetic images, historically\nordered by their generative models, to test their ability to generalize to new\ngenerative models, such as the transition from GANs to diffusion models. Our\nbenchmark focuses on high-quality, diverse visual content and overcomes key\nlimitations of current approaches, including arbitrary dataset splits, unfair\ncomparisons, and excessive computational demands. Ai-GenBench provides a\ncomprehensive dataset, a standardized evaluation protocol, and accessible tools\nfor both researchers and non-experts (e.g., journalists, fact-checkers),\nensuring reproducibility while maintaining practical training requirements. By\nestablishing clear evaluation rules and controlled augmentation strategies,\nAi-GenBench enables meaningful comparison of detection methods and scalable\nsolutions. Code and data are publicly available to ensure reproducibility and\nto support the development of robust forensic detectors to keep pace with the\nrise of new synthetic generators.\n", "link": "http://arxiv.org/abs/2504.20865v2", "date": "2025-07-09", "relevancy": 2.2713, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6018}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5688}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-GenBench%3A%20A%20New%20Ongoing%20Benchmark%20for%20AI-Generated%20Image%20Detection&body=Title%3A%20AI-GenBench%3A%20A%20New%20Ongoing%20Benchmark%20for%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Lorenzo%20Pellegrini%20and%20Davide%20Cozzolino%20and%20Serafino%20Pandolfini%20and%20Davide%20Maltoni%20and%20Matteo%20Ferrara%20and%20Luisa%20Verdoliva%20and%20Marco%20Prati%20and%20Marco%20Ramilli%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20generative%20AI%20has%20revolutionized%20image%20creation%2C%0Aenabling%20high-quality%20synthesis%20from%20text%20prompts%20while%20raising%20critical%0Achallenges%20for%20media%20authenticity.%20We%20present%20Ai-GenBench%2C%20a%20novel%20benchmark%0Adesigned%20to%20address%20the%20urgent%20need%20for%20robust%20detection%20of%20AI-generated%20images%0Ain%20real-world%20scenarios.%20Unlike%20existing%20solutions%20that%20evaluate%20models%20on%0Astatic%20datasets%2C%20Ai-GenBench%20introduces%20a%20temporal%20evaluation%20framework%20where%0Adetection%20methods%20are%20incrementally%20trained%20on%20synthetic%20images%2C%20historically%0Aordered%20by%20their%20generative%20models%2C%20to%20test%20their%20ability%20to%20generalize%20to%20new%0Agenerative%20models%2C%20such%20as%20the%20transition%20from%20GANs%20to%20diffusion%20models.%20Our%0Abenchmark%20focuses%20on%20high-quality%2C%20diverse%20visual%20content%20and%20overcomes%20key%0Alimitations%20of%20current%20approaches%2C%20including%20arbitrary%20dataset%20splits%2C%20unfair%0Acomparisons%2C%20and%20excessive%20computational%20demands.%20Ai-GenBench%20provides%20a%0Acomprehensive%20dataset%2C%20a%20standardized%20evaluation%20protocol%2C%20and%20accessible%20tools%0Afor%20both%20researchers%20and%20non-experts%20%28e.g.%2C%20journalists%2C%20fact-checkers%29%2C%0Aensuring%20reproducibility%20while%20maintaining%20practical%20training%20requirements.%20By%0Aestablishing%20clear%20evaluation%20rules%20and%20controlled%20augmentation%20strategies%2C%0AAi-GenBench%20enables%20meaningful%20comparison%20of%20detection%20methods%20and%20scalable%0Asolutions.%20Code%20and%20data%20are%20publicly%20available%20to%20ensure%20reproducibility%20and%0Ato%20support%20the%20development%20of%20robust%20forensic%20detectors%20to%20keep%20pace%20with%20the%0Arise%20of%20new%20synthetic%20generators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20865v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-GenBench%253A%2520A%2520New%2520Ongoing%2520Benchmark%2520for%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DLorenzo%2520Pellegrini%2520and%2520Davide%2520Cozzolino%2520and%2520Serafino%2520Pandolfini%2520and%2520Davide%2520Maltoni%2520and%2520Matteo%2520Ferrara%2520and%2520Luisa%2520Verdoliva%2520and%2520Marco%2520Prati%2520and%2520Marco%2520Ramilli%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520generative%2520AI%2520has%2520revolutionized%2520image%2520creation%252C%250Aenabling%2520high-quality%2520synthesis%2520from%2520text%2520prompts%2520while%2520raising%2520critical%250Achallenges%2520for%2520media%2520authenticity.%2520We%2520present%2520Ai-GenBench%252C%2520a%2520novel%2520benchmark%250Adesigned%2520to%2520address%2520the%2520urgent%2520need%2520for%2520robust%2520detection%2520of%2520AI-generated%2520images%250Ain%2520real-world%2520scenarios.%2520Unlike%2520existing%2520solutions%2520that%2520evaluate%2520models%2520on%250Astatic%2520datasets%252C%2520Ai-GenBench%2520introduces%2520a%2520temporal%2520evaluation%2520framework%2520where%250Adetection%2520methods%2520are%2520incrementally%2520trained%2520on%2520synthetic%2520images%252C%2520historically%250Aordered%2520by%2520their%2520generative%2520models%252C%2520to%2520test%2520their%2520ability%2520to%2520generalize%2520to%2520new%250Agenerative%2520models%252C%2520such%2520as%2520the%2520transition%2520from%2520GANs%2520to%2520diffusion%2520models.%2520Our%250Abenchmark%2520focuses%2520on%2520high-quality%252C%2520diverse%2520visual%2520content%2520and%2520overcomes%2520key%250Alimitations%2520of%2520current%2520approaches%252C%2520including%2520arbitrary%2520dataset%2520splits%252C%2520unfair%250Acomparisons%252C%2520and%2520excessive%2520computational%2520demands.%2520Ai-GenBench%2520provides%2520a%250Acomprehensive%2520dataset%252C%2520a%2520standardized%2520evaluation%2520protocol%252C%2520and%2520accessible%2520tools%250Afor%2520both%2520researchers%2520and%2520non-experts%2520%2528e.g.%252C%2520journalists%252C%2520fact-checkers%2529%252C%250Aensuring%2520reproducibility%2520while%2520maintaining%2520practical%2520training%2520requirements.%2520By%250Aestablishing%2520clear%2520evaluation%2520rules%2520and%2520controlled%2520augmentation%2520strategies%252C%250AAi-GenBench%2520enables%2520meaningful%2520comparison%2520of%2520detection%2520methods%2520and%2520scalable%250Asolutions.%2520Code%2520and%2520data%2520are%2520publicly%2520available%2520to%2520ensure%2520reproducibility%2520and%250Ato%2520support%2520the%2520development%2520of%2520robust%2520forensic%2520detectors%2520to%2520keep%2520pace%2520with%2520the%250Arise%2520of%2520new%2520synthetic%2520generators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20865v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-GenBench%3A%20A%20New%20Ongoing%20Benchmark%20for%20AI-Generated%20Image%20Detection&entry.906535625=Lorenzo%20Pellegrini%20and%20Davide%20Cozzolino%20and%20Serafino%20Pandolfini%20and%20Davide%20Maltoni%20and%20Matteo%20Ferrara%20and%20Luisa%20Verdoliva%20and%20Marco%20Prati%20and%20Marco%20Ramilli&entry.1292438233=%20%20The%20rapid%20advancement%20of%20generative%20AI%20has%20revolutionized%20image%20creation%2C%0Aenabling%20high-quality%20synthesis%20from%20text%20prompts%20while%20raising%20critical%0Achallenges%20for%20media%20authenticity.%20We%20present%20Ai-GenBench%2C%20a%20novel%20benchmark%0Adesigned%20to%20address%20the%20urgent%20need%20for%20robust%20detection%20of%20AI-generated%20images%0Ain%20real-world%20scenarios.%20Unlike%20existing%20solutions%20that%20evaluate%20models%20on%0Astatic%20datasets%2C%20Ai-GenBench%20introduces%20a%20temporal%20evaluation%20framework%20where%0Adetection%20methods%20are%20incrementally%20trained%20on%20synthetic%20images%2C%20historically%0Aordered%20by%20their%20generative%20models%2C%20to%20test%20their%20ability%20to%20generalize%20to%20new%0Agenerative%20models%2C%20such%20as%20the%20transition%20from%20GANs%20to%20diffusion%20models.%20Our%0Abenchmark%20focuses%20on%20high-quality%2C%20diverse%20visual%20content%20and%20overcomes%20key%0Alimitations%20of%20current%20approaches%2C%20including%20arbitrary%20dataset%20splits%2C%20unfair%0Acomparisons%2C%20and%20excessive%20computational%20demands.%20Ai-GenBench%20provides%20a%0Acomprehensive%20dataset%2C%20a%20standardized%20evaluation%20protocol%2C%20and%20accessible%20tools%0Afor%20both%20researchers%20and%20non-experts%20%28e.g.%2C%20journalists%2C%20fact-checkers%29%2C%0Aensuring%20reproducibility%20while%20maintaining%20practical%20training%20requirements.%20By%0Aestablishing%20clear%20evaluation%20rules%20and%20controlled%20augmentation%20strategies%2C%0AAi-GenBench%20enables%20meaningful%20comparison%20of%20detection%20methods%20and%20scalable%0Asolutions.%20Code%20and%20data%20are%20publicly%20available%20to%20ensure%20reproducibility%20and%0Ato%20support%20the%20development%20of%20robust%20forensic%20detectors%20to%20keep%20pace%20with%20the%0Arise%20of%20new%20synthetic%20generators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20865v2&entry.124074799=Read"},
{"title": "ADPv2: A Hierarchical Histological Tissue Type-Annotated Dataset for\n  Potential Biomarker Discovery of Colorectal Disease", "author": "Zhiyuan Yang and Kai Li and Sophia Ghamoshi Ramandi and Patricia Brassard and Hakim Khellaf and Vincent Quoc-Huy Trinh and Jennifer Zhang and Lina Chen and Corwyn Rowsell and Sonal Varma and Kostas Plataniotis and Mahdi S. Hosseini", "abstract": "  Computational pathology (CoPath) leverages histopathology images to enhance\ndiagnostic precision and reproducibility in clinical pathology. However,\npublicly available datasets for CoPath that are annotated with extensive\nhistological tissue type (HTT) taxonomies at a granular level remain scarce due\nto the significant expertise and high annotation costs required. Existing\ndatasets, such as the Atlas of Digital Pathology (ADP), address this by\noffering diverse HTT annotations generalized to multiple organs, but limit the\ncapability for in-depth studies on specific organ diseases. Building upon this\nfoundation, we introduce ADPv2, a novel dataset focused on gastrointestinal\nhistopathology. Our dataset comprises 20,004 image patches derived from healthy\ncolon biopsy slides, annotated according to a hierarchical taxonomy of 32\ndistinct HTTs of 3 levels. Furthermore, we train a multilabel representation\nlearning model following a two-stage training procedure on our ADPv2 dataset.\nWe leverage the VMamba architecture and achieving a mean average precision\n(mAP) of 0.88 in multilabel classification of colon HTTs. Finally, we show that\nour dataset is capable of an organ-specific in-depth study for potential\nbiomarker discovery by analyzing the model's prediction behavior on tissues\naffected by different colon diseases, which reveals statistical patterns that\nconfirm the two pathological pathways of colon cancer development. Our dataset\nis publicly available at https://zenodo.org/records/15307021\n", "link": "http://arxiv.org/abs/2507.05656v2", "date": "2025-07-09", "relevancy": 2.2616, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4953}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.437}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADPv2%3A%20A%20Hierarchical%20Histological%20Tissue%20Type-Annotated%20Dataset%20for%0A%20%20Potential%20Biomarker%20Discovery%20of%20Colorectal%20Disease&body=Title%3A%20ADPv2%3A%20A%20Hierarchical%20Histological%20Tissue%20Type-Annotated%20Dataset%20for%0A%20%20Potential%20Biomarker%20Discovery%20of%20Colorectal%20Disease%0AAuthor%3A%20Zhiyuan%20Yang%20and%20Kai%20Li%20and%20Sophia%20Ghamoshi%20Ramandi%20and%20Patricia%20Brassard%20and%20Hakim%20Khellaf%20and%20Vincent%20Quoc-Huy%20Trinh%20and%20Jennifer%20Zhang%20and%20Lina%20Chen%20and%20Corwyn%20Rowsell%20and%20Sonal%20Varma%20and%20Kostas%20Plataniotis%20and%20Mahdi%20S.%20Hosseini%0AAbstract%3A%20%20%20Computational%20pathology%20%28CoPath%29%20leverages%20histopathology%20images%20to%20enhance%0Adiagnostic%20precision%20and%20reproducibility%20in%20clinical%20pathology.%20However%2C%0Apublicly%20available%20datasets%20for%20CoPath%20that%20are%20annotated%20with%20extensive%0Ahistological%20tissue%20type%20%28HTT%29%20taxonomies%20at%20a%20granular%20level%20remain%20scarce%20due%0Ato%20the%20significant%20expertise%20and%20high%20annotation%20costs%20required.%20Existing%0Adatasets%2C%20such%20as%20the%20Atlas%20of%20Digital%20Pathology%20%28ADP%29%2C%20address%20this%20by%0Aoffering%20diverse%20HTT%20annotations%20generalized%20to%20multiple%20organs%2C%20but%20limit%20the%0Acapability%20for%20in-depth%20studies%20on%20specific%20organ%20diseases.%20Building%20upon%20this%0Afoundation%2C%20we%20introduce%20ADPv2%2C%20a%20novel%20dataset%20focused%20on%20gastrointestinal%0Ahistopathology.%20Our%20dataset%20comprises%2020%2C004%20image%20patches%20derived%20from%20healthy%0Acolon%20biopsy%20slides%2C%20annotated%20according%20to%20a%20hierarchical%20taxonomy%20of%2032%0Adistinct%20HTTs%20of%203%20levels.%20Furthermore%2C%20we%20train%20a%20multilabel%20representation%0Alearning%20model%20following%20a%20two-stage%20training%20procedure%20on%20our%20ADPv2%20dataset.%0AWe%20leverage%20the%20VMamba%20architecture%20and%20achieving%20a%20mean%20average%20precision%0A%28mAP%29%20of%200.88%20in%20multilabel%20classification%20of%20colon%20HTTs.%20Finally%2C%20we%20show%20that%0Aour%20dataset%20is%20capable%20of%20an%20organ-specific%20in-depth%20study%20for%20potential%0Abiomarker%20discovery%20by%20analyzing%20the%20model%27s%20prediction%20behavior%20on%20tissues%0Aaffected%20by%20different%20colon%20diseases%2C%20which%20reveals%20statistical%20patterns%20that%0Aconfirm%20the%20two%20pathological%20pathways%20of%20colon%20cancer%20development.%20Our%20dataset%0Ais%20publicly%20available%20at%20https%3A//zenodo.org/records/15307021%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05656v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADPv2%253A%2520A%2520Hierarchical%2520Histological%2520Tissue%2520Type-Annotated%2520Dataset%2520for%250A%2520%2520Potential%2520Biomarker%2520Discovery%2520of%2520Colorectal%2520Disease%26entry.906535625%3DZhiyuan%2520Yang%2520and%2520Kai%2520Li%2520and%2520Sophia%2520Ghamoshi%2520Ramandi%2520and%2520Patricia%2520Brassard%2520and%2520Hakim%2520Khellaf%2520and%2520Vincent%2520Quoc-Huy%2520Trinh%2520and%2520Jennifer%2520Zhang%2520and%2520Lina%2520Chen%2520and%2520Corwyn%2520Rowsell%2520and%2520Sonal%2520Varma%2520and%2520Kostas%2520Plataniotis%2520and%2520Mahdi%2520S.%2520Hosseini%26entry.1292438233%3D%2520%2520Computational%2520pathology%2520%2528CoPath%2529%2520leverages%2520histopathology%2520images%2520to%2520enhance%250Adiagnostic%2520precision%2520and%2520reproducibility%2520in%2520clinical%2520pathology.%2520However%252C%250Apublicly%2520available%2520datasets%2520for%2520CoPath%2520that%2520are%2520annotated%2520with%2520extensive%250Ahistological%2520tissue%2520type%2520%2528HTT%2529%2520taxonomies%2520at%2520a%2520granular%2520level%2520remain%2520scarce%2520due%250Ato%2520the%2520significant%2520expertise%2520and%2520high%2520annotation%2520costs%2520required.%2520Existing%250Adatasets%252C%2520such%2520as%2520the%2520Atlas%2520of%2520Digital%2520Pathology%2520%2528ADP%2529%252C%2520address%2520this%2520by%250Aoffering%2520diverse%2520HTT%2520annotations%2520generalized%2520to%2520multiple%2520organs%252C%2520but%2520limit%2520the%250Acapability%2520for%2520in-depth%2520studies%2520on%2520specific%2520organ%2520diseases.%2520Building%2520upon%2520this%250Afoundation%252C%2520we%2520introduce%2520ADPv2%252C%2520a%2520novel%2520dataset%2520focused%2520on%2520gastrointestinal%250Ahistopathology.%2520Our%2520dataset%2520comprises%252020%252C004%2520image%2520patches%2520derived%2520from%2520healthy%250Acolon%2520biopsy%2520slides%252C%2520annotated%2520according%2520to%2520a%2520hierarchical%2520taxonomy%2520of%252032%250Adistinct%2520HTTs%2520of%25203%2520levels.%2520Furthermore%252C%2520we%2520train%2520a%2520multilabel%2520representation%250Alearning%2520model%2520following%2520a%2520two-stage%2520training%2520procedure%2520on%2520our%2520ADPv2%2520dataset.%250AWe%2520leverage%2520the%2520VMamba%2520architecture%2520and%2520achieving%2520a%2520mean%2520average%2520precision%250A%2528mAP%2529%2520of%25200.88%2520in%2520multilabel%2520classification%2520of%2520colon%2520HTTs.%2520Finally%252C%2520we%2520show%2520that%250Aour%2520dataset%2520is%2520capable%2520of%2520an%2520organ-specific%2520in-depth%2520study%2520for%2520potential%250Abiomarker%2520discovery%2520by%2520analyzing%2520the%2520model%2527s%2520prediction%2520behavior%2520on%2520tissues%250Aaffected%2520by%2520different%2520colon%2520diseases%252C%2520which%2520reveals%2520statistical%2520patterns%2520that%250Aconfirm%2520the%2520two%2520pathological%2520pathways%2520of%2520colon%2520cancer%2520development.%2520Our%2520dataset%250Ais%2520publicly%2520available%2520at%2520https%253A//zenodo.org/records/15307021%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05656v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADPv2%3A%20A%20Hierarchical%20Histological%20Tissue%20Type-Annotated%20Dataset%20for%0A%20%20Potential%20Biomarker%20Discovery%20of%20Colorectal%20Disease&entry.906535625=Zhiyuan%20Yang%20and%20Kai%20Li%20and%20Sophia%20Ghamoshi%20Ramandi%20and%20Patricia%20Brassard%20and%20Hakim%20Khellaf%20and%20Vincent%20Quoc-Huy%20Trinh%20and%20Jennifer%20Zhang%20and%20Lina%20Chen%20and%20Corwyn%20Rowsell%20and%20Sonal%20Varma%20and%20Kostas%20Plataniotis%20and%20Mahdi%20S.%20Hosseini&entry.1292438233=%20%20Computational%20pathology%20%28CoPath%29%20leverages%20histopathology%20images%20to%20enhance%0Adiagnostic%20precision%20and%20reproducibility%20in%20clinical%20pathology.%20However%2C%0Apublicly%20available%20datasets%20for%20CoPath%20that%20are%20annotated%20with%20extensive%0Ahistological%20tissue%20type%20%28HTT%29%20taxonomies%20at%20a%20granular%20level%20remain%20scarce%20due%0Ato%20the%20significant%20expertise%20and%20high%20annotation%20costs%20required.%20Existing%0Adatasets%2C%20such%20as%20the%20Atlas%20of%20Digital%20Pathology%20%28ADP%29%2C%20address%20this%20by%0Aoffering%20diverse%20HTT%20annotations%20generalized%20to%20multiple%20organs%2C%20but%20limit%20the%0Acapability%20for%20in-depth%20studies%20on%20specific%20organ%20diseases.%20Building%20upon%20this%0Afoundation%2C%20we%20introduce%20ADPv2%2C%20a%20novel%20dataset%20focused%20on%20gastrointestinal%0Ahistopathology.%20Our%20dataset%20comprises%2020%2C004%20image%20patches%20derived%20from%20healthy%0Acolon%20biopsy%20slides%2C%20annotated%20according%20to%20a%20hierarchical%20taxonomy%20of%2032%0Adistinct%20HTTs%20of%203%20levels.%20Furthermore%2C%20we%20train%20a%20multilabel%20representation%0Alearning%20model%20following%20a%20two-stage%20training%20procedure%20on%20our%20ADPv2%20dataset.%0AWe%20leverage%20the%20VMamba%20architecture%20and%20achieving%20a%20mean%20average%20precision%0A%28mAP%29%20of%200.88%20in%20multilabel%20classification%20of%20colon%20HTTs.%20Finally%2C%20we%20show%20that%0Aour%20dataset%20is%20capable%20of%20an%20organ-specific%20in-depth%20study%20for%20potential%0Abiomarker%20discovery%20by%20analyzing%20the%20model%27s%20prediction%20behavior%20on%20tissues%0Aaffected%20by%20different%20colon%20diseases%2C%20which%20reveals%20statistical%20patterns%20that%0Aconfirm%20the%20two%20pathological%20pathways%20of%20colon%20cancer%20development.%20Our%20dataset%0Ais%20publicly%20available%20at%20https%3A//zenodo.org/records/15307021%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05656v2&entry.124074799=Read"},
{"title": "Beyond Complete Shapes: A Quantitative Evaluation of 3D Shape Matching\n  Algorithms", "author": "Viktoria Ehm and Nafie El Amrani and Yizheng Xie and Lennart Bastian and Maolin Gao and Weikang Wang and Lu Sang and Dongliang Cao and Tobias Wei\u00dfberg and Zorah L\u00e4hner and Daniel Cremers and Florian Bernard", "abstract": "  Finding correspondences between 3D shapes is an important and long-standing\nproblem in computer vision, graphics and beyond. While approaches based on\nmachine learning dominate modern 3D shape matching, almost all existing\n(learning-based) methods require that at least one of the involved shapes is\ncomplete. In contrast, the most challenging and arguably most practically\nrelevant setting of matching partially observed shapes, is currently\nunderexplored. One important factor is that existing datasets contain only a\nsmall number of shapes (typically below 100), which are unable to serve\ndata-hungry machine learning approaches, particularly in the unsupervised\nregime. In addition, the type of partiality present in existing datasets is\noften artificial and far from realistic. To address these limitations and to\nencourage research on these relevant settings, we provide a generic and\nflexible framework for the procedural generation of challenging partial shape\nmatching scenarios. Our framework allows for a virtually infinite generation of\npartial shape matching instances from a finite set of shapes with complete\ngeometry. Further, we manually create cross-dataset correspondences between\nseven existing (complete geometry) shape matching datasets, leading to a total\nof 2543 shapes. Based on this, we propose several challenging partial benchmark\nsettings, for which we evaluate respective state-of-the-art methods as\nbaselines.\n", "link": "http://arxiv.org/abs/2411.03511v2", "date": "2025-07-09", "relevancy": 2.2538, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5789}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5533}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Complete%20Shapes%3A%20A%20Quantitative%20Evaluation%20of%203D%20Shape%20Matching%0A%20%20Algorithms&body=Title%3A%20Beyond%20Complete%20Shapes%3A%20A%20Quantitative%20Evaluation%20of%203D%20Shape%20Matching%0A%20%20Algorithms%0AAuthor%3A%20Viktoria%20Ehm%20and%20Nafie%20El%20Amrani%20and%20Yizheng%20Xie%20and%20Lennart%20Bastian%20and%20Maolin%20Gao%20and%20Weikang%20Wang%20and%20Lu%20Sang%20and%20Dongliang%20Cao%20and%20Tobias%20Wei%C3%9Fberg%20and%20Zorah%20L%C3%A4hner%20and%20Daniel%20Cremers%20and%20Florian%20Bernard%0AAbstract%3A%20%20%20Finding%20correspondences%20between%203D%20shapes%20is%20an%20important%20and%20long-standing%0Aproblem%20in%20computer%20vision%2C%20graphics%20and%20beyond.%20While%20approaches%20based%20on%0Amachine%20learning%20dominate%20modern%203D%20shape%20matching%2C%20almost%20all%20existing%0A%28learning-based%29%20methods%20require%20that%20at%20least%20one%20of%20the%20involved%20shapes%20is%0Acomplete.%20In%20contrast%2C%20the%20most%20challenging%20and%20arguably%20most%20practically%0Arelevant%20setting%20of%20matching%20partially%20observed%20shapes%2C%20is%20currently%0Aunderexplored.%20One%20important%20factor%20is%20that%20existing%20datasets%20contain%20only%20a%0Asmall%20number%20of%20shapes%20%28typically%20below%20100%29%2C%20which%20are%20unable%20to%20serve%0Adata-hungry%20machine%20learning%20approaches%2C%20particularly%20in%20the%20unsupervised%0Aregime.%20In%20addition%2C%20the%20type%20of%20partiality%20present%20in%20existing%20datasets%20is%0Aoften%20artificial%20and%20far%20from%20realistic.%20To%20address%20these%20limitations%20and%20to%0Aencourage%20research%20on%20these%20relevant%20settings%2C%20we%20provide%20a%20generic%20and%0Aflexible%20framework%20for%20the%20procedural%20generation%20of%20challenging%20partial%20shape%0Amatching%20scenarios.%20Our%20framework%20allows%20for%20a%20virtually%20infinite%20generation%20of%0Apartial%20shape%20matching%20instances%20from%20a%20finite%20set%20of%20shapes%20with%20complete%0Ageometry.%20Further%2C%20we%20manually%20create%20cross-dataset%20correspondences%20between%0Aseven%20existing%20%28complete%20geometry%29%20shape%20matching%20datasets%2C%20leading%20to%20a%20total%0Aof%202543%20shapes.%20Based%20on%20this%2C%20we%20propose%20several%20challenging%20partial%20benchmark%0Asettings%2C%20for%20which%20we%20evaluate%20respective%20state-of-the-art%20methods%20as%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Complete%2520Shapes%253A%2520A%2520Quantitative%2520Evaluation%2520of%25203D%2520Shape%2520Matching%250A%2520%2520Algorithms%26entry.906535625%3DViktoria%2520Ehm%2520and%2520Nafie%2520El%2520Amrani%2520and%2520Yizheng%2520Xie%2520and%2520Lennart%2520Bastian%2520and%2520Maolin%2520Gao%2520and%2520Weikang%2520Wang%2520and%2520Lu%2520Sang%2520and%2520Dongliang%2520Cao%2520and%2520Tobias%2520Wei%25C3%259Fberg%2520and%2520Zorah%2520L%25C3%25A4hner%2520and%2520Daniel%2520Cremers%2520and%2520Florian%2520Bernard%26entry.1292438233%3D%2520%2520Finding%2520correspondences%2520between%25203D%2520shapes%2520is%2520an%2520important%2520and%2520long-standing%250Aproblem%2520in%2520computer%2520vision%252C%2520graphics%2520and%2520beyond.%2520While%2520approaches%2520based%2520on%250Amachine%2520learning%2520dominate%2520modern%25203D%2520shape%2520matching%252C%2520almost%2520all%2520existing%250A%2528learning-based%2529%2520methods%2520require%2520that%2520at%2520least%2520one%2520of%2520the%2520involved%2520shapes%2520is%250Acomplete.%2520In%2520contrast%252C%2520the%2520most%2520challenging%2520and%2520arguably%2520most%2520practically%250Arelevant%2520setting%2520of%2520matching%2520partially%2520observed%2520shapes%252C%2520is%2520currently%250Aunderexplored.%2520One%2520important%2520factor%2520is%2520that%2520existing%2520datasets%2520contain%2520only%2520a%250Asmall%2520number%2520of%2520shapes%2520%2528typically%2520below%2520100%2529%252C%2520which%2520are%2520unable%2520to%2520serve%250Adata-hungry%2520machine%2520learning%2520approaches%252C%2520particularly%2520in%2520the%2520unsupervised%250Aregime.%2520In%2520addition%252C%2520the%2520type%2520of%2520partiality%2520present%2520in%2520existing%2520datasets%2520is%250Aoften%2520artificial%2520and%2520far%2520from%2520realistic.%2520To%2520address%2520these%2520limitations%2520and%2520to%250Aencourage%2520research%2520on%2520these%2520relevant%2520settings%252C%2520we%2520provide%2520a%2520generic%2520and%250Aflexible%2520framework%2520for%2520the%2520procedural%2520generation%2520of%2520challenging%2520partial%2520shape%250Amatching%2520scenarios.%2520Our%2520framework%2520allows%2520for%2520a%2520virtually%2520infinite%2520generation%2520of%250Apartial%2520shape%2520matching%2520instances%2520from%2520a%2520finite%2520set%2520of%2520shapes%2520with%2520complete%250Ageometry.%2520Further%252C%2520we%2520manually%2520create%2520cross-dataset%2520correspondences%2520between%250Aseven%2520existing%2520%2528complete%2520geometry%2529%2520shape%2520matching%2520datasets%252C%2520leading%2520to%2520a%2520total%250Aof%25202543%2520shapes.%2520Based%2520on%2520this%252C%2520we%2520propose%2520several%2520challenging%2520partial%2520benchmark%250Asettings%252C%2520for%2520which%2520we%2520evaluate%2520respective%2520state-of-the-art%2520methods%2520as%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Complete%20Shapes%3A%20A%20Quantitative%20Evaluation%20of%203D%20Shape%20Matching%0A%20%20Algorithms&entry.906535625=Viktoria%20Ehm%20and%20Nafie%20El%20Amrani%20and%20Yizheng%20Xie%20and%20Lennart%20Bastian%20and%20Maolin%20Gao%20and%20Weikang%20Wang%20and%20Lu%20Sang%20and%20Dongliang%20Cao%20and%20Tobias%20Wei%C3%9Fberg%20and%20Zorah%20L%C3%A4hner%20and%20Daniel%20Cremers%20and%20Florian%20Bernard&entry.1292438233=%20%20Finding%20correspondences%20between%203D%20shapes%20is%20an%20important%20and%20long-standing%0Aproblem%20in%20computer%20vision%2C%20graphics%20and%20beyond.%20While%20approaches%20based%20on%0Amachine%20learning%20dominate%20modern%203D%20shape%20matching%2C%20almost%20all%20existing%0A%28learning-based%29%20methods%20require%20that%20at%20least%20one%20of%20the%20involved%20shapes%20is%0Acomplete.%20In%20contrast%2C%20the%20most%20challenging%20and%20arguably%20most%20practically%0Arelevant%20setting%20of%20matching%20partially%20observed%20shapes%2C%20is%20currently%0Aunderexplored.%20One%20important%20factor%20is%20that%20existing%20datasets%20contain%20only%20a%0Asmall%20number%20of%20shapes%20%28typically%20below%20100%29%2C%20which%20are%20unable%20to%20serve%0Adata-hungry%20machine%20learning%20approaches%2C%20particularly%20in%20the%20unsupervised%0Aregime.%20In%20addition%2C%20the%20type%20of%20partiality%20present%20in%20existing%20datasets%20is%0Aoften%20artificial%20and%20far%20from%20realistic.%20To%20address%20these%20limitations%20and%20to%0Aencourage%20research%20on%20these%20relevant%20settings%2C%20we%20provide%20a%20generic%20and%0Aflexible%20framework%20for%20the%20procedural%20generation%20of%20challenging%20partial%20shape%0Amatching%20scenarios.%20Our%20framework%20allows%20for%20a%20virtually%20infinite%20generation%20of%0Apartial%20shape%20matching%20instances%20from%20a%20finite%20set%20of%20shapes%20with%20complete%0Ageometry.%20Further%2C%20we%20manually%20create%20cross-dataset%20correspondences%20between%0Aseven%20existing%20%28complete%20geometry%29%20shape%20matching%20datasets%2C%20leading%20to%20a%20total%0Aof%202543%20shapes.%20Based%20on%20this%2C%20we%20propose%20several%20challenging%20partial%20benchmark%0Asettings%2C%20for%20which%20we%20evaluate%20respective%20state-of-the-art%20methods%20as%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03511v2&entry.124074799=Read"},
{"title": "Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark\n  Enriched with Contextual Metadata", "author": "Bruce Coburn and Jiangpeng He and Megan E. Rollo and Satvinder S. Dhaliwal and Deborah A. Kerr and Fengqing Zhu", "abstract": "  Large Multimodal Models (LMMs) are increasingly applied to meal images for\nnutrition analysis. However, existing work primarily evaluates proprietary\nmodels, such as GPT-4. This leaves the broad range of LLMs underexplored.\nAdditionally, the influence of integrating contextual metadata and its\ninteraction with various reasoning modifiers remains largely uncharted. This\nwork investigates how interpreting contextual metadata derived from GPS\ncoordinates (converted to location/venue type), timestamps (transformed into\nmeal/day type), and the food items present can enhance LMM performance in\nestimating key nutritional values. These values include calories,\nmacronutrients (protein, carbohydrates, fat), and portion sizes. We also\nintroduce ACETADA, a new food-image dataset slated for public release. This\nopen dataset provides nutrition information verified by the dietitian and\nserves as the foundation for our analysis. Our evaluation across eight LMMs\n(four open-weight and four closed-weight) first establishes the benefit of\ncontextual metadata integration over straightforward prompting with images\nalone. We then demonstrate how this incorporation of contextual information\nenhances the efficacy of reasoning modifiers, such as Chain-of-Thought,\nMultimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.\nEmpirical results show that integrating metadata intelligently, when applied\nthrough straightforward prompting strategies, can significantly reduce the Mean\nAbsolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted\nnutritional values. This work highlights the potential of context-aware LMMs\nfor improved nutrition analysis.\n", "link": "http://arxiv.org/abs/2507.07048v1", "date": "2025-07-09", "relevancy": 2.2354, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Large%20Multimodal%20Models%20for%20Nutrition%20Analysis%3A%20A%20Benchmark%0A%20%20Enriched%20with%20Contextual%20Metadata&body=Title%3A%20Evaluating%20Large%20Multimodal%20Models%20for%20Nutrition%20Analysis%3A%20A%20Benchmark%0A%20%20Enriched%20with%20Contextual%20Metadata%0AAuthor%3A%20Bruce%20Coburn%20and%20Jiangpeng%20He%20and%20Megan%20E.%20Rollo%20and%20Satvinder%20S.%20Dhaliwal%20and%20Deborah%20A.%20Kerr%20and%20Fengqing%20Zhu%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20are%20increasingly%20applied%20to%20meal%20images%20for%0Anutrition%20analysis.%20However%2C%20existing%20work%20primarily%20evaluates%20proprietary%0Amodels%2C%20such%20as%20GPT-4.%20This%20leaves%20the%20broad%20range%20of%20LLMs%20underexplored.%0AAdditionally%2C%20the%20influence%20of%20integrating%20contextual%20metadata%20and%20its%0Ainteraction%20with%20various%20reasoning%20modifiers%20remains%20largely%20uncharted.%20This%0Awork%20investigates%20how%20interpreting%20contextual%20metadata%20derived%20from%20GPS%0Acoordinates%20%28converted%20to%20location/venue%20type%29%2C%20timestamps%20%28transformed%20into%0Ameal/day%20type%29%2C%20and%20the%20food%20items%20present%20can%20enhance%20LMM%20performance%20in%0Aestimating%20key%20nutritional%20values.%20These%20values%20include%20calories%2C%0Amacronutrients%20%28protein%2C%20carbohydrates%2C%20fat%29%2C%20and%20portion%20sizes.%20We%20also%0Aintroduce%20ACETADA%2C%20a%20new%20food-image%20dataset%20slated%20for%20public%20release.%20This%0Aopen%20dataset%20provides%20nutrition%20information%20verified%20by%20the%20dietitian%20and%0Aserves%20as%20the%20foundation%20for%20our%20analysis.%20Our%20evaluation%20across%20eight%20LMMs%0A%28four%20open-weight%20and%20four%20closed-weight%29%20first%20establishes%20the%20benefit%20of%0Acontextual%20metadata%20integration%20over%20straightforward%20prompting%20with%20images%0Aalone.%20We%20then%20demonstrate%20how%20this%20incorporation%20of%20contextual%20information%0Aenhances%20the%20efficacy%20of%20reasoning%20modifiers%2C%20such%20as%20Chain-of-Thought%2C%0AMultimodal%20Chain-of-Thought%2C%20Scale%20Hint%2C%20Few-Shot%2C%20and%20Expert%20Persona.%0AEmpirical%20results%20show%20that%20integrating%20metadata%20intelligently%2C%20when%20applied%0Athrough%20straightforward%20prompting%20strategies%2C%20can%20significantly%20reduce%20the%20Mean%0AAbsolute%20Error%20%28MAE%29%20and%20Mean%20Absolute%20Percentage%20Error%20%28MAPE%29%20in%20predicted%0Anutritional%20values.%20This%20work%20highlights%20the%20potential%20of%20context-aware%20LMMs%0Afor%20improved%20nutrition%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Large%2520Multimodal%2520Models%2520for%2520Nutrition%2520Analysis%253A%2520A%2520Benchmark%250A%2520%2520Enriched%2520with%2520Contextual%2520Metadata%26entry.906535625%3DBruce%2520Coburn%2520and%2520Jiangpeng%2520He%2520and%2520Megan%2520E.%2520Rollo%2520and%2520Satvinder%2520S.%2520Dhaliwal%2520and%2520Deborah%2520A.%2520Kerr%2520and%2520Fengqing%2520Zhu%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520are%2520increasingly%2520applied%2520to%2520meal%2520images%2520for%250Anutrition%2520analysis.%2520However%252C%2520existing%2520work%2520primarily%2520evaluates%2520proprietary%250Amodels%252C%2520such%2520as%2520GPT-4.%2520This%2520leaves%2520the%2520broad%2520range%2520of%2520LLMs%2520underexplored.%250AAdditionally%252C%2520the%2520influence%2520of%2520integrating%2520contextual%2520metadata%2520and%2520its%250Ainteraction%2520with%2520various%2520reasoning%2520modifiers%2520remains%2520largely%2520uncharted.%2520This%250Awork%2520investigates%2520how%2520interpreting%2520contextual%2520metadata%2520derived%2520from%2520GPS%250Acoordinates%2520%2528converted%2520to%2520location/venue%2520type%2529%252C%2520timestamps%2520%2528transformed%2520into%250Ameal/day%2520type%2529%252C%2520and%2520the%2520food%2520items%2520present%2520can%2520enhance%2520LMM%2520performance%2520in%250Aestimating%2520key%2520nutritional%2520values.%2520These%2520values%2520include%2520calories%252C%250Amacronutrients%2520%2528protein%252C%2520carbohydrates%252C%2520fat%2529%252C%2520and%2520portion%2520sizes.%2520We%2520also%250Aintroduce%2520ACETADA%252C%2520a%2520new%2520food-image%2520dataset%2520slated%2520for%2520public%2520release.%2520This%250Aopen%2520dataset%2520provides%2520nutrition%2520information%2520verified%2520by%2520the%2520dietitian%2520and%250Aserves%2520as%2520the%2520foundation%2520for%2520our%2520analysis.%2520Our%2520evaluation%2520across%2520eight%2520LMMs%250A%2528four%2520open-weight%2520and%2520four%2520closed-weight%2529%2520first%2520establishes%2520the%2520benefit%2520of%250Acontextual%2520metadata%2520integration%2520over%2520straightforward%2520prompting%2520with%2520images%250Aalone.%2520We%2520then%2520demonstrate%2520how%2520this%2520incorporation%2520of%2520contextual%2520information%250Aenhances%2520the%2520efficacy%2520of%2520reasoning%2520modifiers%252C%2520such%2520as%2520Chain-of-Thought%252C%250AMultimodal%2520Chain-of-Thought%252C%2520Scale%2520Hint%252C%2520Few-Shot%252C%2520and%2520Expert%2520Persona.%250AEmpirical%2520results%2520show%2520that%2520integrating%2520metadata%2520intelligently%252C%2520when%2520applied%250Athrough%2520straightforward%2520prompting%2520strategies%252C%2520can%2520significantly%2520reduce%2520the%2520Mean%250AAbsolute%2520Error%2520%2528MAE%2529%2520and%2520Mean%2520Absolute%2520Percentage%2520Error%2520%2528MAPE%2529%2520in%2520predicted%250Anutritional%2520values.%2520This%2520work%2520highlights%2520the%2520potential%2520of%2520context-aware%2520LMMs%250Afor%2520improved%2520nutrition%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Large%20Multimodal%20Models%20for%20Nutrition%20Analysis%3A%20A%20Benchmark%0A%20%20Enriched%20with%20Contextual%20Metadata&entry.906535625=Bruce%20Coburn%20and%20Jiangpeng%20He%20and%20Megan%20E.%20Rollo%20and%20Satvinder%20S.%20Dhaliwal%20and%20Deborah%20A.%20Kerr%20and%20Fengqing%20Zhu&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20are%20increasingly%20applied%20to%20meal%20images%20for%0Anutrition%20analysis.%20However%2C%20existing%20work%20primarily%20evaluates%20proprietary%0Amodels%2C%20such%20as%20GPT-4.%20This%20leaves%20the%20broad%20range%20of%20LLMs%20underexplored.%0AAdditionally%2C%20the%20influence%20of%20integrating%20contextual%20metadata%20and%20its%0Ainteraction%20with%20various%20reasoning%20modifiers%20remains%20largely%20uncharted.%20This%0Awork%20investigates%20how%20interpreting%20contextual%20metadata%20derived%20from%20GPS%0Acoordinates%20%28converted%20to%20location/venue%20type%29%2C%20timestamps%20%28transformed%20into%0Ameal/day%20type%29%2C%20and%20the%20food%20items%20present%20can%20enhance%20LMM%20performance%20in%0Aestimating%20key%20nutritional%20values.%20These%20values%20include%20calories%2C%0Amacronutrients%20%28protein%2C%20carbohydrates%2C%20fat%29%2C%20and%20portion%20sizes.%20We%20also%0Aintroduce%20ACETADA%2C%20a%20new%20food-image%20dataset%20slated%20for%20public%20release.%20This%0Aopen%20dataset%20provides%20nutrition%20information%20verified%20by%20the%20dietitian%20and%0Aserves%20as%20the%20foundation%20for%20our%20analysis.%20Our%20evaluation%20across%20eight%20LMMs%0A%28four%20open-weight%20and%20four%20closed-weight%29%20first%20establishes%20the%20benefit%20of%0Acontextual%20metadata%20integration%20over%20straightforward%20prompting%20with%20images%0Aalone.%20We%20then%20demonstrate%20how%20this%20incorporation%20of%20contextual%20information%0Aenhances%20the%20efficacy%20of%20reasoning%20modifiers%2C%20such%20as%20Chain-of-Thought%2C%0AMultimodal%20Chain-of-Thought%2C%20Scale%20Hint%2C%20Few-Shot%2C%20and%20Expert%20Persona.%0AEmpirical%20results%20show%20that%20integrating%20metadata%20intelligently%2C%20when%20applied%0Athrough%20straightforward%20prompting%20strategies%2C%20can%20significantly%20reduce%20the%20Mean%0AAbsolute%20Error%20%28MAE%29%20and%20Mean%20Absolute%20Percentage%20Error%20%28MAPE%29%20in%20predicted%0Anutritional%20values.%20This%20work%20highlights%20the%20potential%20of%20context-aware%20LMMs%0Afor%20improved%20nutrition%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07048v1&entry.124074799=Read"},
{"title": "HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement", "author": "Qingsen Yan and Kangbiao Shi and Yixu Feng and Tao Hu and Peng Wu and Guansong Pang and Yanning Zhang", "abstract": "  Low-Light Image Enhancement (LLIE) aims to restore vivid content and details\nfrom corrupted low-light images. However, existing standard RGB (sRGB) color\nspace-based LLIE methods often produce color bias and brightness artifacts due\nto the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)\ncolor space can decouple brightness and color, it introduces significant red\nand black noise artifacts. To address this problem, we propose a new color\nspace for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV\ncolor map and learnable intensity. The HV color map enforces small distances\nfor the red coordinates to remove red noise artifacts, while the learnable\nintensity compresses the low-light regions to remove black noise artifacts.\nAdditionally, we introduce the Color and Intensity Decoupling Network+\n(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and\nmitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+\nleverages abundant contextual and degraded knowledge extracted from low-light\nimages using pre-trained vision-language models, integrated via a novel\nPrior-guided Attention Block (PAB). Within the PAB, latent semantic priors can\npromote content restoration, while degraded representations guide precise color\ncorrection, both particularly in extremely dark regions through the\nmeticulously designed cross-attention fusion mechanism. Furthermore, we\nconstruct a Region Refinement Block that employs convolution for\ninformation-rich regions and self-attention for information-scarce regions,\nensuring accurate brightness adjustments. Comprehensive results from benchmark\nexperiments demonstrate that the proposed HVI-CIDNet+ outperforms the\nstate-of-the-art methods on 10 datasets.\n", "link": "http://arxiv.org/abs/2507.06814v1", "date": "2025-07-09", "relevancy": 2.2305, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5708}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5562}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HVI-CIDNet%2B%3A%20Beyond%20Extreme%20Darkness%20for%20Low-Light%20Image%20Enhancement&body=Title%3A%20HVI-CIDNet%2B%3A%20Beyond%20Extreme%20Darkness%20for%20Low-Light%20Image%20Enhancement%0AAuthor%3A%20Qingsen%20Yan%20and%20Kangbiao%20Shi%20and%20Yixu%20Feng%20and%20Tao%20Hu%20and%20Peng%20Wu%20and%20Guansong%20Pang%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20aims%20to%20restore%20vivid%20content%20and%20details%0Afrom%20corrupted%20low-light%20images.%20However%2C%20existing%20standard%20RGB%20%28sRGB%29%20color%0Aspace-based%20LLIE%20methods%20often%20produce%20color%20bias%20and%20brightness%20artifacts%20due%0Ato%20the%20inherent%20high%20color%20sensitivity.%20While%20Hue%2C%20Saturation%2C%20and%20Value%20%28HSV%29%0Acolor%20space%20can%20decouple%20brightness%20and%20color%2C%20it%20introduces%20significant%20red%0Aand%20black%20noise%20artifacts.%20To%20address%20this%20problem%2C%20we%20propose%20a%20new%20color%0Aspace%20for%20LLIE%2C%20namely%20Horizontal/Vertical-Intensity%20%28HVI%29%2C%20defined%20by%20the%20HV%0Acolor%20map%20and%20learnable%20intensity.%20The%20HV%20color%20map%20enforces%20small%20distances%0Afor%20the%20red%20coordinates%20to%20remove%20red%20noise%20artifacts%2C%20while%20the%20learnable%0Aintensity%20compresses%20the%20low-light%20regions%20to%20remove%20black%20noise%20artifacts.%0AAdditionally%2C%20we%20introduce%20the%20Color%20and%20Intensity%20Decoupling%20Network%2B%0A%28HVI-CIDNet%2B%29%2C%20built%20upon%20the%20HVI%20color%20space%2C%20to%20restore%20damaged%20content%20and%0Amitigate%20color%20distortion%20in%20extremely%20dark%20regions.%20Specifically%2C%20HVI-CIDNet%2B%0Aleverages%20abundant%20contextual%20and%20degraded%20knowledge%20extracted%20from%20low-light%0Aimages%20using%20pre-trained%20vision-language%20models%2C%20integrated%20via%20a%20novel%0APrior-guided%20Attention%20Block%20%28PAB%29.%20Within%20the%20PAB%2C%20latent%20semantic%20priors%20can%0Apromote%20content%20restoration%2C%20while%20degraded%20representations%20guide%20precise%20color%0Acorrection%2C%20both%20particularly%20in%20extremely%20dark%20regions%20through%20the%0Ameticulously%20designed%20cross-attention%20fusion%20mechanism.%20Furthermore%2C%20we%0Aconstruct%20a%20Region%20Refinement%20Block%20that%20employs%20convolution%20for%0Ainformation-rich%20regions%20and%20self-attention%20for%20information-scarce%20regions%2C%0Aensuring%20accurate%20brightness%20adjustments.%20Comprehensive%20results%20from%20benchmark%0Aexperiments%20demonstrate%20that%20the%20proposed%20HVI-CIDNet%2B%20outperforms%20the%0Astate-of-the-art%20methods%20on%2010%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHVI-CIDNet%252B%253A%2520Beyond%2520Extreme%2520Darkness%2520for%2520Low-Light%2520Image%2520Enhancement%26entry.906535625%3DQingsen%2520Yan%2520and%2520Kangbiao%2520Shi%2520and%2520Yixu%2520Feng%2520and%2520Tao%2520Hu%2520and%2520Peng%2520Wu%2520and%2520Guansong%2520Pang%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%2520Low-Light%2520Image%2520Enhancement%2520%2528LLIE%2529%2520aims%2520to%2520restore%2520vivid%2520content%2520and%2520details%250Afrom%2520corrupted%2520low-light%2520images.%2520However%252C%2520existing%2520standard%2520RGB%2520%2528sRGB%2529%2520color%250Aspace-based%2520LLIE%2520methods%2520often%2520produce%2520color%2520bias%2520and%2520brightness%2520artifacts%2520due%250Ato%2520the%2520inherent%2520high%2520color%2520sensitivity.%2520While%2520Hue%252C%2520Saturation%252C%2520and%2520Value%2520%2528HSV%2529%250Acolor%2520space%2520can%2520decouple%2520brightness%2520and%2520color%252C%2520it%2520introduces%2520significant%2520red%250Aand%2520black%2520noise%2520artifacts.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520new%2520color%250Aspace%2520for%2520LLIE%252C%2520namely%2520Horizontal/Vertical-Intensity%2520%2528HVI%2529%252C%2520defined%2520by%2520the%2520HV%250Acolor%2520map%2520and%2520learnable%2520intensity.%2520The%2520HV%2520color%2520map%2520enforces%2520small%2520distances%250Afor%2520the%2520red%2520coordinates%2520to%2520remove%2520red%2520noise%2520artifacts%252C%2520while%2520the%2520learnable%250Aintensity%2520compresses%2520the%2520low-light%2520regions%2520to%2520remove%2520black%2520noise%2520artifacts.%250AAdditionally%252C%2520we%2520introduce%2520the%2520Color%2520and%2520Intensity%2520Decoupling%2520Network%252B%250A%2528HVI-CIDNet%252B%2529%252C%2520built%2520upon%2520the%2520HVI%2520color%2520space%252C%2520to%2520restore%2520damaged%2520content%2520and%250Amitigate%2520color%2520distortion%2520in%2520extremely%2520dark%2520regions.%2520Specifically%252C%2520HVI-CIDNet%252B%250Aleverages%2520abundant%2520contextual%2520and%2520degraded%2520knowledge%2520extracted%2520from%2520low-light%250Aimages%2520using%2520pre-trained%2520vision-language%2520models%252C%2520integrated%2520via%2520a%2520novel%250APrior-guided%2520Attention%2520Block%2520%2528PAB%2529.%2520Within%2520the%2520PAB%252C%2520latent%2520semantic%2520priors%2520can%250Apromote%2520content%2520restoration%252C%2520while%2520degraded%2520representations%2520guide%2520precise%2520color%250Acorrection%252C%2520both%2520particularly%2520in%2520extremely%2520dark%2520regions%2520through%2520the%250Ameticulously%2520designed%2520cross-attention%2520fusion%2520mechanism.%2520Furthermore%252C%2520we%250Aconstruct%2520a%2520Region%2520Refinement%2520Block%2520that%2520employs%2520convolution%2520for%250Ainformation-rich%2520regions%2520and%2520self-attention%2520for%2520information-scarce%2520regions%252C%250Aensuring%2520accurate%2520brightness%2520adjustments.%2520Comprehensive%2520results%2520from%2520benchmark%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520HVI-CIDNet%252B%2520outperforms%2520the%250Astate-of-the-art%2520methods%2520on%252010%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HVI-CIDNet%2B%3A%20Beyond%20Extreme%20Darkness%20for%20Low-Light%20Image%20Enhancement&entry.906535625=Qingsen%20Yan%20and%20Kangbiao%20Shi%20and%20Yixu%20Feng%20and%20Tao%20Hu%20and%20Peng%20Wu%20and%20Guansong%20Pang%20and%20Yanning%20Zhang&entry.1292438233=%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20aims%20to%20restore%20vivid%20content%20and%20details%0Afrom%20corrupted%20low-light%20images.%20However%2C%20existing%20standard%20RGB%20%28sRGB%29%20color%0Aspace-based%20LLIE%20methods%20often%20produce%20color%20bias%20and%20brightness%20artifacts%20due%0Ato%20the%20inherent%20high%20color%20sensitivity.%20While%20Hue%2C%20Saturation%2C%20and%20Value%20%28HSV%29%0Acolor%20space%20can%20decouple%20brightness%20and%20color%2C%20it%20introduces%20significant%20red%0Aand%20black%20noise%20artifacts.%20To%20address%20this%20problem%2C%20we%20propose%20a%20new%20color%0Aspace%20for%20LLIE%2C%20namely%20Horizontal/Vertical-Intensity%20%28HVI%29%2C%20defined%20by%20the%20HV%0Acolor%20map%20and%20learnable%20intensity.%20The%20HV%20color%20map%20enforces%20small%20distances%0Afor%20the%20red%20coordinates%20to%20remove%20red%20noise%20artifacts%2C%20while%20the%20learnable%0Aintensity%20compresses%20the%20low-light%20regions%20to%20remove%20black%20noise%20artifacts.%0AAdditionally%2C%20we%20introduce%20the%20Color%20and%20Intensity%20Decoupling%20Network%2B%0A%28HVI-CIDNet%2B%29%2C%20built%20upon%20the%20HVI%20color%20space%2C%20to%20restore%20damaged%20content%20and%0Amitigate%20color%20distortion%20in%20extremely%20dark%20regions.%20Specifically%2C%20HVI-CIDNet%2B%0Aleverages%20abundant%20contextual%20and%20degraded%20knowledge%20extracted%20from%20low-light%0Aimages%20using%20pre-trained%20vision-language%20models%2C%20integrated%20via%20a%20novel%0APrior-guided%20Attention%20Block%20%28PAB%29.%20Within%20the%20PAB%2C%20latent%20semantic%20priors%20can%0Apromote%20content%20restoration%2C%20while%20degraded%20representations%20guide%20precise%20color%0Acorrection%2C%20both%20particularly%20in%20extremely%20dark%20regions%20through%20the%0Ameticulously%20designed%20cross-attention%20fusion%20mechanism.%20Furthermore%2C%20we%0Aconstruct%20a%20Region%20Refinement%20Block%20that%20employs%20convolution%20for%0Ainformation-rich%20regions%20and%20self-attention%20for%20information-scarce%20regions%2C%0Aensuring%20accurate%20brightness%20adjustments.%20Comprehensive%20results%20from%20benchmark%0Aexperiments%20demonstrate%20that%20the%20proposed%20HVI-CIDNet%2B%20outperforms%20the%0Astate-of-the-art%20methods%20on%2010%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06814v1&entry.124074799=Read"},
{"title": "Artificial Generals Intelligence: Mastering Generals.io with\n  Reinforcement Learning", "author": "Matej Straka and Martin Schmid", "abstract": "  We introduce a real-time strategy game environment built on Generals.io, a\ngame that hosts thousands of active players each week across multiple game\nformats. Our environment is fully compatible with Gymnasium and PettingZoo,\ncapable of running thousands of frames per second on commodity hardware. Our\nreference agent -- trained with supervised pre-training and self-play -- hits\nthe top 0.003\\% of the 1v1 human leaderboard after just 36 hours on a single\nH100 GPU. To accelerate learning, we incorporate potential-based reward shaping\nand memory features. Our contributions -- a modular RTS benchmark and a\ncompetitive, state-of-the-art baseline agent -- provide an accessible yet\nchallenging platform for advancing multi-agent reinforcement learning research.\n", "link": "http://arxiv.org/abs/2507.06825v1", "date": "2025-07-09", "relevancy": 2.2268, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5914}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5399}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Generals%20Intelligence%3A%20Mastering%20Generals.io%20with%0A%20%20Reinforcement%20Learning&body=Title%3A%20Artificial%20Generals%20Intelligence%3A%20Mastering%20Generals.io%20with%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Matej%20Straka%20and%20Martin%20Schmid%0AAbstract%3A%20%20%20We%20introduce%20a%20real-time%20strategy%20game%20environment%20built%20on%20Generals.io%2C%20a%0Agame%20that%20hosts%20thousands%20of%20active%20players%20each%20week%20across%20multiple%20game%0Aformats.%20Our%20environment%20is%20fully%20compatible%20with%20Gymnasium%20and%20PettingZoo%2C%0Acapable%20of%20running%20thousands%20of%20frames%20per%20second%20on%20commodity%20hardware.%20Our%0Areference%20agent%20--%20trained%20with%20supervised%20pre-training%20and%20self-play%20--%20hits%0Athe%20top%200.003%5C%25%20of%20the%201v1%20human%20leaderboard%20after%20just%2036%20hours%20on%20a%20single%0AH100%20GPU.%20To%20accelerate%20learning%2C%20we%20incorporate%20potential-based%20reward%20shaping%0Aand%20memory%20features.%20Our%20contributions%20--%20a%20modular%20RTS%20benchmark%20and%20a%0Acompetitive%2C%20state-of-the-art%20baseline%20agent%20--%20provide%20an%20accessible%20yet%0Achallenging%20platform%20for%20advancing%20multi-agent%20reinforcement%20learning%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Generals%2520Intelligence%253A%2520Mastering%2520Generals.io%2520with%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DMatej%2520Straka%2520and%2520Martin%2520Schmid%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520real-time%2520strategy%2520game%2520environment%2520built%2520on%2520Generals.io%252C%2520a%250Agame%2520that%2520hosts%2520thousands%2520of%2520active%2520players%2520each%2520week%2520across%2520multiple%2520game%250Aformats.%2520Our%2520environment%2520is%2520fully%2520compatible%2520with%2520Gymnasium%2520and%2520PettingZoo%252C%250Acapable%2520of%2520running%2520thousands%2520of%2520frames%2520per%2520second%2520on%2520commodity%2520hardware.%2520Our%250Areference%2520agent%2520--%2520trained%2520with%2520supervised%2520pre-training%2520and%2520self-play%2520--%2520hits%250Athe%2520top%25200.003%255C%2525%2520of%2520the%25201v1%2520human%2520leaderboard%2520after%2520just%252036%2520hours%2520on%2520a%2520single%250AH100%2520GPU.%2520To%2520accelerate%2520learning%252C%2520we%2520incorporate%2520potential-based%2520reward%2520shaping%250Aand%2520memory%2520features.%2520Our%2520contributions%2520--%2520a%2520modular%2520RTS%2520benchmark%2520and%2520a%250Acompetitive%252C%2520state-of-the-art%2520baseline%2520agent%2520--%2520provide%2520an%2520accessible%2520yet%250Achallenging%2520platform%2520for%2520advancing%2520multi-agent%2520reinforcement%2520learning%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Generals%20Intelligence%3A%20Mastering%20Generals.io%20with%0A%20%20Reinforcement%20Learning&entry.906535625=Matej%20Straka%20and%20Martin%20Schmid&entry.1292438233=%20%20We%20introduce%20a%20real-time%20strategy%20game%20environment%20built%20on%20Generals.io%2C%20a%0Agame%20that%20hosts%20thousands%20of%20active%20players%20each%20week%20across%20multiple%20game%0Aformats.%20Our%20environment%20is%20fully%20compatible%20with%20Gymnasium%20and%20PettingZoo%2C%0Acapable%20of%20running%20thousands%20of%20frames%20per%20second%20on%20commodity%20hardware.%20Our%0Areference%20agent%20--%20trained%20with%20supervised%20pre-training%20and%20self-play%20--%20hits%0Athe%20top%200.003%5C%25%20of%20the%201v1%20human%20leaderboard%20after%20just%2036%20hours%20on%20a%20single%0AH100%20GPU.%20To%20accelerate%20learning%2C%20we%20incorporate%20potential-based%20reward%20shaping%0Aand%20memory%20features.%20Our%20contributions%20--%20a%20modular%20RTS%20benchmark%20and%20a%0Acompetitive%2C%20state-of-the-art%20baseline%20agent%20--%20provide%20an%20accessible%20yet%0Achallenging%20platform%20for%20advancing%20multi-agent%20reinforcement%20learning%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06825v1&entry.124074799=Read"},
{"title": "DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via\n  Joint LiDAR-Based 3D Object Detection and Denoising", "author": "Sven Teufel and Dominique Mayer and J\u00f6rg Gamerdinger and Oliver Bringmann", "abstract": "  While automated vehicles hold the potential to significantly reduce traffic\naccidents, their perception systems remain vulnerable to sensor degradation\ncaused by adverse weather and environmental occlusions. Collective perception,\nwhich enables vehicles to share information, offers a promising approach to\novercoming these limitations. However, to this date collective perception in\nadverse weather is mostly unstudied. Therefore, we conduct the first study of\nLiDAR-based collective perception under diverse weather conditions and present\na novel multi-task architecture for LiDAR-based collective perception under\nadverse weather. Adverse weather conditions can not only degrade perception\ncapabilities, but also negatively affect bandwidth requirements and latency due\nto the introduced noise that is also transmitted and processed. Denoising prior\nto communication can effectively mitigate these issues. Therefore, we propose\nDenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective\nperception under adverse weather conditions. DenoiseCP-Net integrates\nvoxel-level noise filtering and object detection into a unified sparse\nconvolution backbone, eliminating redundant computations associated with\ntwo-stage pipelines. This design not only reduces inference latency and\ncomputational cost but also minimizes communication overhead by removing\nnon-informative noise. We extended the well-known OPV2V dataset by simulating\nrain, snow, and fog using our realistic weather simulation models. We\ndemonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in\nadverse weather, reduces the bandwidth requirements by up to 23.6% while\nmaintaining the same detection accuracy and reducing the inference latency for\ncooperative vehicles.\n", "link": "http://arxiv.org/abs/2507.06976v1", "date": "2025-07-09", "relevancy": 2.2257, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5953}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5331}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DenoiseCP-Net%3A%20Efficient%20Collective%20Perception%20in%20Adverse%20Weather%20via%0A%20%20Joint%20LiDAR-Based%203D%20Object%20Detection%20and%20Denoising&body=Title%3A%20DenoiseCP-Net%3A%20Efficient%20Collective%20Perception%20in%20Adverse%20Weather%20via%0A%20%20Joint%20LiDAR-Based%203D%20Object%20Detection%20and%20Denoising%0AAuthor%3A%20Sven%20Teufel%20and%20Dominique%20Mayer%20and%20J%C3%B6rg%20Gamerdinger%20and%20Oliver%20Bringmann%0AAbstract%3A%20%20%20While%20automated%20vehicles%20hold%20the%20potential%20to%20significantly%20reduce%20traffic%0Aaccidents%2C%20their%20perception%20systems%20remain%20vulnerable%20to%20sensor%20degradation%0Acaused%20by%20adverse%20weather%20and%20environmental%20occlusions.%20Collective%20perception%2C%0Awhich%20enables%20vehicles%20to%20share%20information%2C%20offers%20a%20promising%20approach%20to%0Aovercoming%20these%20limitations.%20However%2C%20to%20this%20date%20collective%20perception%20in%0Aadverse%20weather%20is%20mostly%20unstudied.%20Therefore%2C%20we%20conduct%20the%20first%20study%20of%0ALiDAR-based%20collective%20perception%20under%20diverse%20weather%20conditions%20and%20present%0Aa%20novel%20multi-task%20architecture%20for%20LiDAR-based%20collective%20perception%20under%0Aadverse%20weather.%20Adverse%20weather%20conditions%20can%20not%20only%20degrade%20perception%0Acapabilities%2C%20but%20also%20negatively%20affect%20bandwidth%20requirements%20and%20latency%20due%0Ato%20the%20introduced%20noise%20that%20is%20also%20transmitted%20and%20processed.%20Denoising%20prior%0Ato%20communication%20can%20effectively%20mitigate%20these%20issues.%20Therefore%2C%20we%20propose%0ADenoiseCP-Net%2C%20a%20novel%20multi-task%20architecture%20for%20LiDAR-based%20collective%0Aperception%20under%20adverse%20weather%20conditions.%20DenoiseCP-Net%20integrates%0Avoxel-level%20noise%20filtering%20and%20object%20detection%20into%20a%20unified%20sparse%0Aconvolution%20backbone%2C%20eliminating%20redundant%20computations%20associated%20with%0Atwo-stage%20pipelines.%20This%20design%20not%20only%20reduces%20inference%20latency%20and%0Acomputational%20cost%20but%20also%20minimizes%20communication%20overhead%20by%20removing%0Anon-informative%20noise.%20We%20extended%20the%20well-known%20OPV2V%20dataset%20by%20simulating%0Arain%2C%20snow%2C%20and%20fog%20using%20our%20realistic%20weather%20simulation%20models.%20We%0Ademonstrate%20that%20DenoiseCP-Net%20achieves%20near-perfect%20denoising%20accuracy%20in%0Aadverse%20weather%2C%20reduces%20the%20bandwidth%20requirements%20by%20up%20to%2023.6%25%20while%0Amaintaining%20the%20same%20detection%20accuracy%20and%20reducing%20the%20inference%20latency%20for%0Acooperative%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoiseCP-Net%253A%2520Efficient%2520Collective%2520Perception%2520in%2520Adverse%2520Weather%2520via%250A%2520%2520Joint%2520LiDAR-Based%25203D%2520Object%2520Detection%2520and%2520Denoising%26entry.906535625%3DSven%2520Teufel%2520and%2520Dominique%2520Mayer%2520and%2520J%25C3%25B6rg%2520Gamerdinger%2520and%2520Oliver%2520Bringmann%26entry.1292438233%3D%2520%2520While%2520automated%2520vehicles%2520hold%2520the%2520potential%2520to%2520significantly%2520reduce%2520traffic%250Aaccidents%252C%2520their%2520perception%2520systems%2520remain%2520vulnerable%2520to%2520sensor%2520degradation%250Acaused%2520by%2520adverse%2520weather%2520and%2520environmental%2520occlusions.%2520Collective%2520perception%252C%250Awhich%2520enables%2520vehicles%2520to%2520share%2520information%252C%2520offers%2520a%2520promising%2520approach%2520to%250Aovercoming%2520these%2520limitations.%2520However%252C%2520to%2520this%2520date%2520collective%2520perception%2520in%250Aadverse%2520weather%2520is%2520mostly%2520unstudied.%2520Therefore%252C%2520we%2520conduct%2520the%2520first%2520study%2520of%250ALiDAR-based%2520collective%2520perception%2520under%2520diverse%2520weather%2520conditions%2520and%2520present%250Aa%2520novel%2520multi-task%2520architecture%2520for%2520LiDAR-based%2520collective%2520perception%2520under%250Aadverse%2520weather.%2520Adverse%2520weather%2520conditions%2520can%2520not%2520only%2520degrade%2520perception%250Acapabilities%252C%2520but%2520also%2520negatively%2520affect%2520bandwidth%2520requirements%2520and%2520latency%2520due%250Ato%2520the%2520introduced%2520noise%2520that%2520is%2520also%2520transmitted%2520and%2520processed.%2520Denoising%2520prior%250Ato%2520communication%2520can%2520effectively%2520mitigate%2520these%2520issues.%2520Therefore%252C%2520we%2520propose%250ADenoiseCP-Net%252C%2520a%2520novel%2520multi-task%2520architecture%2520for%2520LiDAR-based%2520collective%250Aperception%2520under%2520adverse%2520weather%2520conditions.%2520DenoiseCP-Net%2520integrates%250Avoxel-level%2520noise%2520filtering%2520and%2520object%2520detection%2520into%2520a%2520unified%2520sparse%250Aconvolution%2520backbone%252C%2520eliminating%2520redundant%2520computations%2520associated%2520with%250Atwo-stage%2520pipelines.%2520This%2520design%2520not%2520only%2520reduces%2520inference%2520latency%2520and%250Acomputational%2520cost%2520but%2520also%2520minimizes%2520communication%2520overhead%2520by%2520removing%250Anon-informative%2520noise.%2520We%2520extended%2520the%2520well-known%2520OPV2V%2520dataset%2520by%2520simulating%250Arain%252C%2520snow%252C%2520and%2520fog%2520using%2520our%2520realistic%2520weather%2520simulation%2520models.%2520We%250Ademonstrate%2520that%2520DenoiseCP-Net%2520achieves%2520near-perfect%2520denoising%2520accuracy%2520in%250Aadverse%2520weather%252C%2520reduces%2520the%2520bandwidth%2520requirements%2520by%2520up%2520to%252023.6%2525%2520while%250Amaintaining%2520the%2520same%2520detection%2520accuracy%2520and%2520reducing%2520the%2520inference%2520latency%2520for%250Acooperative%2520vehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenoiseCP-Net%3A%20Efficient%20Collective%20Perception%20in%20Adverse%20Weather%20via%0A%20%20Joint%20LiDAR-Based%203D%20Object%20Detection%20and%20Denoising&entry.906535625=Sven%20Teufel%20and%20Dominique%20Mayer%20and%20J%C3%B6rg%20Gamerdinger%20and%20Oliver%20Bringmann&entry.1292438233=%20%20While%20automated%20vehicles%20hold%20the%20potential%20to%20significantly%20reduce%20traffic%0Aaccidents%2C%20their%20perception%20systems%20remain%20vulnerable%20to%20sensor%20degradation%0Acaused%20by%20adverse%20weather%20and%20environmental%20occlusions.%20Collective%20perception%2C%0Awhich%20enables%20vehicles%20to%20share%20information%2C%20offers%20a%20promising%20approach%20to%0Aovercoming%20these%20limitations.%20However%2C%20to%20this%20date%20collective%20perception%20in%0Aadverse%20weather%20is%20mostly%20unstudied.%20Therefore%2C%20we%20conduct%20the%20first%20study%20of%0ALiDAR-based%20collective%20perception%20under%20diverse%20weather%20conditions%20and%20present%0Aa%20novel%20multi-task%20architecture%20for%20LiDAR-based%20collective%20perception%20under%0Aadverse%20weather.%20Adverse%20weather%20conditions%20can%20not%20only%20degrade%20perception%0Acapabilities%2C%20but%20also%20negatively%20affect%20bandwidth%20requirements%20and%20latency%20due%0Ato%20the%20introduced%20noise%20that%20is%20also%20transmitted%20and%20processed.%20Denoising%20prior%0Ato%20communication%20can%20effectively%20mitigate%20these%20issues.%20Therefore%2C%20we%20propose%0ADenoiseCP-Net%2C%20a%20novel%20multi-task%20architecture%20for%20LiDAR-based%20collective%0Aperception%20under%20adverse%20weather%20conditions.%20DenoiseCP-Net%20integrates%0Avoxel-level%20noise%20filtering%20and%20object%20detection%20into%20a%20unified%20sparse%0Aconvolution%20backbone%2C%20eliminating%20redundant%20computations%20associated%20with%0Atwo-stage%20pipelines.%20This%20design%20not%20only%20reduces%20inference%20latency%20and%0Acomputational%20cost%20but%20also%20minimizes%20communication%20overhead%20by%20removing%0Anon-informative%20noise.%20We%20extended%20the%20well-known%20OPV2V%20dataset%20by%20simulating%0Arain%2C%20snow%2C%20and%20fog%20using%20our%20realistic%20weather%20simulation%20models.%20We%0Ademonstrate%20that%20DenoiseCP-Net%20achieves%20near-perfect%20denoising%20accuracy%20in%0Aadverse%20weather%2C%20reduces%20the%20bandwidth%20requirements%20by%20up%20to%2023.6%25%20while%0Amaintaining%20the%20same%20detection%20accuracy%20and%20reducing%20the%20inference%20latency%20for%0Acooperative%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06976v1&entry.124074799=Read"},
{"title": "Geometry-Informed Neural Operator Transformer", "author": "Qibang Liu and Weiheng Zhong and Hadi Meidani and Diab Abueidda and Seid Koric and Philippe Geubelle", "abstract": "  Machine-learning-based surrogate models offer significant computational\nefficiency and faster simulations compared to traditional numerical methods,\nespecially for problems requiring repeated evaluations of partial differential\nequations. This work introduces the Geometry-Informed Neural Operator\nTransformer (GINOT), which integrates the transformer architecture with the\nneural operator framework to enable forward predictions on arbitrary\ngeometries. GINOT employs a sampling and grouping strategy together with an\nattention mechanism to encode surface point clouds that are unordered, exhibit\nnon-uniform point densities, and contain varying numbers of points for\ndifferent geometries. The geometry information is seamlessly integrated with\nquery points in the solution decoder through the attention mechanism. The\nperformance of GINOT is validated on multiple challenging datasets, showcasing\nits high accuracy and strong generalization capabilities for complex and\narbitrary 2D and 3D geometries.\n", "link": "http://arxiv.org/abs/2504.19452v4", "date": "2025-07-09", "relevancy": 2.2127, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.562}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5566}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Informed%20Neural%20Operator%20Transformer&body=Title%3A%20Geometry-Informed%20Neural%20Operator%20Transformer%0AAuthor%3A%20Qibang%20Liu%20and%20Weiheng%20Zhong%20and%20Hadi%20Meidani%20and%20Diab%20Abueidda%20and%20Seid%20Koric%20and%20Philippe%20Geubelle%0AAbstract%3A%20%20%20Machine-learning-based%20surrogate%20models%20offer%20significant%20computational%0Aefficiency%20and%20faster%20simulations%20compared%20to%20traditional%20numerical%20methods%2C%0Aespecially%20for%20problems%20requiring%20repeated%20evaluations%20of%20partial%20differential%0Aequations.%20This%20work%20introduces%20the%20Geometry-Informed%20Neural%20Operator%0ATransformer%20%28GINOT%29%2C%20which%20integrates%20the%20transformer%20architecture%20with%20the%0Aneural%20operator%20framework%20to%20enable%20forward%20predictions%20on%20arbitrary%0Ageometries.%20GINOT%20employs%20a%20sampling%20and%20grouping%20strategy%20together%20with%20an%0Aattention%20mechanism%20to%20encode%20surface%20point%20clouds%20that%20are%20unordered%2C%20exhibit%0Anon-uniform%20point%20densities%2C%20and%20contain%20varying%20numbers%20of%20points%20for%0Adifferent%20geometries.%20The%20geometry%20information%20is%20seamlessly%20integrated%20with%0Aquery%20points%20in%20the%20solution%20decoder%20through%20the%20attention%20mechanism.%20The%0Aperformance%20of%20GINOT%20is%20validated%20on%20multiple%20challenging%20datasets%2C%20showcasing%0Aits%20high%20accuracy%20and%20strong%20generalization%20capabilities%20for%20complex%20and%0Aarbitrary%202D%20and%203D%20geometries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19452v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Informed%2520Neural%2520Operator%2520Transformer%26entry.906535625%3DQibang%2520Liu%2520and%2520Weiheng%2520Zhong%2520and%2520Hadi%2520Meidani%2520and%2520Diab%2520Abueidda%2520and%2520Seid%2520Koric%2520and%2520Philippe%2520Geubelle%26entry.1292438233%3D%2520%2520Machine-learning-based%2520surrogate%2520models%2520offer%2520significant%2520computational%250Aefficiency%2520and%2520faster%2520simulations%2520compared%2520to%2520traditional%2520numerical%2520methods%252C%250Aespecially%2520for%2520problems%2520requiring%2520repeated%2520evaluations%2520of%2520partial%2520differential%250Aequations.%2520This%2520work%2520introduces%2520the%2520Geometry-Informed%2520Neural%2520Operator%250ATransformer%2520%2528GINOT%2529%252C%2520which%2520integrates%2520the%2520transformer%2520architecture%2520with%2520the%250Aneural%2520operator%2520framework%2520to%2520enable%2520forward%2520predictions%2520on%2520arbitrary%250Ageometries.%2520GINOT%2520employs%2520a%2520sampling%2520and%2520grouping%2520strategy%2520together%2520with%2520an%250Aattention%2520mechanism%2520to%2520encode%2520surface%2520point%2520clouds%2520that%2520are%2520unordered%252C%2520exhibit%250Anon-uniform%2520point%2520densities%252C%2520and%2520contain%2520varying%2520numbers%2520of%2520points%2520for%250Adifferent%2520geometries.%2520The%2520geometry%2520information%2520is%2520seamlessly%2520integrated%2520with%250Aquery%2520points%2520in%2520the%2520solution%2520decoder%2520through%2520the%2520attention%2520mechanism.%2520The%250Aperformance%2520of%2520GINOT%2520is%2520validated%2520on%2520multiple%2520challenging%2520datasets%252C%2520showcasing%250Aits%2520high%2520accuracy%2520and%2520strong%2520generalization%2520capabilities%2520for%2520complex%2520and%250Aarbitrary%25202D%2520and%25203D%2520geometries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19452v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Informed%20Neural%20Operator%20Transformer&entry.906535625=Qibang%20Liu%20and%20Weiheng%20Zhong%20and%20Hadi%20Meidani%20and%20Diab%20Abueidda%20and%20Seid%20Koric%20and%20Philippe%20Geubelle&entry.1292438233=%20%20Machine-learning-based%20surrogate%20models%20offer%20significant%20computational%0Aefficiency%20and%20faster%20simulations%20compared%20to%20traditional%20numerical%20methods%2C%0Aespecially%20for%20problems%20requiring%20repeated%20evaluations%20of%20partial%20differential%0Aequations.%20This%20work%20introduces%20the%20Geometry-Informed%20Neural%20Operator%0ATransformer%20%28GINOT%29%2C%20which%20integrates%20the%20transformer%20architecture%20with%20the%0Aneural%20operator%20framework%20to%20enable%20forward%20predictions%20on%20arbitrary%0Ageometries.%20GINOT%20employs%20a%20sampling%20and%20grouping%20strategy%20together%20with%20an%0Aattention%20mechanism%20to%20encode%20surface%20point%20clouds%20that%20are%20unordered%2C%20exhibit%0Anon-uniform%20point%20densities%2C%20and%20contain%20varying%20numbers%20of%20points%20for%0Adifferent%20geometries.%20The%20geometry%20information%20is%20seamlessly%20integrated%20with%0Aquery%20points%20in%20the%20solution%20decoder%20through%20the%20attention%20mechanism.%20The%0Aperformance%20of%20GINOT%20is%20validated%20on%20multiple%20challenging%20datasets%2C%20showcasing%0Aits%20high%20accuracy%20and%20strong%20generalization%20capabilities%20for%20complex%20and%0Aarbitrary%202D%20and%203D%20geometries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19452v4&entry.124074799=Read"},
{"title": "Reading a Ruler in the Wild", "author": "Yimu Pan and Manas Mehta and Gwen Sincerbeaux and Jeffery A. Goldstein and Alison D. Gernand and James Z. Wang", "abstract": "  Accurately converting pixel measurements into absolute real-world dimensions\nremains a fundamental challenge in computer vision and limits progress in key\napplications such as biomedicine, forensics, nutritional analysis, and\ne-commerce. We introduce RulerNet, a deep learning framework that robustly\ninfers scale \"in the wild\" by reformulating ruler reading as a unified\nkeypoint-detection problem and by representing the ruler with\ngeometric-progression parameters that are invariant to perspective\ntransformations. Unlike traditional methods that rely on handcrafted thresholds\nor rigid, ruler-specific pipelines, RulerNet directly localizes centimeter\nmarks using a distortion-invariant annotation and training strategy, enabling\nstrong generalization across diverse ruler types and imaging conditions while\nmitigating data scarcity. We also present a scalable synthetic-data pipeline\nthat combines graphics-based ruler generation with ControlNet to add\nphotorealistic context, greatly increasing training diversity and improving\nperformance. To further enhance robustness and efficiency, we propose DeepGP, a\nlightweight feed-forward network that regresses geometric-progression\nparameters from noisy marks and eliminates iterative optimization, enabling\nreal-time scale estimation on mobile or edge devices. Experiments show that\nRulerNet delivers accurate, consistent, and efficient scale estimates under\nchallenging real-world conditions. These results underscore its utility as a\ngeneralizable measurement tool and its potential for integration with other\nvision components for automated, scale-aware analysis in high-impact domains. A\nlive demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.\n", "link": "http://arxiv.org/abs/2507.07077v1", "date": "2025-07-09", "relevancy": 2.1661, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5522}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5396}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reading%20a%20Ruler%20in%20the%20Wild&body=Title%3A%20Reading%20a%20Ruler%20in%20the%20Wild%0AAuthor%3A%20Yimu%20Pan%20and%20Manas%20Mehta%20and%20Gwen%20Sincerbeaux%20and%20Jeffery%20A.%20Goldstein%20and%20Alison%20D.%20Gernand%20and%20James%20Z.%20Wang%0AAbstract%3A%20%20%20Accurately%20converting%20pixel%20measurements%20into%20absolute%20real-world%20dimensions%0Aremains%20a%20fundamental%20challenge%20in%20computer%20vision%20and%20limits%20progress%20in%20key%0Aapplications%20such%20as%20biomedicine%2C%20forensics%2C%20nutritional%20analysis%2C%20and%0Ae-commerce.%20We%20introduce%20RulerNet%2C%20a%20deep%20learning%20framework%20that%20robustly%0Ainfers%20scale%20%22in%20the%20wild%22%20by%20reformulating%20ruler%20reading%20as%20a%20unified%0Akeypoint-detection%20problem%20and%20by%20representing%20the%20ruler%20with%0Ageometric-progression%20parameters%20that%20are%20invariant%20to%20perspective%0Atransformations.%20Unlike%20traditional%20methods%20that%20rely%20on%20handcrafted%20thresholds%0Aor%20rigid%2C%20ruler-specific%20pipelines%2C%20RulerNet%20directly%20localizes%20centimeter%0Amarks%20using%20a%20distortion-invariant%20annotation%20and%20training%20strategy%2C%20enabling%0Astrong%20generalization%20across%20diverse%20ruler%20types%20and%20imaging%20conditions%20while%0Amitigating%20data%20scarcity.%20We%20also%20present%20a%20scalable%20synthetic-data%20pipeline%0Athat%20combines%20graphics-based%20ruler%20generation%20with%20ControlNet%20to%20add%0Aphotorealistic%20context%2C%20greatly%20increasing%20training%20diversity%20and%20improving%0Aperformance.%20To%20further%20enhance%20robustness%20and%20efficiency%2C%20we%20propose%20DeepGP%2C%20a%0Alightweight%20feed-forward%20network%20that%20regresses%20geometric-progression%0Aparameters%20from%20noisy%20marks%20and%20eliminates%20iterative%20optimization%2C%20enabling%0Areal-time%20scale%20estimation%20on%20mobile%20or%20edge%20devices.%20Experiments%20show%20that%0ARulerNet%20delivers%20accurate%2C%20consistent%2C%20and%20efficient%20scale%20estimates%20under%0Achallenging%20real-world%20conditions.%20These%20results%20underscore%20its%20utility%20as%20a%0Ageneralizable%20measurement%20tool%20and%20its%20potential%20for%20integration%20with%20other%0Avision%20components%20for%20automated%2C%20scale-aware%20analysis%20in%20high-impact%20domains.%20A%0Alive%20demo%20is%20available%20at%20https%3A//huggingface.co/spaces/ymp5078/RulerNet-Demo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReading%2520a%2520Ruler%2520in%2520the%2520Wild%26entry.906535625%3DYimu%2520Pan%2520and%2520Manas%2520Mehta%2520and%2520Gwen%2520Sincerbeaux%2520and%2520Jeffery%2520A.%2520Goldstein%2520and%2520Alison%2520D.%2520Gernand%2520and%2520James%2520Z.%2520Wang%26entry.1292438233%3D%2520%2520Accurately%2520converting%2520pixel%2520measurements%2520into%2520absolute%2520real-world%2520dimensions%250Aremains%2520a%2520fundamental%2520challenge%2520in%2520computer%2520vision%2520and%2520limits%2520progress%2520in%2520key%250Aapplications%2520such%2520as%2520biomedicine%252C%2520forensics%252C%2520nutritional%2520analysis%252C%2520and%250Ae-commerce.%2520We%2520introduce%2520RulerNet%252C%2520a%2520deep%2520learning%2520framework%2520that%2520robustly%250Ainfers%2520scale%2520%2522in%2520the%2520wild%2522%2520by%2520reformulating%2520ruler%2520reading%2520as%2520a%2520unified%250Akeypoint-detection%2520problem%2520and%2520by%2520representing%2520the%2520ruler%2520with%250Ageometric-progression%2520parameters%2520that%2520are%2520invariant%2520to%2520perspective%250Atransformations.%2520Unlike%2520traditional%2520methods%2520that%2520rely%2520on%2520handcrafted%2520thresholds%250Aor%2520rigid%252C%2520ruler-specific%2520pipelines%252C%2520RulerNet%2520directly%2520localizes%2520centimeter%250Amarks%2520using%2520a%2520distortion-invariant%2520annotation%2520and%2520training%2520strategy%252C%2520enabling%250Astrong%2520generalization%2520across%2520diverse%2520ruler%2520types%2520and%2520imaging%2520conditions%2520while%250Amitigating%2520data%2520scarcity.%2520We%2520also%2520present%2520a%2520scalable%2520synthetic-data%2520pipeline%250Athat%2520combines%2520graphics-based%2520ruler%2520generation%2520with%2520ControlNet%2520to%2520add%250Aphotorealistic%2520context%252C%2520greatly%2520increasing%2520training%2520diversity%2520and%2520improving%250Aperformance.%2520To%2520further%2520enhance%2520robustness%2520and%2520efficiency%252C%2520we%2520propose%2520DeepGP%252C%2520a%250Alightweight%2520feed-forward%2520network%2520that%2520regresses%2520geometric-progression%250Aparameters%2520from%2520noisy%2520marks%2520and%2520eliminates%2520iterative%2520optimization%252C%2520enabling%250Areal-time%2520scale%2520estimation%2520on%2520mobile%2520or%2520edge%2520devices.%2520Experiments%2520show%2520that%250ARulerNet%2520delivers%2520accurate%252C%2520consistent%252C%2520and%2520efficient%2520scale%2520estimates%2520under%250Achallenging%2520real-world%2520conditions.%2520These%2520results%2520underscore%2520its%2520utility%2520as%2520a%250Ageneralizable%2520measurement%2520tool%2520and%2520its%2520potential%2520for%2520integration%2520with%2520other%250Avision%2520components%2520for%2520automated%252C%2520scale-aware%2520analysis%2520in%2520high-impact%2520domains.%2520A%250Alive%2520demo%2520is%2520available%2520at%2520https%253A//huggingface.co/spaces/ymp5078/RulerNet-Demo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reading%20a%20Ruler%20in%20the%20Wild&entry.906535625=Yimu%20Pan%20and%20Manas%20Mehta%20and%20Gwen%20Sincerbeaux%20and%20Jeffery%20A.%20Goldstein%20and%20Alison%20D.%20Gernand%20and%20James%20Z.%20Wang&entry.1292438233=%20%20Accurately%20converting%20pixel%20measurements%20into%20absolute%20real-world%20dimensions%0Aremains%20a%20fundamental%20challenge%20in%20computer%20vision%20and%20limits%20progress%20in%20key%0Aapplications%20such%20as%20biomedicine%2C%20forensics%2C%20nutritional%20analysis%2C%20and%0Ae-commerce.%20We%20introduce%20RulerNet%2C%20a%20deep%20learning%20framework%20that%20robustly%0Ainfers%20scale%20%22in%20the%20wild%22%20by%20reformulating%20ruler%20reading%20as%20a%20unified%0Akeypoint-detection%20problem%20and%20by%20representing%20the%20ruler%20with%0Ageometric-progression%20parameters%20that%20are%20invariant%20to%20perspective%0Atransformations.%20Unlike%20traditional%20methods%20that%20rely%20on%20handcrafted%20thresholds%0Aor%20rigid%2C%20ruler-specific%20pipelines%2C%20RulerNet%20directly%20localizes%20centimeter%0Amarks%20using%20a%20distortion-invariant%20annotation%20and%20training%20strategy%2C%20enabling%0Astrong%20generalization%20across%20diverse%20ruler%20types%20and%20imaging%20conditions%20while%0Amitigating%20data%20scarcity.%20We%20also%20present%20a%20scalable%20synthetic-data%20pipeline%0Athat%20combines%20graphics-based%20ruler%20generation%20with%20ControlNet%20to%20add%0Aphotorealistic%20context%2C%20greatly%20increasing%20training%20diversity%20and%20improving%0Aperformance.%20To%20further%20enhance%20robustness%20and%20efficiency%2C%20we%20propose%20DeepGP%2C%20a%0Alightweight%20feed-forward%20network%20that%20regresses%20geometric-progression%0Aparameters%20from%20noisy%20marks%20and%20eliminates%20iterative%20optimization%2C%20enabling%0Areal-time%20scale%20estimation%20on%20mobile%20or%20edge%20devices.%20Experiments%20show%20that%0ARulerNet%20delivers%20accurate%2C%20consistent%2C%20and%20efficient%20scale%20estimates%20under%0Achallenging%20real-world%20conditions.%20These%20results%20underscore%20its%20utility%20as%20a%0Ageneralizable%20measurement%20tool%20and%20its%20potential%20for%20integration%20with%20other%0Avision%20components%20for%20automated%2C%20scale-aware%20analysis%20in%20high-impact%20domains.%20A%0Alive%20demo%20is%20available%20at%20https%3A//huggingface.co/spaces/ymp5078/RulerNet-Demo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07077v1&entry.124074799=Read"},
{"title": "VQ-SGen: A Vector Quantized Stroke Representation for Creative Sketch\n  Generation", "author": "Jiawei Wang and Zhiming Cui and Changjian Li", "abstract": "  This paper presents VQ-SGen, a novel algorithm for high-quality creative\nsketch generation. Recent approaches have framed the task as pixel-based\ngeneration either as a whole or part-by-part, neglecting the intrinsic and\ncontextual relationships among individual strokes, such as the shape and\nspatial positioning of both proximal and distant strokes. To overcome these\nlimitations, we propose treating each stroke within a sketch as an entity and\nintroducing a vector-quantized (VQ) stroke representation for fine-grained\nsketch generation. Our method follows a two-stage framework - in stage one, we\ndecouple each stroke's shape and location information to ensure the VQ\nrepresentation prioritizes stroke shape learning. In stage two, we feed the\nprecise and compact representation into an auto-decoding Transformer to\nincorporate stroke semantics, positions, and shapes into the generation\nprocess. By utilizing tokenized stroke representation, our approach generates\nstrokes with high fidelity and facilitates novel applications, such as text or\nclass label conditioned generation and sketch completion. Comprehensive\nexperiments demonstrate our method surpasses existing state-of-the-art\ntechniques on the CreativeSketch dataset, underscoring its effectiveness.\n", "link": "http://arxiv.org/abs/2411.16446v3", "date": "2025-07-09", "relevancy": 2.1626, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5412}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5407}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VQ-SGen%3A%20A%20Vector%20Quantized%20Stroke%20Representation%20for%20Creative%20Sketch%0A%20%20Generation&body=Title%3A%20VQ-SGen%3A%20A%20Vector%20Quantized%20Stroke%20Representation%20for%20Creative%20Sketch%0A%20%20Generation%0AAuthor%3A%20Jiawei%20Wang%20and%20Zhiming%20Cui%20and%20Changjian%20Li%0AAbstract%3A%20%20%20This%20paper%20presents%20VQ-SGen%2C%20a%20novel%20algorithm%20for%20high-quality%20creative%0Asketch%20generation.%20Recent%20approaches%20have%20framed%20the%20task%20as%20pixel-based%0Ageneration%20either%20as%20a%20whole%20or%20part-by-part%2C%20neglecting%20the%20intrinsic%20and%0Acontextual%20relationships%20among%20individual%20strokes%2C%20such%20as%20the%20shape%20and%0Aspatial%20positioning%20of%20both%20proximal%20and%20distant%20strokes.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20treating%20each%20stroke%20within%20a%20sketch%20as%20an%20entity%20and%0Aintroducing%20a%20vector-quantized%20%28VQ%29%20stroke%20representation%20for%20fine-grained%0Asketch%20generation.%20Our%20method%20follows%20a%20two-stage%20framework%20-%20in%20stage%20one%2C%20we%0Adecouple%20each%20stroke%27s%20shape%20and%20location%20information%20to%20ensure%20the%20VQ%0Arepresentation%20prioritizes%20stroke%20shape%20learning.%20In%20stage%20two%2C%20we%20feed%20the%0Aprecise%20and%20compact%20representation%20into%20an%20auto-decoding%20Transformer%20to%0Aincorporate%20stroke%20semantics%2C%20positions%2C%20and%20shapes%20into%20the%20generation%0Aprocess.%20By%20utilizing%20tokenized%20stroke%20representation%2C%20our%20approach%20generates%0Astrokes%20with%20high%20fidelity%20and%20facilitates%20novel%20applications%2C%20such%20as%20text%20or%0Aclass%20label%20conditioned%20generation%20and%20sketch%20completion.%20Comprehensive%0Aexperiments%20demonstrate%20our%20method%20surpasses%20existing%20state-of-the-art%0Atechniques%20on%20the%20CreativeSketch%20dataset%2C%20underscoring%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16446v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVQ-SGen%253A%2520A%2520Vector%2520Quantized%2520Stroke%2520Representation%2520for%2520Creative%2520Sketch%250A%2520%2520Generation%26entry.906535625%3DJiawei%2520Wang%2520and%2520Zhiming%2520Cui%2520and%2520Changjian%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520VQ-SGen%252C%2520a%2520novel%2520algorithm%2520for%2520high-quality%2520creative%250Asketch%2520generation.%2520Recent%2520approaches%2520have%2520framed%2520the%2520task%2520as%2520pixel-based%250Ageneration%2520either%2520as%2520a%2520whole%2520or%2520part-by-part%252C%2520neglecting%2520the%2520intrinsic%2520and%250Acontextual%2520relationships%2520among%2520individual%2520strokes%252C%2520such%2520as%2520the%2520shape%2520and%250Aspatial%2520positioning%2520of%2520both%2520proximal%2520and%2520distant%2520strokes.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520treating%2520each%2520stroke%2520within%2520a%2520sketch%2520as%2520an%2520entity%2520and%250Aintroducing%2520a%2520vector-quantized%2520%2528VQ%2529%2520stroke%2520representation%2520for%2520fine-grained%250Asketch%2520generation.%2520Our%2520method%2520follows%2520a%2520two-stage%2520framework%2520-%2520in%2520stage%2520one%252C%2520we%250Adecouple%2520each%2520stroke%2527s%2520shape%2520and%2520location%2520information%2520to%2520ensure%2520the%2520VQ%250Arepresentation%2520prioritizes%2520stroke%2520shape%2520learning.%2520In%2520stage%2520two%252C%2520we%2520feed%2520the%250Aprecise%2520and%2520compact%2520representation%2520into%2520an%2520auto-decoding%2520Transformer%2520to%250Aincorporate%2520stroke%2520semantics%252C%2520positions%252C%2520and%2520shapes%2520into%2520the%2520generation%250Aprocess.%2520By%2520utilizing%2520tokenized%2520stroke%2520representation%252C%2520our%2520approach%2520generates%250Astrokes%2520with%2520high%2520fidelity%2520and%2520facilitates%2520novel%2520applications%252C%2520such%2520as%2520text%2520or%250Aclass%2520label%2520conditioned%2520generation%2520and%2520sketch%2520completion.%2520Comprehensive%250Aexperiments%2520demonstrate%2520our%2520method%2520surpasses%2520existing%2520state-of-the-art%250Atechniques%2520on%2520the%2520CreativeSketch%2520dataset%252C%2520underscoring%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16446v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQ-SGen%3A%20A%20Vector%20Quantized%20Stroke%20Representation%20for%20Creative%20Sketch%0A%20%20Generation&entry.906535625=Jiawei%20Wang%20and%20Zhiming%20Cui%20and%20Changjian%20Li&entry.1292438233=%20%20This%20paper%20presents%20VQ-SGen%2C%20a%20novel%20algorithm%20for%20high-quality%20creative%0Asketch%20generation.%20Recent%20approaches%20have%20framed%20the%20task%20as%20pixel-based%0Ageneration%20either%20as%20a%20whole%20or%20part-by-part%2C%20neglecting%20the%20intrinsic%20and%0Acontextual%20relationships%20among%20individual%20strokes%2C%20such%20as%20the%20shape%20and%0Aspatial%20positioning%20of%20both%20proximal%20and%20distant%20strokes.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20treating%20each%20stroke%20within%20a%20sketch%20as%20an%20entity%20and%0Aintroducing%20a%20vector-quantized%20%28VQ%29%20stroke%20representation%20for%20fine-grained%0Asketch%20generation.%20Our%20method%20follows%20a%20two-stage%20framework%20-%20in%20stage%20one%2C%20we%0Adecouple%20each%20stroke%27s%20shape%20and%20location%20information%20to%20ensure%20the%20VQ%0Arepresentation%20prioritizes%20stroke%20shape%20learning.%20In%20stage%20two%2C%20we%20feed%20the%0Aprecise%20and%20compact%20representation%20into%20an%20auto-decoding%20Transformer%20to%0Aincorporate%20stroke%20semantics%2C%20positions%2C%20and%20shapes%20into%20the%20generation%0Aprocess.%20By%20utilizing%20tokenized%20stroke%20representation%2C%20our%20approach%20generates%0Astrokes%20with%20high%20fidelity%20and%20facilitates%20novel%20applications%2C%20such%20as%20text%20or%0Aclass%20label%20conditioned%20generation%20and%20sketch%20completion.%20Comprehensive%0Aexperiments%20demonstrate%20our%20method%20surpasses%20existing%20state-of-the-art%0Atechniques%20on%20the%20CreativeSketch%20dataset%2C%20underscoring%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16446v3&entry.124074799=Read"},
{"title": "Winning and losing with Artificial Intelligence: What public discourse\n  about ChatGPT tells us about how societies make sense of technological change", "author": "Adrian Rauchfleisch and Joshua Philip Suarez and Nikka Marie Sales and Andreas Jungherr", "abstract": "  Public product launches in Artificial Intelligence can serve as focusing\nevents for collective attention, surfacing how societies react to technological\nchange. Social media provide a window into the sensemaking around these events,\nsurfacing hopes and fears and showing who chooses to engage in the discourse\nand when. We demonstrate that public sensemaking about AI is shaped by economic\ninterests and cultural values of those involved. We analyze 3.8 million tweets\nposted by 1.6 million users across 117 countries in response to the public\nlaunch of ChatGPT in 2022. Our analysis shows how economic self-interest,\nproxied by occupational skill types in writing, programming, and mathematics,\nand national cultural orientations, as measured by Hofstede's individualism,\nuncertainty avoidance, and power distance dimensions, shape who speaks, when\nthey speak, and their stance towards ChatGPT. Roles requiring more technical\nskills, such as programming and mathematics, tend to engage earlier and express\nmore positive stances, whereas writing-centric occupations join later with\ngreater skepticism. At the cultural level, individualism predicts both earlier\nengagement and a more negative stance, and uncertainty avoidance reduces the\nprevalence of positive stances but does not delay when users first engage with\nChatGPT. Aggregate sentiment trends mask the dynamics observed in our study.\nThe shift toward a more critical stance towards ChatGPT over time stems\nprimarily from the entry of more skeptical voices rather than a change of heart\namong early adopters. Our findings underscore the importance of both the\noccupational background and cultural context in understanding public reactions\nto AI.\n", "link": "http://arxiv.org/abs/2507.06876v1", "date": "2025-07-09", "relevancy": 2.1528, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4604}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4346}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Winning%20and%20losing%20with%20Artificial%20Intelligence%3A%20What%20public%20discourse%0A%20%20about%20ChatGPT%20tells%20us%20about%20how%20societies%20make%20sense%20of%20technological%20change&body=Title%3A%20Winning%20and%20losing%20with%20Artificial%20Intelligence%3A%20What%20public%20discourse%0A%20%20about%20ChatGPT%20tells%20us%20about%20how%20societies%20make%20sense%20of%20technological%20change%0AAuthor%3A%20Adrian%20Rauchfleisch%20and%20Joshua%20Philip%20Suarez%20and%20Nikka%20Marie%20Sales%20and%20Andreas%20Jungherr%0AAbstract%3A%20%20%20Public%20product%20launches%20in%20Artificial%20Intelligence%20can%20serve%20as%20focusing%0Aevents%20for%20collective%20attention%2C%20surfacing%20how%20societies%20react%20to%20technological%0Achange.%20Social%20media%20provide%20a%20window%20into%20the%20sensemaking%20around%20these%20events%2C%0Asurfacing%20hopes%20and%20fears%20and%20showing%20who%20chooses%20to%20engage%20in%20the%20discourse%0Aand%20when.%20We%20demonstrate%20that%20public%20sensemaking%20about%20AI%20is%20shaped%20by%20economic%0Ainterests%20and%20cultural%20values%20of%20those%20involved.%20We%20analyze%203.8%20million%20tweets%0Aposted%20by%201.6%20million%20users%20across%20117%20countries%20in%20response%20to%20the%20public%0Alaunch%20of%20ChatGPT%20in%202022.%20Our%20analysis%20shows%20how%20economic%20self-interest%2C%0Aproxied%20by%20occupational%20skill%20types%20in%20writing%2C%20programming%2C%20and%20mathematics%2C%0Aand%20national%20cultural%20orientations%2C%20as%20measured%20by%20Hofstede%27s%20individualism%2C%0Auncertainty%20avoidance%2C%20and%20power%20distance%20dimensions%2C%20shape%20who%20speaks%2C%20when%0Athey%20speak%2C%20and%20their%20stance%20towards%20ChatGPT.%20Roles%20requiring%20more%20technical%0Askills%2C%20such%20as%20programming%20and%20mathematics%2C%20tend%20to%20engage%20earlier%20and%20express%0Amore%20positive%20stances%2C%20whereas%20writing-centric%20occupations%20join%20later%20with%0Agreater%20skepticism.%20At%20the%20cultural%20level%2C%20individualism%20predicts%20both%20earlier%0Aengagement%20and%20a%20more%20negative%20stance%2C%20and%20uncertainty%20avoidance%20reduces%20the%0Aprevalence%20of%20positive%20stances%20but%20does%20not%20delay%20when%20users%20first%20engage%20with%0AChatGPT.%20Aggregate%20sentiment%20trends%20mask%20the%20dynamics%20observed%20in%20our%20study.%0AThe%20shift%20toward%20a%20more%20critical%20stance%20towards%20ChatGPT%20over%20time%20stems%0Aprimarily%20from%20the%20entry%20of%20more%20skeptical%20voices%20rather%20than%20a%20change%20of%20heart%0Aamong%20early%20adopters.%20Our%20findings%20underscore%20the%20importance%20of%20both%20the%0Aoccupational%20background%20and%20cultural%20context%20in%20understanding%20public%20reactions%0Ato%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWinning%2520and%2520losing%2520with%2520Artificial%2520Intelligence%253A%2520What%2520public%2520discourse%250A%2520%2520about%2520ChatGPT%2520tells%2520us%2520about%2520how%2520societies%2520make%2520sense%2520of%2520technological%2520change%26entry.906535625%3DAdrian%2520Rauchfleisch%2520and%2520Joshua%2520Philip%2520Suarez%2520and%2520Nikka%2520Marie%2520Sales%2520and%2520Andreas%2520Jungherr%26entry.1292438233%3D%2520%2520Public%2520product%2520launches%2520in%2520Artificial%2520Intelligence%2520can%2520serve%2520as%2520focusing%250Aevents%2520for%2520collective%2520attention%252C%2520surfacing%2520how%2520societies%2520react%2520to%2520technological%250Achange.%2520Social%2520media%2520provide%2520a%2520window%2520into%2520the%2520sensemaking%2520around%2520these%2520events%252C%250Asurfacing%2520hopes%2520and%2520fears%2520and%2520showing%2520who%2520chooses%2520to%2520engage%2520in%2520the%2520discourse%250Aand%2520when.%2520We%2520demonstrate%2520that%2520public%2520sensemaking%2520about%2520AI%2520is%2520shaped%2520by%2520economic%250Ainterests%2520and%2520cultural%2520values%2520of%2520those%2520involved.%2520We%2520analyze%25203.8%2520million%2520tweets%250Aposted%2520by%25201.6%2520million%2520users%2520across%2520117%2520countries%2520in%2520response%2520to%2520the%2520public%250Alaunch%2520of%2520ChatGPT%2520in%25202022.%2520Our%2520analysis%2520shows%2520how%2520economic%2520self-interest%252C%250Aproxied%2520by%2520occupational%2520skill%2520types%2520in%2520writing%252C%2520programming%252C%2520and%2520mathematics%252C%250Aand%2520national%2520cultural%2520orientations%252C%2520as%2520measured%2520by%2520Hofstede%2527s%2520individualism%252C%250Auncertainty%2520avoidance%252C%2520and%2520power%2520distance%2520dimensions%252C%2520shape%2520who%2520speaks%252C%2520when%250Athey%2520speak%252C%2520and%2520their%2520stance%2520towards%2520ChatGPT.%2520Roles%2520requiring%2520more%2520technical%250Askills%252C%2520such%2520as%2520programming%2520and%2520mathematics%252C%2520tend%2520to%2520engage%2520earlier%2520and%2520express%250Amore%2520positive%2520stances%252C%2520whereas%2520writing-centric%2520occupations%2520join%2520later%2520with%250Agreater%2520skepticism.%2520At%2520the%2520cultural%2520level%252C%2520individualism%2520predicts%2520both%2520earlier%250Aengagement%2520and%2520a%2520more%2520negative%2520stance%252C%2520and%2520uncertainty%2520avoidance%2520reduces%2520the%250Aprevalence%2520of%2520positive%2520stances%2520but%2520does%2520not%2520delay%2520when%2520users%2520first%2520engage%2520with%250AChatGPT.%2520Aggregate%2520sentiment%2520trends%2520mask%2520the%2520dynamics%2520observed%2520in%2520our%2520study.%250AThe%2520shift%2520toward%2520a%2520more%2520critical%2520stance%2520towards%2520ChatGPT%2520over%2520time%2520stems%250Aprimarily%2520from%2520the%2520entry%2520of%2520more%2520skeptical%2520voices%2520rather%2520than%2520a%2520change%2520of%2520heart%250Aamong%2520early%2520adopters.%2520Our%2520findings%2520underscore%2520the%2520importance%2520of%2520both%2520the%250Aoccupational%2520background%2520and%2520cultural%2520context%2520in%2520understanding%2520public%2520reactions%250Ato%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Winning%20and%20losing%20with%20Artificial%20Intelligence%3A%20What%20public%20discourse%0A%20%20about%20ChatGPT%20tells%20us%20about%20how%20societies%20make%20sense%20of%20technological%20change&entry.906535625=Adrian%20Rauchfleisch%20and%20Joshua%20Philip%20Suarez%20and%20Nikka%20Marie%20Sales%20and%20Andreas%20Jungherr&entry.1292438233=%20%20Public%20product%20launches%20in%20Artificial%20Intelligence%20can%20serve%20as%20focusing%0Aevents%20for%20collective%20attention%2C%20surfacing%20how%20societies%20react%20to%20technological%0Achange.%20Social%20media%20provide%20a%20window%20into%20the%20sensemaking%20around%20these%20events%2C%0Asurfacing%20hopes%20and%20fears%20and%20showing%20who%20chooses%20to%20engage%20in%20the%20discourse%0Aand%20when.%20We%20demonstrate%20that%20public%20sensemaking%20about%20AI%20is%20shaped%20by%20economic%0Ainterests%20and%20cultural%20values%20of%20those%20involved.%20We%20analyze%203.8%20million%20tweets%0Aposted%20by%201.6%20million%20users%20across%20117%20countries%20in%20response%20to%20the%20public%0Alaunch%20of%20ChatGPT%20in%202022.%20Our%20analysis%20shows%20how%20economic%20self-interest%2C%0Aproxied%20by%20occupational%20skill%20types%20in%20writing%2C%20programming%2C%20and%20mathematics%2C%0Aand%20national%20cultural%20orientations%2C%20as%20measured%20by%20Hofstede%27s%20individualism%2C%0Auncertainty%20avoidance%2C%20and%20power%20distance%20dimensions%2C%20shape%20who%20speaks%2C%20when%0Athey%20speak%2C%20and%20their%20stance%20towards%20ChatGPT.%20Roles%20requiring%20more%20technical%0Askills%2C%20such%20as%20programming%20and%20mathematics%2C%20tend%20to%20engage%20earlier%20and%20express%0Amore%20positive%20stances%2C%20whereas%20writing-centric%20occupations%20join%20later%20with%0Agreater%20skepticism.%20At%20the%20cultural%20level%2C%20individualism%20predicts%20both%20earlier%0Aengagement%20and%20a%20more%20negative%20stance%2C%20and%20uncertainty%20avoidance%20reduces%20the%0Aprevalence%20of%20positive%20stances%20but%20does%20not%20delay%20when%20users%20first%20engage%20with%0AChatGPT.%20Aggregate%20sentiment%20trends%20mask%20the%20dynamics%20observed%20in%20our%20study.%0AThe%20shift%20toward%20a%20more%20critical%20stance%20towards%20ChatGPT%20over%20time%20stems%0Aprimarily%20from%20the%20entry%20of%20more%20skeptical%20voices%20rather%20than%20a%20change%20of%20heart%0Aamong%20early%20adopters.%20Our%20findings%20underscore%20the%20importance%20of%20both%20the%0Aoccupational%20background%20and%20cultural%20context%20in%20understanding%20public%20reactions%0Ato%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06876v1&entry.124074799=Read"},
{"title": "Adaptive Elicitation of Latent Information Using Natural Language", "author": "Jimmy Wang and Thomas Zollo and Richard Zemel and Hongseok Namkoong", "abstract": "  Eliciting information to reduce uncertainty about a latent entity is a\ncritical task in many application domains, e.g., assessing individual student\nlearning outcomes, diagnosing underlying diseases, or learning user\npreferences. Though natural language is a powerful medium for this purpose,\nlarge language models (LLMs) and existing fine-tuning algorithms lack\nmechanisms for strategically gathering information to refine their own\nunderstanding of the latent entity. To harness the generalization power and\nworld knowledge of LLMs in developing effective information-gathering\nstrategies, we propose an adaptive elicitation framework that actively reduces\nuncertainty on the latent entity. Since probabilistic modeling of an abstract\nlatent entity is difficult, our framework adopts a predictive view of\nuncertainty, using a meta-learned language model to simulate future\nobservations and enable scalable uncertainty quantification over complex\nnatural language. Through autoregressive forward simulation, our model\nquantifies how new questions reduce epistemic uncertainty, enabling the\ndevelopment of sophisticated information-gathering strategies to choose the\nmost informative next queries. In experiments on the 20 questions game, dynamic\nopinion polling, and adaptive student assessment, our method consistently\noutperforms baselines in identifying critical unknowns and improving downstream\npredictions, illustrating the promise of strategic information gathering in\nnatural language settings.\n", "link": "http://arxiv.org/abs/2504.04204v2", "date": "2025-07-09", "relevancy": 2.1515, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5896}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5324}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Elicitation%20of%20Latent%20Information%20Using%20Natural%20Language&body=Title%3A%20Adaptive%20Elicitation%20of%20Latent%20Information%20Using%20Natural%20Language%0AAuthor%3A%20Jimmy%20Wang%20and%20Thomas%20Zollo%20and%20Richard%20Zemel%20and%20Hongseok%20Namkoong%0AAbstract%3A%20%20%20Eliciting%20information%20to%20reduce%20uncertainty%20about%20a%20latent%20entity%20is%20a%0Acritical%20task%20in%20many%20application%20domains%2C%20e.g.%2C%20assessing%20individual%20student%0Alearning%20outcomes%2C%20diagnosing%20underlying%20diseases%2C%20or%20learning%20user%0Apreferences.%20Though%20natural%20language%20is%20a%20powerful%20medium%20for%20this%20purpose%2C%0Alarge%20language%20models%20%28LLMs%29%20and%20existing%20fine-tuning%20algorithms%20lack%0Amechanisms%20for%20strategically%20gathering%20information%20to%20refine%20their%20own%0Aunderstanding%20of%20the%20latent%20entity.%20To%20harness%20the%20generalization%20power%20and%0Aworld%20knowledge%20of%20LLMs%20in%20developing%20effective%20information-gathering%0Astrategies%2C%20we%20propose%20an%20adaptive%20elicitation%20framework%20that%20actively%20reduces%0Auncertainty%20on%20the%20latent%20entity.%20Since%20probabilistic%20modeling%20of%20an%20abstract%0Alatent%20entity%20is%20difficult%2C%20our%20framework%20adopts%20a%20predictive%20view%20of%0Auncertainty%2C%20using%20a%20meta-learned%20language%20model%20to%20simulate%20future%0Aobservations%20and%20enable%20scalable%20uncertainty%20quantification%20over%20complex%0Anatural%20language.%20Through%20autoregressive%20forward%20simulation%2C%20our%20model%0Aquantifies%20how%20new%20questions%20reduce%20epistemic%20uncertainty%2C%20enabling%20the%0Adevelopment%20of%20sophisticated%20information-gathering%20strategies%20to%20choose%20the%0Amost%20informative%20next%20queries.%20In%20experiments%20on%20the%2020%20questions%20game%2C%20dynamic%0Aopinion%20polling%2C%20and%20adaptive%20student%20assessment%2C%20our%20method%20consistently%0Aoutperforms%20baselines%20in%20identifying%20critical%20unknowns%20and%20improving%20downstream%0Apredictions%2C%20illustrating%20the%20promise%20of%20strategic%20information%20gathering%20in%0Anatural%20language%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Elicitation%2520of%2520Latent%2520Information%2520Using%2520Natural%2520Language%26entry.906535625%3DJimmy%2520Wang%2520and%2520Thomas%2520Zollo%2520and%2520Richard%2520Zemel%2520and%2520Hongseok%2520Namkoong%26entry.1292438233%3D%2520%2520Eliciting%2520information%2520to%2520reduce%2520uncertainty%2520about%2520a%2520latent%2520entity%2520is%2520a%250Acritical%2520task%2520in%2520many%2520application%2520domains%252C%2520e.g.%252C%2520assessing%2520individual%2520student%250Alearning%2520outcomes%252C%2520diagnosing%2520underlying%2520diseases%252C%2520or%2520learning%2520user%250Apreferences.%2520Though%2520natural%2520language%2520is%2520a%2520powerful%2520medium%2520for%2520this%2520purpose%252C%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520and%2520existing%2520fine-tuning%2520algorithms%2520lack%250Amechanisms%2520for%2520strategically%2520gathering%2520information%2520to%2520refine%2520their%2520own%250Aunderstanding%2520of%2520the%2520latent%2520entity.%2520To%2520harness%2520the%2520generalization%2520power%2520and%250Aworld%2520knowledge%2520of%2520LLMs%2520in%2520developing%2520effective%2520information-gathering%250Astrategies%252C%2520we%2520propose%2520an%2520adaptive%2520elicitation%2520framework%2520that%2520actively%2520reduces%250Auncertainty%2520on%2520the%2520latent%2520entity.%2520Since%2520probabilistic%2520modeling%2520of%2520an%2520abstract%250Alatent%2520entity%2520is%2520difficult%252C%2520our%2520framework%2520adopts%2520a%2520predictive%2520view%2520of%250Auncertainty%252C%2520using%2520a%2520meta-learned%2520language%2520model%2520to%2520simulate%2520future%250Aobservations%2520and%2520enable%2520scalable%2520uncertainty%2520quantification%2520over%2520complex%250Anatural%2520language.%2520Through%2520autoregressive%2520forward%2520simulation%252C%2520our%2520model%250Aquantifies%2520how%2520new%2520questions%2520reduce%2520epistemic%2520uncertainty%252C%2520enabling%2520the%250Adevelopment%2520of%2520sophisticated%2520information-gathering%2520strategies%2520to%2520choose%2520the%250Amost%2520informative%2520next%2520queries.%2520In%2520experiments%2520on%2520the%252020%2520questions%2520game%252C%2520dynamic%250Aopinion%2520polling%252C%2520and%2520adaptive%2520student%2520assessment%252C%2520our%2520method%2520consistently%250Aoutperforms%2520baselines%2520in%2520identifying%2520critical%2520unknowns%2520and%2520improving%2520downstream%250Apredictions%252C%2520illustrating%2520the%2520promise%2520of%2520strategic%2520information%2520gathering%2520in%250Anatural%2520language%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Elicitation%20of%20Latent%20Information%20Using%20Natural%20Language&entry.906535625=Jimmy%20Wang%20and%20Thomas%20Zollo%20and%20Richard%20Zemel%20and%20Hongseok%20Namkoong&entry.1292438233=%20%20Eliciting%20information%20to%20reduce%20uncertainty%20about%20a%20latent%20entity%20is%20a%0Acritical%20task%20in%20many%20application%20domains%2C%20e.g.%2C%20assessing%20individual%20student%0Alearning%20outcomes%2C%20diagnosing%20underlying%20diseases%2C%20or%20learning%20user%0Apreferences.%20Though%20natural%20language%20is%20a%20powerful%20medium%20for%20this%20purpose%2C%0Alarge%20language%20models%20%28LLMs%29%20and%20existing%20fine-tuning%20algorithms%20lack%0Amechanisms%20for%20strategically%20gathering%20information%20to%20refine%20their%20own%0Aunderstanding%20of%20the%20latent%20entity.%20To%20harness%20the%20generalization%20power%20and%0Aworld%20knowledge%20of%20LLMs%20in%20developing%20effective%20information-gathering%0Astrategies%2C%20we%20propose%20an%20adaptive%20elicitation%20framework%20that%20actively%20reduces%0Auncertainty%20on%20the%20latent%20entity.%20Since%20probabilistic%20modeling%20of%20an%20abstract%0Alatent%20entity%20is%20difficult%2C%20our%20framework%20adopts%20a%20predictive%20view%20of%0Auncertainty%2C%20using%20a%20meta-learned%20language%20model%20to%20simulate%20future%0Aobservations%20and%20enable%20scalable%20uncertainty%20quantification%20over%20complex%0Anatural%20language.%20Through%20autoregressive%20forward%20simulation%2C%20our%20model%0Aquantifies%20how%20new%20questions%20reduce%20epistemic%20uncertainty%2C%20enabling%20the%0Adevelopment%20of%20sophisticated%20information-gathering%20strategies%20to%20choose%20the%0Amost%20informative%20next%20queries.%20In%20experiments%20on%20the%2020%20questions%20game%2C%20dynamic%0Aopinion%20polling%2C%20and%20adaptive%20student%20assessment%2C%20our%20method%20consistently%0Aoutperforms%20baselines%20in%20identifying%20critical%20unknowns%20and%20improving%20downstream%0Apredictions%2C%20illustrating%20the%20promise%20of%20strategic%20information%20gathering%20in%0Anatural%20language%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04204v2&entry.124074799=Read"},
{"title": "Bayesian Multi-Scale Neural Network for Crowd Counting", "author": "Abhinav Sagar", "abstract": "  Crowd counting is a challenging yet critical task in computer vision with\napplications ranging from public safety to urban planning. Recent advances\nusing Convolutional Neural Networks (CNNs) that estimate density maps have\nshown significant success. However, accurately counting individuals in highly\ncongested scenes remains an open problem due to severe occlusions, scale\nvariations, and perspective distortions, where people appear at drastically\ndifferent sizes across the image. In this work, we propose a novel deep\nlearning architecture that effectively addresses these challenges. Our network\nintegrates a ResNet-based feature extractor for capturing rich hierarchical\nrepresentations, followed by a downsampling block employing dilated\nconvolutions to preserve spatial resolution while expanding the receptive\nfield. An upsampling block using transposed convolutions reconstructs the\nhigh-resolution density map. Central to our architecture is a novel\nPerspective-aware Aggregation Module (PAM) designed to enhance robustness to\nscale and perspective variations by adaptively aggregating multi-scale\ncontextual information. We detail the training procedure, including the loss\nfunctions and optimization strategies used. Our method is evaluated on three\nwidely used benchmark datasets using Mean Absolute Error (MAE) and Mean Squared\nError (MSE) as evaluation metrics. Experimental results demonstrate that our\nmodel achieves superior performance compared to existing state-of-the-art\nmethods. Additionally, we incorporate principled Bayesian inference techniques\nto provide uncertainty estimates along with the crowd count predictions,\noffering a measure of confidence in the model's outputs.\n", "link": "http://arxiv.org/abs/2007.14245v4", "date": "2025-07-09", "relevancy": 2.149, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5533}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Multi-Scale%20Neural%20Network%20for%20Crowd%20Counting&body=Title%3A%20Bayesian%20Multi-Scale%20Neural%20Network%20for%20Crowd%20Counting%0AAuthor%3A%20Abhinav%20Sagar%0AAbstract%3A%20%20%20Crowd%20counting%20is%20a%20challenging%20yet%20critical%20task%20in%20computer%20vision%20with%0Aapplications%20ranging%20from%20public%20safety%20to%20urban%20planning.%20Recent%20advances%0Ausing%20Convolutional%20Neural%20Networks%20%28CNNs%29%20that%20estimate%20density%20maps%20have%0Ashown%20significant%20success.%20However%2C%20accurately%20counting%20individuals%20in%20highly%0Acongested%20scenes%20remains%20an%20open%20problem%20due%20to%20severe%20occlusions%2C%20scale%0Avariations%2C%20and%20perspective%20distortions%2C%20where%20people%20appear%20at%20drastically%0Adifferent%20sizes%20across%20the%20image.%20In%20this%20work%2C%20we%20propose%20a%20novel%20deep%0Alearning%20architecture%20that%20effectively%20addresses%20these%20challenges.%20Our%20network%0Aintegrates%20a%20ResNet-based%20feature%20extractor%20for%20capturing%20rich%20hierarchical%0Arepresentations%2C%20followed%20by%20a%20downsampling%20block%20employing%20dilated%0Aconvolutions%20to%20preserve%20spatial%20resolution%20while%20expanding%20the%20receptive%0Afield.%20An%20upsampling%20block%20using%20transposed%20convolutions%20reconstructs%20the%0Ahigh-resolution%20density%20map.%20Central%20to%20our%20architecture%20is%20a%20novel%0APerspective-aware%20Aggregation%20Module%20%28PAM%29%20designed%20to%20enhance%20robustness%20to%0Ascale%20and%20perspective%20variations%20by%20adaptively%20aggregating%20multi-scale%0Acontextual%20information.%20We%20detail%20the%20training%20procedure%2C%20including%20the%20loss%0Afunctions%20and%20optimization%20strategies%20used.%20Our%20method%20is%20evaluated%20on%20three%0Awidely%20used%20benchmark%20datasets%20using%20Mean%20Absolute%20Error%20%28MAE%29%20and%20Mean%20Squared%0AError%20%28MSE%29%20as%20evaluation%20metrics.%20Experimental%20results%20demonstrate%20that%20our%0Amodel%20achieves%20superior%20performance%20compared%20to%20existing%20state-of-the-art%0Amethods.%20Additionally%2C%20we%20incorporate%20principled%20Bayesian%20inference%20techniques%0Ato%20provide%20uncertainty%20estimates%20along%20with%20the%20crowd%20count%20predictions%2C%0Aoffering%20a%20measure%20of%20confidence%20in%20the%20model%27s%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2007.14245v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Multi-Scale%2520Neural%2520Network%2520for%2520Crowd%2520Counting%26entry.906535625%3DAbhinav%2520Sagar%26entry.1292438233%3D%2520%2520Crowd%2520counting%2520is%2520a%2520challenging%2520yet%2520critical%2520task%2520in%2520computer%2520vision%2520with%250Aapplications%2520ranging%2520from%2520public%2520safety%2520to%2520urban%2520planning.%2520Recent%2520advances%250Ausing%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520that%2520estimate%2520density%2520maps%2520have%250Ashown%2520significant%2520success.%2520However%252C%2520accurately%2520counting%2520individuals%2520in%2520highly%250Acongested%2520scenes%2520remains%2520an%2520open%2520problem%2520due%2520to%2520severe%2520occlusions%252C%2520scale%250Avariations%252C%2520and%2520perspective%2520distortions%252C%2520where%2520people%2520appear%2520at%2520drastically%250Adifferent%2520sizes%2520across%2520the%2520image.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520deep%250Alearning%2520architecture%2520that%2520effectively%2520addresses%2520these%2520challenges.%2520Our%2520network%250Aintegrates%2520a%2520ResNet-based%2520feature%2520extractor%2520for%2520capturing%2520rich%2520hierarchical%250Arepresentations%252C%2520followed%2520by%2520a%2520downsampling%2520block%2520employing%2520dilated%250Aconvolutions%2520to%2520preserve%2520spatial%2520resolution%2520while%2520expanding%2520the%2520receptive%250Afield.%2520An%2520upsampling%2520block%2520using%2520transposed%2520convolutions%2520reconstructs%2520the%250Ahigh-resolution%2520density%2520map.%2520Central%2520to%2520our%2520architecture%2520is%2520a%2520novel%250APerspective-aware%2520Aggregation%2520Module%2520%2528PAM%2529%2520designed%2520to%2520enhance%2520robustness%2520to%250Ascale%2520and%2520perspective%2520variations%2520by%2520adaptively%2520aggregating%2520multi-scale%250Acontextual%2520information.%2520We%2520detail%2520the%2520training%2520procedure%252C%2520including%2520the%2520loss%250Afunctions%2520and%2520optimization%2520strategies%2520used.%2520Our%2520method%2520is%2520evaluated%2520on%2520three%250Awidely%2520used%2520benchmark%2520datasets%2520using%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520and%2520Mean%2520Squared%250AError%2520%2528MSE%2529%2520as%2520evaluation%2520metrics.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amodel%2520achieves%2520superior%2520performance%2520compared%2520to%2520existing%2520state-of-the-art%250Amethods.%2520Additionally%252C%2520we%2520incorporate%2520principled%2520Bayesian%2520inference%2520techniques%250Ato%2520provide%2520uncertainty%2520estimates%2520along%2520with%2520the%2520crowd%2520count%2520predictions%252C%250Aoffering%2520a%2520measure%2520of%2520confidence%2520in%2520the%2520model%2527s%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2007.14245v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Multi-Scale%20Neural%20Network%20for%20Crowd%20Counting&entry.906535625=Abhinav%20Sagar&entry.1292438233=%20%20Crowd%20counting%20is%20a%20challenging%20yet%20critical%20task%20in%20computer%20vision%20with%0Aapplications%20ranging%20from%20public%20safety%20to%20urban%20planning.%20Recent%20advances%0Ausing%20Convolutional%20Neural%20Networks%20%28CNNs%29%20that%20estimate%20density%20maps%20have%0Ashown%20significant%20success.%20However%2C%20accurately%20counting%20individuals%20in%20highly%0Acongested%20scenes%20remains%20an%20open%20problem%20due%20to%20severe%20occlusions%2C%20scale%0Avariations%2C%20and%20perspective%20distortions%2C%20where%20people%20appear%20at%20drastically%0Adifferent%20sizes%20across%20the%20image.%20In%20this%20work%2C%20we%20propose%20a%20novel%20deep%0Alearning%20architecture%20that%20effectively%20addresses%20these%20challenges.%20Our%20network%0Aintegrates%20a%20ResNet-based%20feature%20extractor%20for%20capturing%20rich%20hierarchical%0Arepresentations%2C%20followed%20by%20a%20downsampling%20block%20employing%20dilated%0Aconvolutions%20to%20preserve%20spatial%20resolution%20while%20expanding%20the%20receptive%0Afield.%20An%20upsampling%20block%20using%20transposed%20convolutions%20reconstructs%20the%0Ahigh-resolution%20density%20map.%20Central%20to%20our%20architecture%20is%20a%20novel%0APerspective-aware%20Aggregation%20Module%20%28PAM%29%20designed%20to%20enhance%20robustness%20to%0Ascale%20and%20perspective%20variations%20by%20adaptively%20aggregating%20multi-scale%0Acontextual%20information.%20We%20detail%20the%20training%20procedure%2C%20including%20the%20loss%0Afunctions%20and%20optimization%20strategies%20used.%20Our%20method%20is%20evaluated%20on%20three%0Awidely%20used%20benchmark%20datasets%20using%20Mean%20Absolute%20Error%20%28MAE%29%20and%20Mean%20Squared%0AError%20%28MSE%29%20as%20evaluation%20metrics.%20Experimental%20results%20demonstrate%20that%20our%0Amodel%20achieves%20superior%20performance%20compared%20to%20existing%20state-of-the-art%0Amethods.%20Additionally%2C%20we%20incorporate%20principled%20Bayesian%20inference%20techniques%0Ato%20provide%20uncertainty%20estimates%20along%20with%20the%20crowd%20count%20predictions%2C%0Aoffering%20a%20measure%20of%20confidence%20in%20the%20model%27s%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2007.14245v4&entry.124074799=Read"},
{"title": "4KAgent: Agentic Any Image to 4K Super-Resolution", "author": "Yushen Zuo and Qi Zheng and Mingyang Wu and Xinrui Jiang and Renjie Li and Jian Wang and Yide Zhang and Gengchen Mai and Lihong V. Wang and James Zou and Xiaoyu Wang and Ming-Hsuan Yang and Zhengzhong Tu", "abstract": "  We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.\n", "link": "http://arxiv.org/abs/2507.07105v1", "date": "2025-07-09", "relevancy": 2.1462, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5419}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5391}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204KAgent%3A%20Agentic%20Any%20Image%20to%204K%20Super-Resolution&body=Title%3A%204KAgent%3A%20Agentic%20Any%20Image%20to%204K%20Super-Resolution%0AAuthor%3A%20Yushen%20Zuo%20and%20Qi%20Zheng%20and%20Mingyang%20Wu%20and%20Xinrui%20Jiang%20and%20Renjie%20Li%20and%20Jian%20Wang%20and%20Yide%20Zhang%20and%20Gengchen%20Mai%20and%20Lihong%20V.%20Wang%20and%20James%20Zou%20and%20Xiaoyu%20Wang%20and%20Ming-Hsuan%20Yang%20and%20Zhengzhong%20Tu%0AAbstract%3A%20%20%20We%20present%204KAgent%2C%20a%20unified%20agentic%20super-resolution%20generalist%20system%0Adesigned%20to%20universally%20upscale%20any%20image%20to%204K%20resolution%20%28and%20even%20higher%2C%20if%0Aapplied%20iteratively%29.%20Our%20system%20can%20transform%20images%20from%20extremely%20low%0Aresolutions%20with%20severe%20degradations%2C%20for%20example%2C%20highly%20distorted%20inputs%20at%0A256x256%2C%20into%20crystal-clear%2C%20photorealistic%204K%20outputs.%204KAgent%20comprises%20three%0Acore%20components%3A%20%281%29%20Profiling%2C%20a%20module%20that%20customizes%20the%204KAgent%20pipeline%0Abased%20on%20bespoke%20use%20cases%3B%20%282%29%20A%20Perception%20Agent%2C%20which%20leverages%0Avision-language%20models%20alongside%20image%20quality%20assessment%20experts%20to%20analyze%0Athe%20input%20image%20and%20make%20a%20tailored%20restoration%20plan%3B%20and%20%283%29%20A%20Restoration%0AAgent%2C%20which%20executes%20the%20plan%2C%20following%20a%20recursive%20execution-reflection%0Aparadigm%2C%20guided%20by%20a%20quality-driven%20mixture-of-expert%20policy%20to%20select%20the%0Aoptimal%20output%20for%20each%20step.%20Additionally%2C%204KAgent%20embeds%20a%20specialized%20face%0Arestoration%20pipeline%2C%20significantly%20enhancing%20facial%20details%20in%20portrait%20and%0Aselfie%20photos.%20We%20rigorously%20evaluate%20our%204KAgent%20across%2011%20distinct%20task%0Acategories%20encompassing%20a%20total%20of%2026%20diverse%20benchmarks%2C%20setting%20new%0Astate-of-the-art%20on%20a%20broad%20spectrum%20of%20imaging%20domains.%20Our%20evaluations%20cover%0Anatural%20images%2C%20portrait%20photos%2C%20AI-generated%20content%2C%20satellite%20imagery%2C%0Afluorescence%20microscopy%2C%20and%20medical%20imaging%20like%20fundoscopy%2C%20ultrasound%2C%20and%0AX-ray%2C%20demonstrating%20superior%20performance%20in%20terms%20of%20both%20perceptual%20%28e.g.%2C%0ANIQE%2C%20MUSIQ%29%20and%20fidelity%20%28e.g.%2C%20PSNR%29%20metrics.%20By%20establishing%20a%20novel%20agentic%0Aparadigm%20for%20low-level%20vision%20tasks%2C%20we%20aim%20to%20catalyze%20broader%20interest%20and%0Ainnovation%20within%20vision-centric%20autonomous%20agents%20across%20diverse%20research%0Acommunities.%20We%20will%20release%20all%20the%20code%2C%20models%2C%20and%20results%20at%3A%0Ahttps%3A//4kagent.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4KAgent%253A%2520Agentic%2520Any%2520Image%2520to%25204K%2520Super-Resolution%26entry.906535625%3DYushen%2520Zuo%2520and%2520Qi%2520Zheng%2520and%2520Mingyang%2520Wu%2520and%2520Xinrui%2520Jiang%2520and%2520Renjie%2520Li%2520and%2520Jian%2520Wang%2520and%2520Yide%2520Zhang%2520and%2520Gengchen%2520Mai%2520and%2520Lihong%2520V.%2520Wang%2520and%2520James%2520Zou%2520and%2520Xiaoyu%2520Wang%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3D%2520%2520We%2520present%25204KAgent%252C%2520a%2520unified%2520agentic%2520super-resolution%2520generalist%2520system%250Adesigned%2520to%2520universally%2520upscale%2520any%2520image%2520to%25204K%2520resolution%2520%2528and%2520even%2520higher%252C%2520if%250Aapplied%2520iteratively%2529.%2520Our%2520system%2520can%2520transform%2520images%2520from%2520extremely%2520low%250Aresolutions%2520with%2520severe%2520degradations%252C%2520for%2520example%252C%2520highly%2520distorted%2520inputs%2520at%250A256x256%252C%2520into%2520crystal-clear%252C%2520photorealistic%25204K%2520outputs.%25204KAgent%2520comprises%2520three%250Acore%2520components%253A%2520%25281%2529%2520Profiling%252C%2520a%2520module%2520that%2520customizes%2520the%25204KAgent%2520pipeline%250Abased%2520on%2520bespoke%2520use%2520cases%253B%2520%25282%2529%2520A%2520Perception%2520Agent%252C%2520which%2520leverages%250Avision-language%2520models%2520alongside%2520image%2520quality%2520assessment%2520experts%2520to%2520analyze%250Athe%2520input%2520image%2520and%2520make%2520a%2520tailored%2520restoration%2520plan%253B%2520and%2520%25283%2529%2520A%2520Restoration%250AAgent%252C%2520which%2520executes%2520the%2520plan%252C%2520following%2520a%2520recursive%2520execution-reflection%250Aparadigm%252C%2520guided%2520by%2520a%2520quality-driven%2520mixture-of-expert%2520policy%2520to%2520select%2520the%250Aoptimal%2520output%2520for%2520each%2520step.%2520Additionally%252C%25204KAgent%2520embeds%2520a%2520specialized%2520face%250Arestoration%2520pipeline%252C%2520significantly%2520enhancing%2520facial%2520details%2520in%2520portrait%2520and%250Aselfie%2520photos.%2520We%2520rigorously%2520evaluate%2520our%25204KAgent%2520across%252011%2520distinct%2520task%250Acategories%2520encompassing%2520a%2520total%2520of%252026%2520diverse%2520benchmarks%252C%2520setting%2520new%250Astate-of-the-art%2520on%2520a%2520broad%2520spectrum%2520of%2520imaging%2520domains.%2520Our%2520evaluations%2520cover%250Anatural%2520images%252C%2520portrait%2520photos%252C%2520AI-generated%2520content%252C%2520satellite%2520imagery%252C%250Afluorescence%2520microscopy%252C%2520and%2520medical%2520imaging%2520like%2520fundoscopy%252C%2520ultrasound%252C%2520and%250AX-ray%252C%2520demonstrating%2520superior%2520performance%2520in%2520terms%2520of%2520both%2520perceptual%2520%2528e.g.%252C%250ANIQE%252C%2520MUSIQ%2529%2520and%2520fidelity%2520%2528e.g.%252C%2520PSNR%2529%2520metrics.%2520By%2520establishing%2520a%2520novel%2520agentic%250Aparadigm%2520for%2520low-level%2520vision%2520tasks%252C%2520we%2520aim%2520to%2520catalyze%2520broader%2520interest%2520and%250Ainnovation%2520within%2520vision-centric%2520autonomous%2520agents%2520across%2520diverse%2520research%250Acommunities.%2520We%2520will%2520release%2520all%2520the%2520code%252C%2520models%252C%2520and%2520results%2520at%253A%250Ahttps%253A//4kagent.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4KAgent%3A%20Agentic%20Any%20Image%20to%204K%20Super-Resolution&entry.906535625=Yushen%20Zuo%20and%20Qi%20Zheng%20and%20Mingyang%20Wu%20and%20Xinrui%20Jiang%20and%20Renjie%20Li%20and%20Jian%20Wang%20and%20Yide%20Zhang%20and%20Gengchen%20Mai%20and%20Lihong%20V.%20Wang%20and%20James%20Zou%20and%20Xiaoyu%20Wang%20and%20Ming-Hsuan%20Yang%20and%20Zhengzhong%20Tu&entry.1292438233=%20%20We%20present%204KAgent%2C%20a%20unified%20agentic%20super-resolution%20generalist%20system%0Adesigned%20to%20universally%20upscale%20any%20image%20to%204K%20resolution%20%28and%20even%20higher%2C%20if%0Aapplied%20iteratively%29.%20Our%20system%20can%20transform%20images%20from%20extremely%20low%0Aresolutions%20with%20severe%20degradations%2C%20for%20example%2C%20highly%20distorted%20inputs%20at%0A256x256%2C%20into%20crystal-clear%2C%20photorealistic%204K%20outputs.%204KAgent%20comprises%20three%0Acore%20components%3A%20%281%29%20Profiling%2C%20a%20module%20that%20customizes%20the%204KAgent%20pipeline%0Abased%20on%20bespoke%20use%20cases%3B%20%282%29%20A%20Perception%20Agent%2C%20which%20leverages%0Avision-language%20models%20alongside%20image%20quality%20assessment%20experts%20to%20analyze%0Athe%20input%20image%20and%20make%20a%20tailored%20restoration%20plan%3B%20and%20%283%29%20A%20Restoration%0AAgent%2C%20which%20executes%20the%20plan%2C%20following%20a%20recursive%20execution-reflection%0Aparadigm%2C%20guided%20by%20a%20quality-driven%20mixture-of-expert%20policy%20to%20select%20the%0Aoptimal%20output%20for%20each%20step.%20Additionally%2C%204KAgent%20embeds%20a%20specialized%20face%0Arestoration%20pipeline%2C%20significantly%20enhancing%20facial%20details%20in%20portrait%20and%0Aselfie%20photos.%20We%20rigorously%20evaluate%20our%204KAgent%20across%2011%20distinct%20task%0Acategories%20encompassing%20a%20total%20of%2026%20diverse%20benchmarks%2C%20setting%20new%0Astate-of-the-art%20on%20a%20broad%20spectrum%20of%20imaging%20domains.%20Our%20evaluations%20cover%0Anatural%20images%2C%20portrait%20photos%2C%20AI-generated%20content%2C%20satellite%20imagery%2C%0Afluorescence%20microscopy%2C%20and%20medical%20imaging%20like%20fundoscopy%2C%20ultrasound%2C%20and%0AX-ray%2C%20demonstrating%20superior%20performance%20in%20terms%20of%20both%20perceptual%20%28e.g.%2C%0ANIQE%2C%20MUSIQ%29%20and%20fidelity%20%28e.g.%2C%20PSNR%29%20metrics.%20By%20establishing%20a%20novel%20agentic%0Aparadigm%20for%20low-level%20vision%20tasks%2C%20we%20aim%20to%20catalyze%20broader%20interest%20and%0Ainnovation%20within%20vision-centric%20autonomous%20agents%20across%20diverse%20research%0Acommunities.%20We%20will%20release%20all%20the%20code%2C%20models%2C%20and%20results%20at%3A%0Ahttps%3A//4kagent.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07105v1&entry.124074799=Read"},
{"title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced\n  Planning, Navigation, and Dynamic Adaptation", "author": "Jieren Deng and Aleksandar Cvetkovic and Pak Kiu Chung and Dragomir Yankov and Chiqun Zhang", "abstract": "  Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.\n", "link": "http://arxiv.org/abs/2507.06993v1", "date": "2025-07-09", "relevancy": 2.1429, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5637}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5357}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20User-Centric%20Geo-Experience%3A%20An%20LLM-Powered%20Framework%20for%20Enhanced%0A%20%20Planning%2C%20Navigation%2C%20and%20Dynamic%20Adaptation&body=Title%3A%20The%20User-Centric%20Geo-Experience%3A%20An%20LLM-Powered%20Framework%20for%20Enhanced%0A%20%20Planning%2C%20Navigation%2C%20and%20Dynamic%20Adaptation%0AAuthor%3A%20Jieren%20Deng%20and%20Aleksandar%20Cvetkovic%20and%20Pak%20Kiu%20Chung%20and%20Dragomir%20Yankov%20and%20Chiqun%20Zhang%0AAbstract%3A%20%20%20Traditional%20travel-planning%20systems%20are%20often%20static%20and%20fragmented%2C%20leaving%0Athem%20ill-equipped%20to%20handle%20real-world%20complexities%20such%20as%20evolving%0Aenvironmental%20conditions%20and%20unexpected%20itinerary%20disruptions.%20In%20this%20paper%2C%0Awe%20identify%20three%20gaps%20between%20existing%20service%20providers%20causing%20frustrating%0Auser%20experience%3A%20intelligent%20trip%20planning%2C%20precision%20%22last-100-meter%22%0Anavigation%2C%20and%20dynamic%20itinerary%20adaptation.%20We%20propose%20three%20cooperative%0Aagents%3A%20a%20Travel%20Planning%20Agent%20that%20employs%20grid-based%20spatial%20grounding%20and%0Amap%20analysis%20to%20help%20resolve%20complex%20multi-modal%20user%20queries%3B%20a%20Destination%0AAssistant%20Agent%20that%20provides%20fine-grained%20guidance%20for%20the%20final%20navigation%0Aleg%20of%20each%20journey%3B%20and%20a%20Local%20Discovery%20Agent%20that%20leverages%20image%0Aembeddings%20and%20Retrieval-Augmented%20Generation%20%28RAG%29%20to%20detect%20and%20respond%20to%0Atrip%20plan%20disruptions.%20With%20evaluations%20and%20experiments%2C%20our%20system%0Ademonstrates%20substantial%20improvements%20in%20query%20interpretation%2C%20navigation%0Aaccuracy%2C%20and%20disruption%20resilience%2C%20underscoring%20its%20promise%20for%20applications%0Afrom%20urban%20exploration%20to%20emergency%20response.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520User-Centric%2520Geo-Experience%253A%2520An%2520LLM-Powered%2520Framework%2520for%2520Enhanced%250A%2520%2520Planning%252C%2520Navigation%252C%2520and%2520Dynamic%2520Adaptation%26entry.906535625%3DJieren%2520Deng%2520and%2520Aleksandar%2520Cvetkovic%2520and%2520Pak%2520Kiu%2520Chung%2520and%2520Dragomir%2520Yankov%2520and%2520Chiqun%2520Zhang%26entry.1292438233%3D%2520%2520Traditional%2520travel-planning%2520systems%2520are%2520often%2520static%2520and%2520fragmented%252C%2520leaving%250Athem%2520ill-equipped%2520to%2520handle%2520real-world%2520complexities%2520such%2520as%2520evolving%250Aenvironmental%2520conditions%2520and%2520unexpected%2520itinerary%2520disruptions.%2520In%2520this%2520paper%252C%250Awe%2520identify%2520three%2520gaps%2520between%2520existing%2520service%2520providers%2520causing%2520frustrating%250Auser%2520experience%253A%2520intelligent%2520trip%2520planning%252C%2520precision%2520%2522last-100-meter%2522%250Anavigation%252C%2520and%2520dynamic%2520itinerary%2520adaptation.%2520We%2520propose%2520three%2520cooperative%250Aagents%253A%2520a%2520Travel%2520Planning%2520Agent%2520that%2520employs%2520grid-based%2520spatial%2520grounding%2520and%250Amap%2520analysis%2520to%2520help%2520resolve%2520complex%2520multi-modal%2520user%2520queries%253B%2520a%2520Destination%250AAssistant%2520Agent%2520that%2520provides%2520fine-grained%2520guidance%2520for%2520the%2520final%2520navigation%250Aleg%2520of%2520each%2520journey%253B%2520and%2520a%2520Local%2520Discovery%2520Agent%2520that%2520leverages%2520image%250Aembeddings%2520and%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520to%2520detect%2520and%2520respond%2520to%250Atrip%2520plan%2520disruptions.%2520With%2520evaluations%2520and%2520experiments%252C%2520our%2520system%250Ademonstrates%2520substantial%2520improvements%2520in%2520query%2520interpretation%252C%2520navigation%250Aaccuracy%252C%2520and%2520disruption%2520resilience%252C%2520underscoring%2520its%2520promise%2520for%2520applications%250Afrom%2520urban%2520exploration%2520to%2520emergency%2520response.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20User-Centric%20Geo-Experience%3A%20An%20LLM-Powered%20Framework%20for%20Enhanced%0A%20%20Planning%2C%20Navigation%2C%20and%20Dynamic%20Adaptation&entry.906535625=Jieren%20Deng%20and%20Aleksandar%20Cvetkovic%20and%20Pak%20Kiu%20Chung%20and%20Dragomir%20Yankov%20and%20Chiqun%20Zhang&entry.1292438233=%20%20Traditional%20travel-planning%20systems%20are%20often%20static%20and%20fragmented%2C%20leaving%0Athem%20ill-equipped%20to%20handle%20real-world%20complexities%20such%20as%20evolving%0Aenvironmental%20conditions%20and%20unexpected%20itinerary%20disruptions.%20In%20this%20paper%2C%0Awe%20identify%20three%20gaps%20between%20existing%20service%20providers%20causing%20frustrating%0Auser%20experience%3A%20intelligent%20trip%20planning%2C%20precision%20%22last-100-meter%22%0Anavigation%2C%20and%20dynamic%20itinerary%20adaptation.%20We%20propose%20three%20cooperative%0Aagents%3A%20a%20Travel%20Planning%20Agent%20that%20employs%20grid-based%20spatial%20grounding%20and%0Amap%20analysis%20to%20help%20resolve%20complex%20multi-modal%20user%20queries%3B%20a%20Destination%0AAssistant%20Agent%20that%20provides%20fine-grained%20guidance%20for%20the%20final%20navigation%0Aleg%20of%20each%20journey%3B%20and%20a%20Local%20Discovery%20Agent%20that%20leverages%20image%0Aembeddings%20and%20Retrieval-Augmented%20Generation%20%28RAG%29%20to%20detect%20and%20respond%20to%0Atrip%20plan%20disruptions.%20With%20evaluations%20and%20experiments%2C%20our%20system%0Ademonstrates%20substantial%20improvements%20in%20query%20interpretation%2C%20navigation%0Aaccuracy%2C%20and%20disruption%20resilience%2C%20underscoring%20its%20promise%20for%20applications%0Afrom%20urban%20exploration%20to%20emergency%20response.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06993v1&entry.124074799=Read"},
{"title": "An AI Approach for Learning the Spectrum of the Laplace-Beltrami\n  Operator", "author": "Yulin An and Enrique del Castillo", "abstract": "  The spectrum of the Laplace-Beltrami (LB) operator is central in geometric\ndeep learning tasks, capturing intrinsic properties of the shape of the object\nunder consideration. The best established method for its estimation, from a\ntriangulated mesh of the object, is based on the Finite Element Method (FEM),\nand computes the top k LB eigenvalues with a complexity of O(Nk), where N is\nthe number of points. This can render the FEM method inefficient when\nrepeatedly applied to databases of CAD mechanical parts, or in quality control\napplications where part metrology is acquired as large meshes and decisions\nabout the quality of each part are needed quickly and frequently. As a solution\nto this problem, we present a geometric deep learning framework to predict the\nLB spectrum efficiently given the CAD mesh of a part, achieving significant\ncomputational savings without sacrificing accuracy, demonstrating that the LB\nspectrum is learnable. The proposed Graph Neural Network architecture uses a\nrich set of part mesh features - including Gaussian curvature, mean curvature,\nand principal curvatures. In addition to our trained network, we make\navailable, for repeatability, a large curated dataset of real-world mechanical\nCAD models derived from the publicly available ABC dataset used for training\nand testing. Experimental results show that our method reduces computation time\nof the LB spectrum by approximately 5 times over linear FEM while delivering\ncompetitive accuracy.\n", "link": "http://arxiv.org/abs/2507.07073v1", "date": "2025-07-09", "relevancy": 2.137, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5553}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.52}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20AI%20Approach%20for%20Learning%20the%20Spectrum%20of%20the%20Laplace-Beltrami%0A%20%20Operator&body=Title%3A%20An%20AI%20Approach%20for%20Learning%20the%20Spectrum%20of%20the%20Laplace-Beltrami%0A%20%20Operator%0AAuthor%3A%20Yulin%20An%20and%20Enrique%20del%20Castillo%0AAbstract%3A%20%20%20The%20spectrum%20of%20the%20Laplace-Beltrami%20%28LB%29%20operator%20is%20central%20in%20geometric%0Adeep%20learning%20tasks%2C%20capturing%20intrinsic%20properties%20of%20the%20shape%20of%20the%20object%0Aunder%20consideration.%20The%20best%20established%20method%20for%20its%20estimation%2C%20from%20a%0Atriangulated%20mesh%20of%20the%20object%2C%20is%20based%20on%20the%20Finite%20Element%20Method%20%28FEM%29%2C%0Aand%20computes%20the%20top%20k%20LB%20eigenvalues%20with%20a%20complexity%20of%20O%28Nk%29%2C%20where%20N%20is%0Athe%20number%20of%20points.%20This%20can%20render%20the%20FEM%20method%20inefficient%20when%0Arepeatedly%20applied%20to%20databases%20of%20CAD%20mechanical%20parts%2C%20or%20in%20quality%20control%0Aapplications%20where%20part%20metrology%20is%20acquired%20as%20large%20meshes%20and%20decisions%0Aabout%20the%20quality%20of%20each%20part%20are%20needed%20quickly%20and%20frequently.%20As%20a%20solution%0Ato%20this%20problem%2C%20we%20present%20a%20geometric%20deep%20learning%20framework%20to%20predict%20the%0ALB%20spectrum%20efficiently%20given%20the%20CAD%20mesh%20of%20a%20part%2C%20achieving%20significant%0Acomputational%20savings%20without%20sacrificing%20accuracy%2C%20demonstrating%20that%20the%20LB%0Aspectrum%20is%20learnable.%20The%20proposed%20Graph%20Neural%20Network%20architecture%20uses%20a%0Arich%20set%20of%20part%20mesh%20features%20-%20including%20Gaussian%20curvature%2C%20mean%20curvature%2C%0Aand%20principal%20curvatures.%20In%20addition%20to%20our%20trained%20network%2C%20we%20make%0Aavailable%2C%20for%20repeatability%2C%20a%20large%20curated%20dataset%20of%20real-world%20mechanical%0ACAD%20models%20derived%20from%20the%20publicly%20available%20ABC%20dataset%20used%20for%20training%0Aand%20testing.%20Experimental%20results%20show%20that%20our%20method%20reduces%20computation%20time%0Aof%20the%20LB%20spectrum%20by%20approximately%205%20times%20over%20linear%20FEM%20while%20delivering%0Acompetitive%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520AI%2520Approach%2520for%2520Learning%2520the%2520Spectrum%2520of%2520the%2520Laplace-Beltrami%250A%2520%2520Operator%26entry.906535625%3DYulin%2520An%2520and%2520Enrique%2520del%2520Castillo%26entry.1292438233%3D%2520%2520The%2520spectrum%2520of%2520the%2520Laplace-Beltrami%2520%2528LB%2529%2520operator%2520is%2520central%2520in%2520geometric%250Adeep%2520learning%2520tasks%252C%2520capturing%2520intrinsic%2520properties%2520of%2520the%2520shape%2520of%2520the%2520object%250Aunder%2520consideration.%2520The%2520best%2520established%2520method%2520for%2520its%2520estimation%252C%2520from%2520a%250Atriangulated%2520mesh%2520of%2520the%2520object%252C%2520is%2520based%2520on%2520the%2520Finite%2520Element%2520Method%2520%2528FEM%2529%252C%250Aand%2520computes%2520the%2520top%2520k%2520LB%2520eigenvalues%2520with%2520a%2520complexity%2520of%2520O%2528Nk%2529%252C%2520where%2520N%2520is%250Athe%2520number%2520of%2520points.%2520This%2520can%2520render%2520the%2520FEM%2520method%2520inefficient%2520when%250Arepeatedly%2520applied%2520to%2520databases%2520of%2520CAD%2520mechanical%2520parts%252C%2520or%2520in%2520quality%2520control%250Aapplications%2520where%2520part%2520metrology%2520is%2520acquired%2520as%2520large%2520meshes%2520and%2520decisions%250Aabout%2520the%2520quality%2520of%2520each%2520part%2520are%2520needed%2520quickly%2520and%2520frequently.%2520As%2520a%2520solution%250Ato%2520this%2520problem%252C%2520we%2520present%2520a%2520geometric%2520deep%2520learning%2520framework%2520to%2520predict%2520the%250ALB%2520spectrum%2520efficiently%2520given%2520the%2520CAD%2520mesh%2520of%2520a%2520part%252C%2520achieving%2520significant%250Acomputational%2520savings%2520without%2520sacrificing%2520accuracy%252C%2520demonstrating%2520that%2520the%2520LB%250Aspectrum%2520is%2520learnable.%2520The%2520proposed%2520Graph%2520Neural%2520Network%2520architecture%2520uses%2520a%250Arich%2520set%2520of%2520part%2520mesh%2520features%2520-%2520including%2520Gaussian%2520curvature%252C%2520mean%2520curvature%252C%250Aand%2520principal%2520curvatures.%2520In%2520addition%2520to%2520our%2520trained%2520network%252C%2520we%2520make%250Aavailable%252C%2520for%2520repeatability%252C%2520a%2520large%2520curated%2520dataset%2520of%2520real-world%2520mechanical%250ACAD%2520models%2520derived%2520from%2520the%2520publicly%2520available%2520ABC%2520dataset%2520used%2520for%2520training%250Aand%2520testing.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520reduces%2520computation%2520time%250Aof%2520the%2520LB%2520spectrum%2520by%2520approximately%25205%2520times%2520over%2520linear%2520FEM%2520while%2520delivering%250Acompetitive%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20AI%20Approach%20for%20Learning%20the%20Spectrum%20of%20the%20Laplace-Beltrami%0A%20%20Operator&entry.906535625=Yulin%20An%20and%20Enrique%20del%20Castillo&entry.1292438233=%20%20The%20spectrum%20of%20the%20Laplace-Beltrami%20%28LB%29%20operator%20is%20central%20in%20geometric%0Adeep%20learning%20tasks%2C%20capturing%20intrinsic%20properties%20of%20the%20shape%20of%20the%20object%0Aunder%20consideration.%20The%20best%20established%20method%20for%20its%20estimation%2C%20from%20a%0Atriangulated%20mesh%20of%20the%20object%2C%20is%20based%20on%20the%20Finite%20Element%20Method%20%28FEM%29%2C%0Aand%20computes%20the%20top%20k%20LB%20eigenvalues%20with%20a%20complexity%20of%20O%28Nk%29%2C%20where%20N%20is%0Athe%20number%20of%20points.%20This%20can%20render%20the%20FEM%20method%20inefficient%20when%0Arepeatedly%20applied%20to%20databases%20of%20CAD%20mechanical%20parts%2C%20or%20in%20quality%20control%0Aapplications%20where%20part%20metrology%20is%20acquired%20as%20large%20meshes%20and%20decisions%0Aabout%20the%20quality%20of%20each%20part%20are%20needed%20quickly%20and%20frequently.%20As%20a%20solution%0Ato%20this%20problem%2C%20we%20present%20a%20geometric%20deep%20learning%20framework%20to%20predict%20the%0ALB%20spectrum%20efficiently%20given%20the%20CAD%20mesh%20of%20a%20part%2C%20achieving%20significant%0Acomputational%20savings%20without%20sacrificing%20accuracy%2C%20demonstrating%20that%20the%20LB%0Aspectrum%20is%20learnable.%20The%20proposed%20Graph%20Neural%20Network%20architecture%20uses%20a%0Arich%20set%20of%20part%20mesh%20features%20-%20including%20Gaussian%20curvature%2C%20mean%20curvature%2C%0Aand%20principal%20curvatures.%20In%20addition%20to%20our%20trained%20network%2C%20we%20make%0Aavailable%2C%20for%20repeatability%2C%20a%20large%20curated%20dataset%20of%20real-world%20mechanical%0ACAD%20models%20derived%20from%20the%20publicly%20available%20ABC%20dataset%20used%20for%20training%0Aand%20testing.%20Experimental%20results%20show%20that%20our%20method%20reduces%20computation%20time%0Aof%20the%20LB%20spectrum%20by%20approximately%205%20times%20over%20linear%20FEM%20while%20delivering%0Acompetitive%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07073v1&entry.124074799=Read"},
{"title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning\n  Large Language Models", "author": "Qiguang Chen and Libo Qin and Jinhao Liu and Dengyun Peng and Jiannan Guan and Peng Wang and Mengkang Hu and Yuhang Zhou and Te Gao and Wanxiang Che", "abstract": "  Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"inference-time scaling.\" This survey seeks\nto fill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and inference-time scaling,\noffering insights into how these processes manifest in practice. (4) Finally,\nwe identify significant research gaps and highlight promising future\ndirections, including the integration of multi-modal reasoning, efficiency\nimprovements, and enhanced knowledge frameworks. By providing a structured\noverview, this survey aims to inspire future research and further the\ndevelopment of logical reasoning in artificial intelligence.\n", "link": "http://arxiv.org/abs/2503.09567v4", "date": "2025-07-09", "relevancy": 2.1222, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Reasoning%20Era%3A%20A%20Survey%20of%20Long%20Chain-of-Thought%20for%20Reasoning%0A%20%20Large%20Language%20Models&body=Title%3A%20Towards%20Reasoning%20Era%3A%20A%20Survey%20of%20Long%20Chain-of-Thought%20for%20Reasoning%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Qiguang%20Chen%20and%20Libo%20Qin%20and%20Jinhao%20Liu%20and%20Dengyun%20Peng%20and%20Jiannan%20Guan%20and%20Peng%20Wang%20and%20Mengkang%20Hu%20and%20Yuhang%20Zhou%20and%20Te%20Gao%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20Recent%20advancements%20in%20reasoning%20with%20large%20language%20models%20%28RLLMs%29%2C%20such%20as%0AOpenAI-O1%20and%20DeepSeek-R1%2C%20have%20demonstrated%20their%20impressive%20capabilities%20in%0Acomplex%20domains%20like%20mathematics%20and%20coding.%20A%20central%20factor%20in%20their%20success%0Alies%20in%20the%20application%20of%20long%20chain-of-thought%20%28Long%20CoT%29%20characteristics%2C%0Awhich%20enhance%20reasoning%20abilities%20and%20enable%20the%20solution%20of%20intricate%0Aproblems.%20However%2C%20despite%20these%20developments%2C%20a%20comprehensive%20survey%20on%20Long%0ACoT%20is%20still%20lacking%2C%20limiting%20our%20understanding%20of%20its%20distinctions%20from%0Atraditional%20short%20chain-of-thought%20%28Short%20CoT%29%20and%20complicating%20ongoing%20debates%0Aon%20issues%20like%20%22overthinking%22%20and%20%22inference-time%20scaling.%22%20This%20survey%20seeks%0Ato%20fill%20this%20gap%20by%20offering%20a%20unified%20perspective%20on%20Long%20CoT.%20%281%29%20We%20first%0Adistinguish%20Long%20CoT%20from%20Short%20CoT%20and%20introduce%20a%20novel%20taxonomy%20to%0Acategorize%20current%20reasoning%20paradigms.%20%282%29%20Next%2C%20we%20explore%20the%20key%0Acharacteristics%20of%20Long%20CoT%3A%20deep%20reasoning%2C%20extensive%20exploration%2C%20and%0Afeasible%20reflection%2C%20which%20enable%20models%20to%20handle%20more%20complex%20tasks%20and%0Aproduce%20more%20efficient%2C%20coherent%20outcomes%20compared%20to%20the%20shallower%20Short%20CoT.%0A%283%29%20We%20then%20investigate%20key%20phenomena%20such%20as%20the%20emergence%20of%20Long%20CoT%20with%0Athese%20characteristics%2C%20including%20overthinking%2C%20and%20inference-time%20scaling%2C%0Aoffering%20insights%20into%20how%20these%20processes%20manifest%20in%20practice.%20%284%29%20Finally%2C%0Awe%20identify%20significant%20research%20gaps%20and%20highlight%20promising%20future%0Adirections%2C%20including%20the%20integration%20of%20multi-modal%20reasoning%2C%20efficiency%0Aimprovements%2C%20and%20enhanced%20knowledge%20frameworks.%20By%20providing%20a%20structured%0Aoverview%2C%20this%20survey%20aims%20to%20inspire%20future%20research%20and%20further%20the%0Adevelopment%20of%20logical%20reasoning%20in%20artificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09567v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Reasoning%2520Era%253A%2520A%2520Survey%2520of%2520Long%2520Chain-of-Thought%2520for%2520Reasoning%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DQiguang%2520Chen%2520and%2520Libo%2520Qin%2520and%2520Jinhao%2520Liu%2520and%2520Dengyun%2520Peng%2520and%2520Jiannan%2520Guan%2520and%2520Peng%2520Wang%2520and%2520Mengkang%2520Hu%2520and%2520Yuhang%2520Zhou%2520and%2520Te%2520Gao%2520and%2520Wanxiang%2520Che%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520reasoning%2520with%2520large%2520language%2520models%2520%2528RLLMs%2529%252C%2520such%2520as%250AOpenAI-O1%2520and%2520DeepSeek-R1%252C%2520have%2520demonstrated%2520their%2520impressive%2520capabilities%2520in%250Acomplex%2520domains%2520like%2520mathematics%2520and%2520coding.%2520A%2520central%2520factor%2520in%2520their%2520success%250Alies%2520in%2520the%2520application%2520of%2520long%2520chain-of-thought%2520%2528Long%2520CoT%2529%2520characteristics%252C%250Awhich%2520enhance%2520reasoning%2520abilities%2520and%2520enable%2520the%2520solution%2520of%2520intricate%250Aproblems.%2520However%252C%2520despite%2520these%2520developments%252C%2520a%2520comprehensive%2520survey%2520on%2520Long%250ACoT%2520is%2520still%2520lacking%252C%2520limiting%2520our%2520understanding%2520of%2520its%2520distinctions%2520from%250Atraditional%2520short%2520chain-of-thought%2520%2528Short%2520CoT%2529%2520and%2520complicating%2520ongoing%2520debates%250Aon%2520issues%2520like%2520%2522overthinking%2522%2520and%2520%2522inference-time%2520scaling.%2522%2520This%2520survey%2520seeks%250Ato%2520fill%2520this%2520gap%2520by%2520offering%2520a%2520unified%2520perspective%2520on%2520Long%2520CoT.%2520%25281%2529%2520We%2520first%250Adistinguish%2520Long%2520CoT%2520from%2520Short%2520CoT%2520and%2520introduce%2520a%2520novel%2520taxonomy%2520to%250Acategorize%2520current%2520reasoning%2520paradigms.%2520%25282%2529%2520Next%252C%2520we%2520explore%2520the%2520key%250Acharacteristics%2520of%2520Long%2520CoT%253A%2520deep%2520reasoning%252C%2520extensive%2520exploration%252C%2520and%250Afeasible%2520reflection%252C%2520which%2520enable%2520models%2520to%2520handle%2520more%2520complex%2520tasks%2520and%250Aproduce%2520more%2520efficient%252C%2520coherent%2520outcomes%2520compared%2520to%2520the%2520shallower%2520Short%2520CoT.%250A%25283%2529%2520We%2520then%2520investigate%2520key%2520phenomena%2520such%2520as%2520the%2520emergence%2520of%2520Long%2520CoT%2520with%250Athese%2520characteristics%252C%2520including%2520overthinking%252C%2520and%2520inference-time%2520scaling%252C%250Aoffering%2520insights%2520into%2520how%2520these%2520processes%2520manifest%2520in%2520practice.%2520%25284%2529%2520Finally%252C%250Awe%2520identify%2520significant%2520research%2520gaps%2520and%2520highlight%2520promising%2520future%250Adirections%252C%2520including%2520the%2520integration%2520of%2520multi-modal%2520reasoning%252C%2520efficiency%250Aimprovements%252C%2520and%2520enhanced%2520knowledge%2520frameworks.%2520By%2520providing%2520a%2520structured%250Aoverview%252C%2520this%2520survey%2520aims%2520to%2520inspire%2520future%2520research%2520and%2520further%2520the%250Adevelopment%2520of%2520logical%2520reasoning%2520in%2520artificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09567v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Reasoning%20Era%3A%20A%20Survey%20of%20Long%20Chain-of-Thought%20for%20Reasoning%0A%20%20Large%20Language%20Models&entry.906535625=Qiguang%20Chen%20and%20Libo%20Qin%20and%20Jinhao%20Liu%20and%20Dengyun%20Peng%20and%20Jiannan%20Guan%20and%20Peng%20Wang%20and%20Mengkang%20Hu%20and%20Yuhang%20Zhou%20and%20Te%20Gao%20and%20Wanxiang%20Che&entry.1292438233=%20%20Recent%20advancements%20in%20reasoning%20with%20large%20language%20models%20%28RLLMs%29%2C%20such%20as%0AOpenAI-O1%20and%20DeepSeek-R1%2C%20have%20demonstrated%20their%20impressive%20capabilities%20in%0Acomplex%20domains%20like%20mathematics%20and%20coding.%20A%20central%20factor%20in%20their%20success%0Alies%20in%20the%20application%20of%20long%20chain-of-thought%20%28Long%20CoT%29%20characteristics%2C%0Awhich%20enhance%20reasoning%20abilities%20and%20enable%20the%20solution%20of%20intricate%0Aproblems.%20However%2C%20despite%20these%20developments%2C%20a%20comprehensive%20survey%20on%20Long%0ACoT%20is%20still%20lacking%2C%20limiting%20our%20understanding%20of%20its%20distinctions%20from%0Atraditional%20short%20chain-of-thought%20%28Short%20CoT%29%20and%20complicating%20ongoing%20debates%0Aon%20issues%20like%20%22overthinking%22%20and%20%22inference-time%20scaling.%22%20This%20survey%20seeks%0Ato%20fill%20this%20gap%20by%20offering%20a%20unified%20perspective%20on%20Long%20CoT.%20%281%29%20We%20first%0Adistinguish%20Long%20CoT%20from%20Short%20CoT%20and%20introduce%20a%20novel%20taxonomy%20to%0Acategorize%20current%20reasoning%20paradigms.%20%282%29%20Next%2C%20we%20explore%20the%20key%0Acharacteristics%20of%20Long%20CoT%3A%20deep%20reasoning%2C%20extensive%20exploration%2C%20and%0Afeasible%20reflection%2C%20which%20enable%20models%20to%20handle%20more%20complex%20tasks%20and%0Aproduce%20more%20efficient%2C%20coherent%20outcomes%20compared%20to%20the%20shallower%20Short%20CoT.%0A%283%29%20We%20then%20investigate%20key%20phenomena%20such%20as%20the%20emergence%20of%20Long%20CoT%20with%0Athese%20characteristics%2C%20including%20overthinking%2C%20and%20inference-time%20scaling%2C%0Aoffering%20insights%20into%20how%20these%20processes%20manifest%20in%20practice.%20%284%29%20Finally%2C%0Awe%20identify%20significant%20research%20gaps%20and%20highlight%20promising%20future%0Adirections%2C%20including%20the%20integration%20of%20multi-modal%20reasoning%2C%20efficiency%0Aimprovements%2C%20and%20enhanced%20knowledge%20frameworks.%20By%20providing%20a%20structured%0Aoverview%2C%20this%20survey%20aims%20to%20inspire%20future%20research%20and%20further%20the%0Adevelopment%20of%20logical%20reasoning%20in%20artificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09567v4&entry.124074799=Read"},
{"title": "Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with\n  Online EM", "author": "Qiyuan Dai and Sibei Yang", "abstract": "  Vision-Language Models (VLMs) have become prominent in open-world image\nrecognition for their strong generalization abilities. Yet, their effectiveness\nin practical applications is compromised by domain shifts and distributional\nchanges, especially when test data distributions diverge from training data.\nTherefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the\nuse of online off-the-shelf data at test time, supporting independent sample\npredictions, and eliminating reliance on test annotations. Traditional TTA\nmethods, however, often rely on costly training or optimization processes, or\nmake unrealistic assumptions about accessing or storing historical training and\ntest data. Instead, this study proposes FreeTTA, a training-free and\nuniversally available method that makes no assumptions, to enhance the\nflexibility of TTA. More importantly, FreeTTA is the first to explicitly model\nthe test data distribution, enabling the use of intrinsic relationships among\ntest samples to enhance predictions of individual samples without simultaneous\naccess--a direction not previously explored. FreeTTA achieves these advantages\nby introducing an online EM algorithm that utilizes zero-shot predictions from\nVLMs as priors to iteratively compute the posterior probabilities of each\nonline test sample and update parameters. Experiments demonstrate that FreeTTA\nachieves stable and significant improvements compared to state-of-the-art\nmethods across 15 datasets in both cross-domain and out-of-distribution\nsettings.\n", "link": "http://arxiv.org/abs/2507.06973v1", "date": "2025-07-09", "relevancy": 2.1207, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.545}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5265}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free%20on%20the%20Fly%3A%20Enhancing%20Flexibility%20in%20Test-Time%20Adaptation%20with%0A%20%20Online%20EM&body=Title%3A%20Free%20on%20the%20Fly%3A%20Enhancing%20Flexibility%20in%20Test-Time%20Adaptation%20with%0A%20%20Online%20EM%0AAuthor%3A%20Qiyuan%20Dai%20and%20Sibei%20Yang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20become%20prominent%20in%20open-world%20image%0Arecognition%20for%20their%20strong%20generalization%20abilities.%20Yet%2C%20their%20effectiveness%0Ain%20practical%20applications%20is%20compromised%20by%20domain%20shifts%20and%20distributional%0Achanges%2C%20especially%20when%20test%20data%20distributions%20diverge%20from%20training%20data.%0ATherefore%2C%20the%20paradigm%20of%20test-time%20adaptation%20%28TTA%29%20has%20emerged%2C%20enabling%20the%0Ause%20of%20online%20off-the-shelf%20data%20at%20test%20time%2C%20supporting%20independent%20sample%0Apredictions%2C%20and%20eliminating%20reliance%20on%20test%20annotations.%20Traditional%20TTA%0Amethods%2C%20however%2C%20often%20rely%20on%20costly%20training%20or%20optimization%20processes%2C%20or%0Amake%20unrealistic%20assumptions%20about%20accessing%20or%20storing%20historical%20training%20and%0Atest%20data.%20Instead%2C%20this%20study%20proposes%20FreeTTA%2C%20a%20training-free%20and%0Auniversally%20available%20method%20that%20makes%20no%20assumptions%2C%20to%20enhance%20the%0Aflexibility%20of%20TTA.%20More%20importantly%2C%20FreeTTA%20is%20the%20first%20to%20explicitly%20model%0Athe%20test%20data%20distribution%2C%20enabling%20the%20use%20of%20intrinsic%20relationships%20among%0Atest%20samples%20to%20enhance%20predictions%20of%20individual%20samples%20without%20simultaneous%0Aaccess--a%20direction%20not%20previously%20explored.%20FreeTTA%20achieves%20these%20advantages%0Aby%20introducing%20an%20online%20EM%20algorithm%20that%20utilizes%20zero-shot%20predictions%20from%0AVLMs%20as%20priors%20to%20iteratively%20compute%20the%20posterior%20probabilities%20of%20each%0Aonline%20test%20sample%20and%20update%20parameters.%20Experiments%20demonstrate%20that%20FreeTTA%0Aachieves%20stable%20and%20significant%20improvements%20compared%20to%20state-of-the-art%0Amethods%20across%2015%20datasets%20in%20both%20cross-domain%20and%20out-of-distribution%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree%2520on%2520the%2520Fly%253A%2520Enhancing%2520Flexibility%2520in%2520Test-Time%2520Adaptation%2520with%250A%2520%2520Online%2520EM%26entry.906535625%3DQiyuan%2520Dai%2520and%2520Sibei%2520Yang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520become%2520prominent%2520in%2520open-world%2520image%250Arecognition%2520for%2520their%2520strong%2520generalization%2520abilities.%2520Yet%252C%2520their%2520effectiveness%250Ain%2520practical%2520applications%2520is%2520compromised%2520by%2520domain%2520shifts%2520and%2520distributional%250Achanges%252C%2520especially%2520when%2520test%2520data%2520distributions%2520diverge%2520from%2520training%2520data.%250ATherefore%252C%2520the%2520paradigm%2520of%2520test-time%2520adaptation%2520%2528TTA%2529%2520has%2520emerged%252C%2520enabling%2520the%250Ause%2520of%2520online%2520off-the-shelf%2520data%2520at%2520test%2520time%252C%2520supporting%2520independent%2520sample%250Apredictions%252C%2520and%2520eliminating%2520reliance%2520on%2520test%2520annotations.%2520Traditional%2520TTA%250Amethods%252C%2520however%252C%2520often%2520rely%2520on%2520costly%2520training%2520or%2520optimization%2520processes%252C%2520or%250Amake%2520unrealistic%2520assumptions%2520about%2520accessing%2520or%2520storing%2520historical%2520training%2520and%250Atest%2520data.%2520Instead%252C%2520this%2520study%2520proposes%2520FreeTTA%252C%2520a%2520training-free%2520and%250Auniversally%2520available%2520method%2520that%2520makes%2520no%2520assumptions%252C%2520to%2520enhance%2520the%250Aflexibility%2520of%2520TTA.%2520More%2520importantly%252C%2520FreeTTA%2520is%2520the%2520first%2520to%2520explicitly%2520model%250Athe%2520test%2520data%2520distribution%252C%2520enabling%2520the%2520use%2520of%2520intrinsic%2520relationships%2520among%250Atest%2520samples%2520to%2520enhance%2520predictions%2520of%2520individual%2520samples%2520without%2520simultaneous%250Aaccess--a%2520direction%2520not%2520previously%2520explored.%2520FreeTTA%2520achieves%2520these%2520advantages%250Aby%2520introducing%2520an%2520online%2520EM%2520algorithm%2520that%2520utilizes%2520zero-shot%2520predictions%2520from%250AVLMs%2520as%2520priors%2520to%2520iteratively%2520compute%2520the%2520posterior%2520probabilities%2520of%2520each%250Aonline%2520test%2520sample%2520and%2520update%2520parameters.%2520Experiments%2520demonstrate%2520that%2520FreeTTA%250Aachieves%2520stable%2520and%2520significant%2520improvements%2520compared%2520to%2520state-of-the-art%250Amethods%2520across%252015%2520datasets%2520in%2520both%2520cross-domain%2520and%2520out-of-distribution%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free%20on%20the%20Fly%3A%20Enhancing%20Flexibility%20in%20Test-Time%20Adaptation%20with%0A%20%20Online%20EM&entry.906535625=Qiyuan%20Dai%20and%20Sibei%20Yang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20become%20prominent%20in%20open-world%20image%0Arecognition%20for%20their%20strong%20generalization%20abilities.%20Yet%2C%20their%20effectiveness%0Ain%20practical%20applications%20is%20compromised%20by%20domain%20shifts%20and%20distributional%0Achanges%2C%20especially%20when%20test%20data%20distributions%20diverge%20from%20training%20data.%0ATherefore%2C%20the%20paradigm%20of%20test-time%20adaptation%20%28TTA%29%20has%20emerged%2C%20enabling%20the%0Ause%20of%20online%20off-the-shelf%20data%20at%20test%20time%2C%20supporting%20independent%20sample%0Apredictions%2C%20and%20eliminating%20reliance%20on%20test%20annotations.%20Traditional%20TTA%0Amethods%2C%20however%2C%20often%20rely%20on%20costly%20training%20or%20optimization%20processes%2C%20or%0Amake%20unrealistic%20assumptions%20about%20accessing%20or%20storing%20historical%20training%20and%0Atest%20data.%20Instead%2C%20this%20study%20proposes%20FreeTTA%2C%20a%20training-free%20and%0Auniversally%20available%20method%20that%20makes%20no%20assumptions%2C%20to%20enhance%20the%0Aflexibility%20of%20TTA.%20More%20importantly%2C%20FreeTTA%20is%20the%20first%20to%20explicitly%20model%0Athe%20test%20data%20distribution%2C%20enabling%20the%20use%20of%20intrinsic%20relationships%20among%0Atest%20samples%20to%20enhance%20predictions%20of%20individual%20samples%20without%20simultaneous%0Aaccess--a%20direction%20not%20previously%20explored.%20FreeTTA%20achieves%20these%20advantages%0Aby%20introducing%20an%20online%20EM%20algorithm%20that%20utilizes%20zero-shot%20predictions%20from%0AVLMs%20as%20priors%20to%20iteratively%20compute%20the%20posterior%20probabilities%20of%20each%0Aonline%20test%20sample%20and%20update%20parameters.%20Experiments%20demonstrate%20that%20FreeTTA%0Aachieves%20stable%20and%20significant%20improvements%20compared%20to%20state-of-the-art%0Amethods%20across%2015%20datasets%20in%20both%20cross-domain%20and%20out-of-distribution%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06973v1&entry.124074799=Read"},
{"title": "Path-following model predictive control for autonomous e-scooters", "author": "David Meister and Robin Str\u00e4sser and Felix Br\u00e4ndle and Marc Seidel and Benno Bassler and Nathan Gerber and Jan Kautz and Elena Rommel and Frank Allg\u00f6wer", "abstract": "  In order to mitigate economical, ecological, and societal challenges in\nelectric scooter (e-scooter) sharing systems, we develop an autonomous\ne-scooter prototype. Our vision is to design a fully autonomous prototype that\ncan find its way to the next parking spot, high-demand area, or charging\nstation. In this work, we propose a path-following model predictive control\nsolution to enable localization and navigation in an urban environment with a\nprovided path to follow. We design a closed-loop architecture that solves the\nlocalization and path following problem while allowing the e-scooter to\nmaintain its balance with a previously developed reaction wheel mechanism. Our\nmodel predictive control approach facilitates state and input constraints,\ne.g., adhering to the path width, while remaining executable on a Raspberry Pi\n5. We demonstrate the efficacy of our approach in a real-world experiment on\nour prototype.\n", "link": "http://arxiv.org/abs/2505.05314v2", "date": "2025-07-09", "relevancy": 2.1203, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5361}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Path-following%20model%20predictive%20control%20for%20autonomous%20e-scooters&body=Title%3A%20Path-following%20model%20predictive%20control%20for%20autonomous%20e-scooters%0AAuthor%3A%20David%20Meister%20and%20Robin%20Str%C3%A4sser%20and%20Felix%20Br%C3%A4ndle%20and%20Marc%20Seidel%20and%20Benno%20Bassler%20and%20Nathan%20Gerber%20and%20Jan%20Kautz%20and%20Elena%20Rommel%20and%20Frank%20Allg%C3%B6wer%0AAbstract%3A%20%20%20In%20order%20to%20mitigate%20economical%2C%20ecological%2C%20and%20societal%20challenges%20in%0Aelectric%20scooter%20%28e-scooter%29%20sharing%20systems%2C%20we%20develop%20an%20autonomous%0Ae-scooter%20prototype.%20Our%20vision%20is%20to%20design%20a%20fully%20autonomous%20prototype%20that%0Acan%20find%20its%20way%20to%20the%20next%20parking%20spot%2C%20high-demand%20area%2C%20or%20charging%0Astation.%20In%20this%20work%2C%20we%20propose%20a%20path-following%20model%20predictive%20control%0Asolution%20to%20enable%20localization%20and%20navigation%20in%20an%20urban%20environment%20with%20a%0Aprovided%20path%20to%20follow.%20We%20design%20a%20closed-loop%20architecture%20that%20solves%20the%0Alocalization%20and%20path%20following%20problem%20while%20allowing%20the%20e-scooter%20to%0Amaintain%20its%20balance%20with%20a%20previously%20developed%20reaction%20wheel%20mechanism.%20Our%0Amodel%20predictive%20control%20approach%20facilitates%20state%20and%20input%20constraints%2C%0Ae.g.%2C%20adhering%20to%20the%20path%20width%2C%20while%20remaining%20executable%20on%20a%20Raspberry%20Pi%0A5.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%20a%20real-world%20experiment%20on%0Aour%20prototype.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05314v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPath-following%2520model%2520predictive%2520control%2520for%2520autonomous%2520e-scooters%26entry.906535625%3DDavid%2520Meister%2520and%2520Robin%2520Str%25C3%25A4sser%2520and%2520Felix%2520Br%25C3%25A4ndle%2520and%2520Marc%2520Seidel%2520and%2520Benno%2520Bassler%2520and%2520Nathan%2520Gerber%2520and%2520Jan%2520Kautz%2520and%2520Elena%2520Rommel%2520and%2520Frank%2520Allg%25C3%25B6wer%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520mitigate%2520economical%252C%2520ecological%252C%2520and%2520societal%2520challenges%2520in%250Aelectric%2520scooter%2520%2528e-scooter%2529%2520sharing%2520systems%252C%2520we%2520develop%2520an%2520autonomous%250Ae-scooter%2520prototype.%2520Our%2520vision%2520is%2520to%2520design%2520a%2520fully%2520autonomous%2520prototype%2520that%250Acan%2520find%2520its%2520way%2520to%2520the%2520next%2520parking%2520spot%252C%2520high-demand%2520area%252C%2520or%2520charging%250Astation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520path-following%2520model%2520predictive%2520control%250Asolution%2520to%2520enable%2520localization%2520and%2520navigation%2520in%2520an%2520urban%2520environment%2520with%2520a%250Aprovided%2520path%2520to%2520follow.%2520We%2520design%2520a%2520closed-loop%2520architecture%2520that%2520solves%2520the%250Alocalization%2520and%2520path%2520following%2520problem%2520while%2520allowing%2520the%2520e-scooter%2520to%250Amaintain%2520its%2520balance%2520with%2520a%2520previously%2520developed%2520reaction%2520wheel%2520mechanism.%2520Our%250Amodel%2520predictive%2520control%2520approach%2520facilitates%2520state%2520and%2520input%2520constraints%252C%250Ae.g.%252C%2520adhering%2520to%2520the%2520path%2520width%252C%2520while%2520remaining%2520executable%2520on%2520a%2520Raspberry%2520Pi%250A5.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach%2520in%2520a%2520real-world%2520experiment%2520on%250Aour%2520prototype.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05314v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Path-following%20model%20predictive%20control%20for%20autonomous%20e-scooters&entry.906535625=David%20Meister%20and%20Robin%20Str%C3%A4sser%20and%20Felix%20Br%C3%A4ndle%20and%20Marc%20Seidel%20and%20Benno%20Bassler%20and%20Nathan%20Gerber%20and%20Jan%20Kautz%20and%20Elena%20Rommel%20and%20Frank%20Allg%C3%B6wer&entry.1292438233=%20%20In%20order%20to%20mitigate%20economical%2C%20ecological%2C%20and%20societal%20challenges%20in%0Aelectric%20scooter%20%28e-scooter%29%20sharing%20systems%2C%20we%20develop%20an%20autonomous%0Ae-scooter%20prototype.%20Our%20vision%20is%20to%20design%20a%20fully%20autonomous%20prototype%20that%0Acan%20find%20its%20way%20to%20the%20next%20parking%20spot%2C%20high-demand%20area%2C%20or%20charging%0Astation.%20In%20this%20work%2C%20we%20propose%20a%20path-following%20model%20predictive%20control%0Asolution%20to%20enable%20localization%20and%20navigation%20in%20an%20urban%20environment%20with%20a%0Aprovided%20path%20to%20follow.%20We%20design%20a%20closed-loop%20architecture%20that%20solves%20the%0Alocalization%20and%20path%20following%20problem%20while%20allowing%20the%20e-scooter%20to%0Amaintain%20its%20balance%20with%20a%20previously%20developed%20reaction%20wheel%20mechanism.%20Our%0Amodel%20predictive%20control%20approach%20facilitates%20state%20and%20input%20constraints%2C%0Ae.g.%2C%20adhering%20to%20the%20path%20width%2C%20while%20remaining%20executable%20on%20a%20Raspberry%20Pi%0A5.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%20a%20real-world%20experiment%20on%0Aour%20prototype.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05314v2&entry.124074799=Read"},
{"title": "Conformal Prediction for Long-Tailed Classification", "author": "Tiffany Ding and Jean-Baptiste Fermanian and Joseph Salmon", "abstract": "  Many real-world classification problems, such as plant identification, have\nextremely long-tailed class distributions. In order for prediction sets to be\nuseful in such settings, they should (i) provide good class-conditional\ncoverage, ensuring that rare classes are not systematically omitted from the\nprediction sets, and (ii) be a reasonable size, allowing users to easily verify\ncandidate labels. Unfortunately, existing conformal prediction methods, when\napplied to the long-tailed setting, force practitioners to make a binary choice\nbetween small sets with poor class-conditional coverage or sets with very good\nclass-conditional coverage but that are extremely large. We propose methods\nwith guaranteed marginal coverage that smoothly trade off between set size and\nclass-conditional coverage. First, we propose a conformal score function,\nprevalence-adjusted softmax, that targets a relaxed notion of class-conditional\ncoverage called macro-coverage. Second, we propose a label-weighted conformal\nprediction method that allows us to interpolate between marginal and\nclass-conditional conformal prediction. We demonstrate our methods on Pl@ntNet\nand iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,\nrespectively.\n", "link": "http://arxiv.org/abs/2507.06867v1", "date": "2025-07-09", "relevancy": 2.1093, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5724}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5027}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20for%20Long-Tailed%20Classification&body=Title%3A%20Conformal%20Prediction%20for%20Long-Tailed%20Classification%0AAuthor%3A%20Tiffany%20Ding%20and%20Jean-Baptiste%20Fermanian%20and%20Joseph%20Salmon%0AAbstract%3A%20%20%20Many%20real-world%20classification%20problems%2C%20such%20as%20plant%20identification%2C%20have%0Aextremely%20long-tailed%20class%20distributions.%20In%20order%20for%20prediction%20sets%20to%20be%0Auseful%20in%20such%20settings%2C%20they%20should%20%28i%29%20provide%20good%20class-conditional%0Acoverage%2C%20ensuring%20that%20rare%20classes%20are%20not%20systematically%20omitted%20from%20the%0Aprediction%20sets%2C%20and%20%28ii%29%20be%20a%20reasonable%20size%2C%20allowing%20users%20to%20easily%20verify%0Acandidate%20labels.%20Unfortunately%2C%20existing%20conformal%20prediction%20methods%2C%20when%0Aapplied%20to%20the%20long-tailed%20setting%2C%20force%20practitioners%20to%20make%20a%20binary%20choice%0Abetween%20small%20sets%20with%20poor%20class-conditional%20coverage%20or%20sets%20with%20very%20good%0Aclass-conditional%20coverage%20but%20that%20are%20extremely%20large.%20We%20propose%20methods%0Awith%20guaranteed%20marginal%20coverage%20that%20smoothly%20trade%20off%20between%20set%20size%20and%0Aclass-conditional%20coverage.%20First%2C%20we%20propose%20a%20conformal%20score%20function%2C%0Aprevalence-adjusted%20softmax%2C%20that%20targets%20a%20relaxed%20notion%20of%20class-conditional%0Acoverage%20called%20macro-coverage.%20Second%2C%20we%20propose%20a%20label-weighted%20conformal%0Aprediction%20method%20that%20allows%20us%20to%20interpolate%20between%20marginal%20and%0Aclass-conditional%20conformal%20prediction.%20We%20demonstrate%20our%20methods%20on%20Pl%40ntNet%0Aand%20iNaturalist%2C%20two%20long-tailed%20image%20datasets%20with%201%2C081%20and%208%2C142%20classes%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Prediction%2520for%2520Long-Tailed%2520Classification%26entry.906535625%3DTiffany%2520Ding%2520and%2520Jean-Baptiste%2520Fermanian%2520and%2520Joseph%2520Salmon%26entry.1292438233%3D%2520%2520Many%2520real-world%2520classification%2520problems%252C%2520such%2520as%2520plant%2520identification%252C%2520have%250Aextremely%2520long-tailed%2520class%2520distributions.%2520In%2520order%2520for%2520prediction%2520sets%2520to%2520be%250Auseful%2520in%2520such%2520settings%252C%2520they%2520should%2520%2528i%2529%2520provide%2520good%2520class-conditional%250Acoverage%252C%2520ensuring%2520that%2520rare%2520classes%2520are%2520not%2520systematically%2520omitted%2520from%2520the%250Aprediction%2520sets%252C%2520and%2520%2528ii%2529%2520be%2520a%2520reasonable%2520size%252C%2520allowing%2520users%2520to%2520easily%2520verify%250Acandidate%2520labels.%2520Unfortunately%252C%2520existing%2520conformal%2520prediction%2520methods%252C%2520when%250Aapplied%2520to%2520the%2520long-tailed%2520setting%252C%2520force%2520practitioners%2520to%2520make%2520a%2520binary%2520choice%250Abetween%2520small%2520sets%2520with%2520poor%2520class-conditional%2520coverage%2520or%2520sets%2520with%2520very%2520good%250Aclass-conditional%2520coverage%2520but%2520that%2520are%2520extremely%2520large.%2520We%2520propose%2520methods%250Awith%2520guaranteed%2520marginal%2520coverage%2520that%2520smoothly%2520trade%2520off%2520between%2520set%2520size%2520and%250Aclass-conditional%2520coverage.%2520First%252C%2520we%2520propose%2520a%2520conformal%2520score%2520function%252C%250Aprevalence-adjusted%2520softmax%252C%2520that%2520targets%2520a%2520relaxed%2520notion%2520of%2520class-conditional%250Acoverage%2520called%2520macro-coverage.%2520Second%252C%2520we%2520propose%2520a%2520label-weighted%2520conformal%250Aprediction%2520method%2520that%2520allows%2520us%2520to%2520interpolate%2520between%2520marginal%2520and%250Aclass-conditional%2520conformal%2520prediction.%2520We%2520demonstrate%2520our%2520methods%2520on%2520Pl%2540ntNet%250Aand%2520iNaturalist%252C%2520two%2520long-tailed%2520image%2520datasets%2520with%25201%252C081%2520and%25208%252C142%2520classes%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20for%20Long-Tailed%20Classification&entry.906535625=Tiffany%20Ding%20and%20Jean-Baptiste%20Fermanian%20and%20Joseph%20Salmon&entry.1292438233=%20%20Many%20real-world%20classification%20problems%2C%20such%20as%20plant%20identification%2C%20have%0Aextremely%20long-tailed%20class%20distributions.%20In%20order%20for%20prediction%20sets%20to%20be%0Auseful%20in%20such%20settings%2C%20they%20should%20%28i%29%20provide%20good%20class-conditional%0Acoverage%2C%20ensuring%20that%20rare%20classes%20are%20not%20systematically%20omitted%20from%20the%0Aprediction%20sets%2C%20and%20%28ii%29%20be%20a%20reasonable%20size%2C%20allowing%20users%20to%20easily%20verify%0Acandidate%20labels.%20Unfortunately%2C%20existing%20conformal%20prediction%20methods%2C%20when%0Aapplied%20to%20the%20long-tailed%20setting%2C%20force%20practitioners%20to%20make%20a%20binary%20choice%0Abetween%20small%20sets%20with%20poor%20class-conditional%20coverage%20or%20sets%20with%20very%20good%0Aclass-conditional%20coverage%20but%20that%20are%20extremely%20large.%20We%20propose%20methods%0Awith%20guaranteed%20marginal%20coverage%20that%20smoothly%20trade%20off%20between%20set%20size%20and%0Aclass-conditional%20coverage.%20First%2C%20we%20propose%20a%20conformal%20score%20function%2C%0Aprevalence-adjusted%20softmax%2C%20that%20targets%20a%20relaxed%20notion%20of%20class-conditional%0Acoverage%20called%20macro-coverage.%20Second%2C%20we%20propose%20a%20label-weighted%20conformal%0Aprediction%20method%20that%20allows%20us%20to%20interpolate%20between%20marginal%20and%0Aclass-conditional%20conformal%20prediction.%20We%20demonstrate%20our%20methods%20on%20Pl%40ntNet%0Aand%20iNaturalist%2C%20two%20long-tailed%20image%20datasets%20with%201%2C081%20and%208%2C142%20classes%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06867v1&entry.124074799=Read"},
{"title": "TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for\n  Benchmarking Target Person Tracking", "author": "Hanjing Ye and Yu Zhan and Weixi Situ and Guangcheng Chen and Jingwen Yu and Ziqi Zhao and Kuanqi Cai and Arash Ajoudani and Hong Zhang", "abstract": "  Tracking a target person from robot-egocentric views is crucial for\ndeveloping autonomous robots that provide continuous personalized assistance or\ncollaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most\nexisting target person tracking (TPT) benchmarks are limited to controlled\nlaboratory environments with few distractions, clean backgrounds, and\nshort-term occlusions. In this paper, we introduce a large-scale dataset\ndesigned for TPT in crowded and unstructured environments, demonstrated through\na robot-person following task. The dataset is collected by a human pushing a\nsensor-equipped cart while following a target person, capturing human-like\nfollowing behavior and emphasizing long-term tracking challenges, including\nfrequent occlusions and the need for re-identification from numerous\npedestrians. It includes multi-modal data streams, including odometry, 3D\nLiDAR, IMU, panoramic images, and RGB-D images, along with exhaustively\nannotated 2D bounding boxes of the target person across 48 sequences, both\nindoors and outdoors. Using this dataset and visual annotations, we perform\nextensive experiments with existing SOTA TPT methods, offering a thorough\nanalysis of their limitations and suggesting future research directions.\n", "link": "http://arxiv.org/abs/2505.07446v2", "date": "2025-07-09", "relevancy": 2.1092, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5352}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5307}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TPT-Bench%3A%20A%20Large-Scale%2C%20Long-Term%20and%20Robot-Egocentric%20Dataset%20for%0A%20%20Benchmarking%20Target%20Person%20Tracking&body=Title%3A%20TPT-Bench%3A%20A%20Large-Scale%2C%20Long-Term%20and%20Robot-Egocentric%20Dataset%20for%0A%20%20Benchmarking%20Target%20Person%20Tracking%0AAuthor%3A%20Hanjing%20Ye%20and%20Yu%20Zhan%20and%20Weixi%20Situ%20and%20Guangcheng%20Chen%20and%20Jingwen%20Yu%20and%20Ziqi%20Zhao%20and%20Kuanqi%20Cai%20and%20Arash%20Ajoudani%20and%20Hong%20Zhang%0AAbstract%3A%20%20%20Tracking%20a%20target%20person%20from%20robot-egocentric%20views%20is%20crucial%20for%0Adeveloping%20autonomous%20robots%20that%20provide%20continuous%20personalized%20assistance%20or%0Acollaboration%20in%20Human-Robot%20Interaction%20%28HRI%29%20and%20Embodied%20AI.%20However%2C%20most%0Aexisting%20target%20person%20tracking%20%28TPT%29%20benchmarks%20are%20limited%20to%20controlled%0Alaboratory%20environments%20with%20few%20distractions%2C%20clean%20backgrounds%2C%20and%0Ashort-term%20occlusions.%20In%20this%20paper%2C%20we%20introduce%20a%20large-scale%20dataset%0Adesigned%20for%20TPT%20in%20crowded%20and%20unstructured%20environments%2C%20demonstrated%20through%0Aa%20robot-person%20following%20task.%20The%20dataset%20is%20collected%20by%20a%20human%20pushing%20a%0Asensor-equipped%20cart%20while%20following%20a%20target%20person%2C%20capturing%20human-like%0Afollowing%20behavior%20and%20emphasizing%20long-term%20tracking%20challenges%2C%20including%0Afrequent%20occlusions%20and%20the%20need%20for%20re-identification%20from%20numerous%0Apedestrians.%20It%20includes%20multi-modal%20data%20streams%2C%20including%20odometry%2C%203D%0ALiDAR%2C%20IMU%2C%20panoramic%20images%2C%20and%20RGB-D%20images%2C%20along%20with%20exhaustively%0Aannotated%202D%20bounding%20boxes%20of%20the%20target%20person%20across%2048%20sequences%2C%20both%0Aindoors%20and%20outdoors.%20Using%20this%20dataset%20and%20visual%20annotations%2C%20we%20perform%0Aextensive%20experiments%20with%20existing%20SOTA%20TPT%20methods%2C%20offering%20a%20thorough%0Aanalysis%20of%20their%20limitations%20and%20suggesting%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07446v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTPT-Bench%253A%2520A%2520Large-Scale%252C%2520Long-Term%2520and%2520Robot-Egocentric%2520Dataset%2520for%250A%2520%2520Benchmarking%2520Target%2520Person%2520Tracking%26entry.906535625%3DHanjing%2520Ye%2520and%2520Yu%2520Zhan%2520and%2520Weixi%2520Situ%2520and%2520Guangcheng%2520Chen%2520and%2520Jingwen%2520Yu%2520and%2520Ziqi%2520Zhao%2520and%2520Kuanqi%2520Cai%2520and%2520Arash%2520Ajoudani%2520and%2520Hong%2520Zhang%26entry.1292438233%3D%2520%2520Tracking%2520a%2520target%2520person%2520from%2520robot-egocentric%2520views%2520is%2520crucial%2520for%250Adeveloping%2520autonomous%2520robots%2520that%2520provide%2520continuous%2520personalized%2520assistance%2520or%250Acollaboration%2520in%2520Human-Robot%2520Interaction%2520%2528HRI%2529%2520and%2520Embodied%2520AI.%2520However%252C%2520most%250Aexisting%2520target%2520person%2520tracking%2520%2528TPT%2529%2520benchmarks%2520are%2520limited%2520to%2520controlled%250Alaboratory%2520environments%2520with%2520few%2520distractions%252C%2520clean%2520backgrounds%252C%2520and%250Ashort-term%2520occlusions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520large-scale%2520dataset%250Adesigned%2520for%2520TPT%2520in%2520crowded%2520and%2520unstructured%2520environments%252C%2520demonstrated%2520through%250Aa%2520robot-person%2520following%2520task.%2520The%2520dataset%2520is%2520collected%2520by%2520a%2520human%2520pushing%2520a%250Asensor-equipped%2520cart%2520while%2520following%2520a%2520target%2520person%252C%2520capturing%2520human-like%250Afollowing%2520behavior%2520and%2520emphasizing%2520long-term%2520tracking%2520challenges%252C%2520including%250Afrequent%2520occlusions%2520and%2520the%2520need%2520for%2520re-identification%2520from%2520numerous%250Apedestrians.%2520It%2520includes%2520multi-modal%2520data%2520streams%252C%2520including%2520odometry%252C%25203D%250ALiDAR%252C%2520IMU%252C%2520panoramic%2520images%252C%2520and%2520RGB-D%2520images%252C%2520along%2520with%2520exhaustively%250Aannotated%25202D%2520bounding%2520boxes%2520of%2520the%2520target%2520person%2520across%252048%2520sequences%252C%2520both%250Aindoors%2520and%2520outdoors.%2520Using%2520this%2520dataset%2520and%2520visual%2520annotations%252C%2520we%2520perform%250Aextensive%2520experiments%2520with%2520existing%2520SOTA%2520TPT%2520methods%252C%2520offering%2520a%2520thorough%250Aanalysis%2520of%2520their%2520limitations%2520and%2520suggesting%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07446v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TPT-Bench%3A%20A%20Large-Scale%2C%20Long-Term%20and%20Robot-Egocentric%20Dataset%20for%0A%20%20Benchmarking%20Target%20Person%20Tracking&entry.906535625=Hanjing%20Ye%20and%20Yu%20Zhan%20and%20Weixi%20Situ%20and%20Guangcheng%20Chen%20and%20Jingwen%20Yu%20and%20Ziqi%20Zhao%20and%20Kuanqi%20Cai%20and%20Arash%20Ajoudani%20and%20Hong%20Zhang&entry.1292438233=%20%20Tracking%20a%20target%20person%20from%20robot-egocentric%20views%20is%20crucial%20for%0Adeveloping%20autonomous%20robots%20that%20provide%20continuous%20personalized%20assistance%20or%0Acollaboration%20in%20Human-Robot%20Interaction%20%28HRI%29%20and%20Embodied%20AI.%20However%2C%20most%0Aexisting%20target%20person%20tracking%20%28TPT%29%20benchmarks%20are%20limited%20to%20controlled%0Alaboratory%20environments%20with%20few%20distractions%2C%20clean%20backgrounds%2C%20and%0Ashort-term%20occlusions.%20In%20this%20paper%2C%20we%20introduce%20a%20large-scale%20dataset%0Adesigned%20for%20TPT%20in%20crowded%20and%20unstructured%20environments%2C%20demonstrated%20through%0Aa%20robot-person%20following%20task.%20The%20dataset%20is%20collected%20by%20a%20human%20pushing%20a%0Asensor-equipped%20cart%20while%20following%20a%20target%20person%2C%20capturing%20human-like%0Afollowing%20behavior%20and%20emphasizing%20long-term%20tracking%20challenges%2C%20including%0Afrequent%20occlusions%20and%20the%20need%20for%20re-identification%20from%20numerous%0Apedestrians.%20It%20includes%20multi-modal%20data%20streams%2C%20including%20odometry%2C%203D%0ALiDAR%2C%20IMU%2C%20panoramic%20images%2C%20and%20RGB-D%20images%2C%20along%20with%20exhaustively%0Aannotated%202D%20bounding%20boxes%20of%20the%20target%20person%20across%2048%20sequences%2C%20both%0Aindoors%20and%20outdoors.%20Using%20this%20dataset%20and%20visual%20annotations%2C%20we%20perform%0Aextensive%20experiments%20with%20existing%20SOTA%20TPT%20methods%2C%20offering%20a%20thorough%0Aanalysis%20of%20their%20limitations%20and%20suggesting%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07446v2&entry.124074799=Read"},
{"title": "Pullback Flow Matching on Data Manifolds", "author": "Friso de Kruiff and Erik Bekkers and Ozan \u00d6ktem and Carola-Bibiane Sch\u00f6nlieb and Willem Diepeveen", "abstract": "  We propose Pullback Flow Matching (PFM), a novel framework for generative\nmodeling on data manifolds. Unlike existing methods that assume or learn\nrestrictive closed-form manifold mappings for training Riemannian Flow Matching\n(RFM) models, PFM leverages pullback geometry and isometric learning to\npreserve the underlying manifold's geometry while enabling efficient generation\nand precise interpolation in latent space. This approach not only facilitates\nclosed-form mappings on the data manifold but also allows for designable latent\nspaces, using assumed metrics on both data and latent manifolds. By enhancing\nisometric learning through Neural ODEs and proposing a scalable training\nobjective, we achieve a latent space more suitable for interpolation, leading\nto improved manifold learning and generative performance. We demonstrate PFM's\neffectiveness through applications in synthetic data, protein dynamics and\nprotein sequence data, generating novel proteins with specific properties. This\nmethod shows strong potential for drug discovery and materials science, where\ngenerating novel samples with specific properties is of great interest.\n", "link": "http://arxiv.org/abs/2410.04543v2", "date": "2025-07-09", "relevancy": 2.1023, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5625}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5271}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pullback%20Flow%20Matching%20on%20Data%20Manifolds&body=Title%3A%20Pullback%20Flow%20Matching%20on%20Data%20Manifolds%0AAuthor%3A%20Friso%20de%20Kruiff%20and%20Erik%20Bekkers%20and%20Ozan%20%C3%96ktem%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Willem%20Diepeveen%0AAbstract%3A%20%20%20We%20propose%20Pullback%20Flow%20Matching%20%28PFM%29%2C%20a%20novel%20framework%20for%20generative%0Amodeling%20on%20data%20manifolds.%20Unlike%20existing%20methods%20that%20assume%20or%20learn%0Arestrictive%20closed-form%20manifold%20mappings%20for%20training%20Riemannian%20Flow%20Matching%0A%28RFM%29%20models%2C%20PFM%20leverages%20pullback%20geometry%20and%20isometric%20learning%20to%0Apreserve%20the%20underlying%20manifold%27s%20geometry%20while%20enabling%20efficient%20generation%0Aand%20precise%20interpolation%20in%20latent%20space.%20This%20approach%20not%20only%20facilitates%0Aclosed-form%20mappings%20on%20the%20data%20manifold%20but%20also%20allows%20for%20designable%20latent%0Aspaces%2C%20using%20assumed%20metrics%20on%20both%20data%20and%20latent%20manifolds.%20By%20enhancing%0Aisometric%20learning%20through%20Neural%20ODEs%20and%20proposing%20a%20scalable%20training%0Aobjective%2C%20we%20achieve%20a%20latent%20space%20more%20suitable%20for%20interpolation%2C%20leading%0Ato%20improved%20manifold%20learning%20and%20generative%20performance.%20We%20demonstrate%20PFM%27s%0Aeffectiveness%20through%20applications%20in%20synthetic%20data%2C%20protein%20dynamics%20and%0Aprotein%20sequence%20data%2C%20generating%20novel%20proteins%20with%20specific%20properties.%20This%0Amethod%20shows%20strong%20potential%20for%20drug%20discovery%20and%20materials%20science%2C%20where%0Agenerating%20novel%20samples%20with%20specific%20properties%20is%20of%20great%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPullback%2520Flow%2520Matching%2520on%2520Data%2520Manifolds%26entry.906535625%3DFriso%2520de%2520Kruiff%2520and%2520Erik%2520Bekkers%2520and%2520Ozan%2520%25C3%2596ktem%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Willem%2520Diepeveen%26entry.1292438233%3D%2520%2520We%2520propose%2520Pullback%2520Flow%2520Matching%2520%2528PFM%2529%252C%2520a%2520novel%2520framework%2520for%2520generative%250Amodeling%2520on%2520data%2520manifolds.%2520Unlike%2520existing%2520methods%2520that%2520assume%2520or%2520learn%250Arestrictive%2520closed-form%2520manifold%2520mappings%2520for%2520training%2520Riemannian%2520Flow%2520Matching%250A%2528RFM%2529%2520models%252C%2520PFM%2520leverages%2520pullback%2520geometry%2520and%2520isometric%2520learning%2520to%250Apreserve%2520the%2520underlying%2520manifold%2527s%2520geometry%2520while%2520enabling%2520efficient%2520generation%250Aand%2520precise%2520interpolation%2520in%2520latent%2520space.%2520This%2520approach%2520not%2520only%2520facilitates%250Aclosed-form%2520mappings%2520on%2520the%2520data%2520manifold%2520but%2520also%2520allows%2520for%2520designable%2520latent%250Aspaces%252C%2520using%2520assumed%2520metrics%2520on%2520both%2520data%2520and%2520latent%2520manifolds.%2520By%2520enhancing%250Aisometric%2520learning%2520through%2520Neural%2520ODEs%2520and%2520proposing%2520a%2520scalable%2520training%250Aobjective%252C%2520we%2520achieve%2520a%2520latent%2520space%2520more%2520suitable%2520for%2520interpolation%252C%2520leading%250Ato%2520improved%2520manifold%2520learning%2520and%2520generative%2520performance.%2520We%2520demonstrate%2520PFM%2527s%250Aeffectiveness%2520through%2520applications%2520in%2520synthetic%2520data%252C%2520protein%2520dynamics%2520and%250Aprotein%2520sequence%2520data%252C%2520generating%2520novel%2520proteins%2520with%2520specific%2520properties.%2520This%250Amethod%2520shows%2520strong%2520potential%2520for%2520drug%2520discovery%2520and%2520materials%2520science%252C%2520where%250Agenerating%2520novel%2520samples%2520with%2520specific%2520properties%2520is%2520of%2520great%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pullback%20Flow%20Matching%20on%20Data%20Manifolds&entry.906535625=Friso%20de%20Kruiff%20and%20Erik%20Bekkers%20and%20Ozan%20%C3%96ktem%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Willem%20Diepeveen&entry.1292438233=%20%20We%20propose%20Pullback%20Flow%20Matching%20%28PFM%29%2C%20a%20novel%20framework%20for%20generative%0Amodeling%20on%20data%20manifolds.%20Unlike%20existing%20methods%20that%20assume%20or%20learn%0Arestrictive%20closed-form%20manifold%20mappings%20for%20training%20Riemannian%20Flow%20Matching%0A%28RFM%29%20models%2C%20PFM%20leverages%20pullback%20geometry%20and%20isometric%20learning%20to%0Apreserve%20the%20underlying%20manifold%27s%20geometry%20while%20enabling%20efficient%20generation%0Aand%20precise%20interpolation%20in%20latent%20space.%20This%20approach%20not%20only%20facilitates%0Aclosed-form%20mappings%20on%20the%20data%20manifold%20but%20also%20allows%20for%20designable%20latent%0Aspaces%2C%20using%20assumed%20metrics%20on%20both%20data%20and%20latent%20manifolds.%20By%20enhancing%0Aisometric%20learning%20through%20Neural%20ODEs%20and%20proposing%20a%20scalable%20training%0Aobjective%2C%20we%20achieve%20a%20latent%20space%20more%20suitable%20for%20interpolation%2C%20leading%0Ato%20improved%20manifold%20learning%20and%20generative%20performance.%20We%20demonstrate%20PFM%27s%0Aeffectiveness%20through%20applications%20in%20synthetic%20data%2C%20protein%20dynamics%20and%0Aprotein%20sequence%20data%2C%20generating%20novel%20proteins%20with%20specific%20properties.%20This%0Amethod%20shows%20strong%20potential%20for%20drug%20discovery%20and%20materials%20science%2C%20where%0Agenerating%20novel%20samples%20with%20specific%20properties%20is%20of%20great%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04543v2&entry.124074799=Read"},
{"title": "Horizontal and Vertical Federated Causal Structure Learning via\n  Higher-order Cumulants", "author": "Wei Chen and Wanyang Gu and Linjun Peng and Ruichu Cai and Zhifeng Hao and Kun Zhang", "abstract": "  Federated causal discovery aims to uncover the causal relationships between\nentities while protecting data privacy, which has significant importance and\nnumerous applications in real-world scenarios. Existing federated causal\nstructure learning methods primarily focus on horizontal federated settings.\nHowever, in practical situations, different clients may not necessarily contain\ndata on the same variables. In a single client, the incomplete set of variables\ncan easily lead to spurious causal relationships, thereby affecting the\ninformation transmitted to other clients. To address this issue, we\ncomprehensively consider causal structure learning methods under both\nhorizontal and vertical federated settings. We provide the identification\ntheories and methods for learning causal structure in the horizontal and\nvertical federal setting via higher-order cumulants. Specifically, we first\naggregate higher-order cumulant information from all participating clients to\nconstruct global cumulant estimates. These global estimates are then used for\nrecursive source identification, ultimately yielding a global causal strength\nmatrix. Our approach not only enables the reconstruction of causal graphs but\nalso facilitates the estimation of causal strength coefficients. Our algorithm\ndemonstrates superior performance in experiments conducted on both synthetic\ndata and real-world data.\n", "link": "http://arxiv.org/abs/2507.06888v1", "date": "2025-07-09", "relevancy": 2.0965, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4352}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4156}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Horizontal%20and%20Vertical%20Federated%20Causal%20Structure%20Learning%20via%0A%20%20Higher-order%20Cumulants&body=Title%3A%20Horizontal%20and%20Vertical%20Federated%20Causal%20Structure%20Learning%20via%0A%20%20Higher-order%20Cumulants%0AAuthor%3A%20Wei%20Chen%20and%20Wanyang%20Gu%20and%20Linjun%20Peng%20and%20Ruichu%20Cai%20and%20Zhifeng%20Hao%20and%20Kun%20Zhang%0AAbstract%3A%20%20%20Federated%20causal%20discovery%20aims%20to%20uncover%20the%20causal%20relationships%20between%0Aentities%20while%20protecting%20data%20privacy%2C%20which%20has%20significant%20importance%20and%0Anumerous%20applications%20in%20real-world%20scenarios.%20Existing%20federated%20causal%0Astructure%20learning%20methods%20primarily%20focus%20on%20horizontal%20federated%20settings.%0AHowever%2C%20in%20practical%20situations%2C%20different%20clients%20may%20not%20necessarily%20contain%0Adata%20on%20the%20same%20variables.%20In%20a%20single%20client%2C%20the%20incomplete%20set%20of%20variables%0Acan%20easily%20lead%20to%20spurious%20causal%20relationships%2C%20thereby%20affecting%20the%0Ainformation%20transmitted%20to%20other%20clients.%20To%20address%20this%20issue%2C%20we%0Acomprehensively%20consider%20causal%20structure%20learning%20methods%20under%20both%0Ahorizontal%20and%20vertical%20federated%20settings.%20We%20provide%20the%20identification%0Atheories%20and%20methods%20for%20learning%20causal%20structure%20in%20the%20horizontal%20and%0Avertical%20federal%20setting%20via%20higher-order%20cumulants.%20Specifically%2C%20we%20first%0Aaggregate%20higher-order%20cumulant%20information%20from%20all%20participating%20clients%20to%0Aconstruct%20global%20cumulant%20estimates.%20These%20global%20estimates%20are%20then%20used%20for%0Arecursive%20source%20identification%2C%20ultimately%20yielding%20a%20global%20causal%20strength%0Amatrix.%20Our%20approach%20not%20only%20enables%20the%20reconstruction%20of%20causal%20graphs%20but%0Aalso%20facilitates%20the%20estimation%20of%20causal%20strength%20coefficients.%20Our%20algorithm%0Ademonstrates%20superior%20performance%20in%20experiments%20conducted%20on%20both%20synthetic%0Adata%20and%20real-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHorizontal%2520and%2520Vertical%2520Federated%2520Causal%2520Structure%2520Learning%2520via%250A%2520%2520Higher-order%2520Cumulants%26entry.906535625%3DWei%2520Chen%2520and%2520Wanyang%2520Gu%2520and%2520Linjun%2520Peng%2520and%2520Ruichu%2520Cai%2520and%2520Zhifeng%2520Hao%2520and%2520Kun%2520Zhang%26entry.1292438233%3D%2520%2520Federated%2520causal%2520discovery%2520aims%2520to%2520uncover%2520the%2520causal%2520relationships%2520between%250Aentities%2520while%2520protecting%2520data%2520privacy%252C%2520which%2520has%2520significant%2520importance%2520and%250Anumerous%2520applications%2520in%2520real-world%2520scenarios.%2520Existing%2520federated%2520causal%250Astructure%2520learning%2520methods%2520primarily%2520focus%2520on%2520horizontal%2520federated%2520settings.%250AHowever%252C%2520in%2520practical%2520situations%252C%2520different%2520clients%2520may%2520not%2520necessarily%2520contain%250Adata%2520on%2520the%2520same%2520variables.%2520In%2520a%2520single%2520client%252C%2520the%2520incomplete%2520set%2520of%2520variables%250Acan%2520easily%2520lead%2520to%2520spurious%2520causal%2520relationships%252C%2520thereby%2520affecting%2520the%250Ainformation%2520transmitted%2520to%2520other%2520clients.%2520To%2520address%2520this%2520issue%252C%2520we%250Acomprehensively%2520consider%2520causal%2520structure%2520learning%2520methods%2520under%2520both%250Ahorizontal%2520and%2520vertical%2520federated%2520settings.%2520We%2520provide%2520the%2520identification%250Atheories%2520and%2520methods%2520for%2520learning%2520causal%2520structure%2520in%2520the%2520horizontal%2520and%250Avertical%2520federal%2520setting%2520via%2520higher-order%2520cumulants.%2520Specifically%252C%2520we%2520first%250Aaggregate%2520higher-order%2520cumulant%2520information%2520from%2520all%2520participating%2520clients%2520to%250Aconstruct%2520global%2520cumulant%2520estimates.%2520These%2520global%2520estimates%2520are%2520then%2520used%2520for%250Arecursive%2520source%2520identification%252C%2520ultimately%2520yielding%2520a%2520global%2520causal%2520strength%250Amatrix.%2520Our%2520approach%2520not%2520only%2520enables%2520the%2520reconstruction%2520of%2520causal%2520graphs%2520but%250Aalso%2520facilitates%2520the%2520estimation%2520of%2520causal%2520strength%2520coefficients.%2520Our%2520algorithm%250Ademonstrates%2520superior%2520performance%2520in%2520experiments%2520conducted%2520on%2520both%2520synthetic%250Adata%2520and%2520real-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Horizontal%20and%20Vertical%20Federated%20Causal%20Structure%20Learning%20via%0A%20%20Higher-order%20Cumulants&entry.906535625=Wei%20Chen%20and%20Wanyang%20Gu%20and%20Linjun%20Peng%20and%20Ruichu%20Cai%20and%20Zhifeng%20Hao%20and%20Kun%20Zhang&entry.1292438233=%20%20Federated%20causal%20discovery%20aims%20to%20uncover%20the%20causal%20relationships%20between%0Aentities%20while%20protecting%20data%20privacy%2C%20which%20has%20significant%20importance%20and%0Anumerous%20applications%20in%20real-world%20scenarios.%20Existing%20federated%20causal%0Astructure%20learning%20methods%20primarily%20focus%20on%20horizontal%20federated%20settings.%0AHowever%2C%20in%20practical%20situations%2C%20different%20clients%20may%20not%20necessarily%20contain%0Adata%20on%20the%20same%20variables.%20In%20a%20single%20client%2C%20the%20incomplete%20set%20of%20variables%0Acan%20easily%20lead%20to%20spurious%20causal%20relationships%2C%20thereby%20affecting%20the%0Ainformation%20transmitted%20to%20other%20clients.%20To%20address%20this%20issue%2C%20we%0Acomprehensively%20consider%20causal%20structure%20learning%20methods%20under%20both%0Ahorizontal%20and%20vertical%20federated%20settings.%20We%20provide%20the%20identification%0Atheories%20and%20methods%20for%20learning%20causal%20structure%20in%20the%20horizontal%20and%0Avertical%20federal%20setting%20via%20higher-order%20cumulants.%20Specifically%2C%20we%20first%0Aaggregate%20higher-order%20cumulant%20information%20from%20all%20participating%20clients%20to%0Aconstruct%20global%20cumulant%20estimates.%20These%20global%20estimates%20are%20then%20used%20for%0Arecursive%20source%20identification%2C%20ultimately%20yielding%20a%20global%20causal%20strength%0Amatrix.%20Our%20approach%20not%20only%20enables%20the%20reconstruction%20of%20causal%20graphs%20but%0Aalso%20facilitates%20the%20estimation%20of%20causal%20strength%20coefficients.%20Our%20algorithm%0Ademonstrates%20superior%20performance%20in%20experiments%20conducted%20on%20both%20synthetic%0Adata%20and%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06888v1&entry.124074799=Read"},
{"title": "Comprehensive Evaluation of Prototype Neural Networks", "author": "Philipp Schlinge and Steffen Meinert and Martin Atzmueller", "abstract": "  Prototype models are an important method for explainable artificial\nintelligence (XAI) and interpretable machine learning. In this paper, we\nperform an in-depth analysis of a set of prominent prototype models including\nProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive\nset of metrics. In addition to applying standard metrics from literature, we\npropose several new metrics to further complement the analysis of model\ninterpretability. In our experimentation, we apply the set of prototype models\non a diverse set of datasets including fine-grained classification, Non-IID\nsettings and multi-label classification to further contrast the performance.\nFurthermore, we also provide our code as an open-source library, which\nfacilitates simple application of the metrics itself, as well as extensibility\n- providing the option for easily adding new metrics and models.\nhttps://github.com/uos-sis/quanproto\n", "link": "http://arxiv.org/abs/2507.06819v1", "date": "2025-07-09", "relevancy": 2.0946, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Evaluation%20of%20Prototype%20Neural%20Networks&body=Title%3A%20Comprehensive%20Evaluation%20of%20Prototype%20Neural%20Networks%0AAuthor%3A%20Philipp%20Schlinge%20and%20Steffen%20Meinert%20and%20Martin%20Atzmueller%0AAbstract%3A%20%20%20Prototype%20models%20are%20an%20important%20method%20for%20explainable%20artificial%0Aintelligence%20%28XAI%29%20and%20interpretable%20machine%20learning.%20In%20this%20paper%2C%20we%0Aperform%20an%20in-depth%20analysis%20of%20a%20set%20of%20prominent%20prototype%20models%20including%0AProtoPNet%2C%20ProtoPool%20and%20PIPNet.%20For%20their%20assessment%2C%20we%20apply%20a%20comprehensive%0Aset%20of%20metrics.%20In%20addition%20to%20applying%20standard%20metrics%20from%20literature%2C%20we%0Apropose%20several%20new%20metrics%20to%20further%20complement%20the%20analysis%20of%20model%0Ainterpretability.%20In%20our%20experimentation%2C%20we%20apply%20the%20set%20of%20prototype%20models%0Aon%20a%20diverse%20set%20of%20datasets%20including%20fine-grained%20classification%2C%20Non-IID%0Asettings%20and%20multi-label%20classification%20to%20further%20contrast%20the%20performance.%0AFurthermore%2C%20we%20also%20provide%20our%20code%20as%20an%20open-source%20library%2C%20which%0Afacilitates%20simple%20application%20of%20the%20metrics%20itself%2C%20as%20well%20as%20extensibility%0A-%20providing%20the%20option%20for%20easily%20adding%20new%20metrics%20and%20models.%0Ahttps%3A//github.com/uos-sis/quanproto%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Evaluation%2520of%2520Prototype%2520Neural%2520Networks%26entry.906535625%3DPhilipp%2520Schlinge%2520and%2520Steffen%2520Meinert%2520and%2520Martin%2520Atzmueller%26entry.1292438233%3D%2520%2520Prototype%2520models%2520are%2520an%2520important%2520method%2520for%2520explainable%2520artificial%250Aintelligence%2520%2528XAI%2529%2520and%2520interpretable%2520machine%2520learning.%2520In%2520this%2520paper%252C%2520we%250Aperform%2520an%2520in-depth%2520analysis%2520of%2520a%2520set%2520of%2520prominent%2520prototype%2520models%2520including%250AProtoPNet%252C%2520ProtoPool%2520and%2520PIPNet.%2520For%2520their%2520assessment%252C%2520we%2520apply%2520a%2520comprehensive%250Aset%2520of%2520metrics.%2520In%2520addition%2520to%2520applying%2520standard%2520metrics%2520from%2520literature%252C%2520we%250Apropose%2520several%2520new%2520metrics%2520to%2520further%2520complement%2520the%2520analysis%2520of%2520model%250Ainterpretability.%2520In%2520our%2520experimentation%252C%2520we%2520apply%2520the%2520set%2520of%2520prototype%2520models%250Aon%2520a%2520diverse%2520set%2520of%2520datasets%2520including%2520fine-grained%2520classification%252C%2520Non-IID%250Asettings%2520and%2520multi-label%2520classification%2520to%2520further%2520contrast%2520the%2520performance.%250AFurthermore%252C%2520we%2520also%2520provide%2520our%2520code%2520as%2520an%2520open-source%2520library%252C%2520which%250Afacilitates%2520simple%2520application%2520of%2520the%2520metrics%2520itself%252C%2520as%2520well%2520as%2520extensibility%250A-%2520providing%2520the%2520option%2520for%2520easily%2520adding%2520new%2520metrics%2520and%2520models.%250Ahttps%253A//github.com/uos-sis/quanproto%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Evaluation%20of%20Prototype%20Neural%20Networks&entry.906535625=Philipp%20Schlinge%20and%20Steffen%20Meinert%20and%20Martin%20Atzmueller&entry.1292438233=%20%20Prototype%20models%20are%20an%20important%20method%20for%20explainable%20artificial%0Aintelligence%20%28XAI%29%20and%20interpretable%20machine%20learning.%20In%20this%20paper%2C%20we%0Aperform%20an%20in-depth%20analysis%20of%20a%20set%20of%20prominent%20prototype%20models%20including%0AProtoPNet%2C%20ProtoPool%20and%20PIPNet.%20For%20their%20assessment%2C%20we%20apply%20a%20comprehensive%0Aset%20of%20metrics.%20In%20addition%20to%20applying%20standard%20metrics%20from%20literature%2C%20we%0Apropose%20several%20new%20metrics%20to%20further%20complement%20the%20analysis%20of%20model%0Ainterpretability.%20In%20our%20experimentation%2C%20we%20apply%20the%20set%20of%20prototype%20models%0Aon%20a%20diverse%20set%20of%20datasets%20including%20fine-grained%20classification%2C%20Non-IID%0Asettings%20and%20multi-label%20classification%20to%20further%20contrast%20the%20performance.%0AFurthermore%2C%20we%20also%20provide%20our%20code%20as%20an%20open-source%20library%2C%20which%0Afacilitates%20simple%20application%20of%20the%20metrics%20itself%2C%20as%20well%20as%20extensibility%0A-%20providing%20the%20option%20for%20easily%20adding%20new%20metrics%20and%20models.%0Ahttps%3A//github.com/uos-sis/quanproto%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06819v1&entry.124074799=Read"},
{"title": "Deep Brain Net: An Optimized Deep Learning Model for Brain tumor\n  Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer\n  Learning", "author": "Daniel Onah and Ravish Desai", "abstract": "  In recent years, deep learning has shown great promise in the automated\ndetection and classification of brain tumors from MRI images. However,\nachieving high accuracy and computational efficiency remains a challenge. In\nthis research, we propose Deep Brain Net, a novel deep learning system designed\nto optimize performance in the detection of brain tumors. The model integrates\nthe strengths of two advanced neural network architectures which are\nEfficientNetB0 and ResNet50, combined with transfer learning to improve\ngeneralization and reduce training time. The EfficientNetB0 architecture\nenhances model efficiency by utilizing mobile inverted bottleneck blocks, which\nincorporate depth wise separable convolutions. This design significantly\nreduces the number of parameters and computational cost while preserving the\nability of models to learn complex feature representations. The ResNet50\narchitecture, pre trained on large scale datasets like ImageNet, is fine tuned\nfor brain tumor classification. Its use of residual connections allows for\ntraining deeper networks by mitigating the vanishing gradient problem and\navoiding performance degradation. The integration of these components ensures\nthat the proposed system is both computationally efficient and highly accurate.\nExtensive experiments performed on publicly available MRI datasets demonstrate\nthat Deep Brain Net consistently outperforms existing state of the art methods\nin terms of classification accuracy, precision, recall, and computational\nefficiency. The result is an accuracy of 88 percent, a weighted F1 score of\n88.75 percent, and a macro AUC ROC score of 98.17 percent which demonstrates\nthe robustness and clinical potential of Deep Brain Net in assisting\nradiologists with brain tumor diagnosis.\n", "link": "http://arxiv.org/abs/2507.07011v1", "date": "2025-07-09", "relevancy": 2.0883, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5757}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4926}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Brain%20Net%3A%20An%20Optimized%20Deep%20Learning%20Model%20for%20Brain%20tumor%0A%20%20Detection%20in%20MRI%20Images%20Using%20EfficientNetB0%20and%20ResNet50%20with%20Transfer%0A%20%20Learning&body=Title%3A%20Deep%20Brain%20Net%3A%20An%20Optimized%20Deep%20Learning%20Model%20for%20Brain%20tumor%0A%20%20Detection%20in%20MRI%20Images%20Using%20EfficientNetB0%20and%20ResNet50%20with%20Transfer%0A%20%20Learning%0AAuthor%3A%20Daniel%20Onah%20and%20Ravish%20Desai%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20learning%20has%20shown%20great%20promise%20in%20the%20automated%0Adetection%20and%20classification%20of%20brain%20tumors%20from%20MRI%20images.%20However%2C%0Aachieving%20high%20accuracy%20and%20computational%20efficiency%20remains%20a%20challenge.%20In%0Athis%20research%2C%20we%20propose%20Deep%20Brain%20Net%2C%20a%20novel%20deep%20learning%20system%20designed%0Ato%20optimize%20performance%20in%20the%20detection%20of%20brain%20tumors.%20The%20model%20integrates%0Athe%20strengths%20of%20two%20advanced%20neural%20network%20architectures%20which%20are%0AEfficientNetB0%20and%20ResNet50%2C%20combined%20with%20transfer%20learning%20to%20improve%0Ageneralization%20and%20reduce%20training%20time.%20The%20EfficientNetB0%20architecture%0Aenhances%20model%20efficiency%20by%20utilizing%20mobile%20inverted%20bottleneck%20blocks%2C%20which%0Aincorporate%20depth%20wise%20separable%20convolutions.%20This%20design%20significantly%0Areduces%20the%20number%20of%20parameters%20and%20computational%20cost%20while%20preserving%20the%0Aability%20of%20models%20to%20learn%20complex%20feature%20representations.%20The%20ResNet50%0Aarchitecture%2C%20pre%20trained%20on%20large%20scale%20datasets%20like%20ImageNet%2C%20is%20fine%20tuned%0Afor%20brain%20tumor%20classification.%20Its%20use%20of%20residual%20connections%20allows%20for%0Atraining%20deeper%20networks%20by%20mitigating%20the%20vanishing%20gradient%20problem%20and%0Aavoiding%20performance%20degradation.%20The%20integration%20of%20these%20components%20ensures%0Athat%20the%20proposed%20system%20is%20both%20computationally%20efficient%20and%20highly%20accurate.%0AExtensive%20experiments%20performed%20on%20publicly%20available%20MRI%20datasets%20demonstrate%0Athat%20Deep%20Brain%20Net%20consistently%20outperforms%20existing%20state%20of%20the%20art%20methods%0Ain%20terms%20of%20classification%20accuracy%2C%20precision%2C%20recall%2C%20and%20computational%0Aefficiency.%20The%20result%20is%20an%20accuracy%20of%2088%20percent%2C%20a%20weighted%20F1%20score%20of%0A88.75%20percent%2C%20and%20a%20macro%20AUC%20ROC%20score%20of%2098.17%20percent%20which%20demonstrates%0Athe%20robustness%20and%20clinical%20potential%20of%20Deep%20Brain%20Net%20in%20assisting%0Aradiologists%20with%20brain%20tumor%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Brain%2520Net%253A%2520An%2520Optimized%2520Deep%2520Learning%2520Model%2520for%2520Brain%2520tumor%250A%2520%2520Detection%2520in%2520MRI%2520Images%2520Using%2520EfficientNetB0%2520and%2520ResNet50%2520with%2520Transfer%250A%2520%2520Learning%26entry.906535625%3DDaniel%2520Onah%2520and%2520Ravish%2520Desai%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deep%2520learning%2520has%2520shown%2520great%2520promise%2520in%2520the%2520automated%250Adetection%2520and%2520classification%2520of%2520brain%2520tumors%2520from%2520MRI%2520images.%2520However%252C%250Aachieving%2520high%2520accuracy%2520and%2520computational%2520efficiency%2520remains%2520a%2520challenge.%2520In%250Athis%2520research%252C%2520we%2520propose%2520Deep%2520Brain%2520Net%252C%2520a%2520novel%2520deep%2520learning%2520system%2520designed%250Ato%2520optimize%2520performance%2520in%2520the%2520detection%2520of%2520brain%2520tumors.%2520The%2520model%2520integrates%250Athe%2520strengths%2520of%2520two%2520advanced%2520neural%2520network%2520architectures%2520which%2520are%250AEfficientNetB0%2520and%2520ResNet50%252C%2520combined%2520with%2520transfer%2520learning%2520to%2520improve%250Ageneralization%2520and%2520reduce%2520training%2520time.%2520The%2520EfficientNetB0%2520architecture%250Aenhances%2520model%2520efficiency%2520by%2520utilizing%2520mobile%2520inverted%2520bottleneck%2520blocks%252C%2520which%250Aincorporate%2520depth%2520wise%2520separable%2520convolutions.%2520This%2520design%2520significantly%250Areduces%2520the%2520number%2520of%2520parameters%2520and%2520computational%2520cost%2520while%2520preserving%2520the%250Aability%2520of%2520models%2520to%2520learn%2520complex%2520feature%2520representations.%2520The%2520ResNet50%250Aarchitecture%252C%2520pre%2520trained%2520on%2520large%2520scale%2520datasets%2520like%2520ImageNet%252C%2520is%2520fine%2520tuned%250Afor%2520brain%2520tumor%2520classification.%2520Its%2520use%2520of%2520residual%2520connections%2520allows%2520for%250Atraining%2520deeper%2520networks%2520by%2520mitigating%2520the%2520vanishing%2520gradient%2520problem%2520and%250Aavoiding%2520performance%2520degradation.%2520The%2520integration%2520of%2520these%2520components%2520ensures%250Athat%2520the%2520proposed%2520system%2520is%2520both%2520computationally%2520efficient%2520and%2520highly%2520accurate.%250AExtensive%2520experiments%2520performed%2520on%2520publicly%2520available%2520MRI%2520datasets%2520demonstrate%250Athat%2520Deep%2520Brain%2520Net%2520consistently%2520outperforms%2520existing%2520state%2520of%2520the%2520art%2520methods%250Ain%2520terms%2520of%2520classification%2520accuracy%252C%2520precision%252C%2520recall%252C%2520and%2520computational%250Aefficiency.%2520The%2520result%2520is%2520an%2520accuracy%2520of%252088%2520percent%252C%2520a%2520weighted%2520F1%2520score%2520of%250A88.75%2520percent%252C%2520and%2520a%2520macro%2520AUC%2520ROC%2520score%2520of%252098.17%2520percent%2520which%2520demonstrates%250Athe%2520robustness%2520and%2520clinical%2520potential%2520of%2520Deep%2520Brain%2520Net%2520in%2520assisting%250Aradiologists%2520with%2520brain%2520tumor%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Brain%20Net%3A%20An%20Optimized%20Deep%20Learning%20Model%20for%20Brain%20tumor%0A%20%20Detection%20in%20MRI%20Images%20Using%20EfficientNetB0%20and%20ResNet50%20with%20Transfer%0A%20%20Learning&entry.906535625=Daniel%20Onah%20and%20Ravish%20Desai&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20learning%20has%20shown%20great%20promise%20in%20the%20automated%0Adetection%20and%20classification%20of%20brain%20tumors%20from%20MRI%20images.%20However%2C%0Aachieving%20high%20accuracy%20and%20computational%20efficiency%20remains%20a%20challenge.%20In%0Athis%20research%2C%20we%20propose%20Deep%20Brain%20Net%2C%20a%20novel%20deep%20learning%20system%20designed%0Ato%20optimize%20performance%20in%20the%20detection%20of%20brain%20tumors.%20The%20model%20integrates%0Athe%20strengths%20of%20two%20advanced%20neural%20network%20architectures%20which%20are%0AEfficientNetB0%20and%20ResNet50%2C%20combined%20with%20transfer%20learning%20to%20improve%0Ageneralization%20and%20reduce%20training%20time.%20The%20EfficientNetB0%20architecture%0Aenhances%20model%20efficiency%20by%20utilizing%20mobile%20inverted%20bottleneck%20blocks%2C%20which%0Aincorporate%20depth%20wise%20separable%20convolutions.%20This%20design%20significantly%0Areduces%20the%20number%20of%20parameters%20and%20computational%20cost%20while%20preserving%20the%0Aability%20of%20models%20to%20learn%20complex%20feature%20representations.%20The%20ResNet50%0Aarchitecture%2C%20pre%20trained%20on%20large%20scale%20datasets%20like%20ImageNet%2C%20is%20fine%20tuned%0Afor%20brain%20tumor%20classification.%20Its%20use%20of%20residual%20connections%20allows%20for%0Atraining%20deeper%20networks%20by%20mitigating%20the%20vanishing%20gradient%20problem%20and%0Aavoiding%20performance%20degradation.%20The%20integration%20of%20these%20components%20ensures%0Athat%20the%20proposed%20system%20is%20both%20computationally%20efficient%20and%20highly%20accurate.%0AExtensive%20experiments%20performed%20on%20publicly%20available%20MRI%20datasets%20demonstrate%0Athat%20Deep%20Brain%20Net%20consistently%20outperforms%20existing%20state%20of%20the%20art%20methods%0Ain%20terms%20of%20classification%20accuracy%2C%20precision%2C%20recall%2C%20and%20computational%0Aefficiency.%20The%20result%20is%20an%20accuracy%20of%2088%20percent%2C%20a%20weighted%20F1%20score%20of%0A88.75%20percent%2C%20and%20a%20macro%20AUC%20ROC%20score%20of%2098.17%20percent%20which%20demonstrates%0Athe%20robustness%20and%20clinical%20potential%20of%20Deep%20Brain%20Net%20in%20assisting%0Aradiologists%20with%20brain%20tumor%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07011v1&entry.124074799=Read"},
{"title": "Less can be more for predicting properties with large language models", "author": "Nawaf Alampara and Santiago Miret and Kevin Maik Jablonka", "abstract": "  Predicting properties from coordinate-category data -- sets of vectors paired\nwith categorical information -- is fundamental to computational science. In\nmaterials science, this challenge manifests as predicting properties like\nformation energies or elastic moduli from crystal structures comprising atomic\npositions (vectors) and element types (categorical information). While large\nlanguage models (LLMs) have increasingly been applied to such tasks, with\nresearchers encoding structural data as text, optimal strategies for achieving\nreliable predictions remain elusive. Here, we report fundamental limitations in\nLLM's ability to learn from coordinate information in coordinate-category data.\nThrough systematic experiments using synthetic datasets with tunable coordinate\nand category contributions, combined with a comprehensive benchmarking\nframework (MatText) spanning multiple representations and model scales, we find\nthat LLMs consistently fail to capture coordinate information while excelling\nat category patterns. This geometric blindness persists regardless of model\nsize (up to 70B parameters), dataset scale (up to 2M structures), or text\nrepresentation strategy. Our findings suggest immediate practical implications:\nfor materials property prediction tasks dominated by structural effects,\nspecialized geometric architectures consistently outperform LLMs by significant\nmargins, as evidenced by a clear \"GNN-LM wall\" in performance benchmarks. Based\non our analysis, we provide concrete guidelines for architecture selection in\nscientific machine learning, while highlighting the critical importance of\nunderstanding model inductive biases when tackling scientific prediction\nproblems.\n", "link": "http://arxiv.org/abs/2406.17295v3", "date": "2025-07-09", "relevancy": 2.0882, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20can%20be%20more%20for%20predicting%20properties%20with%20large%20language%20models&body=Title%3A%20Less%20can%20be%20more%20for%20predicting%20properties%20with%20large%20language%20models%0AAuthor%3A%20Nawaf%20Alampara%20and%20Santiago%20Miret%20and%20Kevin%20Maik%20Jablonka%0AAbstract%3A%20%20%20Predicting%20properties%20from%20coordinate-category%20data%20--%20sets%20of%20vectors%20paired%0Awith%20categorical%20information%20--%20is%20fundamental%20to%20computational%20science.%20In%0Amaterials%20science%2C%20this%20challenge%20manifests%20as%20predicting%20properties%20like%0Aformation%20energies%20or%20elastic%20moduli%20from%20crystal%20structures%20comprising%20atomic%0Apositions%20%28vectors%29%20and%20element%20types%20%28categorical%20information%29.%20While%20large%0Alanguage%20models%20%28LLMs%29%20have%20increasingly%20been%20applied%20to%20such%20tasks%2C%20with%0Aresearchers%20encoding%20structural%20data%20as%20text%2C%20optimal%20strategies%20for%20achieving%0Areliable%20predictions%20remain%20elusive.%20Here%2C%20we%20report%20fundamental%20limitations%20in%0ALLM%27s%20ability%20to%20learn%20from%20coordinate%20information%20in%20coordinate-category%20data.%0AThrough%20systematic%20experiments%20using%20synthetic%20datasets%20with%20tunable%20coordinate%0Aand%20category%20contributions%2C%20combined%20with%20a%20comprehensive%20benchmarking%0Aframework%20%28MatText%29%20spanning%20multiple%20representations%20and%20model%20scales%2C%20we%20find%0Athat%20LLMs%20consistently%20fail%20to%20capture%20coordinate%20information%20while%20excelling%0Aat%20category%20patterns.%20This%20geometric%20blindness%20persists%20regardless%20of%20model%0Asize%20%28up%20to%2070B%20parameters%29%2C%20dataset%20scale%20%28up%20to%202M%20structures%29%2C%20or%20text%0Arepresentation%20strategy.%20Our%20findings%20suggest%20immediate%20practical%20implications%3A%0Afor%20materials%20property%20prediction%20tasks%20dominated%20by%20structural%20effects%2C%0Aspecialized%20geometric%20architectures%20consistently%20outperform%20LLMs%20by%20significant%0Amargins%2C%20as%20evidenced%20by%20a%20clear%20%22GNN-LM%20wall%22%20in%20performance%20benchmarks.%20Based%0Aon%20our%20analysis%2C%20we%20provide%20concrete%20guidelines%20for%20architecture%20selection%20in%0Ascientific%20machine%20learning%2C%20while%20highlighting%20the%20critical%20importance%20of%0Aunderstanding%20model%20inductive%20biases%20when%20tackling%20scientific%20prediction%0Aproblems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17295v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520can%2520be%2520more%2520for%2520predicting%2520properties%2520with%2520large%2520language%2520models%26entry.906535625%3DNawaf%2520Alampara%2520and%2520Santiago%2520Miret%2520and%2520Kevin%2520Maik%2520Jablonka%26entry.1292438233%3D%2520%2520Predicting%2520properties%2520from%2520coordinate-category%2520data%2520--%2520sets%2520of%2520vectors%2520paired%250Awith%2520categorical%2520information%2520--%2520is%2520fundamental%2520to%2520computational%2520science.%2520In%250Amaterials%2520science%252C%2520this%2520challenge%2520manifests%2520as%2520predicting%2520properties%2520like%250Aformation%2520energies%2520or%2520elastic%2520moduli%2520from%2520crystal%2520structures%2520comprising%2520atomic%250Apositions%2520%2528vectors%2529%2520and%2520element%2520types%2520%2528categorical%2520information%2529.%2520While%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520increasingly%2520been%2520applied%2520to%2520such%2520tasks%252C%2520with%250Aresearchers%2520encoding%2520structural%2520data%2520as%2520text%252C%2520optimal%2520strategies%2520for%2520achieving%250Areliable%2520predictions%2520remain%2520elusive.%2520Here%252C%2520we%2520report%2520fundamental%2520limitations%2520in%250ALLM%2527s%2520ability%2520to%2520learn%2520from%2520coordinate%2520information%2520in%2520coordinate-category%2520data.%250AThrough%2520systematic%2520experiments%2520using%2520synthetic%2520datasets%2520with%2520tunable%2520coordinate%250Aand%2520category%2520contributions%252C%2520combined%2520with%2520a%2520comprehensive%2520benchmarking%250Aframework%2520%2528MatText%2529%2520spanning%2520multiple%2520representations%2520and%2520model%2520scales%252C%2520we%2520find%250Athat%2520LLMs%2520consistently%2520fail%2520to%2520capture%2520coordinate%2520information%2520while%2520excelling%250Aat%2520category%2520patterns.%2520This%2520geometric%2520blindness%2520persists%2520regardless%2520of%2520model%250Asize%2520%2528up%2520to%252070B%2520parameters%2529%252C%2520dataset%2520scale%2520%2528up%2520to%25202M%2520structures%2529%252C%2520or%2520text%250Arepresentation%2520strategy.%2520Our%2520findings%2520suggest%2520immediate%2520practical%2520implications%253A%250Afor%2520materials%2520property%2520prediction%2520tasks%2520dominated%2520by%2520structural%2520effects%252C%250Aspecialized%2520geometric%2520architectures%2520consistently%2520outperform%2520LLMs%2520by%2520significant%250Amargins%252C%2520as%2520evidenced%2520by%2520a%2520clear%2520%2522GNN-LM%2520wall%2522%2520in%2520performance%2520benchmarks.%2520Based%250Aon%2520our%2520analysis%252C%2520we%2520provide%2520concrete%2520guidelines%2520for%2520architecture%2520selection%2520in%250Ascientific%2520machine%2520learning%252C%2520while%2520highlighting%2520the%2520critical%2520importance%2520of%250Aunderstanding%2520model%2520inductive%2520biases%2520when%2520tackling%2520scientific%2520prediction%250Aproblems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17295v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20can%20be%20more%20for%20predicting%20properties%20with%20large%20language%20models&entry.906535625=Nawaf%20Alampara%20and%20Santiago%20Miret%20and%20Kevin%20Maik%20Jablonka&entry.1292438233=%20%20Predicting%20properties%20from%20coordinate-category%20data%20--%20sets%20of%20vectors%20paired%0Awith%20categorical%20information%20--%20is%20fundamental%20to%20computational%20science.%20In%0Amaterials%20science%2C%20this%20challenge%20manifests%20as%20predicting%20properties%20like%0Aformation%20energies%20or%20elastic%20moduli%20from%20crystal%20structures%20comprising%20atomic%0Apositions%20%28vectors%29%20and%20element%20types%20%28categorical%20information%29.%20While%20large%0Alanguage%20models%20%28LLMs%29%20have%20increasingly%20been%20applied%20to%20such%20tasks%2C%20with%0Aresearchers%20encoding%20structural%20data%20as%20text%2C%20optimal%20strategies%20for%20achieving%0Areliable%20predictions%20remain%20elusive.%20Here%2C%20we%20report%20fundamental%20limitations%20in%0ALLM%27s%20ability%20to%20learn%20from%20coordinate%20information%20in%20coordinate-category%20data.%0AThrough%20systematic%20experiments%20using%20synthetic%20datasets%20with%20tunable%20coordinate%0Aand%20category%20contributions%2C%20combined%20with%20a%20comprehensive%20benchmarking%0Aframework%20%28MatText%29%20spanning%20multiple%20representations%20and%20model%20scales%2C%20we%20find%0Athat%20LLMs%20consistently%20fail%20to%20capture%20coordinate%20information%20while%20excelling%0Aat%20category%20patterns.%20This%20geometric%20blindness%20persists%20regardless%20of%20model%0Asize%20%28up%20to%2070B%20parameters%29%2C%20dataset%20scale%20%28up%20to%202M%20structures%29%2C%20or%20text%0Arepresentation%20strategy.%20Our%20findings%20suggest%20immediate%20practical%20implications%3A%0Afor%20materials%20property%20prediction%20tasks%20dominated%20by%20structural%20effects%2C%0Aspecialized%20geometric%20architectures%20consistently%20outperform%20LLMs%20by%20significant%0Amargins%2C%20as%20evidenced%20by%20a%20clear%20%22GNN-LM%20wall%22%20in%20performance%20benchmarks.%20Based%0Aon%20our%20analysis%2C%20we%20provide%20concrete%20guidelines%20for%20architecture%20selection%20in%0Ascientific%20machine%20learning%2C%20while%20highlighting%20the%20critical%20importance%20of%0Aunderstanding%20model%20inductive%20biases%20when%20tackling%20scientific%20prediction%0Aproblems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17295v3&entry.124074799=Read"},
{"title": "CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual\n  Rationale", "author": "Xiao Liang and Jiawei Hu and Di Wang and Zhi Ma and Lin Zhao and Ronghan Li and Bo Wan and Quan Wang", "abstract": "  Vision-language models (VLMs) are prone to hallucinations that critically\ncompromise reliability in medical applications. While preference optimization\ncan mitigate these hallucinations through clinical feedback, its implementation\nfaces challenges such as clinically irrelevant training samples, imbalanced\ndata distributions, and prohibitive expert annotation costs. To address these\nchallenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy\nthat combines confidence-similarity joint mining with counterfactual rationale.\nOur approach begins by synthesizing a unified, fine-grained multi-task chest\nX-ray visual instruction dataset across different question types for supervised\nfine-tuning (SFT). We then identify hard examples through token-level\nconfidence analysis of SFT failures and use similarity-based retrieval to\nexpand hard examples for balancing preference sample distributions, while\nsynthetic counterfactual rationales provide fine-grained clinical preferences,\neliminating the need for additional expert input. Experiments show that CheXPO\nachieves 8.93% relative performance gain using only 5% of SFT samples, reaching\nstate-of-the-art performance across diverse clinical tasks and providing a\nscalable, interpretable solution for real-world radiology applications.\n", "link": "http://arxiv.org/abs/2507.06959v1", "date": "2025-07-09", "relevancy": 2.0849, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CheXPO%3A%20Preference%20Optimization%20for%20Chest%20X-ray%20VLMs%20with%20Counterfactual%0A%20%20Rationale&body=Title%3A%20CheXPO%3A%20Preference%20Optimization%20for%20Chest%20X-ray%20VLMs%20with%20Counterfactual%0A%20%20Rationale%0AAuthor%3A%20Xiao%20Liang%20and%20Jiawei%20Hu%20and%20Di%20Wang%20and%20Zhi%20Ma%20and%20Lin%20Zhao%20and%20Ronghan%20Li%20and%20Bo%20Wan%20and%20Quan%20Wang%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20are%20prone%20to%20hallucinations%20that%20critically%0Acompromise%20reliability%20in%20medical%20applications.%20While%20preference%20optimization%0Acan%20mitigate%20these%20hallucinations%20through%20clinical%20feedback%2C%20its%20implementation%0Afaces%20challenges%20such%20as%20clinically%20irrelevant%20training%20samples%2C%20imbalanced%0Adata%20distributions%2C%20and%20prohibitive%20expert%20annotation%20costs.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20CheXPO%2C%20a%20Chest%20X-ray%20Preference%20Optimization%20strategy%0Athat%20combines%20confidence-similarity%20joint%20mining%20with%20counterfactual%20rationale.%0AOur%20approach%20begins%20by%20synthesizing%20a%20unified%2C%20fine-grained%20multi-task%20chest%0AX-ray%20visual%20instruction%20dataset%20across%20different%20question%20types%20for%20supervised%0Afine-tuning%20%28SFT%29.%20We%20then%20identify%20hard%20examples%20through%20token-level%0Aconfidence%20analysis%20of%20SFT%20failures%20and%20use%20similarity-based%20retrieval%20to%0Aexpand%20hard%20examples%20for%20balancing%20preference%20sample%20distributions%2C%20while%0Asynthetic%20counterfactual%20rationales%20provide%20fine-grained%20clinical%20preferences%2C%0Aeliminating%20the%20need%20for%20additional%20expert%20input.%20Experiments%20show%20that%20CheXPO%0Aachieves%208.93%25%20relative%20performance%20gain%20using%20only%205%25%20of%20SFT%20samples%2C%20reaching%0Astate-of-the-art%20performance%20across%20diverse%20clinical%20tasks%20and%20providing%20a%0Ascalable%2C%20interpretable%20solution%20for%20real-world%20radiology%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCheXPO%253A%2520Preference%2520Optimization%2520for%2520Chest%2520X-ray%2520VLMs%2520with%2520Counterfactual%250A%2520%2520Rationale%26entry.906535625%3DXiao%2520Liang%2520and%2520Jiawei%2520Hu%2520and%2520Di%2520Wang%2520and%2520Zhi%2520Ma%2520and%2520Lin%2520Zhao%2520and%2520Ronghan%2520Li%2520and%2520Bo%2520Wan%2520and%2520Quan%2520Wang%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520are%2520prone%2520to%2520hallucinations%2520that%2520critically%250Acompromise%2520reliability%2520in%2520medical%2520applications.%2520While%2520preference%2520optimization%250Acan%2520mitigate%2520these%2520hallucinations%2520through%2520clinical%2520feedback%252C%2520its%2520implementation%250Afaces%2520challenges%2520such%2520as%2520clinically%2520irrelevant%2520training%2520samples%252C%2520imbalanced%250Adata%2520distributions%252C%2520and%2520prohibitive%2520expert%2520annotation%2520costs.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520CheXPO%252C%2520a%2520Chest%2520X-ray%2520Preference%2520Optimization%2520strategy%250Athat%2520combines%2520confidence-similarity%2520joint%2520mining%2520with%2520counterfactual%2520rationale.%250AOur%2520approach%2520begins%2520by%2520synthesizing%2520a%2520unified%252C%2520fine-grained%2520multi-task%2520chest%250AX-ray%2520visual%2520instruction%2520dataset%2520across%2520different%2520question%2520types%2520for%2520supervised%250Afine-tuning%2520%2528SFT%2529.%2520We%2520then%2520identify%2520hard%2520examples%2520through%2520token-level%250Aconfidence%2520analysis%2520of%2520SFT%2520failures%2520and%2520use%2520similarity-based%2520retrieval%2520to%250Aexpand%2520hard%2520examples%2520for%2520balancing%2520preference%2520sample%2520distributions%252C%2520while%250Asynthetic%2520counterfactual%2520rationales%2520provide%2520fine-grained%2520clinical%2520preferences%252C%250Aeliminating%2520the%2520need%2520for%2520additional%2520expert%2520input.%2520Experiments%2520show%2520that%2520CheXPO%250Aachieves%25208.93%2525%2520relative%2520performance%2520gain%2520using%2520only%25205%2525%2520of%2520SFT%2520samples%252C%2520reaching%250Astate-of-the-art%2520performance%2520across%2520diverse%2520clinical%2520tasks%2520and%2520providing%2520a%250Ascalable%252C%2520interpretable%2520solution%2520for%2520real-world%2520radiology%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CheXPO%3A%20Preference%20Optimization%20for%20Chest%20X-ray%20VLMs%20with%20Counterfactual%0A%20%20Rationale&entry.906535625=Xiao%20Liang%20and%20Jiawei%20Hu%20and%20Di%20Wang%20and%20Zhi%20Ma%20and%20Lin%20Zhao%20and%20Ronghan%20Li%20and%20Bo%20Wan%20and%20Quan%20Wang&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20are%20prone%20to%20hallucinations%20that%20critically%0Acompromise%20reliability%20in%20medical%20applications.%20While%20preference%20optimization%0Acan%20mitigate%20these%20hallucinations%20through%20clinical%20feedback%2C%20its%20implementation%0Afaces%20challenges%20such%20as%20clinically%20irrelevant%20training%20samples%2C%20imbalanced%0Adata%20distributions%2C%20and%20prohibitive%20expert%20annotation%20costs.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20CheXPO%2C%20a%20Chest%20X-ray%20Preference%20Optimization%20strategy%0Athat%20combines%20confidence-similarity%20joint%20mining%20with%20counterfactual%20rationale.%0AOur%20approach%20begins%20by%20synthesizing%20a%20unified%2C%20fine-grained%20multi-task%20chest%0AX-ray%20visual%20instruction%20dataset%20across%20different%20question%20types%20for%20supervised%0Afine-tuning%20%28SFT%29.%20We%20then%20identify%20hard%20examples%20through%20token-level%0Aconfidence%20analysis%20of%20SFT%20failures%20and%20use%20similarity-based%20retrieval%20to%0Aexpand%20hard%20examples%20for%20balancing%20preference%20sample%20distributions%2C%20while%0Asynthetic%20counterfactual%20rationales%20provide%20fine-grained%20clinical%20preferences%2C%0Aeliminating%20the%20need%20for%20additional%20expert%20input.%20Experiments%20show%20that%20CheXPO%0Aachieves%208.93%25%20relative%20performance%20gain%20using%20only%205%25%20of%20SFT%20samples%2C%20reaching%0Astate-of-the-art%20performance%20across%20diverse%20clinical%20tasks%20and%20providing%20a%0Ascalable%2C%20interpretable%20solution%20for%20real-world%20radiology%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06959v1&entry.124074799=Read"},
{"title": "Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean\n  Data", "author": "Xuesong Li and Nassir Navab and Zhongliang Jiang", "abstract": "  Image denoising is a fundamental task in computer vision, particularly in\nmedical ultrasound (US) imaging, where speckle noise significantly degrades\nimage quality. Although recent advancements in deep neural networks have led to\nsubstantial improvements in denoising for natural images, these methods cannot\nbe directly applied to US speckle noise, as it is not purely random. Instead,\nUS speckle arises from complex wave interference within the body\nmicrostructure, making it tissue-dependent. This dependency means that\nobtaining two independent noisy observations of the same scene, as required by\npioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also\ncannot handle US speckle noise due to its high spatial dependency. To address\nthis challenge, we introduce Speckle2Self, a novel self-supervised algorithm\nfor speckle reduction using only single noisy observations. The key insight is\nthat applying a multi-scale perturbation (MSP) operation introduces\ntissue-dependent variations in the speckle pattern across different scales,\nwhile preserving the shared anatomical structure. This enables effective\nspeckle suppression by modeling the clean image as a low-rank signal and\nisolating the sparse noise component. To demonstrate its effectiveness,\nSpeckle2Self is comprehensively compared with conventional filter-based\ndenoising algorithms and SOTA learning-based methods, using both realistic\nsimulated US images and human carotid US images. Additionally, data from\nmultiple US machines are employed to evaluate model generalization and\nadaptability to images from unseen domains. \\textit{Code and datasets will be\nreleased upon acceptance.\n", "link": "http://arxiv.org/abs/2507.06828v1", "date": "2025-07-09", "relevancy": 2.0746, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.531}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.51}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speckle2Self%3A%20Self-Supervised%20Ultrasound%20Speckle%20Reduction%20Without%20Clean%0A%20%20Data&body=Title%3A%20Speckle2Self%3A%20Self-Supervised%20Ultrasound%20Speckle%20Reduction%20Without%20Clean%0A%20%20Data%0AAuthor%3A%20Xuesong%20Li%20and%20Nassir%20Navab%20and%20Zhongliang%20Jiang%0AAbstract%3A%20%20%20Image%20denoising%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20particularly%20in%0Amedical%20ultrasound%20%28US%29%20imaging%2C%20where%20speckle%20noise%20significantly%20degrades%0Aimage%20quality.%20Although%20recent%20advancements%20in%20deep%20neural%20networks%20have%20led%20to%0Asubstantial%20improvements%20in%20denoising%20for%20natural%20images%2C%20these%20methods%20cannot%0Abe%20directly%20applied%20to%20US%20speckle%20noise%2C%20as%20it%20is%20not%20purely%20random.%20Instead%2C%0AUS%20speckle%20arises%20from%20complex%20wave%20interference%20within%20the%20body%0Amicrostructure%2C%20making%20it%20tissue-dependent.%20This%20dependency%20means%20that%0Aobtaining%20two%20independent%20noisy%20observations%20of%20the%20same%20scene%2C%20as%20required%20by%0Apioneering%20Noise2Noise%2C%20is%20not%20feasible.%20Additionally%2C%20blind-spot%20networks%20also%0Acannot%20handle%20US%20speckle%20noise%20due%20to%20its%20high%20spatial%20dependency.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20Speckle2Self%2C%20a%20novel%20self-supervised%20algorithm%0Afor%20speckle%20reduction%20using%20only%20single%20noisy%20observations.%20The%20key%20insight%20is%0Athat%20applying%20a%20multi-scale%20perturbation%20%28MSP%29%20operation%20introduces%0Atissue-dependent%20variations%20in%20the%20speckle%20pattern%20across%20different%20scales%2C%0Awhile%20preserving%20the%20shared%20anatomical%20structure.%20This%20enables%20effective%0Aspeckle%20suppression%20by%20modeling%20the%20clean%20image%20as%20a%20low-rank%20signal%20and%0Aisolating%20the%20sparse%20noise%20component.%20To%20demonstrate%20its%20effectiveness%2C%0ASpeckle2Self%20is%20comprehensively%20compared%20with%20conventional%20filter-based%0Adenoising%20algorithms%20and%20SOTA%20learning-based%20methods%2C%20using%20both%20realistic%0Asimulated%20US%20images%20and%20human%20carotid%20US%20images.%20Additionally%2C%20data%20from%0Amultiple%20US%20machines%20are%20employed%20to%20evaluate%20model%20generalization%20and%0Aadaptability%20to%20images%20from%20unseen%20domains.%20%5Ctextit%7BCode%20and%20datasets%20will%20be%0Areleased%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeckle2Self%253A%2520Self-Supervised%2520Ultrasound%2520Speckle%2520Reduction%2520Without%2520Clean%250A%2520%2520Data%26entry.906535625%3DXuesong%2520Li%2520and%2520Nassir%2520Navab%2520and%2520Zhongliang%2520Jiang%26entry.1292438233%3D%2520%2520Image%2520denoising%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%252C%2520particularly%2520in%250Amedical%2520ultrasound%2520%2528US%2529%2520imaging%252C%2520where%2520speckle%2520noise%2520significantly%2520degrades%250Aimage%2520quality.%2520Although%2520recent%2520advancements%2520in%2520deep%2520neural%2520networks%2520have%2520led%2520to%250Asubstantial%2520improvements%2520in%2520denoising%2520for%2520natural%2520images%252C%2520these%2520methods%2520cannot%250Abe%2520directly%2520applied%2520to%2520US%2520speckle%2520noise%252C%2520as%2520it%2520is%2520not%2520purely%2520random.%2520Instead%252C%250AUS%2520speckle%2520arises%2520from%2520complex%2520wave%2520interference%2520within%2520the%2520body%250Amicrostructure%252C%2520making%2520it%2520tissue-dependent.%2520This%2520dependency%2520means%2520that%250Aobtaining%2520two%2520independent%2520noisy%2520observations%2520of%2520the%2520same%2520scene%252C%2520as%2520required%2520by%250Apioneering%2520Noise2Noise%252C%2520is%2520not%2520feasible.%2520Additionally%252C%2520blind-spot%2520networks%2520also%250Acannot%2520handle%2520US%2520speckle%2520noise%2520due%2520to%2520its%2520high%2520spatial%2520dependency.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520introduce%2520Speckle2Self%252C%2520a%2520novel%2520self-supervised%2520algorithm%250Afor%2520speckle%2520reduction%2520using%2520only%2520single%2520noisy%2520observations.%2520The%2520key%2520insight%2520is%250Athat%2520applying%2520a%2520multi-scale%2520perturbation%2520%2528MSP%2529%2520operation%2520introduces%250Atissue-dependent%2520variations%2520in%2520the%2520speckle%2520pattern%2520across%2520different%2520scales%252C%250Awhile%2520preserving%2520the%2520shared%2520anatomical%2520structure.%2520This%2520enables%2520effective%250Aspeckle%2520suppression%2520by%2520modeling%2520the%2520clean%2520image%2520as%2520a%2520low-rank%2520signal%2520and%250Aisolating%2520the%2520sparse%2520noise%2520component.%2520To%2520demonstrate%2520its%2520effectiveness%252C%250ASpeckle2Self%2520is%2520comprehensively%2520compared%2520with%2520conventional%2520filter-based%250Adenoising%2520algorithms%2520and%2520SOTA%2520learning-based%2520methods%252C%2520using%2520both%2520realistic%250Asimulated%2520US%2520images%2520and%2520human%2520carotid%2520US%2520images.%2520Additionally%252C%2520data%2520from%250Amultiple%2520US%2520machines%2520are%2520employed%2520to%2520evaluate%2520model%2520generalization%2520and%250Aadaptability%2520to%2520images%2520from%2520unseen%2520domains.%2520%255Ctextit%257BCode%2520and%2520datasets%2520will%2520be%250Areleased%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speckle2Self%3A%20Self-Supervised%20Ultrasound%20Speckle%20Reduction%20Without%20Clean%0A%20%20Data&entry.906535625=Xuesong%20Li%20and%20Nassir%20Navab%20and%20Zhongliang%20Jiang&entry.1292438233=%20%20Image%20denoising%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20particularly%20in%0Amedical%20ultrasound%20%28US%29%20imaging%2C%20where%20speckle%20noise%20significantly%20degrades%0Aimage%20quality.%20Although%20recent%20advancements%20in%20deep%20neural%20networks%20have%20led%20to%0Asubstantial%20improvements%20in%20denoising%20for%20natural%20images%2C%20these%20methods%20cannot%0Abe%20directly%20applied%20to%20US%20speckle%20noise%2C%20as%20it%20is%20not%20purely%20random.%20Instead%2C%0AUS%20speckle%20arises%20from%20complex%20wave%20interference%20within%20the%20body%0Amicrostructure%2C%20making%20it%20tissue-dependent.%20This%20dependency%20means%20that%0Aobtaining%20two%20independent%20noisy%20observations%20of%20the%20same%20scene%2C%20as%20required%20by%0Apioneering%20Noise2Noise%2C%20is%20not%20feasible.%20Additionally%2C%20blind-spot%20networks%20also%0Acannot%20handle%20US%20speckle%20noise%20due%20to%20its%20high%20spatial%20dependency.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20Speckle2Self%2C%20a%20novel%20self-supervised%20algorithm%0Afor%20speckle%20reduction%20using%20only%20single%20noisy%20observations.%20The%20key%20insight%20is%0Athat%20applying%20a%20multi-scale%20perturbation%20%28MSP%29%20operation%20introduces%0Atissue-dependent%20variations%20in%20the%20speckle%20pattern%20across%20different%20scales%2C%0Awhile%20preserving%20the%20shared%20anatomical%20structure.%20This%20enables%20effective%0Aspeckle%20suppression%20by%20modeling%20the%20clean%20image%20as%20a%20low-rank%20signal%20and%0Aisolating%20the%20sparse%20noise%20component.%20To%20demonstrate%20its%20effectiveness%2C%0ASpeckle2Self%20is%20comprehensively%20compared%20with%20conventional%20filter-based%0Adenoising%20algorithms%20and%20SOTA%20learning-based%20methods%2C%20using%20both%20realistic%0Asimulated%20US%20images%20and%20human%20carotid%20US%20images.%20Additionally%2C%20data%20from%0Amultiple%20US%20machines%20are%20employed%20to%20evaluate%20model%20generalization%20and%0Aadaptability%20to%20images%20from%20unseen%20domains.%20%5Ctextit%7BCode%20and%20datasets%20will%20be%0Areleased%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06828v1&entry.124074799=Read"},
{"title": "Bounomodes: the grazing ox algorithm for exploration of clustered\n  anomalies", "author": "Samuel Matloob and Ayan Dutta and O. Patrick Kreidl and Swapnonel Roy and Ladislau B\u00f6l\u00f6ni", "abstract": "  A common class of algorithms for informative path planning (IPP) follows\nboustrophedon (\"as the ox turns\") patterns, which aim to achieve uniform area\ncoverage. However, IPP is often applied in scenarios where anomalies, such as\nplant diseases, pollution, or hurricane damage, appear in clusters. In such\ncases, prioritizing the exploration of anomalous regions over uniform coverage\nis beneficial. This work introduces a class of algorithms referred to as\nbounom\\=odes (\"as the ox grazes\"), which alternates between uniform\nboustrophedon sampling and targeted exploration of detected anomaly clusters.\nWhile uniform sampling can be designed using geometric principles, close\nexploration of clusters depends on the spatial distribution of anomalies and\nmust be learned. In our implementation, the close exploration behavior is\nlearned using deep reinforcement learning algorithms. Experimental evaluations\ndemonstrate that the proposed approach outperforms several established\nbaselines.\n", "link": "http://arxiv.org/abs/2507.06960v1", "date": "2025-07-09", "relevancy": 2.0673, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5267}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5234}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bounomodes%3A%20the%20grazing%20ox%20algorithm%20for%20exploration%20of%20clustered%0A%20%20anomalies&body=Title%3A%20Bounomodes%3A%20the%20grazing%20ox%20algorithm%20for%20exploration%20of%20clustered%0A%20%20anomalies%0AAuthor%3A%20Samuel%20Matloob%20and%20Ayan%20Dutta%20and%20O.%20Patrick%20Kreidl%20and%20Swapnonel%20Roy%20and%20Ladislau%20B%C3%B6l%C3%B6ni%0AAbstract%3A%20%20%20A%20common%20class%20of%20algorithms%20for%20informative%20path%20planning%20%28IPP%29%20follows%0Aboustrophedon%20%28%22as%20the%20ox%20turns%22%29%20patterns%2C%20which%20aim%20to%20achieve%20uniform%20area%0Acoverage.%20However%2C%20IPP%20is%20often%20applied%20in%20scenarios%20where%20anomalies%2C%20such%20as%0Aplant%20diseases%2C%20pollution%2C%20or%20hurricane%20damage%2C%20appear%20in%20clusters.%20In%20such%0Acases%2C%20prioritizing%20the%20exploration%20of%20anomalous%20regions%20over%20uniform%20coverage%0Ais%20beneficial.%20This%20work%20introduces%20a%20class%20of%20algorithms%20referred%20to%20as%0Abounom%5C%3Dodes%20%28%22as%20the%20ox%20grazes%22%29%2C%20which%20alternates%20between%20uniform%0Aboustrophedon%20sampling%20and%20targeted%20exploration%20of%20detected%20anomaly%20clusters.%0AWhile%20uniform%20sampling%20can%20be%20designed%20using%20geometric%20principles%2C%20close%0Aexploration%20of%20clusters%20depends%20on%20the%20spatial%20distribution%20of%20anomalies%20and%0Amust%20be%20learned.%20In%20our%20implementation%2C%20the%20close%20exploration%20behavior%20is%0Alearned%20using%20deep%20reinforcement%20learning%20algorithms.%20Experimental%20evaluations%0Ademonstrate%20that%20the%20proposed%20approach%20outperforms%20several%20established%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBounomodes%253A%2520the%2520grazing%2520ox%2520algorithm%2520for%2520exploration%2520of%2520clustered%250A%2520%2520anomalies%26entry.906535625%3DSamuel%2520Matloob%2520and%2520Ayan%2520Dutta%2520and%2520O.%2520Patrick%2520Kreidl%2520and%2520Swapnonel%2520Roy%2520and%2520Ladislau%2520B%25C3%25B6l%25C3%25B6ni%26entry.1292438233%3D%2520%2520A%2520common%2520class%2520of%2520algorithms%2520for%2520informative%2520path%2520planning%2520%2528IPP%2529%2520follows%250Aboustrophedon%2520%2528%2522as%2520the%2520ox%2520turns%2522%2529%2520patterns%252C%2520which%2520aim%2520to%2520achieve%2520uniform%2520area%250Acoverage.%2520However%252C%2520IPP%2520is%2520often%2520applied%2520in%2520scenarios%2520where%2520anomalies%252C%2520such%2520as%250Aplant%2520diseases%252C%2520pollution%252C%2520or%2520hurricane%2520damage%252C%2520appear%2520in%2520clusters.%2520In%2520such%250Acases%252C%2520prioritizing%2520the%2520exploration%2520of%2520anomalous%2520regions%2520over%2520uniform%2520coverage%250Ais%2520beneficial.%2520This%2520work%2520introduces%2520a%2520class%2520of%2520algorithms%2520referred%2520to%2520as%250Abounom%255C%253Dodes%2520%2528%2522as%2520the%2520ox%2520grazes%2522%2529%252C%2520which%2520alternates%2520between%2520uniform%250Aboustrophedon%2520sampling%2520and%2520targeted%2520exploration%2520of%2520detected%2520anomaly%2520clusters.%250AWhile%2520uniform%2520sampling%2520can%2520be%2520designed%2520using%2520geometric%2520principles%252C%2520close%250Aexploration%2520of%2520clusters%2520depends%2520on%2520the%2520spatial%2520distribution%2520of%2520anomalies%2520and%250Amust%2520be%2520learned.%2520In%2520our%2520implementation%252C%2520the%2520close%2520exploration%2520behavior%2520is%250Alearned%2520using%2520deep%2520reinforcement%2520learning%2520algorithms.%2520Experimental%2520evaluations%250Ademonstrate%2520that%2520the%2520proposed%2520approach%2520outperforms%2520several%2520established%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bounomodes%3A%20the%20grazing%20ox%20algorithm%20for%20exploration%20of%20clustered%0A%20%20anomalies&entry.906535625=Samuel%20Matloob%20and%20Ayan%20Dutta%20and%20O.%20Patrick%20Kreidl%20and%20Swapnonel%20Roy%20and%20Ladislau%20B%C3%B6l%C3%B6ni&entry.1292438233=%20%20A%20common%20class%20of%20algorithms%20for%20informative%20path%20planning%20%28IPP%29%20follows%0Aboustrophedon%20%28%22as%20the%20ox%20turns%22%29%20patterns%2C%20which%20aim%20to%20achieve%20uniform%20area%0Acoverage.%20However%2C%20IPP%20is%20often%20applied%20in%20scenarios%20where%20anomalies%2C%20such%20as%0Aplant%20diseases%2C%20pollution%2C%20or%20hurricane%20damage%2C%20appear%20in%20clusters.%20In%20such%0Acases%2C%20prioritizing%20the%20exploration%20of%20anomalous%20regions%20over%20uniform%20coverage%0Ais%20beneficial.%20This%20work%20introduces%20a%20class%20of%20algorithms%20referred%20to%20as%0Abounom%5C%3Dodes%20%28%22as%20the%20ox%20grazes%22%29%2C%20which%20alternates%20between%20uniform%0Aboustrophedon%20sampling%20and%20targeted%20exploration%20of%20detected%20anomaly%20clusters.%0AWhile%20uniform%20sampling%20can%20be%20designed%20using%20geometric%20principles%2C%20close%0Aexploration%20of%20clusters%20depends%20on%20the%20spatial%20distribution%20of%20anomalies%20and%0Amust%20be%20learned.%20In%20our%20implementation%2C%20the%20close%20exploration%20behavior%20is%0Alearned%20using%20deep%20reinforcement%20learning%20algorithms.%20Experimental%20evaluations%0Ademonstrate%20that%20the%20proposed%20approach%20outperforms%20several%20established%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06960v1&entry.124074799=Read"},
{"title": "First Return, Entropy-Eliciting Explore", "author": "Tianyu Zheng and Tianshun Xing and Qingshui Gu and Taoran Liang and Xingwei Qu and Xin Zhou and Yizhi Li and Zhoufutu Wen and Chenghua Lin and Wenhao Huang and Qian Liu and Ge Zhang and Zejun Ma", "abstract": "  Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.\n", "link": "http://arxiv.org/abs/2507.07017v1", "date": "2025-07-09", "relevancy": 2.0599, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20First%20Return%2C%20Entropy-Eliciting%20Explore&body=Title%3A%20First%20Return%2C%20Entropy-Eliciting%20Explore%0AAuthor%3A%20Tianyu%20Zheng%20and%20Tianshun%20Xing%20and%20Qingshui%20Gu%20and%20Taoran%20Liang%20and%20Xingwei%20Qu%20and%20Xin%20Zhou%20and%20Yizhi%20Li%20and%20Zhoufutu%20Wen%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Qian%20Liu%20and%20Ge%20Zhang%20and%20Zejun%20Ma%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Verifiable%20Rewards%20%28RLVR%29%20improves%20the%20reasoning%0Aabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20but%20it%20struggles%20with%20unstable%0Aexploration.%20We%20propose%20FR3E%20%28First%20Return%2C%20Entropy-Eliciting%20Explore%29%2C%20a%0Astructured%20exploration%20framework%20that%20identifies%20high-uncertainty%20decision%0Apoints%20in%20reasoning%20trajectories%20and%20performs%20targeted%20rollouts%20to%20construct%0Asemantically%20grounded%20intermediate%20feedback.%20Our%20method%20provides%20targeted%0Aguidance%20without%20relying%20on%20dense%20supervision.%20Empirical%20results%20on%0Amathematical%20reasoning%20benchmarks%28AIME24%29%20show%20that%20FR3E%20promotes%20more%20stable%0Atraining%2C%20produces%20longer%20and%20more%20coherent%20responses%2C%20and%20increases%20the%0Aproportion%20of%20fully%20correct%20trajectories.%20These%20results%20highlight%20the%0Aframework%27s%20effectiveness%20in%20improving%20LLM%20reasoning%20through%20more%20robust%20and%0Astructured%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFirst%2520Return%252C%2520Entropy-Eliciting%2520Explore%26entry.906535625%3DTianyu%2520Zheng%2520and%2520Tianshun%2520Xing%2520and%2520Qingshui%2520Gu%2520and%2520Taoran%2520Liang%2520and%2520Xingwei%2520Qu%2520and%2520Xin%2520Zhou%2520and%2520Yizhi%2520Li%2520and%2520Zhoufutu%2520Wen%2520and%2520Chenghua%2520Lin%2520and%2520Wenhao%2520Huang%2520and%2520Qian%2520Liu%2520and%2520Ge%2520Zhang%2520and%2520Zejun%2520Ma%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520improves%2520the%2520reasoning%250Aabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520but%2520it%2520struggles%2520with%2520unstable%250Aexploration.%2520We%2520propose%2520FR3E%2520%2528First%2520Return%252C%2520Entropy-Eliciting%2520Explore%2529%252C%2520a%250Astructured%2520exploration%2520framework%2520that%2520identifies%2520high-uncertainty%2520decision%250Apoints%2520in%2520reasoning%2520trajectories%2520and%2520performs%2520targeted%2520rollouts%2520to%2520construct%250Asemantically%2520grounded%2520intermediate%2520feedback.%2520Our%2520method%2520provides%2520targeted%250Aguidance%2520without%2520relying%2520on%2520dense%2520supervision.%2520Empirical%2520results%2520on%250Amathematical%2520reasoning%2520benchmarks%2528AIME24%2529%2520show%2520that%2520FR3E%2520promotes%2520more%2520stable%250Atraining%252C%2520produces%2520longer%2520and%2520more%2520coherent%2520responses%252C%2520and%2520increases%2520the%250Aproportion%2520of%2520fully%2520correct%2520trajectories.%2520These%2520results%2520highlight%2520the%250Aframework%2527s%2520effectiveness%2520in%2520improving%2520LLM%2520reasoning%2520through%2520more%2520robust%2520and%250Astructured%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=First%20Return%2C%20Entropy-Eliciting%20Explore&entry.906535625=Tianyu%20Zheng%20and%20Tianshun%20Xing%20and%20Qingshui%20Gu%20and%20Taoran%20Liang%20and%20Xingwei%20Qu%20and%20Xin%20Zhou%20and%20Yizhi%20Li%20and%20Zhoufutu%20Wen%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Qian%20Liu%20and%20Ge%20Zhang%20and%20Zejun%20Ma&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Verifiable%20Rewards%20%28RLVR%29%20improves%20the%20reasoning%0Aabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20but%20it%20struggles%20with%20unstable%0Aexploration.%20We%20propose%20FR3E%20%28First%20Return%2C%20Entropy-Eliciting%20Explore%29%2C%20a%0Astructured%20exploration%20framework%20that%20identifies%20high-uncertainty%20decision%0Apoints%20in%20reasoning%20trajectories%20and%20performs%20targeted%20rollouts%20to%20construct%0Asemantically%20grounded%20intermediate%20feedback.%20Our%20method%20provides%20targeted%0Aguidance%20without%20relying%20on%20dense%20supervision.%20Empirical%20results%20on%0Amathematical%20reasoning%20benchmarks%28AIME24%29%20show%20that%20FR3E%20promotes%20more%20stable%0Atraining%2C%20produces%20longer%20and%20more%20coherent%20responses%2C%20and%20increases%20the%0Aproportion%20of%20fully%20correct%20trajectories.%20These%20results%20highlight%20the%0Aframework%27s%20effectiveness%20in%20improving%20LLM%20reasoning%20through%20more%20robust%20and%0Astructured%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07017v1&entry.124074799=Read"},
{"title": "From Video to EEG: Adapting Joint Embedding Predictive Architecture to\n  Uncover Visual Concepts in Brain Signal Analysis", "author": "Amirabbas Hojjati and Lu Li and Ibrahim Hameed and Anis Yazidi and Pedro G. Lind and Rabindra Khadka", "abstract": "  EEG signals capture brain activity with high temporal and low spatial\nresolution, supporting applications such as neurological diagnosis, cognitive\nmonitoring, and brain-computer interfaces. However, effective analysis is\nhindered by limited labeled data, high dimensionality, and the absence of\nscalable models that fully capture spatiotemporal dependencies. Existing\nself-supervised learning (SSL) methods often focus on either spatial or\ntemporal features, leading to suboptimal representations. To this end, we\npropose EEG-VJEPA, a novel adaptation of the Video Joint Embedding Predictive\nArchitecture (V-JEPA) for EEG classification. By treating EEG as video-like\nsequences, EEG-VJEPA learns semantically meaningful spatiotemporal\nrepresentations using joint embeddings and adaptive masking. To our knowledge,\nthis is the first work that exploits V-JEPA for EEG classification and explores\nthe visual concepts learned by the model. Evaluations on the publicly available\nTemple University Hospital (TUH) Abnormal EEG dataset show that EEG-VJEPA\noutperforms existing state-of-the-art models in classification accuracy. Beyond\nclassification accuracy, EEG-VJEPA captures physiologically relevant spatial\nand temporal signal patterns, offering interpretable embeddings that may\nsupport human-AI collaboration in diagnostic workflows. These findings position\nEEG-VJEPA as a promising framework for scalable, trustworthy EEG analysis in\nreal-world clinical settings.\n", "link": "http://arxiv.org/abs/2507.03633v3", "date": "2025-07-09", "relevancy": 2.0532, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Video%20to%20EEG%3A%20Adapting%20Joint%20Embedding%20Predictive%20Architecture%20to%0A%20%20Uncover%20Visual%20Concepts%20in%20Brain%20Signal%20Analysis&body=Title%3A%20From%20Video%20to%20EEG%3A%20Adapting%20Joint%20Embedding%20Predictive%20Architecture%20to%0A%20%20Uncover%20Visual%20Concepts%20in%20Brain%20Signal%20Analysis%0AAuthor%3A%20Amirabbas%20Hojjati%20and%20Lu%20Li%20and%20Ibrahim%20Hameed%20and%20Anis%20Yazidi%20and%20Pedro%20G.%20Lind%20and%20Rabindra%20Khadka%0AAbstract%3A%20%20%20EEG%20signals%20capture%20brain%20activity%20with%20high%20temporal%20and%20low%20spatial%0Aresolution%2C%20supporting%20applications%20such%20as%20neurological%20diagnosis%2C%20cognitive%0Amonitoring%2C%20and%20brain-computer%20interfaces.%20However%2C%20effective%20analysis%20is%0Ahindered%20by%20limited%20labeled%20data%2C%20high%20dimensionality%2C%20and%20the%20absence%20of%0Ascalable%20models%20that%20fully%20capture%20spatiotemporal%20dependencies.%20Existing%0Aself-supervised%20learning%20%28SSL%29%20methods%20often%20focus%20on%20either%20spatial%20or%0Atemporal%20features%2C%20leading%20to%20suboptimal%20representations.%20To%20this%20end%2C%20we%0Apropose%20EEG-VJEPA%2C%20a%20novel%20adaptation%20of%20the%20Video%20Joint%20Embedding%20Predictive%0AArchitecture%20%28V-JEPA%29%20for%20EEG%20classification.%20By%20treating%20EEG%20as%20video-like%0Asequences%2C%20EEG-VJEPA%20learns%20semantically%20meaningful%20spatiotemporal%0Arepresentations%20using%20joint%20embeddings%20and%20adaptive%20masking.%20To%20our%20knowledge%2C%0Athis%20is%20the%20first%20work%20that%20exploits%20V-JEPA%20for%20EEG%20classification%20and%20explores%0Athe%20visual%20concepts%20learned%20by%20the%20model.%20Evaluations%20on%20the%20publicly%20available%0ATemple%20University%20Hospital%20%28TUH%29%20Abnormal%20EEG%20dataset%20show%20that%20EEG-VJEPA%0Aoutperforms%20existing%20state-of-the-art%20models%20in%20classification%20accuracy.%20Beyond%0Aclassification%20accuracy%2C%20EEG-VJEPA%20captures%20physiologically%20relevant%20spatial%0Aand%20temporal%20signal%20patterns%2C%20offering%20interpretable%20embeddings%20that%20may%0Asupport%20human-AI%20collaboration%20in%20diagnostic%20workflows.%20These%20findings%20position%0AEEG-VJEPA%20as%20a%20promising%20framework%20for%20scalable%2C%20trustworthy%20EEG%20analysis%20in%0Areal-world%20clinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03633v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Video%2520to%2520EEG%253A%2520Adapting%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520to%250A%2520%2520Uncover%2520Visual%2520Concepts%2520in%2520Brain%2520Signal%2520Analysis%26entry.906535625%3DAmirabbas%2520Hojjati%2520and%2520Lu%2520Li%2520and%2520Ibrahim%2520Hameed%2520and%2520Anis%2520Yazidi%2520and%2520Pedro%2520G.%2520Lind%2520and%2520Rabindra%2520Khadka%26entry.1292438233%3D%2520%2520EEG%2520signals%2520capture%2520brain%2520activity%2520with%2520high%2520temporal%2520and%2520low%2520spatial%250Aresolution%252C%2520supporting%2520applications%2520such%2520as%2520neurological%2520diagnosis%252C%2520cognitive%250Amonitoring%252C%2520and%2520brain-computer%2520interfaces.%2520However%252C%2520effective%2520analysis%2520is%250Ahindered%2520by%2520limited%2520labeled%2520data%252C%2520high%2520dimensionality%252C%2520and%2520the%2520absence%2520of%250Ascalable%2520models%2520that%2520fully%2520capture%2520spatiotemporal%2520dependencies.%2520Existing%250Aself-supervised%2520learning%2520%2528SSL%2529%2520methods%2520often%2520focus%2520on%2520either%2520spatial%2520or%250Atemporal%2520features%252C%2520leading%2520to%2520suboptimal%2520representations.%2520To%2520this%2520end%252C%2520we%250Apropose%2520EEG-VJEPA%252C%2520a%2520novel%2520adaptation%2520of%2520the%2520Video%2520Joint%2520Embedding%2520Predictive%250AArchitecture%2520%2528V-JEPA%2529%2520for%2520EEG%2520classification.%2520By%2520treating%2520EEG%2520as%2520video-like%250Asequences%252C%2520EEG-VJEPA%2520learns%2520semantically%2520meaningful%2520spatiotemporal%250Arepresentations%2520using%2520joint%2520embeddings%2520and%2520adaptive%2520masking.%2520To%2520our%2520knowledge%252C%250Athis%2520is%2520the%2520first%2520work%2520that%2520exploits%2520V-JEPA%2520for%2520EEG%2520classification%2520and%2520explores%250Athe%2520visual%2520concepts%2520learned%2520by%2520the%2520model.%2520Evaluations%2520on%2520the%2520publicly%2520available%250ATemple%2520University%2520Hospital%2520%2528TUH%2529%2520Abnormal%2520EEG%2520dataset%2520show%2520that%2520EEG-VJEPA%250Aoutperforms%2520existing%2520state-of-the-art%2520models%2520in%2520classification%2520accuracy.%2520Beyond%250Aclassification%2520accuracy%252C%2520EEG-VJEPA%2520captures%2520physiologically%2520relevant%2520spatial%250Aand%2520temporal%2520signal%2520patterns%252C%2520offering%2520interpretable%2520embeddings%2520that%2520may%250Asupport%2520human-AI%2520collaboration%2520in%2520diagnostic%2520workflows.%2520These%2520findings%2520position%250AEEG-VJEPA%2520as%2520a%2520promising%2520framework%2520for%2520scalable%252C%2520trustworthy%2520EEG%2520analysis%2520in%250Areal-world%2520clinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03633v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Video%20to%20EEG%3A%20Adapting%20Joint%20Embedding%20Predictive%20Architecture%20to%0A%20%20Uncover%20Visual%20Concepts%20in%20Brain%20Signal%20Analysis&entry.906535625=Amirabbas%20Hojjati%20and%20Lu%20Li%20and%20Ibrahim%20Hameed%20and%20Anis%20Yazidi%20and%20Pedro%20G.%20Lind%20and%20Rabindra%20Khadka&entry.1292438233=%20%20EEG%20signals%20capture%20brain%20activity%20with%20high%20temporal%20and%20low%20spatial%0Aresolution%2C%20supporting%20applications%20such%20as%20neurological%20diagnosis%2C%20cognitive%0Amonitoring%2C%20and%20brain-computer%20interfaces.%20However%2C%20effective%20analysis%20is%0Ahindered%20by%20limited%20labeled%20data%2C%20high%20dimensionality%2C%20and%20the%20absence%20of%0Ascalable%20models%20that%20fully%20capture%20spatiotemporal%20dependencies.%20Existing%0Aself-supervised%20learning%20%28SSL%29%20methods%20often%20focus%20on%20either%20spatial%20or%0Atemporal%20features%2C%20leading%20to%20suboptimal%20representations.%20To%20this%20end%2C%20we%0Apropose%20EEG-VJEPA%2C%20a%20novel%20adaptation%20of%20the%20Video%20Joint%20Embedding%20Predictive%0AArchitecture%20%28V-JEPA%29%20for%20EEG%20classification.%20By%20treating%20EEG%20as%20video-like%0Asequences%2C%20EEG-VJEPA%20learns%20semantically%20meaningful%20spatiotemporal%0Arepresentations%20using%20joint%20embeddings%20and%20adaptive%20masking.%20To%20our%20knowledge%2C%0Athis%20is%20the%20first%20work%20that%20exploits%20V-JEPA%20for%20EEG%20classification%20and%20explores%0Athe%20visual%20concepts%20learned%20by%20the%20model.%20Evaluations%20on%20the%20publicly%20available%0ATemple%20University%20Hospital%20%28TUH%29%20Abnormal%20EEG%20dataset%20show%20that%20EEG-VJEPA%0Aoutperforms%20existing%20state-of-the-art%20models%20in%20classification%20accuracy.%20Beyond%0Aclassification%20accuracy%2C%20EEG-VJEPA%20captures%20physiologically%20relevant%20spatial%0Aand%20temporal%20signal%20patterns%2C%20offering%20interpretable%20embeddings%20that%20may%0Asupport%20human-AI%20collaboration%20in%20diagnostic%20workflows.%20These%20findings%20position%0AEEG-VJEPA%20as%20a%20promising%20framework%20for%20scalable%2C%20trustworthy%20EEG%20analysis%20in%0Areal-world%20clinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03633v3&entry.124074799=Read"},
{"title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits", "author": "Duy Nguyen and Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal", "abstract": "  Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling.\n", "link": "http://arxiv.org/abs/2410.01735v2", "date": "2025-07-09", "relevancy": 2.051, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.529}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LASeR%3A%20Learning%20to%20Adaptively%20Select%20Reward%20Models%20with%20Multi-Armed%0A%20%20Bandits&body=Title%3A%20LASeR%3A%20Learning%20to%20Adaptively%20Select%20Reward%20Models%20with%20Multi-Armed%0A%20%20Bandits%0AAuthor%3A%20Duy%20Nguyen%20and%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Reward%20Models%20%28RMs%29%20are%20crucial%20to%20aligning%20large%20language%20models%20%28LLMs%29%2C%20but%0Athe%20degree%20to%20which%20an%20RM%20specialized%20to%20one%20task%20%28e.g.%20writing%29%20generalizes%20to%0Anew%20tasks%20%28e.g.%20math%29%20is%20often%20not%20known%20a%20priori%2C%20often%20making%20using%20only%20one%0Afixed%20RM%20to%20train%20LLMs%20suboptimal.%20However%2C%20optimizing%20LLMs%20with%20multiple%20RMs%0Asimultaneously%20can%20incur%20a%20prohibitively%20high%20computational%20cost%20and%20lead%20to%0Aconflicting%20signals%20from%20different%20RMs%20that%20may%20degrade%20performance.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20LASeR%20%28Learning%20to%20Adaptively%20Select%20Rewards%29%2C%0Awhich%20frames%20reward%20model%20selection%20as%20a%20multi-armed%20bandit%20problem%2C%0Aefficiently%20and%20iteratively%20training%20LLMs%20using%20multiple%20RMs%20by%20selecting%20the%0Amost%20well-suited%20RM%20for%20each%20instance.%20On%20commonsense%20and%20math%20reasoning%20tasks%2C%0Awe%20show%20that%20LASeR%20boosts%20iterative%20LLM%20training%2C%20improving%20the%20absolute%0Aaverage%20accuracy%20of%20Llama-3-8B%20over%20three%20datasets%20by%202.67%25%20over%20an%20ensemble%20of%0ARM%20scores%20while%20also%20showing%20superior%20efficiency%20%28e.g.%2C%20a%202x%20speedup%29.%0AMoreover%2C%20on%20WildChat%20%28open-ended%20instruction-following%20tasks%29%2C%20LASeR%20leads%20to%0Aa%2072.69%25%20AlpacaEval%20win%20rate%20over%20the%20RM%20score%20ensemble%20baseline.%20Extending%20to%0Along-context%20generation%2C%20LASeR%20improves%20by%202.96%20F1%20points%20%28avg.%29%20on%0Asingle-document%20QA%20tasks%20and%202.97%20F1%20points%20on%20few-shot%20learning%20over%20the%20RM%0Ascore%20ensemble%20baseline%20with%20best-of-n%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01735v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLASeR%253A%2520Learning%2520to%2520Adaptively%2520Select%2520Reward%2520Models%2520with%2520Multi-Armed%250A%2520%2520Bandits%26entry.906535625%3DDuy%2520Nguyen%2520and%2520Archiki%2520Prasad%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Reward%2520Models%2520%2528RMs%2529%2520are%2520crucial%2520to%2520aligning%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520but%250Athe%2520degree%2520to%2520which%2520an%2520RM%2520specialized%2520to%2520one%2520task%2520%2528e.g.%2520writing%2529%2520generalizes%2520to%250Anew%2520tasks%2520%2528e.g.%2520math%2529%2520is%2520often%2520not%2520known%2520a%2520priori%252C%2520often%2520making%2520using%2520only%2520one%250Afixed%2520RM%2520to%2520train%2520LLMs%2520suboptimal.%2520However%252C%2520optimizing%2520LLMs%2520with%2520multiple%2520RMs%250Asimultaneously%2520can%2520incur%2520a%2520prohibitively%2520high%2520computational%2520cost%2520and%2520lead%2520to%250Aconflicting%2520signals%2520from%2520different%2520RMs%2520that%2520may%2520degrade%2520performance.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520LASeR%2520%2528Learning%2520to%2520Adaptively%2520Select%2520Rewards%2529%252C%250Awhich%2520frames%2520reward%2520model%2520selection%2520as%2520a%2520multi-armed%2520bandit%2520problem%252C%250Aefficiently%2520and%2520iteratively%2520training%2520LLMs%2520using%2520multiple%2520RMs%2520by%2520selecting%2520the%250Amost%2520well-suited%2520RM%2520for%2520each%2520instance.%2520On%2520commonsense%2520and%2520math%2520reasoning%2520tasks%252C%250Awe%2520show%2520that%2520LASeR%2520boosts%2520iterative%2520LLM%2520training%252C%2520improving%2520the%2520absolute%250Aaverage%2520accuracy%2520of%2520Llama-3-8B%2520over%2520three%2520datasets%2520by%25202.67%2525%2520over%2520an%2520ensemble%2520of%250ARM%2520scores%2520while%2520also%2520showing%2520superior%2520efficiency%2520%2528e.g.%252C%2520a%25202x%2520speedup%2529.%250AMoreover%252C%2520on%2520WildChat%2520%2528open-ended%2520instruction-following%2520tasks%2529%252C%2520LASeR%2520leads%2520to%250Aa%252072.69%2525%2520AlpacaEval%2520win%2520rate%2520over%2520the%2520RM%2520score%2520ensemble%2520baseline.%2520Extending%2520to%250Along-context%2520generation%252C%2520LASeR%2520improves%2520by%25202.96%2520F1%2520points%2520%2528avg.%2529%2520on%250Asingle-document%2520QA%2520tasks%2520and%25202.97%2520F1%2520points%2520on%2520few-shot%2520learning%2520over%2520the%2520RM%250Ascore%2520ensemble%2520baseline%2520with%2520best-of-n%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01735v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LASeR%3A%20Learning%20to%20Adaptively%20Select%20Reward%20Models%20with%20Multi-Armed%0A%20%20Bandits&entry.906535625=Duy%20Nguyen%20and%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal&entry.1292438233=%20%20Reward%20Models%20%28RMs%29%20are%20crucial%20to%20aligning%20large%20language%20models%20%28LLMs%29%2C%20but%0Athe%20degree%20to%20which%20an%20RM%20specialized%20to%20one%20task%20%28e.g.%20writing%29%20generalizes%20to%0Anew%20tasks%20%28e.g.%20math%29%20is%20often%20not%20known%20a%20priori%2C%20often%20making%20using%20only%20one%0Afixed%20RM%20to%20train%20LLMs%20suboptimal.%20However%2C%20optimizing%20LLMs%20with%20multiple%20RMs%0Asimultaneously%20can%20incur%20a%20prohibitively%20high%20computational%20cost%20and%20lead%20to%0Aconflicting%20signals%20from%20different%20RMs%20that%20may%20degrade%20performance.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20LASeR%20%28Learning%20to%20Adaptively%20Select%20Rewards%29%2C%0Awhich%20frames%20reward%20model%20selection%20as%20a%20multi-armed%20bandit%20problem%2C%0Aefficiently%20and%20iteratively%20training%20LLMs%20using%20multiple%20RMs%20by%20selecting%20the%0Amost%20well-suited%20RM%20for%20each%20instance.%20On%20commonsense%20and%20math%20reasoning%20tasks%2C%0Awe%20show%20that%20LASeR%20boosts%20iterative%20LLM%20training%2C%20improving%20the%20absolute%0Aaverage%20accuracy%20of%20Llama-3-8B%20over%20three%20datasets%20by%202.67%25%20over%20an%20ensemble%20of%0ARM%20scores%20while%20also%20showing%20superior%20efficiency%20%28e.g.%2C%20a%202x%20speedup%29.%0AMoreover%2C%20on%20WildChat%20%28open-ended%20instruction-following%20tasks%29%2C%20LASeR%20leads%20to%0Aa%2072.69%25%20AlpacaEval%20win%20rate%20over%20the%20RM%20score%20ensemble%20baseline.%20Extending%20to%0Along-context%20generation%2C%20LASeR%20improves%20by%202.96%20F1%20points%20%28avg.%29%20on%0Asingle-document%20QA%20tasks%20and%202.97%20F1%20points%20on%20few-shot%20learning%20over%20the%20RM%0Ascore%20ensemble%20baseline%20with%20best-of-n%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01735v2&entry.124074799=Read"},
{"title": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming", "author": "Yilun Hao and Yang Zhang and Chuchu Fan", "abstract": "  While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp.\n", "link": "http://arxiv.org/abs/2410.12112v3", "date": "2025-07-09", "relevancy": 2.0414, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planning%20Anything%20with%20Rigor%3A%20General-Purpose%20Zero-Shot%20Planning%20with%0A%20%20LLM-based%20Formalized%20Programming&body=Title%3A%20Planning%20Anything%20with%20Rigor%3A%20General-Purpose%20Zero-Shot%20Planning%20with%0A%20%20LLM-based%20Formalized%20Programming%0AAuthor%3A%20Yilun%20Hao%20and%20Yang%20Zhang%20and%20Chuchu%20Fan%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20have%20recently%20demonstrated%20strong%0Apotential%20in%20solving%20planning%20problems%2C%20there%20is%20a%20trade-off%20between%0Aflexibility%20and%20complexity.%20LLMs%2C%20as%20zero-shot%20planners%20themselves%2C%20are%20still%0Anot%20capable%20of%20directly%20generating%20valid%20plans%20for%20complex%20planning%20problems%0Asuch%20as%20multi-constraint%20or%20long-horizon%20tasks.%20On%20the%20other%20hand%2C%20many%0Aframeworks%20aiming%20to%20solve%20complex%20planning%20problems%20often%20rely%20on%0Atask-specific%20preparatory%20efforts%2C%20such%20as%20task-specific%20in-context%20examples%0Aand%20pre-defined%20critics/verifiers%2C%20which%20limits%20their%20cross-task%20generalization%0Acapability.%20In%20this%20paper%2C%20we%20tackle%20these%20challenges%20by%20observing%20that%20the%0Acore%20of%20many%20planning%20problems%20lies%20in%20optimization%20problems%3A%20searching%20for%20the%0Aoptimal%20solution%20%28best%20plan%29%20with%20goals%20subject%20to%20constraints%20%28preconditions%0Aand%20effects%20of%20decisions%29.%20With%20LLMs%27%20commonsense%2C%20reasoning%2C%20and%20programming%0Acapabilities%2C%20this%20opens%20up%20the%20possibilities%20of%20a%20universal%20LLM-based%20approach%0Ato%20planning%20problems.%20Inspired%20by%20this%20observation%2C%20we%20propose%20LLMFP%2C%20a%0Ageneral-purpose%20framework%20that%20leverages%20LLMs%20to%20capture%20key%20information%20from%0Aplanning%20problems%20and%20formally%20formulate%20and%20solve%20them%20as%20optimization%0Aproblems%20from%20scratch%2C%20with%20no%20task-specific%20examples%20needed.%20We%20apply%20LLMFP%20to%0A9%20planning%20problems%2C%20ranging%20from%20multi-constraint%20decision%20making%20to%0Amulti-step%20planning%20problems%2C%20and%20demonstrate%20that%20LLMFP%20achieves%20on%20average%0A83.7%25%20and%2086.8%25%20optimal%20rate%20across%209%20tasks%20for%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%0Asignificantly%20outperforming%20the%20best%20baseline%20%28direct%20planning%20with%20OpenAI%0Ao1-preview%29%20with%2037.6%25%20and%2040.7%25%20improvements.%20We%20also%20validate%20components%20of%0ALLMFP%20with%20ablation%20experiments%20and%20analyzed%20the%20underlying%20success%20and%20failure%0Areasons.%20Project%20page%3A%20https%3A//sites.google.com/view/llmfp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12112v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanning%2520Anything%2520with%2520Rigor%253A%2520General-Purpose%2520Zero-Shot%2520Planning%2520with%250A%2520%2520LLM-based%2520Formalized%2520Programming%26entry.906535625%3DYilun%2520Hao%2520and%2520Yang%2520Zhang%2520and%2520Chuchu%2520Fan%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520demonstrated%2520strong%250Apotential%2520in%2520solving%2520planning%2520problems%252C%2520there%2520is%2520a%2520trade-off%2520between%250Aflexibility%2520and%2520complexity.%2520LLMs%252C%2520as%2520zero-shot%2520planners%2520themselves%252C%2520are%2520still%250Anot%2520capable%2520of%2520directly%2520generating%2520valid%2520plans%2520for%2520complex%2520planning%2520problems%250Asuch%2520as%2520multi-constraint%2520or%2520long-horizon%2520tasks.%2520On%2520the%2520other%2520hand%252C%2520many%250Aframeworks%2520aiming%2520to%2520solve%2520complex%2520planning%2520problems%2520often%2520rely%2520on%250Atask-specific%2520preparatory%2520efforts%252C%2520such%2520as%2520task-specific%2520in-context%2520examples%250Aand%2520pre-defined%2520critics/verifiers%252C%2520which%2520limits%2520their%2520cross-task%2520generalization%250Acapability.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520these%2520challenges%2520by%2520observing%2520that%2520the%250Acore%2520of%2520many%2520planning%2520problems%2520lies%2520in%2520optimization%2520problems%253A%2520searching%2520for%2520the%250Aoptimal%2520solution%2520%2528best%2520plan%2529%2520with%2520goals%2520subject%2520to%2520constraints%2520%2528preconditions%250Aand%2520effects%2520of%2520decisions%2529.%2520With%2520LLMs%2527%2520commonsense%252C%2520reasoning%252C%2520and%2520programming%250Acapabilities%252C%2520this%2520opens%2520up%2520the%2520possibilities%2520of%2520a%2520universal%2520LLM-based%2520approach%250Ato%2520planning%2520problems.%2520Inspired%2520by%2520this%2520observation%252C%2520we%2520propose%2520LLMFP%252C%2520a%250Ageneral-purpose%2520framework%2520that%2520leverages%2520LLMs%2520to%2520capture%2520key%2520information%2520from%250Aplanning%2520problems%2520and%2520formally%2520formulate%2520and%2520solve%2520them%2520as%2520optimization%250Aproblems%2520from%2520scratch%252C%2520with%2520no%2520task-specific%2520examples%2520needed.%2520We%2520apply%2520LLMFP%2520to%250A9%2520planning%2520problems%252C%2520ranging%2520from%2520multi-constraint%2520decision%2520making%2520to%250Amulti-step%2520planning%2520problems%252C%2520and%2520demonstrate%2520that%2520LLMFP%2520achieves%2520on%2520average%250A83.7%2525%2520and%252086.8%2525%2520optimal%2520rate%2520across%25209%2520tasks%2520for%2520GPT-4o%2520and%2520Claude%25203.5%2520Sonnet%252C%250Asignificantly%2520outperforming%2520the%2520best%2520baseline%2520%2528direct%2520planning%2520with%2520OpenAI%250Ao1-preview%2529%2520with%252037.6%2525%2520and%252040.7%2525%2520improvements.%2520We%2520also%2520validate%2520components%2520of%250ALLMFP%2520with%2520ablation%2520experiments%2520and%2520analyzed%2520the%2520underlying%2520success%2520and%2520failure%250Areasons.%2520Project%2520page%253A%2520https%253A//sites.google.com/view/llmfp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12112v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planning%20Anything%20with%20Rigor%3A%20General-Purpose%20Zero-Shot%20Planning%20with%0A%20%20LLM-based%20Formalized%20Programming&entry.906535625=Yilun%20Hao%20and%20Yang%20Zhang%20and%20Chuchu%20Fan&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20have%20recently%20demonstrated%20strong%0Apotential%20in%20solving%20planning%20problems%2C%20there%20is%20a%20trade-off%20between%0Aflexibility%20and%20complexity.%20LLMs%2C%20as%20zero-shot%20planners%20themselves%2C%20are%20still%0Anot%20capable%20of%20directly%20generating%20valid%20plans%20for%20complex%20planning%20problems%0Asuch%20as%20multi-constraint%20or%20long-horizon%20tasks.%20On%20the%20other%20hand%2C%20many%0Aframeworks%20aiming%20to%20solve%20complex%20planning%20problems%20often%20rely%20on%0Atask-specific%20preparatory%20efforts%2C%20such%20as%20task-specific%20in-context%20examples%0Aand%20pre-defined%20critics/verifiers%2C%20which%20limits%20their%20cross-task%20generalization%0Acapability.%20In%20this%20paper%2C%20we%20tackle%20these%20challenges%20by%20observing%20that%20the%0Acore%20of%20many%20planning%20problems%20lies%20in%20optimization%20problems%3A%20searching%20for%20the%0Aoptimal%20solution%20%28best%20plan%29%20with%20goals%20subject%20to%20constraints%20%28preconditions%0Aand%20effects%20of%20decisions%29.%20With%20LLMs%27%20commonsense%2C%20reasoning%2C%20and%20programming%0Acapabilities%2C%20this%20opens%20up%20the%20possibilities%20of%20a%20universal%20LLM-based%20approach%0Ato%20planning%20problems.%20Inspired%20by%20this%20observation%2C%20we%20propose%20LLMFP%2C%20a%0Ageneral-purpose%20framework%20that%20leverages%20LLMs%20to%20capture%20key%20information%20from%0Aplanning%20problems%20and%20formally%20formulate%20and%20solve%20them%20as%20optimization%0Aproblems%20from%20scratch%2C%20with%20no%20task-specific%20examples%20needed.%20We%20apply%20LLMFP%20to%0A9%20planning%20problems%2C%20ranging%20from%20multi-constraint%20decision%20making%20to%0Amulti-step%20planning%20problems%2C%20and%20demonstrate%20that%20LLMFP%20achieves%20on%20average%0A83.7%25%20and%2086.8%25%20optimal%20rate%20across%209%20tasks%20for%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%0Asignificantly%20outperforming%20the%20best%20baseline%20%28direct%20planning%20with%20OpenAI%0Ao1-preview%29%20with%2037.6%25%20and%2040.7%25%20improvements.%20We%20also%20validate%20components%20of%0ALLMFP%20with%20ablation%20experiments%20and%20analyzed%20the%20underlying%20success%20and%20failure%0Areasons.%20Project%20page%3A%20https%3A//sites.google.com/view/llmfp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12112v3&entry.124074799=Read"},
{"title": "Integrating Pathology Foundation Models and Spatial Transcriptomics for\n  Cellular Decomposition from Histology Images", "author": "Yutong Sun and Sichen Zhu and Peng Qiu", "abstract": "  The rapid development of digital pathology and modern deep learning has\nfacilitated the emergence of pathology foundation models that are expected to\nsolve general pathology problems under various disease conditions in one\nunified model, with or without fine-tuning. In parallel, spatial\ntranscriptomics has emerged as a transformative technology that enables the\nprofiling of gene expression on hematoxylin and eosin (H&E) stained histology\nimages. Spatial transcriptomics unlocks the unprecedented opportunity to dive\ninto existing histology images at a more granular, cellular level. In this\nwork, we propose a lightweight and training-efficient approach to predict\ncellular composition directly from H&E-stained histology images by leveraging\ninformation-enriched feature embeddings extracted from pre-trained pathology\nfoundation models. By training a lightweight multi-layer perceptron (MLP)\nregressor on cell-type abundances derived via cell2location, our method\nefficiently distills knowledge from pathology foundation models and\ndemonstrates the ability to accurately predict cell-type compositions from\nhistology images, without physically performing the costly spatial\ntranscriptomics. Our method demonstrates competitive performance compared to\nexisting methods such as Hist2Cell, while significantly reducing computational\ncomplexity.\n", "link": "http://arxiv.org/abs/2507.07013v1", "date": "2025-07-09", "relevancy": 2.0277, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.52}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Pathology%20Foundation%20Models%20and%20Spatial%20Transcriptomics%20for%0A%20%20Cellular%20Decomposition%20from%20Histology%20Images&body=Title%3A%20Integrating%20Pathology%20Foundation%20Models%20and%20Spatial%20Transcriptomics%20for%0A%20%20Cellular%20Decomposition%20from%20Histology%20Images%0AAuthor%3A%20Yutong%20Sun%20and%20Sichen%20Zhu%20and%20Peng%20Qiu%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20digital%20pathology%20and%20modern%20deep%20learning%20has%0Afacilitated%20the%20emergence%20of%20pathology%20foundation%20models%20that%20are%20expected%20to%0Asolve%20general%20pathology%20problems%20under%20various%20disease%20conditions%20in%20one%0Aunified%20model%2C%20with%20or%20without%20fine-tuning.%20In%20parallel%2C%20spatial%0Atranscriptomics%20has%20emerged%20as%20a%20transformative%20technology%20that%20enables%20the%0Aprofiling%20of%20gene%20expression%20on%20hematoxylin%20and%20eosin%20%28H%26E%29%20stained%20histology%0Aimages.%20Spatial%20transcriptomics%20unlocks%20the%20unprecedented%20opportunity%20to%20dive%0Ainto%20existing%20histology%20images%20at%20a%20more%20granular%2C%20cellular%20level.%20In%20this%0Awork%2C%20we%20propose%20a%20lightweight%20and%20training-efficient%20approach%20to%20predict%0Acellular%20composition%20directly%20from%20H%26E-stained%20histology%20images%20by%20leveraging%0Ainformation-enriched%20feature%20embeddings%20extracted%20from%20pre-trained%20pathology%0Afoundation%20models.%20By%20training%20a%20lightweight%20multi-layer%20perceptron%20%28MLP%29%0Aregressor%20on%20cell-type%20abundances%20derived%20via%20cell2location%2C%20our%20method%0Aefficiently%20distills%20knowledge%20from%20pathology%20foundation%20models%20and%0Ademonstrates%20the%20ability%20to%20accurately%20predict%20cell-type%20compositions%20from%0Ahistology%20images%2C%20without%20physically%20performing%20the%20costly%20spatial%0Atranscriptomics.%20Our%20method%20demonstrates%20competitive%20performance%20compared%20to%0Aexisting%20methods%20such%20as%20Hist2Cell%2C%20while%20significantly%20reducing%20computational%0Acomplexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Pathology%2520Foundation%2520Models%2520and%2520Spatial%2520Transcriptomics%2520for%250A%2520%2520Cellular%2520Decomposition%2520from%2520Histology%2520Images%26entry.906535625%3DYutong%2520Sun%2520and%2520Sichen%2520Zhu%2520and%2520Peng%2520Qiu%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520digital%2520pathology%2520and%2520modern%2520deep%2520learning%2520has%250Afacilitated%2520the%2520emergence%2520of%2520pathology%2520foundation%2520models%2520that%2520are%2520expected%2520to%250Asolve%2520general%2520pathology%2520problems%2520under%2520various%2520disease%2520conditions%2520in%2520one%250Aunified%2520model%252C%2520with%2520or%2520without%2520fine-tuning.%2520In%2520parallel%252C%2520spatial%250Atranscriptomics%2520has%2520emerged%2520as%2520a%2520transformative%2520technology%2520that%2520enables%2520the%250Aprofiling%2520of%2520gene%2520expression%2520on%2520hematoxylin%2520and%2520eosin%2520%2528H%2526E%2529%2520stained%2520histology%250Aimages.%2520Spatial%2520transcriptomics%2520unlocks%2520the%2520unprecedented%2520opportunity%2520to%2520dive%250Ainto%2520existing%2520histology%2520images%2520at%2520a%2520more%2520granular%252C%2520cellular%2520level.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520lightweight%2520and%2520training-efficient%2520approach%2520to%2520predict%250Acellular%2520composition%2520directly%2520from%2520H%2526E-stained%2520histology%2520images%2520by%2520leveraging%250Ainformation-enriched%2520feature%2520embeddings%2520extracted%2520from%2520pre-trained%2520pathology%250Afoundation%2520models.%2520By%2520training%2520a%2520lightweight%2520multi-layer%2520perceptron%2520%2528MLP%2529%250Aregressor%2520on%2520cell-type%2520abundances%2520derived%2520via%2520cell2location%252C%2520our%2520method%250Aefficiently%2520distills%2520knowledge%2520from%2520pathology%2520foundation%2520models%2520and%250Ademonstrates%2520the%2520ability%2520to%2520accurately%2520predict%2520cell-type%2520compositions%2520from%250Ahistology%2520images%252C%2520without%2520physically%2520performing%2520the%2520costly%2520spatial%250Atranscriptomics.%2520Our%2520method%2520demonstrates%2520competitive%2520performance%2520compared%2520to%250Aexisting%2520methods%2520such%2520as%2520Hist2Cell%252C%2520while%2520significantly%2520reducing%2520computational%250Acomplexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Pathology%20Foundation%20Models%20and%20Spatial%20Transcriptomics%20for%0A%20%20Cellular%20Decomposition%20from%20Histology%20Images&entry.906535625=Yutong%20Sun%20and%20Sichen%20Zhu%20and%20Peng%20Qiu&entry.1292438233=%20%20The%20rapid%20development%20of%20digital%20pathology%20and%20modern%20deep%20learning%20has%0Afacilitated%20the%20emergence%20of%20pathology%20foundation%20models%20that%20are%20expected%20to%0Asolve%20general%20pathology%20problems%20under%20various%20disease%20conditions%20in%20one%0Aunified%20model%2C%20with%20or%20without%20fine-tuning.%20In%20parallel%2C%20spatial%0Atranscriptomics%20has%20emerged%20as%20a%20transformative%20technology%20that%20enables%20the%0Aprofiling%20of%20gene%20expression%20on%20hematoxylin%20and%20eosin%20%28H%26E%29%20stained%20histology%0Aimages.%20Spatial%20transcriptomics%20unlocks%20the%20unprecedented%20opportunity%20to%20dive%0Ainto%20existing%20histology%20images%20at%20a%20more%20granular%2C%20cellular%20level.%20In%20this%0Awork%2C%20we%20propose%20a%20lightweight%20and%20training-efficient%20approach%20to%20predict%0Acellular%20composition%20directly%20from%20H%26E-stained%20histology%20images%20by%20leveraging%0Ainformation-enriched%20feature%20embeddings%20extracted%20from%20pre-trained%20pathology%0Afoundation%20models.%20By%20training%20a%20lightweight%20multi-layer%20perceptron%20%28MLP%29%0Aregressor%20on%20cell-type%20abundances%20derived%20via%20cell2location%2C%20our%20method%0Aefficiently%20distills%20knowledge%20from%20pathology%20foundation%20models%20and%0Ademonstrates%20the%20ability%20to%20accurately%20predict%20cell-type%20compositions%20from%0Ahistology%20images%2C%20without%20physically%20performing%20the%20costly%20spatial%0Atranscriptomics.%20Our%20method%20demonstrates%20competitive%20performance%20compared%20to%0Aexisting%20methods%20such%20as%20Hist2Cell%2C%20while%20significantly%20reducing%20computational%0Acomplexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07013v1&entry.124074799=Read"},
{"title": "PBCAT: Patch-based composite adversarial training against physically\n  realizable attacks on object detection", "author": "Xiao Li and Yiming Zhu and Yifan Huang and Wei Zhang and Yingzhe He and Jie Shi and Xiaolin Hu", "abstract": "  Object detection plays a crucial role in many security-sensitive\napplications. However, several recent studies have shown that object detectors\ncan be easily fooled by physically realizable attacks, \\eg, adversarial patches\nand recent adversarial textures, which pose realistic and urgent threats.\nAdversarial Training (AT) has been recognized as the most effective defense\nagainst adversarial attacks. While AT has been extensively studied in the\n$l_\\infty$ attack settings on classification models, AT against physically\nrealizable attacks on object detectors has received limited exploration. Early\nattempts are only performed to defend against adversarial patches, leaving AT\nagainst a wider range of physically realizable attacks under-explored. In this\nwork, we consider defending against various physically realizable attacks with\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\ncombination of small-area gradient-guided adversarial patches and imperceptible\nglobal adversarial perturbations covering the entire image. With these designs,\nPBCAT has the potential to defend against not only adversarial patches but also\nunseen physically realizable attacks such as adversarial textures. Extensive\nexperiments in multiple settings demonstrated that PBCAT significantly improved\nrobustness against various physically realizable attacks over state-of-the-art\ndefense methods. Notably, it improved the detection accuracy by 29.7\\% over\nprevious defense methods under one recent adversarial texture attack.\n", "link": "http://arxiv.org/abs/2506.23581v2", "date": "2025-07-09", "relevancy": 2.023, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5164}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5113}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PBCAT%3A%20Patch-based%20composite%20adversarial%20training%20against%20physically%0A%20%20realizable%20attacks%20on%20object%20detection&body=Title%3A%20PBCAT%3A%20Patch-based%20composite%20adversarial%20training%20against%20physically%0A%20%20realizable%20attacks%20on%20object%20detection%0AAuthor%3A%20Xiao%20Li%20and%20Yiming%20Zhu%20and%20Yifan%20Huang%20and%20Wei%20Zhang%20and%20Yingzhe%20He%20and%20Jie%20Shi%20and%20Xiaolin%20Hu%0AAbstract%3A%20%20%20Object%20detection%20plays%20a%20crucial%20role%20in%20many%20security-sensitive%0Aapplications.%20However%2C%20several%20recent%20studies%20have%20shown%20that%20object%20detectors%0Acan%20be%20easily%20fooled%20by%20physically%20realizable%20attacks%2C%20%5Ceg%2C%20adversarial%20patches%0Aand%20recent%20adversarial%20textures%2C%20which%20pose%20realistic%20and%20urgent%20threats.%0AAdversarial%20Training%20%28AT%29%20has%20been%20recognized%20as%20the%20most%20effective%20defense%0Aagainst%20adversarial%20attacks.%20While%20AT%20has%20been%20extensively%20studied%20in%20the%0A%24l_%5Cinfty%24%20attack%20settings%20on%20classification%20models%2C%20AT%20against%20physically%0Arealizable%20attacks%20on%20object%20detectors%20has%20received%20limited%20exploration.%20Early%0Aattempts%20are%20only%20performed%20to%20defend%20against%20adversarial%20patches%2C%20leaving%20AT%0Aagainst%20a%20wider%20range%20of%20physically%20realizable%20attacks%20under-explored.%20In%20this%0Awork%2C%20we%20consider%20defending%20against%20various%20physically%20realizable%20attacks%20with%0Aa%20unified%20AT%20method.%20We%20propose%20PBCAT%2C%20a%20novel%20Patch-Based%20Composite%0AAdversarial%20Training%20strategy.%20PBCAT%20optimizes%20the%20model%20by%20incorporating%20the%0Acombination%20of%20small-area%20gradient-guided%20adversarial%20patches%20and%20imperceptible%0Aglobal%20adversarial%20perturbations%20covering%20the%20entire%20image.%20With%20these%20designs%2C%0APBCAT%20has%20the%20potential%20to%20defend%20against%20not%20only%20adversarial%20patches%20but%20also%0Aunseen%20physically%20realizable%20attacks%20such%20as%20adversarial%20textures.%20Extensive%0Aexperiments%20in%20multiple%20settings%20demonstrated%20that%20PBCAT%20significantly%20improved%0Arobustness%20against%20various%20physically%20realizable%20attacks%20over%20state-of-the-art%0Adefense%20methods.%20Notably%2C%20it%20improved%20the%20detection%20accuracy%20by%2029.7%5C%25%20over%0Aprevious%20defense%20methods%20under%20one%20recent%20adversarial%20texture%20attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23581v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPBCAT%253A%2520Patch-based%2520composite%2520adversarial%2520training%2520against%2520physically%250A%2520%2520realizable%2520attacks%2520on%2520object%2520detection%26entry.906535625%3DXiao%2520Li%2520and%2520Yiming%2520Zhu%2520and%2520Yifan%2520Huang%2520and%2520Wei%2520Zhang%2520and%2520Yingzhe%2520He%2520and%2520Jie%2520Shi%2520and%2520Xiaolin%2520Hu%26entry.1292438233%3D%2520%2520Object%2520detection%2520plays%2520a%2520crucial%2520role%2520in%2520many%2520security-sensitive%250Aapplications.%2520However%252C%2520several%2520recent%2520studies%2520have%2520shown%2520that%2520object%2520detectors%250Acan%2520be%2520easily%2520fooled%2520by%2520physically%2520realizable%2520attacks%252C%2520%255Ceg%252C%2520adversarial%2520patches%250Aand%2520recent%2520adversarial%2520textures%252C%2520which%2520pose%2520realistic%2520and%2520urgent%2520threats.%250AAdversarial%2520Training%2520%2528AT%2529%2520has%2520been%2520recognized%2520as%2520the%2520most%2520effective%2520defense%250Aagainst%2520adversarial%2520attacks.%2520While%2520AT%2520has%2520been%2520extensively%2520studied%2520in%2520the%250A%2524l_%255Cinfty%2524%2520attack%2520settings%2520on%2520classification%2520models%252C%2520AT%2520against%2520physically%250Arealizable%2520attacks%2520on%2520object%2520detectors%2520has%2520received%2520limited%2520exploration.%2520Early%250Aattempts%2520are%2520only%2520performed%2520to%2520defend%2520against%2520adversarial%2520patches%252C%2520leaving%2520AT%250Aagainst%2520a%2520wider%2520range%2520of%2520physically%2520realizable%2520attacks%2520under-explored.%2520In%2520this%250Awork%252C%2520we%2520consider%2520defending%2520against%2520various%2520physically%2520realizable%2520attacks%2520with%250Aa%2520unified%2520AT%2520method.%2520We%2520propose%2520PBCAT%252C%2520a%2520novel%2520Patch-Based%2520Composite%250AAdversarial%2520Training%2520strategy.%2520PBCAT%2520optimizes%2520the%2520model%2520by%2520incorporating%2520the%250Acombination%2520of%2520small-area%2520gradient-guided%2520adversarial%2520patches%2520and%2520imperceptible%250Aglobal%2520adversarial%2520perturbations%2520covering%2520the%2520entire%2520image.%2520With%2520these%2520designs%252C%250APBCAT%2520has%2520the%2520potential%2520to%2520defend%2520against%2520not%2520only%2520adversarial%2520patches%2520but%2520also%250Aunseen%2520physically%2520realizable%2520attacks%2520such%2520as%2520adversarial%2520textures.%2520Extensive%250Aexperiments%2520in%2520multiple%2520settings%2520demonstrated%2520that%2520PBCAT%2520significantly%2520improved%250Arobustness%2520against%2520various%2520physically%2520realizable%2520attacks%2520over%2520state-of-the-art%250Adefense%2520methods.%2520Notably%252C%2520it%2520improved%2520the%2520detection%2520accuracy%2520by%252029.7%255C%2525%2520over%250Aprevious%2520defense%2520methods%2520under%2520one%2520recent%2520adversarial%2520texture%2520attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23581v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PBCAT%3A%20Patch-based%20composite%20adversarial%20training%20against%20physically%0A%20%20realizable%20attacks%20on%20object%20detection&entry.906535625=Xiao%20Li%20and%20Yiming%20Zhu%20and%20Yifan%20Huang%20and%20Wei%20Zhang%20and%20Yingzhe%20He%20and%20Jie%20Shi%20and%20Xiaolin%20Hu&entry.1292438233=%20%20Object%20detection%20plays%20a%20crucial%20role%20in%20many%20security-sensitive%0Aapplications.%20However%2C%20several%20recent%20studies%20have%20shown%20that%20object%20detectors%0Acan%20be%20easily%20fooled%20by%20physically%20realizable%20attacks%2C%20%5Ceg%2C%20adversarial%20patches%0Aand%20recent%20adversarial%20textures%2C%20which%20pose%20realistic%20and%20urgent%20threats.%0AAdversarial%20Training%20%28AT%29%20has%20been%20recognized%20as%20the%20most%20effective%20defense%0Aagainst%20adversarial%20attacks.%20While%20AT%20has%20been%20extensively%20studied%20in%20the%0A%24l_%5Cinfty%24%20attack%20settings%20on%20classification%20models%2C%20AT%20against%20physically%0Arealizable%20attacks%20on%20object%20detectors%20has%20received%20limited%20exploration.%20Early%0Aattempts%20are%20only%20performed%20to%20defend%20against%20adversarial%20patches%2C%20leaving%20AT%0Aagainst%20a%20wider%20range%20of%20physically%20realizable%20attacks%20under-explored.%20In%20this%0Awork%2C%20we%20consider%20defending%20against%20various%20physically%20realizable%20attacks%20with%0Aa%20unified%20AT%20method.%20We%20propose%20PBCAT%2C%20a%20novel%20Patch-Based%20Composite%0AAdversarial%20Training%20strategy.%20PBCAT%20optimizes%20the%20model%20by%20incorporating%20the%0Acombination%20of%20small-area%20gradient-guided%20adversarial%20patches%20and%20imperceptible%0Aglobal%20adversarial%20perturbations%20covering%20the%20entire%20image.%20With%20these%20designs%2C%0APBCAT%20has%20the%20potential%20to%20defend%20against%20not%20only%20adversarial%20patches%20but%20also%0Aunseen%20physically%20realizable%20attacks%20such%20as%20adversarial%20textures.%20Extensive%0Aexperiments%20in%20multiple%20settings%20demonstrated%20that%20PBCAT%20significantly%20improved%0Arobustness%20against%20various%20physically%20realizable%20attacks%20over%20state-of-the-art%0Adefense%20methods.%20Notably%2C%20it%20improved%20the%20detection%20accuracy%20by%2029.7%5C%25%20over%0Aprevious%20defense%20methods%20under%20one%20recent%20adversarial%20texture%20attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23581v2&entry.124074799=Read"},
{"title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection", "author": "Ziyan Liu and Chunxiao Fan and Haoran Lou and Yuexin Wu and Kaiwei Deng", "abstract": "  The rapid expansion of memes on social media has highlighted the urgent need\nfor effective approaches to detect harmful content. However, traditional\ndata-driven approaches struggle to detect new memes due to their evolving\nnature and the lack of up-to-date annotated data. To address this issue, we\npropose MIND, a multi-agent framework for zero-shot harmful meme detection that\ndoes not rely on annotated data. MIND implements three key strategies: 1) We\nretrieve similar memes from an unannotated reference set to provide contextual\ninformation. 2) We propose a bi-directional insight derivation mechanism to\nextract a comprehensive understanding of similar memes. 3) We then employ a\nmulti-agent debate mechanism to ensure robust decision-making through reasoned\narbitration. Extensive experiments on three meme datasets demonstrate that our\nproposed framework not only outperforms existing zero-shot approaches but also\nshows strong generalization across different model architectures and parameter\nscales, providing a scalable solution for harmful meme detection. The code is\navailable at https://github.com/destroy-lonely/MIND.\n", "link": "http://arxiv.org/abs/2507.06908v1", "date": "2025-07-09", "relevancy": 2.0123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5125}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4964}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIND%3A%20A%20Multi-agent%20Framework%20for%20Zero-shot%20Harmful%20Meme%20Detection&body=Title%3A%20MIND%3A%20A%20Multi-agent%20Framework%20for%20Zero-shot%20Harmful%20Meme%20Detection%0AAuthor%3A%20Ziyan%20Liu%20and%20Chunxiao%20Fan%20and%20Haoran%20Lou%20and%20Yuexin%20Wu%20and%20Kaiwei%20Deng%0AAbstract%3A%20%20%20The%20rapid%20expansion%20of%20memes%20on%20social%20media%20has%20highlighted%20the%20urgent%20need%0Afor%20effective%20approaches%20to%20detect%20harmful%20content.%20However%2C%20traditional%0Adata-driven%20approaches%20struggle%20to%20detect%20new%20memes%20due%20to%20their%20evolving%0Anature%20and%20the%20lack%20of%20up-to-date%20annotated%20data.%20To%20address%20this%20issue%2C%20we%0Apropose%20MIND%2C%20a%20multi-agent%20framework%20for%20zero-shot%20harmful%20meme%20detection%20that%0Adoes%20not%20rely%20on%20annotated%20data.%20MIND%20implements%20three%20key%20strategies%3A%201%29%20We%0Aretrieve%20similar%20memes%20from%20an%20unannotated%20reference%20set%20to%20provide%20contextual%0Ainformation.%202%29%20We%20propose%20a%20bi-directional%20insight%20derivation%20mechanism%20to%0Aextract%20a%20comprehensive%20understanding%20of%20similar%20memes.%203%29%20We%20then%20employ%20a%0Amulti-agent%20debate%20mechanism%20to%20ensure%20robust%20decision-making%20through%20reasoned%0Aarbitration.%20Extensive%20experiments%20on%20three%20meme%20datasets%20demonstrate%20that%20our%0Aproposed%20framework%20not%20only%20outperforms%20existing%20zero-shot%20approaches%20but%20also%0Ashows%20strong%20generalization%20across%20different%20model%20architectures%20and%20parameter%0Ascales%2C%20providing%20a%20scalable%20solution%20for%20harmful%20meme%20detection.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/destroy-lonely/MIND.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIND%253A%2520A%2520Multi-agent%2520Framework%2520for%2520Zero-shot%2520Harmful%2520Meme%2520Detection%26entry.906535625%3DZiyan%2520Liu%2520and%2520Chunxiao%2520Fan%2520and%2520Haoran%2520Lou%2520and%2520Yuexin%2520Wu%2520and%2520Kaiwei%2520Deng%26entry.1292438233%3D%2520%2520The%2520rapid%2520expansion%2520of%2520memes%2520on%2520social%2520media%2520has%2520highlighted%2520the%2520urgent%2520need%250Afor%2520effective%2520approaches%2520to%2520detect%2520harmful%2520content.%2520However%252C%2520traditional%250Adata-driven%2520approaches%2520struggle%2520to%2520detect%2520new%2520memes%2520due%2520to%2520their%2520evolving%250Anature%2520and%2520the%2520lack%2520of%2520up-to-date%2520annotated%2520data.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520MIND%252C%2520a%2520multi-agent%2520framework%2520for%2520zero-shot%2520harmful%2520meme%2520detection%2520that%250Adoes%2520not%2520rely%2520on%2520annotated%2520data.%2520MIND%2520implements%2520three%2520key%2520strategies%253A%25201%2529%2520We%250Aretrieve%2520similar%2520memes%2520from%2520an%2520unannotated%2520reference%2520set%2520to%2520provide%2520contextual%250Ainformation.%25202%2529%2520We%2520propose%2520a%2520bi-directional%2520insight%2520derivation%2520mechanism%2520to%250Aextract%2520a%2520comprehensive%2520understanding%2520of%2520similar%2520memes.%25203%2529%2520We%2520then%2520employ%2520a%250Amulti-agent%2520debate%2520mechanism%2520to%2520ensure%2520robust%2520decision-making%2520through%2520reasoned%250Aarbitration.%2520Extensive%2520experiments%2520on%2520three%2520meme%2520datasets%2520demonstrate%2520that%2520our%250Aproposed%2520framework%2520not%2520only%2520outperforms%2520existing%2520zero-shot%2520approaches%2520but%2520also%250Ashows%2520strong%2520generalization%2520across%2520different%2520model%2520architectures%2520and%2520parameter%250Ascales%252C%2520providing%2520a%2520scalable%2520solution%2520for%2520harmful%2520meme%2520detection.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/destroy-lonely/MIND.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIND%3A%20A%20Multi-agent%20Framework%20for%20Zero-shot%20Harmful%20Meme%20Detection&entry.906535625=Ziyan%20Liu%20and%20Chunxiao%20Fan%20and%20Haoran%20Lou%20and%20Yuexin%20Wu%20and%20Kaiwei%20Deng&entry.1292438233=%20%20The%20rapid%20expansion%20of%20memes%20on%20social%20media%20has%20highlighted%20the%20urgent%20need%0Afor%20effective%20approaches%20to%20detect%20harmful%20content.%20However%2C%20traditional%0Adata-driven%20approaches%20struggle%20to%20detect%20new%20memes%20due%20to%20their%20evolving%0Anature%20and%20the%20lack%20of%20up-to-date%20annotated%20data.%20To%20address%20this%20issue%2C%20we%0Apropose%20MIND%2C%20a%20multi-agent%20framework%20for%20zero-shot%20harmful%20meme%20detection%20that%0Adoes%20not%20rely%20on%20annotated%20data.%20MIND%20implements%20three%20key%20strategies%3A%201%29%20We%0Aretrieve%20similar%20memes%20from%20an%20unannotated%20reference%20set%20to%20provide%20contextual%0Ainformation.%202%29%20We%20propose%20a%20bi-directional%20insight%20derivation%20mechanism%20to%0Aextract%20a%20comprehensive%20understanding%20of%20similar%20memes.%203%29%20We%20then%20employ%20a%0Amulti-agent%20debate%20mechanism%20to%20ensure%20robust%20decision-making%20through%20reasoned%0Aarbitration.%20Extensive%20experiments%20on%20three%20meme%20datasets%20demonstrate%20that%20our%0Aproposed%20framework%20not%20only%20outperforms%20existing%20zero-shot%20approaches%20but%20also%0Ashows%20strong%20generalization%20across%20different%20model%20architectures%20and%20parameter%0Ascales%2C%20providing%20a%20scalable%20solution%20for%20harmful%20meme%20detection.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/destroy-lonely/MIND.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06908v1&entry.124074799=Read"},
{"title": "IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware\n  Localization and Perturbation Optimization", "author": "Subrat Kishore Dutta and Xiao Zhang", "abstract": "  Despite modifying only a small localized input region, adversarial patches\ncan drastically change the prediction of computer vision models. However, prior\nmethods either cannot perform satisfactorily under targeted attack scenarios or\nfail to produce contextually coherent adversarial patches, causing them to be\neasily noticeable by human examiners and insufficiently stealthy against\nautomatic patch defenses. In this paper, we introduce IAP, a novel attack\nframework that generates highly invisible adversarial patches based on\nperceptibility-aware localization and perturbation optimization schemes.\nSpecifically, IAP first searches for a proper location to place the patch by\nleveraging classwise localization and sensitivity maps, balancing the\nsusceptibility of patch location to both victim model prediction and human\nvisual system, then employs a perceptibility-regularized adversarial loss and a\ngradient update rule that prioritizes color constancy for optimizing invisible\nperturbations. Comprehensive experiments across various image benchmarks and\nmodel architectures demonstrate that IAP consistently achieves competitive\nattack success rates in targeted settings with significantly improved patch\ninvisibility compared to existing baselines. In addition to being highly\nimperceptible to humans, IAP is shown to be stealthy enough to render several\nstate-of-the-art patch defenses ineffective.\n", "link": "http://arxiv.org/abs/2507.06856v1", "date": "2025-07-09", "relevancy": 2.0091, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5425}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4737}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IAP%3A%20Invisible%20Adversarial%20Patch%20Attack%20through%20Perceptibility-Aware%0A%20%20Localization%20and%20Perturbation%20Optimization&body=Title%3A%20IAP%3A%20Invisible%20Adversarial%20Patch%20Attack%20through%20Perceptibility-Aware%0A%20%20Localization%20and%20Perturbation%20Optimization%0AAuthor%3A%20Subrat%20Kishore%20Dutta%20and%20Xiao%20Zhang%0AAbstract%3A%20%20%20Despite%20modifying%20only%20a%20small%20localized%20input%20region%2C%20adversarial%20patches%0Acan%20drastically%20change%20the%20prediction%20of%20computer%20vision%20models.%20However%2C%20prior%0Amethods%20either%20cannot%20perform%20satisfactorily%20under%20targeted%20attack%20scenarios%20or%0Afail%20to%20produce%20contextually%20coherent%20adversarial%20patches%2C%20causing%20them%20to%20be%0Aeasily%20noticeable%20by%20human%20examiners%20and%20insufficiently%20stealthy%20against%0Aautomatic%20patch%20defenses.%20In%20this%20paper%2C%20we%20introduce%20IAP%2C%20a%20novel%20attack%0Aframework%20that%20generates%20highly%20invisible%20adversarial%20patches%20based%20on%0Aperceptibility-aware%20localization%20and%20perturbation%20optimization%20schemes.%0ASpecifically%2C%20IAP%20first%20searches%20for%20a%20proper%20location%20to%20place%20the%20patch%20by%0Aleveraging%20classwise%20localization%20and%20sensitivity%20maps%2C%20balancing%20the%0Asusceptibility%20of%20patch%20location%20to%20both%20victim%20model%20prediction%20and%20human%0Avisual%20system%2C%20then%20employs%20a%20perceptibility-regularized%20adversarial%20loss%20and%20a%0Agradient%20update%20rule%20that%20prioritizes%20color%20constancy%20for%20optimizing%20invisible%0Aperturbations.%20Comprehensive%20experiments%20across%20various%20image%20benchmarks%20and%0Amodel%20architectures%20demonstrate%20that%20IAP%20consistently%20achieves%20competitive%0Aattack%20success%20rates%20in%20targeted%20settings%20with%20significantly%20improved%20patch%0Ainvisibility%20compared%20to%20existing%20baselines.%20In%20addition%20to%20being%20highly%0Aimperceptible%20to%20humans%2C%20IAP%20is%20shown%20to%20be%20stealthy%20enough%20to%20render%20several%0Astate-of-the-art%20patch%20defenses%20ineffective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIAP%253A%2520Invisible%2520Adversarial%2520Patch%2520Attack%2520through%2520Perceptibility-Aware%250A%2520%2520Localization%2520and%2520Perturbation%2520Optimization%26entry.906535625%3DSubrat%2520Kishore%2520Dutta%2520and%2520Xiao%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520modifying%2520only%2520a%2520small%2520localized%2520input%2520region%252C%2520adversarial%2520patches%250Acan%2520drastically%2520change%2520the%2520prediction%2520of%2520computer%2520vision%2520models.%2520However%252C%2520prior%250Amethods%2520either%2520cannot%2520perform%2520satisfactorily%2520under%2520targeted%2520attack%2520scenarios%2520or%250Afail%2520to%2520produce%2520contextually%2520coherent%2520adversarial%2520patches%252C%2520causing%2520them%2520to%2520be%250Aeasily%2520noticeable%2520by%2520human%2520examiners%2520and%2520insufficiently%2520stealthy%2520against%250Aautomatic%2520patch%2520defenses.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520IAP%252C%2520a%2520novel%2520attack%250Aframework%2520that%2520generates%2520highly%2520invisible%2520adversarial%2520patches%2520based%2520on%250Aperceptibility-aware%2520localization%2520and%2520perturbation%2520optimization%2520schemes.%250ASpecifically%252C%2520IAP%2520first%2520searches%2520for%2520a%2520proper%2520location%2520to%2520place%2520the%2520patch%2520by%250Aleveraging%2520classwise%2520localization%2520and%2520sensitivity%2520maps%252C%2520balancing%2520the%250Asusceptibility%2520of%2520patch%2520location%2520to%2520both%2520victim%2520model%2520prediction%2520and%2520human%250Avisual%2520system%252C%2520then%2520employs%2520a%2520perceptibility-regularized%2520adversarial%2520loss%2520and%2520a%250Agradient%2520update%2520rule%2520that%2520prioritizes%2520color%2520constancy%2520for%2520optimizing%2520invisible%250Aperturbations.%2520Comprehensive%2520experiments%2520across%2520various%2520image%2520benchmarks%2520and%250Amodel%2520architectures%2520demonstrate%2520that%2520IAP%2520consistently%2520achieves%2520competitive%250Aattack%2520success%2520rates%2520in%2520targeted%2520settings%2520with%2520significantly%2520improved%2520patch%250Ainvisibility%2520compared%2520to%2520existing%2520baselines.%2520In%2520addition%2520to%2520being%2520highly%250Aimperceptible%2520to%2520humans%252C%2520IAP%2520is%2520shown%2520to%2520be%2520stealthy%2520enough%2520to%2520render%2520several%250Astate-of-the-art%2520patch%2520defenses%2520ineffective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IAP%3A%20Invisible%20Adversarial%20Patch%20Attack%20through%20Perceptibility-Aware%0A%20%20Localization%20and%20Perturbation%20Optimization&entry.906535625=Subrat%20Kishore%20Dutta%20and%20Xiao%20Zhang&entry.1292438233=%20%20Despite%20modifying%20only%20a%20small%20localized%20input%20region%2C%20adversarial%20patches%0Acan%20drastically%20change%20the%20prediction%20of%20computer%20vision%20models.%20However%2C%20prior%0Amethods%20either%20cannot%20perform%20satisfactorily%20under%20targeted%20attack%20scenarios%20or%0Afail%20to%20produce%20contextually%20coherent%20adversarial%20patches%2C%20causing%20them%20to%20be%0Aeasily%20noticeable%20by%20human%20examiners%20and%20insufficiently%20stealthy%20against%0Aautomatic%20patch%20defenses.%20In%20this%20paper%2C%20we%20introduce%20IAP%2C%20a%20novel%20attack%0Aframework%20that%20generates%20highly%20invisible%20adversarial%20patches%20based%20on%0Aperceptibility-aware%20localization%20and%20perturbation%20optimization%20schemes.%0ASpecifically%2C%20IAP%20first%20searches%20for%20a%20proper%20location%20to%20place%20the%20patch%20by%0Aleveraging%20classwise%20localization%20and%20sensitivity%20maps%2C%20balancing%20the%0Asusceptibility%20of%20patch%20location%20to%20both%20victim%20model%20prediction%20and%20human%0Avisual%20system%2C%20then%20employs%20a%20perceptibility-regularized%20adversarial%20loss%20and%20a%0Agradient%20update%20rule%20that%20prioritizes%20color%20constancy%20for%20optimizing%20invisible%0Aperturbations.%20Comprehensive%20experiments%20across%20various%20image%20benchmarks%20and%0Amodel%20architectures%20demonstrate%20that%20IAP%20consistently%20achieves%20competitive%0Aattack%20success%20rates%20in%20targeted%20settings%20with%20significantly%20improved%20patch%0Ainvisibility%20compared%20to%20existing%20baselines.%20In%20addition%20to%20being%20highly%0Aimperceptible%20to%20humans%2C%20IAP%20is%20shown%20to%20be%20stealthy%20enough%20to%20render%20several%0Astate-of-the-art%20patch%20defenses%20ineffective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06856v1&entry.124074799=Read"},
{"title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning", "author": "Lingxiao Kong and Cong Yang and Susanne Neufang and Oya Deniz Beyan and Zeyd Boukhers", "abstract": "  Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including competing objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the fine-tuning to improve efficiency and\nflexibility. Our method is the first to aggregate the hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text classification models to score the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.\n", "link": "http://arxiv.org/abs/2505.02579v3", "date": "2025-07-09", "relevancy": 2.009, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5418}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMORL%3A%20Ensemble%20Multi-Objective%20Reinforcement%20Learning%20for%20Efficient%20and%0A%20%20Flexible%20LLM%20Fine-Tuning&body=Title%3A%20EMORL%3A%20Ensemble%20Multi-Objective%20Reinforcement%20Learning%20for%20Efficient%20and%0A%20%20Flexible%20LLM%20Fine-Tuning%0AAuthor%3A%20Lingxiao%20Kong%20and%20Cong%20Yang%20and%20Susanne%20Neufang%20and%20Oya%20Deniz%20Beyan%20and%20Zeyd%20Boukhers%0AAbstract%3A%20%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20for%20large%20language%20model%20%28LLM%29%0Afine-tuning%20show%20promise%20in%20addressing%20multi-objective%20tasks%20but%20still%20face%0Asignificant%20challenges%2C%20including%20competing%20objective%20balancing%2C%20low%20training%0Aefficiency%2C%20poor%20scalability%2C%20and%20limited%20explainability.%20Leveraging%20ensemble%0Alearning%20principles%2C%20we%20introduce%20an%20Ensemble%20Multi-Objective%20RL%20%28EMORL%29%0Aframework%20that%20fine-tunes%20multiple%20models%20with%20individual%20objectives%20while%0Aoptimizing%20their%20aggregation%20after%20the%20fine-tuning%20to%20improve%20efficiency%20and%0Aflexibility.%20Our%20method%20is%20the%20first%20to%20aggregate%20the%20hidden%20states%20of%0Aindividual%20models%2C%20incorporating%20contextual%20information%20from%20multiple%0Aobjectives.%20This%20approach%20is%20supported%20by%20a%20hierarchical%20grid%20search%20algorithm%0Athat%20identifies%20optimal%20weighted%20combinations.%20We%20evaluate%20EMORL%20on%20counselor%0Areflection%20generation%20tasks%2C%20using%20text%20classification%20models%20to%20score%20the%0Agenerations%20and%20provide%20rewards%20during%20RL%20fine-tuning.%20Through%20comprehensive%0Aexperiments%20on%20the%20PAIR%20and%20Psych8k%20datasets%2C%20we%20demonstrate%20the%20advantages%20of%0AEMORL%20against%20existing%20baselines%3A%20significantly%20lower%20and%20more%20stable%20training%0Aconsumption%20%28%2417%2C529%5Cpm%201%2C650%24%20data%20points%20and%20%246%2C573%5Cpm%20147.43%24%20seconds%29%2C%0Aimproved%20scalability%20and%20explainability%2C%20and%20comparable%20performance%20across%0Amultiple%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02579v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMORL%253A%2520Ensemble%2520Multi-Objective%2520Reinforcement%2520Learning%2520for%2520Efficient%2520and%250A%2520%2520Flexible%2520LLM%2520Fine-Tuning%26entry.906535625%3DLingxiao%2520Kong%2520and%2520Cong%2520Yang%2520and%2520Susanne%2520Neufang%2520and%2520Oya%2520Deniz%2520Beyan%2520and%2520Zeyd%2520Boukhers%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520large%2520language%2520model%2520%2528LLM%2529%250Afine-tuning%2520show%2520promise%2520in%2520addressing%2520multi-objective%2520tasks%2520but%2520still%2520face%250Asignificant%2520challenges%252C%2520including%2520competing%2520objective%2520balancing%252C%2520low%2520training%250Aefficiency%252C%2520poor%2520scalability%252C%2520and%2520limited%2520explainability.%2520Leveraging%2520ensemble%250Alearning%2520principles%252C%2520we%2520introduce%2520an%2520Ensemble%2520Multi-Objective%2520RL%2520%2528EMORL%2529%250Aframework%2520that%2520fine-tunes%2520multiple%2520models%2520with%2520individual%2520objectives%2520while%250Aoptimizing%2520their%2520aggregation%2520after%2520the%2520fine-tuning%2520to%2520improve%2520efficiency%2520and%250Aflexibility.%2520Our%2520method%2520is%2520the%2520first%2520to%2520aggregate%2520the%2520hidden%2520states%2520of%250Aindividual%2520models%252C%2520incorporating%2520contextual%2520information%2520from%2520multiple%250Aobjectives.%2520This%2520approach%2520is%2520supported%2520by%2520a%2520hierarchical%2520grid%2520search%2520algorithm%250Athat%2520identifies%2520optimal%2520weighted%2520combinations.%2520We%2520evaluate%2520EMORL%2520on%2520counselor%250Areflection%2520generation%2520tasks%252C%2520using%2520text%2520classification%2520models%2520to%2520score%2520the%250Agenerations%2520and%2520provide%2520rewards%2520during%2520RL%2520fine-tuning.%2520Through%2520comprehensive%250Aexperiments%2520on%2520the%2520PAIR%2520and%2520Psych8k%2520datasets%252C%2520we%2520demonstrate%2520the%2520advantages%2520of%250AEMORL%2520against%2520existing%2520baselines%253A%2520significantly%2520lower%2520and%2520more%2520stable%2520training%250Aconsumption%2520%2528%252417%252C529%255Cpm%25201%252C650%2524%2520data%2520points%2520and%2520%25246%252C573%255Cpm%2520147.43%2524%2520seconds%2529%252C%250Aimproved%2520scalability%2520and%2520explainability%252C%2520and%2520comparable%2520performance%2520across%250Amultiple%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02579v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMORL%3A%20Ensemble%20Multi-Objective%20Reinforcement%20Learning%20for%20Efficient%20and%0A%20%20Flexible%20LLM%20Fine-Tuning&entry.906535625=Lingxiao%20Kong%20and%20Cong%20Yang%20and%20Susanne%20Neufang%20and%20Oya%20Deniz%20Beyan%20and%20Zeyd%20Boukhers&entry.1292438233=%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20for%20large%20language%20model%20%28LLM%29%0Afine-tuning%20show%20promise%20in%20addressing%20multi-objective%20tasks%20but%20still%20face%0Asignificant%20challenges%2C%20including%20competing%20objective%20balancing%2C%20low%20training%0Aefficiency%2C%20poor%20scalability%2C%20and%20limited%20explainability.%20Leveraging%20ensemble%0Alearning%20principles%2C%20we%20introduce%20an%20Ensemble%20Multi-Objective%20RL%20%28EMORL%29%0Aframework%20that%20fine-tunes%20multiple%20models%20with%20individual%20objectives%20while%0Aoptimizing%20their%20aggregation%20after%20the%20fine-tuning%20to%20improve%20efficiency%20and%0Aflexibility.%20Our%20method%20is%20the%20first%20to%20aggregate%20the%20hidden%20states%20of%0Aindividual%20models%2C%20incorporating%20contextual%20information%20from%20multiple%0Aobjectives.%20This%20approach%20is%20supported%20by%20a%20hierarchical%20grid%20search%20algorithm%0Athat%20identifies%20optimal%20weighted%20combinations.%20We%20evaluate%20EMORL%20on%20counselor%0Areflection%20generation%20tasks%2C%20using%20text%20classification%20models%20to%20score%20the%0Agenerations%20and%20provide%20rewards%20during%20RL%20fine-tuning.%20Through%20comprehensive%0Aexperiments%20on%20the%20PAIR%20and%20Psych8k%20datasets%2C%20we%20demonstrate%20the%20advantages%20of%0AEMORL%20against%20existing%20baselines%3A%20significantly%20lower%20and%20more%20stable%20training%0Aconsumption%20%28%2417%2C529%5Cpm%201%2C650%24%20data%20points%20and%20%246%2C573%5Cpm%20147.43%24%20seconds%29%2C%0Aimproved%20scalability%20and%20explainability%2C%20and%20comparable%20performance%20across%0Amultiple%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02579v3&entry.124074799=Read"},
{"title": "OpenDPDv2: A Unified Learning and Optimization Framework for Neural\n  Network Digital Predistortion", "author": "Yizhuo Wu and Ang Li and Chang Gao", "abstract": "  Neural network (NN)-based Digital Predistortion (DPD) stands out in improving\nsignal quality in wideband radio frequency (RF) power amplifiers (PAs)\nemploying complex modulation. However, NN DPDs usually rely on a large number\nof parameters for effective linearization and can significantly contribute to\nthe energy consumption of the digital back-end in RF systems. This paper\npresents OpenDPDv2, a unified framework for PA modeling, DPD learning, and\nmodel optimization to reduce power consumption while maintaining high\nlinearization performance. The optimization techniques feature a novel DPD\nalgorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The\ntop-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an\nAdjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude\n(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal\nsparsity of input signals and hidden neurons, the inference energy of our model\ncan be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM\nwith 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth\n256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,\ndatasets, and documentation are publicly accessible at:\nhttps://github.com/lab-emi/OpenDPD.\n", "link": "http://arxiv.org/abs/2507.06849v1", "date": "2025-07-09", "relevancy": 2.0088, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5089}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5026}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenDPDv2%3A%20A%20Unified%20Learning%20and%20Optimization%20Framework%20for%20Neural%0A%20%20Network%20Digital%20Predistortion&body=Title%3A%20OpenDPDv2%3A%20A%20Unified%20Learning%20and%20Optimization%20Framework%20for%20Neural%0A%20%20Network%20Digital%20Predistortion%0AAuthor%3A%20Yizhuo%20Wu%20and%20Ang%20Li%20and%20Chang%20Gao%0AAbstract%3A%20%20%20Neural%20network%20%28NN%29-based%20Digital%20Predistortion%20%28DPD%29%20stands%20out%20in%20improving%0Asignal%20quality%20in%20wideband%20radio%20frequency%20%28RF%29%20power%20amplifiers%20%28PAs%29%0Aemploying%20complex%20modulation.%20However%2C%20NN%20DPDs%20usually%20rely%20on%20a%20large%20number%0Aof%20parameters%20for%20effective%20linearization%20and%20can%20significantly%20contribute%20to%0Athe%20energy%20consumption%20of%20the%20digital%20back-end%20in%20RF%20systems.%20This%20paper%0Apresents%20OpenDPDv2%2C%20a%20unified%20framework%20for%20PA%20modeling%2C%20DPD%20learning%2C%20and%0Amodel%20optimization%20to%20reduce%20power%20consumption%20while%20maintaining%20high%0Alinearization%20performance.%20The%20optimization%20techniques%20feature%20a%20novel%20DPD%0Aalgorithm%2C%20TRes-DeltaGRU%2C%20alongside%20two%20energy-efficient%20methods.%20The%0Atop-performing%2032-bit%20floating-point%20%28FP32%29%20TRes-DeltaGRU-DPD%20model%20achieves%20an%0AAdjacent%20Channel%20Power%20Ratio%20%28ACPR%29%20of%20-59.4%20dBc%20and%20Error%20Vector%20Magnitude%0A%28EVM%29%20of%20-42.1%20dBc.%20By%20exploiting%20fixed-point%20quantization%20and%20dynamic%20temporal%0Asparsity%20of%20input%20signals%20and%20hidden%20neurons%2C%20the%20inference%20energy%20of%20our%20model%0Acan%20be%20reduced%20by%204.5X%20while%20still%20maintaining%20-50.3%20dBc%20ACPR%20and%20-35.2%20dB%20EVM%0Awith%2056%25%20temporal%20sparsity.%20This%20was%20evaluated%20using%20a%20TM3.1a%20200%20MHz%20bandwidth%0A256-QAM%20OFDM%20signal%20applied%20to%20a%203.5%20GHz%20GaN%20Doherty%20RF%20PA.%20OpenDPDv2%20code%2C%0Adatasets%2C%20and%20documentation%20are%20publicly%20accessible%20at%3A%0Ahttps%3A//github.com/lab-emi/OpenDPD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenDPDv2%253A%2520A%2520Unified%2520Learning%2520and%2520Optimization%2520Framework%2520for%2520Neural%250A%2520%2520Network%2520Digital%2520Predistortion%26entry.906535625%3DYizhuo%2520Wu%2520and%2520Ang%2520Li%2520and%2520Chang%2520Gao%26entry.1292438233%3D%2520%2520Neural%2520network%2520%2528NN%2529-based%2520Digital%2520Predistortion%2520%2528DPD%2529%2520stands%2520out%2520in%2520improving%250Asignal%2520quality%2520in%2520wideband%2520radio%2520frequency%2520%2528RF%2529%2520power%2520amplifiers%2520%2528PAs%2529%250Aemploying%2520complex%2520modulation.%2520However%252C%2520NN%2520DPDs%2520usually%2520rely%2520on%2520a%2520large%2520number%250Aof%2520parameters%2520for%2520effective%2520linearization%2520and%2520can%2520significantly%2520contribute%2520to%250Athe%2520energy%2520consumption%2520of%2520the%2520digital%2520back-end%2520in%2520RF%2520systems.%2520This%2520paper%250Apresents%2520OpenDPDv2%252C%2520a%2520unified%2520framework%2520for%2520PA%2520modeling%252C%2520DPD%2520learning%252C%2520and%250Amodel%2520optimization%2520to%2520reduce%2520power%2520consumption%2520while%2520maintaining%2520high%250Alinearization%2520performance.%2520The%2520optimization%2520techniques%2520feature%2520a%2520novel%2520DPD%250Aalgorithm%252C%2520TRes-DeltaGRU%252C%2520alongside%2520two%2520energy-efficient%2520methods.%2520The%250Atop-performing%252032-bit%2520floating-point%2520%2528FP32%2529%2520TRes-DeltaGRU-DPD%2520model%2520achieves%2520an%250AAdjacent%2520Channel%2520Power%2520Ratio%2520%2528ACPR%2529%2520of%2520-59.4%2520dBc%2520and%2520Error%2520Vector%2520Magnitude%250A%2528EVM%2529%2520of%2520-42.1%2520dBc.%2520By%2520exploiting%2520fixed-point%2520quantization%2520and%2520dynamic%2520temporal%250Asparsity%2520of%2520input%2520signals%2520and%2520hidden%2520neurons%252C%2520the%2520inference%2520energy%2520of%2520our%2520model%250Acan%2520be%2520reduced%2520by%25204.5X%2520while%2520still%2520maintaining%2520-50.3%2520dBc%2520ACPR%2520and%2520-35.2%2520dB%2520EVM%250Awith%252056%2525%2520temporal%2520sparsity.%2520This%2520was%2520evaluated%2520using%2520a%2520TM3.1a%2520200%2520MHz%2520bandwidth%250A256-QAM%2520OFDM%2520signal%2520applied%2520to%2520a%25203.5%2520GHz%2520GaN%2520Doherty%2520RF%2520PA.%2520OpenDPDv2%2520code%252C%250Adatasets%252C%2520and%2520documentation%2520are%2520publicly%2520accessible%2520at%253A%250Ahttps%253A//github.com/lab-emi/OpenDPD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenDPDv2%3A%20A%20Unified%20Learning%20and%20Optimization%20Framework%20for%20Neural%0A%20%20Network%20Digital%20Predistortion&entry.906535625=Yizhuo%20Wu%20and%20Ang%20Li%20and%20Chang%20Gao&entry.1292438233=%20%20Neural%20network%20%28NN%29-based%20Digital%20Predistortion%20%28DPD%29%20stands%20out%20in%20improving%0Asignal%20quality%20in%20wideband%20radio%20frequency%20%28RF%29%20power%20amplifiers%20%28PAs%29%0Aemploying%20complex%20modulation.%20However%2C%20NN%20DPDs%20usually%20rely%20on%20a%20large%20number%0Aof%20parameters%20for%20effective%20linearization%20and%20can%20significantly%20contribute%20to%0Athe%20energy%20consumption%20of%20the%20digital%20back-end%20in%20RF%20systems.%20This%20paper%0Apresents%20OpenDPDv2%2C%20a%20unified%20framework%20for%20PA%20modeling%2C%20DPD%20learning%2C%20and%0Amodel%20optimization%20to%20reduce%20power%20consumption%20while%20maintaining%20high%0Alinearization%20performance.%20The%20optimization%20techniques%20feature%20a%20novel%20DPD%0Aalgorithm%2C%20TRes-DeltaGRU%2C%20alongside%20two%20energy-efficient%20methods.%20The%0Atop-performing%2032-bit%20floating-point%20%28FP32%29%20TRes-DeltaGRU-DPD%20model%20achieves%20an%0AAdjacent%20Channel%20Power%20Ratio%20%28ACPR%29%20of%20-59.4%20dBc%20and%20Error%20Vector%20Magnitude%0A%28EVM%29%20of%20-42.1%20dBc.%20By%20exploiting%20fixed-point%20quantization%20and%20dynamic%20temporal%0Asparsity%20of%20input%20signals%20and%20hidden%20neurons%2C%20the%20inference%20energy%20of%20our%20model%0Acan%20be%20reduced%20by%204.5X%20while%20still%20maintaining%20-50.3%20dBc%20ACPR%20and%20-35.2%20dB%20EVM%0Awith%2056%25%20temporal%20sparsity.%20This%20was%20evaluated%20using%20a%20TM3.1a%20200%20MHz%20bandwidth%0A256-QAM%20OFDM%20signal%20applied%20to%20a%203.5%20GHz%20GaN%20Doherty%20RF%20PA.%20OpenDPDv2%20code%2C%0Adatasets%2C%20and%20documentation%20are%20publicly%20accessible%20at%3A%0Ahttps%3A//github.com/lab-emi/OpenDPD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06849v1&entry.124074799=Read"},
{"title": "Designing Robust Software Sensors for Nonlinear Systems via Neural\n  Networks and Adaptive Sliding Mode Control", "author": "Ayoub Farkane and Mohamed Boutayeb and Mustapha Oudani and Mounir Ghogho", "abstract": "  Accurate knowledge of the state variables in a dynamical system is critical\nfor effective control, diagnosis, and supervision, especially when direct\nmeasurements of all states are infeasible. This paper presents a novel approach\nto designing software sensors for nonlinear dynamical systems expressed in\ntheir most general form. Unlike traditional model-based observers that rely on\nexplicit transformations or linearization, the proposed framework integrates\nneural networks with adaptive Sliding Mode Control (SMC) to design a robust\nstate observer under a less restrictive set of conditions. The learning process\nis driven by available sensor measurements, which are used to correct the\nobserver's state estimate. The training methodology leverages the system's\ngoverning equations as a physics-based constraint, enabling observer synthesis\nwithout access to ground-truth state trajectories. By employing a time-varying\ngain matrix dynamically adjusted by the neural network, the observer adapts in\nreal-time to system changes, ensuring robustness against noise, external\ndisturbances, and variations in system dynamics. Furthermore, we provide\nsufficient conditions to guarantee estimation error convergence, establishing a\ntheoretical foundation for the observer's reliability. The methodology's\neffectiveness is validated through simulations on challenging examples,\nincluding systems with non-differentiable dynamics and varying observability\nconditions. These examples, which are often problematic for conventional\ntechniques, serve to demonstrate the robustness and broad applicability of our\napproach. The results show rapid convergence and high accuracy, underscoring\nthe method's potential for addressing complex state estimation challenges in\nreal-world applications.\n", "link": "http://arxiv.org/abs/2507.06817v1", "date": "2025-07-09", "relevancy": 2.0061, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5667}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4972}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20Robust%20Software%20Sensors%20for%20Nonlinear%20Systems%20via%20Neural%0A%20%20Networks%20and%20Adaptive%20Sliding%20Mode%20Control&body=Title%3A%20Designing%20Robust%20Software%20Sensors%20for%20Nonlinear%20Systems%20via%20Neural%0A%20%20Networks%20and%20Adaptive%20Sliding%20Mode%20Control%0AAuthor%3A%20Ayoub%20Farkane%20and%20Mohamed%20Boutayeb%20and%20Mustapha%20Oudani%20and%20Mounir%20Ghogho%0AAbstract%3A%20%20%20Accurate%20knowledge%20of%20the%20state%20variables%20in%20a%20dynamical%20system%20is%20critical%0Afor%20effective%20control%2C%20diagnosis%2C%20and%20supervision%2C%20especially%20when%20direct%0Ameasurements%20of%20all%20states%20are%20infeasible.%20This%20paper%20presents%20a%20novel%20approach%0Ato%20designing%20software%20sensors%20for%20nonlinear%20dynamical%20systems%20expressed%20in%0Atheir%20most%20general%20form.%20Unlike%20traditional%20model-based%20observers%20that%20rely%20on%0Aexplicit%20transformations%20or%20linearization%2C%20the%20proposed%20framework%20integrates%0Aneural%20networks%20with%20adaptive%20Sliding%20Mode%20Control%20%28SMC%29%20to%20design%20a%20robust%0Astate%20observer%20under%20a%20less%20restrictive%20set%20of%20conditions.%20The%20learning%20process%0Ais%20driven%20by%20available%20sensor%20measurements%2C%20which%20are%20used%20to%20correct%20the%0Aobserver%27s%20state%20estimate.%20The%20training%20methodology%20leverages%20the%20system%27s%0Agoverning%20equations%20as%20a%20physics-based%20constraint%2C%20enabling%20observer%20synthesis%0Awithout%20access%20to%20ground-truth%20state%20trajectories.%20By%20employing%20a%20time-varying%0Again%20matrix%20dynamically%20adjusted%20by%20the%20neural%20network%2C%20the%20observer%20adapts%20in%0Areal-time%20to%20system%20changes%2C%20ensuring%20robustness%20against%20noise%2C%20external%0Adisturbances%2C%20and%20variations%20in%20system%20dynamics.%20Furthermore%2C%20we%20provide%0Asufficient%20conditions%20to%20guarantee%20estimation%20error%20convergence%2C%20establishing%20a%0Atheoretical%20foundation%20for%20the%20observer%27s%20reliability.%20The%20methodology%27s%0Aeffectiveness%20is%20validated%20through%20simulations%20on%20challenging%20examples%2C%0Aincluding%20systems%20with%20non-differentiable%20dynamics%20and%20varying%20observability%0Aconditions.%20These%20examples%2C%20which%20are%20often%20problematic%20for%20conventional%0Atechniques%2C%20serve%20to%20demonstrate%20the%20robustness%20and%20broad%20applicability%20of%20our%0Aapproach.%20The%20results%20show%20rapid%20convergence%20and%20high%20accuracy%2C%20underscoring%0Athe%20method%27s%20potential%20for%20addressing%20complex%20state%20estimation%20challenges%20in%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520Robust%2520Software%2520Sensors%2520for%2520Nonlinear%2520Systems%2520via%2520Neural%250A%2520%2520Networks%2520and%2520Adaptive%2520Sliding%2520Mode%2520Control%26entry.906535625%3DAyoub%2520Farkane%2520and%2520Mohamed%2520Boutayeb%2520and%2520Mustapha%2520Oudani%2520and%2520Mounir%2520Ghogho%26entry.1292438233%3D%2520%2520Accurate%2520knowledge%2520of%2520the%2520state%2520variables%2520in%2520a%2520dynamical%2520system%2520is%2520critical%250Afor%2520effective%2520control%252C%2520diagnosis%252C%2520and%2520supervision%252C%2520especially%2520when%2520direct%250Ameasurements%2520of%2520all%2520states%2520are%2520infeasible.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%250Ato%2520designing%2520software%2520sensors%2520for%2520nonlinear%2520dynamical%2520systems%2520expressed%2520in%250Atheir%2520most%2520general%2520form.%2520Unlike%2520traditional%2520model-based%2520observers%2520that%2520rely%2520on%250Aexplicit%2520transformations%2520or%2520linearization%252C%2520the%2520proposed%2520framework%2520integrates%250Aneural%2520networks%2520with%2520adaptive%2520Sliding%2520Mode%2520Control%2520%2528SMC%2529%2520to%2520design%2520a%2520robust%250Astate%2520observer%2520under%2520a%2520less%2520restrictive%2520set%2520of%2520conditions.%2520The%2520learning%2520process%250Ais%2520driven%2520by%2520available%2520sensor%2520measurements%252C%2520which%2520are%2520used%2520to%2520correct%2520the%250Aobserver%2527s%2520state%2520estimate.%2520The%2520training%2520methodology%2520leverages%2520the%2520system%2527s%250Agoverning%2520equations%2520as%2520a%2520physics-based%2520constraint%252C%2520enabling%2520observer%2520synthesis%250Awithout%2520access%2520to%2520ground-truth%2520state%2520trajectories.%2520By%2520employing%2520a%2520time-varying%250Again%2520matrix%2520dynamically%2520adjusted%2520by%2520the%2520neural%2520network%252C%2520the%2520observer%2520adapts%2520in%250Areal-time%2520to%2520system%2520changes%252C%2520ensuring%2520robustness%2520against%2520noise%252C%2520external%250Adisturbances%252C%2520and%2520variations%2520in%2520system%2520dynamics.%2520Furthermore%252C%2520we%2520provide%250Asufficient%2520conditions%2520to%2520guarantee%2520estimation%2520error%2520convergence%252C%2520establishing%2520a%250Atheoretical%2520foundation%2520for%2520the%2520observer%2527s%2520reliability.%2520The%2520methodology%2527s%250Aeffectiveness%2520is%2520validated%2520through%2520simulations%2520on%2520challenging%2520examples%252C%250Aincluding%2520systems%2520with%2520non-differentiable%2520dynamics%2520and%2520varying%2520observability%250Aconditions.%2520These%2520examples%252C%2520which%2520are%2520often%2520problematic%2520for%2520conventional%250Atechniques%252C%2520serve%2520to%2520demonstrate%2520the%2520robustness%2520and%2520broad%2520applicability%2520of%2520our%250Aapproach.%2520The%2520results%2520show%2520rapid%2520convergence%2520and%2520high%2520accuracy%252C%2520underscoring%250Athe%2520method%2527s%2520potential%2520for%2520addressing%2520complex%2520state%2520estimation%2520challenges%2520in%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20Robust%20Software%20Sensors%20for%20Nonlinear%20Systems%20via%20Neural%0A%20%20Networks%20and%20Adaptive%20Sliding%20Mode%20Control&entry.906535625=Ayoub%20Farkane%20and%20Mohamed%20Boutayeb%20and%20Mustapha%20Oudani%20and%20Mounir%20Ghogho&entry.1292438233=%20%20Accurate%20knowledge%20of%20the%20state%20variables%20in%20a%20dynamical%20system%20is%20critical%0Afor%20effective%20control%2C%20diagnosis%2C%20and%20supervision%2C%20especially%20when%20direct%0Ameasurements%20of%20all%20states%20are%20infeasible.%20This%20paper%20presents%20a%20novel%20approach%0Ato%20designing%20software%20sensors%20for%20nonlinear%20dynamical%20systems%20expressed%20in%0Atheir%20most%20general%20form.%20Unlike%20traditional%20model-based%20observers%20that%20rely%20on%0Aexplicit%20transformations%20or%20linearization%2C%20the%20proposed%20framework%20integrates%0Aneural%20networks%20with%20adaptive%20Sliding%20Mode%20Control%20%28SMC%29%20to%20design%20a%20robust%0Astate%20observer%20under%20a%20less%20restrictive%20set%20of%20conditions.%20The%20learning%20process%0Ais%20driven%20by%20available%20sensor%20measurements%2C%20which%20are%20used%20to%20correct%20the%0Aobserver%27s%20state%20estimate.%20The%20training%20methodology%20leverages%20the%20system%27s%0Agoverning%20equations%20as%20a%20physics-based%20constraint%2C%20enabling%20observer%20synthesis%0Awithout%20access%20to%20ground-truth%20state%20trajectories.%20By%20employing%20a%20time-varying%0Again%20matrix%20dynamically%20adjusted%20by%20the%20neural%20network%2C%20the%20observer%20adapts%20in%0Areal-time%20to%20system%20changes%2C%20ensuring%20robustness%20against%20noise%2C%20external%0Adisturbances%2C%20and%20variations%20in%20system%20dynamics.%20Furthermore%2C%20we%20provide%0Asufficient%20conditions%20to%20guarantee%20estimation%20error%20convergence%2C%20establishing%20a%0Atheoretical%20foundation%20for%20the%20observer%27s%20reliability.%20The%20methodology%27s%0Aeffectiveness%20is%20validated%20through%20simulations%20on%20challenging%20examples%2C%0Aincluding%20systems%20with%20non-differentiable%20dynamics%20and%20varying%20observability%0Aconditions.%20These%20examples%2C%20which%20are%20often%20problematic%20for%20conventional%0Atechniques%2C%20serve%20to%20demonstrate%20the%20robustness%20and%20broad%20applicability%20of%20our%0Aapproach.%20The%20results%20show%20rapid%20convergence%20and%20high%20accuracy%2C%20underscoring%0Athe%20method%27s%20potential%20for%20addressing%20complex%20state%20estimation%20challenges%20in%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06817v1&entry.124074799=Read"},
{"title": "Segmentation Regularized Training for Multi-Domain Deep Learning\n  Registration applied to MR-Guided Prostate Cancer Radiotherapy", "author": "Sudharsan Madhavan and Chengcheng Gui and Lando Bosma and Josiah Simeth and Jue Jiang and Nicolas Cote and Nima Hassan Rezaeian and Himanshu Nagar and Victoria Brennan and Neelam Tyagi and Harini Veeraraghavan", "abstract": "  Background: Accurate deformable image registration (DIR) is required for\ncontour propagation and dose accumulation in MR-guided adaptive radiotherapy\n(MRgART). This study trained and evaluated a deep learning DIR method for\ndomain invariant MR-MR registration. Methods: A progressively refined\nregistration and segmentation (ProRSeg) method was trained with 262 pairs of 3T\nMR simulation scans from prostate cancer patients using weighted segmentation\nconsistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR\nLinac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour\npropagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose\naccumulation was performed for 42 patients undergoing 5-fraction MRgART.\nResults: ProRSeg demonstrated generalization for bladder with similar Dice\nSimilarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,\nperformance was domain-dependent with higher accuracy on cross-domain MRL\ndataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain\nperformance prompted us to study the feasibility of using it for dose\naccumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95\n>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients\nachieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under\nupper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain\nMR-MR registration performance for prostate cancer patients with preliminary\nfeasibility for evaluating treatment compliance to clinical constraints.\n", "link": "http://arxiv.org/abs/2507.06966v1", "date": "2025-07-09", "relevancy": 1.9838, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4982}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4944}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation%20Regularized%20Training%20for%20Multi-Domain%20Deep%20Learning%0A%20%20Registration%20applied%20to%20MR-Guided%20Prostate%20Cancer%20Radiotherapy&body=Title%3A%20Segmentation%20Regularized%20Training%20for%20Multi-Domain%20Deep%20Learning%0A%20%20Registration%20applied%20to%20MR-Guided%20Prostate%20Cancer%20Radiotherapy%0AAuthor%3A%20Sudharsan%20Madhavan%20and%20Chengcheng%20Gui%20and%20Lando%20Bosma%20and%20Josiah%20Simeth%20and%20Jue%20Jiang%20and%20Nicolas%20Cote%20and%20Nima%20Hassan%20Rezaeian%20and%20Himanshu%20Nagar%20and%20Victoria%20Brennan%20and%20Neelam%20Tyagi%20and%20Harini%20Veeraraghavan%0AAbstract%3A%20%20%20Background%3A%20Accurate%20deformable%20image%20registration%20%28DIR%29%20is%20required%20for%0Acontour%20propagation%20and%20dose%20accumulation%20in%20MR-guided%20adaptive%20radiotherapy%0A%28MRgART%29.%20This%20study%20trained%20and%20evaluated%20a%20deep%20learning%20DIR%20method%20for%0Adomain%20invariant%20MR-MR%20registration.%20Methods%3A%20A%20progressively%20refined%0Aregistration%20and%20segmentation%20%28ProRSeg%29%20method%20was%20trained%20with%20262%20pairs%20of%203T%0AMR%20simulation%20scans%20from%20prostate%20cancer%20patients%20using%20weighted%20segmentation%0Aconsistency%20loss.%20ProRSeg%20was%20tested%20on%20same-%20%2858%20pairs%29%2C%20cross-%20%2872%201.5T%20MR%0ALinac%20pairs%29%2C%20and%20mixed-domain%20%2842%20MRSim-MRL%20pairs%29%20datasets%20for%20contour%0Apropagation%20accuracy%20of%20clinical%20target%20volume%20%28CTV%29%2C%20bladder%2C%20and%20rectum.%20Dose%0Aaccumulation%20was%20performed%20for%2042%20patients%20undergoing%205-fraction%20MRgART.%0AResults%3A%20ProRSeg%20demonstrated%20generalization%20for%20bladder%20with%20similar%20Dice%0ASimilarity%20Coefficients%20across%20domains%20%280.88%2C%200.87%2C%200.86%29.%20For%20rectum%20and%20CTV%2C%0Aperformance%20was%20domain-dependent%20with%20higher%20accuracy%20on%20cross-domain%20MRL%0Adataset%20%28DSCs%200.89%29%20versus%20same-domain%20data.%20The%20model%27s%20strong%20cross-domain%0Aperformance%20prompted%20us%20to%20study%20the%20feasibility%20of%20using%20it%20for%20dose%0Aaccumulation.%20Dose%20accumulation%20showed%2083.3%25%20of%20patients%20met%20CTV%20coverage%20%28D95%0A%3E%3D%2040.0%20Gy%29%20and%20bladder%20sparing%20%28D50%20%3C%3D%2020.0%20Gy%29%20constraints.%20All%20patients%0Aachieved%20minimum%20mean%20target%20dose%20%28%3E40.4%20Gy%29%2C%20but%20only%209.5%25%20remained%20under%0Aupper%20limit%20%28%3C42.0%20Gy%29.%20Conclusions%3A%20ProRSeg%20showed%20reasonable%20multi-domain%0AMR-MR%20registration%20performance%20for%20prostate%20cancer%20patients%20with%20preliminary%0Afeasibility%20for%20evaluating%20treatment%20compliance%20to%20clinical%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation%2520Regularized%2520Training%2520for%2520Multi-Domain%2520Deep%2520Learning%250A%2520%2520Registration%2520applied%2520to%2520MR-Guided%2520Prostate%2520Cancer%2520Radiotherapy%26entry.906535625%3DSudharsan%2520Madhavan%2520and%2520Chengcheng%2520Gui%2520and%2520Lando%2520Bosma%2520and%2520Josiah%2520Simeth%2520and%2520Jue%2520Jiang%2520and%2520Nicolas%2520Cote%2520and%2520Nima%2520Hassan%2520Rezaeian%2520and%2520Himanshu%2520Nagar%2520and%2520Victoria%2520Brennan%2520and%2520Neelam%2520Tyagi%2520and%2520Harini%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Background%253A%2520Accurate%2520deformable%2520image%2520registration%2520%2528DIR%2529%2520is%2520required%2520for%250Acontour%2520propagation%2520and%2520dose%2520accumulation%2520in%2520MR-guided%2520adaptive%2520radiotherapy%250A%2528MRgART%2529.%2520This%2520study%2520trained%2520and%2520evaluated%2520a%2520deep%2520learning%2520DIR%2520method%2520for%250Adomain%2520invariant%2520MR-MR%2520registration.%2520Methods%253A%2520A%2520progressively%2520refined%250Aregistration%2520and%2520segmentation%2520%2528ProRSeg%2529%2520method%2520was%2520trained%2520with%2520262%2520pairs%2520of%25203T%250AMR%2520simulation%2520scans%2520from%2520prostate%2520cancer%2520patients%2520using%2520weighted%2520segmentation%250Aconsistency%2520loss.%2520ProRSeg%2520was%2520tested%2520on%2520same-%2520%252858%2520pairs%2529%252C%2520cross-%2520%252872%25201.5T%2520MR%250ALinac%2520pairs%2529%252C%2520and%2520mixed-domain%2520%252842%2520MRSim-MRL%2520pairs%2529%2520datasets%2520for%2520contour%250Apropagation%2520accuracy%2520of%2520clinical%2520target%2520volume%2520%2528CTV%2529%252C%2520bladder%252C%2520and%2520rectum.%2520Dose%250Aaccumulation%2520was%2520performed%2520for%252042%2520patients%2520undergoing%25205-fraction%2520MRgART.%250AResults%253A%2520ProRSeg%2520demonstrated%2520generalization%2520for%2520bladder%2520with%2520similar%2520Dice%250ASimilarity%2520Coefficients%2520across%2520domains%2520%25280.88%252C%25200.87%252C%25200.86%2529.%2520For%2520rectum%2520and%2520CTV%252C%250Aperformance%2520was%2520domain-dependent%2520with%2520higher%2520accuracy%2520on%2520cross-domain%2520MRL%250Adataset%2520%2528DSCs%25200.89%2529%2520versus%2520same-domain%2520data.%2520The%2520model%2527s%2520strong%2520cross-domain%250Aperformance%2520prompted%2520us%2520to%2520study%2520the%2520feasibility%2520of%2520using%2520it%2520for%2520dose%250Aaccumulation.%2520Dose%2520accumulation%2520showed%252083.3%2525%2520of%2520patients%2520met%2520CTV%2520coverage%2520%2528D95%250A%253E%253D%252040.0%2520Gy%2529%2520and%2520bladder%2520sparing%2520%2528D50%2520%253C%253D%252020.0%2520Gy%2529%2520constraints.%2520All%2520patients%250Aachieved%2520minimum%2520mean%2520target%2520dose%2520%2528%253E40.4%2520Gy%2529%252C%2520but%2520only%25209.5%2525%2520remained%2520under%250Aupper%2520limit%2520%2528%253C42.0%2520Gy%2529.%2520Conclusions%253A%2520ProRSeg%2520showed%2520reasonable%2520multi-domain%250AMR-MR%2520registration%2520performance%2520for%2520prostate%2520cancer%2520patients%2520with%2520preliminary%250Afeasibility%2520for%2520evaluating%2520treatment%2520compliance%2520to%2520clinical%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation%20Regularized%20Training%20for%20Multi-Domain%20Deep%20Learning%0A%20%20Registration%20applied%20to%20MR-Guided%20Prostate%20Cancer%20Radiotherapy&entry.906535625=Sudharsan%20Madhavan%20and%20Chengcheng%20Gui%20and%20Lando%20Bosma%20and%20Josiah%20Simeth%20and%20Jue%20Jiang%20and%20Nicolas%20Cote%20and%20Nima%20Hassan%20Rezaeian%20and%20Himanshu%20Nagar%20and%20Victoria%20Brennan%20and%20Neelam%20Tyagi%20and%20Harini%20Veeraraghavan&entry.1292438233=%20%20Background%3A%20Accurate%20deformable%20image%20registration%20%28DIR%29%20is%20required%20for%0Acontour%20propagation%20and%20dose%20accumulation%20in%20MR-guided%20adaptive%20radiotherapy%0A%28MRgART%29.%20This%20study%20trained%20and%20evaluated%20a%20deep%20learning%20DIR%20method%20for%0Adomain%20invariant%20MR-MR%20registration.%20Methods%3A%20A%20progressively%20refined%0Aregistration%20and%20segmentation%20%28ProRSeg%29%20method%20was%20trained%20with%20262%20pairs%20of%203T%0AMR%20simulation%20scans%20from%20prostate%20cancer%20patients%20using%20weighted%20segmentation%0Aconsistency%20loss.%20ProRSeg%20was%20tested%20on%20same-%20%2858%20pairs%29%2C%20cross-%20%2872%201.5T%20MR%0ALinac%20pairs%29%2C%20and%20mixed-domain%20%2842%20MRSim-MRL%20pairs%29%20datasets%20for%20contour%0Apropagation%20accuracy%20of%20clinical%20target%20volume%20%28CTV%29%2C%20bladder%2C%20and%20rectum.%20Dose%0Aaccumulation%20was%20performed%20for%2042%20patients%20undergoing%205-fraction%20MRgART.%0AResults%3A%20ProRSeg%20demonstrated%20generalization%20for%20bladder%20with%20similar%20Dice%0ASimilarity%20Coefficients%20across%20domains%20%280.88%2C%200.87%2C%200.86%29.%20For%20rectum%20and%20CTV%2C%0Aperformance%20was%20domain-dependent%20with%20higher%20accuracy%20on%20cross-domain%20MRL%0Adataset%20%28DSCs%200.89%29%20versus%20same-domain%20data.%20The%20model%27s%20strong%20cross-domain%0Aperformance%20prompted%20us%20to%20study%20the%20feasibility%20of%20using%20it%20for%20dose%0Aaccumulation.%20Dose%20accumulation%20showed%2083.3%25%20of%20patients%20met%20CTV%20coverage%20%28D95%0A%3E%3D%2040.0%20Gy%29%20and%20bladder%20sparing%20%28D50%20%3C%3D%2020.0%20Gy%29%20constraints.%20All%20patients%0Aachieved%20minimum%20mean%20target%20dose%20%28%3E40.4%20Gy%29%2C%20but%20only%209.5%25%20remained%20under%0Aupper%20limit%20%28%3C42.0%20Gy%29.%20Conclusions%3A%20ProRSeg%20showed%20reasonable%20multi-domain%0AMR-MR%20registration%20performance%20for%20prostate%20cancer%20patients%20with%20preliminary%0Afeasibility%20for%20evaluating%20treatment%20compliance%20to%20clinical%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06966v1&entry.124074799=Read"},
{"title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label\n  Contrastive Learning and Bayesian kNN", "author": "Luca Mariotti and Veronica Guidetti and Federica Mandreoli", "abstract": "  The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.\n", "link": "http://arxiv.org/abs/2507.06895v1", "date": "2025-07-09", "relevancy": 1.9697, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4925}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCoRE%3A%20Streamlined%20Corpus-based%20Relation%20Extraction%20using%20Multi-Label%0A%20%20Contrastive%20Learning%20and%20Bayesian%20kNN&body=Title%3A%20SCoRE%3A%20Streamlined%20Corpus-based%20Relation%20Extraction%20using%20Multi-Label%0A%20%20Contrastive%20Learning%20and%20Bayesian%20kNN%0AAuthor%3A%20Luca%20Mariotti%20and%20Veronica%20Guidetti%20and%20Federica%20Mandreoli%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20efficient%20knowledge%20graph%20%28KG%29%20enrichment%20leveraging%0Aexternal%20corpora%20has%20intensified%20interest%20in%20relation%20extraction%20%28RE%29%2C%0Aparticularly%20under%20low-supervision%20settings.%20To%20address%20the%20need%20for%20adaptable%0Aand%20noise-resilient%20RE%20solutions%20that%20integrate%20seamlessly%20with%20pre-trained%0Alarge%20language%20models%20%28PLMs%29%2C%20we%20introduce%20SCoRE%2C%20a%20modular%20and%20cost-effective%0Asentence-level%20RE%20system.%20SCoRE%20enables%20easy%20PLM%20switching%2C%20requires%20no%0Afinetuning%2C%20and%20adapts%20smoothly%20to%20diverse%20corpora%20and%20KGs.%20By%20combining%0Asupervised%20contrastive%20learning%20with%20a%20Bayesian%20k-Nearest%20Neighbors%20%28kNN%29%0Aclassifier%20for%20multi-label%20classification%2C%20it%20delivers%20robust%20performance%0Adespite%20the%20noisy%20annotations%20of%20distantly%20supervised%20corpora.%20To%20improve%20RE%0Aevaluation%2C%20we%20propose%20two%20novel%20metrics%3A%20Correlation%20Structure%20Distance%20%28CSD%29%2C%0Ameasuring%20the%20alignment%20between%20learned%20relational%20patterns%20and%20KG%20structures%2C%0Aand%20Precision%20at%20R%20%28P%40R%29%2C%20assessing%20utility%20as%20a%20recommender%20system.%20We%20also%0Arelease%20Wiki20d%2C%20a%20benchmark%20dataset%20replicating%20real-world%20RE%20conditions%20where%0Aonly%20KG-derived%20annotations%20are%20available.%20Experiments%20on%20five%20benchmarks%20show%0Athat%20SCoRE%20matches%20or%20surpasses%20state-of-the-art%20methods%20while%20significantly%0Areducing%20energy%20consumption.%20Further%20analyses%20reveal%20that%20increasing%20model%0Acomplexity%2C%20as%20seen%20in%20prior%20work%2C%20degrades%20performance%2C%20highlighting%20the%0Aadvantages%20of%20SCoRE%27s%20minimal%20design.%20Combining%20efficiency%2C%20modularity%2C%20and%0Ascalability%2C%20SCoRE%20stands%20as%20an%20optimal%20choice%20for%20real-world%20RE%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCoRE%253A%2520Streamlined%2520Corpus-based%2520Relation%2520Extraction%2520using%2520Multi-Label%250A%2520%2520Contrastive%2520Learning%2520and%2520Bayesian%2520kNN%26entry.906535625%3DLuca%2520Mariotti%2520and%2520Veronica%2520Guidetti%2520and%2520Federica%2520Mandreoli%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520efficient%2520knowledge%2520graph%2520%2528KG%2529%2520enrichment%2520leveraging%250Aexternal%2520corpora%2520has%2520intensified%2520interest%2520in%2520relation%2520extraction%2520%2528RE%2529%252C%250Aparticularly%2520under%2520low-supervision%2520settings.%2520To%2520address%2520the%2520need%2520for%2520adaptable%250Aand%2520noise-resilient%2520RE%2520solutions%2520that%2520integrate%2520seamlessly%2520with%2520pre-trained%250Alarge%2520language%2520models%2520%2528PLMs%2529%252C%2520we%2520introduce%2520SCoRE%252C%2520a%2520modular%2520and%2520cost-effective%250Asentence-level%2520RE%2520system.%2520SCoRE%2520enables%2520easy%2520PLM%2520switching%252C%2520requires%2520no%250Afinetuning%252C%2520and%2520adapts%2520smoothly%2520to%2520diverse%2520corpora%2520and%2520KGs.%2520By%2520combining%250Asupervised%2520contrastive%2520learning%2520with%2520a%2520Bayesian%2520k-Nearest%2520Neighbors%2520%2528kNN%2529%250Aclassifier%2520for%2520multi-label%2520classification%252C%2520it%2520delivers%2520robust%2520performance%250Adespite%2520the%2520noisy%2520annotations%2520of%2520distantly%2520supervised%2520corpora.%2520To%2520improve%2520RE%250Aevaluation%252C%2520we%2520propose%2520two%2520novel%2520metrics%253A%2520Correlation%2520Structure%2520Distance%2520%2528CSD%2529%252C%250Ameasuring%2520the%2520alignment%2520between%2520learned%2520relational%2520patterns%2520and%2520KG%2520structures%252C%250Aand%2520Precision%2520at%2520R%2520%2528P%2540R%2529%252C%2520assessing%2520utility%2520as%2520a%2520recommender%2520system.%2520We%2520also%250Arelease%2520Wiki20d%252C%2520a%2520benchmark%2520dataset%2520replicating%2520real-world%2520RE%2520conditions%2520where%250Aonly%2520KG-derived%2520annotations%2520are%2520available.%2520Experiments%2520on%2520five%2520benchmarks%2520show%250Athat%2520SCoRE%2520matches%2520or%2520surpasses%2520state-of-the-art%2520methods%2520while%2520significantly%250Areducing%2520energy%2520consumption.%2520Further%2520analyses%2520reveal%2520that%2520increasing%2520model%250Acomplexity%252C%2520as%2520seen%2520in%2520prior%2520work%252C%2520degrades%2520performance%252C%2520highlighting%2520the%250Aadvantages%2520of%2520SCoRE%2527s%2520minimal%2520design.%2520Combining%2520efficiency%252C%2520modularity%252C%2520and%250Ascalability%252C%2520SCoRE%2520stands%2520as%2520an%2520optimal%2520choice%2520for%2520real-world%2520RE%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCoRE%3A%20Streamlined%20Corpus-based%20Relation%20Extraction%20using%20Multi-Label%0A%20%20Contrastive%20Learning%20and%20Bayesian%20kNN&entry.906535625=Luca%20Mariotti%20and%20Veronica%20Guidetti%20and%20Federica%20Mandreoli&entry.1292438233=%20%20The%20growing%20demand%20for%20efficient%20knowledge%20graph%20%28KG%29%20enrichment%20leveraging%0Aexternal%20corpora%20has%20intensified%20interest%20in%20relation%20extraction%20%28RE%29%2C%0Aparticularly%20under%20low-supervision%20settings.%20To%20address%20the%20need%20for%20adaptable%0Aand%20noise-resilient%20RE%20solutions%20that%20integrate%20seamlessly%20with%20pre-trained%0Alarge%20language%20models%20%28PLMs%29%2C%20we%20introduce%20SCoRE%2C%20a%20modular%20and%20cost-effective%0Asentence-level%20RE%20system.%20SCoRE%20enables%20easy%20PLM%20switching%2C%20requires%20no%0Afinetuning%2C%20and%20adapts%20smoothly%20to%20diverse%20corpora%20and%20KGs.%20By%20combining%0Asupervised%20contrastive%20learning%20with%20a%20Bayesian%20k-Nearest%20Neighbors%20%28kNN%29%0Aclassifier%20for%20multi-label%20classification%2C%20it%20delivers%20robust%20performance%0Adespite%20the%20noisy%20annotations%20of%20distantly%20supervised%20corpora.%20To%20improve%20RE%0Aevaluation%2C%20we%20propose%20two%20novel%20metrics%3A%20Correlation%20Structure%20Distance%20%28CSD%29%2C%0Ameasuring%20the%20alignment%20between%20learned%20relational%20patterns%20and%20KG%20structures%2C%0Aand%20Precision%20at%20R%20%28P%40R%29%2C%20assessing%20utility%20as%20a%20recommender%20system.%20We%20also%0Arelease%20Wiki20d%2C%20a%20benchmark%20dataset%20replicating%20real-world%20RE%20conditions%20where%0Aonly%20KG-derived%20annotations%20are%20available.%20Experiments%20on%20five%20benchmarks%20show%0Athat%20SCoRE%20matches%20or%20surpasses%20state-of-the-art%20methods%20while%20significantly%0Areducing%20energy%20consumption.%20Further%20analyses%20reveal%20that%20increasing%20model%0Acomplexity%2C%20as%20seen%20in%20prior%20work%2C%20degrades%20performance%2C%20highlighting%20the%0Aadvantages%20of%20SCoRE%27s%20minimal%20design.%20Combining%20efficiency%2C%20modularity%2C%20and%0Ascalability%2C%20SCoRE%20stands%20as%20an%20optimal%20choice%20for%20real-world%20RE%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06895v1&entry.124074799=Read"},
{"title": "FlexOlmo: Open Language Models for Flexible Data Use", "author": "Weijia Shi and Akshita Bhagia and Kevin Farhat and Niklas Muennighoff and Pete Walsh and Jacob Morrison and Dustin Schwenk and Shayne Longpre and Jake Poznanski and Allyson Ettinger and Daogao Liu and Margaret Li and Dirk Groeneveld and Mike Lewis and Wen-tau Yih and Luca Soldaini and Kyle Lo and Noah A. Smith and Luke Zettlemoyer and Pang Wei Koh and Hannaneh Hajishirzi and Ali Farhadi and Sewon Min", "abstract": "  We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.\n", "link": "http://arxiv.org/abs/2507.07024v1", "date": "2025-07-09", "relevancy": 1.9657, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5242}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4885}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexOlmo%3A%20Open%20Language%20Models%20for%20Flexible%20Data%20Use&body=Title%3A%20FlexOlmo%3A%20Open%20Language%20Models%20for%20Flexible%20Data%20Use%0AAuthor%3A%20Weijia%20Shi%20and%20Akshita%20Bhagia%20and%20Kevin%20Farhat%20and%20Niklas%20Muennighoff%20and%20Pete%20Walsh%20and%20Jacob%20Morrison%20and%20Dustin%20Schwenk%20and%20Shayne%20Longpre%20and%20Jake%20Poznanski%20and%20Allyson%20Ettinger%20and%20Daogao%20Liu%20and%20Margaret%20Li%20and%20Dirk%20Groeneveld%20and%20Mike%20Lewis%20and%20Wen-tau%20Yih%20and%20Luca%20Soldaini%20and%20Kyle%20Lo%20and%20Noah%20A.%20Smith%20and%20Luke%20Zettlemoyer%20and%20Pang%20Wei%20Koh%20and%20Hannaneh%20Hajishirzi%20and%20Ali%20Farhadi%20and%20Sewon%20Min%0AAbstract%3A%20%20%20We%20introduce%20FlexOlmo%2C%20a%20new%20class%20of%20language%20models%20%28LMs%29%20that%20supports%20%281%29%0Adistributed%20training%20without%20data%20sharing%2C%20where%20different%20model%20parameters%20are%0Aindependently%20trained%20on%20closed%20datasets%2C%20and%20%282%29%20data-flexible%20inference%2C%0Awhere%20these%20parameters%20along%20with%20their%20associated%20data%20can%20be%20flexibly%0Aincluded%20or%20excluded%20from%20model%20inferences%20with%20no%20further%20training.%20FlexOlmo%0Aemploys%20a%20mixture-of-experts%20%28MoE%29%20architecture%20where%20each%20expert%20is%20trained%0Aindependently%20on%20closed%20datasets%20and%20later%20integrated%20through%20a%20new%0Adomain-informed%20routing%20without%20any%20joint%20training.%20FlexOlmo%20is%20trained%20on%0AFlexMix%2C%20a%20corpus%20we%20curate%20comprising%20publicly%20available%20datasets%20alongside%0Aseven%20domain-specific%20sets%2C%20representing%20realistic%20approximations%20of%20closed%0Asets.%20We%20evaluate%20models%20with%20up%20to%2037%20billion%20parameters%20%2820%20billion%20active%29%0Aon%2031%20diverse%20downstream%20tasks.%20We%20show%20that%20a%20general%20expert%20trained%20on%20public%0Adata%20can%20be%20effectively%20combined%20with%20independently%20trained%20experts%20from%20other%0Adata%20owners%2C%20leading%20to%20an%20average%2041%25%20relative%20improvement%20while%20allowing%0Ausers%20to%20opt%20out%20of%20certain%20data%20based%20on%20data%20licensing%20or%20permission%0Arequirements.%20Our%20approach%20also%20outperforms%20prior%20model%20merging%20methods%20by%0A10.1%25%20on%20average%20and%20surpasses%20the%20standard%20MoE%20trained%20without%20data%0Arestrictions%20using%20the%20same%20training%20FLOPs.%20Altogether%2C%20this%20research%20presents%0Aa%20solution%20for%20both%20data%20owners%20and%20researchers%20in%20regulated%20industries%20with%0Asensitive%20or%20protected%20data.%20FlexOlmo%20enables%20benefiting%20from%20closed%20data%20while%0Arespecting%20data%20owners%27%20preferences%20by%20keeping%20their%20data%20local%20and%20supporting%0Afine-grained%20control%20of%20data%20access%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexOlmo%253A%2520Open%2520Language%2520Models%2520for%2520Flexible%2520Data%2520Use%26entry.906535625%3DWeijia%2520Shi%2520and%2520Akshita%2520Bhagia%2520and%2520Kevin%2520Farhat%2520and%2520Niklas%2520Muennighoff%2520and%2520Pete%2520Walsh%2520and%2520Jacob%2520Morrison%2520and%2520Dustin%2520Schwenk%2520and%2520Shayne%2520Longpre%2520and%2520Jake%2520Poznanski%2520and%2520Allyson%2520Ettinger%2520and%2520Daogao%2520Liu%2520and%2520Margaret%2520Li%2520and%2520Dirk%2520Groeneveld%2520and%2520Mike%2520Lewis%2520and%2520Wen-tau%2520Yih%2520and%2520Luca%2520Soldaini%2520and%2520Kyle%2520Lo%2520and%2520Noah%2520A.%2520Smith%2520and%2520Luke%2520Zettlemoyer%2520and%2520Pang%2520Wei%2520Koh%2520and%2520Hannaneh%2520Hajishirzi%2520and%2520Ali%2520Farhadi%2520and%2520Sewon%2520Min%26entry.1292438233%3D%2520%2520We%2520introduce%2520FlexOlmo%252C%2520a%2520new%2520class%2520of%2520language%2520models%2520%2528LMs%2529%2520that%2520supports%2520%25281%2529%250Adistributed%2520training%2520without%2520data%2520sharing%252C%2520where%2520different%2520model%2520parameters%2520are%250Aindependently%2520trained%2520on%2520closed%2520datasets%252C%2520and%2520%25282%2529%2520data-flexible%2520inference%252C%250Awhere%2520these%2520parameters%2520along%2520with%2520their%2520associated%2520data%2520can%2520be%2520flexibly%250Aincluded%2520or%2520excluded%2520from%2520model%2520inferences%2520with%2520no%2520further%2520training.%2520FlexOlmo%250Aemploys%2520a%2520mixture-of-experts%2520%2528MoE%2529%2520architecture%2520where%2520each%2520expert%2520is%2520trained%250Aindependently%2520on%2520closed%2520datasets%2520and%2520later%2520integrated%2520through%2520a%2520new%250Adomain-informed%2520routing%2520without%2520any%2520joint%2520training.%2520FlexOlmo%2520is%2520trained%2520on%250AFlexMix%252C%2520a%2520corpus%2520we%2520curate%2520comprising%2520publicly%2520available%2520datasets%2520alongside%250Aseven%2520domain-specific%2520sets%252C%2520representing%2520realistic%2520approximations%2520of%2520closed%250Asets.%2520We%2520evaluate%2520models%2520with%2520up%2520to%252037%2520billion%2520parameters%2520%252820%2520billion%2520active%2529%250Aon%252031%2520diverse%2520downstream%2520tasks.%2520We%2520show%2520that%2520a%2520general%2520expert%2520trained%2520on%2520public%250Adata%2520can%2520be%2520effectively%2520combined%2520with%2520independently%2520trained%2520experts%2520from%2520other%250Adata%2520owners%252C%2520leading%2520to%2520an%2520average%252041%2525%2520relative%2520improvement%2520while%2520allowing%250Ausers%2520to%2520opt%2520out%2520of%2520certain%2520data%2520based%2520on%2520data%2520licensing%2520or%2520permission%250Arequirements.%2520Our%2520approach%2520also%2520outperforms%2520prior%2520model%2520merging%2520methods%2520by%250A10.1%2525%2520on%2520average%2520and%2520surpasses%2520the%2520standard%2520MoE%2520trained%2520without%2520data%250Arestrictions%2520using%2520the%2520same%2520training%2520FLOPs.%2520Altogether%252C%2520this%2520research%2520presents%250Aa%2520solution%2520for%2520both%2520data%2520owners%2520and%2520researchers%2520in%2520regulated%2520industries%2520with%250Asensitive%2520or%2520protected%2520data.%2520FlexOlmo%2520enables%2520benefiting%2520from%2520closed%2520data%2520while%250Arespecting%2520data%2520owners%2527%2520preferences%2520by%2520keeping%2520their%2520data%2520local%2520and%2520supporting%250Afine-grained%2520control%2520of%2520data%2520access%2520during%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexOlmo%3A%20Open%20Language%20Models%20for%20Flexible%20Data%20Use&entry.906535625=Weijia%20Shi%20and%20Akshita%20Bhagia%20and%20Kevin%20Farhat%20and%20Niklas%20Muennighoff%20and%20Pete%20Walsh%20and%20Jacob%20Morrison%20and%20Dustin%20Schwenk%20and%20Shayne%20Longpre%20and%20Jake%20Poznanski%20and%20Allyson%20Ettinger%20and%20Daogao%20Liu%20and%20Margaret%20Li%20and%20Dirk%20Groeneveld%20and%20Mike%20Lewis%20and%20Wen-tau%20Yih%20and%20Luca%20Soldaini%20and%20Kyle%20Lo%20and%20Noah%20A.%20Smith%20and%20Luke%20Zettlemoyer%20and%20Pang%20Wei%20Koh%20and%20Hannaneh%20Hajishirzi%20and%20Ali%20Farhadi%20and%20Sewon%20Min&entry.1292438233=%20%20We%20introduce%20FlexOlmo%2C%20a%20new%20class%20of%20language%20models%20%28LMs%29%20that%20supports%20%281%29%0Adistributed%20training%20without%20data%20sharing%2C%20where%20different%20model%20parameters%20are%0Aindependently%20trained%20on%20closed%20datasets%2C%20and%20%282%29%20data-flexible%20inference%2C%0Awhere%20these%20parameters%20along%20with%20their%20associated%20data%20can%20be%20flexibly%0Aincluded%20or%20excluded%20from%20model%20inferences%20with%20no%20further%20training.%20FlexOlmo%0Aemploys%20a%20mixture-of-experts%20%28MoE%29%20architecture%20where%20each%20expert%20is%20trained%0Aindependently%20on%20closed%20datasets%20and%20later%20integrated%20through%20a%20new%0Adomain-informed%20routing%20without%20any%20joint%20training.%20FlexOlmo%20is%20trained%20on%0AFlexMix%2C%20a%20corpus%20we%20curate%20comprising%20publicly%20available%20datasets%20alongside%0Aseven%20domain-specific%20sets%2C%20representing%20realistic%20approximations%20of%20closed%0Asets.%20We%20evaluate%20models%20with%20up%20to%2037%20billion%20parameters%20%2820%20billion%20active%29%0Aon%2031%20diverse%20downstream%20tasks.%20We%20show%20that%20a%20general%20expert%20trained%20on%20public%0Adata%20can%20be%20effectively%20combined%20with%20independently%20trained%20experts%20from%20other%0Adata%20owners%2C%20leading%20to%20an%20average%2041%25%20relative%20improvement%20while%20allowing%0Ausers%20to%20opt%20out%20of%20certain%20data%20based%20on%20data%20licensing%20or%20permission%0Arequirements.%20Our%20approach%20also%20outperforms%20prior%20model%20merging%20methods%20by%0A10.1%25%20on%20average%20and%20surpasses%20the%20standard%20MoE%20trained%20without%20data%0Arestrictions%20using%20the%20same%20training%20FLOPs.%20Altogether%2C%20this%20research%20presents%0Aa%20solution%20for%20both%20data%20owners%20and%20researchers%20in%20regulated%20industries%20with%0Asensitive%20or%20protected%20data.%20FlexOlmo%20enables%20benefiting%20from%20closed%20data%20while%0Arespecting%20data%20owners%27%20preferences%20by%20keeping%20their%20data%20local%20and%20supporting%0Afine-grained%20control%20of%20data%20access%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07024v1&entry.124074799=Read"},
{"title": "Towards Enterprise-Ready Computer Using Generalist Agent", "author": "Sami Marreed and Alon Oved and Avi Yaeli and Segev Shlomov and Ido Levy and Offer Akrabi and Aviad Sela and Asaf Adi and Nir Mashkif", "abstract": "  This paper presents our ongoing work toward developing an enterprise-ready\nComputer Using Generalist Agent (CUGA) system. Our research highlights the\nevolutionary nature of building agentic systems suitable for enterprise\nenvironments. By integrating state-of-the-art agentic AI techniques with a\nsystematic approach to iterative evaluation, analysis, and refinement, we have\nachieved rapid and cost-effective performance gains, notably reaching a new\nstate-of-the-art performance on the WebArena and AppWorld benchmarks. We detail\nour development roadmap, the methodology and tools that facilitated rapid\nlearning from failures and continuous system refinement, and discuss key\nlessons learned and future challenges for enterprise adoption.\n", "link": "http://arxiv.org/abs/2503.01861v3", "date": "2025-07-09", "relevancy": 1.9631, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5059}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.49}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Enterprise-Ready%20Computer%20Using%20Generalist%20Agent&body=Title%3A%20Towards%20Enterprise-Ready%20Computer%20Using%20Generalist%20Agent%0AAuthor%3A%20Sami%20Marreed%20and%20Alon%20Oved%20and%20Avi%20Yaeli%20and%20Segev%20Shlomov%20and%20Ido%20Levy%20and%20Offer%20Akrabi%20and%20Aviad%20Sela%20and%20Asaf%20Adi%20and%20Nir%20Mashkif%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20ongoing%20work%20toward%20developing%20an%20enterprise-ready%0AComputer%20Using%20Generalist%20Agent%20%28CUGA%29%20system.%20Our%20research%20highlights%20the%0Aevolutionary%20nature%20of%20building%20agentic%20systems%20suitable%20for%20enterprise%0Aenvironments.%20By%20integrating%20state-of-the-art%20agentic%20AI%20techniques%20with%20a%0Asystematic%20approach%20to%20iterative%20evaluation%2C%20analysis%2C%20and%20refinement%2C%20we%20have%0Aachieved%20rapid%20and%20cost-effective%20performance%20gains%2C%20notably%20reaching%20a%20new%0Astate-of-the-art%20performance%20on%20the%20WebArena%20and%20AppWorld%20benchmarks.%20We%20detail%0Aour%20development%20roadmap%2C%20the%20methodology%20and%20tools%20that%20facilitated%20rapid%0Alearning%20from%20failures%20and%20continuous%20system%20refinement%2C%20and%20discuss%20key%0Alessons%20learned%20and%20future%20challenges%20for%20enterprise%20adoption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01861v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Enterprise-Ready%2520Computer%2520Using%2520Generalist%2520Agent%26entry.906535625%3DSami%2520Marreed%2520and%2520Alon%2520Oved%2520and%2520Avi%2520Yaeli%2520and%2520Segev%2520Shlomov%2520and%2520Ido%2520Levy%2520and%2520Offer%2520Akrabi%2520and%2520Aviad%2520Sela%2520and%2520Asaf%2520Adi%2520and%2520Nir%2520Mashkif%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520our%2520ongoing%2520work%2520toward%2520developing%2520an%2520enterprise-ready%250AComputer%2520Using%2520Generalist%2520Agent%2520%2528CUGA%2529%2520system.%2520Our%2520research%2520highlights%2520the%250Aevolutionary%2520nature%2520of%2520building%2520agentic%2520systems%2520suitable%2520for%2520enterprise%250Aenvironments.%2520By%2520integrating%2520state-of-the-art%2520agentic%2520AI%2520techniques%2520with%2520a%250Asystematic%2520approach%2520to%2520iterative%2520evaluation%252C%2520analysis%252C%2520and%2520refinement%252C%2520we%2520have%250Aachieved%2520rapid%2520and%2520cost-effective%2520performance%2520gains%252C%2520notably%2520reaching%2520a%2520new%250Astate-of-the-art%2520performance%2520on%2520the%2520WebArena%2520and%2520AppWorld%2520benchmarks.%2520We%2520detail%250Aour%2520development%2520roadmap%252C%2520the%2520methodology%2520and%2520tools%2520that%2520facilitated%2520rapid%250Alearning%2520from%2520failures%2520and%2520continuous%2520system%2520refinement%252C%2520and%2520discuss%2520key%250Alessons%2520learned%2520and%2520future%2520challenges%2520for%2520enterprise%2520adoption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01861v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Enterprise-Ready%20Computer%20Using%20Generalist%20Agent&entry.906535625=Sami%20Marreed%20and%20Alon%20Oved%20and%20Avi%20Yaeli%20and%20Segev%20Shlomov%20and%20Ido%20Levy%20and%20Offer%20Akrabi%20and%20Aviad%20Sela%20and%20Asaf%20Adi%20and%20Nir%20Mashkif&entry.1292438233=%20%20This%20paper%20presents%20our%20ongoing%20work%20toward%20developing%20an%20enterprise-ready%0AComputer%20Using%20Generalist%20Agent%20%28CUGA%29%20system.%20Our%20research%20highlights%20the%0Aevolutionary%20nature%20of%20building%20agentic%20systems%20suitable%20for%20enterprise%0Aenvironments.%20By%20integrating%20state-of-the-art%20agentic%20AI%20techniques%20with%20a%0Asystematic%20approach%20to%20iterative%20evaluation%2C%20analysis%2C%20and%20refinement%2C%20we%20have%0Aachieved%20rapid%20and%20cost-effective%20performance%20gains%2C%20notably%20reaching%20a%20new%0Astate-of-the-art%20performance%20on%20the%20WebArena%20and%20AppWorld%20benchmarks.%20We%20detail%0Aour%20development%20roadmap%2C%20the%20methodology%20and%20tools%20that%20facilitated%20rapid%0Alearning%20from%20failures%20and%20continuous%20system%20refinement%2C%20and%20discuss%20key%0Alessons%20learned%20and%20future%20challenges%20for%20enterprise%20adoption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01861v3&entry.124074799=Read"},
{"title": "Self-Supervised Learning at the Edge: The Cost of Labeling", "author": "Roberto Pereira and Fernanda Fam\u00e1 and Asal Rangrazi and Marco Miozzo and Charalampos Kalalas and Paolo Dini", "abstract": "  Contrastive learning (CL) has recently emerged as an alternative to\ntraditional supervised machine learning solutions by enabling rich\nrepresentations from unstructured and unlabeled data. However, CL and, more\nbroadly, self-supervised learning (SSL) methods often demand a large amount of\ndata and computational resources, posing challenges for deployment on\nresource-constrained edge devices. In this work, we explore the feasibility and\nefficiency of SSL techniques for edge-based learning, focusing on trade-offs\nbetween model performance and energy efficiency. In particular, we analyze how\ndifferent SSL techniques adapt to limited computational, data, and energy\nbudgets, evaluating their effectiveness in learning robust representations\nunder resource-constrained settings. Moreover, we also consider the energy\ncosts involved in labeling data and assess how semi-supervised learning may\nassist in reducing the overall energy consumed to train CL models. Through\nextensive experiments, we demonstrate that tailored SSL strategies can achieve\ncompetitive performance while reducing resource consumption by up to 4X,\nunderscoring their potential for energy-efficient learning at the edge.\n", "link": "http://arxiv.org/abs/2507.07033v1", "date": "2025-07-09", "relevancy": 1.9572, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5307}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4932}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20at%20the%20Edge%3A%20The%20Cost%20of%20Labeling&body=Title%3A%20Self-Supervised%20Learning%20at%20the%20Edge%3A%20The%20Cost%20of%20Labeling%0AAuthor%3A%20Roberto%20Pereira%20and%20Fernanda%20Fam%C3%A1%20and%20Asal%20Rangrazi%20and%20Marco%20Miozzo%20and%20Charalampos%20Kalalas%20and%20Paolo%20Dini%0AAbstract%3A%20%20%20Contrastive%20learning%20%28CL%29%20has%20recently%20emerged%20as%20an%20alternative%20to%0Atraditional%20supervised%20machine%20learning%20solutions%20by%20enabling%20rich%0Arepresentations%20from%20unstructured%20and%20unlabeled%20data.%20However%2C%20CL%20and%2C%20more%0Abroadly%2C%20self-supervised%20learning%20%28SSL%29%20methods%20often%20demand%20a%20large%20amount%20of%0Adata%20and%20computational%20resources%2C%20posing%20challenges%20for%20deployment%20on%0Aresource-constrained%20edge%20devices.%20In%20this%20work%2C%20we%20explore%20the%20feasibility%20and%0Aefficiency%20of%20SSL%20techniques%20for%20edge-based%20learning%2C%20focusing%20on%20trade-offs%0Abetween%20model%20performance%20and%20energy%20efficiency.%20In%20particular%2C%20we%20analyze%20how%0Adifferent%20SSL%20techniques%20adapt%20to%20limited%20computational%2C%20data%2C%20and%20energy%0Abudgets%2C%20evaluating%20their%20effectiveness%20in%20learning%20robust%20representations%0Aunder%20resource-constrained%20settings.%20Moreover%2C%20we%20also%20consider%20the%20energy%0Acosts%20involved%20in%20labeling%20data%20and%20assess%20how%20semi-supervised%20learning%20may%0Aassist%20in%20reducing%20the%20overall%20energy%20consumed%20to%20train%20CL%20models.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20that%20tailored%20SSL%20strategies%20can%20achieve%0Acompetitive%20performance%20while%20reducing%20resource%20consumption%20by%20up%20to%204X%2C%0Aunderscoring%20their%20potential%20for%20energy-efficient%20learning%20at%20the%20edge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520at%2520the%2520Edge%253A%2520The%2520Cost%2520of%2520Labeling%26entry.906535625%3DRoberto%2520Pereira%2520and%2520Fernanda%2520Fam%25C3%25A1%2520and%2520Asal%2520Rangrazi%2520and%2520Marco%2520Miozzo%2520and%2520Charalampos%2520Kalalas%2520and%2520Paolo%2520Dini%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520%2528CL%2529%2520has%2520recently%2520emerged%2520as%2520an%2520alternative%2520to%250Atraditional%2520supervised%2520machine%2520learning%2520solutions%2520by%2520enabling%2520rich%250Arepresentations%2520from%2520unstructured%2520and%2520unlabeled%2520data.%2520However%252C%2520CL%2520and%252C%2520more%250Abroadly%252C%2520self-supervised%2520learning%2520%2528SSL%2529%2520methods%2520often%2520demand%2520a%2520large%2520amount%2520of%250Adata%2520and%2520computational%2520resources%252C%2520posing%2520challenges%2520for%2520deployment%2520on%250Aresource-constrained%2520edge%2520devices.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520feasibility%2520and%250Aefficiency%2520of%2520SSL%2520techniques%2520for%2520edge-based%2520learning%252C%2520focusing%2520on%2520trade-offs%250Abetween%2520model%2520performance%2520and%2520energy%2520efficiency.%2520In%2520particular%252C%2520we%2520analyze%2520how%250Adifferent%2520SSL%2520techniques%2520adapt%2520to%2520limited%2520computational%252C%2520data%252C%2520and%2520energy%250Abudgets%252C%2520evaluating%2520their%2520effectiveness%2520in%2520learning%2520robust%2520representations%250Aunder%2520resource-constrained%2520settings.%2520Moreover%252C%2520we%2520also%2520consider%2520the%2520energy%250Acosts%2520involved%2520in%2520labeling%2520data%2520and%2520assess%2520how%2520semi-supervised%2520learning%2520may%250Aassist%2520in%2520reducing%2520the%2520overall%2520energy%2520consumed%2520to%2520train%2520CL%2520models.%2520Through%250Aextensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520tailored%2520SSL%2520strategies%2520can%2520achieve%250Acompetitive%2520performance%2520while%2520reducing%2520resource%2520consumption%2520by%2520up%2520to%25204X%252C%250Aunderscoring%2520their%2520potential%2520for%2520energy-efficient%2520learning%2520at%2520the%2520edge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20at%20the%20Edge%3A%20The%20Cost%20of%20Labeling&entry.906535625=Roberto%20Pereira%20and%20Fernanda%20Fam%C3%A1%20and%20Asal%20Rangrazi%20and%20Marco%20Miozzo%20and%20Charalampos%20Kalalas%20and%20Paolo%20Dini&entry.1292438233=%20%20Contrastive%20learning%20%28CL%29%20has%20recently%20emerged%20as%20an%20alternative%20to%0Atraditional%20supervised%20machine%20learning%20solutions%20by%20enabling%20rich%0Arepresentations%20from%20unstructured%20and%20unlabeled%20data.%20However%2C%20CL%20and%2C%20more%0Abroadly%2C%20self-supervised%20learning%20%28SSL%29%20methods%20often%20demand%20a%20large%20amount%20of%0Adata%20and%20computational%20resources%2C%20posing%20challenges%20for%20deployment%20on%0Aresource-constrained%20edge%20devices.%20In%20this%20work%2C%20we%20explore%20the%20feasibility%20and%0Aefficiency%20of%20SSL%20techniques%20for%20edge-based%20learning%2C%20focusing%20on%20trade-offs%0Abetween%20model%20performance%20and%20energy%20efficiency.%20In%20particular%2C%20we%20analyze%20how%0Adifferent%20SSL%20techniques%20adapt%20to%20limited%20computational%2C%20data%2C%20and%20energy%0Abudgets%2C%20evaluating%20their%20effectiveness%20in%20learning%20robust%20representations%0Aunder%20resource-constrained%20settings.%20Moreover%2C%20we%20also%20consider%20the%20energy%0Acosts%20involved%20in%20labeling%20data%20and%20assess%20how%20semi-supervised%20learning%20may%0Aassist%20in%20reducing%20the%20overall%20energy%20consumed%20to%20train%20CL%20models.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20that%20tailored%20SSL%20strategies%20can%20achieve%0Acompetitive%20performance%20while%20reducing%20resource%20consumption%20by%20up%20to%204X%2C%0Aunderscoring%20their%20potential%20for%20energy-efficient%20learning%20at%20the%20edge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07033v1&entry.124074799=Read"},
{"title": "LARP: Learner-Agnostic Robust Data Prefiltering", "author": "Kristian Minchev and Dimitar Iliev Dimitrov and Nikola Konstantinov", "abstract": "  The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets.\n", "link": "http://arxiv.org/abs/2506.20573v2", "date": "2025-07-09", "relevancy": 1.9339, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5024}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4999}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LARP%3A%20Learner-Agnostic%20Robust%20Data%20Prefiltering&body=Title%3A%20LARP%3A%20Learner-Agnostic%20Robust%20Data%20Prefiltering%0AAuthor%3A%20Kristian%20Minchev%20and%20Dimitar%20Iliev%20Dimitrov%20and%20Nikola%20Konstantinov%0AAbstract%3A%20%20%20The%20widespread%20availability%20of%20large%20public%20datasets%20is%20a%20key%20factor%20behind%0Athe%20recent%20successes%20of%20statistical%20inference%20and%20machine%20learning%20methods.%0AHowever%2C%20these%20datasets%20often%20contain%20some%20low-quality%20or%20contaminated%20data%2C%20to%0Awhich%20many%20learning%20procedures%20are%20sensitive.%20Therefore%2C%20the%20question%20of%0Awhether%20and%20how%20public%20datasets%20should%20be%20prefiltered%20to%20facilitate%20accurate%0Adownstream%20learning%20arises.%20On%20a%20technical%20level%20this%20requires%20the%20construction%0Aof%20principled%20data%20prefiltering%20methods%20which%20are%20learner-agnostic%20robust%2C%20in%0Athe%20sense%20of%20provably%20protecting%20a%20set%20of%20pre-specified%20downstream%20learners%0Afrom%20corrupted%20data.%20In%20this%20work%2C%20we%20formalize%20the%20problem%20of%20Learner-Agnostic%0ARobust%20data%20Prefiltering%20%28LARP%29%2C%20which%20aims%20at%20finding%20prefiltering%20procedures%0Athat%20minimize%20a%20worst-case%20loss%20over%20a%20pre-specified%20set%20of%20learners.%20We%20first%0Ainstantiate%20our%20framework%20in%20the%20context%20of%20scalar%20mean%20estimation%20with%20Huber%0Aestimators%20under%20the%20Huber%20data%20contamination%20model.%20We%20provide%20a%20hardness%0Aresult%20on%20a%20specific%20problem%20instance%20and%20analyze%20several%20natural%20prefiltering%0Aprocedures.%20Our%20theoretical%20results%20indicate%20that%20performing%20LARP%20on%20a%0Aheterogeneous%20set%20of%20learners%20leads%20to%20some%20loss%20in%20model%20performance%20compared%0Ato%20the%20alternative%20of%20prefiltering%20data%20for%20each%20learner/use-case%20individually.%0AWe%20explore%20the%20resulting%20utility%20loss%20and%20its%20dependence%20on%20the%20problem%0Aparameters%20via%20extensive%20experiments%20on%20real-world%20image%20and%20tabular%20data%2C%0Aobserving%20statistically%20significant%20reduction%20in%20utility.%20Finally%2C%20we%20model%20the%0Atrade-off%20between%20the%20utility%20drop%20and%20the%20cost%20of%20repeated%20%28learner-specific%29%0Aprefiltering%20within%20a%20game-theoretic%20framework%20and%20showcase%20benefits%20of%20LARP%0Afor%20large%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20573v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLARP%253A%2520Learner-Agnostic%2520Robust%2520Data%2520Prefiltering%26entry.906535625%3DKristian%2520Minchev%2520and%2520Dimitar%2520Iliev%2520Dimitrov%2520and%2520Nikola%2520Konstantinov%26entry.1292438233%3D%2520%2520The%2520widespread%2520availability%2520of%2520large%2520public%2520datasets%2520is%2520a%2520key%2520factor%2520behind%250Athe%2520recent%2520successes%2520of%2520statistical%2520inference%2520and%2520machine%2520learning%2520methods.%250AHowever%252C%2520these%2520datasets%2520often%2520contain%2520some%2520low-quality%2520or%2520contaminated%2520data%252C%2520to%250Awhich%2520many%2520learning%2520procedures%2520are%2520sensitive.%2520Therefore%252C%2520the%2520question%2520of%250Awhether%2520and%2520how%2520public%2520datasets%2520should%2520be%2520prefiltered%2520to%2520facilitate%2520accurate%250Adownstream%2520learning%2520arises.%2520On%2520a%2520technical%2520level%2520this%2520requires%2520the%2520construction%250Aof%2520principled%2520data%2520prefiltering%2520methods%2520which%2520are%2520learner-agnostic%2520robust%252C%2520in%250Athe%2520sense%2520of%2520provably%2520protecting%2520a%2520set%2520of%2520pre-specified%2520downstream%2520learners%250Afrom%2520corrupted%2520data.%2520In%2520this%2520work%252C%2520we%2520formalize%2520the%2520problem%2520of%2520Learner-Agnostic%250ARobust%2520data%2520Prefiltering%2520%2528LARP%2529%252C%2520which%2520aims%2520at%2520finding%2520prefiltering%2520procedures%250Athat%2520minimize%2520a%2520worst-case%2520loss%2520over%2520a%2520pre-specified%2520set%2520of%2520learners.%2520We%2520first%250Ainstantiate%2520our%2520framework%2520in%2520the%2520context%2520of%2520scalar%2520mean%2520estimation%2520with%2520Huber%250Aestimators%2520under%2520the%2520Huber%2520data%2520contamination%2520model.%2520We%2520provide%2520a%2520hardness%250Aresult%2520on%2520a%2520specific%2520problem%2520instance%2520and%2520analyze%2520several%2520natural%2520prefiltering%250Aprocedures.%2520Our%2520theoretical%2520results%2520indicate%2520that%2520performing%2520LARP%2520on%2520a%250Aheterogeneous%2520set%2520of%2520learners%2520leads%2520to%2520some%2520loss%2520in%2520model%2520performance%2520compared%250Ato%2520the%2520alternative%2520of%2520prefiltering%2520data%2520for%2520each%2520learner/use-case%2520individually.%250AWe%2520explore%2520the%2520resulting%2520utility%2520loss%2520and%2520its%2520dependence%2520on%2520the%2520problem%250Aparameters%2520via%2520extensive%2520experiments%2520on%2520real-world%2520image%2520and%2520tabular%2520data%252C%250Aobserving%2520statistically%2520significant%2520reduction%2520in%2520utility.%2520Finally%252C%2520we%2520model%2520the%250Atrade-off%2520between%2520the%2520utility%2520drop%2520and%2520the%2520cost%2520of%2520repeated%2520%2528learner-specific%2529%250Aprefiltering%2520within%2520a%2520game-theoretic%2520framework%2520and%2520showcase%2520benefits%2520of%2520LARP%250Afor%2520large%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20573v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LARP%3A%20Learner-Agnostic%20Robust%20Data%20Prefiltering&entry.906535625=Kristian%20Minchev%20and%20Dimitar%20Iliev%20Dimitrov%20and%20Nikola%20Konstantinov&entry.1292438233=%20%20The%20widespread%20availability%20of%20large%20public%20datasets%20is%20a%20key%20factor%20behind%0Athe%20recent%20successes%20of%20statistical%20inference%20and%20machine%20learning%20methods.%0AHowever%2C%20these%20datasets%20often%20contain%20some%20low-quality%20or%20contaminated%20data%2C%20to%0Awhich%20many%20learning%20procedures%20are%20sensitive.%20Therefore%2C%20the%20question%20of%0Awhether%20and%20how%20public%20datasets%20should%20be%20prefiltered%20to%20facilitate%20accurate%0Adownstream%20learning%20arises.%20On%20a%20technical%20level%20this%20requires%20the%20construction%0Aof%20principled%20data%20prefiltering%20methods%20which%20are%20learner-agnostic%20robust%2C%20in%0Athe%20sense%20of%20provably%20protecting%20a%20set%20of%20pre-specified%20downstream%20learners%0Afrom%20corrupted%20data.%20In%20this%20work%2C%20we%20formalize%20the%20problem%20of%20Learner-Agnostic%0ARobust%20data%20Prefiltering%20%28LARP%29%2C%20which%20aims%20at%20finding%20prefiltering%20procedures%0Athat%20minimize%20a%20worst-case%20loss%20over%20a%20pre-specified%20set%20of%20learners.%20We%20first%0Ainstantiate%20our%20framework%20in%20the%20context%20of%20scalar%20mean%20estimation%20with%20Huber%0Aestimators%20under%20the%20Huber%20data%20contamination%20model.%20We%20provide%20a%20hardness%0Aresult%20on%20a%20specific%20problem%20instance%20and%20analyze%20several%20natural%20prefiltering%0Aprocedures.%20Our%20theoretical%20results%20indicate%20that%20performing%20LARP%20on%20a%0Aheterogeneous%20set%20of%20learners%20leads%20to%20some%20loss%20in%20model%20performance%20compared%0Ato%20the%20alternative%20of%20prefiltering%20data%20for%20each%20learner/use-case%20individually.%0AWe%20explore%20the%20resulting%20utility%20loss%20and%20its%20dependence%20on%20the%20problem%0Aparameters%20via%20extensive%20experiments%20on%20real-world%20image%20and%20tabular%20data%2C%0Aobserving%20statistically%20significant%20reduction%20in%20utility.%20Finally%2C%20we%20model%20the%0Atrade-off%20between%20the%20utility%20drop%20and%20the%20cost%20of%20repeated%20%28learner-specific%29%0Aprefiltering%20within%20a%20game-theoretic%20framework%20and%20showcase%20benefits%20of%20LARP%0Afor%20large%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20573v2&entry.124074799=Read"},
{"title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual\n  Grounding Manipulation", "author": "Ziang Ye and Yang Zhang and Wentao Shi and Xiaoyu You and Fuli Feng and Tat-Seng Chua", "abstract": "  Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents.\n", "link": "http://arxiv.org/abs/2507.06899v1", "date": "2025-07-09", "relevancy": 1.9195, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4992}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4834}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualTrap%3A%20A%20Stealthy%20Backdoor%20Attack%20on%20GUI%20Agents%20via%20Visual%0A%20%20Grounding%20Manipulation&body=Title%3A%20VisualTrap%3A%20A%20Stealthy%20Backdoor%20Attack%20on%20GUI%20Agents%20via%20Visual%0A%20%20Grounding%20Manipulation%0AAuthor%3A%20Ziang%20Ye%20and%20Yang%20Zhang%20and%20Wentao%20Shi%20and%20Xiaoyu%20You%20and%20Fuli%20Feng%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28GUI%29%20agents%20powered%20by%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20have%20emerged%20as%20a%20revolutionary%20approach%20to%20automating%20human-machine%0Ainteractions%2C%20capable%20of%20autonomously%20operating%20personal%20devices%20%28e.g.%2C%20mobile%0Aphones%29%20or%20applications%20within%20the%20device%20to%20perform%20complex%20real-world%20tasks%0Ain%20a%20human-like%20manner.%20However%2C%20their%20close%20integration%20with%20personal%20devices%0Araises%20significant%20security%20concerns%2C%20with%20many%20threats%2C%20including%20backdoor%0Aattacks%2C%20remaining%20largely%20unexplored.%20This%20work%20reveals%20that%20the%20visual%0Agrounding%20of%20GUI%20agent-mapping%20textual%20plans%20to%20GUI%20elements-can%20introduce%0Avulnerabilities%2C%20enabling%20new%20types%20of%20backdoor%20attacks.%20With%20backdoor%20attack%0Atargeting%20visual%20grounding%2C%20the%20agent%27s%20behavior%20can%20be%20compromised%20even%20when%0Agiven%20correct%20task-solving%20plans.%20To%20validate%20this%20vulnerability%2C%20we%20propose%0AVisualTrap%2C%20a%20method%20that%20can%20hijack%20the%20grounding%20by%20misleading%20the%20agent%20to%0Alocate%20textual%20plans%20to%20trigger%20locations%20instead%20of%20the%20intended%20targets.%0AVisualTrap%20uses%20the%20common%20method%20of%20injecting%20poisoned%20data%20for%20attacks%2C%20and%0Adoes%20so%20during%20the%20pre-training%20of%20visual%20grounding%20to%20ensure%20practical%0Afeasibility%20of%20attacking.%20Empirical%20results%20show%20that%20VisualTrap%20can%0Aeffectively%20hijack%20visual%20grounding%20with%20as%20little%20as%205%25%20poisoned%20data%20and%0Ahighly%20stealthy%20visual%20triggers%20%28invisible%20to%20the%20human%20eye%29%3B%20and%20the%20attack%0Acan%20be%20generalized%20to%20downstream%20tasks%2C%20even%20after%20clean%20fine-tuning.%20Moreover%2C%0Athe%20injected%20trigger%20can%20remain%20effective%20across%20different%20GUI%20environments%2C%0Ae.g.%2C%20being%20trained%20on%20mobile/web%20and%20generalizing%20to%20desktop%20environments.%0AThese%20findings%20underscore%20the%20urgent%20need%20for%20further%20research%20on%20backdoor%0Aattack%20risks%20in%20GUI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualTrap%253A%2520A%2520Stealthy%2520Backdoor%2520Attack%2520on%2520GUI%2520Agents%2520via%2520Visual%250A%2520%2520Grounding%2520Manipulation%26entry.906535625%3DZiang%2520Ye%2520and%2520Yang%2520Zhang%2520and%2520Wentao%2520Shi%2520and%2520Xiaoyu%2520You%2520and%2520Fuli%2520Feng%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520powered%2520by%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529%2520have%2520emerged%2520as%2520a%2520revolutionary%2520approach%2520to%2520automating%2520human-machine%250Ainteractions%252C%2520capable%2520of%2520autonomously%2520operating%2520personal%2520devices%2520%2528e.g.%252C%2520mobile%250Aphones%2529%2520or%2520applications%2520within%2520the%2520device%2520to%2520perform%2520complex%2520real-world%2520tasks%250Ain%2520a%2520human-like%2520manner.%2520However%252C%2520their%2520close%2520integration%2520with%2520personal%2520devices%250Araises%2520significant%2520security%2520concerns%252C%2520with%2520many%2520threats%252C%2520including%2520backdoor%250Aattacks%252C%2520remaining%2520largely%2520unexplored.%2520This%2520work%2520reveals%2520that%2520the%2520visual%250Agrounding%2520of%2520GUI%2520agent-mapping%2520textual%2520plans%2520to%2520GUI%2520elements-can%2520introduce%250Avulnerabilities%252C%2520enabling%2520new%2520types%2520of%2520backdoor%2520attacks.%2520With%2520backdoor%2520attack%250Atargeting%2520visual%2520grounding%252C%2520the%2520agent%2527s%2520behavior%2520can%2520be%2520compromised%2520even%2520when%250Agiven%2520correct%2520task-solving%2520plans.%2520To%2520validate%2520this%2520vulnerability%252C%2520we%2520propose%250AVisualTrap%252C%2520a%2520method%2520that%2520can%2520hijack%2520the%2520grounding%2520by%2520misleading%2520the%2520agent%2520to%250Alocate%2520textual%2520plans%2520to%2520trigger%2520locations%2520instead%2520of%2520the%2520intended%2520targets.%250AVisualTrap%2520uses%2520the%2520common%2520method%2520of%2520injecting%2520poisoned%2520data%2520for%2520attacks%252C%2520and%250Adoes%2520so%2520during%2520the%2520pre-training%2520of%2520visual%2520grounding%2520to%2520ensure%2520practical%250Afeasibility%2520of%2520attacking.%2520Empirical%2520results%2520show%2520that%2520VisualTrap%2520can%250Aeffectively%2520hijack%2520visual%2520grounding%2520with%2520as%2520little%2520as%25205%2525%2520poisoned%2520data%2520and%250Ahighly%2520stealthy%2520visual%2520triggers%2520%2528invisible%2520to%2520the%2520human%2520eye%2529%253B%2520and%2520the%2520attack%250Acan%2520be%2520generalized%2520to%2520downstream%2520tasks%252C%2520even%2520after%2520clean%2520fine-tuning.%2520Moreover%252C%250Athe%2520injected%2520trigger%2520can%2520remain%2520effective%2520across%2520different%2520GUI%2520environments%252C%250Ae.g.%252C%2520being%2520trained%2520on%2520mobile/web%2520and%2520generalizing%2520to%2520desktop%2520environments.%250AThese%2520findings%2520underscore%2520the%2520urgent%2520need%2520for%2520further%2520research%2520on%2520backdoor%250Aattack%2520risks%2520in%2520GUI%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualTrap%3A%20A%20Stealthy%20Backdoor%20Attack%20on%20GUI%20Agents%20via%20Visual%0A%20%20Grounding%20Manipulation&entry.906535625=Ziang%20Ye%20and%20Yang%20Zhang%20and%20Wentao%20Shi%20and%20Xiaoyu%20You%20and%20Fuli%20Feng%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Graphical%20User%20Interface%20%28GUI%29%20agents%20powered%20by%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20have%20emerged%20as%20a%20revolutionary%20approach%20to%20automating%20human-machine%0Ainteractions%2C%20capable%20of%20autonomously%20operating%20personal%20devices%20%28e.g.%2C%20mobile%0Aphones%29%20or%20applications%20within%20the%20device%20to%20perform%20complex%20real-world%20tasks%0Ain%20a%20human-like%20manner.%20However%2C%20their%20close%20integration%20with%20personal%20devices%0Araises%20significant%20security%20concerns%2C%20with%20many%20threats%2C%20including%20backdoor%0Aattacks%2C%20remaining%20largely%20unexplored.%20This%20work%20reveals%20that%20the%20visual%0Agrounding%20of%20GUI%20agent-mapping%20textual%20plans%20to%20GUI%20elements-can%20introduce%0Avulnerabilities%2C%20enabling%20new%20types%20of%20backdoor%20attacks.%20With%20backdoor%20attack%0Atargeting%20visual%20grounding%2C%20the%20agent%27s%20behavior%20can%20be%20compromised%20even%20when%0Agiven%20correct%20task-solving%20plans.%20To%20validate%20this%20vulnerability%2C%20we%20propose%0AVisualTrap%2C%20a%20method%20that%20can%20hijack%20the%20grounding%20by%20misleading%20the%20agent%20to%0Alocate%20textual%20plans%20to%20trigger%20locations%20instead%20of%20the%20intended%20targets.%0AVisualTrap%20uses%20the%20common%20method%20of%20injecting%20poisoned%20data%20for%20attacks%2C%20and%0Adoes%20so%20during%20the%20pre-training%20of%20visual%20grounding%20to%20ensure%20practical%0Afeasibility%20of%20attacking.%20Empirical%20results%20show%20that%20VisualTrap%20can%0Aeffectively%20hijack%20visual%20grounding%20with%20as%20little%20as%205%25%20poisoned%20data%20and%0Ahighly%20stealthy%20visual%20triggers%20%28invisible%20to%20the%20human%20eye%29%3B%20and%20the%20attack%0Acan%20be%20generalized%20to%20downstream%20tasks%2C%20even%20after%20clean%20fine-tuning.%20Moreover%2C%0Athe%20injected%20trigger%20can%20remain%20effective%20across%20different%20GUI%20environments%2C%0Ae.g.%2C%20being%20trained%20on%20mobile/web%20and%20generalizing%20to%20desktop%20environments.%0AThese%20findings%20underscore%20the%20urgent%20need%20for%20further%20research%20on%20backdoor%0Aattack%20risks%20in%20GUI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06899v1&entry.124074799=Read"},
{"title": "LLM Agent for Hyper-Parameter Optimization", "author": "Wanzhe Wang and Jianqiu Peng and Menghao Hu and Weihuang Zhong and Tong Zhang and Shuai Wang and Yixin Zhang and Mingjie Shao and Wanli Ni", "abstract": "  Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters optimization\napproaches for Warm-Start Particles Swarm Optimization with Crossover and\nMutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial\nvehicle (UAV) trajectory and communication, are primarily heuristic-based,\nexhibiting low levels of automation and improvable performance. In this paper,\nwe design an Large Language Model (LLM) agent for automatic\nhyper-parameters-tuning, where an iterative framework and Model Context\nProtocol (MCP) are applied. In particular, the LLM agent is first set up via a\nprofile, which specifies the boundary of hyper-parameters, task objective,\nterminal condition, conservative or aggressive strategy of optimizing\nhyper-parameters, and LLM configurations. Then, the LLM agent iteratively\ninvokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the\nloop based on the terminal condition and returns an optimized set of\nhyperparameters. Our experiment results show that the minimal sum-rate achieved\nby hyper-parameters generated via our LLM agent is significantly higher than\nthose by both human heuristics and random generation methods. This indicates\nthat an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in\nseeking high-performance hyper-parameters.\n", "link": "http://arxiv.org/abs/2506.15167v2", "date": "2025-07-09", "relevancy": 1.9136, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4863}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4769}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Agent%20for%20Hyper-Parameter%20Optimization&body=Title%3A%20LLM%20Agent%20for%20Hyper-Parameter%20Optimization%0AAuthor%3A%20Wanzhe%20Wang%20and%20Jianqiu%20Peng%20and%20Menghao%20Hu%20and%20Weihuang%20Zhong%20and%20Tong%20Zhang%20and%20Shuai%20Wang%20and%20Yixin%20Zhang%20and%20Mingjie%20Shao%20and%20Wanli%20Ni%0AAbstract%3A%20%20%20Hyper-parameters%20are%20essential%20and%20critical%20for%20the%20performance%20of%0Acommunication%20algorithms.%20However%2C%20current%20hyper-parameters%20optimization%0Aapproaches%20for%20Warm-Start%20Particles%20Swarm%20Optimization%20with%20Crossover%20and%0AMutation%20%28WS-PSO-CM%29%20algorithm%2C%20designed%20for%20radio%20map-enabled%20unmanned%20aerial%0Avehicle%20%28UAV%29%20trajectory%20and%20communication%2C%20are%20primarily%20heuristic-based%2C%0Aexhibiting%20low%20levels%20of%20automation%20and%20improvable%20performance.%20In%20this%20paper%2C%0Awe%20design%20an%20Large%20Language%20Model%20%28LLM%29%20agent%20for%20automatic%0Ahyper-parameters-tuning%2C%20where%20an%20iterative%20framework%20and%20Model%20Context%0AProtocol%20%28MCP%29%20are%20applied.%20In%20particular%2C%20the%20LLM%20agent%20is%20first%20set%20up%20via%20a%0Aprofile%2C%20which%20specifies%20the%20boundary%20of%20hyper-parameters%2C%20task%20objective%2C%0Aterminal%20condition%2C%20conservative%20or%20aggressive%20strategy%20of%20optimizing%0Ahyper-parameters%2C%20and%20LLM%20configurations.%20Then%2C%20the%20LLM%20agent%20iteratively%0Ainvokes%20WS-PSO-CM%20algorithm%20for%20exploration.%20Finally%2C%20the%20LLM%20agent%20exits%20the%0Aloop%20based%20on%20the%20terminal%20condition%20and%20returns%20an%20optimized%20set%20of%0Ahyperparameters.%20Our%20experiment%20results%20show%20that%20the%20minimal%20sum-rate%20achieved%0Aby%20hyper-parameters%20generated%20via%20our%20LLM%20agent%20is%20significantly%20higher%20than%0Athose%20by%20both%20human%20heuristics%20and%20random%20generation%20methods.%20This%20indicates%0Athat%20an%20LLM%20agent%20with%20PSO%20and%20WS-PSO-CM%20algorithm%20knowledge%20is%20useful%20in%0Aseeking%20high-performance%20hyper-parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15167v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Agent%2520for%2520Hyper-Parameter%2520Optimization%26entry.906535625%3DWanzhe%2520Wang%2520and%2520Jianqiu%2520Peng%2520and%2520Menghao%2520Hu%2520and%2520Weihuang%2520Zhong%2520and%2520Tong%2520Zhang%2520and%2520Shuai%2520Wang%2520and%2520Yixin%2520Zhang%2520and%2520Mingjie%2520Shao%2520and%2520Wanli%2520Ni%26entry.1292438233%3D%2520%2520Hyper-parameters%2520are%2520essential%2520and%2520critical%2520for%2520the%2520performance%2520of%250Acommunication%2520algorithms.%2520However%252C%2520current%2520hyper-parameters%2520optimization%250Aapproaches%2520for%2520Warm-Start%2520Particles%2520Swarm%2520Optimization%2520with%2520Crossover%2520and%250AMutation%2520%2528WS-PSO-CM%2529%2520algorithm%252C%2520designed%2520for%2520radio%2520map-enabled%2520unmanned%2520aerial%250Avehicle%2520%2528UAV%2529%2520trajectory%2520and%2520communication%252C%2520are%2520primarily%2520heuristic-based%252C%250Aexhibiting%2520low%2520levels%2520of%2520automation%2520and%2520improvable%2520performance.%2520In%2520this%2520paper%252C%250Awe%2520design%2520an%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agent%2520for%2520automatic%250Ahyper-parameters-tuning%252C%2520where%2520an%2520iterative%2520framework%2520and%2520Model%2520Context%250AProtocol%2520%2528MCP%2529%2520are%2520applied.%2520In%2520particular%252C%2520the%2520LLM%2520agent%2520is%2520first%2520set%2520up%2520via%2520a%250Aprofile%252C%2520which%2520specifies%2520the%2520boundary%2520of%2520hyper-parameters%252C%2520task%2520objective%252C%250Aterminal%2520condition%252C%2520conservative%2520or%2520aggressive%2520strategy%2520of%2520optimizing%250Ahyper-parameters%252C%2520and%2520LLM%2520configurations.%2520Then%252C%2520the%2520LLM%2520agent%2520iteratively%250Ainvokes%2520WS-PSO-CM%2520algorithm%2520for%2520exploration.%2520Finally%252C%2520the%2520LLM%2520agent%2520exits%2520the%250Aloop%2520based%2520on%2520the%2520terminal%2520condition%2520and%2520returns%2520an%2520optimized%2520set%2520of%250Ahyperparameters.%2520Our%2520experiment%2520results%2520show%2520that%2520the%2520minimal%2520sum-rate%2520achieved%250Aby%2520hyper-parameters%2520generated%2520via%2520our%2520LLM%2520agent%2520is%2520significantly%2520higher%2520than%250Athose%2520by%2520both%2520human%2520heuristics%2520and%2520random%2520generation%2520methods.%2520This%2520indicates%250Athat%2520an%2520LLM%2520agent%2520with%2520PSO%2520and%2520WS-PSO-CM%2520algorithm%2520knowledge%2520is%2520useful%2520in%250Aseeking%2520high-performance%2520hyper-parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15167v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Agent%20for%20Hyper-Parameter%20Optimization&entry.906535625=Wanzhe%20Wang%20and%20Jianqiu%20Peng%20and%20Menghao%20Hu%20and%20Weihuang%20Zhong%20and%20Tong%20Zhang%20and%20Shuai%20Wang%20and%20Yixin%20Zhang%20and%20Mingjie%20Shao%20and%20Wanli%20Ni&entry.1292438233=%20%20Hyper-parameters%20are%20essential%20and%20critical%20for%20the%20performance%20of%0Acommunication%20algorithms.%20However%2C%20current%20hyper-parameters%20optimization%0Aapproaches%20for%20Warm-Start%20Particles%20Swarm%20Optimization%20with%20Crossover%20and%0AMutation%20%28WS-PSO-CM%29%20algorithm%2C%20designed%20for%20radio%20map-enabled%20unmanned%20aerial%0Avehicle%20%28UAV%29%20trajectory%20and%20communication%2C%20are%20primarily%20heuristic-based%2C%0Aexhibiting%20low%20levels%20of%20automation%20and%20improvable%20performance.%20In%20this%20paper%2C%0Awe%20design%20an%20Large%20Language%20Model%20%28LLM%29%20agent%20for%20automatic%0Ahyper-parameters-tuning%2C%20where%20an%20iterative%20framework%20and%20Model%20Context%0AProtocol%20%28MCP%29%20are%20applied.%20In%20particular%2C%20the%20LLM%20agent%20is%20first%20set%20up%20via%20a%0Aprofile%2C%20which%20specifies%20the%20boundary%20of%20hyper-parameters%2C%20task%20objective%2C%0Aterminal%20condition%2C%20conservative%20or%20aggressive%20strategy%20of%20optimizing%0Ahyper-parameters%2C%20and%20LLM%20configurations.%20Then%2C%20the%20LLM%20agent%20iteratively%0Ainvokes%20WS-PSO-CM%20algorithm%20for%20exploration.%20Finally%2C%20the%20LLM%20agent%20exits%20the%0Aloop%20based%20on%20the%20terminal%20condition%20and%20returns%20an%20optimized%20set%20of%0Ahyperparameters.%20Our%20experiment%20results%20show%20that%20the%20minimal%20sum-rate%20achieved%0Aby%20hyper-parameters%20generated%20via%20our%20LLM%20agent%20is%20significantly%20higher%20than%0Athose%20by%20both%20human%20heuristics%20and%20random%20generation%20methods.%20This%20indicates%0Athat%20an%20LLM%20agent%20with%20PSO%20and%20WS-PSO-CM%20algorithm%20knowledge%20is%20useful%20in%0Aseeking%20high-performance%20hyper-parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15167v2&entry.124074799=Read"},
{"title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts", "author": "Hongyu Chen and Seraphina Goldfarb-Tarrant", "abstract": "  Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.\n", "link": "http://arxiv.org/abs/2503.09347v3", "date": "2025-07-09", "relevancy": 1.912, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5338}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safer%20or%20Luckier%3F%20LLMs%20as%20Safety%20Evaluators%20Are%20Not%20Robust%20to%20Artifacts&body=Title%3A%20Safer%20or%20Luckier%3F%20LLMs%20as%20Safety%20Evaluators%20Are%20Not%20Robust%20to%20Artifacts%0AAuthor%3A%20Hongyu%20Chen%20and%20Seraphina%20Goldfarb-Tarrant%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20employed%20as%20automated%0Aevaluators%20to%20assess%20the%20safety%20of%20generated%20content%2C%20yet%20their%20reliability%20in%0Athis%20role%20remains%20uncertain.%20This%20study%20evaluates%20a%20diverse%20set%20of%2011%20LLM%20judge%0Amodels%20across%20critical%20safety%20domains%2C%20examining%20three%20key%20aspects%3A%0Aself-consistency%20in%20repeated%20judging%20tasks%2C%20alignment%20with%20human%20judgments%2C%20and%0Asusceptibility%20to%20input%20artifacts%20such%20as%20apologetic%20or%20verbose%20phrasing.%20Our%0Afindings%20reveal%20that%20biases%20in%20LLM%20judges%20can%20significantly%20distort%20the%20final%0Averdict%20on%20which%20content%20source%20is%20safer%2C%20undermining%20the%20validity%20of%0Acomparative%20evaluations.%20Notably%2C%20apologetic%20language%20artifacts%20alone%20can%20skew%0Aevaluator%20preferences%20by%20up%20to%2098%5C%25.%20Contrary%20to%20expectations%2C%20larger%20models%20do%0Anot%20consistently%20exhibit%20greater%20robustness%2C%20while%20smaller%20models%20sometimes%0Ashow%20higher%20resistance%20to%20specific%20artifacts.%20To%20mitigate%20LLM%20evaluator%0Arobustness%20issues%2C%20we%20investigate%20jury-based%20evaluations%20aggregating%20decisions%0Afrom%20multiple%20models.%20Although%20this%20approach%20both%20improves%20robustness%20and%0Aenhances%20alignment%20to%20human%20judgements%2C%20artifact%20sensitivity%20persists%20even%20with%0Athe%20best%20jury%20configurations.%20These%20results%20highlight%20the%20urgent%20need%20for%0Adiversified%2C%20artifact-resistant%20methodologies%20to%20ensure%20reliable%20safety%0Aassessments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09347v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafer%2520or%2520Luckier%253F%2520LLMs%2520as%2520Safety%2520Evaluators%2520Are%2520Not%2520Robust%2520to%2520Artifacts%26entry.906535625%3DHongyu%2520Chen%2520and%2520Seraphina%2520Goldfarb-Tarrant%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520employed%2520as%2520automated%250Aevaluators%2520to%2520assess%2520the%2520safety%2520of%2520generated%2520content%252C%2520yet%2520their%2520reliability%2520in%250Athis%2520role%2520remains%2520uncertain.%2520This%2520study%2520evaluates%2520a%2520diverse%2520set%2520of%252011%2520LLM%2520judge%250Amodels%2520across%2520critical%2520safety%2520domains%252C%2520examining%2520three%2520key%2520aspects%253A%250Aself-consistency%2520in%2520repeated%2520judging%2520tasks%252C%2520alignment%2520with%2520human%2520judgments%252C%2520and%250Asusceptibility%2520to%2520input%2520artifacts%2520such%2520as%2520apologetic%2520or%2520verbose%2520phrasing.%2520Our%250Afindings%2520reveal%2520that%2520biases%2520in%2520LLM%2520judges%2520can%2520significantly%2520distort%2520the%2520final%250Averdict%2520on%2520which%2520content%2520source%2520is%2520safer%252C%2520undermining%2520the%2520validity%2520of%250Acomparative%2520evaluations.%2520Notably%252C%2520apologetic%2520language%2520artifacts%2520alone%2520can%2520skew%250Aevaluator%2520preferences%2520by%2520up%2520to%252098%255C%2525.%2520Contrary%2520to%2520expectations%252C%2520larger%2520models%2520do%250Anot%2520consistently%2520exhibit%2520greater%2520robustness%252C%2520while%2520smaller%2520models%2520sometimes%250Ashow%2520higher%2520resistance%2520to%2520specific%2520artifacts.%2520To%2520mitigate%2520LLM%2520evaluator%250Arobustness%2520issues%252C%2520we%2520investigate%2520jury-based%2520evaluations%2520aggregating%2520decisions%250Afrom%2520multiple%2520models.%2520Although%2520this%2520approach%2520both%2520improves%2520robustness%2520and%250Aenhances%2520alignment%2520to%2520human%2520judgements%252C%2520artifact%2520sensitivity%2520persists%2520even%2520with%250Athe%2520best%2520jury%2520configurations.%2520These%2520results%2520highlight%2520the%2520urgent%2520need%2520for%250Adiversified%252C%2520artifact-resistant%2520methodologies%2520to%2520ensure%2520reliable%2520safety%250Aassessments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09347v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safer%20or%20Luckier%3F%20LLMs%20as%20Safety%20Evaluators%20Are%20Not%20Robust%20to%20Artifacts&entry.906535625=Hongyu%20Chen%20and%20Seraphina%20Goldfarb-Tarrant&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20employed%20as%20automated%0Aevaluators%20to%20assess%20the%20safety%20of%20generated%20content%2C%20yet%20their%20reliability%20in%0Athis%20role%20remains%20uncertain.%20This%20study%20evaluates%20a%20diverse%20set%20of%2011%20LLM%20judge%0Amodels%20across%20critical%20safety%20domains%2C%20examining%20three%20key%20aspects%3A%0Aself-consistency%20in%20repeated%20judging%20tasks%2C%20alignment%20with%20human%20judgments%2C%20and%0Asusceptibility%20to%20input%20artifacts%20such%20as%20apologetic%20or%20verbose%20phrasing.%20Our%0Afindings%20reveal%20that%20biases%20in%20LLM%20judges%20can%20significantly%20distort%20the%20final%0Averdict%20on%20which%20content%20source%20is%20safer%2C%20undermining%20the%20validity%20of%0Acomparative%20evaluations.%20Notably%2C%20apologetic%20language%20artifacts%20alone%20can%20skew%0Aevaluator%20preferences%20by%20up%20to%2098%5C%25.%20Contrary%20to%20expectations%2C%20larger%20models%20do%0Anot%20consistently%20exhibit%20greater%20robustness%2C%20while%20smaller%20models%20sometimes%0Ashow%20higher%20resistance%20to%20specific%20artifacts.%20To%20mitigate%20LLM%20evaluator%0Arobustness%20issues%2C%20we%20investigate%20jury-based%20evaluations%20aggregating%20decisions%0Afrom%20multiple%20models.%20Although%20this%20approach%20both%20improves%20robustness%20and%0Aenhances%20alignment%20to%20human%20judgements%2C%20artifact%20sensitivity%20persists%20even%20with%0Athe%20best%20jury%20configurations.%20These%20results%20highlight%20the%20urgent%20need%20for%0Adiversified%2C%20artifact-resistant%20methodologies%20to%20ensure%20reliable%20safety%0Aassessments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09347v3&entry.124074799=Read"},
{"title": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology\n  Report Generation", "author": "Qilong Xing and Zikai Song and Youjia Zhang and Na Feng and Junqing Yu and Wei Yang", "abstract": "  Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation.\n", "link": "http://arxiv.org/abs/2507.06992v1", "date": "2025-07-09", "relevancy": 1.9021, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4827}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCA-RG%3A%20Enhancing%20LLMs%20with%20Medical%20Concept%20Alignment%20for%20Radiology%0A%20%20Report%20Generation&body=Title%3A%20MCA-RG%3A%20Enhancing%20LLMs%20with%20Medical%20Concept%20Alignment%20for%20Radiology%0A%20%20Report%20Generation%0AAuthor%3A%20Qilong%20Xing%20and%20Zikai%20Song%20and%20Youjia%20Zhang%20and%20Na%20Feng%20and%20Junqing%20Yu%20and%20Wei%20Yang%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20adapting%20Large%20Language%20Models%20%28LLMs%29%20for%0Aradiology%20report%20generation%20%28RRG%29%2C%20clinical%20adoption%20remains%20challenging%20due%20to%0Adifficulties%20in%20accurately%20mapping%20pathological%20and%20anatomical%20features%20to%0Atheir%20corresponding%20text%20descriptions.%20Additionally%2C%20semantic%20agnostic%20feature%0Aextraction%20further%20hampers%20the%20generation%20of%20accurate%20diagnostic%20reports.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20Medical%20Concept%20Aligned%20Radiology%20Report%0AGeneration%20%28MCA-RG%29%2C%20a%20knowledge-driven%20framework%20that%20explicitly%20aligns%20visual%0Afeatures%20with%20distinct%20medical%20concepts%20to%20enhance%20the%20report%20generation%0Aprocess.%20MCA-RG%20utilizes%20two%20curated%20concept%20banks%3A%20a%20pathology%20bank%20containing%0Alesion-related%20knowledge%2C%20and%20an%20anatomy%20bank%20with%20anatomical%20descriptions.%20The%0Avisual%20features%20are%20aligned%20with%20these%20medical%20concepts%20and%20undergo%20tailored%0Aenhancement.%20We%20further%20propose%20an%20anatomy-based%20contrastive%20learning%20procedure%0Ato%20improve%20the%20generalization%20of%20anatomical%20features%2C%20coupled%20with%20a%20matching%0Aloss%20for%20pathological%20features%20to%20prioritize%20clinically%20relevant%20regions.%0AAdditionally%2C%20a%20feature%20gating%20mechanism%20is%20employed%20to%20filter%20out%20low-quality%0Aconcept%20features.%20Finally%2C%20the%20visual%20features%20are%20corresponding%20to%20individual%0Amedical%20concepts%2C%20and%20are%20leveraged%20to%20guide%20the%20report%20generation%20process.%0AExperiments%20on%20two%20public%20benchmarks%20%28MIMIC-CXR%20and%20CheXpert%20Plus%29%20demonstrate%0Athat%20MCA-RG%20achieves%20superior%20performance%2C%20highlighting%20its%20effectiveness%20in%0Aradiology%20report%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCA-RG%253A%2520Enhancing%2520LLMs%2520with%2520Medical%2520Concept%2520Alignment%2520for%2520Radiology%250A%2520%2520Report%2520Generation%26entry.906535625%3DQilong%2520Xing%2520and%2520Zikai%2520Song%2520and%2520Youjia%2520Zhang%2520and%2520Na%2520Feng%2520and%2520Junqing%2520Yu%2520and%2520Wei%2520Yang%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520adapting%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%250Aradiology%2520report%2520generation%2520%2528RRG%2529%252C%2520clinical%2520adoption%2520remains%2520challenging%2520due%2520to%250Adifficulties%2520in%2520accurately%2520mapping%2520pathological%2520and%2520anatomical%2520features%2520to%250Atheir%2520corresponding%2520text%2520descriptions.%2520Additionally%252C%2520semantic%2520agnostic%2520feature%250Aextraction%2520further%2520hampers%2520the%2520generation%2520of%2520accurate%2520diagnostic%2520reports.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520Medical%2520Concept%2520Aligned%2520Radiology%2520Report%250AGeneration%2520%2528MCA-RG%2529%252C%2520a%2520knowledge-driven%2520framework%2520that%2520explicitly%2520aligns%2520visual%250Afeatures%2520with%2520distinct%2520medical%2520concepts%2520to%2520enhance%2520the%2520report%2520generation%250Aprocess.%2520MCA-RG%2520utilizes%2520two%2520curated%2520concept%2520banks%253A%2520a%2520pathology%2520bank%2520containing%250Alesion-related%2520knowledge%252C%2520and%2520an%2520anatomy%2520bank%2520with%2520anatomical%2520descriptions.%2520The%250Avisual%2520features%2520are%2520aligned%2520with%2520these%2520medical%2520concepts%2520and%2520undergo%2520tailored%250Aenhancement.%2520We%2520further%2520propose%2520an%2520anatomy-based%2520contrastive%2520learning%2520procedure%250Ato%2520improve%2520the%2520generalization%2520of%2520anatomical%2520features%252C%2520coupled%2520with%2520a%2520matching%250Aloss%2520for%2520pathological%2520features%2520to%2520prioritize%2520clinically%2520relevant%2520regions.%250AAdditionally%252C%2520a%2520feature%2520gating%2520mechanism%2520is%2520employed%2520to%2520filter%2520out%2520low-quality%250Aconcept%2520features.%2520Finally%252C%2520the%2520visual%2520features%2520are%2520corresponding%2520to%2520individual%250Amedical%2520concepts%252C%2520and%2520are%2520leveraged%2520to%2520guide%2520the%2520report%2520generation%2520process.%250AExperiments%2520on%2520two%2520public%2520benchmarks%2520%2528MIMIC-CXR%2520and%2520CheXpert%2520Plus%2529%2520demonstrate%250Athat%2520MCA-RG%2520achieves%2520superior%2520performance%252C%2520highlighting%2520its%2520effectiveness%2520in%250Aradiology%2520report%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCA-RG%3A%20Enhancing%20LLMs%20with%20Medical%20Concept%20Alignment%20for%20Radiology%0A%20%20Report%20Generation&entry.906535625=Qilong%20Xing%20and%20Zikai%20Song%20and%20Youjia%20Zhang%20and%20Na%20Feng%20and%20Junqing%20Yu%20and%20Wei%20Yang&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20adapting%20Large%20Language%20Models%20%28LLMs%29%20for%0Aradiology%20report%20generation%20%28RRG%29%2C%20clinical%20adoption%20remains%20challenging%20due%20to%0Adifficulties%20in%20accurately%20mapping%20pathological%20and%20anatomical%20features%20to%0Atheir%20corresponding%20text%20descriptions.%20Additionally%2C%20semantic%20agnostic%20feature%0Aextraction%20further%20hampers%20the%20generation%20of%20accurate%20diagnostic%20reports.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20Medical%20Concept%20Aligned%20Radiology%20Report%0AGeneration%20%28MCA-RG%29%2C%20a%20knowledge-driven%20framework%20that%20explicitly%20aligns%20visual%0Afeatures%20with%20distinct%20medical%20concepts%20to%20enhance%20the%20report%20generation%0Aprocess.%20MCA-RG%20utilizes%20two%20curated%20concept%20banks%3A%20a%20pathology%20bank%20containing%0Alesion-related%20knowledge%2C%20and%20an%20anatomy%20bank%20with%20anatomical%20descriptions.%20The%0Avisual%20features%20are%20aligned%20with%20these%20medical%20concepts%20and%20undergo%20tailored%0Aenhancement.%20We%20further%20propose%20an%20anatomy-based%20contrastive%20learning%20procedure%0Ato%20improve%20the%20generalization%20of%20anatomical%20features%2C%20coupled%20with%20a%20matching%0Aloss%20for%20pathological%20features%20to%20prioritize%20clinically%20relevant%20regions.%0AAdditionally%2C%20a%20feature%20gating%20mechanism%20is%20employed%20to%20filter%20out%20low-quality%0Aconcept%20features.%20Finally%2C%20the%20visual%20features%20are%20corresponding%20to%20individual%0Amedical%20concepts%2C%20and%20are%20leveraged%20to%20guide%20the%20report%20generation%20process.%0AExperiments%20on%20two%20public%20benchmarks%20%28MIMIC-CXR%20and%20CheXpert%20Plus%29%20demonstrate%0Athat%20MCA-RG%20achieves%20superior%20performance%2C%20highlighting%20its%20effectiveness%20in%0Aradiology%20report%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06992v1&entry.124074799=Read"},
{"title": "Near-Optimal Consistency-Robustness Trade-Offs for Learning-Augmented\n  Online Knapsack Problems", "author": "Mohammadreza Daneshvaramoli and Helia Karisani and Adam Lechowicz and Bo Sun and Cameron Musco and Mohammad Hajiesmaili", "abstract": "  This paper introduces a family of learning-augmented algorithms for online\nknapsack problems that achieve near Pareto-optimal consistency-robustness\ntrade-offs through a simple combination of trusted learning-augmented and\nworst-case algorithms. Our approach relies on succinct, practical predictions\n-- single values or intervals estimating the minimum value of any item in an\noffline solution. Additionally, we propose a novel fractional-to-integral\nconversion procedure, offering new insights for online algorithm design.\n", "link": "http://arxiv.org/abs/2406.18752v2", "date": "2025-07-09", "relevancy": 1.8967, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5024}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4772}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-Optimal%20Consistency-Robustness%20Trade-Offs%20for%20Learning-Augmented%0A%20%20Online%20Knapsack%20Problems&body=Title%3A%20Near-Optimal%20Consistency-Robustness%20Trade-Offs%20for%20Learning-Augmented%0A%20%20Online%20Knapsack%20Problems%0AAuthor%3A%20Mohammadreza%20Daneshvaramoli%20and%20Helia%20Karisani%20and%20Adam%20Lechowicz%20and%20Bo%20Sun%20and%20Cameron%20Musco%20and%20Mohammad%20Hajiesmaili%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20family%20of%20learning-augmented%20algorithms%20for%20online%0Aknapsack%20problems%20that%20achieve%20near%20Pareto-optimal%20consistency-robustness%0Atrade-offs%20through%20a%20simple%20combination%20of%20trusted%20learning-augmented%20and%0Aworst-case%20algorithms.%20Our%20approach%20relies%20on%20succinct%2C%20practical%20predictions%0A--%20single%20values%20or%20intervals%20estimating%20the%20minimum%20value%20of%20any%20item%20in%20an%0Aoffline%20solution.%20Additionally%2C%20we%20propose%20a%20novel%20fractional-to-integral%0Aconversion%20procedure%2C%20offering%20new%20insights%20for%20online%20algorithm%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18752v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-Optimal%2520Consistency-Robustness%2520Trade-Offs%2520for%2520Learning-Augmented%250A%2520%2520Online%2520Knapsack%2520Problems%26entry.906535625%3DMohammadreza%2520Daneshvaramoli%2520and%2520Helia%2520Karisani%2520and%2520Adam%2520Lechowicz%2520and%2520Bo%2520Sun%2520and%2520Cameron%2520Musco%2520and%2520Mohammad%2520Hajiesmaili%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520family%2520of%2520learning-augmented%2520algorithms%2520for%2520online%250Aknapsack%2520problems%2520that%2520achieve%2520near%2520Pareto-optimal%2520consistency-robustness%250Atrade-offs%2520through%2520a%2520simple%2520combination%2520of%2520trusted%2520learning-augmented%2520and%250Aworst-case%2520algorithms.%2520Our%2520approach%2520relies%2520on%2520succinct%252C%2520practical%2520predictions%250A--%2520single%2520values%2520or%2520intervals%2520estimating%2520the%2520minimum%2520value%2520of%2520any%2520item%2520in%2520an%250Aoffline%2520solution.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520fractional-to-integral%250Aconversion%2520procedure%252C%2520offering%2520new%2520insights%2520for%2520online%2520algorithm%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18752v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-Optimal%20Consistency-Robustness%20Trade-Offs%20for%20Learning-Augmented%0A%20%20Online%20Knapsack%20Problems&entry.906535625=Mohammadreza%20Daneshvaramoli%20and%20Helia%20Karisani%20and%20Adam%20Lechowicz%20and%20Bo%20Sun%20and%20Cameron%20Musco%20and%20Mohammad%20Hajiesmaili&entry.1292438233=%20%20This%20paper%20introduces%20a%20family%20of%20learning-augmented%20algorithms%20for%20online%0Aknapsack%20problems%20that%20achieve%20near%20Pareto-optimal%20consistency-robustness%0Atrade-offs%20through%20a%20simple%20combination%20of%20trusted%20learning-augmented%20and%0Aworst-case%20algorithms.%20Our%20approach%20relies%20on%20succinct%2C%20practical%20predictions%0A--%20single%20values%20or%20intervals%20estimating%20the%20minimum%20value%20of%20any%20item%20in%20an%0Aoffline%20solution.%20Additionally%2C%20we%20propose%20a%20novel%20fractional-to-integral%0Aconversion%20procedure%2C%20offering%20new%20insights%20for%20online%20algorithm%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18752v2&entry.124074799=Read"},
{"title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor", "author": "Vatsal Agarwal and Matthew Gwilliam and Gefen Kohavi and Eshan Verma and Daniel Ulbricht and Abhinav Shrivastava", "abstract": "  Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/.\n", "link": "http://arxiv.org/abs/2507.07106v1", "date": "2025-07-09", "relevancy": 1.8894, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6479}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6364}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Multimodal%20Understanding%20via%20Stable%20Diffusion%20as%20a%20Task-Aware%0A%20%20Feature%20Extractor&body=Title%3A%20Towards%20Multimodal%20Understanding%20via%20Stable%20Diffusion%20as%20a%20Task-Aware%0A%20%20Feature%20Extractor%0AAuthor%3A%20Vatsal%20Agarwal%20and%20Matthew%20Gwilliam%20and%20Gefen%20Kohavi%20and%20Eshan%20Verma%20and%20Daniel%20Ulbricht%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%0Aimage-based%20question-answering%20capabilities.%20However%2C%20a%20key%20limitation%20is%20the%0Ause%20of%20CLIP%20as%20the%20visual%20encoder%3B%20while%20it%20can%20capture%20coarse%20global%0Ainformation%2C%20it%20often%20can%20miss%20fine-grained%20details%20that%20are%20relevant%20to%20the%0Ainput%20query.%20To%20address%20these%20shortcomings%2C%20this%20work%20studies%20whether%0Apre-trained%20text-to-image%20diffusion%20models%20can%20serve%20as%20instruction-aware%0Avisual%20encoders.%20Through%20an%20analysis%20of%20their%20internal%20representations%2C%20we%20find%0Adiffusion%20features%20are%20both%20rich%20in%20semantics%20and%20can%20encode%20strong%20image-text%0Aalignment.%20Moreover%2C%20we%20find%20that%20we%20can%20leverage%20text%20conditioning%20to%20focus%0Athe%20model%20on%20regions%20relevant%20to%20the%20input%20question.%20We%20then%20investigate%20how%20to%0Aalign%20these%20features%20with%20large%20language%20models%20and%20uncover%20a%20leakage%0Aphenomenon%2C%20where%20the%20LLM%20can%20inadvertently%20recover%20information%20from%20the%0Aoriginal%20diffusion%20prompt.%20We%20analyze%20the%20causes%20of%20this%20leakage%20and%20propose%20a%0Amitigation%20strategy.%20Based%20on%20these%20insights%2C%20we%20explore%20a%20simple%20fusion%0Astrategy%20that%20utilizes%20both%20CLIP%20and%20conditional%20diffusion%20features.%20We%0Aevaluate%20our%20approach%20on%20both%20general%20VQA%20and%20specialized%20MLLM%20benchmarks%2C%0Ademonstrating%20the%20promise%20of%20diffusion%20models%20for%20visual%20understanding%2C%0Aparticularly%20in%20vision-centric%20tasks%20that%20require%20spatial%20and%20compositional%0Areasoning.%20Our%20project%20page%20can%20be%20found%0Ahttps%3A//vatsalag99.github.io/mustafar/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Multimodal%2520Understanding%2520via%2520Stable%2520Diffusion%2520as%2520a%2520Task-Aware%250A%2520%2520Feature%2520Extractor%26entry.906535625%3DVatsal%2520Agarwal%2520and%2520Matthew%2520Gwilliam%2520and%2520Gefen%2520Kohavi%2520and%2520Eshan%2520Verma%2520and%2520Daniel%2520Ulbricht%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520enabled%250Aimage-based%2520question-answering%2520capabilities.%2520However%252C%2520a%2520key%2520limitation%2520is%2520the%250Ause%2520of%2520CLIP%2520as%2520the%2520visual%2520encoder%253B%2520while%2520it%2520can%2520capture%2520coarse%2520global%250Ainformation%252C%2520it%2520often%2520can%2520miss%2520fine-grained%2520details%2520that%2520are%2520relevant%2520to%2520the%250Ainput%2520query.%2520To%2520address%2520these%2520shortcomings%252C%2520this%2520work%2520studies%2520whether%250Apre-trained%2520text-to-image%2520diffusion%2520models%2520can%2520serve%2520as%2520instruction-aware%250Avisual%2520encoders.%2520Through%2520an%2520analysis%2520of%2520their%2520internal%2520representations%252C%2520we%2520find%250Adiffusion%2520features%2520are%2520both%2520rich%2520in%2520semantics%2520and%2520can%2520encode%2520strong%2520image-text%250Aalignment.%2520Moreover%252C%2520we%2520find%2520that%2520we%2520can%2520leverage%2520text%2520conditioning%2520to%2520focus%250Athe%2520model%2520on%2520regions%2520relevant%2520to%2520the%2520input%2520question.%2520We%2520then%2520investigate%2520how%2520to%250Aalign%2520these%2520features%2520with%2520large%2520language%2520models%2520and%2520uncover%2520a%2520leakage%250Aphenomenon%252C%2520where%2520the%2520LLM%2520can%2520inadvertently%2520recover%2520information%2520from%2520the%250Aoriginal%2520diffusion%2520prompt.%2520We%2520analyze%2520the%2520causes%2520of%2520this%2520leakage%2520and%2520propose%2520a%250Amitigation%2520strategy.%2520Based%2520on%2520these%2520insights%252C%2520we%2520explore%2520a%2520simple%2520fusion%250Astrategy%2520that%2520utilizes%2520both%2520CLIP%2520and%2520conditional%2520diffusion%2520features.%2520We%250Aevaluate%2520our%2520approach%2520on%2520both%2520general%2520VQA%2520and%2520specialized%2520MLLM%2520benchmarks%252C%250Ademonstrating%2520the%2520promise%2520of%2520diffusion%2520models%2520for%2520visual%2520understanding%252C%250Aparticularly%2520in%2520vision-centric%2520tasks%2520that%2520require%2520spatial%2520and%2520compositional%250Areasoning.%2520Our%2520project%2520page%2520can%2520be%2520found%250Ahttps%253A//vatsalag99.github.io/mustafar/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Multimodal%20Understanding%20via%20Stable%20Diffusion%20as%20a%20Task-Aware%0A%20%20Feature%20Extractor&entry.906535625=Vatsal%20Agarwal%20and%20Matthew%20Gwilliam%20and%20Gefen%20Kohavi%20and%20Eshan%20Verma%20and%20Daniel%20Ulbricht%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%0Aimage-based%20question-answering%20capabilities.%20However%2C%20a%20key%20limitation%20is%20the%0Ause%20of%20CLIP%20as%20the%20visual%20encoder%3B%20while%20it%20can%20capture%20coarse%20global%0Ainformation%2C%20it%20often%20can%20miss%20fine-grained%20details%20that%20are%20relevant%20to%20the%0Ainput%20query.%20To%20address%20these%20shortcomings%2C%20this%20work%20studies%20whether%0Apre-trained%20text-to-image%20diffusion%20models%20can%20serve%20as%20instruction-aware%0Avisual%20encoders.%20Through%20an%20analysis%20of%20their%20internal%20representations%2C%20we%20find%0Adiffusion%20features%20are%20both%20rich%20in%20semantics%20and%20can%20encode%20strong%20image-text%0Aalignment.%20Moreover%2C%20we%20find%20that%20we%20can%20leverage%20text%20conditioning%20to%20focus%0Athe%20model%20on%20regions%20relevant%20to%20the%20input%20question.%20We%20then%20investigate%20how%20to%0Aalign%20these%20features%20with%20large%20language%20models%20and%20uncover%20a%20leakage%0Aphenomenon%2C%20where%20the%20LLM%20can%20inadvertently%20recover%20information%20from%20the%0Aoriginal%20diffusion%20prompt.%20We%20analyze%20the%20causes%20of%20this%20leakage%20and%20propose%20a%0Amitigation%20strategy.%20Based%20on%20these%20insights%2C%20we%20explore%20a%20simple%20fusion%0Astrategy%20that%20utilizes%20both%20CLIP%20and%20conditional%20diffusion%20features.%20We%0Aevaluate%20our%20approach%20on%20both%20general%20VQA%20and%20specialized%20MLLM%20benchmarks%2C%0Ademonstrating%20the%20promise%20of%20diffusion%20models%20for%20visual%20understanding%2C%0Aparticularly%20in%20vision-centric%20tasks%20that%20require%20spatial%20and%20compositional%0Areasoning.%20Our%20project%20page%20can%20be%20found%0Ahttps%3A//vatsalag99.github.io/mustafar/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07106v1&entry.124074799=Read"},
{"title": "Noise tolerance via reinforcement: Learning a reinforced quantum\n  dynamics", "author": "Abolfazl Ramezanpour", "abstract": "  The performance of quantum simulations heavily depends on the efficiency of\nnoise mitigation techniques and error correction algorithms. Reinforcement has\nemerged as a powerful strategy to enhance the efficiency of learning and\noptimization algorithms. In this study, we demonstrate that a reinforced\nquantum dynamics can exhibit significant robustness against interactions with a\nnoisy environment. We study a quantum annealing process where, through\nreinforcement, the system is encouraged to maintain its current state or follow\na noise-free evolution. A learning algorithm is employed to derive a concise\napproximation of this reinforced dynamics, reducing the total evolution time\nand, consequently, the system's exposure to noisy interactions. This also\navoids the complexities associated with implementing quantum feedback in such\nreinforcement algorithms. The efficacy of our method is demonstrated through\nnumerical simulations of reinforced quantum annealing with one- and two-qubit\nsystems under Pauli noise.\n", "link": "http://arxiv.org/abs/2506.12418v2", "date": "2025-07-09", "relevancy": 1.8847, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5215}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4382}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise%20tolerance%20via%20reinforcement%3A%20Learning%20a%20reinforced%20quantum%0A%20%20dynamics&body=Title%3A%20Noise%20tolerance%20via%20reinforcement%3A%20Learning%20a%20reinforced%20quantum%0A%20%20dynamics%0AAuthor%3A%20Abolfazl%20Ramezanpour%0AAbstract%3A%20%20%20The%20performance%20of%20quantum%20simulations%20heavily%20depends%20on%20the%20efficiency%20of%0Anoise%20mitigation%20techniques%20and%20error%20correction%20algorithms.%20Reinforcement%20has%0Aemerged%20as%20a%20powerful%20strategy%20to%20enhance%20the%20efficiency%20of%20learning%20and%0Aoptimization%20algorithms.%20In%20this%20study%2C%20we%20demonstrate%20that%20a%20reinforced%0Aquantum%20dynamics%20can%20exhibit%20significant%20robustness%20against%20interactions%20with%20a%0Anoisy%20environment.%20We%20study%20a%20quantum%20annealing%20process%20where%2C%20through%0Areinforcement%2C%20the%20system%20is%20encouraged%20to%20maintain%20its%20current%20state%20or%20follow%0Aa%20noise-free%20evolution.%20A%20learning%20algorithm%20is%20employed%20to%20derive%20a%20concise%0Aapproximation%20of%20this%20reinforced%20dynamics%2C%20reducing%20the%20total%20evolution%20time%0Aand%2C%20consequently%2C%20the%20system%27s%20exposure%20to%20noisy%20interactions.%20This%20also%0Aavoids%20the%20complexities%20associated%20with%20implementing%20quantum%20feedback%20in%20such%0Areinforcement%20algorithms.%20The%20efficacy%20of%20our%20method%20is%20demonstrated%20through%0Anumerical%20simulations%20of%20reinforced%20quantum%20annealing%20with%20one-%20and%20two-qubit%0Asystems%20under%20Pauli%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12418v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise%2520tolerance%2520via%2520reinforcement%253A%2520Learning%2520a%2520reinforced%2520quantum%250A%2520%2520dynamics%26entry.906535625%3DAbolfazl%2520Ramezanpour%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520quantum%2520simulations%2520heavily%2520depends%2520on%2520the%2520efficiency%2520of%250Anoise%2520mitigation%2520techniques%2520and%2520error%2520correction%2520algorithms.%2520Reinforcement%2520has%250Aemerged%2520as%2520a%2520powerful%2520strategy%2520to%2520enhance%2520the%2520efficiency%2520of%2520learning%2520and%250Aoptimization%2520algorithms.%2520In%2520this%2520study%252C%2520we%2520demonstrate%2520that%2520a%2520reinforced%250Aquantum%2520dynamics%2520can%2520exhibit%2520significant%2520robustness%2520against%2520interactions%2520with%2520a%250Anoisy%2520environment.%2520We%2520study%2520a%2520quantum%2520annealing%2520process%2520where%252C%2520through%250Areinforcement%252C%2520the%2520system%2520is%2520encouraged%2520to%2520maintain%2520its%2520current%2520state%2520or%2520follow%250Aa%2520noise-free%2520evolution.%2520A%2520learning%2520algorithm%2520is%2520employed%2520to%2520derive%2520a%2520concise%250Aapproximation%2520of%2520this%2520reinforced%2520dynamics%252C%2520reducing%2520the%2520total%2520evolution%2520time%250Aand%252C%2520consequently%252C%2520the%2520system%2527s%2520exposure%2520to%2520noisy%2520interactions.%2520This%2520also%250Aavoids%2520the%2520complexities%2520associated%2520with%2520implementing%2520quantum%2520feedback%2520in%2520such%250Areinforcement%2520algorithms.%2520The%2520efficacy%2520of%2520our%2520method%2520is%2520demonstrated%2520through%250Anumerical%2520simulations%2520of%2520reinforced%2520quantum%2520annealing%2520with%2520one-%2520and%2520two-qubit%250Asystems%2520under%2520Pauli%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12418v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise%20tolerance%20via%20reinforcement%3A%20Learning%20a%20reinforced%20quantum%0A%20%20dynamics&entry.906535625=Abolfazl%20Ramezanpour&entry.1292438233=%20%20The%20performance%20of%20quantum%20simulations%20heavily%20depends%20on%20the%20efficiency%20of%0Anoise%20mitigation%20techniques%20and%20error%20correction%20algorithms.%20Reinforcement%20has%0Aemerged%20as%20a%20powerful%20strategy%20to%20enhance%20the%20efficiency%20of%20learning%20and%0Aoptimization%20algorithms.%20In%20this%20study%2C%20we%20demonstrate%20that%20a%20reinforced%0Aquantum%20dynamics%20can%20exhibit%20significant%20robustness%20against%20interactions%20with%20a%0Anoisy%20environment.%20We%20study%20a%20quantum%20annealing%20process%20where%2C%20through%0Areinforcement%2C%20the%20system%20is%20encouraged%20to%20maintain%20its%20current%20state%20or%20follow%0Aa%20noise-free%20evolution.%20A%20learning%20algorithm%20is%20employed%20to%20derive%20a%20concise%0Aapproximation%20of%20this%20reinforced%20dynamics%2C%20reducing%20the%20total%20evolution%20time%0Aand%2C%20consequently%2C%20the%20system%27s%20exposure%20to%20noisy%20interactions.%20This%20also%0Aavoids%20the%20complexities%20associated%20with%20implementing%20quantum%20feedback%20in%20such%0Areinforcement%20algorithms.%20The%20efficacy%20of%20our%20method%20is%20demonstrated%20through%0Anumerical%20simulations%20of%20reinforced%20quantum%20annealing%20with%20one-%20and%20two-qubit%0Asystems%20under%20Pauli%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12418v2&entry.124074799=Read"},
{"title": "GreenHyperSpectra: A multi-source hyperspectral dataset for global\n  vegetation trait prediction", "author": "Eya Cherif and Arthur Ouaknine and Luke A. Brown and Phuong D. Dao and Kyle R. Kovach and Bing Lu and Daniel Mederer and Hannes Feilhauer and Teja Kattenborn and David Rolnick", "abstract": "  Plant traits such as leaf carbon content and leaf mass are essential\nvariables in the study of biodiversity and climate change. However,\nconventional field sampling cannot feasibly cover trait variation at\necologically meaningful spatial scales. Machine learning represents a valuable\nsolution for plant trait prediction across ecosystems, leveraging hyperspectral\ndata from remote sensing. Nevertheless, trait prediction from hyperspectral\ndata is challenged by label scarcity and substantial domain shifts (\\eg across\nsensors, ecological distributions), requiring robust cross-domain methods.\nHere, we present GreenHyperSpectra, a pretraining dataset encompassing\nreal-world cross-sensor and cross-ecosystem samples designed to benchmark trait\nprediction with semi- and self-supervised methods. We adopt an evaluation\nframework encompassing in-distribution and out-of-distribution scenarios. We\nsuccessfully leverage GreenHyperSpectra to pretrain label-efficient\nmulti-output regression models that outperform the state-of-the-art supervised\nbaseline. Our empirical analyses demonstrate substantial improvements in\nlearning spectral representations for trait prediction, establishing a\ncomprehensive methodological framework to catalyze research at the intersection\nof representation learning and plant functional traits assessment. All code and\ndata are available at: https://github.com/echerif18/HyspectraSSL.\n", "link": "http://arxiv.org/abs/2507.06806v1", "date": "2025-07-09", "relevancy": 1.882, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4821}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4684}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GreenHyperSpectra%3A%20A%20multi-source%20hyperspectral%20dataset%20for%20global%0A%20%20vegetation%20trait%20prediction&body=Title%3A%20GreenHyperSpectra%3A%20A%20multi-source%20hyperspectral%20dataset%20for%20global%0A%20%20vegetation%20trait%20prediction%0AAuthor%3A%20Eya%20Cherif%20and%20Arthur%20Ouaknine%20and%20Luke%20A.%20Brown%20and%20Phuong%20D.%20Dao%20and%20Kyle%20R.%20Kovach%20and%20Bing%20Lu%20and%20Daniel%20Mederer%20and%20Hannes%20Feilhauer%20and%20Teja%20Kattenborn%20and%20David%20Rolnick%0AAbstract%3A%20%20%20Plant%20traits%20such%20as%20leaf%20carbon%20content%20and%20leaf%20mass%20are%20essential%0Avariables%20in%20the%20study%20of%20biodiversity%20and%20climate%20change.%20However%2C%0Aconventional%20field%20sampling%20cannot%20feasibly%20cover%20trait%20variation%20at%0Aecologically%20meaningful%20spatial%20scales.%20Machine%20learning%20represents%20a%20valuable%0Asolution%20for%20plant%20trait%20prediction%20across%20ecosystems%2C%20leveraging%20hyperspectral%0Adata%20from%20remote%20sensing.%20Nevertheless%2C%20trait%20prediction%20from%20hyperspectral%0Adata%20is%20challenged%20by%20label%20scarcity%20and%20substantial%20domain%20shifts%20%28%5Ceg%20across%0Asensors%2C%20ecological%20distributions%29%2C%20requiring%20robust%20cross-domain%20methods.%0AHere%2C%20we%20present%20GreenHyperSpectra%2C%20a%20pretraining%20dataset%20encompassing%0Areal-world%20cross-sensor%20and%20cross-ecosystem%20samples%20designed%20to%20benchmark%20trait%0Aprediction%20with%20semi-%20and%20self-supervised%20methods.%20We%20adopt%20an%20evaluation%0Aframework%20encompassing%20in-distribution%20and%20out-of-distribution%20scenarios.%20We%0Asuccessfully%20leverage%20GreenHyperSpectra%20to%20pretrain%20label-efficient%0Amulti-output%20regression%20models%20that%20outperform%20the%20state-of-the-art%20supervised%0Abaseline.%20Our%20empirical%20analyses%20demonstrate%20substantial%20improvements%20in%0Alearning%20spectral%20representations%20for%20trait%20prediction%2C%20establishing%20a%0Acomprehensive%20methodological%20framework%20to%20catalyze%20research%20at%20the%20intersection%0Aof%20representation%20learning%20and%20plant%20functional%20traits%20assessment.%20All%20code%20and%0Adata%20are%20available%20at%3A%20https%3A//github.com/echerif18/HyspectraSSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreenHyperSpectra%253A%2520A%2520multi-source%2520hyperspectral%2520dataset%2520for%2520global%250A%2520%2520vegetation%2520trait%2520prediction%26entry.906535625%3DEya%2520Cherif%2520and%2520Arthur%2520Ouaknine%2520and%2520Luke%2520A.%2520Brown%2520and%2520Phuong%2520D.%2520Dao%2520and%2520Kyle%2520R.%2520Kovach%2520and%2520Bing%2520Lu%2520and%2520Daniel%2520Mederer%2520and%2520Hannes%2520Feilhauer%2520and%2520Teja%2520Kattenborn%2520and%2520David%2520Rolnick%26entry.1292438233%3D%2520%2520Plant%2520traits%2520such%2520as%2520leaf%2520carbon%2520content%2520and%2520leaf%2520mass%2520are%2520essential%250Avariables%2520in%2520the%2520study%2520of%2520biodiversity%2520and%2520climate%2520change.%2520However%252C%250Aconventional%2520field%2520sampling%2520cannot%2520feasibly%2520cover%2520trait%2520variation%2520at%250Aecologically%2520meaningful%2520spatial%2520scales.%2520Machine%2520learning%2520represents%2520a%2520valuable%250Asolution%2520for%2520plant%2520trait%2520prediction%2520across%2520ecosystems%252C%2520leveraging%2520hyperspectral%250Adata%2520from%2520remote%2520sensing.%2520Nevertheless%252C%2520trait%2520prediction%2520from%2520hyperspectral%250Adata%2520is%2520challenged%2520by%2520label%2520scarcity%2520and%2520substantial%2520domain%2520shifts%2520%2528%255Ceg%2520across%250Asensors%252C%2520ecological%2520distributions%2529%252C%2520requiring%2520robust%2520cross-domain%2520methods.%250AHere%252C%2520we%2520present%2520GreenHyperSpectra%252C%2520a%2520pretraining%2520dataset%2520encompassing%250Areal-world%2520cross-sensor%2520and%2520cross-ecosystem%2520samples%2520designed%2520to%2520benchmark%2520trait%250Aprediction%2520with%2520semi-%2520and%2520self-supervised%2520methods.%2520We%2520adopt%2520an%2520evaluation%250Aframework%2520encompassing%2520in-distribution%2520and%2520out-of-distribution%2520scenarios.%2520We%250Asuccessfully%2520leverage%2520GreenHyperSpectra%2520to%2520pretrain%2520label-efficient%250Amulti-output%2520regression%2520models%2520that%2520outperform%2520the%2520state-of-the-art%2520supervised%250Abaseline.%2520Our%2520empirical%2520analyses%2520demonstrate%2520substantial%2520improvements%2520in%250Alearning%2520spectral%2520representations%2520for%2520trait%2520prediction%252C%2520establishing%2520a%250Acomprehensive%2520methodological%2520framework%2520to%2520catalyze%2520research%2520at%2520the%2520intersection%250Aof%2520representation%2520learning%2520and%2520plant%2520functional%2520traits%2520assessment.%2520All%2520code%2520and%250Adata%2520are%2520available%2520at%253A%2520https%253A//github.com/echerif18/HyspectraSSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GreenHyperSpectra%3A%20A%20multi-source%20hyperspectral%20dataset%20for%20global%0A%20%20vegetation%20trait%20prediction&entry.906535625=Eya%20Cherif%20and%20Arthur%20Ouaknine%20and%20Luke%20A.%20Brown%20and%20Phuong%20D.%20Dao%20and%20Kyle%20R.%20Kovach%20and%20Bing%20Lu%20and%20Daniel%20Mederer%20and%20Hannes%20Feilhauer%20and%20Teja%20Kattenborn%20and%20David%20Rolnick&entry.1292438233=%20%20Plant%20traits%20such%20as%20leaf%20carbon%20content%20and%20leaf%20mass%20are%20essential%0Avariables%20in%20the%20study%20of%20biodiversity%20and%20climate%20change.%20However%2C%0Aconventional%20field%20sampling%20cannot%20feasibly%20cover%20trait%20variation%20at%0Aecologically%20meaningful%20spatial%20scales.%20Machine%20learning%20represents%20a%20valuable%0Asolution%20for%20plant%20trait%20prediction%20across%20ecosystems%2C%20leveraging%20hyperspectral%0Adata%20from%20remote%20sensing.%20Nevertheless%2C%20trait%20prediction%20from%20hyperspectral%0Adata%20is%20challenged%20by%20label%20scarcity%20and%20substantial%20domain%20shifts%20%28%5Ceg%20across%0Asensors%2C%20ecological%20distributions%29%2C%20requiring%20robust%20cross-domain%20methods.%0AHere%2C%20we%20present%20GreenHyperSpectra%2C%20a%20pretraining%20dataset%20encompassing%0Areal-world%20cross-sensor%20and%20cross-ecosystem%20samples%20designed%20to%20benchmark%20trait%0Aprediction%20with%20semi-%20and%20self-supervised%20methods.%20We%20adopt%20an%20evaluation%0Aframework%20encompassing%20in-distribution%20and%20out-of-distribution%20scenarios.%20We%0Asuccessfully%20leverage%20GreenHyperSpectra%20to%20pretrain%20label-efficient%0Amulti-output%20regression%20models%20that%20outperform%20the%20state-of-the-art%20supervised%0Abaseline.%20Our%20empirical%20analyses%20demonstrate%20substantial%20improvements%20in%0Alearning%20spectral%20representations%20for%20trait%20prediction%2C%20establishing%20a%0Acomprehensive%20methodological%20framework%20to%20catalyze%20research%20at%20the%20intersection%0Aof%20representation%20learning%20and%20plant%20functional%20traits%20assessment.%20All%20code%20and%0Adata%20are%20available%20at%3A%20https%3A//github.com/echerif18/HyspectraSSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06806v1&entry.124074799=Read"},
{"title": "Low-Rank Adaptation Secretly Imitates Differentially Private SGD", "author": "Saber Malekmohammadi and Golnoosh Farnadi", "abstract": "  As pre-trained language models grow in size, full fine-tuning their\nparameters on task adaptation data becomes increasingly impractical. To address\nthis challenge, some methods for low-rank adaptation of language models have\nbeen proposed, e.g. LoRA, which incorporates trainable low-rank decomposition\nmatrices into only some parameters of the pre-trained model, called adapters.\nThis approach significantly reduces the number of trainable parameters compared\nto fine-tuning all parameters or adapters. In this work, we look at low-rank\nadaptation method from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA is equivalent to fine-tuning adapters with\nnoisy batch gradients - just like what DPSGD algorithm does. We also quantify\nthe variance of the injected noise as a decreasing function of adaptation rank.\nBy establishing a Berry-Esseen type bound on the total variation distance\nbetween the injected noise distribution and a Gaussian noise distribution with\nthe same variance, we show that the dynamics of low-rank adaptation is very\nclose to when DPSGD is performed w.r.t the adapters. Following our theoretical\nfindings and approved by our experimental results, we show that low-rank\nadaptation provides robustness to membership inference attacks w.r.t the\nfine-tuning data.\n", "link": "http://arxiv.org/abs/2409.17538v7", "date": "2025-07-09", "relevancy": 1.88, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4711}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4705}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Adaptation%20Secretly%20Imitates%20Differentially%20Private%20SGD&body=Title%3A%20Low-Rank%20Adaptation%20Secretly%20Imitates%20Differentially%20Private%20SGD%0AAuthor%3A%20Saber%20Malekmohammadi%20and%20Golnoosh%20Farnadi%0AAbstract%3A%20%20%20As%20pre-trained%20language%20models%20grow%20in%20size%2C%20full%20fine-tuning%20their%0Aparameters%20on%20task%20adaptation%20data%20becomes%20increasingly%20impractical.%20To%20address%0Athis%20challenge%2C%20some%20methods%20for%20low-rank%20adaptation%20of%20language%20models%20have%0Abeen%20proposed%2C%20e.g.%20LoRA%2C%20which%20incorporates%20trainable%20low-rank%20decomposition%0Amatrices%20into%20only%20some%20parameters%20of%20the%20pre-trained%20model%2C%20called%20adapters.%0AThis%20approach%20significantly%20reduces%20the%20number%20of%20trainable%20parameters%20compared%0Ato%20fine-tuning%20all%20parameters%20or%20adapters.%20In%20this%20work%2C%20we%20look%20at%20low-rank%0Aadaptation%20method%20from%20the%20lens%20of%20data%20privacy.%20We%20show%20theoretically%20that%20the%0Alow-rank%20adaptation%20used%20in%20LoRA%20is%20equivalent%20to%20fine-tuning%20adapters%20with%0Anoisy%20batch%20gradients%20-%20just%20like%20what%20DPSGD%20algorithm%20does.%20We%20also%20quantify%0Athe%20variance%20of%20the%20injected%20noise%20as%20a%20decreasing%20function%20of%20adaptation%20rank.%0ABy%20establishing%20a%20Berry-Esseen%20type%20bound%20on%20the%20total%20variation%20distance%0Abetween%20the%20injected%20noise%20distribution%20and%20a%20Gaussian%20noise%20distribution%20with%0Athe%20same%20variance%2C%20we%20show%20that%20the%20dynamics%20of%20low-rank%20adaptation%20is%20very%0Aclose%20to%20when%20DPSGD%20is%20performed%20w.r.t%20the%20adapters.%20Following%20our%20theoretical%0Afindings%20and%20approved%20by%20our%20experimental%20results%2C%20we%20show%20that%20low-rank%0Aadaptation%20provides%20robustness%20to%20membership%20inference%20attacks%20w.r.t%20the%0Afine-tuning%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17538v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Adaptation%2520Secretly%2520Imitates%2520Differentially%2520Private%2520SGD%26entry.906535625%3DSaber%2520Malekmohammadi%2520and%2520Golnoosh%2520Farnadi%26entry.1292438233%3D%2520%2520As%2520pre-trained%2520language%2520models%2520grow%2520in%2520size%252C%2520full%2520fine-tuning%2520their%250Aparameters%2520on%2520task%2520adaptation%2520data%2520becomes%2520increasingly%2520impractical.%2520To%2520address%250Athis%2520challenge%252C%2520some%2520methods%2520for%2520low-rank%2520adaptation%2520of%2520language%2520models%2520have%250Abeen%2520proposed%252C%2520e.g.%2520LoRA%252C%2520which%2520incorporates%2520trainable%2520low-rank%2520decomposition%250Amatrices%2520into%2520only%2520some%2520parameters%2520of%2520the%2520pre-trained%2520model%252C%2520called%2520adapters.%250AThis%2520approach%2520significantly%2520reduces%2520the%2520number%2520of%2520trainable%2520parameters%2520compared%250Ato%2520fine-tuning%2520all%2520parameters%2520or%2520adapters.%2520In%2520this%2520work%252C%2520we%2520look%2520at%2520low-rank%250Aadaptation%2520method%2520from%2520the%2520lens%2520of%2520data%2520privacy.%2520We%2520show%2520theoretically%2520that%2520the%250Alow-rank%2520adaptation%2520used%2520in%2520LoRA%2520is%2520equivalent%2520to%2520fine-tuning%2520adapters%2520with%250Anoisy%2520batch%2520gradients%2520-%2520just%2520like%2520what%2520DPSGD%2520algorithm%2520does.%2520We%2520also%2520quantify%250Athe%2520variance%2520of%2520the%2520injected%2520noise%2520as%2520a%2520decreasing%2520function%2520of%2520adaptation%2520rank.%250ABy%2520establishing%2520a%2520Berry-Esseen%2520type%2520bound%2520on%2520the%2520total%2520variation%2520distance%250Abetween%2520the%2520injected%2520noise%2520distribution%2520and%2520a%2520Gaussian%2520noise%2520distribution%2520with%250Athe%2520same%2520variance%252C%2520we%2520show%2520that%2520the%2520dynamics%2520of%2520low-rank%2520adaptation%2520is%2520very%250Aclose%2520to%2520when%2520DPSGD%2520is%2520performed%2520w.r.t%2520the%2520adapters.%2520Following%2520our%2520theoretical%250Afindings%2520and%2520approved%2520by%2520our%2520experimental%2520results%252C%2520we%2520show%2520that%2520low-rank%250Aadaptation%2520provides%2520robustness%2520to%2520membership%2520inference%2520attacks%2520w.r.t%2520the%250Afine-tuning%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17538v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Adaptation%20Secretly%20Imitates%20Differentially%20Private%20SGD&entry.906535625=Saber%20Malekmohammadi%20and%20Golnoosh%20Farnadi&entry.1292438233=%20%20As%20pre-trained%20language%20models%20grow%20in%20size%2C%20full%20fine-tuning%20their%0Aparameters%20on%20task%20adaptation%20data%20becomes%20increasingly%20impractical.%20To%20address%0Athis%20challenge%2C%20some%20methods%20for%20low-rank%20adaptation%20of%20language%20models%20have%0Abeen%20proposed%2C%20e.g.%20LoRA%2C%20which%20incorporates%20trainable%20low-rank%20decomposition%0Amatrices%20into%20only%20some%20parameters%20of%20the%20pre-trained%20model%2C%20called%20adapters.%0AThis%20approach%20significantly%20reduces%20the%20number%20of%20trainable%20parameters%20compared%0Ato%20fine-tuning%20all%20parameters%20or%20adapters.%20In%20this%20work%2C%20we%20look%20at%20low-rank%0Aadaptation%20method%20from%20the%20lens%20of%20data%20privacy.%20We%20show%20theoretically%20that%20the%0Alow-rank%20adaptation%20used%20in%20LoRA%20is%20equivalent%20to%20fine-tuning%20adapters%20with%0Anoisy%20batch%20gradients%20-%20just%20like%20what%20DPSGD%20algorithm%20does.%20We%20also%20quantify%0Athe%20variance%20of%20the%20injected%20noise%20as%20a%20decreasing%20function%20of%20adaptation%20rank.%0ABy%20establishing%20a%20Berry-Esseen%20type%20bound%20on%20the%20total%20variation%20distance%0Abetween%20the%20injected%20noise%20distribution%20and%20a%20Gaussian%20noise%20distribution%20with%0Athe%20same%20variance%2C%20we%20show%20that%20the%20dynamics%20of%20low-rank%20adaptation%20is%20very%0Aclose%20to%20when%20DPSGD%20is%20performed%20w.r.t%20the%20adapters.%20Following%20our%20theoretical%0Afindings%20and%20approved%20by%20our%20experimental%20results%2C%20we%20show%20that%20low-rank%0Aadaptation%20provides%20robustness%20to%20membership%20inference%20attacks%20w.r.t%20the%0Afine-tuning%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17538v7&entry.124074799=Read"},
{"title": "Generating Multi-Table Time Series EHR from Latent Space with Minimal\n  Preprocessing", "author": "Eunbyeol Cho and Jiyoun Kim and Minjae Lee and Sungjin Park and Edward Choi", "abstract": "  Electronic Health Records (EHR) are time-series relational databases that\nrecord patient interactions and medical events over time, serving as a critical\nresource for healthcare research and applications. However, privacy concerns\nand regulatory restrictions limit the sharing and utilization of such sensitive\ndata, necessitating the generation of synthetic EHR datasets. Unlike previous\nEHR synthesis methods, which typically generate medical records consisting of\nexpert-chosen features (e.g. a few vital signs or structured codes only), we\nintroduce RawMed, the first framework to synthesize multi-table, time-series\nEHR data that closely resembles raw EHRs. Using text-based representation and\ncompression techniques, RawMed captures complex structures and temporal\ndynamics with minimal preprocessing. We also propose a new evaluation framework\nfor multi-table time-series synthetic EHRs, assessing distributional\nsimilarity, inter-table relationships, temporal dynamics, and privacy.\nValidated on two open-source EHR datasets, RawMed outperforms baseline models\nin fidelity and utility. The code is available at\nhttps://github.com/eunbyeol-cho/RawMed.\n", "link": "http://arxiv.org/abs/2507.06996v1", "date": "2025-07-09", "relevancy": 1.872, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5054}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4619}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Multi-Table%20Time%20Series%20EHR%20from%20Latent%20Space%20with%20Minimal%0A%20%20Preprocessing&body=Title%3A%20Generating%20Multi-Table%20Time%20Series%20EHR%20from%20Latent%20Space%20with%20Minimal%0A%20%20Preprocessing%0AAuthor%3A%20Eunbyeol%20Cho%20and%20Jiyoun%20Kim%20and%20Minjae%20Lee%20and%20Sungjin%20Park%20and%20Edward%20Choi%0AAbstract%3A%20%20%20Electronic%20Health%20Records%20%28EHR%29%20are%20time-series%20relational%20databases%20that%0Arecord%20patient%20interactions%20and%20medical%20events%20over%20time%2C%20serving%20as%20a%20critical%0Aresource%20for%20healthcare%20research%20and%20applications.%20However%2C%20privacy%20concerns%0Aand%20regulatory%20restrictions%20limit%20the%20sharing%20and%20utilization%20of%20such%20sensitive%0Adata%2C%20necessitating%20the%20generation%20of%20synthetic%20EHR%20datasets.%20Unlike%20previous%0AEHR%20synthesis%20methods%2C%20which%20typically%20generate%20medical%20records%20consisting%20of%0Aexpert-chosen%20features%20%28e.g.%20a%20few%20vital%20signs%20or%20structured%20codes%20only%29%2C%20we%0Aintroduce%20RawMed%2C%20the%20first%20framework%20to%20synthesize%20multi-table%2C%20time-series%0AEHR%20data%20that%20closely%20resembles%20raw%20EHRs.%20Using%20text-based%20representation%20and%0Acompression%20techniques%2C%20RawMed%20captures%20complex%20structures%20and%20temporal%0Adynamics%20with%20minimal%20preprocessing.%20We%20also%20propose%20a%20new%20evaluation%20framework%0Afor%20multi-table%20time-series%20synthetic%20EHRs%2C%20assessing%20distributional%0Asimilarity%2C%20inter-table%20relationships%2C%20temporal%20dynamics%2C%20and%20privacy.%0AValidated%20on%20two%20open-source%20EHR%20datasets%2C%20RawMed%20outperforms%20baseline%20models%0Ain%20fidelity%20and%20utility.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/eunbyeol-cho/RawMed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Multi-Table%2520Time%2520Series%2520EHR%2520from%2520Latent%2520Space%2520with%2520Minimal%250A%2520%2520Preprocessing%26entry.906535625%3DEunbyeol%2520Cho%2520and%2520Jiyoun%2520Kim%2520and%2520Minjae%2520Lee%2520and%2520Sungjin%2520Park%2520and%2520Edward%2520Choi%26entry.1292438233%3D%2520%2520Electronic%2520Health%2520Records%2520%2528EHR%2529%2520are%2520time-series%2520relational%2520databases%2520that%250Arecord%2520patient%2520interactions%2520and%2520medical%2520events%2520over%2520time%252C%2520serving%2520as%2520a%2520critical%250Aresource%2520for%2520healthcare%2520research%2520and%2520applications.%2520However%252C%2520privacy%2520concerns%250Aand%2520regulatory%2520restrictions%2520limit%2520the%2520sharing%2520and%2520utilization%2520of%2520such%2520sensitive%250Adata%252C%2520necessitating%2520the%2520generation%2520of%2520synthetic%2520EHR%2520datasets.%2520Unlike%2520previous%250AEHR%2520synthesis%2520methods%252C%2520which%2520typically%2520generate%2520medical%2520records%2520consisting%2520of%250Aexpert-chosen%2520features%2520%2528e.g.%2520a%2520few%2520vital%2520signs%2520or%2520structured%2520codes%2520only%2529%252C%2520we%250Aintroduce%2520RawMed%252C%2520the%2520first%2520framework%2520to%2520synthesize%2520multi-table%252C%2520time-series%250AEHR%2520data%2520that%2520closely%2520resembles%2520raw%2520EHRs.%2520Using%2520text-based%2520representation%2520and%250Acompression%2520techniques%252C%2520RawMed%2520captures%2520complex%2520structures%2520and%2520temporal%250Adynamics%2520with%2520minimal%2520preprocessing.%2520We%2520also%2520propose%2520a%2520new%2520evaluation%2520framework%250Afor%2520multi-table%2520time-series%2520synthetic%2520EHRs%252C%2520assessing%2520distributional%250Asimilarity%252C%2520inter-table%2520relationships%252C%2520temporal%2520dynamics%252C%2520and%2520privacy.%250AValidated%2520on%2520two%2520open-source%2520EHR%2520datasets%252C%2520RawMed%2520outperforms%2520baseline%2520models%250Ain%2520fidelity%2520and%2520utility.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/eunbyeol-cho/RawMed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Multi-Table%20Time%20Series%20EHR%20from%20Latent%20Space%20with%20Minimal%0A%20%20Preprocessing&entry.906535625=Eunbyeol%20Cho%20and%20Jiyoun%20Kim%20and%20Minjae%20Lee%20and%20Sungjin%20Park%20and%20Edward%20Choi&entry.1292438233=%20%20Electronic%20Health%20Records%20%28EHR%29%20are%20time-series%20relational%20databases%20that%0Arecord%20patient%20interactions%20and%20medical%20events%20over%20time%2C%20serving%20as%20a%20critical%0Aresource%20for%20healthcare%20research%20and%20applications.%20However%2C%20privacy%20concerns%0Aand%20regulatory%20restrictions%20limit%20the%20sharing%20and%20utilization%20of%20such%20sensitive%0Adata%2C%20necessitating%20the%20generation%20of%20synthetic%20EHR%20datasets.%20Unlike%20previous%0AEHR%20synthesis%20methods%2C%20which%20typically%20generate%20medical%20records%20consisting%20of%0Aexpert-chosen%20features%20%28e.g.%20a%20few%20vital%20signs%20or%20structured%20codes%20only%29%2C%20we%0Aintroduce%20RawMed%2C%20the%20first%20framework%20to%20synthesize%20multi-table%2C%20time-series%0AEHR%20data%20that%20closely%20resembles%20raw%20EHRs.%20Using%20text-based%20representation%20and%0Acompression%20techniques%2C%20RawMed%20captures%20complex%20structures%20and%20temporal%0Adynamics%20with%20minimal%20preprocessing.%20We%20also%20propose%20a%20new%20evaluation%20framework%0Afor%20multi-table%20time-series%20synthetic%20EHRs%2C%20assessing%20distributional%0Asimilarity%2C%20inter-table%20relationships%2C%20temporal%20dynamics%2C%20and%20privacy.%0AValidated%20on%20two%20open-source%20EHR%20datasets%2C%20RawMed%20outperforms%20baseline%20models%0Ain%20fidelity%20and%20utility.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/eunbyeol-cho/RawMed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06996v1&entry.124074799=Read"},
{"title": "Scaling Towards the Information Boundary of Instruction Set:\n  InfinityInstruct-Subject Technical Report", "author": "Li Du and Hanyu Zhao and Yiming Ju and Tengfei Pan", "abstract": "  Instruction tuning has become a foundation for unlocking the capabilities of\nlarge-scale pretrained models and improving their performance on complex tasks.\nThus, the construction of high-quality instruction datasets is crucial for\nenhancing model performance and generalizability. Although current instruction\ndatasets have reached tens of millions of samples, models finetuned on them may\nstill struggle with complex instruction following and tasks in rare domains.\nThis is primarily due to limited expansion in both ``coverage'' (coverage of\ntask types and knowledge areas) and ``depth'' (instruction complexity) of the\ninstruction set. To address this issue, we propose a systematic instruction\ndata construction framework, which integrates a hierarchical labeling system,\nan informative seed selection algorithm, an evolutionary data synthesis\nprocess, and a model deficiency diagnosis with targeted data generation. These\ncomponents form an iterative closed-loop to continuously enhance the coverage\nand depth of instruction data. Based on this framework, we construct\nInfinityInstruct-Subject, a high-quality dataset containing ~1.5 million\ninstructions. Experiments on multiple foundation models and benchmark tasks\ndemonstrate its effectiveness in improving instruction-following capabilities.\nFurther analyses suggest that InfinityInstruct-Subject shows enlarged coverage\nand depth compared to comparable synthesized instruction datasets. Our work\nlays a theoretical and practical foundation for the efficient, continuous\nevolution of instruction datasets, moving from data quantity expansion to\nqualitative improvement.\n", "link": "http://arxiv.org/abs/2507.06968v1", "date": "2025-07-09", "relevancy": 1.8619, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Towards%20the%20Information%20Boundary%20of%20Instruction%20Set%3A%0A%20%20InfinityInstruct-Subject%20Technical%20Report&body=Title%3A%20Scaling%20Towards%20the%20Information%20Boundary%20of%20Instruction%20Set%3A%0A%20%20InfinityInstruct-Subject%20Technical%20Report%0AAuthor%3A%20Li%20Du%20and%20Hanyu%20Zhao%20and%20Yiming%20Ju%20and%20Tengfei%20Pan%0AAbstract%3A%20%20%20Instruction%20tuning%20has%20become%20a%20foundation%20for%20unlocking%20the%20capabilities%20of%0Alarge-scale%20pretrained%20models%20and%20improving%20their%20performance%20on%20complex%20tasks.%0AThus%2C%20the%20construction%20of%20high-quality%20instruction%20datasets%20is%20crucial%20for%0Aenhancing%20model%20performance%20and%20generalizability.%20Although%20current%20instruction%0Adatasets%20have%20reached%20tens%20of%20millions%20of%20samples%2C%20models%20finetuned%20on%20them%20may%0Astill%20struggle%20with%20complex%20instruction%20following%20and%20tasks%20in%20rare%20domains.%0AThis%20is%20primarily%20due%20to%20limited%20expansion%20in%20both%20%60%60coverage%27%27%20%28coverage%20of%0Atask%20types%20and%20knowledge%20areas%29%20and%20%60%60depth%27%27%20%28instruction%20complexity%29%20of%20the%0Ainstruction%20set.%20To%20address%20this%20issue%2C%20we%20propose%20a%20systematic%20instruction%0Adata%20construction%20framework%2C%20which%20integrates%20a%20hierarchical%20labeling%20system%2C%0Aan%20informative%20seed%20selection%20algorithm%2C%20an%20evolutionary%20data%20synthesis%0Aprocess%2C%20and%20a%20model%20deficiency%20diagnosis%20with%20targeted%20data%20generation.%20These%0Acomponents%20form%20an%20iterative%20closed-loop%20to%20continuously%20enhance%20the%20coverage%0Aand%20depth%20of%20instruction%20data.%20Based%20on%20this%20framework%2C%20we%20construct%0AInfinityInstruct-Subject%2C%20a%20high-quality%20dataset%20containing%20~1.5%20million%0Ainstructions.%20Experiments%20on%20multiple%20foundation%20models%20and%20benchmark%20tasks%0Ademonstrate%20its%20effectiveness%20in%20improving%20instruction-following%20capabilities.%0AFurther%20analyses%20suggest%20that%20InfinityInstruct-Subject%20shows%20enlarged%20coverage%0Aand%20depth%20compared%20to%20comparable%20synthesized%20instruction%20datasets.%20Our%20work%0Alays%20a%20theoretical%20and%20practical%20foundation%20for%20the%20efficient%2C%20continuous%0Aevolution%20of%20instruction%20datasets%2C%20moving%20from%20data%20quantity%20expansion%20to%0Aqualitative%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Towards%2520the%2520Information%2520Boundary%2520of%2520Instruction%2520Set%253A%250A%2520%2520InfinityInstruct-Subject%2520Technical%2520Report%26entry.906535625%3DLi%2520Du%2520and%2520Hanyu%2520Zhao%2520and%2520Yiming%2520Ju%2520and%2520Tengfei%2520Pan%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520has%2520become%2520a%2520foundation%2520for%2520unlocking%2520the%2520capabilities%2520of%250Alarge-scale%2520pretrained%2520models%2520and%2520improving%2520their%2520performance%2520on%2520complex%2520tasks.%250AThus%252C%2520the%2520construction%2520of%2520high-quality%2520instruction%2520datasets%2520is%2520crucial%2520for%250Aenhancing%2520model%2520performance%2520and%2520generalizability.%2520Although%2520current%2520instruction%250Adatasets%2520have%2520reached%2520tens%2520of%2520millions%2520of%2520samples%252C%2520models%2520finetuned%2520on%2520them%2520may%250Astill%2520struggle%2520with%2520complex%2520instruction%2520following%2520and%2520tasks%2520in%2520rare%2520domains.%250AThis%2520is%2520primarily%2520due%2520to%2520limited%2520expansion%2520in%2520both%2520%2560%2560coverage%2527%2527%2520%2528coverage%2520of%250Atask%2520types%2520and%2520knowledge%2520areas%2529%2520and%2520%2560%2560depth%2527%2527%2520%2528instruction%2520complexity%2529%2520of%2520the%250Ainstruction%2520set.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520systematic%2520instruction%250Adata%2520construction%2520framework%252C%2520which%2520integrates%2520a%2520hierarchical%2520labeling%2520system%252C%250Aan%2520informative%2520seed%2520selection%2520algorithm%252C%2520an%2520evolutionary%2520data%2520synthesis%250Aprocess%252C%2520and%2520a%2520model%2520deficiency%2520diagnosis%2520with%2520targeted%2520data%2520generation.%2520These%250Acomponents%2520form%2520an%2520iterative%2520closed-loop%2520to%2520continuously%2520enhance%2520the%2520coverage%250Aand%2520depth%2520of%2520instruction%2520data.%2520Based%2520on%2520this%2520framework%252C%2520we%2520construct%250AInfinityInstruct-Subject%252C%2520a%2520high-quality%2520dataset%2520containing%2520~1.5%2520million%250Ainstructions.%2520Experiments%2520on%2520multiple%2520foundation%2520models%2520and%2520benchmark%2520tasks%250Ademonstrate%2520its%2520effectiveness%2520in%2520improving%2520instruction-following%2520capabilities.%250AFurther%2520analyses%2520suggest%2520that%2520InfinityInstruct-Subject%2520shows%2520enlarged%2520coverage%250Aand%2520depth%2520compared%2520to%2520comparable%2520synthesized%2520instruction%2520datasets.%2520Our%2520work%250Alays%2520a%2520theoretical%2520and%2520practical%2520foundation%2520for%2520the%2520efficient%252C%2520continuous%250Aevolution%2520of%2520instruction%2520datasets%252C%2520moving%2520from%2520data%2520quantity%2520expansion%2520to%250Aqualitative%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Towards%20the%20Information%20Boundary%20of%20Instruction%20Set%3A%0A%20%20InfinityInstruct-Subject%20Technical%20Report&entry.906535625=Li%20Du%20and%20Hanyu%20Zhao%20and%20Yiming%20Ju%20and%20Tengfei%20Pan&entry.1292438233=%20%20Instruction%20tuning%20has%20become%20a%20foundation%20for%20unlocking%20the%20capabilities%20of%0Alarge-scale%20pretrained%20models%20and%20improving%20their%20performance%20on%20complex%20tasks.%0AThus%2C%20the%20construction%20of%20high-quality%20instruction%20datasets%20is%20crucial%20for%0Aenhancing%20model%20performance%20and%20generalizability.%20Although%20current%20instruction%0Adatasets%20have%20reached%20tens%20of%20millions%20of%20samples%2C%20models%20finetuned%20on%20them%20may%0Astill%20struggle%20with%20complex%20instruction%20following%20and%20tasks%20in%20rare%20domains.%0AThis%20is%20primarily%20due%20to%20limited%20expansion%20in%20both%20%60%60coverage%27%27%20%28coverage%20of%0Atask%20types%20and%20knowledge%20areas%29%20and%20%60%60depth%27%27%20%28instruction%20complexity%29%20of%20the%0Ainstruction%20set.%20To%20address%20this%20issue%2C%20we%20propose%20a%20systematic%20instruction%0Adata%20construction%20framework%2C%20which%20integrates%20a%20hierarchical%20labeling%20system%2C%0Aan%20informative%20seed%20selection%20algorithm%2C%20an%20evolutionary%20data%20synthesis%0Aprocess%2C%20and%20a%20model%20deficiency%20diagnosis%20with%20targeted%20data%20generation.%20These%0Acomponents%20form%20an%20iterative%20closed-loop%20to%20continuously%20enhance%20the%20coverage%0Aand%20depth%20of%20instruction%20data.%20Based%20on%20this%20framework%2C%20we%20construct%0AInfinityInstruct-Subject%2C%20a%20high-quality%20dataset%20containing%20~1.5%20million%0Ainstructions.%20Experiments%20on%20multiple%20foundation%20models%20and%20benchmark%20tasks%0Ademonstrate%20its%20effectiveness%20in%20improving%20instruction-following%20capabilities.%0AFurther%20analyses%20suggest%20that%20InfinityInstruct-Subject%20shows%20enlarged%20coverage%0Aand%20depth%20compared%20to%20comparable%20synthesized%20instruction%20datasets.%20Our%20work%0Alays%20a%20theoretical%20and%20practical%20foundation%20for%20the%20efficient%2C%20continuous%0Aevolution%20of%20instruction%20datasets%2C%20moving%20from%20data%20quantity%20expansion%20to%0Aqualitative%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06968v1&entry.124074799=Read"},
{"title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal\n  Prediction", "author": "Xiao Wang and Jiahuan Pei and Diancheng Shui and Zhiguang Han and Xin Sun and Dawei Zhu and Xiaoyu Shen", "abstract": "  Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.\n", "link": "http://arxiv.org/abs/2507.06909v1", "date": "2025-07-09", "relevancy": 1.4274, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5006}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4869}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiJustice%3A%20A%20Chinese%20Dataset%20for%20Multi-Party%2C%20Multi-Charge%20Legal%0A%20%20Prediction&body=Title%3A%20MultiJustice%3A%20A%20Chinese%20Dataset%20for%20Multi-Party%2C%20Multi-Charge%20Legal%0A%20%20Prediction%0AAuthor%3A%20Xiao%20Wang%20and%20Jiahuan%20Pei%20and%20Diancheng%20Shui%20and%20Zhiguang%20Han%20and%20Xin%20Sun%20and%20Dawei%20Zhu%20and%20Xiaoyu%20Shen%0AAbstract%3A%20%20%20Legal%20judgment%20prediction%20offers%20a%20compelling%20method%20to%20aid%20legal%0Apractitioners%20and%20researchers.%20However%2C%20the%20research%20question%20remains%0Arelatively%20under-explored%3A%20Should%20multiple%20defendants%20and%20charges%20be%20treated%0Aseparately%20in%20LJP%3F%20To%20address%20this%2C%20we%20introduce%20a%20new%20dataset%20namely%0Amulti-person%20multi-charge%20prediction%20%28MPMCP%29%2C%20and%20seek%20the%20answer%20by%20evaluating%0Athe%20performance%20of%20several%20prevailing%20legal%20large%20language%20models%20%28LLMs%29%20on%0Afour%20practical%20legal%20judgment%20scenarios%3A%20%28S1%29%20single%20defendant%20with%20a%20single%0Acharge%2C%20%28S2%29%20single%20defendant%20with%20multiple%20charges%2C%20%28S3%29%20multiple%20defendants%0Awith%20a%20single%20charge%2C%20and%20%28S4%29%20multiple%20defendants%20with%20multiple%20charges.%20We%0Aevaluate%20the%20dataset%20across%20two%20LJP%20tasks%2C%20i.e.%2C%20charge%20prediction%20and%20penalty%0Aterm%20prediction.%20We%20have%20conducted%20extensive%20experiments%20and%20found%20that%20the%0Ascenario%20involving%20multiple%20defendants%20and%20multiple%20charges%20%28S4%29%20poses%20the%0Agreatest%20challenges%2C%20followed%20by%20S2%2C%20S3%2C%20and%20S1.%20The%20impact%20varies%0Asignificantly%20depending%20on%20the%20model.%20For%20example%2C%20in%20S4%20compared%20to%20S1%2C%0AInternLM2%20achieves%20approximately%204.5%25%20lower%20F1-score%20and%202.8%25%20higher%20LogD%2C%0Awhile%20Lawformer%20demonstrates%20around%2019.7%25%20lower%20F1-score%20and%2019.0%25%20higher%20LogD.%0AOur%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/lololo-xiao/MultiJustice-MPMCP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiJustice%253A%2520A%2520Chinese%2520Dataset%2520for%2520Multi-Party%252C%2520Multi-Charge%2520Legal%250A%2520%2520Prediction%26entry.906535625%3DXiao%2520Wang%2520and%2520Jiahuan%2520Pei%2520and%2520Diancheng%2520Shui%2520and%2520Zhiguang%2520Han%2520and%2520Xin%2520Sun%2520and%2520Dawei%2520Zhu%2520and%2520Xiaoyu%2520Shen%26entry.1292438233%3D%2520%2520Legal%2520judgment%2520prediction%2520offers%2520a%2520compelling%2520method%2520to%2520aid%2520legal%250Apractitioners%2520and%2520researchers.%2520However%252C%2520the%2520research%2520question%2520remains%250Arelatively%2520under-explored%253A%2520Should%2520multiple%2520defendants%2520and%2520charges%2520be%2520treated%250Aseparately%2520in%2520LJP%253F%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520new%2520dataset%2520namely%250Amulti-person%2520multi-charge%2520prediction%2520%2528MPMCP%2529%252C%2520and%2520seek%2520the%2520answer%2520by%2520evaluating%250Athe%2520performance%2520of%2520several%2520prevailing%2520legal%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%250Afour%2520practical%2520legal%2520judgment%2520scenarios%253A%2520%2528S1%2529%2520single%2520defendant%2520with%2520a%2520single%250Acharge%252C%2520%2528S2%2529%2520single%2520defendant%2520with%2520multiple%2520charges%252C%2520%2528S3%2529%2520multiple%2520defendants%250Awith%2520a%2520single%2520charge%252C%2520and%2520%2528S4%2529%2520multiple%2520defendants%2520with%2520multiple%2520charges.%2520We%250Aevaluate%2520the%2520dataset%2520across%2520two%2520LJP%2520tasks%252C%2520i.e.%252C%2520charge%2520prediction%2520and%2520penalty%250Aterm%2520prediction.%2520We%2520have%2520conducted%2520extensive%2520experiments%2520and%2520found%2520that%2520the%250Ascenario%2520involving%2520multiple%2520defendants%2520and%2520multiple%2520charges%2520%2528S4%2529%2520poses%2520the%250Agreatest%2520challenges%252C%2520followed%2520by%2520S2%252C%2520S3%252C%2520and%2520S1.%2520The%2520impact%2520varies%250Asignificantly%2520depending%2520on%2520the%2520model.%2520For%2520example%252C%2520in%2520S4%2520compared%2520to%2520S1%252C%250AInternLM2%2520achieves%2520approximately%25204.5%2525%2520lower%2520F1-score%2520and%25202.8%2525%2520higher%2520LogD%252C%250Awhile%2520Lawformer%2520demonstrates%2520around%252019.7%2525%2520lower%2520F1-score%2520and%252019.0%2525%2520higher%2520LogD.%250AOur%2520dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/lololo-xiao/MultiJustice-MPMCP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiJustice%3A%20A%20Chinese%20Dataset%20for%20Multi-Party%2C%20Multi-Charge%20Legal%0A%20%20Prediction&entry.906535625=Xiao%20Wang%20and%20Jiahuan%20Pei%20and%20Diancheng%20Shui%20and%20Zhiguang%20Han%20and%20Xin%20Sun%20and%20Dawei%20Zhu%20and%20Xiaoyu%20Shen&entry.1292438233=%20%20Legal%20judgment%20prediction%20offers%20a%20compelling%20method%20to%20aid%20legal%0Apractitioners%20and%20researchers.%20However%2C%20the%20research%20question%20remains%0Arelatively%20under-explored%3A%20Should%20multiple%20defendants%20and%20charges%20be%20treated%0Aseparately%20in%20LJP%3F%20To%20address%20this%2C%20we%20introduce%20a%20new%20dataset%20namely%0Amulti-person%20multi-charge%20prediction%20%28MPMCP%29%2C%20and%20seek%20the%20answer%20by%20evaluating%0Athe%20performance%20of%20several%20prevailing%20legal%20large%20language%20models%20%28LLMs%29%20on%0Afour%20practical%20legal%20judgment%20scenarios%3A%20%28S1%29%20single%20defendant%20with%20a%20single%0Acharge%2C%20%28S2%29%20single%20defendant%20with%20multiple%20charges%2C%20%28S3%29%20multiple%20defendants%0Awith%20a%20single%20charge%2C%20and%20%28S4%29%20multiple%20defendants%20with%20multiple%20charges.%20We%0Aevaluate%20the%20dataset%20across%20two%20LJP%20tasks%2C%20i.e.%2C%20charge%20prediction%20and%20penalty%0Aterm%20prediction.%20We%20have%20conducted%20extensive%20experiments%20and%20found%20that%20the%0Ascenario%20involving%20multiple%20defendants%20and%20multiple%20charges%20%28S4%29%20poses%20the%0Agreatest%20challenges%2C%20followed%20by%20S2%2C%20S3%2C%20and%20S1.%20The%20impact%20varies%0Asignificantly%20depending%20on%20the%20model.%20For%20example%2C%20in%20S4%20compared%20to%20S1%2C%0AInternLM2%20achieves%20approximately%204.5%25%20lower%20F1-score%20and%202.8%25%20higher%20LogD%2C%0Awhile%20Lawformer%20demonstrates%20around%2019.7%25%20lower%20F1-score%20and%2019.0%25%20higher%20LogD.%0AOur%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/lololo-xiao/MultiJustice-MPMCP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06909v1&entry.124074799=Read"},
{"title": "Representative Ranking for Deliberation in the Public Sphere", "author": "Manon Revel and Smitha Milli and Tyler Lu and Jamelle Watson-Daniels and Max Nickel", "abstract": "  Online comment sections, such as those on news sites or social media, have\nthe potential to foster informal public deliberation, However, this potential\nis often undermined by the frequency of toxic or low-quality exchanges that\noccur in these settings. To combat this, platforms increasingly leverage\nalgorithmic ranking to facilitate higher-quality discussions, e.g., by using\ncivility classifiers or forms of prosocial ranking. Yet, these interventions\nmay also inadvertently reduce the visibility of legitimate viewpoints,\nundermining another key aspect of deliberation: representation of diverse\nviews. We seek to remedy this problem by introducing guarantees of\nrepresentation into these methods. In particular, we adopt the notion of\njustified representation (JR) from the social choice literature and incorporate\na JR constraint into the comment ranking setting. We find that enforcing JR\nleads to greater inclusion of diverse viewpoints while still being compatible\nwith optimizing for user engagement or other measures of conversational\nquality.\n", "link": "http://arxiv.org/abs/2503.18962v2", "date": "2025-07-09", "relevancy": 1.1434, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3889}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3794}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representative%20Ranking%20for%20Deliberation%20in%20the%20Public%20Sphere&body=Title%3A%20Representative%20Ranking%20for%20Deliberation%20in%20the%20Public%20Sphere%0AAuthor%3A%20Manon%20Revel%20and%20Smitha%20Milli%20and%20Tyler%20Lu%20and%20Jamelle%20Watson-Daniels%20and%20Max%20Nickel%0AAbstract%3A%20%20%20Online%20comment%20sections%2C%20such%20as%20those%20on%20news%20sites%20or%20social%20media%2C%20have%0Athe%20potential%20to%20foster%20informal%20public%20deliberation%2C%20However%2C%20this%20potential%0Ais%20often%20undermined%20by%20the%20frequency%20of%20toxic%20or%20low-quality%20exchanges%20that%0Aoccur%20in%20these%20settings.%20To%20combat%20this%2C%20platforms%20increasingly%20leverage%0Aalgorithmic%20ranking%20to%20facilitate%20higher-quality%20discussions%2C%20e.g.%2C%20by%20using%0Acivility%20classifiers%20or%20forms%20of%20prosocial%20ranking.%20Yet%2C%20these%20interventions%0Amay%20also%20inadvertently%20reduce%20the%20visibility%20of%20legitimate%20viewpoints%2C%0Aundermining%20another%20key%20aspect%20of%20deliberation%3A%20representation%20of%20diverse%0Aviews.%20We%20seek%20to%20remedy%20this%20problem%20by%20introducing%20guarantees%20of%0Arepresentation%20into%20these%20methods.%20In%20particular%2C%20we%20adopt%20the%20notion%20of%0Ajustified%20representation%20%28JR%29%20from%20the%20social%20choice%20literature%20and%20incorporate%0Aa%20JR%20constraint%20into%20the%20comment%20ranking%20setting.%20We%20find%20that%20enforcing%20JR%0Aleads%20to%20greater%20inclusion%20of%20diverse%20viewpoints%20while%20still%20being%20compatible%0Awith%20optimizing%20for%20user%20engagement%20or%20other%20measures%20of%20conversational%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18962v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentative%2520Ranking%2520for%2520Deliberation%2520in%2520the%2520Public%2520Sphere%26entry.906535625%3DManon%2520Revel%2520and%2520Smitha%2520Milli%2520and%2520Tyler%2520Lu%2520and%2520Jamelle%2520Watson-Daniels%2520and%2520Max%2520Nickel%26entry.1292438233%3D%2520%2520Online%2520comment%2520sections%252C%2520such%2520as%2520those%2520on%2520news%2520sites%2520or%2520social%2520media%252C%2520have%250Athe%2520potential%2520to%2520foster%2520informal%2520public%2520deliberation%252C%2520However%252C%2520this%2520potential%250Ais%2520often%2520undermined%2520by%2520the%2520frequency%2520of%2520toxic%2520or%2520low-quality%2520exchanges%2520that%250Aoccur%2520in%2520these%2520settings.%2520To%2520combat%2520this%252C%2520platforms%2520increasingly%2520leverage%250Aalgorithmic%2520ranking%2520to%2520facilitate%2520higher-quality%2520discussions%252C%2520e.g.%252C%2520by%2520using%250Acivility%2520classifiers%2520or%2520forms%2520of%2520prosocial%2520ranking.%2520Yet%252C%2520these%2520interventions%250Amay%2520also%2520inadvertently%2520reduce%2520the%2520visibility%2520of%2520legitimate%2520viewpoints%252C%250Aundermining%2520another%2520key%2520aspect%2520of%2520deliberation%253A%2520representation%2520of%2520diverse%250Aviews.%2520We%2520seek%2520to%2520remedy%2520this%2520problem%2520by%2520introducing%2520guarantees%2520of%250Arepresentation%2520into%2520these%2520methods.%2520In%2520particular%252C%2520we%2520adopt%2520the%2520notion%2520of%250Ajustified%2520representation%2520%2528JR%2529%2520from%2520the%2520social%2520choice%2520literature%2520and%2520incorporate%250Aa%2520JR%2520constraint%2520into%2520the%2520comment%2520ranking%2520setting.%2520We%2520find%2520that%2520enforcing%2520JR%250Aleads%2520to%2520greater%2520inclusion%2520of%2520diverse%2520viewpoints%2520while%2520still%2520being%2520compatible%250Awith%2520optimizing%2520for%2520user%2520engagement%2520or%2520other%2520measures%2520of%2520conversational%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18962v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representative%20Ranking%20for%20Deliberation%20in%20the%20Public%20Sphere&entry.906535625=Manon%20Revel%20and%20Smitha%20Milli%20and%20Tyler%20Lu%20and%20Jamelle%20Watson-Daniels%20and%20Max%20Nickel&entry.1292438233=%20%20Online%20comment%20sections%2C%20such%20as%20those%20on%20news%20sites%20or%20social%20media%2C%20have%0Athe%20potential%20to%20foster%20informal%20public%20deliberation%2C%20However%2C%20this%20potential%0Ais%20often%20undermined%20by%20the%20frequency%20of%20toxic%20or%20low-quality%20exchanges%20that%0Aoccur%20in%20these%20settings.%20To%20combat%20this%2C%20platforms%20increasingly%20leverage%0Aalgorithmic%20ranking%20to%20facilitate%20higher-quality%20discussions%2C%20e.g.%2C%20by%20using%0Acivility%20classifiers%20or%20forms%20of%20prosocial%20ranking.%20Yet%2C%20these%20interventions%0Amay%20also%20inadvertently%20reduce%20the%20visibility%20of%20legitimate%20viewpoints%2C%0Aundermining%20another%20key%20aspect%20of%20deliberation%3A%20representation%20of%20diverse%0Aviews.%20We%20seek%20to%20remedy%20this%20problem%20by%20introducing%20guarantees%20of%0Arepresentation%20into%20these%20methods.%20In%20particular%2C%20we%20adopt%20the%20notion%20of%0Ajustified%20representation%20%28JR%29%20from%20the%20social%20choice%20literature%20and%20incorporate%0Aa%20JR%20constraint%20into%20the%20comment%20ranking%20setting.%20We%20find%20that%20enforcing%20JR%0Aleads%20to%20greater%20inclusion%20of%20diverse%20viewpoints%20while%20still%20being%20compatible%0Awith%20optimizing%20for%20user%20engagement%20or%20other%20measures%20of%20conversational%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18962v2&entry.124074799=Read"},
{"title": "Cross-Modality Masked Learning for Survival Prediction in ICI Treated\n  NSCLC Patients", "author": "Qilong Xing and Zikai Song and Bingxin Gong and Lian Yang and Junqing Yu and Wei Yang", "abstract": "  Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing\nimmunotherapy is essential for personalized treatment planning, enabling\ninformed patient decisions, and improving both treatment outcomes and quality\nof life. However, the lack of large, relevant datasets and effective\nmulti-modal feature fusion strategies pose significant challenges in this\ndomain. To address these challenges, we present a large-scale dataset and\nintroduce a novel framework for multi-modal feature fusion aimed at enhancing\nthe accuracy of survival prediction. The dataset comprises 3D CT images and\ncorresponding clinical records from NSCLC patients treated with immune\ncheckpoint inhibitors (ICI), along with progression-free survival (PFS) and\noverall survival (OS) data. We further propose a cross-modality masked learning\napproach for medical feature fusion, consisting of two distinct branches, each\ntailored to its respective modality: a Slice-Depth Transformer for extracting\n3D features from CT images and a graph-based Transformer for learning node\nfeatures and relationships among clinical variables in tabular data. The fusion\nprocess is guided by a masked modality learning strategy, wherein the model\nutilizes the intact modality to reconstruct missing components. This mechanism\nimproves the integration of modality-specific features, fostering more\neffective inter-modality relationships and feature interactions. Our approach\ndemonstrates superior performance in multi-modal integration for NSCLC survival\nprediction, surpassing existing methods and setting a new benchmark for\nprognostic models in this context.\n", "link": "http://arxiv.org/abs/2507.06994v1", "date": "2025-07-09", "relevancy": 1.5524, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4936}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modality%20Masked%20Learning%20for%20Survival%20Prediction%20in%20ICI%20Treated%0A%20%20NSCLC%20Patients&body=Title%3A%20Cross-Modality%20Masked%20Learning%20for%20Survival%20Prediction%20in%20ICI%20Treated%0A%20%20NSCLC%20Patients%0AAuthor%3A%20Qilong%20Xing%20and%20Zikai%20Song%20and%20Bingxin%20Gong%20and%20Lian%20Yang%20and%20Junqing%20Yu%20and%20Wei%20Yang%0AAbstract%3A%20%20%20Accurate%20prognosis%20of%20non-small%20cell%20lung%20cancer%20%28NSCLC%29%20patients%20undergoing%0Aimmunotherapy%20is%20essential%20for%20personalized%20treatment%20planning%2C%20enabling%0Ainformed%20patient%20decisions%2C%20and%20improving%20both%20treatment%20outcomes%20and%20quality%0Aof%20life.%20However%2C%20the%20lack%20of%20large%2C%20relevant%20datasets%20and%20effective%0Amulti-modal%20feature%20fusion%20strategies%20pose%20significant%20challenges%20in%20this%0Adomain.%20To%20address%20these%20challenges%2C%20we%20present%20a%20large-scale%20dataset%20and%0Aintroduce%20a%20novel%20framework%20for%20multi-modal%20feature%20fusion%20aimed%20at%20enhancing%0Athe%20accuracy%20of%20survival%20prediction.%20The%20dataset%20comprises%203D%20CT%20images%20and%0Acorresponding%20clinical%20records%20from%20NSCLC%20patients%20treated%20with%20immune%0Acheckpoint%20inhibitors%20%28ICI%29%2C%20along%20with%20progression-free%20survival%20%28PFS%29%20and%0Aoverall%20survival%20%28OS%29%20data.%20We%20further%20propose%20a%20cross-modality%20masked%20learning%0Aapproach%20for%20medical%20feature%20fusion%2C%20consisting%20of%20two%20distinct%20branches%2C%20each%0Atailored%20to%20its%20respective%20modality%3A%20a%20Slice-Depth%20Transformer%20for%20extracting%0A3D%20features%20from%20CT%20images%20and%20a%20graph-based%20Transformer%20for%20learning%20node%0Afeatures%20and%20relationships%20among%20clinical%20variables%20in%20tabular%20data.%20The%20fusion%0Aprocess%20is%20guided%20by%20a%20masked%20modality%20learning%20strategy%2C%20wherein%20the%20model%0Autilizes%20the%20intact%20modality%20to%20reconstruct%20missing%20components.%20This%20mechanism%0Aimproves%20the%20integration%20of%20modality-specific%20features%2C%20fostering%20more%0Aeffective%20inter-modality%20relationships%20and%20feature%20interactions.%20Our%20approach%0Ademonstrates%20superior%20performance%20in%20multi-modal%20integration%20for%20NSCLC%20survival%0Aprediction%2C%20surpassing%20existing%20methods%20and%20setting%20a%20new%20benchmark%20for%0Aprognostic%20models%20in%20this%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modality%2520Masked%2520Learning%2520for%2520Survival%2520Prediction%2520in%2520ICI%2520Treated%250A%2520%2520NSCLC%2520Patients%26entry.906535625%3DQilong%2520Xing%2520and%2520Zikai%2520Song%2520and%2520Bingxin%2520Gong%2520and%2520Lian%2520Yang%2520and%2520Junqing%2520Yu%2520and%2520Wei%2520Yang%26entry.1292438233%3D%2520%2520Accurate%2520prognosis%2520of%2520non-small%2520cell%2520lung%2520cancer%2520%2528NSCLC%2529%2520patients%2520undergoing%250Aimmunotherapy%2520is%2520essential%2520for%2520personalized%2520treatment%2520planning%252C%2520enabling%250Ainformed%2520patient%2520decisions%252C%2520and%2520improving%2520both%2520treatment%2520outcomes%2520and%2520quality%250Aof%2520life.%2520However%252C%2520the%2520lack%2520of%2520large%252C%2520relevant%2520datasets%2520and%2520effective%250Amulti-modal%2520feature%2520fusion%2520strategies%2520pose%2520significant%2520challenges%2520in%2520this%250Adomain.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520a%2520large-scale%2520dataset%2520and%250Aintroduce%2520a%2520novel%2520framework%2520for%2520multi-modal%2520feature%2520fusion%2520aimed%2520at%2520enhancing%250Athe%2520accuracy%2520of%2520survival%2520prediction.%2520The%2520dataset%2520comprises%25203D%2520CT%2520images%2520and%250Acorresponding%2520clinical%2520records%2520from%2520NSCLC%2520patients%2520treated%2520with%2520immune%250Acheckpoint%2520inhibitors%2520%2528ICI%2529%252C%2520along%2520with%2520progression-free%2520survival%2520%2528PFS%2529%2520and%250Aoverall%2520survival%2520%2528OS%2529%2520data.%2520We%2520further%2520propose%2520a%2520cross-modality%2520masked%2520learning%250Aapproach%2520for%2520medical%2520feature%2520fusion%252C%2520consisting%2520of%2520two%2520distinct%2520branches%252C%2520each%250Atailored%2520to%2520its%2520respective%2520modality%253A%2520a%2520Slice-Depth%2520Transformer%2520for%2520extracting%250A3D%2520features%2520from%2520CT%2520images%2520and%2520a%2520graph-based%2520Transformer%2520for%2520learning%2520node%250Afeatures%2520and%2520relationships%2520among%2520clinical%2520variables%2520in%2520tabular%2520data.%2520The%2520fusion%250Aprocess%2520is%2520guided%2520by%2520a%2520masked%2520modality%2520learning%2520strategy%252C%2520wherein%2520the%2520model%250Autilizes%2520the%2520intact%2520modality%2520to%2520reconstruct%2520missing%2520components.%2520This%2520mechanism%250Aimproves%2520the%2520integration%2520of%2520modality-specific%2520features%252C%2520fostering%2520more%250Aeffective%2520inter-modality%2520relationships%2520and%2520feature%2520interactions.%2520Our%2520approach%250Ademonstrates%2520superior%2520performance%2520in%2520multi-modal%2520integration%2520for%2520NSCLC%2520survival%250Aprediction%252C%2520surpassing%2520existing%2520methods%2520and%2520setting%2520a%2520new%2520benchmark%2520for%250Aprognostic%2520models%2520in%2520this%2520context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modality%20Masked%20Learning%20for%20Survival%20Prediction%20in%20ICI%20Treated%0A%20%20NSCLC%20Patients&entry.906535625=Qilong%20Xing%20and%20Zikai%20Song%20and%20Bingxin%20Gong%20and%20Lian%20Yang%20and%20Junqing%20Yu%20and%20Wei%20Yang&entry.1292438233=%20%20Accurate%20prognosis%20of%20non-small%20cell%20lung%20cancer%20%28NSCLC%29%20patients%20undergoing%0Aimmunotherapy%20is%20essential%20for%20personalized%20treatment%20planning%2C%20enabling%0Ainformed%20patient%20decisions%2C%20and%20improving%20both%20treatment%20outcomes%20and%20quality%0Aof%20life.%20However%2C%20the%20lack%20of%20large%2C%20relevant%20datasets%20and%20effective%0Amulti-modal%20feature%20fusion%20strategies%20pose%20significant%20challenges%20in%20this%0Adomain.%20To%20address%20these%20challenges%2C%20we%20present%20a%20large-scale%20dataset%20and%0Aintroduce%20a%20novel%20framework%20for%20multi-modal%20feature%20fusion%20aimed%20at%20enhancing%0Athe%20accuracy%20of%20survival%20prediction.%20The%20dataset%20comprises%203D%20CT%20images%20and%0Acorresponding%20clinical%20records%20from%20NSCLC%20patients%20treated%20with%20immune%0Acheckpoint%20inhibitors%20%28ICI%29%2C%20along%20with%20progression-free%20survival%20%28PFS%29%20and%0Aoverall%20survival%20%28OS%29%20data.%20We%20further%20propose%20a%20cross-modality%20masked%20learning%0Aapproach%20for%20medical%20feature%20fusion%2C%20consisting%20of%20two%20distinct%20branches%2C%20each%0Atailored%20to%20its%20respective%20modality%3A%20a%20Slice-Depth%20Transformer%20for%20extracting%0A3D%20features%20from%20CT%20images%20and%20a%20graph-based%20Transformer%20for%20learning%20node%0Afeatures%20and%20relationships%20among%20clinical%20variables%20in%20tabular%20data.%20The%20fusion%0Aprocess%20is%20guided%20by%20a%20masked%20modality%20learning%20strategy%2C%20wherein%20the%20model%0Autilizes%20the%20intact%20modality%20to%20reconstruct%20missing%20components.%20This%20mechanism%0Aimproves%20the%20integration%20of%20modality-specific%20features%2C%20fostering%20more%0Aeffective%20inter-modality%20relationships%20and%20feature%20interactions.%20Our%20approach%0Ademonstrates%20superior%20performance%20in%20multi-modal%20integration%20for%20NSCLC%20survival%0Aprediction%2C%20surpassing%20existing%20methods%20and%20setting%20a%20new%20benchmark%20for%0Aprognostic%20models%20in%20this%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06994v1&entry.124074799=Read"},
{"title": "Small Batch Size Training for Language Models: When Vanilla SGD Works,\n  and Why Gradient Accumulation Is Wasteful", "author": "Martin Marek and Sanae Lotfi and Aditya Somasundaram and Andrew Gordon Wilson and Micah Goldblum", "abstract": "  Conventional wisdom dictates that small batch sizes make language model\npretraining and fine-tuning unstable, motivating gradient accumulation, which\ntrades off the number of optimizer steps for a proportional increase in batch\nsize. While it is common to decrease the learning rate for smaller batch sizes,\nother hyperparameters are often held fixed. In this work, we revisit small\nbatch sizes all the way down to batch size one, and we propose a rule for\nscaling Adam hyperparameters to small batch sizes. We find that small batch\nsizes (1) train stably, (2) are consistently more robust to hyperparameter\nchoices, (3) achieve equal or better per-FLOP performance than larger batch\nsizes, and (4) notably enable stable language model training with vanilla SGD,\neven without momentum, despite storing no optimizer state. Building on these\nresults, we provide practical recommendations for selecting a batch size and\nsetting optimizer hyperparameters. We further recommend against gradient\naccumulation unless training on multiple devices with multiple model replicas,\nbottlenecked by inter-device bandwidth.\n", "link": "http://arxiv.org/abs/2507.07101v1", "date": "2025-07-09", "relevancy": 1.5415, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5254}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5091}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Small%20Batch%20Size%20Training%20for%20Language%20Models%3A%20When%20Vanilla%20SGD%20Works%2C%0A%20%20and%20Why%20Gradient%20Accumulation%20Is%20Wasteful&body=Title%3A%20Small%20Batch%20Size%20Training%20for%20Language%20Models%3A%20When%20Vanilla%20SGD%20Works%2C%0A%20%20and%20Why%20Gradient%20Accumulation%20Is%20Wasteful%0AAuthor%3A%20Martin%20Marek%20and%20Sanae%20Lotfi%20and%20Aditya%20Somasundaram%20and%20Andrew%20Gordon%20Wilson%20and%20Micah%20Goldblum%0AAbstract%3A%20%20%20Conventional%20wisdom%20dictates%20that%20small%20batch%20sizes%20make%20language%20model%0Apretraining%20and%20fine-tuning%20unstable%2C%20motivating%20gradient%20accumulation%2C%20which%0Atrades%20off%20the%20number%20of%20optimizer%20steps%20for%20a%20proportional%20increase%20in%20batch%0Asize.%20While%20it%20is%20common%20to%20decrease%20the%20learning%20rate%20for%20smaller%20batch%20sizes%2C%0Aother%20hyperparameters%20are%20often%20held%20fixed.%20In%20this%20work%2C%20we%20revisit%20small%0Abatch%20sizes%20all%20the%20way%20down%20to%20batch%20size%20one%2C%20and%20we%20propose%20a%20rule%20for%0Ascaling%20Adam%20hyperparameters%20to%20small%20batch%20sizes.%20We%20find%20that%20small%20batch%0Asizes%20%281%29%20train%20stably%2C%20%282%29%20are%20consistently%20more%20robust%20to%20hyperparameter%0Achoices%2C%20%283%29%20achieve%20equal%20or%20better%20per-FLOP%20performance%20than%20larger%20batch%0Asizes%2C%20and%20%284%29%20notably%20enable%20stable%20language%20model%20training%20with%20vanilla%20SGD%2C%0Aeven%20without%20momentum%2C%20despite%20storing%20no%20optimizer%20state.%20Building%20on%20these%0Aresults%2C%20we%20provide%20practical%20recommendations%20for%20selecting%20a%20batch%20size%20and%0Asetting%20optimizer%20hyperparameters.%20We%20further%20recommend%20against%20gradient%0Aaccumulation%20unless%20training%20on%20multiple%20devices%20with%20multiple%20model%20replicas%2C%0Abottlenecked%20by%20inter-device%20bandwidth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmall%2520Batch%2520Size%2520Training%2520for%2520Language%2520Models%253A%2520When%2520Vanilla%2520SGD%2520Works%252C%250A%2520%2520and%2520Why%2520Gradient%2520Accumulation%2520Is%2520Wasteful%26entry.906535625%3DMartin%2520Marek%2520and%2520Sanae%2520Lotfi%2520and%2520Aditya%2520Somasundaram%2520and%2520Andrew%2520Gordon%2520Wilson%2520and%2520Micah%2520Goldblum%26entry.1292438233%3D%2520%2520Conventional%2520wisdom%2520dictates%2520that%2520small%2520batch%2520sizes%2520make%2520language%2520model%250Apretraining%2520and%2520fine-tuning%2520unstable%252C%2520motivating%2520gradient%2520accumulation%252C%2520which%250Atrades%2520off%2520the%2520number%2520of%2520optimizer%2520steps%2520for%2520a%2520proportional%2520increase%2520in%2520batch%250Asize.%2520While%2520it%2520is%2520common%2520to%2520decrease%2520the%2520learning%2520rate%2520for%2520smaller%2520batch%2520sizes%252C%250Aother%2520hyperparameters%2520are%2520often%2520held%2520fixed.%2520In%2520this%2520work%252C%2520we%2520revisit%2520small%250Abatch%2520sizes%2520all%2520the%2520way%2520down%2520to%2520batch%2520size%2520one%252C%2520and%2520we%2520propose%2520a%2520rule%2520for%250Ascaling%2520Adam%2520hyperparameters%2520to%2520small%2520batch%2520sizes.%2520We%2520find%2520that%2520small%2520batch%250Asizes%2520%25281%2529%2520train%2520stably%252C%2520%25282%2529%2520are%2520consistently%2520more%2520robust%2520to%2520hyperparameter%250Achoices%252C%2520%25283%2529%2520achieve%2520equal%2520or%2520better%2520per-FLOP%2520performance%2520than%2520larger%2520batch%250Asizes%252C%2520and%2520%25284%2529%2520notably%2520enable%2520stable%2520language%2520model%2520training%2520with%2520vanilla%2520SGD%252C%250Aeven%2520without%2520momentum%252C%2520despite%2520storing%2520no%2520optimizer%2520state.%2520Building%2520on%2520these%250Aresults%252C%2520we%2520provide%2520practical%2520recommendations%2520for%2520selecting%2520a%2520batch%2520size%2520and%250Asetting%2520optimizer%2520hyperparameters.%2520We%2520further%2520recommend%2520against%2520gradient%250Aaccumulation%2520unless%2520training%2520on%2520multiple%2520devices%2520with%2520multiple%2520model%2520replicas%252C%250Abottlenecked%2520by%2520inter-device%2520bandwidth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small%20Batch%20Size%20Training%20for%20Language%20Models%3A%20When%20Vanilla%20SGD%20Works%2C%0A%20%20and%20Why%20Gradient%20Accumulation%20Is%20Wasteful&entry.906535625=Martin%20Marek%20and%20Sanae%20Lotfi%20and%20Aditya%20Somasundaram%20and%20Andrew%20Gordon%20Wilson%20and%20Micah%20Goldblum&entry.1292438233=%20%20Conventional%20wisdom%20dictates%20that%20small%20batch%20sizes%20make%20language%20model%0Apretraining%20and%20fine-tuning%20unstable%2C%20motivating%20gradient%20accumulation%2C%20which%0Atrades%20off%20the%20number%20of%20optimizer%20steps%20for%20a%20proportional%20increase%20in%20batch%0Asize.%20While%20it%20is%20common%20to%20decrease%20the%20learning%20rate%20for%20smaller%20batch%20sizes%2C%0Aother%20hyperparameters%20are%20often%20held%20fixed.%20In%20this%20work%2C%20we%20revisit%20small%0Abatch%20sizes%20all%20the%20way%20down%20to%20batch%20size%20one%2C%20and%20we%20propose%20a%20rule%20for%0Ascaling%20Adam%20hyperparameters%20to%20small%20batch%20sizes.%20We%20find%20that%20small%20batch%0Asizes%20%281%29%20train%20stably%2C%20%282%29%20are%20consistently%20more%20robust%20to%20hyperparameter%0Achoices%2C%20%283%29%20achieve%20equal%20or%20better%20per-FLOP%20performance%20than%20larger%20batch%0Asizes%2C%20and%20%284%29%20notably%20enable%20stable%20language%20model%20training%20with%20vanilla%20SGD%2C%0Aeven%20without%20momentum%2C%20despite%20storing%20no%20optimizer%20state.%20Building%20on%20these%0Aresults%2C%20we%20provide%20practical%20recommendations%20for%20selecting%20a%20batch%20size%20and%0Asetting%20optimizer%20hyperparameters.%20We%20further%20recommend%20against%20gradient%0Aaccumulation%20unless%20training%20on%20multiple%20devices%20with%20multiple%20model%20replicas%2C%0Abottlenecked%20by%20inter-device%20bandwidth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07101v1&entry.124074799=Read"},
{"title": "Multi-Attribute Steering of Language Models via Targeted Intervention", "author": "Duy Nguyen and Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal", "abstract": "  Inference-time intervention (ITI) has emerged as a promising method for\nsteering large language model (LLM) behavior in a particular direction (e.g.,\nimproving helpfulness) by intervening on token representations without costly\nupdates to the LLM's parameters. However, existing ITI approaches fail to scale\nto multi-attribute settings with conflicts, such as enhancing helpfulness while\nalso reducing toxicity. To address this, we introduce Multi-Attribute Targeted\nSteering (MAT-Steer), a novel steering framework designed for selective\ntoken-level intervention across multiple attributes. MAT-Steer learns steering\nvectors using an alignment objective that shifts the model's internal\nrepresentations of undesirable outputs closer to those of desirable ones while\nenforcing sparsity and orthogonality among vectors for different attributes,\nthereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two\ndistinct settings: (i) on question answering (QA) tasks where we balance\nattributes like truthfulness, bias, and toxicity; (ii) on generative tasks\nwhere we simultaneously improve attributes like helpfulness, correctness, and\ncoherence. MAT-Steer outperforms existing ITI and parameter-efficient\nfine-tuning approaches across both task types (e.g., 3% average accuracy gain\nacross QA tasks and 55.82% win rate against the best ITI baseline).\n", "link": "http://arxiv.org/abs/2502.12446v2", "date": "2025-07-09", "relevancy": 1.4887, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5165}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4736}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Attribute%20Steering%20of%20Language%20Models%20via%20Targeted%20Intervention&body=Title%3A%20Multi-Attribute%20Steering%20of%20Language%20Models%20via%20Targeted%20Intervention%0AAuthor%3A%20Duy%20Nguyen%20and%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Inference-time%20intervention%20%28ITI%29%20has%20emerged%20as%20a%20promising%20method%20for%0Asteering%20large%20language%20model%20%28LLM%29%20behavior%20in%20a%20particular%20direction%20%28e.g.%2C%0Aimproving%20helpfulness%29%20by%20intervening%20on%20token%20representations%20without%20costly%0Aupdates%20to%20the%20LLM%27s%20parameters.%20However%2C%20existing%20ITI%20approaches%20fail%20to%20scale%0Ato%20multi-attribute%20settings%20with%20conflicts%2C%20such%20as%20enhancing%20helpfulness%20while%0Aalso%20reducing%20toxicity.%20To%20address%20this%2C%20we%20introduce%20Multi-Attribute%20Targeted%0ASteering%20%28MAT-Steer%29%2C%20a%20novel%20steering%20framework%20designed%20for%20selective%0Atoken-level%20intervention%20across%20multiple%20attributes.%20MAT-Steer%20learns%20steering%0Avectors%20using%20an%20alignment%20objective%20that%20shifts%20the%20model%27s%20internal%0Arepresentations%20of%20undesirable%20outputs%20closer%20to%20those%20of%20desirable%20ones%20while%0Aenforcing%20sparsity%20and%20orthogonality%20among%20vectors%20for%20different%20attributes%2C%0Athereby%20reducing%20inter-attribute%20conflicts.%20We%20evaluate%20MAT-Steer%20in%20two%0Adistinct%20settings%3A%20%28i%29%20on%20question%20answering%20%28QA%29%20tasks%20where%20we%20balance%0Aattributes%20like%20truthfulness%2C%20bias%2C%20and%20toxicity%3B%20%28ii%29%20on%20generative%20tasks%0Awhere%20we%20simultaneously%20improve%20attributes%20like%20helpfulness%2C%20correctness%2C%20and%0Acoherence.%20MAT-Steer%20outperforms%20existing%20ITI%20and%20parameter-efficient%0Afine-tuning%20approaches%20across%20both%20task%20types%20%28e.g.%2C%203%25%20average%20accuracy%20gain%0Aacross%20QA%20tasks%20and%2055.82%25%20win%20rate%20against%20the%20best%20ITI%20baseline%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12446v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Attribute%2520Steering%2520of%2520Language%2520Models%2520via%2520Targeted%2520Intervention%26entry.906535625%3DDuy%2520Nguyen%2520and%2520Archiki%2520Prasad%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Inference-time%2520intervention%2520%2528ITI%2529%2520has%2520emerged%2520as%2520a%2520promising%2520method%2520for%250Asteering%2520large%2520language%2520model%2520%2528LLM%2529%2520behavior%2520in%2520a%2520particular%2520direction%2520%2528e.g.%252C%250Aimproving%2520helpfulness%2529%2520by%2520intervening%2520on%2520token%2520representations%2520without%2520costly%250Aupdates%2520to%2520the%2520LLM%2527s%2520parameters.%2520However%252C%2520existing%2520ITI%2520approaches%2520fail%2520to%2520scale%250Ato%2520multi-attribute%2520settings%2520with%2520conflicts%252C%2520such%2520as%2520enhancing%2520helpfulness%2520while%250Aalso%2520reducing%2520toxicity.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Multi-Attribute%2520Targeted%250ASteering%2520%2528MAT-Steer%2529%252C%2520a%2520novel%2520steering%2520framework%2520designed%2520for%2520selective%250Atoken-level%2520intervention%2520across%2520multiple%2520attributes.%2520MAT-Steer%2520learns%2520steering%250Avectors%2520using%2520an%2520alignment%2520objective%2520that%2520shifts%2520the%2520model%2527s%2520internal%250Arepresentations%2520of%2520undesirable%2520outputs%2520closer%2520to%2520those%2520of%2520desirable%2520ones%2520while%250Aenforcing%2520sparsity%2520and%2520orthogonality%2520among%2520vectors%2520for%2520different%2520attributes%252C%250Athereby%2520reducing%2520inter-attribute%2520conflicts.%2520We%2520evaluate%2520MAT-Steer%2520in%2520two%250Adistinct%2520settings%253A%2520%2528i%2529%2520on%2520question%2520answering%2520%2528QA%2529%2520tasks%2520where%2520we%2520balance%250Aattributes%2520like%2520truthfulness%252C%2520bias%252C%2520and%2520toxicity%253B%2520%2528ii%2529%2520on%2520generative%2520tasks%250Awhere%2520we%2520simultaneously%2520improve%2520attributes%2520like%2520helpfulness%252C%2520correctness%252C%2520and%250Acoherence.%2520MAT-Steer%2520outperforms%2520existing%2520ITI%2520and%2520parameter-efficient%250Afine-tuning%2520approaches%2520across%2520both%2520task%2520types%2520%2528e.g.%252C%25203%2525%2520average%2520accuracy%2520gain%250Aacross%2520QA%2520tasks%2520and%252055.82%2525%2520win%2520rate%2520against%2520the%2520best%2520ITI%2520baseline%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12446v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Attribute%20Steering%20of%20Language%20Models%20via%20Targeted%20Intervention&entry.906535625=Duy%20Nguyen%20and%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal&entry.1292438233=%20%20Inference-time%20intervention%20%28ITI%29%20has%20emerged%20as%20a%20promising%20method%20for%0Asteering%20large%20language%20model%20%28LLM%29%20behavior%20in%20a%20particular%20direction%20%28e.g.%2C%0Aimproving%20helpfulness%29%20by%20intervening%20on%20token%20representations%20without%20costly%0Aupdates%20to%20the%20LLM%27s%20parameters.%20However%2C%20existing%20ITI%20approaches%20fail%20to%20scale%0Ato%20multi-attribute%20settings%20with%20conflicts%2C%20such%20as%20enhancing%20helpfulness%20while%0Aalso%20reducing%20toxicity.%20To%20address%20this%2C%20we%20introduce%20Multi-Attribute%20Targeted%0ASteering%20%28MAT-Steer%29%2C%20a%20novel%20steering%20framework%20designed%20for%20selective%0Atoken-level%20intervention%20across%20multiple%20attributes.%20MAT-Steer%20learns%20steering%0Avectors%20using%20an%20alignment%20objective%20that%20shifts%20the%20model%27s%20internal%0Arepresentations%20of%20undesirable%20outputs%20closer%20to%20those%20of%20desirable%20ones%20while%0Aenforcing%20sparsity%20and%20orthogonality%20among%20vectors%20for%20different%20attributes%2C%0Athereby%20reducing%20inter-attribute%20conflicts.%20We%20evaluate%20MAT-Steer%20in%20two%0Adistinct%20settings%3A%20%28i%29%20on%20question%20answering%20%28QA%29%20tasks%20where%20we%20balance%0Aattributes%20like%20truthfulness%2C%20bias%2C%20and%20toxicity%3B%20%28ii%29%20on%20generative%20tasks%0Awhere%20we%20simultaneously%20improve%20attributes%20like%20helpfulness%2C%20correctness%2C%20and%0Acoherence.%20MAT-Steer%20outperforms%20existing%20ITI%20and%20parameter-efficient%0Afine-tuning%20approaches%20across%20both%20task%20types%20%28e.g.%2C%203%25%20average%20accuracy%20gain%0Aacross%20QA%20tasks%20and%2055.82%25%20win%20rate%20against%20the%20best%20ITI%20baseline%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12446v2&entry.124074799=Read"},
{"title": "Non-Asymptotic Analysis of Online Local Private Learning with SGD", "author": "Enze Shi and Jinhan Xie and Bei Jiang and Linglong Kong and Xuming He", "abstract": "  Differentially Private Stochastic Gradient Descent (DP-SGD) has been widely\nused for solving optimization problems with privacy guarantees in machine\nlearning and statistics. Despite this, a systematic non-asymptotic convergence\nanalysis for DP-SGD, particularly in the context of online problems and local\ndifferential privacy (LDP) models, remains largely elusive. Existing\nnon-asymptotic analyses have focused on non-private optimization methods, and\nhence are not applicable to privacy-preserving optimization problems. This work\ninitiates the analysis to bridge this gap and opens the door to non-asymptotic\nconvergence analysis of private optimization problems. A general framework is\ninvestigated for the online LDP model in stochastic optimization problems. We\nassume that sensitive information from individuals is collected sequentially\nand aim to estimate, in real-time, a static parameter that pertains to the\npopulation of interest. Most importantly, we conduct a comprehensive\nnon-asymptotic convergence analysis of the proposed estimators in finite-sample\nsituations, which gives their users practical guidelines regarding the effect\nof various hyperparameters, such as step size, parameter dimensions, and\nprivacy budgets, on convergence rates. Our proposed estimators are validated in\nthe theoretical and practical realms by rigorous mathematical derivations and\ncarefully constructed numerical experiments.\n", "link": "http://arxiv.org/abs/2507.07041v1", "date": "2025-07-09", "relevancy": 1.7803, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4679}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4442}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Asymptotic%20Analysis%20of%20Online%20Local%20Private%20Learning%20with%20SGD&body=Title%3A%20Non-Asymptotic%20Analysis%20of%20Online%20Local%20Private%20Learning%20with%20SGD%0AAuthor%3A%20Enze%20Shi%20and%20Jinhan%20Xie%20and%20Bei%20Jiang%20and%20Linglong%20Kong%20and%20Xuming%20He%0AAbstract%3A%20%20%20Differentially%20Private%20Stochastic%20Gradient%20Descent%20%28DP-SGD%29%20has%20been%20widely%0Aused%20for%20solving%20optimization%20problems%20with%20privacy%20guarantees%20in%20machine%0Alearning%20and%20statistics.%20Despite%20this%2C%20a%20systematic%20non-asymptotic%20convergence%0Aanalysis%20for%20DP-SGD%2C%20particularly%20in%20the%20context%20of%20online%20problems%20and%20local%0Adifferential%20privacy%20%28LDP%29%20models%2C%20remains%20largely%20elusive.%20Existing%0Anon-asymptotic%20analyses%20have%20focused%20on%20non-private%20optimization%20methods%2C%20and%0Ahence%20are%20not%20applicable%20to%20privacy-preserving%20optimization%20problems.%20This%20work%0Ainitiates%20the%20analysis%20to%20bridge%20this%20gap%20and%20opens%20the%20door%20to%20non-asymptotic%0Aconvergence%20analysis%20of%20private%20optimization%20problems.%20A%20general%20framework%20is%0Ainvestigated%20for%20the%20online%20LDP%20model%20in%20stochastic%20optimization%20problems.%20We%0Aassume%20that%20sensitive%20information%20from%20individuals%20is%20collected%20sequentially%0Aand%20aim%20to%20estimate%2C%20in%20real-time%2C%20a%20static%20parameter%20that%20pertains%20to%20the%0Apopulation%20of%20interest.%20Most%20importantly%2C%20we%20conduct%20a%20comprehensive%0Anon-asymptotic%20convergence%20analysis%20of%20the%20proposed%20estimators%20in%20finite-sample%0Asituations%2C%20which%20gives%20their%20users%20practical%20guidelines%20regarding%20the%20effect%0Aof%20various%20hyperparameters%2C%20such%20as%20step%20size%2C%20parameter%20dimensions%2C%20and%0Aprivacy%20budgets%2C%20on%20convergence%20rates.%20Our%20proposed%20estimators%20are%20validated%20in%0Athe%20theoretical%20and%20practical%20realms%20by%20rigorous%20mathematical%20derivations%20and%0Acarefully%20constructed%20numerical%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Asymptotic%2520Analysis%2520of%2520Online%2520Local%2520Private%2520Learning%2520with%2520SGD%26entry.906535625%3DEnze%2520Shi%2520and%2520Jinhan%2520Xie%2520and%2520Bei%2520Jiang%2520and%2520Linglong%2520Kong%2520and%2520Xuming%2520He%26entry.1292438233%3D%2520%2520Differentially%2520Private%2520Stochastic%2520Gradient%2520Descent%2520%2528DP-SGD%2529%2520has%2520been%2520widely%250Aused%2520for%2520solving%2520optimization%2520problems%2520with%2520privacy%2520guarantees%2520in%2520machine%250Alearning%2520and%2520statistics.%2520Despite%2520this%252C%2520a%2520systematic%2520non-asymptotic%2520convergence%250Aanalysis%2520for%2520DP-SGD%252C%2520particularly%2520in%2520the%2520context%2520of%2520online%2520problems%2520and%2520local%250Adifferential%2520privacy%2520%2528LDP%2529%2520models%252C%2520remains%2520largely%2520elusive.%2520Existing%250Anon-asymptotic%2520analyses%2520have%2520focused%2520on%2520non-private%2520optimization%2520methods%252C%2520and%250Ahence%2520are%2520not%2520applicable%2520to%2520privacy-preserving%2520optimization%2520problems.%2520This%2520work%250Ainitiates%2520the%2520analysis%2520to%2520bridge%2520this%2520gap%2520and%2520opens%2520the%2520door%2520to%2520non-asymptotic%250Aconvergence%2520analysis%2520of%2520private%2520optimization%2520problems.%2520A%2520general%2520framework%2520is%250Ainvestigated%2520for%2520the%2520online%2520LDP%2520model%2520in%2520stochastic%2520optimization%2520problems.%2520We%250Aassume%2520that%2520sensitive%2520information%2520from%2520individuals%2520is%2520collected%2520sequentially%250Aand%2520aim%2520to%2520estimate%252C%2520in%2520real-time%252C%2520a%2520static%2520parameter%2520that%2520pertains%2520to%2520the%250Apopulation%2520of%2520interest.%2520Most%2520importantly%252C%2520we%2520conduct%2520a%2520comprehensive%250Anon-asymptotic%2520convergence%2520analysis%2520of%2520the%2520proposed%2520estimators%2520in%2520finite-sample%250Asituations%252C%2520which%2520gives%2520their%2520users%2520practical%2520guidelines%2520regarding%2520the%2520effect%250Aof%2520various%2520hyperparameters%252C%2520such%2520as%2520step%2520size%252C%2520parameter%2520dimensions%252C%2520and%250Aprivacy%2520budgets%252C%2520on%2520convergence%2520rates.%2520Our%2520proposed%2520estimators%2520are%2520validated%2520in%250Athe%2520theoretical%2520and%2520practical%2520realms%2520by%2520rigorous%2520mathematical%2520derivations%2520and%250Acarefully%2520constructed%2520numerical%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Asymptotic%20Analysis%20of%20Online%20Local%20Private%20Learning%20with%20SGD&entry.906535625=Enze%20Shi%20and%20Jinhan%20Xie%20and%20Bei%20Jiang%20and%20Linglong%20Kong%20and%20Xuming%20He&entry.1292438233=%20%20Differentially%20Private%20Stochastic%20Gradient%20Descent%20%28DP-SGD%29%20has%20been%20widely%0Aused%20for%20solving%20optimization%20problems%20with%20privacy%20guarantees%20in%20machine%0Alearning%20and%20statistics.%20Despite%20this%2C%20a%20systematic%20non-asymptotic%20convergence%0Aanalysis%20for%20DP-SGD%2C%20particularly%20in%20the%20context%20of%20online%20problems%20and%20local%0Adifferential%20privacy%20%28LDP%29%20models%2C%20remains%20largely%20elusive.%20Existing%0Anon-asymptotic%20analyses%20have%20focused%20on%20non-private%20optimization%20methods%2C%20and%0Ahence%20are%20not%20applicable%20to%20privacy-preserving%20optimization%20problems.%20This%20work%0Ainitiates%20the%20analysis%20to%20bridge%20this%20gap%20and%20opens%20the%20door%20to%20non-asymptotic%0Aconvergence%20analysis%20of%20private%20optimization%20problems.%20A%20general%20framework%20is%0Ainvestigated%20for%20the%20online%20LDP%20model%20in%20stochastic%20optimization%20problems.%20We%0Aassume%20that%20sensitive%20information%20from%20individuals%20is%20collected%20sequentially%0Aand%20aim%20to%20estimate%2C%20in%20real-time%2C%20a%20static%20parameter%20that%20pertains%20to%20the%0Apopulation%20of%20interest.%20Most%20importantly%2C%20we%20conduct%20a%20comprehensive%0Anon-asymptotic%20convergence%20analysis%20of%20the%20proposed%20estimators%20in%20finite-sample%0Asituations%2C%20which%20gives%20their%20users%20practical%20guidelines%20regarding%20the%20effect%0Aof%20various%20hyperparameters%2C%20such%20as%20step%20size%2C%20parameter%20dimensions%2C%20and%0Aprivacy%20budgets%2C%20on%20convergence%20rates.%20Our%20proposed%20estimators%20are%20validated%20in%0Athe%20theoretical%20and%20practical%20realms%20by%20rigorous%20mathematical%20derivations%20and%0Acarefully%20constructed%20numerical%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07041v1&entry.124074799=Read"},
{"title": "On-Device Training of PV Power Forecasting Models in a Smart Meter for\n  Grid Edge Intelligence", "author": "Jian Huang and Yongli Zhu and Linna Xu and Zhe Zheng and Wenpeng Cui and Mingyang Sun", "abstract": "  In this paper, an edge-side model training study is conducted on a\nresource-limited smart meter. The motivation of grid-edge intelligence and the\nconcept of on-device training are introduced. Then, the technical preparation\nsteps for on-device training are described. A case study on the task of\nphotovoltaic power forecasting is presented, where two representative machine\nlearning models are investigated: a gradient boosting tree model and a\nrecurrent neural network model. To adapt to the resource-limited situation in\nthe smart meter, \"mixed\"- and \"reduced\"-precision training schemes are also\ndevised. Experiment results demonstrate the feasibility of economically\nachieving grid-edge intelligence via the existing advanced metering\ninfrastructures.\n", "link": "http://arxiv.org/abs/2507.07016v1", "date": "2025-07-09", "relevancy": 1.7183, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4396}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4356}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-Device%20Training%20of%20PV%20Power%20Forecasting%20Models%20in%20a%20Smart%20Meter%20for%0A%20%20Grid%20Edge%20Intelligence&body=Title%3A%20On-Device%20Training%20of%20PV%20Power%20Forecasting%20Models%20in%20a%20Smart%20Meter%20for%0A%20%20Grid%20Edge%20Intelligence%0AAuthor%3A%20Jian%20Huang%20and%20Yongli%20Zhu%20and%20Linna%20Xu%20and%20Zhe%20Zheng%20and%20Wenpeng%20Cui%20and%20Mingyang%20Sun%0AAbstract%3A%20%20%20In%20this%20paper%2C%20an%20edge-side%20model%20training%20study%20is%20conducted%20on%20a%0Aresource-limited%20smart%20meter.%20The%20motivation%20of%20grid-edge%20intelligence%20and%20the%0Aconcept%20of%20on-device%20training%20are%20introduced.%20Then%2C%20the%20technical%20preparation%0Asteps%20for%20on-device%20training%20are%20described.%20A%20case%20study%20on%20the%20task%20of%0Aphotovoltaic%20power%20forecasting%20is%20presented%2C%20where%20two%20representative%20machine%0Alearning%20models%20are%20investigated%3A%20a%20gradient%20boosting%20tree%20model%20and%20a%0Arecurrent%20neural%20network%20model.%20To%20adapt%20to%20the%20resource-limited%20situation%20in%0Athe%20smart%20meter%2C%20%22mixed%22-%20and%20%22reduced%22-precision%20training%20schemes%20are%20also%0Adevised.%20Experiment%20results%20demonstrate%20the%20feasibility%20of%20economically%0Aachieving%20grid-edge%20intelligence%20via%20the%20existing%20advanced%20metering%0Ainfrastructures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-Device%2520Training%2520of%2520PV%2520Power%2520Forecasting%2520Models%2520in%2520a%2520Smart%2520Meter%2520for%250A%2520%2520Grid%2520Edge%2520Intelligence%26entry.906535625%3DJian%2520Huang%2520and%2520Yongli%2520Zhu%2520and%2520Linna%2520Xu%2520and%2520Zhe%2520Zheng%2520and%2520Wenpeng%2520Cui%2520and%2520Mingyang%2520Sun%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520an%2520edge-side%2520model%2520training%2520study%2520is%2520conducted%2520on%2520a%250Aresource-limited%2520smart%2520meter.%2520The%2520motivation%2520of%2520grid-edge%2520intelligence%2520and%2520the%250Aconcept%2520of%2520on-device%2520training%2520are%2520introduced.%2520Then%252C%2520the%2520technical%2520preparation%250Asteps%2520for%2520on-device%2520training%2520are%2520described.%2520A%2520case%2520study%2520on%2520the%2520task%2520of%250Aphotovoltaic%2520power%2520forecasting%2520is%2520presented%252C%2520where%2520two%2520representative%2520machine%250Alearning%2520models%2520are%2520investigated%253A%2520a%2520gradient%2520boosting%2520tree%2520model%2520and%2520a%250Arecurrent%2520neural%2520network%2520model.%2520To%2520adapt%2520to%2520the%2520resource-limited%2520situation%2520in%250Athe%2520smart%2520meter%252C%2520%2522mixed%2522-%2520and%2520%2522reduced%2522-precision%2520training%2520schemes%2520are%2520also%250Adevised.%2520Experiment%2520results%2520demonstrate%2520the%2520feasibility%2520of%2520economically%250Aachieving%2520grid-edge%2520intelligence%2520via%2520the%2520existing%2520advanced%2520metering%250Ainfrastructures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-Device%20Training%20of%20PV%20Power%20Forecasting%20Models%20in%20a%20Smart%20Meter%20for%0A%20%20Grid%20Edge%20Intelligence&entry.906535625=Jian%20Huang%20and%20Yongli%20Zhu%20and%20Linna%20Xu%20and%20Zhe%20Zheng%20and%20Wenpeng%20Cui%20and%20Mingyang%20Sun&entry.1292438233=%20%20In%20this%20paper%2C%20an%20edge-side%20model%20training%20study%20is%20conducted%20on%20a%0Aresource-limited%20smart%20meter.%20The%20motivation%20of%20grid-edge%20intelligence%20and%20the%0Aconcept%20of%20on-device%20training%20are%20introduced.%20Then%2C%20the%20technical%20preparation%0Asteps%20for%20on-device%20training%20are%20described.%20A%20case%20study%20on%20the%20task%20of%0Aphotovoltaic%20power%20forecasting%20is%20presented%2C%20where%20two%20representative%20machine%0Alearning%20models%20are%20investigated%3A%20a%20gradient%20boosting%20tree%20model%20and%20a%0Arecurrent%20neural%20network%20model.%20To%20adapt%20to%20the%20resource-limited%20situation%20in%0Athe%20smart%20meter%2C%20%22mixed%22-%20and%20%22reduced%22-precision%20training%20schemes%20are%20also%0Adevised.%20Experiment%20results%20demonstrate%20the%20feasibility%20of%20economically%0Aachieving%20grid-edge%20intelligence%20via%20the%20existing%20advanced%20metering%0Ainfrastructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07016v1&entry.124074799=Read"},
{"title": "Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A\n  Validated Approach to Task Ordering in Cooperative Coordination Environments", "author": "Farhaan Ebadulla and Dharini Hindlatti and Srinivaasan NS and Apoorva VH and Ayman Aftab", "abstract": "  Multi-agent reinforcement learning (MARL) faces significant challenges in\ntask sequencing and curriculum design, particularly for cooperative\ncoordination scenarios. While curriculum learning has demonstrated success in\nsingle-agent domains, principled approaches for multi-agent coordination remain\nlimited due to the absence of validated task complexity metrics. This approach\npresents a graph-based coordination complexity metric that integrates agent\ndependency entropy, spatial interference patterns, and goal overlap analysis to\npredict task difficulty in multi-agent environments. The complexity metric\nachieves strong empirical validation with rho = 0.952 correlation (p < 0.001)\nbetween predicted complexity and empirical difficulty determined by random\nagent performance evaluation. This approach evaluates the curriculum learning\nframework using MADDPG across two distinct coordination environments: achieving\n56x performance improvement in tight coordination tasks (MultiWalker) and\ndemonstrating systematic task progression in cooperative navigation (Simple\nSpread). Through systematic analysis, coordination tightness emerges as a\npredictor of curriculum learning effectiveness, where environments requiring\nstrict agent interdependence benefit substantially from structured progression.\nThis approach provides a validated complexity metric for multi-agent curriculum\ndesign and establishes empirical guidelines for multi-robot coordination\napplications.\n", "link": "http://arxiv.org/abs/2507.07074v1", "date": "2025-07-09", "relevancy": 1.5585, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5593}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5379}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Based%20Complexity%20Metrics%20for%20Multi-Agent%20Curriculum%20Learning%3A%20A%0A%20%20Validated%20Approach%20to%20Task%20Ordering%20in%20Cooperative%20Coordination%20Environments&body=Title%3A%20Graph-Based%20Complexity%20Metrics%20for%20Multi-Agent%20Curriculum%20Learning%3A%20A%0A%20%20Validated%20Approach%20to%20Task%20Ordering%20in%20Cooperative%20Coordination%20Environments%0AAuthor%3A%20Farhaan%20Ebadulla%20and%20Dharini%20Hindlatti%20and%20Srinivaasan%20NS%20and%20Apoorva%20VH%20and%20Ayman%20Aftab%0AAbstract%3A%20%20%20Multi-agent%20reinforcement%20learning%20%28MARL%29%20faces%20significant%20challenges%20in%0Atask%20sequencing%20and%20curriculum%20design%2C%20particularly%20for%20cooperative%0Acoordination%20scenarios.%20While%20curriculum%20learning%20has%20demonstrated%20success%20in%0Asingle-agent%20domains%2C%20principled%20approaches%20for%20multi-agent%20coordination%20remain%0Alimited%20due%20to%20the%20absence%20of%20validated%20task%20complexity%20metrics.%20This%20approach%0Apresents%20a%20graph-based%20coordination%20complexity%20metric%20that%20integrates%20agent%0Adependency%20entropy%2C%20spatial%20interference%20patterns%2C%20and%20goal%20overlap%20analysis%20to%0Apredict%20task%20difficulty%20in%20multi-agent%20environments.%20The%20complexity%20metric%0Aachieves%20strong%20empirical%20validation%20with%20rho%20%3D%200.952%20correlation%20%28p%20%3C%200.001%29%0Abetween%20predicted%20complexity%20and%20empirical%20difficulty%20determined%20by%20random%0Aagent%20performance%20evaluation.%20This%20approach%20evaluates%20the%20curriculum%20learning%0Aframework%20using%20MADDPG%20across%20two%20distinct%20coordination%20environments%3A%20achieving%0A56x%20performance%20improvement%20in%20tight%20coordination%20tasks%20%28MultiWalker%29%20and%0Ademonstrating%20systematic%20task%20progression%20in%20cooperative%20navigation%20%28Simple%0ASpread%29.%20Through%20systematic%20analysis%2C%20coordination%20tightness%20emerges%20as%20a%0Apredictor%20of%20curriculum%20learning%20effectiveness%2C%20where%20environments%20requiring%0Astrict%20agent%20interdependence%20benefit%20substantially%20from%20structured%20progression.%0AThis%20approach%20provides%20a%20validated%20complexity%20metric%20for%20multi-agent%20curriculum%0Adesign%20and%20establishes%20empirical%20guidelines%20for%20multi-robot%20coordination%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Based%2520Complexity%2520Metrics%2520for%2520Multi-Agent%2520Curriculum%2520Learning%253A%2520A%250A%2520%2520Validated%2520Approach%2520to%2520Task%2520Ordering%2520in%2520Cooperative%2520Coordination%2520Environments%26entry.906535625%3DFarhaan%2520Ebadulla%2520and%2520Dharini%2520Hindlatti%2520and%2520Srinivaasan%2520NS%2520and%2520Apoorva%2520VH%2520and%2520Ayman%2520Aftab%26entry.1292438233%3D%2520%2520Multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520faces%2520significant%2520challenges%2520in%250Atask%2520sequencing%2520and%2520curriculum%2520design%252C%2520particularly%2520for%2520cooperative%250Acoordination%2520scenarios.%2520While%2520curriculum%2520learning%2520has%2520demonstrated%2520success%2520in%250Asingle-agent%2520domains%252C%2520principled%2520approaches%2520for%2520multi-agent%2520coordination%2520remain%250Alimited%2520due%2520to%2520the%2520absence%2520of%2520validated%2520task%2520complexity%2520metrics.%2520This%2520approach%250Apresents%2520a%2520graph-based%2520coordination%2520complexity%2520metric%2520that%2520integrates%2520agent%250Adependency%2520entropy%252C%2520spatial%2520interference%2520patterns%252C%2520and%2520goal%2520overlap%2520analysis%2520to%250Apredict%2520task%2520difficulty%2520in%2520multi-agent%2520environments.%2520The%2520complexity%2520metric%250Aachieves%2520strong%2520empirical%2520validation%2520with%2520rho%2520%253D%25200.952%2520correlation%2520%2528p%2520%253C%25200.001%2529%250Abetween%2520predicted%2520complexity%2520and%2520empirical%2520difficulty%2520determined%2520by%2520random%250Aagent%2520performance%2520evaluation.%2520This%2520approach%2520evaluates%2520the%2520curriculum%2520learning%250Aframework%2520using%2520MADDPG%2520across%2520two%2520distinct%2520coordination%2520environments%253A%2520achieving%250A56x%2520performance%2520improvement%2520in%2520tight%2520coordination%2520tasks%2520%2528MultiWalker%2529%2520and%250Ademonstrating%2520systematic%2520task%2520progression%2520in%2520cooperative%2520navigation%2520%2528Simple%250ASpread%2529.%2520Through%2520systematic%2520analysis%252C%2520coordination%2520tightness%2520emerges%2520as%2520a%250Apredictor%2520of%2520curriculum%2520learning%2520effectiveness%252C%2520where%2520environments%2520requiring%250Astrict%2520agent%2520interdependence%2520benefit%2520substantially%2520from%2520structured%2520progression.%250AThis%2520approach%2520provides%2520a%2520validated%2520complexity%2520metric%2520for%2520multi-agent%2520curriculum%250Adesign%2520and%2520establishes%2520empirical%2520guidelines%2520for%2520multi-robot%2520coordination%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Based%20Complexity%20Metrics%20for%20Multi-Agent%20Curriculum%20Learning%3A%20A%0A%20%20Validated%20Approach%20to%20Task%20Ordering%20in%20Cooperative%20Coordination%20Environments&entry.906535625=Farhaan%20Ebadulla%20and%20Dharini%20Hindlatti%20and%20Srinivaasan%20NS%20and%20Apoorva%20VH%20and%20Ayman%20Aftab&entry.1292438233=%20%20Multi-agent%20reinforcement%20learning%20%28MARL%29%20faces%20significant%20challenges%20in%0Atask%20sequencing%20and%20curriculum%20design%2C%20particularly%20for%20cooperative%0Acoordination%20scenarios.%20While%20curriculum%20learning%20has%20demonstrated%20success%20in%0Asingle-agent%20domains%2C%20principled%20approaches%20for%20multi-agent%20coordination%20remain%0Alimited%20due%20to%20the%20absence%20of%20validated%20task%20complexity%20metrics.%20This%20approach%0Apresents%20a%20graph-based%20coordination%20complexity%20metric%20that%20integrates%20agent%0Adependency%20entropy%2C%20spatial%20interference%20patterns%2C%20and%20goal%20overlap%20analysis%20to%0Apredict%20task%20difficulty%20in%20multi-agent%20environments.%20The%20complexity%20metric%0Aachieves%20strong%20empirical%20validation%20with%20rho%20%3D%200.952%20correlation%20%28p%20%3C%200.001%29%0Abetween%20predicted%20complexity%20and%20empirical%20difficulty%20determined%20by%20random%0Aagent%20performance%20evaluation.%20This%20approach%20evaluates%20the%20curriculum%20learning%0Aframework%20using%20MADDPG%20across%20two%20distinct%20coordination%20environments%3A%20achieving%0A56x%20performance%20improvement%20in%20tight%20coordination%20tasks%20%28MultiWalker%29%20and%0Ademonstrating%20systematic%20task%20progression%20in%20cooperative%20navigation%20%28Simple%0ASpread%29.%20Through%20systematic%20analysis%2C%20coordination%20tightness%20emerges%20as%20a%0Apredictor%20of%20curriculum%20learning%20effectiveness%2C%20where%20environments%20requiring%0Astrict%20agent%20interdependence%20benefit%20substantially%20from%20structured%20progression.%0AThis%20approach%20provides%20a%20validated%20complexity%20metric%20for%20multi-agent%20curriculum%0Adesign%20and%20establishes%20empirical%20guidelines%20for%20multi-robot%20coordination%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07074v1&entry.124074799=Read"},
{"title": "Noisy PDE Training Requires Bigger PINNs", "author": "Sebastien Andre-Sloan and Anirbit Mukherjee and Matthew Colbrook", "abstract": "  Physics-Informed Neural Networks (PINNs) are increasingly used to approximate\nsolutions of partial differential equations (PDEs), especially in high\ndimensions. In real-world applications, data samples are noisy, so it is\nimportant to know when a predictor can still achieve low empirical risk.\nHowever, little is known about the conditions under which a PINN can do so\neffectively. We prove a lower bound on the size of neural networks required for\nthe supervised PINN empirical risk to fall below the variance of noisy\nsupervision labels. Specifically, if a predictor achieves an empirical risk\n$O(\\eta)$ below $\\sigma^2$ (variance of supervision data), then necessarily\n$d_N\\log d_N\\gtrsim N_s \\eta^2$, where $N_s$ is the number of samples and $d_N$\nis the number of trainable parameters of the PINN. A similar constraint applies\nto the fully unsupervised PINN setting when boundary labels are sampled\nnoisily. Consequently, increasing the number of noisy supervision labels alone\ndoes not provide a ``free lunch'' in reducing empirical risk. We also show\nempirically that PINNs can indeed achieve empirical risks below $\\sigma^2$\nunder such conditions. As a case study, we investigate PINNs applied to the\nHamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for\nquantitatively understanding the parameter requirements for training PINNs in\nthe presence of noise.\n", "link": "http://arxiv.org/abs/2507.06967v1", "date": "2025-07-09", "relevancy": 1.8039, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4627}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4463}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noisy%20PDE%20Training%20Requires%20Bigger%20PINNs&body=Title%3A%20Noisy%20PDE%20Training%20Requires%20Bigger%20PINNs%0AAuthor%3A%20Sebastien%20Andre-Sloan%20and%20Anirbit%20Mukherjee%20and%20Matthew%20Colbrook%0AAbstract%3A%20%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20are%20increasingly%20used%20to%20approximate%0Asolutions%20of%20partial%20differential%20equations%20%28PDEs%29%2C%20especially%20in%20high%0Adimensions.%20In%20real-world%20applications%2C%20data%20samples%20are%20noisy%2C%20so%20it%20is%0Aimportant%20to%20know%20when%20a%20predictor%20can%20still%20achieve%20low%20empirical%20risk.%0AHowever%2C%20little%20is%20known%20about%20the%20conditions%20under%20which%20a%20PINN%20can%20do%20so%0Aeffectively.%20We%20prove%20a%20lower%20bound%20on%20the%20size%20of%20neural%20networks%20required%20for%0Athe%20supervised%20PINN%20empirical%20risk%20to%20fall%20below%20the%20variance%20of%20noisy%0Asupervision%20labels.%20Specifically%2C%20if%20a%20predictor%20achieves%20an%20empirical%20risk%0A%24O%28%5Ceta%29%24%20below%20%24%5Csigma%5E2%24%20%28variance%20of%20supervision%20data%29%2C%20then%20necessarily%0A%24d_N%5Clog%20d_N%5Cgtrsim%20N_s%20%5Ceta%5E2%24%2C%20where%20%24N_s%24%20is%20the%20number%20of%20samples%20and%20%24d_N%24%0Ais%20the%20number%20of%20trainable%20parameters%20of%20the%20PINN.%20A%20similar%20constraint%20applies%0Ato%20the%20fully%20unsupervised%20PINN%20setting%20when%20boundary%20labels%20are%20sampled%0Anoisily.%20Consequently%2C%20increasing%20the%20number%20of%20noisy%20supervision%20labels%20alone%0Adoes%20not%20provide%20a%20%60%60free%20lunch%27%27%20in%20reducing%20empirical%20risk.%20We%20also%20show%0Aempirically%20that%20PINNs%20can%20indeed%20achieve%20empirical%20risks%20below%20%24%5Csigma%5E2%24%0Aunder%20such%20conditions.%20As%20a%20case%20study%2C%20we%20investigate%20PINNs%20applied%20to%20the%0AHamilton--Jacobi--Bellman%20%28HJB%29%20PDE.%20Our%20findings%20lay%20the%20groundwork%20for%0Aquantitatively%20understanding%20the%20parameter%20requirements%20for%20training%20PINNs%20in%0Athe%20presence%20of%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoisy%2520PDE%2520Training%2520Requires%2520Bigger%2520PINNs%26entry.906535625%3DSebastien%2520Andre-Sloan%2520and%2520Anirbit%2520Mukherjee%2520and%2520Matthew%2520Colbrook%26entry.1292438233%3D%2520%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520are%2520increasingly%2520used%2520to%2520approximate%250Asolutions%2520of%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%2520especially%2520in%2520high%250Adimensions.%2520In%2520real-world%2520applications%252C%2520data%2520samples%2520are%2520noisy%252C%2520so%2520it%2520is%250Aimportant%2520to%2520know%2520when%2520a%2520predictor%2520can%2520still%2520achieve%2520low%2520empirical%2520risk.%250AHowever%252C%2520little%2520is%2520known%2520about%2520the%2520conditions%2520under%2520which%2520a%2520PINN%2520can%2520do%2520so%250Aeffectively.%2520We%2520prove%2520a%2520lower%2520bound%2520on%2520the%2520size%2520of%2520neural%2520networks%2520required%2520for%250Athe%2520supervised%2520PINN%2520empirical%2520risk%2520to%2520fall%2520below%2520the%2520variance%2520of%2520noisy%250Asupervision%2520labels.%2520Specifically%252C%2520if%2520a%2520predictor%2520achieves%2520an%2520empirical%2520risk%250A%2524O%2528%255Ceta%2529%2524%2520below%2520%2524%255Csigma%255E2%2524%2520%2528variance%2520of%2520supervision%2520data%2529%252C%2520then%2520necessarily%250A%2524d_N%255Clog%2520d_N%255Cgtrsim%2520N_s%2520%255Ceta%255E2%2524%252C%2520where%2520%2524N_s%2524%2520is%2520the%2520number%2520of%2520samples%2520and%2520%2524d_N%2524%250Ais%2520the%2520number%2520of%2520trainable%2520parameters%2520of%2520the%2520PINN.%2520A%2520similar%2520constraint%2520applies%250Ato%2520the%2520fully%2520unsupervised%2520PINN%2520setting%2520when%2520boundary%2520labels%2520are%2520sampled%250Anoisily.%2520Consequently%252C%2520increasing%2520the%2520number%2520of%2520noisy%2520supervision%2520labels%2520alone%250Adoes%2520not%2520provide%2520a%2520%2560%2560free%2520lunch%2527%2527%2520in%2520reducing%2520empirical%2520risk.%2520We%2520also%2520show%250Aempirically%2520that%2520PINNs%2520can%2520indeed%2520achieve%2520empirical%2520risks%2520below%2520%2524%255Csigma%255E2%2524%250Aunder%2520such%2520conditions.%2520As%2520a%2520case%2520study%252C%2520we%2520investigate%2520PINNs%2520applied%2520to%2520the%250AHamilton--Jacobi--Bellman%2520%2528HJB%2529%2520PDE.%2520Our%2520findings%2520lay%2520the%2520groundwork%2520for%250Aquantitatively%2520understanding%2520the%2520parameter%2520requirements%2520for%2520training%2520PINNs%2520in%250Athe%2520presence%2520of%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noisy%20PDE%20Training%20Requires%20Bigger%20PINNs&entry.906535625=Sebastien%20Andre-Sloan%20and%20Anirbit%20Mukherjee%20and%20Matthew%20Colbrook&entry.1292438233=%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20are%20increasingly%20used%20to%20approximate%0Asolutions%20of%20partial%20differential%20equations%20%28PDEs%29%2C%20especially%20in%20high%0Adimensions.%20In%20real-world%20applications%2C%20data%20samples%20are%20noisy%2C%20so%20it%20is%0Aimportant%20to%20know%20when%20a%20predictor%20can%20still%20achieve%20low%20empirical%20risk.%0AHowever%2C%20little%20is%20known%20about%20the%20conditions%20under%20which%20a%20PINN%20can%20do%20so%0Aeffectively.%20We%20prove%20a%20lower%20bound%20on%20the%20size%20of%20neural%20networks%20required%20for%0Athe%20supervised%20PINN%20empirical%20risk%20to%20fall%20below%20the%20variance%20of%20noisy%0Asupervision%20labels.%20Specifically%2C%20if%20a%20predictor%20achieves%20an%20empirical%20risk%0A%24O%28%5Ceta%29%24%20below%20%24%5Csigma%5E2%24%20%28variance%20of%20supervision%20data%29%2C%20then%20necessarily%0A%24d_N%5Clog%20d_N%5Cgtrsim%20N_s%20%5Ceta%5E2%24%2C%20where%20%24N_s%24%20is%20the%20number%20of%20samples%20and%20%24d_N%24%0Ais%20the%20number%20of%20trainable%20parameters%20of%20the%20PINN.%20A%20similar%20constraint%20applies%0Ato%20the%20fully%20unsupervised%20PINN%20setting%20when%20boundary%20labels%20are%20sampled%0Anoisily.%20Consequently%2C%20increasing%20the%20number%20of%20noisy%20supervision%20labels%20alone%0Adoes%20not%20provide%20a%20%60%60free%20lunch%27%27%20in%20reducing%20empirical%20risk.%20We%20also%20show%0Aempirically%20that%20PINNs%20can%20indeed%20achieve%20empirical%20risks%20below%20%24%5Csigma%5E2%24%0Aunder%20such%20conditions.%20As%20a%20case%20study%2C%20we%20investigate%20PINNs%20applied%20to%20the%0AHamilton--Jacobi--Bellman%20%28HJB%29%20PDE.%20Our%20findings%20lay%20the%20groundwork%20for%0Aquantitatively%20understanding%20the%20parameter%20requirements%20for%20training%20PINNs%20in%0Athe%20presence%20of%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06967v1&entry.124074799=Read"},
{"title": "HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for\n  Emotion Distribution Learning", "author": "Chuhang Zheng and Chunwei Tian and Jie Wen and Daoqiang Zhang and Qi Zhu", "abstract": "  Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning.\n", "link": "http://arxiv.org/abs/2507.06821v1", "date": "2025-07-09", "relevancy": 1.5414, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5554}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.502}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeLo%3A%20Heterogeneous%20Multi-Modal%20Fusion%20with%20Label%20Correlation%20for%0A%20%20Emotion%20Distribution%20Learning&body=Title%3A%20HeLo%3A%20Heterogeneous%20Multi-Modal%20Fusion%20with%20Label%20Correlation%20for%0A%20%20Emotion%20Distribution%20Learning%0AAuthor%3A%20Chuhang%20Zheng%20and%20Chunwei%20Tian%20and%20Jie%20Wen%20and%20Daoqiang%20Zhang%20and%20Qi%20Zhu%0AAbstract%3A%20%20%20Multi-modal%20emotion%20recognition%20has%20garnered%20increasing%20attention%20as%20it%20plays%0Aa%20significant%20role%20in%20human-computer%20interaction%20%28HCI%29%20in%20recent%20years.%20Since%0Adifferent%20discrete%20emotions%20may%20exist%20at%20the%20same%20time%2C%20compared%20with%0Asingle-class%20emotion%20recognition%2C%20emotion%20distribution%20learning%20%28EDL%29%20that%0Aidentifies%20a%20mixture%20of%20basic%20emotions%20has%20gradually%20emerged%20as%20a%20trend.%0AHowever%2C%20existing%20EDL%20methods%20face%20challenges%20in%20mining%20the%20heterogeneity%20among%0Amultiple%20modalities.%20Besides%2C%20rich%20semantic%20correlations%20across%20arbitrary%20basic%0Aemotions%20are%20not%20fully%20exploited.%20In%20this%20paper%2C%20we%20propose%20a%20multi-modal%0Aemotion%20distribution%20learning%20framework%2C%20named%20HeLo%2C%20aimed%20at%20fully%20exploring%0Athe%20heterogeneity%20and%20complementary%20information%20in%20multi-modal%20emotional%20data%0Aand%20label%20correlation%20within%20mixed%20basic%20emotions.%20Specifically%2C%20we%20first%20adopt%0Across-attention%20to%20effectively%20fuse%20the%20physiological%20data.%20Then%2C%20an%20optimal%0Atransport%20%28OT%29-based%20heterogeneity%20mining%20module%20is%20devised%20to%20mine%20the%0Ainteraction%20and%20heterogeneity%20between%20the%20physiological%20and%20behavioral%0Arepresentations.%20To%20facilitate%20label%20correlation%20learning%2C%20we%20introduce%20a%0Alearnable%20label%20embedding%20optimized%20by%20correlation%20matrix%20alignment.%20Finally%2C%0Athe%20learnable%20label%20embeddings%20and%20label%20correlation%20matrices%20are%20integrated%0Awith%20the%20multi-modal%20representations%20through%20a%20novel%20label%20correlation-driven%0Across-attention%20mechanism%20for%20accurate%20emotion%20distribution%20learning.%0AExperimental%20results%20on%20two%20publicly%20available%20datasets%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20method%20in%20emotion%20distribution%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeLo%253A%2520Heterogeneous%2520Multi-Modal%2520Fusion%2520with%2520Label%2520Correlation%2520for%250A%2520%2520Emotion%2520Distribution%2520Learning%26entry.906535625%3DChuhang%2520Zheng%2520and%2520Chunwei%2520Tian%2520and%2520Jie%2520Wen%2520and%2520Daoqiang%2520Zhang%2520and%2520Qi%2520Zhu%26entry.1292438233%3D%2520%2520Multi-modal%2520emotion%2520recognition%2520has%2520garnered%2520increasing%2520attention%2520as%2520it%2520plays%250Aa%2520significant%2520role%2520in%2520human-computer%2520interaction%2520%2528HCI%2529%2520in%2520recent%2520years.%2520Since%250Adifferent%2520discrete%2520emotions%2520may%2520exist%2520at%2520the%2520same%2520time%252C%2520compared%2520with%250Asingle-class%2520emotion%2520recognition%252C%2520emotion%2520distribution%2520learning%2520%2528EDL%2529%2520that%250Aidentifies%2520a%2520mixture%2520of%2520basic%2520emotions%2520has%2520gradually%2520emerged%2520as%2520a%2520trend.%250AHowever%252C%2520existing%2520EDL%2520methods%2520face%2520challenges%2520in%2520mining%2520the%2520heterogeneity%2520among%250Amultiple%2520modalities.%2520Besides%252C%2520rich%2520semantic%2520correlations%2520across%2520arbitrary%2520basic%250Aemotions%2520are%2520not%2520fully%2520exploited.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520multi-modal%250Aemotion%2520distribution%2520learning%2520framework%252C%2520named%2520HeLo%252C%2520aimed%2520at%2520fully%2520exploring%250Athe%2520heterogeneity%2520and%2520complementary%2520information%2520in%2520multi-modal%2520emotional%2520data%250Aand%2520label%2520correlation%2520within%2520mixed%2520basic%2520emotions.%2520Specifically%252C%2520we%2520first%2520adopt%250Across-attention%2520to%2520effectively%2520fuse%2520the%2520physiological%2520data.%2520Then%252C%2520an%2520optimal%250Atransport%2520%2528OT%2529-based%2520heterogeneity%2520mining%2520module%2520is%2520devised%2520to%2520mine%2520the%250Ainteraction%2520and%2520heterogeneity%2520between%2520the%2520physiological%2520and%2520behavioral%250Arepresentations.%2520To%2520facilitate%2520label%2520correlation%2520learning%252C%2520we%2520introduce%2520a%250Alearnable%2520label%2520embedding%2520optimized%2520by%2520correlation%2520matrix%2520alignment.%2520Finally%252C%250Athe%2520learnable%2520label%2520embeddings%2520and%2520label%2520correlation%2520matrices%2520are%2520integrated%250Awith%2520the%2520multi-modal%2520representations%2520through%2520a%2520novel%2520label%2520correlation-driven%250Across-attention%2520mechanism%2520for%2520accurate%2520emotion%2520distribution%2520learning.%250AExperimental%2520results%2520on%2520two%2520publicly%2520available%2520datasets%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520proposed%2520method%2520in%2520emotion%2520distribution%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeLo%3A%20Heterogeneous%20Multi-Modal%20Fusion%20with%20Label%20Correlation%20for%0A%20%20Emotion%20Distribution%20Learning&entry.906535625=Chuhang%20Zheng%20and%20Chunwei%20Tian%20and%20Jie%20Wen%20and%20Daoqiang%20Zhang%20and%20Qi%20Zhu&entry.1292438233=%20%20Multi-modal%20emotion%20recognition%20has%20garnered%20increasing%20attention%20as%20it%20plays%0Aa%20significant%20role%20in%20human-computer%20interaction%20%28HCI%29%20in%20recent%20years.%20Since%0Adifferent%20discrete%20emotions%20may%20exist%20at%20the%20same%20time%2C%20compared%20with%0Asingle-class%20emotion%20recognition%2C%20emotion%20distribution%20learning%20%28EDL%29%20that%0Aidentifies%20a%20mixture%20of%20basic%20emotions%20has%20gradually%20emerged%20as%20a%20trend.%0AHowever%2C%20existing%20EDL%20methods%20face%20challenges%20in%20mining%20the%20heterogeneity%20among%0Amultiple%20modalities.%20Besides%2C%20rich%20semantic%20correlations%20across%20arbitrary%20basic%0Aemotions%20are%20not%20fully%20exploited.%20In%20this%20paper%2C%20we%20propose%20a%20multi-modal%0Aemotion%20distribution%20learning%20framework%2C%20named%20HeLo%2C%20aimed%20at%20fully%20exploring%0Athe%20heterogeneity%20and%20complementary%20information%20in%20multi-modal%20emotional%20data%0Aand%20label%20correlation%20within%20mixed%20basic%20emotions.%20Specifically%2C%20we%20first%20adopt%0Across-attention%20to%20effectively%20fuse%20the%20physiological%20data.%20Then%2C%20an%20optimal%0Atransport%20%28OT%29-based%20heterogeneity%20mining%20module%20is%20devised%20to%20mine%20the%0Ainteraction%20and%20heterogeneity%20between%20the%20physiological%20and%20behavioral%0Arepresentations.%20To%20facilitate%20label%20correlation%20learning%2C%20we%20introduce%20a%0Alearnable%20label%20embedding%20optimized%20by%20correlation%20matrix%20alignment.%20Finally%2C%0Athe%20learnable%20label%20embeddings%20and%20label%20correlation%20matrices%20are%20integrated%0Awith%20the%20multi-modal%20representations%20through%20a%20novel%20label%20correlation-driven%0Across-attention%20mechanism%20for%20accurate%20emotion%20distribution%20learning.%0AExperimental%20results%20on%20two%20publicly%20available%20datasets%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20method%20in%20emotion%20distribution%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06821v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


