<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241231.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Metric3Dv2: A Versatile Monocular Geometric Foundation Model for\n  Zero-shot Metric Depth and Surface Normal Estimation", "author": "Mu Hu and Wei Yin and Chi Zhang and Zhipeng Cai and Xiaoxiao Long and Kaixuan Wang and Hao Chen and Gang Yu and Chunhua Shen and Shaojie Shen", "abstract": "  We introduce Metric3D v2, a geometric foundation model for zero-shot metric\ndepth and surface normal estimation from a single image, which is crucial for\nmetric 3D recovery. While depth and normal are geometrically related and highly\ncomplimentary, they present distinct challenges. SoTA monocular depth methods\nachieve zero-shot generalization by learning affine-invariant depths, which\ncannot recover real-world metrics. Meanwhile, SoTA normal estimation methods\nhave limited zero-shot performance due to the lack of large-scale labeled data.\nTo tackle these issues, we propose solutions for both metric depth estimation\nand surface normal estimation. For metric depth estimation, we show that the\nkey to a zero-shot single-view model lies in resolving the metric ambiguity\nfrom various camera models and large-scale data training. We propose a\ncanonical camera space transformation module, which explicitly addresses the\nambiguity problem and can be effortlessly plugged into existing monocular\nmodels. For surface normal estimation, we propose a joint depth-normal\noptimization module to distill diverse data knowledge from metric depth,\nenabling normal estimators to learn beyond normal labels. Equipped with these\nmodules, our depth-normal models can be stably trained with over 16 million of\nimages from thousands of camera models with different-type annotations,\nresulting in zero-shot generalization to in-the-wild images with unseen camera\nsettings. Our method enables the accurate recovery of metric 3D structures on\nrandomly collected internet images, paving the way for plausible single-image\nmetrology. Our project page is at https://JUGGHM.github.io/Metric3Dv2.\n", "link": "http://arxiv.org/abs/2404.15506v4", "date": "2025-01-03", "relevancy": 2.9334, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5889}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5861}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metric3Dv2%3A%20A%20Versatile%20Monocular%20Geometric%20Foundation%20Model%20for%0A%20%20Zero-shot%20Metric%20Depth%20and%20Surface%20Normal%20Estimation&body=Title%3A%20Metric3Dv2%3A%20A%20Versatile%20Monocular%20Geometric%20Foundation%20Model%20for%0A%20%20Zero-shot%20Metric%20Depth%20and%20Surface%20Normal%20Estimation%0AAuthor%3A%20Mu%20Hu%20and%20Wei%20Yin%20and%20Chi%20Zhang%20and%20Zhipeng%20Cai%20and%20Xiaoxiao%20Long%20and%20Kaixuan%20Wang%20and%20Hao%20Chen%20and%20Gang%20Yu%20and%20Chunhua%20Shen%20and%20Shaojie%20Shen%0AAbstract%3A%20%20%20We%20introduce%20Metric3D%20v2%2C%20a%20geometric%20foundation%20model%20for%20zero-shot%20metric%0Adepth%20and%20surface%20normal%20estimation%20from%20a%20single%20image%2C%20which%20is%20crucial%20for%0Ametric%203D%20recovery.%20While%20depth%20and%20normal%20are%20geometrically%20related%20and%20highly%0Acomplimentary%2C%20they%20present%20distinct%20challenges.%20SoTA%20monocular%20depth%20methods%0Aachieve%20zero-shot%20generalization%20by%20learning%20affine-invariant%20depths%2C%20which%0Acannot%20recover%20real-world%20metrics.%20Meanwhile%2C%20SoTA%20normal%20estimation%20methods%0Ahave%20limited%20zero-shot%20performance%20due%20to%20the%20lack%20of%20large-scale%20labeled%20data.%0ATo%20tackle%20these%20issues%2C%20we%20propose%20solutions%20for%20both%20metric%20depth%20estimation%0Aand%20surface%20normal%20estimation.%20For%20metric%20depth%20estimation%2C%20we%20show%20that%20the%0Akey%20to%20a%20zero-shot%20single-view%20model%20lies%20in%20resolving%20the%20metric%20ambiguity%0Afrom%20various%20camera%20models%20and%20large-scale%20data%20training.%20We%20propose%20a%0Acanonical%20camera%20space%20transformation%20module%2C%20which%20explicitly%20addresses%20the%0Aambiguity%20problem%20and%20can%20be%20effortlessly%20plugged%20into%20existing%20monocular%0Amodels.%20For%20surface%20normal%20estimation%2C%20we%20propose%20a%20joint%20depth-normal%0Aoptimization%20module%20to%20distill%20diverse%20data%20knowledge%20from%20metric%20depth%2C%0Aenabling%20normal%20estimators%20to%20learn%20beyond%20normal%20labels.%20Equipped%20with%20these%0Amodules%2C%20our%20depth-normal%20models%20can%20be%20stably%20trained%20with%20over%2016%20million%20of%0Aimages%20from%20thousands%20of%20camera%20models%20with%20different-type%20annotations%2C%0Aresulting%20in%20zero-shot%20generalization%20to%20in-the-wild%20images%20with%20unseen%20camera%0Asettings.%20Our%20method%20enables%20the%20accurate%20recovery%20of%20metric%203D%20structures%20on%0Arandomly%20collected%20internet%20images%2C%20paving%20the%20way%20for%20plausible%20single-image%0Ametrology.%20Our%20project%20page%20is%20at%20https%3A//JUGGHM.github.io/Metric3Dv2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15506v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetric3Dv2%253A%2520A%2520Versatile%2520Monocular%2520Geometric%2520Foundation%2520Model%2520for%250A%2520%2520Zero-shot%2520Metric%2520Depth%2520and%2520Surface%2520Normal%2520Estimation%26entry.906535625%3DMu%2520Hu%2520and%2520Wei%2520Yin%2520and%2520Chi%2520Zhang%2520and%2520Zhipeng%2520Cai%2520and%2520Xiaoxiao%2520Long%2520and%2520Kaixuan%2520Wang%2520and%2520Hao%2520Chen%2520and%2520Gang%2520Yu%2520and%2520Chunhua%2520Shen%2520and%2520Shaojie%2520Shen%26entry.1292438233%3D%2520%2520We%2520introduce%2520Metric3D%2520v2%252C%2520a%2520geometric%2520foundation%2520model%2520for%2520zero-shot%2520metric%250Adepth%2520and%2520surface%2520normal%2520estimation%2520from%2520a%2520single%2520image%252C%2520which%2520is%2520crucial%2520for%250Ametric%25203D%2520recovery.%2520While%2520depth%2520and%2520normal%2520are%2520geometrically%2520related%2520and%2520highly%250Acomplimentary%252C%2520they%2520present%2520distinct%2520challenges.%2520SoTA%2520monocular%2520depth%2520methods%250Aachieve%2520zero-shot%2520generalization%2520by%2520learning%2520affine-invariant%2520depths%252C%2520which%250Acannot%2520recover%2520real-world%2520metrics.%2520Meanwhile%252C%2520SoTA%2520normal%2520estimation%2520methods%250Ahave%2520limited%2520zero-shot%2520performance%2520due%2520to%2520the%2520lack%2520of%2520large-scale%2520labeled%2520data.%250ATo%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520solutions%2520for%2520both%2520metric%2520depth%2520estimation%250Aand%2520surface%2520normal%2520estimation.%2520For%2520metric%2520depth%2520estimation%252C%2520we%2520show%2520that%2520the%250Akey%2520to%2520a%2520zero-shot%2520single-view%2520model%2520lies%2520in%2520resolving%2520the%2520metric%2520ambiguity%250Afrom%2520various%2520camera%2520models%2520and%2520large-scale%2520data%2520training.%2520We%2520propose%2520a%250Acanonical%2520camera%2520space%2520transformation%2520module%252C%2520which%2520explicitly%2520addresses%2520the%250Aambiguity%2520problem%2520and%2520can%2520be%2520effortlessly%2520plugged%2520into%2520existing%2520monocular%250Amodels.%2520For%2520surface%2520normal%2520estimation%252C%2520we%2520propose%2520a%2520joint%2520depth-normal%250Aoptimization%2520module%2520to%2520distill%2520diverse%2520data%2520knowledge%2520from%2520metric%2520depth%252C%250Aenabling%2520normal%2520estimators%2520to%2520learn%2520beyond%2520normal%2520labels.%2520Equipped%2520with%2520these%250Amodules%252C%2520our%2520depth-normal%2520models%2520can%2520be%2520stably%2520trained%2520with%2520over%252016%2520million%2520of%250Aimages%2520from%2520thousands%2520of%2520camera%2520models%2520with%2520different-type%2520annotations%252C%250Aresulting%2520in%2520zero-shot%2520generalization%2520to%2520in-the-wild%2520images%2520with%2520unseen%2520camera%250Asettings.%2520Our%2520method%2520enables%2520the%2520accurate%2520recovery%2520of%2520metric%25203D%2520structures%2520on%250Arandomly%2520collected%2520internet%2520images%252C%2520paving%2520the%2520way%2520for%2520plausible%2520single-image%250Ametrology.%2520Our%2520project%2520page%2520is%2520at%2520https%253A//JUGGHM.github.io/Metric3Dv2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15506v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metric3Dv2%3A%20A%20Versatile%20Monocular%20Geometric%20Foundation%20Model%20for%0A%20%20Zero-shot%20Metric%20Depth%20and%20Surface%20Normal%20Estimation&entry.906535625=Mu%20Hu%20and%20Wei%20Yin%20and%20Chi%20Zhang%20and%20Zhipeng%20Cai%20and%20Xiaoxiao%20Long%20and%20Kaixuan%20Wang%20and%20Hao%20Chen%20and%20Gang%20Yu%20and%20Chunhua%20Shen%20and%20Shaojie%20Shen&entry.1292438233=%20%20We%20introduce%20Metric3D%20v2%2C%20a%20geometric%20foundation%20model%20for%20zero-shot%20metric%0Adepth%20and%20surface%20normal%20estimation%20from%20a%20single%20image%2C%20which%20is%20crucial%20for%0Ametric%203D%20recovery.%20While%20depth%20and%20normal%20are%20geometrically%20related%20and%20highly%0Acomplimentary%2C%20they%20present%20distinct%20challenges.%20SoTA%20monocular%20depth%20methods%0Aachieve%20zero-shot%20generalization%20by%20learning%20affine-invariant%20depths%2C%20which%0Acannot%20recover%20real-world%20metrics.%20Meanwhile%2C%20SoTA%20normal%20estimation%20methods%0Ahave%20limited%20zero-shot%20performance%20due%20to%20the%20lack%20of%20large-scale%20labeled%20data.%0ATo%20tackle%20these%20issues%2C%20we%20propose%20solutions%20for%20both%20metric%20depth%20estimation%0Aand%20surface%20normal%20estimation.%20For%20metric%20depth%20estimation%2C%20we%20show%20that%20the%0Akey%20to%20a%20zero-shot%20single-view%20model%20lies%20in%20resolving%20the%20metric%20ambiguity%0Afrom%20various%20camera%20models%20and%20large-scale%20data%20training.%20We%20propose%20a%0Acanonical%20camera%20space%20transformation%20module%2C%20which%20explicitly%20addresses%20the%0Aambiguity%20problem%20and%20can%20be%20effortlessly%20plugged%20into%20existing%20monocular%0Amodels.%20For%20surface%20normal%20estimation%2C%20we%20propose%20a%20joint%20depth-normal%0Aoptimization%20module%20to%20distill%20diverse%20data%20knowledge%20from%20metric%20depth%2C%0Aenabling%20normal%20estimators%20to%20learn%20beyond%20normal%20labels.%20Equipped%20with%20these%0Amodules%2C%20our%20depth-normal%20models%20can%20be%20stably%20trained%20with%20over%2016%20million%20of%0Aimages%20from%20thousands%20of%20camera%20models%20with%20different-type%20annotations%2C%0Aresulting%20in%20zero-shot%20generalization%20to%20in-the-wild%20images%20with%20unseen%20camera%0Asettings.%20Our%20method%20enables%20the%20accurate%20recovery%20of%20metric%203D%20structures%20on%0Arandomly%20collected%20internet%20images%2C%20paving%20the%20way%20for%20plausible%20single-image%0Ametrology.%20Our%20project%20page%20is%20at%20https%3A//JUGGHM.github.io/Metric3Dv2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15506v4&entry.124074799=Read"},
{"title": "VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\n  Alignment", "author": "Wenyan Cong and Kevin Wang and Jiahui Lei and Colton Stearns and Yuanhao Cai and Dilin Wang and Rakesh Ranjan and Matt Feiszli and Leonidas Guibas and Zhangyang Wang and Weiyao Wang and Zhiwen Fan", "abstract": "  Efficiently reconstructing accurate 3D models from monocular video is a key\nchallenge in computer vision, critical for advancing applications in virtual\nreality, robotics, and scene understanding. Existing approaches typically\nrequire pre-computed camera parameters and frame-by-frame reconstruction\npipelines, which are prone to error accumulation and entail significant\ncomputational overhead. To address these limitations, we introduce VideoLifter,\na novel framework that leverages geometric priors from a learnable model to\nincrementally optimize a globally sparse to dense 3D representation directly\nfrom video sequences. VideoLifter segments the video sequence into local\nwindows, where it matches and registers frames, constructs consistent\nfragments, and aligns them hierarchically to produce a unified 3D model. By\ntracking and propagating sparse point correspondences across frames and\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\nminimizing reprojection error for improved accuracy and robustness. This\napproach significantly accelerates the reconstruction process, reducing\ntraining time by over 82% while surpassing current state-of-the-art methods in\nvisual fidelity and computational efficiency.\n", "link": "http://arxiv.org/abs/2501.01949v1", "date": "2025-01-03", "relevancy": 2.8553, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoLifter%3A%20Lifting%20Videos%20to%203D%20with%20Fast%20Hierarchical%20Stereo%0A%20%20Alignment&body=Title%3A%20VideoLifter%3A%20Lifting%20Videos%20to%203D%20with%20Fast%20Hierarchical%20Stereo%0A%20%20Alignment%0AAuthor%3A%20Wenyan%20Cong%20and%20Kevin%20Wang%20and%20Jiahui%20Lei%20and%20Colton%20Stearns%20and%20Yuanhao%20Cai%20and%20Dilin%20Wang%20and%20Rakesh%20Ranjan%20and%20Matt%20Feiszli%20and%20Leonidas%20Guibas%20and%20Zhangyang%20Wang%20and%20Weiyao%20Wang%20and%20Zhiwen%20Fan%0AAbstract%3A%20%20%20Efficiently%20reconstructing%20accurate%203D%20models%20from%20monocular%20video%20is%20a%20key%0Achallenge%20in%20computer%20vision%2C%20critical%20for%20advancing%20applications%20in%20virtual%0Areality%2C%20robotics%2C%20and%20scene%20understanding.%20Existing%20approaches%20typically%0Arequire%20pre-computed%20camera%20parameters%20and%20frame-by-frame%20reconstruction%0Apipelines%2C%20which%20are%20prone%20to%20error%20accumulation%20and%20entail%20significant%0Acomputational%20overhead.%20To%20address%20these%20limitations%2C%20we%20introduce%20VideoLifter%2C%0Aa%20novel%20framework%20that%20leverages%20geometric%20priors%20from%20a%20learnable%20model%20to%0Aincrementally%20optimize%20a%20globally%20sparse%20to%20dense%203D%20representation%20directly%0Afrom%20video%20sequences.%20VideoLifter%20segments%20the%20video%20sequence%20into%20local%0Awindows%2C%20where%20it%20matches%20and%20registers%20frames%2C%20constructs%20consistent%0Afragments%2C%20and%20aligns%20them%20hierarchically%20to%20produce%20a%20unified%203D%20model.%20By%0Atracking%20and%20propagating%20sparse%20point%20correspondences%20across%20frames%20and%0Afragments%2C%20VideoLifter%20incrementally%20refines%20camera%20poses%20and%203D%20structure%2C%0Aminimizing%20reprojection%20error%20for%20improved%20accuracy%20and%20robustness.%20This%0Aapproach%20significantly%20accelerates%20the%20reconstruction%20process%2C%20reducing%0Atraining%20time%20by%20over%2082%25%20while%20surpassing%20current%20state-of-the-art%20methods%20in%0Avisual%20fidelity%20and%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoLifter%253A%2520Lifting%2520Videos%2520to%25203D%2520with%2520Fast%2520Hierarchical%2520Stereo%250A%2520%2520Alignment%26entry.906535625%3DWenyan%2520Cong%2520and%2520Kevin%2520Wang%2520and%2520Jiahui%2520Lei%2520and%2520Colton%2520Stearns%2520and%2520Yuanhao%2520Cai%2520and%2520Dilin%2520Wang%2520and%2520Rakesh%2520Ranjan%2520and%2520Matt%2520Feiszli%2520and%2520Leonidas%2520Guibas%2520and%2520Zhangyang%2520Wang%2520and%2520Weiyao%2520Wang%2520and%2520Zhiwen%2520Fan%26entry.1292438233%3D%2520%2520Efficiently%2520reconstructing%2520accurate%25203D%2520models%2520from%2520monocular%2520video%2520is%2520a%2520key%250Achallenge%2520in%2520computer%2520vision%252C%2520critical%2520for%2520advancing%2520applications%2520in%2520virtual%250Areality%252C%2520robotics%252C%2520and%2520scene%2520understanding.%2520Existing%2520approaches%2520typically%250Arequire%2520pre-computed%2520camera%2520parameters%2520and%2520frame-by-frame%2520reconstruction%250Apipelines%252C%2520which%2520are%2520prone%2520to%2520error%2520accumulation%2520and%2520entail%2520significant%250Acomputational%2520overhead.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520VideoLifter%252C%250Aa%2520novel%2520framework%2520that%2520leverages%2520geometric%2520priors%2520from%2520a%2520learnable%2520model%2520to%250Aincrementally%2520optimize%2520a%2520globally%2520sparse%2520to%2520dense%25203D%2520representation%2520directly%250Afrom%2520video%2520sequences.%2520VideoLifter%2520segments%2520the%2520video%2520sequence%2520into%2520local%250Awindows%252C%2520where%2520it%2520matches%2520and%2520registers%2520frames%252C%2520constructs%2520consistent%250Afragments%252C%2520and%2520aligns%2520them%2520hierarchically%2520to%2520produce%2520a%2520unified%25203D%2520model.%2520By%250Atracking%2520and%2520propagating%2520sparse%2520point%2520correspondences%2520across%2520frames%2520and%250Afragments%252C%2520VideoLifter%2520incrementally%2520refines%2520camera%2520poses%2520and%25203D%2520structure%252C%250Aminimizing%2520reprojection%2520error%2520for%2520improved%2520accuracy%2520and%2520robustness.%2520This%250Aapproach%2520significantly%2520accelerates%2520the%2520reconstruction%2520process%252C%2520reducing%250Atraining%2520time%2520by%2520over%252082%2525%2520while%2520surpassing%2520current%2520state-of-the-art%2520methods%2520in%250Avisual%2520fidelity%2520and%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoLifter%3A%20Lifting%20Videos%20to%203D%20with%20Fast%20Hierarchical%20Stereo%0A%20%20Alignment&entry.906535625=Wenyan%20Cong%20and%20Kevin%20Wang%20and%20Jiahui%20Lei%20and%20Colton%20Stearns%20and%20Yuanhao%20Cai%20and%20Dilin%20Wang%20and%20Rakesh%20Ranjan%20and%20Matt%20Feiszli%20and%20Leonidas%20Guibas%20and%20Zhangyang%20Wang%20and%20Weiyao%20Wang%20and%20Zhiwen%20Fan&entry.1292438233=%20%20Efficiently%20reconstructing%20accurate%203D%20models%20from%20monocular%20video%20is%20a%20key%0Achallenge%20in%20computer%20vision%2C%20critical%20for%20advancing%20applications%20in%20virtual%0Areality%2C%20robotics%2C%20and%20scene%20understanding.%20Existing%20approaches%20typically%0Arequire%20pre-computed%20camera%20parameters%20and%20frame-by-frame%20reconstruction%0Apipelines%2C%20which%20are%20prone%20to%20error%20accumulation%20and%20entail%20significant%0Acomputational%20overhead.%20To%20address%20these%20limitations%2C%20we%20introduce%20VideoLifter%2C%0Aa%20novel%20framework%20that%20leverages%20geometric%20priors%20from%20a%20learnable%20model%20to%0Aincrementally%20optimize%20a%20globally%20sparse%20to%20dense%203D%20representation%20directly%0Afrom%20video%20sequences.%20VideoLifter%20segments%20the%20video%20sequence%20into%20local%0Awindows%2C%20where%20it%20matches%20and%20registers%20frames%2C%20constructs%20consistent%0Afragments%2C%20and%20aligns%20them%20hierarchically%20to%20produce%20a%20unified%203D%20model.%20By%0Atracking%20and%20propagating%20sparse%20point%20correspondences%20across%20frames%20and%0Afragments%2C%20VideoLifter%20incrementally%20refines%20camera%20poses%20and%203D%20structure%2C%0Aminimizing%20reprojection%20error%20for%20improved%20accuracy%20and%20robustness.%20This%0Aapproach%20significantly%20accelerates%20the%20reconstruction%20process%2C%20reducing%0Atraining%20time%20by%20over%2082%25%20while%20surpassing%20current%20state-of-the-art%20methods%20in%0Avisual%20fidelity%20and%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01949v1&entry.124074799=Read"},
{"title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction", "author": "Chaoyou Fu and Haojia Lin and Xiong Wang and Yi-Fan Zhang and Yunhang Shen and Xiaoyu Liu and Yangze Li and Zuwei Long and Heting Gao and Ke Li and Xiawu Zheng and Rongrong Ji and Xing Sun and Caifeng Shan and Ran He", "abstract": "  Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.\n", "link": "http://arxiv.org/abs/2501.01957v1", "date": "2025-01-03", "relevancy": 2.7807, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITA-1.5%3A%20Towards%20GPT-4o%20Level%20Real-Time%20Vision%20and%20Speech%20Interaction&body=Title%3A%20VITA-1.5%3A%20Towards%20GPT-4o%20Level%20Real-Time%20Vision%20and%20Speech%20Interaction%0AAuthor%3A%20Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Xiong%20Wang%20and%20Yi-Fan%20Zhang%20and%20Yunhang%20Shen%20and%20Xiaoyu%20Liu%20and%20Yangze%20Li%20and%20Zuwei%20Long%20and%20Heting%20Gao%20and%20Ke%20Li%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Xing%20Sun%20and%20Caifeng%20Shan%20and%20Ran%20He%0AAbstract%3A%20%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20typically%20focused%20on%0Aintegrating%20visual%20and%20textual%20modalities%2C%20with%20less%20emphasis%20placed%20on%20the%0Arole%20of%20speech%20in%20enhancing%20interaction.%20However%2C%20speech%20plays%20a%20crucial%20role%0Ain%20multimodal%20dialogue%20systems%2C%20and%20implementing%20high-performance%20in%20both%0Avision%20and%20speech%20tasks%20remains%20a%20significant%20challenge%20due%20to%20the%20fundamental%0Amodality%20differences.%20In%20this%20paper%2C%20we%20propose%20a%20carefully%20designed%0Amulti-stage%20training%20methodology%20that%20progressively%20trains%20LLM%20to%20understand%0Aboth%20visual%20and%20speech%20information%2C%20ultimately%20enabling%20fluent%20vision%20and%0Aspeech%20interaction.%20Our%20approach%20not%20only%20preserves%20strong%20vision-language%0Acapacity%2C%20but%20also%20enables%20efficient%20speech-to-speech%20dialogue%20capabilities%0Awithout%20separate%20ASR%20and%20TTS%20modules%2C%20significantly%20accelerating%20multimodal%0Aend-to-end%20response%20speed.%20By%20comparing%20our%20method%20against%20state-of-the-art%0Acounterparts%20across%20benchmarks%20for%20image%2C%20video%2C%20and%20speech%20tasks%2C%20we%0Ademonstrate%20that%20our%20model%20is%20equipped%20with%20both%20strong%20visual%20and%20speech%0Acapabilities%2C%20making%20near%20real-time%20vision%20and%20speech%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITA-1.5%253A%2520Towards%2520GPT-4o%2520Level%2520Real-Time%2520Vision%2520and%2520Speech%2520Interaction%26entry.906535625%3DChaoyou%2520Fu%2520and%2520Haojia%2520Lin%2520and%2520Xiong%2520Wang%2520and%2520Yi-Fan%2520Zhang%2520and%2520Yunhang%2520Shen%2520and%2520Xiaoyu%2520Liu%2520and%2520Yangze%2520Li%2520and%2520Zuwei%2520Long%2520and%2520Heting%2520Gao%2520and%2520Ke%2520Li%2520and%2520Xiawu%2520Zheng%2520and%2520Rongrong%2520Ji%2520and%2520Xing%2520Sun%2520and%2520Caifeng%2520Shan%2520and%2520Ran%2520He%26entry.1292438233%3D%2520%2520Recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520typically%2520focused%2520on%250Aintegrating%2520visual%2520and%2520textual%2520modalities%252C%2520with%2520less%2520emphasis%2520placed%2520on%2520the%250Arole%2520of%2520speech%2520in%2520enhancing%2520interaction.%2520However%252C%2520speech%2520plays%2520a%2520crucial%2520role%250Ain%2520multimodal%2520dialogue%2520systems%252C%2520and%2520implementing%2520high-performance%2520in%2520both%250Avision%2520and%2520speech%2520tasks%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520fundamental%250Amodality%2520differences.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520carefully%2520designed%250Amulti-stage%2520training%2520methodology%2520that%2520progressively%2520trains%2520LLM%2520to%2520understand%250Aboth%2520visual%2520and%2520speech%2520information%252C%2520ultimately%2520enabling%2520fluent%2520vision%2520and%250Aspeech%2520interaction.%2520Our%2520approach%2520not%2520only%2520preserves%2520strong%2520vision-language%250Acapacity%252C%2520but%2520also%2520enables%2520efficient%2520speech-to-speech%2520dialogue%2520capabilities%250Awithout%2520separate%2520ASR%2520and%2520TTS%2520modules%252C%2520significantly%2520accelerating%2520multimodal%250Aend-to-end%2520response%2520speed.%2520By%2520comparing%2520our%2520method%2520against%2520state-of-the-art%250Acounterparts%2520across%2520benchmarks%2520for%2520image%252C%2520video%252C%2520and%2520speech%2520tasks%252C%2520we%250Ademonstrate%2520that%2520our%2520model%2520is%2520equipped%2520with%2520both%2520strong%2520visual%2520and%2520speech%250Acapabilities%252C%2520making%2520near%2520real-time%2520vision%2520and%2520speech%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITA-1.5%3A%20Towards%20GPT-4o%20Level%20Real-Time%20Vision%20and%20Speech%20Interaction&entry.906535625=Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Xiong%20Wang%20and%20Yi-Fan%20Zhang%20and%20Yunhang%20Shen%20and%20Xiaoyu%20Liu%20and%20Yangze%20Li%20and%20Zuwei%20Long%20and%20Heting%20Gao%20and%20Ke%20Li%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Xing%20Sun%20and%20Caifeng%20Shan%20and%20Ran%20He&entry.1292438233=%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20typically%20focused%20on%0Aintegrating%20visual%20and%20textual%20modalities%2C%20with%20less%20emphasis%20placed%20on%20the%0Arole%20of%20speech%20in%20enhancing%20interaction.%20However%2C%20speech%20plays%20a%20crucial%20role%0Ain%20multimodal%20dialogue%20systems%2C%20and%20implementing%20high-performance%20in%20both%0Avision%20and%20speech%20tasks%20remains%20a%20significant%20challenge%20due%20to%20the%20fundamental%0Amodality%20differences.%20In%20this%20paper%2C%20we%20propose%20a%20carefully%20designed%0Amulti-stage%20training%20methodology%20that%20progressively%20trains%20LLM%20to%20understand%0Aboth%20visual%20and%20speech%20information%2C%20ultimately%20enabling%20fluent%20vision%20and%0Aspeech%20interaction.%20Our%20approach%20not%20only%20preserves%20strong%20vision-language%0Acapacity%2C%20but%20also%20enables%20efficient%20speech-to-speech%20dialogue%20capabilities%0Awithout%20separate%20ASR%20and%20TTS%20modules%2C%20significantly%20accelerating%20multimodal%0Aend-to-end%20response%20speed.%20By%20comparing%20our%20method%20against%20state-of-the-art%0Acounterparts%20across%20benchmarks%20for%20image%2C%20video%2C%20and%20speech%20tasks%2C%20we%0Ademonstrate%20that%20our%20model%20is%20equipped%20with%20both%20strong%20visual%20and%20speech%0Acapabilities%2C%20making%20near%20real-time%20vision%20and%20speech%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01957v1&entry.124074799=Read"},
{"title": "ViiNeuS: Volumetric Initialization for Implicit Neural Surface\n  reconstruction of urban scenes with limited image overlap", "author": "Hala Djeghim and Nathan Piasco and Moussab Bennehar and Luis Rold\u00e3o and Dzmitry Tsishkou and D\u00e9sir\u00e9 Sidib\u00e9", "abstract": "  Neural implicit surface representation methods have recently shown impressive\n3D reconstruction results. However, existing solutions struggle to reconstruct\ndriving scenes due to their large size, highly complex nature and their limited\nvisual observation overlap. Hence, to achieve accurate reconstructions,\nadditional supervision data such as LiDAR, strong geometric priors, and long\ntraining times are required. To tackle such limitations, we present ViiNeuS, a\nnew hybrid implicit surface learning method that efficiently initializes the\nsigned distance field to reconstruct large driving scenes from 2D street view\nimages. ViiNeuS's hybrid architecture models two separate implicit fields: one\nrepresenting the volumetric density of the scene, and another one representing\nthe signed distance to the surface. To accurately reconstruct urban outdoor\ndriving scenarios, we introduce a novel volume-rendering strategy that relies\non self-supervised probabilistic density estimation to sample points near the\nsurface and transition progressively from volumetric to surface representation.\nOur solution permits a proper and fast initialization of the signed distance\nfield without relying on any geometric prior on the scene, compared to\nconcurrent methods. By conducting extensive experiments on four outdoor driving\ndatasets, we show that ViiNeuS can learn an accurate and detailed 3D surface\nrepresentation of various urban scene while being two times faster to train\ncompared to previous state-of-the-art solutions.\n", "link": "http://arxiv.org/abs/2403.10344v4", "date": "2025-01-03", "relevancy": 2.7523, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.574}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5415}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViiNeuS%3A%20Volumetric%20Initialization%20for%20Implicit%20Neural%20Surface%0A%20%20reconstruction%20of%20urban%20scenes%20with%20limited%20image%20overlap&body=Title%3A%20ViiNeuS%3A%20Volumetric%20Initialization%20for%20Implicit%20Neural%20Surface%0A%20%20reconstruction%20of%20urban%20scenes%20with%20limited%20image%20overlap%0AAuthor%3A%20Hala%20Djeghim%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou%20and%20D%C3%A9sir%C3%A9%20Sidib%C3%A9%0AAbstract%3A%20%20%20Neural%20implicit%20surface%20representation%20methods%20have%20recently%20shown%20impressive%0A3D%20reconstruction%20results.%20However%2C%20existing%20solutions%20struggle%20to%20reconstruct%0Adriving%20scenes%20due%20to%20their%20large%20size%2C%20highly%20complex%20nature%20and%20their%20limited%0Avisual%20observation%20overlap.%20Hence%2C%20to%20achieve%20accurate%20reconstructions%2C%0Aadditional%20supervision%20data%20such%20as%20LiDAR%2C%20strong%20geometric%20priors%2C%20and%20long%0Atraining%20times%20are%20required.%20To%20tackle%20such%20limitations%2C%20we%20present%20ViiNeuS%2C%20a%0Anew%20hybrid%20implicit%20surface%20learning%20method%20that%20efficiently%20initializes%20the%0Asigned%20distance%20field%20to%20reconstruct%20large%20driving%20scenes%20from%202D%20street%20view%0Aimages.%20ViiNeuS%27s%20hybrid%20architecture%20models%20two%20separate%20implicit%20fields%3A%20one%0Arepresenting%20the%20volumetric%20density%20of%20the%20scene%2C%20and%20another%20one%20representing%0Athe%20signed%20distance%20to%20the%20surface.%20To%20accurately%20reconstruct%20urban%20outdoor%0Adriving%20scenarios%2C%20we%20introduce%20a%20novel%20volume-rendering%20strategy%20that%20relies%0Aon%20self-supervised%20probabilistic%20density%20estimation%20to%20sample%20points%20near%20the%0Asurface%20and%20transition%20progressively%20from%20volumetric%20to%20surface%20representation.%0AOur%20solution%20permits%20a%20proper%20and%20fast%20initialization%20of%20the%20signed%20distance%0Afield%20without%20relying%20on%20any%20geometric%20prior%20on%20the%20scene%2C%20compared%20to%0Aconcurrent%20methods.%20By%20conducting%20extensive%20experiments%20on%20four%20outdoor%20driving%0Adatasets%2C%20we%20show%20that%20ViiNeuS%20can%20learn%20an%20accurate%20and%20detailed%203D%20surface%0Arepresentation%20of%20various%20urban%20scene%20while%20being%20two%20times%20faster%20to%20train%0Acompared%20to%20previous%20state-of-the-art%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10344v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViiNeuS%253A%2520Volumetric%2520Initialization%2520for%2520Implicit%2520Neural%2520Surface%250A%2520%2520reconstruction%2520of%2520urban%2520scenes%2520with%2520limited%2520image%2520overlap%26entry.906535625%3DHala%2520Djeghim%2520and%2520Nathan%2520Piasco%2520and%2520Moussab%2520Bennehar%2520and%2520Luis%2520Rold%25C3%25A3o%2520and%2520Dzmitry%2520Tsishkou%2520and%2520D%25C3%25A9sir%25C3%25A9%2520Sidib%25C3%25A9%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520surface%2520representation%2520methods%2520have%2520recently%2520shown%2520impressive%250A3D%2520reconstruction%2520results.%2520However%252C%2520existing%2520solutions%2520struggle%2520to%2520reconstruct%250Adriving%2520scenes%2520due%2520to%2520their%2520large%2520size%252C%2520highly%2520complex%2520nature%2520and%2520their%2520limited%250Avisual%2520observation%2520overlap.%2520Hence%252C%2520to%2520achieve%2520accurate%2520reconstructions%252C%250Aadditional%2520supervision%2520data%2520such%2520as%2520LiDAR%252C%2520strong%2520geometric%2520priors%252C%2520and%2520long%250Atraining%2520times%2520are%2520required.%2520To%2520tackle%2520such%2520limitations%252C%2520we%2520present%2520ViiNeuS%252C%2520a%250Anew%2520hybrid%2520implicit%2520surface%2520learning%2520method%2520that%2520efficiently%2520initializes%2520the%250Asigned%2520distance%2520field%2520to%2520reconstruct%2520large%2520driving%2520scenes%2520from%25202D%2520street%2520view%250Aimages.%2520ViiNeuS%2527s%2520hybrid%2520architecture%2520models%2520two%2520separate%2520implicit%2520fields%253A%2520one%250Arepresenting%2520the%2520volumetric%2520density%2520of%2520the%2520scene%252C%2520and%2520another%2520one%2520representing%250Athe%2520signed%2520distance%2520to%2520the%2520surface.%2520To%2520accurately%2520reconstruct%2520urban%2520outdoor%250Adriving%2520scenarios%252C%2520we%2520introduce%2520a%2520novel%2520volume-rendering%2520strategy%2520that%2520relies%250Aon%2520self-supervised%2520probabilistic%2520density%2520estimation%2520to%2520sample%2520points%2520near%2520the%250Asurface%2520and%2520transition%2520progressively%2520from%2520volumetric%2520to%2520surface%2520representation.%250AOur%2520solution%2520permits%2520a%2520proper%2520and%2520fast%2520initialization%2520of%2520the%2520signed%2520distance%250Afield%2520without%2520relying%2520on%2520any%2520geometric%2520prior%2520on%2520the%2520scene%252C%2520compared%2520to%250Aconcurrent%2520methods.%2520By%2520conducting%2520extensive%2520experiments%2520on%2520four%2520outdoor%2520driving%250Adatasets%252C%2520we%2520show%2520that%2520ViiNeuS%2520can%2520learn%2520an%2520accurate%2520and%2520detailed%25203D%2520surface%250Arepresentation%2520of%2520various%2520urban%2520scene%2520while%2520being%2520two%2520times%2520faster%2520to%2520train%250Acompared%2520to%2520previous%2520state-of-the-art%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10344v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViiNeuS%3A%20Volumetric%20Initialization%20for%20Implicit%20Neural%20Surface%0A%20%20reconstruction%20of%20urban%20scenes%20with%20limited%20image%20overlap&entry.906535625=Hala%20Djeghim%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou%20and%20D%C3%A9sir%C3%A9%20Sidib%C3%A9&entry.1292438233=%20%20Neural%20implicit%20surface%20representation%20methods%20have%20recently%20shown%20impressive%0A3D%20reconstruction%20results.%20However%2C%20existing%20solutions%20struggle%20to%20reconstruct%0Adriving%20scenes%20due%20to%20their%20large%20size%2C%20highly%20complex%20nature%20and%20their%20limited%0Avisual%20observation%20overlap.%20Hence%2C%20to%20achieve%20accurate%20reconstructions%2C%0Aadditional%20supervision%20data%20such%20as%20LiDAR%2C%20strong%20geometric%20priors%2C%20and%20long%0Atraining%20times%20are%20required.%20To%20tackle%20such%20limitations%2C%20we%20present%20ViiNeuS%2C%20a%0Anew%20hybrid%20implicit%20surface%20learning%20method%20that%20efficiently%20initializes%20the%0Asigned%20distance%20field%20to%20reconstruct%20large%20driving%20scenes%20from%202D%20street%20view%0Aimages.%20ViiNeuS%27s%20hybrid%20architecture%20models%20two%20separate%20implicit%20fields%3A%20one%0Arepresenting%20the%20volumetric%20density%20of%20the%20scene%2C%20and%20another%20one%20representing%0Athe%20signed%20distance%20to%20the%20surface.%20To%20accurately%20reconstruct%20urban%20outdoor%0Adriving%20scenarios%2C%20we%20introduce%20a%20novel%20volume-rendering%20strategy%20that%20relies%0Aon%20self-supervised%20probabilistic%20density%20estimation%20to%20sample%20points%20near%20the%0Asurface%20and%20transition%20progressively%20from%20volumetric%20to%20surface%20representation.%0AOur%20solution%20permits%20a%20proper%20and%20fast%20initialization%20of%20the%20signed%20distance%0Afield%20without%20relying%20on%20any%20geometric%20prior%20on%20the%20scene%2C%20compared%20to%0Aconcurrent%20methods.%20By%20conducting%20extensive%20experiments%20on%20four%20outdoor%20driving%0Adatasets%2C%20we%20show%20that%20ViiNeuS%20can%20learn%20an%20accurate%20and%20detailed%203D%20surface%0Arepresentation%20of%20various%20urban%20scene%20while%20being%20two%20times%20faster%20to%20train%0Acompared%20to%20previous%20state-of-the-art%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10344v4&entry.124074799=Read"},
{"title": "ANTHROPOS-V: benchmarking the novel task of Crowd Volume Estimation", "author": "Luca Collorone and Stefano D'Arrigo and Massimiliano Pappa and Guido Maria D'Amely di Melendugno and Giovanni Ficarra and Fabio Galasso", "abstract": "  We introduce the novel task of Crowd Volume Estimation (CVE), defined as the\nprocess of estimating the collective body volume of crowds using only RGB\nimages. Besides event management and public safety, CVE can be instrumental in\napproximating body weight, unlocking weight sensitive applications such as\ninfrastructure stress assessment, and assuring even weight balance. We propose\nthe first benchmark for CVE, comprising ANTHROPOS-V, a synthetic photorealistic\nvideo dataset featuring crowds in diverse urban environments. Its annotations\ninclude each person's volume, SMPL shape parameters, and keypoints. Also, we\nexplore metrics pertinent to CVE, define baseline models adapted from Human\nMesh Recovery and Crowd Counting domains, and propose a CVE specific\nmethodology that surpasses baselines. Although synthetic, the weights and\nheights of individuals are aligned with the real-world population distribution\nacross genders, and they transfer to the downstream task of CVE from real\nimages. Benchmark and code are available at\ngithub.com/colloroneluca/Crowd-Volume-Estimation.\n", "link": "http://arxiv.org/abs/2501.01877v1", "date": "2025-01-03", "relevancy": 2.6723, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5378}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5378}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ANTHROPOS-V%3A%20benchmarking%20the%20novel%20task%20of%20Crowd%20Volume%20Estimation&body=Title%3A%20ANTHROPOS-V%3A%20benchmarking%20the%20novel%20task%20of%20Crowd%20Volume%20Estimation%0AAuthor%3A%20Luca%20Collorone%20and%20Stefano%20D%27Arrigo%20and%20Massimiliano%20Pappa%20and%20Guido%20Maria%20D%27Amely%20di%20Melendugno%20and%20Giovanni%20Ficarra%20and%20Fabio%20Galasso%0AAbstract%3A%20%20%20We%20introduce%20the%20novel%20task%20of%20Crowd%20Volume%20Estimation%20%28CVE%29%2C%20defined%20as%20the%0Aprocess%20of%20estimating%20the%20collective%20body%20volume%20of%20crowds%20using%20only%20RGB%0Aimages.%20Besides%20event%20management%20and%20public%20safety%2C%20CVE%20can%20be%20instrumental%20in%0Aapproximating%20body%20weight%2C%20unlocking%20weight%20sensitive%20applications%20such%20as%0Ainfrastructure%20stress%20assessment%2C%20and%20assuring%20even%20weight%20balance.%20We%20propose%0Athe%20first%20benchmark%20for%20CVE%2C%20comprising%20ANTHROPOS-V%2C%20a%20synthetic%20photorealistic%0Avideo%20dataset%20featuring%20crowds%20in%20diverse%20urban%20environments.%20Its%20annotations%0Ainclude%20each%20person%27s%20volume%2C%20SMPL%20shape%20parameters%2C%20and%20keypoints.%20Also%2C%20we%0Aexplore%20metrics%20pertinent%20to%20CVE%2C%20define%20baseline%20models%20adapted%20from%20Human%0AMesh%20Recovery%20and%20Crowd%20Counting%20domains%2C%20and%20propose%20a%20CVE%20specific%0Amethodology%20that%20surpasses%20baselines.%20Although%20synthetic%2C%20the%20weights%20and%0Aheights%20of%20individuals%20are%20aligned%20with%20the%20real-world%20population%20distribution%0Aacross%20genders%2C%20and%20they%20transfer%20to%20the%20downstream%20task%20of%20CVE%20from%20real%0Aimages.%20Benchmark%20and%20code%20are%20available%20at%0Agithub.com/colloroneluca/Crowd-Volume-Estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DANTHROPOS-V%253A%2520benchmarking%2520the%2520novel%2520task%2520of%2520Crowd%2520Volume%2520Estimation%26entry.906535625%3DLuca%2520Collorone%2520and%2520Stefano%2520D%2527Arrigo%2520and%2520Massimiliano%2520Pappa%2520and%2520Guido%2520Maria%2520D%2527Amely%2520di%2520Melendugno%2520and%2520Giovanni%2520Ficarra%2520and%2520Fabio%2520Galasso%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520novel%2520task%2520of%2520Crowd%2520Volume%2520Estimation%2520%2528CVE%2529%252C%2520defined%2520as%2520the%250Aprocess%2520of%2520estimating%2520the%2520collective%2520body%2520volume%2520of%2520crowds%2520using%2520only%2520RGB%250Aimages.%2520Besides%2520event%2520management%2520and%2520public%2520safety%252C%2520CVE%2520can%2520be%2520instrumental%2520in%250Aapproximating%2520body%2520weight%252C%2520unlocking%2520weight%2520sensitive%2520applications%2520such%2520as%250Ainfrastructure%2520stress%2520assessment%252C%2520and%2520assuring%2520even%2520weight%2520balance.%2520We%2520propose%250Athe%2520first%2520benchmark%2520for%2520CVE%252C%2520comprising%2520ANTHROPOS-V%252C%2520a%2520synthetic%2520photorealistic%250Avideo%2520dataset%2520featuring%2520crowds%2520in%2520diverse%2520urban%2520environments.%2520Its%2520annotations%250Ainclude%2520each%2520person%2527s%2520volume%252C%2520SMPL%2520shape%2520parameters%252C%2520and%2520keypoints.%2520Also%252C%2520we%250Aexplore%2520metrics%2520pertinent%2520to%2520CVE%252C%2520define%2520baseline%2520models%2520adapted%2520from%2520Human%250AMesh%2520Recovery%2520and%2520Crowd%2520Counting%2520domains%252C%2520and%2520propose%2520a%2520CVE%2520specific%250Amethodology%2520that%2520surpasses%2520baselines.%2520Although%2520synthetic%252C%2520the%2520weights%2520and%250Aheights%2520of%2520individuals%2520are%2520aligned%2520with%2520the%2520real-world%2520population%2520distribution%250Aacross%2520genders%252C%2520and%2520they%2520transfer%2520to%2520the%2520downstream%2520task%2520of%2520CVE%2520from%2520real%250Aimages.%2520Benchmark%2520and%2520code%2520are%2520available%2520at%250Agithub.com/colloroneluca/Crowd-Volume-Estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANTHROPOS-V%3A%20benchmarking%20the%20novel%20task%20of%20Crowd%20Volume%20Estimation&entry.906535625=Luca%20Collorone%20and%20Stefano%20D%27Arrigo%20and%20Massimiliano%20Pappa%20and%20Guido%20Maria%20D%27Amely%20di%20Melendugno%20and%20Giovanni%20Ficarra%20and%20Fabio%20Galasso&entry.1292438233=%20%20We%20introduce%20the%20novel%20task%20of%20Crowd%20Volume%20Estimation%20%28CVE%29%2C%20defined%20as%20the%0Aprocess%20of%20estimating%20the%20collective%20body%20volume%20of%20crowds%20using%20only%20RGB%0Aimages.%20Besides%20event%20management%20and%20public%20safety%2C%20CVE%20can%20be%20instrumental%20in%0Aapproximating%20body%20weight%2C%20unlocking%20weight%20sensitive%20applications%20such%20as%0Ainfrastructure%20stress%20assessment%2C%20and%20assuring%20even%20weight%20balance.%20We%20propose%0Athe%20first%20benchmark%20for%20CVE%2C%20comprising%20ANTHROPOS-V%2C%20a%20synthetic%20photorealistic%0Avideo%20dataset%20featuring%20crowds%20in%20diverse%20urban%20environments.%20Its%20annotations%0Ainclude%20each%20person%27s%20volume%2C%20SMPL%20shape%20parameters%2C%20and%20keypoints.%20Also%2C%20we%0Aexplore%20metrics%20pertinent%20to%20CVE%2C%20define%20baseline%20models%20adapted%20from%20Human%0AMesh%20Recovery%20and%20Crowd%20Counting%20domains%2C%20and%20propose%20a%20CVE%20specific%0Amethodology%20that%20surpasses%20baselines.%20Although%20synthetic%2C%20the%20weights%20and%0Aheights%20of%20individuals%20are%20aligned%20with%20the%20real-world%20population%20distribution%0Aacross%20genders%2C%20and%20they%20transfer%20to%20the%20downstream%20task%20of%20CVE%20from%20real%0Aimages.%20Benchmark%20and%20code%20are%20available%20at%0Agithub.com/colloroneluca/Crowd-Volume-Estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01877v1&entry.124074799=Read"},
{"title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM", "author": "Yifan Du and Zikang Liu and Yifan Li and Wayne Xin Zhao and Yuqi Huo and Bingning Wang and Weipeng Chen and Zheng Liu and Zhongyuan Wang and Ji-Rong Wen", "abstract": "  Recently, slow-thinking reasoning systems, built upon large language models\n(LLMs), have garnered widespread attention by scaling the thinking time during\ninference. There is also growing interest in adapting this capability to\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\ndata semantics across different modalities, it is intuitively more challenging\nto implement multimodal slow-thinking systems.\n  To address this issue, in this paper, we explore a straightforward approach\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\nwith long thought). We find that these long-form reasoning processes, expressed\nin natural language, can be effectively transferred to MLLMs. Moreover, it\nseems that such textual reasoning data can be even more effective than visual\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\nwork is preliminary, it demonstrates that slow-thinking capacities are\nfundamentally associated with the language model component, which can be\ntransferred across modalities or domains. This finding can be leveraged to\nguide the development of more powerful slow-thinking reasoning systems. We\nrelease our resources at https://github.com/RUCAIBox/Virgo.\n", "link": "http://arxiv.org/abs/2501.01904v1", "date": "2025-01-03", "relevancy": 2.5129, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virgo%3A%20A%20Preliminary%20Exploration%20on%20Reproducing%20o1-like%20MLLM&body=Title%3A%20Virgo%3A%20A%20Preliminary%20Exploration%20on%20Reproducing%20o1-like%20MLLM%0AAuthor%3A%20Yifan%20Du%20and%20Zikang%20Liu%20and%20Yifan%20Li%20and%20Wayne%20Xin%20Zhao%20and%20Yuqi%20Huo%20and%20Bingning%20Wang%20and%20Weipeng%20Chen%20and%20Zheng%20Liu%20and%20Zhongyuan%20Wang%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Recently%2C%20slow-thinking%20reasoning%20systems%2C%20built%20upon%20large%20language%20models%0A%28LLMs%29%2C%20have%20garnered%20widespread%20attention%20by%20scaling%20the%20thinking%20time%20during%0Ainference.%20There%20is%20also%20growing%20interest%20in%20adapting%20this%20capability%20to%0Amultimodal%20large%20language%20models%20%28MLLMs%29.%20Given%20that%20MLLMs%20handle%20more%20complex%0Adata%20semantics%20across%20different%20modalities%2C%20it%20is%20intuitively%20more%20challenging%0Ato%20implement%20multimodal%20slow-thinking%20systems.%0A%20%20To%20address%20this%20issue%2C%20in%20this%20paper%2C%20we%20explore%20a%20straightforward%20approach%0Aby%20fine-tuning%20a%20capable%20MLLM%20with%20a%20small%20amount%20of%20textual%20long-form%20thought%0Adata%2C%20resulting%20in%20a%20multimodal%20slow-thinking%20system%2C%20Virgo%20%28Visual%20reasoning%0Awith%20long%20thought%29.%20We%20find%20that%20these%20long-form%20reasoning%20processes%2C%20expressed%0Ain%20natural%20language%2C%20can%20be%20effectively%20transferred%20to%20MLLMs.%20Moreover%2C%20it%0Aseems%20that%20such%20textual%20reasoning%20data%20can%20be%20even%20more%20effective%20than%20visual%0Areasoning%20data%20in%20eliciting%20the%20slow-thinking%20capacities%20of%20MLLMs.%20While%20this%0Awork%20is%20preliminary%2C%20it%20demonstrates%20that%20slow-thinking%20capacities%20are%0Afundamentally%20associated%20with%20the%20language%20model%20component%2C%20which%20can%20be%0Atransferred%20across%20modalities%20or%20domains.%20This%20finding%20can%20be%20leveraged%20to%0Aguide%20the%20development%20of%20more%20powerful%20slow-thinking%20reasoning%20systems.%20We%0Arelease%20our%20resources%20at%20https%3A//github.com/RUCAIBox/Virgo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirgo%253A%2520A%2520Preliminary%2520Exploration%2520on%2520Reproducing%2520o1-like%2520MLLM%26entry.906535625%3DYifan%2520Du%2520and%2520Zikang%2520Liu%2520and%2520Yifan%2520Li%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Yuqi%2520Huo%2520and%2520Bingning%2520Wang%2520and%2520Weipeng%2520Chen%2520and%2520Zheng%2520Liu%2520and%2520Zhongyuan%2520Wang%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Recently%252C%2520slow-thinking%2520reasoning%2520systems%252C%2520built%2520upon%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520have%2520garnered%2520widespread%2520attention%2520by%2520scaling%2520the%2520thinking%2520time%2520during%250Ainference.%2520There%2520is%2520also%2520growing%2520interest%2520in%2520adapting%2520this%2520capability%2520to%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520Given%2520that%2520MLLMs%2520handle%2520more%2520complex%250Adata%2520semantics%2520across%2520different%2520modalities%252C%2520it%2520is%2520intuitively%2520more%2520challenging%250Ato%2520implement%2520multimodal%2520slow-thinking%2520systems.%250A%2520%2520To%2520address%2520this%2520issue%252C%2520in%2520this%2520paper%252C%2520we%2520explore%2520a%2520straightforward%2520approach%250Aby%2520fine-tuning%2520a%2520capable%2520MLLM%2520with%2520a%2520small%2520amount%2520of%2520textual%2520long-form%2520thought%250Adata%252C%2520resulting%2520in%2520a%2520multimodal%2520slow-thinking%2520system%252C%2520Virgo%2520%2528Visual%2520reasoning%250Awith%2520long%2520thought%2529.%2520We%2520find%2520that%2520these%2520long-form%2520reasoning%2520processes%252C%2520expressed%250Ain%2520natural%2520language%252C%2520can%2520be%2520effectively%2520transferred%2520to%2520MLLMs.%2520Moreover%252C%2520it%250Aseems%2520that%2520such%2520textual%2520reasoning%2520data%2520can%2520be%2520even%2520more%2520effective%2520than%2520visual%250Areasoning%2520data%2520in%2520eliciting%2520the%2520slow-thinking%2520capacities%2520of%2520MLLMs.%2520While%2520this%250Awork%2520is%2520preliminary%252C%2520it%2520demonstrates%2520that%2520slow-thinking%2520capacities%2520are%250Afundamentally%2520associated%2520with%2520the%2520language%2520model%2520component%252C%2520which%2520can%2520be%250Atransferred%2520across%2520modalities%2520or%2520domains.%2520This%2520finding%2520can%2520be%2520leveraged%2520to%250Aguide%2520the%2520development%2520of%2520more%2520powerful%2520slow-thinking%2520reasoning%2520systems.%2520We%250Arelease%2520our%2520resources%2520at%2520https%253A//github.com/RUCAIBox/Virgo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virgo%3A%20A%20Preliminary%20Exploration%20on%20Reproducing%20o1-like%20MLLM&entry.906535625=Yifan%20Du%20and%20Zikang%20Liu%20and%20Yifan%20Li%20and%20Wayne%20Xin%20Zhao%20and%20Yuqi%20Huo%20and%20Bingning%20Wang%20and%20Weipeng%20Chen%20and%20Zheng%20Liu%20and%20Zhongyuan%20Wang%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Recently%2C%20slow-thinking%20reasoning%20systems%2C%20built%20upon%20large%20language%20models%0A%28LLMs%29%2C%20have%20garnered%20widespread%20attention%20by%20scaling%20the%20thinking%20time%20during%0Ainference.%20There%20is%20also%20growing%20interest%20in%20adapting%20this%20capability%20to%0Amultimodal%20large%20language%20models%20%28MLLMs%29.%20Given%20that%20MLLMs%20handle%20more%20complex%0Adata%20semantics%20across%20different%20modalities%2C%20it%20is%20intuitively%20more%20challenging%0Ato%20implement%20multimodal%20slow-thinking%20systems.%0A%20%20To%20address%20this%20issue%2C%20in%20this%20paper%2C%20we%20explore%20a%20straightforward%20approach%0Aby%20fine-tuning%20a%20capable%20MLLM%20with%20a%20small%20amount%20of%20textual%20long-form%20thought%0Adata%2C%20resulting%20in%20a%20multimodal%20slow-thinking%20system%2C%20Virgo%20%28Visual%20reasoning%0Awith%20long%20thought%29.%20We%20find%20that%20these%20long-form%20reasoning%20processes%2C%20expressed%0Ain%20natural%20language%2C%20can%20be%20effectively%20transferred%20to%20MLLMs.%20Moreover%2C%20it%0Aseems%20that%20such%20textual%20reasoning%20data%20can%20be%20even%20more%20effective%20than%20visual%0Areasoning%20data%20in%20eliciting%20the%20slow-thinking%20capacities%20of%20MLLMs.%20While%20this%0Awork%20is%20preliminary%2C%20it%20demonstrates%20that%20slow-thinking%20capacities%20are%0Afundamentally%20associated%20with%20the%20language%20model%20component%2C%20which%20can%20be%0Atransferred%20across%20modalities%20or%20domains.%20This%20finding%20can%20be%20leveraged%20to%0Aguide%20the%20development%20of%20more%20powerful%20slow-thinking%20reasoning%20systems.%20We%0Arelease%20our%20resources%20at%20https%3A//github.com/RUCAIBox/Virgo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01904v1&entry.124074799=Read"},
{"title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation", "author": "Siyuan Huang and Liliang Chen and Pengfei Zhou and Shengcong Chen and Zhengkai Jiang and Yue Hu and Peng Gao and Hongsheng Li and Maoqing Yao and Guanghui Ren", "abstract": "  We introduce EnerVerse, a comprehensive framework for embodied future space\ngeneration specifically designed for robotic manipulation tasks. EnerVerse\nseamlessly integrates convolutional and bidirectional attention mechanisms for\ninner-chunk space modeling, ensuring low-level consistency and continuity.\nRecognizing the inherent redundancy in video data, we propose a sparse memory\ncontext combined with a chunkwise unidirectional generative paradigm to enable\nthe generation of infinitely long sequences. To further augment robotic\ncapabilities, we introduce the Free Anchor View (FAV) space, which provides\nflexible perspectives to enhance observation and analysis. The FAV space\nmitigates motion modeling ambiguity, removes physical constraints in confined\nenvironments, and significantly improves the robot's generalization and\nadaptability across various tasks and settings. To address the prohibitive\ncosts and labor intensity of acquiring multi-camera observations, we present a\ndata engine pipeline that integrates a generative model with 4D Gaussian\nSplatting (4DGS). This pipeline leverages the generative model's robust\ngeneralization capabilities and the spatial constraints provided by 4DGS,\nenabling an iterative enhancement of data quality and diversity, thus creating\na data flywheel effect that effectively narrows the sim-to-real gap. Finally,\nour experiments demonstrate that the embodied future space generation prior\nsubstantially enhances policy predictive capabilities, resulting in improved\noverall performance, particularly in long-range robotic manipulation tasks.\n", "link": "http://arxiv.org/abs/2501.01895v1", "date": "2025-01-03", "relevancy": 2.5061, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6378}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6356}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnerVerse%3A%20Envisioning%20Embodied%20Future%20Space%20for%20Robotics%20Manipulation&body=Title%3A%20EnerVerse%3A%20Envisioning%20Embodied%20Future%20Space%20for%20Robotics%20Manipulation%0AAuthor%3A%20Siyuan%20Huang%20and%20Liliang%20Chen%20and%20Pengfei%20Zhou%20and%20Shengcong%20Chen%20and%20Zhengkai%20Jiang%20and%20Yue%20Hu%20and%20Peng%20Gao%20and%20Hongsheng%20Li%20and%20Maoqing%20Yao%20and%20Guanghui%20Ren%0AAbstract%3A%20%20%20We%20introduce%20EnerVerse%2C%20a%20comprehensive%20framework%20for%20embodied%20future%20space%0Ageneration%20specifically%20designed%20for%20robotic%20manipulation%20tasks.%20EnerVerse%0Aseamlessly%20integrates%20convolutional%20and%20bidirectional%20attention%20mechanisms%20for%0Ainner-chunk%20space%20modeling%2C%20ensuring%20low-level%20consistency%20and%20continuity.%0ARecognizing%20the%20inherent%20redundancy%20in%20video%20data%2C%20we%20propose%20a%20sparse%20memory%0Acontext%20combined%20with%20a%20chunkwise%20unidirectional%20generative%20paradigm%20to%20enable%0Athe%20generation%20of%20infinitely%20long%20sequences.%20To%20further%20augment%20robotic%0Acapabilities%2C%20we%20introduce%20the%20Free%20Anchor%20View%20%28FAV%29%20space%2C%20which%20provides%0Aflexible%20perspectives%20to%20enhance%20observation%20and%20analysis.%20The%20FAV%20space%0Amitigates%20motion%20modeling%20ambiguity%2C%20removes%20physical%20constraints%20in%20confined%0Aenvironments%2C%20and%20significantly%20improves%20the%20robot%27s%20generalization%20and%0Aadaptability%20across%20various%20tasks%20and%20settings.%20To%20address%20the%20prohibitive%0Acosts%20and%20labor%20intensity%20of%20acquiring%20multi-camera%20observations%2C%20we%20present%20a%0Adata%20engine%20pipeline%20that%20integrates%20a%20generative%20model%20with%204D%20Gaussian%0ASplatting%20%284DGS%29.%20This%20pipeline%20leverages%20the%20generative%20model%27s%20robust%0Ageneralization%20capabilities%20and%20the%20spatial%20constraints%20provided%20by%204DGS%2C%0Aenabling%20an%20iterative%20enhancement%20of%20data%20quality%20and%20diversity%2C%20thus%20creating%0Aa%20data%20flywheel%20effect%20that%20effectively%20narrows%20the%20sim-to-real%20gap.%20Finally%2C%0Aour%20experiments%20demonstrate%20that%20the%20embodied%20future%20space%20generation%20prior%0Asubstantially%20enhances%20policy%20predictive%20capabilities%2C%20resulting%20in%20improved%0Aoverall%20performance%2C%20particularly%20in%20long-range%20robotic%20manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnerVerse%253A%2520Envisioning%2520Embodied%2520Future%2520Space%2520for%2520Robotics%2520Manipulation%26entry.906535625%3DSiyuan%2520Huang%2520and%2520Liliang%2520Chen%2520and%2520Pengfei%2520Zhou%2520and%2520Shengcong%2520Chen%2520and%2520Zhengkai%2520Jiang%2520and%2520Yue%2520Hu%2520and%2520Peng%2520Gao%2520and%2520Hongsheng%2520Li%2520and%2520Maoqing%2520Yao%2520and%2520Guanghui%2520Ren%26entry.1292438233%3D%2520%2520We%2520introduce%2520EnerVerse%252C%2520a%2520comprehensive%2520framework%2520for%2520embodied%2520future%2520space%250Ageneration%2520specifically%2520designed%2520for%2520robotic%2520manipulation%2520tasks.%2520EnerVerse%250Aseamlessly%2520integrates%2520convolutional%2520and%2520bidirectional%2520attention%2520mechanisms%2520for%250Ainner-chunk%2520space%2520modeling%252C%2520ensuring%2520low-level%2520consistency%2520and%2520continuity.%250ARecognizing%2520the%2520inherent%2520redundancy%2520in%2520video%2520data%252C%2520we%2520propose%2520a%2520sparse%2520memory%250Acontext%2520combined%2520with%2520a%2520chunkwise%2520unidirectional%2520generative%2520paradigm%2520to%2520enable%250Athe%2520generation%2520of%2520infinitely%2520long%2520sequences.%2520To%2520further%2520augment%2520robotic%250Acapabilities%252C%2520we%2520introduce%2520the%2520Free%2520Anchor%2520View%2520%2528FAV%2529%2520space%252C%2520which%2520provides%250Aflexible%2520perspectives%2520to%2520enhance%2520observation%2520and%2520analysis.%2520The%2520FAV%2520space%250Amitigates%2520motion%2520modeling%2520ambiguity%252C%2520removes%2520physical%2520constraints%2520in%2520confined%250Aenvironments%252C%2520and%2520significantly%2520improves%2520the%2520robot%2527s%2520generalization%2520and%250Aadaptability%2520across%2520various%2520tasks%2520and%2520settings.%2520To%2520address%2520the%2520prohibitive%250Acosts%2520and%2520labor%2520intensity%2520of%2520acquiring%2520multi-camera%2520observations%252C%2520we%2520present%2520a%250Adata%2520engine%2520pipeline%2520that%2520integrates%2520a%2520generative%2520model%2520with%25204D%2520Gaussian%250ASplatting%2520%25284DGS%2529.%2520This%2520pipeline%2520leverages%2520the%2520generative%2520model%2527s%2520robust%250Ageneralization%2520capabilities%2520and%2520the%2520spatial%2520constraints%2520provided%2520by%25204DGS%252C%250Aenabling%2520an%2520iterative%2520enhancement%2520of%2520data%2520quality%2520and%2520diversity%252C%2520thus%2520creating%250Aa%2520data%2520flywheel%2520effect%2520that%2520effectively%2520narrows%2520the%2520sim-to-real%2520gap.%2520Finally%252C%250Aour%2520experiments%2520demonstrate%2520that%2520the%2520embodied%2520future%2520space%2520generation%2520prior%250Asubstantially%2520enhances%2520policy%2520predictive%2520capabilities%252C%2520resulting%2520in%2520improved%250Aoverall%2520performance%252C%2520particularly%2520in%2520long-range%2520robotic%2520manipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnerVerse%3A%20Envisioning%20Embodied%20Future%20Space%20for%20Robotics%20Manipulation&entry.906535625=Siyuan%20Huang%20and%20Liliang%20Chen%20and%20Pengfei%20Zhou%20and%20Shengcong%20Chen%20and%20Zhengkai%20Jiang%20and%20Yue%20Hu%20and%20Peng%20Gao%20and%20Hongsheng%20Li%20and%20Maoqing%20Yao%20and%20Guanghui%20Ren&entry.1292438233=%20%20We%20introduce%20EnerVerse%2C%20a%20comprehensive%20framework%20for%20embodied%20future%20space%0Ageneration%20specifically%20designed%20for%20robotic%20manipulation%20tasks.%20EnerVerse%0Aseamlessly%20integrates%20convolutional%20and%20bidirectional%20attention%20mechanisms%20for%0Ainner-chunk%20space%20modeling%2C%20ensuring%20low-level%20consistency%20and%20continuity.%0ARecognizing%20the%20inherent%20redundancy%20in%20video%20data%2C%20we%20propose%20a%20sparse%20memory%0Acontext%20combined%20with%20a%20chunkwise%20unidirectional%20generative%20paradigm%20to%20enable%0Athe%20generation%20of%20infinitely%20long%20sequences.%20To%20further%20augment%20robotic%0Acapabilities%2C%20we%20introduce%20the%20Free%20Anchor%20View%20%28FAV%29%20space%2C%20which%20provides%0Aflexible%20perspectives%20to%20enhance%20observation%20and%20analysis.%20The%20FAV%20space%0Amitigates%20motion%20modeling%20ambiguity%2C%20removes%20physical%20constraints%20in%20confined%0Aenvironments%2C%20and%20significantly%20improves%20the%20robot%27s%20generalization%20and%0Aadaptability%20across%20various%20tasks%20and%20settings.%20To%20address%20the%20prohibitive%0Acosts%20and%20labor%20intensity%20of%20acquiring%20multi-camera%20observations%2C%20we%20present%20a%0Adata%20engine%20pipeline%20that%20integrates%20a%20generative%20model%20with%204D%20Gaussian%0ASplatting%20%284DGS%29.%20This%20pipeline%20leverages%20the%20generative%20model%27s%20robust%0Ageneralization%20capabilities%20and%20the%20spatial%20constraints%20provided%20by%204DGS%2C%0Aenabling%20an%20iterative%20enhancement%20of%20data%20quality%20and%20diversity%2C%20thus%20creating%0Aa%20data%20flywheel%20effect%20that%20effectively%20narrows%20the%20sim-to-real%20gap.%20Finally%2C%0Aour%20experiments%20demonstrate%20that%20the%20embodied%20future%20space%20generation%20prior%0Asubstantially%20enhances%20policy%20predictive%20capabilities%2C%20resulting%20in%20improved%0Aoverall%20performance%2C%20particularly%20in%20long-range%20robotic%20manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01895v1&entry.124074799=Read"},
{"title": "Coverage and Bias of Street View Imagery in Mapping the Urban\n  Environment", "author": "Zicheng Fan and Chen-Chieh Feng and Filip Biljecki", "abstract": "  Street View Imagery (SVI) has emerged as a valuable data form in urban\nstudies, enabling new ways to map and sense urban environments. However,\nfundamental concerns regarding the representativeness, quality, and reliability\nof SVI remain underexplored, e.g. to what extent can cities be captured by such\ndata and do data gaps result in bias. This research, positioned at the\nintersection of spatial data quality and urban analytics, addresses these\nconcerns by proposing a novel and effective method to estimate SVI's\nelement-level coverage in the urban environment. The method integrates the\npositional relationships between SVI and target elements, as well as the impact\nof physical obstructions. Expanding the domain of data quality to SVI, we\nintroduce an indicator system that evaluates the extent of coverage, focusing\non the completeness and frequency dimensions. Taking London as a case study,\nthree experiments are conducted to identify potential biases in SVI's ability\nto cover and represent urban environmental elements, using building facades as\nan example. It is found that despite their high availability along urban road\nnetworks, Google Street View covers only 62.4% of buildings in the case study\narea. The average facade coverage per building is 12.4%. SVI tends to\nover-represent non-residential buildings, thus possibly resulting in biased\nanalyses, and its coverage of environmental elements is position-dependent. The\nresearch also highlights the variability of SVI coverage under different data\nacquisition practices and proposes an optimal sampling interval range of 50-60\nm for SVI collection. The findings suggest that while SVI offers valuable\ninsights, it is no panacea - its application in urban research requires careful\nconsideration of data coverage and element-level representativeness to ensure\nreliable results.\n", "link": "http://arxiv.org/abs/2409.15386v2", "date": "2025-01-03", "relevancy": 2.4814, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coverage%20and%20Bias%20of%20Street%20View%20Imagery%20in%20Mapping%20the%20Urban%0A%20%20Environment&body=Title%3A%20Coverage%20and%20Bias%20of%20Street%20View%20Imagery%20in%20Mapping%20the%20Urban%0A%20%20Environment%0AAuthor%3A%20Zicheng%20Fan%20and%20Chen-Chieh%20Feng%20and%20Filip%20Biljecki%0AAbstract%3A%20%20%20Street%20View%20Imagery%20%28SVI%29%20has%20emerged%20as%20a%20valuable%20data%20form%20in%20urban%0Astudies%2C%20enabling%20new%20ways%20to%20map%20and%20sense%20urban%20environments.%20However%2C%0Afundamental%20concerns%20regarding%20the%20representativeness%2C%20quality%2C%20and%20reliability%0Aof%20SVI%20remain%20underexplored%2C%20e.g.%20to%20what%20extent%20can%20cities%20be%20captured%20by%20such%0Adata%20and%20do%20data%20gaps%20result%20in%20bias.%20This%20research%2C%20positioned%20at%20the%0Aintersection%20of%20spatial%20data%20quality%20and%20urban%20analytics%2C%20addresses%20these%0Aconcerns%20by%20proposing%20a%20novel%20and%20effective%20method%20to%20estimate%20SVI%27s%0Aelement-level%20coverage%20in%20the%20urban%20environment.%20The%20method%20integrates%20the%0Apositional%20relationships%20between%20SVI%20and%20target%20elements%2C%20as%20well%20as%20the%20impact%0Aof%20physical%20obstructions.%20Expanding%20the%20domain%20of%20data%20quality%20to%20SVI%2C%20we%0Aintroduce%20an%20indicator%20system%20that%20evaluates%20the%20extent%20of%20coverage%2C%20focusing%0Aon%20the%20completeness%20and%20frequency%20dimensions.%20Taking%20London%20as%20a%20case%20study%2C%0Athree%20experiments%20are%20conducted%20to%20identify%20potential%20biases%20in%20SVI%27s%20ability%0Ato%20cover%20and%20represent%20urban%20environmental%20elements%2C%20using%20building%20facades%20as%0Aan%20example.%20It%20is%20found%20that%20despite%20their%20high%20availability%20along%20urban%20road%0Anetworks%2C%20Google%20Street%20View%20covers%20only%2062.4%25%20of%20buildings%20in%20the%20case%20study%0Aarea.%20The%20average%20facade%20coverage%20per%20building%20is%2012.4%25.%20SVI%20tends%20to%0Aover-represent%20non-residential%20buildings%2C%20thus%20possibly%20resulting%20in%20biased%0Aanalyses%2C%20and%20its%20coverage%20of%20environmental%20elements%20is%20position-dependent.%20The%0Aresearch%20also%20highlights%20the%20variability%20of%20SVI%20coverage%20under%20different%20data%0Aacquisition%20practices%20and%20proposes%20an%20optimal%20sampling%20interval%20range%20of%2050-60%0Am%20for%20SVI%20collection.%20The%20findings%20suggest%20that%20while%20SVI%20offers%20valuable%0Ainsights%2C%20it%20is%20no%20panacea%20-%20its%20application%20in%20urban%20research%20requires%20careful%0Aconsideration%20of%20data%20coverage%20and%20element-level%20representativeness%20to%20ensure%0Areliable%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoverage%2520and%2520Bias%2520of%2520Street%2520View%2520Imagery%2520in%2520Mapping%2520the%2520Urban%250A%2520%2520Environment%26entry.906535625%3DZicheng%2520Fan%2520and%2520Chen-Chieh%2520Feng%2520and%2520Filip%2520Biljecki%26entry.1292438233%3D%2520%2520Street%2520View%2520Imagery%2520%2528SVI%2529%2520has%2520emerged%2520as%2520a%2520valuable%2520data%2520form%2520in%2520urban%250Astudies%252C%2520enabling%2520new%2520ways%2520to%2520map%2520and%2520sense%2520urban%2520environments.%2520However%252C%250Afundamental%2520concerns%2520regarding%2520the%2520representativeness%252C%2520quality%252C%2520and%2520reliability%250Aof%2520SVI%2520remain%2520underexplored%252C%2520e.g.%2520to%2520what%2520extent%2520can%2520cities%2520be%2520captured%2520by%2520such%250Adata%2520and%2520do%2520data%2520gaps%2520result%2520in%2520bias.%2520This%2520research%252C%2520positioned%2520at%2520the%250Aintersection%2520of%2520spatial%2520data%2520quality%2520and%2520urban%2520analytics%252C%2520addresses%2520these%250Aconcerns%2520by%2520proposing%2520a%2520novel%2520and%2520effective%2520method%2520to%2520estimate%2520SVI%2527s%250Aelement-level%2520coverage%2520in%2520the%2520urban%2520environment.%2520The%2520method%2520integrates%2520the%250Apositional%2520relationships%2520between%2520SVI%2520and%2520target%2520elements%252C%2520as%2520well%2520as%2520the%2520impact%250Aof%2520physical%2520obstructions.%2520Expanding%2520the%2520domain%2520of%2520data%2520quality%2520to%2520SVI%252C%2520we%250Aintroduce%2520an%2520indicator%2520system%2520that%2520evaluates%2520the%2520extent%2520of%2520coverage%252C%2520focusing%250Aon%2520the%2520completeness%2520and%2520frequency%2520dimensions.%2520Taking%2520London%2520as%2520a%2520case%2520study%252C%250Athree%2520experiments%2520are%2520conducted%2520to%2520identify%2520potential%2520biases%2520in%2520SVI%2527s%2520ability%250Ato%2520cover%2520and%2520represent%2520urban%2520environmental%2520elements%252C%2520using%2520building%2520facades%2520as%250Aan%2520example.%2520It%2520is%2520found%2520that%2520despite%2520their%2520high%2520availability%2520along%2520urban%2520road%250Anetworks%252C%2520Google%2520Street%2520View%2520covers%2520only%252062.4%2525%2520of%2520buildings%2520in%2520the%2520case%2520study%250Aarea.%2520The%2520average%2520facade%2520coverage%2520per%2520building%2520is%252012.4%2525.%2520SVI%2520tends%2520to%250Aover-represent%2520non-residential%2520buildings%252C%2520thus%2520possibly%2520resulting%2520in%2520biased%250Aanalyses%252C%2520and%2520its%2520coverage%2520of%2520environmental%2520elements%2520is%2520position-dependent.%2520The%250Aresearch%2520also%2520highlights%2520the%2520variability%2520of%2520SVI%2520coverage%2520under%2520different%2520data%250Aacquisition%2520practices%2520and%2520proposes%2520an%2520optimal%2520sampling%2520interval%2520range%2520of%252050-60%250Am%2520for%2520SVI%2520collection.%2520The%2520findings%2520suggest%2520that%2520while%2520SVI%2520offers%2520valuable%250Ainsights%252C%2520it%2520is%2520no%2520panacea%2520-%2520its%2520application%2520in%2520urban%2520research%2520requires%2520careful%250Aconsideration%2520of%2520data%2520coverage%2520and%2520element-level%2520representativeness%2520to%2520ensure%250Areliable%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coverage%20and%20Bias%20of%20Street%20View%20Imagery%20in%20Mapping%20the%20Urban%0A%20%20Environment&entry.906535625=Zicheng%20Fan%20and%20Chen-Chieh%20Feng%20and%20Filip%20Biljecki&entry.1292438233=%20%20Street%20View%20Imagery%20%28SVI%29%20has%20emerged%20as%20a%20valuable%20data%20form%20in%20urban%0Astudies%2C%20enabling%20new%20ways%20to%20map%20and%20sense%20urban%20environments.%20However%2C%0Afundamental%20concerns%20regarding%20the%20representativeness%2C%20quality%2C%20and%20reliability%0Aof%20SVI%20remain%20underexplored%2C%20e.g.%20to%20what%20extent%20can%20cities%20be%20captured%20by%20such%0Adata%20and%20do%20data%20gaps%20result%20in%20bias.%20This%20research%2C%20positioned%20at%20the%0Aintersection%20of%20spatial%20data%20quality%20and%20urban%20analytics%2C%20addresses%20these%0Aconcerns%20by%20proposing%20a%20novel%20and%20effective%20method%20to%20estimate%20SVI%27s%0Aelement-level%20coverage%20in%20the%20urban%20environment.%20The%20method%20integrates%20the%0Apositional%20relationships%20between%20SVI%20and%20target%20elements%2C%20as%20well%20as%20the%20impact%0Aof%20physical%20obstructions.%20Expanding%20the%20domain%20of%20data%20quality%20to%20SVI%2C%20we%0Aintroduce%20an%20indicator%20system%20that%20evaluates%20the%20extent%20of%20coverage%2C%20focusing%0Aon%20the%20completeness%20and%20frequency%20dimensions.%20Taking%20London%20as%20a%20case%20study%2C%0Athree%20experiments%20are%20conducted%20to%20identify%20potential%20biases%20in%20SVI%27s%20ability%0Ato%20cover%20and%20represent%20urban%20environmental%20elements%2C%20using%20building%20facades%20as%0Aan%20example.%20It%20is%20found%20that%20despite%20their%20high%20availability%20along%20urban%20road%0Anetworks%2C%20Google%20Street%20View%20covers%20only%2062.4%25%20of%20buildings%20in%20the%20case%20study%0Aarea.%20The%20average%20facade%20coverage%20per%20building%20is%2012.4%25.%20SVI%20tends%20to%0Aover-represent%20non-residential%20buildings%2C%20thus%20possibly%20resulting%20in%20biased%0Aanalyses%2C%20and%20its%20coverage%20of%20environmental%20elements%20is%20position-dependent.%20The%0Aresearch%20also%20highlights%20the%20variability%20of%20SVI%20coverage%20under%20different%20data%0Aacquisition%20practices%20and%20proposes%20an%20optimal%20sampling%20interval%20range%20of%2050-60%0Am%20for%20SVI%20collection.%20The%20findings%20suggest%20that%20while%20SVI%20offers%20valuable%0Ainsights%2C%20it%20is%20no%20panacea%20-%20its%20application%20in%20urban%20research%20requires%20careful%0Aconsideration%20of%20data%20coverage%20and%20element-level%20representativeness%20to%20ensure%0Areliable%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15386v2&entry.124074799=Read"},
{"title": "Generic Objects as Pose Probes for Few-shot View Synthesis", "author": "Zhirui Gao and Renjiao Yi and Chenyang Zhu and Ke Zhuang and Wei Chen and Kai Xu", "abstract": "  Radiance fields including NeRFs and 3D Gaussians demonstrate great potential\nin high-fidelity rendering and scene reconstruction, while they require a\nsubstantial number of posed images as inputs. COLMAP is frequently employed for\npreprocessing to estimate poses, while it necessitates a large number of\nfeature matches to operate effectively, and it struggles with scenes\ncharacterized by sparse features, large baselines between images, or a limited\nnumber of input images. We aim to tackle few-view NeRF reconstruction using\nonly 3 to 6 unposed scene images. Traditional methods often use calibration\nboards but they are not common in images. We propose a novel idea of utilizing\neveryday objects, commonly found in both images and real life, as \"pose\nprobes\". The probe object is automatically segmented by SAM, whose shape is\ninitialized from a cube. We apply a dual-branch volume rendering optimization\n(object NeRF and scene NeRF) to constrain the pose optimization and jointly\nrefine the geometry. Specifically, object poses of two views are first\nestimated by PnP matching in an SDF representation, which serves as initial\nposes. PnP matching, requiring only a few features, is suitable for\nfeature-sparse scenes. Additional views are incrementally incorporated to\nrefine poses from preceding views. In experiments, PoseProbe achieves\nstate-of-the-art performance in both pose estimation and novel view synthesis\nacross multiple datasets. We demonstrate its effectiveness, particularly in\nfew-view and large-baseline scenes where COLMAP struggles. In ablations, using\ndifferent objects in a scene yields comparable performance. Our project page is\navailable at: \\href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this\nhttps URL}\n", "link": "http://arxiv.org/abs/2408.16690v3", "date": "2025-01-03", "relevancy": 2.4469, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6125}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generic%20Objects%20as%20Pose%20Probes%20for%20Few-shot%20View%20Synthesis&body=Title%3A%20Generic%20Objects%20as%20Pose%20Probes%20for%20Few-shot%20View%20Synthesis%0AAuthor%3A%20Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Chenyang%20Zhu%20and%20Ke%20Zhuang%20and%20Wei%20Chen%20and%20Kai%20Xu%0AAbstract%3A%20%20%20Radiance%20fields%20including%20NeRFs%20and%203D%20Gaussians%20demonstrate%20great%20potential%0Ain%20high-fidelity%20rendering%20and%20scene%20reconstruction%2C%20while%20they%20require%20a%0Asubstantial%20number%20of%20posed%20images%20as%20inputs.%20COLMAP%20is%20frequently%20employed%20for%0Apreprocessing%20to%20estimate%20poses%2C%20while%20it%20necessitates%20a%20large%20number%20of%0Afeature%20matches%20to%20operate%20effectively%2C%20and%20it%20struggles%20with%20scenes%0Acharacterized%20by%20sparse%20features%2C%20large%20baselines%20between%20images%2C%20or%20a%20limited%0Anumber%20of%20input%20images.%20We%20aim%20to%20tackle%20few-view%20NeRF%20reconstruction%20using%0Aonly%203%20to%206%20unposed%20scene%20images.%20Traditional%20methods%20often%20use%20calibration%0Aboards%20but%20they%20are%20not%20common%20in%20images.%20We%20propose%20a%20novel%20idea%20of%20utilizing%0Aeveryday%20objects%2C%20commonly%20found%20in%20both%20images%20and%20real%20life%2C%20as%20%22pose%0Aprobes%22.%20The%20probe%20object%20is%20automatically%20segmented%20by%20SAM%2C%20whose%20shape%20is%0Ainitialized%20from%20a%20cube.%20We%20apply%20a%20dual-branch%20volume%20rendering%20optimization%0A%28object%20NeRF%20and%20scene%20NeRF%29%20to%20constrain%20the%20pose%20optimization%20and%20jointly%0Arefine%20the%20geometry.%20Specifically%2C%20object%20poses%20of%20two%20views%20are%20first%0Aestimated%20by%20PnP%20matching%20in%20an%20SDF%20representation%2C%20which%20serves%20as%20initial%0Aposes.%20PnP%20matching%2C%20requiring%20only%20a%20few%20features%2C%20is%20suitable%20for%0Afeature-sparse%20scenes.%20Additional%20views%20are%20incrementally%20incorporated%20to%0Arefine%20poses%20from%20preceding%20views.%20In%20experiments%2C%20PoseProbe%20achieves%0Astate-of-the-art%20performance%20in%20both%20pose%20estimation%20and%20novel%20view%20synthesis%0Aacross%20multiple%20datasets.%20We%20demonstrate%20its%20effectiveness%2C%20particularly%20in%0Afew-view%20and%20large-baseline%20scenes%20where%20COLMAP%20struggles.%20In%20ablations%2C%20using%0Adifferent%20objects%20in%20a%20scene%20yields%20comparable%20performance.%20Our%20project%20page%20is%0Aavailable%20at%3A%20%5Chref%7Bhttps%3A//zhirui-gao.github.io/PoseProbe.github.io/%7D%7Bthis%0Ahttps%20URL%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16690v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneric%2520Objects%2520as%2520Pose%2520Probes%2520for%2520Few-shot%2520View%2520Synthesis%26entry.906535625%3DZhirui%2520Gao%2520and%2520Renjiao%2520Yi%2520and%2520Chenyang%2520Zhu%2520and%2520Ke%2520Zhuang%2520and%2520Wei%2520Chen%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520Radiance%2520fields%2520including%2520NeRFs%2520and%25203D%2520Gaussians%2520demonstrate%2520great%2520potential%250Ain%2520high-fidelity%2520rendering%2520and%2520scene%2520reconstruction%252C%2520while%2520they%2520require%2520a%250Asubstantial%2520number%2520of%2520posed%2520images%2520as%2520inputs.%2520COLMAP%2520is%2520frequently%2520employed%2520for%250Apreprocessing%2520to%2520estimate%2520poses%252C%2520while%2520it%2520necessitates%2520a%2520large%2520number%2520of%250Afeature%2520matches%2520to%2520operate%2520effectively%252C%2520and%2520it%2520struggles%2520with%2520scenes%250Acharacterized%2520by%2520sparse%2520features%252C%2520large%2520baselines%2520between%2520images%252C%2520or%2520a%2520limited%250Anumber%2520of%2520input%2520images.%2520We%2520aim%2520to%2520tackle%2520few-view%2520NeRF%2520reconstruction%2520using%250Aonly%25203%2520to%25206%2520unposed%2520scene%2520images.%2520Traditional%2520methods%2520often%2520use%2520calibration%250Aboards%2520but%2520they%2520are%2520not%2520common%2520in%2520images.%2520We%2520propose%2520a%2520novel%2520idea%2520of%2520utilizing%250Aeveryday%2520objects%252C%2520commonly%2520found%2520in%2520both%2520images%2520and%2520real%2520life%252C%2520as%2520%2522pose%250Aprobes%2522.%2520The%2520probe%2520object%2520is%2520automatically%2520segmented%2520by%2520SAM%252C%2520whose%2520shape%2520is%250Ainitialized%2520from%2520a%2520cube.%2520We%2520apply%2520a%2520dual-branch%2520volume%2520rendering%2520optimization%250A%2528object%2520NeRF%2520and%2520scene%2520NeRF%2529%2520to%2520constrain%2520the%2520pose%2520optimization%2520and%2520jointly%250Arefine%2520the%2520geometry.%2520Specifically%252C%2520object%2520poses%2520of%2520two%2520views%2520are%2520first%250Aestimated%2520by%2520PnP%2520matching%2520in%2520an%2520SDF%2520representation%252C%2520which%2520serves%2520as%2520initial%250Aposes.%2520PnP%2520matching%252C%2520requiring%2520only%2520a%2520few%2520features%252C%2520is%2520suitable%2520for%250Afeature-sparse%2520scenes.%2520Additional%2520views%2520are%2520incrementally%2520incorporated%2520to%250Arefine%2520poses%2520from%2520preceding%2520views.%2520In%2520experiments%252C%2520PoseProbe%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520pose%2520estimation%2520and%2520novel%2520view%2520synthesis%250Aacross%2520multiple%2520datasets.%2520We%2520demonstrate%2520its%2520effectiveness%252C%2520particularly%2520in%250Afew-view%2520and%2520large-baseline%2520scenes%2520where%2520COLMAP%2520struggles.%2520In%2520ablations%252C%2520using%250Adifferent%2520objects%2520in%2520a%2520scene%2520yields%2520comparable%2520performance.%2520Our%2520project%2520page%2520is%250Aavailable%2520at%253A%2520%255Chref%257Bhttps%253A//zhirui-gao.github.io/PoseProbe.github.io/%257D%257Bthis%250Ahttps%2520URL%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16690v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generic%20Objects%20as%20Pose%20Probes%20for%20Few-shot%20View%20Synthesis&entry.906535625=Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Chenyang%20Zhu%20and%20Ke%20Zhuang%20and%20Wei%20Chen%20and%20Kai%20Xu&entry.1292438233=%20%20Radiance%20fields%20including%20NeRFs%20and%203D%20Gaussians%20demonstrate%20great%20potential%0Ain%20high-fidelity%20rendering%20and%20scene%20reconstruction%2C%20while%20they%20require%20a%0Asubstantial%20number%20of%20posed%20images%20as%20inputs.%20COLMAP%20is%20frequently%20employed%20for%0Apreprocessing%20to%20estimate%20poses%2C%20while%20it%20necessitates%20a%20large%20number%20of%0Afeature%20matches%20to%20operate%20effectively%2C%20and%20it%20struggles%20with%20scenes%0Acharacterized%20by%20sparse%20features%2C%20large%20baselines%20between%20images%2C%20or%20a%20limited%0Anumber%20of%20input%20images.%20We%20aim%20to%20tackle%20few-view%20NeRF%20reconstruction%20using%0Aonly%203%20to%206%20unposed%20scene%20images.%20Traditional%20methods%20often%20use%20calibration%0Aboards%20but%20they%20are%20not%20common%20in%20images.%20We%20propose%20a%20novel%20idea%20of%20utilizing%0Aeveryday%20objects%2C%20commonly%20found%20in%20both%20images%20and%20real%20life%2C%20as%20%22pose%0Aprobes%22.%20The%20probe%20object%20is%20automatically%20segmented%20by%20SAM%2C%20whose%20shape%20is%0Ainitialized%20from%20a%20cube.%20We%20apply%20a%20dual-branch%20volume%20rendering%20optimization%0A%28object%20NeRF%20and%20scene%20NeRF%29%20to%20constrain%20the%20pose%20optimization%20and%20jointly%0Arefine%20the%20geometry.%20Specifically%2C%20object%20poses%20of%20two%20views%20are%20first%0Aestimated%20by%20PnP%20matching%20in%20an%20SDF%20representation%2C%20which%20serves%20as%20initial%0Aposes.%20PnP%20matching%2C%20requiring%20only%20a%20few%20features%2C%20is%20suitable%20for%0Afeature-sparse%20scenes.%20Additional%20views%20are%20incrementally%20incorporated%20to%0Arefine%20poses%20from%20preceding%20views.%20In%20experiments%2C%20PoseProbe%20achieves%0Astate-of-the-art%20performance%20in%20both%20pose%20estimation%20and%20novel%20view%20synthesis%0Aacross%20multiple%20datasets.%20We%20demonstrate%20its%20effectiveness%2C%20particularly%20in%0Afew-view%20and%20large-baseline%20scenes%20where%20COLMAP%20struggles.%20In%20ablations%2C%20using%0Adifferent%20objects%20in%20a%20scene%20yields%20comparable%20performance.%20Our%20project%20page%20is%0Aavailable%20at%3A%20%5Chref%7Bhttps%3A//zhirui-gao.github.io/PoseProbe.github.io/%7D%7Bthis%0Ahttps%20URL%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16690v3&entry.124074799=Read"},
{"title": "A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree\n  Aggregation, Use BLTs", "author": "H. Brendan McMahan and Zheng Xu and Yanxiang Zhang", "abstract": "  The state-of-the-art for training on-device language models for mobile\nkeyboard applications combines federated learning (FL) with differential\nprivacy (DP) via the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm. Two\nvariants of DP-FTRL are used in practice, tree aggregation and matrix\nfactorization. However, tree aggregation suffers from significantly suboptimal\nprivacy/utility tradeoffs, while matrix mechanisms require expensive\noptimization parameterized by hard-to-estimate-in-advance constants, and high\nruntime memory costs.This paper extends the recently introduced Buffered Linear\nToeplitz (BLT) mechanism to multi-participation scenarios. Our BLT-DP-FTRL\nmaintains the ease-of-use advantages of tree aggregation, while essentially\nmatching matrix factorization in terms of utility and privacy. We evaluate\nBLT-DP-FTRL on the StackOverflow dataset, serving as a re-producible simulation\nbenchmark, and across four on-device language model tasks in a production FL\nsystem. Our empirical results highlight the advantages of the BLT mechanism and\nelevate the practicality and effectiveness of DP in real-world scenarios.\n", "link": "http://arxiv.org/abs/2408.08868v2", "date": "2025-01-03", "relevancy": 2.4081, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4903}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4847}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hassle-free%20Algorithm%20for%20Private%20Learning%20in%20Practice%3A%20Don%27t%20Use%20Tree%0A%20%20Aggregation%2C%20Use%20BLTs&body=Title%3A%20A%20Hassle-free%20Algorithm%20for%20Private%20Learning%20in%20Practice%3A%20Don%27t%20Use%20Tree%0A%20%20Aggregation%2C%20Use%20BLTs%0AAuthor%3A%20H.%20Brendan%20McMahan%20and%20Zheng%20Xu%20and%20Yanxiang%20Zhang%0AAbstract%3A%20%20%20The%20state-of-the-art%20for%20training%20on-device%20language%20models%20for%20mobile%0Akeyboard%20applications%20combines%20federated%20learning%20%28FL%29%20with%20differential%0Aprivacy%20%28DP%29%20via%20the%20DP-Follow-the-Regularized-Leader%20%28DP-FTRL%29%20algorithm.%20Two%0Avariants%20of%20DP-FTRL%20are%20used%20in%20practice%2C%20tree%20aggregation%20and%20matrix%0Afactorization.%20However%2C%20tree%20aggregation%20suffers%20from%20significantly%20suboptimal%0Aprivacy/utility%20tradeoffs%2C%20while%20matrix%20mechanisms%20require%20expensive%0Aoptimization%20parameterized%20by%20hard-to-estimate-in-advance%20constants%2C%20and%20high%0Aruntime%20memory%20costs.This%20paper%20extends%20the%20recently%20introduced%20Buffered%20Linear%0AToeplitz%20%28BLT%29%20mechanism%20to%20multi-participation%20scenarios.%20Our%20BLT-DP-FTRL%0Amaintains%20the%20ease-of-use%20advantages%20of%20tree%20aggregation%2C%20while%20essentially%0Amatching%20matrix%20factorization%20in%20terms%20of%20utility%20and%20privacy.%20We%20evaluate%0ABLT-DP-FTRL%20on%20the%20StackOverflow%20dataset%2C%20serving%20as%20a%20re-producible%20simulation%0Abenchmark%2C%20and%20across%20four%20on-device%20language%20model%20tasks%20in%20a%20production%20FL%0Asystem.%20Our%20empirical%20results%20highlight%20the%20advantages%20of%20the%20BLT%20mechanism%20and%0Aelevate%20the%20practicality%20and%20effectiveness%20of%20DP%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hassle-free%2520Algorithm%2520for%2520Private%2520Learning%2520in%2520Practice%253A%2520Don%2527t%2520Use%2520Tree%250A%2520%2520Aggregation%252C%2520Use%2520BLTs%26entry.906535625%3DH.%2520Brendan%2520McMahan%2520and%2520Zheng%2520Xu%2520and%2520Yanxiang%2520Zhang%26entry.1292438233%3D%2520%2520The%2520state-of-the-art%2520for%2520training%2520on-device%2520language%2520models%2520for%2520mobile%250Akeyboard%2520applications%2520combines%2520federated%2520learning%2520%2528FL%2529%2520with%2520differential%250Aprivacy%2520%2528DP%2529%2520via%2520the%2520DP-Follow-the-Regularized-Leader%2520%2528DP-FTRL%2529%2520algorithm.%2520Two%250Avariants%2520of%2520DP-FTRL%2520are%2520used%2520in%2520practice%252C%2520tree%2520aggregation%2520and%2520matrix%250Afactorization.%2520However%252C%2520tree%2520aggregation%2520suffers%2520from%2520significantly%2520suboptimal%250Aprivacy/utility%2520tradeoffs%252C%2520while%2520matrix%2520mechanisms%2520require%2520expensive%250Aoptimization%2520parameterized%2520by%2520hard-to-estimate-in-advance%2520constants%252C%2520and%2520high%250Aruntime%2520memory%2520costs.This%2520paper%2520extends%2520the%2520recently%2520introduced%2520Buffered%2520Linear%250AToeplitz%2520%2528BLT%2529%2520mechanism%2520to%2520multi-participation%2520scenarios.%2520Our%2520BLT-DP-FTRL%250Amaintains%2520the%2520ease-of-use%2520advantages%2520of%2520tree%2520aggregation%252C%2520while%2520essentially%250Amatching%2520matrix%2520factorization%2520in%2520terms%2520of%2520utility%2520and%2520privacy.%2520We%2520evaluate%250ABLT-DP-FTRL%2520on%2520the%2520StackOverflow%2520dataset%252C%2520serving%2520as%2520a%2520re-producible%2520simulation%250Abenchmark%252C%2520and%2520across%2520four%2520on-device%2520language%2520model%2520tasks%2520in%2520a%2520production%2520FL%250Asystem.%2520Our%2520empirical%2520results%2520highlight%2520the%2520advantages%2520of%2520the%2520BLT%2520mechanism%2520and%250Aelevate%2520the%2520practicality%2520and%2520effectiveness%2520of%2520DP%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hassle-free%20Algorithm%20for%20Private%20Learning%20in%20Practice%3A%20Don%27t%20Use%20Tree%0A%20%20Aggregation%2C%20Use%20BLTs&entry.906535625=H.%20Brendan%20McMahan%20and%20Zheng%20Xu%20and%20Yanxiang%20Zhang&entry.1292438233=%20%20The%20state-of-the-art%20for%20training%20on-device%20language%20models%20for%20mobile%0Akeyboard%20applications%20combines%20federated%20learning%20%28FL%29%20with%20differential%0Aprivacy%20%28DP%29%20via%20the%20DP-Follow-the-Regularized-Leader%20%28DP-FTRL%29%20algorithm.%20Two%0Avariants%20of%20DP-FTRL%20are%20used%20in%20practice%2C%20tree%20aggregation%20and%20matrix%0Afactorization.%20However%2C%20tree%20aggregation%20suffers%20from%20significantly%20suboptimal%0Aprivacy/utility%20tradeoffs%2C%20while%20matrix%20mechanisms%20require%20expensive%0Aoptimization%20parameterized%20by%20hard-to-estimate-in-advance%20constants%2C%20and%20high%0Aruntime%20memory%20costs.This%20paper%20extends%20the%20recently%20introduced%20Buffered%20Linear%0AToeplitz%20%28BLT%29%20mechanism%20to%20multi-participation%20scenarios.%20Our%20BLT-DP-FTRL%0Amaintains%20the%20ease-of-use%20advantages%20of%20tree%20aggregation%2C%20while%20essentially%0Amatching%20matrix%20factorization%20in%20terms%20of%20utility%20and%20privacy.%20We%20evaluate%0ABLT-DP-FTRL%20on%20the%20StackOverflow%20dataset%2C%20serving%20as%20a%20re-producible%20simulation%0Abenchmark%2C%20and%20across%20four%20on-device%20language%20model%20tasks%20in%20a%20production%20FL%0Asystem.%20Our%20empirical%20results%20highlight%20the%20advantages%20of%20the%20BLT%20mechanism%20and%0Aelevate%20the%20practicality%20and%20effectiveness%20of%20DP%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08868v2&entry.124074799=Read"},
{"title": "MADGEN -- Mass-Spec attends to De Novo Molecular generation", "author": "Yinkai Wang and Xiaohui Chen and Liping Liu and Soha Hassoun", "abstract": "  The annotation (assigning structural chemical identities) of MS/MS spectra\nremains a significant challenge due to the enormous molecular diversity in\nbiological samples and the limited scope of reference databases. Currently, the\nvast majority of spectral measurements remain in the \"dark chemical space\"\nwithout structural annotations. To improve annotation, we propose MADGEN\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\nfor de novo molecular structure generation guided by mass spectrometry data.\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\nmolecular generation starting with the scaffold. In the first stage, given an\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\nspectrum to guide an attention-based generative model to generate the final\nmolecule. Our approach constrains the molecular generation search space,\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\nperformance with a predictive scaffold retriever and with an oracle retriever.\nWe demonstrate the effectiveness of using attention to integrate spectral\ninformation throughout the generation process to achieve strong results with\nthe oracle retriever.\n", "link": "http://arxiv.org/abs/2501.01950v1", "date": "2025-01-03", "relevancy": 2.2587, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4693}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4484}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MADGEN%20--%20Mass-Spec%20attends%20to%20De%20Novo%20Molecular%20generation&body=Title%3A%20MADGEN%20--%20Mass-Spec%20attends%20to%20De%20Novo%20Molecular%20generation%0AAuthor%3A%20Yinkai%20Wang%20and%20Xiaohui%20Chen%20and%20Liping%20Liu%20and%20Soha%20Hassoun%0AAbstract%3A%20%20%20The%20annotation%20%28assigning%20structural%20chemical%20identities%29%20of%20MS/MS%20spectra%0Aremains%20a%20significant%20challenge%20due%20to%20the%20enormous%20molecular%20diversity%20in%0Abiological%20samples%20and%20the%20limited%20scope%20of%20reference%20databases.%20Currently%2C%20the%0Avast%20majority%20of%20spectral%20measurements%20remain%20in%20the%20%22dark%20chemical%20space%22%0Awithout%20structural%20annotations.%20To%20improve%20annotation%2C%20we%20propose%20MADGEN%0A%28Mass-spec%20Attends%20to%20De%20Novo%20Molecular%20GENeration%29%2C%20a%20scaffold-based%20method%0Afor%20de%20novo%20molecular%20structure%20generation%20guided%20by%20mass%20spectrometry%20data.%0AMADGEN%20operates%20in%20two%20stages%3A%20scaffold%20retrieval%20and%20spectra-conditioned%0Amolecular%20generation%20starting%20with%20the%20scaffold.%20In%20the%20first%20stage%2C%20given%20an%0AMS/MS%20spectrum%2C%20we%20formulate%20scaffold%20retrieval%20as%20a%20ranking%20problem%20and%20employ%0Acontrastive%20learning%20to%20align%20mass%20spectra%20with%20candidate%20molecular%20scaffolds.%0AIn%20the%20second%20stage%2C%20starting%20from%20the%20retrieved%20scaffold%2C%20we%20employ%20the%20MS/MS%0Aspectrum%20to%20guide%20an%20attention-based%20generative%20model%20to%20generate%20the%20final%0Amolecule.%20Our%20approach%20constrains%20the%20molecular%20generation%20search%20space%2C%0Areducing%20its%20complexity%20and%20improving%20generation%20accuracy.%20We%20evaluate%20MADGEN%0Aon%20three%20datasets%20%28NIST23%2C%20CANOPUS%2C%20and%20MassSpecGym%29%20and%20evaluate%20MADGEN%27s%0Aperformance%20with%20a%20predictive%20scaffold%20retriever%20and%20with%20an%20oracle%20retriever.%0AWe%20demonstrate%20the%20effectiveness%20of%20using%20attention%20to%20integrate%20spectral%0Ainformation%20throughout%20the%20generation%20process%20to%20achieve%20strong%20results%20with%0Athe%20oracle%20retriever.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMADGEN%2520--%2520Mass-Spec%2520attends%2520to%2520De%2520Novo%2520Molecular%2520generation%26entry.906535625%3DYinkai%2520Wang%2520and%2520Xiaohui%2520Chen%2520and%2520Liping%2520Liu%2520and%2520Soha%2520Hassoun%26entry.1292438233%3D%2520%2520The%2520annotation%2520%2528assigning%2520structural%2520chemical%2520identities%2529%2520of%2520MS/MS%2520spectra%250Aremains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520enormous%2520molecular%2520diversity%2520in%250Abiological%2520samples%2520and%2520the%2520limited%2520scope%2520of%2520reference%2520databases.%2520Currently%252C%2520the%250Avast%2520majority%2520of%2520spectral%2520measurements%2520remain%2520in%2520the%2520%2522dark%2520chemical%2520space%2522%250Awithout%2520structural%2520annotations.%2520To%2520improve%2520annotation%252C%2520we%2520propose%2520MADGEN%250A%2528Mass-spec%2520Attends%2520to%2520De%2520Novo%2520Molecular%2520GENeration%2529%252C%2520a%2520scaffold-based%2520method%250Afor%2520de%2520novo%2520molecular%2520structure%2520generation%2520guided%2520by%2520mass%2520spectrometry%2520data.%250AMADGEN%2520operates%2520in%2520two%2520stages%253A%2520scaffold%2520retrieval%2520and%2520spectra-conditioned%250Amolecular%2520generation%2520starting%2520with%2520the%2520scaffold.%2520In%2520the%2520first%2520stage%252C%2520given%2520an%250AMS/MS%2520spectrum%252C%2520we%2520formulate%2520scaffold%2520retrieval%2520as%2520a%2520ranking%2520problem%2520and%2520employ%250Acontrastive%2520learning%2520to%2520align%2520mass%2520spectra%2520with%2520candidate%2520molecular%2520scaffolds.%250AIn%2520the%2520second%2520stage%252C%2520starting%2520from%2520the%2520retrieved%2520scaffold%252C%2520we%2520employ%2520the%2520MS/MS%250Aspectrum%2520to%2520guide%2520an%2520attention-based%2520generative%2520model%2520to%2520generate%2520the%2520final%250Amolecule.%2520Our%2520approach%2520constrains%2520the%2520molecular%2520generation%2520search%2520space%252C%250Areducing%2520its%2520complexity%2520and%2520improving%2520generation%2520accuracy.%2520We%2520evaluate%2520MADGEN%250Aon%2520three%2520datasets%2520%2528NIST23%252C%2520CANOPUS%252C%2520and%2520MassSpecGym%2529%2520and%2520evaluate%2520MADGEN%2527s%250Aperformance%2520with%2520a%2520predictive%2520scaffold%2520retriever%2520and%2520with%2520an%2520oracle%2520retriever.%250AWe%2520demonstrate%2520the%2520effectiveness%2520of%2520using%2520attention%2520to%2520integrate%2520spectral%250Ainformation%2520throughout%2520the%2520generation%2520process%2520to%2520achieve%2520strong%2520results%2520with%250Athe%2520oracle%2520retriever.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MADGEN%20--%20Mass-Spec%20attends%20to%20De%20Novo%20Molecular%20generation&entry.906535625=Yinkai%20Wang%20and%20Xiaohui%20Chen%20and%20Liping%20Liu%20and%20Soha%20Hassoun&entry.1292438233=%20%20The%20annotation%20%28assigning%20structural%20chemical%20identities%29%20of%20MS/MS%20spectra%0Aremains%20a%20significant%20challenge%20due%20to%20the%20enormous%20molecular%20diversity%20in%0Abiological%20samples%20and%20the%20limited%20scope%20of%20reference%20databases.%20Currently%2C%20the%0Avast%20majority%20of%20spectral%20measurements%20remain%20in%20the%20%22dark%20chemical%20space%22%0Awithout%20structural%20annotations.%20To%20improve%20annotation%2C%20we%20propose%20MADGEN%0A%28Mass-spec%20Attends%20to%20De%20Novo%20Molecular%20GENeration%29%2C%20a%20scaffold-based%20method%0Afor%20de%20novo%20molecular%20structure%20generation%20guided%20by%20mass%20spectrometry%20data.%0AMADGEN%20operates%20in%20two%20stages%3A%20scaffold%20retrieval%20and%20spectra-conditioned%0Amolecular%20generation%20starting%20with%20the%20scaffold.%20In%20the%20first%20stage%2C%20given%20an%0AMS/MS%20spectrum%2C%20we%20formulate%20scaffold%20retrieval%20as%20a%20ranking%20problem%20and%20employ%0Acontrastive%20learning%20to%20align%20mass%20spectra%20with%20candidate%20molecular%20scaffolds.%0AIn%20the%20second%20stage%2C%20starting%20from%20the%20retrieved%20scaffold%2C%20we%20employ%20the%20MS/MS%0Aspectrum%20to%20guide%20an%20attention-based%20generative%20model%20to%20generate%20the%20final%0Amolecule.%20Our%20approach%20constrains%20the%20molecular%20generation%20search%20space%2C%0Areducing%20its%20complexity%20and%20improving%20generation%20accuracy.%20We%20evaluate%20MADGEN%0Aon%20three%20datasets%20%28NIST23%2C%20CANOPUS%2C%20and%20MassSpecGym%29%20and%20evaluate%20MADGEN%27s%0Aperformance%20with%20a%20predictive%20scaffold%20retriever%20and%20with%20an%20oracle%20retriever.%0AWe%20demonstrate%20the%20effectiveness%20of%20using%20attention%20to%20integrate%20spectral%0Ainformation%20throughout%20the%20generation%20process%20to%20achieve%20strong%20results%20with%0Athe%20oracle%20retriever.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01950v1&entry.124074799=Read"},
{"title": "LCFed: An Efficient Clustered Federated Learning Framework for\n  Heterogeneous Data", "author": "Yuxin Zhang and Haoyu Chen and Zheng Lin and Zhe Chen and Jin Zhao", "abstract": "  Clustered federated learning (CFL) addresses the performance challenges posed\nby data heterogeneity in federated learning (FL) by organizing edge devices\nwith similar data distributions into clusters, enabling collaborative model\ntraining tailored to each group. However, existing CFL approaches strictly\nlimit knowledge sharing to within clusters, lacking the integration of global\nknowledge with intra-cluster training, which leads to suboptimal performance.\nMoreover, traditional clustering methods incur significant computational\noverhead, especially as the number of edge devices increases. In this paper, we\npropose LCFed, an efficient CFL framework to combat these challenges. By\nleveraging model partitioning and adopting distinct aggregation strategies for\neach sub-model, LCFed effectively incorporates global knowledge into\nintra-cluster co-training, achieving optimal training performance.\nAdditionally, LCFed customizes a computationally efficient model similarity\nmeasurement method based on low-rank models, enabling real-time cluster updates\nwith minimal computational overhead. Extensive experiments show that LCFed\noutperforms state-of-the-art benchmarks in both test accuracy and clustering\ncomputational efficiency.\n", "link": "http://arxiv.org/abs/2501.01850v1", "date": "2025-01-03", "relevancy": 2.2336, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4523}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4475}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LCFed%3A%20An%20Efficient%20Clustered%20Federated%20Learning%20Framework%20for%0A%20%20Heterogeneous%20Data&body=Title%3A%20LCFed%3A%20An%20Efficient%20Clustered%20Federated%20Learning%20Framework%20for%0A%20%20Heterogeneous%20Data%0AAuthor%3A%20Yuxin%20Zhang%20and%20Haoyu%20Chen%20and%20Zheng%20Lin%20and%20Zhe%20Chen%20and%20Jin%20Zhao%0AAbstract%3A%20%20%20Clustered%20federated%20learning%20%28CFL%29%20addresses%20the%20performance%20challenges%20posed%0Aby%20data%20heterogeneity%20in%20federated%20learning%20%28FL%29%20by%20organizing%20edge%20devices%0Awith%20similar%20data%20distributions%20into%20clusters%2C%20enabling%20collaborative%20model%0Atraining%20tailored%20to%20each%20group.%20However%2C%20existing%20CFL%20approaches%20strictly%0Alimit%20knowledge%20sharing%20to%20within%20clusters%2C%20lacking%20the%20integration%20of%20global%0Aknowledge%20with%20intra-cluster%20training%2C%20which%20leads%20to%20suboptimal%20performance.%0AMoreover%2C%20traditional%20clustering%20methods%20incur%20significant%20computational%0Aoverhead%2C%20especially%20as%20the%20number%20of%20edge%20devices%20increases.%20In%20this%20paper%2C%20we%0Apropose%20LCFed%2C%20an%20efficient%20CFL%20framework%20to%20combat%20these%20challenges.%20By%0Aleveraging%20model%20partitioning%20and%20adopting%20distinct%20aggregation%20strategies%20for%0Aeach%20sub-model%2C%20LCFed%20effectively%20incorporates%20global%20knowledge%20into%0Aintra-cluster%20co-training%2C%20achieving%20optimal%20training%20performance.%0AAdditionally%2C%20LCFed%20customizes%20a%20computationally%20efficient%20model%20similarity%0Ameasurement%20method%20based%20on%20low-rank%20models%2C%20enabling%20real-time%20cluster%20updates%0Awith%20minimal%20computational%20overhead.%20Extensive%20experiments%20show%20that%20LCFed%0Aoutperforms%20state-of-the-art%20benchmarks%20in%20both%20test%20accuracy%20and%20clustering%0Acomputational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLCFed%253A%2520An%2520Efficient%2520Clustered%2520Federated%2520Learning%2520Framework%2520for%250A%2520%2520Heterogeneous%2520Data%26entry.906535625%3DYuxin%2520Zhang%2520and%2520Haoyu%2520Chen%2520and%2520Zheng%2520Lin%2520and%2520Zhe%2520Chen%2520and%2520Jin%2520Zhao%26entry.1292438233%3D%2520%2520Clustered%2520federated%2520learning%2520%2528CFL%2529%2520addresses%2520the%2520performance%2520challenges%2520posed%250Aby%2520data%2520heterogeneity%2520in%2520federated%2520learning%2520%2528FL%2529%2520by%2520organizing%2520edge%2520devices%250Awith%2520similar%2520data%2520distributions%2520into%2520clusters%252C%2520enabling%2520collaborative%2520model%250Atraining%2520tailored%2520to%2520each%2520group.%2520However%252C%2520existing%2520CFL%2520approaches%2520strictly%250Alimit%2520knowledge%2520sharing%2520to%2520within%2520clusters%252C%2520lacking%2520the%2520integration%2520of%2520global%250Aknowledge%2520with%2520intra-cluster%2520training%252C%2520which%2520leads%2520to%2520suboptimal%2520performance.%250AMoreover%252C%2520traditional%2520clustering%2520methods%2520incur%2520significant%2520computational%250Aoverhead%252C%2520especially%2520as%2520the%2520number%2520of%2520edge%2520devices%2520increases.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520LCFed%252C%2520an%2520efficient%2520CFL%2520framework%2520to%2520combat%2520these%2520challenges.%2520By%250Aleveraging%2520model%2520partitioning%2520and%2520adopting%2520distinct%2520aggregation%2520strategies%2520for%250Aeach%2520sub-model%252C%2520LCFed%2520effectively%2520incorporates%2520global%2520knowledge%2520into%250Aintra-cluster%2520co-training%252C%2520achieving%2520optimal%2520training%2520performance.%250AAdditionally%252C%2520LCFed%2520customizes%2520a%2520computationally%2520efficient%2520model%2520similarity%250Ameasurement%2520method%2520based%2520on%2520low-rank%2520models%252C%2520enabling%2520real-time%2520cluster%2520updates%250Awith%2520minimal%2520computational%2520overhead.%2520Extensive%2520experiments%2520show%2520that%2520LCFed%250Aoutperforms%2520state-of-the-art%2520benchmarks%2520in%2520both%2520test%2520accuracy%2520and%2520clustering%250Acomputational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCFed%3A%20An%20Efficient%20Clustered%20Federated%20Learning%20Framework%20for%0A%20%20Heterogeneous%20Data&entry.906535625=Yuxin%20Zhang%20and%20Haoyu%20Chen%20and%20Zheng%20Lin%20and%20Zhe%20Chen%20and%20Jin%20Zhao&entry.1292438233=%20%20Clustered%20federated%20learning%20%28CFL%29%20addresses%20the%20performance%20challenges%20posed%0Aby%20data%20heterogeneity%20in%20federated%20learning%20%28FL%29%20by%20organizing%20edge%20devices%0Awith%20similar%20data%20distributions%20into%20clusters%2C%20enabling%20collaborative%20model%0Atraining%20tailored%20to%20each%20group.%20However%2C%20existing%20CFL%20approaches%20strictly%0Alimit%20knowledge%20sharing%20to%20within%20clusters%2C%20lacking%20the%20integration%20of%20global%0Aknowledge%20with%20intra-cluster%20training%2C%20which%20leads%20to%20suboptimal%20performance.%0AMoreover%2C%20traditional%20clustering%20methods%20incur%20significant%20computational%0Aoverhead%2C%20especially%20as%20the%20number%20of%20edge%20devices%20increases.%20In%20this%20paper%2C%20we%0Apropose%20LCFed%2C%20an%20efficient%20CFL%20framework%20to%20combat%20these%20challenges.%20By%0Aleveraging%20model%20partitioning%20and%20adopting%20distinct%20aggregation%20strategies%20for%0Aeach%20sub-model%2C%20LCFed%20effectively%20incorporates%20global%20knowledge%20into%0Aintra-cluster%20co-training%2C%20achieving%20optimal%20training%20performance.%0AAdditionally%2C%20LCFed%20customizes%20a%20computationally%20efficient%20model%20similarity%0Ameasurement%20method%20based%20on%20low-rank%20models%2C%20enabling%20real-time%20cluster%20updates%0Awith%20minimal%20computational%20overhead.%20Extensive%20experiments%20show%20that%20LCFed%0Aoutperforms%20state-of-the-art%20benchmarks%20in%20both%20test%20accuracy%20and%20clustering%0Acomputational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01850v1&entry.124074799=Read"},
{"title": "Mitigating Hallucination for Large Vision Language Model by\n  Inter-Modality Correlation Calibration Decoding", "author": "Jiaming Li and Jiacheng Zhang and Zequn Jie and Lin Ma and Guanbin Li", "abstract": "  Large vision-language models (LVLMs) have shown remarkable capabilities in\nvisual-language understanding for downstream multi-modal tasks. Despite their\nsuccess, LVLMs still suffer from generating hallucinations in complex\ngeneration tasks, leading to inconsistencies between visual inputs and\ngenerated content. To address this issue, some approaches have introduced\ninference-time interventions, such as contrastive decoding and attention\nrectification, to reduce overreliance on language priors. However, these\napproaches overlook hallucinations stemming from spurious inter-modality\ncorrelations. In this paper, we propose an Inter-Modality Correlation\nCalibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a\ntraining-free manner. In this method, we design a Cross-Modal Value-Enhanced\nDecoding(CMVED) module to alleviate hallucination by a novel contrastive\ndecoding mechanism. During the estimation of distorted distribution, CMVED\nmasks the value vectors associated with significant cross-modal attention\nweights, which address both uni-modality overreliance and misleading\ninter-modality correlations. Additionally, a Content-Driven Attention\nRefinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to\nfocus on important visual content. Experimental results on diverse\nhallucination benchmarks validate the superiority of our method over existing\nstate-of-the-art techniques in reducing hallucinations in LVLM text generation.\nOur code will be available at https://github.com/lijm48/IMCCD.\n", "link": "http://arxiv.org/abs/2501.01926v1", "date": "2025-01-03", "relevancy": 2.1798, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucination%20for%20Large%20Vision%20Language%20Model%20by%0A%20%20Inter-Modality%20Correlation%20Calibration%20Decoding&body=Title%3A%20Mitigating%20Hallucination%20for%20Large%20Vision%20Language%20Model%20by%0A%20%20Inter-Modality%20Correlation%20Calibration%20Decoding%0AAuthor%3A%20Jiaming%20Li%20and%20Jiacheng%20Zhang%20and%20Zequn%20Jie%20and%20Lin%20Ma%20and%20Guanbin%20Li%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20shown%20remarkable%20capabilities%20in%0Avisual-language%20understanding%20for%20downstream%20multi-modal%20tasks.%20Despite%20their%0Asuccess%2C%20LVLMs%20still%20suffer%20from%20generating%20hallucinations%20in%20complex%0Ageneration%20tasks%2C%20leading%20to%20inconsistencies%20between%20visual%20inputs%20and%0Agenerated%20content.%20To%20address%20this%20issue%2C%20some%20approaches%20have%20introduced%0Ainference-time%20interventions%2C%20such%20as%20contrastive%20decoding%20and%20attention%0Arectification%2C%20to%20reduce%20overreliance%20on%20language%20priors.%20However%2C%20these%0Aapproaches%20overlook%20hallucinations%20stemming%20from%20spurious%20inter-modality%0Acorrelations.%20In%20this%20paper%2C%20we%20propose%20an%20Inter-Modality%20Correlation%0ACalibration%20Decoding%20%28IMCCD%29%20method%20to%20mitigate%20hallucinations%20in%20LVLMs%20in%20a%0Atraining-free%20manner.%20In%20this%20method%2C%20we%20design%20a%20Cross-Modal%20Value-Enhanced%0ADecoding%28CMVED%29%20module%20to%20alleviate%20hallucination%20by%20a%20novel%20contrastive%0Adecoding%20mechanism.%20During%20the%20estimation%20of%20distorted%20distribution%2C%20CMVED%0Amasks%20the%20value%20vectors%20associated%20with%20significant%20cross-modal%20attention%0Aweights%2C%20which%20address%20both%20uni-modality%20overreliance%20and%20misleading%0Ainter-modality%20correlations.%20Additionally%2C%20a%20Content-Driven%20Attention%0ARefinement%28CDAR%29%20module%20refines%20cross-modal%20attention%20weights%2C%20guiding%20LVLMs%20to%0Afocus%20on%20important%20visual%20content.%20Experimental%20results%20on%20diverse%0Ahallucination%20benchmarks%20validate%20the%20superiority%20of%20our%20method%20over%20existing%0Astate-of-the-art%20techniques%20in%20reducing%20hallucinations%20in%20LVLM%20text%20generation.%0AOur%20code%20will%20be%20available%20at%20https%3A//github.com/lijm48/IMCCD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucination%2520for%2520Large%2520Vision%2520Language%2520Model%2520by%250A%2520%2520Inter-Modality%2520Correlation%2520Calibration%2520Decoding%26entry.906535625%3DJiaming%2520Li%2520and%2520Jiacheng%2520Zhang%2520and%2520Zequn%2520Jie%2520and%2520Lin%2520Ma%2520and%2520Guanbin%2520Li%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%250Avisual-language%2520understanding%2520for%2520downstream%2520multi-modal%2520tasks.%2520Despite%2520their%250Asuccess%252C%2520LVLMs%2520still%2520suffer%2520from%2520generating%2520hallucinations%2520in%2520complex%250Ageneration%2520tasks%252C%2520leading%2520to%2520inconsistencies%2520between%2520visual%2520inputs%2520and%250Agenerated%2520content.%2520To%2520address%2520this%2520issue%252C%2520some%2520approaches%2520have%2520introduced%250Ainference-time%2520interventions%252C%2520such%2520as%2520contrastive%2520decoding%2520and%2520attention%250Arectification%252C%2520to%2520reduce%2520overreliance%2520on%2520language%2520priors.%2520However%252C%2520these%250Aapproaches%2520overlook%2520hallucinations%2520stemming%2520from%2520spurious%2520inter-modality%250Acorrelations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520Inter-Modality%2520Correlation%250ACalibration%2520Decoding%2520%2528IMCCD%2529%2520method%2520to%2520mitigate%2520hallucinations%2520in%2520LVLMs%2520in%2520a%250Atraining-free%2520manner.%2520In%2520this%2520method%252C%2520we%2520design%2520a%2520Cross-Modal%2520Value-Enhanced%250ADecoding%2528CMVED%2529%2520module%2520to%2520alleviate%2520hallucination%2520by%2520a%2520novel%2520contrastive%250Adecoding%2520mechanism.%2520During%2520the%2520estimation%2520of%2520distorted%2520distribution%252C%2520CMVED%250Amasks%2520the%2520value%2520vectors%2520associated%2520with%2520significant%2520cross-modal%2520attention%250Aweights%252C%2520which%2520address%2520both%2520uni-modality%2520overreliance%2520and%2520misleading%250Ainter-modality%2520correlations.%2520Additionally%252C%2520a%2520Content-Driven%2520Attention%250ARefinement%2528CDAR%2529%2520module%2520refines%2520cross-modal%2520attention%2520weights%252C%2520guiding%2520LVLMs%2520to%250Afocus%2520on%2520important%2520visual%2520content.%2520Experimental%2520results%2520on%2520diverse%250Ahallucination%2520benchmarks%2520validate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520existing%250Astate-of-the-art%2520techniques%2520in%2520reducing%2520hallucinations%2520in%2520LVLM%2520text%2520generation.%250AOur%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/lijm48/IMCCD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucination%20for%20Large%20Vision%20Language%20Model%20by%0A%20%20Inter-Modality%20Correlation%20Calibration%20Decoding&entry.906535625=Jiaming%20Li%20and%20Jiacheng%20Zhang%20and%20Zequn%20Jie%20and%20Lin%20Ma%20and%20Guanbin%20Li&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20shown%20remarkable%20capabilities%20in%0Avisual-language%20understanding%20for%20downstream%20multi-modal%20tasks.%20Despite%20their%0Asuccess%2C%20LVLMs%20still%20suffer%20from%20generating%20hallucinations%20in%20complex%0Ageneration%20tasks%2C%20leading%20to%20inconsistencies%20between%20visual%20inputs%20and%0Agenerated%20content.%20To%20address%20this%20issue%2C%20some%20approaches%20have%20introduced%0Ainference-time%20interventions%2C%20such%20as%20contrastive%20decoding%20and%20attention%0Arectification%2C%20to%20reduce%20overreliance%20on%20language%20priors.%20However%2C%20these%0Aapproaches%20overlook%20hallucinations%20stemming%20from%20spurious%20inter-modality%0Acorrelations.%20In%20this%20paper%2C%20we%20propose%20an%20Inter-Modality%20Correlation%0ACalibration%20Decoding%20%28IMCCD%29%20method%20to%20mitigate%20hallucinations%20in%20LVLMs%20in%20a%0Atraining-free%20manner.%20In%20this%20method%2C%20we%20design%20a%20Cross-Modal%20Value-Enhanced%0ADecoding%28CMVED%29%20module%20to%20alleviate%20hallucination%20by%20a%20novel%20contrastive%0Adecoding%20mechanism.%20During%20the%20estimation%20of%20distorted%20distribution%2C%20CMVED%0Amasks%20the%20value%20vectors%20associated%20with%20significant%20cross-modal%20attention%0Aweights%2C%20which%20address%20both%20uni-modality%20overreliance%20and%20misleading%0Ainter-modality%20correlations.%20Additionally%2C%20a%20Content-Driven%20Attention%0ARefinement%28CDAR%29%20module%20refines%20cross-modal%20attention%20weights%2C%20guiding%20LVLMs%20to%0Afocus%20on%20important%20visual%20content.%20Experimental%20results%20on%20diverse%0Ahallucination%20benchmarks%20validate%20the%20superiority%20of%20our%20method%20over%20existing%0Astate-of-the-art%20techniques%20in%20reducing%20hallucinations%20in%20LVLM%20text%20generation.%0AOur%20code%20will%20be%20available%20at%20https%3A//github.com/lijm48/IMCCD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01926v1&entry.124074799=Read"},
{"title": "Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues\n  and Challenges", "author": "Shagun Sinha", "abstract": "  This thesis presents Abstractive Text Summarization models for contemporary\nSanskrit prose. The first chapter, titled Introduction, presents the motivation\nbehind this work, the research questions, and the conceptual framework.\nSanskrit is a low-resource inflectional language. The key research question\nthat this thesis investigates is what the challenges in developing an\nabstractive TS for Sanskrit. To answer the key research questions,\nsub-questions based on four different themes have been posed in this work. The\nsecond chapter, Literature Review, surveys the previous works done. The third\nchapter, data preparation, answers the remaining three questions from the third\ntheme. It reports the data collection and preprocessing challenges for both\nlanguage model and summarization model trainings. The fourth chapter reports\nthe training and inference of models and the results obtained therein. This\nresearch has initiated a pipeline for Sanskrit abstractive text summarization\nand has reported the challenges faced at every stage of the development. The\nresearch questions based on every theme have been answered to answer the key\nresearch question.\n", "link": "http://arxiv.org/abs/2501.01933v1", "date": "2025-01-03", "relevancy": 2.1414, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4398}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abstractive%20Text%20Summarization%20for%20Contemporary%20Sanskrit%20Prose%3A%20Issues%0A%20%20and%20Challenges&body=Title%3A%20Abstractive%20Text%20Summarization%20for%20Contemporary%20Sanskrit%20Prose%3A%20Issues%0A%20%20and%20Challenges%0AAuthor%3A%20Shagun%20Sinha%0AAbstract%3A%20%20%20This%20thesis%20presents%20Abstractive%20Text%20Summarization%20models%20for%20contemporary%0ASanskrit%20prose.%20The%20first%20chapter%2C%20titled%20Introduction%2C%20presents%20the%20motivation%0Abehind%20this%20work%2C%20the%20research%20questions%2C%20and%20the%20conceptual%20framework.%0ASanskrit%20is%20a%20low-resource%20inflectional%20language.%20The%20key%20research%20question%0Athat%20this%20thesis%20investigates%20is%20what%20the%20challenges%20in%20developing%20an%0Aabstractive%20TS%20for%20Sanskrit.%20To%20answer%20the%20key%20research%20questions%2C%0Asub-questions%20based%20on%20four%20different%20themes%20have%20been%20posed%20in%20this%20work.%20The%0Asecond%20chapter%2C%20Literature%20Review%2C%20surveys%20the%20previous%20works%20done.%20The%20third%0Achapter%2C%20data%20preparation%2C%20answers%20the%20remaining%20three%20questions%20from%20the%20third%0Atheme.%20It%20reports%20the%20data%20collection%20and%20preprocessing%20challenges%20for%20both%0Alanguage%20model%20and%20summarization%20model%20trainings.%20The%20fourth%20chapter%20reports%0Athe%20training%20and%20inference%20of%20models%20and%20the%20results%20obtained%20therein.%20This%0Aresearch%20has%20initiated%20a%20pipeline%20for%20Sanskrit%20abstractive%20text%20summarization%0Aand%20has%20reported%20the%20challenges%20faced%20at%20every%20stage%20of%20the%20development.%20The%0Aresearch%20questions%20based%20on%20every%20theme%20have%20been%20answered%20to%20answer%20the%20key%0Aresearch%20question.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbstractive%2520Text%2520Summarization%2520for%2520Contemporary%2520Sanskrit%2520Prose%253A%2520Issues%250A%2520%2520and%2520Challenges%26entry.906535625%3DShagun%2520Sinha%26entry.1292438233%3D%2520%2520This%2520thesis%2520presents%2520Abstractive%2520Text%2520Summarization%2520models%2520for%2520contemporary%250ASanskrit%2520prose.%2520The%2520first%2520chapter%252C%2520titled%2520Introduction%252C%2520presents%2520the%2520motivation%250Abehind%2520this%2520work%252C%2520the%2520research%2520questions%252C%2520and%2520the%2520conceptual%2520framework.%250ASanskrit%2520is%2520a%2520low-resource%2520inflectional%2520language.%2520The%2520key%2520research%2520question%250Athat%2520this%2520thesis%2520investigates%2520is%2520what%2520the%2520challenges%2520in%2520developing%2520an%250Aabstractive%2520TS%2520for%2520Sanskrit.%2520To%2520answer%2520the%2520key%2520research%2520questions%252C%250Asub-questions%2520based%2520on%2520four%2520different%2520themes%2520have%2520been%2520posed%2520in%2520this%2520work.%2520The%250Asecond%2520chapter%252C%2520Literature%2520Review%252C%2520surveys%2520the%2520previous%2520works%2520done.%2520The%2520third%250Achapter%252C%2520data%2520preparation%252C%2520answers%2520the%2520remaining%2520three%2520questions%2520from%2520the%2520third%250Atheme.%2520It%2520reports%2520the%2520data%2520collection%2520and%2520preprocessing%2520challenges%2520for%2520both%250Alanguage%2520model%2520and%2520summarization%2520model%2520trainings.%2520The%2520fourth%2520chapter%2520reports%250Athe%2520training%2520and%2520inference%2520of%2520models%2520and%2520the%2520results%2520obtained%2520therein.%2520This%250Aresearch%2520has%2520initiated%2520a%2520pipeline%2520for%2520Sanskrit%2520abstractive%2520text%2520summarization%250Aand%2520has%2520reported%2520the%2520challenges%2520faced%2520at%2520every%2520stage%2520of%2520the%2520development.%2520The%250Aresearch%2520questions%2520based%2520on%2520every%2520theme%2520have%2520been%2520answered%2520to%2520answer%2520the%2520key%250Aresearch%2520question.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abstractive%20Text%20Summarization%20for%20Contemporary%20Sanskrit%20Prose%3A%20Issues%0A%20%20and%20Challenges&entry.906535625=Shagun%20Sinha&entry.1292438233=%20%20This%20thesis%20presents%20Abstractive%20Text%20Summarization%20models%20for%20contemporary%0ASanskrit%20prose.%20The%20first%20chapter%2C%20titled%20Introduction%2C%20presents%20the%20motivation%0Abehind%20this%20work%2C%20the%20research%20questions%2C%20and%20the%20conceptual%20framework.%0ASanskrit%20is%20a%20low-resource%20inflectional%20language.%20The%20key%20research%20question%0Athat%20this%20thesis%20investigates%20is%20what%20the%20challenges%20in%20developing%20an%0Aabstractive%20TS%20for%20Sanskrit.%20To%20answer%20the%20key%20research%20questions%2C%0Asub-questions%20based%20on%20four%20different%20themes%20have%20been%20posed%20in%20this%20work.%20The%0Asecond%20chapter%2C%20Literature%20Review%2C%20surveys%20the%20previous%20works%20done.%20The%20third%0Achapter%2C%20data%20preparation%2C%20answers%20the%20remaining%20three%20questions%20from%20the%20third%0Atheme.%20It%20reports%20the%20data%20collection%20and%20preprocessing%20challenges%20for%20both%0Alanguage%20model%20and%20summarization%20model%20trainings.%20The%20fourth%20chapter%20reports%0Athe%20training%20and%20inference%20of%20models%20and%20the%20results%20obtained%20therein.%20This%0Aresearch%20has%20initiated%20a%20pipeline%20for%20Sanskrit%20abstractive%20text%20summarization%0Aand%20has%20reported%20the%20challenges%20faced%20at%20every%20stage%20of%20the%20development.%20The%0Aresearch%20questions%20based%20on%20every%20theme%20have%20been%20answered%20to%20answer%20the%20key%0Aresearch%20question.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01933v1&entry.124074799=Read"},
{"title": "UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial\n  Vehicle Imagery", "author": "Huaxiang Zhang and Kai Liu and Zhongxue Gan and Guo-Niu Zhu", "abstract": "  Unmanned aerial vehicle object detection (UAV-OD) has been widely used in\nvarious scenarios. However, most existing UAV-OD algorithms rely on manually\ndesigned components, which require extensive tuning. End-to-end models that do\nnot depend on such manually designed components are mainly designed for natural\nimages, which are less effective for UAV imagery. To address such challenges,\nthis paper proposes an efficient detection transformer (DETR) framework\ntailored for UAV imagery, i.e., UAV-DETR. The framework includes a multi-scale\nfeature fusion with frequency enhancement module, which captures both spatial\nand frequency information at different scales. In addition, a frequency-focused\ndown-sampling module is presented to retain critical spatial details during\ndown-sampling. A semantic alignment and calibration module is developed to\nalign and fuse features from different fusion paths. Experimental results\ndemonstrate the effectiveness and generalization of our approach across various\nUAV imagery datasets. On the VisDrone dataset, our method improves AP by 3.1\\%\nand $\\text{AP}_{50}$ by 4.2\\% over the baseline. Similar enhancements are\nobserved on the UAVVaste dataset. The project page:\nhttps://github.com/ValiantDiligent/UAV-DETR\n", "link": "http://arxiv.org/abs/2501.01855v1", "date": "2025-01-03", "relevancy": 2.1373, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5615}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5297}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAV-DETR%3A%20Efficient%20End-to-End%20Object%20Detection%20for%20Unmanned%20Aerial%0A%20%20Vehicle%20Imagery&body=Title%3A%20UAV-DETR%3A%20Efficient%20End-to-End%20Object%20Detection%20for%20Unmanned%20Aerial%0A%20%20Vehicle%20Imagery%0AAuthor%3A%20Huaxiang%20Zhang%20and%20Kai%20Liu%20and%20Zhongxue%20Gan%20and%20Guo-Niu%20Zhu%0AAbstract%3A%20%20%20Unmanned%20aerial%20vehicle%20object%20detection%20%28UAV-OD%29%20has%20been%20widely%20used%20in%0Avarious%20scenarios.%20However%2C%20most%20existing%20UAV-OD%20algorithms%20rely%20on%20manually%0Adesigned%20components%2C%20which%20require%20extensive%20tuning.%20End-to-end%20models%20that%20do%0Anot%20depend%20on%20such%20manually%20designed%20components%20are%20mainly%20designed%20for%20natural%0Aimages%2C%20which%20are%20less%20effective%20for%20UAV%20imagery.%20To%20address%20such%20challenges%2C%0Athis%20paper%20proposes%20an%20efficient%20detection%20transformer%20%28DETR%29%20framework%0Atailored%20for%20UAV%20imagery%2C%20i.e.%2C%20UAV-DETR.%20The%20framework%20includes%20a%20multi-scale%0Afeature%20fusion%20with%20frequency%20enhancement%20module%2C%20which%20captures%20both%20spatial%0Aand%20frequency%20information%20at%20different%20scales.%20In%20addition%2C%20a%20frequency-focused%0Adown-sampling%20module%20is%20presented%20to%20retain%20critical%20spatial%20details%20during%0Adown-sampling.%20A%20semantic%20alignment%20and%20calibration%20module%20is%20developed%20to%0Aalign%20and%20fuse%20features%20from%20different%20fusion%20paths.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20and%20generalization%20of%20our%20approach%20across%20various%0AUAV%20imagery%20datasets.%20On%20the%20VisDrone%20dataset%2C%20our%20method%20improves%20AP%20by%203.1%5C%25%0Aand%20%24%5Ctext%7BAP%7D_%7B50%7D%24%20by%204.2%5C%25%20over%20the%20baseline.%20Similar%20enhancements%20are%0Aobserved%20on%20the%20UAVVaste%20dataset.%20The%20project%20page%3A%0Ahttps%3A//github.com/ValiantDiligent/UAV-DETR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAV-DETR%253A%2520Efficient%2520End-to-End%2520Object%2520Detection%2520for%2520Unmanned%2520Aerial%250A%2520%2520Vehicle%2520Imagery%26entry.906535625%3DHuaxiang%2520Zhang%2520and%2520Kai%2520Liu%2520and%2520Zhongxue%2520Gan%2520and%2520Guo-Niu%2520Zhu%26entry.1292438233%3D%2520%2520Unmanned%2520aerial%2520vehicle%2520object%2520detection%2520%2528UAV-OD%2529%2520has%2520been%2520widely%2520used%2520in%250Avarious%2520scenarios.%2520However%252C%2520most%2520existing%2520UAV-OD%2520algorithms%2520rely%2520on%2520manually%250Adesigned%2520components%252C%2520which%2520require%2520extensive%2520tuning.%2520End-to-end%2520models%2520that%2520do%250Anot%2520depend%2520on%2520such%2520manually%2520designed%2520components%2520are%2520mainly%2520designed%2520for%2520natural%250Aimages%252C%2520which%2520are%2520less%2520effective%2520for%2520UAV%2520imagery.%2520To%2520address%2520such%2520challenges%252C%250Athis%2520paper%2520proposes%2520an%2520efficient%2520detection%2520transformer%2520%2528DETR%2529%2520framework%250Atailored%2520for%2520UAV%2520imagery%252C%2520i.e.%252C%2520UAV-DETR.%2520The%2520framework%2520includes%2520a%2520multi-scale%250Afeature%2520fusion%2520with%2520frequency%2520enhancement%2520module%252C%2520which%2520captures%2520both%2520spatial%250Aand%2520frequency%2520information%2520at%2520different%2520scales.%2520In%2520addition%252C%2520a%2520frequency-focused%250Adown-sampling%2520module%2520is%2520presented%2520to%2520retain%2520critical%2520spatial%2520details%2520during%250Adown-sampling.%2520A%2520semantic%2520alignment%2520and%2520calibration%2520module%2520is%2520developed%2520to%250Aalign%2520and%2520fuse%2520features%2520from%2520different%2520fusion%2520paths.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520and%2520generalization%2520of%2520our%2520approach%2520across%2520various%250AUAV%2520imagery%2520datasets.%2520On%2520the%2520VisDrone%2520dataset%252C%2520our%2520method%2520improves%2520AP%2520by%25203.1%255C%2525%250Aand%2520%2524%255Ctext%257BAP%257D_%257B50%257D%2524%2520by%25204.2%255C%2525%2520over%2520the%2520baseline.%2520Similar%2520enhancements%2520are%250Aobserved%2520on%2520the%2520UAVVaste%2520dataset.%2520The%2520project%2520page%253A%250Ahttps%253A//github.com/ValiantDiligent/UAV-DETR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAV-DETR%3A%20Efficient%20End-to-End%20Object%20Detection%20for%20Unmanned%20Aerial%0A%20%20Vehicle%20Imagery&entry.906535625=Huaxiang%20Zhang%20and%20Kai%20Liu%20and%20Zhongxue%20Gan%20and%20Guo-Niu%20Zhu&entry.1292438233=%20%20Unmanned%20aerial%20vehicle%20object%20detection%20%28UAV-OD%29%20has%20been%20widely%20used%20in%0Avarious%20scenarios.%20However%2C%20most%20existing%20UAV-OD%20algorithms%20rely%20on%20manually%0Adesigned%20components%2C%20which%20require%20extensive%20tuning.%20End-to-end%20models%20that%20do%0Anot%20depend%20on%20such%20manually%20designed%20components%20are%20mainly%20designed%20for%20natural%0Aimages%2C%20which%20are%20less%20effective%20for%20UAV%20imagery.%20To%20address%20such%20challenges%2C%0Athis%20paper%20proposes%20an%20efficient%20detection%20transformer%20%28DETR%29%20framework%0Atailored%20for%20UAV%20imagery%2C%20i.e.%2C%20UAV-DETR.%20The%20framework%20includes%20a%20multi-scale%0Afeature%20fusion%20with%20frequency%20enhancement%20module%2C%20which%20captures%20both%20spatial%0Aand%20frequency%20information%20at%20different%20scales.%20In%20addition%2C%20a%20frequency-focused%0Adown-sampling%20module%20is%20presented%20to%20retain%20critical%20spatial%20details%20during%0Adown-sampling.%20A%20semantic%20alignment%20and%20calibration%20module%20is%20developed%20to%0Aalign%20and%20fuse%20features%20from%20different%20fusion%20paths.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20and%20generalization%20of%20our%20approach%20across%20various%0AUAV%20imagery%20datasets.%20On%20the%20VisDrone%20dataset%2C%20our%20method%20improves%20AP%20by%203.1%5C%25%0Aand%20%24%5Ctext%7BAP%7D_%7B50%7D%24%20by%204.2%5C%25%20over%20the%20baseline.%20Similar%20enhancements%20are%0Aobserved%20on%20the%20UAVVaste%20dataset.%20The%20project%20page%3A%0Ahttps%3A//github.com/ValiantDiligent/UAV-DETR%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01855v1&entry.124074799=Read"},
{"title": "PB-UAP: Hybrid Universal Adversarial Attack For Image Segmentation", "author": "Yufei Song and Ziqi Zhou and Minghui Li and Xianlong Wang and Hangtao Zhang and Menghao Deng and Wei Wan and Shengshan Hu and Leo Yu Zhang", "abstract": "  With the rapid advancement of deep learning, the model robustness has become\na significant research hotspot, \\ie, adversarial attacks on deep neural\nnetworks. Existing works primarily focus on image classification tasks, aiming\nto alter the model's predicted labels. Due to the output complexity and deeper\nnetwork architectures, research on adversarial examples for segmentation models\nis still limited, particularly for universal adversarial perturbations. In this\npaper, we propose a novel universal adversarial attack method designed for\nsegmentation models, which includes dual feature separation and low-frequency\nscattering modules. The two modules guide the training of adversarial examples\nin the pixel and frequency space, respectively. Experiments demonstrate that\nour method achieves high attack success rates surpassing the state-of-the-art\nmethods, and exhibits strong transferability across different models.\n", "link": "http://arxiv.org/abs/2412.16651v2", "date": "2025-01-03", "relevancy": 2.1105, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5365}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5275}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PB-UAP%3A%20Hybrid%20Universal%20Adversarial%20Attack%20For%20Image%20Segmentation&body=Title%3A%20PB-UAP%3A%20Hybrid%20Universal%20Adversarial%20Attack%20For%20Image%20Segmentation%0AAuthor%3A%20Yufei%20Song%20and%20Ziqi%20Zhou%20and%20Minghui%20Li%20and%20Xianlong%20Wang%20and%20Hangtao%20Zhang%20and%20Menghao%20Deng%20and%20Wei%20Wan%20and%20Shengshan%20Hu%20and%20Leo%20Yu%20Zhang%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20deep%20learning%2C%20the%20model%20robustness%20has%20become%0Aa%20significant%20research%20hotspot%2C%20%5Cie%2C%20adversarial%20attacks%20on%20deep%20neural%0Anetworks.%20Existing%20works%20primarily%20focus%20on%20image%20classification%20tasks%2C%20aiming%0Ato%20alter%20the%20model%27s%20predicted%20labels.%20Due%20to%20the%20output%20complexity%20and%20deeper%0Anetwork%20architectures%2C%20research%20on%20adversarial%20examples%20for%20segmentation%20models%0Ais%20still%20limited%2C%20particularly%20for%20universal%20adversarial%20perturbations.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20universal%20adversarial%20attack%20method%20designed%20for%0Asegmentation%20models%2C%20which%20includes%20dual%20feature%20separation%20and%20low-frequency%0Ascattering%20modules.%20The%20two%20modules%20guide%20the%20training%20of%20adversarial%20examples%0Ain%20the%20pixel%20and%20frequency%20space%2C%20respectively.%20Experiments%20demonstrate%20that%0Aour%20method%20achieves%20high%20attack%20success%20rates%20surpassing%20the%20state-of-the-art%0Amethods%2C%20and%20exhibits%20strong%20transferability%20across%20different%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16651v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPB-UAP%253A%2520Hybrid%2520Universal%2520Adversarial%2520Attack%2520For%2520Image%2520Segmentation%26entry.906535625%3DYufei%2520Song%2520and%2520Ziqi%2520Zhou%2520and%2520Minghui%2520Li%2520and%2520Xianlong%2520Wang%2520and%2520Hangtao%2520Zhang%2520and%2520Menghao%2520Deng%2520and%2520Wei%2520Wan%2520and%2520Shengshan%2520Hu%2520and%2520Leo%2520Yu%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520deep%2520learning%252C%2520the%2520model%2520robustness%2520has%2520become%250Aa%2520significant%2520research%2520hotspot%252C%2520%255Cie%252C%2520adversarial%2520attacks%2520on%2520deep%2520neural%250Anetworks.%2520Existing%2520works%2520primarily%2520focus%2520on%2520image%2520classification%2520tasks%252C%2520aiming%250Ato%2520alter%2520the%2520model%2527s%2520predicted%2520labels.%2520Due%2520to%2520the%2520output%2520complexity%2520and%2520deeper%250Anetwork%2520architectures%252C%2520research%2520on%2520adversarial%2520examples%2520for%2520segmentation%2520models%250Ais%2520still%2520limited%252C%2520particularly%2520for%2520universal%2520adversarial%2520perturbations.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520universal%2520adversarial%2520attack%2520method%2520designed%2520for%250Asegmentation%2520models%252C%2520which%2520includes%2520dual%2520feature%2520separation%2520and%2520low-frequency%250Ascattering%2520modules.%2520The%2520two%2520modules%2520guide%2520the%2520training%2520of%2520adversarial%2520examples%250Ain%2520the%2520pixel%2520and%2520frequency%2520space%252C%2520respectively.%2520Experiments%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520high%2520attack%2520success%2520rates%2520surpassing%2520the%2520state-of-the-art%250Amethods%252C%2520and%2520exhibits%2520strong%2520transferability%2520across%2520different%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16651v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PB-UAP%3A%20Hybrid%20Universal%20Adversarial%20Attack%20For%20Image%20Segmentation&entry.906535625=Yufei%20Song%20and%20Ziqi%20Zhou%20and%20Minghui%20Li%20and%20Xianlong%20Wang%20and%20Hangtao%20Zhang%20and%20Menghao%20Deng%20and%20Wei%20Wan%20and%20Shengshan%20Hu%20and%20Leo%20Yu%20Zhang&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20deep%20learning%2C%20the%20model%20robustness%20has%20become%0Aa%20significant%20research%20hotspot%2C%20%5Cie%2C%20adversarial%20attacks%20on%20deep%20neural%0Anetworks.%20Existing%20works%20primarily%20focus%20on%20image%20classification%20tasks%2C%20aiming%0Ato%20alter%20the%20model%27s%20predicted%20labels.%20Due%20to%20the%20output%20complexity%20and%20deeper%0Anetwork%20architectures%2C%20research%20on%20adversarial%20examples%20for%20segmentation%20models%0Ais%20still%20limited%2C%20particularly%20for%20universal%20adversarial%20perturbations.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20universal%20adversarial%20attack%20method%20designed%20for%0Asegmentation%20models%2C%20which%20includes%20dual%20feature%20separation%20and%20low-frequency%0Ascattering%20modules.%20The%20two%20modules%20guide%20the%20training%20of%20adversarial%20examples%0Ain%20the%20pixel%20and%20frequency%20space%2C%20respectively.%20Experiments%20demonstrate%20that%0Aour%20method%20achieves%20high%20attack%20success%20rates%20surpassing%20the%20state-of-the-art%0Amethods%2C%20and%20exhibits%20strong%20transferability%20across%20different%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16651v2&entry.124074799=Read"},
{"title": "FS-Net: Full Scale Network and Adaptive Threshold for Improving\n  Extraction of Micro-Retinal Vessel Structures", "author": "Melaku N. Getahun and Oleg Y. Rogov and Dmitry V. Dylov and Andrey Somov and Ahmed Bouridane and Rifat Hamoudi", "abstract": "  Retinal vascular segmentation, a widely researched topic in biomedical image\nprocessing, aims to reduce the workload of ophthalmologists in treating and\ndetecting retinal disorders. Segmenting retinal vessels presents unique\nchallenges; previous techniques often failed to effectively segment branches\nand microvascular structures. Recent neural network approaches struggle to\nbalance local and global properties and frequently miss tiny end vessels,\nhindering the achievement of desired results. To address these issues in\nretinal vessel segmentation, we propose a comprehensive micro-vessel extraction\nmechanism based on an encoder-decoder neural network architecture. This network\nincludes residual, encoder booster, bottleneck enhancement, squeeze, and\nexcitation building blocks. These components synergistically enhance feature\nextraction and improve the prediction accuracy of the segmentation map. Our\nsolution has been evaluated using the DRIVE, CHASE-DB1, and STARE datasets,\nyielding competitive results compared to previous studies. The AUC and accuracy\non the DRIVE dataset are 0.9884 and 0.9702, respectively. For the CHASE-DB1\ndataset, these scores are 0.9903 and 0.9755, respectively, and for the STARE\ndataset, they are 0.9916 and 0.9750. Given its accurate and robust performance,\nthe proposed approach is a solid candidate for being implemented in real-life\ndiagnostic centers and aiding ophthalmologists.\n", "link": "http://arxiv.org/abs/2311.08059v4", "date": "2025-01-03", "relevancy": 2.0836, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5406}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5188}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FS-Net%3A%20Full%20Scale%20Network%20and%20Adaptive%20Threshold%20for%20Improving%0A%20%20Extraction%20of%20Micro-Retinal%20Vessel%20Structures&body=Title%3A%20FS-Net%3A%20Full%20Scale%20Network%20and%20Adaptive%20Threshold%20for%20Improving%0A%20%20Extraction%20of%20Micro-Retinal%20Vessel%20Structures%0AAuthor%3A%20Melaku%20N.%20Getahun%20and%20Oleg%20Y.%20Rogov%20and%20Dmitry%20V.%20Dylov%20and%20Andrey%20Somov%20and%20Ahmed%20Bouridane%20and%20Rifat%20Hamoudi%0AAbstract%3A%20%20%20Retinal%20vascular%20segmentation%2C%20a%20widely%20researched%20topic%20in%20biomedical%20image%0Aprocessing%2C%20aims%20to%20reduce%20the%20workload%20of%20ophthalmologists%20in%20treating%20and%0Adetecting%20retinal%20disorders.%20Segmenting%20retinal%20vessels%20presents%20unique%0Achallenges%3B%20previous%20techniques%20often%20failed%20to%20effectively%20segment%20branches%0Aand%20microvascular%20structures.%20Recent%20neural%20network%20approaches%20struggle%20to%0Abalance%20local%20and%20global%20properties%20and%20frequently%20miss%20tiny%20end%20vessels%2C%0Ahindering%20the%20achievement%20of%20desired%20results.%20To%20address%20these%20issues%20in%0Aretinal%20vessel%20segmentation%2C%20we%20propose%20a%20comprehensive%20micro-vessel%20extraction%0Amechanism%20based%20on%20an%20encoder-decoder%20neural%20network%20architecture.%20This%20network%0Aincludes%20residual%2C%20encoder%20booster%2C%20bottleneck%20enhancement%2C%20squeeze%2C%20and%0Aexcitation%20building%20blocks.%20These%20components%20synergistically%20enhance%20feature%0Aextraction%20and%20improve%20the%20prediction%20accuracy%20of%20the%20segmentation%20map.%20Our%0Asolution%20has%20been%20evaluated%20using%20the%20DRIVE%2C%20CHASE-DB1%2C%20and%20STARE%20datasets%2C%0Ayielding%20competitive%20results%20compared%20to%20previous%20studies.%20The%20AUC%20and%20accuracy%0Aon%20the%20DRIVE%20dataset%20are%200.9884%20and%200.9702%2C%20respectively.%20For%20the%20CHASE-DB1%0Adataset%2C%20these%20scores%20are%200.9903%20and%200.9755%2C%20respectively%2C%20and%20for%20the%20STARE%0Adataset%2C%20they%20are%200.9916%20and%200.9750.%20Given%20its%20accurate%20and%20robust%20performance%2C%0Athe%20proposed%20approach%20is%20a%20solid%20candidate%20for%20being%20implemented%20in%20real-life%0Adiagnostic%20centers%20and%20aiding%20ophthalmologists.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08059v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFS-Net%253A%2520Full%2520Scale%2520Network%2520and%2520Adaptive%2520Threshold%2520for%2520Improving%250A%2520%2520Extraction%2520of%2520Micro-Retinal%2520Vessel%2520Structures%26entry.906535625%3DMelaku%2520N.%2520Getahun%2520and%2520Oleg%2520Y.%2520Rogov%2520and%2520Dmitry%2520V.%2520Dylov%2520and%2520Andrey%2520Somov%2520and%2520Ahmed%2520Bouridane%2520and%2520Rifat%2520Hamoudi%26entry.1292438233%3D%2520%2520Retinal%2520vascular%2520segmentation%252C%2520a%2520widely%2520researched%2520topic%2520in%2520biomedical%2520image%250Aprocessing%252C%2520aims%2520to%2520reduce%2520the%2520workload%2520of%2520ophthalmologists%2520in%2520treating%2520and%250Adetecting%2520retinal%2520disorders.%2520Segmenting%2520retinal%2520vessels%2520presents%2520unique%250Achallenges%253B%2520previous%2520techniques%2520often%2520failed%2520to%2520effectively%2520segment%2520branches%250Aand%2520microvascular%2520structures.%2520Recent%2520neural%2520network%2520approaches%2520struggle%2520to%250Abalance%2520local%2520and%2520global%2520properties%2520and%2520frequently%2520miss%2520tiny%2520end%2520vessels%252C%250Ahindering%2520the%2520achievement%2520of%2520desired%2520results.%2520To%2520address%2520these%2520issues%2520in%250Aretinal%2520vessel%2520segmentation%252C%2520we%2520propose%2520a%2520comprehensive%2520micro-vessel%2520extraction%250Amechanism%2520based%2520on%2520an%2520encoder-decoder%2520neural%2520network%2520architecture.%2520This%2520network%250Aincludes%2520residual%252C%2520encoder%2520booster%252C%2520bottleneck%2520enhancement%252C%2520squeeze%252C%2520and%250Aexcitation%2520building%2520blocks.%2520These%2520components%2520synergistically%2520enhance%2520feature%250Aextraction%2520and%2520improve%2520the%2520prediction%2520accuracy%2520of%2520the%2520segmentation%2520map.%2520Our%250Asolution%2520has%2520been%2520evaluated%2520using%2520the%2520DRIVE%252C%2520CHASE-DB1%252C%2520and%2520STARE%2520datasets%252C%250Ayielding%2520competitive%2520results%2520compared%2520to%2520previous%2520studies.%2520The%2520AUC%2520and%2520accuracy%250Aon%2520the%2520DRIVE%2520dataset%2520are%25200.9884%2520and%25200.9702%252C%2520respectively.%2520For%2520the%2520CHASE-DB1%250Adataset%252C%2520these%2520scores%2520are%25200.9903%2520and%25200.9755%252C%2520respectively%252C%2520and%2520for%2520the%2520STARE%250Adataset%252C%2520they%2520are%25200.9916%2520and%25200.9750.%2520Given%2520its%2520accurate%2520and%2520robust%2520performance%252C%250Athe%2520proposed%2520approach%2520is%2520a%2520solid%2520candidate%2520for%2520being%2520implemented%2520in%2520real-life%250Adiagnostic%2520centers%2520and%2520aiding%2520ophthalmologists.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08059v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FS-Net%3A%20Full%20Scale%20Network%20and%20Adaptive%20Threshold%20for%20Improving%0A%20%20Extraction%20of%20Micro-Retinal%20Vessel%20Structures&entry.906535625=Melaku%20N.%20Getahun%20and%20Oleg%20Y.%20Rogov%20and%20Dmitry%20V.%20Dylov%20and%20Andrey%20Somov%20and%20Ahmed%20Bouridane%20and%20Rifat%20Hamoudi&entry.1292438233=%20%20Retinal%20vascular%20segmentation%2C%20a%20widely%20researched%20topic%20in%20biomedical%20image%0Aprocessing%2C%20aims%20to%20reduce%20the%20workload%20of%20ophthalmologists%20in%20treating%20and%0Adetecting%20retinal%20disorders.%20Segmenting%20retinal%20vessels%20presents%20unique%0Achallenges%3B%20previous%20techniques%20often%20failed%20to%20effectively%20segment%20branches%0Aand%20microvascular%20structures.%20Recent%20neural%20network%20approaches%20struggle%20to%0Abalance%20local%20and%20global%20properties%20and%20frequently%20miss%20tiny%20end%20vessels%2C%0Ahindering%20the%20achievement%20of%20desired%20results.%20To%20address%20these%20issues%20in%0Aretinal%20vessel%20segmentation%2C%20we%20propose%20a%20comprehensive%20micro-vessel%20extraction%0Amechanism%20based%20on%20an%20encoder-decoder%20neural%20network%20architecture.%20This%20network%0Aincludes%20residual%2C%20encoder%20booster%2C%20bottleneck%20enhancement%2C%20squeeze%2C%20and%0Aexcitation%20building%20blocks.%20These%20components%20synergistically%20enhance%20feature%0Aextraction%20and%20improve%20the%20prediction%20accuracy%20of%20the%20segmentation%20map.%20Our%0Asolution%20has%20been%20evaluated%20using%20the%20DRIVE%2C%20CHASE-DB1%2C%20and%20STARE%20datasets%2C%0Ayielding%20competitive%20results%20compared%20to%20previous%20studies.%20The%20AUC%20and%20accuracy%0Aon%20the%20DRIVE%20dataset%20are%200.9884%20and%200.9702%2C%20respectively.%20For%20the%20CHASE-DB1%0Adataset%2C%20these%20scores%20are%200.9903%20and%200.9755%2C%20respectively%2C%20and%20for%20the%20STARE%0Adataset%2C%20they%20are%200.9916%20and%200.9750.%20Given%20its%20accurate%20and%20robust%20performance%2C%0Athe%20proposed%20approach%20is%20a%20solid%20candidate%20for%20being%20implemented%20in%20real-life%0Adiagnostic%20centers%20and%20aiding%20ophthalmologists.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08059v4&entry.124074799=Read"},
{"title": "Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent\n  Hypersonic Flows on Arbitrary Grids", "author": "Ahmad Peyvan and Varun Kumar", "abstract": "  Designing re-entry vehicles requires accurate predictions of hypersonic flow\naround their geometry. Rapid prediction of such flows can revolutionize vehicle\ndesign, particularly for morphing geometries. We evaluate advanced neural\noperator models such as Deep Operator Networks (DeepONet),\nparameter-conditioned U-Net, Fourier Neural Operator (FNO), and MeshGraphNet,\nwith the objective of addressing the challenge of learning geometry-dependent\nhypersonic flow fields with limited data. Specifically, we compare the\nperformance of these models for two grid types: uniform Cartesian and irregular\ngrids. To train these models, we use 36 unique elliptic geometries for\ngenerating high-fidelity simulations with a high-order entropy-stable DGSEM\nsolver, emphasizing the challenge of working with a scarce dataset. We evaluate\nand compare the four operator-based models for their efficacy in predicting\nhypersonic flow field around the elliptic body. Moreover, we develop a novel\nframework, called Fusion DeepONet, which leverages neural field concepts and\ngeneralizes effectively across varying geometries. Despite the scarcity of\ntraining data, Fusion DeepONet achieves performance comparable to\nparameter-conditioned U-Net on uniform grids while it outperforms MeshGraphNet\nand vanilla DeepONet on irregular, arbitrary grids. Fusion DeepONet requires\nsignificantly fewer trainable parameters as compared to U-Net, MeshGraphNet,\nand FNO, making it computationally efficient. We also analyze the basis\nfunctions of the Fusion DeepONet model using Singular Value Decomposition. This\nanalysis reveals that Fusion DeepONet generalizes effectively to unseen\nsolutions and adapts to varying geometries and grid points, demonstrating its\nrobustness in scenarios with limited training data.\n", "link": "http://arxiv.org/abs/2501.01934v1", "date": "2025-01-03", "relevancy": 2.0692, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5503}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5365}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion%20DeepONet%3A%20A%20Data-Efficient%20Neural%20Operator%20for%20Geometry-Dependent%0A%20%20Hypersonic%20Flows%20on%20Arbitrary%20Grids&body=Title%3A%20Fusion%20DeepONet%3A%20A%20Data-Efficient%20Neural%20Operator%20for%20Geometry-Dependent%0A%20%20Hypersonic%20Flows%20on%20Arbitrary%20Grids%0AAuthor%3A%20Ahmad%20Peyvan%20and%20Varun%20Kumar%0AAbstract%3A%20%20%20Designing%20re-entry%20vehicles%20requires%20accurate%20predictions%20of%20hypersonic%20flow%0Aaround%20their%20geometry.%20Rapid%20prediction%20of%20such%20flows%20can%20revolutionize%20vehicle%0Adesign%2C%20particularly%20for%20morphing%20geometries.%20We%20evaluate%20advanced%20neural%0Aoperator%20models%20such%20as%20Deep%20Operator%20Networks%20%28DeepONet%29%2C%0Aparameter-conditioned%20U-Net%2C%20Fourier%20Neural%20Operator%20%28FNO%29%2C%20and%20MeshGraphNet%2C%0Awith%20the%20objective%20of%20addressing%20the%20challenge%20of%20learning%20geometry-dependent%0Ahypersonic%20flow%20fields%20with%20limited%20data.%20Specifically%2C%20we%20compare%20the%0Aperformance%20of%20these%20models%20for%20two%20grid%20types%3A%20uniform%20Cartesian%20and%20irregular%0Agrids.%20To%20train%20these%20models%2C%20we%20use%2036%20unique%20elliptic%20geometries%20for%0Agenerating%20high-fidelity%20simulations%20with%20a%20high-order%20entropy-stable%20DGSEM%0Asolver%2C%20emphasizing%20the%20challenge%20of%20working%20with%20a%20scarce%20dataset.%20We%20evaluate%0Aand%20compare%20the%20four%20operator-based%20models%20for%20their%20efficacy%20in%20predicting%0Ahypersonic%20flow%20field%20around%20the%20elliptic%20body.%20Moreover%2C%20we%20develop%20a%20novel%0Aframework%2C%20called%20Fusion%20DeepONet%2C%20which%20leverages%20neural%20field%20concepts%20and%0Ageneralizes%20effectively%20across%20varying%20geometries.%20Despite%20the%20scarcity%20of%0Atraining%20data%2C%20Fusion%20DeepONet%20achieves%20performance%20comparable%20to%0Aparameter-conditioned%20U-Net%20on%20uniform%20grids%20while%20it%20outperforms%20MeshGraphNet%0Aand%20vanilla%20DeepONet%20on%20irregular%2C%20arbitrary%20grids.%20Fusion%20DeepONet%20requires%0Asignificantly%20fewer%20trainable%20parameters%20as%20compared%20to%20U-Net%2C%20MeshGraphNet%2C%0Aand%20FNO%2C%20making%20it%20computationally%20efficient.%20We%20also%20analyze%20the%20basis%0Afunctions%20of%20the%20Fusion%20DeepONet%20model%20using%20Singular%20Value%20Decomposition.%20This%0Aanalysis%20reveals%20that%20Fusion%20DeepONet%20generalizes%20effectively%20to%20unseen%0Asolutions%20and%20adapts%20to%20varying%20geometries%20and%20grid%20points%2C%20demonstrating%20its%0Arobustness%20in%20scenarios%20with%20limited%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion%2520DeepONet%253A%2520A%2520Data-Efficient%2520Neural%2520Operator%2520for%2520Geometry-Dependent%250A%2520%2520Hypersonic%2520Flows%2520on%2520Arbitrary%2520Grids%26entry.906535625%3DAhmad%2520Peyvan%2520and%2520Varun%2520Kumar%26entry.1292438233%3D%2520%2520Designing%2520re-entry%2520vehicles%2520requires%2520accurate%2520predictions%2520of%2520hypersonic%2520flow%250Aaround%2520their%2520geometry.%2520Rapid%2520prediction%2520of%2520such%2520flows%2520can%2520revolutionize%2520vehicle%250Adesign%252C%2520particularly%2520for%2520morphing%2520geometries.%2520We%2520evaluate%2520advanced%2520neural%250Aoperator%2520models%2520such%2520as%2520Deep%2520Operator%2520Networks%2520%2528DeepONet%2529%252C%250Aparameter-conditioned%2520U-Net%252C%2520Fourier%2520Neural%2520Operator%2520%2528FNO%2529%252C%2520and%2520MeshGraphNet%252C%250Awith%2520the%2520objective%2520of%2520addressing%2520the%2520challenge%2520of%2520learning%2520geometry-dependent%250Ahypersonic%2520flow%2520fields%2520with%2520limited%2520data.%2520Specifically%252C%2520we%2520compare%2520the%250Aperformance%2520of%2520these%2520models%2520for%2520two%2520grid%2520types%253A%2520uniform%2520Cartesian%2520and%2520irregular%250Agrids.%2520To%2520train%2520these%2520models%252C%2520we%2520use%252036%2520unique%2520elliptic%2520geometries%2520for%250Agenerating%2520high-fidelity%2520simulations%2520with%2520a%2520high-order%2520entropy-stable%2520DGSEM%250Asolver%252C%2520emphasizing%2520the%2520challenge%2520of%2520working%2520with%2520a%2520scarce%2520dataset.%2520We%2520evaluate%250Aand%2520compare%2520the%2520four%2520operator-based%2520models%2520for%2520their%2520efficacy%2520in%2520predicting%250Ahypersonic%2520flow%2520field%2520around%2520the%2520elliptic%2520body.%2520Moreover%252C%2520we%2520develop%2520a%2520novel%250Aframework%252C%2520called%2520Fusion%2520DeepONet%252C%2520which%2520leverages%2520neural%2520field%2520concepts%2520and%250Ageneralizes%2520effectively%2520across%2520varying%2520geometries.%2520Despite%2520the%2520scarcity%2520of%250Atraining%2520data%252C%2520Fusion%2520DeepONet%2520achieves%2520performance%2520comparable%2520to%250Aparameter-conditioned%2520U-Net%2520on%2520uniform%2520grids%2520while%2520it%2520outperforms%2520MeshGraphNet%250Aand%2520vanilla%2520DeepONet%2520on%2520irregular%252C%2520arbitrary%2520grids.%2520Fusion%2520DeepONet%2520requires%250Asignificantly%2520fewer%2520trainable%2520parameters%2520as%2520compared%2520to%2520U-Net%252C%2520MeshGraphNet%252C%250Aand%2520FNO%252C%2520making%2520it%2520computationally%2520efficient.%2520We%2520also%2520analyze%2520the%2520basis%250Afunctions%2520of%2520the%2520Fusion%2520DeepONet%2520model%2520using%2520Singular%2520Value%2520Decomposition.%2520This%250Aanalysis%2520reveals%2520that%2520Fusion%2520DeepONet%2520generalizes%2520effectively%2520to%2520unseen%250Asolutions%2520and%2520adapts%2520to%2520varying%2520geometries%2520and%2520grid%2520points%252C%2520demonstrating%2520its%250Arobustness%2520in%2520scenarios%2520with%2520limited%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20DeepONet%3A%20A%20Data-Efficient%20Neural%20Operator%20for%20Geometry-Dependent%0A%20%20Hypersonic%20Flows%20on%20Arbitrary%20Grids&entry.906535625=Ahmad%20Peyvan%20and%20Varun%20Kumar&entry.1292438233=%20%20Designing%20re-entry%20vehicles%20requires%20accurate%20predictions%20of%20hypersonic%20flow%0Aaround%20their%20geometry.%20Rapid%20prediction%20of%20such%20flows%20can%20revolutionize%20vehicle%0Adesign%2C%20particularly%20for%20morphing%20geometries.%20We%20evaluate%20advanced%20neural%0Aoperator%20models%20such%20as%20Deep%20Operator%20Networks%20%28DeepONet%29%2C%0Aparameter-conditioned%20U-Net%2C%20Fourier%20Neural%20Operator%20%28FNO%29%2C%20and%20MeshGraphNet%2C%0Awith%20the%20objective%20of%20addressing%20the%20challenge%20of%20learning%20geometry-dependent%0Ahypersonic%20flow%20fields%20with%20limited%20data.%20Specifically%2C%20we%20compare%20the%0Aperformance%20of%20these%20models%20for%20two%20grid%20types%3A%20uniform%20Cartesian%20and%20irregular%0Agrids.%20To%20train%20these%20models%2C%20we%20use%2036%20unique%20elliptic%20geometries%20for%0Agenerating%20high-fidelity%20simulations%20with%20a%20high-order%20entropy-stable%20DGSEM%0Asolver%2C%20emphasizing%20the%20challenge%20of%20working%20with%20a%20scarce%20dataset.%20We%20evaluate%0Aand%20compare%20the%20four%20operator-based%20models%20for%20their%20efficacy%20in%20predicting%0Ahypersonic%20flow%20field%20around%20the%20elliptic%20body.%20Moreover%2C%20we%20develop%20a%20novel%0Aframework%2C%20called%20Fusion%20DeepONet%2C%20which%20leverages%20neural%20field%20concepts%20and%0Ageneralizes%20effectively%20across%20varying%20geometries.%20Despite%20the%20scarcity%20of%0Atraining%20data%2C%20Fusion%20DeepONet%20achieves%20performance%20comparable%20to%0Aparameter-conditioned%20U-Net%20on%20uniform%20grids%20while%20it%20outperforms%20MeshGraphNet%0Aand%20vanilla%20DeepONet%20on%20irregular%2C%20arbitrary%20grids.%20Fusion%20DeepONet%20requires%0Asignificantly%20fewer%20trainable%20parameters%20as%20compared%20to%20U-Net%2C%20MeshGraphNet%2C%0Aand%20FNO%2C%20making%20it%20computationally%20efficient.%20We%20also%20analyze%20the%20basis%0Afunctions%20of%20the%20Fusion%20DeepONet%20model%20using%20Singular%20Value%20Decomposition.%20This%0Aanalysis%20reveals%20that%20Fusion%20DeepONet%20generalizes%20effectively%20to%20unseen%0Asolutions%20and%20adapts%20to%20varying%20geometries%20and%20grid%20points%2C%20demonstrating%20its%0Arobustness%20in%20scenarios%20with%20limited%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01934v1&entry.124074799=Read"},
{"title": "Knowledge Circuits in Pretrained Transformers", "author": "Yunzhi Yao and Ningyu Zhang and Zekun Xi and Mengru Wang and Ziwen Xu and Shumin Deng and Huajun Chen", "abstract": "  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n", "link": "http://arxiv.org/abs/2405.17969v4", "date": "2025-01-03", "relevancy": 2.0588, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Circuits%20in%20Pretrained%20Transformers&body=Title%3A%20Knowledge%20Circuits%20in%20Pretrained%20Transformers%0AAuthor%3A%20Yunzhi%20Yao%20and%20Ningyu%20Zhang%20and%20Zekun%20Xi%20and%20Mengru%20Wang%20and%20Ziwen%20Xu%20and%20Shumin%20Deng%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20The%20remarkable%20capabilities%20of%20modern%20large%20language%20models%20are%20rooted%20in%0Atheir%20vast%20repositories%20of%20knowledge%20encoded%20within%20their%20parameters%2C%20enabling%0Athem%20to%20perceive%20the%20world%20and%20engage%20in%20reasoning.%20The%20inner%20workings%20of%20how%0Athese%20models%20store%20knowledge%20have%20long%20been%20a%20subject%20of%20intense%20interest%20and%0Ainvestigation%20among%20researchers.%20To%20date%2C%20most%20studies%20have%20concentrated%20on%0Aisolated%20components%20within%20these%20models%2C%20such%20as%20the%20Multilayer%20Perceptrons%20and%0Aattention%20head.%20In%20this%20paper%2C%20we%20delve%20into%20the%20computation%20graph%20of%20the%0Alanguage%20model%20to%20uncover%20the%20knowledge%20circuits%20that%20are%20instrumental%20in%0Aarticulating%20specific%20knowledge.%20The%20experiments%2C%20conducted%20with%20GPT2%20and%0ATinyLLAMA%2C%20have%20allowed%20us%20to%20observe%20how%20certain%20information%20heads%2C%20relation%0Aheads%2C%20and%20Multilayer%20Perceptrons%20collaboratively%20encode%20knowledge%20within%20the%0Amodel.%20Moreover%2C%20we%20evaluate%20the%20impact%20of%20current%20knowledge%20editing%20techniques%0Aon%20these%20knowledge%20circuits%2C%20providing%20deeper%20insights%20into%20the%20functioning%20and%0Aconstraints%20of%20these%20editing%20methodologies.%20Finally%2C%20we%20utilize%20knowledge%0Acircuits%20to%20analyze%20and%20interpret%20language%20model%20behaviors%20such%20as%0Ahallucinations%20and%20in-context%20learning.%20We%20believe%20the%20knowledge%20circuits%20hold%0Apotential%20for%20advancing%20our%20understanding%20of%20Transformers%20and%20guiding%20the%0Aimproved%20design%20of%20knowledge%20editing.%20Code%20and%20data%20are%20available%20in%0Ahttps%3A//github.com/zjunlp/KnowledgeCircuits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17969v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Circuits%2520in%2520Pretrained%2520Transformers%26entry.906535625%3DYunzhi%2520Yao%2520and%2520Ningyu%2520Zhang%2520and%2520Zekun%2520Xi%2520and%2520Mengru%2520Wang%2520and%2520Ziwen%2520Xu%2520and%2520Shumin%2520Deng%2520and%2520Huajun%2520Chen%26entry.1292438233%3D%2520%2520The%2520remarkable%2520capabilities%2520of%2520modern%2520large%2520language%2520models%2520are%2520rooted%2520in%250Atheir%2520vast%2520repositories%2520of%2520knowledge%2520encoded%2520within%2520their%2520parameters%252C%2520enabling%250Athem%2520to%2520perceive%2520the%2520world%2520and%2520engage%2520in%2520reasoning.%2520The%2520inner%2520workings%2520of%2520how%250Athese%2520models%2520store%2520knowledge%2520have%2520long%2520been%2520a%2520subject%2520of%2520intense%2520interest%2520and%250Ainvestigation%2520among%2520researchers.%2520To%2520date%252C%2520most%2520studies%2520have%2520concentrated%2520on%250Aisolated%2520components%2520within%2520these%2520models%252C%2520such%2520as%2520the%2520Multilayer%2520Perceptrons%2520and%250Aattention%2520head.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520the%2520computation%2520graph%2520of%2520the%250Alanguage%2520model%2520to%2520uncover%2520the%2520knowledge%2520circuits%2520that%2520are%2520instrumental%2520in%250Aarticulating%2520specific%2520knowledge.%2520The%2520experiments%252C%2520conducted%2520with%2520GPT2%2520and%250ATinyLLAMA%252C%2520have%2520allowed%2520us%2520to%2520observe%2520how%2520certain%2520information%2520heads%252C%2520relation%250Aheads%252C%2520and%2520Multilayer%2520Perceptrons%2520collaboratively%2520encode%2520knowledge%2520within%2520the%250Amodel.%2520Moreover%252C%2520we%2520evaluate%2520the%2520impact%2520of%2520current%2520knowledge%2520editing%2520techniques%250Aon%2520these%2520knowledge%2520circuits%252C%2520providing%2520deeper%2520insights%2520into%2520the%2520functioning%2520and%250Aconstraints%2520of%2520these%2520editing%2520methodologies.%2520Finally%252C%2520we%2520utilize%2520knowledge%250Acircuits%2520to%2520analyze%2520and%2520interpret%2520language%2520model%2520behaviors%2520such%2520as%250Ahallucinations%2520and%2520in-context%2520learning.%2520We%2520believe%2520the%2520knowledge%2520circuits%2520hold%250Apotential%2520for%2520advancing%2520our%2520understanding%2520of%2520Transformers%2520and%2520guiding%2520the%250Aimproved%2520design%2520of%2520knowledge%2520editing.%2520Code%2520and%2520data%2520are%2520available%2520in%250Ahttps%253A//github.com/zjunlp/KnowledgeCircuits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17969v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Circuits%20in%20Pretrained%20Transformers&entry.906535625=Yunzhi%20Yao%20and%20Ningyu%20Zhang%20and%20Zekun%20Xi%20and%20Mengru%20Wang%20and%20Ziwen%20Xu%20and%20Shumin%20Deng%20and%20Huajun%20Chen&entry.1292438233=%20%20The%20remarkable%20capabilities%20of%20modern%20large%20language%20models%20are%20rooted%20in%0Atheir%20vast%20repositories%20of%20knowledge%20encoded%20within%20their%20parameters%2C%20enabling%0Athem%20to%20perceive%20the%20world%20and%20engage%20in%20reasoning.%20The%20inner%20workings%20of%20how%0Athese%20models%20store%20knowledge%20have%20long%20been%20a%20subject%20of%20intense%20interest%20and%0Ainvestigation%20among%20researchers.%20To%20date%2C%20most%20studies%20have%20concentrated%20on%0Aisolated%20components%20within%20these%20models%2C%20such%20as%20the%20Multilayer%20Perceptrons%20and%0Aattention%20head.%20In%20this%20paper%2C%20we%20delve%20into%20the%20computation%20graph%20of%20the%0Alanguage%20model%20to%20uncover%20the%20knowledge%20circuits%20that%20are%20instrumental%20in%0Aarticulating%20specific%20knowledge.%20The%20experiments%2C%20conducted%20with%20GPT2%20and%0ATinyLLAMA%2C%20have%20allowed%20us%20to%20observe%20how%20certain%20information%20heads%2C%20relation%0Aheads%2C%20and%20Multilayer%20Perceptrons%20collaboratively%20encode%20knowledge%20within%20the%0Amodel.%20Moreover%2C%20we%20evaluate%20the%20impact%20of%20current%20knowledge%20editing%20techniques%0Aon%20these%20knowledge%20circuits%2C%20providing%20deeper%20insights%20into%20the%20functioning%20and%0Aconstraints%20of%20these%20editing%20methodologies.%20Finally%2C%20we%20utilize%20knowledge%0Acircuits%20to%20analyze%20and%20interpret%20language%20model%20behaviors%20such%20as%0Ahallucinations%20and%20in-context%20learning.%20We%20believe%20the%20knowledge%20circuits%20hold%0Apotential%20for%20advancing%20our%20understanding%20of%20Transformers%20and%20guiding%20the%0Aimproved%20design%20of%20knowledge%20editing.%20Code%20and%20data%20are%20available%20in%0Ahttps%3A//github.com/zjunlp/KnowledgeCircuits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17969v4&entry.124074799=Read"},
{"title": "DFF: Decision-Focused Fine-tuning for Smarter Predict-then-Optimize with\n  Limited Data", "author": "Jiaqi Yang and Enming Liang and Zicheng Su and Zhichao Zou and Peng Zhen and Jiecheng Guo and Wanjing Ma and Kun An", "abstract": "  Decision-focused learning (DFL) offers an end-to-end approach to the\npredict-then-optimize (PO) framework by training predictive models directly on\ndecision loss (DL), enhancing decision-making performance within PO contexts.\nHowever, the implementation of DFL poses distinct challenges. Primarily, DL can\nresult in deviation from the physical significance of the predictions under\nlimited data. Additionally, some predictive models are non-differentiable or\nblack-box, which cannot be adjusted using gradient-based methods. To tackle the\nabove challenges, we propose a novel framework, Decision-Focused Fine-tuning\n(DFF), which embeds the DFL module into the PO pipeline via a novel bias\ncorrection module. DFF is formulated as a constrained optimization problem that\nmaintains the proximity of the DL-enhanced model to the original predictive\nmodel within a defined trust region. We theoretically prove that DFF strictly\nconfines prediction bias within a predetermined upper bound, even with limited\ndatasets, thereby substantially reducing prediction shifts caused by DL under\nlimited data. Furthermore, the bias correction module can be integrated into\ndiverse predictive models, enhancing adaptability to a broad range of PO tasks.\nExtensive evaluations on synthetic and real-world datasets, including network\nflow, portfolio optimization, and resource allocation problems with different\npredictive models, demonstrate that DFF not only improves decision performance\nbut also adheres to fine-tuning constraints, showcasing robust adaptability\nacross various scenarios.\n", "link": "http://arxiv.org/abs/2501.01874v1", "date": "2025-01-03", "relevancy": 2.0577, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5396}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5005}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DFF%3A%20Decision-Focused%20Fine-tuning%20for%20Smarter%20Predict-then-Optimize%20with%0A%20%20Limited%20Data&body=Title%3A%20DFF%3A%20Decision-Focused%20Fine-tuning%20for%20Smarter%20Predict-then-Optimize%20with%0A%20%20Limited%20Data%0AAuthor%3A%20Jiaqi%20Yang%20and%20Enming%20Liang%20and%20Zicheng%20Su%20and%20Zhichao%20Zou%20and%20Peng%20Zhen%20and%20Jiecheng%20Guo%20and%20Wanjing%20Ma%20and%20Kun%20An%0AAbstract%3A%20%20%20Decision-focused%20learning%20%28DFL%29%20offers%20an%20end-to-end%20approach%20to%20the%0Apredict-then-optimize%20%28PO%29%20framework%20by%20training%20predictive%20models%20directly%20on%0Adecision%20loss%20%28DL%29%2C%20enhancing%20decision-making%20performance%20within%20PO%20contexts.%0AHowever%2C%20the%20implementation%20of%20DFL%20poses%20distinct%20challenges.%20Primarily%2C%20DL%20can%0Aresult%20in%20deviation%20from%20the%20physical%20significance%20of%20the%20predictions%20under%0Alimited%20data.%20Additionally%2C%20some%20predictive%20models%20are%20non-differentiable%20or%0Ablack-box%2C%20which%20cannot%20be%20adjusted%20using%20gradient-based%20methods.%20To%20tackle%20the%0Aabove%20challenges%2C%20we%20propose%20a%20novel%20framework%2C%20Decision-Focused%20Fine-tuning%0A%28DFF%29%2C%20which%20embeds%20the%20DFL%20module%20into%20the%20PO%20pipeline%20via%20a%20novel%20bias%0Acorrection%20module.%20DFF%20is%20formulated%20as%20a%20constrained%20optimization%20problem%20that%0Amaintains%20the%20proximity%20of%20the%20DL-enhanced%20model%20to%20the%20original%20predictive%0Amodel%20within%20a%20defined%20trust%20region.%20We%20theoretically%20prove%20that%20DFF%20strictly%0Aconfines%20prediction%20bias%20within%20a%20predetermined%20upper%20bound%2C%20even%20with%20limited%0Adatasets%2C%20thereby%20substantially%20reducing%20prediction%20shifts%20caused%20by%20DL%20under%0Alimited%20data.%20Furthermore%2C%20the%20bias%20correction%20module%20can%20be%20integrated%20into%0Adiverse%20predictive%20models%2C%20enhancing%20adaptability%20to%20a%20broad%20range%20of%20PO%20tasks.%0AExtensive%20evaluations%20on%20synthetic%20and%20real-world%20datasets%2C%20including%20network%0Aflow%2C%20portfolio%20optimization%2C%20and%20resource%20allocation%20problems%20with%20different%0Apredictive%20models%2C%20demonstrate%20that%20DFF%20not%20only%20improves%20decision%20performance%0Abut%20also%20adheres%20to%20fine-tuning%20constraints%2C%20showcasing%20robust%20adaptability%0Aacross%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDFF%253A%2520Decision-Focused%2520Fine-tuning%2520for%2520Smarter%2520Predict-then-Optimize%2520with%250A%2520%2520Limited%2520Data%26entry.906535625%3DJiaqi%2520Yang%2520and%2520Enming%2520Liang%2520and%2520Zicheng%2520Su%2520and%2520Zhichao%2520Zou%2520and%2520Peng%2520Zhen%2520and%2520Jiecheng%2520Guo%2520and%2520Wanjing%2520Ma%2520and%2520Kun%2520An%26entry.1292438233%3D%2520%2520Decision-focused%2520learning%2520%2528DFL%2529%2520offers%2520an%2520end-to-end%2520approach%2520to%2520the%250Apredict-then-optimize%2520%2528PO%2529%2520framework%2520by%2520training%2520predictive%2520models%2520directly%2520on%250Adecision%2520loss%2520%2528DL%2529%252C%2520enhancing%2520decision-making%2520performance%2520within%2520PO%2520contexts.%250AHowever%252C%2520the%2520implementation%2520of%2520DFL%2520poses%2520distinct%2520challenges.%2520Primarily%252C%2520DL%2520can%250Aresult%2520in%2520deviation%2520from%2520the%2520physical%2520significance%2520of%2520the%2520predictions%2520under%250Alimited%2520data.%2520Additionally%252C%2520some%2520predictive%2520models%2520are%2520non-differentiable%2520or%250Ablack-box%252C%2520which%2520cannot%2520be%2520adjusted%2520using%2520gradient-based%2520methods.%2520To%2520tackle%2520the%250Aabove%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520Decision-Focused%2520Fine-tuning%250A%2528DFF%2529%252C%2520which%2520embeds%2520the%2520DFL%2520module%2520into%2520the%2520PO%2520pipeline%2520via%2520a%2520novel%2520bias%250Acorrection%2520module.%2520DFF%2520is%2520formulated%2520as%2520a%2520constrained%2520optimization%2520problem%2520that%250Amaintains%2520the%2520proximity%2520of%2520the%2520DL-enhanced%2520model%2520to%2520the%2520original%2520predictive%250Amodel%2520within%2520a%2520defined%2520trust%2520region.%2520We%2520theoretically%2520prove%2520that%2520DFF%2520strictly%250Aconfines%2520prediction%2520bias%2520within%2520a%2520predetermined%2520upper%2520bound%252C%2520even%2520with%2520limited%250Adatasets%252C%2520thereby%2520substantially%2520reducing%2520prediction%2520shifts%2520caused%2520by%2520DL%2520under%250Alimited%2520data.%2520Furthermore%252C%2520the%2520bias%2520correction%2520module%2520can%2520be%2520integrated%2520into%250Adiverse%2520predictive%2520models%252C%2520enhancing%2520adaptability%2520to%2520a%2520broad%2520range%2520of%2520PO%2520tasks.%250AExtensive%2520evaluations%2520on%2520synthetic%2520and%2520real-world%2520datasets%252C%2520including%2520network%250Aflow%252C%2520portfolio%2520optimization%252C%2520and%2520resource%2520allocation%2520problems%2520with%2520different%250Apredictive%2520models%252C%2520demonstrate%2520that%2520DFF%2520not%2520only%2520improves%2520decision%2520performance%250Abut%2520also%2520adheres%2520to%2520fine-tuning%2520constraints%252C%2520showcasing%2520robust%2520adaptability%250Aacross%2520various%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DFF%3A%20Decision-Focused%20Fine-tuning%20for%20Smarter%20Predict-then-Optimize%20with%0A%20%20Limited%20Data&entry.906535625=Jiaqi%20Yang%20and%20Enming%20Liang%20and%20Zicheng%20Su%20and%20Zhichao%20Zou%20and%20Peng%20Zhen%20and%20Jiecheng%20Guo%20and%20Wanjing%20Ma%20and%20Kun%20An&entry.1292438233=%20%20Decision-focused%20learning%20%28DFL%29%20offers%20an%20end-to-end%20approach%20to%20the%0Apredict-then-optimize%20%28PO%29%20framework%20by%20training%20predictive%20models%20directly%20on%0Adecision%20loss%20%28DL%29%2C%20enhancing%20decision-making%20performance%20within%20PO%20contexts.%0AHowever%2C%20the%20implementation%20of%20DFL%20poses%20distinct%20challenges.%20Primarily%2C%20DL%20can%0Aresult%20in%20deviation%20from%20the%20physical%20significance%20of%20the%20predictions%20under%0Alimited%20data.%20Additionally%2C%20some%20predictive%20models%20are%20non-differentiable%20or%0Ablack-box%2C%20which%20cannot%20be%20adjusted%20using%20gradient-based%20methods.%20To%20tackle%20the%0Aabove%20challenges%2C%20we%20propose%20a%20novel%20framework%2C%20Decision-Focused%20Fine-tuning%0A%28DFF%29%2C%20which%20embeds%20the%20DFL%20module%20into%20the%20PO%20pipeline%20via%20a%20novel%20bias%0Acorrection%20module.%20DFF%20is%20formulated%20as%20a%20constrained%20optimization%20problem%20that%0Amaintains%20the%20proximity%20of%20the%20DL-enhanced%20model%20to%20the%20original%20predictive%0Amodel%20within%20a%20defined%20trust%20region.%20We%20theoretically%20prove%20that%20DFF%20strictly%0Aconfines%20prediction%20bias%20within%20a%20predetermined%20upper%20bound%2C%20even%20with%20limited%0Adatasets%2C%20thereby%20substantially%20reducing%20prediction%20shifts%20caused%20by%20DL%20under%0Alimited%20data.%20Furthermore%2C%20the%20bias%20correction%20module%20can%20be%20integrated%20into%0Adiverse%20predictive%20models%2C%20enhancing%20adaptability%20to%20a%20broad%20range%20of%20PO%20tasks.%0AExtensive%20evaluations%20on%20synthetic%20and%20real-world%20datasets%2C%20including%20network%0Aflow%2C%20portfolio%20optimization%2C%20and%20resource%20allocation%20problems%20with%20different%0Apredictive%20models%2C%20demonstrate%20that%20DFF%20not%20only%20improves%20decision%20performance%0Abut%20also%20adheres%20to%20fine-tuning%20constraints%2C%20showcasing%20robust%20adaptability%0Aacross%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01874v1&entry.124074799=Read"},
{"title": "TabTreeFormer: Tabular Data Generation Using Hybrid Tree-Transformer", "author": "Jiayu Li and Bingyin Zhao and Zilong Zhao and Kevin Yee and Uzair Javaid and Yingjie Lao and Biplab Sikdar", "abstract": "  Transformers have achieved remarkable success in tabular data generation.\nHowever, they lack domain-specific inductive biases which are critical to\npreserving the intrinsic characteristics of tabular data. Meanwhile, they\nsuffer from poor scalability and efficiency due to quadratic computational\ncomplexity. In this paper, we propose TabTreeFormer, a hybrid transformer\narchitecture that incorporates a tree-based model that retains tabular-specific\ninductive biases of non-smooth and potentially low-correlated patterns due to\nits discreteness and non-rotational invariance, and hence enhances the fidelity\nand utility of synthetic data. In addition, we devise a dual-quantization\ntokenizer to capture the multimodal continuous distribution and further\nfacilitate the learning of numerical value distribution. Moreover, our proposed\ntokenizer reduces the vocabulary size and sequence length due to the limited\ndimension-wise semantic meaning and training set size of tabular data,\nrendering a significant model size shrink without sacrificing the capability of\nthe transformer model. We evaluate TabTreeFormer on 10 datasets against\nmultiple generative models on various metrics; our experimental results show\nthat TabTreeFormer achieves superior fidelity, utility, privacy, and\nefficiency. Our best model yields a 40% utility improvement with 1/16 of the\nbaseline model size.\n", "link": "http://arxiv.org/abs/2501.01216v2", "date": "2025-01-03", "relevancy": 2.056, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5917}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5137}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabTreeFormer%3A%20Tabular%20Data%20Generation%20Using%20Hybrid%20Tree-Transformer&body=Title%3A%20TabTreeFormer%3A%20Tabular%20Data%20Generation%20Using%20Hybrid%20Tree-Transformer%0AAuthor%3A%20Jiayu%20Li%20and%20Bingyin%20Zhao%20and%20Zilong%20Zhao%20and%20Kevin%20Yee%20and%20Uzair%20Javaid%20and%20Yingjie%20Lao%20and%20Biplab%20Sikdar%0AAbstract%3A%20%20%20Transformers%20have%20achieved%20remarkable%20success%20in%20tabular%20data%20generation.%0AHowever%2C%20they%20lack%20domain-specific%20inductive%20biases%20which%20are%20critical%20to%0Apreserving%20the%20intrinsic%20characteristics%20of%20tabular%20data.%20Meanwhile%2C%20they%0Asuffer%20from%20poor%20scalability%20and%20efficiency%20due%20to%20quadratic%20computational%0Acomplexity.%20In%20this%20paper%2C%20we%20propose%20TabTreeFormer%2C%20a%20hybrid%20transformer%0Aarchitecture%20that%20incorporates%20a%20tree-based%20model%20that%20retains%20tabular-specific%0Ainductive%20biases%20of%20non-smooth%20and%20potentially%20low-correlated%20patterns%20due%20to%0Aits%20discreteness%20and%20non-rotational%20invariance%2C%20and%20hence%20enhances%20the%20fidelity%0Aand%20utility%20of%20synthetic%20data.%20In%20addition%2C%20we%20devise%20a%20dual-quantization%0Atokenizer%20to%20capture%20the%20multimodal%20continuous%20distribution%20and%20further%0Afacilitate%20the%20learning%20of%20numerical%20value%20distribution.%20Moreover%2C%20our%20proposed%0Atokenizer%20reduces%20the%20vocabulary%20size%20and%20sequence%20length%20due%20to%20the%20limited%0Adimension-wise%20semantic%20meaning%20and%20training%20set%20size%20of%20tabular%20data%2C%0Arendering%20a%20significant%20model%20size%20shrink%20without%20sacrificing%20the%20capability%20of%0Athe%20transformer%20model.%20We%20evaluate%20TabTreeFormer%20on%2010%20datasets%20against%0Amultiple%20generative%20models%20on%20various%20metrics%3B%20our%20experimental%20results%20show%0Athat%20TabTreeFormer%20achieves%20superior%20fidelity%2C%20utility%2C%20privacy%2C%20and%0Aefficiency.%20Our%20best%20model%20yields%20a%2040%25%20utility%20improvement%20with%201/16%20of%20the%0Abaseline%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabTreeFormer%253A%2520Tabular%2520Data%2520Generation%2520Using%2520Hybrid%2520Tree-Transformer%26entry.906535625%3DJiayu%2520Li%2520and%2520Bingyin%2520Zhao%2520and%2520Zilong%2520Zhao%2520and%2520Kevin%2520Yee%2520and%2520Uzair%2520Javaid%2520and%2520Yingjie%2520Lao%2520and%2520Biplab%2520Sikdar%26entry.1292438233%3D%2520%2520Transformers%2520have%2520achieved%2520remarkable%2520success%2520in%2520tabular%2520data%2520generation.%250AHowever%252C%2520they%2520lack%2520domain-specific%2520inductive%2520biases%2520which%2520are%2520critical%2520to%250Apreserving%2520the%2520intrinsic%2520characteristics%2520of%2520tabular%2520data.%2520Meanwhile%252C%2520they%250Asuffer%2520from%2520poor%2520scalability%2520and%2520efficiency%2520due%2520to%2520quadratic%2520computational%250Acomplexity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TabTreeFormer%252C%2520a%2520hybrid%2520transformer%250Aarchitecture%2520that%2520incorporates%2520a%2520tree-based%2520model%2520that%2520retains%2520tabular-specific%250Ainductive%2520biases%2520of%2520non-smooth%2520and%2520potentially%2520low-correlated%2520patterns%2520due%2520to%250Aits%2520discreteness%2520and%2520non-rotational%2520invariance%252C%2520and%2520hence%2520enhances%2520the%2520fidelity%250Aand%2520utility%2520of%2520synthetic%2520data.%2520In%2520addition%252C%2520we%2520devise%2520a%2520dual-quantization%250Atokenizer%2520to%2520capture%2520the%2520multimodal%2520continuous%2520distribution%2520and%2520further%250Afacilitate%2520the%2520learning%2520of%2520numerical%2520value%2520distribution.%2520Moreover%252C%2520our%2520proposed%250Atokenizer%2520reduces%2520the%2520vocabulary%2520size%2520and%2520sequence%2520length%2520due%2520to%2520the%2520limited%250Adimension-wise%2520semantic%2520meaning%2520and%2520training%2520set%2520size%2520of%2520tabular%2520data%252C%250Arendering%2520a%2520significant%2520model%2520size%2520shrink%2520without%2520sacrificing%2520the%2520capability%2520of%250Athe%2520transformer%2520model.%2520We%2520evaluate%2520TabTreeFormer%2520on%252010%2520datasets%2520against%250Amultiple%2520generative%2520models%2520on%2520various%2520metrics%253B%2520our%2520experimental%2520results%2520show%250Athat%2520TabTreeFormer%2520achieves%2520superior%2520fidelity%252C%2520utility%252C%2520privacy%252C%2520and%250Aefficiency.%2520Our%2520best%2520model%2520yields%2520a%252040%2525%2520utility%2520improvement%2520with%25201/16%2520of%2520the%250Abaseline%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabTreeFormer%3A%20Tabular%20Data%20Generation%20Using%20Hybrid%20Tree-Transformer&entry.906535625=Jiayu%20Li%20and%20Bingyin%20Zhao%20and%20Zilong%20Zhao%20and%20Kevin%20Yee%20and%20Uzair%20Javaid%20and%20Yingjie%20Lao%20and%20Biplab%20Sikdar&entry.1292438233=%20%20Transformers%20have%20achieved%20remarkable%20success%20in%20tabular%20data%20generation.%0AHowever%2C%20they%20lack%20domain-specific%20inductive%20biases%20which%20are%20critical%20to%0Apreserving%20the%20intrinsic%20characteristics%20of%20tabular%20data.%20Meanwhile%2C%20they%0Asuffer%20from%20poor%20scalability%20and%20efficiency%20due%20to%20quadratic%20computational%0Acomplexity.%20In%20this%20paper%2C%20we%20propose%20TabTreeFormer%2C%20a%20hybrid%20transformer%0Aarchitecture%20that%20incorporates%20a%20tree-based%20model%20that%20retains%20tabular-specific%0Ainductive%20biases%20of%20non-smooth%20and%20potentially%20low-correlated%20patterns%20due%20to%0Aits%20discreteness%20and%20non-rotational%20invariance%2C%20and%20hence%20enhances%20the%20fidelity%0Aand%20utility%20of%20synthetic%20data.%20In%20addition%2C%20we%20devise%20a%20dual-quantization%0Atokenizer%20to%20capture%20the%20multimodal%20continuous%20distribution%20and%20further%0Afacilitate%20the%20learning%20of%20numerical%20value%20distribution.%20Moreover%2C%20our%20proposed%0Atokenizer%20reduces%20the%20vocabulary%20size%20and%20sequence%20length%20due%20to%20the%20limited%0Adimension-wise%20semantic%20meaning%20and%20training%20set%20size%20of%20tabular%20data%2C%0Arendering%20a%20significant%20model%20size%20shrink%20without%20sacrificing%20the%20capability%20of%0Athe%20transformer%20model.%20We%20evaluate%20TabTreeFormer%20on%2010%20datasets%20against%0Amultiple%20generative%20models%20on%20various%20metrics%3B%20our%20experimental%20results%20show%0Athat%20TabTreeFormer%20achieves%20superior%20fidelity%2C%20utility%2C%20privacy%2C%20and%0Aefficiency.%20Our%20best%20model%20yields%20a%2040%25%20utility%20improvement%20with%201/16%20of%20the%0Abaseline%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01216v2&entry.124074799=Read"},
{"title": "DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring", "author": "Mahmut S. Gokmen and Caner Ozcan and Moneera N. Haque and Steve W. Leung and C. Seth Parker and W. Brent Seales and Cody Bumgardner", "abstract": "  Coronary artery disease (CAD), one of the leading causes of mortality\nworldwide, necessitates effective risk assessment strategies, with coronary\nartery calcium (CAC) scoring via computed tomography (CT) being a key method\nfor prevention. Traditional methods, primarily based on UNET architectures\nimplemented on pre-built models, face challenges like the scarcity of annotated\nCT scans containing CAC and imbalanced datasets, leading to reduced performance\nin segmentation and scoring tasks. In this study, we address these limitations\nby incorporating the self-supervised learning (SSL) technique of DINO\n(self-distillation with no labels), which trains without requiring CAC-specific\nannotations, enhancing its robustness in generating distinct features. The\nDINO-LG model, which leverages label guidance to focus on calcified areas,\nachieves significant improvements, with a sensitivity of 89% and specificity of\n90% for detecting CAC-containing CT slices, compared to the standard DINO\nmodel's sensitivity of 79% and specificity of 77%. Additionally, false-negative\nand false-positive rates are reduced by 49% and 59%, respectively, instilling\ngreater confidence in clinicians when ruling out calcification in low-risk\npatients and minimizing unnecessary imaging reviews by radiologists. Further,\nCAC scoring and segmentation tasks are conducted using a basic UNET\narchitecture, applied specifically to CT slices identified by the DINO-LG model\nas containing calcified areas. This targeted approach enhances CAC scoring\naccuracy by feeding the UNET model with relevant slices, significantly\nimproving diagnostic precision, reducing both false positives and false\nnegatives, and ultimately lowering overall healthcare costs by minimizing\nunnecessary tests and treatments, presenting a valuable advancement in CAD risk\nassessment.\n", "link": "http://arxiv.org/abs/2411.07976v6", "date": "2025-01-03", "relevancy": 2.0543, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5428}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5077}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-LG%3A%20A%20Task-Specific%20DINO%20Model%20for%20Coronary%20Calcium%20Scoring&body=Title%3A%20DINO-LG%3A%20A%20Task-Specific%20DINO%20Model%20for%20Coronary%20Calcium%20Scoring%0AAuthor%3A%20Mahmut%20S.%20Gokmen%20and%20Caner%20Ozcan%20and%20Moneera%20N.%20Haque%20and%20Steve%20W.%20Leung%20and%20C.%20Seth%20Parker%20and%20W.%20Brent%20Seales%20and%20Cody%20Bumgardner%0AAbstract%3A%20%20%20Coronary%20artery%20disease%20%28CAD%29%2C%20one%20of%20the%20leading%20causes%20of%20mortality%0Aworldwide%2C%20necessitates%20effective%20risk%20assessment%20strategies%2C%20with%20coronary%0Aartery%20calcium%20%28CAC%29%20scoring%20via%20computed%20tomography%20%28CT%29%20being%20a%20key%20method%0Afor%20prevention.%20Traditional%20methods%2C%20primarily%20based%20on%20UNET%20architectures%0Aimplemented%20on%20pre-built%20models%2C%20face%20challenges%20like%20the%20scarcity%20of%20annotated%0ACT%20scans%20containing%20CAC%20and%20imbalanced%20datasets%2C%20leading%20to%20reduced%20performance%0Ain%20segmentation%20and%20scoring%20tasks.%20In%20this%20study%2C%20we%20address%20these%20limitations%0Aby%20incorporating%20the%20self-supervised%20learning%20%28SSL%29%20technique%20of%20DINO%0A%28self-distillation%20with%20no%20labels%29%2C%20which%20trains%20without%20requiring%20CAC-specific%0Aannotations%2C%20enhancing%20its%20robustness%20in%20generating%20distinct%20features.%20The%0ADINO-LG%20model%2C%20which%20leverages%20label%20guidance%20to%20focus%20on%20calcified%20areas%2C%0Aachieves%20significant%20improvements%2C%20with%20a%20sensitivity%20of%2089%25%20and%20specificity%20of%0A90%25%20for%20detecting%20CAC-containing%20CT%20slices%2C%20compared%20to%20the%20standard%20DINO%0Amodel%27s%20sensitivity%20of%2079%25%20and%20specificity%20of%2077%25.%20Additionally%2C%20false-negative%0Aand%20false-positive%20rates%20are%20reduced%20by%2049%25%20and%2059%25%2C%20respectively%2C%20instilling%0Agreater%20confidence%20in%20clinicians%20when%20ruling%20out%20calcification%20in%20low-risk%0Apatients%20and%20minimizing%20unnecessary%20imaging%20reviews%20by%20radiologists.%20Further%2C%0ACAC%20scoring%20and%20segmentation%20tasks%20are%20conducted%20using%20a%20basic%20UNET%0Aarchitecture%2C%20applied%20specifically%20to%20CT%20slices%20identified%20by%20the%20DINO-LG%20model%0Aas%20containing%20calcified%20areas.%20This%20targeted%20approach%20enhances%20CAC%20scoring%0Aaccuracy%20by%20feeding%20the%20UNET%20model%20with%20relevant%20slices%2C%20significantly%0Aimproving%20diagnostic%20precision%2C%20reducing%20both%20false%20positives%20and%20false%0Anegatives%2C%20and%20ultimately%20lowering%20overall%20healthcare%20costs%20by%20minimizing%0Aunnecessary%20tests%20and%20treatments%2C%20presenting%20a%20valuable%20advancement%20in%20CAD%20risk%0Aassessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07976v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-LG%253A%2520A%2520Task-Specific%2520DINO%2520Model%2520for%2520Coronary%2520Calcium%2520Scoring%26entry.906535625%3DMahmut%2520S.%2520Gokmen%2520and%2520Caner%2520Ozcan%2520and%2520Moneera%2520N.%2520Haque%2520and%2520Steve%2520W.%2520Leung%2520and%2520C.%2520Seth%2520Parker%2520and%2520W.%2520Brent%2520Seales%2520and%2520Cody%2520Bumgardner%26entry.1292438233%3D%2520%2520Coronary%2520artery%2520disease%2520%2528CAD%2529%252C%2520one%2520of%2520the%2520leading%2520causes%2520of%2520mortality%250Aworldwide%252C%2520necessitates%2520effective%2520risk%2520assessment%2520strategies%252C%2520with%2520coronary%250Aartery%2520calcium%2520%2528CAC%2529%2520scoring%2520via%2520computed%2520tomography%2520%2528CT%2529%2520being%2520a%2520key%2520method%250Afor%2520prevention.%2520Traditional%2520methods%252C%2520primarily%2520based%2520on%2520UNET%2520architectures%250Aimplemented%2520on%2520pre-built%2520models%252C%2520face%2520challenges%2520like%2520the%2520scarcity%2520of%2520annotated%250ACT%2520scans%2520containing%2520CAC%2520and%2520imbalanced%2520datasets%252C%2520leading%2520to%2520reduced%2520performance%250Ain%2520segmentation%2520and%2520scoring%2520tasks.%2520In%2520this%2520study%252C%2520we%2520address%2520these%2520limitations%250Aby%2520incorporating%2520the%2520self-supervised%2520learning%2520%2528SSL%2529%2520technique%2520of%2520DINO%250A%2528self-distillation%2520with%2520no%2520labels%2529%252C%2520which%2520trains%2520without%2520requiring%2520CAC-specific%250Aannotations%252C%2520enhancing%2520its%2520robustness%2520in%2520generating%2520distinct%2520features.%2520The%250ADINO-LG%2520model%252C%2520which%2520leverages%2520label%2520guidance%2520to%2520focus%2520on%2520calcified%2520areas%252C%250Aachieves%2520significant%2520improvements%252C%2520with%2520a%2520sensitivity%2520of%252089%2525%2520and%2520specificity%2520of%250A90%2525%2520for%2520detecting%2520CAC-containing%2520CT%2520slices%252C%2520compared%2520to%2520the%2520standard%2520DINO%250Amodel%2527s%2520sensitivity%2520of%252079%2525%2520and%2520specificity%2520of%252077%2525.%2520Additionally%252C%2520false-negative%250Aand%2520false-positive%2520rates%2520are%2520reduced%2520by%252049%2525%2520and%252059%2525%252C%2520respectively%252C%2520instilling%250Agreater%2520confidence%2520in%2520clinicians%2520when%2520ruling%2520out%2520calcification%2520in%2520low-risk%250Apatients%2520and%2520minimizing%2520unnecessary%2520imaging%2520reviews%2520by%2520radiologists.%2520Further%252C%250ACAC%2520scoring%2520and%2520segmentation%2520tasks%2520are%2520conducted%2520using%2520a%2520basic%2520UNET%250Aarchitecture%252C%2520applied%2520specifically%2520to%2520CT%2520slices%2520identified%2520by%2520the%2520DINO-LG%2520model%250Aas%2520containing%2520calcified%2520areas.%2520This%2520targeted%2520approach%2520enhances%2520CAC%2520scoring%250Aaccuracy%2520by%2520feeding%2520the%2520UNET%2520model%2520with%2520relevant%2520slices%252C%2520significantly%250Aimproving%2520diagnostic%2520precision%252C%2520reducing%2520both%2520false%2520positives%2520and%2520false%250Anegatives%252C%2520and%2520ultimately%2520lowering%2520overall%2520healthcare%2520costs%2520by%2520minimizing%250Aunnecessary%2520tests%2520and%2520treatments%252C%2520presenting%2520a%2520valuable%2520advancement%2520in%2520CAD%2520risk%250Aassessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07976v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-LG%3A%20A%20Task-Specific%20DINO%20Model%20for%20Coronary%20Calcium%20Scoring&entry.906535625=Mahmut%20S.%20Gokmen%20and%20Caner%20Ozcan%20and%20Moneera%20N.%20Haque%20and%20Steve%20W.%20Leung%20and%20C.%20Seth%20Parker%20and%20W.%20Brent%20Seales%20and%20Cody%20Bumgardner&entry.1292438233=%20%20Coronary%20artery%20disease%20%28CAD%29%2C%20one%20of%20the%20leading%20causes%20of%20mortality%0Aworldwide%2C%20necessitates%20effective%20risk%20assessment%20strategies%2C%20with%20coronary%0Aartery%20calcium%20%28CAC%29%20scoring%20via%20computed%20tomography%20%28CT%29%20being%20a%20key%20method%0Afor%20prevention.%20Traditional%20methods%2C%20primarily%20based%20on%20UNET%20architectures%0Aimplemented%20on%20pre-built%20models%2C%20face%20challenges%20like%20the%20scarcity%20of%20annotated%0ACT%20scans%20containing%20CAC%20and%20imbalanced%20datasets%2C%20leading%20to%20reduced%20performance%0Ain%20segmentation%20and%20scoring%20tasks.%20In%20this%20study%2C%20we%20address%20these%20limitations%0Aby%20incorporating%20the%20self-supervised%20learning%20%28SSL%29%20technique%20of%20DINO%0A%28self-distillation%20with%20no%20labels%29%2C%20which%20trains%20without%20requiring%20CAC-specific%0Aannotations%2C%20enhancing%20its%20robustness%20in%20generating%20distinct%20features.%20The%0ADINO-LG%20model%2C%20which%20leverages%20label%20guidance%20to%20focus%20on%20calcified%20areas%2C%0Aachieves%20significant%20improvements%2C%20with%20a%20sensitivity%20of%2089%25%20and%20specificity%20of%0A90%25%20for%20detecting%20CAC-containing%20CT%20slices%2C%20compared%20to%20the%20standard%20DINO%0Amodel%27s%20sensitivity%20of%2079%25%20and%20specificity%20of%2077%25.%20Additionally%2C%20false-negative%0Aand%20false-positive%20rates%20are%20reduced%20by%2049%25%20and%2059%25%2C%20respectively%2C%20instilling%0Agreater%20confidence%20in%20clinicians%20when%20ruling%20out%20calcification%20in%20low-risk%0Apatients%20and%20minimizing%20unnecessary%20imaging%20reviews%20by%20radiologists.%20Further%2C%0ACAC%20scoring%20and%20segmentation%20tasks%20are%20conducted%20using%20a%20basic%20UNET%0Aarchitecture%2C%20applied%20specifically%20to%20CT%20slices%20identified%20by%20the%20DINO-LG%20model%0Aas%20containing%20calcified%20areas.%20This%20targeted%20approach%20enhances%20CAC%20scoring%0Aaccuracy%20by%20feeding%20the%20UNET%20model%20with%20relevant%20slices%2C%20significantly%0Aimproving%20diagnostic%20precision%2C%20reducing%20both%20false%20positives%20and%20false%0Anegatives%2C%20and%20ultimately%20lowering%20overall%20healthcare%20costs%20by%20minimizing%0Aunnecessary%20tests%20and%20treatments%2C%20presenting%20a%20valuable%20advancement%20in%20CAD%20risk%0Aassessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07976v6&entry.124074799=Read"},
{"title": "MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of\n  Accelerators", "author": "Cheng Wan and Runkao Tao and Zheng Du and Yang Katie Zhao and Yingyan Celine Lin", "abstract": "  Graph convolutional networks (GCNs) have demonstrated superiority in\ngraph-based learning tasks. However, training GCNs on full graphs is\nparticularly challenging, due to the following two challenges: (1) the\nassociated feature tensors can easily explode the memory and block the\ncommunication bandwidth of modern accelerators, and (2) the computation\nworkflow in training GCNs alternates between sparse and dense matrix\noperations, complicating the efficient utilization of computational resources.\nExisting solutions for scalable distributed full-graph GCN training mostly\nadopt partition parallelism, which is unsatisfactory as they only partially\naddress the first challenge while incurring scaled-out communication volume. To\nthis end, we propose MixGCN aiming to simultaneously address both the\naforementioned challenges towards GCN training. To tackle the first challenge,\nMixGCN integrates mixture of parallelism. Both theoretical and empirical\nanalysis verify its constant communication volumes and enhanced balanced\nworkload; For handling the second challenge, we consider mixture of\naccelerators (i.e., sparse and dense accelerators) with a dedicated accelerator\nfor GCN training and a fine-grain pipeline. Extensive experiments show that\nMixGCN achieves boosted training efficiency and scalability.\n", "link": "http://arxiv.org/abs/2501.01951v1", "date": "2025-01-03", "relevancy": 2.051, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5325}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5078}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MixGCN%3A%20Scalable%20GCN%20Training%20by%20Mixture%20of%20Parallelism%20and%20Mixture%20of%0A%20%20Accelerators&body=Title%3A%20MixGCN%3A%20Scalable%20GCN%20Training%20by%20Mixture%20of%20Parallelism%20and%20Mixture%20of%0A%20%20Accelerators%0AAuthor%3A%20Cheng%20Wan%20and%20Runkao%20Tao%20and%20Zheng%20Du%20and%20Yang%20Katie%20Zhao%20and%20Yingyan%20Celine%20Lin%0AAbstract%3A%20%20%20Graph%20convolutional%20networks%20%28GCNs%29%20have%20demonstrated%20superiority%20in%0Agraph-based%20learning%20tasks.%20However%2C%20training%20GCNs%20on%20full%20graphs%20is%0Aparticularly%20challenging%2C%20due%20to%20the%20following%20two%20challenges%3A%20%281%29%20the%0Aassociated%20feature%20tensors%20can%20easily%20explode%20the%20memory%20and%20block%20the%0Acommunication%20bandwidth%20of%20modern%20accelerators%2C%20and%20%282%29%20the%20computation%0Aworkflow%20in%20training%20GCNs%20alternates%20between%20sparse%20and%20dense%20matrix%0Aoperations%2C%20complicating%20the%20efficient%20utilization%20of%20computational%20resources.%0AExisting%20solutions%20for%20scalable%20distributed%20full-graph%20GCN%20training%20mostly%0Aadopt%20partition%20parallelism%2C%20which%20is%20unsatisfactory%20as%20they%20only%20partially%0Aaddress%20the%20first%20challenge%20while%20incurring%20scaled-out%20communication%20volume.%20To%0Athis%20end%2C%20we%20propose%20MixGCN%20aiming%20to%20simultaneously%20address%20both%20the%0Aaforementioned%20challenges%20towards%20GCN%20training.%20To%20tackle%20the%20first%20challenge%2C%0AMixGCN%20integrates%20mixture%20of%20parallelism.%20Both%20theoretical%20and%20empirical%0Aanalysis%20verify%20its%20constant%20communication%20volumes%20and%20enhanced%20balanced%0Aworkload%3B%20For%20handling%20the%20second%20challenge%2C%20we%20consider%20mixture%20of%0Aaccelerators%20%28i.e.%2C%20sparse%20and%20dense%20accelerators%29%20with%20a%20dedicated%20accelerator%0Afor%20GCN%20training%20and%20a%20fine-grain%20pipeline.%20Extensive%20experiments%20show%20that%0AMixGCN%20achieves%20boosted%20training%20efficiency%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixGCN%253A%2520Scalable%2520GCN%2520Training%2520by%2520Mixture%2520of%2520Parallelism%2520and%2520Mixture%2520of%250A%2520%2520Accelerators%26entry.906535625%3DCheng%2520Wan%2520and%2520Runkao%2520Tao%2520and%2520Zheng%2520Du%2520and%2520Yang%2520Katie%2520Zhao%2520and%2520Yingyan%2520Celine%2520Lin%26entry.1292438233%3D%2520%2520Graph%2520convolutional%2520networks%2520%2528GCNs%2529%2520have%2520demonstrated%2520superiority%2520in%250Agraph-based%2520learning%2520tasks.%2520However%252C%2520training%2520GCNs%2520on%2520full%2520graphs%2520is%250Aparticularly%2520challenging%252C%2520due%2520to%2520the%2520following%2520two%2520challenges%253A%2520%25281%2529%2520the%250Aassociated%2520feature%2520tensors%2520can%2520easily%2520explode%2520the%2520memory%2520and%2520block%2520the%250Acommunication%2520bandwidth%2520of%2520modern%2520accelerators%252C%2520and%2520%25282%2529%2520the%2520computation%250Aworkflow%2520in%2520training%2520GCNs%2520alternates%2520between%2520sparse%2520and%2520dense%2520matrix%250Aoperations%252C%2520complicating%2520the%2520efficient%2520utilization%2520of%2520computational%2520resources.%250AExisting%2520solutions%2520for%2520scalable%2520distributed%2520full-graph%2520GCN%2520training%2520mostly%250Aadopt%2520partition%2520parallelism%252C%2520which%2520is%2520unsatisfactory%2520as%2520they%2520only%2520partially%250Aaddress%2520the%2520first%2520challenge%2520while%2520incurring%2520scaled-out%2520communication%2520volume.%2520To%250Athis%2520end%252C%2520we%2520propose%2520MixGCN%2520aiming%2520to%2520simultaneously%2520address%2520both%2520the%250Aaforementioned%2520challenges%2520towards%2520GCN%2520training.%2520To%2520tackle%2520the%2520first%2520challenge%252C%250AMixGCN%2520integrates%2520mixture%2520of%2520parallelism.%2520Both%2520theoretical%2520and%2520empirical%250Aanalysis%2520verify%2520its%2520constant%2520communication%2520volumes%2520and%2520enhanced%2520balanced%250Aworkload%253B%2520For%2520handling%2520the%2520second%2520challenge%252C%2520we%2520consider%2520mixture%2520of%250Aaccelerators%2520%2528i.e.%252C%2520sparse%2520and%2520dense%2520accelerators%2529%2520with%2520a%2520dedicated%2520accelerator%250Afor%2520GCN%2520training%2520and%2520a%2520fine-grain%2520pipeline.%2520Extensive%2520experiments%2520show%2520that%250AMixGCN%2520achieves%2520boosted%2520training%2520efficiency%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MixGCN%3A%20Scalable%20GCN%20Training%20by%20Mixture%20of%20Parallelism%20and%20Mixture%20of%0A%20%20Accelerators&entry.906535625=Cheng%20Wan%20and%20Runkao%20Tao%20and%20Zheng%20Du%20and%20Yang%20Katie%20Zhao%20and%20Yingyan%20Celine%20Lin&entry.1292438233=%20%20Graph%20convolutional%20networks%20%28GCNs%29%20have%20demonstrated%20superiority%20in%0Agraph-based%20learning%20tasks.%20However%2C%20training%20GCNs%20on%20full%20graphs%20is%0Aparticularly%20challenging%2C%20due%20to%20the%20following%20two%20challenges%3A%20%281%29%20the%0Aassociated%20feature%20tensors%20can%20easily%20explode%20the%20memory%20and%20block%20the%0Acommunication%20bandwidth%20of%20modern%20accelerators%2C%20and%20%282%29%20the%20computation%0Aworkflow%20in%20training%20GCNs%20alternates%20between%20sparse%20and%20dense%20matrix%0Aoperations%2C%20complicating%20the%20efficient%20utilization%20of%20computational%20resources.%0AExisting%20solutions%20for%20scalable%20distributed%20full-graph%20GCN%20training%20mostly%0Aadopt%20partition%20parallelism%2C%20which%20is%20unsatisfactory%20as%20they%20only%20partially%0Aaddress%20the%20first%20challenge%20while%20incurring%20scaled-out%20communication%20volume.%20To%0Athis%20end%2C%20we%20propose%20MixGCN%20aiming%20to%20simultaneously%20address%20both%20the%0Aaforementioned%20challenges%20towards%20GCN%20training.%20To%20tackle%20the%20first%20challenge%2C%0AMixGCN%20integrates%20mixture%20of%20parallelism.%20Both%20theoretical%20and%20empirical%0Aanalysis%20verify%20its%20constant%20communication%20volumes%20and%20enhanced%20balanced%0Aworkload%3B%20For%20handling%20the%20second%20challenge%2C%20we%20consider%20mixture%20of%0Aaccelerators%20%28i.e.%2C%20sparse%20and%20dense%20accelerators%29%20with%20a%20dedicated%20accelerator%0Afor%20GCN%20training%20and%20a%20fine-grain%20pipeline.%20Extensive%20experiments%20show%20that%0AMixGCN%20achieves%20boosted%20training%20efficiency%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01951v1&entry.124074799=Read"},
{"title": "Exoplanet Detection via Differentiable Rendering", "author": "Brandon Y. Feng and Rodrigo Ferrer-Ch\u00e1vez and Aviad Levis and Jason J. Wang and Katherine L. Bouman and William T. Freeman", "abstract": "  Direct imaging of exoplanets is crucial for advancing our understanding of\nplanetary systems beyond our solar system, but it faces significant challenges\ndue to the high contrast between host stars and their planets. Wavefront\naberrations introduce speckles in the telescope science images, which are\npatterns of diffracted starlight that can mimic the appearance of planets,\ncomplicating the detection of faint exoplanet signals. Traditional\npost-processing methods, operating primarily in the image intensity domain, do\nnot integrate wavefront sensing data. These data, measured mainly for adaptive\noptics corrections, have been overlooked as a potential resource for\npost-processing, partly due to the challenge of the evolving nature of\nwavefront aberrations. In this paper, we present a differentiable rendering\napproach that leverages these wavefront sensing data to improve exoplanet\ndetection. Our differentiable renderer models wave-based light propagation\nthrough a coronagraphic telescope system, allowing gradient-based optimization\nto significantly improve starlight subtraction and increase sensitivity to\nfaint exoplanets. Simulation experiments based on the James Webb Space\nTelescope configuration demonstrate the effectiveness of our approach,\nachieving substantial improvements in contrast and planet detection limits. Our\nresults showcase how the computational advancements enabled by differentiable\nrendering can revitalize previously underexploited wavefront data, opening new\navenues for enhancing exoplanet imaging and characterization.\n", "link": "http://arxiv.org/abs/2501.01912v1", "date": "2025-01-03", "relevancy": 2.025, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5206}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5034}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exoplanet%20Detection%20via%20Differentiable%20Rendering&body=Title%3A%20Exoplanet%20Detection%20via%20Differentiable%20Rendering%0AAuthor%3A%20Brandon%20Y.%20Feng%20and%20Rodrigo%20Ferrer-Ch%C3%A1vez%20and%20Aviad%20Levis%20and%20Jason%20J.%20Wang%20and%20Katherine%20L.%20Bouman%20and%20William%20T.%20Freeman%0AAbstract%3A%20%20%20Direct%20imaging%20of%20exoplanets%20is%20crucial%20for%20advancing%20our%20understanding%20of%0Aplanetary%20systems%20beyond%20our%20solar%20system%2C%20but%20it%20faces%20significant%20challenges%0Adue%20to%20the%20high%20contrast%20between%20host%20stars%20and%20their%20planets.%20Wavefront%0Aaberrations%20introduce%20speckles%20in%20the%20telescope%20science%20images%2C%20which%20are%0Apatterns%20of%20diffracted%20starlight%20that%20can%20mimic%20the%20appearance%20of%20planets%2C%0Acomplicating%20the%20detection%20of%20faint%20exoplanet%20signals.%20Traditional%0Apost-processing%20methods%2C%20operating%20primarily%20in%20the%20image%20intensity%20domain%2C%20do%0Anot%20integrate%20wavefront%20sensing%20data.%20These%20data%2C%20measured%20mainly%20for%20adaptive%0Aoptics%20corrections%2C%20have%20been%20overlooked%20as%20a%20potential%20resource%20for%0Apost-processing%2C%20partly%20due%20to%20the%20challenge%20of%20the%20evolving%20nature%20of%0Awavefront%20aberrations.%20In%20this%20paper%2C%20we%20present%20a%20differentiable%20rendering%0Aapproach%20that%20leverages%20these%20wavefront%20sensing%20data%20to%20improve%20exoplanet%0Adetection.%20Our%20differentiable%20renderer%20models%20wave-based%20light%20propagation%0Athrough%20a%20coronagraphic%20telescope%20system%2C%20allowing%20gradient-based%20optimization%0Ato%20significantly%20improve%20starlight%20subtraction%20and%20increase%20sensitivity%20to%0Afaint%20exoplanets.%20Simulation%20experiments%20based%20on%20the%20James%20Webb%20Space%0ATelescope%20configuration%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Aachieving%20substantial%20improvements%20in%20contrast%20and%20planet%20detection%20limits.%20Our%0Aresults%20showcase%20how%20the%20computational%20advancements%20enabled%20by%20differentiable%0Arendering%20can%20revitalize%20previously%20underexploited%20wavefront%20data%2C%20opening%20new%0Aavenues%20for%20enhancing%20exoplanet%20imaging%20and%20characterization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExoplanet%2520Detection%2520via%2520Differentiable%2520Rendering%26entry.906535625%3DBrandon%2520Y.%2520Feng%2520and%2520Rodrigo%2520Ferrer-Ch%25C3%25A1vez%2520and%2520Aviad%2520Levis%2520and%2520Jason%2520J.%2520Wang%2520and%2520Katherine%2520L.%2520Bouman%2520and%2520William%2520T.%2520Freeman%26entry.1292438233%3D%2520%2520Direct%2520imaging%2520of%2520exoplanets%2520is%2520crucial%2520for%2520advancing%2520our%2520understanding%2520of%250Aplanetary%2520systems%2520beyond%2520our%2520solar%2520system%252C%2520but%2520it%2520faces%2520significant%2520challenges%250Adue%2520to%2520the%2520high%2520contrast%2520between%2520host%2520stars%2520and%2520their%2520planets.%2520Wavefront%250Aaberrations%2520introduce%2520speckles%2520in%2520the%2520telescope%2520science%2520images%252C%2520which%2520are%250Apatterns%2520of%2520diffracted%2520starlight%2520that%2520can%2520mimic%2520the%2520appearance%2520of%2520planets%252C%250Acomplicating%2520the%2520detection%2520of%2520faint%2520exoplanet%2520signals.%2520Traditional%250Apost-processing%2520methods%252C%2520operating%2520primarily%2520in%2520the%2520image%2520intensity%2520domain%252C%2520do%250Anot%2520integrate%2520wavefront%2520sensing%2520data.%2520These%2520data%252C%2520measured%2520mainly%2520for%2520adaptive%250Aoptics%2520corrections%252C%2520have%2520been%2520overlooked%2520as%2520a%2520potential%2520resource%2520for%250Apost-processing%252C%2520partly%2520due%2520to%2520the%2520challenge%2520of%2520the%2520evolving%2520nature%2520of%250Awavefront%2520aberrations.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520differentiable%2520rendering%250Aapproach%2520that%2520leverages%2520these%2520wavefront%2520sensing%2520data%2520to%2520improve%2520exoplanet%250Adetection.%2520Our%2520differentiable%2520renderer%2520models%2520wave-based%2520light%2520propagation%250Athrough%2520a%2520coronagraphic%2520telescope%2520system%252C%2520allowing%2520gradient-based%2520optimization%250Ato%2520significantly%2520improve%2520starlight%2520subtraction%2520and%2520increase%2520sensitivity%2520to%250Afaint%2520exoplanets.%2520Simulation%2520experiments%2520based%2520on%2520the%2520James%2520Webb%2520Space%250ATelescope%2520configuration%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%250Aachieving%2520substantial%2520improvements%2520in%2520contrast%2520and%2520planet%2520detection%2520limits.%2520Our%250Aresults%2520showcase%2520how%2520the%2520computational%2520advancements%2520enabled%2520by%2520differentiable%250Arendering%2520can%2520revitalize%2520previously%2520underexploited%2520wavefront%2520data%252C%2520opening%2520new%250Aavenues%2520for%2520enhancing%2520exoplanet%2520imaging%2520and%2520characterization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exoplanet%20Detection%20via%20Differentiable%20Rendering&entry.906535625=Brandon%20Y.%20Feng%20and%20Rodrigo%20Ferrer-Ch%C3%A1vez%20and%20Aviad%20Levis%20and%20Jason%20J.%20Wang%20and%20Katherine%20L.%20Bouman%20and%20William%20T.%20Freeman&entry.1292438233=%20%20Direct%20imaging%20of%20exoplanets%20is%20crucial%20for%20advancing%20our%20understanding%20of%0Aplanetary%20systems%20beyond%20our%20solar%20system%2C%20but%20it%20faces%20significant%20challenges%0Adue%20to%20the%20high%20contrast%20between%20host%20stars%20and%20their%20planets.%20Wavefront%0Aaberrations%20introduce%20speckles%20in%20the%20telescope%20science%20images%2C%20which%20are%0Apatterns%20of%20diffracted%20starlight%20that%20can%20mimic%20the%20appearance%20of%20planets%2C%0Acomplicating%20the%20detection%20of%20faint%20exoplanet%20signals.%20Traditional%0Apost-processing%20methods%2C%20operating%20primarily%20in%20the%20image%20intensity%20domain%2C%20do%0Anot%20integrate%20wavefront%20sensing%20data.%20These%20data%2C%20measured%20mainly%20for%20adaptive%0Aoptics%20corrections%2C%20have%20been%20overlooked%20as%20a%20potential%20resource%20for%0Apost-processing%2C%20partly%20due%20to%20the%20challenge%20of%20the%20evolving%20nature%20of%0Awavefront%20aberrations.%20In%20this%20paper%2C%20we%20present%20a%20differentiable%20rendering%0Aapproach%20that%20leverages%20these%20wavefront%20sensing%20data%20to%20improve%20exoplanet%0Adetection.%20Our%20differentiable%20renderer%20models%20wave-based%20light%20propagation%0Athrough%20a%20coronagraphic%20telescope%20system%2C%20allowing%20gradient-based%20optimization%0Ato%20significantly%20improve%20starlight%20subtraction%20and%20increase%20sensitivity%20to%0Afaint%20exoplanets.%20Simulation%20experiments%20based%20on%20the%20James%20Webb%20Space%0ATelescope%20configuration%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Aachieving%20substantial%20improvements%20in%20contrast%20and%20planet%20detection%20limits.%20Our%0Aresults%20showcase%20how%20the%20computational%20advancements%20enabled%20by%20differentiable%0Arendering%20can%20revitalize%20previously%20underexploited%20wavefront%20data%2C%20opening%20new%0Aavenues%20for%20enhancing%20exoplanet%20imaging%20and%20characterization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01912v1&entry.124074799=Read"},
{"title": "Bridging Classification and Segmentation in Osteosarcoma Assessment via\n  Foundation and Discrete Diffusion Models", "author": "Manh Duong Nguyen and Dac Thai Nguyen and Trung Viet Nguyen and Homi Yamada and Huy Hieu Pham and Phi Le Nguyen", "abstract": "  Osteosarcoma, the most common primary bone cancer, often requires accurate\nnecrosis assessment from whole slide images (WSIs) for effective treatment\nplanning and prognosis. However, manual assessments are subjective and prone to\nvariability. In response, we introduce FDDM, a novel framework bridging the gap\nbetween patch classification and region-based segmentation. FDDM operates in\ntwo stages: patch-based classification, followed by region-based refinement,\nenabling cross-patch information intergation. Leveraging a newly curated\ndataset of osteosarcoma images, FDDM demonstrates superior segmentation\nperformance, achieving up to a 10% improvement mIOU and a 32.12% enhancement in\nnecrosis rate estimation over state-of-the-art methods. This framework sets a\nnew benchmark in osteosarcoma assessment, highlighting the potential of\nfoundation models and diffusion-based refinements in complex medical imaging\ntasks.\n", "link": "http://arxiv.org/abs/2501.01932v1", "date": "2025-01-03", "relevancy": 2.0061, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Classification%20and%20Segmentation%20in%20Osteosarcoma%20Assessment%20via%0A%20%20Foundation%20and%20Discrete%20Diffusion%20Models&body=Title%3A%20Bridging%20Classification%20and%20Segmentation%20in%20Osteosarcoma%20Assessment%20via%0A%20%20Foundation%20and%20Discrete%20Diffusion%20Models%0AAuthor%3A%20Manh%20Duong%20Nguyen%20and%20Dac%20Thai%20Nguyen%20and%20Trung%20Viet%20Nguyen%20and%20Homi%20Yamada%20and%20Huy%20Hieu%20Pham%20and%20Phi%20Le%20Nguyen%0AAbstract%3A%20%20%20Osteosarcoma%2C%20the%20most%20common%20primary%20bone%20cancer%2C%20often%20requires%20accurate%0Anecrosis%20assessment%20from%20whole%20slide%20images%20%28WSIs%29%20for%20effective%20treatment%0Aplanning%20and%20prognosis.%20However%2C%20manual%20assessments%20are%20subjective%20and%20prone%20to%0Avariability.%20In%20response%2C%20we%20introduce%20FDDM%2C%20a%20novel%20framework%20bridging%20the%20gap%0Abetween%20patch%20classification%20and%20region-based%20segmentation.%20FDDM%20operates%20in%0Atwo%20stages%3A%20patch-based%20classification%2C%20followed%20by%20region-based%20refinement%2C%0Aenabling%20cross-patch%20information%20intergation.%20Leveraging%20a%20newly%20curated%0Adataset%20of%20osteosarcoma%20images%2C%20FDDM%20demonstrates%20superior%20segmentation%0Aperformance%2C%20achieving%20up%20to%20a%2010%25%20improvement%20mIOU%20and%20a%2032.12%25%20enhancement%20in%0Anecrosis%20rate%20estimation%20over%20state-of-the-art%20methods.%20This%20framework%20sets%20a%0Anew%20benchmark%20in%20osteosarcoma%20assessment%2C%20highlighting%20the%20potential%20of%0Afoundation%20models%20and%20diffusion-based%20refinements%20in%20complex%20medical%20imaging%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Classification%2520and%2520Segmentation%2520in%2520Osteosarcoma%2520Assessment%2520via%250A%2520%2520Foundation%2520and%2520Discrete%2520Diffusion%2520Models%26entry.906535625%3DManh%2520Duong%2520Nguyen%2520and%2520Dac%2520Thai%2520Nguyen%2520and%2520Trung%2520Viet%2520Nguyen%2520and%2520Homi%2520Yamada%2520and%2520Huy%2520Hieu%2520Pham%2520and%2520Phi%2520Le%2520Nguyen%26entry.1292438233%3D%2520%2520Osteosarcoma%252C%2520the%2520most%2520common%2520primary%2520bone%2520cancer%252C%2520often%2520requires%2520accurate%250Anecrosis%2520assessment%2520from%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520for%2520effective%2520treatment%250Aplanning%2520and%2520prognosis.%2520However%252C%2520manual%2520assessments%2520are%2520subjective%2520and%2520prone%2520to%250Avariability.%2520In%2520response%252C%2520we%2520introduce%2520FDDM%252C%2520a%2520novel%2520framework%2520bridging%2520the%2520gap%250Abetween%2520patch%2520classification%2520and%2520region-based%2520segmentation.%2520FDDM%2520operates%2520in%250Atwo%2520stages%253A%2520patch-based%2520classification%252C%2520followed%2520by%2520region-based%2520refinement%252C%250Aenabling%2520cross-patch%2520information%2520intergation.%2520Leveraging%2520a%2520newly%2520curated%250Adataset%2520of%2520osteosarcoma%2520images%252C%2520FDDM%2520demonstrates%2520superior%2520segmentation%250Aperformance%252C%2520achieving%2520up%2520to%2520a%252010%2525%2520improvement%2520mIOU%2520and%2520a%252032.12%2525%2520enhancement%2520in%250Anecrosis%2520rate%2520estimation%2520over%2520state-of-the-art%2520methods.%2520This%2520framework%2520sets%2520a%250Anew%2520benchmark%2520in%2520osteosarcoma%2520assessment%252C%2520highlighting%2520the%2520potential%2520of%250Afoundation%2520models%2520and%2520diffusion-based%2520refinements%2520in%2520complex%2520medical%2520imaging%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Classification%20and%20Segmentation%20in%20Osteosarcoma%20Assessment%20via%0A%20%20Foundation%20and%20Discrete%20Diffusion%20Models&entry.906535625=Manh%20Duong%20Nguyen%20and%20Dac%20Thai%20Nguyen%20and%20Trung%20Viet%20Nguyen%20and%20Homi%20Yamada%20and%20Huy%20Hieu%20Pham%20and%20Phi%20Le%20Nguyen&entry.1292438233=%20%20Osteosarcoma%2C%20the%20most%20common%20primary%20bone%20cancer%2C%20often%20requires%20accurate%0Anecrosis%20assessment%20from%20whole%20slide%20images%20%28WSIs%29%20for%20effective%20treatment%0Aplanning%20and%20prognosis.%20However%2C%20manual%20assessments%20are%20subjective%20and%20prone%20to%0Avariability.%20In%20response%2C%20we%20introduce%20FDDM%2C%20a%20novel%20framework%20bridging%20the%20gap%0Abetween%20patch%20classification%20and%20region-based%20segmentation.%20FDDM%20operates%20in%0Atwo%20stages%3A%20patch-based%20classification%2C%20followed%20by%20region-based%20refinement%2C%0Aenabling%20cross-patch%20information%20intergation.%20Leveraging%20a%20newly%20curated%0Adataset%20of%20osteosarcoma%20images%2C%20FDDM%20demonstrates%20superior%20segmentation%0Aperformance%2C%20achieving%20up%20to%20a%2010%25%20improvement%20mIOU%20and%20a%2032.12%25%20enhancement%20in%0Anecrosis%20rate%20estimation%20over%20state-of-the-art%20methods.%20This%20framework%20sets%20a%0Anew%20benchmark%20in%20osteosarcoma%20assessment%2C%20highlighting%20the%20potential%20of%0Afoundation%20models%20and%20diffusion-based%20refinements%20in%20complex%20medical%20imaging%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01932v1&entry.124074799=Read"},
{"title": "QuArch: A Question-Answering Dataset for AI Agents in Computer\n  Architecture", "author": "Shvetank Prakash and Andrew Cheng and Jason Yik and Arya Tschand and Radhika Ghosal and Ikechukwu Uchendu and Jessica Quaye and Jeffrey Ma and Shreyas Grampurohit and Sofia Giannuzzi and Arnav Balyan and Fin Amin and Aadya Pipersenia and Yash Choudhary and Ankita Nayak and Amir Yazdanbakhsh and Vijay Janapa Reddi", "abstract": "  We introduce QuArch, a dataset of 1500 human-validated question-answer pairs\ndesigned to evaluate and enhance language models' understanding of computer\narchitecture. The dataset covers areas including processor design, memory\nsystems, and performance optimization. Our analysis highlights a significant\nperformance gap: the best closed-source model achieves 84% accuracy, while the\ntop small open-source model reaches 72%. We observe notable struggles in memory\nsystems, interconnection networks, and benchmarking. Fine-tuning with QuArch\nimproves small model accuracy by up to 8%, establishing a foundation for\nadvancing AI-driven computer architecture research. The dataset and leaderboard\nare at https://harvard-edge.github.io/QuArch/.\n", "link": "http://arxiv.org/abs/2501.01892v1", "date": "2025-01-03", "relevancy": 1.9996, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuArch%3A%20A%20Question-Answering%20Dataset%20for%20AI%20Agents%20in%20Computer%0A%20%20Architecture&body=Title%3A%20QuArch%3A%20A%20Question-Answering%20Dataset%20for%20AI%20Agents%20in%20Computer%0A%20%20Architecture%0AAuthor%3A%20Shvetank%20Prakash%20and%20Andrew%20Cheng%20and%20Jason%20Yik%20and%20Arya%20Tschand%20and%20Radhika%20Ghosal%20and%20Ikechukwu%20Uchendu%20and%20Jessica%20Quaye%20and%20Jeffrey%20Ma%20and%20Shreyas%20Grampurohit%20and%20Sofia%20Giannuzzi%20and%20Arnav%20Balyan%20and%20Fin%20Amin%20and%20Aadya%20Pipersenia%20and%20Yash%20Choudhary%20and%20Ankita%20Nayak%20and%20Amir%20Yazdanbakhsh%20and%20Vijay%20Janapa%20Reddi%0AAbstract%3A%20%20%20We%20introduce%20QuArch%2C%20a%20dataset%20of%201500%20human-validated%20question-answer%20pairs%0Adesigned%20to%20evaluate%20and%20enhance%20language%20models%27%20understanding%20of%20computer%0Aarchitecture.%20The%20dataset%20covers%20areas%20including%20processor%20design%2C%20memory%0Asystems%2C%20and%20performance%20optimization.%20Our%20analysis%20highlights%20a%20significant%0Aperformance%20gap%3A%20the%20best%20closed-source%20model%20achieves%2084%25%20accuracy%2C%20while%20the%0Atop%20small%20open-source%20model%20reaches%2072%25.%20We%20observe%20notable%20struggles%20in%20memory%0Asystems%2C%20interconnection%20networks%2C%20and%20benchmarking.%20Fine-tuning%20with%20QuArch%0Aimproves%20small%20model%20accuracy%20by%20up%20to%208%25%2C%20establishing%20a%20foundation%20for%0Aadvancing%20AI-driven%20computer%20architecture%20research.%20The%20dataset%20and%20leaderboard%0Aare%20at%20https%3A//harvard-edge.github.io/QuArch/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuArch%253A%2520A%2520Question-Answering%2520Dataset%2520for%2520AI%2520Agents%2520in%2520Computer%250A%2520%2520Architecture%26entry.906535625%3DShvetank%2520Prakash%2520and%2520Andrew%2520Cheng%2520and%2520Jason%2520Yik%2520and%2520Arya%2520Tschand%2520and%2520Radhika%2520Ghosal%2520and%2520Ikechukwu%2520Uchendu%2520and%2520Jessica%2520Quaye%2520and%2520Jeffrey%2520Ma%2520and%2520Shreyas%2520Grampurohit%2520and%2520Sofia%2520Giannuzzi%2520and%2520Arnav%2520Balyan%2520and%2520Fin%2520Amin%2520and%2520Aadya%2520Pipersenia%2520and%2520Yash%2520Choudhary%2520and%2520Ankita%2520Nayak%2520and%2520Amir%2520Yazdanbakhsh%2520and%2520Vijay%2520Janapa%2520Reddi%26entry.1292438233%3D%2520%2520We%2520introduce%2520QuArch%252C%2520a%2520dataset%2520of%25201500%2520human-validated%2520question-answer%2520pairs%250Adesigned%2520to%2520evaluate%2520and%2520enhance%2520language%2520models%2527%2520understanding%2520of%2520computer%250Aarchitecture.%2520The%2520dataset%2520covers%2520areas%2520including%2520processor%2520design%252C%2520memory%250Asystems%252C%2520and%2520performance%2520optimization.%2520Our%2520analysis%2520highlights%2520a%2520significant%250Aperformance%2520gap%253A%2520the%2520best%2520closed-source%2520model%2520achieves%252084%2525%2520accuracy%252C%2520while%2520the%250Atop%2520small%2520open-source%2520model%2520reaches%252072%2525.%2520We%2520observe%2520notable%2520struggles%2520in%2520memory%250Asystems%252C%2520interconnection%2520networks%252C%2520and%2520benchmarking.%2520Fine-tuning%2520with%2520QuArch%250Aimproves%2520small%2520model%2520accuracy%2520by%2520up%2520to%25208%2525%252C%2520establishing%2520a%2520foundation%2520for%250Aadvancing%2520AI-driven%2520computer%2520architecture%2520research.%2520The%2520dataset%2520and%2520leaderboard%250Aare%2520at%2520https%253A//harvard-edge.github.io/QuArch/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuArch%3A%20A%20Question-Answering%20Dataset%20for%20AI%20Agents%20in%20Computer%0A%20%20Architecture&entry.906535625=Shvetank%20Prakash%20and%20Andrew%20Cheng%20and%20Jason%20Yik%20and%20Arya%20Tschand%20and%20Radhika%20Ghosal%20and%20Ikechukwu%20Uchendu%20and%20Jessica%20Quaye%20and%20Jeffrey%20Ma%20and%20Shreyas%20Grampurohit%20and%20Sofia%20Giannuzzi%20and%20Arnav%20Balyan%20and%20Fin%20Amin%20and%20Aadya%20Pipersenia%20and%20Yash%20Choudhary%20and%20Ankita%20Nayak%20and%20Amir%20Yazdanbakhsh%20and%20Vijay%20Janapa%20Reddi&entry.1292438233=%20%20We%20introduce%20QuArch%2C%20a%20dataset%20of%201500%20human-validated%20question-answer%20pairs%0Adesigned%20to%20evaluate%20and%20enhance%20language%20models%27%20understanding%20of%20computer%0Aarchitecture.%20The%20dataset%20covers%20areas%20including%20processor%20design%2C%20memory%0Asystems%2C%20and%20performance%20optimization.%20Our%20analysis%20highlights%20a%20significant%0Aperformance%20gap%3A%20the%20best%20closed-source%20model%20achieves%2084%25%20accuracy%2C%20while%20the%0Atop%20small%20open-source%20model%20reaches%2072%25.%20We%20observe%20notable%20struggles%20in%20memory%0Asystems%2C%20interconnection%20networks%2C%20and%20benchmarking.%20Fine-tuning%20with%20QuArch%0Aimproves%20small%20model%20accuracy%20by%20up%20to%208%25%2C%20establishing%20a%20foundation%20for%0Aadvancing%20AI-driven%20computer%20architecture%20research.%20The%20dataset%20and%20leaderboard%0Aare%20at%20https%3A//harvard-edge.github.io/QuArch/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01892v1&entry.124074799=Read"},
{"title": "Learning Chemical Reaction Representation with Reactant-Product\n  Alignment", "author": "Kaipeng Zeng and Xianbin Liu and Yu Zhang and Xiaokang Yang and Yaohui Jin and Yanyan Xu", "abstract": "  Organic synthesis stands as a cornerstone of the chemical industry. The\ndevelopment of robust machine learning models to support tasks associated with\norganic reactions is of significant interest. However, current methods rely on\nhand-crafted features or direct adaptations of model architectures from other\ndomains, which lack feasibility as data scales increase or ignore the rich\nchemical information inherent in reactions. To address these issues, this paper\nintroduces RAlign, a novel chemical reaction representation learning model for\nvarious organic reaction-related tasks. By integrating atomic correspondence\nbetween reactants and products, our model discerns the molecular\ntransformations that occur during the reaction, thereby enhancing comprehension\nof the reaction mechanism. We have designed an adapter structure to incorporate\nreaction conditions into the chemical reaction representation, allowing the\nmodel to handle various reaction conditions and to adapt to various datasets\nand downstream tasks. Additionally, we introduce a reaction-center-aware\nattention mechanism that enables the model to concentrate on key functional\ngroups, thereby generating potent representations for chemical reactions. Our\nmodel has been evaluated on a range of downstream tasks. Experimental results\nindicate that our model markedly outperforms existing chemical reaction\nrepresentation learning architectures on most of the datasets. We plan to\nopen-source the code contingent upon the acceptance of the paper.\n", "link": "http://arxiv.org/abs/2411.17629v2", "date": "2025-01-03", "relevancy": 1.9459, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4945}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4817}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Chemical%20Reaction%20Representation%20with%20Reactant-Product%0A%20%20Alignment&body=Title%3A%20Learning%20Chemical%20Reaction%20Representation%20with%20Reactant-Product%0A%20%20Alignment%0AAuthor%3A%20Kaipeng%20Zeng%20and%20Xianbin%20Liu%20and%20Yu%20Zhang%20and%20Xiaokang%20Yang%20and%20Yaohui%20Jin%20and%20Yanyan%20Xu%0AAbstract%3A%20%20%20Organic%20synthesis%20stands%20as%20a%20cornerstone%20of%20the%20chemical%20industry.%20The%0Adevelopment%20of%20robust%20machine%20learning%20models%20to%20support%20tasks%20associated%20with%0Aorganic%20reactions%20is%20of%20significant%20interest.%20However%2C%20current%20methods%20rely%20on%0Ahand-crafted%20features%20or%20direct%20adaptations%20of%20model%20architectures%20from%20other%0Adomains%2C%20which%20lack%20feasibility%20as%20data%20scales%20increase%20or%20ignore%20the%20rich%0Achemical%20information%20inherent%20in%20reactions.%20To%20address%20these%20issues%2C%20this%20paper%0Aintroduces%20RAlign%2C%20a%20novel%20chemical%20reaction%20representation%20learning%20model%20for%0Avarious%20organic%20reaction-related%20tasks.%20By%20integrating%20atomic%20correspondence%0Abetween%20reactants%20and%20products%2C%20our%20model%20discerns%20the%20molecular%0Atransformations%20that%20occur%20during%20the%20reaction%2C%20thereby%20enhancing%20comprehension%0Aof%20the%20reaction%20mechanism.%20We%20have%20designed%20an%20adapter%20structure%20to%20incorporate%0Areaction%20conditions%20into%20the%20chemical%20reaction%20representation%2C%20allowing%20the%0Amodel%20to%20handle%20various%20reaction%20conditions%20and%20to%20adapt%20to%20various%20datasets%0Aand%20downstream%20tasks.%20Additionally%2C%20we%20introduce%20a%20reaction-center-aware%0Aattention%20mechanism%20that%20enables%20the%20model%20to%20concentrate%20on%20key%20functional%0Agroups%2C%20thereby%20generating%20potent%20representations%20for%20chemical%20reactions.%20Our%0Amodel%20has%20been%20evaluated%20on%20a%20range%20of%20downstream%20tasks.%20Experimental%20results%0Aindicate%20that%20our%20model%20markedly%20outperforms%20existing%20chemical%20reaction%0Arepresentation%20learning%20architectures%20on%20most%20of%20the%20datasets.%20We%20plan%20to%0Aopen-source%20the%20code%20contingent%20upon%20the%20acceptance%20of%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17629v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Chemical%2520Reaction%2520Representation%2520with%2520Reactant-Product%250A%2520%2520Alignment%26entry.906535625%3DKaipeng%2520Zeng%2520and%2520Xianbin%2520Liu%2520and%2520Yu%2520Zhang%2520and%2520Xiaokang%2520Yang%2520and%2520Yaohui%2520Jin%2520and%2520Yanyan%2520Xu%26entry.1292438233%3D%2520%2520Organic%2520synthesis%2520stands%2520as%2520a%2520cornerstone%2520of%2520the%2520chemical%2520industry.%2520The%250Adevelopment%2520of%2520robust%2520machine%2520learning%2520models%2520to%2520support%2520tasks%2520associated%2520with%250Aorganic%2520reactions%2520is%2520of%2520significant%2520interest.%2520However%252C%2520current%2520methods%2520rely%2520on%250Ahand-crafted%2520features%2520or%2520direct%2520adaptations%2520of%2520model%2520architectures%2520from%2520other%250Adomains%252C%2520which%2520lack%2520feasibility%2520as%2520data%2520scales%2520increase%2520or%2520ignore%2520the%2520rich%250Achemical%2520information%2520inherent%2520in%2520reactions.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%250Aintroduces%2520RAlign%252C%2520a%2520novel%2520chemical%2520reaction%2520representation%2520learning%2520model%2520for%250Avarious%2520organic%2520reaction-related%2520tasks.%2520By%2520integrating%2520atomic%2520correspondence%250Abetween%2520reactants%2520and%2520products%252C%2520our%2520model%2520discerns%2520the%2520molecular%250Atransformations%2520that%2520occur%2520during%2520the%2520reaction%252C%2520thereby%2520enhancing%2520comprehension%250Aof%2520the%2520reaction%2520mechanism.%2520We%2520have%2520designed%2520an%2520adapter%2520structure%2520to%2520incorporate%250Areaction%2520conditions%2520into%2520the%2520chemical%2520reaction%2520representation%252C%2520allowing%2520the%250Amodel%2520to%2520handle%2520various%2520reaction%2520conditions%2520and%2520to%2520adapt%2520to%2520various%2520datasets%250Aand%2520downstream%2520tasks.%2520Additionally%252C%2520we%2520introduce%2520a%2520reaction-center-aware%250Aattention%2520mechanism%2520that%2520enables%2520the%2520model%2520to%2520concentrate%2520on%2520key%2520functional%250Agroups%252C%2520thereby%2520generating%2520potent%2520representations%2520for%2520chemical%2520reactions.%2520Our%250Amodel%2520has%2520been%2520evaluated%2520on%2520a%2520range%2520of%2520downstream%2520tasks.%2520Experimental%2520results%250Aindicate%2520that%2520our%2520model%2520markedly%2520outperforms%2520existing%2520chemical%2520reaction%250Arepresentation%2520learning%2520architectures%2520on%2520most%2520of%2520the%2520datasets.%2520We%2520plan%2520to%250Aopen-source%2520the%2520code%2520contingent%2520upon%2520the%2520acceptance%2520of%2520the%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17629v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Chemical%20Reaction%20Representation%20with%20Reactant-Product%0A%20%20Alignment&entry.906535625=Kaipeng%20Zeng%20and%20Xianbin%20Liu%20and%20Yu%20Zhang%20and%20Xiaokang%20Yang%20and%20Yaohui%20Jin%20and%20Yanyan%20Xu&entry.1292438233=%20%20Organic%20synthesis%20stands%20as%20a%20cornerstone%20of%20the%20chemical%20industry.%20The%0Adevelopment%20of%20robust%20machine%20learning%20models%20to%20support%20tasks%20associated%20with%0Aorganic%20reactions%20is%20of%20significant%20interest.%20However%2C%20current%20methods%20rely%20on%0Ahand-crafted%20features%20or%20direct%20adaptations%20of%20model%20architectures%20from%20other%0Adomains%2C%20which%20lack%20feasibility%20as%20data%20scales%20increase%20or%20ignore%20the%20rich%0Achemical%20information%20inherent%20in%20reactions.%20To%20address%20these%20issues%2C%20this%20paper%0Aintroduces%20RAlign%2C%20a%20novel%20chemical%20reaction%20representation%20learning%20model%20for%0Avarious%20organic%20reaction-related%20tasks.%20By%20integrating%20atomic%20correspondence%0Abetween%20reactants%20and%20products%2C%20our%20model%20discerns%20the%20molecular%0Atransformations%20that%20occur%20during%20the%20reaction%2C%20thereby%20enhancing%20comprehension%0Aof%20the%20reaction%20mechanism.%20We%20have%20designed%20an%20adapter%20structure%20to%20incorporate%0Areaction%20conditions%20into%20the%20chemical%20reaction%20representation%2C%20allowing%20the%0Amodel%20to%20handle%20various%20reaction%20conditions%20and%20to%20adapt%20to%20various%20datasets%0Aand%20downstream%20tasks.%20Additionally%2C%20we%20introduce%20a%20reaction-center-aware%0Aattention%20mechanism%20that%20enables%20the%20model%20to%20concentrate%20on%20key%20functional%0Agroups%2C%20thereby%20generating%20potent%20representations%20for%20chemical%20reactions.%20Our%0Amodel%20has%20been%20evaluated%20on%20a%20range%20of%20downstream%20tasks.%20Experimental%20results%0Aindicate%20that%20our%20model%20markedly%20outperforms%20existing%20chemical%20reaction%0Arepresentation%20learning%20architectures%20on%20most%20of%20the%20datasets.%20We%20plan%20to%0Aopen-source%20the%20code%20contingent%20upon%20the%20acceptance%20of%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17629v2&entry.124074799=Read"},
{"title": "GoBERT: Gene Ontology Graph Informed BERT for Universal Gene Function\n  Prediction", "author": "Yuwei Miao and Yuzhi Guo and Hehuan Ma and Jingquan Yan and Feng Jiang and Rui Liao and Junzhou Huang", "abstract": "  Exploring the functions of genes and gene products is crucial to a wide range\nof fields, including medical research, evolutionary biology, and environmental\nscience. However, discovering new functions largely relies on expensive and\nexhaustive wet lab experiments. Existing methods of automatic function\nannotation or prediction mainly focus on protein function prediction with\nsequence, 3D-structures or protein family information. In this study, we\npropose to tackle the gene function prediction problem by exploring Gene\nOntology graph and annotation with BERT (GoBERT) to decipher the underlying\nrelationships among gene functions. Our proposed novel function prediction task\nutilizes existing functions as inputs and generalizes the function prediction\nto gene and gene products. Specifically, two pre-train tasks are designed to\njointly train GoBERT to capture both explicit and implicit relations of\nfunctions. Neighborhood prediction is a self-supervised multi-label\nclassification task that captures the explicit function relations. Specified\nmasking and recovering task helps GoBERT in finding implicit patterns among\nfunctions. The pre-trained GoBERT possess the ability to predict novel\nfunctions for various gene and gene products based on known functional\nannotations. Extensive experiments, biological case studies, and ablation\nstudies are conducted to demonstrate the superiority of our proposed GoBERT.\n", "link": "http://arxiv.org/abs/2501.01930v1", "date": "2025-01-03", "relevancy": 1.836, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.477}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4605}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GoBERT%3A%20Gene%20Ontology%20Graph%20Informed%20BERT%20for%20Universal%20Gene%20Function%0A%20%20Prediction&body=Title%3A%20GoBERT%3A%20Gene%20Ontology%20Graph%20Informed%20BERT%20for%20Universal%20Gene%20Function%0A%20%20Prediction%0AAuthor%3A%20Yuwei%20Miao%20and%20Yuzhi%20Guo%20and%20Hehuan%20Ma%20and%20Jingquan%20Yan%20and%20Feng%20Jiang%20and%20Rui%20Liao%20and%20Junzhou%20Huang%0AAbstract%3A%20%20%20Exploring%20the%20functions%20of%20genes%20and%20gene%20products%20is%20crucial%20to%20a%20wide%20range%0Aof%20fields%2C%20including%20medical%20research%2C%20evolutionary%20biology%2C%20and%20environmental%0Ascience.%20However%2C%20discovering%20new%20functions%20largely%20relies%20on%20expensive%20and%0Aexhaustive%20wet%20lab%20experiments.%20Existing%20methods%20of%20automatic%20function%0Aannotation%20or%20prediction%20mainly%20focus%20on%20protein%20function%20prediction%20with%0Asequence%2C%203D-structures%20or%20protein%20family%20information.%20In%20this%20study%2C%20we%0Apropose%20to%20tackle%20the%20gene%20function%20prediction%20problem%20by%20exploring%20Gene%0AOntology%20graph%20and%20annotation%20with%20BERT%20%28GoBERT%29%20to%20decipher%20the%20underlying%0Arelationships%20among%20gene%20functions.%20Our%20proposed%20novel%20function%20prediction%20task%0Autilizes%20existing%20functions%20as%20inputs%20and%20generalizes%20the%20function%20prediction%0Ato%20gene%20and%20gene%20products.%20Specifically%2C%20two%20pre-train%20tasks%20are%20designed%20to%0Ajointly%20train%20GoBERT%20to%20capture%20both%20explicit%20and%20implicit%20relations%20of%0Afunctions.%20Neighborhood%20prediction%20is%20a%20self-supervised%20multi-label%0Aclassification%20task%20that%20captures%20the%20explicit%20function%20relations.%20Specified%0Amasking%20and%20recovering%20task%20helps%20GoBERT%20in%20finding%20implicit%20patterns%20among%0Afunctions.%20The%20pre-trained%20GoBERT%20possess%20the%20ability%20to%20predict%20novel%0Afunctions%20for%20various%20gene%20and%20gene%20products%20based%20on%20known%20functional%0Aannotations.%20Extensive%20experiments%2C%20biological%20case%20studies%2C%20and%20ablation%0Astudies%20are%20conducted%20to%20demonstrate%20the%20superiority%20of%20our%20proposed%20GoBERT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoBERT%253A%2520Gene%2520Ontology%2520Graph%2520Informed%2520BERT%2520for%2520Universal%2520Gene%2520Function%250A%2520%2520Prediction%26entry.906535625%3DYuwei%2520Miao%2520and%2520Yuzhi%2520Guo%2520and%2520Hehuan%2520Ma%2520and%2520Jingquan%2520Yan%2520and%2520Feng%2520Jiang%2520and%2520Rui%2520Liao%2520and%2520Junzhou%2520Huang%26entry.1292438233%3D%2520%2520Exploring%2520the%2520functions%2520of%2520genes%2520and%2520gene%2520products%2520is%2520crucial%2520to%2520a%2520wide%2520range%250Aof%2520fields%252C%2520including%2520medical%2520research%252C%2520evolutionary%2520biology%252C%2520and%2520environmental%250Ascience.%2520However%252C%2520discovering%2520new%2520functions%2520largely%2520relies%2520on%2520expensive%2520and%250Aexhaustive%2520wet%2520lab%2520experiments.%2520Existing%2520methods%2520of%2520automatic%2520function%250Aannotation%2520or%2520prediction%2520mainly%2520focus%2520on%2520protein%2520function%2520prediction%2520with%250Asequence%252C%25203D-structures%2520or%2520protein%2520family%2520information.%2520In%2520this%2520study%252C%2520we%250Apropose%2520to%2520tackle%2520the%2520gene%2520function%2520prediction%2520problem%2520by%2520exploring%2520Gene%250AOntology%2520graph%2520and%2520annotation%2520with%2520BERT%2520%2528GoBERT%2529%2520to%2520decipher%2520the%2520underlying%250Arelationships%2520among%2520gene%2520functions.%2520Our%2520proposed%2520novel%2520function%2520prediction%2520task%250Autilizes%2520existing%2520functions%2520as%2520inputs%2520and%2520generalizes%2520the%2520function%2520prediction%250Ato%2520gene%2520and%2520gene%2520products.%2520Specifically%252C%2520two%2520pre-train%2520tasks%2520are%2520designed%2520to%250Ajointly%2520train%2520GoBERT%2520to%2520capture%2520both%2520explicit%2520and%2520implicit%2520relations%2520of%250Afunctions.%2520Neighborhood%2520prediction%2520is%2520a%2520self-supervised%2520multi-label%250Aclassification%2520task%2520that%2520captures%2520the%2520explicit%2520function%2520relations.%2520Specified%250Amasking%2520and%2520recovering%2520task%2520helps%2520GoBERT%2520in%2520finding%2520implicit%2520patterns%2520among%250Afunctions.%2520The%2520pre-trained%2520GoBERT%2520possess%2520the%2520ability%2520to%2520predict%2520novel%250Afunctions%2520for%2520various%2520gene%2520and%2520gene%2520products%2520based%2520on%2520known%2520functional%250Aannotations.%2520Extensive%2520experiments%252C%2520biological%2520case%2520studies%252C%2520and%2520ablation%250Astudies%2520are%2520conducted%2520to%2520demonstrate%2520the%2520superiority%2520of%2520our%2520proposed%2520GoBERT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GoBERT%3A%20Gene%20Ontology%20Graph%20Informed%20BERT%20for%20Universal%20Gene%20Function%0A%20%20Prediction&entry.906535625=Yuwei%20Miao%20and%20Yuzhi%20Guo%20and%20Hehuan%20Ma%20and%20Jingquan%20Yan%20and%20Feng%20Jiang%20and%20Rui%20Liao%20and%20Junzhou%20Huang&entry.1292438233=%20%20Exploring%20the%20functions%20of%20genes%20and%20gene%20products%20is%20crucial%20to%20a%20wide%20range%0Aof%20fields%2C%20including%20medical%20research%2C%20evolutionary%20biology%2C%20and%20environmental%0Ascience.%20However%2C%20discovering%20new%20functions%20largely%20relies%20on%20expensive%20and%0Aexhaustive%20wet%20lab%20experiments.%20Existing%20methods%20of%20automatic%20function%0Aannotation%20or%20prediction%20mainly%20focus%20on%20protein%20function%20prediction%20with%0Asequence%2C%203D-structures%20or%20protein%20family%20information.%20In%20this%20study%2C%20we%0Apropose%20to%20tackle%20the%20gene%20function%20prediction%20problem%20by%20exploring%20Gene%0AOntology%20graph%20and%20annotation%20with%20BERT%20%28GoBERT%29%20to%20decipher%20the%20underlying%0Arelationships%20among%20gene%20functions.%20Our%20proposed%20novel%20function%20prediction%20task%0Autilizes%20existing%20functions%20as%20inputs%20and%20generalizes%20the%20function%20prediction%0Ato%20gene%20and%20gene%20products.%20Specifically%2C%20two%20pre-train%20tasks%20are%20designed%20to%0Ajointly%20train%20GoBERT%20to%20capture%20both%20explicit%20and%20implicit%20relations%20of%0Afunctions.%20Neighborhood%20prediction%20is%20a%20self-supervised%20multi-label%0Aclassification%20task%20that%20captures%20the%20explicit%20function%20relations.%20Specified%0Amasking%20and%20recovering%20task%20helps%20GoBERT%20in%20finding%20implicit%20patterns%20among%0Afunctions.%20The%20pre-trained%20GoBERT%20possess%20the%20ability%20to%20predict%20novel%0Afunctions%20for%20various%20gene%20and%20gene%20products%20based%20on%20known%20functional%0Aannotations.%20Extensive%20experiments%2C%20biological%20case%20studies%2C%20and%20ablation%0Astudies%20are%20conducted%20to%20demonstrate%20the%20superiority%20of%20our%20proposed%20GoBERT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01930v1&entry.124074799=Read"},
{"title": "Mingling with the Good to Backdoor Federated Learning", "author": "Nuno Neves", "abstract": "  Federated learning (FL) is a decentralized machine learning technique that\nallows multiple entities to jointly train a model while preserving dataset\nprivacy. However, its distributed nature has raised various security concerns,\nwhich have been addressed by increasingly sophisticated defenses. These\nprotections utilize a range of data sources and metrics to, for example, filter\nout malicious model updates, ensuring that the impact of attacks is minimized\nor eliminated.\n  This paper explores the feasibility of designing a generic attack method\ncapable of installing backdoors in FL while evading a diverse array of\ndefenses. Specifically, we focus on an attacker strategy called MIGO, which\naims to produce model updates that subtly blend with legitimate ones. The\nresulting effect is a gradual integration of a backdoor into the global model,\noften ensuring its persistence long after the attack concludes, while\ngenerating enough ambiguity to hinder the effectiveness of defenses.\n  MIGO was employed to implant three types of backdoors across five datasets\nand different model architectures. The results demonstrate the significant\nthreat posed by these backdoors, as MIGO consistently achieved exceptionally\nhigh backdoor accuracy (exceeding 90%) while maintaining the utility of the\nmain task. Moreover, MIGO exhibited strong evasion capabilities against ten\ndefenses, including several state-of-the-art methods. When compared to four\nother attack strategies, MIGO consistently outperformed them across most\nconfigurations. Notably, even in extreme scenarios where the attacker controls\njust 0.1% of the clients, the results indicate that successful backdoor\ninsertion is possible if the attacker can persist for a sufficient number of\nrounds.\n", "link": "http://arxiv.org/abs/2501.01913v1", "date": "2025-01-03", "relevancy": 1.8147, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4566}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4562}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mingling%20with%20the%20Good%20to%20Backdoor%20Federated%20Learning&body=Title%3A%20Mingling%20with%20the%20Good%20to%20Backdoor%20Federated%20Learning%0AAuthor%3A%20Nuno%20Neves%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20decentralized%20machine%20learning%20technique%20that%0Aallows%20multiple%20entities%20to%20jointly%20train%20a%20model%20while%20preserving%20dataset%0Aprivacy.%20However%2C%20its%20distributed%20nature%20has%20raised%20various%20security%20concerns%2C%0Awhich%20have%20been%20addressed%20by%20increasingly%20sophisticated%20defenses.%20These%0Aprotections%20utilize%20a%20range%20of%20data%20sources%20and%20metrics%20to%2C%20for%20example%2C%20filter%0Aout%20malicious%20model%20updates%2C%20ensuring%20that%20the%20impact%20of%20attacks%20is%20minimized%0Aor%20eliminated.%0A%20%20This%20paper%20explores%20the%20feasibility%20of%20designing%20a%20generic%20attack%20method%0Acapable%20of%20installing%20backdoors%20in%20FL%20while%20evading%20a%20diverse%20array%20of%0Adefenses.%20Specifically%2C%20we%20focus%20on%20an%20attacker%20strategy%20called%20MIGO%2C%20which%0Aaims%20to%20produce%20model%20updates%20that%20subtly%20blend%20with%20legitimate%20ones.%20The%0Aresulting%20effect%20is%20a%20gradual%20integration%20of%20a%20backdoor%20into%20the%20global%20model%2C%0Aoften%20ensuring%20its%20persistence%20long%20after%20the%20attack%20concludes%2C%20while%0Agenerating%20enough%20ambiguity%20to%20hinder%20the%20effectiveness%20of%20defenses.%0A%20%20MIGO%20was%20employed%20to%20implant%20three%20types%20of%20backdoors%20across%20five%20datasets%0Aand%20different%20model%20architectures.%20The%20results%20demonstrate%20the%20significant%0Athreat%20posed%20by%20these%20backdoors%2C%20as%20MIGO%20consistently%20achieved%20exceptionally%0Ahigh%20backdoor%20accuracy%20%28exceeding%2090%25%29%20while%20maintaining%20the%20utility%20of%20the%0Amain%20task.%20Moreover%2C%20MIGO%20exhibited%20strong%20evasion%20capabilities%20against%20ten%0Adefenses%2C%20including%20several%20state-of-the-art%20methods.%20When%20compared%20to%20four%0Aother%20attack%20strategies%2C%20MIGO%20consistently%20outperformed%20them%20across%20most%0Aconfigurations.%20Notably%2C%20even%20in%20extreme%20scenarios%20where%20the%20attacker%20controls%0Ajust%200.1%25%20of%20the%20clients%2C%20the%20results%20indicate%20that%20successful%20backdoor%0Ainsertion%20is%20possible%20if%20the%20attacker%20can%20persist%20for%20a%20sufficient%20number%20of%0Arounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMingling%2520with%2520the%2520Good%2520to%2520Backdoor%2520Federated%2520Learning%26entry.906535625%3DNuno%2520Neves%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520decentralized%2520machine%2520learning%2520technique%2520that%250Aallows%2520multiple%2520entities%2520to%2520jointly%2520train%2520a%2520model%2520while%2520preserving%2520dataset%250Aprivacy.%2520However%252C%2520its%2520distributed%2520nature%2520has%2520raised%2520various%2520security%2520concerns%252C%250Awhich%2520have%2520been%2520addressed%2520by%2520increasingly%2520sophisticated%2520defenses.%2520These%250Aprotections%2520utilize%2520a%2520range%2520of%2520data%2520sources%2520and%2520metrics%2520to%252C%2520for%2520example%252C%2520filter%250Aout%2520malicious%2520model%2520updates%252C%2520ensuring%2520that%2520the%2520impact%2520of%2520attacks%2520is%2520minimized%250Aor%2520eliminated.%250A%2520%2520This%2520paper%2520explores%2520the%2520feasibility%2520of%2520designing%2520a%2520generic%2520attack%2520method%250Acapable%2520of%2520installing%2520backdoors%2520in%2520FL%2520while%2520evading%2520a%2520diverse%2520array%2520of%250Adefenses.%2520Specifically%252C%2520we%2520focus%2520on%2520an%2520attacker%2520strategy%2520called%2520MIGO%252C%2520which%250Aaims%2520to%2520produce%2520model%2520updates%2520that%2520subtly%2520blend%2520with%2520legitimate%2520ones.%2520The%250Aresulting%2520effect%2520is%2520a%2520gradual%2520integration%2520of%2520a%2520backdoor%2520into%2520the%2520global%2520model%252C%250Aoften%2520ensuring%2520its%2520persistence%2520long%2520after%2520the%2520attack%2520concludes%252C%2520while%250Agenerating%2520enough%2520ambiguity%2520to%2520hinder%2520the%2520effectiveness%2520of%2520defenses.%250A%2520%2520MIGO%2520was%2520employed%2520to%2520implant%2520three%2520types%2520of%2520backdoors%2520across%2520five%2520datasets%250Aand%2520different%2520model%2520architectures.%2520The%2520results%2520demonstrate%2520the%2520significant%250Athreat%2520posed%2520by%2520these%2520backdoors%252C%2520as%2520MIGO%2520consistently%2520achieved%2520exceptionally%250Ahigh%2520backdoor%2520accuracy%2520%2528exceeding%252090%2525%2529%2520while%2520maintaining%2520the%2520utility%2520of%2520the%250Amain%2520task.%2520Moreover%252C%2520MIGO%2520exhibited%2520strong%2520evasion%2520capabilities%2520against%2520ten%250Adefenses%252C%2520including%2520several%2520state-of-the-art%2520methods.%2520When%2520compared%2520to%2520four%250Aother%2520attack%2520strategies%252C%2520MIGO%2520consistently%2520outperformed%2520them%2520across%2520most%250Aconfigurations.%2520Notably%252C%2520even%2520in%2520extreme%2520scenarios%2520where%2520the%2520attacker%2520controls%250Ajust%25200.1%2525%2520of%2520the%2520clients%252C%2520the%2520results%2520indicate%2520that%2520successful%2520backdoor%250Ainsertion%2520is%2520possible%2520if%2520the%2520attacker%2520can%2520persist%2520for%2520a%2520sufficient%2520number%2520of%250Arounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mingling%20with%20the%20Good%20to%20Backdoor%20Federated%20Learning&entry.906535625=Nuno%20Neves&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20decentralized%20machine%20learning%20technique%20that%0Aallows%20multiple%20entities%20to%20jointly%20train%20a%20model%20while%20preserving%20dataset%0Aprivacy.%20However%2C%20its%20distributed%20nature%20has%20raised%20various%20security%20concerns%2C%0Awhich%20have%20been%20addressed%20by%20increasingly%20sophisticated%20defenses.%20These%0Aprotections%20utilize%20a%20range%20of%20data%20sources%20and%20metrics%20to%2C%20for%20example%2C%20filter%0Aout%20malicious%20model%20updates%2C%20ensuring%20that%20the%20impact%20of%20attacks%20is%20minimized%0Aor%20eliminated.%0A%20%20This%20paper%20explores%20the%20feasibility%20of%20designing%20a%20generic%20attack%20method%0Acapable%20of%20installing%20backdoors%20in%20FL%20while%20evading%20a%20diverse%20array%20of%0Adefenses.%20Specifically%2C%20we%20focus%20on%20an%20attacker%20strategy%20called%20MIGO%2C%20which%0Aaims%20to%20produce%20model%20updates%20that%20subtly%20blend%20with%20legitimate%20ones.%20The%0Aresulting%20effect%20is%20a%20gradual%20integration%20of%20a%20backdoor%20into%20the%20global%20model%2C%0Aoften%20ensuring%20its%20persistence%20long%20after%20the%20attack%20concludes%2C%20while%0Agenerating%20enough%20ambiguity%20to%20hinder%20the%20effectiveness%20of%20defenses.%0A%20%20MIGO%20was%20employed%20to%20implant%20three%20types%20of%20backdoors%20across%20five%20datasets%0Aand%20different%20model%20architectures.%20The%20results%20demonstrate%20the%20significant%0Athreat%20posed%20by%20these%20backdoors%2C%20as%20MIGO%20consistently%20achieved%20exceptionally%0Ahigh%20backdoor%20accuracy%20%28exceeding%2090%25%29%20while%20maintaining%20the%20utility%20of%20the%0Amain%20task.%20Moreover%2C%20MIGO%20exhibited%20strong%20evasion%20capabilities%20against%20ten%0Adefenses%2C%20including%20several%20state-of-the-art%20methods.%20When%20compared%20to%20four%0Aother%20attack%20strategies%2C%20MIGO%20consistently%20outperformed%20them%20across%20most%0Aconfigurations.%20Notably%2C%20even%20in%20extreme%20scenarios%20where%20the%20attacker%20controls%0Ajust%200.1%25%20of%20the%20clients%2C%20the%20results%20indicate%20that%20successful%20backdoor%0Ainsertion%20is%20possible%20if%20the%20attacker%20can%20persist%20for%20a%20sufficient%20number%20of%0Arounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01913v1&entry.124074799=Read"},
{"title": "Cold-Start Recommendation towards the Era of Large Language Models\n  (LLMs): A Comprehensive Survey and Roadmap", "author": "Weizhi Zhang and Yuanchen Bei and Liangwei Yang and Henry Peng Zou and Peilin Zhou and Aiwei Liu and Yinghui Li and Hao Chen and Jianling Wang and Yu Wang and Feiran Huang and Sheng Zhou and Jiajun Bu and Allen Lin and James Caverlee and Fakhri Karray and Irwin King and Philip S. Yu", "abstract": "  Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.\n", "link": "http://arxiv.org/abs/2501.01945v1", "date": "2025-01-03", "relevancy": 1.7463, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4371}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cold-Start%20Recommendation%20towards%20the%20Era%20of%20Large%20Language%20Models%0A%20%20%28LLMs%29%3A%20A%20Comprehensive%20Survey%20and%20Roadmap&body=Title%3A%20Cold-Start%20Recommendation%20towards%20the%20Era%20of%20Large%20Language%20Models%0A%20%20%28LLMs%29%3A%20A%20Comprehensive%20Survey%20and%20Roadmap%0AAuthor%3A%20Weizhi%20Zhang%20and%20Yuanchen%20Bei%20and%20Liangwei%20Yang%20and%20Henry%20Peng%20Zou%20and%20Peilin%20Zhou%20and%20Aiwei%20Liu%20and%20Yinghui%20Li%20and%20Hao%20Chen%20and%20Jianling%20Wang%20and%20Yu%20Wang%20and%20Feiran%20Huang%20and%20Sheng%20Zhou%20and%20Jiajun%20Bu%20and%20Allen%20Lin%20and%20James%20Caverlee%20and%20Fakhri%20Karray%20and%20Irwin%20King%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20Cold-start%20problem%20is%20one%20of%20the%20long-standing%20challenges%20in%20recommender%0Asystems%2C%20focusing%20on%20accurately%20modeling%20new%20or%20interaction-limited%20users%20or%0Aitems%20to%20provide%20better%20recommendations.%20Due%20to%20the%20diversification%20of%20internet%0Aplatforms%20and%20the%20exponential%20growth%20of%20users%20and%20items%2C%20the%20importance%20of%0Acold-start%20recommendation%20%28CSR%29%20is%20becoming%20increasingly%20evident.%20At%20the%20same%0Atime%2C%20large%20language%20models%20%28LLMs%29%20have%20achieved%20tremendous%20success%20and%20possess%0Astrong%20capabilities%20in%20modeling%20user%20and%20item%20information%2C%20providing%20new%0Apotential%20for%20cold-start%20recommendations.%20However%2C%20the%20research%20community%20on%0ACSR%20still%20lacks%20a%20comprehensive%20review%20and%20reflection%20in%20this%20field.%20Based%20on%0Athis%2C%20in%20this%20paper%2C%20we%20stand%20in%20the%20context%20of%20the%20era%20of%20large%20language%0Amodels%20and%20provide%20a%20comprehensive%20review%20and%20discussion%20on%20the%20roadmap%2C%0Arelated%20literature%2C%20and%20future%20directions%20of%20CSR.%20Specifically%2C%20we%20have%0Aconducted%20an%20exploration%20of%20the%20development%20path%20of%20how%20existing%20CSR%20utilizes%0Ainformation%2C%20from%20content%20features%2C%20graph%20relations%2C%20and%20domain%20information%2C%20to%0Athe%20world%20knowledge%20possessed%20by%20large%20language%20models%2C%20aiming%20to%20provide%20new%0Ainsights%20for%20both%20the%20research%20and%20industrial%20communities%20on%20CSR.%20Related%0Aresources%20of%20cold-start%20recommendations%20are%20collected%20and%20continuously%20updated%0Afor%20the%20community%20in%0Ahttps%3A//github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCold-Start%2520Recommendation%2520towards%2520the%2520Era%2520of%2520Large%2520Language%2520Models%250A%2520%2520%2528LLMs%2529%253A%2520A%2520Comprehensive%2520Survey%2520and%2520Roadmap%26entry.906535625%3DWeizhi%2520Zhang%2520and%2520Yuanchen%2520Bei%2520and%2520Liangwei%2520Yang%2520and%2520Henry%2520Peng%2520Zou%2520and%2520Peilin%2520Zhou%2520and%2520Aiwei%2520Liu%2520and%2520Yinghui%2520Li%2520and%2520Hao%2520Chen%2520and%2520Jianling%2520Wang%2520and%2520Yu%2520Wang%2520and%2520Feiran%2520Huang%2520and%2520Sheng%2520Zhou%2520and%2520Jiajun%2520Bu%2520and%2520Allen%2520Lin%2520and%2520James%2520Caverlee%2520and%2520Fakhri%2520Karray%2520and%2520Irwin%2520King%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520Cold-start%2520problem%2520is%2520one%2520of%2520the%2520long-standing%2520challenges%2520in%2520recommender%250Asystems%252C%2520focusing%2520on%2520accurately%2520modeling%2520new%2520or%2520interaction-limited%2520users%2520or%250Aitems%2520to%2520provide%2520better%2520recommendations.%2520Due%2520to%2520the%2520diversification%2520of%2520internet%250Aplatforms%2520and%2520the%2520exponential%2520growth%2520of%2520users%2520and%2520items%252C%2520the%2520importance%2520of%250Acold-start%2520recommendation%2520%2528CSR%2529%2520is%2520becoming%2520increasingly%2520evident.%2520At%2520the%2520same%250Atime%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520tremendous%2520success%2520and%2520possess%250Astrong%2520capabilities%2520in%2520modeling%2520user%2520and%2520item%2520information%252C%2520providing%2520new%250Apotential%2520for%2520cold-start%2520recommendations.%2520However%252C%2520the%2520research%2520community%2520on%250ACSR%2520still%2520lacks%2520a%2520comprehensive%2520review%2520and%2520reflection%2520in%2520this%2520field.%2520Based%2520on%250Athis%252C%2520in%2520this%2520paper%252C%2520we%2520stand%2520in%2520the%2520context%2520of%2520the%2520era%2520of%2520large%2520language%250Amodels%2520and%2520provide%2520a%2520comprehensive%2520review%2520and%2520discussion%2520on%2520the%2520roadmap%252C%250Arelated%2520literature%252C%2520and%2520future%2520directions%2520of%2520CSR.%2520Specifically%252C%2520we%2520have%250Aconducted%2520an%2520exploration%2520of%2520the%2520development%2520path%2520of%2520how%2520existing%2520CSR%2520utilizes%250Ainformation%252C%2520from%2520content%2520features%252C%2520graph%2520relations%252C%2520and%2520domain%2520information%252C%2520to%250Athe%2520world%2520knowledge%2520possessed%2520by%2520large%2520language%2520models%252C%2520aiming%2520to%2520provide%2520new%250Ainsights%2520for%2520both%2520the%2520research%2520and%2520industrial%2520communities%2520on%2520CSR.%2520Related%250Aresources%2520of%2520cold-start%2520recommendations%2520are%2520collected%2520and%2520continuously%2520updated%250Afor%2520the%2520community%2520in%250Ahttps%253A//github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cold-Start%20Recommendation%20towards%20the%20Era%20of%20Large%20Language%20Models%0A%20%20%28LLMs%29%3A%20A%20Comprehensive%20Survey%20and%20Roadmap&entry.906535625=Weizhi%20Zhang%20and%20Yuanchen%20Bei%20and%20Liangwei%20Yang%20and%20Henry%20Peng%20Zou%20and%20Peilin%20Zhou%20and%20Aiwei%20Liu%20and%20Yinghui%20Li%20and%20Hao%20Chen%20and%20Jianling%20Wang%20and%20Yu%20Wang%20and%20Feiran%20Huang%20and%20Sheng%20Zhou%20and%20Jiajun%20Bu%20and%20Allen%20Lin%20and%20James%20Caverlee%20and%20Fakhri%20Karray%20and%20Irwin%20King%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20Cold-start%20problem%20is%20one%20of%20the%20long-standing%20challenges%20in%20recommender%0Asystems%2C%20focusing%20on%20accurately%20modeling%20new%20or%20interaction-limited%20users%20or%0Aitems%20to%20provide%20better%20recommendations.%20Due%20to%20the%20diversification%20of%20internet%0Aplatforms%20and%20the%20exponential%20growth%20of%20users%20and%20items%2C%20the%20importance%20of%0Acold-start%20recommendation%20%28CSR%29%20is%20becoming%20increasingly%20evident.%20At%20the%20same%0Atime%2C%20large%20language%20models%20%28LLMs%29%20have%20achieved%20tremendous%20success%20and%20possess%0Astrong%20capabilities%20in%20modeling%20user%20and%20item%20information%2C%20providing%20new%0Apotential%20for%20cold-start%20recommendations.%20However%2C%20the%20research%20community%20on%0ACSR%20still%20lacks%20a%20comprehensive%20review%20and%20reflection%20in%20this%20field.%20Based%20on%0Athis%2C%20in%20this%20paper%2C%20we%20stand%20in%20the%20context%20of%20the%20era%20of%20large%20language%0Amodels%20and%20provide%20a%20comprehensive%20review%20and%20discussion%20on%20the%20roadmap%2C%0Arelated%20literature%2C%20and%20future%20directions%20of%20CSR.%20Specifically%2C%20we%20have%0Aconducted%20an%20exploration%20of%20the%20development%20path%20of%20how%20existing%20CSR%20utilizes%0Ainformation%2C%20from%20content%20features%2C%20graph%20relations%2C%20and%20domain%20information%2C%20to%0Athe%20world%20knowledge%20possessed%20by%20large%20language%20models%2C%20aiming%20to%20provide%20new%0Ainsights%20for%20both%20the%20research%20and%20industrial%20communities%20on%20CSR.%20Related%0Aresources%20of%20cold-start%20recommendations%20are%20collected%20and%20continuously%20updated%0Afor%20the%20community%20in%0Ahttps%3A//github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01945v1&entry.124074799=Read"},
{"title": "Quantum Channel Learning", "author": "Mikhail Gennadievich Belov and Victor Victorovich Dubov and Alexey Vladimirovich Filimonov and Vladislav Gennadievich Malyshkin", "abstract": "  The problem of an optimal mapping between Hilbert spaces $IN$ and $OUT$,\nbased on a series of density matrix mapping measurements $\\rho^{(l)} \\to\n\\varrho^{(l)}$, $l=1\\dots M$, is formulated as an optimization problem\nmaximizing the total fidelity $\\mathcal{F}=\\sum_{l=1}^{M} \\omega^{(l)}\nF\\left(\\varrho^{(l)},\\sum_s B_s \\rho^{(l)} B^{\\dagger}_s\\right)$ subject to\nprobability preservation constraints on Kraus operators $B_s$. For\n$F(\\varrho,\\sigma)$ in the form that total fidelity can be represented as a\nquadratic form with superoperator $\\mathcal{F}=\\sum_s\\left\\langle\nB_s\\middle|S\\middle| B_s \\right\\rangle$ (either exactly or as an approximation)\nan iterative algorithm is developed. The work introduces two important\ngeneralizations of unitary learning: 1. $IN$/$OUT$ states are represented as\ndensity matrices. 2. The mapping itself is formulated as a mixed unitary\nquantum channel $A^{OUT}=\\sum_s |w_s|^2 \\mathcal{U}_s A^{IN}\n\\mathcal{U}_s^{\\dagger}$ (no general quantum channel yet). This marks a crucial\nadvancement from the commonly studied unitary mapping of pure states\n$\\phi_l=\\mathcal{U} \\psi_l$ to a quantum channel, what allows us to distinguish\nprobabilistic mixture of states and their superposition. An application of the\napproach is demonstrated on unitary learning of density matrix mapping\n$\\varrho^{(l)}=\\mathcal{U} \\rho^{(l)} \\mathcal{U}^{\\dagger}$, in this case a\nquadratic on $\\mathcal{U}$ fidelity can be constructed by considering\n$\\sqrt{\\rho^{(l)}} \\to \\sqrt{\\varrho^{(l)}}$ mapping, and on a quantum channel,\nwhere quadratic on $B_s$ fidelity is an approximation -- a quantum channel is\nthen obtained as a hierarchy of unitary mappings, a mixed unitary channel. The\napproach can be applied to studying quantum inverse problems, variational\nquantum algorithms, quantum tomography, and more.\n", "link": "http://arxiv.org/abs/2407.04406v2", "date": "2025-01-03", "relevancy": 1.7319, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4626}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4302}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Channel%20Learning&body=Title%3A%20Quantum%20Channel%20Learning%0AAuthor%3A%20Mikhail%20Gennadievich%20Belov%20and%20Victor%20Victorovich%20Dubov%20and%20Alexey%20Vladimirovich%20Filimonov%20and%20Vladislav%20Gennadievich%20Malyshkin%0AAbstract%3A%20%20%20The%20problem%20of%20an%20optimal%20mapping%20between%20Hilbert%20spaces%20%24IN%24%20and%20%24OUT%24%2C%0Abased%20on%20a%20series%20of%20density%20matrix%20mapping%20measurements%20%24%5Crho%5E%7B%28l%29%7D%20%5Cto%0A%5Cvarrho%5E%7B%28l%29%7D%24%2C%20%24l%3D1%5Cdots%20M%24%2C%20is%20formulated%20as%20an%20optimization%20problem%0Amaximizing%20the%20total%20fidelity%20%24%5Cmathcal%7BF%7D%3D%5Csum_%7Bl%3D1%7D%5E%7BM%7D%20%5Comega%5E%7B%28l%29%7D%0AF%5Cleft%28%5Cvarrho%5E%7B%28l%29%7D%2C%5Csum_s%20B_s%20%5Crho%5E%7B%28l%29%7D%20B%5E%7B%5Cdagger%7D_s%5Cright%29%24%20subject%20to%0Aprobability%20preservation%20constraints%20on%20Kraus%20operators%20%24B_s%24.%20For%0A%24F%28%5Cvarrho%2C%5Csigma%29%24%20in%20the%20form%20that%20total%20fidelity%20can%20be%20represented%20as%20a%0Aquadratic%20form%20with%20superoperator%20%24%5Cmathcal%7BF%7D%3D%5Csum_s%5Cleft%5Clangle%0AB_s%5Cmiddle%7CS%5Cmiddle%7C%20B_s%20%5Cright%5Crangle%24%20%28either%20exactly%20or%20as%20an%20approximation%29%0Aan%20iterative%20algorithm%20is%20developed.%20The%20work%20introduces%20two%20important%0Ageneralizations%20of%20unitary%20learning%3A%201.%20%24IN%24/%24OUT%24%20states%20are%20represented%20as%0Adensity%20matrices.%202.%20The%20mapping%20itself%20is%20formulated%20as%20a%20mixed%20unitary%0Aquantum%20channel%20%24A%5E%7BOUT%7D%3D%5Csum_s%20%7Cw_s%7C%5E2%20%5Cmathcal%7BU%7D_s%20A%5E%7BIN%7D%0A%5Cmathcal%7BU%7D_s%5E%7B%5Cdagger%7D%24%20%28no%20general%20quantum%20channel%20yet%29.%20This%20marks%20a%20crucial%0Aadvancement%20from%20the%20commonly%20studied%20unitary%20mapping%20of%20pure%20states%0A%24%5Cphi_l%3D%5Cmathcal%7BU%7D%20%5Cpsi_l%24%20to%20a%20quantum%20channel%2C%20what%20allows%20us%20to%20distinguish%0Aprobabilistic%20mixture%20of%20states%20and%20their%20superposition.%20An%20application%20of%20the%0Aapproach%20is%20demonstrated%20on%20unitary%20learning%20of%20density%20matrix%20mapping%0A%24%5Cvarrho%5E%7B%28l%29%7D%3D%5Cmathcal%7BU%7D%20%5Crho%5E%7B%28l%29%7D%20%5Cmathcal%7BU%7D%5E%7B%5Cdagger%7D%24%2C%20in%20this%20case%20a%0Aquadratic%20on%20%24%5Cmathcal%7BU%7D%24%20fidelity%20can%20be%20constructed%20by%20considering%0A%24%5Csqrt%7B%5Crho%5E%7B%28l%29%7D%7D%20%5Cto%20%5Csqrt%7B%5Cvarrho%5E%7B%28l%29%7D%7D%24%20mapping%2C%20and%20on%20a%20quantum%20channel%2C%0Awhere%20quadratic%20on%20%24B_s%24%20fidelity%20is%20an%20approximation%20--%20a%20quantum%20channel%20is%0Athen%20obtained%20as%20a%20hierarchy%20of%20unitary%20mappings%2C%20a%20mixed%20unitary%20channel.%20The%0Aapproach%20can%20be%20applied%20to%20studying%20quantum%20inverse%20problems%2C%20variational%0Aquantum%20algorithms%2C%20quantum%20tomography%2C%20and%20more.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04406v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Channel%2520Learning%26entry.906535625%3DMikhail%2520Gennadievich%2520Belov%2520and%2520Victor%2520Victorovich%2520Dubov%2520and%2520Alexey%2520Vladimirovich%2520Filimonov%2520and%2520Vladislav%2520Gennadievich%2520Malyshkin%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520an%2520optimal%2520mapping%2520between%2520Hilbert%2520spaces%2520%2524IN%2524%2520and%2520%2524OUT%2524%252C%250Abased%2520on%2520a%2520series%2520of%2520density%2520matrix%2520mapping%2520measurements%2520%2524%255Crho%255E%257B%2528l%2529%257D%2520%255Cto%250A%255Cvarrho%255E%257B%2528l%2529%257D%2524%252C%2520%2524l%253D1%255Cdots%2520M%2524%252C%2520is%2520formulated%2520as%2520an%2520optimization%2520problem%250Amaximizing%2520the%2520total%2520fidelity%2520%2524%255Cmathcal%257BF%257D%253D%255Csum_%257Bl%253D1%257D%255E%257BM%257D%2520%255Comega%255E%257B%2528l%2529%257D%250AF%255Cleft%2528%255Cvarrho%255E%257B%2528l%2529%257D%252C%255Csum_s%2520B_s%2520%255Crho%255E%257B%2528l%2529%257D%2520B%255E%257B%255Cdagger%257D_s%255Cright%2529%2524%2520subject%2520to%250Aprobability%2520preservation%2520constraints%2520on%2520Kraus%2520operators%2520%2524B_s%2524.%2520For%250A%2524F%2528%255Cvarrho%252C%255Csigma%2529%2524%2520in%2520the%2520form%2520that%2520total%2520fidelity%2520can%2520be%2520represented%2520as%2520a%250Aquadratic%2520form%2520with%2520superoperator%2520%2524%255Cmathcal%257BF%257D%253D%255Csum_s%255Cleft%255Clangle%250AB_s%255Cmiddle%257CS%255Cmiddle%257C%2520B_s%2520%255Cright%255Crangle%2524%2520%2528either%2520exactly%2520or%2520as%2520an%2520approximation%2529%250Aan%2520iterative%2520algorithm%2520is%2520developed.%2520The%2520work%2520introduces%2520two%2520important%250Ageneralizations%2520of%2520unitary%2520learning%253A%25201.%2520%2524IN%2524/%2524OUT%2524%2520states%2520are%2520represented%2520as%250Adensity%2520matrices.%25202.%2520The%2520mapping%2520itself%2520is%2520formulated%2520as%2520a%2520mixed%2520unitary%250Aquantum%2520channel%2520%2524A%255E%257BOUT%257D%253D%255Csum_s%2520%257Cw_s%257C%255E2%2520%255Cmathcal%257BU%257D_s%2520A%255E%257BIN%257D%250A%255Cmathcal%257BU%257D_s%255E%257B%255Cdagger%257D%2524%2520%2528no%2520general%2520quantum%2520channel%2520yet%2529.%2520This%2520marks%2520a%2520crucial%250Aadvancement%2520from%2520the%2520commonly%2520studied%2520unitary%2520mapping%2520of%2520pure%2520states%250A%2524%255Cphi_l%253D%255Cmathcal%257BU%257D%2520%255Cpsi_l%2524%2520to%2520a%2520quantum%2520channel%252C%2520what%2520allows%2520us%2520to%2520distinguish%250Aprobabilistic%2520mixture%2520of%2520states%2520and%2520their%2520superposition.%2520An%2520application%2520of%2520the%250Aapproach%2520is%2520demonstrated%2520on%2520unitary%2520learning%2520of%2520density%2520matrix%2520mapping%250A%2524%255Cvarrho%255E%257B%2528l%2529%257D%253D%255Cmathcal%257BU%257D%2520%255Crho%255E%257B%2528l%2529%257D%2520%255Cmathcal%257BU%257D%255E%257B%255Cdagger%257D%2524%252C%2520in%2520this%2520case%2520a%250Aquadratic%2520on%2520%2524%255Cmathcal%257BU%257D%2524%2520fidelity%2520can%2520be%2520constructed%2520by%2520considering%250A%2524%255Csqrt%257B%255Crho%255E%257B%2528l%2529%257D%257D%2520%255Cto%2520%255Csqrt%257B%255Cvarrho%255E%257B%2528l%2529%257D%257D%2524%2520mapping%252C%2520and%2520on%2520a%2520quantum%2520channel%252C%250Awhere%2520quadratic%2520on%2520%2524B_s%2524%2520fidelity%2520is%2520an%2520approximation%2520--%2520a%2520quantum%2520channel%2520is%250Athen%2520obtained%2520as%2520a%2520hierarchy%2520of%2520unitary%2520mappings%252C%2520a%2520mixed%2520unitary%2520channel.%2520The%250Aapproach%2520can%2520be%2520applied%2520to%2520studying%2520quantum%2520inverse%2520problems%252C%2520variational%250Aquantum%2520algorithms%252C%2520quantum%2520tomography%252C%2520and%2520more.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04406v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Channel%20Learning&entry.906535625=Mikhail%20Gennadievich%20Belov%20and%20Victor%20Victorovich%20Dubov%20and%20Alexey%20Vladimirovich%20Filimonov%20and%20Vladislav%20Gennadievich%20Malyshkin&entry.1292438233=%20%20The%20problem%20of%20an%20optimal%20mapping%20between%20Hilbert%20spaces%20%24IN%24%20and%20%24OUT%24%2C%0Abased%20on%20a%20series%20of%20density%20matrix%20mapping%20measurements%20%24%5Crho%5E%7B%28l%29%7D%20%5Cto%0A%5Cvarrho%5E%7B%28l%29%7D%24%2C%20%24l%3D1%5Cdots%20M%24%2C%20is%20formulated%20as%20an%20optimization%20problem%0Amaximizing%20the%20total%20fidelity%20%24%5Cmathcal%7BF%7D%3D%5Csum_%7Bl%3D1%7D%5E%7BM%7D%20%5Comega%5E%7B%28l%29%7D%0AF%5Cleft%28%5Cvarrho%5E%7B%28l%29%7D%2C%5Csum_s%20B_s%20%5Crho%5E%7B%28l%29%7D%20B%5E%7B%5Cdagger%7D_s%5Cright%29%24%20subject%20to%0Aprobability%20preservation%20constraints%20on%20Kraus%20operators%20%24B_s%24.%20For%0A%24F%28%5Cvarrho%2C%5Csigma%29%24%20in%20the%20form%20that%20total%20fidelity%20can%20be%20represented%20as%20a%0Aquadratic%20form%20with%20superoperator%20%24%5Cmathcal%7BF%7D%3D%5Csum_s%5Cleft%5Clangle%0AB_s%5Cmiddle%7CS%5Cmiddle%7C%20B_s%20%5Cright%5Crangle%24%20%28either%20exactly%20or%20as%20an%20approximation%29%0Aan%20iterative%20algorithm%20is%20developed.%20The%20work%20introduces%20two%20important%0Ageneralizations%20of%20unitary%20learning%3A%201.%20%24IN%24/%24OUT%24%20states%20are%20represented%20as%0Adensity%20matrices.%202.%20The%20mapping%20itself%20is%20formulated%20as%20a%20mixed%20unitary%0Aquantum%20channel%20%24A%5E%7BOUT%7D%3D%5Csum_s%20%7Cw_s%7C%5E2%20%5Cmathcal%7BU%7D_s%20A%5E%7BIN%7D%0A%5Cmathcal%7BU%7D_s%5E%7B%5Cdagger%7D%24%20%28no%20general%20quantum%20channel%20yet%29.%20This%20marks%20a%20crucial%0Aadvancement%20from%20the%20commonly%20studied%20unitary%20mapping%20of%20pure%20states%0A%24%5Cphi_l%3D%5Cmathcal%7BU%7D%20%5Cpsi_l%24%20to%20a%20quantum%20channel%2C%20what%20allows%20us%20to%20distinguish%0Aprobabilistic%20mixture%20of%20states%20and%20their%20superposition.%20An%20application%20of%20the%0Aapproach%20is%20demonstrated%20on%20unitary%20learning%20of%20density%20matrix%20mapping%0A%24%5Cvarrho%5E%7B%28l%29%7D%3D%5Cmathcal%7BU%7D%20%5Crho%5E%7B%28l%29%7D%20%5Cmathcal%7BU%7D%5E%7B%5Cdagger%7D%24%2C%20in%20this%20case%20a%0Aquadratic%20on%20%24%5Cmathcal%7BU%7D%24%20fidelity%20can%20be%20constructed%20by%20considering%0A%24%5Csqrt%7B%5Crho%5E%7B%28l%29%7D%7D%20%5Cto%20%5Csqrt%7B%5Cvarrho%5E%7B%28l%29%7D%7D%24%20mapping%2C%20and%20on%20a%20quantum%20channel%2C%0Awhere%20quadratic%20on%20%24B_s%24%20fidelity%20is%20an%20approximation%20--%20a%20quantum%20channel%20is%0Athen%20obtained%20as%20a%20hierarchy%20of%20unitary%20mappings%2C%20a%20mixed%20unitary%20channel.%20The%0Aapproach%20can%20be%20applied%20to%20studying%20quantum%20inverse%20problems%2C%20variational%0Aquantum%20algorithms%2C%20quantum%20tomography%2C%20and%20more.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04406v2&entry.124074799=Read"},
{"title": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness", "author": "Jingxiang Guo and Jiayu Luo and Zhenyu Wei and Yiwen Hou and Zhixuan Xu and Xiaoyi Lin and Chongkai Gao and Lin Shao", "abstract": "  Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website https://nus-lins-lab.github.io/telepreview/.\n", "link": "http://arxiv.org/abs/2412.13548v2", "date": "2025-01-03", "relevancy": 1.7183, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5859}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5634}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TelePreview%3A%20A%20User-Friendly%20Teleoperation%20System%20with%20Virtual%20Arm%0A%20%20Assistance%20for%20Enhanced%20Effectiveness&body=Title%3A%20TelePreview%3A%20A%20User-Friendly%20Teleoperation%20System%20with%20Virtual%20Arm%0A%20%20Assistance%20for%20Enhanced%20Effectiveness%0AAuthor%3A%20Jingxiang%20Guo%20and%20Jiayu%20Luo%20and%20Zhenyu%20Wei%20and%20Yiwen%20Hou%20and%20Zhixuan%20Xu%20and%20Xiaoyi%20Lin%20and%20Chongkai%20Gao%20and%20Lin%20Shao%0AAbstract%3A%20%20%20Teleoperation%20provides%20an%20effective%20way%20to%20collect%20robot%20data%2C%20which%20is%0Acrucial%20for%20learning%20from%20demonstrations.%20In%20this%20field%2C%20teleoperation%20faces%0Aseveral%20key%20challenges%3A%20user-friendliness%20for%20new%20users%2C%20safety%20assurance%2C%20and%0Atransferability%20across%20different%20platforms.%20While%20collecting%20real%20robot%0Adexterous%20manipulation%20data%20by%20teleoperation%20to%20train%20robots%20has%20shown%0Aimpressive%20results%20on%20diverse%20tasks%2C%20due%20to%20the%20morphological%20differences%0Abetween%20human%20and%20robot%20hands%2C%20it%20is%20not%20only%20hard%20for%20new%20users%20to%20understand%0Athe%20action%20mapping%20but%20also%20raises%20potential%20safety%20concerns%20during%20operation.%0ATo%20address%20these%20limitations%2C%20we%20introduce%20TelePreview.%20This%20teleoperation%0Asystem%20offers%20real-time%20visual%20feedback%20on%20robot%20actions%20based%20on%20human%20user%0Ainputs%2C%20with%20a%20total%20hardware%20cost%20of%20less%20than%20%241%2C000.%20TelePreview%20allows%20the%0Auser%20to%20see%20a%20virtual%20robot%20that%20represents%20the%20outcome%20of%20the%20user%27s%20next%0Amovement.%20By%20enabling%20flexible%20switching%20between%20command%20visualization%20and%0Aactual%20execution%2C%20this%20system%20helps%20new%20users%20learn%20how%20to%20demonstrate%20quickly%0Aand%20safely.%20We%20demonstrate%20that%20it%20outperforms%20other%20teleoperation%20systems%0Aacross%20five%20tasks%2C%20emphasize%20its%20ease%20of%20use%2C%20and%20highlight%20its%20straightforward%0Adeployment%20across%20diverse%20robotic%20platforms.%20We%20release%20our%20code%20and%20a%0Adeployment%20document%20on%20our%20website%20https%3A//nus-lins-lab.github.io/telepreview/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13548v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTelePreview%253A%2520A%2520User-Friendly%2520Teleoperation%2520System%2520with%2520Virtual%2520Arm%250A%2520%2520Assistance%2520for%2520Enhanced%2520Effectiveness%26entry.906535625%3DJingxiang%2520Guo%2520and%2520Jiayu%2520Luo%2520and%2520Zhenyu%2520Wei%2520and%2520Yiwen%2520Hou%2520and%2520Zhixuan%2520Xu%2520and%2520Xiaoyi%2520Lin%2520and%2520Chongkai%2520Gao%2520and%2520Lin%2520Shao%26entry.1292438233%3D%2520%2520Teleoperation%2520provides%2520an%2520effective%2520way%2520to%2520collect%2520robot%2520data%252C%2520which%2520is%250Acrucial%2520for%2520learning%2520from%2520demonstrations.%2520In%2520this%2520field%252C%2520teleoperation%2520faces%250Aseveral%2520key%2520challenges%253A%2520user-friendliness%2520for%2520new%2520users%252C%2520safety%2520assurance%252C%2520and%250Atransferability%2520across%2520different%2520platforms.%2520While%2520collecting%2520real%2520robot%250Adexterous%2520manipulation%2520data%2520by%2520teleoperation%2520to%2520train%2520robots%2520has%2520shown%250Aimpressive%2520results%2520on%2520diverse%2520tasks%252C%2520due%2520to%2520the%2520morphological%2520differences%250Abetween%2520human%2520and%2520robot%2520hands%252C%2520it%2520is%2520not%2520only%2520hard%2520for%2520new%2520users%2520to%2520understand%250Athe%2520action%2520mapping%2520but%2520also%2520raises%2520potential%2520safety%2520concerns%2520during%2520operation.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520TelePreview.%2520This%2520teleoperation%250Asystem%2520offers%2520real-time%2520visual%2520feedback%2520on%2520robot%2520actions%2520based%2520on%2520human%2520user%250Ainputs%252C%2520with%2520a%2520total%2520hardware%2520cost%2520of%2520less%2520than%2520%25241%252C000.%2520TelePreview%2520allows%2520the%250Auser%2520to%2520see%2520a%2520virtual%2520robot%2520that%2520represents%2520the%2520outcome%2520of%2520the%2520user%2527s%2520next%250Amovement.%2520By%2520enabling%2520flexible%2520switching%2520between%2520command%2520visualization%2520and%250Aactual%2520execution%252C%2520this%2520system%2520helps%2520new%2520users%2520learn%2520how%2520to%2520demonstrate%2520quickly%250Aand%2520safely.%2520We%2520demonstrate%2520that%2520it%2520outperforms%2520other%2520teleoperation%2520systems%250Aacross%2520five%2520tasks%252C%2520emphasize%2520its%2520ease%2520of%2520use%252C%2520and%2520highlight%2520its%2520straightforward%250Adeployment%2520across%2520diverse%2520robotic%2520platforms.%2520We%2520release%2520our%2520code%2520and%2520a%250Adeployment%2520document%2520on%2520our%2520website%2520https%253A//nus-lins-lab.github.io/telepreview/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13548v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TelePreview%3A%20A%20User-Friendly%20Teleoperation%20System%20with%20Virtual%20Arm%0A%20%20Assistance%20for%20Enhanced%20Effectiveness&entry.906535625=Jingxiang%20Guo%20and%20Jiayu%20Luo%20and%20Zhenyu%20Wei%20and%20Yiwen%20Hou%20and%20Zhixuan%20Xu%20and%20Xiaoyi%20Lin%20and%20Chongkai%20Gao%20and%20Lin%20Shao&entry.1292438233=%20%20Teleoperation%20provides%20an%20effective%20way%20to%20collect%20robot%20data%2C%20which%20is%0Acrucial%20for%20learning%20from%20demonstrations.%20In%20this%20field%2C%20teleoperation%20faces%0Aseveral%20key%20challenges%3A%20user-friendliness%20for%20new%20users%2C%20safety%20assurance%2C%20and%0Atransferability%20across%20different%20platforms.%20While%20collecting%20real%20robot%0Adexterous%20manipulation%20data%20by%20teleoperation%20to%20train%20robots%20has%20shown%0Aimpressive%20results%20on%20diverse%20tasks%2C%20due%20to%20the%20morphological%20differences%0Abetween%20human%20and%20robot%20hands%2C%20it%20is%20not%20only%20hard%20for%20new%20users%20to%20understand%0Athe%20action%20mapping%20but%20also%20raises%20potential%20safety%20concerns%20during%20operation.%0ATo%20address%20these%20limitations%2C%20we%20introduce%20TelePreview.%20This%20teleoperation%0Asystem%20offers%20real-time%20visual%20feedback%20on%20robot%20actions%20based%20on%20human%20user%0Ainputs%2C%20with%20a%20total%20hardware%20cost%20of%20less%20than%20%241%2C000.%20TelePreview%20allows%20the%0Auser%20to%20see%20a%20virtual%20robot%20that%20represents%20the%20outcome%20of%20the%20user%27s%20next%0Amovement.%20By%20enabling%20flexible%20switching%20between%20command%20visualization%20and%0Aactual%20execution%2C%20this%20system%20helps%20new%20users%20learn%20how%20to%20demonstrate%20quickly%0Aand%20safely.%20We%20demonstrate%20that%20it%20outperforms%20other%20teleoperation%20systems%0Aacross%20five%20tasks%2C%20emphasize%20its%20ease%20of%20use%2C%20and%20highlight%20its%20straightforward%0Adeployment%20across%20diverse%20robotic%20platforms.%20We%20release%20our%20code%20and%20a%0Adeployment%20document%20on%20our%20website%20https%3A//nus-lins-lab.github.io/telepreview/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13548v2&entry.124074799=Read"},
{"title": "InvSeg: Test-Time Prompt Inversion for Semantic Segmentation", "author": "Jiayi Lin and Jiabo Huang and Jian Hu and Shaogang Gong", "abstract": "  Visual-textual correlations in the attention maps derived from text-to-image\ndiffusion models are proven beneficial to dense visual prediction tasks, e.g.,\nsemantic segmentation. However, a significant challenge arises due to the input\ndistributional discrepancy between the context-rich sentences used for image\ngeneration and the isolated class names typically used in semantic\nsegmentation. This discrepancy hinders diffusion models from capturing accurate\nvisual-textual correlations. To solve this, we propose InvSeg, a test-time\nprompt inversion method that tackles open-vocabulary semantic segmentation by\ninverting image-specific visual context into text prompt embedding space,\nleveraging structure information derived from the diffusion model's\nreconstruction process to enrich text prompts so as to associate each class\nwith a structure-consistent mask. Specifically, we introduce Contrastive Soft\nClustering (CSC) to align derived masks with the image's structure information,\nsoftly selecting anchors for each class and calculating weighted distances to\npush inner-class pixels closer while separating inter-class pixels, thereby\nensuring mask distinction and internal consistency. By incorporating\nsample-specific context, InvSeg learns context-rich text prompts in embedding\nspace and achieves accurate semantic alignment across modalities. Experiments\nshow that InvSeg achieves state-of-the-art performance on the PASCAL VOC,\nPASCAL Context and COCO Object datasets.\n", "link": "http://arxiv.org/abs/2410.11473v2", "date": "2025-01-03", "relevancy": 1.6471, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5655}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5514}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InvSeg%3A%20Test-Time%20Prompt%20Inversion%20for%20Semantic%20Segmentation&body=Title%3A%20InvSeg%3A%20Test-Time%20Prompt%20Inversion%20for%20Semantic%20Segmentation%0AAuthor%3A%20Jiayi%20Lin%20and%20Jiabo%20Huang%20and%20Jian%20Hu%20and%20Shaogang%20Gong%0AAbstract%3A%20%20%20Visual-textual%20correlations%20in%20the%20attention%20maps%20derived%20from%20text-to-image%0Adiffusion%20models%20are%20proven%20beneficial%20to%20dense%20visual%20prediction%20tasks%2C%20e.g.%2C%0Asemantic%20segmentation.%20However%2C%20a%20significant%20challenge%20arises%20due%20to%20the%20input%0Adistributional%20discrepancy%20between%20the%20context-rich%20sentences%20used%20for%20image%0Ageneration%20and%20the%20isolated%20class%20names%20typically%20used%20in%20semantic%0Asegmentation.%20This%20discrepancy%20hinders%20diffusion%20models%20from%20capturing%20accurate%0Avisual-textual%20correlations.%20To%20solve%20this%2C%20we%20propose%20InvSeg%2C%20a%20test-time%0Aprompt%20inversion%20method%20that%20tackles%20open-vocabulary%20semantic%20segmentation%20by%0Ainverting%20image-specific%20visual%20context%20into%20text%20prompt%20embedding%20space%2C%0Aleveraging%20structure%20information%20derived%20from%20the%20diffusion%20model%27s%0Areconstruction%20process%20to%20enrich%20text%20prompts%20so%20as%20to%20associate%20each%20class%0Awith%20a%20structure-consistent%20mask.%20Specifically%2C%20we%20introduce%20Contrastive%20Soft%0AClustering%20%28CSC%29%20to%20align%20derived%20masks%20with%20the%20image%27s%20structure%20information%2C%0Asoftly%20selecting%20anchors%20for%20each%20class%20and%20calculating%20weighted%20distances%20to%0Apush%20inner-class%20pixels%20closer%20while%20separating%20inter-class%20pixels%2C%20thereby%0Aensuring%20mask%20distinction%20and%20internal%20consistency.%20By%20incorporating%0Asample-specific%20context%2C%20InvSeg%20learns%20context-rich%20text%20prompts%20in%20embedding%0Aspace%20and%20achieves%20accurate%20semantic%20alignment%20across%20modalities.%20Experiments%0Ashow%20that%20InvSeg%20achieves%20state-of-the-art%20performance%20on%20the%20PASCAL%20VOC%2C%0APASCAL%20Context%20and%20COCO%20Object%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvSeg%253A%2520Test-Time%2520Prompt%2520Inversion%2520for%2520Semantic%2520Segmentation%26entry.906535625%3DJiayi%2520Lin%2520and%2520Jiabo%2520Huang%2520and%2520Jian%2520Hu%2520and%2520Shaogang%2520Gong%26entry.1292438233%3D%2520%2520Visual-textual%2520correlations%2520in%2520the%2520attention%2520maps%2520derived%2520from%2520text-to-image%250Adiffusion%2520models%2520are%2520proven%2520beneficial%2520to%2520dense%2520visual%2520prediction%2520tasks%252C%2520e.g.%252C%250Asemantic%2520segmentation.%2520However%252C%2520a%2520significant%2520challenge%2520arises%2520due%2520to%2520the%2520input%250Adistributional%2520discrepancy%2520between%2520the%2520context-rich%2520sentences%2520used%2520for%2520image%250Ageneration%2520and%2520the%2520isolated%2520class%2520names%2520typically%2520used%2520in%2520semantic%250Asegmentation.%2520This%2520discrepancy%2520hinders%2520diffusion%2520models%2520from%2520capturing%2520accurate%250Avisual-textual%2520correlations.%2520To%2520solve%2520this%252C%2520we%2520propose%2520InvSeg%252C%2520a%2520test-time%250Aprompt%2520inversion%2520method%2520that%2520tackles%2520open-vocabulary%2520semantic%2520segmentation%2520by%250Ainverting%2520image-specific%2520visual%2520context%2520into%2520text%2520prompt%2520embedding%2520space%252C%250Aleveraging%2520structure%2520information%2520derived%2520from%2520the%2520diffusion%2520model%2527s%250Areconstruction%2520process%2520to%2520enrich%2520text%2520prompts%2520so%2520as%2520to%2520associate%2520each%2520class%250Awith%2520a%2520structure-consistent%2520mask.%2520Specifically%252C%2520we%2520introduce%2520Contrastive%2520Soft%250AClustering%2520%2528CSC%2529%2520to%2520align%2520derived%2520masks%2520with%2520the%2520image%2527s%2520structure%2520information%252C%250Asoftly%2520selecting%2520anchors%2520for%2520each%2520class%2520and%2520calculating%2520weighted%2520distances%2520to%250Apush%2520inner-class%2520pixels%2520closer%2520while%2520separating%2520inter-class%2520pixels%252C%2520thereby%250Aensuring%2520mask%2520distinction%2520and%2520internal%2520consistency.%2520By%2520incorporating%250Asample-specific%2520context%252C%2520InvSeg%2520learns%2520context-rich%2520text%2520prompts%2520in%2520embedding%250Aspace%2520and%2520achieves%2520accurate%2520semantic%2520alignment%2520across%2520modalities.%2520Experiments%250Ashow%2520that%2520InvSeg%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520PASCAL%2520VOC%252C%250APASCAL%2520Context%2520and%2520COCO%2520Object%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InvSeg%3A%20Test-Time%20Prompt%20Inversion%20for%20Semantic%20Segmentation&entry.906535625=Jiayi%20Lin%20and%20Jiabo%20Huang%20and%20Jian%20Hu%20and%20Shaogang%20Gong&entry.1292438233=%20%20Visual-textual%20correlations%20in%20the%20attention%20maps%20derived%20from%20text-to-image%0Adiffusion%20models%20are%20proven%20beneficial%20to%20dense%20visual%20prediction%20tasks%2C%20e.g.%2C%0Asemantic%20segmentation.%20However%2C%20a%20significant%20challenge%20arises%20due%20to%20the%20input%0Adistributional%20discrepancy%20between%20the%20context-rich%20sentences%20used%20for%20image%0Ageneration%20and%20the%20isolated%20class%20names%20typically%20used%20in%20semantic%0Asegmentation.%20This%20discrepancy%20hinders%20diffusion%20models%20from%20capturing%20accurate%0Avisual-textual%20correlations.%20To%20solve%20this%2C%20we%20propose%20InvSeg%2C%20a%20test-time%0Aprompt%20inversion%20method%20that%20tackles%20open-vocabulary%20semantic%20segmentation%20by%0Ainverting%20image-specific%20visual%20context%20into%20text%20prompt%20embedding%20space%2C%0Aleveraging%20structure%20information%20derived%20from%20the%20diffusion%20model%27s%0Areconstruction%20process%20to%20enrich%20text%20prompts%20so%20as%20to%20associate%20each%20class%0Awith%20a%20structure-consistent%20mask.%20Specifically%2C%20we%20introduce%20Contrastive%20Soft%0AClustering%20%28CSC%29%20to%20align%20derived%20masks%20with%20the%20image%27s%20structure%20information%2C%0Asoftly%20selecting%20anchors%20for%20each%20class%20and%20calculating%20weighted%20distances%20to%0Apush%20inner-class%20pixels%20closer%20while%20separating%20inter-class%20pixels%2C%20thereby%0Aensuring%20mask%20distinction%20and%20internal%20consistency.%20By%20incorporating%0Asample-specific%20context%2C%20InvSeg%20learns%20context-rich%20text%20prompts%20in%20embedding%0Aspace%20and%20achieves%20accurate%20semantic%20alignment%20across%20modalities.%20Experiments%0Ashow%20that%20InvSeg%20achieves%20state-of-the-art%20performance%20on%20the%20PASCAL%20VOC%2C%0APASCAL%20Context%20and%20COCO%20Object%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11473v2&entry.124074799=Read"},
{"title": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model\n  Internals", "author": "Jaden Fiotto-Kaufman and Alexander R. Loftus and Eric Todd and Jannik Brinkmann and Koyena Pal and Dmitrii Troitskii and Michael Ripa and Adam Belfki and Can Rager and Caden Juang and Aaron Mueller and Samuel Marks and Arnab Sen Sharma and Francesca Lucchetti and Nikhil Prakash and Carla Brodley and Arjun Guha and Jonathan Bell and Byron C. Wallace and David Bau", "abstract": "  We introduce NNsight and NDIF, technologies that work in tandem to enable\nscientific study of very large neural networks. NNsight is an open-source\nsystem that extends PyTorch to introduce deferred remote execution. NDIF is a\nscalable inference service that executes NNsight requests, allowing users to\nshare GPU resources and pretrained models. These technologies are enabled by\nthe intervention graph, an architecture developed to decouple experiment design\nfrom model runtime. Together, this framework provides transparent and efficient\naccess to the internals of deep neural networks such as very large language\nmodels (LLMs) without imposing the cost or complexity of hosting customized\nmodels individually. We conduct a quantitative survey of the machine learning\nliterature that reveals a growing gap in the study of the internals of\nlarge-scale AI. We demonstrate the design and use of our framework to address\nthis gap by enabling a range of research methods on huge models. Finally, we\nconduct benchmarks to compare performance with previous approaches. Code\ndocumentation, and materials are available at https://nnsight.net/.\n", "link": "http://arxiv.org/abs/2407.14561v3", "date": "2025-01-03", "relevancy": 1.6174, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5496}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5388}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NNsight%20and%20NDIF%3A%20Democratizing%20Access%20to%20Open-Weight%20Foundation%20Model%0A%20%20Internals&body=Title%3A%20NNsight%20and%20NDIF%3A%20Democratizing%20Access%20to%20Open-Weight%20Foundation%20Model%0A%20%20Internals%0AAuthor%3A%20Jaden%20Fiotto-Kaufman%20and%20Alexander%20R.%20Loftus%20and%20Eric%20Todd%20and%20Jannik%20Brinkmann%20and%20Koyena%20Pal%20and%20Dmitrii%20Troitskii%20and%20Michael%20Ripa%20and%20Adam%20Belfki%20and%20Can%20Rager%20and%20Caden%20Juang%20and%20Aaron%20Mueller%20and%20Samuel%20Marks%20and%20Arnab%20Sen%20Sharma%20and%20Francesca%20Lucchetti%20and%20Nikhil%20Prakash%20and%20Carla%20Brodley%20and%20Arjun%20Guha%20and%20Jonathan%20Bell%20and%20Byron%20C.%20Wallace%20and%20David%20Bau%0AAbstract%3A%20%20%20We%20introduce%20NNsight%20and%20NDIF%2C%20technologies%20that%20work%20in%20tandem%20to%20enable%0Ascientific%20study%20of%20very%20large%20neural%20networks.%20NNsight%20is%20an%20open-source%0Asystem%20that%20extends%20PyTorch%20to%20introduce%20deferred%20remote%20execution.%20NDIF%20is%20a%0Ascalable%20inference%20service%20that%20executes%20NNsight%20requests%2C%20allowing%20users%20to%0Ashare%20GPU%20resources%20and%20pretrained%20models.%20These%20technologies%20are%20enabled%20by%0Athe%20intervention%20graph%2C%20an%20architecture%20developed%20to%20decouple%20experiment%20design%0Afrom%20model%20runtime.%20Together%2C%20this%20framework%20provides%20transparent%20and%20efficient%0Aaccess%20to%20the%20internals%20of%20deep%20neural%20networks%20such%20as%20very%20large%20language%0Amodels%20%28LLMs%29%20without%20imposing%20the%20cost%20or%20complexity%20of%20hosting%20customized%0Amodels%20individually.%20We%20conduct%20a%20quantitative%20survey%20of%20the%20machine%20learning%0Aliterature%20that%20reveals%20a%20growing%20gap%20in%20the%20study%20of%20the%20internals%20of%0Alarge-scale%20AI.%20We%20demonstrate%20the%20design%20and%20use%20of%20our%20framework%20to%20address%0Athis%20gap%20by%20enabling%20a%20range%20of%20research%20methods%20on%20huge%20models.%20Finally%2C%20we%0Aconduct%20benchmarks%20to%20compare%20performance%20with%20previous%20approaches.%20Code%0Adocumentation%2C%20and%20materials%20are%20available%20at%20https%3A//nnsight.net/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14561v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNNsight%2520and%2520NDIF%253A%2520Democratizing%2520Access%2520to%2520Open-Weight%2520Foundation%2520Model%250A%2520%2520Internals%26entry.906535625%3DJaden%2520Fiotto-Kaufman%2520and%2520Alexander%2520R.%2520Loftus%2520and%2520Eric%2520Todd%2520and%2520Jannik%2520Brinkmann%2520and%2520Koyena%2520Pal%2520and%2520Dmitrii%2520Troitskii%2520and%2520Michael%2520Ripa%2520and%2520Adam%2520Belfki%2520and%2520Can%2520Rager%2520and%2520Caden%2520Juang%2520and%2520Aaron%2520Mueller%2520and%2520Samuel%2520Marks%2520and%2520Arnab%2520Sen%2520Sharma%2520and%2520Francesca%2520Lucchetti%2520and%2520Nikhil%2520Prakash%2520and%2520Carla%2520Brodley%2520and%2520Arjun%2520Guha%2520and%2520Jonathan%2520Bell%2520and%2520Byron%2520C.%2520Wallace%2520and%2520David%2520Bau%26entry.1292438233%3D%2520%2520We%2520introduce%2520NNsight%2520and%2520NDIF%252C%2520technologies%2520that%2520work%2520in%2520tandem%2520to%2520enable%250Ascientific%2520study%2520of%2520very%2520large%2520neural%2520networks.%2520NNsight%2520is%2520an%2520open-source%250Asystem%2520that%2520extends%2520PyTorch%2520to%2520introduce%2520deferred%2520remote%2520execution.%2520NDIF%2520is%2520a%250Ascalable%2520inference%2520service%2520that%2520executes%2520NNsight%2520requests%252C%2520allowing%2520users%2520to%250Ashare%2520GPU%2520resources%2520and%2520pretrained%2520models.%2520These%2520technologies%2520are%2520enabled%2520by%250Athe%2520intervention%2520graph%252C%2520an%2520architecture%2520developed%2520to%2520decouple%2520experiment%2520design%250Afrom%2520model%2520runtime.%2520Together%252C%2520this%2520framework%2520provides%2520transparent%2520and%2520efficient%250Aaccess%2520to%2520the%2520internals%2520of%2520deep%2520neural%2520networks%2520such%2520as%2520very%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520without%2520imposing%2520the%2520cost%2520or%2520complexity%2520of%2520hosting%2520customized%250Amodels%2520individually.%2520We%2520conduct%2520a%2520quantitative%2520survey%2520of%2520the%2520machine%2520learning%250Aliterature%2520that%2520reveals%2520a%2520growing%2520gap%2520in%2520the%2520study%2520of%2520the%2520internals%2520of%250Alarge-scale%2520AI.%2520We%2520demonstrate%2520the%2520design%2520and%2520use%2520of%2520our%2520framework%2520to%2520address%250Athis%2520gap%2520by%2520enabling%2520a%2520range%2520of%2520research%2520methods%2520on%2520huge%2520models.%2520Finally%252C%2520we%250Aconduct%2520benchmarks%2520to%2520compare%2520performance%2520with%2520previous%2520approaches.%2520Code%250Adocumentation%252C%2520and%2520materials%2520are%2520available%2520at%2520https%253A//nnsight.net/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14561v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NNsight%20and%20NDIF%3A%20Democratizing%20Access%20to%20Open-Weight%20Foundation%20Model%0A%20%20Internals&entry.906535625=Jaden%20Fiotto-Kaufman%20and%20Alexander%20R.%20Loftus%20and%20Eric%20Todd%20and%20Jannik%20Brinkmann%20and%20Koyena%20Pal%20and%20Dmitrii%20Troitskii%20and%20Michael%20Ripa%20and%20Adam%20Belfki%20and%20Can%20Rager%20and%20Caden%20Juang%20and%20Aaron%20Mueller%20and%20Samuel%20Marks%20and%20Arnab%20Sen%20Sharma%20and%20Francesca%20Lucchetti%20and%20Nikhil%20Prakash%20and%20Carla%20Brodley%20and%20Arjun%20Guha%20and%20Jonathan%20Bell%20and%20Byron%20C.%20Wallace%20and%20David%20Bau&entry.1292438233=%20%20We%20introduce%20NNsight%20and%20NDIF%2C%20technologies%20that%20work%20in%20tandem%20to%20enable%0Ascientific%20study%20of%20very%20large%20neural%20networks.%20NNsight%20is%20an%20open-source%0Asystem%20that%20extends%20PyTorch%20to%20introduce%20deferred%20remote%20execution.%20NDIF%20is%20a%0Ascalable%20inference%20service%20that%20executes%20NNsight%20requests%2C%20allowing%20users%20to%0Ashare%20GPU%20resources%20and%20pretrained%20models.%20These%20technologies%20are%20enabled%20by%0Athe%20intervention%20graph%2C%20an%20architecture%20developed%20to%20decouple%20experiment%20design%0Afrom%20model%20runtime.%20Together%2C%20this%20framework%20provides%20transparent%20and%20efficient%0Aaccess%20to%20the%20internals%20of%20deep%20neural%20networks%20such%20as%20very%20large%20language%0Amodels%20%28LLMs%29%20without%20imposing%20the%20cost%20or%20complexity%20of%20hosting%20customized%0Amodels%20individually.%20We%20conduct%20a%20quantitative%20survey%20of%20the%20machine%20learning%0Aliterature%20that%20reveals%20a%20growing%20gap%20in%20the%20study%20of%20the%20internals%20of%0Alarge-scale%20AI.%20We%20demonstrate%20the%20design%20and%20use%20of%20our%20framework%20to%20address%0Athis%20gap%20by%20enabling%20a%20range%20of%20research%20methods%20on%20huge%20models.%20Finally%2C%20we%0Aconduct%20benchmarks%20to%20compare%20performance%20with%20previous%20approaches.%20Code%0Adocumentation%2C%20and%20materials%20are%20available%20at%20https%3A//nnsight.net/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14561v3&entry.124074799=Read"},
{"title": "Transformer-Driven Inverse Problem Transform for Fast Blind\n  Hyperspectral Image Dehazing", "author": "Po-Wei Tang and Chia-Hsiang Lin and Yangrui Liu", "abstract": "  Hyperspectral dehazing (HyDHZ) has become a crucial signal processing\ntechnology to facilitate the subsequent identification and classification\ntasks, as the airborne visible/infrared imaging spectrometer (AVIRIS) data\nportal reports a massive portion of haze-corrupted areas in typical\nhyperspectral remote sensing images. The idea of inverse problem transform\n(IPT) has been proposed in recent remote sensing literature in order to\nreformulate a hardly tractable inverse problem (e.g., HyDHZ) into a relatively\nsimple one. Considering the emerging spectral super-resolution (SSR) technique,\nwhich spectrally upsamples multispectral data to hyperspectral data, we aim to\nsolve the challenging HyDHZ problem by reformulating it as an SSR problem.\nRoughly speaking, the proposed algorithm first automatically selects some\nuncorrupted/informative spectral bands, from which SSR is applied to spectrally\nupsample the selected bands in the feature space, thereby obtaining a clean\nhyperspectral image (HSI). The clean HSI is then further refined by a deep\ntransformer network to obtain the final dehazed HSI, where a global attention\nmechanism is designed to capture nonlocal information. There are very few HyDHZ\nworks in existing literature, and this article introduces the powerful\nspatial-spectral transformer into HyDHZ for the first time. Remarkably, the\nproposed transformer-driven IPT-based HyDHZ (T2HyDHZ) is a blind algorithm\nwithout requiring the user to manually select the corrupted region. Extensive\nexperiments demonstrate the superiority of T2HyDHZ with less color distortion.\n", "link": "http://arxiv.org/abs/2501.01924v1", "date": "2025-01-03", "relevancy": 1.5852, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5497}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5289}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer-Driven%20Inverse%20Problem%20Transform%20for%20Fast%20Blind%0A%20%20Hyperspectral%20Image%20Dehazing&body=Title%3A%20Transformer-Driven%20Inverse%20Problem%20Transform%20for%20Fast%20Blind%0A%20%20Hyperspectral%20Image%20Dehazing%0AAuthor%3A%20Po-Wei%20Tang%20and%20Chia-Hsiang%20Lin%20and%20Yangrui%20Liu%0AAbstract%3A%20%20%20Hyperspectral%20dehazing%20%28HyDHZ%29%20has%20become%20a%20crucial%20signal%20processing%0Atechnology%20to%20facilitate%20the%20subsequent%20identification%20and%20classification%0Atasks%2C%20as%20the%20airborne%20visible/infrared%20imaging%20spectrometer%20%28AVIRIS%29%20data%0Aportal%20reports%20a%20massive%20portion%20of%20haze-corrupted%20areas%20in%20typical%0Ahyperspectral%20remote%20sensing%20images.%20The%20idea%20of%20inverse%20problem%20transform%0A%28IPT%29%20has%20been%20proposed%20in%20recent%20remote%20sensing%20literature%20in%20order%20to%0Areformulate%20a%20hardly%20tractable%20inverse%20problem%20%28e.g.%2C%20HyDHZ%29%20into%20a%20relatively%0Asimple%20one.%20Considering%20the%20emerging%20spectral%20super-resolution%20%28SSR%29%20technique%2C%0Awhich%20spectrally%20upsamples%20multispectral%20data%20to%20hyperspectral%20data%2C%20we%20aim%20to%0Asolve%20the%20challenging%20HyDHZ%20problem%20by%20reformulating%20it%20as%20an%20SSR%20problem.%0ARoughly%20speaking%2C%20the%20proposed%20algorithm%20first%20automatically%20selects%20some%0Auncorrupted/informative%20spectral%20bands%2C%20from%20which%20SSR%20is%20applied%20to%20spectrally%0Aupsample%20the%20selected%20bands%20in%20the%20feature%20space%2C%20thereby%20obtaining%20a%20clean%0Ahyperspectral%20image%20%28HSI%29.%20The%20clean%20HSI%20is%20then%20further%20refined%20by%20a%20deep%0Atransformer%20network%20to%20obtain%20the%20final%20dehazed%20HSI%2C%20where%20a%20global%20attention%0Amechanism%20is%20designed%20to%20capture%20nonlocal%20information.%20There%20are%20very%20few%20HyDHZ%0Aworks%20in%20existing%20literature%2C%20and%20this%20article%20introduces%20the%20powerful%0Aspatial-spectral%20transformer%20into%20HyDHZ%20for%20the%20first%20time.%20Remarkably%2C%20the%0Aproposed%20transformer-driven%20IPT-based%20HyDHZ%20%28T2HyDHZ%29%20is%20a%20blind%20algorithm%0Awithout%20requiring%20the%20user%20to%20manually%20select%20the%20corrupted%20region.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20T2HyDHZ%20with%20less%20color%20distortion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer-Driven%2520Inverse%2520Problem%2520Transform%2520for%2520Fast%2520Blind%250A%2520%2520Hyperspectral%2520Image%2520Dehazing%26entry.906535625%3DPo-Wei%2520Tang%2520and%2520Chia-Hsiang%2520Lin%2520and%2520Yangrui%2520Liu%26entry.1292438233%3D%2520%2520Hyperspectral%2520dehazing%2520%2528HyDHZ%2529%2520has%2520become%2520a%2520crucial%2520signal%2520processing%250Atechnology%2520to%2520facilitate%2520the%2520subsequent%2520identification%2520and%2520classification%250Atasks%252C%2520as%2520the%2520airborne%2520visible/infrared%2520imaging%2520spectrometer%2520%2528AVIRIS%2529%2520data%250Aportal%2520reports%2520a%2520massive%2520portion%2520of%2520haze-corrupted%2520areas%2520in%2520typical%250Ahyperspectral%2520remote%2520sensing%2520images.%2520The%2520idea%2520of%2520inverse%2520problem%2520transform%250A%2528IPT%2529%2520has%2520been%2520proposed%2520in%2520recent%2520remote%2520sensing%2520literature%2520in%2520order%2520to%250Areformulate%2520a%2520hardly%2520tractable%2520inverse%2520problem%2520%2528e.g.%252C%2520HyDHZ%2529%2520into%2520a%2520relatively%250Asimple%2520one.%2520Considering%2520the%2520emerging%2520spectral%2520super-resolution%2520%2528SSR%2529%2520technique%252C%250Awhich%2520spectrally%2520upsamples%2520multispectral%2520data%2520to%2520hyperspectral%2520data%252C%2520we%2520aim%2520to%250Asolve%2520the%2520challenging%2520HyDHZ%2520problem%2520by%2520reformulating%2520it%2520as%2520an%2520SSR%2520problem.%250ARoughly%2520speaking%252C%2520the%2520proposed%2520algorithm%2520first%2520automatically%2520selects%2520some%250Auncorrupted/informative%2520spectral%2520bands%252C%2520from%2520which%2520SSR%2520is%2520applied%2520to%2520spectrally%250Aupsample%2520the%2520selected%2520bands%2520in%2520the%2520feature%2520space%252C%2520thereby%2520obtaining%2520a%2520clean%250Ahyperspectral%2520image%2520%2528HSI%2529.%2520The%2520clean%2520HSI%2520is%2520then%2520further%2520refined%2520by%2520a%2520deep%250Atransformer%2520network%2520to%2520obtain%2520the%2520final%2520dehazed%2520HSI%252C%2520where%2520a%2520global%2520attention%250Amechanism%2520is%2520designed%2520to%2520capture%2520nonlocal%2520information.%2520There%2520are%2520very%2520few%2520HyDHZ%250Aworks%2520in%2520existing%2520literature%252C%2520and%2520this%2520article%2520introduces%2520the%2520powerful%250Aspatial-spectral%2520transformer%2520into%2520HyDHZ%2520for%2520the%2520first%2520time.%2520Remarkably%252C%2520the%250Aproposed%2520transformer-driven%2520IPT-based%2520HyDHZ%2520%2528T2HyDHZ%2529%2520is%2520a%2520blind%2520algorithm%250Awithout%2520requiring%2520the%2520user%2520to%2520manually%2520select%2520the%2520corrupted%2520region.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520T2HyDHZ%2520with%2520less%2520color%2520distortion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-Driven%20Inverse%20Problem%20Transform%20for%20Fast%20Blind%0A%20%20Hyperspectral%20Image%20Dehazing&entry.906535625=Po-Wei%20Tang%20and%20Chia-Hsiang%20Lin%20and%20Yangrui%20Liu&entry.1292438233=%20%20Hyperspectral%20dehazing%20%28HyDHZ%29%20has%20become%20a%20crucial%20signal%20processing%0Atechnology%20to%20facilitate%20the%20subsequent%20identification%20and%20classification%0Atasks%2C%20as%20the%20airborne%20visible/infrared%20imaging%20spectrometer%20%28AVIRIS%29%20data%0Aportal%20reports%20a%20massive%20portion%20of%20haze-corrupted%20areas%20in%20typical%0Ahyperspectral%20remote%20sensing%20images.%20The%20idea%20of%20inverse%20problem%20transform%0A%28IPT%29%20has%20been%20proposed%20in%20recent%20remote%20sensing%20literature%20in%20order%20to%0Areformulate%20a%20hardly%20tractable%20inverse%20problem%20%28e.g.%2C%20HyDHZ%29%20into%20a%20relatively%0Asimple%20one.%20Considering%20the%20emerging%20spectral%20super-resolution%20%28SSR%29%20technique%2C%0Awhich%20spectrally%20upsamples%20multispectral%20data%20to%20hyperspectral%20data%2C%20we%20aim%20to%0Asolve%20the%20challenging%20HyDHZ%20problem%20by%20reformulating%20it%20as%20an%20SSR%20problem.%0ARoughly%20speaking%2C%20the%20proposed%20algorithm%20first%20automatically%20selects%20some%0Auncorrupted/informative%20spectral%20bands%2C%20from%20which%20SSR%20is%20applied%20to%20spectrally%0Aupsample%20the%20selected%20bands%20in%20the%20feature%20space%2C%20thereby%20obtaining%20a%20clean%0Ahyperspectral%20image%20%28HSI%29.%20The%20clean%20HSI%20is%20then%20further%20refined%20by%20a%20deep%0Atransformer%20network%20to%20obtain%20the%20final%20dehazed%20HSI%2C%20where%20a%20global%20attention%0Amechanism%20is%20designed%20to%20capture%20nonlocal%20information.%20There%20are%20very%20few%20HyDHZ%0Aworks%20in%20existing%20literature%2C%20and%20this%20article%20introduces%20the%20powerful%0Aspatial-spectral%20transformer%20into%20HyDHZ%20for%20the%20first%20time.%20Remarkably%2C%20the%0Aproposed%20transformer-driven%20IPT-based%20HyDHZ%20%28T2HyDHZ%29%20is%20a%20blind%20algorithm%0Awithout%20requiring%20the%20user%20to%20manually%20select%20the%20corrupted%20region.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20T2HyDHZ%20with%20less%20color%20distortion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01924v1&entry.124074799=Read"},
{"title": "Evaluating Scenario-based Decision-making for Interactive Autonomous\n  Driving Using Rational Criteria: A Survey", "author": "Zhen Tian and Zhihao Lin and Dezong Zhao and Wenjing Zhao and David Flynn and Shuja Ansari and Chongfeng Wei", "abstract": "  Autonomous vehicles (AVs) can significantly promote the advances in road\ntransport mobility in terms of safety, reliability, and decarbonization.\nHowever, ensuring safety and efficiency in interactive during within dynamic\nand diverse environments is still a primary barrier to large-scale AV adoption.\nIn recent years, deep reinforcement learning (DRL) has emerged as an advanced\nAI-based approach, enabling AVs to learn decision-making strategies adaptively\nfrom data and interactions. DRL strategies are better suited than traditional\nrule-based methods for handling complex, dynamic, and unpredictable driving\nenvironments due to their adaptivity. However, varying driving scenarios\npresent distinct challenges, such as avoiding obstacles on highways and\nreaching specific exits at intersections, requiring different scenario-specific\ndecision-making algorithms. Many DRL algorithms have been proposed in\ninteractive decision-making. However, a rationale review of these DRL\nalgorithms across various scenarios is lacking. Therefore, a comprehensive\nevaluation is essential to assess these algorithms from multiple perspectives,\nincluding those of vehicle users and vehicle manufacturers. This survey reviews\nthe application of DRL algorithms in autonomous driving across typical\nscenarios, summarizing road features and recent advancements. The scenarios\ninclude highways, on-ramp merging, roundabouts, and unsignalized intersections.\nFurthermore, DRL-based algorithms are evaluated based on five rationale\ncriteria: driving safety, driving efficiency, training efficiency,\nunselfishness, and interpretability (DDTUI). Each criterion of DDTUI is\nspecifically analyzed in relation to the reviewed algorithms. Finally, the\nchallenges for future DRL-based decision-making algorithms are summarized.\n", "link": "http://arxiv.org/abs/2501.01886v1", "date": "2025-01-03", "relevancy": 1.5701, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5319}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5249}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Scenario-based%20Decision-making%20for%20Interactive%20Autonomous%0A%20%20Driving%20Using%20Rational%20Criteria%3A%20A%20Survey&body=Title%3A%20Evaluating%20Scenario-based%20Decision-making%20for%20Interactive%20Autonomous%0A%20%20Driving%20Using%20Rational%20Criteria%3A%20A%20Survey%0AAuthor%3A%20Zhen%20Tian%20and%20Zhihao%20Lin%20and%20Dezong%20Zhao%20and%20Wenjing%20Zhao%20and%20David%20Flynn%20and%20Shuja%20Ansari%20and%20Chongfeng%20Wei%0AAbstract%3A%20%20%20Autonomous%20vehicles%20%28AVs%29%20can%20significantly%20promote%20the%20advances%20in%20road%0Atransport%20mobility%20in%20terms%20of%20safety%2C%20reliability%2C%20and%20decarbonization.%0AHowever%2C%20ensuring%20safety%20and%20efficiency%20in%20interactive%20during%20within%20dynamic%0Aand%20diverse%20environments%20is%20still%20a%20primary%20barrier%20to%20large-scale%20AV%20adoption.%0AIn%20recent%20years%2C%20deep%20reinforcement%20learning%20%28DRL%29%20has%20emerged%20as%20an%20advanced%0AAI-based%20approach%2C%20enabling%20AVs%20to%20learn%20decision-making%20strategies%20adaptively%0Afrom%20data%20and%20interactions.%20DRL%20strategies%20are%20better%20suited%20than%20traditional%0Arule-based%20methods%20for%20handling%20complex%2C%20dynamic%2C%20and%20unpredictable%20driving%0Aenvironments%20due%20to%20their%20adaptivity.%20However%2C%20varying%20driving%20scenarios%0Apresent%20distinct%20challenges%2C%20such%20as%20avoiding%20obstacles%20on%20highways%20and%0Areaching%20specific%20exits%20at%20intersections%2C%20requiring%20different%20scenario-specific%0Adecision-making%20algorithms.%20Many%20DRL%20algorithms%20have%20been%20proposed%20in%0Ainteractive%20decision-making.%20However%2C%20a%20rationale%20review%20of%20these%20DRL%0Aalgorithms%20across%20various%20scenarios%20is%20lacking.%20Therefore%2C%20a%20comprehensive%0Aevaluation%20is%20essential%20to%20assess%20these%20algorithms%20from%20multiple%20perspectives%2C%0Aincluding%20those%20of%20vehicle%20users%20and%20vehicle%20manufacturers.%20This%20survey%20reviews%0Athe%20application%20of%20DRL%20algorithms%20in%20autonomous%20driving%20across%20typical%0Ascenarios%2C%20summarizing%20road%20features%20and%20recent%20advancements.%20The%20scenarios%0Ainclude%20highways%2C%20on-ramp%20merging%2C%20roundabouts%2C%20and%20unsignalized%20intersections.%0AFurthermore%2C%20DRL-based%20algorithms%20are%20evaluated%20based%20on%20five%20rationale%0Acriteria%3A%20driving%20safety%2C%20driving%20efficiency%2C%20training%20efficiency%2C%0Aunselfishness%2C%20and%20interpretability%20%28DDTUI%29.%20Each%20criterion%20of%20DDTUI%20is%0Aspecifically%20analyzed%20in%20relation%20to%20the%20reviewed%20algorithms.%20Finally%2C%20the%0Achallenges%20for%20future%20DRL-based%20decision-making%20algorithms%20are%20summarized.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Scenario-based%2520Decision-making%2520for%2520Interactive%2520Autonomous%250A%2520%2520Driving%2520Using%2520Rational%2520Criteria%253A%2520A%2520Survey%26entry.906535625%3DZhen%2520Tian%2520and%2520Zhihao%2520Lin%2520and%2520Dezong%2520Zhao%2520and%2520Wenjing%2520Zhao%2520and%2520David%2520Flynn%2520and%2520Shuja%2520Ansari%2520and%2520Chongfeng%2520Wei%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520%2528AVs%2529%2520can%2520significantly%2520promote%2520the%2520advances%2520in%2520road%250Atransport%2520mobility%2520in%2520terms%2520of%2520safety%252C%2520reliability%252C%2520and%2520decarbonization.%250AHowever%252C%2520ensuring%2520safety%2520and%2520efficiency%2520in%2520interactive%2520during%2520within%2520dynamic%250Aand%2520diverse%2520environments%2520is%2520still%2520a%2520primary%2520barrier%2520to%2520large-scale%2520AV%2520adoption.%250AIn%2520recent%2520years%252C%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520has%2520emerged%2520as%2520an%2520advanced%250AAI-based%2520approach%252C%2520enabling%2520AVs%2520to%2520learn%2520decision-making%2520strategies%2520adaptively%250Afrom%2520data%2520and%2520interactions.%2520DRL%2520strategies%2520are%2520better%2520suited%2520than%2520traditional%250Arule-based%2520methods%2520for%2520handling%2520complex%252C%2520dynamic%252C%2520and%2520unpredictable%2520driving%250Aenvironments%2520due%2520to%2520their%2520adaptivity.%2520However%252C%2520varying%2520driving%2520scenarios%250Apresent%2520distinct%2520challenges%252C%2520such%2520as%2520avoiding%2520obstacles%2520on%2520highways%2520and%250Areaching%2520specific%2520exits%2520at%2520intersections%252C%2520requiring%2520different%2520scenario-specific%250Adecision-making%2520algorithms.%2520Many%2520DRL%2520algorithms%2520have%2520been%2520proposed%2520in%250Ainteractive%2520decision-making.%2520However%252C%2520a%2520rationale%2520review%2520of%2520these%2520DRL%250Aalgorithms%2520across%2520various%2520scenarios%2520is%2520lacking.%2520Therefore%252C%2520a%2520comprehensive%250Aevaluation%2520is%2520essential%2520to%2520assess%2520these%2520algorithms%2520from%2520multiple%2520perspectives%252C%250Aincluding%2520those%2520of%2520vehicle%2520users%2520and%2520vehicle%2520manufacturers.%2520This%2520survey%2520reviews%250Athe%2520application%2520of%2520DRL%2520algorithms%2520in%2520autonomous%2520driving%2520across%2520typical%250Ascenarios%252C%2520summarizing%2520road%2520features%2520and%2520recent%2520advancements.%2520The%2520scenarios%250Ainclude%2520highways%252C%2520on-ramp%2520merging%252C%2520roundabouts%252C%2520and%2520unsignalized%2520intersections.%250AFurthermore%252C%2520DRL-based%2520algorithms%2520are%2520evaluated%2520based%2520on%2520five%2520rationale%250Acriteria%253A%2520driving%2520safety%252C%2520driving%2520efficiency%252C%2520training%2520efficiency%252C%250Aunselfishness%252C%2520and%2520interpretability%2520%2528DDTUI%2529.%2520Each%2520criterion%2520of%2520DDTUI%2520is%250Aspecifically%2520analyzed%2520in%2520relation%2520to%2520the%2520reviewed%2520algorithms.%2520Finally%252C%2520the%250Achallenges%2520for%2520future%2520DRL-based%2520decision-making%2520algorithms%2520are%2520summarized.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Scenario-based%20Decision-making%20for%20Interactive%20Autonomous%0A%20%20Driving%20Using%20Rational%20Criteria%3A%20A%20Survey&entry.906535625=Zhen%20Tian%20and%20Zhihao%20Lin%20and%20Dezong%20Zhao%20and%20Wenjing%20Zhao%20and%20David%20Flynn%20and%20Shuja%20Ansari%20and%20Chongfeng%20Wei&entry.1292438233=%20%20Autonomous%20vehicles%20%28AVs%29%20can%20significantly%20promote%20the%20advances%20in%20road%0Atransport%20mobility%20in%20terms%20of%20safety%2C%20reliability%2C%20and%20decarbonization.%0AHowever%2C%20ensuring%20safety%20and%20efficiency%20in%20interactive%20during%20within%20dynamic%0Aand%20diverse%20environments%20is%20still%20a%20primary%20barrier%20to%20large-scale%20AV%20adoption.%0AIn%20recent%20years%2C%20deep%20reinforcement%20learning%20%28DRL%29%20has%20emerged%20as%20an%20advanced%0AAI-based%20approach%2C%20enabling%20AVs%20to%20learn%20decision-making%20strategies%20adaptively%0Afrom%20data%20and%20interactions.%20DRL%20strategies%20are%20better%20suited%20than%20traditional%0Arule-based%20methods%20for%20handling%20complex%2C%20dynamic%2C%20and%20unpredictable%20driving%0Aenvironments%20due%20to%20their%20adaptivity.%20However%2C%20varying%20driving%20scenarios%0Apresent%20distinct%20challenges%2C%20such%20as%20avoiding%20obstacles%20on%20highways%20and%0Areaching%20specific%20exits%20at%20intersections%2C%20requiring%20different%20scenario-specific%0Adecision-making%20algorithms.%20Many%20DRL%20algorithms%20have%20been%20proposed%20in%0Ainteractive%20decision-making.%20However%2C%20a%20rationale%20review%20of%20these%20DRL%0Aalgorithms%20across%20various%20scenarios%20is%20lacking.%20Therefore%2C%20a%20comprehensive%0Aevaluation%20is%20essential%20to%20assess%20these%20algorithms%20from%20multiple%20perspectives%2C%0Aincluding%20those%20of%20vehicle%20users%20and%20vehicle%20manufacturers.%20This%20survey%20reviews%0Athe%20application%20of%20DRL%20algorithms%20in%20autonomous%20driving%20across%20typical%0Ascenarios%2C%20summarizing%20road%20features%20and%20recent%20advancements.%20The%20scenarios%0Ainclude%20highways%2C%20on-ramp%20merging%2C%20roundabouts%2C%20and%20unsignalized%20intersections.%0AFurthermore%2C%20DRL-based%20algorithms%20are%20evaluated%20based%20on%20five%20rationale%0Acriteria%3A%20driving%20safety%2C%20driving%20efficiency%2C%20training%20efficiency%2C%0Aunselfishness%2C%20and%20interpretability%20%28DDTUI%29.%20Each%20criterion%20of%20DDTUI%20is%0Aspecifically%20analyzed%20in%20relation%20to%20the%20reviewed%20algorithms.%20Finally%2C%20the%0Achallenges%20for%20future%20DRL-based%20decision-making%20algorithms%20are%20summarized.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01886v1&entry.124074799=Read"},
{"title": "Towards Hard and Soft Shadow Removal via Dual-Branch Separation Network\n  and Vision Transformer", "author": "Jiajia Liang", "abstract": "  Image shadow removal is a crucial task in computer vision. In real-world\nscenes, shadows alter image color and brightness, posing challenges for\nperception and texture recognition. Traditional and deep learning methods often\noverlook the distinct needs for handling hard and soft shadows, thereby lacking\ndetailed processing to specifically address each type of shadow in images.We\npropose a dual-path model that processes these shadows separately using\nspecially designed loss functions to accomplish the hard and soft shadow\nremoval. The model classifies shadow types and processes them through\nappropriate paths to produce shadow-free outputs, integrating a Vision\nTransformer with UNet++ for enhanced edge detail and feature fusion. Our model\noutperforms state-of-the-art methods and achieves 2.905 RMSE value on the ISTD\ndataset, which demonstrates greater effectiveness than typical single-path\napproaches.\n", "link": "http://arxiv.org/abs/2501.01864v1", "date": "2025-01-03", "relevancy": 1.5683, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5352}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.521}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Hard%20and%20Soft%20Shadow%20Removal%20via%20Dual-Branch%20Separation%20Network%0A%20%20and%20Vision%20Transformer&body=Title%3A%20Towards%20Hard%20and%20Soft%20Shadow%20Removal%20via%20Dual-Branch%20Separation%20Network%0A%20%20and%20Vision%20Transformer%0AAuthor%3A%20Jiajia%20Liang%0AAbstract%3A%20%20%20Image%20shadow%20removal%20is%20a%20crucial%20task%20in%20computer%20vision.%20In%20real-world%0Ascenes%2C%20shadows%20alter%20image%20color%20and%20brightness%2C%20posing%20challenges%20for%0Aperception%20and%20texture%20recognition.%20Traditional%20and%20deep%20learning%20methods%20often%0Aoverlook%20the%20distinct%20needs%20for%20handling%20hard%20and%20soft%20shadows%2C%20thereby%20lacking%0Adetailed%20processing%20to%20specifically%20address%20each%20type%20of%20shadow%20in%20images.We%0Apropose%20a%20dual-path%20model%20that%20processes%20these%20shadows%20separately%20using%0Aspecially%20designed%20loss%20functions%20to%20accomplish%20the%20hard%20and%20soft%20shadow%0Aremoval.%20The%20model%20classifies%20shadow%20types%20and%20processes%20them%20through%0Aappropriate%20paths%20to%20produce%20shadow-free%20outputs%2C%20integrating%20a%20Vision%0ATransformer%20with%20UNet%2B%2B%20for%20enhanced%20edge%20detail%20and%20feature%20fusion.%20Our%20model%0Aoutperforms%20state-of-the-art%20methods%20and%20achieves%202.905%20RMSE%20value%20on%20the%20ISTD%0Adataset%2C%20which%20demonstrates%20greater%20effectiveness%20than%20typical%20single-path%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Hard%2520and%2520Soft%2520Shadow%2520Removal%2520via%2520Dual-Branch%2520Separation%2520Network%250A%2520%2520and%2520Vision%2520Transformer%26entry.906535625%3DJiajia%2520Liang%26entry.1292438233%3D%2520%2520Image%2520shadow%2520removal%2520is%2520a%2520crucial%2520task%2520in%2520computer%2520vision.%2520In%2520real-world%250Ascenes%252C%2520shadows%2520alter%2520image%2520color%2520and%2520brightness%252C%2520posing%2520challenges%2520for%250Aperception%2520and%2520texture%2520recognition.%2520Traditional%2520and%2520deep%2520learning%2520methods%2520often%250Aoverlook%2520the%2520distinct%2520needs%2520for%2520handling%2520hard%2520and%2520soft%2520shadows%252C%2520thereby%2520lacking%250Adetailed%2520processing%2520to%2520specifically%2520address%2520each%2520type%2520of%2520shadow%2520in%2520images.We%250Apropose%2520a%2520dual-path%2520model%2520that%2520processes%2520these%2520shadows%2520separately%2520using%250Aspecially%2520designed%2520loss%2520functions%2520to%2520accomplish%2520the%2520hard%2520and%2520soft%2520shadow%250Aremoval.%2520The%2520model%2520classifies%2520shadow%2520types%2520and%2520processes%2520them%2520through%250Aappropriate%2520paths%2520to%2520produce%2520shadow-free%2520outputs%252C%2520integrating%2520a%2520Vision%250ATransformer%2520with%2520UNet%252B%252B%2520for%2520enhanced%2520edge%2520detail%2520and%2520feature%2520fusion.%2520Our%2520model%250Aoutperforms%2520state-of-the-art%2520methods%2520and%2520achieves%25202.905%2520RMSE%2520value%2520on%2520the%2520ISTD%250Adataset%252C%2520which%2520demonstrates%2520greater%2520effectiveness%2520than%2520typical%2520single-path%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Hard%20and%20Soft%20Shadow%20Removal%20via%20Dual-Branch%20Separation%20Network%0A%20%20and%20Vision%20Transformer&entry.906535625=Jiajia%20Liang&entry.1292438233=%20%20Image%20shadow%20removal%20is%20a%20crucial%20task%20in%20computer%20vision.%20In%20real-world%0Ascenes%2C%20shadows%20alter%20image%20color%20and%20brightness%2C%20posing%20challenges%20for%0Aperception%20and%20texture%20recognition.%20Traditional%20and%20deep%20learning%20methods%20often%0Aoverlook%20the%20distinct%20needs%20for%20handling%20hard%20and%20soft%20shadows%2C%20thereby%20lacking%0Adetailed%20processing%20to%20specifically%20address%20each%20type%20of%20shadow%20in%20images.We%0Apropose%20a%20dual-path%20model%20that%20processes%20these%20shadows%20separately%20using%0Aspecially%20designed%20loss%20functions%20to%20accomplish%20the%20hard%20and%20soft%20shadow%0Aremoval.%20The%20model%20classifies%20shadow%20types%20and%20processes%20them%20through%0Aappropriate%20paths%20to%20produce%20shadow-free%20outputs%2C%20integrating%20a%20Vision%0ATransformer%20with%20UNet%2B%2B%20for%20enhanced%20edge%20detail%20and%20feature%20fusion.%20Our%20model%0Aoutperforms%20state-of-the-art%20methods%20and%20achieves%202.905%20RMSE%20value%20on%20the%20ISTD%0Adataset%2C%20which%20demonstrates%20greater%20effectiveness%20than%20typical%20single-path%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01864v1&entry.124074799=Read"},
{"title": "Social Processes: Probabilistic Meta-learning for Adaptive Multiparty\n  Interaction Forecasting", "author": "Augustinas Ju\u010das and Chirag Raman", "abstract": "  Adaptively forecasting human behavior in social settings is an important step\ntoward achieving Artificial General Intelligence. Most existing research in\nsocial forecasting has focused either on unfocused interactions, such as\npedestrian trajectory prediction, or on monadic and dyadic behavior\nforecasting. In contrast, social psychology emphasizes the importance of group\ninteractions for understanding complex social dynamics. This creates a gap that\nwe address in this paper: forecasting social interactions at the group\n(conversation) level. Additionally, it is important for a forecasting model to\nbe able to adapt to groups unseen at train time, as even the same individual\nbehaves differently across different groups. This highlights the need for a\nforecasting model to explicitly account for each group's unique dynamics. To\nachieve this, we adopt a meta-learning approach to human behavior forecasting,\ntreating every group as a separate meta-learning task. As a result, our method\nconditions its predictions on the specific behaviors within the group, leading\nto generalization to unseen groups. Specifically, we introduce Social Process\n(SP) models, which predict a distribution over future multimodal cues jointly\nfor all group members based on their preceding low-level multimodal cues, while\nincorporating other past sequences of the same group's interactions. In this\nwork we also analyze the generalization capabilities of SP models in both their\noutputs and latent spaces through the use of realistic synthetic datasets.\n", "link": "http://arxiv.org/abs/2501.01915v1", "date": "2025-01-03", "relevancy": 1.5668, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5964}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5084}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Social%20Processes%3A%20Probabilistic%20Meta-learning%20for%20Adaptive%20Multiparty%0A%20%20Interaction%20Forecasting&body=Title%3A%20Social%20Processes%3A%20Probabilistic%20Meta-learning%20for%20Adaptive%20Multiparty%0A%20%20Interaction%20Forecasting%0AAuthor%3A%20Augustinas%20Ju%C4%8Das%20and%20Chirag%20Raman%0AAbstract%3A%20%20%20Adaptively%20forecasting%20human%20behavior%20in%20social%20settings%20is%20an%20important%20step%0Atoward%20achieving%20Artificial%20General%20Intelligence.%20Most%20existing%20research%20in%0Asocial%20forecasting%20has%20focused%20either%20on%20unfocused%20interactions%2C%20such%20as%0Apedestrian%20trajectory%20prediction%2C%20or%20on%20monadic%20and%20dyadic%20behavior%0Aforecasting.%20In%20contrast%2C%20social%20psychology%20emphasizes%20the%20importance%20of%20group%0Ainteractions%20for%20understanding%20complex%20social%20dynamics.%20This%20creates%20a%20gap%20that%0Awe%20address%20in%20this%20paper%3A%20forecasting%20social%20interactions%20at%20the%20group%0A%28conversation%29%20level.%20Additionally%2C%20it%20is%20important%20for%20a%20forecasting%20model%20to%0Abe%20able%20to%20adapt%20to%20groups%20unseen%20at%20train%20time%2C%20as%20even%20the%20same%20individual%0Abehaves%20differently%20across%20different%20groups.%20This%20highlights%20the%20need%20for%20a%0Aforecasting%20model%20to%20explicitly%20account%20for%20each%20group%27s%20unique%20dynamics.%20To%0Aachieve%20this%2C%20we%20adopt%20a%20meta-learning%20approach%20to%20human%20behavior%20forecasting%2C%0Atreating%20every%20group%20as%20a%20separate%20meta-learning%20task.%20As%20a%20result%2C%20our%20method%0Aconditions%20its%20predictions%20on%20the%20specific%20behaviors%20within%20the%20group%2C%20leading%0Ato%20generalization%20to%20unseen%20groups.%20Specifically%2C%20we%20introduce%20Social%20Process%0A%28SP%29%20models%2C%20which%20predict%20a%20distribution%20over%20future%20multimodal%20cues%20jointly%0Afor%20all%20group%20members%20based%20on%20their%20preceding%20low-level%20multimodal%20cues%2C%20while%0Aincorporating%20other%20past%20sequences%20of%20the%20same%20group%27s%20interactions.%20In%20this%0Awork%20we%20also%20analyze%20the%20generalization%20capabilities%20of%20SP%20models%20in%20both%20their%0Aoutputs%20and%20latent%20spaces%20through%20the%20use%20of%20realistic%20synthetic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocial%2520Processes%253A%2520Probabilistic%2520Meta-learning%2520for%2520Adaptive%2520Multiparty%250A%2520%2520Interaction%2520Forecasting%26entry.906535625%3DAugustinas%2520Ju%25C4%258Das%2520and%2520Chirag%2520Raman%26entry.1292438233%3D%2520%2520Adaptively%2520forecasting%2520human%2520behavior%2520in%2520social%2520settings%2520is%2520an%2520important%2520step%250Atoward%2520achieving%2520Artificial%2520General%2520Intelligence.%2520Most%2520existing%2520research%2520in%250Asocial%2520forecasting%2520has%2520focused%2520either%2520on%2520unfocused%2520interactions%252C%2520such%2520as%250Apedestrian%2520trajectory%2520prediction%252C%2520or%2520on%2520monadic%2520and%2520dyadic%2520behavior%250Aforecasting.%2520In%2520contrast%252C%2520social%2520psychology%2520emphasizes%2520the%2520importance%2520of%2520group%250Ainteractions%2520for%2520understanding%2520complex%2520social%2520dynamics.%2520This%2520creates%2520a%2520gap%2520that%250Awe%2520address%2520in%2520this%2520paper%253A%2520forecasting%2520social%2520interactions%2520at%2520the%2520group%250A%2528conversation%2529%2520level.%2520Additionally%252C%2520it%2520is%2520important%2520for%2520a%2520forecasting%2520model%2520to%250Abe%2520able%2520to%2520adapt%2520to%2520groups%2520unseen%2520at%2520train%2520time%252C%2520as%2520even%2520the%2520same%2520individual%250Abehaves%2520differently%2520across%2520different%2520groups.%2520This%2520highlights%2520the%2520need%2520for%2520a%250Aforecasting%2520model%2520to%2520explicitly%2520account%2520for%2520each%2520group%2527s%2520unique%2520dynamics.%2520To%250Aachieve%2520this%252C%2520we%2520adopt%2520a%2520meta-learning%2520approach%2520to%2520human%2520behavior%2520forecasting%252C%250Atreating%2520every%2520group%2520as%2520a%2520separate%2520meta-learning%2520task.%2520As%2520a%2520result%252C%2520our%2520method%250Aconditions%2520its%2520predictions%2520on%2520the%2520specific%2520behaviors%2520within%2520the%2520group%252C%2520leading%250Ato%2520generalization%2520to%2520unseen%2520groups.%2520Specifically%252C%2520we%2520introduce%2520Social%2520Process%250A%2528SP%2529%2520models%252C%2520which%2520predict%2520a%2520distribution%2520over%2520future%2520multimodal%2520cues%2520jointly%250Afor%2520all%2520group%2520members%2520based%2520on%2520their%2520preceding%2520low-level%2520multimodal%2520cues%252C%2520while%250Aincorporating%2520other%2520past%2520sequences%2520of%2520the%2520same%2520group%2527s%2520interactions.%2520In%2520this%250Awork%2520we%2520also%2520analyze%2520the%2520generalization%2520capabilities%2520of%2520SP%2520models%2520in%2520both%2520their%250Aoutputs%2520and%2520latent%2520spaces%2520through%2520the%2520use%2520of%2520realistic%2520synthetic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Social%20Processes%3A%20Probabilistic%20Meta-learning%20for%20Adaptive%20Multiparty%0A%20%20Interaction%20Forecasting&entry.906535625=Augustinas%20Ju%C4%8Das%20and%20Chirag%20Raman&entry.1292438233=%20%20Adaptively%20forecasting%20human%20behavior%20in%20social%20settings%20is%20an%20important%20step%0Atoward%20achieving%20Artificial%20General%20Intelligence.%20Most%20existing%20research%20in%0Asocial%20forecasting%20has%20focused%20either%20on%20unfocused%20interactions%2C%20such%20as%0Apedestrian%20trajectory%20prediction%2C%20or%20on%20monadic%20and%20dyadic%20behavior%0Aforecasting.%20In%20contrast%2C%20social%20psychology%20emphasizes%20the%20importance%20of%20group%0Ainteractions%20for%20understanding%20complex%20social%20dynamics.%20This%20creates%20a%20gap%20that%0Awe%20address%20in%20this%20paper%3A%20forecasting%20social%20interactions%20at%20the%20group%0A%28conversation%29%20level.%20Additionally%2C%20it%20is%20important%20for%20a%20forecasting%20model%20to%0Abe%20able%20to%20adapt%20to%20groups%20unseen%20at%20train%20time%2C%20as%20even%20the%20same%20individual%0Abehaves%20differently%20across%20different%20groups.%20This%20highlights%20the%20need%20for%20a%0Aforecasting%20model%20to%20explicitly%20account%20for%20each%20group%27s%20unique%20dynamics.%20To%0Aachieve%20this%2C%20we%20adopt%20a%20meta-learning%20approach%20to%20human%20behavior%20forecasting%2C%0Atreating%20every%20group%20as%20a%20separate%20meta-learning%20task.%20As%20a%20result%2C%20our%20method%0Aconditions%20its%20predictions%20on%20the%20specific%20behaviors%20within%20the%20group%2C%20leading%0Ato%20generalization%20to%20unseen%20groups.%20Specifically%2C%20we%20introduce%20Social%20Process%0A%28SP%29%20models%2C%20which%20predict%20a%20distribution%20over%20future%20multimodal%20cues%20jointly%0Afor%20all%20group%20members%20based%20on%20their%20preceding%20low-level%20multimodal%20cues%2C%20while%0Aincorporating%20other%20past%20sequences%20of%20the%20same%20group%27s%20interactions.%20In%20this%0Awork%20we%20also%20analyze%20the%20generalization%20capabilities%20of%20SP%20models%20in%20both%20their%0Aoutputs%20and%20latent%20spaces%20through%20the%20use%20of%20realistic%20synthetic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01915v1&entry.124074799=Read"},
{"title": "CUQDS: Conformal Uncertainty Quantification under Distribution Shift for\n  Trajectory Prediction", "author": "Huiqun Huang and Sihong He and Fei Miao", "abstract": "  Trajectory prediction models that can infer both finite future trajectories\nand their associated uncertainties of the target vehicles in an online setting\n(e.g., real-world application scenarios) is crucial for ensuring the safe and\nrobust navigation and path planning of autonomous vehicle motion. However, the\nmajority of existing trajectory prediction models have neither considered\nreducing the uncertainty as one objective during the training stage nor\nprovided reliable uncertainty quantification during inference stage under\npotential distribution shift. Therefore, in this paper, we propose the\nConformal Uncertainty Quantification under Distribution Shift framework, CUQDS,\nto quantify the uncertainty of the predicted trajectories of existing\ntrajectory prediction models under potential data distribution shift, while\nconsidering improving the prediction accuracy of the models and reducing the\nestimated uncertainty during the training stage. Specifically, CUQDS includes\n1) a learning-based Gaussian process regression module that models the output\ndistribution of the base model (any existing trajectory prediction or time\nseries forecasting neural networks) and reduces the estimated uncertainty by\nadditional loss term, and 2) a statistical-based Conformal P control module to\ncalibrate the estimated uncertainty from the Gaussian process regression module\nin an online setting under potential distribution shift between training and\ntesting data.\n", "link": "http://arxiv.org/abs/2406.12100v3", "date": "2025-01-03", "relevancy": 1.5485, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5214}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5189}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CUQDS%3A%20Conformal%20Uncertainty%20Quantification%20under%20Distribution%20Shift%20for%0A%20%20Trajectory%20Prediction&body=Title%3A%20CUQDS%3A%20Conformal%20Uncertainty%20Quantification%20under%20Distribution%20Shift%20for%0A%20%20Trajectory%20Prediction%0AAuthor%3A%20Huiqun%20Huang%20and%20Sihong%20He%20and%20Fei%20Miao%0AAbstract%3A%20%20%20Trajectory%20prediction%20models%20that%20can%20infer%20both%20finite%20future%20trajectories%0Aand%20their%20associated%20uncertainties%20of%20the%20target%20vehicles%20in%20an%20online%20setting%0A%28e.g.%2C%20real-world%20application%20scenarios%29%20is%20crucial%20for%20ensuring%20the%20safe%20and%0Arobust%20navigation%20and%20path%20planning%20of%20autonomous%20vehicle%20motion.%20However%2C%20the%0Amajority%20of%20existing%20trajectory%20prediction%20models%20have%20neither%20considered%0Areducing%20the%20uncertainty%20as%20one%20objective%20during%20the%20training%20stage%20nor%0Aprovided%20reliable%20uncertainty%20quantification%20during%20inference%20stage%20under%0Apotential%20distribution%20shift.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20the%0AConformal%20Uncertainty%20Quantification%20under%20Distribution%20Shift%20framework%2C%20CUQDS%2C%0Ato%20quantify%20the%20uncertainty%20of%20the%20predicted%20trajectories%20of%20existing%0Atrajectory%20prediction%20models%20under%20potential%20data%20distribution%20shift%2C%20while%0Aconsidering%20improving%20the%20prediction%20accuracy%20of%20the%20models%20and%20reducing%20the%0Aestimated%20uncertainty%20during%20the%20training%20stage.%20Specifically%2C%20CUQDS%20includes%0A1%29%20a%20learning-based%20Gaussian%20process%20regression%20module%20that%20models%20the%20output%0Adistribution%20of%20the%20base%20model%20%28any%20existing%20trajectory%20prediction%20or%20time%0Aseries%20forecasting%20neural%20networks%29%20and%20reduces%20the%20estimated%20uncertainty%20by%0Aadditional%20loss%20term%2C%20and%202%29%20a%20statistical-based%20Conformal%20P%20control%20module%20to%0Acalibrate%20the%20estimated%20uncertainty%20from%20the%20Gaussian%20process%20regression%20module%0Ain%20an%20online%20setting%20under%20potential%20distribution%20shift%20between%20training%20and%0Atesting%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12100v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCUQDS%253A%2520Conformal%2520Uncertainty%2520Quantification%2520under%2520Distribution%2520Shift%2520for%250A%2520%2520Trajectory%2520Prediction%26entry.906535625%3DHuiqun%2520Huang%2520and%2520Sihong%2520He%2520and%2520Fei%2520Miao%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520models%2520that%2520can%2520infer%2520both%2520finite%2520future%2520trajectories%250Aand%2520their%2520associated%2520uncertainties%2520of%2520the%2520target%2520vehicles%2520in%2520an%2520online%2520setting%250A%2528e.g.%252C%2520real-world%2520application%2520scenarios%2529%2520is%2520crucial%2520for%2520ensuring%2520the%2520safe%2520and%250Arobust%2520navigation%2520and%2520path%2520planning%2520of%2520autonomous%2520vehicle%2520motion.%2520However%252C%2520the%250Amajority%2520of%2520existing%2520trajectory%2520prediction%2520models%2520have%2520neither%2520considered%250Areducing%2520the%2520uncertainty%2520as%2520one%2520objective%2520during%2520the%2520training%2520stage%2520nor%250Aprovided%2520reliable%2520uncertainty%2520quantification%2520during%2520inference%2520stage%2520under%250Apotential%2520distribution%2520shift.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520the%250AConformal%2520Uncertainty%2520Quantification%2520under%2520Distribution%2520Shift%2520framework%252C%2520CUQDS%252C%250Ato%2520quantify%2520the%2520uncertainty%2520of%2520the%2520predicted%2520trajectories%2520of%2520existing%250Atrajectory%2520prediction%2520models%2520under%2520potential%2520data%2520distribution%2520shift%252C%2520while%250Aconsidering%2520improving%2520the%2520prediction%2520accuracy%2520of%2520the%2520models%2520and%2520reducing%2520the%250Aestimated%2520uncertainty%2520during%2520the%2520training%2520stage.%2520Specifically%252C%2520CUQDS%2520includes%250A1%2529%2520a%2520learning-based%2520Gaussian%2520process%2520regression%2520module%2520that%2520models%2520the%2520output%250Adistribution%2520of%2520the%2520base%2520model%2520%2528any%2520existing%2520trajectory%2520prediction%2520or%2520time%250Aseries%2520forecasting%2520neural%2520networks%2529%2520and%2520reduces%2520the%2520estimated%2520uncertainty%2520by%250Aadditional%2520loss%2520term%252C%2520and%25202%2529%2520a%2520statistical-based%2520Conformal%2520P%2520control%2520module%2520to%250Acalibrate%2520the%2520estimated%2520uncertainty%2520from%2520the%2520Gaussian%2520process%2520regression%2520module%250Ain%2520an%2520online%2520setting%2520under%2520potential%2520distribution%2520shift%2520between%2520training%2520and%250Atesting%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12100v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CUQDS%3A%20Conformal%20Uncertainty%20Quantification%20under%20Distribution%20Shift%20for%0A%20%20Trajectory%20Prediction&entry.906535625=Huiqun%20Huang%20and%20Sihong%20He%20and%20Fei%20Miao&entry.1292438233=%20%20Trajectory%20prediction%20models%20that%20can%20infer%20both%20finite%20future%20trajectories%0Aand%20their%20associated%20uncertainties%20of%20the%20target%20vehicles%20in%20an%20online%20setting%0A%28e.g.%2C%20real-world%20application%20scenarios%29%20is%20crucial%20for%20ensuring%20the%20safe%20and%0Arobust%20navigation%20and%20path%20planning%20of%20autonomous%20vehicle%20motion.%20However%2C%20the%0Amajority%20of%20existing%20trajectory%20prediction%20models%20have%20neither%20considered%0Areducing%20the%20uncertainty%20as%20one%20objective%20during%20the%20training%20stage%20nor%0Aprovided%20reliable%20uncertainty%20quantification%20during%20inference%20stage%20under%0Apotential%20distribution%20shift.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20the%0AConformal%20Uncertainty%20Quantification%20under%20Distribution%20Shift%20framework%2C%20CUQDS%2C%0Ato%20quantify%20the%20uncertainty%20of%20the%20predicted%20trajectories%20of%20existing%0Atrajectory%20prediction%20models%20under%20potential%20data%20distribution%20shift%2C%20while%0Aconsidering%20improving%20the%20prediction%20accuracy%20of%20the%20models%20and%20reducing%20the%0Aestimated%20uncertainty%20during%20the%20training%20stage.%20Specifically%2C%20CUQDS%20includes%0A1%29%20a%20learning-based%20Gaussian%20process%20regression%20module%20that%20models%20the%20output%0Adistribution%20of%20the%20base%20model%20%28any%20existing%20trajectory%20prediction%20or%20time%0Aseries%20forecasting%20neural%20networks%29%20and%20reduces%20the%20estimated%20uncertainty%20by%0Aadditional%20loss%20term%2C%20and%202%29%20a%20statistical-based%20Conformal%20P%20control%20module%20to%0Acalibrate%20the%20estimated%20uncertainty%20from%20the%20Gaussian%20process%20regression%20module%0Ain%20an%20online%20setting%20under%20potential%20distribution%20shift%20between%20training%20and%0Atesting%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12100v3&entry.124074799=Read"},
{"title": "Agent Planning with World Knowledge Model", "author": "Shuofei Qiao and Runnan Fang and Ningyu Zhang and Yuqi Zhu and Xiang Chen and Shumin Deng and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen", "abstract": "  Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM.\n", "link": "http://arxiv.org/abs/2405.14205v4", "date": "2025-01-03", "relevancy": 1.5152, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.518}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5115}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Planning%20with%20World%20Knowledge%20Model&body=Title%3A%20Agent%20Planning%20with%20World%20Knowledge%20Model%0AAuthor%3A%20Shuofei%20Qiao%20and%20Runnan%20Fang%20and%20Ningyu%20Zhang%20and%20Yuqi%20Zhu%20and%20Xiang%20Chen%20and%20Shumin%20Deng%20and%20Yong%20Jiang%20and%20Pengjun%20Xie%20and%20Fei%20Huang%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20Recent%20endeavors%20towards%20directly%20using%20large%20language%20models%20%28LLMs%29%20as%20agent%0Amodels%20to%20execute%20interactive%20planning%20tasks%20have%20shown%20commendable%20results.%0ADespite%20their%20achievements%2C%20however%2C%20they%20still%20struggle%20with%20brainless%0Atrial-and-error%20in%20global%20planning%20and%20generating%20hallucinatory%20actions%20in%0Alocal%20planning%20due%20to%20their%20poor%20understanding%20of%20the%20%60%60real%27%27%20physical%20world.%0AImitating%20humans%27%20mental%20world%20knowledge%20model%20which%20provides%20global%20prior%0Aknowledge%20before%20the%20task%20and%20maintains%20local%20dynamic%20knowledge%20during%20the%0Atask%2C%20in%20this%20paper%2C%20we%20introduce%20parametric%20World%20Knowledge%20Model%20%28WKM%29%20to%0Afacilitate%20agent%20planning.%20Concretely%2C%20we%20steer%20the%20agent%20model%20to%0Aself-synthesize%20knowledge%20from%20both%20expert%20and%20sampled%20trajectories.%20Then%20we%0Adevelop%20WKM%2C%20providing%20prior%20task%20knowledge%20to%20guide%20the%20global%20planning%20and%0Adynamic%20state%20knowledge%20to%20assist%20the%20local%20planning.%20Experimental%20results%20on%0Athree%20complex%20real-world%20simulated%20datasets%20with%20three%20state-of-the-art%0Aopen-source%20LLMs%2C%20Mistral-7B%2C%20Gemma-7B%2C%20and%20Llama-3-8B%2C%20demonstrate%20that%20our%0Amethod%20can%20achieve%20superior%20performance%20compared%20to%20various%20strong%20baselines.%0ABesides%2C%20we%20analyze%20to%20illustrate%20that%20our%20WKM%20can%20effectively%20alleviate%20the%0Ablind%20trial-and-error%20and%20hallucinatory%20action%20issues%2C%20providing%20strong%20support%0Afor%20the%20agent%27s%20understanding%20of%20the%20world.%20Other%20interesting%20findings%20include%3A%0A1%29%20our%20instance-level%20task%20knowledge%20can%20generalize%20better%20to%20unseen%20tasks%2C%202%29%0Aweak%20WKM%20can%20guide%20strong%20agent%20model%20planning%2C%20and%203%29%20unified%20WKM%20training%20has%0Apromising%20potential%20for%20further%20development.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/WKM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14205v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Planning%2520with%2520World%2520Knowledge%2520Model%26entry.906535625%3DShuofei%2520Qiao%2520and%2520Runnan%2520Fang%2520and%2520Ningyu%2520Zhang%2520and%2520Yuqi%2520Zhu%2520and%2520Xiang%2520Chen%2520and%2520Shumin%2520Deng%2520and%2520Yong%2520Jiang%2520and%2520Pengjun%2520Xie%2520and%2520Fei%2520Huang%2520and%2520Huajun%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520endeavors%2520towards%2520directly%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520agent%250Amodels%2520to%2520execute%2520interactive%2520planning%2520tasks%2520have%2520shown%2520commendable%2520results.%250ADespite%2520their%2520achievements%252C%2520however%252C%2520they%2520still%2520struggle%2520with%2520brainless%250Atrial-and-error%2520in%2520global%2520planning%2520and%2520generating%2520hallucinatory%2520actions%2520in%250Alocal%2520planning%2520due%2520to%2520their%2520poor%2520understanding%2520of%2520the%2520%2560%2560real%2527%2527%2520physical%2520world.%250AImitating%2520humans%2527%2520mental%2520world%2520knowledge%2520model%2520which%2520provides%2520global%2520prior%250Aknowledge%2520before%2520the%2520task%2520and%2520maintains%2520local%2520dynamic%2520knowledge%2520during%2520the%250Atask%252C%2520in%2520this%2520paper%252C%2520we%2520introduce%2520parametric%2520World%2520Knowledge%2520Model%2520%2528WKM%2529%2520to%250Afacilitate%2520agent%2520planning.%2520Concretely%252C%2520we%2520steer%2520the%2520agent%2520model%2520to%250Aself-synthesize%2520knowledge%2520from%2520both%2520expert%2520and%2520sampled%2520trajectories.%2520Then%2520we%250Adevelop%2520WKM%252C%2520providing%2520prior%2520task%2520knowledge%2520to%2520guide%2520the%2520global%2520planning%2520and%250Adynamic%2520state%2520knowledge%2520to%2520assist%2520the%2520local%2520planning.%2520Experimental%2520results%2520on%250Athree%2520complex%2520real-world%2520simulated%2520datasets%2520with%2520three%2520state-of-the-art%250Aopen-source%2520LLMs%252C%2520Mistral-7B%252C%2520Gemma-7B%252C%2520and%2520Llama-3-8B%252C%2520demonstrate%2520that%2520our%250Amethod%2520can%2520achieve%2520superior%2520performance%2520compared%2520to%2520various%2520strong%2520baselines.%250ABesides%252C%2520we%2520analyze%2520to%2520illustrate%2520that%2520our%2520WKM%2520can%2520effectively%2520alleviate%2520the%250Ablind%2520trial-and-error%2520and%2520hallucinatory%2520action%2520issues%252C%2520providing%2520strong%2520support%250Afor%2520the%2520agent%2527s%2520understanding%2520of%2520the%2520world.%2520Other%2520interesting%2520findings%2520include%253A%250A1%2529%2520our%2520instance-level%2520task%2520knowledge%2520can%2520generalize%2520better%2520to%2520unseen%2520tasks%252C%25202%2529%250Aweak%2520WKM%2520can%2520guide%2520strong%2520agent%2520model%2520planning%252C%2520and%25203%2529%2520unified%2520WKM%2520training%2520has%250Apromising%2520potential%2520for%2520further%2520development.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zjunlp/WKM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14205v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Planning%20with%20World%20Knowledge%20Model&entry.906535625=Shuofei%20Qiao%20and%20Runnan%20Fang%20and%20Ningyu%20Zhang%20and%20Yuqi%20Zhu%20and%20Xiang%20Chen%20and%20Shumin%20Deng%20and%20Yong%20Jiang%20and%20Pengjun%20Xie%20and%20Fei%20Huang%20and%20Huajun%20Chen&entry.1292438233=%20%20Recent%20endeavors%20towards%20directly%20using%20large%20language%20models%20%28LLMs%29%20as%20agent%0Amodels%20to%20execute%20interactive%20planning%20tasks%20have%20shown%20commendable%20results.%0ADespite%20their%20achievements%2C%20however%2C%20they%20still%20struggle%20with%20brainless%0Atrial-and-error%20in%20global%20planning%20and%20generating%20hallucinatory%20actions%20in%0Alocal%20planning%20due%20to%20their%20poor%20understanding%20of%20the%20%60%60real%27%27%20physical%20world.%0AImitating%20humans%27%20mental%20world%20knowledge%20model%20which%20provides%20global%20prior%0Aknowledge%20before%20the%20task%20and%20maintains%20local%20dynamic%20knowledge%20during%20the%0Atask%2C%20in%20this%20paper%2C%20we%20introduce%20parametric%20World%20Knowledge%20Model%20%28WKM%29%20to%0Afacilitate%20agent%20planning.%20Concretely%2C%20we%20steer%20the%20agent%20model%20to%0Aself-synthesize%20knowledge%20from%20both%20expert%20and%20sampled%20trajectories.%20Then%20we%0Adevelop%20WKM%2C%20providing%20prior%20task%20knowledge%20to%20guide%20the%20global%20planning%20and%0Adynamic%20state%20knowledge%20to%20assist%20the%20local%20planning.%20Experimental%20results%20on%0Athree%20complex%20real-world%20simulated%20datasets%20with%20three%20state-of-the-art%0Aopen-source%20LLMs%2C%20Mistral-7B%2C%20Gemma-7B%2C%20and%20Llama-3-8B%2C%20demonstrate%20that%20our%0Amethod%20can%20achieve%20superior%20performance%20compared%20to%20various%20strong%20baselines.%0ABesides%2C%20we%20analyze%20to%20illustrate%20that%20our%20WKM%20can%20effectively%20alleviate%20the%0Ablind%20trial-and-error%20and%20hallucinatory%20action%20issues%2C%20providing%20strong%20support%0Afor%20the%20agent%27s%20understanding%20of%20the%20world.%20Other%20interesting%20findings%20include%3A%0A1%29%20our%20instance-level%20task%20knowledge%20can%20generalize%20better%20to%20unseen%20tasks%2C%202%29%0Aweak%20WKM%20can%20guide%20strong%20agent%20model%20planning%2C%20and%203%29%20unified%20WKM%20training%20has%0Apromising%20potential%20for%20further%20development.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/WKM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14205v4&entry.124074799=Read"},
{"title": "Physics-constrained coupled neural differential equations for one\n  dimensional blood flow modeling", "author": "Hunor Csala and Arvind Mohan and Daniel Livescu and Amirhossein Arzani", "abstract": "  Computational cardiovascular flow modeling plays a crucial role in\nunderstanding blood flow dynamics. While 3D models provide acute details, they\nare computationally expensive, especially with fluid-structure interaction\n(FSI) simulations. 1D models offer a computationally efficient alternative, by\nsimplifying the 3D Navier-Stokes equations through axisymmetric flow assumption\nand cross-sectional averaging. However, traditional 1D models based on finite\nelement methods (FEM) often lack accuracy compared to 3D averaged solutions.\nThis study introduces a novel physics-constrained machine learning technique\nthat enhances the accuracy of 1D blood flow models while maintaining\ncomputational efficiency. Our approach, utilizing a physics-constrained coupled\nneural differential equation (PCNDE) framework, demonstrates superior\nperformance compared to conventional FEM-based 1D models across a wide range of\ninlet boundary condition waveforms and stenosis blockage ratios. A key\ninnovation lies in the spatial formulation of the momentum conservation\nequation, departing from the traditional temporal approach and capitalizing on\nthe inherent temporal periodicity of blood flow. This spatial neural\ndifferential equation formulation switches space and time and overcomes issues\nrelated to coupling stability and smoothness, while simplifying boundary\ncondition implementation. The model accurately captures flow rate, area, and\npressure variations for unseen waveforms and geometries. We evaluate the\nmodel's robustness to input noise and explore the loss landscapes associated\nwith the inclusion of different physics terms. This advanced 1D modeling\ntechnique offers promising potential for rapid cardiovascular simulations,\nachieving computational efficiency and accuracy. By combining the strengths of\nphysics-based and data-driven modeling, this approach enables fast and accurate\ncardiovascular simulations.\n", "link": "http://arxiv.org/abs/2411.05631v2", "date": "2025-01-03", "relevancy": 1.509, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5339}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5041}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-constrained%20coupled%20neural%20differential%20equations%20for%20one%0A%20%20dimensional%20blood%20flow%20modeling&body=Title%3A%20Physics-constrained%20coupled%20neural%20differential%20equations%20for%20one%0A%20%20dimensional%20blood%20flow%20modeling%0AAuthor%3A%20Hunor%20Csala%20and%20Arvind%20Mohan%20and%20Daniel%20Livescu%20and%20Amirhossein%20Arzani%0AAbstract%3A%20%20%20Computational%20cardiovascular%20flow%20modeling%20plays%20a%20crucial%20role%20in%0Aunderstanding%20blood%20flow%20dynamics.%20While%203D%20models%20provide%20acute%20details%2C%20they%0Aare%20computationally%20expensive%2C%20especially%20with%20fluid-structure%20interaction%0A%28FSI%29%20simulations.%201D%20models%20offer%20a%20computationally%20efficient%20alternative%2C%20by%0Asimplifying%20the%203D%20Navier-Stokes%20equations%20through%20axisymmetric%20flow%20assumption%0Aand%20cross-sectional%20averaging.%20However%2C%20traditional%201D%20models%20based%20on%20finite%0Aelement%20methods%20%28FEM%29%20often%20lack%20accuracy%20compared%20to%203D%20averaged%20solutions.%0AThis%20study%20introduces%20a%20novel%20physics-constrained%20machine%20learning%20technique%0Athat%20enhances%20the%20accuracy%20of%201D%20blood%20flow%20models%20while%20maintaining%0Acomputational%20efficiency.%20Our%20approach%2C%20utilizing%20a%20physics-constrained%20coupled%0Aneural%20differential%20equation%20%28PCNDE%29%20framework%2C%20demonstrates%20superior%0Aperformance%20compared%20to%20conventional%20FEM-based%201D%20models%20across%20a%20wide%20range%20of%0Ainlet%20boundary%20condition%20waveforms%20and%20stenosis%20blockage%20ratios.%20A%20key%0Ainnovation%20lies%20in%20the%20spatial%20formulation%20of%20the%20momentum%20conservation%0Aequation%2C%20departing%20from%20the%20traditional%20temporal%20approach%20and%20capitalizing%20on%0Athe%20inherent%20temporal%20periodicity%20of%20blood%20flow.%20This%20spatial%20neural%0Adifferential%20equation%20formulation%20switches%20space%20and%20time%20and%20overcomes%20issues%0Arelated%20to%20coupling%20stability%20and%20smoothness%2C%20while%20simplifying%20boundary%0Acondition%20implementation.%20The%20model%20accurately%20captures%20flow%20rate%2C%20area%2C%20and%0Apressure%20variations%20for%20unseen%20waveforms%20and%20geometries.%20We%20evaluate%20the%0Amodel%27s%20robustness%20to%20input%20noise%20and%20explore%20the%20loss%20landscapes%20associated%0Awith%20the%20inclusion%20of%20different%20physics%20terms.%20This%20advanced%201D%20modeling%0Atechnique%20offers%20promising%20potential%20for%20rapid%20cardiovascular%20simulations%2C%0Aachieving%20computational%20efficiency%20and%20accuracy.%20By%20combining%20the%20strengths%20of%0Aphysics-based%20and%20data-driven%20modeling%2C%20this%20approach%20enables%20fast%20and%20accurate%0Acardiovascular%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-constrained%2520coupled%2520neural%2520differential%2520equations%2520for%2520one%250A%2520%2520dimensional%2520blood%2520flow%2520modeling%26entry.906535625%3DHunor%2520Csala%2520and%2520Arvind%2520Mohan%2520and%2520Daniel%2520Livescu%2520and%2520Amirhossein%2520Arzani%26entry.1292438233%3D%2520%2520Computational%2520cardiovascular%2520flow%2520modeling%2520plays%2520a%2520crucial%2520role%2520in%250Aunderstanding%2520blood%2520flow%2520dynamics.%2520While%25203D%2520models%2520provide%2520acute%2520details%252C%2520they%250Aare%2520computationally%2520expensive%252C%2520especially%2520with%2520fluid-structure%2520interaction%250A%2528FSI%2529%2520simulations.%25201D%2520models%2520offer%2520a%2520computationally%2520efficient%2520alternative%252C%2520by%250Asimplifying%2520the%25203D%2520Navier-Stokes%2520equations%2520through%2520axisymmetric%2520flow%2520assumption%250Aand%2520cross-sectional%2520averaging.%2520However%252C%2520traditional%25201D%2520models%2520based%2520on%2520finite%250Aelement%2520methods%2520%2528FEM%2529%2520often%2520lack%2520accuracy%2520compared%2520to%25203D%2520averaged%2520solutions.%250AThis%2520study%2520introduces%2520a%2520novel%2520physics-constrained%2520machine%2520learning%2520technique%250Athat%2520enhances%2520the%2520accuracy%2520of%25201D%2520blood%2520flow%2520models%2520while%2520maintaining%250Acomputational%2520efficiency.%2520Our%2520approach%252C%2520utilizing%2520a%2520physics-constrained%2520coupled%250Aneural%2520differential%2520equation%2520%2528PCNDE%2529%2520framework%252C%2520demonstrates%2520superior%250Aperformance%2520compared%2520to%2520conventional%2520FEM-based%25201D%2520models%2520across%2520a%2520wide%2520range%2520of%250Ainlet%2520boundary%2520condition%2520waveforms%2520and%2520stenosis%2520blockage%2520ratios.%2520A%2520key%250Ainnovation%2520lies%2520in%2520the%2520spatial%2520formulation%2520of%2520the%2520momentum%2520conservation%250Aequation%252C%2520departing%2520from%2520the%2520traditional%2520temporal%2520approach%2520and%2520capitalizing%2520on%250Athe%2520inherent%2520temporal%2520periodicity%2520of%2520blood%2520flow.%2520This%2520spatial%2520neural%250Adifferential%2520equation%2520formulation%2520switches%2520space%2520and%2520time%2520and%2520overcomes%2520issues%250Arelated%2520to%2520coupling%2520stability%2520and%2520smoothness%252C%2520while%2520simplifying%2520boundary%250Acondition%2520implementation.%2520The%2520model%2520accurately%2520captures%2520flow%2520rate%252C%2520area%252C%2520and%250Apressure%2520variations%2520for%2520unseen%2520waveforms%2520and%2520geometries.%2520We%2520evaluate%2520the%250Amodel%2527s%2520robustness%2520to%2520input%2520noise%2520and%2520explore%2520the%2520loss%2520landscapes%2520associated%250Awith%2520the%2520inclusion%2520of%2520different%2520physics%2520terms.%2520This%2520advanced%25201D%2520modeling%250Atechnique%2520offers%2520promising%2520potential%2520for%2520rapid%2520cardiovascular%2520simulations%252C%250Aachieving%2520computational%2520efficiency%2520and%2520accuracy.%2520By%2520combining%2520the%2520strengths%2520of%250Aphysics-based%2520and%2520data-driven%2520modeling%252C%2520this%2520approach%2520enables%2520fast%2520and%2520accurate%250Acardiovascular%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-constrained%20coupled%20neural%20differential%20equations%20for%20one%0A%20%20dimensional%20blood%20flow%20modeling&entry.906535625=Hunor%20Csala%20and%20Arvind%20Mohan%20and%20Daniel%20Livescu%20and%20Amirhossein%20Arzani&entry.1292438233=%20%20Computational%20cardiovascular%20flow%20modeling%20plays%20a%20crucial%20role%20in%0Aunderstanding%20blood%20flow%20dynamics.%20While%203D%20models%20provide%20acute%20details%2C%20they%0Aare%20computationally%20expensive%2C%20especially%20with%20fluid-structure%20interaction%0A%28FSI%29%20simulations.%201D%20models%20offer%20a%20computationally%20efficient%20alternative%2C%20by%0Asimplifying%20the%203D%20Navier-Stokes%20equations%20through%20axisymmetric%20flow%20assumption%0Aand%20cross-sectional%20averaging.%20However%2C%20traditional%201D%20models%20based%20on%20finite%0Aelement%20methods%20%28FEM%29%20often%20lack%20accuracy%20compared%20to%203D%20averaged%20solutions.%0AThis%20study%20introduces%20a%20novel%20physics-constrained%20machine%20learning%20technique%0Athat%20enhances%20the%20accuracy%20of%201D%20blood%20flow%20models%20while%20maintaining%0Acomputational%20efficiency.%20Our%20approach%2C%20utilizing%20a%20physics-constrained%20coupled%0Aneural%20differential%20equation%20%28PCNDE%29%20framework%2C%20demonstrates%20superior%0Aperformance%20compared%20to%20conventional%20FEM-based%201D%20models%20across%20a%20wide%20range%20of%0Ainlet%20boundary%20condition%20waveforms%20and%20stenosis%20blockage%20ratios.%20A%20key%0Ainnovation%20lies%20in%20the%20spatial%20formulation%20of%20the%20momentum%20conservation%0Aequation%2C%20departing%20from%20the%20traditional%20temporal%20approach%20and%20capitalizing%20on%0Athe%20inherent%20temporal%20periodicity%20of%20blood%20flow.%20This%20spatial%20neural%0Adifferential%20equation%20formulation%20switches%20space%20and%20time%20and%20overcomes%20issues%0Arelated%20to%20coupling%20stability%20and%20smoothness%2C%20while%20simplifying%20boundary%0Acondition%20implementation.%20The%20model%20accurately%20captures%20flow%20rate%2C%20area%2C%20and%0Apressure%20variations%20for%20unseen%20waveforms%20and%20geometries.%20We%20evaluate%20the%0Amodel%27s%20robustness%20to%20input%20noise%20and%20explore%20the%20loss%20landscapes%20associated%0Awith%20the%20inclusion%20of%20different%20physics%20terms.%20This%20advanced%201D%20modeling%0Atechnique%20offers%20promising%20potential%20for%20rapid%20cardiovascular%20simulations%2C%0Aachieving%20computational%20efficiency%20and%20accuracy.%20By%20combining%20the%20strengths%20of%0Aphysics-based%20and%20data-driven%20modeling%2C%20this%20approach%20enables%20fast%20and%20accurate%0Acardiovascular%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05631v2&entry.124074799=Read"},
{"title": "Detecting and Mitigating Adversarial Attacks on Deep Learning-Based MRI\n  Reconstruction Without Any Retraining", "author": "Mahdi Saberi and Chi Zhang and Mehmet Akcakaya", "abstract": "  Deep learning (DL) methods, especially those based on physics-driven DL, have\nbecome the state-of-the-art for reconstructing sub-sampled magnetic resonance\nimaging (MRI) data. However, studies have shown that these methods are\nsusceptible to small adversarial input perturbations, or attacks, resulting in\nmajor distortions in the output images. Various strategies have been proposed\nto reduce the effects of these attacks, but they require retraining and may\nlower reconstruction quality for non-perturbed/clean inputs. In this work, we\npropose a novel approach for detecting and mitigating adversarial attacks on\nMRI reconstruction models without any retraining. Our detection strategy is\nbased on the idea of cyclic measurement consistency. The output of the model is\nmapped to another set of MRI measurements for a different sub-sampling pattern,\nand this synthesized data is reconstructed with the same model. Intuitively,\nwithout an attack, the second reconstruction is expected to be consistent with\nthe first, while with an attack, disruptions are present. Subsequently, this\nidea is extended to devise a novel objective function, which is minimized\nwithin a small ball around the attack input for mitigation. Experimental\nresults show that our method substantially reduces the impact of adversarial\nperturbations across different datasets, attack types/strengths and PD-DL\nnetworks, and qualitatively and quantitatively outperforms conventional\nmitigation methods that involve retraining.\n", "link": "http://arxiv.org/abs/2501.01908v1", "date": "2025-01-03", "relevancy": 1.5029, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5238}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4976}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Mitigating%20Adversarial%20Attacks%20on%20Deep%20Learning-Based%20MRI%0A%20%20Reconstruction%20Without%20Any%20Retraining&body=Title%3A%20Detecting%20and%20Mitigating%20Adversarial%20Attacks%20on%20Deep%20Learning-Based%20MRI%0A%20%20Reconstruction%20Without%20Any%20Retraining%0AAuthor%3A%20Mahdi%20Saberi%20and%20Chi%20Zhang%20and%20Mehmet%20Akcakaya%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20methods%2C%20especially%20those%20based%20on%20physics-driven%20DL%2C%20have%0Abecome%20the%20state-of-the-art%20for%20reconstructing%20sub-sampled%20magnetic%20resonance%0Aimaging%20%28MRI%29%20data.%20However%2C%20studies%20have%20shown%20that%20these%20methods%20are%0Asusceptible%20to%20small%20adversarial%20input%20perturbations%2C%20or%20attacks%2C%20resulting%20in%0Amajor%20distortions%20in%20the%20output%20images.%20Various%20strategies%20have%20been%20proposed%0Ato%20reduce%20the%20effects%20of%20these%20attacks%2C%20but%20they%20require%20retraining%20and%20may%0Alower%20reconstruction%20quality%20for%20non-perturbed/clean%20inputs.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20approach%20for%20detecting%20and%20mitigating%20adversarial%20attacks%20on%0AMRI%20reconstruction%20models%20without%20any%20retraining.%20Our%20detection%20strategy%20is%0Abased%20on%20the%20idea%20of%20cyclic%20measurement%20consistency.%20The%20output%20of%20the%20model%20is%0Amapped%20to%20another%20set%20of%20MRI%20measurements%20for%20a%20different%20sub-sampling%20pattern%2C%0Aand%20this%20synthesized%20data%20is%20reconstructed%20with%20the%20same%20model.%20Intuitively%2C%0Awithout%20an%20attack%2C%20the%20second%20reconstruction%20is%20expected%20to%20be%20consistent%20with%0Athe%20first%2C%20while%20with%20an%20attack%2C%20disruptions%20are%20present.%20Subsequently%2C%20this%0Aidea%20is%20extended%20to%20devise%20a%20novel%20objective%20function%2C%20which%20is%20minimized%0Awithin%20a%20small%20ball%20around%20the%20attack%20input%20for%20mitigation.%20Experimental%0Aresults%20show%20that%20our%20method%20substantially%20reduces%20the%20impact%20of%20adversarial%0Aperturbations%20across%20different%20datasets%2C%20attack%20types/strengths%20and%20PD-DL%0Anetworks%2C%20and%20qualitatively%20and%20quantitatively%20outperforms%20conventional%0Amitigation%20methods%20that%20involve%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Mitigating%2520Adversarial%2520Attacks%2520on%2520Deep%2520Learning-Based%2520MRI%250A%2520%2520Reconstruction%2520Without%2520Any%2520Retraining%26entry.906535625%3DMahdi%2520Saberi%2520and%2520Chi%2520Zhang%2520and%2520Mehmet%2520Akcakaya%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520methods%252C%2520especially%2520those%2520based%2520on%2520physics-driven%2520DL%252C%2520have%250Abecome%2520the%2520state-of-the-art%2520for%2520reconstructing%2520sub-sampled%2520magnetic%2520resonance%250Aimaging%2520%2528MRI%2529%2520data.%2520However%252C%2520studies%2520have%2520shown%2520that%2520these%2520methods%2520are%250Asusceptible%2520to%2520small%2520adversarial%2520input%2520perturbations%252C%2520or%2520attacks%252C%2520resulting%2520in%250Amajor%2520distortions%2520in%2520the%2520output%2520images.%2520Various%2520strategies%2520have%2520been%2520proposed%250Ato%2520reduce%2520the%2520effects%2520of%2520these%2520attacks%252C%2520but%2520they%2520require%2520retraining%2520and%2520may%250Alower%2520reconstruction%2520quality%2520for%2520non-perturbed/clean%2520inputs.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520for%2520detecting%2520and%2520mitigating%2520adversarial%2520attacks%2520on%250AMRI%2520reconstruction%2520models%2520without%2520any%2520retraining.%2520Our%2520detection%2520strategy%2520is%250Abased%2520on%2520the%2520idea%2520of%2520cyclic%2520measurement%2520consistency.%2520The%2520output%2520of%2520the%2520model%2520is%250Amapped%2520to%2520another%2520set%2520of%2520MRI%2520measurements%2520for%2520a%2520different%2520sub-sampling%2520pattern%252C%250Aand%2520this%2520synthesized%2520data%2520is%2520reconstructed%2520with%2520the%2520same%2520model.%2520Intuitively%252C%250Awithout%2520an%2520attack%252C%2520the%2520second%2520reconstruction%2520is%2520expected%2520to%2520be%2520consistent%2520with%250Athe%2520first%252C%2520while%2520with%2520an%2520attack%252C%2520disruptions%2520are%2520present.%2520Subsequently%252C%2520this%250Aidea%2520is%2520extended%2520to%2520devise%2520a%2520novel%2520objective%2520function%252C%2520which%2520is%2520minimized%250Awithin%2520a%2520small%2520ball%2520around%2520the%2520attack%2520input%2520for%2520mitigation.%2520Experimental%250Aresults%2520show%2520that%2520our%2520method%2520substantially%2520reduces%2520the%2520impact%2520of%2520adversarial%250Aperturbations%2520across%2520different%2520datasets%252C%2520attack%2520types/strengths%2520and%2520PD-DL%250Anetworks%252C%2520and%2520qualitatively%2520and%2520quantitatively%2520outperforms%2520conventional%250Amitigation%2520methods%2520that%2520involve%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Mitigating%20Adversarial%20Attacks%20on%20Deep%20Learning-Based%20MRI%0A%20%20Reconstruction%20Without%20Any%20Retraining&entry.906535625=Mahdi%20Saberi%20and%20Chi%20Zhang%20and%20Mehmet%20Akcakaya&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20methods%2C%20especially%20those%20based%20on%20physics-driven%20DL%2C%20have%0Abecome%20the%20state-of-the-art%20for%20reconstructing%20sub-sampled%20magnetic%20resonance%0Aimaging%20%28MRI%29%20data.%20However%2C%20studies%20have%20shown%20that%20these%20methods%20are%0Asusceptible%20to%20small%20adversarial%20input%20perturbations%2C%20or%20attacks%2C%20resulting%20in%0Amajor%20distortions%20in%20the%20output%20images.%20Various%20strategies%20have%20been%20proposed%0Ato%20reduce%20the%20effects%20of%20these%20attacks%2C%20but%20they%20require%20retraining%20and%20may%0Alower%20reconstruction%20quality%20for%20non-perturbed/clean%20inputs.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20approach%20for%20detecting%20and%20mitigating%20adversarial%20attacks%20on%0AMRI%20reconstruction%20models%20without%20any%20retraining.%20Our%20detection%20strategy%20is%0Abased%20on%20the%20idea%20of%20cyclic%20measurement%20consistency.%20The%20output%20of%20the%20model%20is%0Amapped%20to%20another%20set%20of%20MRI%20measurements%20for%20a%20different%20sub-sampling%20pattern%2C%0Aand%20this%20synthesized%20data%20is%20reconstructed%20with%20the%20same%20model.%20Intuitively%2C%0Awithout%20an%20attack%2C%20the%20second%20reconstruction%20is%20expected%20to%20be%20consistent%20with%0Athe%20first%2C%20while%20with%20an%20attack%2C%20disruptions%20are%20present.%20Subsequently%2C%20this%0Aidea%20is%20extended%20to%20devise%20a%20novel%20objective%20function%2C%20which%20is%20minimized%0Awithin%20a%20small%20ball%20around%20the%20attack%20input%20for%20mitigation.%20Experimental%0Aresults%20show%20that%20our%20method%20substantially%20reduces%20the%20impact%20of%20adversarial%0Aperturbations%20across%20different%20datasets%2C%20attack%20types/strengths%20and%20PD-DL%0Anetworks%2C%20and%20qualitatively%20and%20quantitatively%20outperforms%20conventional%0Amitigation%20methods%20that%20involve%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01908v1&entry.124074799=Read"},
{"title": "Improving Transducer-Based Spoken Language Understanding with\n  Self-Conditioned CTC and Knowledge Transfer", "author": "Vishal Sunder and Eric Fosler-Lussier", "abstract": "  In this paper, we propose to improve end-to-end (E2E) spoken language\nunderstand (SLU) in an RNN transducer model (RNN-T) by incorporating a joint\nself-conditioned CTC automatic speech recognition (ASR) objective. Our proposed\nmodel is akin to an E2E differentiable cascaded model which performs ASR and\nSLU sequentially and we ensure that the SLU task is conditioned on the ASR task\nby having CTC self conditioning. This novel joint modeling of ASR and SLU\nimproves SLU performance significantly over just using SLU optimization. We\nfurther improve the performance by aligning the acoustic embeddings of this\nmodel with the semantically richer BERT model. Our proposed knowledge transfer\nstrategy makes use of a bag-of-entity prediction layer on the aligned\nembeddings and the output of this is used to condition the RNN-T based SLU\ndecoding. These techniques show significant improvement over several strong\nbaselines and can perform at par with large models like Whisper with\nsignificantly fewer parameters.\n", "link": "http://arxiv.org/abs/2501.01936v1", "date": "2025-01-03", "relevancy": 1.4934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4956}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Transducer-Based%20Spoken%20Language%20Understanding%20with%0A%20%20Self-Conditioned%20CTC%20and%20Knowledge%20Transfer&body=Title%3A%20Improving%20Transducer-Based%20Spoken%20Language%20Understanding%20with%0A%20%20Self-Conditioned%20CTC%20and%20Knowledge%20Transfer%0AAuthor%3A%20Vishal%20Sunder%20and%20Eric%20Fosler-Lussier%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20to%20improve%20end-to-end%20%28E2E%29%20spoken%20language%0Aunderstand%20%28SLU%29%20in%20an%20RNN%20transducer%20model%20%28RNN-T%29%20by%20incorporating%20a%20joint%0Aself-conditioned%20CTC%20automatic%20speech%20recognition%20%28ASR%29%20objective.%20Our%20proposed%0Amodel%20is%20akin%20to%20an%20E2E%20differentiable%20cascaded%20model%20which%20performs%20ASR%20and%0ASLU%20sequentially%20and%20we%20ensure%20that%20the%20SLU%20task%20is%20conditioned%20on%20the%20ASR%20task%0Aby%20having%20CTC%20self%20conditioning.%20This%20novel%20joint%20modeling%20of%20ASR%20and%20SLU%0Aimproves%20SLU%20performance%20significantly%20over%20just%20using%20SLU%20optimization.%20We%0Afurther%20improve%20the%20performance%20by%20aligning%20the%20acoustic%20embeddings%20of%20this%0Amodel%20with%20the%20semantically%20richer%20BERT%20model.%20Our%20proposed%20knowledge%20transfer%0Astrategy%20makes%20use%20of%20a%20bag-of-entity%20prediction%20layer%20on%20the%20aligned%0Aembeddings%20and%20the%20output%20of%20this%20is%20used%20to%20condition%20the%20RNN-T%20based%20SLU%0Adecoding.%20These%20techniques%20show%20significant%20improvement%20over%20several%20strong%0Abaselines%20and%20can%20perform%20at%20par%20with%20large%20models%20like%20Whisper%20with%0Asignificantly%20fewer%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Transducer-Based%2520Spoken%2520Language%2520Understanding%2520with%250A%2520%2520Self-Conditioned%2520CTC%2520and%2520Knowledge%2520Transfer%26entry.906535625%3DVishal%2520Sunder%2520and%2520Eric%2520Fosler-Lussier%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520improve%2520end-to-end%2520%2528E2E%2529%2520spoken%2520language%250Aunderstand%2520%2528SLU%2529%2520in%2520an%2520RNN%2520transducer%2520model%2520%2528RNN-T%2529%2520by%2520incorporating%2520a%2520joint%250Aself-conditioned%2520CTC%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520objective.%2520Our%2520proposed%250Amodel%2520is%2520akin%2520to%2520an%2520E2E%2520differentiable%2520cascaded%2520model%2520which%2520performs%2520ASR%2520and%250ASLU%2520sequentially%2520and%2520we%2520ensure%2520that%2520the%2520SLU%2520task%2520is%2520conditioned%2520on%2520the%2520ASR%2520task%250Aby%2520having%2520CTC%2520self%2520conditioning.%2520This%2520novel%2520joint%2520modeling%2520of%2520ASR%2520and%2520SLU%250Aimproves%2520SLU%2520performance%2520significantly%2520over%2520just%2520using%2520SLU%2520optimization.%2520We%250Afurther%2520improve%2520the%2520performance%2520by%2520aligning%2520the%2520acoustic%2520embeddings%2520of%2520this%250Amodel%2520with%2520the%2520semantically%2520richer%2520BERT%2520model.%2520Our%2520proposed%2520knowledge%2520transfer%250Astrategy%2520makes%2520use%2520of%2520a%2520bag-of-entity%2520prediction%2520layer%2520on%2520the%2520aligned%250Aembeddings%2520and%2520the%2520output%2520of%2520this%2520is%2520used%2520to%2520condition%2520the%2520RNN-T%2520based%2520SLU%250Adecoding.%2520These%2520techniques%2520show%2520significant%2520improvement%2520over%2520several%2520strong%250Abaselines%2520and%2520can%2520perform%2520at%2520par%2520with%2520large%2520models%2520like%2520Whisper%2520with%250Asignificantly%2520fewer%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Transducer-Based%20Spoken%20Language%20Understanding%20with%0A%20%20Self-Conditioned%20CTC%20and%20Knowledge%20Transfer&entry.906535625=Vishal%20Sunder%20and%20Eric%20Fosler-Lussier&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20to%20improve%20end-to-end%20%28E2E%29%20spoken%20language%0Aunderstand%20%28SLU%29%20in%20an%20RNN%20transducer%20model%20%28RNN-T%29%20by%20incorporating%20a%20joint%0Aself-conditioned%20CTC%20automatic%20speech%20recognition%20%28ASR%29%20objective.%20Our%20proposed%0Amodel%20is%20akin%20to%20an%20E2E%20differentiable%20cascaded%20model%20which%20performs%20ASR%20and%0ASLU%20sequentially%20and%20we%20ensure%20that%20the%20SLU%20task%20is%20conditioned%20on%20the%20ASR%20task%0Aby%20having%20CTC%20self%20conditioning.%20This%20novel%20joint%20modeling%20of%20ASR%20and%20SLU%0Aimproves%20SLU%20performance%20significantly%20over%20just%20using%20SLU%20optimization.%20We%0Afurther%20improve%20the%20performance%20by%20aligning%20the%20acoustic%20embeddings%20of%20this%0Amodel%20with%20the%20semantically%20richer%20BERT%20model.%20Our%20proposed%20knowledge%20transfer%0Astrategy%20makes%20use%20of%20a%20bag-of-entity%20prediction%20layer%20on%20the%20aligned%0Aembeddings%20and%20the%20output%20of%20this%20is%20used%20to%20condition%20the%20RNN-T%20based%20SLU%0Adecoding.%20These%20techniques%20show%20significant%20improvement%20over%20several%20strong%0Abaselines%20and%20can%20perform%20at%20par%20with%20large%20models%20like%20Whisper%20with%0Asignificantly%20fewer%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01936v1&entry.124074799=Read"},
{"title": "LMS-AutoTSF: Learnable Multi-Scale Decomposition and Integrated\n  Autocorrelation for Time Series Forecasting", "author": "Ibrahim Delibasoglu and Sanjay Chakraborty and Fredrik Heintz", "abstract": "  Time series forecasting is an important challenge with significant\napplications in areas such as weather prediction, stock market analysis,\nscientific simulations and industrial process analysis. In this work, we\nintroduce LMS-AutoTSF, a novel time series forecasting architecture that\nincorporates autocorrelation while leveraging dual encoders operating at\nmultiple scales. Unlike models that rely on predefined trend and seasonal\ncomponents, LMS-AutoTSF employs two separate encoders per scale: one focusing\non low-pass filtering to capture trends and the other utilizing high-pass\nfiltering to model seasonal variations. These filters are learnable, allowing\nthe model to dynamically adapt and isolate trend and seasonal components\ndirectly in the frequency domain. A key innovation in our approach is the\nintegration of autocorrelation, achieved by computing lagged differences in\ntime steps, which enables the model to capture dependencies across time more\neffectively. Each encoder processes the input through fully connected layers to\nhandle temporal and channel interactions. By combining frequency-domain\nfiltering, autocorrelation-based temporal modeling, and channel-wise\ntransformations, LMS-AutoTSF not only accurately captures long-term\ndependencies and fine-grained patterns but also operates more efficiently\ncompared to other state-of-the-art methods. Its lightweight design ensures\nfaster processing while maintaining high precision in forecasting across\ndiverse time horizons. The source code is publicly available at\n\\url{http://github.com/mribrahim/LMS-TSF}\n", "link": "http://arxiv.org/abs/2412.06866v2", "date": "2025-01-03", "relevancy": 1.4282, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMS-AutoTSF%3A%20Learnable%20Multi-Scale%20Decomposition%20and%20Integrated%0A%20%20Autocorrelation%20for%20Time%20Series%20Forecasting&body=Title%3A%20LMS-AutoTSF%3A%20Learnable%20Multi-Scale%20Decomposition%20and%20Integrated%0A%20%20Autocorrelation%20for%20Time%20Series%20Forecasting%0AAuthor%3A%20Ibrahim%20Delibasoglu%20and%20Sanjay%20Chakraborty%20and%20Fredrik%20Heintz%0AAbstract%3A%20%20%20Time%20series%20forecasting%20is%20an%20important%20challenge%20with%20significant%0Aapplications%20in%20areas%20such%20as%20weather%20prediction%2C%20stock%20market%20analysis%2C%0Ascientific%20simulations%20and%20industrial%20process%20analysis.%20In%20this%20work%2C%20we%0Aintroduce%20LMS-AutoTSF%2C%20a%20novel%20time%20series%20forecasting%20architecture%20that%0Aincorporates%20autocorrelation%20while%20leveraging%20dual%20encoders%20operating%20at%0Amultiple%20scales.%20Unlike%20models%20that%20rely%20on%20predefined%20trend%20and%20seasonal%0Acomponents%2C%20LMS-AutoTSF%20employs%20two%20separate%20encoders%20per%20scale%3A%20one%20focusing%0Aon%20low-pass%20filtering%20to%20capture%20trends%20and%20the%20other%20utilizing%20high-pass%0Afiltering%20to%20model%20seasonal%20variations.%20These%20filters%20are%20learnable%2C%20allowing%0Athe%20model%20to%20dynamically%20adapt%20and%20isolate%20trend%20and%20seasonal%20components%0Adirectly%20in%20the%20frequency%20domain.%20A%20key%20innovation%20in%20our%20approach%20is%20the%0Aintegration%20of%20autocorrelation%2C%20achieved%20by%20computing%20lagged%20differences%20in%0Atime%20steps%2C%20which%20enables%20the%20model%20to%20capture%20dependencies%20across%20time%20more%0Aeffectively.%20Each%20encoder%20processes%20the%20input%20through%20fully%20connected%20layers%20to%0Ahandle%20temporal%20and%20channel%20interactions.%20By%20combining%20frequency-domain%0Afiltering%2C%20autocorrelation-based%20temporal%20modeling%2C%20and%20channel-wise%0Atransformations%2C%20LMS-AutoTSF%20not%20only%20accurately%20captures%20long-term%0Adependencies%20and%20fine-grained%20patterns%20but%20also%20operates%20more%20efficiently%0Acompared%20to%20other%20state-of-the-art%20methods.%20Its%20lightweight%20design%20ensures%0Afaster%20processing%20while%20maintaining%20high%20precision%20in%20forecasting%20across%0Adiverse%20time%20horizons.%20The%20source%20code%20is%20publicly%20available%20at%0A%5Curl%7Bhttp%3A//github.com/mribrahim/LMS-TSF%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06866v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMS-AutoTSF%253A%2520Learnable%2520Multi-Scale%2520Decomposition%2520and%2520Integrated%250A%2520%2520Autocorrelation%2520for%2520Time%2520Series%2520Forecasting%26entry.906535625%3DIbrahim%2520Delibasoglu%2520and%2520Sanjay%2520Chakraborty%2520and%2520Fredrik%2520Heintz%26entry.1292438233%3D%2520%2520Time%2520series%2520forecasting%2520is%2520an%2520important%2520challenge%2520with%2520significant%250Aapplications%2520in%2520areas%2520such%2520as%2520weather%2520prediction%252C%2520stock%2520market%2520analysis%252C%250Ascientific%2520simulations%2520and%2520industrial%2520process%2520analysis.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520LMS-AutoTSF%252C%2520a%2520novel%2520time%2520series%2520forecasting%2520architecture%2520that%250Aincorporates%2520autocorrelation%2520while%2520leveraging%2520dual%2520encoders%2520operating%2520at%250Amultiple%2520scales.%2520Unlike%2520models%2520that%2520rely%2520on%2520predefined%2520trend%2520and%2520seasonal%250Acomponents%252C%2520LMS-AutoTSF%2520employs%2520two%2520separate%2520encoders%2520per%2520scale%253A%2520one%2520focusing%250Aon%2520low-pass%2520filtering%2520to%2520capture%2520trends%2520and%2520the%2520other%2520utilizing%2520high-pass%250Afiltering%2520to%2520model%2520seasonal%2520variations.%2520These%2520filters%2520are%2520learnable%252C%2520allowing%250Athe%2520model%2520to%2520dynamically%2520adapt%2520and%2520isolate%2520trend%2520and%2520seasonal%2520components%250Adirectly%2520in%2520the%2520frequency%2520domain.%2520A%2520key%2520innovation%2520in%2520our%2520approach%2520is%2520the%250Aintegration%2520of%2520autocorrelation%252C%2520achieved%2520by%2520computing%2520lagged%2520differences%2520in%250Atime%2520steps%252C%2520which%2520enables%2520the%2520model%2520to%2520capture%2520dependencies%2520across%2520time%2520more%250Aeffectively.%2520Each%2520encoder%2520processes%2520the%2520input%2520through%2520fully%2520connected%2520layers%2520to%250Ahandle%2520temporal%2520and%2520channel%2520interactions.%2520By%2520combining%2520frequency-domain%250Afiltering%252C%2520autocorrelation-based%2520temporal%2520modeling%252C%2520and%2520channel-wise%250Atransformations%252C%2520LMS-AutoTSF%2520not%2520only%2520accurately%2520captures%2520long-term%250Adependencies%2520and%2520fine-grained%2520patterns%2520but%2520also%2520operates%2520more%2520efficiently%250Acompared%2520to%2520other%2520state-of-the-art%2520methods.%2520Its%2520lightweight%2520design%2520ensures%250Afaster%2520processing%2520while%2520maintaining%2520high%2520precision%2520in%2520forecasting%2520across%250Adiverse%2520time%2520horizons.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%250A%255Curl%257Bhttp%253A//github.com/mribrahim/LMS-TSF%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06866v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMS-AutoTSF%3A%20Learnable%20Multi-Scale%20Decomposition%20and%20Integrated%0A%20%20Autocorrelation%20for%20Time%20Series%20Forecasting&entry.906535625=Ibrahim%20Delibasoglu%20and%20Sanjay%20Chakraborty%20and%20Fredrik%20Heintz&entry.1292438233=%20%20Time%20series%20forecasting%20is%20an%20important%20challenge%20with%20significant%0Aapplications%20in%20areas%20such%20as%20weather%20prediction%2C%20stock%20market%20analysis%2C%0Ascientific%20simulations%20and%20industrial%20process%20analysis.%20In%20this%20work%2C%20we%0Aintroduce%20LMS-AutoTSF%2C%20a%20novel%20time%20series%20forecasting%20architecture%20that%0Aincorporates%20autocorrelation%20while%20leveraging%20dual%20encoders%20operating%20at%0Amultiple%20scales.%20Unlike%20models%20that%20rely%20on%20predefined%20trend%20and%20seasonal%0Acomponents%2C%20LMS-AutoTSF%20employs%20two%20separate%20encoders%20per%20scale%3A%20one%20focusing%0Aon%20low-pass%20filtering%20to%20capture%20trends%20and%20the%20other%20utilizing%20high-pass%0Afiltering%20to%20model%20seasonal%20variations.%20These%20filters%20are%20learnable%2C%20allowing%0Athe%20model%20to%20dynamically%20adapt%20and%20isolate%20trend%20and%20seasonal%20components%0Adirectly%20in%20the%20frequency%20domain.%20A%20key%20innovation%20in%20our%20approach%20is%20the%0Aintegration%20of%20autocorrelation%2C%20achieved%20by%20computing%20lagged%20differences%20in%0Atime%20steps%2C%20which%20enables%20the%20model%20to%20capture%20dependencies%20across%20time%20more%0Aeffectively.%20Each%20encoder%20processes%20the%20input%20through%20fully%20connected%20layers%20to%0Ahandle%20temporal%20and%20channel%20interactions.%20By%20combining%20frequency-domain%0Afiltering%2C%20autocorrelation-based%20temporal%20modeling%2C%20and%20channel-wise%0Atransformations%2C%20LMS-AutoTSF%20not%20only%20accurately%20captures%20long-term%0Adependencies%20and%20fine-grained%20patterns%20but%20also%20operates%20more%20efficiently%0Acompared%20to%20other%20state-of-the-art%20methods.%20Its%20lightweight%20design%20ensures%0Afaster%20processing%20while%20maintaining%20high%20precision%20in%20forecasting%20across%0Adiverse%20time%20horizons.%20The%20source%20code%20is%20publicly%20available%20at%0A%5Curl%7Bhttp%3A//github.com/mribrahim/LMS-TSF%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06866v2&entry.124074799=Read"},
{"title": "Transfer Learning with Partially Observable Offline Data via Causal\n  Bounds", "author": "Xueping Gong and Wei You and Jiheng Zhang", "abstract": "  Transfer learning has emerged as an effective approach to accelerate learning\nby integrating knowledge from related source agents. However, challenges arise\ndue to data heterogeneity-such as differences in feature sets or incomplete\ndatasets-which often results in the nonidentifiability of causal effects. In\nthis paper, we investigate transfer learning in partially observable contextual\nbandits, where agents operate with incomplete information and limited access to\nhidden confounders. To address the challenges posed by unobserved confounders,\nwe formulate optimization problems to derive tight bounds on the\nnonidentifiable causal effects. We then propose an efficient method that\ndiscretizes the functional constraints of unknown distributions into linear\nconstraints, allowing us to sample compatible causal models through a\nsequential process of solving linear programs. This method takes into account\nestimation errors and exhibits strong convergence properties, ensuring robust\nand reliable causal bounds. Leveraging these causal bounds, we improve\nclassical bandit algorithms, achieving tighter regret upper and lower bounds\nrelative to the sizes of action sets and function spaces. In tasks involving\nfunction approximation, which are crucial for handling complex context spaces,\nour method significantly improves the dependence on function space size\ncompared to previous work. We formally prove that our causally enhanced\nalgorithms outperform classical bandit algorithms, achieving notably faster\nconvergence rates. The applicability of our approach is further illustrated\nthrough an example of offline pricing policy learning with censored demand.\nSimulations confirm the superiority of our approach over state-of-the-art\nmethods, demonstrating its potential to enhance contextual bandit agents in\nreal-world applications, especially when data is scarce, costly, or restricted\ndue to privacy concerns.\n", "link": "http://arxiv.org/abs/2308.03572v4", "date": "2025-01-03", "relevancy": 1.4189, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.51}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.463}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20with%20Partially%20Observable%20Offline%20Data%20via%20Causal%0A%20%20Bounds&body=Title%3A%20Transfer%20Learning%20with%20Partially%20Observable%20Offline%20Data%20via%20Causal%0A%20%20Bounds%0AAuthor%3A%20Xueping%20Gong%20and%20Wei%20You%20and%20Jiheng%20Zhang%0AAbstract%3A%20%20%20Transfer%20learning%20has%20emerged%20as%20an%20effective%20approach%20to%20accelerate%20learning%0Aby%20integrating%20knowledge%20from%20related%20source%20agents.%20However%2C%20challenges%20arise%0Adue%20to%20data%20heterogeneity-such%20as%20differences%20in%20feature%20sets%20or%20incomplete%0Adatasets-which%20often%20results%20in%20the%20nonidentifiability%20of%20causal%20effects.%20In%0Athis%20paper%2C%20we%20investigate%20transfer%20learning%20in%20partially%20observable%20contextual%0Abandits%2C%20where%20agents%20operate%20with%20incomplete%20information%20and%20limited%20access%20to%0Ahidden%20confounders.%20To%20address%20the%20challenges%20posed%20by%20unobserved%20confounders%2C%0Awe%20formulate%20optimization%20problems%20to%20derive%20tight%20bounds%20on%20the%0Anonidentifiable%20causal%20effects.%20We%20then%20propose%20an%20efficient%20method%20that%0Adiscretizes%20the%20functional%20constraints%20of%20unknown%20distributions%20into%20linear%0Aconstraints%2C%20allowing%20us%20to%20sample%20compatible%20causal%20models%20through%20a%0Asequential%20process%20of%20solving%20linear%20programs.%20This%20method%20takes%20into%20account%0Aestimation%20errors%20and%20exhibits%20strong%20convergence%20properties%2C%20ensuring%20robust%0Aand%20reliable%20causal%20bounds.%20Leveraging%20these%20causal%20bounds%2C%20we%20improve%0Aclassical%20bandit%20algorithms%2C%20achieving%20tighter%20regret%20upper%20and%20lower%20bounds%0Arelative%20to%20the%20sizes%20of%20action%20sets%20and%20function%20spaces.%20In%20tasks%20involving%0Afunction%20approximation%2C%20which%20are%20crucial%20for%20handling%20complex%20context%20spaces%2C%0Aour%20method%20significantly%20improves%20the%20dependence%20on%20function%20space%20size%0Acompared%20to%20previous%20work.%20We%20formally%20prove%20that%20our%20causally%20enhanced%0Aalgorithms%20outperform%20classical%20bandit%20algorithms%2C%20achieving%20notably%20faster%0Aconvergence%20rates.%20The%20applicability%20of%20our%20approach%20is%20further%20illustrated%0Athrough%20an%20example%20of%20offline%20pricing%20policy%20learning%20with%20censored%20demand.%0ASimulations%20confirm%20the%20superiority%20of%20our%20approach%20over%20state-of-the-art%0Amethods%2C%20demonstrating%20its%20potential%20to%20enhance%20contextual%20bandit%20agents%20in%0Areal-world%20applications%2C%20especially%20when%20data%20is%20scarce%2C%20costly%2C%20or%20restricted%0Adue%20to%20privacy%20concerns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.03572v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520with%2520Partially%2520Observable%2520Offline%2520Data%2520via%2520Causal%250A%2520%2520Bounds%26entry.906535625%3DXueping%2520Gong%2520and%2520Wei%2520You%2520and%2520Jiheng%2520Zhang%26entry.1292438233%3D%2520%2520Transfer%2520learning%2520has%2520emerged%2520as%2520an%2520effective%2520approach%2520to%2520accelerate%2520learning%250Aby%2520integrating%2520knowledge%2520from%2520related%2520source%2520agents.%2520However%252C%2520challenges%2520arise%250Adue%2520to%2520data%2520heterogeneity-such%2520as%2520differences%2520in%2520feature%2520sets%2520or%2520incomplete%250Adatasets-which%2520often%2520results%2520in%2520the%2520nonidentifiability%2520of%2520causal%2520effects.%2520In%250Athis%2520paper%252C%2520we%2520investigate%2520transfer%2520learning%2520in%2520partially%2520observable%2520contextual%250Abandits%252C%2520where%2520agents%2520operate%2520with%2520incomplete%2520information%2520and%2520limited%2520access%2520to%250Ahidden%2520confounders.%2520To%2520address%2520the%2520challenges%2520posed%2520by%2520unobserved%2520confounders%252C%250Awe%2520formulate%2520optimization%2520problems%2520to%2520derive%2520tight%2520bounds%2520on%2520the%250Anonidentifiable%2520causal%2520effects.%2520We%2520then%2520propose%2520an%2520efficient%2520method%2520that%250Adiscretizes%2520the%2520functional%2520constraints%2520of%2520unknown%2520distributions%2520into%2520linear%250Aconstraints%252C%2520allowing%2520us%2520to%2520sample%2520compatible%2520causal%2520models%2520through%2520a%250Asequential%2520process%2520of%2520solving%2520linear%2520programs.%2520This%2520method%2520takes%2520into%2520account%250Aestimation%2520errors%2520and%2520exhibits%2520strong%2520convergence%2520properties%252C%2520ensuring%2520robust%250Aand%2520reliable%2520causal%2520bounds.%2520Leveraging%2520these%2520causal%2520bounds%252C%2520we%2520improve%250Aclassical%2520bandit%2520algorithms%252C%2520achieving%2520tighter%2520regret%2520upper%2520and%2520lower%2520bounds%250Arelative%2520to%2520the%2520sizes%2520of%2520action%2520sets%2520and%2520function%2520spaces.%2520In%2520tasks%2520involving%250Afunction%2520approximation%252C%2520which%2520are%2520crucial%2520for%2520handling%2520complex%2520context%2520spaces%252C%250Aour%2520method%2520significantly%2520improves%2520the%2520dependence%2520on%2520function%2520space%2520size%250Acompared%2520to%2520previous%2520work.%2520We%2520formally%2520prove%2520that%2520our%2520causally%2520enhanced%250Aalgorithms%2520outperform%2520classical%2520bandit%2520algorithms%252C%2520achieving%2520notably%2520faster%250Aconvergence%2520rates.%2520The%2520applicability%2520of%2520our%2520approach%2520is%2520further%2520illustrated%250Athrough%2520an%2520example%2520of%2520offline%2520pricing%2520policy%2520learning%2520with%2520censored%2520demand.%250ASimulations%2520confirm%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520state-of-the-art%250Amethods%252C%2520demonstrating%2520its%2520potential%2520to%2520enhance%2520contextual%2520bandit%2520agents%2520in%250Areal-world%2520applications%252C%2520especially%2520when%2520data%2520is%2520scarce%252C%2520costly%252C%2520or%2520restricted%250Adue%2520to%2520privacy%2520concerns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.03572v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20with%20Partially%20Observable%20Offline%20Data%20via%20Causal%0A%20%20Bounds&entry.906535625=Xueping%20Gong%20and%20Wei%20You%20and%20Jiheng%20Zhang&entry.1292438233=%20%20Transfer%20learning%20has%20emerged%20as%20an%20effective%20approach%20to%20accelerate%20learning%0Aby%20integrating%20knowledge%20from%20related%20source%20agents.%20However%2C%20challenges%20arise%0Adue%20to%20data%20heterogeneity-such%20as%20differences%20in%20feature%20sets%20or%20incomplete%0Adatasets-which%20often%20results%20in%20the%20nonidentifiability%20of%20causal%20effects.%20In%0Athis%20paper%2C%20we%20investigate%20transfer%20learning%20in%20partially%20observable%20contextual%0Abandits%2C%20where%20agents%20operate%20with%20incomplete%20information%20and%20limited%20access%20to%0Ahidden%20confounders.%20To%20address%20the%20challenges%20posed%20by%20unobserved%20confounders%2C%0Awe%20formulate%20optimization%20problems%20to%20derive%20tight%20bounds%20on%20the%0Anonidentifiable%20causal%20effects.%20We%20then%20propose%20an%20efficient%20method%20that%0Adiscretizes%20the%20functional%20constraints%20of%20unknown%20distributions%20into%20linear%0Aconstraints%2C%20allowing%20us%20to%20sample%20compatible%20causal%20models%20through%20a%0Asequential%20process%20of%20solving%20linear%20programs.%20This%20method%20takes%20into%20account%0Aestimation%20errors%20and%20exhibits%20strong%20convergence%20properties%2C%20ensuring%20robust%0Aand%20reliable%20causal%20bounds.%20Leveraging%20these%20causal%20bounds%2C%20we%20improve%0Aclassical%20bandit%20algorithms%2C%20achieving%20tighter%20regret%20upper%20and%20lower%20bounds%0Arelative%20to%20the%20sizes%20of%20action%20sets%20and%20function%20spaces.%20In%20tasks%20involving%0Afunction%20approximation%2C%20which%20are%20crucial%20for%20handling%20complex%20context%20spaces%2C%0Aour%20method%20significantly%20improves%20the%20dependence%20on%20function%20space%20size%0Acompared%20to%20previous%20work.%20We%20formally%20prove%20that%20our%20causally%20enhanced%0Aalgorithms%20outperform%20classical%20bandit%20algorithms%2C%20achieving%20notably%20faster%0Aconvergence%20rates.%20The%20applicability%20of%20our%20approach%20is%20further%20illustrated%0Athrough%20an%20example%20of%20offline%20pricing%20policy%20learning%20with%20censored%20demand.%0ASimulations%20confirm%20the%20superiority%20of%20our%20approach%20over%20state-of-the-art%0Amethods%2C%20demonstrating%20its%20potential%20to%20enhance%20contextual%20bandit%20agents%20in%0Areal-world%20applications%2C%20especially%20when%20data%20is%20scarce%2C%20costly%2C%20or%20restricted%0Adue%20to%20privacy%20concerns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.03572v4&entry.124074799=Read"},
{"title": "Exploring Equality: An Investigation into Custom Loss Functions for\n  Fairness Definitions", "author": "Gordon Lee and Simeon Sayer", "abstract": "  This paper explores the complex tradeoffs between various fairness metrics\nsuch as equalized odds, disparate impact, and equal opportunity and predictive\naccuracy within COMPAS by building neural networks trained with custom loss\nfunctions optimized to specific fairness criteria. This paper creates the first\nfairness-driven implementation of the novel Group Accuracy Parity (GAP)\nframework, as theoretically proposed by Gupta et al. (2024), and applies it to\nCOMPAS. To operationalize and accurately compare the fairness of COMPAS models\noptimized to differing fairness ideals, this paper develops and proposes a\ncombinatory analytical procedure that incorporates Pareto front and\nmultivariate analysis, leveraging data visualizations such as violin graphs.\nThis paper concludes that GAP achieves an enhanced equilibrium between fairness\nand accuracy compared to COMPAS's current nationwide implementation and\nalternative implementations of COMPAS optimized to more traditional fairness\ndefinitions. While this paper's algorithmic improvements of COMPAS\nsignificantly augment its fairness, external biases undermine the fairness of\nits implementation. Practices such as predictive policing and issues such as\nthe lack of transparency regarding COMPAS's internal workings have contributed\nto the algorithm's historical injustice. In conjunction with developments\nregarding COMPAS's predictive methodology, legal and institutional changes must\nhappen for COMPAS's just deployment.\n", "link": "http://arxiv.org/abs/2501.01889v1", "date": "2025-01-03", "relevancy": 1.3481, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4601}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4377}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Equality%3A%20An%20Investigation%20into%20Custom%20Loss%20Functions%20for%0A%20%20Fairness%20Definitions&body=Title%3A%20Exploring%20Equality%3A%20An%20Investigation%20into%20Custom%20Loss%20Functions%20for%0A%20%20Fairness%20Definitions%0AAuthor%3A%20Gordon%20Lee%20and%20Simeon%20Sayer%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20complex%20tradeoffs%20between%20various%20fairness%20metrics%0Asuch%20as%20equalized%20odds%2C%20disparate%20impact%2C%20and%20equal%20opportunity%20and%20predictive%0Aaccuracy%20within%20COMPAS%20by%20building%20neural%20networks%20trained%20with%20custom%20loss%0Afunctions%20optimized%20to%20specific%20fairness%20criteria.%20This%20paper%20creates%20the%20first%0Afairness-driven%20implementation%20of%20the%20novel%20Group%20Accuracy%20Parity%20%28GAP%29%0Aframework%2C%20as%20theoretically%20proposed%20by%20Gupta%20et%20al.%20%282024%29%2C%20and%20applies%20it%20to%0ACOMPAS.%20To%20operationalize%20and%20accurately%20compare%20the%20fairness%20of%20COMPAS%20models%0Aoptimized%20to%20differing%20fairness%20ideals%2C%20this%20paper%20develops%20and%20proposes%20a%0Acombinatory%20analytical%20procedure%20that%20incorporates%20Pareto%20front%20and%0Amultivariate%20analysis%2C%20leveraging%20data%20visualizations%20such%20as%20violin%20graphs.%0AThis%20paper%20concludes%20that%20GAP%20achieves%20an%20enhanced%20equilibrium%20between%20fairness%0Aand%20accuracy%20compared%20to%20COMPAS%27s%20current%20nationwide%20implementation%20and%0Aalternative%20implementations%20of%20COMPAS%20optimized%20to%20more%20traditional%20fairness%0Adefinitions.%20While%20this%20paper%27s%20algorithmic%20improvements%20of%20COMPAS%0Asignificantly%20augment%20its%20fairness%2C%20external%20biases%20undermine%20the%20fairness%20of%0Aits%20implementation.%20Practices%20such%20as%20predictive%20policing%20and%20issues%20such%20as%0Athe%20lack%20of%20transparency%20regarding%20COMPAS%27s%20internal%20workings%20have%20contributed%0Ato%20the%20algorithm%27s%20historical%20injustice.%20In%20conjunction%20with%20developments%0Aregarding%20COMPAS%27s%20predictive%20methodology%2C%20legal%20and%20institutional%20changes%20must%0Ahappen%20for%20COMPAS%27s%20just%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Equality%253A%2520An%2520Investigation%2520into%2520Custom%2520Loss%2520Functions%2520for%250A%2520%2520Fairness%2520Definitions%26entry.906535625%3DGordon%2520Lee%2520and%2520Simeon%2520Sayer%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520complex%2520tradeoffs%2520between%2520various%2520fairness%2520metrics%250Asuch%2520as%2520equalized%2520odds%252C%2520disparate%2520impact%252C%2520and%2520equal%2520opportunity%2520and%2520predictive%250Aaccuracy%2520within%2520COMPAS%2520by%2520building%2520neural%2520networks%2520trained%2520with%2520custom%2520loss%250Afunctions%2520optimized%2520to%2520specific%2520fairness%2520criteria.%2520This%2520paper%2520creates%2520the%2520first%250Afairness-driven%2520implementation%2520of%2520the%2520novel%2520Group%2520Accuracy%2520Parity%2520%2528GAP%2529%250Aframework%252C%2520as%2520theoretically%2520proposed%2520by%2520Gupta%2520et%2520al.%2520%25282024%2529%252C%2520and%2520applies%2520it%2520to%250ACOMPAS.%2520To%2520operationalize%2520and%2520accurately%2520compare%2520the%2520fairness%2520of%2520COMPAS%2520models%250Aoptimized%2520to%2520differing%2520fairness%2520ideals%252C%2520this%2520paper%2520develops%2520and%2520proposes%2520a%250Acombinatory%2520analytical%2520procedure%2520that%2520incorporates%2520Pareto%2520front%2520and%250Amultivariate%2520analysis%252C%2520leveraging%2520data%2520visualizations%2520such%2520as%2520violin%2520graphs.%250AThis%2520paper%2520concludes%2520that%2520GAP%2520achieves%2520an%2520enhanced%2520equilibrium%2520between%2520fairness%250Aand%2520accuracy%2520compared%2520to%2520COMPAS%2527s%2520current%2520nationwide%2520implementation%2520and%250Aalternative%2520implementations%2520of%2520COMPAS%2520optimized%2520to%2520more%2520traditional%2520fairness%250Adefinitions.%2520While%2520this%2520paper%2527s%2520algorithmic%2520improvements%2520of%2520COMPAS%250Asignificantly%2520augment%2520its%2520fairness%252C%2520external%2520biases%2520undermine%2520the%2520fairness%2520of%250Aits%2520implementation.%2520Practices%2520such%2520as%2520predictive%2520policing%2520and%2520issues%2520such%2520as%250Athe%2520lack%2520of%2520transparency%2520regarding%2520COMPAS%2527s%2520internal%2520workings%2520have%2520contributed%250Ato%2520the%2520algorithm%2527s%2520historical%2520injustice.%2520In%2520conjunction%2520with%2520developments%250Aregarding%2520COMPAS%2527s%2520predictive%2520methodology%252C%2520legal%2520and%2520institutional%2520changes%2520must%250Ahappen%2520for%2520COMPAS%2527s%2520just%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Equality%3A%20An%20Investigation%20into%20Custom%20Loss%20Functions%20for%0A%20%20Fairness%20Definitions&entry.906535625=Gordon%20Lee%20and%20Simeon%20Sayer&entry.1292438233=%20%20This%20paper%20explores%20the%20complex%20tradeoffs%20between%20various%20fairness%20metrics%0Asuch%20as%20equalized%20odds%2C%20disparate%20impact%2C%20and%20equal%20opportunity%20and%20predictive%0Aaccuracy%20within%20COMPAS%20by%20building%20neural%20networks%20trained%20with%20custom%20loss%0Afunctions%20optimized%20to%20specific%20fairness%20criteria.%20This%20paper%20creates%20the%20first%0Afairness-driven%20implementation%20of%20the%20novel%20Group%20Accuracy%20Parity%20%28GAP%29%0Aframework%2C%20as%20theoretically%20proposed%20by%20Gupta%20et%20al.%20%282024%29%2C%20and%20applies%20it%20to%0ACOMPAS.%20To%20operationalize%20and%20accurately%20compare%20the%20fairness%20of%20COMPAS%20models%0Aoptimized%20to%20differing%20fairness%20ideals%2C%20this%20paper%20develops%20and%20proposes%20a%0Acombinatory%20analytical%20procedure%20that%20incorporates%20Pareto%20front%20and%0Amultivariate%20analysis%2C%20leveraging%20data%20visualizations%20such%20as%20violin%20graphs.%0AThis%20paper%20concludes%20that%20GAP%20achieves%20an%20enhanced%20equilibrium%20between%20fairness%0Aand%20accuracy%20compared%20to%20COMPAS%27s%20current%20nationwide%20implementation%20and%0Aalternative%20implementations%20of%20COMPAS%20optimized%20to%20more%20traditional%20fairness%0Adefinitions.%20While%20this%20paper%27s%20algorithmic%20improvements%20of%20COMPAS%0Asignificantly%20augment%20its%20fairness%2C%20external%20biases%20undermine%20the%20fairness%20of%0Aits%20implementation.%20Practices%20such%20as%20predictive%20policing%20and%20issues%20such%20as%0Athe%20lack%20of%20transparency%20regarding%20COMPAS%27s%20internal%20workings%20have%20contributed%0Ato%20the%20algorithm%27s%20historical%20injustice.%20In%20conjunction%20with%20developments%0Aregarding%20COMPAS%27s%20predictive%20methodology%2C%20legal%20and%20institutional%20changes%20must%0Ahappen%20for%20COMPAS%27s%20just%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01889v1&entry.124074799=Read"},
{"title": "Can AI Help with Your Personal Finances?", "author": "Oudom Hean and Utsha Saha and Binita Saha", "abstract": "  In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising.\n", "link": "http://arxiv.org/abs/2412.19784v2", "date": "2025-01-03", "relevancy": 1.2778, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4451}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20AI%20Help%20with%20Your%20Personal%20Finances%3F&body=Title%3A%20Can%20AI%20Help%20with%20Your%20Personal%20Finances%3F%0AAuthor%3A%20Oudom%20Hean%20and%20Utsha%20Saha%20and%20Binita%20Saha%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%0Atransformative%20development%20in%20artificial%20intelligence%20%28AI%29%2C%20drawing%20significant%0Aattention%20from%20industry%20and%20academia.%20Trained%20on%20vast%20datasets%2C%20these%0Asophisticated%20AI%20systems%20exhibit%20impressive%20natural%20language%20processing%20and%0Acontent%20generation%20capabilities.%20This%20paper%20explores%20the%20potential%20of%20LLMs%20to%0Aaddress%20key%20challenges%20in%20personal%20finance%2C%20focusing%20on%20the%20United%20States.%20We%0Aevaluate%20several%20leading%20LLMs%2C%20including%20OpenAI%27s%20ChatGPT%2C%20Google%27s%20Gemini%2C%0AAnthropic%27s%20Claude%2C%20and%20Meta%27s%20Llama%2C%20to%20assess%20their%20effectiveness%20in%0Aproviding%20accurate%20financial%20advice%20on%20topics%20such%20as%20mortgages%2C%20taxes%2C%20loans%2C%0Aand%20investments.%20Our%20findings%20show%20that%20while%20these%20models%20achieve%20an%20average%0Aaccuracy%20rate%20of%20approximately%2070%25%2C%20they%20also%20display%20notable%20limitations%20in%0Acertain%20areas.%20Specifically%2C%20LLMs%20struggle%20to%20provide%20accurate%20responses%20for%0Acomplex%20financial%20queries%2C%20with%20performance%20varying%20significantly%20across%0Adifferent%20topics.%20Despite%20these%20limitations%2C%20the%20analysis%20reveals%20notable%0Aimprovements%20in%20newer%20versions%20of%20these%20models%2C%20highlighting%20their%20growing%0Autility%20for%20individuals%20and%20financial%20advisors.%20As%20these%20AI%20systems%20continue%20to%0Aevolve%2C%20their%20potential%20for%20advancing%20AI-driven%20applications%20in%20personal%0Afinance%20becomes%20increasingly%20promising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520AI%2520Help%2520with%2520Your%2520Personal%2520Finances%253F%26entry.906535625%3DOudom%2520Hean%2520and%2520Utsha%2520Saha%2520and%2520Binita%2520Saha%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520a%250Atransformative%2520development%2520in%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520drawing%2520significant%250Aattention%2520from%2520industry%2520and%2520academia.%2520Trained%2520on%2520vast%2520datasets%252C%2520these%250Asophisticated%2520AI%2520systems%2520exhibit%2520impressive%2520natural%2520language%2520processing%2520and%250Acontent%2520generation%2520capabilities.%2520This%2520paper%2520explores%2520the%2520potential%2520of%2520LLMs%2520to%250Aaddress%2520key%2520challenges%2520in%2520personal%2520finance%252C%2520focusing%2520on%2520the%2520United%2520States.%2520We%250Aevaluate%2520several%2520leading%2520LLMs%252C%2520including%2520OpenAI%2527s%2520ChatGPT%252C%2520Google%2527s%2520Gemini%252C%250AAnthropic%2527s%2520Claude%252C%2520and%2520Meta%2527s%2520Llama%252C%2520to%2520assess%2520their%2520effectiveness%2520in%250Aproviding%2520accurate%2520financial%2520advice%2520on%2520topics%2520such%2520as%2520mortgages%252C%2520taxes%252C%2520loans%252C%250Aand%2520investments.%2520Our%2520findings%2520show%2520that%2520while%2520these%2520models%2520achieve%2520an%2520average%250Aaccuracy%2520rate%2520of%2520approximately%252070%2525%252C%2520they%2520also%2520display%2520notable%2520limitations%2520in%250Acertain%2520areas.%2520Specifically%252C%2520LLMs%2520struggle%2520to%2520provide%2520accurate%2520responses%2520for%250Acomplex%2520financial%2520queries%252C%2520with%2520performance%2520varying%2520significantly%2520across%250Adifferent%2520topics.%2520Despite%2520these%2520limitations%252C%2520the%2520analysis%2520reveals%2520notable%250Aimprovements%2520in%2520newer%2520versions%2520of%2520these%2520models%252C%2520highlighting%2520their%2520growing%250Autility%2520for%2520individuals%2520and%2520financial%2520advisors.%2520As%2520these%2520AI%2520systems%2520continue%2520to%250Aevolve%252C%2520their%2520potential%2520for%2520advancing%2520AI-driven%2520applications%2520in%2520personal%250Afinance%2520becomes%2520increasingly%2520promising.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20AI%20Help%20with%20Your%20Personal%20Finances%3F&entry.906535625=Oudom%20Hean%20and%20Utsha%20Saha%20and%20Binita%20Saha&entry.1292438233=%20%20In%20recent%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%0Atransformative%20development%20in%20artificial%20intelligence%20%28AI%29%2C%20drawing%20significant%0Aattention%20from%20industry%20and%20academia.%20Trained%20on%20vast%20datasets%2C%20these%0Asophisticated%20AI%20systems%20exhibit%20impressive%20natural%20language%20processing%20and%0Acontent%20generation%20capabilities.%20This%20paper%20explores%20the%20potential%20of%20LLMs%20to%0Aaddress%20key%20challenges%20in%20personal%20finance%2C%20focusing%20on%20the%20United%20States.%20We%0Aevaluate%20several%20leading%20LLMs%2C%20including%20OpenAI%27s%20ChatGPT%2C%20Google%27s%20Gemini%2C%0AAnthropic%27s%20Claude%2C%20and%20Meta%27s%20Llama%2C%20to%20assess%20their%20effectiveness%20in%0Aproviding%20accurate%20financial%20advice%20on%20topics%20such%20as%20mortgages%2C%20taxes%2C%20loans%2C%0Aand%20investments.%20Our%20findings%20show%20that%20while%20these%20models%20achieve%20an%20average%0Aaccuracy%20rate%20of%20approximately%2070%25%2C%20they%20also%20display%20notable%20limitations%20in%0Acertain%20areas.%20Specifically%2C%20LLMs%20struggle%20to%20provide%20accurate%20responses%20for%0Acomplex%20financial%20queries%2C%20with%20performance%20varying%20significantly%20across%0Adifferent%20topics.%20Despite%20these%20limitations%2C%20the%20analysis%20reveals%20notable%0Aimprovements%20in%20newer%20versions%20of%20these%20models%2C%20highlighting%20their%20growing%0Autility%20for%20individuals%20and%20financial%20advisors.%20As%20these%20AI%20systems%20continue%20to%0Aevolve%2C%20their%20potential%20for%20advancing%20AI-driven%20applications%20in%20personal%0Afinance%20becomes%20increasingly%20promising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19784v2&entry.124074799=Read"},
{"title": "Conditional Consistency Guided Image Translation and Enhancement", "author": "Amil Bhagat and Milind Jain and A. V. Subramanyam", "abstract": "  Consistency models have emerged as a promising alternative to diffusion\nmodels, offering high-quality generative capabilities through single-step\nsample generation. However, their application to multi-domain image translation\ntasks, such as cross-modal translation and low-light image enhancement remains\nlargely unexplored. In this paper, we introduce Conditional Consistency Models\n(CCMs) for multi-domain image translation by incorporating additional\nconditional inputs. We implement these modifications by introducing\ntask-specific conditional inputs that guide the denoising process, ensuring\nthat the generated outputs retain structural and contextual information from\nthe corresponding input domain. We evaluate CCMs on 10 different datasets\ndemonstrating their effectiveness in producing high-quality translated images\nacross multiple domains. Code is available at\nhttps://github.com/amilbhagat/Conditional-Consistency-Models.\n", "link": "http://arxiv.org/abs/2501.01223v2", "date": "2025-01-03", "relevancy": 1.1377, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5904}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5613}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Consistency%20Guided%20Image%20Translation%20and%20Enhancement&body=Title%3A%20Conditional%20Consistency%20Guided%20Image%20Translation%20and%20Enhancement%0AAuthor%3A%20Amil%20Bhagat%20and%20Milind%20Jain%20and%20A.%20V.%20Subramanyam%0AAbstract%3A%20%20%20Consistency%20models%20have%20emerged%20as%20a%20promising%20alternative%20to%20diffusion%0Amodels%2C%20offering%20high-quality%20generative%20capabilities%20through%20single-step%0Asample%20generation.%20However%2C%20their%20application%20to%20multi-domain%20image%20translation%0Atasks%2C%20such%20as%20cross-modal%20translation%20and%20low-light%20image%20enhancement%20remains%0Alargely%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20Conditional%20Consistency%20Models%0A%28CCMs%29%20for%20multi-domain%20image%20translation%20by%20incorporating%20additional%0Aconditional%20inputs.%20We%20implement%20these%20modifications%20by%20introducing%0Atask-specific%20conditional%20inputs%20that%20guide%20the%20denoising%20process%2C%20ensuring%0Athat%20the%20generated%20outputs%20retain%20structural%20and%20contextual%20information%20from%0Athe%20corresponding%20input%20domain.%20We%20evaluate%20CCMs%20on%2010%20different%20datasets%0Ademonstrating%20their%20effectiveness%20in%20producing%20high-quality%20translated%20images%0Aacross%20multiple%20domains.%20Code%20is%20available%20at%0Ahttps%3A//github.com/amilbhagat/Conditional-Consistency-Models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01223v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Consistency%2520Guided%2520Image%2520Translation%2520and%2520Enhancement%26entry.906535625%3DAmil%2520Bhagat%2520and%2520Milind%2520Jain%2520and%2520A.%2520V.%2520Subramanyam%26entry.1292438233%3D%2520%2520Consistency%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520alternative%2520to%2520diffusion%250Amodels%252C%2520offering%2520high-quality%2520generative%2520capabilities%2520through%2520single-step%250Asample%2520generation.%2520However%252C%2520their%2520application%2520to%2520multi-domain%2520image%2520translation%250Atasks%252C%2520such%2520as%2520cross-modal%2520translation%2520and%2520low-light%2520image%2520enhancement%2520remains%250Alargely%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Conditional%2520Consistency%2520Models%250A%2528CCMs%2529%2520for%2520multi-domain%2520image%2520translation%2520by%2520incorporating%2520additional%250Aconditional%2520inputs.%2520We%2520implement%2520these%2520modifications%2520by%2520introducing%250Atask-specific%2520conditional%2520inputs%2520that%2520guide%2520the%2520denoising%2520process%252C%2520ensuring%250Athat%2520the%2520generated%2520outputs%2520retain%2520structural%2520and%2520contextual%2520information%2520from%250Athe%2520corresponding%2520input%2520domain.%2520We%2520evaluate%2520CCMs%2520on%252010%2520different%2520datasets%250Ademonstrating%2520their%2520effectiveness%2520in%2520producing%2520high-quality%2520translated%2520images%250Aacross%2520multiple%2520domains.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/amilbhagat/Conditional-Consistency-Models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01223v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Consistency%20Guided%20Image%20Translation%20and%20Enhancement&entry.906535625=Amil%20Bhagat%20and%20Milind%20Jain%20and%20A.%20V.%20Subramanyam&entry.1292438233=%20%20Consistency%20models%20have%20emerged%20as%20a%20promising%20alternative%20to%20diffusion%0Amodels%2C%20offering%20high-quality%20generative%20capabilities%20through%20single-step%0Asample%20generation.%20However%2C%20their%20application%20to%20multi-domain%20image%20translation%0Atasks%2C%20such%20as%20cross-modal%20translation%20and%20low-light%20image%20enhancement%20remains%0Alargely%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20Conditional%20Consistency%20Models%0A%28CCMs%29%20for%20multi-domain%20image%20translation%20by%20incorporating%20additional%0Aconditional%20inputs.%20We%20implement%20these%20modifications%20by%20introducing%0Atask-specific%20conditional%20inputs%20that%20guide%20the%20denoising%20process%2C%20ensuring%0Athat%20the%20generated%20outputs%20retain%20structural%20and%20contextual%20information%20from%0Athe%20corresponding%20input%20domain.%20We%20evaluate%20CCMs%20on%2010%20different%20datasets%0Ademonstrating%20their%20effectiveness%20in%20producing%20high-quality%20translated%20images%0Aacross%20multiple%20domains.%20Code%20is%20available%20at%0Ahttps%3A//github.com/amilbhagat/Conditional-Consistency-Models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01223v2&entry.124074799=Read"},
{"title": "Simultaneous Latent State Estimation and Latent Linear Dynamics\n  Discovery from Image Observations", "author": "Nikita Kostin", "abstract": "  The problem of state estimation has a long history with many successful\nalgorithms that allow analytical derivation or approximation of posterior\nfiltering distribution given the noisy observations. This report tries to\nconclude previous works to resolve the problem of latent state estimation given\nimage-based observations and also suggests a new solution to this problem.\n", "link": "http://arxiv.org/abs/2501.01339v2", "date": "2025-01-03", "relevancy": 1.067, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5802}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Latent%20State%20Estimation%20and%20Latent%20Linear%20Dynamics%0A%20%20Discovery%20from%20Image%20Observations&body=Title%3A%20Simultaneous%20Latent%20State%20Estimation%20and%20Latent%20Linear%20Dynamics%0A%20%20Discovery%20from%20Image%20Observations%0AAuthor%3A%20Nikita%20Kostin%0AAbstract%3A%20%20%20The%20problem%20of%20state%20estimation%20has%20a%20long%20history%20with%20many%20successful%0Aalgorithms%20that%20allow%20analytical%20derivation%20or%20approximation%20of%20posterior%0Afiltering%20distribution%20given%20the%20noisy%20observations.%20This%20report%20tries%20to%0Aconclude%20previous%20works%20to%20resolve%20the%20problem%20of%20latent%20state%20estimation%20given%0Aimage-based%20observations%20and%20also%20suggests%20a%20new%20solution%20to%20this%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520Latent%2520State%2520Estimation%2520and%2520Latent%2520Linear%2520Dynamics%250A%2520%2520Discovery%2520from%2520Image%2520Observations%26entry.906535625%3DNikita%2520Kostin%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520state%2520estimation%2520has%2520a%2520long%2520history%2520with%2520many%2520successful%250Aalgorithms%2520that%2520allow%2520analytical%2520derivation%2520or%2520approximation%2520of%2520posterior%250Afiltering%2520distribution%2520given%2520the%2520noisy%2520observations.%2520This%2520report%2520tries%2520to%250Aconclude%2520previous%2520works%2520to%2520resolve%2520the%2520problem%2520of%2520latent%2520state%2520estimation%2520given%250Aimage-based%2520observations%2520and%2520also%2520suggests%2520a%2520new%2520solution%2520to%2520this%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Latent%20State%20Estimation%20and%20Latent%20Linear%20Dynamics%0A%20%20Discovery%20from%20Image%20Observations&entry.906535625=Nikita%20Kostin&entry.1292438233=%20%20The%20problem%20of%20state%20estimation%20has%20a%20long%20history%20with%20many%20successful%0Aalgorithms%20that%20allow%20analytical%20derivation%20or%20approximation%20of%20posterior%0Afiltering%20distribution%20given%20the%20noisy%20observations.%20This%20report%20tries%20to%0Aconclude%20previous%20works%20to%20resolve%20the%20problem%20of%20latent%20state%20estimation%20given%0Aimage-based%20observations%20and%20also%20suggests%20a%20new%20solution%20to%20this%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01339v2&entry.124074799=Read"},
{"title": "Alleviating Overfitting in Transformation-Interaction-Rational Symbolic\n  Regression with Multi-Objective Optimization", "author": "Fabricio Olivetti de Franca", "abstract": "  The Transformation-Interaction-Rational is a representation for symbolic\nregression that limits the search space of functions to the ratio of two\nnonlinear functions each one defined as the linear regression of transformed\nvariables. This representation has the main objective to bias the search\ntowards simpler expressions while keeping the approximation power of standard\napproaches.\n  The performance of using Genetic Programming with this representation was\nsubstantially better than with its predecessor (Interaction-Transformation) and\nranked close to the state-of-the-art on a contemporary Symbolic Regression\nbenchmark. On a closer look at these results, we observed that the performance\ncould be further improved with an additional selective pressure for smaller\nexpressions when the dataset contains just a few data points. The introduction\nof a penalization term applied to the fitness measure improved the results on\nthese smaller datasets. One problem with this approach is that it introduces\ntwo additional hyperparameters: i) a criteria to when the penalization should\nbe activated and, ii) the amount of penalization to the fitness function.\n  In this paper, we extend Transformation-Interaction-Rational to support\nmulti-objective optimization, specifically the NSGA-II algorithm, and apply\nthat to the same benchmark. A detailed analysis of the results show that the\nuse of multi-objective optimization benefits the overall performance on a\nsubset of the benchmarks while keeping the results similar to the\nsingle-objective approach on the remainder of the datasets. Specifically to the\nsmall datasets, we observe a small (and statistically insignificant)\nimprovement of the results suggesting that further strategies must be explored.\n", "link": "http://arxiv.org/abs/2501.01905v1", "date": "2025-01-03", "relevancy": 0.956, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4948}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4802}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alleviating%20Overfitting%20in%20Transformation-Interaction-Rational%20Symbolic%0A%20%20Regression%20with%20Multi-Objective%20Optimization&body=Title%3A%20Alleviating%20Overfitting%20in%20Transformation-Interaction-Rational%20Symbolic%0A%20%20Regression%20with%20Multi-Objective%20Optimization%0AAuthor%3A%20Fabricio%20Olivetti%20de%20Franca%0AAbstract%3A%20%20%20The%20Transformation-Interaction-Rational%20is%20a%20representation%20for%20symbolic%0Aregression%20that%20limits%20the%20search%20space%20of%20functions%20to%20the%20ratio%20of%20two%0Anonlinear%20functions%20each%20one%20defined%20as%20the%20linear%20regression%20of%20transformed%0Avariables.%20This%20representation%20has%20the%20main%20objective%20to%20bias%20the%20search%0Atowards%20simpler%20expressions%20while%20keeping%20the%20approximation%20power%20of%20standard%0Aapproaches.%0A%20%20The%20performance%20of%20using%20Genetic%20Programming%20with%20this%20representation%20was%0Asubstantially%20better%20than%20with%20its%20predecessor%20%28Interaction-Transformation%29%20and%0Aranked%20close%20to%20the%20state-of-the-art%20on%20a%20contemporary%20Symbolic%20Regression%0Abenchmark.%20On%20a%20closer%20look%20at%20these%20results%2C%20we%20observed%20that%20the%20performance%0Acould%20be%20further%20improved%20with%20an%20additional%20selective%20pressure%20for%20smaller%0Aexpressions%20when%20the%20dataset%20contains%20just%20a%20few%20data%20points.%20The%20introduction%0Aof%20a%20penalization%20term%20applied%20to%20the%20fitness%20measure%20improved%20the%20results%20on%0Athese%20smaller%20datasets.%20One%20problem%20with%20this%20approach%20is%20that%20it%20introduces%0Atwo%20additional%20hyperparameters%3A%20i%29%20a%20criteria%20to%20when%20the%20penalization%20should%0Abe%20activated%20and%2C%20ii%29%20the%20amount%20of%20penalization%20to%20the%20fitness%20function.%0A%20%20In%20this%20paper%2C%20we%20extend%20Transformation-Interaction-Rational%20to%20support%0Amulti-objective%20optimization%2C%20specifically%20the%20NSGA-II%20algorithm%2C%20and%20apply%0Athat%20to%20the%20same%20benchmark.%20A%20detailed%20analysis%20of%20the%20results%20show%20that%20the%0Ause%20of%20multi-objective%20optimization%20benefits%20the%20overall%20performance%20on%20a%0Asubset%20of%20the%20benchmarks%20while%20keeping%20the%20results%20similar%20to%20the%0Asingle-objective%20approach%20on%20the%20remainder%20of%20the%20datasets.%20Specifically%20to%20the%0Asmall%20datasets%2C%20we%20observe%20a%20small%20%28and%20statistically%20insignificant%29%0Aimprovement%20of%20the%20results%20suggesting%20that%20further%20strategies%20must%20be%20explored.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlleviating%2520Overfitting%2520in%2520Transformation-Interaction-Rational%2520Symbolic%250A%2520%2520Regression%2520with%2520Multi-Objective%2520Optimization%26entry.906535625%3DFabricio%2520Olivetti%2520de%2520Franca%26entry.1292438233%3D%2520%2520The%2520Transformation-Interaction-Rational%2520is%2520a%2520representation%2520for%2520symbolic%250Aregression%2520that%2520limits%2520the%2520search%2520space%2520of%2520functions%2520to%2520the%2520ratio%2520of%2520two%250Anonlinear%2520functions%2520each%2520one%2520defined%2520as%2520the%2520linear%2520regression%2520of%2520transformed%250Avariables.%2520This%2520representation%2520has%2520the%2520main%2520objective%2520to%2520bias%2520the%2520search%250Atowards%2520simpler%2520expressions%2520while%2520keeping%2520the%2520approximation%2520power%2520of%2520standard%250Aapproaches.%250A%2520%2520The%2520performance%2520of%2520using%2520Genetic%2520Programming%2520with%2520this%2520representation%2520was%250Asubstantially%2520better%2520than%2520with%2520its%2520predecessor%2520%2528Interaction-Transformation%2529%2520and%250Aranked%2520close%2520to%2520the%2520state-of-the-art%2520on%2520a%2520contemporary%2520Symbolic%2520Regression%250Abenchmark.%2520On%2520a%2520closer%2520look%2520at%2520these%2520results%252C%2520we%2520observed%2520that%2520the%2520performance%250Acould%2520be%2520further%2520improved%2520with%2520an%2520additional%2520selective%2520pressure%2520for%2520smaller%250Aexpressions%2520when%2520the%2520dataset%2520contains%2520just%2520a%2520few%2520data%2520points.%2520The%2520introduction%250Aof%2520a%2520penalization%2520term%2520applied%2520to%2520the%2520fitness%2520measure%2520improved%2520the%2520results%2520on%250Athese%2520smaller%2520datasets.%2520One%2520problem%2520with%2520this%2520approach%2520is%2520that%2520it%2520introduces%250Atwo%2520additional%2520hyperparameters%253A%2520i%2529%2520a%2520criteria%2520to%2520when%2520the%2520penalization%2520should%250Abe%2520activated%2520and%252C%2520ii%2529%2520the%2520amount%2520of%2520penalization%2520to%2520the%2520fitness%2520function.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520extend%2520Transformation-Interaction-Rational%2520to%2520support%250Amulti-objective%2520optimization%252C%2520specifically%2520the%2520NSGA-II%2520algorithm%252C%2520and%2520apply%250Athat%2520to%2520the%2520same%2520benchmark.%2520A%2520detailed%2520analysis%2520of%2520the%2520results%2520show%2520that%2520the%250Ause%2520of%2520multi-objective%2520optimization%2520benefits%2520the%2520overall%2520performance%2520on%2520a%250Asubset%2520of%2520the%2520benchmarks%2520while%2520keeping%2520the%2520results%2520similar%2520to%2520the%250Asingle-objective%2520approach%2520on%2520the%2520remainder%2520of%2520the%2520datasets.%2520Specifically%2520to%2520the%250Asmall%2520datasets%252C%2520we%2520observe%2520a%2520small%2520%2528and%2520statistically%2520insignificant%2529%250Aimprovement%2520of%2520the%2520results%2520suggesting%2520that%2520further%2520strategies%2520must%2520be%2520explored.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alleviating%20Overfitting%20in%20Transformation-Interaction-Rational%20Symbolic%0A%20%20Regression%20with%20Multi-Objective%20Optimization&entry.906535625=Fabricio%20Olivetti%20de%20Franca&entry.1292438233=%20%20The%20Transformation-Interaction-Rational%20is%20a%20representation%20for%20symbolic%0Aregression%20that%20limits%20the%20search%20space%20of%20functions%20to%20the%20ratio%20of%20two%0Anonlinear%20functions%20each%20one%20defined%20as%20the%20linear%20regression%20of%20transformed%0Avariables.%20This%20representation%20has%20the%20main%20objective%20to%20bias%20the%20search%0Atowards%20simpler%20expressions%20while%20keeping%20the%20approximation%20power%20of%20standard%0Aapproaches.%0A%20%20The%20performance%20of%20using%20Genetic%20Programming%20with%20this%20representation%20was%0Asubstantially%20better%20than%20with%20its%20predecessor%20%28Interaction-Transformation%29%20and%0Aranked%20close%20to%20the%20state-of-the-art%20on%20a%20contemporary%20Symbolic%20Regression%0Abenchmark.%20On%20a%20closer%20look%20at%20these%20results%2C%20we%20observed%20that%20the%20performance%0Acould%20be%20further%20improved%20with%20an%20additional%20selective%20pressure%20for%20smaller%0Aexpressions%20when%20the%20dataset%20contains%20just%20a%20few%20data%20points.%20The%20introduction%0Aof%20a%20penalization%20term%20applied%20to%20the%20fitness%20measure%20improved%20the%20results%20on%0Athese%20smaller%20datasets.%20One%20problem%20with%20this%20approach%20is%20that%20it%20introduces%0Atwo%20additional%20hyperparameters%3A%20i%29%20a%20criteria%20to%20when%20the%20penalization%20should%0Abe%20activated%20and%2C%20ii%29%20the%20amount%20of%20penalization%20to%20the%20fitness%20function.%0A%20%20In%20this%20paper%2C%20we%20extend%20Transformation-Interaction-Rational%20to%20support%0Amulti-objective%20optimization%2C%20specifically%20the%20NSGA-II%20algorithm%2C%20and%20apply%0Athat%20to%20the%20same%20benchmark.%20A%20detailed%20analysis%20of%20the%20results%20show%20that%20the%0Ause%20of%20multi-objective%20optimization%20benefits%20the%20overall%20performance%20on%20a%0Asubset%20of%20the%20benchmarks%20while%20keeping%20the%20results%20similar%20to%20the%0Asingle-objective%20approach%20on%20the%20remainder%20of%20the%20datasets.%20Specifically%20to%20the%0Asmall%20datasets%2C%20we%20observe%20a%20small%20%28and%20statistically%20insignificant%29%0Aimprovement%20of%20the%20results%20suggesting%20that%20further%20strategies%20must%20be%20explored.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01905v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


