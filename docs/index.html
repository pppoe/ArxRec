<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Edge Detectors Can Make Deep Convolutional Neural Networks More Robust", "author": "Jin Ding and Jie-Chao Zhao and Yong-Zhi Sun and Ping Tan and Jia-Wei Wang and Ji-En Ma and You-Tong Fang", "abstract": "  Deep convolutional neural networks (DCNN for short) are vulnerable to\nexamples with small perturbations. Improving DCNN's robustness is of great\nsignificance to the safety-critical applications, such as autonomous driving\nand industry automation. Inspired by the principal way that human eyes\nrecognize objects, i.e., largely relying on the shape features, this paper\nfirst employs the edge detectors as layer kernels and designs a binary edge\nfeature branch (BEFB for short) to learn the binary edge features, which can be\neasily integrated into any popular backbone. The four edge detectors can learn\nthe horizontal, vertical, positive diagonal, and negative diagonal edge\nfeatures, respectively, and the branch is stacked by multiple Sobel layers\n(using edge detectors as kernels) and one threshold layer. The binary edge\nfeatures learned by the branch, concatenated with the texture features learned\nby the backbone, are fed into the fully connected layers for classification. We\nintegrate the proposed branch into VGG16 and ResNet34, respectively, and\nconduct experiments on multiple datasets. Experimental results demonstrate the\nBEFB is lightweight and has no side effects on training. And the accuracy of\nthe BEFB integrated models is better than the original ones on all datasets\nwhen facing FGSM, PGD, and C\\&W attacks. Besides, BEFB integrated models\nequipped with the robustness enhancing techniques can achieve better\nclassification accuracy compared to the original models. The work in this paper\nfor the first time shows it is feasible to enhance the robustness of DCNNs\nthrough combining both shape-like features and texture features.\n", "link": "http://arxiv.org/abs/2402.16479v1", "date": "2024-02-26", "relevancy": 2.6404, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5918}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4976}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4948}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge%20Detectors%20Can%20Make%20Deep%20Convolutional%20Neural%20Networks%20More%20Robust&entry.906535625=Jin%20Ding%20and%20Jie-Chao%20Zhao%20and%20Yong-Zhi%20Sun%20and%20Ping%20Tan%20and%20Jia-Wei%20Wang%20and%20Ji-En%20Ma%20and%20You-Tong%20Fang&entry.1292438233=%20%20Deep%20convolutional%20neural%20networks%20%28DCNN%20for%20short%29%20are%20vulnerable%20to%0Aexamples%20with%20small%20perturbations.%20Improving%20DCNN%27s%20robustness%20is%20of%20great%0Asignificance%20to%20the%20safety-critical%20applications%2C%20such%20as%20autonomous%20driving%0Aand%20industry%20automation.%20Inspired%20by%20the%20principal%20way%20that%20human%20eyes%0Arecognize%20objects%2C%20i.e.%2C%20largely%20relying%20on%20the%20shape%20features%2C%20this%20paper%0Afirst%20employs%20the%20edge%20detectors%20as%20layer%20kernels%20and%20designs%20a%20binary%20edge%0Afeature%20branch%20%28BEFB%20for%20short%29%20to%20learn%20the%20binary%20edge%20features%2C%20which%20can%20be%0Aeasily%20integrated%20into%20any%20popular%20backbone.%20The%20four%20edge%20detectors%20can%20learn%0Athe%20horizontal%2C%20vertical%2C%20positive%20diagonal%2C%20and%20negative%20diagonal%20edge%0Afeatures%2C%20respectively%2C%20and%20the%20branch%20is%20stacked%20by%20multiple%20Sobel%20layers%0A%28using%20edge%20detectors%20as%20kernels%29%20and%20one%20threshold%20layer.%20The%20binary%20edge%0Afeatures%20learned%20by%20the%20branch%2C%20concatenated%20with%20the%20texture%20features%20learned%0Aby%20the%20backbone%2C%20are%20fed%20into%20the%20fully%20connected%20layers%20for%20classification.%20We%0Aintegrate%20the%20proposed%20branch%20into%20VGG16%20and%20ResNet34%2C%20respectively%2C%20and%0Aconduct%20experiments%20on%20multiple%20datasets.%20Experimental%20results%20demonstrate%20the%0ABEFB%20is%20lightweight%20and%20has%20no%20side%20effects%20on%20training.%20And%20the%20accuracy%20of%0Athe%20BEFB%20integrated%20models%20is%20better%20than%20the%20original%20ones%20on%20all%20datasets%0Awhen%20facing%20FGSM%2C%20PGD%2C%20and%20C%5C%26W%20attacks.%20Besides%2C%20BEFB%20integrated%20models%0Aequipped%20with%20the%20robustness%20enhancing%20techniques%20can%20achieve%20better%0Aclassification%20accuracy%20compared%20to%20the%20original%20models.%20The%20work%20in%20this%20paper%0Afor%20the%20first%20time%20shows%20it%20is%20feasible%20to%20enhance%20the%20robustness%20of%20DCNNs%0Athrough%20combining%20both%20shape-like%20features%20and%20texture%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16479v1&entry.124074799=Read"},
{"title": "DCVSMNet: Double Cost Volume Stereo Matching Network", "author": "Mahmoud Tahmasebi and Saif Huq and Kevin Meehan and Marion McAfee", "abstract": "  We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a\nnovel architecture characterised by by two small upper (group-wise) and lower\n(norm correlation) cost volumes. Each cost volume is processed separately, and\na coupling module is proposed to fuse the geometry information extracted from\nthe upper and lower cost volumes. DCVSMNet is a fast stereo matching network\nwith a 67 ms inference time and strong generalization ability which can produce\ncompetitive results compared to state-of-the-art methods. The results on\nseveral bench mark datasets show that DCVSMNet achieves better accuracy than\nmethods such as CGI-Stereo and BGNet at the cost of greater inference time.\n", "link": "http://arxiv.org/abs/2402.16473v1", "date": "2024-02-26", "relevancy": 2.4291, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5263}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4788}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4524}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCVSMNet%3A%20Double%20Cost%20Volume%20Stereo%20Matching%20Network&entry.906535625=Mahmoud%20Tahmasebi%20and%20Saif%20Huq%20and%20Kevin%20Meehan%20and%20Marion%20McAfee&entry.1292438233=%20%20We%20introduce%20Double%20Cost%20Volume%20Stereo%20Matching%20Network%28DCVSMNet%29%20which%20is%20a%0Anovel%20architecture%20characterised%20by%20by%20two%20small%20upper%20%28group-wise%29%20and%20lower%0A%28norm%20correlation%29%20cost%20volumes.%20Each%20cost%20volume%20is%20processed%20separately%2C%20and%0Aa%20coupling%20module%20is%20proposed%20to%20fuse%20the%20geometry%20information%20extracted%20from%0Athe%20upper%20and%20lower%20cost%20volumes.%20DCVSMNet%20is%20a%20fast%20stereo%20matching%20network%0Awith%20a%2067%20ms%20inference%20time%20and%20strong%20generalization%20ability%20which%20can%20produce%0Acompetitive%20results%20compared%20to%20state-of-the-art%20methods.%20The%20results%20on%0Aseveral%20bench%20mark%20datasets%20show%20that%20DCVSMNet%20achieves%20better%20accuracy%20than%0Amethods%20such%20as%20CGI-Stereo%20and%20BGNet%20at%20the%20cost%20of%20greater%20inference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16473v1&entry.124074799=Read"},
{"title": "VOOM: Robust Visual Object Odometry and Mapping using Hierarchical\n  Landmarks", "author": "Yutong Wang and Chaoyang Jiang and Xieyuanli Chen", "abstract": "  In recent years, object-oriented simultaneous localization and mapping (SLAM)\nhas attracted increasing attention due to its ability to provide high-level\nsemantic information while maintaining computational efficiency. Some\nresearchers have attempted to enhance localization accuracy by integrating the\nmodeled object residuals into bundle adjustment. However, few have demonstrated\nbetter results than feature-based visual SLAM systems, as the generic coarse\nobject models, such as cuboids or ellipsoids, are less accurate than feature\npoints. In this paper, we propose a Visual Object Odometry and Mapping\nframework VOOM using high-level objects and low-level points as the\nhierarchical landmarks in a coarse-to-fine manner instead of directly using\nobject residuals in bundle adjustment. Firstly, we introduce an improved\nobservation model and a novel data association method for dual quadrics,\nemployed to represent physical objects. It facilitates the creation of a 3D map\nthat closely reflects reality. Next, we use object information to enhance the\ndata association of feature points and consequently update the map. In the\nvisual object odometry backend, the updated map is employed to further optimize\nthe camera pose and the objects. Meanwhile, local bundle adjustment is\nperformed utilizing the objects and points-based covisibility graphs in our\nvisual object mapping process. Experiments show that VOOM outperforms both\nobject-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms\nof localization. The implementation of our method is available at\nhttps://github.com/yutongwangBIT/VOOM.git.\n", "link": "http://arxiv.org/abs/2402.13609v2", "date": "2024-02-26", "relevancy": 2.4029, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6436}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5785}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.549}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VOOM%3A%20Robust%20Visual%20Object%20Odometry%20and%20Mapping%20using%20Hierarchical%0A%20%20Landmarks&entry.906535625=Yutong%20Wang%20and%20Chaoyang%20Jiang%20and%20Xieyuanli%20Chen&entry.1292438233=%20%20In%20recent%20years%2C%20object-oriented%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%0Ahas%20attracted%20increasing%20attention%20due%20to%20its%20ability%20to%20provide%20high-level%0Asemantic%20information%20while%20maintaining%20computational%20efficiency.%20Some%0Aresearchers%20have%20attempted%20to%20enhance%20localization%20accuracy%20by%20integrating%20the%0Amodeled%20object%20residuals%20into%20bundle%20adjustment.%20However%2C%20few%20have%20demonstrated%0Abetter%20results%20than%20feature-based%20visual%20SLAM%20systems%2C%20as%20the%20generic%20coarse%0Aobject%20models%2C%20such%20as%20cuboids%20or%20ellipsoids%2C%20are%20less%20accurate%20than%20feature%0Apoints.%20In%20this%20paper%2C%20we%20propose%20a%20Visual%20Object%20Odometry%20and%20Mapping%0Aframework%20VOOM%20using%20high-level%20objects%20and%20low-level%20points%20as%20the%0Ahierarchical%20landmarks%20in%20a%20coarse-to-fine%20manner%20instead%20of%20directly%20using%0Aobject%20residuals%20in%20bundle%20adjustment.%20Firstly%2C%20we%20introduce%20an%20improved%0Aobservation%20model%20and%20a%20novel%20data%20association%20method%20for%20dual%20quadrics%2C%0Aemployed%20to%20represent%20physical%20objects.%20It%20facilitates%20the%20creation%20of%20a%203D%20map%0Athat%20closely%20reflects%20reality.%20Next%2C%20we%20use%20object%20information%20to%20enhance%20the%0Adata%20association%20of%20feature%20points%20and%20consequently%20update%20the%20map.%20In%20the%0Avisual%20object%20odometry%20backend%2C%20the%20updated%20map%20is%20employed%20to%20further%20optimize%0Athe%20camera%20pose%20and%20the%20objects.%20Meanwhile%2C%20local%20bundle%20adjustment%20is%0Aperformed%20utilizing%20the%20objects%20and%20points-based%20covisibility%20graphs%20in%20our%0Avisual%20object%20mapping%20process.%20Experiments%20show%20that%20VOOM%20outperforms%20both%0Aobject-oriented%20SLAM%20and%20feature%20points%20SLAM%20systems%20such%20as%20ORB-SLAM2%20in%20terms%0Aof%20localization.%20The%20implementation%20of%20our%20method%20is%20available%20at%0Ahttps%3A//github.com/yutongwangBIT/VOOM.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13609v2&entry.124074799=Read"},
{"title": "Intelligent Known and Novel Aircraft Recognition -- A Shift from\n  Classification to Similarity Learning for Combat Identification", "author": "Ahmad Saeed and Haasha Bin Atif and Usman Habib and Mohsin Bilal", "abstract": "  Precise aircraft recognition in low-resolution remote sensing imagery is a\nchallenging yet crucial task in aviation, especially combat identification.\nThis research addresses this problem with a novel, scalable, and AI-driven\nsolution. The primary hurdle in combat identification in remote sensing imagery\nis the accurate recognition of Novel/Unknown types of aircraft in addition to\nKnown types. Traditional methods, human expert-driven combat identification and\nimage classification, fall short in identifying Novel classes. Our methodology\nemploys similarity learning to discern features of a broad spectrum of military\nand civilian aircraft. It discerns both Known and Novel aircraft types,\nleveraging metric learning for the identification and supervised few-shot\nlearning for aircraft type classification. To counter the challenge of limited\nlow-resolution remote sensing data, we propose an end-to-end framework that\nadapts to the diverse and versatile process of military aircraft recognition by\ntraining a generalized embedder in fully supervised manner. Comparative\nanalysis with earlier aircraft image classification methods shows that our\napproach is effective for aircraft image classification (F1-score Aircraft Type\nof 0.861) and pioneering for quantifying the identification of Novel types\n(F1-score Bipartitioning of 0.936). The proposed methodology effectively\naddresses inherent challenges in remote sensing data, thereby setting new\nstandards in dataset quality. The research opens new avenues for domain experts\nand demonstrates unique capabilities in distinguishing various aircraft types,\ncontributing to a more robust, domain-adapted potential for real-time aircraft\nrecognition.\n", "link": "http://arxiv.org/abs/2402.16486v1", "date": "2024-02-26", "relevancy": 2.3452, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4781}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4769}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4521}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Known%20and%20Novel%20Aircraft%20Recognition%20--%20A%20Shift%20from%0A%20%20Classification%20to%20Similarity%20Learning%20for%20Combat%20Identification&entry.906535625=Ahmad%20Saeed%20and%20Haasha%20Bin%20Atif%20and%20Usman%20Habib%20and%20Mohsin%20Bilal&entry.1292438233=%20%20Precise%20aircraft%20recognition%20in%20low-resolution%20remote%20sensing%20imagery%20is%20a%0Achallenging%20yet%20crucial%20task%20in%20aviation%2C%20especially%20combat%20identification.%0AThis%20research%20addresses%20this%20problem%20with%20a%20novel%2C%20scalable%2C%20and%20AI-driven%0Asolution.%20The%20primary%20hurdle%20in%20combat%20identification%20in%20remote%20sensing%20imagery%0Ais%20the%20accurate%20recognition%20of%20Novel/Unknown%20types%20of%20aircraft%20in%20addition%20to%0AKnown%20types.%20Traditional%20methods%2C%20human%20expert-driven%20combat%20identification%20and%0Aimage%20classification%2C%20fall%20short%20in%20identifying%20Novel%20classes.%20Our%20methodology%0Aemploys%20similarity%20learning%20to%20discern%20features%20of%20a%20broad%20spectrum%20of%20military%0Aand%20civilian%20aircraft.%20It%20discerns%20both%20Known%20and%20Novel%20aircraft%20types%2C%0Aleveraging%20metric%20learning%20for%20the%20identification%20and%20supervised%20few-shot%0Alearning%20for%20aircraft%20type%20classification.%20To%20counter%20the%20challenge%20of%20limited%0Alow-resolution%20remote%20sensing%20data%2C%20we%20propose%20an%20end-to-end%20framework%20that%0Aadapts%20to%20the%20diverse%20and%20versatile%20process%20of%20military%20aircraft%20recognition%20by%0Atraining%20a%20generalized%20embedder%20in%20fully%20supervised%20manner.%20Comparative%0Aanalysis%20with%20earlier%20aircraft%20image%20classification%20methods%20shows%20that%20our%0Aapproach%20is%20effective%20for%20aircraft%20image%20classification%20%28F1-score%20Aircraft%20Type%0Aof%200.861%29%20and%20pioneering%20for%20quantifying%20the%20identification%20of%20Novel%20types%0A%28F1-score%20Bipartitioning%20of%200.936%29.%20The%20proposed%20methodology%20effectively%0Aaddresses%20inherent%20challenges%20in%20remote%20sensing%20data%2C%20thereby%20setting%20new%0Astandards%20in%20dataset%20quality.%20The%20research%20opens%20new%20avenues%20for%20domain%20experts%0Aand%20demonstrates%20unique%20capabilities%20in%20distinguishing%20various%20aircraft%20types%2C%0Acontributing%20to%20a%20more%20robust%2C%20domain-adapted%20potential%20for%20real-time%20aircraft%0Arecognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16486v1&entry.124074799=Read"},
{"title": "Neural Diffusion Models", "author": "Grigory Bartosh and Dmitry Vetrov and Christian A. Naesseth", "abstract": "  Diffusion models have shown remarkable performance on many generative tasks.\nDespite recent success, most diffusion models are restricted in that they only\nallow linear transformation of the data distribution. In contrast, broader\nfamily of transformations can potentially help train generative distributions\nmore efficiently, simplifying the reverse process and closing the gap between\nthe true negative log-likelihood and the variational approximation. In this\npaper, we present Neural Diffusion Models (NDMs), a generalization of\nconventional diffusion models that enables defining and learning time-dependent\nnon-linear transformations of data. We show how to optimise NDMs using a\nvariational bound in a simulation-free setting. Moreover, we derive a\ntime-continuous formulation of NDMs, which allows fast and reliable inference\nusing off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the\nutility of NDMs with learnable transformations through experiments on standard\nimage generation benchmarks, including CIFAR-10, downsampled versions of\nImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms\nof likelihood and produce high-quality samples.\n", "link": "http://arxiv.org/abs/2310.08337v2", "date": "2024-02-26", "relevancy": 2.1429, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5467}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5284}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5265}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Diffusion%20Models&entry.906535625=Grigory%20Bartosh%20and%20Dmitry%20Vetrov%20and%20Christian%20A.%20Naesseth&entry.1292438233=%20%20Diffusion%20models%20have%20shown%20remarkable%20performance%20on%20many%20generative%20tasks.%0ADespite%20recent%20success%2C%20most%20diffusion%20models%20are%20restricted%20in%20that%20they%20only%0Aallow%20linear%20transformation%20of%20the%20data%20distribution.%20In%20contrast%2C%20broader%0Afamily%20of%20transformations%20can%20potentially%20help%20train%20generative%20distributions%0Amore%20efficiently%2C%20simplifying%20the%20reverse%20process%20and%20closing%20the%20gap%20between%0Athe%20true%20negative%20log-likelihood%20and%20the%20variational%20approximation.%20In%20this%0Apaper%2C%20we%20present%20Neural%20Diffusion%20Models%20%28NDMs%29%2C%20a%20generalization%20of%0Aconventional%20diffusion%20models%20that%20enables%20defining%20and%20learning%20time-dependent%0Anon-linear%20transformations%20of%20data.%20We%20show%20how%20to%20optimise%20NDMs%20using%20a%0Avariational%20bound%20in%20a%20simulation-free%20setting.%20Moreover%2C%20we%20derive%20a%0Atime-continuous%20formulation%20of%20NDMs%2C%20which%20allows%20fast%20and%20reliable%20inference%0Ausing%20off-the-shelf%20numerical%20ODE%20and%20SDE%20solvers.%20Finally%2C%20we%20demonstrate%20the%0Autility%20of%20NDMs%20with%20learnable%20transformations%20through%20experiments%20on%20standard%0Aimage%20generation%20benchmarks%2C%20including%20CIFAR-10%2C%20downsampled%20versions%20of%0AImageNet%20and%20CelebA-HQ.%20NDMs%20outperform%20conventional%20diffusion%20models%20in%20terms%0Aof%20likelihood%20and%20produce%20high-quality%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08337v2&entry.124074799=Read"},
{"title": "Barrier-Enhanced Homotopic Parallel Trajectory Optimization for\n  Safety-Critical Autonomous Driving", "author": "Lei Zheng and Rui Yang and Michael Yu Wang and Jun Ma", "abstract": "  Enforcing safety while preventing overly conservative behaviors is essential\nfor autonomous vehicles to achieve high task performance. In this paper, we\npropose a barrier-enhanced homotopic parallel trajectory optimization (BHPTO)\napproach with over-relaxed alternating direction method of multipliers (ADMM)\nfor real-time integrated decision-making and planning. To facilitate safety\ninteractions between the ego vehicle (EV) and surrounding vehicles, a\nspatiotemporal safety module exhibiting bi-convexity is developed on the basis\nof barrier function. Varying barrier coefficients are adopted for different\ntime steps in a planning horizon to account for the motion uncertainties of\nsurrounding HVs and mitigate conservative behaviors. Additionally, we exploit\nthe discrete characteristics of driving maneuvers to initialize nominal\nbehavior-oriented free-end homotopic trajectories based on reachability\nanalysis, and each trajectory is locally constrained to a specific driving\nmaneuver while sharing the same task objectives. By leveraging the bi-convexity\nof the safety module and the kinematics of the EV, we formulate the BHPTO as a\nbi-convex optimization problem. Then constraint transcription and over-relaxed\nADMM are employed to streamline the optimization process, such that multiple\ntrajectories are generated in real time with feasibility guarantees. Through a\nseries of experiments, the proposed development demonstrates improved task\naccuracy, stability, and consistency in various traffic scenarios using\nsynthetic and real-world traffic datasets.\n", "link": "http://arxiv.org/abs/2402.10441v2", "date": "2024-02-26", "relevancy": 2.1223, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5517}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5309}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5093}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Barrier-Enhanced%20Homotopic%20Parallel%20Trajectory%20Optimization%20for%0A%20%20Safety-Critical%20Autonomous%20Driving&entry.906535625=Lei%20Zheng%20and%20Rui%20Yang%20and%20Michael%20Yu%20Wang%20and%20Jun%20Ma&entry.1292438233=%20%20Enforcing%20safety%20while%20preventing%20overly%20conservative%20behaviors%20is%20essential%0Afor%20autonomous%20vehicles%20to%20achieve%20high%20task%20performance.%20In%20this%20paper%2C%20we%0Apropose%20a%20barrier-enhanced%20homotopic%20parallel%20trajectory%20optimization%20%28BHPTO%29%0Aapproach%20with%20over-relaxed%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%0Afor%20real-time%20integrated%20decision-making%20and%20planning.%20To%20facilitate%20safety%0Ainteractions%20between%20the%20ego%20vehicle%20%28EV%29%20and%20surrounding%20vehicles%2C%20a%0Aspatiotemporal%20safety%20module%20exhibiting%20bi-convexity%20is%20developed%20on%20the%20basis%0Aof%20barrier%20function.%20Varying%20barrier%20coefficients%20are%20adopted%20for%20different%0Atime%20steps%20in%20a%20planning%20horizon%20to%20account%20for%20the%20motion%20uncertainties%20of%0Asurrounding%20HVs%20and%20mitigate%20conservative%20behaviors.%20Additionally%2C%20we%20exploit%0Athe%20discrete%20characteristics%20of%20driving%20maneuvers%20to%20initialize%20nominal%0Abehavior-oriented%20free-end%20homotopic%20trajectories%20based%20on%20reachability%0Aanalysis%2C%20and%20each%20trajectory%20is%20locally%20constrained%20to%20a%20specific%20driving%0Amaneuver%20while%20sharing%20the%20same%20task%20objectives.%20By%20leveraging%20the%20bi-convexity%0Aof%20the%20safety%20module%20and%20the%20kinematics%20of%20the%20EV%2C%20we%20formulate%20the%20BHPTO%20as%20a%0Abi-convex%20optimization%20problem.%20Then%20constraint%20transcription%20and%20over-relaxed%0AADMM%20are%20employed%20to%20streamline%20the%20optimization%20process%2C%20such%20that%20multiple%0Atrajectories%20are%20generated%20in%20real%20time%20with%20feasibility%20guarantees.%20Through%20a%0Aseries%20of%20experiments%2C%20the%20proposed%20development%20demonstrates%20improved%20task%0Aaccuracy%2C%20stability%2C%20and%20consistency%20in%20various%20traffic%20scenarios%20using%0Asynthetic%20and%20real-world%20traffic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10441v2&entry.124074799=Read"},
{"title": "Processing and Segmentation of Human Teeth from 2D Images using Weakly\n  Supervised Learning", "author": "Tom\u00e1\u0161 Kunzo and Viktor Kocur and Luk\u00e1\u0161 Gajdo\u0161ech and Martin Madaras", "abstract": "  Teeth segmentation is an essential task in dental image analysis for accurate\ndiagnosis and treatment planning. While supervised deep learning methods can be\nutilized for teeth segmentation, they often require extensive manual annotation\nof segmentation masks, which is time-consuming and costly. In this research, we\npropose a weakly supervised approach for teeth segmentation that reduces the\nneed for manual annotation. Our method utilizes the output heatmaps and\nintermediate feature maps from a keypoint detection network to guide the\nsegmentation process. We introduce the TriDental dataset, consisting of 3000\noral cavity images annotated with teeth keypoints, to train a teeth keypoint\ndetection network. We combine feature maps from different layers of the\nkeypoint detection network, enabling accurate teeth segmentation without\nexplicit segmentation annotations. The detected keypoints are also used for\nfurther refinement of the segmentation masks. Experimental results on the\nTriDental dataset demonstrate the superiority of our approach in terms of\naccuracy and robustness compared to state-of-the-art segmentation methods. Our\nmethod offers a cost-effective and efficient solution for teeth segmentation in\nreal-world dental applications, eliminating the need for extensive manual\nannotation efforts.\n", "link": "http://arxiv.org/abs/2311.07398v2", "date": "2024-02-26", "relevancy": 2.0363, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5196}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5026}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4992}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Processing%20and%20Segmentation%20of%20Human%20Teeth%20from%202D%20Images%20using%20Weakly%0A%20%20Supervised%20Learning&entry.906535625=Tom%C3%A1%C5%A1%20Kunzo%20and%20Viktor%20Kocur%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Martin%20Madaras&entry.1292438233=%20%20Teeth%20segmentation%20is%20an%20essential%20task%20in%20dental%20image%20analysis%20for%20accurate%0Adiagnosis%20and%20treatment%20planning.%20While%20supervised%20deep%20learning%20methods%20can%20be%0Autilized%20for%20teeth%20segmentation%2C%20they%20often%20require%20extensive%20manual%20annotation%0Aof%20segmentation%20masks%2C%20which%20is%20time-consuming%20and%20costly.%20In%20this%20research%2C%20we%0Apropose%20a%20weakly%20supervised%20approach%20for%20teeth%20segmentation%20that%20reduces%20the%0Aneed%20for%20manual%20annotation.%20Our%20method%20utilizes%20the%20output%20heatmaps%20and%0Aintermediate%20feature%20maps%20from%20a%20keypoint%20detection%20network%20to%20guide%20the%0Asegmentation%20process.%20We%20introduce%20the%20TriDental%20dataset%2C%20consisting%20of%203000%0Aoral%20cavity%20images%20annotated%20with%20teeth%20keypoints%2C%20to%20train%20a%20teeth%20keypoint%0Adetection%20network.%20We%20combine%20feature%20maps%20from%20different%20layers%20of%20the%0Akeypoint%20detection%20network%2C%20enabling%20accurate%20teeth%20segmentation%20without%0Aexplicit%20segmentation%20annotations.%20The%20detected%20keypoints%20are%20also%20used%20for%0Afurther%20refinement%20of%20the%20segmentation%20masks.%20Experimental%20results%20on%20the%0ATriDental%20dataset%20demonstrate%20the%20superiority%20of%20our%20approach%20in%20terms%20of%0Aaccuracy%20and%20robustness%20compared%20to%20state-of-the-art%20segmentation%20methods.%20Our%0Amethod%20offers%20a%20cost-effective%20and%20efficient%20solution%20for%20teeth%20segmentation%20in%0Areal-world%20dental%20applications%2C%20eliminating%20the%20need%20for%20extensive%20manual%0Aannotation%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07398v2&entry.124074799=Read"},
{"title": "Trajectory Prediction for Autonomous Driving Using a Transformer Network", "author": "Zhenning Li and Hao Yu", "abstract": "  Predicting the trajectories of surrounding agents is still considered one of\nthe most challenging tasks for autonomous driving. In this paper, we introduce\na multi-modal trajectory prediction framework based on the transformer network.\nThe semantic maps of each agent are used as inputs to convolutional networks to\nautomatically derive relevant contextual information. A novel auxiliary loss\nthat penalizes unfeasible off-road predictions is also proposed in this study.\nExperiments on the Lyft l5kit dataset show that the proposed model achieves\nstate-of-the-art performance, substantially improving the accuracy and\nfeasibility of the prediction outcomes.\n", "link": "http://arxiv.org/abs/2402.16501v1", "date": "2024-02-26", "relevancy": 1.9986, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5612}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4876}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4871}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20Prediction%20for%20Autonomous%20Driving%20Using%20a%20Transformer%20Network&entry.906535625=Zhenning%20Li%20and%20Hao%20Yu&entry.1292438233=%20%20Predicting%20the%20trajectories%20of%20surrounding%20agents%20is%20still%20considered%20one%20of%0Athe%20most%20challenging%20tasks%20for%20autonomous%20driving.%20In%20this%20paper%2C%20we%20introduce%0Aa%20multi-modal%20trajectory%20prediction%20framework%20based%20on%20the%20transformer%20network.%0AThe%20semantic%20maps%20of%20each%20agent%20are%20used%20as%20inputs%20to%20convolutional%20networks%20to%0Aautomatically%20derive%20relevant%20contextual%20information.%20A%20novel%20auxiliary%20loss%0Athat%20penalizes%20unfeasible%20off-road%20predictions%20is%20also%20proposed%20in%20this%20study.%0AExperiments%20on%20the%20Lyft%20l5kit%20dataset%20show%20that%20the%20proposed%20model%20achieves%0Astate-of-the-art%20performance%2C%20substantially%20improving%20the%20accuracy%20and%0Afeasibility%20of%20the%20prediction%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16501v1&entry.124074799=Read"},
{"title": "Contrastive Initial State Buffer for Reinforcement Learning", "author": "Nico Messikommer and Yunlong Song and Davide Scaramuzza", "abstract": "  In Reinforcement Learning, the trade-off between exploration and exploitation\nposes a complex challenge for achieving efficient learning from limited\nsamples. While recent works have been effective in leveraging past experiences\nfor policy updates, they often overlook the potential of reusing past\nexperiences for data collection. Independent of the underlying RL algorithm, we\nintroduce the concept of a Contrastive Initial State Buffer, which\nstrategically selects states from past experiences and uses them to initialize\nthe agent in the environment in order to guide it toward more informative\nstates. We validate our approach on two complex robotic tasks without relying\non any prior information about the environment: (i) locomotion of a quadruped\nrobot traversing challenging terrains and (ii) a quadcopter drone racing\nthrough a track. The experimental results show that our initial state buffer\nachieves higher task performance than the nominal baseline while also speeding\nup training convergence.\n", "link": "http://arxiv.org/abs/2309.09752v3", "date": "2024-02-26", "relevancy": 1.9745, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4967}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4952}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4908}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Initial%20State%20Buffer%20for%20Reinforcement%20Learning&entry.906535625=Nico%20Messikommer%20and%20Yunlong%20Song%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20In%20Reinforcement%20Learning%2C%20the%20trade-off%20between%20exploration%20and%20exploitation%0Aposes%20a%20complex%20challenge%20for%20achieving%20efficient%20learning%20from%20limited%0Asamples.%20While%20recent%20works%20have%20been%20effective%20in%20leveraging%20past%20experiences%0Afor%20policy%20updates%2C%20they%20often%20overlook%20the%20potential%20of%20reusing%20past%0Aexperiences%20for%20data%20collection.%20Independent%20of%20the%20underlying%20RL%20algorithm%2C%20we%0Aintroduce%20the%20concept%20of%20a%20Contrastive%20Initial%20State%20Buffer%2C%20which%0Astrategically%20selects%20states%20from%20past%20experiences%20and%20uses%20them%20to%20initialize%0Athe%20agent%20in%20the%20environment%20in%20order%20to%20guide%20it%20toward%20more%20informative%0Astates.%20We%20validate%20our%20approach%20on%20two%20complex%20robotic%20tasks%20without%20relying%0Aon%20any%20prior%20information%20about%20the%20environment%3A%20%28i%29%20locomotion%20of%20a%20quadruped%0Arobot%20traversing%20challenging%20terrains%20and%20%28ii%29%20a%20quadcopter%20drone%20racing%0Athrough%20a%20track.%20The%20experimental%20results%20show%20that%20our%20initial%20state%20buffer%0Aachieves%20higher%20task%20performance%20than%20the%20nominal%20baseline%20while%20also%20speeding%0Aup%20training%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09752v3&entry.124074799=Read"},
{"title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning", "author": "Mingtian Zhang and Shawn Lan and Peter Hayes and David Barber", "abstract": "  Retrieval Augmented Generation (RAG) has emerged as an effective solution for\nmitigating hallucinations in Large Language Models (LLMs). The retrieval stage\nin RAG typically involves a pre-trained embedding model, which converts queries\nand passages into vectors to capture their semantics. However, a standard\npre-trained embedding model may exhibit sub-optimal performance when applied to\nspecific domain knowledge, necessitating fine-tuning. This paper addresses\nscenarios where the embeddings are only available from a black-box model. We\nintroduce Model augmented fine-tuning (Mafin) -- a novel approach for\nfine-tuning a black-box embedding model by augmenting it with a trainable\nembedding model. Our results demonstrate that Mafin significantly enhances the\nperformance of the black-box embeddings by only requiring the training of a\nsmall augmented model. We validate the effectiveness of our method on both\nlabeled and unlabeled datasets, illustrating its broad applicability and\nefficiency.\n", "link": "http://arxiv.org/abs/2402.12177v2", "date": "2024-02-26", "relevancy": 1.9706, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5112}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5006}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4709}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mafin%3A%20Enhancing%20Black-Box%20Embeddings%20with%20Model%20Augmented%20Fine-Tuning&entry.906535625=Mingtian%20Zhang%20and%20Shawn%20Lan%20and%20Peter%20Hayes%20and%20David%20Barber&entry.1292438233=%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20has%20emerged%20as%20an%20effective%20solution%20for%0Amitigating%20hallucinations%20in%20Large%20Language%20Models%20%28LLMs%29.%20The%20retrieval%20stage%0Ain%20RAG%20typically%20involves%20a%20pre-trained%20embedding%20model%2C%20which%20converts%20queries%0Aand%20passages%20into%20vectors%20to%20capture%20their%20semantics.%20However%2C%20a%20standard%0Apre-trained%20embedding%20model%20may%20exhibit%20sub-optimal%20performance%20when%20applied%20to%0Aspecific%20domain%20knowledge%2C%20necessitating%20fine-tuning.%20This%20paper%20addresses%0Ascenarios%20where%20the%20embeddings%20are%20only%20available%20from%20a%20black-box%20model.%20We%0Aintroduce%20Model%20augmented%20fine-tuning%20%28Mafin%29%20--%20a%20novel%20approach%20for%0Afine-tuning%20a%20black-box%20embedding%20model%20by%20augmenting%20it%20with%20a%20trainable%0Aembedding%20model.%20Our%20results%20demonstrate%20that%20Mafin%20significantly%20enhances%20the%0Aperformance%20of%20the%20black-box%20embeddings%20by%20only%20requiring%20the%20training%20of%20a%0Asmall%20augmented%20model.%20We%20validate%20the%20effectiveness%20of%20our%20method%20on%20both%0Alabeled%20and%20unlabeled%20datasets%2C%20illustrating%20its%20broad%20applicability%20and%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12177v2&entry.124074799=Read"},
{"title": "Closing the Gap Between SGP4 and High-Precision Propagation via\n  Differentiable Programming", "author": "Giacomo Acciarini and At\u0131l\u0131m G\u00fcne\u015f Baydin and Dario Izzo", "abstract": "  The Simplified General Perturbations 4 (SGP4) orbital propagation method is\nwidely used for predicting the positions and velocities of Earth-orbiting\nobjects rapidly and reliably. Despite continuous refinement, SGP models still\nlack the precision of numerical propagators, which offer significantly smaller\nerrors. This study presents dSGP4, a novel differentiable version of SGP4\nimplemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates\nvarious space-related applications, including spacecraft orbit determination,\nstate conversion, covariance transformation, state transition matrix\ncomputation, and covariance propagation. Additionally, dSGP4's PyTorch\nimplementation allows for embarrassingly parallel orbital propagation across\nbatches of Two-Line Element Sets (TLEs), leveraging the computational power of\nCPUs, GPUs, and advanced hardware for distributed prediction of satellite\npositions at future times. Furthermore, dSGP4's differentiability enables\nintegration with modern machine learning techniques. Thus, we propose a novel\norbital propagation paradigm, ML-dSGP4, where neural networks are integrated\ninto the orbital propagator. Through stochastic gradient descent, this combined\nmodel's inputs, outputs, and parameters can be iteratively refined, surpassing\nSGP4's precision. Neural networks act as identity operators by default,\nadhering to SGP4's behavior. However, dSGP4's differentiability allows\nfine-tuning with ephemeris data, enhancing precision while maintaining\ncomputational speed. This empowers satellite operators and researchers to train\nthe model using specific ephemeris or high-precision numerical propagation\ndata, significantly advancing orbital prediction capabilities.\n", "link": "http://arxiv.org/abs/2402.04830v2", "date": "2024-02-26", "relevancy": 1.9459, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5104}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4865}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4626}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Gap%20Between%20SGP4%20and%20High-Precision%20Propagation%20via%0A%20%20Differentiable%20Programming&entry.906535625=Giacomo%20Acciarini%20and%20At%C4%B1l%C4%B1m%20G%C3%BCne%C5%9F%20Baydin%20and%20Dario%20Izzo&entry.1292438233=%20%20The%20Simplified%20General%20Perturbations%204%20%28SGP4%29%20orbital%20propagation%20method%20is%0Awidely%20used%20for%20predicting%20the%20positions%20and%20velocities%20of%20Earth-orbiting%0Aobjects%20rapidly%20and%20reliably.%20Despite%20continuous%20refinement%2C%20SGP%20models%20still%0Alack%20the%20precision%20of%20numerical%20propagators%2C%20which%20offer%20significantly%20smaller%0Aerrors.%20This%20study%20presents%20dSGP4%2C%20a%20novel%20differentiable%20version%20of%20SGP4%0Aimplemented%20using%20PyTorch.%20By%20making%20SGP4%20differentiable%2C%20dSGP4%20facilitates%0Avarious%20space-related%20applications%2C%20including%20spacecraft%20orbit%20determination%2C%0Astate%20conversion%2C%20covariance%20transformation%2C%20state%20transition%20matrix%0Acomputation%2C%20and%20covariance%20propagation.%20Additionally%2C%20dSGP4%27s%20PyTorch%0Aimplementation%20allows%20for%20embarrassingly%20parallel%20orbital%20propagation%20across%0Abatches%20of%20Two-Line%20Element%20Sets%20%28TLEs%29%2C%20leveraging%20the%20computational%20power%20of%0ACPUs%2C%20GPUs%2C%20and%20advanced%20hardware%20for%20distributed%20prediction%20of%20satellite%0Apositions%20at%20future%20times.%20Furthermore%2C%20dSGP4%27s%20differentiability%20enables%0Aintegration%20with%20modern%20machine%20learning%20techniques.%20Thus%2C%20we%20propose%20a%20novel%0Aorbital%20propagation%20paradigm%2C%20ML-dSGP4%2C%20where%20neural%20networks%20are%20integrated%0Ainto%20the%20orbital%20propagator.%20Through%20stochastic%20gradient%20descent%2C%20this%20combined%0Amodel%27s%20inputs%2C%20outputs%2C%20and%20parameters%20can%20be%20iteratively%20refined%2C%20surpassing%0ASGP4%27s%20precision.%20Neural%20networks%20act%20as%20identity%20operators%20by%20default%2C%0Aadhering%20to%20SGP4%27s%20behavior.%20However%2C%20dSGP4%27s%20differentiability%20allows%0Afine-tuning%20with%20ephemeris%20data%2C%20enhancing%20precision%20while%20maintaining%0Acomputational%20speed.%20This%20empowers%20satellite%20operators%20and%20researchers%20to%20train%0Athe%20model%20using%20specific%20ephemeris%20or%20high-precision%20numerical%20propagation%0Adata%2C%20significantly%20advancing%20orbital%20prediction%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04830v2&entry.124074799=Read"},
{"title": "Supersampling of Data from Structured-light Scanner with Deep Learning", "author": "Martin Melicher\u010d\u00edk and Luk\u00e1\u0161 Gajdo\u0161ech and Viktor Kocur and Martin Madaras", "abstract": "  This paper focuses on increasing the resolution of depth maps obtained from\n3D cameras using structured light technology. Two deep learning models FDSR and\nDKN are modified to work with high-resolution data, and data pre-processing\ntechniques are implemented for stable training. The models are trained on our\ncustom dataset of 1200 3D scans. The resulting high-resolution depth maps are\nevaluated using qualitative and quantitative metrics. The approach for depth\nmap upsampling offers benefits such as reducing the processing time of a\npipeline by first downsampling a high-resolution depth map, performing various\nprocessing steps at the lower resolution and upsampling the resulting depth map\nor increasing the resolution of a point cloud captured in lower resolution by a\ncheaper device. The experiments demonstrate that the FDSR model excels in terms\nof faster processing time, making it a suitable choice for applications where\nspeed is crucial. On the other hand, the DKN model provides results with higher\nprecision, making it more suitable for applications that prioritize accuracy.\n", "link": "http://arxiv.org/abs/2311.07432v2", "date": "2024-02-26", "relevancy": 1.9034, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4887}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.471}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4559}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supersampling%20of%20Data%20from%20Structured-light%20Scanner%20with%20Deep%20Learning&entry.906535625=Martin%20Melicher%C4%8D%C3%ADk%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Viktor%20Kocur%20and%20Martin%20Madaras&entry.1292438233=%20%20This%20paper%20focuses%20on%20increasing%20the%20resolution%20of%20depth%20maps%20obtained%20from%0A3D%20cameras%20using%20structured%20light%20technology.%20Two%20deep%20learning%20models%20FDSR%20and%0ADKN%20are%20modified%20to%20work%20with%20high-resolution%20data%2C%20and%20data%20pre-processing%0Atechniques%20are%20implemented%20for%20stable%20training.%20The%20models%20are%20trained%20on%20our%0Acustom%20dataset%20of%201200%203D%20scans.%20The%20resulting%20high-resolution%20depth%20maps%20are%0Aevaluated%20using%20qualitative%20and%20quantitative%20metrics.%20The%20approach%20for%20depth%0Amap%20upsampling%20offers%20benefits%20such%20as%20reducing%20the%20processing%20time%20of%20a%0Apipeline%20by%20first%20downsampling%20a%20high-resolution%20depth%20map%2C%20performing%20various%0Aprocessing%20steps%20at%20the%20lower%20resolution%20and%20upsampling%20the%20resulting%20depth%20map%0Aor%20increasing%20the%20resolution%20of%20a%20point%20cloud%20captured%20in%20lower%20resolution%20by%20a%0Acheaper%20device.%20The%20experiments%20demonstrate%20that%20the%20FDSR%20model%20excels%20in%20terms%0Aof%20faster%20processing%20time%2C%20making%20it%20a%20suitable%20choice%20for%20applications%20where%0Aspeed%20is%20crucial.%20On%20the%20other%20hand%2C%20the%20DKN%20model%20provides%20results%20with%20higher%0Aprecision%2C%20making%20it%20more%20suitable%20for%20applications%20that%20prioritize%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07432v2&entry.124074799=Read"},
{"title": "Don't Miss Out on Novelty: Importance of Novel Features for Deep Anomaly\n  Detection", "author": "Sarath Sivaprasad and Mario Fritz", "abstract": "  Anomaly Detection (AD) is a critical task that involves identifying\nobservations that do not conform to a learned model of normality. Prior work in\ndeep AD is predominantly based on a familiarity hypothesis, where familiar\nfeatures serve as the reference in a pre-trained embedding space. While this\nstrategy has proven highly successful, it turns out that it causes consistent\nfalse negatives when anomalies consist of truly novel features that are not\nwell captured by the pre-trained encoding. We propose a novel approach to AD\nusing explainability to capture such novel features as unexplained observations\nin the input space. We achieve strong performance across a wide range of\nanomaly benchmarks by combining familiarity and novelty in a hybrid approach.\nOur approach establishes a new state-of-the-art across multiple benchmarks,\nhandling diverse anomaly types while eliminating the need for expensive\nbackground models and dense matching. In particular, we show that by taking\naccount of novel features, we reduce false negative anomalies by up to 40% on\nchallenging benchmarks compared to the state-of-the-art. Our method gives\nvisually inspectable explanations for pixel-level anomalies.\n", "link": "http://arxiv.org/abs/2310.00797v4", "date": "2024-02-26", "relevancy": 1.8878, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4997}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4524}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4515}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Miss%20Out%20on%20Novelty%3A%20Importance%20of%20Novel%20Features%20for%20Deep%20Anomaly%0A%20%20Detection&entry.906535625=Sarath%20Sivaprasad%20and%20Mario%20Fritz&entry.1292438233=%20%20Anomaly%20Detection%20%28AD%29%20is%20a%20critical%20task%20that%20involves%20identifying%0Aobservations%20that%20do%20not%20conform%20to%20a%20learned%20model%20of%20normality.%20Prior%20work%20in%0Adeep%20AD%20is%20predominantly%20based%20on%20a%20familiarity%20hypothesis%2C%20where%20familiar%0Afeatures%20serve%20as%20the%20reference%20in%20a%20pre-trained%20embedding%20space.%20While%20this%0Astrategy%20has%20proven%20highly%20successful%2C%20it%20turns%20out%20that%20it%20causes%20consistent%0Afalse%20negatives%20when%20anomalies%20consist%20of%20truly%20novel%20features%20that%20are%20not%0Awell%20captured%20by%20the%20pre-trained%20encoding.%20We%20propose%20a%20novel%20approach%20to%20AD%0Ausing%20explainability%20to%20capture%20such%20novel%20features%20as%20unexplained%20observations%0Ain%20the%20input%20space.%20We%20achieve%20strong%20performance%20across%20a%20wide%20range%20of%0Aanomaly%20benchmarks%20by%20combining%20familiarity%20and%20novelty%20in%20a%20hybrid%20approach.%0AOur%20approach%20establishes%20a%20new%20state-of-the-art%20across%20multiple%20benchmarks%2C%0Ahandling%20diverse%20anomaly%20types%20while%20eliminating%20the%20need%20for%20expensive%0Abackground%20models%20and%20dense%20matching.%20In%20particular%2C%20we%20show%20that%20by%20taking%0Aaccount%20of%20novel%20features%2C%20we%20reduce%20false%20negative%20anomalies%20by%20up%20to%2040%25%20on%0Achallenging%20benchmarks%20compared%20to%20the%20state-of-the-art.%20Our%20method%20gives%0Avisually%20inspectable%20explanations%20for%20pixel-level%20anomalies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00797v4&entry.124074799=Read"},
{"title": "Online Efficient Safety-Critical Control for Mobile Robots in Unknown\n  Dynamic Multi-Obstacle Environments", "author": "Yu Zhang and Guangyao Tian and Long Wen and Xiangtong Yao and Liding Zhang and Zhenshan Bing and Wei He and Alois Knoll", "abstract": "  This paper proposes a LiDAR-based goal-seeking and exploration framework,\naddressing the efficiency of online obstacle avoidance in unstructured\nenvironments populated with static and moving obstacles. This framework\naddresses two significant challenges associated with traditional dynamic\ncontrol barrier functions (D-CBFs): their online construction and the\ndiminished real-time performance caused by utilizing multiple D-CBFs. To tackle\nthe first challenge, the framework's perception component begins with\nclustering point clouds via the DBSCAN algorithm, followed by encapsulating\nthese clusters with the minimum bounding ellipses (MBEs) algorithm to create\nelliptical representations. By comparing the current state of MBEs with those\nstored from previous moments, the differentiation between static and dynamic\nobstacles is realized, and the Kalman filter is utilized to predict the\nmovements of the latter. Such analysis facilitates the D-CBF's online\nconstruction for each MBE. To tackle the second challenge, we introduce buffer\nzones, generating Type-II D-CBFs online for each identified obstacle. Utilizing\nthese buffer zones as activation areas substantially reduces the number of\nD-CBFs that need to be activated. Upon entering these buffer zones, the system\nprioritizes safety, autonomously navigating safe paths, and hence referred to\nas the exploration mode. Exiting these buffer zones triggers the system's\ntransition to goal-seeking mode. We demonstrate that the system's states under\nthis framework achieve safety and asymptotic stabilization. Experimental\nresults in simulated and real-world environments have validated our framework's\ncapability, allowing a LiDAR-equipped mobile robot to efficiently and safely\nreach the desired location within dynamic environments containing multiple\nobstacles.\n", "link": "http://arxiv.org/abs/2402.16449v1", "date": "2024-02-26", "relevancy": 1.8368, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6425}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6087}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6016}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Efficient%20Safety-Critical%20Control%20for%20Mobile%20Robots%20in%20Unknown%0A%20%20Dynamic%20Multi-Obstacle%20Environments&entry.906535625=Yu%20Zhang%20and%20Guangyao%20Tian%20and%20Long%20Wen%20and%20Xiangtong%20Yao%20and%20Liding%20Zhang%20and%20Zhenshan%20Bing%20and%20Wei%20He%20and%20Alois%20Knoll&entry.1292438233=%20%20This%20paper%20proposes%20a%20LiDAR-based%20goal-seeking%20and%20exploration%20framework%2C%0Aaddressing%20the%20efficiency%20of%20online%20obstacle%20avoidance%20in%20unstructured%0Aenvironments%20populated%20with%20static%20and%20moving%20obstacles.%20This%20framework%0Aaddresses%20two%20significant%20challenges%20associated%20with%20traditional%20dynamic%0Acontrol%20barrier%20functions%20%28D-CBFs%29%3A%20their%20online%20construction%20and%20the%0Adiminished%20real-time%20performance%20caused%20by%20utilizing%20multiple%20D-CBFs.%20To%20tackle%0Athe%20first%20challenge%2C%20the%20framework%27s%20perception%20component%20begins%20with%0Aclustering%20point%20clouds%20via%20the%20DBSCAN%20algorithm%2C%20followed%20by%20encapsulating%0Athese%20clusters%20with%20the%20minimum%20bounding%20ellipses%20%28MBEs%29%20algorithm%20to%20create%0Aelliptical%20representations.%20By%20comparing%20the%20current%20state%20of%20MBEs%20with%20those%0Astored%20from%20previous%20moments%2C%20the%20differentiation%20between%20static%20and%20dynamic%0Aobstacles%20is%20realized%2C%20and%20the%20Kalman%20filter%20is%20utilized%20to%20predict%20the%0Amovements%20of%20the%20latter.%20Such%20analysis%20facilitates%20the%20D-CBF%27s%20online%0Aconstruction%20for%20each%20MBE.%20To%20tackle%20the%20second%20challenge%2C%20we%20introduce%20buffer%0Azones%2C%20generating%20Type-II%20D-CBFs%20online%20for%20each%20identified%20obstacle.%20Utilizing%0Athese%20buffer%20zones%20as%20activation%20areas%20substantially%20reduces%20the%20number%20of%0AD-CBFs%20that%20need%20to%20be%20activated.%20Upon%20entering%20these%20buffer%20zones%2C%20the%20system%0Aprioritizes%20safety%2C%20autonomously%20navigating%20safe%20paths%2C%20and%20hence%20referred%20to%0Aas%20the%20exploration%20mode.%20Exiting%20these%20buffer%20zones%20triggers%20the%20system%27s%0Atransition%20to%20goal-seeking%20mode.%20We%20demonstrate%20that%20the%20system%27s%20states%20under%0Athis%20framework%20achieve%20safety%20and%20asymptotic%20stabilization.%20Experimental%0Aresults%20in%20simulated%20and%20real-world%20environments%20have%20validated%20our%20framework%27s%0Acapability%2C%20allowing%20a%20LiDAR-equipped%20mobile%20robot%20to%20efficiently%20and%20safely%0Areach%20the%20desired%20location%20within%20dynamic%20environments%20containing%20multiple%0Aobstacles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16449v1&entry.124074799=Read"},
{"title": "ArabianGPT: Native Arabic GPT-based Large Language Model", "author": "Anis Koubaa and Adel Ammar and Lahouari Ghouti and Omar Najar and Serry Sibaee", "abstract": "  The predominance of English and Latin-based large language models (LLMs) has\nled to a notable deficit in native Arabic LLMs. This discrepancy is accentuated\nby the prevalent inclusion of English tokens in existing Arabic models,\ndetracting from their efficacy in processing native Arabic's intricate\nmorphology and syntax. Consequently, there is a theoretical and practical\nimperative for developing LLMs predominantly focused on Arabic linguistic\nelements. To address this gap, this paper proposes ArabianGPT, a series of\ntransformer-based models within the ArabianLLM suite designed explicitly for\nArabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in\nsize and complexity, aligning with the nuanced linguistic characteristics of\nArabic. The AraNizer tokenizer, integral to these models, addresses the unique\nmorphological aspects of Arabic script, ensuring more accurate text processing.\nEmpirical results from fine-tuning the models on tasks like sentiment analysis\nand summarization demonstrate significant improvements. For sentiment analysis,\nthe fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a\nsubstantial increase from the base model's 56%. Similarly, in summarization\ntasks, fine-tuned models showed enhanced F1 scores, indicating improved\nprecision and recall in generating concise summaries. Comparative analysis of\nfine-tuned ArabianGPT models against their base versions across various\nbenchmarks reveals nuanced differences in performance, with fine-tuning\npositively impacting specific tasks like question answering and summarization.\nThese findings underscore the efficacy of fine-tuning in aligning ArabianGPT\nmodels more closely with specific NLP tasks, highlighting the potential of\ntailored transformer architectures in advancing Arabic NLP.\n", "link": "http://arxiv.org/abs/2402.15313v2", "date": "2024-02-26", "relevancy": 1.6995, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4405}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4187}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4013}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArabianGPT%3A%20Native%20Arabic%20GPT-based%20Large%20Language%20Model&entry.906535625=Anis%20Koubaa%20and%20Adel%20Ammar%20and%20Lahouari%20Ghouti%20and%20Omar%20Najar%20and%20Serry%20Sibaee&entry.1292438233=%20%20The%20predominance%20of%20English%20and%20Latin-based%20large%20language%20models%20%28LLMs%29%20has%0Aled%20to%20a%20notable%20deficit%20in%20native%20Arabic%20LLMs.%20This%20discrepancy%20is%20accentuated%0Aby%20the%20prevalent%20inclusion%20of%20English%20tokens%20in%20existing%20Arabic%20models%2C%0Adetracting%20from%20their%20efficacy%20in%20processing%20native%20Arabic%27s%20intricate%0Amorphology%20and%20syntax.%20Consequently%2C%20there%20is%20a%20theoretical%20and%20practical%0Aimperative%20for%20developing%20LLMs%20predominantly%20focused%20on%20Arabic%20linguistic%0Aelements.%20To%20address%20this%20gap%2C%20this%20paper%20proposes%20ArabianGPT%2C%20a%20series%20of%0Atransformer-based%20models%20within%20the%20ArabianLLM%20suite%20designed%20explicitly%20for%0AArabic.%20These%20models%2C%20including%20ArabianGPT-0.1B%20and%20ArabianGPT-0.3B%2C%20vary%20in%0Asize%20and%20complexity%2C%20aligning%20with%20the%20nuanced%20linguistic%20characteristics%20of%0AArabic.%20The%20AraNizer%20tokenizer%2C%20integral%20to%20these%20models%2C%20addresses%20the%20unique%0Amorphological%20aspects%20of%20Arabic%20script%2C%20ensuring%20more%20accurate%20text%20processing.%0AEmpirical%20results%20from%20fine-tuning%20the%20models%20on%20tasks%20like%20sentiment%20analysis%0Aand%20summarization%20demonstrate%20significant%20improvements.%20For%20sentiment%20analysis%2C%0Athe%20fine-tuned%20ArabianGPT-0.1B%20model%20achieved%20a%20remarkable%20accuracy%20of%2095%25%2C%20a%0Asubstantial%20increase%20from%20the%20base%20model%27s%2056%25.%20Similarly%2C%20in%20summarization%0Atasks%2C%20fine-tuned%20models%20showed%20enhanced%20F1%20scores%2C%20indicating%20improved%0Aprecision%20and%20recall%20in%20generating%20concise%20summaries.%20Comparative%20analysis%20of%0Afine-tuned%20ArabianGPT%20models%20against%20their%20base%20versions%20across%20various%0Abenchmarks%20reveals%20nuanced%20differences%20in%20performance%2C%20with%20fine-tuning%0Apositively%20impacting%20specific%20tasks%20like%20question%20answering%20and%20summarization.%0AThese%20findings%20underscore%20the%20efficacy%20of%20fine-tuning%20in%20aligning%20ArabianGPT%0Amodels%20more%20closely%20with%20specific%20NLP%20tasks%2C%20highlighting%20the%20potential%20of%0Atailored%20transformer%20architectures%20in%20advancing%20Arabic%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15313v2&entry.124074799=Read"},
{"title": "Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic\n  Update and Transient Iteration Complexity", "author": "Boao Kong and Shuchen Zhu and Songtao Lu and Xinmeng Huang and Kun Yuan", "abstract": "  Stochastic bilevel optimization (SBO) is becoming increasingly essential in\nmachine learning due to its versatility in handling nested structures. To\naddress large-scale SBO, decentralized approaches have emerged as effective\nparadigms in which nodes communicate with immediate neighbors without a central\nserver, thereby improving communication efficiency and enhancing algorithmic\nrobustness. However, current decentralized SBO algorithms face challenges,\nincluding expensive inner-loop updates and unclear understanding of the\ninfluence of network topology, data heterogeneity, and the nested bilevel\nalgorithmic structures. In this paper, we introduce a single-loop decentralized\nSBO (D-SOBA) algorithm and establish its transient iteration complexity, which,\nfor the first time, clarifies the joint influence of network topology and data\nheterogeneity on decentralized bilevel algorithms. D-SOBA achieves the\nstate-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and\ntransient iteration complexity under more relaxed assumptions compared to\nexisting methods. Numerical experiments validate our theoretical findings.\n", "link": "http://arxiv.org/abs/2402.03167v2", "date": "2024-02-26", "relevancy": 1.6846, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4312}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4259}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4124}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Bilevel%20Optimization%20over%20Graphs%3A%20Loopless%20Algorithmic%0A%20%20Update%20and%20Transient%20Iteration%20Complexity&entry.906535625=Boao%20Kong%20and%20Shuchen%20Zhu%20and%20Songtao%20Lu%20and%20Xinmeng%20Huang%20and%20Kun%20Yuan&entry.1292438233=%20%20Stochastic%20bilevel%20optimization%20%28SBO%29%20is%20becoming%20increasingly%20essential%20in%0Amachine%20learning%20due%20to%20its%20versatility%20in%20handling%20nested%20structures.%20To%0Aaddress%20large-scale%20SBO%2C%20decentralized%20approaches%20have%20emerged%20as%20effective%0Aparadigms%20in%20which%20nodes%20communicate%20with%20immediate%20neighbors%20without%20a%20central%0Aserver%2C%20thereby%20improving%20communication%20efficiency%20and%20enhancing%20algorithmic%0Arobustness.%20However%2C%20current%20decentralized%20SBO%20algorithms%20face%20challenges%2C%0Aincluding%20expensive%20inner-loop%20updates%20and%20unclear%20understanding%20of%20the%0Ainfluence%20of%20network%20topology%2C%20data%20heterogeneity%2C%20and%20the%20nested%20bilevel%0Aalgorithmic%20structures.%20In%20this%20paper%2C%20we%20introduce%20a%20single-loop%20decentralized%0ASBO%20%28D-SOBA%29%20algorithm%20and%20establish%20its%20transient%20iteration%20complexity%2C%20which%2C%0Afor%20the%20first%20time%2C%20clarifies%20the%20joint%20influence%20of%20network%20topology%20and%20data%0Aheterogeneity%20on%20decentralized%20bilevel%20algorithms.%20D-SOBA%20achieves%20the%0Astate-of-the-art%20asymptotic%20rate%2C%20asymptotic%20gradient/Hessian%20complexity%2C%20and%0Atransient%20iteration%20complexity%20under%20more%20relaxed%20assumptions%20compared%20to%0Aexisting%20methods.%20Numerical%20experiments%20validate%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03167v2&entry.124074799=Read"},
{"title": "Stochastic Conditional Diffusion Models for Semantic Image Synthesis", "author": "Juyeon Ko and Inho Kong and Hyunwoo J. Kim", "abstract": "  Semantic image synthesis (SIS) is a task to generate realistic images\ncorresponding to semantic maps (labels). It can be applied to diverse\nreal-world practices such as photo editing or content creation. However, in\nreal-world applications, SIS often encounters noisy user inputs. To address\nthis, we propose Stochastic Conditional Diffusion Model (SCDM), which is a\nrobust conditional diffusion model that features novel forward and generation\nprocesses tailored for SIS with noisy labels. It enhances robustness by\nstochastically perturbing the semantic label maps through Label Diffusion,\nwhich diffuses the labels with discrete diffusion. Through the diffusion of\nlabels, the noisy and clean semantic maps become similar as the timestep\nincreases, eventually becoming identical at $t=T$. This facilitates the\ngeneration of an image close to a clean image, enabling robust generation.\nFurthermore, we propose a class-wise noise schedule to differentially diffuse\nthe labels depending on the class. We demonstrate that the proposed method\ngenerates high-quality samples through extensive experiments and analyses on\nbenchmark datasets, including a novel experimental setup simulating human\nerrors during real-world applications.\n", "link": "http://arxiv.org/abs/2402.16506v1", "date": "2024-02-26", "relevancy": 1.626, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5508}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5325}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Conditional%20Diffusion%20Models%20for%20Semantic%20Image%20Synthesis&entry.906535625=Juyeon%20Ko%20and%20Inho%20Kong%20and%20Hyunwoo%20J.%20Kim&entry.1292438233=%20%20Semantic%20image%20synthesis%20%28SIS%29%20is%20a%20task%20to%20generate%20realistic%20images%0Acorresponding%20to%20semantic%20maps%20%28labels%29.%20It%20can%20be%20applied%20to%20diverse%0Areal-world%20practices%20such%20as%20photo%20editing%20or%20content%20creation.%20However%2C%20in%0Areal-world%20applications%2C%20SIS%20often%20encounters%20noisy%20user%20inputs.%20To%20address%0Athis%2C%20we%20propose%20Stochastic%20Conditional%20Diffusion%20Model%20%28SCDM%29%2C%20which%20is%20a%0Arobust%20conditional%20diffusion%20model%20that%20features%20novel%20forward%20and%20generation%0Aprocesses%20tailored%20for%20SIS%20with%20noisy%20labels.%20It%20enhances%20robustness%20by%0Astochastically%20perturbing%20the%20semantic%20label%20maps%20through%20Label%20Diffusion%2C%0Awhich%20diffuses%20the%20labels%20with%20discrete%20diffusion.%20Through%20the%20diffusion%20of%0Alabels%2C%20the%20noisy%20and%20clean%20semantic%20maps%20become%20similar%20as%20the%20timestep%0Aincreases%2C%20eventually%20becoming%20identical%20at%20%24t%3DT%24.%20This%20facilitates%20the%0Ageneration%20of%20an%20image%20close%20to%20a%20clean%20image%2C%20enabling%20robust%20generation.%0AFurthermore%2C%20we%20propose%20a%20class-wise%20noise%20schedule%20to%20differentially%20diffuse%0Athe%20labels%20depending%20on%20the%20class.%20We%20demonstrate%20that%20the%20proposed%20method%0Agenerates%20high-quality%20samples%20through%20extensive%20experiments%20and%20analyses%20on%0Abenchmark%20datasets%2C%20including%20a%20novel%20experimental%20setup%20simulating%20human%0Aerrors%20during%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16506v1&entry.124074799=Read"},
{"title": "Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept\n  Intervention, and Probabilistic Interpretations", "author": "Xinyue Xu and Yi Qin and Lu Mi and Hao Wang and Xiaomeng Li", "abstract": "  Existing methods, such as concept bottleneck models (CBMs), have been\nsuccessful in providing concept-based interpretations for black-box deep\nlearning models. They typically work by predicting concepts given the input and\nthen predicting the final class label given the predicted concepts. However,\n(1) they often fail to capture the high-order, nonlinear interaction between\nconcepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not\nhelp correct highly correlated concepts (e.g., \"yellow belly\"), leading to\nsuboptimal final accuracy; (2) they cannot naturally quantify the complex\nconditional dependencies between different concepts and class labels (e.g., for\nan image with the class label \"Kentucky Warbler\" and a concept \"black bill\",\nwhat is the probability that the model correctly predicts another concept\n\"black crown\"), therefore failing to provide deeper insight into how a\nblack-box model works. In response to these limitations, we propose\nEnergy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural\nnetworks to define the joint energy of candidate (input, concept, class)\ntuples. With such a unified interface, prediction, concept correction, and\nconditional dependency quantification are then represented as conditional\nprobabilities, which are generated by composing different energy functions. Our\nECBMs address both limitations of existing CBMs, providing higher accuracy and\nricher concept interpretations. Empirical results show that our approach\noutperforms the state-of-the-art on real-world datasets.\n", "link": "http://arxiv.org/abs/2401.14142v2", "date": "2024-02-26", "relevancy": 1.5093, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5271}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5045}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4755}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Based%20Concept%20Bottleneck%20Models%3A%20Unifying%20Prediction%2C%20Concept%0A%20%20Intervention%2C%20and%20Probabilistic%20Interpretations&entry.906535625=Xinyue%20Xu%20and%20Yi%20Qin%20and%20Lu%20Mi%20and%20Hao%20Wang%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Existing%20methods%2C%20such%20as%20concept%20bottleneck%20models%20%28CBMs%29%2C%20have%20been%0Asuccessful%20in%20providing%20concept-based%20interpretations%20for%20black-box%20deep%0Alearning%20models.%20They%20typically%20work%20by%20predicting%20concepts%20given%20the%20input%20and%0Athen%20predicting%20the%20final%20class%20label%20given%20the%20predicted%20concepts.%20However%2C%0A%281%29%20they%20often%20fail%20to%20capture%20the%20high-order%2C%20nonlinear%20interaction%20between%0Aconcepts%2C%20e.g.%2C%20correcting%20a%20predicted%20concept%20%28e.g.%2C%20%22yellow%20breast%22%29%20does%20not%0Ahelp%20correct%20highly%20correlated%20concepts%20%28e.g.%2C%20%22yellow%20belly%22%29%2C%20leading%20to%0Asuboptimal%20final%20accuracy%3B%20%282%29%20they%20cannot%20naturally%20quantify%20the%20complex%0Aconditional%20dependencies%20between%20different%20concepts%20and%20class%20labels%20%28e.g.%2C%20for%0Aan%20image%20with%20the%20class%20label%20%22Kentucky%20Warbler%22%20and%20a%20concept%20%22black%20bill%22%2C%0Awhat%20is%20the%20probability%20that%20the%20model%20correctly%20predicts%20another%20concept%0A%22black%20crown%22%29%2C%20therefore%20failing%20to%20provide%20deeper%20insight%20into%20how%20a%0Ablack-box%20model%20works.%20In%20response%20to%20these%20limitations%2C%20we%20propose%0AEnergy-based%20Concept%20Bottleneck%20Models%20%28ECBMs%29.%20Our%20ECBMs%20use%20a%20set%20of%20neural%0Anetworks%20to%20define%20the%20joint%20energy%20of%20candidate%20%28input%2C%20concept%2C%20class%29%0Atuples.%20With%20such%20a%20unified%20interface%2C%20prediction%2C%20concept%20correction%2C%20and%0Aconditional%20dependency%20quantification%20are%20then%20represented%20as%20conditional%0Aprobabilities%2C%20which%20are%20generated%20by%20composing%20different%20energy%20functions.%20Our%0AECBMs%20address%20both%20limitations%20of%20existing%20CBMs%2C%20providing%20higher%20accuracy%20and%0Aricher%20concept%20interpretations.%20Empirical%20results%20show%20that%20our%20approach%0Aoutperforms%20the%20state-of-the-art%20on%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14142v2&entry.124074799=Read"},
{"title": "Enhancing Representation in Medical Vision-Language Foundation Models\n  via Multi-Scale Information Extraction Techniques", "author": "Weijian Huang and Cheng Li and Hong-Yu Zhou and Jiarun Liu and Hao Yang and Yong Liang and Guangming Shi and Hairong Zheng and Shanshan Wang", "abstract": "  The development of medical vision-language foundation models has attracted\nsignificant attention in the field of medicine and healthcare due to their\npromising prospect in various clinical applications. While previous studies\nhave commonly focused on feature learning at a single learning scale,\ninvestigation on integrating multi-scale information is lacking, which may\nhinder the potential for mutual reinforcement among these features. This paper\naims to bridge this gap by proposing a method that effectively exploits\nmulti-scale information to enhance the performance of medical foundation\nmodels. The proposed method simultaneously exploits features at the local,\ninstance, modality and global aspects, facilitating comprehensive\nrepresentation learning within the models. We evaluate the effectiveness of the\nproposed method on six open-source datasets across different clinical tasks,\ndemonstrating its ability to enhance the performance of medical foundation\nmodels.\n", "link": "http://arxiv.org/abs/2401.01583v2", "date": "2024-02-26", "relevancy": 1.5022, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5126}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4986}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4943}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Representation%20in%20Medical%20Vision-Language%20Foundation%20Models%0A%20%20via%20Multi-Scale%20Information%20Extraction%20Techniques&entry.906535625=Weijian%20Huang%20and%20Cheng%20Li%20and%20Hong-Yu%20Zhou%20and%20Jiarun%20Liu%20and%20Hao%20Yang%20and%20Yong%20Liang%20and%20Guangming%20Shi%20and%20Hairong%20Zheng%20and%20Shanshan%20Wang&entry.1292438233=%20%20The%20development%20of%20medical%20vision-language%20foundation%20models%20has%20attracted%0Asignificant%20attention%20in%20the%20field%20of%20medicine%20and%20healthcare%20due%20to%20their%0Apromising%20prospect%20in%20various%20clinical%20applications.%20While%20previous%20studies%0Ahave%20commonly%20focused%20on%20feature%20learning%20at%20a%20single%20learning%20scale%2C%0Ainvestigation%20on%20integrating%20multi-scale%20information%20is%20lacking%2C%20which%20may%0Ahinder%20the%20potential%20for%20mutual%20reinforcement%20among%20these%20features.%20This%20paper%0Aaims%20to%20bridge%20this%20gap%20by%20proposing%20a%20method%20that%20effectively%20exploits%0Amulti-scale%20information%20to%20enhance%20the%20performance%20of%20medical%20foundation%0Amodels.%20The%20proposed%20method%20simultaneously%20exploits%20features%20at%20the%20local%2C%0Ainstance%2C%20modality%20and%20global%20aspects%2C%20facilitating%20comprehensive%0Arepresentation%20learning%20within%20the%20models.%20We%20evaluate%20the%20effectiveness%20of%20the%0Aproposed%20method%20on%20six%20open-source%20datasets%20across%20different%20clinical%20tasks%2C%0Ademonstrating%20its%20ability%20to%20enhance%20the%20performance%20of%20medical%20foundation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01583v2&entry.124074799=Read"},
{"title": "Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey", "author": "Zhuo Chen and Yichi Zhang and Yin Fang and Yuxia Geng and Lingbing Guo and Xiang Chen and Qian Li and Wen Zhang and Jiaoyan Chen and Yushan Zhu and Jiaqi Li and Xiaoze Liu and Jeff Z. Pan and Ningyu Zhang and Huajun Chen", "abstract": "  Knowledge Graphs (KGs) play a pivotal role in advancing various AI\napplications, with the semantic web community's exploration into multi-modal\ndimensions unlocking new avenues for innovation. In this survey, we carefully\nreview over 300 articles, focusing on KG-aware research in two principal\naspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal\ntasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into\nthe MMKG realm. We begin by defining KGs and MMKGs, then explore their\nconstruction progress. Our review includes two primary task categories:\nKG-aware multi-modal learning tasks, such as Image Classification and Visual\nQuestion Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph\nCompletion and Entity Alignment, highlighting specific research trajectories.\nFor most of these tasks, we provide definitions, evaluation benchmarks, and\nadditionally outline essential insights for conducting relevant research.\nFinally, we discuss current challenges and identify emerging trends, such as\nprogress in Large Language Modeling and Multi-modal Pre-training strategies.\nThis survey aims to serve as a comprehensive reference for researchers already\ninvolved in or considering delving into KG and multi-modal learning research,\noffering insights into the evolving landscape of MMKG research and supporting\nfuture work.\n", "link": "http://arxiv.org/abs/2402.05391v4", "date": "2024-02-26", "relevancy": 1.4881, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5437}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4845}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4816}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graphs%20Meet%20Multi-Modal%20Learning%3A%20A%20Comprehensive%20Survey&entry.906535625=Zhuo%20Chen%20and%20Yichi%20Zhang%20and%20Yin%20Fang%20and%20Yuxia%20Geng%20and%20Lingbing%20Guo%20and%20Xiang%20Chen%20and%20Qian%20Li%20and%20Wen%20Zhang%20and%20Jiaoyan%20Chen%20and%20Yushan%20Zhu%20and%20Jiaqi%20Li%20and%20Xiaoze%20Liu%20and%20Jeff%20Z.%20Pan%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen&entry.1292438233=%20%20Knowledge%20Graphs%20%28KGs%29%20play%20a%20pivotal%20role%20in%20advancing%20various%20AI%0Aapplications%2C%20with%20the%20semantic%20web%20community%27s%20exploration%20into%20multi-modal%0Adimensions%20unlocking%20new%20avenues%20for%20innovation.%20In%20this%20survey%2C%20we%20carefully%0Areview%20over%20300%20articles%2C%20focusing%20on%20KG-aware%20research%20in%20two%20principal%0Aaspects%3A%20KG-driven%20Multi-Modal%20%28KG4MM%29%20learning%2C%20where%20KGs%20support%20multi-modal%0Atasks%2C%20and%20Multi-Modal%20Knowledge%20Graph%20%28MM4KG%29%2C%20which%20extends%20KG%20studies%20into%0Athe%20MMKG%20realm.%20We%20begin%20by%20defining%20KGs%20and%20MMKGs%2C%20then%20explore%20their%0Aconstruction%20progress.%20Our%20review%20includes%20two%20primary%20task%20categories%3A%0AKG-aware%20multi-modal%20learning%20tasks%2C%20such%20as%20Image%20Classification%20and%20Visual%0AQuestion%20Answering%2C%20and%20intrinsic%20MMKG%20tasks%20like%20Multi-modal%20Knowledge%20Graph%0ACompletion%20and%20Entity%20Alignment%2C%20highlighting%20specific%20research%20trajectories.%0AFor%20most%20of%20these%20tasks%2C%20we%20provide%20definitions%2C%20evaluation%20benchmarks%2C%20and%0Aadditionally%20outline%20essential%20insights%20for%20conducting%20relevant%20research.%0AFinally%2C%20we%20discuss%20current%20challenges%20and%20identify%20emerging%20trends%2C%20such%20as%0Aprogress%20in%20Large%20Language%20Modeling%20and%20Multi-modal%20Pre-training%20strategies.%0AThis%20survey%20aims%20to%20serve%20as%20a%20comprehensive%20reference%20for%20researchers%20already%0Ainvolved%20in%20or%20considering%20delving%20into%20KG%20and%20multi-modal%20learning%20research%2C%0Aoffering%20insights%20into%20the%20evolving%20landscape%20of%20MMKG%20research%20and%20supporting%0Afuture%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05391v4&entry.124074799=Read"},
{"title": "Learning to Schedule Online Tasks with Bandit Feedback", "author": "Yongxin Xu and Shangshang Wang and Hengquan Guo and Xin Liu and Ziyu Shao", "abstract": "  Online task scheduling serves an integral role for task-intensive\napplications in cloud computing and crowdsourcing. Optimal scheduling can\nenhance system performance, typically measured by the reward-to-cost ratio,\nunder some task arrival distribution. On one hand, both reward and cost are\ndependent on task context (e.g., evaluation metric) and remain black-box in\npractice. These render reward and cost hard to model thus unknown before\ndecision making. On the other hand, task arrival behaviors remain sensitive to\nfactors like unpredictable system fluctuation whereby a prior estimation or the\nconventional assumption of arrival distribution (e.g., Poisson) may fail. This\nimplies another practical yet often neglected challenge, i.e., uncertain task\narrival distribution. Towards effective scheduling under a stationary\nenvironment with various uncertainties, we propose a double-optimistic learning\nbased Robbins-Monro (DOL-RM) algorithm. Specifically, DOL-RM integrates a\nlearning module that incorporates optimistic estimation for reward-to-cost\nratio and a decision module that utilizes the Robbins-Monro method to\nimplicitly learn task arrival distribution while making scheduling decisions.\nTheoretically, DOL-RM achieves convergence gap and no regret learning with a\nsub-linear regret of $O(T^{3/4})$, which is the first result for online task\nscheduling under uncertain task arrival distribution and unknown reward and\ncost. Our numerical results in a synthetic experiment and a real-world\napplication demonstrate the effectiveness of DOL-RM in achieving the best\ncumulative reward-to-cost ratio compared with other state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2402.16463v1", "date": "2024-02-26", "relevancy": 1.4592, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.484}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4784}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Schedule%20Online%20Tasks%20with%20Bandit%20Feedback&entry.906535625=Yongxin%20Xu%20and%20Shangshang%20Wang%20and%20Hengquan%20Guo%20and%20Xin%20Liu%20and%20Ziyu%20Shao&entry.1292438233=%20%20Online%20task%20scheduling%20serves%20an%20integral%20role%20for%20task-intensive%0Aapplications%20in%20cloud%20computing%20and%20crowdsourcing.%20Optimal%20scheduling%20can%0Aenhance%20system%20performance%2C%20typically%20measured%20by%20the%20reward-to-cost%20ratio%2C%0Aunder%20some%20task%20arrival%20distribution.%20On%20one%20hand%2C%20both%20reward%20and%20cost%20are%0Adependent%20on%20task%20context%20%28e.g.%2C%20evaluation%20metric%29%20and%20remain%20black-box%20in%0Apractice.%20These%20render%20reward%20and%20cost%20hard%20to%20model%20thus%20unknown%20before%0Adecision%20making.%20On%20the%20other%20hand%2C%20task%20arrival%20behaviors%20remain%20sensitive%20to%0Afactors%20like%20unpredictable%20system%20fluctuation%20whereby%20a%20prior%20estimation%20or%20the%0Aconventional%20assumption%20of%20arrival%20distribution%20%28e.g.%2C%20Poisson%29%20may%20fail.%20This%0Aimplies%20another%20practical%20yet%20often%20neglected%20challenge%2C%20i.e.%2C%20uncertain%20task%0Aarrival%20distribution.%20Towards%20effective%20scheduling%20under%20a%20stationary%0Aenvironment%20with%20various%20uncertainties%2C%20we%20propose%20a%20double-optimistic%20learning%0Abased%20Robbins-Monro%20%28DOL-RM%29%20algorithm.%20Specifically%2C%20DOL-RM%20integrates%20a%0Alearning%20module%20that%20incorporates%20optimistic%20estimation%20for%20reward-to-cost%0Aratio%20and%20a%20decision%20module%20that%20utilizes%20the%20Robbins-Monro%20method%20to%0Aimplicitly%20learn%20task%20arrival%20distribution%20while%20making%20scheduling%20decisions.%0ATheoretically%2C%20DOL-RM%20achieves%20convergence%20gap%20and%20no%20regret%20learning%20with%20a%0Asub-linear%20regret%20of%20%24O%28T%5E%7B3/4%7D%29%24%2C%20which%20is%20the%20first%20result%20for%20online%20task%0Ascheduling%20under%20uncertain%20task%20arrival%20distribution%20and%20unknown%20reward%20and%0Acost.%20Our%20numerical%20results%20in%20a%20synthetic%20experiment%20and%20a%20real-world%0Aapplication%20demonstrate%20the%20effectiveness%20of%20DOL-RM%20in%20achieving%20the%20best%0Acumulative%20reward-to-cost%20ratio%20compared%20with%20other%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16463v1&entry.124074799=Read"},
{"title": "On Languaging a Simulation Engine", "author": "Han Liu and Liantang Li", "abstract": "  Language model intelligence is revolutionizing the way we program materials\nsimulations. However, the diversity of simulation scenarios renders it\nchallenging to precisely transform human language into a tailored simulator.\nHere, using three functionalized types of language model, we propose a\nlanguage-to-simulation (Lang2Sim) framework that enables interactive navigation\non languaging a simulation engine, by taking a scenario instance of water\nsorption in porous matrices. Unlike line-by-line coding of a target simulator,\nthe language models interpret each simulator as an assembly of invariant tool\nfunction and its variant input-output pair. Lang2Sim enables the precise\ntransform of textual description by functionalizing and sequentializing the\nlanguage models of, respectively, rationalizing the tool categorization,\ncustomizing its input-output combinations, and distilling the simulator input\ninto executable format. Importantly, depending on its functionalized type, each\nlanguage model features a distinct processing of chat history to best balance\nits memory limit and information completeness, thus leveraging the model\nintelligence to unstructured nature of human request. Overall, this work\nestablishes language model as an intelligent platform to unlock the era of\nlanguaging a simulation engine.\n", "link": "http://arxiv.org/abs/2402.16482v1", "date": "2024-02-26", "relevancy": 1.4305, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4799}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.479}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4669}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Languaging%20a%20Simulation%20Engine&entry.906535625=Han%20Liu%20and%20Liantang%20Li&entry.1292438233=%20%20Language%20model%20intelligence%20is%20revolutionizing%20the%20way%20we%20program%20materials%0Asimulations.%20However%2C%20the%20diversity%20of%20simulation%20scenarios%20renders%20it%0Achallenging%20to%20precisely%20transform%20human%20language%20into%20a%20tailored%20simulator.%0AHere%2C%20using%20three%20functionalized%20types%20of%20language%20model%2C%20we%20propose%20a%0Alanguage-to-simulation%20%28Lang2Sim%29%20framework%20that%20enables%20interactive%20navigation%0Aon%20languaging%20a%20simulation%20engine%2C%20by%20taking%20a%20scenario%20instance%20of%20water%0Asorption%20in%20porous%20matrices.%20Unlike%20line-by-line%20coding%20of%20a%20target%20simulator%2C%0Athe%20language%20models%20interpret%20each%20simulator%20as%20an%20assembly%20of%20invariant%20tool%0Afunction%20and%20its%20variant%20input-output%20pair.%20Lang2Sim%20enables%20the%20precise%0Atransform%20of%20textual%20description%20by%20functionalizing%20and%20sequentializing%20the%0Alanguage%20models%20of%2C%20respectively%2C%20rationalizing%20the%20tool%20categorization%2C%0Acustomizing%20its%20input-output%20combinations%2C%20and%20distilling%20the%20simulator%20input%0Ainto%20executable%20format.%20Importantly%2C%20depending%20on%20its%20functionalized%20type%2C%20each%0Alanguage%20model%20features%20a%20distinct%20processing%20of%20chat%20history%20to%20best%20balance%0Aits%20memory%20limit%20and%20information%20completeness%2C%20thus%20leveraging%20the%20model%0Aintelligence%20to%20unstructured%20nature%20of%20human%20request.%20Overall%2C%20this%20work%0Aestablishes%20language%20model%20as%20an%20intelligent%20platform%20to%20unlock%20the%20era%20of%0Alanguaging%20a%20simulation%20engine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16482v1&entry.124074799=Read"},
{"title": "Evaluating the Significance of Outdoor Advertising from Driver's\n  Perspective Using Computer Vision", "author": "Zuzana \u010cernekov\u00e1 and Zuzana Berger Haladov\u00e1 and J\u00e1n \u0160pirka and Viktor Kocur", "abstract": "  Outdoor advertising, such as roadside billboards, plays a significant role in\nmarketing campaigns but can also be a distraction for drivers, potentially\nleading to accidents. In this study, we propose a pipeline for evaluating the\nsignificance of roadside billboards in videos captured from a driver's\nperspective. We have collected and annotated a new BillboardLamac dataset,\ncomprising eight videos captured by drivers driving through a predefined path\nwearing eye-tracking devices. The dataset includes annotations of billboards,\nincluding 154 unique IDs and 155 thousand bounding boxes, as well as eye\nfixation data. We evaluate various object tracking methods in combination with\na YOLOv8 detector to identify billboard advertisements with the best approach\nachieving 38.5 HOTA on BillboardLamac. Additionally, we train a random forest\nclassifier to classify billboards into three classes based on the length of\ndriver fixations achieving 75.8% test accuracy. An analysis of the trained\nclassifier reveals that the duration of billboard visibility, its saliency, and\nsize are the most influential features when assessing billboard significance.\n", "link": "http://arxiv.org/abs/2311.07390v2", "date": "2024-02-26", "relevancy": 1.429, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4968}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4531}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4482}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Significance%20of%20Outdoor%20Advertising%20from%20Driver%27s%0A%20%20Perspective%20Using%20Computer%20Vision&entry.906535625=Zuzana%20%C4%8Cernekov%C3%A1%20and%20Zuzana%20Berger%20Haladov%C3%A1%20and%20J%C3%A1n%20%C5%A0pirka%20and%20Viktor%20Kocur&entry.1292438233=%20%20Outdoor%20advertising%2C%20such%20as%20roadside%20billboards%2C%20plays%20a%20significant%20role%20in%0Amarketing%20campaigns%20but%20can%20also%20be%20a%20distraction%20for%20drivers%2C%20potentially%0Aleading%20to%20accidents.%20In%20this%20study%2C%20we%20propose%20a%20pipeline%20for%20evaluating%20the%0Asignificance%20of%20roadside%20billboards%20in%20videos%20captured%20from%20a%20driver%27s%0Aperspective.%20We%20have%20collected%20and%20annotated%20a%20new%20BillboardLamac%20dataset%2C%0Acomprising%20eight%20videos%20captured%20by%20drivers%20driving%20through%20a%20predefined%20path%0Awearing%20eye-tracking%20devices.%20The%20dataset%20includes%20annotations%20of%20billboards%2C%0Aincluding%20154%20unique%20IDs%20and%20155%20thousand%20bounding%20boxes%2C%20as%20well%20as%20eye%0Afixation%20data.%20We%20evaluate%20various%20object%20tracking%20methods%20in%20combination%20with%0Aa%20YOLOv8%20detector%20to%20identify%20billboard%20advertisements%20with%20the%20best%20approach%0Aachieving%2038.5%20HOTA%20on%20BillboardLamac.%20Additionally%2C%20we%20train%20a%20random%20forest%0Aclassifier%20to%20classify%20billboards%20into%20three%20classes%20based%20on%20the%20length%20of%0Adriver%20fixations%20achieving%2075.8%25%20test%20accuracy.%20An%20analysis%20of%20the%20trained%0Aclassifier%20reveals%20that%20the%20duration%20of%20billboard%20visibility%2C%20its%20saliency%2C%20and%0Asize%20are%20the%20most%20influential%20features%20when%20assessing%20billboard%20significance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07390v2&entry.124074799=Read"},
{"title": "Memory GAPS: Would LLM pass the Tulving Test?", "author": "Jean-Marie Chauvet", "abstract": "  The Tulving Test was designed to investigate memory performance in\nrecognition and recall tasks. Its results help assess the relevance of the\n\"Synergistic Ecphory Model\" of memory and similar RK paradigms in human\nperformance. This paper starts investigating whether the more than\nforty-year-old framework sheds some light on LLMs' acts of remembering.\n", "link": "http://arxiv.org/abs/2402.16505v1", "date": "2024-02-26", "relevancy": 1.4118, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3694}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3627}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3326}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20GAPS%3A%20Would%20LLM%20pass%20the%20Tulving%20Test%3F&entry.906535625=Jean-Marie%20Chauvet&entry.1292438233=%20%20The%20Tulving%20Test%20was%20designed%20to%20investigate%20memory%20performance%20in%0Arecognition%20and%20recall%20tasks.%20Its%20results%20help%20assess%20the%20relevance%20of%20the%0A%22Synergistic%20Ecphory%20Model%22%20of%20memory%20and%20similar%20RK%20paradigms%20in%20human%0Aperformance.%20This%20paper%20starts%20investigating%20whether%20the%20more%20than%0Aforty-year-old%20framework%20sheds%20some%20light%20on%20LLMs%27%20acts%20of%20remembering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16505v1&entry.124074799=Read"},
{"title": "On the Tip of the Tongue: Analyzing Conceptual Representation in Large\n  Language Models with Reverse-Dictionary Probe", "author": "Ningyu Xu and Qi Zhang and Menghan Zhang and Peng Qian and Xuanjing Huang", "abstract": "  Probing and enhancing large language models' reasoning capacity remains a\ncrucial open question. Here we re-purpose the reverse dictionary task as a case\nstudy to probe LLMs' capacity for conceptual inference. We use in-context\nlearning to guide the models to generate the term for an object concept implied\nin a linguistic description. Models robustly achieve high accuracy in this\ntask, and their representation space encodes information about object\ncategories and fine-grained features. Further experiments suggest that the\nconceptual inference ability as probed by the reverse-dictionary task predicts\nmodel's general reasoning performance across multiple benchmarks, despite\nsimilar syntactic generalization behaviors across models. Explorative analyses\nsuggest that prompting LLMs with description$\\Rightarrow$word examples may\ninduce generalization beyond surface-level differences in task construals and\nfacilitate models on broader commonsense reasoning problems.\n", "link": "http://arxiv.org/abs/2402.14404v2", "date": "2024-02-26", "relevancy": 1.3862, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4756}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4655}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4553}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Tip%20of%20the%20Tongue%3A%20Analyzing%20Conceptual%20Representation%20in%20Large%0A%20%20Language%20Models%20with%20Reverse-Dictionary%20Probe&entry.906535625=Ningyu%20Xu%20and%20Qi%20Zhang%20and%20Menghan%20Zhang%20and%20Peng%20Qian%20and%20Xuanjing%20Huang&entry.1292438233=%20%20Probing%20and%20enhancing%20large%20language%20models%27%20reasoning%20capacity%20remains%20a%0Acrucial%20open%20question.%20Here%20we%20re-purpose%20the%20reverse%20dictionary%20task%20as%20a%20case%0Astudy%20to%20probe%20LLMs%27%20capacity%20for%20conceptual%20inference.%20We%20use%20in-context%0Alearning%20to%20guide%20the%20models%20to%20generate%20the%20term%20for%20an%20object%20concept%20implied%0Ain%20a%20linguistic%20description.%20Models%20robustly%20achieve%20high%20accuracy%20in%20this%0Atask%2C%20and%20their%20representation%20space%20encodes%20information%20about%20object%0Acategories%20and%20fine-grained%20features.%20Further%20experiments%20suggest%20that%20the%0Aconceptual%20inference%20ability%20as%20probed%20by%20the%20reverse-dictionary%20task%20predicts%0Amodel%27s%20general%20reasoning%20performance%20across%20multiple%20benchmarks%2C%20despite%0Asimilar%20syntactic%20generalization%20behaviors%20across%20models.%20Explorative%20analyses%0Asuggest%20that%20prompting%20LLMs%20with%20description%24%5CRightarrow%24word%20examples%20may%0Ainduce%20generalization%20beyond%20surface-level%20differences%20in%20task%20construals%20and%0Afacilitate%20models%20on%20broader%20commonsense%20reasoning%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14404v2&entry.124074799=Read"},
{"title": "mEdIT: Multilingual Text Editing via Instruction Tuning", "author": "Vipul Raheja and Dimitris Alikaniotis and Vivek Kulkarni and Bashar Alhafni and Dhruv Kumar", "abstract": "  We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent\nstate-of-the-art text editing models for writing assistance. mEdIT models are\ntrained by fine-tuning multi-lingual large, pre-trained language models (LLMs)\nvia instruction tuning. They are designed to take instructions from the user\nspecifying the attributes of the desired text in the form of natural language\ninstructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\\'on\n(Spanish). We build mEdIT by curating data from multiple publicly available\nhuman-annotated text editing datasets for three text editing tasks (Grammatical\nError Correction (GEC), Text Simplification, and Paraphrasing) across diverse\nlanguages belonging to six different language families. We detail the design\nand training of mEdIT models and demonstrate their strong performance on many\nmulti-lingual text editing benchmarks against other multilingual LLMs. We also\nfind that mEdIT generalizes effectively to new languages over multilingual\nbaselines. We publicly release our data, code, and trained models at\nhttps://github.com/vipulraheja/medit.\n", "link": "http://arxiv.org/abs/2402.16472v1", "date": "2024-02-26", "relevancy": 1.3766, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5335}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4493}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4328}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mEdIT%3A%20Multilingual%20Text%20Editing%20via%20Instruction%20Tuning&entry.906535625=Vipul%20Raheja%20and%20Dimitris%20Alikaniotis%20and%20Vivek%20Kulkarni%20and%20Bashar%20Alhafni%20and%20Dhruv%20Kumar&entry.1292438233=%20%20We%20introduce%20mEdIT%2C%20a%20multi-lingual%20extension%20to%20CoEdIT%20--%20the%20recent%0Astate-of-the-art%20text%20editing%20models%20for%20writing%20assistance.%20mEdIT%20models%20are%0Atrained%20by%20fine-tuning%20multi-lingual%20large%2C%20pre-trained%20language%20models%20%28LLMs%29%0Avia%20instruction%20tuning.%20They%20are%20designed%20to%20take%20instructions%20from%20the%20user%0Aspecifying%20the%20attributes%20of%20the%20desired%20text%20in%20the%20form%20of%20natural%20language%0Ainstructions%2C%20such%20as%20Grammatik%20korrigieren%20%28German%29%20or%20Parafrasee%20la%20oraci%5C%27on%0A%28Spanish%29.%20We%20build%20mEdIT%20by%20curating%20data%20from%20multiple%20publicly%20available%0Ahuman-annotated%20text%20editing%20datasets%20for%20three%20text%20editing%20tasks%20%28Grammatical%0AError%20Correction%20%28GEC%29%2C%20Text%20Simplification%2C%20and%20Paraphrasing%29%20across%20diverse%0Alanguages%20belonging%20to%20six%20different%20language%20families.%20We%20detail%20the%20design%0Aand%20training%20of%20mEdIT%20models%20and%20demonstrate%20their%20strong%20performance%20on%20many%0Amulti-lingual%20text%20editing%20benchmarks%20against%20other%20multilingual%20LLMs.%20We%20also%0Afind%20that%20mEdIT%20generalizes%20effectively%20to%20new%20languages%20over%20multilingual%0Abaselines.%20We%20publicly%20release%20our%20data%2C%20code%2C%20and%20trained%20models%20at%0Ahttps%3A//github.com/vipulraheja/medit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16472v1&entry.124074799=Read"},
{"title": "Defending LLMs against Jailbreaking Attacks via Backtranslation", "author": "Yihan Wang and Zhouxing Shi and Andrew Bai and Cho-Jui Hsieh", "abstract": "  Although many large language models (LLMs) have been trained to refuse\nharmful requests, they are still vulnerable to jailbreaking attacks, which\nrewrite the original prompt to conceal its harmful intent. In this paper, we\npropose a new method for defending LLMs against jailbreaking attacks by\n``backtranslation''. Specifically, given an initial response generated by the\ntarget LLM from an input prompt, our backtranslation prompts a language model\nto infer an input prompt that can lead to the response. The inferred prompt is\ncalled the backtranslated prompt which tends to reveal the actual intent of the\noriginal prompt, since it is generated based on the LLM's response and is not\ndirectly manipulated by the attacker. We then run the target LLM again on the\nbacktranslated prompt, and we refuse the original prompt if the model refuses\nthe backtranslated prompt. We explain that the proposed defense provides\nseveral benefits on its effectiveness and efficiency. We empirically\ndemonstrate that our defense significantly outperforms the baselines, in the\ncases that are hard for the baselines, and our defense also has little impact\non the generation quality for benign input prompts.\n", "link": "http://arxiv.org/abs/2402.16459v1", "date": "2024-02-26", "relevancy": 1.2365, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4143}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4123}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4067}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defending%20LLMs%20against%20Jailbreaking%20Attacks%20via%20Backtranslation&entry.906535625=Yihan%20Wang%20and%20Zhouxing%20Shi%20and%20Andrew%20Bai%20and%20Cho-Jui%20Hsieh&entry.1292438233=%20%20Although%20many%20large%20language%20models%20%28LLMs%29%20have%20been%20trained%20to%20refuse%0Aharmful%20requests%2C%20they%20are%20still%20vulnerable%20to%20jailbreaking%20attacks%2C%20which%0Arewrite%20the%20original%20prompt%20to%20conceal%20its%20harmful%20intent.%20In%20this%20paper%2C%20we%0Apropose%20a%20new%20method%20for%20defending%20LLMs%20against%20jailbreaking%20attacks%20by%0A%60%60backtranslation%27%27.%20Specifically%2C%20given%20an%20initial%20response%20generated%20by%20the%0Atarget%20LLM%20from%20an%20input%20prompt%2C%20our%20backtranslation%20prompts%20a%20language%20model%0Ato%20infer%20an%20input%20prompt%20that%20can%20lead%20to%20the%20response.%20The%20inferred%20prompt%20is%0Acalled%20the%20backtranslated%20prompt%20which%20tends%20to%20reveal%20the%20actual%20intent%20of%20the%0Aoriginal%20prompt%2C%20since%20it%20is%20generated%20based%20on%20the%20LLM%27s%20response%20and%20is%20not%0Adirectly%20manipulated%20by%20the%20attacker.%20We%20then%20run%20the%20target%20LLM%20again%20on%20the%0Abacktranslated%20prompt%2C%20and%20we%20refuse%20the%20original%20prompt%20if%20the%20model%20refuses%0Athe%20backtranslated%20prompt.%20We%20explain%20that%20the%20proposed%20defense%20provides%0Aseveral%20benefits%20on%20its%20effectiveness%20and%20efficiency.%20We%20empirically%0Ademonstrate%20that%20our%20defense%20significantly%20outperforms%20the%20baselines%2C%20in%20the%0Acases%20that%20are%20hard%20for%20the%20baselines%2C%20and%20our%20defense%20also%20has%20little%20impact%0Aon%20the%20generation%20quality%20for%20benign%20input%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16459v1&entry.124074799=Read"},
{"title": "Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on\n  Computational Fluid Dynamics Problems", "author": "Jakub Kudela and Ladislav Dobrovsky", "abstract": "  Surrogate-assisted evolutionary algorithms (SAEAs) are recently among the\nmost widely studied methods for their capability to solve expensive real-world\noptimization problems. However, the development of new methods and benchmarking\nwith other techniques still relies almost exclusively on artificially created\nproblems. In this paper, we use two real-world computational fluid dynamics\nproblems to compare the performance of eleven state-of-the-art single-objective\nSAEAs. We analyze the performance by investigating the quality and robustness\nof the obtained solutions and the convergence properties of the selected\nmethods. Our findings suggest that the more recently published methods, as well\nas the techniques that utilize differential evolution as one of their\noptimization mechanisms, perform significantly better than the other considered\nmethods.\n", "link": "http://arxiv.org/abs/2402.16455v1", "date": "2024-02-26", "relevancy": 1.1499, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3889}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.382}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3809}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20Comparison%20of%20Surrogate-Assisted%20Evolutionary%20Algorithms%20on%0A%20%20Computational%20Fluid%20Dynamics%20Problems&entry.906535625=Jakub%20Kudela%20and%20Ladislav%20Dobrovsky&entry.1292438233=%20%20Surrogate-assisted%20evolutionary%20algorithms%20%28SAEAs%29%20are%20recently%20among%20the%0Amost%20widely%20studied%20methods%20for%20their%20capability%20to%20solve%20expensive%20real-world%0Aoptimization%20problems.%20However%2C%20the%20development%20of%20new%20methods%20and%20benchmarking%0Awith%20other%20techniques%20still%20relies%20almost%20exclusively%20on%20artificially%20created%0Aproblems.%20In%20this%20paper%2C%20we%20use%20two%20real-world%20computational%20fluid%20dynamics%0Aproblems%20to%20compare%20the%20performance%20of%20eleven%20state-of-the-art%20single-objective%0ASAEAs.%20We%20analyze%20the%20performance%20by%20investigating%20the%20quality%20and%20robustness%0Aof%20the%20obtained%20solutions%20and%20the%20convergence%20properties%20of%20the%20selected%0Amethods.%20Our%20findings%20suggest%20that%20the%20more%20recently%20published%20methods%2C%20as%20well%0Aas%20the%20techniques%20that%20utilize%20differential%20evolution%20as%20one%20of%20their%0Aoptimization%20mechanisms%2C%20perform%20significantly%20better%20than%20the%20other%20considered%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16455v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


