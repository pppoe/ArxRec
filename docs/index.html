<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240626.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GaussianDreamerPro: Text to Manipulable 3D Gaussians with Highly\n  Enhanced Quality", "author": "Taoran Yi and Jiemin Fang and Zanwei Zhou and Junjie Wang and Guanjun Wu and Lingxi Xie and Xiaopeng Zhang and Wenyu Liu and Xinggang Wang and Qi Tian", "abstract": "  Recently, 3D Gaussian splatting (3D-GS) has achieved great success in\nreconstructing and rendering real-world scenes. To transfer the high rendering\nquality to generation tasks, a series of research works attempt to generate\n3D-Gaussian assets from text. However, the generated assets have not achieved\nthe same quality as those in reconstruction tasks. We observe that Gaussians\ntend to grow without control as the generation process may cause indeterminacy.\nAiming at highly enhancing the generation quality, we propose a novel framework\nnamed GaussianDreamerPro. The main idea is to bind Gaussians to reasonable\ngeometry, which evolves over the whole generation process. Along different\nstages of our framework, both the geometry and appearance can be enriched\nprogressively. The final output asset is constructed with 3D Gaussians bound to\nmesh, which shows significantly enhanced details and quality compared with\nprevious methods. Notably, the generated asset can also be seamlessly\nintegrated into downstream manipulation pipelines, e.g. animation, composition,\nand simulation etc., greatly promoting its potential in wide applications.\nDemos are available at https://taoranyi.com/gaussiandreamerpro/.\n", "link": "http://arxiv.org/abs/2406.18462v1", "date": "2024-06-26", "relevancy": 3.2991, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.702}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6649}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianDreamerPro%3A%20Text%20to%20Manipulable%203D%20Gaussians%20with%20Highly%0A%20%20Enhanced%20Quality&body=Title%3A%20GaussianDreamerPro%3A%20Text%20to%20Manipulable%203D%20Gaussians%20with%20Highly%0A%20%20Enhanced%20Quality%0AAuthor%3A%20Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Zanwei%20Zhou%20and%20Junjie%20Wang%20and%20Guanjun%20Wu%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20splatting%20%283D-GS%29%20has%20achieved%20great%20success%20in%0Areconstructing%20and%20rendering%20real-world%20scenes.%20To%20transfer%20the%20high%20rendering%0Aquality%20to%20generation%20tasks%2C%20a%20series%20of%20research%20works%20attempt%20to%20generate%0A3D-Gaussian%20assets%20from%20text.%20However%2C%20the%20generated%20assets%20have%20not%20achieved%0Athe%20same%20quality%20as%20those%20in%20reconstruction%20tasks.%20We%20observe%20that%20Gaussians%0Atend%20to%20grow%20without%20control%20as%20the%20generation%20process%20may%20cause%20indeterminacy.%0AAiming%20at%20highly%20enhancing%20the%20generation%20quality%2C%20we%20propose%20a%20novel%20framework%0Anamed%20GaussianDreamerPro.%20The%20main%20idea%20is%20to%20bind%20Gaussians%20to%20reasonable%0Ageometry%2C%20which%20evolves%20over%20the%20whole%20generation%20process.%20Along%20different%0Astages%20of%20our%20framework%2C%20both%20the%20geometry%20and%20appearance%20can%20be%20enriched%0Aprogressively.%20The%20final%20output%20asset%20is%20constructed%20with%203D%20Gaussians%20bound%20to%0Amesh%2C%20which%20shows%20significantly%20enhanced%20details%20and%20quality%20compared%20with%0Aprevious%20methods.%20Notably%2C%20the%20generated%20asset%20can%20also%20be%20seamlessly%0Aintegrated%20into%20downstream%20manipulation%20pipelines%2C%20e.g.%20animation%2C%20composition%2C%0Aand%20simulation%20etc.%2C%20greatly%20promoting%20its%20potential%20in%20wide%20applications.%0ADemos%20are%20available%20at%20https%3A//taoranyi.com/gaussiandreamerpro/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianDreamerPro%253A%2520Text%2520to%2520Manipulable%25203D%2520Gaussians%2520with%2520Highly%250A%2520%2520Enhanced%2520Quality%26entry.906535625%3DTaoran%2520Yi%2520and%2520Jiemin%2520Fang%2520and%2520Zanwei%2520Zhou%2520and%2520Junjie%2520Wang%2520and%2520Guanjun%2520Wu%2520and%2520Lingxi%2520Xie%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520splatting%2520%25283D-GS%2529%2520has%2520achieved%2520great%2520success%2520in%250Areconstructing%2520and%2520rendering%2520real-world%2520scenes.%2520To%2520transfer%2520the%2520high%2520rendering%250Aquality%2520to%2520generation%2520tasks%252C%2520a%2520series%2520of%2520research%2520works%2520attempt%2520to%2520generate%250A3D-Gaussian%2520assets%2520from%2520text.%2520However%252C%2520the%2520generated%2520assets%2520have%2520not%2520achieved%250Athe%2520same%2520quality%2520as%2520those%2520in%2520reconstruction%2520tasks.%2520We%2520observe%2520that%2520Gaussians%250Atend%2520to%2520grow%2520without%2520control%2520as%2520the%2520generation%2520process%2520may%2520cause%2520indeterminacy.%250AAiming%2520at%2520highly%2520enhancing%2520the%2520generation%2520quality%252C%2520we%2520propose%2520a%2520novel%2520framework%250Anamed%2520GaussianDreamerPro.%2520The%2520main%2520idea%2520is%2520to%2520bind%2520Gaussians%2520to%2520reasonable%250Ageometry%252C%2520which%2520evolves%2520over%2520the%2520whole%2520generation%2520process.%2520Along%2520different%250Astages%2520of%2520our%2520framework%252C%2520both%2520the%2520geometry%2520and%2520appearance%2520can%2520be%2520enriched%250Aprogressively.%2520The%2520final%2520output%2520asset%2520is%2520constructed%2520with%25203D%2520Gaussians%2520bound%2520to%250Amesh%252C%2520which%2520shows%2520significantly%2520enhanced%2520details%2520and%2520quality%2520compared%2520with%250Aprevious%2520methods.%2520Notably%252C%2520the%2520generated%2520asset%2520can%2520also%2520be%2520seamlessly%250Aintegrated%2520into%2520downstream%2520manipulation%2520pipelines%252C%2520e.g.%2520animation%252C%2520composition%252C%250Aand%2520simulation%2520etc.%252C%2520greatly%2520promoting%2520its%2520potential%2520in%2520wide%2520applications.%250ADemos%2520are%2520available%2520at%2520https%253A//taoranyi.com/gaussiandreamerpro/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianDreamerPro%3A%20Text%20to%20Manipulable%203D%20Gaussians%20with%20Highly%0A%20%20Enhanced%20Quality&entry.906535625=Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Zanwei%20Zhou%20and%20Junjie%20Wang%20and%20Guanjun%20Wu%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Qi%20Tian&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20splatting%20%283D-GS%29%20has%20achieved%20great%20success%20in%0Areconstructing%20and%20rendering%20real-world%20scenes.%20To%20transfer%20the%20high%20rendering%0Aquality%20to%20generation%20tasks%2C%20a%20series%20of%20research%20works%20attempt%20to%20generate%0A3D-Gaussian%20assets%20from%20text.%20However%2C%20the%20generated%20assets%20have%20not%20achieved%0Athe%20same%20quality%20as%20those%20in%20reconstruction%20tasks.%20We%20observe%20that%20Gaussians%0Atend%20to%20grow%20without%20control%20as%20the%20generation%20process%20may%20cause%20indeterminacy.%0AAiming%20at%20highly%20enhancing%20the%20generation%20quality%2C%20we%20propose%20a%20novel%20framework%0Anamed%20GaussianDreamerPro.%20The%20main%20idea%20is%20to%20bind%20Gaussians%20to%20reasonable%0Ageometry%2C%20which%20evolves%20over%20the%20whole%20generation%20process.%20Along%20different%0Astages%20of%20our%20framework%2C%20both%20the%20geometry%20and%20appearance%20can%20be%20enriched%0Aprogressively.%20The%20final%20output%20asset%20is%20constructed%20with%203D%20Gaussians%20bound%20to%0Amesh%2C%20which%20shows%20significantly%20enhanced%20details%20and%20quality%20compared%20with%0Aprevious%20methods.%20Notably%2C%20the%20generated%20asset%20can%20also%20be%20seamlessly%0Aintegrated%20into%20downstream%20manipulation%20pipelines%2C%20e.g.%20animation%2C%20composition%2C%0Aand%20simulation%20etc.%2C%20greatly%20promoting%20its%20potential%20in%20wide%20applications.%0ADemos%20are%20available%20at%20https%3A//taoranyi.com/gaussiandreamerpro/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18462v1&entry.124074799=Read"},
{"title": "Repeat and Concatenate: 2D to 3D Image Translation with 3D to 3D\n  Generative Modeling", "author": "Abril Corona-Figueroa and Hubert P. H. Shum and Chris G. Willcocks", "abstract": "  This paper investigates a 2D to 3D image translation method with a\nstraightforward technique, enabling correlated 2D X-ray to 3D CT-like\nreconstruction. We observe that existing approaches, which integrate\ninformation across multiple 2D views in the latent space, lose valuable signal\ninformation during latent encoding. Instead, we simply repeat and concatenate\nthe 2D views into higher-channel 3D volumes and approach the 3D reconstruction\nchallenge as a straightforward 3D to 3D generative modeling problem,\nsidestepping several complex modeling issues. This method enables the\nreconstructed 3D volume to retain valuable information from the 2D inputs,\nwhich are passed between channel states in a Swin UNETR backbone. Our approach\napplies neural optimal transport, which is fast and stable to train,\neffectively integrating signal information across multiple views without the\nrequirement for precise alignment; it produces non-collapsed reconstructions\nthat are highly faithful to the 2D views, even after limited training. We\ndemonstrate correlated results, both qualitatively and quantitatively, having\ntrained our model on a single dataset and evaluated its generalization ability\nacross six datasets, including out-of-distribution samples.\n", "link": "http://arxiv.org/abs/2406.18422v1", "date": "2024-06-26", "relevancy": 3.1251, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6375}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6375}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Repeat%20and%20Concatenate%3A%202D%20to%203D%20Image%20Translation%20with%203D%20to%203D%0A%20%20Generative%20Modeling&body=Title%3A%20Repeat%20and%20Concatenate%3A%202D%20to%203D%20Image%20Translation%20with%203D%20to%203D%0A%20%20Generative%20Modeling%0AAuthor%3A%20Abril%20Corona-Figueroa%20and%20Hubert%20P.%20H.%20Shum%20and%20Chris%20G.%20Willcocks%0AAbstract%3A%20%20%20This%20paper%20investigates%20a%202D%20to%203D%20image%20translation%20method%20with%20a%0Astraightforward%20technique%2C%20enabling%20correlated%202D%20X-ray%20to%203D%20CT-like%0Areconstruction.%20We%20observe%20that%20existing%20approaches%2C%20which%20integrate%0Ainformation%20across%20multiple%202D%20views%20in%20the%20latent%20space%2C%20lose%20valuable%20signal%0Ainformation%20during%20latent%20encoding.%20Instead%2C%20we%20simply%20repeat%20and%20concatenate%0Athe%202D%20views%20into%20higher-channel%203D%20volumes%20and%20approach%20the%203D%20reconstruction%0Achallenge%20as%20a%20straightforward%203D%20to%203D%20generative%20modeling%20problem%2C%0Asidestepping%20several%20complex%20modeling%20issues.%20This%20method%20enables%20the%0Areconstructed%203D%20volume%20to%20retain%20valuable%20information%20from%20the%202D%20inputs%2C%0Awhich%20are%20passed%20between%20channel%20states%20in%20a%20Swin%20UNETR%20backbone.%20Our%20approach%0Aapplies%20neural%20optimal%20transport%2C%20which%20is%20fast%20and%20stable%20to%20train%2C%0Aeffectively%20integrating%20signal%20information%20across%20multiple%20views%20without%20the%0Arequirement%20for%20precise%20alignment%3B%20it%20produces%20non-collapsed%20reconstructions%0Athat%20are%20highly%20faithful%20to%20the%202D%20views%2C%20even%20after%20limited%20training.%20We%0Ademonstrate%20correlated%20results%2C%20both%20qualitatively%20and%20quantitatively%2C%20having%0Atrained%20our%20model%20on%20a%20single%20dataset%20and%20evaluated%20its%20generalization%20ability%0Aacross%20six%20datasets%2C%20including%20out-of-distribution%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepeat%2520and%2520Concatenate%253A%25202D%2520to%25203D%2520Image%2520Translation%2520with%25203D%2520to%25203D%250A%2520%2520Generative%2520Modeling%26entry.906535625%3DAbril%2520Corona-Figueroa%2520and%2520Hubert%2520P.%2520H.%2520Shum%2520and%2520Chris%2520G.%2520Willcocks%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520a%25202D%2520to%25203D%2520image%2520translation%2520method%2520with%2520a%250Astraightforward%2520technique%252C%2520enabling%2520correlated%25202D%2520X-ray%2520to%25203D%2520CT-like%250Areconstruction.%2520We%2520observe%2520that%2520existing%2520approaches%252C%2520which%2520integrate%250Ainformation%2520across%2520multiple%25202D%2520views%2520in%2520the%2520latent%2520space%252C%2520lose%2520valuable%2520signal%250Ainformation%2520during%2520latent%2520encoding.%2520Instead%252C%2520we%2520simply%2520repeat%2520and%2520concatenate%250Athe%25202D%2520views%2520into%2520higher-channel%25203D%2520volumes%2520and%2520approach%2520the%25203D%2520reconstruction%250Achallenge%2520as%2520a%2520straightforward%25203D%2520to%25203D%2520generative%2520modeling%2520problem%252C%250Asidestepping%2520several%2520complex%2520modeling%2520issues.%2520This%2520method%2520enables%2520the%250Areconstructed%25203D%2520volume%2520to%2520retain%2520valuable%2520information%2520from%2520the%25202D%2520inputs%252C%250Awhich%2520are%2520passed%2520between%2520channel%2520states%2520in%2520a%2520Swin%2520UNETR%2520backbone.%2520Our%2520approach%250Aapplies%2520neural%2520optimal%2520transport%252C%2520which%2520is%2520fast%2520and%2520stable%2520to%2520train%252C%250Aeffectively%2520integrating%2520signal%2520information%2520across%2520multiple%2520views%2520without%2520the%250Arequirement%2520for%2520precise%2520alignment%253B%2520it%2520produces%2520non-collapsed%2520reconstructions%250Athat%2520are%2520highly%2520faithful%2520to%2520the%25202D%2520views%252C%2520even%2520after%2520limited%2520training.%2520We%250Ademonstrate%2520correlated%2520results%252C%2520both%2520qualitatively%2520and%2520quantitatively%252C%2520having%250Atrained%2520our%2520model%2520on%2520a%2520single%2520dataset%2520and%2520evaluated%2520its%2520generalization%2520ability%250Aacross%2520six%2520datasets%252C%2520including%2520out-of-distribution%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Repeat%20and%20Concatenate%3A%202D%20to%203D%20Image%20Translation%20with%203D%20to%203D%0A%20%20Generative%20Modeling&entry.906535625=Abril%20Corona-Figueroa%20and%20Hubert%20P.%20H.%20Shum%20and%20Chris%20G.%20Willcocks&entry.1292438233=%20%20This%20paper%20investigates%20a%202D%20to%203D%20image%20translation%20method%20with%20a%0Astraightforward%20technique%2C%20enabling%20correlated%202D%20X-ray%20to%203D%20CT-like%0Areconstruction.%20We%20observe%20that%20existing%20approaches%2C%20which%20integrate%0Ainformation%20across%20multiple%202D%20views%20in%20the%20latent%20space%2C%20lose%20valuable%20signal%0Ainformation%20during%20latent%20encoding.%20Instead%2C%20we%20simply%20repeat%20and%20concatenate%0Athe%202D%20views%20into%20higher-channel%203D%20volumes%20and%20approach%20the%203D%20reconstruction%0Achallenge%20as%20a%20straightforward%203D%20to%203D%20generative%20modeling%20problem%2C%0Asidestepping%20several%20complex%20modeling%20issues.%20This%20method%20enables%20the%0Areconstructed%203D%20volume%20to%20retain%20valuable%20information%20from%20the%202D%20inputs%2C%0Awhich%20are%20passed%20between%20channel%20states%20in%20a%20Swin%20UNETR%20backbone.%20Our%20approach%0Aapplies%20neural%20optimal%20transport%2C%20which%20is%20fast%20and%20stable%20to%20train%2C%0Aeffectively%20integrating%20signal%20information%20across%20multiple%20views%20without%20the%0Arequirement%20for%20precise%20alignment%3B%20it%20produces%20non-collapsed%20reconstructions%0Athat%20are%20highly%20faithful%20to%20the%202D%20views%2C%20even%20after%20limited%20training.%20We%0Ademonstrate%20correlated%20results%2C%20both%20qualitatively%20and%20quantitatively%2C%20having%0Atrained%20our%20model%20on%20a%20single%20dataset%20and%20evaluated%20its%20generalization%20ability%0Aacross%20six%20datasets%2C%20including%20out-of-distribution%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18422v1&entry.124074799=Read"},
{"title": "RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D\n  Facial Prior-guided Identity Alignment Network", "author": "Xiaozhong Ji and Chuming Lin and Zhonggan Ding and Ying Tai and Jian Yang and Junwei Zhu and Xiaobin Hu and Jiangning Zhang and Donghao Luo and Chengjie Wang", "abstract": "  Person-generic audio-driven face generation is a challenging task in computer\nvision. Previous methods have achieved remarkable progress in audio-visual\nsynchronization, but there is still a significant gap between current results\nand practical applications. The challenges are two-fold: 1) Preserving unique\nindividual traits for achieving high-precision lip synchronization. 2)\nGenerating high-quality facial renderings in real-time performance. In this\npaper, we propose a novel generalized audio-driven framework RealTalk, which\nconsists of an audio-to-expression transformer and a high-fidelity\nexpression-to-face renderer. In the first component, we consider both identity\nand intra-personal variation features related to speaking lip movements. By\nincorporating cross-modal attention on the enriched facial priors, we can\neffectively align lip movements with audio, thus attaining greater precision in\nexpression prediction. In the second component, we design a lightweight facial\nidentity alignment (FIA) module which includes a lip-shape control structure\nand a face texture reference structure. This novel design allows us to generate\nfine details in real-time, without depending on sophisticated and inefficient\nfeature alignment modules. Our experimental results, both quantitative and\nqualitative, on public datasets demonstrate the clear advantages of our method\nin terms of lip-speech synchronization and generation quality. Furthermore, our\nmethod is efficient and requires fewer computational resources, making it\nwell-suited to meet the needs of practical applications.\n", "link": "http://arxiv.org/abs/2406.18284v1", "date": "2024-06-26", "relevancy": 3.1126, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.625}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.625}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealTalk%3A%20Real-time%20and%20Realistic%20Audio-driven%20Face%20Generation%20with%203D%0A%20%20Facial%20Prior-guided%20Identity%20Alignment%20Network&body=Title%3A%20RealTalk%3A%20Real-time%20and%20Realistic%20Audio-driven%20Face%20Generation%20with%203D%0A%20%20Facial%20Prior-guided%20Identity%20Alignment%20Network%0AAuthor%3A%20Xiaozhong%20Ji%20and%20Chuming%20Lin%20and%20Zhonggan%20Ding%20and%20Ying%20Tai%20and%20Jian%20Yang%20and%20Junwei%20Zhu%20and%20Xiaobin%20Hu%20and%20Jiangning%20Zhang%20and%20Donghao%20Luo%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Person-generic%20audio-driven%20face%20generation%20is%20a%20challenging%20task%20in%20computer%0Avision.%20Previous%20methods%20have%20achieved%20remarkable%20progress%20in%20audio-visual%0Asynchronization%2C%20but%20there%20is%20still%20a%20significant%20gap%20between%20current%20results%0Aand%20practical%20applications.%20The%20challenges%20are%20two-fold%3A%201%29%20Preserving%20unique%0Aindividual%20traits%20for%20achieving%20high-precision%20lip%20synchronization.%202%29%0AGenerating%20high-quality%20facial%20renderings%20in%20real-time%20performance.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20generalized%20audio-driven%20framework%20RealTalk%2C%20which%0Aconsists%20of%20an%20audio-to-expression%20transformer%20and%20a%20high-fidelity%0Aexpression-to-face%20renderer.%20In%20the%20first%20component%2C%20we%20consider%20both%20identity%0Aand%20intra-personal%20variation%20features%20related%20to%20speaking%20lip%20movements.%20By%0Aincorporating%20cross-modal%20attention%20on%20the%20enriched%20facial%20priors%2C%20we%20can%0Aeffectively%20align%20lip%20movements%20with%20audio%2C%20thus%20attaining%20greater%20precision%20in%0Aexpression%20prediction.%20In%20the%20second%20component%2C%20we%20design%20a%20lightweight%20facial%0Aidentity%20alignment%20%28FIA%29%20module%20which%20includes%20a%20lip-shape%20control%20structure%0Aand%20a%20face%20texture%20reference%20structure.%20This%20novel%20design%20allows%20us%20to%20generate%0Afine%20details%20in%20real-time%2C%20without%20depending%20on%20sophisticated%20and%20inefficient%0Afeature%20alignment%20modules.%20Our%20experimental%20results%2C%20both%20quantitative%20and%0Aqualitative%2C%20on%20public%20datasets%20demonstrate%20the%20clear%20advantages%20of%20our%20method%0Ain%20terms%20of%20lip-speech%20synchronization%20and%20generation%20quality.%20Furthermore%2C%20our%0Amethod%20is%20efficient%20and%20requires%20fewer%20computational%20resources%2C%20making%20it%0Awell-suited%20to%20meet%20the%20needs%20of%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealTalk%253A%2520Real-time%2520and%2520Realistic%2520Audio-driven%2520Face%2520Generation%2520with%25203D%250A%2520%2520Facial%2520Prior-guided%2520Identity%2520Alignment%2520Network%26entry.906535625%3DXiaozhong%2520Ji%2520and%2520Chuming%2520Lin%2520and%2520Zhonggan%2520Ding%2520and%2520Ying%2520Tai%2520and%2520Jian%2520Yang%2520and%2520Junwei%2520Zhu%2520and%2520Xiaobin%2520Hu%2520and%2520Jiangning%2520Zhang%2520and%2520Donghao%2520Luo%2520and%2520Chengjie%2520Wang%26entry.1292438233%3D%2520%2520Person-generic%2520audio-driven%2520face%2520generation%2520is%2520a%2520challenging%2520task%2520in%2520computer%250Avision.%2520Previous%2520methods%2520have%2520achieved%2520remarkable%2520progress%2520in%2520audio-visual%250Asynchronization%252C%2520but%2520there%2520is%2520still%2520a%2520significant%2520gap%2520between%2520current%2520results%250Aand%2520practical%2520applications.%2520The%2520challenges%2520are%2520two-fold%253A%25201%2529%2520Preserving%2520unique%250Aindividual%2520traits%2520for%2520achieving%2520high-precision%2520lip%2520synchronization.%25202%2529%250AGenerating%2520high-quality%2520facial%2520renderings%2520in%2520real-time%2520performance.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520generalized%2520audio-driven%2520framework%2520RealTalk%252C%2520which%250Aconsists%2520of%2520an%2520audio-to-expression%2520transformer%2520and%2520a%2520high-fidelity%250Aexpression-to-face%2520renderer.%2520In%2520the%2520first%2520component%252C%2520we%2520consider%2520both%2520identity%250Aand%2520intra-personal%2520variation%2520features%2520related%2520to%2520speaking%2520lip%2520movements.%2520By%250Aincorporating%2520cross-modal%2520attention%2520on%2520the%2520enriched%2520facial%2520priors%252C%2520we%2520can%250Aeffectively%2520align%2520lip%2520movements%2520with%2520audio%252C%2520thus%2520attaining%2520greater%2520precision%2520in%250Aexpression%2520prediction.%2520In%2520the%2520second%2520component%252C%2520we%2520design%2520a%2520lightweight%2520facial%250Aidentity%2520alignment%2520%2528FIA%2529%2520module%2520which%2520includes%2520a%2520lip-shape%2520control%2520structure%250Aand%2520a%2520face%2520texture%2520reference%2520structure.%2520This%2520novel%2520design%2520allows%2520us%2520to%2520generate%250Afine%2520details%2520in%2520real-time%252C%2520without%2520depending%2520on%2520sophisticated%2520and%2520inefficient%250Afeature%2520alignment%2520modules.%2520Our%2520experimental%2520results%252C%2520both%2520quantitative%2520and%250Aqualitative%252C%2520on%2520public%2520datasets%2520demonstrate%2520the%2520clear%2520advantages%2520of%2520our%2520method%250Ain%2520terms%2520of%2520lip-speech%2520synchronization%2520and%2520generation%2520quality.%2520Furthermore%252C%2520our%250Amethod%2520is%2520efficient%2520and%2520requires%2520fewer%2520computational%2520resources%252C%2520making%2520it%250Awell-suited%2520to%2520meet%2520the%2520needs%2520of%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealTalk%3A%20Real-time%20and%20Realistic%20Audio-driven%20Face%20Generation%20with%203D%0A%20%20Facial%20Prior-guided%20Identity%20Alignment%20Network&entry.906535625=Xiaozhong%20Ji%20and%20Chuming%20Lin%20and%20Zhonggan%20Ding%20and%20Ying%20Tai%20and%20Jian%20Yang%20and%20Junwei%20Zhu%20and%20Xiaobin%20Hu%20and%20Jiangning%20Zhang%20and%20Donghao%20Luo%20and%20Chengjie%20Wang&entry.1292438233=%20%20Person-generic%20audio-driven%20face%20generation%20is%20a%20challenging%20task%20in%20computer%0Avision.%20Previous%20methods%20have%20achieved%20remarkable%20progress%20in%20audio-visual%0Asynchronization%2C%20but%20there%20is%20still%20a%20significant%20gap%20between%20current%20results%0Aand%20practical%20applications.%20The%20challenges%20are%20two-fold%3A%201%29%20Preserving%20unique%0Aindividual%20traits%20for%20achieving%20high-precision%20lip%20synchronization.%202%29%0AGenerating%20high-quality%20facial%20renderings%20in%20real-time%20performance.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20generalized%20audio-driven%20framework%20RealTalk%2C%20which%0Aconsists%20of%20an%20audio-to-expression%20transformer%20and%20a%20high-fidelity%0Aexpression-to-face%20renderer.%20In%20the%20first%20component%2C%20we%20consider%20both%20identity%0Aand%20intra-personal%20variation%20features%20related%20to%20speaking%20lip%20movements.%20By%0Aincorporating%20cross-modal%20attention%20on%20the%20enriched%20facial%20priors%2C%20we%20can%0Aeffectively%20align%20lip%20movements%20with%20audio%2C%20thus%20attaining%20greater%20precision%20in%0Aexpression%20prediction.%20In%20the%20second%20component%2C%20we%20design%20a%20lightweight%20facial%0Aidentity%20alignment%20%28FIA%29%20module%20which%20includes%20a%20lip-shape%20control%20structure%0Aand%20a%20face%20texture%20reference%20structure.%20This%20novel%20design%20allows%20us%20to%20generate%0Afine%20details%20in%20real-time%2C%20without%20depending%20on%20sophisticated%20and%20inefficient%0Afeature%20alignment%20modules.%20Our%20experimental%20results%2C%20both%20quantitative%20and%0Aqualitative%2C%20on%20public%20datasets%20demonstrate%20the%20clear%20advantages%20of%20our%20method%0Ain%20terms%20of%20lip-speech%20synchronization%20and%20generation%20quality.%20Furthermore%2C%20our%0Amethod%20is%20efficient%20and%20requires%20fewer%20computational%20resources%2C%20making%20it%0Awell-suited%20to%20meet%20the%20needs%20of%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18284v1&entry.124074799=Read"},
{"title": "CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from\n  Monocular Video", "author": "Xingyu Miao and Yang Bai and Haoran Duan and Yawen Huang and Fan Wan and Yang Long and Yefeng Zheng", "abstract": "  The goal of our work is to generate high-quality novel views from monocular\nvideos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have\nshown impressive performance by leveraging time-varying dynamic radiation\nfields. However, these methods have limitations when it comes to accurately\nmodeling the motion of complex objects, which can lead to inaccurate and blurry\nrenderings of details. To address this limitation, we propose a novel approach\nthat builds upon a recent generalization NeRF, which aggregates nearby views\nonto new viewpoints. However, such methods are typically only effective for\nstatic scenes. To overcome this challenge, we introduce a module that operates\nin both the time and frequency domains to aggregate the features of object\nmotion. This allows us to learn the relationship between frames and generate\nhigher-quality images. Our experiments demonstrate significant improvements\nover state-of-the-art methods on dynamic scene datasets. Specifically, our\napproach outperforms existing methods in terms of both the accuracy and visual\nquality of the synthesized views. Our code is available on\nhttps://github.com/xingy038/CTNeRF.\n", "link": "http://arxiv.org/abs/2401.04861v2", "date": "2024-06-26", "relevancy": 3.0734, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6436}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6002}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTNeRF%3A%20Cross-Time%20Transformer%20for%20Dynamic%20Neural%20Radiance%20Field%20from%0A%20%20Monocular%20Video&body=Title%3A%20CTNeRF%3A%20Cross-Time%20Transformer%20for%20Dynamic%20Neural%20Radiance%20Field%20from%0A%20%20Monocular%20Video%0AAuthor%3A%20Xingyu%20Miao%20and%20Yang%20Bai%20and%20Haoran%20Duan%20and%20Yawen%20Huang%20and%20Fan%20Wan%20and%20Yang%20Long%20and%20Yefeng%20Zheng%0AAbstract%3A%20%20%20The%20goal%20of%20our%20work%20is%20to%20generate%20high-quality%20novel%20views%20from%20monocular%0Avideos%20of%20complex%20and%20dynamic%20scenes.%20Prior%20methods%2C%20such%20as%20DynamicNeRF%2C%20have%0Ashown%20impressive%20performance%20by%20leveraging%20time-varying%20dynamic%20radiation%0Afields.%20However%2C%20these%20methods%20have%20limitations%20when%20it%20comes%20to%20accurately%0Amodeling%20the%20motion%20of%20complex%20objects%2C%20which%20can%20lead%20to%20inaccurate%20and%20blurry%0Arenderings%20of%20details.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20approach%0Athat%20builds%20upon%20a%20recent%20generalization%20NeRF%2C%20which%20aggregates%20nearby%20views%0Aonto%20new%20viewpoints.%20However%2C%20such%20methods%20are%20typically%20only%20effective%20for%0Astatic%20scenes.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20a%20module%20that%20operates%0Ain%20both%20the%20time%20and%20frequency%20domains%20to%20aggregate%20the%20features%20of%20object%0Amotion.%20This%20allows%20us%20to%20learn%20the%20relationship%20between%20frames%20and%20generate%0Ahigher-quality%20images.%20Our%20experiments%20demonstrate%20significant%20improvements%0Aover%20state-of-the-art%20methods%20on%20dynamic%20scene%20datasets.%20Specifically%2C%20our%0Aapproach%20outperforms%20existing%20methods%20in%20terms%20of%20both%20the%20accuracy%20and%20visual%0Aquality%20of%20the%20synthesized%20views.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/xingy038/CTNeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04861v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTNeRF%253A%2520Cross-Time%2520Transformer%2520for%2520Dynamic%2520Neural%2520Radiance%2520Field%2520from%250A%2520%2520Monocular%2520Video%26entry.906535625%3DXingyu%2520Miao%2520and%2520Yang%2520Bai%2520and%2520Haoran%2520Duan%2520and%2520Yawen%2520Huang%2520and%2520Fan%2520Wan%2520and%2520Yang%2520Long%2520and%2520Yefeng%2520Zheng%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520our%2520work%2520is%2520to%2520generate%2520high-quality%2520novel%2520views%2520from%2520monocular%250Avideos%2520of%2520complex%2520and%2520dynamic%2520scenes.%2520Prior%2520methods%252C%2520such%2520as%2520DynamicNeRF%252C%2520have%250Ashown%2520impressive%2520performance%2520by%2520leveraging%2520time-varying%2520dynamic%2520radiation%250Afields.%2520However%252C%2520these%2520methods%2520have%2520limitations%2520when%2520it%2520comes%2520to%2520accurately%250Amodeling%2520the%2520motion%2520of%2520complex%2520objects%252C%2520which%2520can%2520lead%2520to%2520inaccurate%2520and%2520blurry%250Arenderings%2520of%2520details.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520approach%250Athat%2520builds%2520upon%2520a%2520recent%2520generalization%2520NeRF%252C%2520which%2520aggregates%2520nearby%2520views%250Aonto%2520new%2520viewpoints.%2520However%252C%2520such%2520methods%2520are%2520typically%2520only%2520effective%2520for%250Astatic%2520scenes.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520module%2520that%2520operates%250Ain%2520both%2520the%2520time%2520and%2520frequency%2520domains%2520to%2520aggregate%2520the%2520features%2520of%2520object%250Amotion.%2520This%2520allows%2520us%2520to%2520learn%2520the%2520relationship%2520between%2520frames%2520and%2520generate%250Ahigher-quality%2520images.%2520Our%2520experiments%2520demonstrate%2520significant%2520improvements%250Aover%2520state-of-the-art%2520methods%2520on%2520dynamic%2520scene%2520datasets.%2520Specifically%252C%2520our%250Aapproach%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520both%2520the%2520accuracy%2520and%2520visual%250Aquality%2520of%2520the%2520synthesized%2520views.%2520Our%2520code%2520is%2520available%2520on%250Ahttps%253A//github.com/xingy038/CTNeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04861v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTNeRF%3A%20Cross-Time%20Transformer%20for%20Dynamic%20Neural%20Radiance%20Field%20from%0A%20%20Monocular%20Video&entry.906535625=Xingyu%20Miao%20and%20Yang%20Bai%20and%20Haoran%20Duan%20and%20Yawen%20Huang%20and%20Fan%20Wan%20and%20Yang%20Long%20and%20Yefeng%20Zheng&entry.1292438233=%20%20The%20goal%20of%20our%20work%20is%20to%20generate%20high-quality%20novel%20views%20from%20monocular%0Avideos%20of%20complex%20and%20dynamic%20scenes.%20Prior%20methods%2C%20such%20as%20DynamicNeRF%2C%20have%0Ashown%20impressive%20performance%20by%20leveraging%20time-varying%20dynamic%20radiation%0Afields.%20However%2C%20these%20methods%20have%20limitations%20when%20it%20comes%20to%20accurately%0Amodeling%20the%20motion%20of%20complex%20objects%2C%20which%20can%20lead%20to%20inaccurate%20and%20blurry%0Arenderings%20of%20details.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20approach%0Athat%20builds%20upon%20a%20recent%20generalization%20NeRF%2C%20which%20aggregates%20nearby%20views%0Aonto%20new%20viewpoints.%20However%2C%20such%20methods%20are%20typically%20only%20effective%20for%0Astatic%20scenes.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20a%20module%20that%20operates%0Ain%20both%20the%20time%20and%20frequency%20domains%20to%20aggregate%20the%20features%20of%20object%0Amotion.%20This%20allows%20us%20to%20learn%20the%20relationship%20between%20frames%20and%20generate%0Ahigher-quality%20images.%20Our%20experiments%20demonstrate%20significant%20improvements%0Aover%20state-of-the-art%20methods%20on%20dynamic%20scene%20datasets.%20Specifically%2C%20our%0Aapproach%20outperforms%20existing%20methods%20in%20terms%20of%20both%20the%20accuracy%20and%20visual%0Aquality%20of%20the%20synthesized%20views.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/xingy038/CTNeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04861v2&entry.124074799=Read"},
{"title": "Trimming the Fat: Efficient Compression of 3D Gaussian Splats through\n  Pruning", "author": "Muhammad Salman Ali and Maryam Qamar and Sung-Ho Bae and Enzo Tartaglione", "abstract": "  In recent times, the utilization of 3D models has gained traction, owing to\nthe capacity for end-to-end training initially offered by Neural Radiance\nFields and more recently by 3D Gaussian Splatting (3DGS) models. The latter\nholds a significant advantage by inherently easing rapid convergence during\ntraining and offering extensive editability. However, despite rapid\nadvancements, the literature still lives in its infancy regarding the\nscalability of these models. In this study, we take some initial steps in\naddressing this gap, showing an approach that enables both the memory and\ncomputational scalability of such models. Specifically, we propose \"Trimming\nthe fat\", a post-hoc gradient-informed iterative pruning technique to eliminate\nredundant information encoded in the model. Our experimental findings on widely\nacknowledged benchmarks attest to the effectiveness of our approach, revealing\nthat up to 75% of the Gaussians can be removed while maintaining or even\nimproving upon baseline performance. Our approach achieves around 50$\\times$\ncompression while preserving performance similar to the baseline model, and is\nable to speed-up computation up to 600~FPS.\n", "link": "http://arxiv.org/abs/2406.18214v1", "date": "2024-06-26", "relevancy": 3.009, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6674}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6018}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trimming%20the%20Fat%3A%20Efficient%20Compression%20of%203D%20Gaussian%20Splats%20through%0A%20%20Pruning&body=Title%3A%20Trimming%20the%20Fat%3A%20Efficient%20Compression%20of%203D%20Gaussian%20Splats%20through%0A%20%20Pruning%0AAuthor%3A%20Muhammad%20Salman%20Ali%20and%20Maryam%20Qamar%20and%20Sung-Ho%20Bae%20and%20Enzo%20Tartaglione%0AAbstract%3A%20%20%20In%20recent%20times%2C%20the%20utilization%20of%203D%20models%20has%20gained%20traction%2C%20owing%20to%0Athe%20capacity%20for%20end-to-end%20training%20initially%20offered%20by%20Neural%20Radiance%0AFields%20and%20more%20recently%20by%203D%20Gaussian%20Splatting%20%283DGS%29%20models.%20The%20latter%0Aholds%20a%20significant%20advantage%20by%20inherently%20easing%20rapid%20convergence%20during%0Atraining%20and%20offering%20extensive%20editability.%20However%2C%20despite%20rapid%0Aadvancements%2C%20the%20literature%20still%20lives%20in%20its%20infancy%20regarding%20the%0Ascalability%20of%20these%20models.%20In%20this%20study%2C%20we%20take%20some%20initial%20steps%20in%0Aaddressing%20this%20gap%2C%20showing%20an%20approach%20that%20enables%20both%20the%20memory%20and%0Acomputational%20scalability%20of%20such%20models.%20Specifically%2C%20we%20propose%20%22Trimming%0Athe%20fat%22%2C%20a%20post-hoc%20gradient-informed%20iterative%20pruning%20technique%20to%20eliminate%0Aredundant%20information%20encoded%20in%20the%20model.%20Our%20experimental%20findings%20on%20widely%0Aacknowledged%20benchmarks%20attest%20to%20the%20effectiveness%20of%20our%20approach%2C%20revealing%0Athat%20up%20to%2075%25%20of%20the%20Gaussians%20can%20be%20removed%20while%20maintaining%20or%20even%0Aimproving%20upon%20baseline%20performance.%20Our%20approach%20achieves%20around%2050%24%5Ctimes%24%0Acompression%20while%20preserving%20performance%20similar%20to%20the%20baseline%20model%2C%20and%20is%0Aable%20to%20speed-up%20computation%20up%20to%20600~FPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrimming%2520the%2520Fat%253A%2520Efficient%2520Compression%2520of%25203D%2520Gaussian%2520Splats%2520through%250A%2520%2520Pruning%26entry.906535625%3DMuhammad%2520Salman%2520Ali%2520and%2520Maryam%2520Qamar%2520and%2520Sung-Ho%2520Bae%2520and%2520Enzo%2520Tartaglione%26entry.1292438233%3D%2520%2520In%2520recent%2520times%252C%2520the%2520utilization%2520of%25203D%2520models%2520has%2520gained%2520traction%252C%2520owing%2520to%250Athe%2520capacity%2520for%2520end-to-end%2520training%2520initially%2520offered%2520by%2520Neural%2520Radiance%250AFields%2520and%2520more%2520recently%2520by%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520models.%2520The%2520latter%250Aholds%2520a%2520significant%2520advantage%2520by%2520inherently%2520easing%2520rapid%2520convergence%2520during%250Atraining%2520and%2520offering%2520extensive%2520editability.%2520However%252C%2520despite%2520rapid%250Aadvancements%252C%2520the%2520literature%2520still%2520lives%2520in%2520its%2520infancy%2520regarding%2520the%250Ascalability%2520of%2520these%2520models.%2520In%2520this%2520study%252C%2520we%2520take%2520some%2520initial%2520steps%2520in%250Aaddressing%2520this%2520gap%252C%2520showing%2520an%2520approach%2520that%2520enables%2520both%2520the%2520memory%2520and%250Acomputational%2520scalability%2520of%2520such%2520models.%2520Specifically%252C%2520we%2520propose%2520%2522Trimming%250Athe%2520fat%2522%252C%2520a%2520post-hoc%2520gradient-informed%2520iterative%2520pruning%2520technique%2520to%2520eliminate%250Aredundant%2520information%2520encoded%2520in%2520the%2520model.%2520Our%2520experimental%2520findings%2520on%2520widely%250Aacknowledged%2520benchmarks%2520attest%2520to%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520revealing%250Athat%2520up%2520to%252075%2525%2520of%2520the%2520Gaussians%2520can%2520be%2520removed%2520while%2520maintaining%2520or%2520even%250Aimproving%2520upon%2520baseline%2520performance.%2520Our%2520approach%2520achieves%2520around%252050%2524%255Ctimes%2524%250Acompression%2520while%2520preserving%2520performance%2520similar%2520to%2520the%2520baseline%2520model%252C%2520and%2520is%250Aable%2520to%2520speed-up%2520computation%2520up%2520to%2520600~FPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trimming%20the%20Fat%3A%20Efficient%20Compression%20of%203D%20Gaussian%20Splats%20through%0A%20%20Pruning&entry.906535625=Muhammad%20Salman%20Ali%20and%20Maryam%20Qamar%20and%20Sung-Ho%20Bae%20and%20Enzo%20Tartaglione&entry.1292438233=%20%20In%20recent%20times%2C%20the%20utilization%20of%203D%20models%20has%20gained%20traction%2C%20owing%20to%0Athe%20capacity%20for%20end-to-end%20training%20initially%20offered%20by%20Neural%20Radiance%0AFields%20and%20more%20recently%20by%203D%20Gaussian%20Splatting%20%283DGS%29%20models.%20The%20latter%0Aholds%20a%20significant%20advantage%20by%20inherently%20easing%20rapid%20convergence%20during%0Atraining%20and%20offering%20extensive%20editability.%20However%2C%20despite%20rapid%0Aadvancements%2C%20the%20literature%20still%20lives%20in%20its%20infancy%20regarding%20the%0Ascalability%20of%20these%20models.%20In%20this%20study%2C%20we%20take%20some%20initial%20steps%20in%0Aaddressing%20this%20gap%2C%20showing%20an%20approach%20that%20enables%20both%20the%20memory%20and%0Acomputational%20scalability%20of%20such%20models.%20Specifically%2C%20we%20propose%20%22Trimming%0Athe%20fat%22%2C%20a%20post-hoc%20gradient-informed%20iterative%20pruning%20technique%20to%20eliminate%0Aredundant%20information%20encoded%20in%20the%20model.%20Our%20experimental%20findings%20on%20widely%0Aacknowledged%20benchmarks%20attest%20to%20the%20effectiveness%20of%20our%20approach%2C%20revealing%0Athat%20up%20to%2075%25%20of%20the%20Gaussians%20can%20be%20removed%20while%20maintaining%20or%20even%0Aimproving%20upon%20baseline%20performance.%20Our%20approach%20achieves%20around%2050%24%5Ctimes%24%0Acompression%20while%20preserving%20performance%20similar%20to%20the%20baseline%20model%2C%20and%20is%0Aable%20to%20speed-up%20computation%20up%20to%20600~FPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18214v1&entry.124074799=Read"},
{"title": "Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration\n  Framework", "author": "Henrique Jesus and Hugo Proen\u00e7a", "abstract": "  Large vision models based in deep learning architectures have been\nconsistently advancing the state-of-the-art in biometric recognition. However,\nthree weaknesses are commonly reported for such kind of approaches: 1) their\nextreme demands in terms of learning data; 2) the difficulties in generalising\nbetween different domains; and 3) the lack of interpretability/explainability,\nwith biometrics being of particular interest, as it is important to provide\nevidence able to be used for forensics/legal purposes (e.g., in courts). To the\nbest of our knowledge, this paper describes the first recognition\nframework/strategy that aims at addressing the three weaknesses simultaneously.\nAt first, it relies exclusively in synthetic samples for learning purposes.\nInstead of requiring a large amount and variety of samples for each subject,\nthe idea is to exclusively enroll a 3D point cloud per identity. Then, using\ngenerative strategies, we synthesize a very large (potentially infinite) number\nof samples, containing all the desired covariates (poses, clothing, distances,\nperspectives, lighting, occlusions,...). Upon the synthesizing method used, it\nis possible to adapt precisely to different kind of domains, which accounts for\ngeneralization purposes. Such data are then used to learn a model that performs\nlocal registration between image pairs, establishing positive correspondences\nbetween body parts that are the key, not only to recognition (according to\ncardinality and distribution), but also to provide an interpretable description\nof the response (e.g.: \"both samples are from the same person, as they have\nsimilar facial shape, hair color and legs thickness\").\n", "link": "http://arxiv.org/abs/2403.06658v2", "date": "2024-06-26", "relevancy": 2.9169, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6019}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5747}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Zero-Shot%20Interpretable%20Human%20Recognition%3A%20A%202D-3D%20Registration%0A%20%20Framework&body=Title%3A%20Towards%20Zero-Shot%20Interpretable%20Human%20Recognition%3A%20A%202D-3D%20Registration%0A%20%20Framework%0AAuthor%3A%20Henrique%20Jesus%20and%20Hugo%20Proen%C3%A7a%0AAbstract%3A%20%20%20Large%20vision%20models%20based%20in%20deep%20learning%20architectures%20have%20been%0Aconsistently%20advancing%20the%20state-of-the-art%20in%20biometric%20recognition.%20However%2C%0Athree%20weaknesses%20are%20commonly%20reported%20for%20such%20kind%20of%20approaches%3A%201%29%20their%0Aextreme%20demands%20in%20terms%20of%20learning%20data%3B%202%29%20the%20difficulties%20in%20generalising%0Abetween%20different%20domains%3B%20and%203%29%20the%20lack%20of%20interpretability/explainability%2C%0Awith%20biometrics%20being%20of%20particular%20interest%2C%20as%20it%20is%20important%20to%20provide%0Aevidence%20able%20to%20be%20used%20for%20forensics/legal%20purposes%20%28e.g.%2C%20in%20courts%29.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20paper%20describes%20the%20first%20recognition%0Aframework/strategy%20that%20aims%20at%20addressing%20the%20three%20weaknesses%20simultaneously.%0AAt%20first%2C%20it%20relies%20exclusively%20in%20synthetic%20samples%20for%20learning%20purposes.%0AInstead%20of%20requiring%20a%20large%20amount%20and%20variety%20of%20samples%20for%20each%20subject%2C%0Athe%20idea%20is%20to%20exclusively%20enroll%20a%203D%20point%20cloud%20per%20identity.%20Then%2C%20using%0Agenerative%20strategies%2C%20we%20synthesize%20a%20very%20large%20%28potentially%20infinite%29%20number%0Aof%20samples%2C%20containing%20all%20the%20desired%20covariates%20%28poses%2C%20clothing%2C%20distances%2C%0Aperspectives%2C%20lighting%2C%20occlusions%2C...%29.%20Upon%20the%20synthesizing%20method%20used%2C%20it%0Ais%20possible%20to%20adapt%20precisely%20to%20different%20kind%20of%20domains%2C%20which%20accounts%20for%0Ageneralization%20purposes.%20Such%20data%20are%20then%20used%20to%20learn%20a%20model%20that%20performs%0Alocal%20registration%20between%20image%20pairs%2C%20establishing%20positive%20correspondences%0Abetween%20body%20parts%20that%20are%20the%20key%2C%20not%20only%20to%20recognition%20%28according%20to%0Acardinality%20and%20distribution%29%2C%20but%20also%20to%20provide%20an%20interpretable%20description%0Aof%20the%20response%20%28e.g.%3A%20%22both%20samples%20are%20from%20the%20same%20person%2C%20as%20they%20have%0Asimilar%20facial%20shape%2C%20hair%20color%20and%20legs%20thickness%22%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06658v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Zero-Shot%2520Interpretable%2520Human%2520Recognition%253A%2520A%25202D-3D%2520Registration%250A%2520%2520Framework%26entry.906535625%3DHenrique%2520Jesus%2520and%2520Hugo%2520Proen%25C3%25A7a%26entry.1292438233%3D%2520%2520Large%2520vision%2520models%2520based%2520in%2520deep%2520learning%2520architectures%2520have%2520been%250Aconsistently%2520advancing%2520the%2520state-of-the-art%2520in%2520biometric%2520recognition.%2520However%252C%250Athree%2520weaknesses%2520are%2520commonly%2520reported%2520for%2520such%2520kind%2520of%2520approaches%253A%25201%2529%2520their%250Aextreme%2520demands%2520in%2520terms%2520of%2520learning%2520data%253B%25202%2529%2520the%2520difficulties%2520in%2520generalising%250Abetween%2520different%2520domains%253B%2520and%25203%2529%2520the%2520lack%2520of%2520interpretability/explainability%252C%250Awith%2520biometrics%2520being%2520of%2520particular%2520interest%252C%2520as%2520it%2520is%2520important%2520to%2520provide%250Aevidence%2520able%2520to%2520be%2520used%2520for%2520forensics/legal%2520purposes%2520%2528e.g.%252C%2520in%2520courts%2529.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520this%2520paper%2520describes%2520the%2520first%2520recognition%250Aframework/strategy%2520that%2520aims%2520at%2520addressing%2520the%2520three%2520weaknesses%2520simultaneously.%250AAt%2520first%252C%2520it%2520relies%2520exclusively%2520in%2520synthetic%2520samples%2520for%2520learning%2520purposes.%250AInstead%2520of%2520requiring%2520a%2520large%2520amount%2520and%2520variety%2520of%2520samples%2520for%2520each%2520subject%252C%250Athe%2520idea%2520is%2520to%2520exclusively%2520enroll%2520a%25203D%2520point%2520cloud%2520per%2520identity.%2520Then%252C%2520using%250Agenerative%2520strategies%252C%2520we%2520synthesize%2520a%2520very%2520large%2520%2528potentially%2520infinite%2529%2520number%250Aof%2520samples%252C%2520containing%2520all%2520the%2520desired%2520covariates%2520%2528poses%252C%2520clothing%252C%2520distances%252C%250Aperspectives%252C%2520lighting%252C%2520occlusions%252C...%2529.%2520Upon%2520the%2520synthesizing%2520method%2520used%252C%2520it%250Ais%2520possible%2520to%2520adapt%2520precisely%2520to%2520different%2520kind%2520of%2520domains%252C%2520which%2520accounts%2520for%250Ageneralization%2520purposes.%2520Such%2520data%2520are%2520then%2520used%2520to%2520learn%2520a%2520model%2520that%2520performs%250Alocal%2520registration%2520between%2520image%2520pairs%252C%2520establishing%2520positive%2520correspondences%250Abetween%2520body%2520parts%2520that%2520are%2520the%2520key%252C%2520not%2520only%2520to%2520recognition%2520%2528according%2520to%250Acardinality%2520and%2520distribution%2529%252C%2520but%2520also%2520to%2520provide%2520an%2520interpretable%2520description%250Aof%2520the%2520response%2520%2528e.g.%253A%2520%2522both%2520samples%2520are%2520from%2520the%2520same%2520person%252C%2520as%2520they%2520have%250Asimilar%2520facial%2520shape%252C%2520hair%2520color%2520and%2520legs%2520thickness%2522%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06658v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Zero-Shot%20Interpretable%20Human%20Recognition%3A%20A%202D-3D%20Registration%0A%20%20Framework&entry.906535625=Henrique%20Jesus%20and%20Hugo%20Proen%C3%A7a&entry.1292438233=%20%20Large%20vision%20models%20based%20in%20deep%20learning%20architectures%20have%20been%0Aconsistently%20advancing%20the%20state-of-the-art%20in%20biometric%20recognition.%20However%2C%0Athree%20weaknesses%20are%20commonly%20reported%20for%20such%20kind%20of%20approaches%3A%201%29%20their%0Aextreme%20demands%20in%20terms%20of%20learning%20data%3B%202%29%20the%20difficulties%20in%20generalising%0Abetween%20different%20domains%3B%20and%203%29%20the%20lack%20of%20interpretability/explainability%2C%0Awith%20biometrics%20being%20of%20particular%20interest%2C%20as%20it%20is%20important%20to%20provide%0Aevidence%20able%20to%20be%20used%20for%20forensics/legal%20purposes%20%28e.g.%2C%20in%20courts%29.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20paper%20describes%20the%20first%20recognition%0Aframework/strategy%20that%20aims%20at%20addressing%20the%20three%20weaknesses%20simultaneously.%0AAt%20first%2C%20it%20relies%20exclusively%20in%20synthetic%20samples%20for%20learning%20purposes.%0AInstead%20of%20requiring%20a%20large%20amount%20and%20variety%20of%20samples%20for%20each%20subject%2C%0Athe%20idea%20is%20to%20exclusively%20enroll%20a%203D%20point%20cloud%20per%20identity.%20Then%2C%20using%0Agenerative%20strategies%2C%20we%20synthesize%20a%20very%20large%20%28potentially%20infinite%29%20number%0Aof%20samples%2C%20containing%20all%20the%20desired%20covariates%20%28poses%2C%20clothing%2C%20distances%2C%0Aperspectives%2C%20lighting%2C%20occlusions%2C...%29.%20Upon%20the%20synthesizing%20method%20used%2C%20it%0Ais%20possible%20to%20adapt%20precisely%20to%20different%20kind%20of%20domains%2C%20which%20accounts%20for%0Ageneralization%20purposes.%20Such%20data%20are%20then%20used%20to%20learn%20a%20model%20that%20performs%0Alocal%20registration%20between%20image%20pairs%2C%20establishing%20positive%20correspondences%0Abetween%20body%20parts%20that%20are%20the%20key%2C%20not%20only%20to%20recognition%20%28according%20to%0Acardinality%20and%20distribution%29%2C%20but%20also%20to%20provide%20an%20interpretable%20description%0Aof%20the%20response%20%28e.g.%3A%20%22both%20samples%20are%20from%20the%20same%20person%2C%20as%20they%20have%0Asimilar%20facial%20shape%2C%20hair%20color%20and%20legs%20thickness%22%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06658v2&entry.124074799=Read"},
{"title": "DoubleTake: Geometry Guided Depth Estimation", "author": "Mohamed Sayed and Filippo Aleotti and Jamie Watson and Zawar Qureshi and Guillermo Garcia-Hernando and Gabriel Brostow and Sara Vicente and Michael Firman", "abstract": "  Estimating depth from a sequence of posed RGB images is a fundamental\ncomputer vision task, with applications in augmented reality, path planning\netc. Prior work typically makes use of previous frames in a multi view stereo\nframework, relying on matching textures in a local neighborhood. In contrast,\nour model leverages historical predictions by giving the latest 3D geometry\ndata as an extra input to our network. This self-generated geometric hint can\nencode information from areas of the scene not covered by the keyframes and it\nis more regularized when compared to individual predicted depth maps for\nprevious frames. We introduce a Hint MLP which combines cost volume features\nwith a hint of the prior geometry, rendered as a depth map from the current\ncamera location, together with a measure of the confidence in the prior\ngeometry. We demonstrate that our method, which can run at interactive speeds,\nachieves state-of-the-art estimates of depth and 3D scene reconstruction in\nboth offline and incremental evaluation scenarios.\n", "link": "http://arxiv.org/abs/2406.18387v1", "date": "2024-06-26", "relevancy": 2.9081, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.595}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5754}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DoubleTake%3A%20Geometry%20Guided%20Depth%20Estimation&body=Title%3A%20DoubleTake%3A%20Geometry%20Guided%20Depth%20Estimation%0AAuthor%3A%20Mohamed%20Sayed%20and%20Filippo%20Aleotti%20and%20Jamie%20Watson%20and%20Zawar%20Qureshi%20and%20Guillermo%20Garcia-Hernando%20and%20Gabriel%20Brostow%20and%20Sara%20Vicente%20and%20Michael%20Firman%0AAbstract%3A%20%20%20Estimating%20depth%20from%20a%20sequence%20of%20posed%20RGB%20images%20is%20a%20fundamental%0Acomputer%20vision%20task%2C%20with%20applications%20in%20augmented%20reality%2C%20path%20planning%0Aetc.%20Prior%20work%20typically%20makes%20use%20of%20previous%20frames%20in%20a%20multi%20view%20stereo%0Aframework%2C%20relying%20on%20matching%20textures%20in%20a%20local%20neighborhood.%20In%20contrast%2C%0Aour%20model%20leverages%20historical%20predictions%20by%20giving%20the%20latest%203D%20geometry%0Adata%20as%20an%20extra%20input%20to%20our%20network.%20This%20self-generated%20geometric%20hint%20can%0Aencode%20information%20from%20areas%20of%20the%20scene%20not%20covered%20by%20the%20keyframes%20and%20it%0Ais%20more%20regularized%20when%20compared%20to%20individual%20predicted%20depth%20maps%20for%0Aprevious%20frames.%20We%20introduce%20a%20Hint%20MLP%20which%20combines%20cost%20volume%20features%0Awith%20a%20hint%20of%20the%20prior%20geometry%2C%20rendered%20as%20a%20depth%20map%20from%20the%20current%0Acamera%20location%2C%20together%20with%20a%20measure%20of%20the%20confidence%20in%20the%20prior%0Ageometry.%20We%20demonstrate%20that%20our%20method%2C%20which%20can%20run%20at%20interactive%20speeds%2C%0Aachieves%20state-of-the-art%20estimates%20of%20depth%20and%203D%20scene%20reconstruction%20in%0Aboth%20offline%20and%20incremental%20evaluation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoubleTake%253A%2520Geometry%2520Guided%2520Depth%2520Estimation%26entry.906535625%3DMohamed%2520Sayed%2520and%2520Filippo%2520Aleotti%2520and%2520Jamie%2520Watson%2520and%2520Zawar%2520Qureshi%2520and%2520Guillermo%2520Garcia-Hernando%2520and%2520Gabriel%2520Brostow%2520and%2520Sara%2520Vicente%2520and%2520Michael%2520Firman%26entry.1292438233%3D%2520%2520Estimating%2520depth%2520from%2520a%2520sequence%2520of%2520posed%2520RGB%2520images%2520is%2520a%2520fundamental%250Acomputer%2520vision%2520task%252C%2520with%2520applications%2520in%2520augmented%2520reality%252C%2520path%2520planning%250Aetc.%2520Prior%2520work%2520typically%2520makes%2520use%2520of%2520previous%2520frames%2520in%2520a%2520multi%2520view%2520stereo%250Aframework%252C%2520relying%2520on%2520matching%2520textures%2520in%2520a%2520local%2520neighborhood.%2520In%2520contrast%252C%250Aour%2520model%2520leverages%2520historical%2520predictions%2520by%2520giving%2520the%2520latest%25203D%2520geometry%250Adata%2520as%2520an%2520extra%2520input%2520to%2520our%2520network.%2520This%2520self-generated%2520geometric%2520hint%2520can%250Aencode%2520information%2520from%2520areas%2520of%2520the%2520scene%2520not%2520covered%2520by%2520the%2520keyframes%2520and%2520it%250Ais%2520more%2520regularized%2520when%2520compared%2520to%2520individual%2520predicted%2520depth%2520maps%2520for%250Aprevious%2520frames.%2520We%2520introduce%2520a%2520Hint%2520MLP%2520which%2520combines%2520cost%2520volume%2520features%250Awith%2520a%2520hint%2520of%2520the%2520prior%2520geometry%252C%2520rendered%2520as%2520a%2520depth%2520map%2520from%2520the%2520current%250Acamera%2520location%252C%2520together%2520with%2520a%2520measure%2520of%2520the%2520confidence%2520in%2520the%2520prior%250Ageometry.%2520We%2520demonstrate%2520that%2520our%2520method%252C%2520which%2520can%2520run%2520at%2520interactive%2520speeds%252C%250Aachieves%2520state-of-the-art%2520estimates%2520of%2520depth%2520and%25203D%2520scene%2520reconstruction%2520in%250Aboth%2520offline%2520and%2520incremental%2520evaluation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DoubleTake%3A%20Geometry%20Guided%20Depth%20Estimation&entry.906535625=Mohamed%20Sayed%20and%20Filippo%20Aleotti%20and%20Jamie%20Watson%20and%20Zawar%20Qureshi%20and%20Guillermo%20Garcia-Hernando%20and%20Gabriel%20Brostow%20and%20Sara%20Vicente%20and%20Michael%20Firman&entry.1292438233=%20%20Estimating%20depth%20from%20a%20sequence%20of%20posed%20RGB%20images%20is%20a%20fundamental%0Acomputer%20vision%20task%2C%20with%20applications%20in%20augmented%20reality%2C%20path%20planning%0Aetc.%20Prior%20work%20typically%20makes%20use%20of%20previous%20frames%20in%20a%20multi%20view%20stereo%0Aframework%2C%20relying%20on%20matching%20textures%20in%20a%20local%20neighborhood.%20In%20contrast%2C%0Aour%20model%20leverages%20historical%20predictions%20by%20giving%20the%20latest%203D%20geometry%0Adata%20as%20an%20extra%20input%20to%20our%20network.%20This%20self-generated%20geometric%20hint%20can%0Aencode%20information%20from%20areas%20of%20the%20scene%20not%20covered%20by%20the%20keyframes%20and%20it%0Ais%20more%20regularized%20when%20compared%20to%20individual%20predicted%20depth%20maps%20for%0Aprevious%20frames.%20We%20introduce%20a%20Hint%20MLP%20which%20combines%20cost%20volume%20features%0Awith%20a%20hint%20of%20the%20prior%20geometry%2C%20rendered%20as%20a%20depth%20map%20from%20the%20current%0Acamera%20location%2C%20together%20with%20a%20measure%20of%20the%20confidence%20in%20the%20prior%0Ageometry.%20We%20demonstrate%20that%20our%20method%2C%20which%20can%20run%20at%20interactive%20speeds%2C%0Aachieves%20state-of-the-art%20estimates%20of%20depth%20and%203D%20scene%20reconstruction%20in%0Aboth%20offline%20and%20incremental%20evaluation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18387v1&entry.124074799=Read"},
{"title": "Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP", "author": "Sedigheh Eslami and Gerard de Melo", "abstract": "  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering two\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? We design AlignCLIP, in\norder to answer these questions and show that answers to both questions are\npositive. Through extensive experiments, we show that AlignCLIP achieves\nnoticeable enhancements in the cross-modal alignment of the embeddings, and\nthereby, reduces the modality gap, while maintaining the performance across\nseveral downstream evaluations, such as zero-shot image classification,\nzero-shot multi-modal retrieval and zero-shot semantic text similarity.\n", "link": "http://arxiv.org/abs/2406.17639v2", "date": "2024-06-26", "relevancy": 2.8067, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6706}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5233}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP&body=Title%3A%20Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP%0AAuthor%3A%20Sedigheh%20Eslami%20and%20Gerard%20de%20Melo%0AAbstract%3A%20%20%20Contrastive%20Language--Image%20Pre-training%20%28CLIP%29%20has%20manifested%20remarkable%0Aimprovements%20in%20zero-shot%20classification%20and%20cross-modal%20vision-language%20tasks.%0AYet%2C%20from%20a%20geometrical%20point%20of%20view%2C%20the%20CLIP%20embedding%20space%20has%20been%20found%0Ato%20have%20a%20pronounced%20modality%20gap.%20This%20gap%20renders%20the%20embedding%20space%20overly%0Asparse%20and%20disconnected%2C%20with%20different%20modalities%20being%20densely%20distributed%20in%0Adistinct%20subregions%20of%20the%20hypersphere.%20In%20this%20work%2C%20we%20aim%20at%20answering%20two%0Amain%20questions%3A%201.%20Does%20sharing%20the%20parameter%20space%20between%20the%20multi-modal%0Aencoders%20reduce%20the%20modality%20gap%3F%202.%20Can%20the%20gap%20be%20mitigated%20by%20pushing%20apart%0Athe%20uni-modal%20embeddings%20via%20intra-modality%20separation%3F%20We%20design%20AlignCLIP%2C%20in%0Aorder%20to%20answer%20these%20questions%20and%20show%20that%20answers%20to%20both%20questions%20are%0Apositive.%20Through%20extensive%20experiments%2C%20we%20show%20that%20AlignCLIP%20achieves%0Anoticeable%20enhancements%20in%20the%20cross-modal%20alignment%20of%20the%20embeddings%2C%20and%0Athereby%2C%20reduces%20the%20modality%20gap%2C%20while%20maintaining%20the%20performance%20across%0Aseveral%20downstream%20evaluations%2C%20such%20as%20zero-shot%20image%20classification%2C%0Azero-shot%20multi-modal%20retrieval%20and%20zero-shot%20semantic%20text%20similarity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigate%2520the%2520Gap%253A%2520Investigating%2520Approaches%2520for%2520Improving%2520Cross-Modal%250A%2520%2520Alignment%2520in%2520CLIP%26entry.906535625%3DSedigheh%2520Eslami%2520and%2520Gerard%2520de%2520Melo%26entry.1292438233%3D%2520%2520Contrastive%2520Language--Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520manifested%2520remarkable%250Aimprovements%2520in%2520zero-shot%2520classification%2520and%2520cross-modal%2520vision-language%2520tasks.%250AYet%252C%2520from%2520a%2520geometrical%2520point%2520of%2520view%252C%2520the%2520CLIP%2520embedding%2520space%2520has%2520been%2520found%250Ato%2520have%2520a%2520pronounced%2520modality%2520gap.%2520This%2520gap%2520renders%2520the%2520embedding%2520space%2520overly%250Asparse%2520and%2520disconnected%252C%2520with%2520different%2520modalities%2520being%2520densely%2520distributed%2520in%250Adistinct%2520subregions%2520of%2520the%2520hypersphere.%2520In%2520this%2520work%252C%2520we%2520aim%2520at%2520answering%2520two%250Amain%2520questions%253A%25201.%2520Does%2520sharing%2520the%2520parameter%2520space%2520between%2520the%2520multi-modal%250Aencoders%2520reduce%2520the%2520modality%2520gap%253F%25202.%2520Can%2520the%2520gap%2520be%2520mitigated%2520by%2520pushing%2520apart%250Athe%2520uni-modal%2520embeddings%2520via%2520intra-modality%2520separation%253F%2520We%2520design%2520AlignCLIP%252C%2520in%250Aorder%2520to%2520answer%2520these%2520questions%2520and%2520show%2520that%2520answers%2520to%2520both%2520questions%2520are%250Apositive.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520AlignCLIP%2520achieves%250Anoticeable%2520enhancements%2520in%2520the%2520cross-modal%2520alignment%2520of%2520the%2520embeddings%252C%2520and%250Athereby%252C%2520reduces%2520the%2520modality%2520gap%252C%2520while%2520maintaining%2520the%2520performance%2520across%250Aseveral%2520downstream%2520evaluations%252C%2520such%2520as%2520zero-shot%2520image%2520classification%252C%250Azero-shot%2520multi-modal%2520retrieval%2520and%2520zero-shot%2520semantic%2520text%2520similarity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP&entry.906535625=Sedigheh%20Eslami%20and%20Gerard%20de%20Melo&entry.1292438233=%20%20Contrastive%20Language--Image%20Pre-training%20%28CLIP%29%20has%20manifested%20remarkable%0Aimprovements%20in%20zero-shot%20classification%20and%20cross-modal%20vision-language%20tasks.%0AYet%2C%20from%20a%20geometrical%20point%20of%20view%2C%20the%20CLIP%20embedding%20space%20has%20been%20found%0Ato%20have%20a%20pronounced%20modality%20gap.%20This%20gap%20renders%20the%20embedding%20space%20overly%0Asparse%20and%20disconnected%2C%20with%20different%20modalities%20being%20densely%20distributed%20in%0Adistinct%20subregions%20of%20the%20hypersphere.%20In%20this%20work%2C%20we%20aim%20at%20answering%20two%0Amain%20questions%3A%201.%20Does%20sharing%20the%20parameter%20space%20between%20the%20multi-modal%0Aencoders%20reduce%20the%20modality%20gap%3F%202.%20Can%20the%20gap%20be%20mitigated%20by%20pushing%20apart%0Athe%20uni-modal%20embeddings%20via%20intra-modality%20separation%3F%20We%20design%20AlignCLIP%2C%20in%0Aorder%20to%20answer%20these%20questions%20and%20show%20that%20answers%20to%20both%20questions%20are%0Apositive.%20Through%20extensive%20experiments%2C%20we%20show%20that%20AlignCLIP%20achieves%0Anoticeable%20enhancements%20in%20the%20cross-modal%20alignment%20of%20the%20embeddings%2C%20and%0Athereby%2C%20reduces%20the%20modality%20gap%2C%20while%20maintaining%20the%20performance%20across%0Aseveral%20downstream%20evaluations%2C%20such%20as%20zero-shot%20image%20classification%2C%0Azero-shot%20multi-modal%20retrieval%20and%20zero-shot%20semantic%20text%20similarity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17639v2&entry.124074799=Read"},
{"title": "Unsupervised Open-Vocabulary Object Localization in Videos", "author": "Ke Fan and Zechen Bai and Tianjun Xiao and Dominik Zietlow and Max Horn and Zixu Zhao and Carl-Johann Simon-Gabriel and Mike Zheng Shou and Francesco Locatello and Bernt Schiele and Thomas Brox and Zheng Zhang and Yanwei Fu and Tong He", "abstract": "  In this paper, we show that recent advances in video representation learning\nand pre-trained vision-language models allow for substantial improvements in\nself-supervised video object localization. We propose a method that first\nlocalizes objects in videos via an object-centric approach with slot attention\nand then assigns text to the obtained slots. The latter is achieved by an\nunsupervised way to read localized semantic information from the pre-trained\nCLIP model. The resulting video object localization is entirely unsupervised\napart from the implicit annotation contained in CLIP, and it is effectively the\nfirst unsupervised approach that yields good results on regular video\nbenchmarks.\n", "link": "http://arxiv.org/abs/2309.09858v2", "date": "2024-06-26", "relevancy": 2.7611, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.57}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5475}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Open-Vocabulary%20Object%20Localization%20in%20Videos&body=Title%3A%20Unsupervised%20Open-Vocabulary%20Object%20Localization%20in%20Videos%0AAuthor%3A%20Ke%20Fan%20and%20Zechen%20Bai%20and%20Tianjun%20Xiao%20and%20Dominik%20Zietlow%20and%20Max%20Horn%20and%20Zixu%20Zhao%20and%20Carl-Johann%20Simon-Gabriel%20and%20Mike%20Zheng%20Shou%20and%20Francesco%20Locatello%20and%20Bernt%20Schiele%20and%20Thomas%20Brox%20and%20Zheng%20Zhang%20and%20Yanwei%20Fu%20and%20Tong%20He%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20show%20that%20recent%20advances%20in%20video%20representation%20learning%0Aand%20pre-trained%20vision-language%20models%20allow%20for%20substantial%20improvements%20in%0Aself-supervised%20video%20object%20localization.%20We%20propose%20a%20method%20that%20first%0Alocalizes%20objects%20in%20videos%20via%20an%20object-centric%20approach%20with%20slot%20attention%0Aand%20then%20assigns%20text%20to%20the%20obtained%20slots.%20The%20latter%20is%20achieved%20by%20an%0Aunsupervised%20way%20to%20read%20localized%20semantic%20information%20from%20the%20pre-trained%0ACLIP%20model.%20The%20resulting%20video%20object%20localization%20is%20entirely%20unsupervised%0Aapart%20from%20the%20implicit%20annotation%20contained%20in%20CLIP%2C%20and%20it%20is%20effectively%20the%0Afirst%20unsupervised%20approach%20that%20yields%20good%20results%20on%20regular%20video%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Open-Vocabulary%2520Object%2520Localization%2520in%2520Videos%26entry.906535625%3DKe%2520Fan%2520and%2520Zechen%2520Bai%2520and%2520Tianjun%2520Xiao%2520and%2520Dominik%2520Zietlow%2520and%2520Max%2520Horn%2520and%2520Zixu%2520Zhao%2520and%2520Carl-Johann%2520Simon-Gabriel%2520and%2520Mike%2520Zheng%2520Shou%2520and%2520Francesco%2520Locatello%2520and%2520Bernt%2520Schiele%2520and%2520Thomas%2520Brox%2520and%2520Zheng%2520Zhang%2520and%2520Yanwei%2520Fu%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520recent%2520advances%2520in%2520video%2520representation%2520learning%250Aand%2520pre-trained%2520vision-language%2520models%2520allow%2520for%2520substantial%2520improvements%2520in%250Aself-supervised%2520video%2520object%2520localization.%2520We%2520propose%2520a%2520method%2520that%2520first%250Alocalizes%2520objects%2520in%2520videos%2520via%2520an%2520object-centric%2520approach%2520with%2520slot%2520attention%250Aand%2520then%2520assigns%2520text%2520to%2520the%2520obtained%2520slots.%2520The%2520latter%2520is%2520achieved%2520by%2520an%250Aunsupervised%2520way%2520to%2520read%2520localized%2520semantic%2520information%2520from%2520the%2520pre-trained%250ACLIP%2520model.%2520The%2520resulting%2520video%2520object%2520localization%2520is%2520entirely%2520unsupervised%250Aapart%2520from%2520the%2520implicit%2520annotation%2520contained%2520in%2520CLIP%252C%2520and%2520it%2520is%2520effectively%2520the%250Afirst%2520unsupervised%2520approach%2520that%2520yields%2520good%2520results%2520on%2520regular%2520video%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Open-Vocabulary%20Object%20Localization%20in%20Videos&entry.906535625=Ke%20Fan%20and%20Zechen%20Bai%20and%20Tianjun%20Xiao%20and%20Dominik%20Zietlow%20and%20Max%20Horn%20and%20Zixu%20Zhao%20and%20Carl-Johann%20Simon-Gabriel%20and%20Mike%20Zheng%20Shou%20and%20Francesco%20Locatello%20and%20Bernt%20Schiele%20and%20Thomas%20Brox%20and%20Zheng%20Zhang%20and%20Yanwei%20Fu%20and%20Tong%20He&entry.1292438233=%20%20In%20this%20paper%2C%20we%20show%20that%20recent%20advances%20in%20video%20representation%20learning%0Aand%20pre-trained%20vision-language%20models%20allow%20for%20substantial%20improvements%20in%0Aself-supervised%20video%20object%20localization.%20We%20propose%20a%20method%20that%20first%0Alocalizes%20objects%20in%20videos%20via%20an%20object-centric%20approach%20with%20slot%20attention%0Aand%20then%20assigns%20text%20to%20the%20obtained%20slots.%20The%20latter%20is%20achieved%20by%20an%0Aunsupervised%20way%20to%20read%20localized%20semantic%20information%20from%20the%20pre-trained%0ACLIP%20model.%20The%20resulting%20video%20object%20localization%20is%20entirely%20unsupervised%0Aapart%20from%20the%20implicit%20annotation%20contained%20in%20CLIP%2C%20and%20it%20is%20effectively%20the%0Afirst%20unsupervised%20approach%20that%20yields%20good%20results%20on%20regular%20video%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09858v2&entry.124074799=Read"},
{"title": "XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis", "author": "Hao Li and Ming Yuan and Yan Zhang and Chenming Wu and Chen Zhao and Chunyu Song and Haocheng Feng and Errui Ding and Dingwen Zhang and Jingdong Wang", "abstract": "  Thoroughly testing autonomy systems is crucial in the pursuit of safe\nautonomous driving vehicles. It necessitates creating safety-critical scenarios\nthat go beyond what can be safely collected from real-world data, as many of\nthese scenarios occur infrequently on public roads. However, the evaluation of\nmost existing NVS methods relies on sporadic sampling of image frames from the\ntraining data, comparing the rendered images with ground truth images using\nmetrics. Unfortunately, this evaluation protocol falls short of meeting the\nactual requirements in closed-loop simulations. Specifically, the true\napplication demands the capability to render novel views that extend beyond the\noriginal trajectory (such as cross-lane views), which are challenging to\ncapture in the real world. To address this, this paper presents a novel driving\nview synthesis dataset and benchmark specifically designed for autonomous\ndriving simulations. This dataset is unique as it includes testing images\ncaptured by deviating from the training trajectory by 1-4 meters. It comprises\nsix sequences encompassing various time and weather conditions. Each sequence\ncontains 450 training images, 150 testing images, and their corresponding\ncamera poses and intrinsic parameters. Leveraging this novel dataset, we\nestablish the first realistic benchmark for evaluating existing NVS approaches\nunder front-only and multi-camera settings. The experimental findings\nunderscore the significant gap that exists in current approaches, revealing\ntheir inadequate ability to fulfill the demanding prerequisites of cross-lane\nor closed-loop simulation. Our dataset is released publicly at the project\npage: https://3d-aigc.github.io/XLD/.\n", "link": "http://arxiv.org/abs/2406.18360v1", "date": "2024-06-26", "relevancy": 2.6965, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5435}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5372}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XLD%3A%20A%20Cross-Lane%20Dataset%20for%20Benchmarking%20Novel%20Driving%20View%20Synthesis&body=Title%3A%20XLD%3A%20A%20Cross-Lane%20Dataset%20for%20Benchmarking%20Novel%20Driving%20View%20Synthesis%0AAuthor%3A%20Hao%20Li%20and%20Ming%20Yuan%20and%20Yan%20Zhang%20and%20Chenming%20Wu%20and%20Chen%20Zhao%20and%20Chunyu%20Song%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Dingwen%20Zhang%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Thoroughly%20testing%20autonomy%20systems%20is%20crucial%20in%20the%20pursuit%20of%20safe%0Aautonomous%20driving%20vehicles.%20It%20necessitates%20creating%20safety-critical%20scenarios%0Athat%20go%20beyond%20what%20can%20be%20safely%20collected%20from%20real-world%20data%2C%20as%20many%20of%0Athese%20scenarios%20occur%20infrequently%20on%20public%20roads.%20However%2C%20the%20evaluation%20of%0Amost%20existing%20NVS%20methods%20relies%20on%20sporadic%20sampling%20of%20image%20frames%20from%20the%0Atraining%20data%2C%20comparing%20the%20rendered%20images%20with%20ground%20truth%20images%20using%0Ametrics.%20Unfortunately%2C%20this%20evaluation%20protocol%20falls%20short%20of%20meeting%20the%0Aactual%20requirements%20in%20closed-loop%20simulations.%20Specifically%2C%20the%20true%0Aapplication%20demands%20the%20capability%20to%20render%20novel%20views%20that%20extend%20beyond%20the%0Aoriginal%20trajectory%20%28such%20as%20cross-lane%20views%29%2C%20which%20are%20challenging%20to%0Acapture%20in%20the%20real%20world.%20To%20address%20this%2C%20this%20paper%20presents%20a%20novel%20driving%0Aview%20synthesis%20dataset%20and%20benchmark%20specifically%20designed%20for%20autonomous%0Adriving%20simulations.%20This%20dataset%20is%20unique%20as%20it%20includes%20testing%20images%0Acaptured%20by%20deviating%20from%20the%20training%20trajectory%20by%201-4%20meters.%20It%20comprises%0Asix%20sequences%20encompassing%20various%20time%20and%20weather%20conditions.%20Each%20sequence%0Acontains%20450%20training%20images%2C%20150%20testing%20images%2C%20and%20their%20corresponding%0Acamera%20poses%20and%20intrinsic%20parameters.%20Leveraging%20this%20novel%20dataset%2C%20we%0Aestablish%20the%20first%20realistic%20benchmark%20for%20evaluating%20existing%20NVS%20approaches%0Aunder%20front-only%20and%20multi-camera%20settings.%20The%20experimental%20findings%0Aunderscore%20the%20significant%20gap%20that%20exists%20in%20current%20approaches%2C%20revealing%0Atheir%20inadequate%20ability%20to%20fulfill%20the%20demanding%20prerequisites%20of%20cross-lane%0Aor%20closed-loop%20simulation.%20Our%20dataset%20is%20released%20publicly%20at%20the%20project%0Apage%3A%20https%3A//3d-aigc.github.io/XLD/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXLD%253A%2520A%2520Cross-Lane%2520Dataset%2520for%2520Benchmarking%2520Novel%2520Driving%2520View%2520Synthesis%26entry.906535625%3DHao%2520Li%2520and%2520Ming%2520Yuan%2520and%2520Yan%2520Zhang%2520and%2520Chenming%2520Wu%2520and%2520Chen%2520Zhao%2520and%2520Chunyu%2520Song%2520and%2520Haocheng%2520Feng%2520and%2520Errui%2520Ding%2520and%2520Dingwen%2520Zhang%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520Thoroughly%2520testing%2520autonomy%2520systems%2520is%2520crucial%2520in%2520the%2520pursuit%2520of%2520safe%250Aautonomous%2520driving%2520vehicles.%2520It%2520necessitates%2520creating%2520safety-critical%2520scenarios%250Athat%2520go%2520beyond%2520what%2520can%2520be%2520safely%2520collected%2520from%2520real-world%2520data%252C%2520as%2520many%2520of%250Athese%2520scenarios%2520occur%2520infrequently%2520on%2520public%2520roads.%2520However%252C%2520the%2520evaluation%2520of%250Amost%2520existing%2520NVS%2520methods%2520relies%2520on%2520sporadic%2520sampling%2520of%2520image%2520frames%2520from%2520the%250Atraining%2520data%252C%2520comparing%2520the%2520rendered%2520images%2520with%2520ground%2520truth%2520images%2520using%250Ametrics.%2520Unfortunately%252C%2520this%2520evaluation%2520protocol%2520falls%2520short%2520of%2520meeting%2520the%250Aactual%2520requirements%2520in%2520closed-loop%2520simulations.%2520Specifically%252C%2520the%2520true%250Aapplication%2520demands%2520the%2520capability%2520to%2520render%2520novel%2520views%2520that%2520extend%2520beyond%2520the%250Aoriginal%2520trajectory%2520%2528such%2520as%2520cross-lane%2520views%2529%252C%2520which%2520are%2520challenging%2520to%250Acapture%2520in%2520the%2520real%2520world.%2520To%2520address%2520this%252C%2520this%2520paper%2520presents%2520a%2520novel%2520driving%250Aview%2520synthesis%2520dataset%2520and%2520benchmark%2520specifically%2520designed%2520for%2520autonomous%250Adriving%2520simulations.%2520This%2520dataset%2520is%2520unique%2520as%2520it%2520includes%2520testing%2520images%250Acaptured%2520by%2520deviating%2520from%2520the%2520training%2520trajectory%2520by%25201-4%2520meters.%2520It%2520comprises%250Asix%2520sequences%2520encompassing%2520various%2520time%2520and%2520weather%2520conditions.%2520Each%2520sequence%250Acontains%2520450%2520training%2520images%252C%2520150%2520testing%2520images%252C%2520and%2520their%2520corresponding%250Acamera%2520poses%2520and%2520intrinsic%2520parameters.%2520Leveraging%2520this%2520novel%2520dataset%252C%2520we%250Aestablish%2520the%2520first%2520realistic%2520benchmark%2520for%2520evaluating%2520existing%2520NVS%2520approaches%250Aunder%2520front-only%2520and%2520multi-camera%2520settings.%2520The%2520experimental%2520findings%250Aunderscore%2520the%2520significant%2520gap%2520that%2520exists%2520in%2520current%2520approaches%252C%2520revealing%250Atheir%2520inadequate%2520ability%2520to%2520fulfill%2520the%2520demanding%2520prerequisites%2520of%2520cross-lane%250Aor%2520closed-loop%2520simulation.%2520Our%2520dataset%2520is%2520released%2520publicly%2520at%2520the%2520project%250Apage%253A%2520https%253A//3d-aigc.github.io/XLD/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XLD%3A%20A%20Cross-Lane%20Dataset%20for%20Benchmarking%20Novel%20Driving%20View%20Synthesis&entry.906535625=Hao%20Li%20and%20Ming%20Yuan%20and%20Yan%20Zhang%20and%20Chenming%20Wu%20and%20Chen%20Zhao%20and%20Chunyu%20Song%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Dingwen%20Zhang%20and%20Jingdong%20Wang&entry.1292438233=%20%20Thoroughly%20testing%20autonomy%20systems%20is%20crucial%20in%20the%20pursuit%20of%20safe%0Aautonomous%20driving%20vehicles.%20It%20necessitates%20creating%20safety-critical%20scenarios%0Athat%20go%20beyond%20what%20can%20be%20safely%20collected%20from%20real-world%20data%2C%20as%20many%20of%0Athese%20scenarios%20occur%20infrequently%20on%20public%20roads.%20However%2C%20the%20evaluation%20of%0Amost%20existing%20NVS%20methods%20relies%20on%20sporadic%20sampling%20of%20image%20frames%20from%20the%0Atraining%20data%2C%20comparing%20the%20rendered%20images%20with%20ground%20truth%20images%20using%0Ametrics.%20Unfortunately%2C%20this%20evaluation%20protocol%20falls%20short%20of%20meeting%20the%0Aactual%20requirements%20in%20closed-loop%20simulations.%20Specifically%2C%20the%20true%0Aapplication%20demands%20the%20capability%20to%20render%20novel%20views%20that%20extend%20beyond%20the%0Aoriginal%20trajectory%20%28such%20as%20cross-lane%20views%29%2C%20which%20are%20challenging%20to%0Acapture%20in%20the%20real%20world.%20To%20address%20this%2C%20this%20paper%20presents%20a%20novel%20driving%0Aview%20synthesis%20dataset%20and%20benchmark%20specifically%20designed%20for%20autonomous%0Adriving%20simulations.%20This%20dataset%20is%20unique%20as%20it%20includes%20testing%20images%0Acaptured%20by%20deviating%20from%20the%20training%20trajectory%20by%201-4%20meters.%20It%20comprises%0Asix%20sequences%20encompassing%20various%20time%20and%20weather%20conditions.%20Each%20sequence%0Acontains%20450%20training%20images%2C%20150%20testing%20images%2C%20and%20their%20corresponding%0Acamera%20poses%20and%20intrinsic%20parameters.%20Leveraging%20this%20novel%20dataset%2C%20we%0Aestablish%20the%20first%20realistic%20benchmark%20for%20evaluating%20existing%20NVS%20approaches%0Aunder%20front-only%20and%20multi-camera%20settings.%20The%20experimental%20findings%0Aunderscore%20the%20significant%20gap%20that%20exists%20in%20current%20approaches%2C%20revealing%0Atheir%20inadequate%20ability%20to%20fulfill%20the%20demanding%20prerequisites%20of%20cross-lane%0Aor%20closed-loop%20simulation.%20Our%20dataset%20is%20released%20publicly%20at%20the%20project%0Apage%3A%20https%3A//3d-aigc.github.io/XLD/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18360v1&entry.124074799=Read"},
{"title": "Spatial-temporal Hierarchical Reinforcement Learning for Interpretable\n  Pathology Image Super-Resolution", "author": "Wenting Chen and Jie Liu and Tommy W. S. Chow and Yixuan Yuan", "abstract": "  Pathology image are essential for accurately interpreting lesion cells in\ncytopathology screening, but acquiring high-resolution digital slides requires\nspecialized equipment and long scanning times. Though super-resolution (SR)\ntechniques can alleviate this problem, existing deep learning models recover\npathology image in a black-box manner, which can lead to untruthful biological\ndetails and misdiagnosis. Additionally, current methods allocate the same\ncomputational resources to recover each pixel of pathology image, leading to\nthe sub-optimal recovery issue due to the large variation of pathology image.\nIn this paper, we propose the first hierarchical reinforcement learning\nframework named Spatial-Temporal hierARchical Reinforcement Learning (STAR-RL),\nmainly for addressing the aforementioned issues in pathology image\nsuper-resolution problem. We reformulate the SR problem as a Markov decision\nprocess of interpretable operations and adopt the hierarchical recovery\nmechanism in patch level, to avoid sub-optimal recovery. Specifically, the\nhigher-level spatial manager is proposed to pick out the most corrupted patch\nfor the lower-level patch worker. Moreover, the higher-level temporal manager\nis advanced to evaluate the selected patch and determine whether the\noptimization should be stopped earlier, thereby avoiding the over-processed\nproblem. Under the guidance of spatial-temporal managers, the lower-level patch\nworker processes the selected patch with pixel-wise interpretable actions at\neach time step. Experimental results on medical images degraded by different\nkernels show the effectiveness of STAR-RL. Furthermore, STAR-RL validates the\npromotion in tumor diagnosis with a large margin and shows generalizability\nunder various degradations. The source code is available at\nhttps://github.com/CUHK-AIM-Group/STAR-RL.\n", "link": "http://arxiv.org/abs/2406.18310v1", "date": "2024-06-26", "relevancy": 2.6873, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-temporal%20Hierarchical%20Reinforcement%20Learning%20for%20Interpretable%0A%20%20Pathology%20Image%20Super-Resolution&body=Title%3A%20Spatial-temporal%20Hierarchical%20Reinforcement%20Learning%20for%20Interpretable%0A%20%20Pathology%20Image%20Super-Resolution%0AAuthor%3A%20Wenting%20Chen%20and%20Jie%20Liu%20and%20Tommy%20W.%20S.%20Chow%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20Pathology%20image%20are%20essential%20for%20accurately%20interpreting%20lesion%20cells%20in%0Acytopathology%20screening%2C%20but%20acquiring%20high-resolution%20digital%20slides%20requires%0Aspecialized%20equipment%20and%20long%20scanning%20times.%20Though%20super-resolution%20%28SR%29%0Atechniques%20can%20alleviate%20this%20problem%2C%20existing%20deep%20learning%20models%20recover%0Apathology%20image%20in%20a%20black-box%20manner%2C%20which%20can%20lead%20to%20untruthful%20biological%0Adetails%20and%20misdiagnosis.%20Additionally%2C%20current%20methods%20allocate%20the%20same%0Acomputational%20resources%20to%20recover%20each%20pixel%20of%20pathology%20image%2C%20leading%20to%0Athe%20sub-optimal%20recovery%20issue%20due%20to%20the%20large%20variation%20of%20pathology%20image.%0AIn%20this%20paper%2C%20we%20propose%20the%20first%20hierarchical%20reinforcement%20learning%0Aframework%20named%20Spatial-Temporal%20hierARchical%20Reinforcement%20Learning%20%28STAR-RL%29%2C%0Amainly%20for%20addressing%20the%20aforementioned%20issues%20in%20pathology%20image%0Asuper-resolution%20problem.%20We%20reformulate%20the%20SR%20problem%20as%20a%20Markov%20decision%0Aprocess%20of%20interpretable%20operations%20and%20adopt%20the%20hierarchical%20recovery%0Amechanism%20in%20patch%20level%2C%20to%20avoid%20sub-optimal%20recovery.%20Specifically%2C%20the%0Ahigher-level%20spatial%20manager%20is%20proposed%20to%20pick%20out%20the%20most%20corrupted%20patch%0Afor%20the%20lower-level%20patch%20worker.%20Moreover%2C%20the%20higher-level%20temporal%20manager%0Ais%20advanced%20to%20evaluate%20the%20selected%20patch%20and%20determine%20whether%20the%0Aoptimization%20should%20be%20stopped%20earlier%2C%20thereby%20avoiding%20the%20over-processed%0Aproblem.%20Under%20the%20guidance%20of%20spatial-temporal%20managers%2C%20the%20lower-level%20patch%0Aworker%20processes%20the%20selected%20patch%20with%20pixel-wise%20interpretable%20actions%20at%0Aeach%20time%20step.%20Experimental%20results%20on%20medical%20images%20degraded%20by%20different%0Akernels%20show%20the%20effectiveness%20of%20STAR-RL.%20Furthermore%2C%20STAR-RL%20validates%20the%0Apromotion%20in%20tumor%20diagnosis%20with%20a%20large%20margin%20and%20shows%20generalizability%0Aunder%20various%20degradations.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/CUHK-AIM-Group/STAR-RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-temporal%2520Hierarchical%2520Reinforcement%2520Learning%2520for%2520Interpretable%250A%2520%2520Pathology%2520Image%2520Super-Resolution%26entry.906535625%3DWenting%2520Chen%2520and%2520Jie%2520Liu%2520and%2520Tommy%2520W.%2520S.%2520Chow%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520Pathology%2520image%2520are%2520essential%2520for%2520accurately%2520interpreting%2520lesion%2520cells%2520in%250Acytopathology%2520screening%252C%2520but%2520acquiring%2520high-resolution%2520digital%2520slides%2520requires%250Aspecialized%2520equipment%2520and%2520long%2520scanning%2520times.%2520Though%2520super-resolution%2520%2528SR%2529%250Atechniques%2520can%2520alleviate%2520this%2520problem%252C%2520existing%2520deep%2520learning%2520models%2520recover%250Apathology%2520image%2520in%2520a%2520black-box%2520manner%252C%2520which%2520can%2520lead%2520to%2520untruthful%2520biological%250Adetails%2520and%2520misdiagnosis.%2520Additionally%252C%2520current%2520methods%2520allocate%2520the%2520same%250Acomputational%2520resources%2520to%2520recover%2520each%2520pixel%2520of%2520pathology%2520image%252C%2520leading%2520to%250Athe%2520sub-optimal%2520recovery%2520issue%2520due%2520to%2520the%2520large%2520variation%2520of%2520pathology%2520image.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%2520hierarchical%2520reinforcement%2520learning%250Aframework%2520named%2520Spatial-Temporal%2520hierARchical%2520Reinforcement%2520Learning%2520%2528STAR-RL%2529%252C%250Amainly%2520for%2520addressing%2520the%2520aforementioned%2520issues%2520in%2520pathology%2520image%250Asuper-resolution%2520problem.%2520We%2520reformulate%2520the%2520SR%2520problem%2520as%2520a%2520Markov%2520decision%250Aprocess%2520of%2520interpretable%2520operations%2520and%2520adopt%2520the%2520hierarchical%2520recovery%250Amechanism%2520in%2520patch%2520level%252C%2520to%2520avoid%2520sub-optimal%2520recovery.%2520Specifically%252C%2520the%250Ahigher-level%2520spatial%2520manager%2520is%2520proposed%2520to%2520pick%2520out%2520the%2520most%2520corrupted%2520patch%250Afor%2520the%2520lower-level%2520patch%2520worker.%2520Moreover%252C%2520the%2520higher-level%2520temporal%2520manager%250Ais%2520advanced%2520to%2520evaluate%2520the%2520selected%2520patch%2520and%2520determine%2520whether%2520the%250Aoptimization%2520should%2520be%2520stopped%2520earlier%252C%2520thereby%2520avoiding%2520the%2520over-processed%250Aproblem.%2520Under%2520the%2520guidance%2520of%2520spatial-temporal%2520managers%252C%2520the%2520lower-level%2520patch%250Aworker%2520processes%2520the%2520selected%2520patch%2520with%2520pixel-wise%2520interpretable%2520actions%2520at%250Aeach%2520time%2520step.%2520Experimental%2520results%2520on%2520medical%2520images%2520degraded%2520by%2520different%250Akernels%2520show%2520the%2520effectiveness%2520of%2520STAR-RL.%2520Furthermore%252C%2520STAR-RL%2520validates%2520the%250Apromotion%2520in%2520tumor%2520diagnosis%2520with%2520a%2520large%2520margin%2520and%2520shows%2520generalizability%250Aunder%2520various%2520degradations.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/CUHK-AIM-Group/STAR-RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-temporal%20Hierarchical%20Reinforcement%20Learning%20for%20Interpretable%0A%20%20Pathology%20Image%20Super-Resolution&entry.906535625=Wenting%20Chen%20and%20Jie%20Liu%20and%20Tommy%20W.%20S.%20Chow%20and%20Yixuan%20Yuan&entry.1292438233=%20%20Pathology%20image%20are%20essential%20for%20accurately%20interpreting%20lesion%20cells%20in%0Acytopathology%20screening%2C%20but%20acquiring%20high-resolution%20digital%20slides%20requires%0Aspecialized%20equipment%20and%20long%20scanning%20times.%20Though%20super-resolution%20%28SR%29%0Atechniques%20can%20alleviate%20this%20problem%2C%20existing%20deep%20learning%20models%20recover%0Apathology%20image%20in%20a%20black-box%20manner%2C%20which%20can%20lead%20to%20untruthful%20biological%0Adetails%20and%20misdiagnosis.%20Additionally%2C%20current%20methods%20allocate%20the%20same%0Acomputational%20resources%20to%20recover%20each%20pixel%20of%20pathology%20image%2C%20leading%20to%0Athe%20sub-optimal%20recovery%20issue%20due%20to%20the%20large%20variation%20of%20pathology%20image.%0AIn%20this%20paper%2C%20we%20propose%20the%20first%20hierarchical%20reinforcement%20learning%0Aframework%20named%20Spatial-Temporal%20hierARchical%20Reinforcement%20Learning%20%28STAR-RL%29%2C%0Amainly%20for%20addressing%20the%20aforementioned%20issues%20in%20pathology%20image%0Asuper-resolution%20problem.%20We%20reformulate%20the%20SR%20problem%20as%20a%20Markov%20decision%0Aprocess%20of%20interpretable%20operations%20and%20adopt%20the%20hierarchical%20recovery%0Amechanism%20in%20patch%20level%2C%20to%20avoid%20sub-optimal%20recovery.%20Specifically%2C%20the%0Ahigher-level%20spatial%20manager%20is%20proposed%20to%20pick%20out%20the%20most%20corrupted%20patch%0Afor%20the%20lower-level%20patch%20worker.%20Moreover%2C%20the%20higher-level%20temporal%20manager%0Ais%20advanced%20to%20evaluate%20the%20selected%20patch%20and%20determine%20whether%20the%0Aoptimization%20should%20be%20stopped%20earlier%2C%20thereby%20avoiding%20the%20over-processed%0Aproblem.%20Under%20the%20guidance%20of%20spatial-temporal%20managers%2C%20the%20lower-level%20patch%0Aworker%20processes%20the%20selected%20patch%20with%20pixel-wise%20interpretable%20actions%20at%0Aeach%20time%20step.%20Experimental%20results%20on%20medical%20images%20degraded%20by%20different%0Akernels%20show%20the%20effectiveness%20of%20STAR-RL.%20Furthermore%2C%20STAR-RL%20validates%20the%0Apromotion%20in%20tumor%20diagnosis%20with%20a%20large%20margin%20and%20shows%20generalizability%0Aunder%20various%20degradations.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/CUHK-AIM-Group/STAR-RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18310v1&entry.124074799=Read"},
{"title": "On Scaling Up 3D Gaussian Splatting Training", "author": "Hexu Zhao and Haoyang Weng and Daohan Lu and Ang Li and Jinyang Li and Aurojit Panda and Saining Xie", "abstract": "  3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction\ndue to its superior visual quality and rendering speed. However, 3DGS training\ncurrently occurs on a single GPU, limiting its ability to handle\nhigh-resolution and large-scale 3D reconstruction tasks due to memory\nconstraints. We introduce Grendel, a distributed system designed to partition\n3DGS parameters and parallelize computation across multiple GPUs. As each\nGaussian affects a small, dynamic subset of rendered pixels, Grendel employs\nsparse all-to-all communication to transfer the necessary Gaussians to pixel\npartitions and performs dynamic load balancing. Unlike existing 3DGS systems\nthat train using one camera view image at a time, Grendel supports batched\ntraining with multiple views. We explore various optimization hyperparameter\nscaling strategies and find that a simple sqrt(batch size) scaling rule is\nhighly effective. Evaluations using large-scale, high-resolution scenes show\nthat Grendel enhances rendering quality by scaling up 3DGS parameters across\nmultiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by\ndistributing 40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28\nusing 11.2 million Gaussians on a single GPU. Grendel is an open-source project\navailable at: https://github.com/nyu-systems/Grendel-GS\n", "link": "http://arxiv.org/abs/2406.18533v1", "date": "2024-06-26", "relevancy": 2.6497, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7254}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6513}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Scaling%20Up%203D%20Gaussian%20Splatting%20Training&body=Title%3A%20On%20Scaling%20Up%203D%20Gaussian%20Splatting%20Training%0AAuthor%3A%20Hexu%20Zhao%20and%20Haoyang%20Weng%20and%20Daohan%20Lu%20and%20Ang%20Li%20and%20Jinyang%20Li%20and%20Aurojit%20Panda%20and%20Saining%20Xie%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20increasingly%20popular%20for%203D%20reconstruction%0Adue%20to%20its%20superior%20visual%20quality%20and%20rendering%20speed.%20However%2C%203DGS%20training%0Acurrently%20occurs%20on%20a%20single%20GPU%2C%20limiting%20its%20ability%20to%20handle%0Ahigh-resolution%20and%20large-scale%203D%20reconstruction%20tasks%20due%20to%20memory%0Aconstraints.%20We%20introduce%20Grendel%2C%20a%20distributed%20system%20designed%20to%20partition%0A3DGS%20parameters%20and%20parallelize%20computation%20across%20multiple%20GPUs.%20As%20each%0AGaussian%20affects%20a%20small%2C%20dynamic%20subset%20of%20rendered%20pixels%2C%20Grendel%20employs%0Asparse%20all-to-all%20communication%20to%20transfer%20the%20necessary%20Gaussians%20to%20pixel%0Apartitions%20and%20performs%20dynamic%20load%20balancing.%20Unlike%20existing%203DGS%20systems%0Athat%20train%20using%20one%20camera%20view%20image%20at%20a%20time%2C%20Grendel%20supports%20batched%0Atraining%20with%20multiple%20views.%20We%20explore%20various%20optimization%20hyperparameter%0Ascaling%20strategies%20and%20find%20that%20a%20simple%20sqrt%28batch%20size%29%20scaling%20rule%20is%0Ahighly%20effective.%20Evaluations%20using%20large-scale%2C%20high-resolution%20scenes%20show%0Athat%20Grendel%20enhances%20rendering%20quality%20by%20scaling%20up%203DGS%20parameters%20across%0Amultiple%20GPUs.%20On%20the%20Rubble%20dataset%2C%20we%20achieve%20a%20test%20PSNR%20of%2027.28%20by%0Adistributing%2040.4%20million%20Gaussians%20across%2016%20GPUs%2C%20compared%20to%20a%20PSNR%20of%2026.28%0Ausing%2011.2%20million%20Gaussians%20on%20a%20single%20GPU.%20Grendel%20is%20an%20open-source%20project%0Aavailable%20at%3A%20https%3A//github.com/nyu-systems/Grendel-GS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Scaling%2520Up%25203D%2520Gaussian%2520Splatting%2520Training%26entry.906535625%3DHexu%2520Zhao%2520and%2520Haoyang%2520Weng%2520and%2520Daohan%2520Lu%2520and%2520Ang%2520Li%2520and%2520Jinyang%2520Li%2520and%2520Aurojit%2520Panda%2520and%2520Saining%2520Xie%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520increasingly%2520popular%2520for%25203D%2520reconstruction%250Adue%2520to%2520its%2520superior%2520visual%2520quality%2520and%2520rendering%2520speed.%2520However%252C%25203DGS%2520training%250Acurrently%2520occurs%2520on%2520a%2520single%2520GPU%252C%2520limiting%2520its%2520ability%2520to%2520handle%250Ahigh-resolution%2520and%2520large-scale%25203D%2520reconstruction%2520tasks%2520due%2520to%2520memory%250Aconstraints.%2520We%2520introduce%2520Grendel%252C%2520a%2520distributed%2520system%2520designed%2520to%2520partition%250A3DGS%2520parameters%2520and%2520parallelize%2520computation%2520across%2520multiple%2520GPUs.%2520As%2520each%250AGaussian%2520affects%2520a%2520small%252C%2520dynamic%2520subset%2520of%2520rendered%2520pixels%252C%2520Grendel%2520employs%250Asparse%2520all-to-all%2520communication%2520to%2520transfer%2520the%2520necessary%2520Gaussians%2520to%2520pixel%250Apartitions%2520and%2520performs%2520dynamic%2520load%2520balancing.%2520Unlike%2520existing%25203DGS%2520systems%250Athat%2520train%2520using%2520one%2520camera%2520view%2520image%2520at%2520a%2520time%252C%2520Grendel%2520supports%2520batched%250Atraining%2520with%2520multiple%2520views.%2520We%2520explore%2520various%2520optimization%2520hyperparameter%250Ascaling%2520strategies%2520and%2520find%2520that%2520a%2520simple%2520sqrt%2528batch%2520size%2529%2520scaling%2520rule%2520is%250Ahighly%2520effective.%2520Evaluations%2520using%2520large-scale%252C%2520high-resolution%2520scenes%2520show%250Athat%2520Grendel%2520enhances%2520rendering%2520quality%2520by%2520scaling%2520up%25203DGS%2520parameters%2520across%250Amultiple%2520GPUs.%2520On%2520the%2520Rubble%2520dataset%252C%2520we%2520achieve%2520a%2520test%2520PSNR%2520of%252027.28%2520by%250Adistributing%252040.4%2520million%2520Gaussians%2520across%252016%2520GPUs%252C%2520compared%2520to%2520a%2520PSNR%2520of%252026.28%250Ausing%252011.2%2520million%2520Gaussians%2520on%2520a%2520single%2520GPU.%2520Grendel%2520is%2520an%2520open-source%2520project%250Aavailable%2520at%253A%2520https%253A//github.com/nyu-systems/Grendel-GS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Scaling%20Up%203D%20Gaussian%20Splatting%20Training&entry.906535625=Hexu%20Zhao%20and%20Haoyang%20Weng%20and%20Daohan%20Lu%20and%20Ang%20Li%20and%20Jinyang%20Li%20and%20Aurojit%20Panda%20and%20Saining%20Xie&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20increasingly%20popular%20for%203D%20reconstruction%0Adue%20to%20its%20superior%20visual%20quality%20and%20rendering%20speed.%20However%2C%203DGS%20training%0Acurrently%20occurs%20on%20a%20single%20GPU%2C%20limiting%20its%20ability%20to%20handle%0Ahigh-resolution%20and%20large-scale%203D%20reconstruction%20tasks%20due%20to%20memory%0Aconstraints.%20We%20introduce%20Grendel%2C%20a%20distributed%20system%20designed%20to%20partition%0A3DGS%20parameters%20and%20parallelize%20computation%20across%20multiple%20GPUs.%20As%20each%0AGaussian%20affects%20a%20small%2C%20dynamic%20subset%20of%20rendered%20pixels%2C%20Grendel%20employs%0Asparse%20all-to-all%20communication%20to%20transfer%20the%20necessary%20Gaussians%20to%20pixel%0Apartitions%20and%20performs%20dynamic%20load%20balancing.%20Unlike%20existing%203DGS%20systems%0Athat%20train%20using%20one%20camera%20view%20image%20at%20a%20time%2C%20Grendel%20supports%20batched%0Atraining%20with%20multiple%20views.%20We%20explore%20various%20optimization%20hyperparameter%0Ascaling%20strategies%20and%20find%20that%20a%20simple%20sqrt%28batch%20size%29%20scaling%20rule%20is%0Ahighly%20effective.%20Evaluations%20using%20large-scale%2C%20high-resolution%20scenes%20show%0Athat%20Grendel%20enhances%20rendering%20quality%20by%20scaling%20up%203DGS%20parameters%20across%0Amultiple%20GPUs.%20On%20the%20Rubble%20dataset%2C%20we%20achieve%20a%20test%20PSNR%20of%2027.28%20by%0Adistributing%2040.4%20million%20Gaussians%20across%2016%20GPUs%2C%20compared%20to%20a%20PSNR%20of%2026.28%0Ausing%2011.2%20million%20Gaussians%20on%20a%20single%20GPU.%20Grendel%20is%20an%20open-source%20project%0Aavailable%20at%3A%20https%3A//github.com/nyu-systems/Grendel-GS%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18533v1&entry.124074799=Read"},
{"title": "MultiDiff: Consistent Novel View Synthesis from a Single Image", "author": "Norman M\u00fcller and Katja Schwarz and Barbara Roessle and Lorenzo Porzi and Samuel Rota Bul\u00f2 and Matthias Nie\u00dfner and Peter Kontschieder", "abstract": "  We introduce MultiDiff, a novel approach for consistent novel view synthesis\nof scenes from a single RGB image. The task of synthesizing novel views from a\nsingle reference image is highly ill-posed by nature, as there exist multiple,\nplausible explanations for unobserved areas. To address this issue, we\nincorporate strong priors in form of monocular depth predictors and\nvideo-diffusion models. Monocular depth enables us to condition our model on\nwarped reference images for the target views, increasing geometric stability.\nThe video-diffusion prior provides a strong proxy for 3D scenes, allowing the\nmodel to learn continuous and pixel-accurate correspondences across generated\nimages. In contrast to approaches relying on autoregressive image generation\nthat are prone to drifts and error accumulation, MultiDiff jointly synthesizes\na sequence of frames yielding high-quality and multi-view consistent results --\neven for long-term scene generation with large camera movements, while reducing\ninference time by an order of magnitude. For additional consistency and image\nquality improvements, we introduce a novel, structured noise distribution. Our\nexperimental results demonstrate that MultiDiff outperforms state-of-the-art\nmethods on the challenging, real-world datasets RealEstate10K and ScanNet.\nFinally, our model naturally supports multi-view consistent editing without the\nneed for further tuning.\n", "link": "http://arxiv.org/abs/2406.18524v1", "date": "2024-06-26", "relevancy": 2.6434, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6706}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6706}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiDiff%3A%20Consistent%20Novel%20View%20Synthesis%20from%20a%20Single%20Image&body=Title%3A%20MultiDiff%3A%20Consistent%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%0AAuthor%3A%20Norman%20M%C3%BCller%20and%20Katja%20Schwarz%20and%20Barbara%20Roessle%20and%20Lorenzo%20Porzi%20and%20Samuel%20Rota%20Bul%C3%B2%20and%20Matthias%20Nie%C3%9Fner%20and%20Peter%20Kontschieder%0AAbstract%3A%20%20%20We%20introduce%20MultiDiff%2C%20a%20novel%20approach%20for%20consistent%20novel%20view%20synthesis%0Aof%20scenes%20from%20a%20single%20RGB%20image.%20The%20task%20of%20synthesizing%20novel%20views%20from%20a%0Asingle%20reference%20image%20is%20highly%20ill-posed%20by%20nature%2C%20as%20there%20exist%20multiple%2C%0Aplausible%20explanations%20for%20unobserved%20areas.%20To%20address%20this%20issue%2C%20we%0Aincorporate%20strong%20priors%20in%20form%20of%20monocular%20depth%20predictors%20and%0Avideo-diffusion%20models.%20Monocular%20depth%20enables%20us%20to%20condition%20our%20model%20on%0Awarped%20reference%20images%20for%20the%20target%20views%2C%20increasing%20geometric%20stability.%0AThe%20video-diffusion%20prior%20provides%20a%20strong%20proxy%20for%203D%20scenes%2C%20allowing%20the%0Amodel%20to%20learn%20continuous%20and%20pixel-accurate%20correspondences%20across%20generated%0Aimages.%20In%20contrast%20to%20approaches%20relying%20on%20autoregressive%20image%20generation%0Athat%20are%20prone%20to%20drifts%20and%20error%20accumulation%2C%20MultiDiff%20jointly%20synthesizes%0Aa%20sequence%20of%20frames%20yielding%20high-quality%20and%20multi-view%20consistent%20results%20--%0Aeven%20for%20long-term%20scene%20generation%20with%20large%20camera%20movements%2C%20while%20reducing%0Ainference%20time%20by%20an%20order%20of%20magnitude.%20For%20additional%20consistency%20and%20image%0Aquality%20improvements%2C%20we%20introduce%20a%20novel%2C%20structured%20noise%20distribution.%20Our%0Aexperimental%20results%20demonstrate%20that%20MultiDiff%20outperforms%20state-of-the-art%0Amethods%20on%20the%20challenging%2C%20real-world%20datasets%20RealEstate10K%20and%20ScanNet.%0AFinally%2C%20our%20model%20naturally%20supports%20multi-view%20consistent%20editing%20without%20the%0Aneed%20for%20further%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiDiff%253A%2520Consistent%2520Novel%2520View%2520Synthesis%2520from%2520a%2520Single%2520Image%26entry.906535625%3DNorman%2520M%25C3%25BCller%2520and%2520Katja%2520Schwarz%2520and%2520Barbara%2520Roessle%2520and%2520Lorenzo%2520Porzi%2520and%2520Samuel%2520Rota%2520Bul%25C3%25B2%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Peter%2520Kontschieder%26entry.1292438233%3D%2520%2520We%2520introduce%2520MultiDiff%252C%2520a%2520novel%2520approach%2520for%2520consistent%2520novel%2520view%2520synthesis%250Aof%2520scenes%2520from%2520a%2520single%2520RGB%2520image.%2520The%2520task%2520of%2520synthesizing%2520novel%2520views%2520from%2520a%250Asingle%2520reference%2520image%2520is%2520highly%2520ill-posed%2520by%2520nature%252C%2520as%2520there%2520exist%2520multiple%252C%250Aplausible%2520explanations%2520for%2520unobserved%2520areas.%2520To%2520address%2520this%2520issue%252C%2520we%250Aincorporate%2520strong%2520priors%2520in%2520form%2520of%2520monocular%2520depth%2520predictors%2520and%250Avideo-diffusion%2520models.%2520Monocular%2520depth%2520enables%2520us%2520to%2520condition%2520our%2520model%2520on%250Awarped%2520reference%2520images%2520for%2520the%2520target%2520views%252C%2520increasing%2520geometric%2520stability.%250AThe%2520video-diffusion%2520prior%2520provides%2520a%2520strong%2520proxy%2520for%25203D%2520scenes%252C%2520allowing%2520the%250Amodel%2520to%2520learn%2520continuous%2520and%2520pixel-accurate%2520correspondences%2520across%2520generated%250Aimages.%2520In%2520contrast%2520to%2520approaches%2520relying%2520on%2520autoregressive%2520image%2520generation%250Athat%2520are%2520prone%2520to%2520drifts%2520and%2520error%2520accumulation%252C%2520MultiDiff%2520jointly%2520synthesizes%250Aa%2520sequence%2520of%2520frames%2520yielding%2520high-quality%2520and%2520multi-view%2520consistent%2520results%2520--%250Aeven%2520for%2520long-term%2520scene%2520generation%2520with%2520large%2520camera%2520movements%252C%2520while%2520reducing%250Ainference%2520time%2520by%2520an%2520order%2520of%2520magnitude.%2520For%2520additional%2520consistency%2520and%2520image%250Aquality%2520improvements%252C%2520we%2520introduce%2520a%2520novel%252C%2520structured%2520noise%2520distribution.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520MultiDiff%2520outperforms%2520state-of-the-art%250Amethods%2520on%2520the%2520challenging%252C%2520real-world%2520datasets%2520RealEstate10K%2520and%2520ScanNet.%250AFinally%252C%2520our%2520model%2520naturally%2520supports%2520multi-view%2520consistent%2520editing%2520without%2520the%250Aneed%2520for%2520further%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiDiff%3A%20Consistent%20Novel%20View%20Synthesis%20from%20a%20Single%20Image&entry.906535625=Norman%20M%C3%BCller%20and%20Katja%20Schwarz%20and%20Barbara%20Roessle%20and%20Lorenzo%20Porzi%20and%20Samuel%20Rota%20Bul%C3%B2%20and%20Matthias%20Nie%C3%9Fner%20and%20Peter%20Kontschieder&entry.1292438233=%20%20We%20introduce%20MultiDiff%2C%20a%20novel%20approach%20for%20consistent%20novel%20view%20synthesis%0Aof%20scenes%20from%20a%20single%20RGB%20image.%20The%20task%20of%20synthesizing%20novel%20views%20from%20a%0Asingle%20reference%20image%20is%20highly%20ill-posed%20by%20nature%2C%20as%20there%20exist%20multiple%2C%0Aplausible%20explanations%20for%20unobserved%20areas.%20To%20address%20this%20issue%2C%20we%0Aincorporate%20strong%20priors%20in%20form%20of%20monocular%20depth%20predictors%20and%0Avideo-diffusion%20models.%20Monocular%20depth%20enables%20us%20to%20condition%20our%20model%20on%0Awarped%20reference%20images%20for%20the%20target%20views%2C%20increasing%20geometric%20stability.%0AThe%20video-diffusion%20prior%20provides%20a%20strong%20proxy%20for%203D%20scenes%2C%20allowing%20the%0Amodel%20to%20learn%20continuous%20and%20pixel-accurate%20correspondences%20across%20generated%0Aimages.%20In%20contrast%20to%20approaches%20relying%20on%20autoregressive%20image%20generation%0Athat%20are%20prone%20to%20drifts%20and%20error%20accumulation%2C%20MultiDiff%20jointly%20synthesizes%0Aa%20sequence%20of%20frames%20yielding%20high-quality%20and%20multi-view%20consistent%20results%20--%0Aeven%20for%20long-term%20scene%20generation%20with%20large%20camera%20movements%2C%20while%20reducing%0Ainference%20time%20by%20an%20order%20of%20magnitude.%20For%20additional%20consistency%20and%20image%0Aquality%20improvements%2C%20we%20introduce%20a%20novel%2C%20structured%20noise%20distribution.%20Our%0Aexperimental%20results%20demonstrate%20that%20MultiDiff%20outperforms%20state-of-the-art%0Amethods%20on%20the%20challenging%2C%20real-world%20datasets%20RealEstate10K%20and%20ScanNet.%0AFinally%2C%20our%20model%20naturally%20supports%20multi-view%20consistent%20editing%20without%20the%0Aneed%20for%20further%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18524v1&entry.124074799=Read"},
{"title": "AlignedCut: Visual Concepts Discovery on Brain-Guided Universal Feature\n  Space", "author": "Huzheng Yang and James Gee and Jianbo Shi", "abstract": "  We study the intriguing connection between visual data, deep networks, and\nthe brain. Our method creates a universal channel alignment by using brain\nvoxel fMRI response prediction as the training objective. We discover that deep\nnetworks, trained with different objectives, share common feature channels\nacross various models. These channels can be clustered into recurring sets,\ncorresponding to distinct brain regions, indicating the formation of visual\nconcepts. Tracing the clusters of channel responses onto the images, we see\nsemantically meaningful object segments emerge, even without any supervised\ndecoder. Furthermore, the universal feature alignment and the clustering of\nchannels produce a picture and quantification of how visual information is\nprocessed through the different network layers, which produces precise\ncomparisons between the networks.\n", "link": "http://arxiv.org/abs/2406.18344v1", "date": "2024-06-26", "relevancy": 2.629, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5421}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5333}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignedCut%3A%20Visual%20Concepts%20Discovery%20on%20Brain-Guided%20Universal%20Feature%0A%20%20Space&body=Title%3A%20AlignedCut%3A%20Visual%20Concepts%20Discovery%20on%20Brain-Guided%20Universal%20Feature%0A%20%20Space%0AAuthor%3A%20Huzheng%20Yang%20and%20James%20Gee%20and%20Jianbo%20Shi%0AAbstract%3A%20%20%20We%20study%20the%20intriguing%20connection%20between%20visual%20data%2C%20deep%20networks%2C%20and%0Athe%20brain.%20Our%20method%20creates%20a%20universal%20channel%20alignment%20by%20using%20brain%0Avoxel%20fMRI%20response%20prediction%20as%20the%20training%20objective.%20We%20discover%20that%20deep%0Anetworks%2C%20trained%20with%20different%20objectives%2C%20share%20common%20feature%20channels%0Aacross%20various%20models.%20These%20channels%20can%20be%20clustered%20into%20recurring%20sets%2C%0Acorresponding%20to%20distinct%20brain%20regions%2C%20indicating%20the%20formation%20of%20visual%0Aconcepts.%20Tracing%20the%20clusters%20of%20channel%20responses%20onto%20the%20images%2C%20we%20see%0Asemantically%20meaningful%20object%20segments%20emerge%2C%20even%20without%20any%20supervised%0Adecoder.%20Furthermore%2C%20the%20universal%20feature%20alignment%20and%20the%20clustering%20of%0Achannels%20produce%20a%20picture%20and%20quantification%20of%20how%20visual%20information%20is%0Aprocessed%20through%20the%20different%20network%20layers%2C%20which%20produces%20precise%0Acomparisons%20between%20the%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignedCut%253A%2520Visual%2520Concepts%2520Discovery%2520on%2520Brain-Guided%2520Universal%2520Feature%250A%2520%2520Space%26entry.906535625%3DHuzheng%2520Yang%2520and%2520James%2520Gee%2520and%2520Jianbo%2520Shi%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520intriguing%2520connection%2520between%2520visual%2520data%252C%2520deep%2520networks%252C%2520and%250Athe%2520brain.%2520Our%2520method%2520creates%2520a%2520universal%2520channel%2520alignment%2520by%2520using%2520brain%250Avoxel%2520fMRI%2520response%2520prediction%2520as%2520the%2520training%2520objective.%2520We%2520discover%2520that%2520deep%250Anetworks%252C%2520trained%2520with%2520different%2520objectives%252C%2520share%2520common%2520feature%2520channels%250Aacross%2520various%2520models.%2520These%2520channels%2520can%2520be%2520clustered%2520into%2520recurring%2520sets%252C%250Acorresponding%2520to%2520distinct%2520brain%2520regions%252C%2520indicating%2520the%2520formation%2520of%2520visual%250Aconcepts.%2520Tracing%2520the%2520clusters%2520of%2520channel%2520responses%2520onto%2520the%2520images%252C%2520we%2520see%250Asemantically%2520meaningful%2520object%2520segments%2520emerge%252C%2520even%2520without%2520any%2520supervised%250Adecoder.%2520Furthermore%252C%2520the%2520universal%2520feature%2520alignment%2520and%2520the%2520clustering%2520of%250Achannels%2520produce%2520a%2520picture%2520and%2520quantification%2520of%2520how%2520visual%2520information%2520is%250Aprocessed%2520through%2520the%2520different%2520network%2520layers%252C%2520which%2520produces%2520precise%250Acomparisons%2520between%2520the%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignedCut%3A%20Visual%20Concepts%20Discovery%20on%20Brain-Guided%20Universal%20Feature%0A%20%20Space&entry.906535625=Huzheng%20Yang%20and%20James%20Gee%20and%20Jianbo%20Shi&entry.1292438233=%20%20We%20study%20the%20intriguing%20connection%20between%20visual%20data%2C%20deep%20networks%2C%20and%0Athe%20brain.%20Our%20method%20creates%20a%20universal%20channel%20alignment%20by%20using%20brain%0Avoxel%20fMRI%20response%20prediction%20as%20the%20training%20objective.%20We%20discover%20that%20deep%0Anetworks%2C%20trained%20with%20different%20objectives%2C%20share%20common%20feature%20channels%0Aacross%20various%20models.%20These%20channels%20can%20be%20clustered%20into%20recurring%20sets%2C%0Acorresponding%20to%20distinct%20brain%20regions%2C%20indicating%20the%20formation%20of%20visual%0Aconcepts.%20Tracing%20the%20clusters%20of%20channel%20responses%20onto%20the%20images%2C%20we%20see%0Asemantically%20meaningful%20object%20segments%20emerge%2C%20even%20without%20any%20supervised%0Adecoder.%20Furthermore%2C%20the%20universal%20feature%20alignment%20and%20the%20clustering%20of%0Achannels%20produce%20a%20picture%20and%20quantification%20of%20how%20visual%20information%20is%0Aprocessed%20through%20the%20different%20network%20layers%2C%20which%20produces%20precise%0Acomparisons%20between%20the%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18344v1&entry.124074799=Read"},
{"title": "Splatter a Video: Video Gaussian Representation for Versatile Processing", "author": "Yang-Tian Sun and Yi-Hua Huang and Lin Ma and Xiaoyang Lyu and Yan-Pei Cao and Xiaojuan Qi", "abstract": "  Video representation is a long-standing problem that is crucial for various\ndown-stream tasks, such as tracking,depth prediction,segmentation,view\nsynthesis,and editing. However, current methods either struggle to model\ncomplex motions due to the absence of 3D structure or rely on implicit 3D\nrepresentations that are ill-suited for manipulation tasks. To address these\nchallenges, we introduce a novel explicit 3D representation-video Gaussian\nrepresentation -- that embeds a video into 3D Gaussians. Our proposed\nrepresentation models video appearance in a 3D canonical space using explicit\nGaussians as proxies and associates each Gaussian with 3D motions for video\nmotion. This approach offers a more intrinsic and explicit representation than\nlayered atlas or volumetric pixel matrices. To obtain such a representation, we\ndistill 2D priors, such as optical flow and depth, from foundation models to\nregularize learning in this ill-posed setting. Extensive applications\ndemonstrate the versatility of our new video representation. It has been proven\neffective in numerous video processing tasks, including tracking, consistent\nvideo depth and feature refinement, motion and appearance editing, and\nstereoscopic video generation. Project page:\nhttps://sunyangtian.github.io/spatter_a_video_web/\n", "link": "http://arxiv.org/abs/2406.13870v2", "date": "2024-06-26", "relevancy": 2.6283, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6675}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6581}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splatter%20a%20Video%3A%20Video%20Gaussian%20Representation%20for%20Versatile%20Processing&body=Title%3A%20Splatter%20a%20Video%3A%20Video%20Gaussian%20Representation%20for%20Versatile%20Processing%0AAuthor%3A%20Yang-Tian%20Sun%20and%20Yi-Hua%20Huang%20and%20Lin%20Ma%20and%20Xiaoyang%20Lyu%20and%20Yan-Pei%20Cao%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20Video%20representation%20is%20a%20long-standing%20problem%20that%20is%20crucial%20for%20various%0Adown-stream%20tasks%2C%20such%20as%20tracking%2Cdepth%20prediction%2Csegmentation%2Cview%0Asynthesis%2Cand%20editing.%20However%2C%20current%20methods%20either%20struggle%20to%20model%0Acomplex%20motions%20due%20to%20the%20absence%20of%203D%20structure%20or%20rely%20on%20implicit%203D%0Arepresentations%20that%20are%20ill-suited%20for%20manipulation%20tasks.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20explicit%203D%20representation-video%20Gaussian%0Arepresentation%20--%20that%20embeds%20a%20video%20into%203D%20Gaussians.%20Our%20proposed%0Arepresentation%20models%20video%20appearance%20in%20a%203D%20canonical%20space%20using%20explicit%0AGaussians%20as%20proxies%20and%20associates%20each%20Gaussian%20with%203D%20motions%20for%20video%0Amotion.%20This%20approach%20offers%20a%20more%20intrinsic%20and%20explicit%20representation%20than%0Alayered%20atlas%20or%20volumetric%20pixel%20matrices.%20To%20obtain%20such%20a%20representation%2C%20we%0Adistill%202D%20priors%2C%20such%20as%20optical%20flow%20and%20depth%2C%20from%20foundation%20models%20to%0Aregularize%20learning%20in%20this%20ill-posed%20setting.%20Extensive%20applications%0Ademonstrate%20the%20versatility%20of%20our%20new%20video%20representation.%20It%20has%20been%20proven%0Aeffective%20in%20numerous%20video%20processing%20tasks%2C%20including%20tracking%2C%20consistent%0Avideo%20depth%20and%20feature%20refinement%2C%20motion%20and%20appearance%20editing%2C%20and%0Astereoscopic%20video%20generation.%20Project%20page%3A%0Ahttps%3A//sunyangtian.github.io/spatter_a_video_web/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatter%2520a%2520Video%253A%2520Video%2520Gaussian%2520Representation%2520for%2520Versatile%2520Processing%26entry.906535625%3DYang-Tian%2520Sun%2520and%2520Yi-Hua%2520Huang%2520and%2520Lin%2520Ma%2520and%2520Xiaoyang%2520Lyu%2520and%2520Yan-Pei%2520Cao%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520Video%2520representation%2520is%2520a%2520long-standing%2520problem%2520that%2520is%2520crucial%2520for%2520various%250Adown-stream%2520tasks%252C%2520such%2520as%2520tracking%252Cdepth%2520prediction%252Csegmentation%252Cview%250Asynthesis%252Cand%2520editing.%2520However%252C%2520current%2520methods%2520either%2520struggle%2520to%2520model%250Acomplex%2520motions%2520due%2520to%2520the%2520absence%2520of%25203D%2520structure%2520or%2520rely%2520on%2520implicit%25203D%250Arepresentations%2520that%2520are%2520ill-suited%2520for%2520manipulation%2520tasks.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520a%2520novel%2520explicit%25203D%2520representation-video%2520Gaussian%250Arepresentation%2520--%2520that%2520embeds%2520a%2520video%2520into%25203D%2520Gaussians.%2520Our%2520proposed%250Arepresentation%2520models%2520video%2520appearance%2520in%2520a%25203D%2520canonical%2520space%2520using%2520explicit%250AGaussians%2520as%2520proxies%2520and%2520associates%2520each%2520Gaussian%2520with%25203D%2520motions%2520for%2520video%250Amotion.%2520This%2520approach%2520offers%2520a%2520more%2520intrinsic%2520and%2520explicit%2520representation%2520than%250Alayered%2520atlas%2520or%2520volumetric%2520pixel%2520matrices.%2520To%2520obtain%2520such%2520a%2520representation%252C%2520we%250Adistill%25202D%2520priors%252C%2520such%2520as%2520optical%2520flow%2520and%2520depth%252C%2520from%2520foundation%2520models%2520to%250Aregularize%2520learning%2520in%2520this%2520ill-posed%2520setting.%2520Extensive%2520applications%250Ademonstrate%2520the%2520versatility%2520of%2520our%2520new%2520video%2520representation.%2520It%2520has%2520been%2520proven%250Aeffective%2520in%2520numerous%2520video%2520processing%2520tasks%252C%2520including%2520tracking%252C%2520consistent%250Avideo%2520depth%2520and%2520feature%2520refinement%252C%2520motion%2520and%2520appearance%2520editing%252C%2520and%250Astereoscopic%2520video%2520generation.%2520Project%2520page%253A%250Ahttps%253A//sunyangtian.github.io/spatter_a_video_web/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splatter%20a%20Video%3A%20Video%20Gaussian%20Representation%20for%20Versatile%20Processing&entry.906535625=Yang-Tian%20Sun%20and%20Yi-Hua%20Huang%20and%20Lin%20Ma%20and%20Xiaoyang%20Lyu%20and%20Yan-Pei%20Cao%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20Video%20representation%20is%20a%20long-standing%20problem%20that%20is%20crucial%20for%20various%0Adown-stream%20tasks%2C%20such%20as%20tracking%2Cdepth%20prediction%2Csegmentation%2Cview%0Asynthesis%2Cand%20editing.%20However%2C%20current%20methods%20either%20struggle%20to%20model%0Acomplex%20motions%20due%20to%20the%20absence%20of%203D%20structure%20or%20rely%20on%20implicit%203D%0Arepresentations%20that%20are%20ill-suited%20for%20manipulation%20tasks.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20explicit%203D%20representation-video%20Gaussian%0Arepresentation%20--%20that%20embeds%20a%20video%20into%203D%20Gaussians.%20Our%20proposed%0Arepresentation%20models%20video%20appearance%20in%20a%203D%20canonical%20space%20using%20explicit%0AGaussians%20as%20proxies%20and%20associates%20each%20Gaussian%20with%203D%20motions%20for%20video%0Amotion.%20This%20approach%20offers%20a%20more%20intrinsic%20and%20explicit%20representation%20than%0Alayered%20atlas%20or%20volumetric%20pixel%20matrices.%20To%20obtain%20such%20a%20representation%2C%20we%0Adistill%202D%20priors%2C%20such%20as%20optical%20flow%20and%20depth%2C%20from%20foundation%20models%20to%0Aregularize%20learning%20in%20this%20ill-posed%20setting.%20Extensive%20applications%0Ademonstrate%20the%20versatility%20of%20our%20new%20video%20representation.%20It%20has%20been%20proven%0Aeffective%20in%20numerous%20video%20processing%20tasks%2C%20including%20tracking%2C%20consistent%0Avideo%20depth%20and%20feature%20refinement%2C%20motion%20and%20appearance%20editing%2C%20and%0Astereoscopic%20video%20generation.%20Project%20page%3A%0Ahttps%3A//sunyangtian.github.io/spatter_a_video_web/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13870v2&entry.124074799=Read"},
{"title": "Generalized Deepfake Attribution", "author": "Sowdagar Mahammad Shahid and Sudev Kumar Padhi and Umesh Kashyap and Sk. Subidh Ali", "abstract": "  The landscape of fake media creation changed with the introduction of\nGenerative Adversarial Networks (GAN s). Fake media creation has been on the\nrise with the rapid advances in generation technology, leading to new\nchallenges in Detecting fake media. A fundamental characteristic of GAN s is\ntheir sensitivity to parameter initialization, known as seeds. Each distinct\nseed utilized during training leads to the creation of unique model instances,\nresulting in divergent image outputs despite employing the same architecture.\nThis means that even if we have one GAN architecture, it can produce countless\nvariations of GAN models depending on the seed used. Existing methods for\nattributing deepfakes work well only if they have seen the specific GAN model\nduring training. If the GAN architectures are retrained with a different seed,\nthese methods struggle to attribute the fakes. This seed dependency issue made\nit difficult to attribute deepfakes with existing methods. We proposed a\ngeneralized deepfake attribution network (GDA-N et) to attribute fake images to\ntheir respective GAN architectures, even if they are generated from a retrained\nversion of the GAN architecture with a different seed (cross-seed) or from the\nfine-tuned version of the existing GAN model. Extensive experiments on\ncross-seed and fine-tuned data of GAN models show that our method is highly\neffective compared to existing methods. We have provided the source code to\nvalidate our results.\n", "link": "http://arxiv.org/abs/2406.18278v1", "date": "2024-06-26", "relevancy": 2.577, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5274}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5237}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Deepfake%20Attribution&body=Title%3A%20Generalized%20Deepfake%20Attribution%0AAuthor%3A%20Sowdagar%20Mahammad%20Shahid%20and%20Sudev%20Kumar%20Padhi%20and%20Umesh%20Kashyap%20and%20Sk.%20Subidh%20Ali%0AAbstract%3A%20%20%20The%20landscape%20of%20fake%20media%20creation%20changed%20with%20the%20introduction%20of%0AGenerative%20Adversarial%20Networks%20%28GAN%20s%29.%20Fake%20media%20creation%20has%20been%20on%20the%0Arise%20with%20the%20rapid%20advances%20in%20generation%20technology%2C%20leading%20to%20new%0Achallenges%20in%20Detecting%20fake%20media.%20A%20fundamental%20characteristic%20of%20GAN%20s%20is%0Atheir%20sensitivity%20to%20parameter%20initialization%2C%20known%20as%20seeds.%20Each%20distinct%0Aseed%20utilized%20during%20training%20leads%20to%20the%20creation%20of%20unique%20model%20instances%2C%0Aresulting%20in%20divergent%20image%20outputs%20despite%20employing%20the%20same%20architecture.%0AThis%20means%20that%20even%20if%20we%20have%20one%20GAN%20architecture%2C%20it%20can%20produce%20countless%0Avariations%20of%20GAN%20models%20depending%20on%20the%20seed%20used.%20Existing%20methods%20for%0Aattributing%20deepfakes%20work%20well%20only%20if%20they%20have%20seen%20the%20specific%20GAN%20model%0Aduring%20training.%20If%20the%20GAN%20architectures%20are%20retrained%20with%20a%20different%20seed%2C%0Athese%20methods%20struggle%20to%20attribute%20the%20fakes.%20This%20seed%20dependency%20issue%20made%0Ait%20difficult%20to%20attribute%20deepfakes%20with%20existing%20methods.%20We%20proposed%20a%0Ageneralized%20deepfake%20attribution%20network%20%28GDA-N%20et%29%20to%20attribute%20fake%20images%20to%0Atheir%20respective%20GAN%20architectures%2C%20even%20if%20they%20are%20generated%20from%20a%20retrained%0Aversion%20of%20the%20GAN%20architecture%20with%20a%20different%20seed%20%28cross-seed%29%20or%20from%20the%0Afine-tuned%20version%20of%20the%20existing%20GAN%20model.%20Extensive%20experiments%20on%0Across-seed%20and%20fine-tuned%20data%20of%20GAN%20models%20show%20that%20our%20method%20is%20highly%0Aeffective%20compared%20to%20existing%20methods.%20We%20have%20provided%20the%20source%20code%20to%0Avalidate%20our%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Deepfake%2520Attribution%26entry.906535625%3DSowdagar%2520Mahammad%2520Shahid%2520and%2520Sudev%2520Kumar%2520Padhi%2520and%2520Umesh%2520Kashyap%2520and%2520Sk.%2520Subidh%2520Ali%26entry.1292438233%3D%2520%2520The%2520landscape%2520of%2520fake%2520media%2520creation%2520changed%2520with%2520the%2520introduction%2520of%250AGenerative%2520Adversarial%2520Networks%2520%2528GAN%2520s%2529.%2520Fake%2520media%2520creation%2520has%2520been%2520on%2520the%250Arise%2520with%2520the%2520rapid%2520advances%2520in%2520generation%2520technology%252C%2520leading%2520to%2520new%250Achallenges%2520in%2520Detecting%2520fake%2520media.%2520A%2520fundamental%2520characteristic%2520of%2520GAN%2520s%2520is%250Atheir%2520sensitivity%2520to%2520parameter%2520initialization%252C%2520known%2520as%2520seeds.%2520Each%2520distinct%250Aseed%2520utilized%2520during%2520training%2520leads%2520to%2520the%2520creation%2520of%2520unique%2520model%2520instances%252C%250Aresulting%2520in%2520divergent%2520image%2520outputs%2520despite%2520employing%2520the%2520same%2520architecture.%250AThis%2520means%2520that%2520even%2520if%2520we%2520have%2520one%2520GAN%2520architecture%252C%2520it%2520can%2520produce%2520countless%250Avariations%2520of%2520GAN%2520models%2520depending%2520on%2520the%2520seed%2520used.%2520Existing%2520methods%2520for%250Aattributing%2520deepfakes%2520work%2520well%2520only%2520if%2520they%2520have%2520seen%2520the%2520specific%2520GAN%2520model%250Aduring%2520training.%2520If%2520the%2520GAN%2520architectures%2520are%2520retrained%2520with%2520a%2520different%2520seed%252C%250Athese%2520methods%2520struggle%2520to%2520attribute%2520the%2520fakes.%2520This%2520seed%2520dependency%2520issue%2520made%250Ait%2520difficult%2520to%2520attribute%2520deepfakes%2520with%2520existing%2520methods.%2520We%2520proposed%2520a%250Ageneralized%2520deepfake%2520attribution%2520network%2520%2528GDA-N%2520et%2529%2520to%2520attribute%2520fake%2520images%2520to%250Atheir%2520respective%2520GAN%2520architectures%252C%2520even%2520if%2520they%2520are%2520generated%2520from%2520a%2520retrained%250Aversion%2520of%2520the%2520GAN%2520architecture%2520with%2520a%2520different%2520seed%2520%2528cross-seed%2529%2520or%2520from%2520the%250Afine-tuned%2520version%2520of%2520the%2520existing%2520GAN%2520model.%2520Extensive%2520experiments%2520on%250Across-seed%2520and%2520fine-tuned%2520data%2520of%2520GAN%2520models%2520show%2520that%2520our%2520method%2520is%2520highly%250Aeffective%2520compared%2520to%2520existing%2520methods.%2520We%2520have%2520provided%2520the%2520source%2520code%2520to%250Avalidate%2520our%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Deepfake%20Attribution&entry.906535625=Sowdagar%20Mahammad%20Shahid%20and%20Sudev%20Kumar%20Padhi%20and%20Umesh%20Kashyap%20and%20Sk.%20Subidh%20Ali&entry.1292438233=%20%20The%20landscape%20of%20fake%20media%20creation%20changed%20with%20the%20introduction%20of%0AGenerative%20Adversarial%20Networks%20%28GAN%20s%29.%20Fake%20media%20creation%20has%20been%20on%20the%0Arise%20with%20the%20rapid%20advances%20in%20generation%20technology%2C%20leading%20to%20new%0Achallenges%20in%20Detecting%20fake%20media.%20A%20fundamental%20characteristic%20of%20GAN%20s%20is%0Atheir%20sensitivity%20to%20parameter%20initialization%2C%20known%20as%20seeds.%20Each%20distinct%0Aseed%20utilized%20during%20training%20leads%20to%20the%20creation%20of%20unique%20model%20instances%2C%0Aresulting%20in%20divergent%20image%20outputs%20despite%20employing%20the%20same%20architecture.%0AThis%20means%20that%20even%20if%20we%20have%20one%20GAN%20architecture%2C%20it%20can%20produce%20countless%0Avariations%20of%20GAN%20models%20depending%20on%20the%20seed%20used.%20Existing%20methods%20for%0Aattributing%20deepfakes%20work%20well%20only%20if%20they%20have%20seen%20the%20specific%20GAN%20model%0Aduring%20training.%20If%20the%20GAN%20architectures%20are%20retrained%20with%20a%20different%20seed%2C%0Athese%20methods%20struggle%20to%20attribute%20the%20fakes.%20This%20seed%20dependency%20issue%20made%0Ait%20difficult%20to%20attribute%20deepfakes%20with%20existing%20methods.%20We%20proposed%20a%0Ageneralized%20deepfake%20attribution%20network%20%28GDA-N%20et%29%20to%20attribute%20fake%20images%20to%0Atheir%20respective%20GAN%20architectures%2C%20even%20if%20they%20are%20generated%20from%20a%20retrained%0Aversion%20of%20the%20GAN%20architecture%20with%20a%20different%20seed%20%28cross-seed%29%20or%20from%20the%0Afine-tuned%20version%20of%20the%20existing%20GAN%20model.%20Extensive%20experiments%20on%0Across-seed%20and%20fine-tuned%20data%20of%20GAN%20models%20show%20that%20our%20method%20is%20highly%0Aeffective%20compared%20to%20existing%20methods.%20We%20have%20provided%20the%20source%20code%20to%0Avalidate%20our%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18278v1&entry.124074799=Read"},
{"title": "General Distribution Learning: A theoretical framework for Deep Learning", "author": "Binchuan Qi and Li Li and Wei Gong", "abstract": "  There remain numerous unanswered research questions on deep learning (DL)\nwithin the classical learning theory framework. These include the remarkable\ngeneralization capabilities of overparametrized neural networks (NNs), the\nefficient optimization performance despite non-convexity of objectives, the\nmechanism of flat minima for generalization, and the exceptional performance of\ndeep architectures in solving physical problems. This paper introduces General\nDistribution Learning (GD Learning), a novel theoretical learning framework\ndesigned to address a comprehensive range of machine learning and statistical\ntasks, including classification, regression and parameter estimation. Departing\nfrom traditional statistical machine learning, GD Learning focuses on the true\nunderlying distribution. In GD Learning, learning error, corresponding to the\nexpected error in classical statistical learning framework, is divided into\nfitting errors due to models and algorithms, as well as sampling errors\nintroduced by limited sampling data. The framework significantly incorporates\nprior knowledge, especially in scenarios characterized by data scarcity,\nthereby enhancing performance. Within the GD Learning framework, we demonstrate\nthat the global optimal solutions in non-convex optimization can be approached\nby minimizing the gradient norm and the non-uniformity of the eigenvalues of\nthe model's Jacobian matrix. This insight leads to the development of the\ngradient structure control algorithm. GD Learning also offers fresh insights\ninto the questions on deep learning, including overparameterization and\nnon-convex optimization, bias-variance trade-off, and the mechanism of flat\nminima.\n", "link": "http://arxiv.org/abs/2406.05666v4", "date": "2024-06-26", "relevancy": 2.5554, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5237}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5102}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Distribution%20Learning%3A%20A%20theoretical%20framework%20for%20Deep%20Learning&body=Title%3A%20General%20Distribution%20Learning%3A%20A%20theoretical%20framework%20for%20Deep%20Learning%0AAuthor%3A%20Binchuan%20Qi%20and%20Li%20Li%20and%20Wei%20Gong%0AAbstract%3A%20%20%20There%20remain%20numerous%20unanswered%20research%20questions%20on%20deep%20learning%20%28DL%29%0Awithin%20the%20classical%20learning%20theory%20framework.%20These%20include%20the%20remarkable%0Ageneralization%20capabilities%20of%20overparametrized%20neural%20networks%20%28NNs%29%2C%20the%0Aefficient%20optimization%20performance%20despite%20non-convexity%20of%20objectives%2C%20the%0Amechanism%20of%20flat%20minima%20for%20generalization%2C%20and%20the%20exceptional%20performance%20of%0Adeep%20architectures%20in%20solving%20physical%20problems.%20This%20paper%20introduces%20General%0ADistribution%20Learning%20%28GD%20Learning%29%2C%20a%20novel%20theoretical%20learning%20framework%0Adesigned%20to%20address%20a%20comprehensive%20range%20of%20machine%20learning%20and%20statistical%0Atasks%2C%20including%20classification%2C%20regression%20and%20parameter%20estimation.%20Departing%0Afrom%20traditional%20statistical%20machine%20learning%2C%20GD%20Learning%20focuses%20on%20the%20true%0Aunderlying%20distribution.%20In%20GD%20Learning%2C%20learning%20error%2C%20corresponding%20to%20the%0Aexpected%20error%20in%20classical%20statistical%20learning%20framework%2C%20is%20divided%20into%0Afitting%20errors%20due%20to%20models%20and%20algorithms%2C%20as%20well%20as%20sampling%20errors%0Aintroduced%20by%20limited%20sampling%20data.%20The%20framework%20significantly%20incorporates%0Aprior%20knowledge%2C%20especially%20in%20scenarios%20characterized%20by%20data%20scarcity%2C%0Athereby%20enhancing%20performance.%20Within%20the%20GD%20Learning%20framework%2C%20we%20demonstrate%0Athat%20the%20global%20optimal%20solutions%20in%20non-convex%20optimization%20can%20be%20approached%0Aby%20minimizing%20the%20gradient%20norm%20and%20the%20non-uniformity%20of%20the%20eigenvalues%20of%0Athe%20model%27s%20Jacobian%20matrix.%20This%20insight%20leads%20to%20the%20development%20of%20the%0Agradient%20structure%20control%20algorithm.%20GD%20Learning%20also%20offers%20fresh%20insights%0Ainto%20the%20questions%20on%20deep%20learning%2C%20including%20overparameterization%20and%0Anon-convex%20optimization%2C%20bias-variance%20trade-off%2C%20and%20the%20mechanism%20of%20flat%0Aminima.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05666v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Distribution%2520Learning%253A%2520A%2520theoretical%2520framework%2520for%2520Deep%2520Learning%26entry.906535625%3DBinchuan%2520Qi%2520and%2520Li%2520Li%2520and%2520Wei%2520Gong%26entry.1292438233%3D%2520%2520There%2520remain%2520numerous%2520unanswered%2520research%2520questions%2520on%2520deep%2520learning%2520%2528DL%2529%250Awithin%2520the%2520classical%2520learning%2520theory%2520framework.%2520These%2520include%2520the%2520remarkable%250Ageneralization%2520capabilities%2520of%2520overparametrized%2520neural%2520networks%2520%2528NNs%2529%252C%2520the%250Aefficient%2520optimization%2520performance%2520despite%2520non-convexity%2520of%2520objectives%252C%2520the%250Amechanism%2520of%2520flat%2520minima%2520for%2520generalization%252C%2520and%2520the%2520exceptional%2520performance%2520of%250Adeep%2520architectures%2520in%2520solving%2520physical%2520problems.%2520This%2520paper%2520introduces%2520General%250ADistribution%2520Learning%2520%2528GD%2520Learning%2529%252C%2520a%2520novel%2520theoretical%2520learning%2520framework%250Adesigned%2520to%2520address%2520a%2520comprehensive%2520range%2520of%2520machine%2520learning%2520and%2520statistical%250Atasks%252C%2520including%2520classification%252C%2520regression%2520and%2520parameter%2520estimation.%2520Departing%250Afrom%2520traditional%2520statistical%2520machine%2520learning%252C%2520GD%2520Learning%2520focuses%2520on%2520the%2520true%250Aunderlying%2520distribution.%2520In%2520GD%2520Learning%252C%2520learning%2520error%252C%2520corresponding%2520to%2520the%250Aexpected%2520error%2520in%2520classical%2520statistical%2520learning%2520framework%252C%2520is%2520divided%2520into%250Afitting%2520errors%2520due%2520to%2520models%2520and%2520algorithms%252C%2520as%2520well%2520as%2520sampling%2520errors%250Aintroduced%2520by%2520limited%2520sampling%2520data.%2520The%2520framework%2520significantly%2520incorporates%250Aprior%2520knowledge%252C%2520especially%2520in%2520scenarios%2520characterized%2520by%2520data%2520scarcity%252C%250Athereby%2520enhancing%2520performance.%2520Within%2520the%2520GD%2520Learning%2520framework%252C%2520we%2520demonstrate%250Athat%2520the%2520global%2520optimal%2520solutions%2520in%2520non-convex%2520optimization%2520can%2520be%2520approached%250Aby%2520minimizing%2520the%2520gradient%2520norm%2520and%2520the%2520non-uniformity%2520of%2520the%2520eigenvalues%2520of%250Athe%2520model%2527s%2520Jacobian%2520matrix.%2520This%2520insight%2520leads%2520to%2520the%2520development%2520of%2520the%250Agradient%2520structure%2520control%2520algorithm.%2520GD%2520Learning%2520also%2520offers%2520fresh%2520insights%250Ainto%2520the%2520questions%2520on%2520deep%2520learning%252C%2520including%2520overparameterization%2520and%250Anon-convex%2520optimization%252C%2520bias-variance%2520trade-off%252C%2520and%2520the%2520mechanism%2520of%2520flat%250Aminima.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05666v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Distribution%20Learning%3A%20A%20theoretical%20framework%20for%20Deep%20Learning&entry.906535625=Binchuan%20Qi%20and%20Li%20Li%20and%20Wei%20Gong&entry.1292438233=%20%20There%20remain%20numerous%20unanswered%20research%20questions%20on%20deep%20learning%20%28DL%29%0Awithin%20the%20classical%20learning%20theory%20framework.%20These%20include%20the%20remarkable%0Ageneralization%20capabilities%20of%20overparametrized%20neural%20networks%20%28NNs%29%2C%20the%0Aefficient%20optimization%20performance%20despite%20non-convexity%20of%20objectives%2C%20the%0Amechanism%20of%20flat%20minima%20for%20generalization%2C%20and%20the%20exceptional%20performance%20of%0Adeep%20architectures%20in%20solving%20physical%20problems.%20This%20paper%20introduces%20General%0ADistribution%20Learning%20%28GD%20Learning%29%2C%20a%20novel%20theoretical%20learning%20framework%0Adesigned%20to%20address%20a%20comprehensive%20range%20of%20machine%20learning%20and%20statistical%0Atasks%2C%20including%20classification%2C%20regression%20and%20parameter%20estimation.%20Departing%0Afrom%20traditional%20statistical%20machine%20learning%2C%20GD%20Learning%20focuses%20on%20the%20true%0Aunderlying%20distribution.%20In%20GD%20Learning%2C%20learning%20error%2C%20corresponding%20to%20the%0Aexpected%20error%20in%20classical%20statistical%20learning%20framework%2C%20is%20divided%20into%0Afitting%20errors%20due%20to%20models%20and%20algorithms%2C%20as%20well%20as%20sampling%20errors%0Aintroduced%20by%20limited%20sampling%20data.%20The%20framework%20significantly%20incorporates%0Aprior%20knowledge%2C%20especially%20in%20scenarios%20characterized%20by%20data%20scarcity%2C%0Athereby%20enhancing%20performance.%20Within%20the%20GD%20Learning%20framework%2C%20we%20demonstrate%0Athat%20the%20global%20optimal%20solutions%20in%20non-convex%20optimization%20can%20be%20approached%0Aby%20minimizing%20the%20gradient%20norm%20and%20the%20non-uniformity%20of%20the%20eigenvalues%20of%0Athe%20model%27s%20Jacobian%20matrix.%20This%20insight%20leads%20to%20the%20development%20of%20the%0Agradient%20structure%20control%20algorithm.%20GD%20Learning%20also%20offers%20fresh%20insights%0Ainto%20the%20questions%20on%20deep%20learning%2C%20including%20overparameterization%20and%0Anon-convex%20optimization%2C%20bias-variance%20trade-off%2C%20and%20the%20mechanism%20of%20flat%0Aminima.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05666v4&entry.124074799=Read"},
{"title": "Weisfeiler Leman for Euclidean Equivariant Machine Learning", "author": "Snir Hordan and Tal Amir and Nadav Dym", "abstract": "  The $k$-Weisfeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a\ncommon method for assessing the expressive power of graph neural networks\n(GNNs). Recently, GNNs whose expressive power is equivalent to the $2$-WL test\nwere proven to be universal on weighted graphs which encode $3\\mathrm{D}$ point\ncloud data, yet this result is limited to invariant continuous functions on\npoint clouds. In this paper, we extend this result in three ways: Firstly, we\nshow that PPGN can simulate $2$-WL uniformly on all point clouds with low\ncomplexity. Secondly, we show that $2$-WL tests can be extended to point clouds\nwhich include both positions and velocities, a scenario often encountered in\napplications. Finally, we provide a general framework for proving equivariant\nuniversality and leverage it to prove that a simple modification of this\ninvariant PPGN architecture can be used to obtain a universal equivariant\narchitecture that can approximate all continuous equivariant functions\nuniformly. Building on our results, we develop our WeLNet architecture, which\nsets new state-of-the-art results on the N-Body dynamics task and the GEOM-QM9\nmolecular conformation generation task.\n", "link": "http://arxiv.org/abs/2402.02484v3", "date": "2024-06-26", "relevancy": 2.5475, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5254}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5093}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weisfeiler%20Leman%20for%20Euclidean%20Equivariant%20Machine%20Learning&body=Title%3A%20Weisfeiler%20Leman%20for%20Euclidean%20Equivariant%20Machine%20Learning%0AAuthor%3A%20Snir%20Hordan%20and%20Tal%20Amir%20and%20Nadav%20Dym%0AAbstract%3A%20%20%20The%20%24k%24-Weisfeiler-Leman%20%28%24k%24-WL%29%20graph%20isomorphism%20test%20hierarchy%20is%20a%0Acommon%20method%20for%20assessing%20the%20expressive%20power%20of%20graph%20neural%20networks%0A%28GNNs%29.%20Recently%2C%20GNNs%20whose%20expressive%20power%20is%20equivalent%20to%20the%20%242%24-WL%20test%0Awere%20proven%20to%20be%20universal%20on%20weighted%20graphs%20which%20encode%20%243%5Cmathrm%7BD%7D%24%20point%0Acloud%20data%2C%20yet%20this%20result%20is%20limited%20to%20invariant%20continuous%20functions%20on%0Apoint%20clouds.%20In%20this%20paper%2C%20we%20extend%20this%20result%20in%20three%20ways%3A%20Firstly%2C%20we%0Ashow%20that%20PPGN%20can%20simulate%20%242%24-WL%20uniformly%20on%20all%20point%20clouds%20with%20low%0Acomplexity.%20Secondly%2C%20we%20show%20that%20%242%24-WL%20tests%20can%20be%20extended%20to%20point%20clouds%0Awhich%20include%20both%20positions%20and%20velocities%2C%20a%20scenario%20often%20encountered%20in%0Aapplications.%20Finally%2C%20we%20provide%20a%20general%20framework%20for%20proving%20equivariant%0Auniversality%20and%20leverage%20it%20to%20prove%20that%20a%20simple%20modification%20of%20this%0Ainvariant%20PPGN%20architecture%20can%20be%20used%20to%20obtain%20a%20universal%20equivariant%0Aarchitecture%20that%20can%20approximate%20all%20continuous%20equivariant%20functions%0Auniformly.%20Building%20on%20our%20results%2C%20we%20develop%20our%20WeLNet%20architecture%2C%20which%0Asets%20new%20state-of-the-art%20results%20on%20the%20N-Body%20dynamics%20task%20and%20the%20GEOM-QM9%0Amolecular%20conformation%20generation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02484v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeisfeiler%2520Leman%2520for%2520Euclidean%2520Equivariant%2520Machine%2520Learning%26entry.906535625%3DSnir%2520Hordan%2520and%2520Tal%2520Amir%2520and%2520Nadav%2520Dym%26entry.1292438233%3D%2520%2520The%2520%2524k%2524-Weisfeiler-Leman%2520%2528%2524k%2524-WL%2529%2520graph%2520isomorphism%2520test%2520hierarchy%2520is%2520a%250Acommon%2520method%2520for%2520assessing%2520the%2520expressive%2520power%2520of%2520graph%2520neural%2520networks%250A%2528GNNs%2529.%2520Recently%252C%2520GNNs%2520whose%2520expressive%2520power%2520is%2520equivalent%2520to%2520the%2520%25242%2524-WL%2520test%250Awere%2520proven%2520to%2520be%2520universal%2520on%2520weighted%2520graphs%2520which%2520encode%2520%25243%255Cmathrm%257BD%257D%2524%2520point%250Acloud%2520data%252C%2520yet%2520this%2520result%2520is%2520limited%2520to%2520invariant%2520continuous%2520functions%2520on%250Apoint%2520clouds.%2520In%2520this%2520paper%252C%2520we%2520extend%2520this%2520result%2520in%2520three%2520ways%253A%2520Firstly%252C%2520we%250Ashow%2520that%2520PPGN%2520can%2520simulate%2520%25242%2524-WL%2520uniformly%2520on%2520all%2520point%2520clouds%2520with%2520low%250Acomplexity.%2520Secondly%252C%2520we%2520show%2520that%2520%25242%2524-WL%2520tests%2520can%2520be%2520extended%2520to%2520point%2520clouds%250Awhich%2520include%2520both%2520positions%2520and%2520velocities%252C%2520a%2520scenario%2520often%2520encountered%2520in%250Aapplications.%2520Finally%252C%2520we%2520provide%2520a%2520general%2520framework%2520for%2520proving%2520equivariant%250Auniversality%2520and%2520leverage%2520it%2520to%2520prove%2520that%2520a%2520simple%2520modification%2520of%2520this%250Ainvariant%2520PPGN%2520architecture%2520can%2520be%2520used%2520to%2520obtain%2520a%2520universal%2520equivariant%250Aarchitecture%2520that%2520can%2520approximate%2520all%2520continuous%2520equivariant%2520functions%250Auniformly.%2520Building%2520on%2520our%2520results%252C%2520we%2520develop%2520our%2520WeLNet%2520architecture%252C%2520which%250Asets%2520new%2520state-of-the-art%2520results%2520on%2520the%2520N-Body%2520dynamics%2520task%2520and%2520the%2520GEOM-QM9%250Amolecular%2520conformation%2520generation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02484v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weisfeiler%20Leman%20for%20Euclidean%20Equivariant%20Machine%20Learning&entry.906535625=Snir%20Hordan%20and%20Tal%20Amir%20and%20Nadav%20Dym&entry.1292438233=%20%20The%20%24k%24-Weisfeiler-Leman%20%28%24k%24-WL%29%20graph%20isomorphism%20test%20hierarchy%20is%20a%0Acommon%20method%20for%20assessing%20the%20expressive%20power%20of%20graph%20neural%20networks%0A%28GNNs%29.%20Recently%2C%20GNNs%20whose%20expressive%20power%20is%20equivalent%20to%20the%20%242%24-WL%20test%0Awere%20proven%20to%20be%20universal%20on%20weighted%20graphs%20which%20encode%20%243%5Cmathrm%7BD%7D%24%20point%0Acloud%20data%2C%20yet%20this%20result%20is%20limited%20to%20invariant%20continuous%20functions%20on%0Apoint%20clouds.%20In%20this%20paper%2C%20we%20extend%20this%20result%20in%20three%20ways%3A%20Firstly%2C%20we%0Ashow%20that%20PPGN%20can%20simulate%20%242%24-WL%20uniformly%20on%20all%20point%20clouds%20with%20low%0Acomplexity.%20Secondly%2C%20we%20show%20that%20%242%24-WL%20tests%20can%20be%20extended%20to%20point%20clouds%0Awhich%20include%20both%20positions%20and%20velocities%2C%20a%20scenario%20often%20encountered%20in%0Aapplications.%20Finally%2C%20we%20provide%20a%20general%20framework%20for%20proving%20equivariant%0Auniversality%20and%20leverage%20it%20to%20prove%20that%20a%20simple%20modification%20of%20this%0Ainvariant%20PPGN%20architecture%20can%20be%20used%20to%20obtain%20a%20universal%20equivariant%0Aarchitecture%20that%20can%20approximate%20all%20continuous%20equivariant%20functions%0Auniformly.%20Building%20on%20our%20results%2C%20we%20develop%20our%20WeLNet%20architecture%2C%20which%0Asets%20new%20state-of-the-art%20results%20on%20the%20N-Body%20dynamics%20task%20and%20the%20GEOM-QM9%0Amolecular%20conformation%20generation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02484v3&entry.124074799=Read"},
{"title": "On the Role of Visual Grounding in VQA", "author": "Daniel Reich and Tanja Schultz", "abstract": "  Visual Grounding (VG) in VQA refers to a model's proclivity to infer answers\nbased on question-relevant image regions. Conceptually, VG identifies as an\naxiomatic requirement of the VQA task. In practice, however, DNN-based VQA\nmodels are notorious for bypassing VG by way of shortcut (SC) learning without\nsuffering obvious performance losses in standard benchmarks. To uncover the\nimpact of SC learning, Out-of-Distribution (OOD) tests have been proposed that\nexpose a lack of VG with low accuracy. These tests have since been at the\ncenter of VG research and served as basis for various investigations into VG's\nimpact on accuracy. However, the role of VG in VQA still remains not fully\nunderstood and has not yet been properly formalized.\n  In this work, we seek to clarify VG's role in VQA by formalizing it on a\nconceptual level. We propose a novel theoretical framework called \"Visually\nGrounded Reasoning\" (VGR) that uses the concepts of VG and Reasoning to\ndescribe VQA inference in ideal OOD testing. By consolidating fundamental\ninsights into VG's role in VQA, VGR helps to reveal rampant VG-related SC\nexploitation in OOD testing, which explains why the relationship between VG and\nOOD accuracy has been difficult to define. Finally, we propose an approach to\ncreate OOD tests that properly emphasize a requirement for VG, and show how to\nimprove performance on them.\n", "link": "http://arxiv.org/abs/2406.18253v1", "date": "2024-06-26", "relevancy": 2.5385, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5272}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5023}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Role%20of%20Visual%20Grounding%20in%20VQA&body=Title%3A%20On%20the%20Role%20of%20Visual%20Grounding%20in%20VQA%0AAuthor%3A%20Daniel%20Reich%20and%20Tanja%20Schultz%0AAbstract%3A%20%20%20Visual%20Grounding%20%28VG%29%20in%20VQA%20refers%20to%20a%20model%27s%20proclivity%20to%20infer%20answers%0Abased%20on%20question-relevant%20image%20regions.%20Conceptually%2C%20VG%20identifies%20as%20an%0Aaxiomatic%20requirement%20of%20the%20VQA%20task.%20In%20practice%2C%20however%2C%20DNN-based%20VQA%0Amodels%20are%20notorious%20for%20bypassing%20VG%20by%20way%20of%20shortcut%20%28SC%29%20learning%20without%0Asuffering%20obvious%20performance%20losses%20in%20standard%20benchmarks.%20To%20uncover%20the%0Aimpact%20of%20SC%20learning%2C%20Out-of-Distribution%20%28OOD%29%20tests%20have%20been%20proposed%20that%0Aexpose%20a%20lack%20of%20VG%20with%20low%20accuracy.%20These%20tests%20have%20since%20been%20at%20the%0Acenter%20of%20VG%20research%20and%20served%20as%20basis%20for%20various%20investigations%20into%20VG%27s%0Aimpact%20on%20accuracy.%20However%2C%20the%20role%20of%20VG%20in%20VQA%20still%20remains%20not%20fully%0Aunderstood%20and%20has%20not%20yet%20been%20properly%20formalized.%0A%20%20In%20this%20work%2C%20we%20seek%20to%20clarify%20VG%27s%20role%20in%20VQA%20by%20formalizing%20it%20on%20a%0Aconceptual%20level.%20We%20propose%20a%20novel%20theoretical%20framework%20called%20%22Visually%0AGrounded%20Reasoning%22%20%28VGR%29%20that%20uses%20the%20concepts%20of%20VG%20and%20Reasoning%20to%0Adescribe%20VQA%20inference%20in%20ideal%20OOD%20testing.%20By%20consolidating%20fundamental%0Ainsights%20into%20VG%27s%20role%20in%20VQA%2C%20VGR%20helps%20to%20reveal%20rampant%20VG-related%20SC%0Aexploitation%20in%20OOD%20testing%2C%20which%20explains%20why%20the%20relationship%20between%20VG%20and%0AOOD%20accuracy%20has%20been%20difficult%20to%20define.%20Finally%2C%20we%20propose%20an%20approach%20to%0Acreate%20OOD%20tests%20that%20properly%20emphasize%20a%20requirement%20for%20VG%2C%20and%20show%20how%20to%0Aimprove%20performance%20on%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Role%2520of%2520Visual%2520Grounding%2520in%2520VQA%26entry.906535625%3DDaniel%2520Reich%2520and%2520Tanja%2520Schultz%26entry.1292438233%3D%2520%2520Visual%2520Grounding%2520%2528VG%2529%2520in%2520VQA%2520refers%2520to%2520a%2520model%2527s%2520proclivity%2520to%2520infer%2520answers%250Abased%2520on%2520question-relevant%2520image%2520regions.%2520Conceptually%252C%2520VG%2520identifies%2520as%2520an%250Aaxiomatic%2520requirement%2520of%2520the%2520VQA%2520task.%2520In%2520practice%252C%2520however%252C%2520DNN-based%2520VQA%250Amodels%2520are%2520notorious%2520for%2520bypassing%2520VG%2520by%2520way%2520of%2520shortcut%2520%2528SC%2529%2520learning%2520without%250Asuffering%2520obvious%2520performance%2520losses%2520in%2520standard%2520benchmarks.%2520To%2520uncover%2520the%250Aimpact%2520of%2520SC%2520learning%252C%2520Out-of-Distribution%2520%2528OOD%2529%2520tests%2520have%2520been%2520proposed%2520that%250Aexpose%2520a%2520lack%2520of%2520VG%2520with%2520low%2520accuracy.%2520These%2520tests%2520have%2520since%2520been%2520at%2520the%250Acenter%2520of%2520VG%2520research%2520and%2520served%2520as%2520basis%2520for%2520various%2520investigations%2520into%2520VG%2527s%250Aimpact%2520on%2520accuracy.%2520However%252C%2520the%2520role%2520of%2520VG%2520in%2520VQA%2520still%2520remains%2520not%2520fully%250Aunderstood%2520and%2520has%2520not%2520yet%2520been%2520properly%2520formalized.%250A%2520%2520In%2520this%2520work%252C%2520we%2520seek%2520to%2520clarify%2520VG%2527s%2520role%2520in%2520VQA%2520by%2520formalizing%2520it%2520on%2520a%250Aconceptual%2520level.%2520We%2520propose%2520a%2520novel%2520theoretical%2520framework%2520called%2520%2522Visually%250AGrounded%2520Reasoning%2522%2520%2528VGR%2529%2520that%2520uses%2520the%2520concepts%2520of%2520VG%2520and%2520Reasoning%2520to%250Adescribe%2520VQA%2520inference%2520in%2520ideal%2520OOD%2520testing.%2520By%2520consolidating%2520fundamental%250Ainsights%2520into%2520VG%2527s%2520role%2520in%2520VQA%252C%2520VGR%2520helps%2520to%2520reveal%2520rampant%2520VG-related%2520SC%250Aexploitation%2520in%2520OOD%2520testing%252C%2520which%2520explains%2520why%2520the%2520relationship%2520between%2520VG%2520and%250AOOD%2520accuracy%2520has%2520been%2520difficult%2520to%2520define.%2520Finally%252C%2520we%2520propose%2520an%2520approach%2520to%250Acreate%2520OOD%2520tests%2520that%2520properly%2520emphasize%2520a%2520requirement%2520for%2520VG%252C%2520and%2520show%2520how%2520to%250Aimprove%2520performance%2520on%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Role%20of%20Visual%20Grounding%20in%20VQA&entry.906535625=Daniel%20Reich%20and%20Tanja%20Schultz&entry.1292438233=%20%20Visual%20Grounding%20%28VG%29%20in%20VQA%20refers%20to%20a%20model%27s%20proclivity%20to%20infer%20answers%0Abased%20on%20question-relevant%20image%20regions.%20Conceptually%2C%20VG%20identifies%20as%20an%0Aaxiomatic%20requirement%20of%20the%20VQA%20task.%20In%20practice%2C%20however%2C%20DNN-based%20VQA%0Amodels%20are%20notorious%20for%20bypassing%20VG%20by%20way%20of%20shortcut%20%28SC%29%20learning%20without%0Asuffering%20obvious%20performance%20losses%20in%20standard%20benchmarks.%20To%20uncover%20the%0Aimpact%20of%20SC%20learning%2C%20Out-of-Distribution%20%28OOD%29%20tests%20have%20been%20proposed%20that%0Aexpose%20a%20lack%20of%20VG%20with%20low%20accuracy.%20These%20tests%20have%20since%20been%20at%20the%0Acenter%20of%20VG%20research%20and%20served%20as%20basis%20for%20various%20investigations%20into%20VG%27s%0Aimpact%20on%20accuracy.%20However%2C%20the%20role%20of%20VG%20in%20VQA%20still%20remains%20not%20fully%0Aunderstood%20and%20has%20not%20yet%20been%20properly%20formalized.%0A%20%20In%20this%20work%2C%20we%20seek%20to%20clarify%20VG%27s%20role%20in%20VQA%20by%20formalizing%20it%20on%20a%0Aconceptual%20level.%20We%20propose%20a%20novel%20theoretical%20framework%20called%20%22Visually%0AGrounded%20Reasoning%22%20%28VGR%29%20that%20uses%20the%20concepts%20of%20VG%20and%20Reasoning%20to%0Adescribe%20VQA%20inference%20in%20ideal%20OOD%20testing.%20By%20consolidating%20fundamental%0Ainsights%20into%20VG%27s%20role%20in%20VQA%2C%20VGR%20helps%20to%20reveal%20rampant%20VG-related%20SC%0Aexploitation%20in%20OOD%20testing%2C%20which%20explains%20why%20the%20relationship%20between%20VG%20and%0AOOD%20accuracy%20has%20been%20difficult%20to%20define.%20Finally%2C%20we%20propose%20an%20approach%20to%0Acreate%20OOD%20tests%20that%20properly%20emphasize%20a%20requirement%20for%20VG%2C%20and%20show%20how%20to%0Aimprove%20performance%20on%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18253v1&entry.124074799=Read"},
{"title": "InstantGroup: Instant Template Generation for Scalable Group of Brain\n  MRI Registration", "author": "Ziyi He and Albert C. S. Chung", "abstract": "  Template generation is a critical step in groupwise image registration, which\ninvolves aligning a group of subjects into a common space. While existing\nmethods can generate high-quality template images, they often incur substantial\ntime costs or are limited by fixed group scales. In this paper, we present\nInstantGroup, an efficient groupwise template generation framework based on\nvariational autoencoder (VAE) models that leverage latent representations'\narithmetic properties, enabling scalability to groups of any size. InstantGroup\nfeatures a Dual VAEs backbone with shared-weight twin networks to handle pairs\nof inputs and incorporates a Displacement Inversion Module (DIM) to maintain\ntemplate unbiasedness and a Subject-Template Alignment Module (STAM) to improve\ntemplate quality and registration accuracy. Experiments on 3D brain MRI scans\nfrom the OASIS and ADNI datasets reveal that InstantGroup dramatically reduces\nruntime, generating templates within seconds for various group sizes while\nmaintaining superior performance compared to state-of-the-art baselines on\nquantitative metrics, including unbiasedness and registration accuracy.\n", "link": "http://arxiv.org/abs/2211.05622v2", "date": "2024-06-26", "relevancy": 2.4808, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4999}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4966}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstantGroup%3A%20Instant%20Template%20Generation%20for%20Scalable%20Group%20of%20Brain%0A%20%20MRI%20Registration&body=Title%3A%20InstantGroup%3A%20Instant%20Template%20Generation%20for%20Scalable%20Group%20of%20Brain%0A%20%20MRI%20Registration%0AAuthor%3A%20Ziyi%20He%20and%20Albert%20C.%20S.%20Chung%0AAbstract%3A%20%20%20Template%20generation%20is%20a%20critical%20step%20in%20groupwise%20image%20registration%2C%20which%0Ainvolves%20aligning%20a%20group%20of%20subjects%20into%20a%20common%20space.%20While%20existing%0Amethods%20can%20generate%20high-quality%20template%20images%2C%20they%20often%20incur%20substantial%0Atime%20costs%20or%20are%20limited%20by%20fixed%20group%20scales.%20In%20this%20paper%2C%20we%20present%0AInstantGroup%2C%20an%20efficient%20groupwise%20template%20generation%20framework%20based%20on%0Avariational%20autoencoder%20%28VAE%29%20models%20that%20leverage%20latent%20representations%27%0Aarithmetic%20properties%2C%20enabling%20scalability%20to%20groups%20of%20any%20size.%20InstantGroup%0Afeatures%20a%20Dual%20VAEs%20backbone%20with%20shared-weight%20twin%20networks%20to%20handle%20pairs%0Aof%20inputs%20and%20incorporates%20a%20Displacement%20Inversion%20Module%20%28DIM%29%20to%20maintain%0Atemplate%20unbiasedness%20and%20a%20Subject-Template%20Alignment%20Module%20%28STAM%29%20to%20improve%0Atemplate%20quality%20and%20registration%20accuracy.%20Experiments%20on%203D%20brain%20MRI%20scans%0Afrom%20the%20OASIS%20and%20ADNI%20datasets%20reveal%20that%20InstantGroup%20dramatically%20reduces%0Aruntime%2C%20generating%20templates%20within%20seconds%20for%20various%20group%20sizes%20while%0Amaintaining%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Aquantitative%20metrics%2C%20including%20unbiasedness%20and%20registration%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.05622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstantGroup%253A%2520Instant%2520Template%2520Generation%2520for%2520Scalable%2520Group%2520of%2520Brain%250A%2520%2520MRI%2520Registration%26entry.906535625%3DZiyi%2520He%2520and%2520Albert%2520C.%2520S.%2520Chung%26entry.1292438233%3D%2520%2520Template%2520generation%2520is%2520a%2520critical%2520step%2520in%2520groupwise%2520image%2520registration%252C%2520which%250Ainvolves%2520aligning%2520a%2520group%2520of%2520subjects%2520into%2520a%2520common%2520space.%2520While%2520existing%250Amethods%2520can%2520generate%2520high-quality%2520template%2520images%252C%2520they%2520often%2520incur%2520substantial%250Atime%2520costs%2520or%2520are%2520limited%2520by%2520fixed%2520group%2520scales.%2520In%2520this%2520paper%252C%2520we%2520present%250AInstantGroup%252C%2520an%2520efficient%2520groupwise%2520template%2520generation%2520framework%2520based%2520on%250Avariational%2520autoencoder%2520%2528VAE%2529%2520models%2520that%2520leverage%2520latent%2520representations%2527%250Aarithmetic%2520properties%252C%2520enabling%2520scalability%2520to%2520groups%2520of%2520any%2520size.%2520InstantGroup%250Afeatures%2520a%2520Dual%2520VAEs%2520backbone%2520with%2520shared-weight%2520twin%2520networks%2520to%2520handle%2520pairs%250Aof%2520inputs%2520and%2520incorporates%2520a%2520Displacement%2520Inversion%2520Module%2520%2528DIM%2529%2520to%2520maintain%250Atemplate%2520unbiasedness%2520and%2520a%2520Subject-Template%2520Alignment%2520Module%2520%2528STAM%2529%2520to%2520improve%250Atemplate%2520quality%2520and%2520registration%2520accuracy.%2520Experiments%2520on%25203D%2520brain%2520MRI%2520scans%250Afrom%2520the%2520OASIS%2520and%2520ADNI%2520datasets%2520reveal%2520that%2520InstantGroup%2520dramatically%2520reduces%250Aruntime%252C%2520generating%2520templates%2520within%2520seconds%2520for%2520various%2520group%2520sizes%2520while%250Amaintaining%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520baselines%2520on%250Aquantitative%2520metrics%252C%2520including%2520unbiasedness%2520and%2520registration%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.05622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstantGroup%3A%20Instant%20Template%20Generation%20for%20Scalable%20Group%20of%20Brain%0A%20%20MRI%20Registration&entry.906535625=Ziyi%20He%20and%20Albert%20C.%20S.%20Chung&entry.1292438233=%20%20Template%20generation%20is%20a%20critical%20step%20in%20groupwise%20image%20registration%2C%20which%0Ainvolves%20aligning%20a%20group%20of%20subjects%20into%20a%20common%20space.%20While%20existing%0Amethods%20can%20generate%20high-quality%20template%20images%2C%20they%20often%20incur%20substantial%0Atime%20costs%20or%20are%20limited%20by%20fixed%20group%20scales.%20In%20this%20paper%2C%20we%20present%0AInstantGroup%2C%20an%20efficient%20groupwise%20template%20generation%20framework%20based%20on%0Avariational%20autoencoder%20%28VAE%29%20models%20that%20leverage%20latent%20representations%27%0Aarithmetic%20properties%2C%20enabling%20scalability%20to%20groups%20of%20any%20size.%20InstantGroup%0Afeatures%20a%20Dual%20VAEs%20backbone%20with%20shared-weight%20twin%20networks%20to%20handle%20pairs%0Aof%20inputs%20and%20incorporates%20a%20Displacement%20Inversion%20Module%20%28DIM%29%20to%20maintain%0Atemplate%20unbiasedness%20and%20a%20Subject-Template%20Alignment%20Module%20%28STAM%29%20to%20improve%0Atemplate%20quality%20and%20registration%20accuracy.%20Experiments%20on%203D%20brain%20MRI%20scans%0Afrom%20the%20OASIS%20and%20ADNI%20datasets%20reveal%20that%20InstantGroup%20dramatically%20reduces%0Aruntime%2C%20generating%20templates%20within%20seconds%20for%20various%20group%20sizes%20while%0Amaintaining%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Aquantitative%20metrics%2C%20including%20unbiasedness%20and%20registration%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.05622v2&entry.124074799=Read"},
{"title": "Towards Compositionality in Concept Learning", "author": "Adam Stein and Aaditya Naik and Yinjun Wu and Mayur Naik and Eric Wong", "abstract": "  Concept-based interpretability methods offer a lens into the internals of\nfoundation models by decomposing their embeddings into high-level concepts.\nThese concept representations are most useful when they are compositional,\nmeaning that the individual concepts compose to explain the full sample. We\nshow that existing unsupervised concept extraction methods find concepts which\nare not compositional. To automatically discover compositional concept\nrepresentations, we identify two salient properties of such representations,\nand propose Compositional Concept Extraction (CCE) for finding concepts which\nobey these properties. We evaluate CCE on five different datasets over image\nand text data. Our evaluation shows that CCE finds more compositional concept\nrepresentations than baselines and yields better accuracy on four downstream\nclassification tasks. Code and data are available at\nhttps://github.com/adaminsky/compositional_concepts .\n", "link": "http://arxiv.org/abs/2406.18534v1", "date": "2024-06-26", "relevancy": 2.4772, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Compositionality%20in%20Concept%20Learning&body=Title%3A%20Towards%20Compositionality%20in%20Concept%20Learning%0AAuthor%3A%20Adam%20Stein%20and%20Aaditya%20Naik%20and%20Yinjun%20Wu%20and%20Mayur%20Naik%20and%20Eric%20Wong%0AAbstract%3A%20%20%20Concept-based%20interpretability%20methods%20offer%20a%20lens%20into%20the%20internals%20of%0Afoundation%20models%20by%20decomposing%20their%20embeddings%20into%20high-level%20concepts.%0AThese%20concept%20representations%20are%20most%20useful%20when%20they%20are%20compositional%2C%0Ameaning%20that%20the%20individual%20concepts%20compose%20to%20explain%20the%20full%20sample.%20We%0Ashow%20that%20existing%20unsupervised%20concept%20extraction%20methods%20find%20concepts%20which%0Aare%20not%20compositional.%20To%20automatically%20discover%20compositional%20concept%0Arepresentations%2C%20we%20identify%20two%20salient%20properties%20of%20such%20representations%2C%0Aand%20propose%20Compositional%20Concept%20Extraction%20%28CCE%29%20for%20finding%20concepts%20which%0Aobey%20these%20properties.%20We%20evaluate%20CCE%20on%20five%20different%20datasets%20over%20image%0Aand%20text%20data.%20Our%20evaluation%20shows%20that%20CCE%20finds%20more%20compositional%20concept%0Arepresentations%20than%20baselines%20and%20yields%20better%20accuracy%20on%20four%20downstream%0Aclassification%20tasks.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/adaminsky/compositional_concepts%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Compositionality%2520in%2520Concept%2520Learning%26entry.906535625%3DAdam%2520Stein%2520and%2520Aaditya%2520Naik%2520and%2520Yinjun%2520Wu%2520and%2520Mayur%2520Naik%2520and%2520Eric%2520Wong%26entry.1292438233%3D%2520%2520Concept-based%2520interpretability%2520methods%2520offer%2520a%2520lens%2520into%2520the%2520internals%2520of%250Afoundation%2520models%2520by%2520decomposing%2520their%2520embeddings%2520into%2520high-level%2520concepts.%250AThese%2520concept%2520representations%2520are%2520most%2520useful%2520when%2520they%2520are%2520compositional%252C%250Ameaning%2520that%2520the%2520individual%2520concepts%2520compose%2520to%2520explain%2520the%2520full%2520sample.%2520We%250Ashow%2520that%2520existing%2520unsupervised%2520concept%2520extraction%2520methods%2520find%2520concepts%2520which%250Aare%2520not%2520compositional.%2520To%2520automatically%2520discover%2520compositional%2520concept%250Arepresentations%252C%2520we%2520identify%2520two%2520salient%2520properties%2520of%2520such%2520representations%252C%250Aand%2520propose%2520Compositional%2520Concept%2520Extraction%2520%2528CCE%2529%2520for%2520finding%2520concepts%2520which%250Aobey%2520these%2520properties.%2520We%2520evaluate%2520CCE%2520on%2520five%2520different%2520datasets%2520over%2520image%250Aand%2520text%2520data.%2520Our%2520evaluation%2520shows%2520that%2520CCE%2520finds%2520more%2520compositional%2520concept%250Arepresentations%2520than%2520baselines%2520and%2520yields%2520better%2520accuracy%2520on%2520four%2520downstream%250Aclassification%2520tasks.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/adaminsky/compositional_concepts%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Compositionality%20in%20Concept%20Learning&entry.906535625=Adam%20Stein%20and%20Aaditya%20Naik%20and%20Yinjun%20Wu%20and%20Mayur%20Naik%20and%20Eric%20Wong&entry.1292438233=%20%20Concept-based%20interpretability%20methods%20offer%20a%20lens%20into%20the%20internals%20of%0Afoundation%20models%20by%20decomposing%20their%20embeddings%20into%20high-level%20concepts.%0AThese%20concept%20representations%20are%20most%20useful%20when%20they%20are%20compositional%2C%0Ameaning%20that%20the%20individual%20concepts%20compose%20to%20explain%20the%20full%20sample.%20We%0Ashow%20that%20existing%20unsupervised%20concept%20extraction%20methods%20find%20concepts%20which%0Aare%20not%20compositional.%20To%20automatically%20discover%20compositional%20concept%0Arepresentations%2C%20we%20identify%20two%20salient%20properties%20of%20such%20representations%2C%0Aand%20propose%20Compositional%20Concept%20Extraction%20%28CCE%29%20for%20finding%20concepts%20which%0Aobey%20these%20properties.%20We%20evaluate%20CCE%20on%20five%20different%20datasets%20over%20image%0Aand%20text%20data.%20Our%20evaluation%20shows%20that%20CCE%20finds%20more%20compositional%20concept%0Arepresentations%20than%20baselines%20and%20yields%20better%20accuracy%20on%20four%20downstream%0Aclassification%20tasks.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/adaminsky/compositional_concepts%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18534v1&entry.124074799=Read"},
{"title": "A Survey of Generative AI for de novo Drug Design: New Frontiers in\n  Molecule and Protein Generation", "author": "Xiangru Tang and Howard Dai and Elizabeth Knight and Fang Wu and Yunyang Li and Tianxiao Li and Mark Gerstein", "abstract": "  Artificial intelligence (AI)-driven methods can vastly improve the\nhistorically costly drug design process, with various generative models already\nin widespread use. Generative models for de novo drug design, in particular,\nfocus on the creation of novel biological compounds entirely from scratch,\nrepresenting a promising future direction. Rapid development in the field,\ncombined with the inherent complexity of the drug design process, creates a\ndifficult landscape for new researchers to enter. In this survey, we organize\nde novo drug design into two overarching themes: small molecule and protein\ngeneration. Within each theme, we identify a variety of subtasks and\napplications, highlighting important datasets, benchmarks, and model\narchitectures and comparing the performance of top models. We take a broad\napproach to AI-driven drug design, allowing for both micro-level comparisons of\nvarious methods within each subtask and macro-level observations across\ndifferent fields. We discuss parallel challenges and approaches between the two\napplications and highlight future directions for AI-driven de novo drug design\nas a whole. An organized repository of all covered sources is available at\nhttps://github.com/gersteinlab/GenAI4Drug.\n", "link": "http://arxiv.org/abs/2402.08703v2", "date": "2024-06-26", "relevancy": 2.4686, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5373}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4791}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Generative%20AI%20for%20de%20novo%20Drug%20Design%3A%20New%20Frontiers%20in%0A%20%20Molecule%20and%20Protein%20Generation&body=Title%3A%20A%20Survey%20of%20Generative%20AI%20for%20de%20novo%20Drug%20Design%3A%20New%20Frontiers%20in%0A%20%20Molecule%20and%20Protein%20Generation%0AAuthor%3A%20Xiangru%20Tang%20and%20Howard%20Dai%20and%20Elizabeth%20Knight%20and%20Fang%20Wu%20and%20Yunyang%20Li%20and%20Tianxiao%20Li%20and%20Mark%20Gerstein%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29-driven%20methods%20can%20vastly%20improve%20the%0Ahistorically%20costly%20drug%20design%20process%2C%20with%20various%20generative%20models%20already%0Ain%20widespread%20use.%20Generative%20models%20for%20de%20novo%20drug%20design%2C%20in%20particular%2C%0Afocus%20on%20the%20creation%20of%20novel%20biological%20compounds%20entirely%20from%20scratch%2C%0Arepresenting%20a%20promising%20future%20direction.%20Rapid%20development%20in%20the%20field%2C%0Acombined%20with%20the%20inherent%20complexity%20of%20the%20drug%20design%20process%2C%20creates%20a%0Adifficult%20landscape%20for%20new%20researchers%20to%20enter.%20In%20this%20survey%2C%20we%20organize%0Ade%20novo%20drug%20design%20into%20two%20overarching%20themes%3A%20small%20molecule%20and%20protein%0Ageneration.%20Within%20each%20theme%2C%20we%20identify%20a%20variety%20of%20subtasks%20and%0Aapplications%2C%20highlighting%20important%20datasets%2C%20benchmarks%2C%20and%20model%0Aarchitectures%20and%20comparing%20the%20performance%20of%20top%20models.%20We%20take%20a%20broad%0Aapproach%20to%20AI-driven%20drug%20design%2C%20allowing%20for%20both%20micro-level%20comparisons%20of%0Avarious%20methods%20within%20each%20subtask%20and%20macro-level%20observations%20across%0Adifferent%20fields.%20We%20discuss%20parallel%20challenges%20and%20approaches%20between%20the%20two%0Aapplications%20and%20highlight%20future%20directions%20for%20AI-driven%20de%20novo%20drug%20design%0Aas%20a%20whole.%20An%20organized%20repository%20of%20all%20covered%20sources%20is%20available%20at%0Ahttps%3A//github.com/gersteinlab/GenAI4Drug.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Generative%2520AI%2520for%2520de%2520novo%2520Drug%2520Design%253A%2520New%2520Frontiers%2520in%250A%2520%2520Molecule%2520and%2520Protein%2520Generation%26entry.906535625%3DXiangru%2520Tang%2520and%2520Howard%2520Dai%2520and%2520Elizabeth%2520Knight%2520and%2520Fang%2520Wu%2520and%2520Yunyang%2520Li%2520and%2520Tianxiao%2520Li%2520and%2520Mark%2520Gerstein%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529-driven%2520methods%2520can%2520vastly%2520improve%2520the%250Ahistorically%2520costly%2520drug%2520design%2520process%252C%2520with%2520various%2520generative%2520models%2520already%250Ain%2520widespread%2520use.%2520Generative%2520models%2520for%2520de%2520novo%2520drug%2520design%252C%2520in%2520particular%252C%250Afocus%2520on%2520the%2520creation%2520of%2520novel%2520biological%2520compounds%2520entirely%2520from%2520scratch%252C%250Arepresenting%2520a%2520promising%2520future%2520direction.%2520Rapid%2520development%2520in%2520the%2520field%252C%250Acombined%2520with%2520the%2520inherent%2520complexity%2520of%2520the%2520drug%2520design%2520process%252C%2520creates%2520a%250Adifficult%2520landscape%2520for%2520new%2520researchers%2520to%2520enter.%2520In%2520this%2520survey%252C%2520we%2520organize%250Ade%2520novo%2520drug%2520design%2520into%2520two%2520overarching%2520themes%253A%2520small%2520molecule%2520and%2520protein%250Ageneration.%2520Within%2520each%2520theme%252C%2520we%2520identify%2520a%2520variety%2520of%2520subtasks%2520and%250Aapplications%252C%2520highlighting%2520important%2520datasets%252C%2520benchmarks%252C%2520and%2520model%250Aarchitectures%2520and%2520comparing%2520the%2520performance%2520of%2520top%2520models.%2520We%2520take%2520a%2520broad%250Aapproach%2520to%2520AI-driven%2520drug%2520design%252C%2520allowing%2520for%2520both%2520micro-level%2520comparisons%2520of%250Avarious%2520methods%2520within%2520each%2520subtask%2520and%2520macro-level%2520observations%2520across%250Adifferent%2520fields.%2520We%2520discuss%2520parallel%2520challenges%2520and%2520approaches%2520between%2520the%2520two%250Aapplications%2520and%2520highlight%2520future%2520directions%2520for%2520AI-driven%2520de%2520novo%2520drug%2520design%250Aas%2520a%2520whole.%2520An%2520organized%2520repository%2520of%2520all%2520covered%2520sources%2520is%2520available%2520at%250Ahttps%253A//github.com/gersteinlab/GenAI4Drug.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Generative%20AI%20for%20de%20novo%20Drug%20Design%3A%20New%20Frontiers%20in%0A%20%20Molecule%20and%20Protein%20Generation&entry.906535625=Xiangru%20Tang%20and%20Howard%20Dai%20and%20Elizabeth%20Knight%20and%20Fang%20Wu%20and%20Yunyang%20Li%20and%20Tianxiao%20Li%20and%20Mark%20Gerstein&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29-driven%20methods%20can%20vastly%20improve%20the%0Ahistorically%20costly%20drug%20design%20process%2C%20with%20various%20generative%20models%20already%0Ain%20widespread%20use.%20Generative%20models%20for%20de%20novo%20drug%20design%2C%20in%20particular%2C%0Afocus%20on%20the%20creation%20of%20novel%20biological%20compounds%20entirely%20from%20scratch%2C%0Arepresenting%20a%20promising%20future%20direction.%20Rapid%20development%20in%20the%20field%2C%0Acombined%20with%20the%20inherent%20complexity%20of%20the%20drug%20design%20process%2C%20creates%20a%0Adifficult%20landscape%20for%20new%20researchers%20to%20enter.%20In%20this%20survey%2C%20we%20organize%0Ade%20novo%20drug%20design%20into%20two%20overarching%20themes%3A%20small%20molecule%20and%20protein%0Ageneration.%20Within%20each%20theme%2C%20we%20identify%20a%20variety%20of%20subtasks%20and%0Aapplications%2C%20highlighting%20important%20datasets%2C%20benchmarks%2C%20and%20model%0Aarchitectures%20and%20comparing%20the%20performance%20of%20top%20models.%20We%20take%20a%20broad%0Aapproach%20to%20AI-driven%20drug%20design%2C%20allowing%20for%20both%20micro-level%20comparisons%20of%0Avarious%20methods%20within%20each%20subtask%20and%20macro-level%20observations%20across%0Adifferent%20fields.%20We%20discuss%20parallel%20challenges%20and%20approaches%20between%20the%20two%0Aapplications%20and%20highlight%20future%20directions%20for%20AI-driven%20de%20novo%20drug%20design%0Aas%20a%20whole.%20An%20organized%20repository%20of%20all%20covered%20sources%20is%20available%20at%0Ahttps%3A//github.com/gersteinlab/GenAI4Drug.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08703v2&entry.124074799=Read"},
{"title": "Towards Human-Level 3D Relative Pose Estimation: Generalizable,\n  Training-Free, with Single Reference", "author": "Yuan Gao and Yajing Luo and Junhong Wang and Kui Jia and Gui-Song Xia", "abstract": "  Humans can easily deduce the relative pose of an unseen object, without\nlabel/training, given only a single query-reference image pair. This is\narguably achieved by incorporating (i) 3D/2.5D shape perception from a single\nimage, (ii) render-and-compare simulation, and (iii) rich semantic cue\nawareness to furnish (coarse) reference-query correspondence. Existing methods\nimplement (i) by a 3D CAD model or well-calibrated multiple images and (ii) by\ntraining a network on specific objects, which necessitate laborious\nground-truth labeling and tedious training, potentially leading to challenges\nin generalization. Moreover, (iii) was less exploited in the paradigm of (ii),\ndespite that the coarse correspondence from (iii) enhances the compare process\nby filtering out non-overlapped parts under substantial pose\ndifferences/occlusions. Motivated by this, we propose a novel 3D generalizable\nrelative pose estimation method by elaborating (i) with a 2.5D shape from an\nRGB-D reference, (ii) with an off-the-shelf differentiable renderer, and (iii)\nwith semantic cues from a pretrained model like DINOv2. Specifically, our\ndifferentiable renderer takes the 2.5D rotatable mesh textured by the RGB and\nthe semantic maps (obtained by DINOv2 from the RGB input), then renders new RGB\nand semantic maps (with back-surface culling) under a novel rotated view. The\nrefinement loss comes from comparing the rendered RGB and semantic maps with\nthe query ones, back-propagating the gradients through the differentiable\nrenderer to refine the 3D relative pose. As a result, our method can be readily\napplied to unseen objects, given only a single RGB-D reference, without\nlabel/training. Extensive experiments on LineMOD, LM-O, and YCB-V show that our\ntraining-free method significantly outperforms the SOTA supervised methods,\nespecially under the rigorous Acc@5/10/15{\\deg} metrics and the challenging\ncross-dataset settings.\n", "link": "http://arxiv.org/abs/2406.18453v1", "date": "2024-06-26", "relevancy": 2.43, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6228}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5976}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Human-Level%203D%20Relative%20Pose%20Estimation%3A%20Generalizable%2C%0A%20%20Training-Free%2C%20with%20Single%20Reference&body=Title%3A%20Towards%20Human-Level%203D%20Relative%20Pose%20Estimation%3A%20Generalizable%2C%0A%20%20Training-Free%2C%20with%20Single%20Reference%0AAuthor%3A%20Yuan%20Gao%20and%20Yajing%20Luo%20and%20Junhong%20Wang%20and%20Kui%20Jia%20and%20Gui-Song%20Xia%0AAbstract%3A%20%20%20Humans%20can%20easily%20deduce%20the%20relative%20pose%20of%20an%20unseen%20object%2C%20without%0Alabel/training%2C%20given%20only%20a%20single%20query-reference%20image%20pair.%20This%20is%0Aarguably%20achieved%20by%20incorporating%20%28i%29%203D/2.5D%20shape%20perception%20from%20a%20single%0Aimage%2C%20%28ii%29%20render-and-compare%20simulation%2C%20and%20%28iii%29%20rich%20semantic%20cue%0Aawareness%20to%20furnish%20%28coarse%29%20reference-query%20correspondence.%20Existing%20methods%0Aimplement%20%28i%29%20by%20a%203D%20CAD%20model%20or%20well-calibrated%20multiple%20images%20and%20%28ii%29%20by%0Atraining%20a%20network%20on%20specific%20objects%2C%20which%20necessitate%20laborious%0Aground-truth%20labeling%20and%20tedious%20training%2C%20potentially%20leading%20to%20challenges%0Ain%20generalization.%20Moreover%2C%20%28iii%29%20was%20less%20exploited%20in%20the%20paradigm%20of%20%28ii%29%2C%0Adespite%20that%20the%20coarse%20correspondence%20from%20%28iii%29%20enhances%20the%20compare%20process%0Aby%20filtering%20out%20non-overlapped%20parts%20under%20substantial%20pose%0Adifferences/occlusions.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%203D%20generalizable%0Arelative%20pose%20estimation%20method%20by%20elaborating%20%28i%29%20with%20a%202.5D%20shape%20from%20an%0ARGB-D%20reference%2C%20%28ii%29%20with%20an%20off-the-shelf%20differentiable%20renderer%2C%20and%20%28iii%29%0Awith%20semantic%20cues%20from%20a%20pretrained%20model%20like%20DINOv2.%20Specifically%2C%20our%0Adifferentiable%20renderer%20takes%20the%202.5D%20rotatable%20mesh%20textured%20by%20the%20RGB%20and%0Athe%20semantic%20maps%20%28obtained%20by%20DINOv2%20from%20the%20RGB%20input%29%2C%20then%20renders%20new%20RGB%0Aand%20semantic%20maps%20%28with%20back-surface%20culling%29%20under%20a%20novel%20rotated%20view.%20The%0Arefinement%20loss%20comes%20from%20comparing%20the%20rendered%20RGB%20and%20semantic%20maps%20with%0Athe%20query%20ones%2C%20back-propagating%20the%20gradients%20through%20the%20differentiable%0Arenderer%20to%20refine%20the%203D%20relative%20pose.%20As%20a%20result%2C%20our%20method%20can%20be%20readily%0Aapplied%20to%20unseen%20objects%2C%20given%20only%20a%20single%20RGB-D%20reference%2C%20without%0Alabel/training.%20Extensive%20experiments%20on%20LineMOD%2C%20LM-O%2C%20and%20YCB-V%20show%20that%20our%0Atraining-free%20method%20significantly%20outperforms%20the%20SOTA%20supervised%20methods%2C%0Aespecially%20under%20the%20rigorous%20Acc%405/10/15%7B%5Cdeg%7D%20metrics%20and%20the%20challenging%0Across-dataset%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Human-Level%25203D%2520Relative%2520Pose%2520Estimation%253A%2520Generalizable%252C%250A%2520%2520Training-Free%252C%2520with%2520Single%2520Reference%26entry.906535625%3DYuan%2520Gao%2520and%2520Yajing%2520Luo%2520and%2520Junhong%2520Wang%2520and%2520Kui%2520Jia%2520and%2520Gui-Song%2520Xia%26entry.1292438233%3D%2520%2520Humans%2520can%2520easily%2520deduce%2520the%2520relative%2520pose%2520of%2520an%2520unseen%2520object%252C%2520without%250Alabel/training%252C%2520given%2520only%2520a%2520single%2520query-reference%2520image%2520pair.%2520This%2520is%250Aarguably%2520achieved%2520by%2520incorporating%2520%2528i%2529%25203D/2.5D%2520shape%2520perception%2520from%2520a%2520single%250Aimage%252C%2520%2528ii%2529%2520render-and-compare%2520simulation%252C%2520and%2520%2528iii%2529%2520rich%2520semantic%2520cue%250Aawareness%2520to%2520furnish%2520%2528coarse%2529%2520reference-query%2520correspondence.%2520Existing%2520methods%250Aimplement%2520%2528i%2529%2520by%2520a%25203D%2520CAD%2520model%2520or%2520well-calibrated%2520multiple%2520images%2520and%2520%2528ii%2529%2520by%250Atraining%2520a%2520network%2520on%2520specific%2520objects%252C%2520which%2520necessitate%2520laborious%250Aground-truth%2520labeling%2520and%2520tedious%2520training%252C%2520potentially%2520leading%2520to%2520challenges%250Ain%2520generalization.%2520Moreover%252C%2520%2528iii%2529%2520was%2520less%2520exploited%2520in%2520the%2520paradigm%2520of%2520%2528ii%2529%252C%250Adespite%2520that%2520the%2520coarse%2520correspondence%2520from%2520%2528iii%2529%2520enhances%2520the%2520compare%2520process%250Aby%2520filtering%2520out%2520non-overlapped%2520parts%2520under%2520substantial%2520pose%250Adifferences/occlusions.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520novel%25203D%2520generalizable%250Arelative%2520pose%2520estimation%2520method%2520by%2520elaborating%2520%2528i%2529%2520with%2520a%25202.5D%2520shape%2520from%2520an%250ARGB-D%2520reference%252C%2520%2528ii%2529%2520with%2520an%2520off-the-shelf%2520differentiable%2520renderer%252C%2520and%2520%2528iii%2529%250Awith%2520semantic%2520cues%2520from%2520a%2520pretrained%2520model%2520like%2520DINOv2.%2520Specifically%252C%2520our%250Adifferentiable%2520renderer%2520takes%2520the%25202.5D%2520rotatable%2520mesh%2520textured%2520by%2520the%2520RGB%2520and%250Athe%2520semantic%2520maps%2520%2528obtained%2520by%2520DINOv2%2520from%2520the%2520RGB%2520input%2529%252C%2520then%2520renders%2520new%2520RGB%250Aand%2520semantic%2520maps%2520%2528with%2520back-surface%2520culling%2529%2520under%2520a%2520novel%2520rotated%2520view.%2520The%250Arefinement%2520loss%2520comes%2520from%2520comparing%2520the%2520rendered%2520RGB%2520and%2520semantic%2520maps%2520with%250Athe%2520query%2520ones%252C%2520back-propagating%2520the%2520gradients%2520through%2520the%2520differentiable%250Arenderer%2520to%2520refine%2520the%25203D%2520relative%2520pose.%2520As%2520a%2520result%252C%2520our%2520method%2520can%2520be%2520readily%250Aapplied%2520to%2520unseen%2520objects%252C%2520given%2520only%2520a%2520single%2520RGB-D%2520reference%252C%2520without%250Alabel/training.%2520Extensive%2520experiments%2520on%2520LineMOD%252C%2520LM-O%252C%2520and%2520YCB-V%2520show%2520that%2520our%250Atraining-free%2520method%2520significantly%2520outperforms%2520the%2520SOTA%2520supervised%2520methods%252C%250Aespecially%2520under%2520the%2520rigorous%2520Acc%25405/10/15%257B%255Cdeg%257D%2520metrics%2520and%2520the%2520challenging%250Across-dataset%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Human-Level%203D%20Relative%20Pose%20Estimation%3A%20Generalizable%2C%0A%20%20Training-Free%2C%20with%20Single%20Reference&entry.906535625=Yuan%20Gao%20and%20Yajing%20Luo%20and%20Junhong%20Wang%20and%20Kui%20Jia%20and%20Gui-Song%20Xia&entry.1292438233=%20%20Humans%20can%20easily%20deduce%20the%20relative%20pose%20of%20an%20unseen%20object%2C%20without%0Alabel/training%2C%20given%20only%20a%20single%20query-reference%20image%20pair.%20This%20is%0Aarguably%20achieved%20by%20incorporating%20%28i%29%203D/2.5D%20shape%20perception%20from%20a%20single%0Aimage%2C%20%28ii%29%20render-and-compare%20simulation%2C%20and%20%28iii%29%20rich%20semantic%20cue%0Aawareness%20to%20furnish%20%28coarse%29%20reference-query%20correspondence.%20Existing%20methods%0Aimplement%20%28i%29%20by%20a%203D%20CAD%20model%20or%20well-calibrated%20multiple%20images%20and%20%28ii%29%20by%0Atraining%20a%20network%20on%20specific%20objects%2C%20which%20necessitate%20laborious%0Aground-truth%20labeling%20and%20tedious%20training%2C%20potentially%20leading%20to%20challenges%0Ain%20generalization.%20Moreover%2C%20%28iii%29%20was%20less%20exploited%20in%20the%20paradigm%20of%20%28ii%29%2C%0Adespite%20that%20the%20coarse%20correspondence%20from%20%28iii%29%20enhances%20the%20compare%20process%0Aby%20filtering%20out%20non-overlapped%20parts%20under%20substantial%20pose%0Adifferences/occlusions.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%203D%20generalizable%0Arelative%20pose%20estimation%20method%20by%20elaborating%20%28i%29%20with%20a%202.5D%20shape%20from%20an%0ARGB-D%20reference%2C%20%28ii%29%20with%20an%20off-the-shelf%20differentiable%20renderer%2C%20and%20%28iii%29%0Awith%20semantic%20cues%20from%20a%20pretrained%20model%20like%20DINOv2.%20Specifically%2C%20our%0Adifferentiable%20renderer%20takes%20the%202.5D%20rotatable%20mesh%20textured%20by%20the%20RGB%20and%0Athe%20semantic%20maps%20%28obtained%20by%20DINOv2%20from%20the%20RGB%20input%29%2C%20then%20renders%20new%20RGB%0Aand%20semantic%20maps%20%28with%20back-surface%20culling%29%20under%20a%20novel%20rotated%20view.%20The%0Arefinement%20loss%20comes%20from%20comparing%20the%20rendered%20RGB%20and%20semantic%20maps%20with%0Athe%20query%20ones%2C%20back-propagating%20the%20gradients%20through%20the%20differentiable%0Arenderer%20to%20refine%20the%203D%20relative%20pose.%20As%20a%20result%2C%20our%20method%20can%20be%20readily%0Aapplied%20to%20unseen%20objects%2C%20given%20only%20a%20single%20RGB-D%20reference%2C%20without%0Alabel/training.%20Extensive%20experiments%20on%20LineMOD%2C%20LM-O%2C%20and%20YCB-V%20show%20that%20our%0Atraining-free%20method%20significantly%20outperforms%20the%20SOTA%20supervised%20methods%2C%0Aespecially%20under%20the%20rigorous%20Acc%405/10/15%7B%5Cdeg%7D%20metrics%20and%20the%20challenging%0Across-dataset%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18453v1&entry.124074799=Read"},
{"title": "GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension", "author": "Jiafeng Liang and Shixin Jiang and Zekun Wang and Haojie Pan and Zerui Chen and Zheng Chu and Ming Liu and Ruiji Fu and Zhongyuan Wang and Bing Qin", "abstract": "  There are substantial instructional videos on the Internet, which provide us\ntutorials for completing various tasks. Existing instructional video datasets\nonly focus on specific steps at the video level, lacking experiential\nguidelines at the task level, which can lead to beginners struggling to learn\nnew tasks due to the lack of relevant experience. Moreover, the specific steps\nwithout guidelines are trivial and unsystematic, making it difficult to provide\na clear tutorial. To address these problems, we present the GUIDE\n(Guideline-Guided) dataset, which contains 3.5K videos of 560 instructional\ntasks in 8 domains related to our daily life. Specifically, we annotate each\ninstructional task with a guideline, representing a common pattern shared by\nall task-related videos. On this basis, we annotate systematic specific steps,\nincluding their associated guideline steps, specific step descriptions and\ntimestamps. Our proposed benchmark consists of three sub-tasks to evaluate\ncomprehension ability of models: (1) Step Captioning: models have to generate\ncaptions for specific steps from videos. (2) Guideline Summarization: models\nhave to mine the common pattern in task-related videos and summarize a\nguideline from them. (3) Guideline-Guided Captioning: models have to generate\ncaptions for specific steps under the guide of guideline. We evaluate plenty of\nfoundation models with GUIDE and perform in-depth analysis. Given the diversity\nand practicality of GUIDE, we believe that it can be used as a better benchmark\nfor instructional video comprehension.\n", "link": "http://arxiv.org/abs/2406.18227v1", "date": "2024-06-26", "relevancy": 2.4175, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5005}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4763}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUIDE%3A%20A%20Guideline-Guided%20Dataset%20for%20Instructional%20Video%20Comprehension&body=Title%3A%20GUIDE%3A%20A%20Guideline-Guided%20Dataset%20for%20Instructional%20Video%20Comprehension%0AAuthor%3A%20Jiafeng%20Liang%20and%20Shixin%20Jiang%20and%20Zekun%20Wang%20and%20Haojie%20Pan%20and%20Zerui%20Chen%20and%20Zheng%20Chu%20and%20Ming%20Liu%20and%20Ruiji%20Fu%20and%20Zhongyuan%20Wang%20and%20Bing%20Qin%0AAbstract%3A%20%20%20There%20are%20substantial%20instructional%20videos%20on%20the%20Internet%2C%20which%20provide%20us%0Atutorials%20for%20completing%20various%20tasks.%20Existing%20instructional%20video%20datasets%0Aonly%20focus%20on%20specific%20steps%20at%20the%20video%20level%2C%20lacking%20experiential%0Aguidelines%20at%20the%20task%20level%2C%20which%20can%20lead%20to%20beginners%20struggling%20to%20learn%0Anew%20tasks%20due%20to%20the%20lack%20of%20relevant%20experience.%20Moreover%2C%20the%20specific%20steps%0Awithout%20guidelines%20are%20trivial%20and%20unsystematic%2C%20making%20it%20difficult%20to%20provide%0Aa%20clear%20tutorial.%20To%20address%20these%20problems%2C%20we%20present%20the%20GUIDE%0A%28Guideline-Guided%29%20dataset%2C%20which%20contains%203.5K%20videos%20of%20560%20instructional%0Atasks%20in%208%20domains%20related%20to%20our%20daily%20life.%20Specifically%2C%20we%20annotate%20each%0Ainstructional%20task%20with%20a%20guideline%2C%20representing%20a%20common%20pattern%20shared%20by%0Aall%20task-related%20videos.%20On%20this%20basis%2C%20we%20annotate%20systematic%20specific%20steps%2C%0Aincluding%20their%20associated%20guideline%20steps%2C%20specific%20step%20descriptions%20and%0Atimestamps.%20Our%20proposed%20benchmark%20consists%20of%20three%20sub-tasks%20to%20evaluate%0Acomprehension%20ability%20of%20models%3A%20%281%29%20Step%20Captioning%3A%20models%20have%20to%20generate%0Acaptions%20for%20specific%20steps%20from%20videos.%20%282%29%20Guideline%20Summarization%3A%20models%0Ahave%20to%20mine%20the%20common%20pattern%20in%20task-related%20videos%20and%20summarize%20a%0Aguideline%20from%20them.%20%283%29%20Guideline-Guided%20Captioning%3A%20models%20have%20to%20generate%0Acaptions%20for%20specific%20steps%20under%20the%20guide%20of%20guideline.%20We%20evaluate%20plenty%20of%0Afoundation%20models%20with%20GUIDE%20and%20perform%20in-depth%20analysis.%20Given%20the%20diversity%0Aand%20practicality%20of%20GUIDE%2C%20we%20believe%20that%20it%20can%20be%20used%20as%20a%20better%20benchmark%0Afor%20instructional%20video%20comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUIDE%253A%2520A%2520Guideline-Guided%2520Dataset%2520for%2520Instructional%2520Video%2520Comprehension%26entry.906535625%3DJiafeng%2520Liang%2520and%2520Shixin%2520Jiang%2520and%2520Zekun%2520Wang%2520and%2520Haojie%2520Pan%2520and%2520Zerui%2520Chen%2520and%2520Zheng%2520Chu%2520and%2520Ming%2520Liu%2520and%2520Ruiji%2520Fu%2520and%2520Zhongyuan%2520Wang%2520and%2520Bing%2520Qin%26entry.1292438233%3D%2520%2520There%2520are%2520substantial%2520instructional%2520videos%2520on%2520the%2520Internet%252C%2520which%2520provide%2520us%250Atutorials%2520for%2520completing%2520various%2520tasks.%2520Existing%2520instructional%2520video%2520datasets%250Aonly%2520focus%2520on%2520specific%2520steps%2520at%2520the%2520video%2520level%252C%2520lacking%2520experiential%250Aguidelines%2520at%2520the%2520task%2520level%252C%2520which%2520can%2520lead%2520to%2520beginners%2520struggling%2520to%2520learn%250Anew%2520tasks%2520due%2520to%2520the%2520lack%2520of%2520relevant%2520experience.%2520Moreover%252C%2520the%2520specific%2520steps%250Awithout%2520guidelines%2520are%2520trivial%2520and%2520unsystematic%252C%2520making%2520it%2520difficult%2520to%2520provide%250Aa%2520clear%2520tutorial.%2520To%2520address%2520these%2520problems%252C%2520we%2520present%2520the%2520GUIDE%250A%2528Guideline-Guided%2529%2520dataset%252C%2520which%2520contains%25203.5K%2520videos%2520of%2520560%2520instructional%250Atasks%2520in%25208%2520domains%2520related%2520to%2520our%2520daily%2520life.%2520Specifically%252C%2520we%2520annotate%2520each%250Ainstructional%2520task%2520with%2520a%2520guideline%252C%2520representing%2520a%2520common%2520pattern%2520shared%2520by%250Aall%2520task-related%2520videos.%2520On%2520this%2520basis%252C%2520we%2520annotate%2520systematic%2520specific%2520steps%252C%250Aincluding%2520their%2520associated%2520guideline%2520steps%252C%2520specific%2520step%2520descriptions%2520and%250Atimestamps.%2520Our%2520proposed%2520benchmark%2520consists%2520of%2520three%2520sub-tasks%2520to%2520evaluate%250Acomprehension%2520ability%2520of%2520models%253A%2520%25281%2529%2520Step%2520Captioning%253A%2520models%2520have%2520to%2520generate%250Acaptions%2520for%2520specific%2520steps%2520from%2520videos.%2520%25282%2529%2520Guideline%2520Summarization%253A%2520models%250Ahave%2520to%2520mine%2520the%2520common%2520pattern%2520in%2520task-related%2520videos%2520and%2520summarize%2520a%250Aguideline%2520from%2520them.%2520%25283%2529%2520Guideline-Guided%2520Captioning%253A%2520models%2520have%2520to%2520generate%250Acaptions%2520for%2520specific%2520steps%2520under%2520the%2520guide%2520of%2520guideline.%2520We%2520evaluate%2520plenty%2520of%250Afoundation%2520models%2520with%2520GUIDE%2520and%2520perform%2520in-depth%2520analysis.%2520Given%2520the%2520diversity%250Aand%2520practicality%2520of%2520GUIDE%252C%2520we%2520believe%2520that%2520it%2520can%2520be%2520used%2520as%2520a%2520better%2520benchmark%250Afor%2520instructional%2520video%2520comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUIDE%3A%20A%20Guideline-Guided%20Dataset%20for%20Instructional%20Video%20Comprehension&entry.906535625=Jiafeng%20Liang%20and%20Shixin%20Jiang%20and%20Zekun%20Wang%20and%20Haojie%20Pan%20and%20Zerui%20Chen%20and%20Zheng%20Chu%20and%20Ming%20Liu%20and%20Ruiji%20Fu%20and%20Zhongyuan%20Wang%20and%20Bing%20Qin&entry.1292438233=%20%20There%20are%20substantial%20instructional%20videos%20on%20the%20Internet%2C%20which%20provide%20us%0Atutorials%20for%20completing%20various%20tasks.%20Existing%20instructional%20video%20datasets%0Aonly%20focus%20on%20specific%20steps%20at%20the%20video%20level%2C%20lacking%20experiential%0Aguidelines%20at%20the%20task%20level%2C%20which%20can%20lead%20to%20beginners%20struggling%20to%20learn%0Anew%20tasks%20due%20to%20the%20lack%20of%20relevant%20experience.%20Moreover%2C%20the%20specific%20steps%0Awithout%20guidelines%20are%20trivial%20and%20unsystematic%2C%20making%20it%20difficult%20to%20provide%0Aa%20clear%20tutorial.%20To%20address%20these%20problems%2C%20we%20present%20the%20GUIDE%0A%28Guideline-Guided%29%20dataset%2C%20which%20contains%203.5K%20videos%20of%20560%20instructional%0Atasks%20in%208%20domains%20related%20to%20our%20daily%20life.%20Specifically%2C%20we%20annotate%20each%0Ainstructional%20task%20with%20a%20guideline%2C%20representing%20a%20common%20pattern%20shared%20by%0Aall%20task-related%20videos.%20On%20this%20basis%2C%20we%20annotate%20systematic%20specific%20steps%2C%0Aincluding%20their%20associated%20guideline%20steps%2C%20specific%20step%20descriptions%20and%0Atimestamps.%20Our%20proposed%20benchmark%20consists%20of%20three%20sub-tasks%20to%20evaluate%0Acomprehension%20ability%20of%20models%3A%20%281%29%20Step%20Captioning%3A%20models%20have%20to%20generate%0Acaptions%20for%20specific%20steps%20from%20videos.%20%282%29%20Guideline%20Summarization%3A%20models%0Ahave%20to%20mine%20the%20common%20pattern%20in%20task-related%20videos%20and%20summarize%20a%0Aguideline%20from%20them.%20%283%29%20Guideline-Guided%20Captioning%3A%20models%20have%20to%20generate%0Acaptions%20for%20specific%20steps%20under%20the%20guide%20of%20guideline.%20We%20evaluate%20plenty%20of%0Afoundation%20models%20with%20GUIDE%20and%20perform%20in-depth%20analysis.%20Given%20the%20diversity%0Aand%20practicality%20of%20GUIDE%2C%20we%20believe%20that%20it%20can%20be%20used%20as%20a%20better%20benchmark%0Afor%20instructional%20video%20comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18227v1&entry.124074799=Read"},
{"title": "APIGen: Automated Pipeline for Generating Verifiable and Diverse\n  Function-Calling Datasets", "author": "Zuxin Liu and Thai Hoang and Jianguo Zhang and Ming Zhu and Tian Lan and Shirley Kokane and Juntao Tan and Weiran Yao and Zhiwei Liu and Yihao Feng and Rithesh Murthy and Liangwei Yang and Silvio Savarese and Juan Carlos Niebles and Huan Wang and Shelby Heinecke and Caiming Xiong", "abstract": "  The advancement of function-calling agent models requires diverse, reliable,\nand high-quality datasets. This paper presents APIGen, an automated data\ngeneration pipeline designed to synthesize verifiable high-quality datasets for\nfunction-calling applications. We leverage APIGen and collect 3,673 executable\nAPIs across 21 different categories to generate diverse function-calling\ndatasets in a scalable and structured manner. Each data in our dataset is\nverified through three hierarchical stages: format checking, actual function\nexecutions, and semantic verification, ensuring its reliability and\ncorrectness. We demonstrate that models trained with our curated datasets, even\nwith only 7B parameters, can achieve state-of-the-art performance on the\nBerkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.\nMoreover, our 1B model achieves exceptional performance, surpassing\nGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000\nhigh-quality entries, aiming to advance the field of function-calling agent\ndomains. The dataset is available on Huggingface:\nhttps://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the\nproject homepage: https://apigen-pipeline.github.io/\n", "link": "http://arxiv.org/abs/2406.18518v1", "date": "2024-06-26", "relevancy": 2.3937, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5094}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4824}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APIGen%3A%20Automated%20Pipeline%20for%20Generating%20Verifiable%20and%20Diverse%0A%20%20Function-Calling%20Datasets&body=Title%3A%20APIGen%3A%20Automated%20Pipeline%20for%20Generating%20Verifiable%20and%20Diverse%0A%20%20Function-Calling%20Datasets%0AAuthor%3A%20Zuxin%20Liu%20and%20Thai%20Hoang%20and%20Jianguo%20Zhang%20and%20Ming%20Zhu%20and%20Tian%20Lan%20and%20Shirley%20Kokane%20and%20Juntao%20Tan%20and%20Weiran%20Yao%20and%20Zhiwei%20Liu%20and%20Yihao%20Feng%20and%20Rithesh%20Murthy%20and%20Liangwei%20Yang%20and%20Silvio%20Savarese%20and%20Juan%20Carlos%20Niebles%20and%20Huan%20Wang%20and%20Shelby%20Heinecke%20and%20Caiming%20Xiong%0AAbstract%3A%20%20%20The%20advancement%20of%20function-calling%20agent%20models%20requires%20diverse%2C%20reliable%2C%0Aand%20high-quality%20datasets.%20This%20paper%20presents%20APIGen%2C%20an%20automated%20data%0Ageneration%20pipeline%20designed%20to%20synthesize%20verifiable%20high-quality%20datasets%20for%0Afunction-calling%20applications.%20We%20leverage%20APIGen%20and%20collect%203%2C673%20executable%0AAPIs%20across%2021%20different%20categories%20to%20generate%20diverse%20function-calling%0Adatasets%20in%20a%20scalable%20and%20structured%20manner.%20Each%20data%20in%20our%20dataset%20is%0Averified%20through%20three%20hierarchical%20stages%3A%20format%20checking%2C%20actual%20function%0Aexecutions%2C%20and%20semantic%20verification%2C%20ensuring%20its%20reliability%20and%0Acorrectness.%20We%20demonstrate%20that%20models%20trained%20with%20our%20curated%20datasets%2C%20even%0Awith%20only%207B%20parameters%2C%20can%20achieve%20state-of-the-art%20performance%20on%20the%0ABerkeley%20Function-Calling%20Benchmark%2C%20outperforming%20multiple%20GPT-4%20models.%0AMoreover%2C%20our%201B%20model%20achieves%20exceptional%20performance%2C%20surpassing%0AGPT-3.5-Turbo%20and%20Claude-3%20Haiku.%20We%20release%20a%20dataset%20containing%2060%2C000%0Ahigh-quality%20entries%2C%20aiming%20to%20advance%20the%20field%20of%20function-calling%20agent%0Adomains.%20The%20dataset%20is%20available%20on%20Huggingface%3A%0Ahttps%3A//huggingface.co/datasets/Salesforce/xlam-function-calling-60k%20and%20the%0Aproject%20homepage%3A%20https%3A//apigen-pipeline.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPIGen%253A%2520Automated%2520Pipeline%2520for%2520Generating%2520Verifiable%2520and%2520Diverse%250A%2520%2520Function-Calling%2520Datasets%26entry.906535625%3DZuxin%2520Liu%2520and%2520Thai%2520Hoang%2520and%2520Jianguo%2520Zhang%2520and%2520Ming%2520Zhu%2520and%2520Tian%2520Lan%2520and%2520Shirley%2520Kokane%2520and%2520Juntao%2520Tan%2520and%2520Weiran%2520Yao%2520and%2520Zhiwei%2520Liu%2520and%2520Yihao%2520Feng%2520and%2520Rithesh%2520Murthy%2520and%2520Liangwei%2520Yang%2520and%2520Silvio%2520Savarese%2520and%2520Juan%2520Carlos%2520Niebles%2520and%2520Huan%2520Wang%2520and%2520Shelby%2520Heinecke%2520and%2520Caiming%2520Xiong%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520function-calling%2520agent%2520models%2520requires%2520diverse%252C%2520reliable%252C%250Aand%2520high-quality%2520datasets.%2520This%2520paper%2520presents%2520APIGen%252C%2520an%2520automated%2520data%250Ageneration%2520pipeline%2520designed%2520to%2520synthesize%2520verifiable%2520high-quality%2520datasets%2520for%250Afunction-calling%2520applications.%2520We%2520leverage%2520APIGen%2520and%2520collect%25203%252C673%2520executable%250AAPIs%2520across%252021%2520different%2520categories%2520to%2520generate%2520diverse%2520function-calling%250Adatasets%2520in%2520a%2520scalable%2520and%2520structured%2520manner.%2520Each%2520data%2520in%2520our%2520dataset%2520is%250Averified%2520through%2520three%2520hierarchical%2520stages%253A%2520format%2520checking%252C%2520actual%2520function%250Aexecutions%252C%2520and%2520semantic%2520verification%252C%2520ensuring%2520its%2520reliability%2520and%250Acorrectness.%2520We%2520demonstrate%2520that%2520models%2520trained%2520with%2520our%2520curated%2520datasets%252C%2520even%250Awith%2520only%25207B%2520parameters%252C%2520can%2520achieve%2520state-of-the-art%2520performance%2520on%2520the%250ABerkeley%2520Function-Calling%2520Benchmark%252C%2520outperforming%2520multiple%2520GPT-4%2520models.%250AMoreover%252C%2520our%25201B%2520model%2520achieves%2520exceptional%2520performance%252C%2520surpassing%250AGPT-3.5-Turbo%2520and%2520Claude-3%2520Haiku.%2520We%2520release%2520a%2520dataset%2520containing%252060%252C000%250Ahigh-quality%2520entries%252C%2520aiming%2520to%2520advance%2520the%2520field%2520of%2520function-calling%2520agent%250Adomains.%2520The%2520dataset%2520is%2520available%2520on%2520Huggingface%253A%250Ahttps%253A//huggingface.co/datasets/Salesforce/xlam-function-calling-60k%2520and%2520the%250Aproject%2520homepage%253A%2520https%253A//apigen-pipeline.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APIGen%3A%20Automated%20Pipeline%20for%20Generating%20Verifiable%20and%20Diverse%0A%20%20Function-Calling%20Datasets&entry.906535625=Zuxin%20Liu%20and%20Thai%20Hoang%20and%20Jianguo%20Zhang%20and%20Ming%20Zhu%20and%20Tian%20Lan%20and%20Shirley%20Kokane%20and%20Juntao%20Tan%20and%20Weiran%20Yao%20and%20Zhiwei%20Liu%20and%20Yihao%20Feng%20and%20Rithesh%20Murthy%20and%20Liangwei%20Yang%20and%20Silvio%20Savarese%20and%20Juan%20Carlos%20Niebles%20and%20Huan%20Wang%20and%20Shelby%20Heinecke%20and%20Caiming%20Xiong&entry.1292438233=%20%20The%20advancement%20of%20function-calling%20agent%20models%20requires%20diverse%2C%20reliable%2C%0Aand%20high-quality%20datasets.%20This%20paper%20presents%20APIGen%2C%20an%20automated%20data%0Ageneration%20pipeline%20designed%20to%20synthesize%20verifiable%20high-quality%20datasets%20for%0Afunction-calling%20applications.%20We%20leverage%20APIGen%20and%20collect%203%2C673%20executable%0AAPIs%20across%2021%20different%20categories%20to%20generate%20diverse%20function-calling%0Adatasets%20in%20a%20scalable%20and%20structured%20manner.%20Each%20data%20in%20our%20dataset%20is%0Averified%20through%20three%20hierarchical%20stages%3A%20format%20checking%2C%20actual%20function%0Aexecutions%2C%20and%20semantic%20verification%2C%20ensuring%20its%20reliability%20and%0Acorrectness.%20We%20demonstrate%20that%20models%20trained%20with%20our%20curated%20datasets%2C%20even%0Awith%20only%207B%20parameters%2C%20can%20achieve%20state-of-the-art%20performance%20on%20the%0ABerkeley%20Function-Calling%20Benchmark%2C%20outperforming%20multiple%20GPT-4%20models.%0AMoreover%2C%20our%201B%20model%20achieves%20exceptional%20performance%2C%20surpassing%0AGPT-3.5-Turbo%20and%20Claude-3%20Haiku.%20We%20release%20a%20dataset%20containing%2060%2C000%0Ahigh-quality%20entries%2C%20aiming%20to%20advance%20the%20field%20of%20function-calling%20agent%0Adomains.%20The%20dataset%20is%20available%20on%20Huggingface%3A%0Ahttps%3A//huggingface.co/datasets/Salesforce/xlam-function-calling-60k%20and%20the%0Aproject%20homepage%3A%20https%3A//apigen-pipeline.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18518v1&entry.124074799=Read"},
{"title": "Zero-shot prompt-based classification: topic labeling in times of\n  foundation models in German Tweets", "author": "Simon M\u00fcnker and Kai Kugler and Achim Rettinger", "abstract": "  Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.\n", "link": "http://arxiv.org/abs/2406.18239v1", "date": "2024-06-26", "relevancy": 2.3617, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4934}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4655}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20prompt-based%20classification%3A%20topic%20labeling%20in%20times%20of%0A%20%20foundation%20models%20in%20German%20Tweets&body=Title%3A%20Zero-shot%20prompt-based%20classification%3A%20topic%20labeling%20in%20times%20of%0A%20%20foundation%20models%20in%20German%20Tweets%0AAuthor%3A%20Simon%20M%C3%BCnker%20and%20Kai%20Kugler%20and%20Achim%20Rettinger%0AAbstract%3A%20%20%20Filtering%20and%20annotating%20textual%20data%20are%20routine%20tasks%20in%20many%20areas%2C%20like%0Asocial%20media%20or%20news%20analytics.%20Automating%20these%20tasks%20allows%20to%20scale%20the%0Aanalyses%20wrt.%20speed%20and%20breadth%20of%20content%20covered%20and%20decreases%20the%20manual%0Aeffort%20required.%20Due%20to%20technical%20advancements%20in%20Natural%20Language%20Processing%2C%0Aspecifically%20the%20success%20of%20large%20foundation%20models%2C%20a%20new%20tool%20for%20automating%0Asuch%20annotation%20processes%20by%20using%20a%20text-to-text%20interface%20given%20written%0Aguidelines%20without%20providing%20training%20samples%20has%20become%20available.%0A%20%20In%20this%20work%2C%20we%20assess%20these%20advancements%20in-the-wild%20by%20empirically%20testing%0Athem%20in%20an%20annotation%20task%20on%20German%20Twitter%20data%20about%20social%20and%20political%0AEuropean%20crises.%20We%20compare%20the%20prompt-based%20results%20with%20our%20human%20annotation%0Aand%20preceding%20classification%20approaches%2C%20including%20Naive%20Bayes%20and%20a%20BERT-based%0Afine-tuning/domain%20adaptation%20pipeline.%20Our%20results%20show%20that%20the%20prompt-based%0Aapproach%20-%20despite%20being%20limited%20by%20local%20computation%20resources%20during%20the%0Amodel%20selection%20-%20is%20comparable%20with%20the%20fine-tuned%20BERT%20but%20without%20any%0Aannotated%20training%20data.%20Our%20findings%20emphasize%20the%20ongoing%20paradigm%20shift%20in%0Athe%20NLP%20landscape%2C%20i.e.%2C%20the%20unification%20of%20downstream%20tasks%20and%20elimination%20of%0Athe%20need%20for%20pre-labeled%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520prompt-based%2520classification%253A%2520topic%2520labeling%2520in%2520times%2520of%250A%2520%2520foundation%2520models%2520in%2520German%2520Tweets%26entry.906535625%3DSimon%2520M%25C3%25BCnker%2520and%2520Kai%2520Kugler%2520and%2520Achim%2520Rettinger%26entry.1292438233%3D%2520%2520Filtering%2520and%2520annotating%2520textual%2520data%2520are%2520routine%2520tasks%2520in%2520many%2520areas%252C%2520like%250Asocial%2520media%2520or%2520news%2520analytics.%2520Automating%2520these%2520tasks%2520allows%2520to%2520scale%2520the%250Aanalyses%2520wrt.%2520speed%2520and%2520breadth%2520of%2520content%2520covered%2520and%2520decreases%2520the%2520manual%250Aeffort%2520required.%2520Due%2520to%2520technical%2520advancements%2520in%2520Natural%2520Language%2520Processing%252C%250Aspecifically%2520the%2520success%2520of%2520large%2520foundation%2520models%252C%2520a%2520new%2520tool%2520for%2520automating%250Asuch%2520annotation%2520processes%2520by%2520using%2520a%2520text-to-text%2520interface%2520given%2520written%250Aguidelines%2520without%2520providing%2520training%2520samples%2520has%2520become%2520available.%250A%2520%2520In%2520this%2520work%252C%2520we%2520assess%2520these%2520advancements%2520in-the-wild%2520by%2520empirically%2520testing%250Athem%2520in%2520an%2520annotation%2520task%2520on%2520German%2520Twitter%2520data%2520about%2520social%2520and%2520political%250AEuropean%2520crises.%2520We%2520compare%2520the%2520prompt-based%2520results%2520with%2520our%2520human%2520annotation%250Aand%2520preceding%2520classification%2520approaches%252C%2520including%2520Naive%2520Bayes%2520and%2520a%2520BERT-based%250Afine-tuning/domain%2520adaptation%2520pipeline.%2520Our%2520results%2520show%2520that%2520the%2520prompt-based%250Aapproach%2520-%2520despite%2520being%2520limited%2520by%2520local%2520computation%2520resources%2520during%2520the%250Amodel%2520selection%2520-%2520is%2520comparable%2520with%2520the%2520fine-tuned%2520BERT%2520but%2520without%2520any%250Aannotated%2520training%2520data.%2520Our%2520findings%2520emphasize%2520the%2520ongoing%2520paradigm%2520shift%2520in%250Athe%2520NLP%2520landscape%252C%2520i.e.%252C%2520the%2520unification%2520of%2520downstream%2520tasks%2520and%2520elimination%2520of%250Athe%2520need%2520for%2520pre-labeled%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20prompt-based%20classification%3A%20topic%20labeling%20in%20times%20of%0A%20%20foundation%20models%20in%20German%20Tweets&entry.906535625=Simon%20M%C3%BCnker%20and%20Kai%20Kugler%20and%20Achim%20Rettinger&entry.1292438233=%20%20Filtering%20and%20annotating%20textual%20data%20are%20routine%20tasks%20in%20many%20areas%2C%20like%0Asocial%20media%20or%20news%20analytics.%20Automating%20these%20tasks%20allows%20to%20scale%20the%0Aanalyses%20wrt.%20speed%20and%20breadth%20of%20content%20covered%20and%20decreases%20the%20manual%0Aeffort%20required.%20Due%20to%20technical%20advancements%20in%20Natural%20Language%20Processing%2C%0Aspecifically%20the%20success%20of%20large%20foundation%20models%2C%20a%20new%20tool%20for%20automating%0Asuch%20annotation%20processes%20by%20using%20a%20text-to-text%20interface%20given%20written%0Aguidelines%20without%20providing%20training%20samples%20has%20become%20available.%0A%20%20In%20this%20work%2C%20we%20assess%20these%20advancements%20in-the-wild%20by%20empirically%20testing%0Athem%20in%20an%20annotation%20task%20on%20German%20Twitter%20data%20about%20social%20and%20political%0AEuropean%20crises.%20We%20compare%20the%20prompt-based%20results%20with%20our%20human%20annotation%0Aand%20preceding%20classification%20approaches%2C%20including%20Naive%20Bayes%20and%20a%20BERT-based%0Afine-tuning/domain%20adaptation%20pipeline.%20Our%20results%20show%20that%20the%20prompt-based%0Aapproach%20-%20despite%20being%20limited%20by%20local%20computation%20resources%20during%20the%0Amodel%20selection%20-%20is%20comparable%20with%20the%20fine-tuned%20BERT%20but%20without%20any%0Aannotated%20training%20data.%20Our%20findings%20emphasize%20the%20ongoing%20paradigm%20shift%20in%0Athe%20NLP%20landscape%2C%20i.e.%2C%20the%20unification%20of%20downstream%20tasks%20and%20elimination%20of%0Athe%20need%20for%20pre-labeled%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18239v1&entry.124074799=Read"},
{"title": "BiTrack: Bidirectional Offline 3D Multi-Object Tracking Using\n  Camera-LiDAR Data", "author": "Kemiao Huang and Meiying Zhang and Qi Hao", "abstract": "  Compared with real-time multi-object tracking (MOT), offline multi-object\ntracking (OMOT) has the advantages to perform 2D-3D detection fusion, erroneous\nlink correction, and full track optimization but has to deal with the\nchallenges from bounding box misalignment and track evaluation, editing, and\nrefinement. This paper proposes \"BiTrack\", a 3D OMOT framework that includes\nmodules of 2D-3D detection fusion, initial trajectory generation, and\nbidirectional trajectory re-optimization to achieve optimal tracking results\nfrom camera-LiDAR data. The novelty of this paper includes threefold: (1)\ndevelopment of a point-level object registration technique that employs a\ndensity-based similarity metric to achieve accurate fusion of 2D-3D detection\nresults; (2) development of a set of data association and track management\nskills that utilizes a vertex-based similarity metric as well as false alarm\nrejection and track recovery mechanisms to generate reliable bidirectional\nobject trajectories; (3) development of a trajectory re-optimization scheme\nthat re-organizes track fragments of different fidelities in a greedy fashion,\nas well as refines each trajectory with completion and smoothing techniques.\nThe experiment results on the KITTI dataset demonstrate that BiTrack achieves\nthe state-of-the-art performance for 3D OMOT tasks in terms of accuracy and\nefficiency.\n", "link": "http://arxiv.org/abs/2406.18414v1", "date": "2024-06-26", "relevancy": 2.3499, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6211}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiTrack%3A%20Bidirectional%20Offline%203D%20Multi-Object%20Tracking%20Using%0A%20%20Camera-LiDAR%20Data&body=Title%3A%20BiTrack%3A%20Bidirectional%20Offline%203D%20Multi-Object%20Tracking%20Using%0A%20%20Camera-LiDAR%20Data%0AAuthor%3A%20Kemiao%20Huang%20and%20Meiying%20Zhang%20and%20Qi%20Hao%0AAbstract%3A%20%20%20Compared%20with%20real-time%20multi-object%20tracking%20%28MOT%29%2C%20offline%20multi-object%0Atracking%20%28OMOT%29%20has%20the%20advantages%20to%20perform%202D-3D%20detection%20fusion%2C%20erroneous%0Alink%20correction%2C%20and%20full%20track%20optimization%20but%20has%20to%20deal%20with%20the%0Achallenges%20from%20bounding%20box%20misalignment%20and%20track%20evaluation%2C%20editing%2C%20and%0Arefinement.%20This%20paper%20proposes%20%22BiTrack%22%2C%20a%203D%20OMOT%20framework%20that%20includes%0Amodules%20of%202D-3D%20detection%20fusion%2C%20initial%20trajectory%20generation%2C%20and%0Abidirectional%20trajectory%20re-optimization%20to%20achieve%20optimal%20tracking%20results%0Afrom%20camera-LiDAR%20data.%20The%20novelty%20of%20this%20paper%20includes%20threefold%3A%20%281%29%0Adevelopment%20of%20a%20point-level%20object%20registration%20technique%20that%20employs%20a%0Adensity-based%20similarity%20metric%20to%20achieve%20accurate%20fusion%20of%202D-3D%20detection%0Aresults%3B%20%282%29%20development%20of%20a%20set%20of%20data%20association%20and%20track%20management%0Askills%20that%20utilizes%20a%20vertex-based%20similarity%20metric%20as%20well%20as%20false%20alarm%0Arejection%20and%20track%20recovery%20mechanisms%20to%20generate%20reliable%20bidirectional%0Aobject%20trajectories%3B%20%283%29%20development%20of%20a%20trajectory%20re-optimization%20scheme%0Athat%20re-organizes%20track%20fragments%20of%20different%20fidelities%20in%20a%20greedy%20fashion%2C%0Aas%20well%20as%20refines%20each%20trajectory%20with%20completion%20and%20smoothing%20techniques.%0AThe%20experiment%20results%20on%20the%20KITTI%20dataset%20demonstrate%20that%20BiTrack%20achieves%0Athe%20state-of-the-art%20performance%20for%203D%20OMOT%20tasks%20in%20terms%20of%20accuracy%20and%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiTrack%253A%2520Bidirectional%2520Offline%25203D%2520Multi-Object%2520Tracking%2520Using%250A%2520%2520Camera-LiDAR%2520Data%26entry.906535625%3DKemiao%2520Huang%2520and%2520Meiying%2520Zhang%2520and%2520Qi%2520Hao%26entry.1292438233%3D%2520%2520Compared%2520with%2520real-time%2520multi-object%2520tracking%2520%2528MOT%2529%252C%2520offline%2520multi-object%250Atracking%2520%2528OMOT%2529%2520has%2520the%2520advantages%2520to%2520perform%25202D-3D%2520detection%2520fusion%252C%2520erroneous%250Alink%2520correction%252C%2520and%2520full%2520track%2520optimization%2520but%2520has%2520to%2520deal%2520with%2520the%250Achallenges%2520from%2520bounding%2520box%2520misalignment%2520and%2520track%2520evaluation%252C%2520editing%252C%2520and%250Arefinement.%2520This%2520paper%2520proposes%2520%2522BiTrack%2522%252C%2520a%25203D%2520OMOT%2520framework%2520that%2520includes%250Amodules%2520of%25202D-3D%2520detection%2520fusion%252C%2520initial%2520trajectory%2520generation%252C%2520and%250Abidirectional%2520trajectory%2520re-optimization%2520to%2520achieve%2520optimal%2520tracking%2520results%250Afrom%2520camera-LiDAR%2520data.%2520The%2520novelty%2520of%2520this%2520paper%2520includes%2520threefold%253A%2520%25281%2529%250Adevelopment%2520of%2520a%2520point-level%2520object%2520registration%2520technique%2520that%2520employs%2520a%250Adensity-based%2520similarity%2520metric%2520to%2520achieve%2520accurate%2520fusion%2520of%25202D-3D%2520detection%250Aresults%253B%2520%25282%2529%2520development%2520of%2520a%2520set%2520of%2520data%2520association%2520and%2520track%2520management%250Askills%2520that%2520utilizes%2520a%2520vertex-based%2520similarity%2520metric%2520as%2520well%2520as%2520false%2520alarm%250Arejection%2520and%2520track%2520recovery%2520mechanisms%2520to%2520generate%2520reliable%2520bidirectional%250Aobject%2520trajectories%253B%2520%25283%2529%2520development%2520of%2520a%2520trajectory%2520re-optimization%2520scheme%250Athat%2520re-organizes%2520track%2520fragments%2520of%2520different%2520fidelities%2520in%2520a%2520greedy%2520fashion%252C%250Aas%2520well%2520as%2520refines%2520each%2520trajectory%2520with%2520completion%2520and%2520smoothing%2520techniques.%250AThe%2520experiment%2520results%2520on%2520the%2520KITTI%2520dataset%2520demonstrate%2520that%2520BiTrack%2520achieves%250Athe%2520state-of-the-art%2520performance%2520for%25203D%2520OMOT%2520tasks%2520in%2520terms%2520of%2520accuracy%2520and%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiTrack%3A%20Bidirectional%20Offline%203D%20Multi-Object%20Tracking%20Using%0A%20%20Camera-LiDAR%20Data&entry.906535625=Kemiao%20Huang%20and%20Meiying%20Zhang%20and%20Qi%20Hao&entry.1292438233=%20%20Compared%20with%20real-time%20multi-object%20tracking%20%28MOT%29%2C%20offline%20multi-object%0Atracking%20%28OMOT%29%20has%20the%20advantages%20to%20perform%202D-3D%20detection%20fusion%2C%20erroneous%0Alink%20correction%2C%20and%20full%20track%20optimization%20but%20has%20to%20deal%20with%20the%0Achallenges%20from%20bounding%20box%20misalignment%20and%20track%20evaluation%2C%20editing%2C%20and%0Arefinement.%20This%20paper%20proposes%20%22BiTrack%22%2C%20a%203D%20OMOT%20framework%20that%20includes%0Amodules%20of%202D-3D%20detection%20fusion%2C%20initial%20trajectory%20generation%2C%20and%0Abidirectional%20trajectory%20re-optimization%20to%20achieve%20optimal%20tracking%20results%0Afrom%20camera-LiDAR%20data.%20The%20novelty%20of%20this%20paper%20includes%20threefold%3A%20%281%29%0Adevelopment%20of%20a%20point-level%20object%20registration%20technique%20that%20employs%20a%0Adensity-based%20similarity%20metric%20to%20achieve%20accurate%20fusion%20of%202D-3D%20detection%0Aresults%3B%20%282%29%20development%20of%20a%20set%20of%20data%20association%20and%20track%20management%0Askills%20that%20utilizes%20a%20vertex-based%20similarity%20metric%20as%20well%20as%20false%20alarm%0Arejection%20and%20track%20recovery%20mechanisms%20to%20generate%20reliable%20bidirectional%0Aobject%20trajectories%3B%20%283%29%20development%20of%20a%20trajectory%20re-optimization%20scheme%0Athat%20re-organizes%20track%20fragments%20of%20different%20fidelities%20in%20a%20greedy%20fashion%2C%0Aas%20well%20as%20refines%20each%20trajectory%20with%20completion%20and%20smoothing%20techniques.%0AThe%20experiment%20results%20on%20the%20KITTI%20dataset%20demonstrate%20that%20BiTrack%20achieves%0Athe%20state-of-the-art%20performance%20for%203D%20OMOT%20tasks%20in%20terms%20of%20accuracy%20and%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18414v1&entry.124074799=Read"},
{"title": "Topological data quality via 0-dimensional persistence matching", "author": "\u00c1lvaro Torras-Casas and Eduardo Paluzo-Hidalgo and Rocio Gonzalez-Diaz", "abstract": "  Data quality is crucial for the successful training, generalization and\nperformance of artificial intelligence models. We propose to measure data\nquality for supervised learning using topological data analysis techniques.\nSpecifically, we provide a novel topological invariant based on persistence\nmatchings induced by inclusions and using $0$-dimensional persistent homology.\nWe show that such an invariant is stable. We provide an algorithm and relate it\nto images, kernels, and cokernels of the induced morphisms. Also, we show that\nthe invariant allows us to understand whether the subset \"represents well\" the\nclusters from the larger dataset or not, and we also use it to estimate bounds\nfor the Hausdorff distance between the subset and the complete dataset. This\napproach enables us to explain why the chosen dataset will lead to poor\nperformance.\n", "link": "http://arxiv.org/abs/2306.02411v2", "date": "2024-06-26", "relevancy": 2.3426, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5002}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4527}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20data%20quality%20via%200-dimensional%20persistence%20matching&body=Title%3A%20Topological%20data%20quality%20via%200-dimensional%20persistence%20matching%0AAuthor%3A%20%C3%81lvaro%20Torras-Casas%20and%20Eduardo%20Paluzo-Hidalgo%20and%20Rocio%20Gonzalez-Diaz%0AAbstract%3A%20%20%20Data%20quality%20is%20crucial%20for%20the%20successful%20training%2C%20generalization%20and%0Aperformance%20of%20artificial%20intelligence%20models.%20We%20propose%20to%20measure%20data%0Aquality%20for%20supervised%20learning%20using%20topological%20data%20analysis%20techniques.%0ASpecifically%2C%20we%20provide%20a%20novel%20topological%20invariant%20based%20on%20persistence%0Amatchings%20induced%20by%20inclusions%20and%20using%20%240%24-dimensional%20persistent%20homology.%0AWe%20show%20that%20such%20an%20invariant%20is%20stable.%20We%20provide%20an%20algorithm%20and%20relate%20it%0Ato%20images%2C%20kernels%2C%20and%20cokernels%20of%20the%20induced%20morphisms.%20Also%2C%20we%20show%20that%0Athe%20invariant%20allows%20us%20to%20understand%20whether%20the%20subset%20%22represents%20well%22%20the%0Aclusters%20from%20the%20larger%20dataset%20or%20not%2C%20and%20we%20also%20use%20it%20to%20estimate%20bounds%0Afor%20the%20Hausdorff%20distance%20between%20the%20subset%20and%20the%20complete%20dataset.%20This%0Aapproach%20enables%20us%20to%20explain%20why%20the%20chosen%20dataset%20will%20lead%20to%20poor%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520data%2520quality%2520via%25200-dimensional%2520persistence%2520matching%26entry.906535625%3D%25C3%2581lvaro%2520Torras-Casas%2520and%2520Eduardo%2520Paluzo-Hidalgo%2520and%2520Rocio%2520Gonzalez-Diaz%26entry.1292438233%3D%2520%2520Data%2520quality%2520is%2520crucial%2520for%2520the%2520successful%2520training%252C%2520generalization%2520and%250Aperformance%2520of%2520artificial%2520intelligence%2520models.%2520We%2520propose%2520to%2520measure%2520data%250Aquality%2520for%2520supervised%2520learning%2520using%2520topological%2520data%2520analysis%2520techniques.%250ASpecifically%252C%2520we%2520provide%2520a%2520novel%2520topological%2520invariant%2520based%2520on%2520persistence%250Amatchings%2520induced%2520by%2520inclusions%2520and%2520using%2520%25240%2524-dimensional%2520persistent%2520homology.%250AWe%2520show%2520that%2520such%2520an%2520invariant%2520is%2520stable.%2520We%2520provide%2520an%2520algorithm%2520and%2520relate%2520it%250Ato%2520images%252C%2520kernels%252C%2520and%2520cokernels%2520of%2520the%2520induced%2520morphisms.%2520Also%252C%2520we%2520show%2520that%250Athe%2520invariant%2520allows%2520us%2520to%2520understand%2520whether%2520the%2520subset%2520%2522represents%2520well%2522%2520the%250Aclusters%2520from%2520the%2520larger%2520dataset%2520or%2520not%252C%2520and%2520we%2520also%2520use%2520it%2520to%2520estimate%2520bounds%250Afor%2520the%2520Hausdorff%2520distance%2520between%2520the%2520subset%2520and%2520the%2520complete%2520dataset.%2520This%250Aapproach%2520enables%2520us%2520to%2520explain%2520why%2520the%2520chosen%2520dataset%2520will%2520lead%2520to%2520poor%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20data%20quality%20via%200-dimensional%20persistence%20matching&entry.906535625=%C3%81lvaro%20Torras-Casas%20and%20Eduardo%20Paluzo-Hidalgo%20and%20Rocio%20Gonzalez-Diaz&entry.1292438233=%20%20Data%20quality%20is%20crucial%20for%20the%20successful%20training%2C%20generalization%20and%0Aperformance%20of%20artificial%20intelligence%20models.%20We%20propose%20to%20measure%20data%0Aquality%20for%20supervised%20learning%20using%20topological%20data%20analysis%20techniques.%0ASpecifically%2C%20we%20provide%20a%20novel%20topological%20invariant%20based%20on%20persistence%0Amatchings%20induced%20by%20inclusions%20and%20using%20%240%24-dimensional%20persistent%20homology.%0AWe%20show%20that%20such%20an%20invariant%20is%20stable.%20We%20provide%20an%20algorithm%20and%20relate%20it%0Ato%20images%2C%20kernels%2C%20and%20cokernels%20of%20the%20induced%20morphisms.%20Also%2C%20we%20show%20that%0Athe%20invariant%20allows%20us%20to%20understand%20whether%20the%20subset%20%22represents%20well%22%20the%0Aclusters%20from%20the%20larger%20dataset%20or%20not%2C%20and%20we%20also%20use%20it%20to%20estimate%20bounds%0Afor%20the%20Hausdorff%20distance%20between%20the%20subset%20and%20the%20complete%20dataset.%20This%0Aapproach%20enables%20us%20to%20explain%20why%20the%20chosen%20dataset%20will%20lead%20to%20poor%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02411v2&entry.124074799=Read"},
{"title": "ContactNet: Geometric-Based Deep Learning Model for Predicting\n  Protein-Protein Interactions", "author": "Matan Halfon and Tomer Cohen and Raanan Fattal and Dina Schneidman-Duhovny", "abstract": "  Deep learning approaches achieved significant progress in predicting protein\nstructures. These methods are often applied to protein-protein interactions\n(PPIs) yet require Multiple Sequence Alignment (MSA) which is unavailable for\nvarious interactions, such as antibody-antigen. Computational docking methods\nare capable of sampling accurate complex models, but also produce thousands of\ninvalid configurations. The design of scoring functions for identifying\naccurate models is a long-standing challenge. We develop a novel\nattention-based Graph Neural Network (GNN), ContactNet, for classifying PPI\nmodels obtained from docking algorithms into accurate and incorrect ones. When\ntrained on docked antigen and modeled antibody structures, ContactNet doubles\nthe accuracy of current state-of-the-art scoring functions, achieving accurate\nmodels among its Top-10 at 43% of the test cases. When applied to unbound\nantibodies, its Top-10 accuracy increases to 65%. This performance is achieved\nwithout MSA and the approach is applicable to other types of interactions, such\nas host-pathogens or general PPIs.\n", "link": "http://arxiv.org/abs/2406.18314v1", "date": "2024-06-26", "relevancy": 2.299, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4662}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4629}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContactNet%3A%20Geometric-Based%20Deep%20Learning%20Model%20for%20Predicting%0A%20%20Protein-Protein%20Interactions&body=Title%3A%20ContactNet%3A%20Geometric-Based%20Deep%20Learning%20Model%20for%20Predicting%0A%20%20Protein-Protein%20Interactions%0AAuthor%3A%20Matan%20Halfon%20and%20Tomer%20Cohen%20and%20Raanan%20Fattal%20and%20Dina%20Schneidman-Duhovny%0AAbstract%3A%20%20%20Deep%20learning%20approaches%20achieved%20significant%20progress%20in%20predicting%20protein%0Astructures.%20These%20methods%20are%20often%20applied%20to%20protein-protein%20interactions%0A%28PPIs%29%20yet%20require%20Multiple%20Sequence%20Alignment%20%28MSA%29%20which%20is%20unavailable%20for%0Avarious%20interactions%2C%20such%20as%20antibody-antigen.%20Computational%20docking%20methods%0Aare%20capable%20of%20sampling%20accurate%20complex%20models%2C%20but%20also%20produce%20thousands%20of%0Ainvalid%20configurations.%20The%20design%20of%20scoring%20functions%20for%20identifying%0Aaccurate%20models%20is%20a%20long-standing%20challenge.%20We%20develop%20a%20novel%0Aattention-based%20Graph%20Neural%20Network%20%28GNN%29%2C%20ContactNet%2C%20for%20classifying%20PPI%0Amodels%20obtained%20from%20docking%20algorithms%20into%20accurate%20and%20incorrect%20ones.%20When%0Atrained%20on%20docked%20antigen%20and%20modeled%20antibody%20structures%2C%20ContactNet%20doubles%0Athe%20accuracy%20of%20current%20state-of-the-art%20scoring%20functions%2C%20achieving%20accurate%0Amodels%20among%20its%20Top-10%20at%2043%25%20of%20the%20test%20cases.%20When%20applied%20to%20unbound%0Aantibodies%2C%20its%20Top-10%20accuracy%20increases%20to%2065%25.%20This%20performance%20is%20achieved%0Awithout%20MSA%20and%20the%20approach%20is%20applicable%20to%20other%20types%20of%20interactions%2C%20such%0Aas%20host-pathogens%20or%20general%20PPIs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContactNet%253A%2520Geometric-Based%2520Deep%2520Learning%2520Model%2520for%2520Predicting%250A%2520%2520Protein-Protein%2520Interactions%26entry.906535625%3DMatan%2520Halfon%2520and%2520Tomer%2520Cohen%2520and%2520Raanan%2520Fattal%2520and%2520Dina%2520Schneidman-Duhovny%26entry.1292438233%3D%2520%2520Deep%2520learning%2520approaches%2520achieved%2520significant%2520progress%2520in%2520predicting%2520protein%250Astructures.%2520These%2520methods%2520are%2520often%2520applied%2520to%2520protein-protein%2520interactions%250A%2528PPIs%2529%2520yet%2520require%2520Multiple%2520Sequence%2520Alignment%2520%2528MSA%2529%2520which%2520is%2520unavailable%2520for%250Avarious%2520interactions%252C%2520such%2520as%2520antibody-antigen.%2520Computational%2520docking%2520methods%250Aare%2520capable%2520of%2520sampling%2520accurate%2520complex%2520models%252C%2520but%2520also%2520produce%2520thousands%2520of%250Ainvalid%2520configurations.%2520The%2520design%2520of%2520scoring%2520functions%2520for%2520identifying%250Aaccurate%2520models%2520is%2520a%2520long-standing%2520challenge.%2520We%2520develop%2520a%2520novel%250Aattention-based%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%252C%2520ContactNet%252C%2520for%2520classifying%2520PPI%250Amodels%2520obtained%2520from%2520docking%2520algorithms%2520into%2520accurate%2520and%2520incorrect%2520ones.%2520When%250Atrained%2520on%2520docked%2520antigen%2520and%2520modeled%2520antibody%2520structures%252C%2520ContactNet%2520doubles%250Athe%2520accuracy%2520of%2520current%2520state-of-the-art%2520scoring%2520functions%252C%2520achieving%2520accurate%250Amodels%2520among%2520its%2520Top-10%2520at%252043%2525%2520of%2520the%2520test%2520cases.%2520When%2520applied%2520to%2520unbound%250Aantibodies%252C%2520its%2520Top-10%2520accuracy%2520increases%2520to%252065%2525.%2520This%2520performance%2520is%2520achieved%250Awithout%2520MSA%2520and%2520the%2520approach%2520is%2520applicable%2520to%2520other%2520types%2520of%2520interactions%252C%2520such%250Aas%2520host-pathogens%2520or%2520general%2520PPIs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContactNet%3A%20Geometric-Based%20Deep%20Learning%20Model%20for%20Predicting%0A%20%20Protein-Protein%20Interactions&entry.906535625=Matan%20Halfon%20and%20Tomer%20Cohen%20and%20Raanan%20Fattal%20and%20Dina%20Schneidman-Duhovny&entry.1292438233=%20%20Deep%20learning%20approaches%20achieved%20significant%20progress%20in%20predicting%20protein%0Astructures.%20These%20methods%20are%20often%20applied%20to%20protein-protein%20interactions%0A%28PPIs%29%20yet%20require%20Multiple%20Sequence%20Alignment%20%28MSA%29%20which%20is%20unavailable%20for%0Avarious%20interactions%2C%20such%20as%20antibody-antigen.%20Computational%20docking%20methods%0Aare%20capable%20of%20sampling%20accurate%20complex%20models%2C%20but%20also%20produce%20thousands%20of%0Ainvalid%20configurations.%20The%20design%20of%20scoring%20functions%20for%20identifying%0Aaccurate%20models%20is%20a%20long-standing%20challenge.%20We%20develop%20a%20novel%0Aattention-based%20Graph%20Neural%20Network%20%28GNN%29%2C%20ContactNet%2C%20for%20classifying%20PPI%0Amodels%20obtained%20from%20docking%20algorithms%20into%20accurate%20and%20incorrect%20ones.%20When%0Atrained%20on%20docked%20antigen%20and%20modeled%20antibody%20structures%2C%20ContactNet%20doubles%0Athe%20accuracy%20of%20current%20state-of-the-art%20scoring%20functions%2C%20achieving%20accurate%0Amodels%20among%20its%20Top-10%20at%2043%25%20of%20the%20test%20cases.%20When%20applied%20to%20unbound%0Aantibodies%2C%20its%20Top-10%20accuracy%20increases%20to%2065%25.%20This%20performance%20is%20achieved%0Awithout%20MSA%20and%20the%20approach%20is%20applicable%20to%20other%20types%20of%20interactions%2C%20such%0Aas%20host-pathogens%20or%20general%20PPIs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18314v1&entry.124074799=Read"},
{"title": "Stable Diffusion Segmentation for Biomedical Images with Single-step\n  Reverse Process", "author": "Tianyu Lin and Zhiguang Chen and Zhonghao Yan and Fudan Zheng and Weijiang Yu", "abstract": "  Diffusion models have demonstrated their effectiveness across various\ngenerative tasks. However, when applied to medical image segmentation, these\nmodels encounter several challenges, including significant resource and time\nrequirements. They also necessitate a multi-step reverse process and multiple\nsamples to produce reliable predictions. To address these challenges, we\nintroduce the first latent diffusion segmentation model, named SDSeg, built\nupon stable diffusion (SD). SDSeg incorporates a straightforward latent\nestimation strategy to facilitate a single-step reverse process and utilizes\nlatent fusion concatenation to remove the necessity for multiple samples.\nExtensive experiments indicate that SDSeg surpasses existing state-of-the-art\nmethods on five benchmark datasets featuring diverse imaging modalities.\nRemarkably, SDSeg is capable of generating stable predictions with a solitary\nreverse step and sample, epitomizing the model's stability as implied by its\nname. The code is available at\nhttps://github.com/lin-tianyu/Stable-Diffusion-Seg\n", "link": "http://arxiv.org/abs/2406.18361v1", "date": "2024-06-26", "relevancy": 2.29, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6744}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5715}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Diffusion%20Segmentation%20for%20Biomedical%20Images%20with%20Single-step%0A%20%20Reverse%20Process&body=Title%3A%20Stable%20Diffusion%20Segmentation%20for%20Biomedical%20Images%20with%20Single-step%0A%20%20Reverse%20Process%0AAuthor%3A%20Tianyu%20Lin%20and%20Zhiguang%20Chen%20and%20Zhonghao%20Yan%20and%20Fudan%20Zheng%20and%20Weijiang%20Yu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20their%20effectiveness%20across%20various%0Agenerative%20tasks.%20However%2C%20when%20applied%20to%20medical%20image%20segmentation%2C%20these%0Amodels%20encounter%20several%20challenges%2C%20including%20significant%20resource%20and%20time%0Arequirements.%20They%20also%20necessitate%20a%20multi-step%20reverse%20process%20and%20multiple%0Asamples%20to%20produce%20reliable%20predictions.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20the%20first%20latent%20diffusion%20segmentation%20model%2C%20named%20SDSeg%2C%20built%0Aupon%20stable%20diffusion%20%28SD%29.%20SDSeg%20incorporates%20a%20straightforward%20latent%0Aestimation%20strategy%20to%20facilitate%20a%20single-step%20reverse%20process%20and%20utilizes%0Alatent%20fusion%20concatenation%20to%20remove%20the%20necessity%20for%20multiple%20samples.%0AExtensive%20experiments%20indicate%20that%20SDSeg%20surpasses%20existing%20state-of-the-art%0Amethods%20on%20five%20benchmark%20datasets%20featuring%20diverse%20imaging%20modalities.%0ARemarkably%2C%20SDSeg%20is%20capable%20of%20generating%20stable%20predictions%20with%20a%20solitary%0Areverse%20step%20and%20sample%2C%20epitomizing%20the%20model%27s%20stability%20as%20implied%20by%20its%0Aname.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lin-tianyu/Stable-Diffusion-Seg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Diffusion%2520Segmentation%2520for%2520Biomedical%2520Images%2520with%2520Single-step%250A%2520%2520Reverse%2520Process%26entry.906535625%3DTianyu%2520Lin%2520and%2520Zhiguang%2520Chen%2520and%2520Zhonghao%2520Yan%2520and%2520Fudan%2520Zheng%2520and%2520Weijiang%2520Yu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520their%2520effectiveness%2520across%2520various%250Agenerative%2520tasks.%2520However%252C%2520when%2520applied%2520to%2520medical%2520image%2520segmentation%252C%2520these%250Amodels%2520encounter%2520several%2520challenges%252C%2520including%2520significant%2520resource%2520and%2520time%250Arequirements.%2520They%2520also%2520necessitate%2520a%2520multi-step%2520reverse%2520process%2520and%2520multiple%250Asamples%2520to%2520produce%2520reliable%2520predictions.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520the%2520first%2520latent%2520diffusion%2520segmentation%2520model%252C%2520named%2520SDSeg%252C%2520built%250Aupon%2520stable%2520diffusion%2520%2528SD%2529.%2520SDSeg%2520incorporates%2520a%2520straightforward%2520latent%250Aestimation%2520strategy%2520to%2520facilitate%2520a%2520single-step%2520reverse%2520process%2520and%2520utilizes%250Alatent%2520fusion%2520concatenation%2520to%2520remove%2520the%2520necessity%2520for%2520multiple%2520samples.%250AExtensive%2520experiments%2520indicate%2520that%2520SDSeg%2520surpasses%2520existing%2520state-of-the-art%250Amethods%2520on%2520five%2520benchmark%2520datasets%2520featuring%2520diverse%2520imaging%2520modalities.%250ARemarkably%252C%2520SDSeg%2520is%2520capable%2520of%2520generating%2520stable%2520predictions%2520with%2520a%2520solitary%250Areverse%2520step%2520and%2520sample%252C%2520epitomizing%2520the%2520model%2527s%2520stability%2520as%2520implied%2520by%2520its%250Aname.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lin-tianyu/Stable-Diffusion-Seg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Diffusion%20Segmentation%20for%20Biomedical%20Images%20with%20Single-step%0A%20%20Reverse%20Process&entry.906535625=Tianyu%20Lin%20and%20Zhiguang%20Chen%20and%20Zhonghao%20Yan%20and%20Fudan%20Zheng%20and%20Weijiang%20Yu&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20their%20effectiveness%20across%20various%0Agenerative%20tasks.%20However%2C%20when%20applied%20to%20medical%20image%20segmentation%2C%20these%0Amodels%20encounter%20several%20challenges%2C%20including%20significant%20resource%20and%20time%0Arequirements.%20They%20also%20necessitate%20a%20multi-step%20reverse%20process%20and%20multiple%0Asamples%20to%20produce%20reliable%20predictions.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20the%20first%20latent%20diffusion%20segmentation%20model%2C%20named%20SDSeg%2C%20built%0Aupon%20stable%20diffusion%20%28SD%29.%20SDSeg%20incorporates%20a%20straightforward%20latent%0Aestimation%20strategy%20to%20facilitate%20a%20single-step%20reverse%20process%20and%20utilizes%0Alatent%20fusion%20concatenation%20to%20remove%20the%20necessity%20for%20multiple%20samples.%0AExtensive%20experiments%20indicate%20that%20SDSeg%20surpasses%20existing%20state-of-the-art%0Amethods%20on%20five%20benchmark%20datasets%20featuring%20diverse%20imaging%20modalities.%0ARemarkably%2C%20SDSeg%20is%20capable%20of%20generating%20stable%20predictions%20with%20a%20solitary%0Areverse%20step%20and%20sample%2C%20epitomizing%20the%20model%27s%20stability%20as%20implied%20by%20its%0Aname.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lin-tianyu/Stable-Diffusion-Seg%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18361v1&entry.124074799=Read"},
{"title": "KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning", "author": "Roman Bresson and Giannis Nikolentzos and George Panagopoulos and Michail Chatzianastasis and Jun Pang and Michalis Vazirgiannis", "abstract": "  In recent years, Graph Neural Networks (GNNs) have become the de facto tool\nfor learning node and graph representations. Most GNNs typically consist of a\nsequence of neighborhood aggregation (a.k.a., message passing) layers. Within\neach of these layers, the representation of each node is updated from an\naggregation and transformation of its neighbours representations at the\nprevious layer. The upper bound for the expressive power of message passing\nGNNs was reached through the use of MLPs as a transformation, due to their\nuniversal approximation capabilities. However, MLPs suffer from well-known\nlimitations, which recently motivated the introduction of Kolmogorov-Arnold\nNetworks (KANs). KANs rely on the Kolmogorov-Arnold representation theorem,\nrendering them a promising alternative to MLPs. In this work, we compare the\nperformance of KANs against that of MLPs in graph learning tasks. We perform\nextensive experiments on node classification, graph classification and graph\nregression datasets. Our preliminary results indicate that while KANs are\non-par with MLPs in classification tasks, they seem to have a clear advantage\nin the graph regression tasks.\n", "link": "http://arxiv.org/abs/2406.18380v1", "date": "2024-06-26", "relevancy": 2.2808, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4735}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4477}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KAGNNs%3A%20Kolmogorov-Arnold%20Networks%20meet%20Graph%20Learning&body=Title%3A%20KAGNNs%3A%20Kolmogorov-Arnold%20Networks%20meet%20Graph%20Learning%0AAuthor%3A%20Roman%20Bresson%20and%20Giannis%20Nikolentzos%20and%20George%20Panagopoulos%20and%20Michail%20Chatzianastasis%20and%20Jun%20Pang%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20the%20de%20facto%20tool%0Afor%20learning%20node%20and%20graph%20representations.%20Most%20GNNs%20typically%20consist%20of%20a%0Asequence%20of%20neighborhood%20aggregation%20%28a.k.a.%2C%20message%20passing%29%20layers.%20Within%0Aeach%20of%20these%20layers%2C%20the%20representation%20of%20each%20node%20is%20updated%20from%20an%0Aaggregation%20and%20transformation%20of%20its%20neighbours%20representations%20at%20the%0Aprevious%20layer.%20The%20upper%20bound%20for%20the%20expressive%20power%20of%20message%20passing%0AGNNs%20was%20reached%20through%20the%20use%20of%20MLPs%20as%20a%20transformation%2C%20due%20to%20their%0Auniversal%20approximation%20capabilities.%20However%2C%20MLPs%20suffer%20from%20well-known%0Alimitations%2C%20which%20recently%20motivated%20the%20introduction%20of%20Kolmogorov-Arnold%0ANetworks%20%28KANs%29.%20KANs%20rely%20on%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%0Arendering%20them%20a%20promising%20alternative%20to%20MLPs.%20In%20this%20work%2C%20we%20compare%20the%0Aperformance%20of%20KANs%20against%20that%20of%20MLPs%20in%20graph%20learning%20tasks.%20We%20perform%0Aextensive%20experiments%20on%20node%20classification%2C%20graph%20classification%20and%20graph%0Aregression%20datasets.%20Our%20preliminary%20results%20indicate%20that%20while%20KANs%20are%0Aon-par%20with%20MLPs%20in%20classification%20tasks%2C%20they%20seem%20to%20have%20a%20clear%20advantage%0Ain%20the%20graph%20regression%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKAGNNs%253A%2520Kolmogorov-Arnold%2520Networks%2520meet%2520Graph%2520Learning%26entry.906535625%3DRoman%2520Bresson%2520and%2520Giannis%2520Nikolentzos%2520and%2520George%2520Panagopoulos%2520and%2520Michail%2520Chatzianastasis%2520and%2520Jun%2520Pang%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520the%2520de%2520facto%2520tool%250Afor%2520learning%2520node%2520and%2520graph%2520representations.%2520Most%2520GNNs%2520typically%2520consist%2520of%2520a%250Asequence%2520of%2520neighborhood%2520aggregation%2520%2528a.k.a.%252C%2520message%2520passing%2529%2520layers.%2520Within%250Aeach%2520of%2520these%2520layers%252C%2520the%2520representation%2520of%2520each%2520node%2520is%2520updated%2520from%2520an%250Aaggregation%2520and%2520transformation%2520of%2520its%2520neighbours%2520representations%2520at%2520the%250Aprevious%2520layer.%2520The%2520upper%2520bound%2520for%2520the%2520expressive%2520power%2520of%2520message%2520passing%250AGNNs%2520was%2520reached%2520through%2520the%2520use%2520of%2520MLPs%2520as%2520a%2520transformation%252C%2520due%2520to%2520their%250Auniversal%2520approximation%2520capabilities.%2520However%252C%2520MLPs%2520suffer%2520from%2520well-known%250Alimitations%252C%2520which%2520recently%2520motivated%2520the%2520introduction%2520of%2520Kolmogorov-Arnold%250ANetworks%2520%2528KANs%2529.%2520KANs%2520rely%2520on%2520the%2520Kolmogorov-Arnold%2520representation%2520theorem%252C%250Arendering%2520them%2520a%2520promising%2520alternative%2520to%2520MLPs.%2520In%2520this%2520work%252C%2520we%2520compare%2520the%250Aperformance%2520of%2520KANs%2520against%2520that%2520of%2520MLPs%2520in%2520graph%2520learning%2520tasks.%2520We%2520perform%250Aextensive%2520experiments%2520on%2520node%2520classification%252C%2520graph%2520classification%2520and%2520graph%250Aregression%2520datasets.%2520Our%2520preliminary%2520results%2520indicate%2520that%2520while%2520KANs%2520are%250Aon-par%2520with%2520MLPs%2520in%2520classification%2520tasks%252C%2520they%2520seem%2520to%2520have%2520a%2520clear%2520advantage%250Ain%2520the%2520graph%2520regression%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAGNNs%3A%20Kolmogorov-Arnold%20Networks%20meet%20Graph%20Learning&entry.906535625=Roman%20Bresson%20and%20Giannis%20Nikolentzos%20and%20George%20Panagopoulos%20and%20Michail%20Chatzianastasis%20and%20Jun%20Pang%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20In%20recent%20years%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20the%20de%20facto%20tool%0Afor%20learning%20node%20and%20graph%20representations.%20Most%20GNNs%20typically%20consist%20of%20a%0Asequence%20of%20neighborhood%20aggregation%20%28a.k.a.%2C%20message%20passing%29%20layers.%20Within%0Aeach%20of%20these%20layers%2C%20the%20representation%20of%20each%20node%20is%20updated%20from%20an%0Aaggregation%20and%20transformation%20of%20its%20neighbours%20representations%20at%20the%0Aprevious%20layer.%20The%20upper%20bound%20for%20the%20expressive%20power%20of%20message%20passing%0AGNNs%20was%20reached%20through%20the%20use%20of%20MLPs%20as%20a%20transformation%2C%20due%20to%20their%0Auniversal%20approximation%20capabilities.%20However%2C%20MLPs%20suffer%20from%20well-known%0Alimitations%2C%20which%20recently%20motivated%20the%20introduction%20of%20Kolmogorov-Arnold%0ANetworks%20%28KANs%29.%20KANs%20rely%20on%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%0Arendering%20them%20a%20promising%20alternative%20to%20MLPs.%20In%20this%20work%2C%20we%20compare%20the%0Aperformance%20of%20KANs%20against%20that%20of%20MLPs%20in%20graph%20learning%20tasks.%20We%20perform%0Aextensive%20experiments%20on%20node%20classification%2C%20graph%20classification%20and%20graph%0Aregression%20datasets.%20Our%20preliminary%20results%20indicate%20that%20while%20KANs%20are%0Aon-par%20with%20MLPs%20in%20classification%20tasks%2C%20they%20seem%20to%20have%20a%20clear%20advantage%0Ain%20the%20graph%20regression%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18380v1&entry.124074799=Read"},
{"title": "Commonsense Prototype for Outdoor Unsupervised 3D Object Detection", "author": "Hai Wu and Shijia Zhao and Xun Huang and Chenglu Wen and Xin Li and Cheng Wang", "abstract": "  The prevalent approaches of unsupervised 3D object detection follow\ncluster-based pseudo-label generation and iterative self-training processes.\nHowever, the challenge arises due to the sparsity of LiDAR scans, which leads\nto pseudo-labels with erroneous size and position, resulting in subpar\ndetection performance. To tackle this problem, this paper introduces a\nCommonsense Prototype-based Detector, termed CPD, for unsupervised 3D object\ndetection. CPD first constructs Commonsense Prototype (CProto) characterized by\nhigh-quality bounding box and dense points, based on commonsense intuition.\nSubsequently, CPD refines the low-quality pseudo-labels by leveraging the size\nprior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely\nscanned objects by the geometric knowledge from CProto. CPD outperforms\nstate-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD),\nPandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD\nand testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on\neasy and moderate car classes, respectively. These achievements position CPD in\nclose proximity to fully supervised detectors, highlighting the significance of\nour method. The code will be available at https://github.com/hailanyi/CPD.\n", "link": "http://arxiv.org/abs/2404.16493v3", "date": "2024-06-26", "relevancy": 2.2754, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5843}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5756}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Commonsense%20Prototype%20for%20Outdoor%20Unsupervised%203D%20Object%20Detection&body=Title%3A%20Commonsense%20Prototype%20for%20Outdoor%20Unsupervised%203D%20Object%20Detection%0AAuthor%3A%20Hai%20Wu%20and%20Shijia%20Zhao%20and%20Xun%20Huang%20and%20Chenglu%20Wen%20and%20Xin%20Li%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20The%20prevalent%20approaches%20of%20unsupervised%203D%20object%20detection%20follow%0Acluster-based%20pseudo-label%20generation%20and%20iterative%20self-training%20processes.%0AHowever%2C%20the%20challenge%20arises%20due%20to%20the%20sparsity%20of%20LiDAR%20scans%2C%20which%20leads%0Ato%20pseudo-labels%20with%20erroneous%20size%20and%20position%2C%20resulting%20in%20subpar%0Adetection%20performance.%20To%20tackle%20this%20problem%2C%20this%20paper%20introduces%20a%0ACommonsense%20Prototype-based%20Detector%2C%20termed%20CPD%2C%20for%20unsupervised%203D%20object%0Adetection.%20CPD%20first%20constructs%20Commonsense%20Prototype%20%28CProto%29%20characterized%20by%0Ahigh-quality%20bounding%20box%20and%20dense%20points%2C%20based%20on%20commonsense%20intuition.%0ASubsequently%2C%20CPD%20refines%20the%20low-quality%20pseudo-labels%20by%20leveraging%20the%20size%0Aprior%20from%20CProto.%20Furthermore%2C%20CPD%20enhances%20the%20detection%20accuracy%20of%20sparsely%0Ascanned%20objects%20by%20the%20geometric%20knowledge%20from%20CProto.%20CPD%20outperforms%0Astate-of-the-art%20unsupervised%203D%20detectors%20on%20Waymo%20Open%20Dataset%20%28WOD%29%2C%0APandaSet%2C%20and%20KITTI%20datasets%20by%20a%20large%20margin.%20Besides%2C%20by%20training%20CPD%20on%20WOD%0Aand%20testing%20on%20KITTI%2C%20CPD%20attains%2090.85%25%20and%2081.01%25%203D%20Average%20Precision%20on%0Aeasy%20and%20moderate%20car%20classes%2C%20respectively.%20These%20achievements%20position%20CPD%20in%0Aclose%20proximity%20to%20fully%20supervised%20detectors%2C%20highlighting%20the%20significance%20of%0Aour%20method.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/hailanyi/CPD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16493v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommonsense%2520Prototype%2520for%2520Outdoor%2520Unsupervised%25203D%2520Object%2520Detection%26entry.906535625%3DHai%2520Wu%2520and%2520Shijia%2520Zhao%2520and%2520Xun%2520Huang%2520and%2520Chenglu%2520Wen%2520and%2520Xin%2520Li%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520The%2520prevalent%2520approaches%2520of%2520unsupervised%25203D%2520object%2520detection%2520follow%250Acluster-based%2520pseudo-label%2520generation%2520and%2520iterative%2520self-training%2520processes.%250AHowever%252C%2520the%2520challenge%2520arises%2520due%2520to%2520the%2520sparsity%2520of%2520LiDAR%2520scans%252C%2520which%2520leads%250Ato%2520pseudo-labels%2520with%2520erroneous%2520size%2520and%2520position%252C%2520resulting%2520in%2520subpar%250Adetection%2520performance.%2520To%2520tackle%2520this%2520problem%252C%2520this%2520paper%2520introduces%2520a%250ACommonsense%2520Prototype-based%2520Detector%252C%2520termed%2520CPD%252C%2520for%2520unsupervised%25203D%2520object%250Adetection.%2520CPD%2520first%2520constructs%2520Commonsense%2520Prototype%2520%2528CProto%2529%2520characterized%2520by%250Ahigh-quality%2520bounding%2520box%2520and%2520dense%2520points%252C%2520based%2520on%2520commonsense%2520intuition.%250ASubsequently%252C%2520CPD%2520refines%2520the%2520low-quality%2520pseudo-labels%2520by%2520leveraging%2520the%2520size%250Aprior%2520from%2520CProto.%2520Furthermore%252C%2520CPD%2520enhances%2520the%2520detection%2520accuracy%2520of%2520sparsely%250Ascanned%2520objects%2520by%2520the%2520geometric%2520knowledge%2520from%2520CProto.%2520CPD%2520outperforms%250Astate-of-the-art%2520unsupervised%25203D%2520detectors%2520on%2520Waymo%2520Open%2520Dataset%2520%2528WOD%2529%252C%250APandaSet%252C%2520and%2520KITTI%2520datasets%2520by%2520a%2520large%2520margin.%2520Besides%252C%2520by%2520training%2520CPD%2520on%2520WOD%250Aand%2520testing%2520on%2520KITTI%252C%2520CPD%2520attains%252090.85%2525%2520and%252081.01%2525%25203D%2520Average%2520Precision%2520on%250Aeasy%2520and%2520moderate%2520car%2520classes%252C%2520respectively.%2520These%2520achievements%2520position%2520CPD%2520in%250Aclose%2520proximity%2520to%2520fully%2520supervised%2520detectors%252C%2520highlighting%2520the%2520significance%2520of%250Aour%2520method.%2520The%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/hailanyi/CPD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16493v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Commonsense%20Prototype%20for%20Outdoor%20Unsupervised%203D%20Object%20Detection&entry.906535625=Hai%20Wu%20and%20Shijia%20Zhao%20and%20Xun%20Huang%20and%20Chenglu%20Wen%20and%20Xin%20Li%20and%20Cheng%20Wang&entry.1292438233=%20%20The%20prevalent%20approaches%20of%20unsupervised%203D%20object%20detection%20follow%0Acluster-based%20pseudo-label%20generation%20and%20iterative%20self-training%20processes.%0AHowever%2C%20the%20challenge%20arises%20due%20to%20the%20sparsity%20of%20LiDAR%20scans%2C%20which%20leads%0Ato%20pseudo-labels%20with%20erroneous%20size%20and%20position%2C%20resulting%20in%20subpar%0Adetection%20performance.%20To%20tackle%20this%20problem%2C%20this%20paper%20introduces%20a%0ACommonsense%20Prototype-based%20Detector%2C%20termed%20CPD%2C%20for%20unsupervised%203D%20object%0Adetection.%20CPD%20first%20constructs%20Commonsense%20Prototype%20%28CProto%29%20characterized%20by%0Ahigh-quality%20bounding%20box%20and%20dense%20points%2C%20based%20on%20commonsense%20intuition.%0ASubsequently%2C%20CPD%20refines%20the%20low-quality%20pseudo-labels%20by%20leveraging%20the%20size%0Aprior%20from%20CProto.%20Furthermore%2C%20CPD%20enhances%20the%20detection%20accuracy%20of%20sparsely%0Ascanned%20objects%20by%20the%20geometric%20knowledge%20from%20CProto.%20CPD%20outperforms%0Astate-of-the-art%20unsupervised%203D%20detectors%20on%20Waymo%20Open%20Dataset%20%28WOD%29%2C%0APandaSet%2C%20and%20KITTI%20datasets%20by%20a%20large%20margin.%20Besides%2C%20by%20training%20CPD%20on%20WOD%0Aand%20testing%20on%20KITTI%2C%20CPD%20attains%2090.85%25%20and%2081.01%25%203D%20Average%20Precision%20on%0Aeasy%20and%20moderate%20car%20classes%2C%20respectively.%20These%20achievements%20position%20CPD%20in%0Aclose%20proximity%20to%20fully%20supervised%20detectors%2C%20highlighting%20the%20significance%20of%0Aour%20method.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/hailanyi/CPD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16493v3&entry.124074799=Read"},
{"title": "SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation", "author": "Bj\u00f6rn Michele and Alexandre Boulch and Gilles Puy and Tuan-Hung Vu and Renaud Marlet and Nicolas Courty", "abstract": "  Learning models on one labeled dataset that generalize well on another domain\nis a difficult task, as several shifts might happen between the data domains.\nThis is notably the case for lidar data, for which models can exhibit large\nperformance discrepancies due for instance to different lidar patterns or\nchanges in acquisition conditions. This paper addresses the corresponding\nUnsupervised Domain Adaptation (UDA) task for semantic segmentation. To\nmitigate this problem, we introduce an unsupervised auxiliary task of learning\nan implicit underlying surface representation simultaneously on source and\ntarget data. As both domains share the same latent representation, the model is\nforced to accommodate discrepancies between the two sources of data. This novel\nstrategy differs from classical minimization of statistical divergences or\nlidar-specific domain adaptation techniques. Our experiments demonstrate that\nour method achieves a better performance than the current state of the art,\nboth in real-to-real and synthetic-to-real scenarios.\n", "link": "http://arxiv.org/abs/2304.03251v4", "date": "2024-06-26", "relevancy": 2.2682, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5726}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5636}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SALUDA%3A%20Surface-based%20Automotive%20Lidar%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20SALUDA%3A%20Surface-based%20Automotive%20Lidar%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Bj%C3%B6rn%20Michele%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Tuan-Hung%20Vu%20and%20Renaud%20Marlet%20and%20Nicolas%20Courty%0AAbstract%3A%20%20%20Learning%20models%20on%20one%20labeled%20dataset%20that%20generalize%20well%20on%20another%20domain%0Ais%20a%20difficult%20task%2C%20as%20several%20shifts%20might%20happen%20between%20the%20data%20domains.%0AThis%20is%20notably%20the%20case%20for%20lidar%20data%2C%20for%20which%20models%20can%20exhibit%20large%0Aperformance%20discrepancies%20due%20for%20instance%20to%20different%20lidar%20patterns%20or%0Achanges%20in%20acquisition%20conditions.%20This%20paper%20addresses%20the%20corresponding%0AUnsupervised%20Domain%20Adaptation%20%28UDA%29%20task%20for%20semantic%20segmentation.%20To%0Amitigate%20this%20problem%2C%20we%20introduce%20an%20unsupervised%20auxiliary%20task%20of%20learning%0Aan%20implicit%20underlying%20surface%20representation%20simultaneously%20on%20source%20and%0Atarget%20data.%20As%20both%20domains%20share%20the%20same%20latent%20representation%2C%20the%20model%20is%0Aforced%20to%20accommodate%20discrepancies%20between%20the%20two%20sources%20of%20data.%20This%20novel%0Astrategy%20differs%20from%20classical%20minimization%20of%20statistical%20divergences%20or%0Alidar-specific%20domain%20adaptation%20techniques.%20Our%20experiments%20demonstrate%20that%0Aour%20method%20achieves%20a%20better%20performance%20than%20the%20current%20state%20of%20the%20art%2C%0Aboth%20in%20real-to-real%20and%20synthetic-to-real%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03251v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSALUDA%253A%2520Surface-based%2520Automotive%2520Lidar%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DBj%25C3%25B6rn%2520Michele%2520and%2520Alexandre%2520Boulch%2520and%2520Gilles%2520Puy%2520and%2520Tuan-Hung%2520Vu%2520and%2520Renaud%2520Marlet%2520and%2520Nicolas%2520Courty%26entry.1292438233%3D%2520%2520Learning%2520models%2520on%2520one%2520labeled%2520dataset%2520that%2520generalize%2520well%2520on%2520another%2520domain%250Ais%2520a%2520difficult%2520task%252C%2520as%2520several%2520shifts%2520might%2520happen%2520between%2520the%2520data%2520domains.%250AThis%2520is%2520notably%2520the%2520case%2520for%2520lidar%2520data%252C%2520for%2520which%2520models%2520can%2520exhibit%2520large%250Aperformance%2520discrepancies%2520due%2520for%2520instance%2520to%2520different%2520lidar%2520patterns%2520or%250Achanges%2520in%2520acquisition%2520conditions.%2520This%2520paper%2520addresses%2520the%2520corresponding%250AUnsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520task%2520for%2520semantic%2520segmentation.%2520To%250Amitigate%2520this%2520problem%252C%2520we%2520introduce%2520an%2520unsupervised%2520auxiliary%2520task%2520of%2520learning%250Aan%2520implicit%2520underlying%2520surface%2520representation%2520simultaneously%2520on%2520source%2520and%250Atarget%2520data.%2520As%2520both%2520domains%2520share%2520the%2520same%2520latent%2520representation%252C%2520the%2520model%2520is%250Aforced%2520to%2520accommodate%2520discrepancies%2520between%2520the%2520two%2520sources%2520of%2520data.%2520This%2520novel%250Astrategy%2520differs%2520from%2520classical%2520minimization%2520of%2520statistical%2520divergences%2520or%250Alidar-specific%2520domain%2520adaptation%2520techniques.%2520Our%2520experiments%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520a%2520better%2520performance%2520than%2520the%2520current%2520state%2520of%2520the%2520art%252C%250Aboth%2520in%2520real-to-real%2520and%2520synthetic-to-real%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.03251v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SALUDA%3A%20Surface-based%20Automotive%20Lidar%20Unsupervised%20Domain%20Adaptation&entry.906535625=Bj%C3%B6rn%20Michele%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Tuan-Hung%20Vu%20and%20Renaud%20Marlet%20and%20Nicolas%20Courty&entry.1292438233=%20%20Learning%20models%20on%20one%20labeled%20dataset%20that%20generalize%20well%20on%20another%20domain%0Ais%20a%20difficult%20task%2C%20as%20several%20shifts%20might%20happen%20between%20the%20data%20domains.%0AThis%20is%20notably%20the%20case%20for%20lidar%20data%2C%20for%20which%20models%20can%20exhibit%20large%0Aperformance%20discrepancies%20due%20for%20instance%20to%20different%20lidar%20patterns%20or%0Achanges%20in%20acquisition%20conditions.%20This%20paper%20addresses%20the%20corresponding%0AUnsupervised%20Domain%20Adaptation%20%28UDA%29%20task%20for%20semantic%20segmentation.%20To%0Amitigate%20this%20problem%2C%20we%20introduce%20an%20unsupervised%20auxiliary%20task%20of%20learning%0Aan%20implicit%20underlying%20surface%20representation%20simultaneously%20on%20source%20and%0Atarget%20data.%20As%20both%20domains%20share%20the%20same%20latent%20representation%2C%20the%20model%20is%0Aforced%20to%20accommodate%20discrepancies%20between%20the%20two%20sources%20of%20data.%20This%20novel%0Astrategy%20differs%20from%20classical%20minimization%20of%20statistical%20divergences%20or%0Alidar-specific%20domain%20adaptation%20techniques.%20Our%20experiments%20demonstrate%20that%0Aour%20method%20achieves%20a%20better%20performance%20than%20the%20current%20state%20of%20the%20art%2C%0Aboth%20in%20real-to-real%20and%20synthetic-to-real%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03251v4&entry.124074799=Read"},
{"title": "Kolmogorov-Arnold Graph Neural Networks", "author": "Gianluca De Carlo and Andrea Mastropietro and Aris Anagnostopoulos", "abstract": "  Graph neural networks (GNNs) excel in learning from network-like data but\noften lack interpretability, making their application challenging in domains\nrequiring transparent decision-making. We propose the Graph Kolmogorov-Arnold\nNetwork (GKAN), a novel GNN model leveraging spline-based activation functions\non edges to enhance both accuracy and interpretability. Our experiments on five\nbenchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN\nmodels in node classification, link prediction, and graph classification tasks.\nIn addition to the improved accuracy, GKAN's design inherently provides clear\ninsights into the model's decision-making process, eliminating the need for\npost-hoc explainability techniques. This paper discusses the methodology,\nperformance, and interpretability of GKAN, highlighting its potential for\napplications in domains where interpretability is crucial.\n", "link": "http://arxiv.org/abs/2406.18354v1", "date": "2024-06-26", "relevancy": 2.2643, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4558}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kolmogorov-Arnold%20Graph%20Neural%20Networks&body=Title%3A%20Kolmogorov-Arnold%20Graph%20Neural%20Networks%0AAuthor%3A%20Gianluca%20De%20Carlo%20and%20Andrea%20Mastropietro%20and%20Aris%20Anagnostopoulos%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20excel%20in%20learning%20from%20network-like%20data%20but%0Aoften%20lack%20interpretability%2C%20making%20their%20application%20challenging%20in%20domains%0Arequiring%20transparent%20decision-making.%20We%20propose%20the%20Graph%20Kolmogorov-Arnold%0ANetwork%20%28GKAN%29%2C%20a%20novel%20GNN%20model%20leveraging%20spline-based%20activation%20functions%0Aon%20edges%20to%20enhance%20both%20accuracy%20and%20interpretability.%20Our%20experiments%20on%20five%0Abenchmark%20datasets%20demonstrate%20that%20GKAN%20outperforms%20state-of-the-art%20GNN%0Amodels%20in%20node%20classification%2C%20link%20prediction%2C%20and%20graph%20classification%20tasks.%0AIn%20addition%20to%20the%20improved%20accuracy%2C%20GKAN%27s%20design%20inherently%20provides%20clear%0Ainsights%20into%20the%20model%27s%20decision-making%20process%2C%20eliminating%20the%20need%20for%0Apost-hoc%20explainability%20techniques.%20This%20paper%20discusses%20the%20methodology%2C%0Aperformance%2C%20and%20interpretability%20of%20GKAN%2C%20highlighting%20its%20potential%20for%0Aapplications%20in%20domains%20where%20interpretability%20is%20crucial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKolmogorov-Arnold%2520Graph%2520Neural%2520Networks%26entry.906535625%3DGianluca%2520De%2520Carlo%2520and%2520Andrea%2520Mastropietro%2520and%2520Aris%2520Anagnostopoulos%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520excel%2520in%2520learning%2520from%2520network-like%2520data%2520but%250Aoften%2520lack%2520interpretability%252C%2520making%2520their%2520application%2520challenging%2520in%2520domains%250Arequiring%2520transparent%2520decision-making.%2520We%2520propose%2520the%2520Graph%2520Kolmogorov-Arnold%250ANetwork%2520%2528GKAN%2529%252C%2520a%2520novel%2520GNN%2520model%2520leveraging%2520spline-based%2520activation%2520functions%250Aon%2520edges%2520to%2520enhance%2520both%2520accuracy%2520and%2520interpretability.%2520Our%2520experiments%2520on%2520five%250Abenchmark%2520datasets%2520demonstrate%2520that%2520GKAN%2520outperforms%2520state-of-the-art%2520GNN%250Amodels%2520in%2520node%2520classification%252C%2520link%2520prediction%252C%2520and%2520graph%2520classification%2520tasks.%250AIn%2520addition%2520to%2520the%2520improved%2520accuracy%252C%2520GKAN%2527s%2520design%2520inherently%2520provides%2520clear%250Ainsights%2520into%2520the%2520model%2527s%2520decision-making%2520process%252C%2520eliminating%2520the%2520need%2520for%250Apost-hoc%2520explainability%2520techniques.%2520This%2520paper%2520discusses%2520the%2520methodology%252C%250Aperformance%252C%2520and%2520interpretability%2520of%2520GKAN%252C%2520highlighting%2520its%2520potential%2520for%250Aapplications%2520in%2520domains%2520where%2520interpretability%2520is%2520crucial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kolmogorov-Arnold%20Graph%20Neural%20Networks&entry.906535625=Gianluca%20De%20Carlo%20and%20Andrea%20Mastropietro%20and%20Aris%20Anagnostopoulos&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20excel%20in%20learning%20from%20network-like%20data%20but%0Aoften%20lack%20interpretability%2C%20making%20their%20application%20challenging%20in%20domains%0Arequiring%20transparent%20decision-making.%20We%20propose%20the%20Graph%20Kolmogorov-Arnold%0ANetwork%20%28GKAN%29%2C%20a%20novel%20GNN%20model%20leveraging%20spline-based%20activation%20functions%0Aon%20edges%20to%20enhance%20both%20accuracy%20and%20interpretability.%20Our%20experiments%20on%20five%0Abenchmark%20datasets%20demonstrate%20that%20GKAN%20outperforms%20state-of-the-art%20GNN%0Amodels%20in%20node%20classification%2C%20link%20prediction%2C%20and%20graph%20classification%20tasks.%0AIn%20addition%20to%20the%20improved%20accuracy%2C%20GKAN%27s%20design%20inherently%20provides%20clear%0Ainsights%20into%20the%20model%27s%20decision-making%20process%2C%20eliminating%20the%20need%20for%0Apost-hoc%20explainability%20techniques.%20This%20paper%20discusses%20the%20methodology%2C%0Aperformance%2C%20and%20interpretability%20of%20GKAN%2C%20highlighting%20its%20potential%20for%0Aapplications%20in%20domains%20where%20interpretability%20is%20crucial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18354v1&entry.124074799=Read"},
{"title": "The Fundamental Limits of Least-Privilege Learning", "author": "Theresa Stadler and Bogdan Kulynych and Michael C. Gastpar and Nicolas Papernot and Carmela Troncoso", "abstract": "  The promise of least-privilege learning -- to find feature representations\nthat are useful for a learning task but prevent inference of any sensitive\ninformation unrelated to this task -- is highly appealing. However, so far this\nconcept has only been stated informally. It thus remains an open question\nwhether and how we can achieve this goal. In this work, we provide the first\nformalisation of the least-privilege principle for machine learning and\ncharacterise its feasibility. We prove that there is a fundamental trade-off\nbetween a representation's utility for a given task and its leakage beyond the\nintended task: it is not possible to learn representations that have high\nutility for the intended task but, at the same time prevent inference of any\nattribute other than the task label itself. This trade-off holds under\nrealistic assumptions on the data distribution and regardless of the technique\nused to learn the feature mappings that produce these representations. We\nempirically validate this result for a wide range of learning techniques, model\narchitectures, and datasets.\n", "link": "http://arxiv.org/abs/2402.12235v2", "date": "2024-06-26", "relevancy": 2.2405, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4569}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4569}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Fundamental%20Limits%20of%20Least-Privilege%20Learning&body=Title%3A%20The%20Fundamental%20Limits%20of%20Least-Privilege%20Learning%0AAuthor%3A%20Theresa%20Stadler%20and%20Bogdan%20Kulynych%20and%20Michael%20C.%20Gastpar%20and%20Nicolas%20Papernot%20and%20Carmela%20Troncoso%0AAbstract%3A%20%20%20The%20promise%20of%20least-privilege%20learning%20--%20to%20find%20feature%20representations%0Athat%20are%20useful%20for%20a%20learning%20task%20but%20prevent%20inference%20of%20any%20sensitive%0Ainformation%20unrelated%20to%20this%20task%20--%20is%20highly%20appealing.%20However%2C%20so%20far%20this%0Aconcept%20has%20only%20been%20stated%20informally.%20It%20thus%20remains%20an%20open%20question%0Awhether%20and%20how%20we%20can%20achieve%20this%20goal.%20In%20this%20work%2C%20we%20provide%20the%20first%0Aformalisation%20of%20the%20least-privilege%20principle%20for%20machine%20learning%20and%0Acharacterise%20its%20feasibility.%20We%20prove%20that%20there%20is%20a%20fundamental%20trade-off%0Abetween%20a%20representation%27s%20utility%20for%20a%20given%20task%20and%20its%20leakage%20beyond%20the%0Aintended%20task%3A%20it%20is%20not%20possible%20to%20learn%20representations%20that%20have%20high%0Autility%20for%20the%20intended%20task%20but%2C%20at%20the%20same%20time%20prevent%20inference%20of%20any%0Aattribute%20other%20than%20the%20task%20label%20itself.%20This%20trade-off%20holds%20under%0Arealistic%20assumptions%20on%20the%20data%20distribution%20and%20regardless%20of%20the%20technique%0Aused%20to%20learn%20the%20feature%20mappings%20that%20produce%20these%20representations.%20We%0Aempirically%20validate%20this%20result%20for%20a%20wide%20range%20of%20learning%20techniques%2C%20model%0Aarchitectures%2C%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Fundamental%2520Limits%2520of%2520Least-Privilege%2520Learning%26entry.906535625%3DTheresa%2520Stadler%2520and%2520Bogdan%2520Kulynych%2520and%2520Michael%2520C.%2520Gastpar%2520and%2520Nicolas%2520Papernot%2520and%2520Carmela%2520Troncoso%26entry.1292438233%3D%2520%2520The%2520promise%2520of%2520least-privilege%2520learning%2520--%2520to%2520find%2520feature%2520representations%250Athat%2520are%2520useful%2520for%2520a%2520learning%2520task%2520but%2520prevent%2520inference%2520of%2520any%2520sensitive%250Ainformation%2520unrelated%2520to%2520this%2520task%2520--%2520is%2520highly%2520appealing.%2520However%252C%2520so%2520far%2520this%250Aconcept%2520has%2520only%2520been%2520stated%2520informally.%2520It%2520thus%2520remains%2520an%2520open%2520question%250Awhether%2520and%2520how%2520we%2520can%2520achieve%2520this%2520goal.%2520In%2520this%2520work%252C%2520we%2520provide%2520the%2520first%250Aformalisation%2520of%2520the%2520least-privilege%2520principle%2520for%2520machine%2520learning%2520and%250Acharacterise%2520its%2520feasibility.%2520We%2520prove%2520that%2520there%2520is%2520a%2520fundamental%2520trade-off%250Abetween%2520a%2520representation%2527s%2520utility%2520for%2520a%2520given%2520task%2520and%2520its%2520leakage%2520beyond%2520the%250Aintended%2520task%253A%2520it%2520is%2520not%2520possible%2520to%2520learn%2520representations%2520that%2520have%2520high%250Autility%2520for%2520the%2520intended%2520task%2520but%252C%2520at%2520the%2520same%2520time%2520prevent%2520inference%2520of%2520any%250Aattribute%2520other%2520than%2520the%2520task%2520label%2520itself.%2520This%2520trade-off%2520holds%2520under%250Arealistic%2520assumptions%2520on%2520the%2520data%2520distribution%2520and%2520regardless%2520of%2520the%2520technique%250Aused%2520to%2520learn%2520the%2520feature%2520mappings%2520that%2520produce%2520these%2520representations.%2520We%250Aempirically%2520validate%2520this%2520result%2520for%2520a%2520wide%2520range%2520of%2520learning%2520techniques%252C%2520model%250Aarchitectures%252C%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Fundamental%20Limits%20of%20Least-Privilege%20Learning&entry.906535625=Theresa%20Stadler%20and%20Bogdan%20Kulynych%20and%20Michael%20C.%20Gastpar%20and%20Nicolas%20Papernot%20and%20Carmela%20Troncoso&entry.1292438233=%20%20The%20promise%20of%20least-privilege%20learning%20--%20to%20find%20feature%20representations%0Athat%20are%20useful%20for%20a%20learning%20task%20but%20prevent%20inference%20of%20any%20sensitive%0Ainformation%20unrelated%20to%20this%20task%20--%20is%20highly%20appealing.%20However%2C%20so%20far%20this%0Aconcept%20has%20only%20been%20stated%20informally.%20It%20thus%20remains%20an%20open%20question%0Awhether%20and%20how%20we%20can%20achieve%20this%20goal.%20In%20this%20work%2C%20we%20provide%20the%20first%0Aformalisation%20of%20the%20least-privilege%20principle%20for%20machine%20learning%20and%0Acharacterise%20its%20feasibility.%20We%20prove%20that%20there%20is%20a%20fundamental%20trade-off%0Abetween%20a%20representation%27s%20utility%20for%20a%20given%20task%20and%20its%20leakage%20beyond%20the%0Aintended%20task%3A%20it%20is%20not%20possible%20to%20learn%20representations%20that%20have%20high%0Autility%20for%20the%20intended%20task%20but%2C%20at%20the%20same%20time%20prevent%20inference%20of%20any%0Aattribute%20other%20than%20the%20task%20label%20itself.%20This%20trade-off%20holds%20under%0Arealistic%20assumptions%20on%20the%20data%20distribution%20and%20regardless%20of%20the%20technique%0Aused%20to%20learn%20the%20feature%20mappings%20that%20produce%20these%20representations.%20We%0Aempirically%20validate%20this%20result%20for%20a%20wide%20range%20of%20learning%20techniques%2C%20model%0Aarchitectures%2C%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12235v2&entry.124074799=Read"},
{"title": "Deep Fusion: Efficient Network Training via Pre-trained Initializations", "author": "Hanna Mazzawi and Xavi Gonzalvo and Michael Wunder and Sammy Jerome and Benoit Dherin", "abstract": "  In recent years, deep learning has made remarkable progress in a wide range\nof domains, with a particularly notable impact on natural language processing\ntasks. One of the challenges associated with training deep neural networks in\nthe context of LLMs is the need for large amounts of computational resources\nand time. To mitigate this, network growing algorithms offer potential cost\nsavings, but their underlying mechanisms are poorly understood. We present two\nnotable contributions in this paper. First, we present Deep Fusion, an\nefficient approach to network training that leverages pre-trained\ninitializations of smaller networks. Second, we propose a theoretical framework\nusing backward error analysis to illustrate the dynamics of mid-training\nnetwork growth. Our experiments show how Deep Fusion is a practical and\neffective approach that not only accelerates the training process but also\nreduces computational requirements, maintaining or surpassing traditional\ntraining methods' performance in various NLP tasks and T5 model sizes. Finally,\nwe validate our theoretical framework, which guides the optimal use of Deep\nFusion, showing that with carefully optimized training dynamics, it\nsignificantly reduces both training time and resource consumption.\n", "link": "http://arxiv.org/abs/2306.11903v3", "date": "2024-06-26", "relevancy": 2.2339, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5635}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5612}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Fusion%3A%20Efficient%20Network%20Training%20via%20Pre-trained%20Initializations&body=Title%3A%20Deep%20Fusion%3A%20Efficient%20Network%20Training%20via%20Pre-trained%20Initializations%0AAuthor%3A%20Hanna%20Mazzawi%20and%20Xavi%20Gonzalvo%20and%20Michael%20Wunder%20and%20Sammy%20Jerome%20and%20Benoit%20Dherin%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20learning%20has%20made%20remarkable%20progress%20in%20a%20wide%20range%0Aof%20domains%2C%20with%20a%20particularly%20notable%20impact%20on%20natural%20language%20processing%0Atasks.%20One%20of%20the%20challenges%20associated%20with%20training%20deep%20neural%20networks%20in%0Athe%20context%20of%20LLMs%20is%20the%20need%20for%20large%20amounts%20of%20computational%20resources%0Aand%20time.%20To%20mitigate%20this%2C%20network%20growing%20algorithms%20offer%20potential%20cost%0Asavings%2C%20but%20their%20underlying%20mechanisms%20are%20poorly%20understood.%20We%20present%20two%0Anotable%20contributions%20in%20this%20paper.%20First%2C%20we%20present%20Deep%20Fusion%2C%20an%0Aefficient%20approach%20to%20network%20training%20that%20leverages%20pre-trained%0Ainitializations%20of%20smaller%20networks.%20Second%2C%20we%20propose%20a%20theoretical%20framework%0Ausing%20backward%20error%20analysis%20to%20illustrate%20the%20dynamics%20of%20mid-training%0Anetwork%20growth.%20Our%20experiments%20show%20how%20Deep%20Fusion%20is%20a%20practical%20and%0Aeffective%20approach%20that%20not%20only%20accelerates%20the%20training%20process%20but%20also%0Areduces%20computational%20requirements%2C%20maintaining%20or%20surpassing%20traditional%0Atraining%20methods%27%20performance%20in%20various%20NLP%20tasks%20and%20T5%20model%20sizes.%20Finally%2C%0Awe%20validate%20our%20theoretical%20framework%2C%20which%20guides%20the%20optimal%20use%20of%20Deep%0AFusion%2C%20showing%20that%20with%20carefully%20optimized%20training%20dynamics%2C%20it%0Asignificantly%20reduces%20both%20training%20time%20and%20resource%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11903v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Fusion%253A%2520Efficient%2520Network%2520Training%2520via%2520Pre-trained%2520Initializations%26entry.906535625%3DHanna%2520Mazzawi%2520and%2520Xavi%2520Gonzalvo%2520and%2520Michael%2520Wunder%2520and%2520Sammy%2520Jerome%2520and%2520Benoit%2520Dherin%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deep%2520learning%2520has%2520made%2520remarkable%2520progress%2520in%2520a%2520wide%2520range%250Aof%2520domains%252C%2520with%2520a%2520particularly%2520notable%2520impact%2520on%2520natural%2520language%2520processing%250Atasks.%2520One%2520of%2520the%2520challenges%2520associated%2520with%2520training%2520deep%2520neural%2520networks%2520in%250Athe%2520context%2520of%2520LLMs%2520is%2520the%2520need%2520for%2520large%2520amounts%2520of%2520computational%2520resources%250Aand%2520time.%2520To%2520mitigate%2520this%252C%2520network%2520growing%2520algorithms%2520offer%2520potential%2520cost%250Asavings%252C%2520but%2520their%2520underlying%2520mechanisms%2520are%2520poorly%2520understood.%2520We%2520present%2520two%250Anotable%2520contributions%2520in%2520this%2520paper.%2520First%252C%2520we%2520present%2520Deep%2520Fusion%252C%2520an%250Aefficient%2520approach%2520to%2520network%2520training%2520that%2520leverages%2520pre-trained%250Ainitializations%2520of%2520smaller%2520networks.%2520Second%252C%2520we%2520propose%2520a%2520theoretical%2520framework%250Ausing%2520backward%2520error%2520analysis%2520to%2520illustrate%2520the%2520dynamics%2520of%2520mid-training%250Anetwork%2520growth.%2520Our%2520experiments%2520show%2520how%2520Deep%2520Fusion%2520is%2520a%2520practical%2520and%250Aeffective%2520approach%2520that%2520not%2520only%2520accelerates%2520the%2520training%2520process%2520but%2520also%250Areduces%2520computational%2520requirements%252C%2520maintaining%2520or%2520surpassing%2520traditional%250Atraining%2520methods%2527%2520performance%2520in%2520various%2520NLP%2520tasks%2520and%2520T5%2520model%2520sizes.%2520Finally%252C%250Awe%2520validate%2520our%2520theoretical%2520framework%252C%2520which%2520guides%2520the%2520optimal%2520use%2520of%2520Deep%250AFusion%252C%2520showing%2520that%2520with%2520carefully%2520optimized%2520training%2520dynamics%252C%2520it%250Asignificantly%2520reduces%2520both%2520training%2520time%2520and%2520resource%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.11903v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Fusion%3A%20Efficient%20Network%20Training%20via%20Pre-trained%20Initializations&entry.906535625=Hanna%20Mazzawi%20and%20Xavi%20Gonzalvo%20and%20Michael%20Wunder%20and%20Sammy%20Jerome%20and%20Benoit%20Dherin&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20learning%20has%20made%20remarkable%20progress%20in%20a%20wide%20range%0Aof%20domains%2C%20with%20a%20particularly%20notable%20impact%20on%20natural%20language%20processing%0Atasks.%20One%20of%20the%20challenges%20associated%20with%20training%20deep%20neural%20networks%20in%0Athe%20context%20of%20LLMs%20is%20the%20need%20for%20large%20amounts%20of%20computational%20resources%0Aand%20time.%20To%20mitigate%20this%2C%20network%20growing%20algorithms%20offer%20potential%20cost%0Asavings%2C%20but%20their%20underlying%20mechanisms%20are%20poorly%20understood.%20We%20present%20two%0Anotable%20contributions%20in%20this%20paper.%20First%2C%20we%20present%20Deep%20Fusion%2C%20an%0Aefficient%20approach%20to%20network%20training%20that%20leverages%20pre-trained%0Ainitializations%20of%20smaller%20networks.%20Second%2C%20we%20propose%20a%20theoretical%20framework%0Ausing%20backward%20error%20analysis%20to%20illustrate%20the%20dynamics%20of%20mid-training%0Anetwork%20growth.%20Our%20experiments%20show%20how%20Deep%20Fusion%20is%20a%20practical%20and%0Aeffective%20approach%20that%20not%20only%20accelerates%20the%20training%20process%20but%20also%0Areduces%20computational%20requirements%2C%20maintaining%20or%20surpassing%20traditional%0Atraining%20methods%27%20performance%20in%20various%20NLP%20tasks%20and%20T5%20model%20sizes.%20Finally%2C%0Awe%20validate%20our%20theoretical%20framework%2C%20which%20guides%20the%20optimal%20use%20of%20Deep%0AFusion%2C%20showing%20that%20with%20carefully%20optimized%20training%20dynamics%2C%20it%0Asignificantly%20reduces%20both%20training%20time%20and%20resource%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11903v3&entry.124074799=Read"},
{"title": "WhaleNet: a Novel Deep Learning Architecture for Marine Mammals\n  Vocalizations on Watkins Marine Mammal Sound Database", "author": "Alessandro Licciardi and Davide Carbone", "abstract": "  Marine mammal communication is a complex field, hindered by the diversity of\nvocalizations and environmental factors. The Watkins Marine Mammal Sound\nDatabase (WMMD) constitutes a comprehensive labeled dataset employed in machine\nlearning applications. Nevertheless, the methodologies for data preparation,\npreprocessing, and classification documented in the literature exhibit\nconsiderable variability and are typically not applied to the dataset in its\nentirety. This study initially undertakes a concise review of the\nstate-of-the-art benchmarks pertaining to the dataset, with a particular focus\non clarifying data preparation and preprocessing techniques. Subsequently, we\nexplore the utilization of the Wavelet Scattering Transform (WST) and Mel\nspectrogram as preprocessing mechanisms for feature extraction. In this paper,\nwe introduce \\textbf{WhaleNet} (Wavelet Highly Adaptive Learning Ensemble\nNetwork), a sophisticated deep ensemble architecture for the classification of\nmarine mammal vocalizations, leveraging both WST and Mel spectrogram for\nenhanced feature discrimination. By integrating the insights derived from WST\nand Mel representations, we achieved an improvement in classification accuracy\nby $8-10\\%$ over existing architectures, corresponding to a classification\naccuracy of $97.61\\%$.\n", "link": "http://arxiv.org/abs/2402.17775v2", "date": "2024-06-26", "relevancy": 2.2335, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4726}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4346}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WhaleNet%3A%20a%20Novel%20Deep%20Learning%20Architecture%20for%20Marine%20Mammals%0A%20%20Vocalizations%20on%20Watkins%20Marine%20Mammal%20Sound%20Database&body=Title%3A%20WhaleNet%3A%20a%20Novel%20Deep%20Learning%20Architecture%20for%20Marine%20Mammals%0A%20%20Vocalizations%20on%20Watkins%20Marine%20Mammal%20Sound%20Database%0AAuthor%3A%20Alessandro%20Licciardi%20and%20Davide%20Carbone%0AAbstract%3A%20%20%20Marine%20mammal%20communication%20is%20a%20complex%20field%2C%20hindered%20by%20the%20diversity%20of%0Avocalizations%20and%20environmental%20factors.%20The%20Watkins%20Marine%20Mammal%20Sound%0ADatabase%20%28WMMD%29%20constitutes%20a%20comprehensive%20labeled%20dataset%20employed%20in%20machine%0Alearning%20applications.%20Nevertheless%2C%20the%20methodologies%20for%20data%20preparation%2C%0Apreprocessing%2C%20and%20classification%20documented%20in%20the%20literature%20exhibit%0Aconsiderable%20variability%20and%20are%20typically%20not%20applied%20to%20the%20dataset%20in%20its%0Aentirety.%20This%20study%20initially%20undertakes%20a%20concise%20review%20of%20the%0Astate-of-the-art%20benchmarks%20pertaining%20to%20the%20dataset%2C%20with%20a%20particular%20focus%0Aon%20clarifying%20data%20preparation%20and%20preprocessing%20techniques.%20Subsequently%2C%20we%0Aexplore%20the%20utilization%20of%20the%20Wavelet%20Scattering%20Transform%20%28WST%29%20and%20Mel%0Aspectrogram%20as%20preprocessing%20mechanisms%20for%20feature%20extraction.%20In%20this%20paper%2C%0Awe%20introduce%20%5Ctextbf%7BWhaleNet%7D%20%28Wavelet%20Highly%20Adaptive%20Learning%20Ensemble%0ANetwork%29%2C%20a%20sophisticated%20deep%20ensemble%20architecture%20for%20the%20classification%20of%0Amarine%20mammal%20vocalizations%2C%20leveraging%20both%20WST%20and%20Mel%20spectrogram%20for%0Aenhanced%20feature%20discrimination.%20By%20integrating%20the%20insights%20derived%20from%20WST%0Aand%20Mel%20representations%2C%20we%20achieved%20an%20improvement%20in%20classification%20accuracy%0Aby%20%248-10%5C%25%24%20over%20existing%20architectures%2C%20corresponding%20to%20a%20classification%0Aaccuracy%20of%20%2497.61%5C%25%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhaleNet%253A%2520a%2520Novel%2520Deep%2520Learning%2520Architecture%2520for%2520Marine%2520Mammals%250A%2520%2520Vocalizations%2520on%2520Watkins%2520Marine%2520Mammal%2520Sound%2520Database%26entry.906535625%3DAlessandro%2520Licciardi%2520and%2520Davide%2520Carbone%26entry.1292438233%3D%2520%2520Marine%2520mammal%2520communication%2520is%2520a%2520complex%2520field%252C%2520hindered%2520by%2520the%2520diversity%2520of%250Avocalizations%2520and%2520environmental%2520factors.%2520The%2520Watkins%2520Marine%2520Mammal%2520Sound%250ADatabase%2520%2528WMMD%2529%2520constitutes%2520a%2520comprehensive%2520labeled%2520dataset%2520employed%2520in%2520machine%250Alearning%2520applications.%2520Nevertheless%252C%2520the%2520methodologies%2520for%2520data%2520preparation%252C%250Apreprocessing%252C%2520and%2520classification%2520documented%2520in%2520the%2520literature%2520exhibit%250Aconsiderable%2520variability%2520and%2520are%2520typically%2520not%2520applied%2520to%2520the%2520dataset%2520in%2520its%250Aentirety.%2520This%2520study%2520initially%2520undertakes%2520a%2520concise%2520review%2520of%2520the%250Astate-of-the-art%2520benchmarks%2520pertaining%2520to%2520the%2520dataset%252C%2520with%2520a%2520particular%2520focus%250Aon%2520clarifying%2520data%2520preparation%2520and%2520preprocessing%2520techniques.%2520Subsequently%252C%2520we%250Aexplore%2520the%2520utilization%2520of%2520the%2520Wavelet%2520Scattering%2520Transform%2520%2528WST%2529%2520and%2520Mel%250Aspectrogram%2520as%2520preprocessing%2520mechanisms%2520for%2520feature%2520extraction.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520%255Ctextbf%257BWhaleNet%257D%2520%2528Wavelet%2520Highly%2520Adaptive%2520Learning%2520Ensemble%250ANetwork%2529%252C%2520a%2520sophisticated%2520deep%2520ensemble%2520architecture%2520for%2520the%2520classification%2520of%250Amarine%2520mammal%2520vocalizations%252C%2520leveraging%2520both%2520WST%2520and%2520Mel%2520spectrogram%2520for%250Aenhanced%2520feature%2520discrimination.%2520By%2520integrating%2520the%2520insights%2520derived%2520from%2520WST%250Aand%2520Mel%2520representations%252C%2520we%2520achieved%2520an%2520improvement%2520in%2520classification%2520accuracy%250Aby%2520%25248-10%255C%2525%2524%2520over%2520existing%2520architectures%252C%2520corresponding%2520to%2520a%2520classification%250Aaccuracy%2520of%2520%252497.61%255C%2525%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WhaleNet%3A%20a%20Novel%20Deep%20Learning%20Architecture%20for%20Marine%20Mammals%0A%20%20Vocalizations%20on%20Watkins%20Marine%20Mammal%20Sound%20Database&entry.906535625=Alessandro%20Licciardi%20and%20Davide%20Carbone&entry.1292438233=%20%20Marine%20mammal%20communication%20is%20a%20complex%20field%2C%20hindered%20by%20the%20diversity%20of%0Avocalizations%20and%20environmental%20factors.%20The%20Watkins%20Marine%20Mammal%20Sound%0ADatabase%20%28WMMD%29%20constitutes%20a%20comprehensive%20labeled%20dataset%20employed%20in%20machine%0Alearning%20applications.%20Nevertheless%2C%20the%20methodologies%20for%20data%20preparation%2C%0Apreprocessing%2C%20and%20classification%20documented%20in%20the%20literature%20exhibit%0Aconsiderable%20variability%20and%20are%20typically%20not%20applied%20to%20the%20dataset%20in%20its%0Aentirety.%20This%20study%20initially%20undertakes%20a%20concise%20review%20of%20the%0Astate-of-the-art%20benchmarks%20pertaining%20to%20the%20dataset%2C%20with%20a%20particular%20focus%0Aon%20clarifying%20data%20preparation%20and%20preprocessing%20techniques.%20Subsequently%2C%20we%0Aexplore%20the%20utilization%20of%20the%20Wavelet%20Scattering%20Transform%20%28WST%29%20and%20Mel%0Aspectrogram%20as%20preprocessing%20mechanisms%20for%20feature%20extraction.%20In%20this%20paper%2C%0Awe%20introduce%20%5Ctextbf%7BWhaleNet%7D%20%28Wavelet%20Highly%20Adaptive%20Learning%20Ensemble%0ANetwork%29%2C%20a%20sophisticated%20deep%20ensemble%20architecture%20for%20the%20classification%20of%0Amarine%20mammal%20vocalizations%2C%20leveraging%20both%20WST%20and%20Mel%20spectrogram%20for%0Aenhanced%20feature%20discrimination.%20By%20integrating%20the%20insights%20derived%20from%20WST%0Aand%20Mel%20representations%2C%20we%20achieved%20an%20improvement%20in%20classification%20accuracy%0Aby%20%248-10%5C%25%24%20over%20existing%20architectures%2C%20corresponding%20to%20a%20classification%0Aaccuracy%20of%20%2497.61%5C%25%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17775v2&entry.124074799=Read"},
{"title": "Multi-modal Evidential Fusion Network for Trusted PET/CT Tumor\n  Segmentation", "author": "Yuxuan Qi and Li Lin and Jiajun Wang and Jingya Zhang and Bin Zhang", "abstract": "  Accurate segmentation of tumors in PET/CT images is important in\ncomputer-aided diagnosis and treatment of cancer. The key issue of such a\nsegmentation problem lies in the effective integration of complementary\ninformation from PET and CT images. However, the quality of PET and CT images\nvaries widely in clinical settings, which leads to uncertainty in the modality\ninformation extracted by networks. To take the uncertainty into account in\nmulti-modal information fusion, this paper proposes a novel Multi-modal\nEvidential Fusion Network (MEFN) comprising a Cross-Modal Feature Learning\n(CFL) module and a Multi-modal Trusted Fusion (MTF) module. The CFL module\nreduces the domain gap upon modality conversion and highlights common tumor\nfeatures, thereby alleviating the needs of the segmentation module to handle\nmodality specificity. The MTF module utilizes mutual attention mechanisms and\nan uncertainty calibrator to fuse modality features based on modality\nuncertainty and then fuse the segmentation results under the guidance of\nDempster-Shafer Theory. Besides, a new uncertainty perceptual loss is\nintroduced to force the model focusing on uncertain features and hence improve\nits ability to extract trusted modality information. Extensive comparative\nexperiments are conducted on two publicly available PET/CT datasets to evaluate\nthe performance of our proposed method whose results demonstrate that our MEFN\nsignificantly outperforms state-of-the-art methods with improvements of 2.15%\nand 3.23% in DSC scores on the AutoPET dataset and the Hecktor dataset,\nrespectively. More importantly, our model can provide radiologists with\ncredible uncertainty of the segmentation results for their decision in\naccepting or rejecting the automatic segmentation results, which is\nparticularly important for clinical applications. Our code will be available at\nhttps://github.com/QPaws/MEFN.\n", "link": "http://arxiv.org/abs/2406.18327v1", "date": "2024-06-26", "relevancy": 2.2263, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5476}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Evidential%20Fusion%20Network%20for%20Trusted%20PET/CT%20Tumor%0A%20%20Segmentation&body=Title%3A%20Multi-modal%20Evidential%20Fusion%20Network%20for%20Trusted%20PET/CT%20Tumor%0A%20%20Segmentation%0AAuthor%3A%20Yuxuan%20Qi%20and%20Li%20Lin%20and%20Jiajun%20Wang%20and%20Jingya%20Zhang%20and%20Bin%20Zhang%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20tumors%20in%20PET/CT%20images%20is%20important%20in%0Acomputer-aided%20diagnosis%20and%20treatment%20of%20cancer.%20The%20key%20issue%20of%20such%20a%0Asegmentation%20problem%20lies%20in%20the%20effective%20integration%20of%20complementary%0Ainformation%20from%20PET%20and%20CT%20images.%20However%2C%20the%20quality%20of%20PET%20and%20CT%20images%0Avaries%20widely%20in%20clinical%20settings%2C%20which%20leads%20to%20uncertainty%20in%20the%20modality%0Ainformation%20extracted%20by%20networks.%20To%20take%20the%20uncertainty%20into%20account%20in%0Amulti-modal%20information%20fusion%2C%20this%20paper%20proposes%20a%20novel%20Multi-modal%0AEvidential%20Fusion%20Network%20%28MEFN%29%20comprising%20a%20Cross-Modal%20Feature%20Learning%0A%28CFL%29%20module%20and%20a%20Multi-modal%20Trusted%20Fusion%20%28MTF%29%20module.%20The%20CFL%20module%0Areduces%20the%20domain%20gap%20upon%20modality%20conversion%20and%20highlights%20common%20tumor%0Afeatures%2C%20thereby%20alleviating%20the%20needs%20of%20the%20segmentation%20module%20to%20handle%0Amodality%20specificity.%20The%20MTF%20module%20utilizes%20mutual%20attention%20mechanisms%20and%0Aan%20uncertainty%20calibrator%20to%20fuse%20modality%20features%20based%20on%20modality%0Auncertainty%20and%20then%20fuse%20the%20segmentation%20results%20under%20the%20guidance%20of%0ADempster-Shafer%20Theory.%20Besides%2C%20a%20new%20uncertainty%20perceptual%20loss%20is%0Aintroduced%20to%20force%20the%20model%20focusing%20on%20uncertain%20features%20and%20hence%20improve%0Aits%20ability%20to%20extract%20trusted%20modality%20information.%20Extensive%20comparative%0Aexperiments%20are%20conducted%20on%20two%20publicly%20available%20PET/CT%20datasets%20to%20evaluate%0Athe%20performance%20of%20our%20proposed%20method%20whose%20results%20demonstrate%20that%20our%20MEFN%0Asignificantly%20outperforms%20state-of-the-art%20methods%20with%20improvements%20of%202.15%25%0Aand%203.23%25%20in%20DSC%20scores%20on%20the%20AutoPET%20dataset%20and%20the%20Hecktor%20dataset%2C%0Arespectively.%20More%20importantly%2C%20our%20model%20can%20provide%20radiologists%20with%0Acredible%20uncertainty%20of%20the%20segmentation%20results%20for%20their%20decision%20in%0Aaccepting%20or%20rejecting%20the%20automatic%20segmentation%20results%2C%20which%20is%0Aparticularly%20important%20for%20clinical%20applications.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/QPaws/MEFN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Evidential%2520Fusion%2520Network%2520for%2520Trusted%2520PET/CT%2520Tumor%250A%2520%2520Segmentation%26entry.906535625%3DYuxuan%2520Qi%2520and%2520Li%2520Lin%2520and%2520Jiajun%2520Wang%2520and%2520Jingya%2520Zhang%2520and%2520Bin%2520Zhang%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520tumors%2520in%2520PET/CT%2520images%2520is%2520important%2520in%250Acomputer-aided%2520diagnosis%2520and%2520treatment%2520of%2520cancer.%2520The%2520key%2520issue%2520of%2520such%2520a%250Asegmentation%2520problem%2520lies%2520in%2520the%2520effective%2520integration%2520of%2520complementary%250Ainformation%2520from%2520PET%2520and%2520CT%2520images.%2520However%252C%2520the%2520quality%2520of%2520PET%2520and%2520CT%2520images%250Avaries%2520widely%2520in%2520clinical%2520settings%252C%2520which%2520leads%2520to%2520uncertainty%2520in%2520the%2520modality%250Ainformation%2520extracted%2520by%2520networks.%2520To%2520take%2520the%2520uncertainty%2520into%2520account%2520in%250Amulti-modal%2520information%2520fusion%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520Multi-modal%250AEvidential%2520Fusion%2520Network%2520%2528MEFN%2529%2520comprising%2520a%2520Cross-Modal%2520Feature%2520Learning%250A%2528CFL%2529%2520module%2520and%2520a%2520Multi-modal%2520Trusted%2520Fusion%2520%2528MTF%2529%2520module.%2520The%2520CFL%2520module%250Areduces%2520the%2520domain%2520gap%2520upon%2520modality%2520conversion%2520and%2520highlights%2520common%2520tumor%250Afeatures%252C%2520thereby%2520alleviating%2520the%2520needs%2520of%2520the%2520segmentation%2520module%2520to%2520handle%250Amodality%2520specificity.%2520The%2520MTF%2520module%2520utilizes%2520mutual%2520attention%2520mechanisms%2520and%250Aan%2520uncertainty%2520calibrator%2520to%2520fuse%2520modality%2520features%2520based%2520on%2520modality%250Auncertainty%2520and%2520then%2520fuse%2520the%2520segmentation%2520results%2520under%2520the%2520guidance%2520of%250ADempster-Shafer%2520Theory.%2520Besides%252C%2520a%2520new%2520uncertainty%2520perceptual%2520loss%2520is%250Aintroduced%2520to%2520force%2520the%2520model%2520focusing%2520on%2520uncertain%2520features%2520and%2520hence%2520improve%250Aits%2520ability%2520to%2520extract%2520trusted%2520modality%2520information.%2520Extensive%2520comparative%250Aexperiments%2520are%2520conducted%2520on%2520two%2520publicly%2520available%2520PET/CT%2520datasets%2520to%2520evaluate%250Athe%2520performance%2520of%2520our%2520proposed%2520method%2520whose%2520results%2520demonstrate%2520that%2520our%2520MEFN%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%2520with%2520improvements%2520of%25202.15%2525%250Aand%25203.23%2525%2520in%2520DSC%2520scores%2520on%2520the%2520AutoPET%2520dataset%2520and%2520the%2520Hecktor%2520dataset%252C%250Arespectively.%2520More%2520importantly%252C%2520our%2520model%2520can%2520provide%2520radiologists%2520with%250Acredible%2520uncertainty%2520of%2520the%2520segmentation%2520results%2520for%2520their%2520decision%2520in%250Aaccepting%2520or%2520rejecting%2520the%2520automatic%2520segmentation%2520results%252C%2520which%2520is%250Aparticularly%2520important%2520for%2520clinical%2520applications.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/QPaws/MEFN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Evidential%20Fusion%20Network%20for%20Trusted%20PET/CT%20Tumor%0A%20%20Segmentation&entry.906535625=Yuxuan%20Qi%20and%20Li%20Lin%20and%20Jiajun%20Wang%20and%20Jingya%20Zhang%20and%20Bin%20Zhang&entry.1292438233=%20%20Accurate%20segmentation%20of%20tumors%20in%20PET/CT%20images%20is%20important%20in%0Acomputer-aided%20diagnosis%20and%20treatment%20of%20cancer.%20The%20key%20issue%20of%20such%20a%0Asegmentation%20problem%20lies%20in%20the%20effective%20integration%20of%20complementary%0Ainformation%20from%20PET%20and%20CT%20images.%20However%2C%20the%20quality%20of%20PET%20and%20CT%20images%0Avaries%20widely%20in%20clinical%20settings%2C%20which%20leads%20to%20uncertainty%20in%20the%20modality%0Ainformation%20extracted%20by%20networks.%20To%20take%20the%20uncertainty%20into%20account%20in%0Amulti-modal%20information%20fusion%2C%20this%20paper%20proposes%20a%20novel%20Multi-modal%0AEvidential%20Fusion%20Network%20%28MEFN%29%20comprising%20a%20Cross-Modal%20Feature%20Learning%0A%28CFL%29%20module%20and%20a%20Multi-modal%20Trusted%20Fusion%20%28MTF%29%20module.%20The%20CFL%20module%0Areduces%20the%20domain%20gap%20upon%20modality%20conversion%20and%20highlights%20common%20tumor%0Afeatures%2C%20thereby%20alleviating%20the%20needs%20of%20the%20segmentation%20module%20to%20handle%0Amodality%20specificity.%20The%20MTF%20module%20utilizes%20mutual%20attention%20mechanisms%20and%0Aan%20uncertainty%20calibrator%20to%20fuse%20modality%20features%20based%20on%20modality%0Auncertainty%20and%20then%20fuse%20the%20segmentation%20results%20under%20the%20guidance%20of%0ADempster-Shafer%20Theory.%20Besides%2C%20a%20new%20uncertainty%20perceptual%20loss%20is%0Aintroduced%20to%20force%20the%20model%20focusing%20on%20uncertain%20features%20and%20hence%20improve%0Aits%20ability%20to%20extract%20trusted%20modality%20information.%20Extensive%20comparative%0Aexperiments%20are%20conducted%20on%20two%20publicly%20available%20PET/CT%20datasets%20to%20evaluate%0Athe%20performance%20of%20our%20proposed%20method%20whose%20results%20demonstrate%20that%20our%20MEFN%0Asignificantly%20outperforms%20state-of-the-art%20methods%20with%20improvements%20of%202.15%25%0Aand%203.23%25%20in%20DSC%20scores%20on%20the%20AutoPET%20dataset%20and%20the%20Hecktor%20dataset%2C%0Arespectively.%20More%20importantly%2C%20our%20model%20can%20provide%20radiologists%20with%0Acredible%20uncertainty%20of%20the%20segmentation%20results%20for%20their%20decision%20in%0Aaccepting%20or%20rejecting%20the%20automatic%20segmentation%20results%2C%20which%20is%0Aparticularly%20important%20for%20clinical%20applications.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/QPaws/MEFN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18327v1&entry.124074799=Read"},
{"title": "Efficient Low-rank Identification via Accelerated Iteratively Reweighted\n  Nuclear Norm Minimization", "author": "Hao Wang and Ye Wang and Xiangyu Yang", "abstract": "  This paper considers the problem of minimizing the sum of a smooth function\nand the Schatten-$p$ norm of the matrix. Our contribution involves proposing\naccelerated iteratively reweighted nuclear norm methods designed for solving\nthe nonconvex low-rank minimization problem. Two major novelties characterize\nour approach. Firstly, the proposed method possesses a rank identification\nproperty, enabling the provable identification of the \"correct\" rank of the\nstationary point within a finite number of iterations. Secondly, we introduce\nan adaptive updating strategy for smoothing parameters. This strategy\nautomatically fixes parameters associated with zero singular values as\nconstants upon detecting the \"correct\" rank while quickly driving the rest of\nthe parameters to zero. This adaptive behavior transforms the algorithm into\none that effectively solves smooth problems after a few iterations, setting our\nwork apart from existing iteratively reweighted methods for low-rank\noptimization. We prove the global convergence of the proposed algorithm,\nguaranteeing that every limit point of the iterates is a critical point.\nFurthermore, a local convergence rate analysis is provided under the\nKurdyka-{\\L}ojasiewicz property. We conduct numerical experiments using both\nsynthetic and real data to showcase our algorithm's efficiency and superiority\nover existing methods.\n", "link": "http://arxiv.org/abs/2406.15713v2", "date": "2024-06-26", "relevancy": 2.2091, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4522}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4428}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Low-rank%20Identification%20via%20Accelerated%20Iteratively%20Reweighted%0A%20%20Nuclear%20Norm%20Minimization&body=Title%3A%20Efficient%20Low-rank%20Identification%20via%20Accelerated%20Iteratively%20Reweighted%0A%20%20Nuclear%20Norm%20Minimization%0AAuthor%3A%20Hao%20Wang%20and%20Ye%20Wang%20and%20Xiangyu%20Yang%0AAbstract%3A%20%20%20This%20paper%20considers%20the%20problem%20of%20minimizing%20the%20sum%20of%20a%20smooth%20function%0Aand%20the%20Schatten-%24p%24%20norm%20of%20the%20matrix.%20Our%20contribution%20involves%20proposing%0Aaccelerated%20iteratively%20reweighted%20nuclear%20norm%20methods%20designed%20for%20solving%0Athe%20nonconvex%20low-rank%20minimization%20problem.%20Two%20major%20novelties%20characterize%0Aour%20approach.%20Firstly%2C%20the%20proposed%20method%20possesses%20a%20rank%20identification%0Aproperty%2C%20enabling%20the%20provable%20identification%20of%20the%20%22correct%22%20rank%20of%20the%0Astationary%20point%20within%20a%20finite%20number%20of%20iterations.%20Secondly%2C%20we%20introduce%0Aan%20adaptive%20updating%20strategy%20for%20smoothing%20parameters.%20This%20strategy%0Aautomatically%20fixes%20parameters%20associated%20with%20zero%20singular%20values%20as%0Aconstants%20upon%20detecting%20the%20%22correct%22%20rank%20while%20quickly%20driving%20the%20rest%20of%0Athe%20parameters%20to%20zero.%20This%20adaptive%20behavior%20transforms%20the%20algorithm%20into%0Aone%20that%20effectively%20solves%20smooth%20problems%20after%20a%20few%20iterations%2C%20setting%20our%0Awork%20apart%20from%20existing%20iteratively%20reweighted%20methods%20for%20low-rank%0Aoptimization.%20We%20prove%20the%20global%20convergence%20of%20the%20proposed%20algorithm%2C%0Aguaranteeing%20that%20every%20limit%20point%20of%20the%20iterates%20is%20a%20critical%20point.%0AFurthermore%2C%20a%20local%20convergence%20rate%20analysis%20is%20provided%20under%20the%0AKurdyka-%7B%5CL%7Dojasiewicz%20property.%20We%20conduct%20numerical%20experiments%20using%20both%0Asynthetic%20and%20real%20data%20to%20showcase%20our%20algorithm%27s%20efficiency%20and%20superiority%0Aover%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Low-rank%2520Identification%2520via%2520Accelerated%2520Iteratively%2520Reweighted%250A%2520%2520Nuclear%2520Norm%2520Minimization%26entry.906535625%3DHao%2520Wang%2520and%2520Ye%2520Wang%2520and%2520Xiangyu%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520the%2520problem%2520of%2520minimizing%2520the%2520sum%2520of%2520a%2520smooth%2520function%250Aand%2520the%2520Schatten-%2524p%2524%2520norm%2520of%2520the%2520matrix.%2520Our%2520contribution%2520involves%2520proposing%250Aaccelerated%2520iteratively%2520reweighted%2520nuclear%2520norm%2520methods%2520designed%2520for%2520solving%250Athe%2520nonconvex%2520low-rank%2520minimization%2520problem.%2520Two%2520major%2520novelties%2520characterize%250Aour%2520approach.%2520Firstly%252C%2520the%2520proposed%2520method%2520possesses%2520a%2520rank%2520identification%250Aproperty%252C%2520enabling%2520the%2520provable%2520identification%2520of%2520the%2520%2522correct%2522%2520rank%2520of%2520the%250Astationary%2520point%2520within%2520a%2520finite%2520number%2520of%2520iterations.%2520Secondly%252C%2520we%2520introduce%250Aan%2520adaptive%2520updating%2520strategy%2520for%2520smoothing%2520parameters.%2520This%2520strategy%250Aautomatically%2520fixes%2520parameters%2520associated%2520with%2520zero%2520singular%2520values%2520as%250Aconstants%2520upon%2520detecting%2520the%2520%2522correct%2522%2520rank%2520while%2520quickly%2520driving%2520the%2520rest%2520of%250Athe%2520parameters%2520to%2520zero.%2520This%2520adaptive%2520behavior%2520transforms%2520the%2520algorithm%2520into%250Aone%2520that%2520effectively%2520solves%2520smooth%2520problems%2520after%2520a%2520few%2520iterations%252C%2520setting%2520our%250Awork%2520apart%2520from%2520existing%2520iteratively%2520reweighted%2520methods%2520for%2520low-rank%250Aoptimization.%2520We%2520prove%2520the%2520global%2520convergence%2520of%2520the%2520proposed%2520algorithm%252C%250Aguaranteeing%2520that%2520every%2520limit%2520point%2520of%2520the%2520iterates%2520is%2520a%2520critical%2520point.%250AFurthermore%252C%2520a%2520local%2520convergence%2520rate%2520analysis%2520is%2520provided%2520under%2520the%250AKurdyka-%257B%255CL%257Dojasiewicz%2520property.%2520We%2520conduct%2520numerical%2520experiments%2520using%2520both%250Asynthetic%2520and%2520real%2520data%2520to%2520showcase%2520our%2520algorithm%2527s%2520efficiency%2520and%2520superiority%250Aover%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Low-rank%20Identification%20via%20Accelerated%20Iteratively%20Reweighted%0A%20%20Nuclear%20Norm%20Minimization&entry.906535625=Hao%20Wang%20and%20Ye%20Wang%20and%20Xiangyu%20Yang&entry.1292438233=%20%20This%20paper%20considers%20the%20problem%20of%20minimizing%20the%20sum%20of%20a%20smooth%20function%0Aand%20the%20Schatten-%24p%24%20norm%20of%20the%20matrix.%20Our%20contribution%20involves%20proposing%0Aaccelerated%20iteratively%20reweighted%20nuclear%20norm%20methods%20designed%20for%20solving%0Athe%20nonconvex%20low-rank%20minimization%20problem.%20Two%20major%20novelties%20characterize%0Aour%20approach.%20Firstly%2C%20the%20proposed%20method%20possesses%20a%20rank%20identification%0Aproperty%2C%20enabling%20the%20provable%20identification%20of%20the%20%22correct%22%20rank%20of%20the%0Astationary%20point%20within%20a%20finite%20number%20of%20iterations.%20Secondly%2C%20we%20introduce%0Aan%20adaptive%20updating%20strategy%20for%20smoothing%20parameters.%20This%20strategy%0Aautomatically%20fixes%20parameters%20associated%20with%20zero%20singular%20values%20as%0Aconstants%20upon%20detecting%20the%20%22correct%22%20rank%20while%20quickly%20driving%20the%20rest%20of%0Athe%20parameters%20to%20zero.%20This%20adaptive%20behavior%20transforms%20the%20algorithm%20into%0Aone%20that%20effectively%20solves%20smooth%20problems%20after%20a%20few%20iterations%2C%20setting%20our%0Awork%20apart%20from%20existing%20iteratively%20reweighted%20methods%20for%20low-rank%0Aoptimization.%20We%20prove%20the%20global%20convergence%20of%20the%20proposed%20algorithm%2C%0Aguaranteeing%20that%20every%20limit%20point%20of%20the%20iterates%20is%20a%20critical%20point.%0AFurthermore%2C%20a%20local%20convergence%20rate%20analysis%20is%20provided%20under%20the%0AKurdyka-%7B%5CL%7Dojasiewicz%20property.%20We%20conduct%20numerical%20experiments%20using%20both%0Asynthetic%20and%20real%20data%20to%20showcase%20our%20algorithm%27s%20efficiency%20and%20superiority%0Aover%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15713v2&entry.124074799=Read"},
{"title": "Combining Reconstruction and Contrastive Methods for Multimodal\n  Representations in RL", "author": "Philipp Becker and Sebastian Mossburger and Fabian Otto and Gerhard Neumann", "abstract": "  Learning self-supervised representations using reconstruction or contrastive\nlosses improves performance and sample complexity of image-based and multimodal\nreinforcement learning (RL). Here, different self-supervised loss functions\nhave distinct advantages and limitations depending on the information density\nof the underlying sensor modality. Reconstruction provides strong learning\nsignals but is susceptible to distractions and spurious information. While\ncontrastive approaches can ignore those, they may fail to capture all relevant\ndetails and can lead to representation collapse. For multimodal RL, this\nsuggests that different modalities should be treated differently based on the\namount of distractions in the signal. We propose Contrastive Reconstructive\nAggregated representation Learning (CoRAL), a unified framework enabling us to\nchoose the most appropriate self-supervised loss for each sensor modality and\nallowing the representation to better focus on relevant aspects. We evaluate\nCoRAL's benefits on a wide range of tasks with images containing distractions\nor occlusions, a new locomotion suite, and a challenging manipulation suite\nwith visually realistic distractions. Our results show that learning a\nmultimodal representation by combining contrastive and reconstruction-based\nlosses can significantly improve performance and solve tasks that are out of\nreach for more naive representation learning approaches and other recent\nbaselines.\n", "link": "http://arxiv.org/abs/2302.05342v4", "date": "2024-06-26", "relevancy": 2.197, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5534}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Reconstruction%20and%20Contrastive%20Methods%20for%20Multimodal%0A%20%20Representations%20in%20RL&body=Title%3A%20Combining%20Reconstruction%20and%20Contrastive%20Methods%20for%20Multimodal%0A%20%20Representations%20in%20RL%0AAuthor%3A%20Philipp%20Becker%20and%20Sebastian%20Mossburger%20and%20Fabian%20Otto%20and%20Gerhard%20Neumann%0AAbstract%3A%20%20%20Learning%20self-supervised%20representations%20using%20reconstruction%20or%20contrastive%0Alosses%20improves%20performance%20and%20sample%20complexity%20of%20image-based%20and%20multimodal%0Areinforcement%20learning%20%28RL%29.%20Here%2C%20different%20self-supervised%20loss%20functions%0Ahave%20distinct%20advantages%20and%20limitations%20depending%20on%20the%20information%20density%0Aof%20the%20underlying%20sensor%20modality.%20Reconstruction%20provides%20strong%20learning%0Asignals%20but%20is%20susceptible%20to%20distractions%20and%20spurious%20information.%20While%0Acontrastive%20approaches%20can%20ignore%20those%2C%20they%20may%20fail%20to%20capture%20all%20relevant%0Adetails%20and%20can%20lead%20to%20representation%20collapse.%20For%20multimodal%20RL%2C%20this%0Asuggests%20that%20different%20modalities%20should%20be%20treated%20differently%20based%20on%20the%0Aamount%20of%20distractions%20in%20the%20signal.%20We%20propose%20Contrastive%20Reconstructive%0AAggregated%20representation%20Learning%20%28CoRAL%29%2C%20a%20unified%20framework%20enabling%20us%20to%0Achoose%20the%20most%20appropriate%20self-supervised%20loss%20for%20each%20sensor%20modality%20and%0Aallowing%20the%20representation%20to%20better%20focus%20on%20relevant%20aspects.%20We%20evaluate%0ACoRAL%27s%20benefits%20on%20a%20wide%20range%20of%20tasks%20with%20images%20containing%20distractions%0Aor%20occlusions%2C%20a%20new%20locomotion%20suite%2C%20and%20a%20challenging%20manipulation%20suite%0Awith%20visually%20realistic%20distractions.%20Our%20results%20show%20that%20learning%20a%0Amultimodal%20representation%20by%20combining%20contrastive%20and%20reconstruction-based%0Alosses%20can%20significantly%20improve%20performance%20and%20solve%20tasks%20that%20are%20out%20of%0Areach%20for%20more%20naive%20representation%20learning%20approaches%20and%20other%20recent%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.05342v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Reconstruction%2520and%2520Contrastive%2520Methods%2520for%2520Multimodal%250A%2520%2520Representations%2520in%2520RL%26entry.906535625%3DPhilipp%2520Becker%2520and%2520Sebastian%2520Mossburger%2520and%2520Fabian%2520Otto%2520and%2520Gerhard%2520Neumann%26entry.1292438233%3D%2520%2520Learning%2520self-supervised%2520representations%2520using%2520reconstruction%2520or%2520contrastive%250Alosses%2520improves%2520performance%2520and%2520sample%2520complexity%2520of%2520image-based%2520and%2520multimodal%250Areinforcement%2520learning%2520%2528RL%2529.%2520Here%252C%2520different%2520self-supervised%2520loss%2520functions%250Ahave%2520distinct%2520advantages%2520and%2520limitations%2520depending%2520on%2520the%2520information%2520density%250Aof%2520the%2520underlying%2520sensor%2520modality.%2520Reconstruction%2520provides%2520strong%2520learning%250Asignals%2520but%2520is%2520susceptible%2520to%2520distractions%2520and%2520spurious%2520information.%2520While%250Acontrastive%2520approaches%2520can%2520ignore%2520those%252C%2520they%2520may%2520fail%2520to%2520capture%2520all%2520relevant%250Adetails%2520and%2520can%2520lead%2520to%2520representation%2520collapse.%2520For%2520multimodal%2520RL%252C%2520this%250Asuggests%2520that%2520different%2520modalities%2520should%2520be%2520treated%2520differently%2520based%2520on%2520the%250Aamount%2520of%2520distractions%2520in%2520the%2520signal.%2520We%2520propose%2520Contrastive%2520Reconstructive%250AAggregated%2520representation%2520Learning%2520%2528CoRAL%2529%252C%2520a%2520unified%2520framework%2520enabling%2520us%2520to%250Achoose%2520the%2520most%2520appropriate%2520self-supervised%2520loss%2520for%2520each%2520sensor%2520modality%2520and%250Aallowing%2520the%2520representation%2520to%2520better%2520focus%2520on%2520relevant%2520aspects.%2520We%2520evaluate%250ACoRAL%2527s%2520benefits%2520on%2520a%2520wide%2520range%2520of%2520tasks%2520with%2520images%2520containing%2520distractions%250Aor%2520occlusions%252C%2520a%2520new%2520locomotion%2520suite%252C%2520and%2520a%2520challenging%2520manipulation%2520suite%250Awith%2520visually%2520realistic%2520distractions.%2520Our%2520results%2520show%2520that%2520learning%2520a%250Amultimodal%2520representation%2520by%2520combining%2520contrastive%2520and%2520reconstruction-based%250Alosses%2520can%2520significantly%2520improve%2520performance%2520and%2520solve%2520tasks%2520that%2520are%2520out%2520of%250Areach%2520for%2520more%2520naive%2520representation%2520learning%2520approaches%2520and%2520other%2520recent%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.05342v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Reconstruction%20and%20Contrastive%20Methods%20for%20Multimodal%0A%20%20Representations%20in%20RL&entry.906535625=Philipp%20Becker%20and%20Sebastian%20Mossburger%20and%20Fabian%20Otto%20and%20Gerhard%20Neumann&entry.1292438233=%20%20Learning%20self-supervised%20representations%20using%20reconstruction%20or%20contrastive%0Alosses%20improves%20performance%20and%20sample%20complexity%20of%20image-based%20and%20multimodal%0Areinforcement%20learning%20%28RL%29.%20Here%2C%20different%20self-supervised%20loss%20functions%0Ahave%20distinct%20advantages%20and%20limitations%20depending%20on%20the%20information%20density%0Aof%20the%20underlying%20sensor%20modality.%20Reconstruction%20provides%20strong%20learning%0Asignals%20but%20is%20susceptible%20to%20distractions%20and%20spurious%20information.%20While%0Acontrastive%20approaches%20can%20ignore%20those%2C%20they%20may%20fail%20to%20capture%20all%20relevant%0Adetails%20and%20can%20lead%20to%20representation%20collapse.%20For%20multimodal%20RL%2C%20this%0Asuggests%20that%20different%20modalities%20should%20be%20treated%20differently%20based%20on%20the%0Aamount%20of%20distractions%20in%20the%20signal.%20We%20propose%20Contrastive%20Reconstructive%0AAggregated%20representation%20Learning%20%28CoRAL%29%2C%20a%20unified%20framework%20enabling%20us%20to%0Achoose%20the%20most%20appropriate%20self-supervised%20loss%20for%20each%20sensor%20modality%20and%0Aallowing%20the%20representation%20to%20better%20focus%20on%20relevant%20aspects.%20We%20evaluate%0ACoRAL%27s%20benefits%20on%20a%20wide%20range%20of%20tasks%20with%20images%20containing%20distractions%0Aor%20occlusions%2C%20a%20new%20locomotion%20suite%2C%20and%20a%20challenging%20manipulation%20suite%0Awith%20visually%20realistic%20distractions.%20Our%20results%20show%20that%20learning%20a%0Amultimodal%20representation%20by%20combining%20contrastive%20and%20reconstruction-based%0Alosses%20can%20significantly%20improve%20performance%20and%20solve%20tasks%20that%20are%20out%20of%0Areach%20for%20more%20naive%20representation%20learning%20approaches%20and%20other%20recent%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.05342v4&entry.124074799=Read"},
{"title": "Editable Scene Simulation for Autonomous Driving via Collaborative\n  LLM-Agents", "author": "Yuxi Wei and Zi Wang and Yifan Lu and Chenxin Xu and Changxing Liu and Hao Zhao and Siheng Chen and Yanfeng Wang", "abstract": "  Scene simulation in autonomous driving has gained significant attention\nbecause of its huge potential for generating customized data. However, existing\neditable scene simulation approaches face limitations in terms of user\ninteraction efficiency, multi-camera photo-realistic rendering and external\ndigital assets integration. To address these challenges, this paper introduces\nChatSim, the first system that enables editable photo-realistic 3D driving\nscene simulations via natural language commands with external digital assets.\nTo enable editing with high command flexibility,~ChatSim leverages a large\nlanguage model (LLM) agent collaboration framework. To generate photo-realistic\noutcomes, ChatSim employs a novel multi-camera neural radiance field method.\nFurthermore, to unleash the potential of extensive high-quality digital assets,\nChatSim employs a novel multi-camera lighting estimation method to achieve\nscene-consistent assets' rendering. Our experiments on Waymo Open Dataset\ndemonstrate that ChatSim can handle complex language commands and generate\ncorresponding photo-realistic scene videos.\n", "link": "http://arxiv.org/abs/2402.05746v3", "date": "2024-06-26", "relevancy": 2.1928, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5779}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5423}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Editable%20Scene%20Simulation%20for%20Autonomous%20Driving%20via%20Collaborative%0A%20%20LLM-Agents&body=Title%3A%20Editable%20Scene%20Simulation%20for%20Autonomous%20Driving%20via%20Collaborative%0A%20%20LLM-Agents%0AAuthor%3A%20Yuxi%20Wei%20and%20Zi%20Wang%20and%20Yifan%20Lu%20and%20Chenxin%20Xu%20and%20Changxing%20Liu%20and%20Hao%20Zhao%20and%20Siheng%20Chen%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Scene%20simulation%20in%20autonomous%20driving%20has%20gained%20significant%20attention%0Abecause%20of%20its%20huge%20potential%20for%20generating%20customized%20data.%20However%2C%20existing%0Aeditable%20scene%20simulation%20approaches%20face%20limitations%20in%20terms%20of%20user%0Ainteraction%20efficiency%2C%20multi-camera%20photo-realistic%20rendering%20and%20external%0Adigital%20assets%20integration.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%0AChatSim%2C%20the%20first%20system%20that%20enables%20editable%20photo-realistic%203D%20driving%0Ascene%20simulations%20via%20natural%20language%20commands%20with%20external%20digital%20assets.%0ATo%20enable%20editing%20with%20high%20command%20flexibility%2C~ChatSim%20leverages%20a%20large%0Alanguage%20model%20%28LLM%29%20agent%20collaboration%20framework.%20To%20generate%20photo-realistic%0Aoutcomes%2C%20ChatSim%20employs%20a%20novel%20multi-camera%20neural%20radiance%20field%20method.%0AFurthermore%2C%20to%20unleash%20the%20potential%20of%20extensive%20high-quality%20digital%20assets%2C%0AChatSim%20employs%20a%20novel%20multi-camera%20lighting%20estimation%20method%20to%20achieve%0Ascene-consistent%20assets%27%20rendering.%20Our%20experiments%20on%20Waymo%20Open%20Dataset%0Ademonstrate%20that%20ChatSim%20can%20handle%20complex%20language%20commands%20and%20generate%0Acorresponding%20photo-realistic%20scene%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05746v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEditable%2520Scene%2520Simulation%2520for%2520Autonomous%2520Driving%2520via%2520Collaborative%250A%2520%2520LLM-Agents%26entry.906535625%3DYuxi%2520Wei%2520and%2520Zi%2520Wang%2520and%2520Yifan%2520Lu%2520and%2520Chenxin%2520Xu%2520and%2520Changxing%2520Liu%2520and%2520Hao%2520Zhao%2520and%2520Siheng%2520Chen%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Scene%2520simulation%2520in%2520autonomous%2520driving%2520has%2520gained%2520significant%2520attention%250Abecause%2520of%2520its%2520huge%2520potential%2520for%2520generating%2520customized%2520data.%2520However%252C%2520existing%250Aeditable%2520scene%2520simulation%2520approaches%2520face%2520limitations%2520in%2520terms%2520of%2520user%250Ainteraction%2520efficiency%252C%2520multi-camera%2520photo-realistic%2520rendering%2520and%2520external%250Adigital%2520assets%2520integration.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520introduces%250AChatSim%252C%2520the%2520first%2520system%2520that%2520enables%2520editable%2520photo-realistic%25203D%2520driving%250Ascene%2520simulations%2520via%2520natural%2520language%2520commands%2520with%2520external%2520digital%2520assets.%250ATo%2520enable%2520editing%2520with%2520high%2520command%2520flexibility%252C~ChatSim%2520leverages%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520agent%2520collaboration%2520framework.%2520To%2520generate%2520photo-realistic%250Aoutcomes%252C%2520ChatSim%2520employs%2520a%2520novel%2520multi-camera%2520neural%2520radiance%2520field%2520method.%250AFurthermore%252C%2520to%2520unleash%2520the%2520potential%2520of%2520extensive%2520high-quality%2520digital%2520assets%252C%250AChatSim%2520employs%2520a%2520novel%2520multi-camera%2520lighting%2520estimation%2520method%2520to%2520achieve%250Ascene-consistent%2520assets%2527%2520rendering.%2520Our%2520experiments%2520on%2520Waymo%2520Open%2520Dataset%250Ademonstrate%2520that%2520ChatSim%2520can%2520handle%2520complex%2520language%2520commands%2520and%2520generate%250Acorresponding%2520photo-realistic%2520scene%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05746v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Editable%20Scene%20Simulation%20for%20Autonomous%20Driving%20via%20Collaborative%0A%20%20LLM-Agents&entry.906535625=Yuxi%20Wei%20and%20Zi%20Wang%20and%20Yifan%20Lu%20and%20Chenxin%20Xu%20and%20Changxing%20Liu%20and%20Hao%20Zhao%20and%20Siheng%20Chen%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Scene%20simulation%20in%20autonomous%20driving%20has%20gained%20significant%20attention%0Abecause%20of%20its%20huge%20potential%20for%20generating%20customized%20data.%20However%2C%20existing%0Aeditable%20scene%20simulation%20approaches%20face%20limitations%20in%20terms%20of%20user%0Ainteraction%20efficiency%2C%20multi-camera%20photo-realistic%20rendering%20and%20external%0Adigital%20assets%20integration.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%0AChatSim%2C%20the%20first%20system%20that%20enables%20editable%20photo-realistic%203D%20driving%0Ascene%20simulations%20via%20natural%20language%20commands%20with%20external%20digital%20assets.%0ATo%20enable%20editing%20with%20high%20command%20flexibility%2C~ChatSim%20leverages%20a%20large%0Alanguage%20model%20%28LLM%29%20agent%20collaboration%20framework.%20To%20generate%20photo-realistic%0Aoutcomes%2C%20ChatSim%20employs%20a%20novel%20multi-camera%20neural%20radiance%20field%20method.%0AFurthermore%2C%20to%20unleash%20the%20potential%20of%20extensive%20high-quality%20digital%20assets%2C%0AChatSim%20employs%20a%20novel%20multi-camera%20lighting%20estimation%20method%20to%20achieve%0Ascene-consistent%20assets%27%20rendering.%20Our%20experiments%20on%20Waymo%20Open%20Dataset%0Ademonstrate%20that%20ChatSim%20can%20handle%20complex%20language%20commands%20and%20generate%0Acorresponding%20photo-realistic%20scene%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05746v3&entry.124074799=Read"},
{"title": "Introducing 3DCNN ResNets for ASD full-body kinematic assessment: a\n  comparison with hand-crafted features", "author": "Alberto Altozano and Maria Eleonora Minissi and Mariano Alca\u00f1iz and Javier Mar\u00edn-Morales", "abstract": "  Autism Spectrum Disorder (ASD) is characterized by challenges in social\ncommunication and restricted patterns, with motor abnormalities gaining\ntraction for early detection. However, kinematic analysis in ASD is limited,\noften lacking robust validation and relying on hand-crafted features for single\ntasks, leading to inconsistencies across studies. End-to-end models have\nemerged as promising methods to overcome the need for feature engineering. Our\naim is to propose a newly adapted 3DCNN ResNet from and compare it to widely\nused hand-crafted features for motor ASD assessment. Specifically, we developed\na virtual reality environment with multiple motor tasks and trained models\nusing both approaches. We prioritized a reliable validation framework with\nrepeated cross-validation. Results show the proposed model achieves a maximum\naccuracy of 85$\\pm$3%, outperforming state-of-the-art end-to-end models with\nshort 1-to-3 minute samples. Our comparative analysis with hand-crafted\nfeatures shows feature-engineered models outperformed our end-to-end model in\ncertain tasks. However, our end-to-end model achieved a higher mean AUC of\n0.80$\\pm$0.03. Additionally, statistical differences were found in model\nvariance, with our end-to-end model providing more consistent results with less\nvariability across all VR tasks, demonstrating domain generalization and\nreliability. These findings show that end-to-end models enable less variable\nand context-independent ASD classification without requiring domain knowledge\nor task specificity. However, they also recognize the effectiveness of\nhand-crafted features in specific task scenarios.\n", "link": "http://arxiv.org/abs/2311.14533v3", "date": "2024-06-26", "relevancy": 2.1889, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5659}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5417}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%203DCNN%20ResNets%20for%20ASD%20full-body%20kinematic%20assessment%3A%20a%0A%20%20comparison%20with%20hand-crafted%20features&body=Title%3A%20Introducing%203DCNN%20ResNets%20for%20ASD%20full-body%20kinematic%20assessment%3A%20a%0A%20%20comparison%20with%20hand-crafted%20features%0AAuthor%3A%20Alberto%20Altozano%20and%20Maria%20Eleonora%20Minissi%20and%20Mariano%20Alca%C3%B1iz%20and%20Javier%20Mar%C3%ADn-Morales%0AAbstract%3A%20%20%20Autism%20Spectrum%20Disorder%20%28ASD%29%20is%20characterized%20by%20challenges%20in%20social%0Acommunication%20and%20restricted%20patterns%2C%20with%20motor%20abnormalities%20gaining%0Atraction%20for%20early%20detection.%20However%2C%20kinematic%20analysis%20in%20ASD%20is%20limited%2C%0Aoften%20lacking%20robust%20validation%20and%20relying%20on%20hand-crafted%20features%20for%20single%0Atasks%2C%20leading%20to%20inconsistencies%20across%20studies.%20End-to-end%20models%20have%0Aemerged%20as%20promising%20methods%20to%20overcome%20the%20need%20for%20feature%20engineering.%20Our%0Aaim%20is%20to%20propose%20a%20newly%20adapted%203DCNN%20ResNet%20from%20and%20compare%20it%20to%20widely%0Aused%20hand-crafted%20features%20for%20motor%20ASD%20assessment.%20Specifically%2C%20we%20developed%0Aa%20virtual%20reality%20environment%20with%20multiple%20motor%20tasks%20and%20trained%20models%0Ausing%20both%20approaches.%20We%20prioritized%20a%20reliable%20validation%20framework%20with%0Arepeated%20cross-validation.%20Results%20show%20the%20proposed%20model%20achieves%20a%20maximum%0Aaccuracy%20of%2085%24%5Cpm%243%25%2C%20outperforming%20state-of-the-art%20end-to-end%20models%20with%0Ashort%201-to-3%20minute%20samples.%20Our%20comparative%20analysis%20with%20hand-crafted%0Afeatures%20shows%20feature-engineered%20models%20outperformed%20our%20end-to-end%20model%20in%0Acertain%20tasks.%20However%2C%20our%20end-to-end%20model%20achieved%20a%20higher%20mean%20AUC%20of%0A0.80%24%5Cpm%240.03.%20Additionally%2C%20statistical%20differences%20were%20found%20in%20model%0Avariance%2C%20with%20our%20end-to-end%20model%20providing%20more%20consistent%20results%20with%20less%0Avariability%20across%20all%20VR%20tasks%2C%20demonstrating%20domain%20generalization%20and%0Areliability.%20These%20findings%20show%20that%20end-to-end%20models%20enable%20less%20variable%0Aand%20context-independent%20ASD%20classification%20without%20requiring%20domain%20knowledge%0Aor%20task%20specificity.%20However%2C%20they%20also%20recognize%20the%20effectiveness%20of%0Ahand-crafted%20features%20in%20specific%20task%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14533v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%25203DCNN%2520ResNets%2520for%2520ASD%2520full-body%2520kinematic%2520assessment%253A%2520a%250A%2520%2520comparison%2520with%2520hand-crafted%2520features%26entry.906535625%3DAlberto%2520Altozano%2520and%2520Maria%2520Eleonora%2520Minissi%2520and%2520Mariano%2520Alca%25C3%25B1iz%2520and%2520Javier%2520Mar%25C3%25ADn-Morales%26entry.1292438233%3D%2520%2520Autism%2520Spectrum%2520Disorder%2520%2528ASD%2529%2520is%2520characterized%2520by%2520challenges%2520in%2520social%250Acommunication%2520and%2520restricted%2520patterns%252C%2520with%2520motor%2520abnormalities%2520gaining%250Atraction%2520for%2520early%2520detection.%2520However%252C%2520kinematic%2520analysis%2520in%2520ASD%2520is%2520limited%252C%250Aoften%2520lacking%2520robust%2520validation%2520and%2520relying%2520on%2520hand-crafted%2520features%2520for%2520single%250Atasks%252C%2520leading%2520to%2520inconsistencies%2520across%2520studies.%2520End-to-end%2520models%2520have%250Aemerged%2520as%2520promising%2520methods%2520to%2520overcome%2520the%2520need%2520for%2520feature%2520engineering.%2520Our%250Aaim%2520is%2520to%2520propose%2520a%2520newly%2520adapted%25203DCNN%2520ResNet%2520from%2520and%2520compare%2520it%2520to%2520widely%250Aused%2520hand-crafted%2520features%2520for%2520motor%2520ASD%2520assessment.%2520Specifically%252C%2520we%2520developed%250Aa%2520virtual%2520reality%2520environment%2520with%2520multiple%2520motor%2520tasks%2520and%2520trained%2520models%250Ausing%2520both%2520approaches.%2520We%2520prioritized%2520a%2520reliable%2520validation%2520framework%2520with%250Arepeated%2520cross-validation.%2520Results%2520show%2520the%2520proposed%2520model%2520achieves%2520a%2520maximum%250Aaccuracy%2520of%252085%2524%255Cpm%25243%2525%252C%2520outperforming%2520state-of-the-art%2520end-to-end%2520models%2520with%250Ashort%25201-to-3%2520minute%2520samples.%2520Our%2520comparative%2520analysis%2520with%2520hand-crafted%250Afeatures%2520shows%2520feature-engineered%2520models%2520outperformed%2520our%2520end-to-end%2520model%2520in%250Acertain%2520tasks.%2520However%252C%2520our%2520end-to-end%2520model%2520achieved%2520a%2520higher%2520mean%2520AUC%2520of%250A0.80%2524%255Cpm%25240.03.%2520Additionally%252C%2520statistical%2520differences%2520were%2520found%2520in%2520model%250Avariance%252C%2520with%2520our%2520end-to-end%2520model%2520providing%2520more%2520consistent%2520results%2520with%2520less%250Avariability%2520across%2520all%2520VR%2520tasks%252C%2520demonstrating%2520domain%2520generalization%2520and%250Areliability.%2520These%2520findings%2520show%2520that%2520end-to-end%2520models%2520enable%2520less%2520variable%250Aand%2520context-independent%2520ASD%2520classification%2520without%2520requiring%2520domain%2520knowledge%250Aor%2520task%2520specificity.%2520However%252C%2520they%2520also%2520recognize%2520the%2520effectiveness%2520of%250Ahand-crafted%2520features%2520in%2520specific%2520task%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14533v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%203DCNN%20ResNets%20for%20ASD%20full-body%20kinematic%20assessment%3A%20a%0A%20%20comparison%20with%20hand-crafted%20features&entry.906535625=Alberto%20Altozano%20and%20Maria%20Eleonora%20Minissi%20and%20Mariano%20Alca%C3%B1iz%20and%20Javier%20Mar%C3%ADn-Morales&entry.1292438233=%20%20Autism%20Spectrum%20Disorder%20%28ASD%29%20is%20characterized%20by%20challenges%20in%20social%0Acommunication%20and%20restricted%20patterns%2C%20with%20motor%20abnormalities%20gaining%0Atraction%20for%20early%20detection.%20However%2C%20kinematic%20analysis%20in%20ASD%20is%20limited%2C%0Aoften%20lacking%20robust%20validation%20and%20relying%20on%20hand-crafted%20features%20for%20single%0Atasks%2C%20leading%20to%20inconsistencies%20across%20studies.%20End-to-end%20models%20have%0Aemerged%20as%20promising%20methods%20to%20overcome%20the%20need%20for%20feature%20engineering.%20Our%0Aaim%20is%20to%20propose%20a%20newly%20adapted%203DCNN%20ResNet%20from%20and%20compare%20it%20to%20widely%0Aused%20hand-crafted%20features%20for%20motor%20ASD%20assessment.%20Specifically%2C%20we%20developed%0Aa%20virtual%20reality%20environment%20with%20multiple%20motor%20tasks%20and%20trained%20models%0Ausing%20both%20approaches.%20We%20prioritized%20a%20reliable%20validation%20framework%20with%0Arepeated%20cross-validation.%20Results%20show%20the%20proposed%20model%20achieves%20a%20maximum%0Aaccuracy%20of%2085%24%5Cpm%243%25%2C%20outperforming%20state-of-the-art%20end-to-end%20models%20with%0Ashort%201-to-3%20minute%20samples.%20Our%20comparative%20analysis%20with%20hand-crafted%0Afeatures%20shows%20feature-engineered%20models%20outperformed%20our%20end-to-end%20model%20in%0Acertain%20tasks.%20However%2C%20our%20end-to-end%20model%20achieved%20a%20higher%20mean%20AUC%20of%0A0.80%24%5Cpm%240.03.%20Additionally%2C%20statistical%20differences%20were%20found%20in%20model%0Avariance%2C%20with%20our%20end-to-end%20model%20providing%20more%20consistent%20results%20with%20less%0Avariability%20across%20all%20VR%20tasks%2C%20demonstrating%20domain%20generalization%20and%0Areliability.%20These%20findings%20show%20that%20end-to-end%20models%20enable%20less%20variable%0Aand%20context-independent%20ASD%20classification%20without%20requiring%20domain%20knowledge%0Aor%20task%20specificity.%20However%2C%20they%20also%20recognize%20the%20effectiveness%20of%0Ahand-crafted%20features%20in%20specific%20task%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14533v3&entry.124074799=Read"},
{"title": "ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of\n  Text-to-Time-lapse Video Generation", "author": "Shenghai Yuan and Jinfa Huang and Yongqi Xu and Yaoyang Liu and Shaofeng Zhang and Yujun Shi and Ruijie Zhu and Xinhua Cheng and Jiebo Luo and Li Yuan", "abstract": "  We propose a novel text-to-video (T2V) generation benchmark,\nChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the\nT2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast\nto existing benchmarks that focus on the visual quality and textual relevance\nof generated videos, ChronoMagic-Bench focuses on the model's ability to\ngenerate time-lapse videos with significant metamorphic amplitude and temporal\ncoherence. The benchmark probes T2V models for their physics, biology, and\nchemistry capabilities, in a free-form text query. For these purposes,\nChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,\ncategorized into four major types of time-lapse videos: biological,\nhuman-created, meteorological, and physical phenomena, which are further\ndivided into 75 subcategories. This categorization comprehensively evaluates\nthe model's capacity to handle diverse and complex transformations. To\naccurately align human preference with the benchmark, we introduce two new\nautomatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic\nattributes and temporal coherence. MTScore measures the metamorphic amplitude,\nreflecting the degree of change over time, while CHScore assesses the temporal\ncoherence, ensuring the generated videos maintain logical progression and\ncontinuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual\nevaluations of ten representative T2V models, revealing their strengths and\nweaknesses across different categories of prompts, and providing a thorough\nevaluation framework that addresses current gaps in video generation research.\nMoreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k\nhigh-quality pairs of 720p time-lapse videos and detailed captions ensuring\nhigh physical pertinence and large metamorphic amplitude.\n", "link": "http://arxiv.org/abs/2406.18522v1", "date": "2024-06-26", "relevancy": 2.1653, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5669}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5546}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChronoMagic-Bench%3A%20A%20Benchmark%20for%20Metamorphic%20Evaluation%20of%0A%20%20Text-to-Time-lapse%20Video%20Generation&body=Title%3A%20ChronoMagic-Bench%3A%20A%20Benchmark%20for%20Metamorphic%20Evaluation%20of%0A%20%20Text-to-Time-lapse%20Video%20Generation%0AAuthor%3A%20Shenghai%20Yuan%20and%20Jinfa%20Huang%20and%20Yongqi%20Xu%20and%20Yaoyang%20Liu%20and%20Shaofeng%20Zhang%20and%20Yujun%20Shi%20and%20Ruijie%20Zhu%20and%20Xinhua%20Cheng%20and%20Jiebo%20Luo%20and%20Li%20Yuan%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20text-to-video%20%28T2V%29%20generation%20benchmark%2C%0AChronoMagic-Bench%2C%20to%20evaluate%20the%20temporal%20and%20metamorphic%20capabilities%20of%20the%0AT2V%20models%20%28e.g.%20Sora%20and%20Lumiere%29%20in%20time-lapse%20video%20generation.%20In%20contrast%0Ato%20existing%20benchmarks%20that%20focus%20on%20the%20visual%20quality%20and%20textual%20relevance%0Aof%20generated%20videos%2C%20ChronoMagic-Bench%20focuses%20on%20the%20model%27s%20ability%20to%0Agenerate%20time-lapse%20videos%20with%20significant%20metamorphic%20amplitude%20and%20temporal%0Acoherence.%20The%20benchmark%20probes%20T2V%20models%20for%20their%20physics%2C%20biology%2C%20and%0Achemistry%20capabilities%2C%20in%20a%20free-form%20text%20query.%20For%20these%20purposes%2C%0AChronoMagic-Bench%20introduces%201%2C649%20prompts%20and%20real-world%20videos%20as%20references%2C%0Acategorized%20into%20four%20major%20types%20of%20time-lapse%20videos%3A%20biological%2C%0Ahuman-created%2C%20meteorological%2C%20and%20physical%20phenomena%2C%20which%20are%20further%0Adivided%20into%2075%20subcategories.%20This%20categorization%20comprehensively%20evaluates%0Athe%20model%27s%20capacity%20to%20handle%20diverse%20and%20complex%20transformations.%20To%0Aaccurately%20align%20human%20preference%20with%20the%20benchmark%2C%20we%20introduce%20two%20new%0Aautomatic%20metrics%2C%20MTScore%20and%20CHScore%2C%20to%20evaluate%20the%20videos%27%20metamorphic%0Aattributes%20and%20temporal%20coherence.%20MTScore%20measures%20the%20metamorphic%20amplitude%2C%0Areflecting%20the%20degree%20of%20change%20over%20time%2C%20while%20CHScore%20assesses%20the%20temporal%0Acoherence%2C%20ensuring%20the%20generated%20videos%20maintain%20logical%20progression%20and%0Acontinuity.%20Based%20on%20the%20ChronoMagic-Bench%2C%20we%20conduct%20comprehensive%20manual%0Aevaluations%20of%20ten%20representative%20T2V%20models%2C%20revealing%20their%20strengths%20and%0Aweaknesses%20across%20different%20categories%20of%20prompts%2C%20and%20providing%20a%20thorough%0Aevaluation%20framework%20that%20addresses%20current%20gaps%20in%20video%20generation%20research.%0AMoreover%2C%20we%20create%20a%20large-scale%20ChronoMagic-Pro%20dataset%2C%20containing%20460k%0Ahigh-quality%20pairs%20of%20720p%20time-lapse%20videos%20and%20detailed%20captions%20ensuring%0Ahigh%20physical%20pertinence%20and%20large%20metamorphic%20amplitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChronoMagic-Bench%253A%2520A%2520Benchmark%2520for%2520Metamorphic%2520Evaluation%2520of%250A%2520%2520Text-to-Time-lapse%2520Video%2520Generation%26entry.906535625%3DShenghai%2520Yuan%2520and%2520Jinfa%2520Huang%2520and%2520Yongqi%2520Xu%2520and%2520Yaoyang%2520Liu%2520and%2520Shaofeng%2520Zhang%2520and%2520Yujun%2520Shi%2520and%2520Ruijie%2520Zhu%2520and%2520Xinhua%2520Cheng%2520and%2520Jiebo%2520Luo%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520text-to-video%2520%2528T2V%2529%2520generation%2520benchmark%252C%250AChronoMagic-Bench%252C%2520to%2520evaluate%2520the%2520temporal%2520and%2520metamorphic%2520capabilities%2520of%2520the%250AT2V%2520models%2520%2528e.g.%2520Sora%2520and%2520Lumiere%2529%2520in%2520time-lapse%2520video%2520generation.%2520In%2520contrast%250Ato%2520existing%2520benchmarks%2520that%2520focus%2520on%2520the%2520visual%2520quality%2520and%2520textual%2520relevance%250Aof%2520generated%2520videos%252C%2520ChronoMagic-Bench%2520focuses%2520on%2520the%2520model%2527s%2520ability%2520to%250Agenerate%2520time-lapse%2520videos%2520with%2520significant%2520metamorphic%2520amplitude%2520and%2520temporal%250Acoherence.%2520The%2520benchmark%2520probes%2520T2V%2520models%2520for%2520their%2520physics%252C%2520biology%252C%2520and%250Achemistry%2520capabilities%252C%2520in%2520a%2520free-form%2520text%2520query.%2520For%2520these%2520purposes%252C%250AChronoMagic-Bench%2520introduces%25201%252C649%2520prompts%2520and%2520real-world%2520videos%2520as%2520references%252C%250Acategorized%2520into%2520four%2520major%2520types%2520of%2520time-lapse%2520videos%253A%2520biological%252C%250Ahuman-created%252C%2520meteorological%252C%2520and%2520physical%2520phenomena%252C%2520which%2520are%2520further%250Adivided%2520into%252075%2520subcategories.%2520This%2520categorization%2520comprehensively%2520evaluates%250Athe%2520model%2527s%2520capacity%2520to%2520handle%2520diverse%2520and%2520complex%2520transformations.%2520To%250Aaccurately%2520align%2520human%2520preference%2520with%2520the%2520benchmark%252C%2520we%2520introduce%2520two%2520new%250Aautomatic%2520metrics%252C%2520MTScore%2520and%2520CHScore%252C%2520to%2520evaluate%2520the%2520videos%2527%2520metamorphic%250Aattributes%2520and%2520temporal%2520coherence.%2520MTScore%2520measures%2520the%2520metamorphic%2520amplitude%252C%250Areflecting%2520the%2520degree%2520of%2520change%2520over%2520time%252C%2520while%2520CHScore%2520assesses%2520the%2520temporal%250Acoherence%252C%2520ensuring%2520the%2520generated%2520videos%2520maintain%2520logical%2520progression%2520and%250Acontinuity.%2520Based%2520on%2520the%2520ChronoMagic-Bench%252C%2520we%2520conduct%2520comprehensive%2520manual%250Aevaluations%2520of%2520ten%2520representative%2520T2V%2520models%252C%2520revealing%2520their%2520strengths%2520and%250Aweaknesses%2520across%2520different%2520categories%2520of%2520prompts%252C%2520and%2520providing%2520a%2520thorough%250Aevaluation%2520framework%2520that%2520addresses%2520current%2520gaps%2520in%2520video%2520generation%2520research.%250AMoreover%252C%2520we%2520create%2520a%2520large-scale%2520ChronoMagic-Pro%2520dataset%252C%2520containing%2520460k%250Ahigh-quality%2520pairs%2520of%2520720p%2520time-lapse%2520videos%2520and%2520detailed%2520captions%2520ensuring%250Ahigh%2520physical%2520pertinence%2520and%2520large%2520metamorphic%2520amplitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChronoMagic-Bench%3A%20A%20Benchmark%20for%20Metamorphic%20Evaluation%20of%0A%20%20Text-to-Time-lapse%20Video%20Generation&entry.906535625=Shenghai%20Yuan%20and%20Jinfa%20Huang%20and%20Yongqi%20Xu%20and%20Yaoyang%20Liu%20and%20Shaofeng%20Zhang%20and%20Yujun%20Shi%20and%20Ruijie%20Zhu%20and%20Xinhua%20Cheng%20and%20Jiebo%20Luo%20and%20Li%20Yuan&entry.1292438233=%20%20We%20propose%20a%20novel%20text-to-video%20%28T2V%29%20generation%20benchmark%2C%0AChronoMagic-Bench%2C%20to%20evaluate%20the%20temporal%20and%20metamorphic%20capabilities%20of%20the%0AT2V%20models%20%28e.g.%20Sora%20and%20Lumiere%29%20in%20time-lapse%20video%20generation.%20In%20contrast%0Ato%20existing%20benchmarks%20that%20focus%20on%20the%20visual%20quality%20and%20textual%20relevance%0Aof%20generated%20videos%2C%20ChronoMagic-Bench%20focuses%20on%20the%20model%27s%20ability%20to%0Agenerate%20time-lapse%20videos%20with%20significant%20metamorphic%20amplitude%20and%20temporal%0Acoherence.%20The%20benchmark%20probes%20T2V%20models%20for%20their%20physics%2C%20biology%2C%20and%0Achemistry%20capabilities%2C%20in%20a%20free-form%20text%20query.%20For%20these%20purposes%2C%0AChronoMagic-Bench%20introduces%201%2C649%20prompts%20and%20real-world%20videos%20as%20references%2C%0Acategorized%20into%20four%20major%20types%20of%20time-lapse%20videos%3A%20biological%2C%0Ahuman-created%2C%20meteorological%2C%20and%20physical%20phenomena%2C%20which%20are%20further%0Adivided%20into%2075%20subcategories.%20This%20categorization%20comprehensively%20evaluates%0Athe%20model%27s%20capacity%20to%20handle%20diverse%20and%20complex%20transformations.%20To%0Aaccurately%20align%20human%20preference%20with%20the%20benchmark%2C%20we%20introduce%20two%20new%0Aautomatic%20metrics%2C%20MTScore%20and%20CHScore%2C%20to%20evaluate%20the%20videos%27%20metamorphic%0Aattributes%20and%20temporal%20coherence.%20MTScore%20measures%20the%20metamorphic%20amplitude%2C%0Areflecting%20the%20degree%20of%20change%20over%20time%2C%20while%20CHScore%20assesses%20the%20temporal%0Acoherence%2C%20ensuring%20the%20generated%20videos%20maintain%20logical%20progression%20and%0Acontinuity.%20Based%20on%20the%20ChronoMagic-Bench%2C%20we%20conduct%20comprehensive%20manual%0Aevaluations%20of%20ten%20representative%20T2V%20models%2C%20revealing%20their%20strengths%20and%0Aweaknesses%20across%20different%20categories%20of%20prompts%2C%20and%20providing%20a%20thorough%0Aevaluation%20framework%20that%20addresses%20current%20gaps%20in%20video%20generation%20research.%0AMoreover%2C%20we%20create%20a%20large-scale%20ChronoMagic-Pro%20dataset%2C%20containing%20460k%0Ahigh-quality%20pairs%20of%20720p%20time-lapse%20videos%20and%20detailed%20captions%20ensuring%0Ahigh%20physical%20pertinence%20and%20large%20metamorphic%20amplitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18522v1&entry.124074799=Read"},
{"title": "Generative artificial intelligence in ophthalmology: multimodal retinal\n  images for the diagnosis of Alzheimer's disease with convolutional neural\n  networks", "author": "I. R. Slootweg and M. Thach and K. R. Curro-Tafili and F. D. Verbraak and F. H. Bouwman and Y. A. L. Pijnenburg and J. F. Boer and J. H. P. de Kwisthout and L. Bagheriye and P. J. Gonz\u00e1lez", "abstract": "  Background/Aim. This study aims to predict Amyloid Positron Emission\nTomography (AmyloidPET) status with multimodal retinal imaging and\nconvolutional neural networks (CNNs) and to improve the performance through\npretraining with synthetic data. Methods. Fundus autofluorescence, optical\ncoherence tomography (OCT), and OCT angiography images from 328 eyes of 59\nAmyloidPET positive subjects and 108 AmyloidPET negative subjects were used for\nclassification. Denoising Diffusion Probabilistic Models (DDPMs) were trained\nto generate synthetic images and unimodal CNNs were pretrained on synthetic\ndata and finetuned on real data or trained solely on real data. Multimodal\nclassifiers were developed to combine predictions of the four unimodal CNNs\nwith patient metadata. Class activation maps of the unimodal classifiers\nprovided insight into the network's attention to inputs. Results. DDPMs\ngenerated diverse, realistic images without memorization. Pretraining unimodal\nCNNs with synthetic data improved AUPR at most from 0.350 to 0.579. Integration\nof metadata in multimodal CNNs improved AUPR from 0.486 to 0.634, which was the\nbest overall best classifier. Class activation maps highlighted relevant\nretinal regions which correlated with AD. Conclusion. Our method for generating\nand leveraging synthetic data has the potential to improve AmyloidPET\nprediction from multimodal retinal imaging. A DDPM can generate realistic and\nunique multimodal synthetic retinal images. Our best performing unimodal and\nmultimodal classifiers were not pretrained on synthetic data, however\npretraining with synthetic data slightly improved classification performance\nfor two out of the four modalities.\n", "link": "http://arxiv.org/abs/2406.18247v1", "date": "2024-06-26", "relevancy": 2.1592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5329}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20artificial%20intelligence%20in%20ophthalmology%3A%20multimodal%20retinal%0A%20%20images%20for%20the%20diagnosis%20of%20Alzheimer%27s%20disease%20with%20convolutional%20neural%0A%20%20networks&body=Title%3A%20Generative%20artificial%20intelligence%20in%20ophthalmology%3A%20multimodal%20retinal%0A%20%20images%20for%20the%20diagnosis%20of%20Alzheimer%27s%20disease%20with%20convolutional%20neural%0A%20%20networks%0AAuthor%3A%20I.%20R.%20Slootweg%20and%20M.%20Thach%20and%20K.%20R.%20Curro-Tafili%20and%20F.%20D.%20Verbraak%20and%20F.%20H.%20Bouwman%20and%20Y.%20A.%20L.%20Pijnenburg%20and%20J.%20F.%20Boer%20and%20J.%20H.%20P.%20de%20Kwisthout%20and%20L.%20Bagheriye%20and%20P.%20J.%20Gonz%C3%A1lez%0AAbstract%3A%20%20%20Background/Aim.%20This%20study%20aims%20to%20predict%20Amyloid%20Positron%20Emission%0ATomography%20%28AmyloidPET%29%20status%20with%20multimodal%20retinal%20imaging%20and%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20to%20improve%20the%20performance%20through%0Apretraining%20with%20synthetic%20data.%20Methods.%20Fundus%20autofluorescence%2C%20optical%0Acoherence%20tomography%20%28OCT%29%2C%20and%20OCT%20angiography%20images%20from%20328%20eyes%20of%2059%0AAmyloidPET%20positive%20subjects%20and%20108%20AmyloidPET%20negative%20subjects%20were%20used%20for%0Aclassification.%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20were%20trained%0Ato%20generate%20synthetic%20images%20and%20unimodal%20CNNs%20were%20pretrained%20on%20synthetic%0Adata%20and%20finetuned%20on%20real%20data%20or%20trained%20solely%20on%20real%20data.%20Multimodal%0Aclassifiers%20were%20developed%20to%20combine%20predictions%20of%20the%20four%20unimodal%20CNNs%0Awith%20patient%20metadata.%20Class%20activation%20maps%20of%20the%20unimodal%20classifiers%0Aprovided%20insight%20into%20the%20network%27s%20attention%20to%20inputs.%20Results.%20DDPMs%0Agenerated%20diverse%2C%20realistic%20images%20without%20memorization.%20Pretraining%20unimodal%0ACNNs%20with%20synthetic%20data%20improved%20AUPR%20at%20most%20from%200.350%20to%200.579.%20Integration%0Aof%20metadata%20in%20multimodal%20CNNs%20improved%20AUPR%20from%200.486%20to%200.634%2C%20which%20was%20the%0Abest%20overall%20best%20classifier.%20Class%20activation%20maps%20highlighted%20relevant%0Aretinal%20regions%20which%20correlated%20with%20AD.%20Conclusion.%20Our%20method%20for%20generating%0Aand%20leveraging%20synthetic%20data%20has%20the%20potential%20to%20improve%20AmyloidPET%0Aprediction%20from%20multimodal%20retinal%20imaging.%20A%20DDPM%20can%20generate%20realistic%20and%0Aunique%20multimodal%20synthetic%20retinal%20images.%20Our%20best%20performing%20unimodal%20and%0Amultimodal%20classifiers%20were%20not%20pretrained%20on%20synthetic%20data%2C%20however%0Apretraining%20with%20synthetic%20data%20slightly%20improved%20classification%20performance%0Afor%20two%20out%20of%20the%20four%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520artificial%2520intelligence%2520in%2520ophthalmology%253A%2520multimodal%2520retinal%250A%2520%2520images%2520for%2520the%2520diagnosis%2520of%2520Alzheimer%2527s%2520disease%2520with%2520convolutional%2520neural%250A%2520%2520networks%26entry.906535625%3DI.%2520R.%2520Slootweg%2520and%2520M.%2520Thach%2520and%2520K.%2520R.%2520Curro-Tafili%2520and%2520F.%2520D.%2520Verbraak%2520and%2520F.%2520H.%2520Bouwman%2520and%2520Y.%2520A.%2520L.%2520Pijnenburg%2520and%2520J.%2520F.%2520Boer%2520and%2520J.%2520H.%2520P.%2520de%2520Kwisthout%2520and%2520L.%2520Bagheriye%2520and%2520P.%2520J.%2520Gonz%25C3%25A1lez%26entry.1292438233%3D%2520%2520Background/Aim.%2520This%2520study%2520aims%2520to%2520predict%2520Amyloid%2520Positron%2520Emission%250ATomography%2520%2528AmyloidPET%2529%2520status%2520with%2520multimodal%2520retinal%2520imaging%2520and%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%2520to%2520improve%2520the%2520performance%2520through%250Apretraining%2520with%2520synthetic%2520data.%2520Methods.%2520Fundus%2520autofluorescence%252C%2520optical%250Acoherence%2520tomography%2520%2528OCT%2529%252C%2520and%2520OCT%2520angiography%2520images%2520from%2520328%2520eyes%2520of%252059%250AAmyloidPET%2520positive%2520subjects%2520and%2520108%2520AmyloidPET%2520negative%2520subjects%2520were%2520used%2520for%250Aclassification.%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%2520were%2520trained%250Ato%2520generate%2520synthetic%2520images%2520and%2520unimodal%2520CNNs%2520were%2520pretrained%2520on%2520synthetic%250Adata%2520and%2520finetuned%2520on%2520real%2520data%2520or%2520trained%2520solely%2520on%2520real%2520data.%2520Multimodal%250Aclassifiers%2520were%2520developed%2520to%2520combine%2520predictions%2520of%2520the%2520four%2520unimodal%2520CNNs%250Awith%2520patient%2520metadata.%2520Class%2520activation%2520maps%2520of%2520the%2520unimodal%2520classifiers%250Aprovided%2520insight%2520into%2520the%2520network%2527s%2520attention%2520to%2520inputs.%2520Results.%2520DDPMs%250Agenerated%2520diverse%252C%2520realistic%2520images%2520without%2520memorization.%2520Pretraining%2520unimodal%250ACNNs%2520with%2520synthetic%2520data%2520improved%2520AUPR%2520at%2520most%2520from%25200.350%2520to%25200.579.%2520Integration%250Aof%2520metadata%2520in%2520multimodal%2520CNNs%2520improved%2520AUPR%2520from%25200.486%2520to%25200.634%252C%2520which%2520was%2520the%250Abest%2520overall%2520best%2520classifier.%2520Class%2520activation%2520maps%2520highlighted%2520relevant%250Aretinal%2520regions%2520which%2520correlated%2520with%2520AD.%2520Conclusion.%2520Our%2520method%2520for%2520generating%250Aand%2520leveraging%2520synthetic%2520data%2520has%2520the%2520potential%2520to%2520improve%2520AmyloidPET%250Aprediction%2520from%2520multimodal%2520retinal%2520imaging.%2520A%2520DDPM%2520can%2520generate%2520realistic%2520and%250Aunique%2520multimodal%2520synthetic%2520retinal%2520images.%2520Our%2520best%2520performing%2520unimodal%2520and%250Amultimodal%2520classifiers%2520were%2520not%2520pretrained%2520on%2520synthetic%2520data%252C%2520however%250Apretraining%2520with%2520synthetic%2520data%2520slightly%2520improved%2520classification%2520performance%250Afor%2520two%2520out%2520of%2520the%2520four%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20artificial%20intelligence%20in%20ophthalmology%3A%20multimodal%20retinal%0A%20%20images%20for%20the%20diagnosis%20of%20Alzheimer%27s%20disease%20with%20convolutional%20neural%0A%20%20networks&entry.906535625=I.%20R.%20Slootweg%20and%20M.%20Thach%20and%20K.%20R.%20Curro-Tafili%20and%20F.%20D.%20Verbraak%20and%20F.%20H.%20Bouwman%20and%20Y.%20A.%20L.%20Pijnenburg%20and%20J.%20F.%20Boer%20and%20J.%20H.%20P.%20de%20Kwisthout%20and%20L.%20Bagheriye%20and%20P.%20J.%20Gonz%C3%A1lez&entry.1292438233=%20%20Background/Aim.%20This%20study%20aims%20to%20predict%20Amyloid%20Positron%20Emission%0ATomography%20%28AmyloidPET%29%20status%20with%20multimodal%20retinal%20imaging%20and%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20to%20improve%20the%20performance%20through%0Apretraining%20with%20synthetic%20data.%20Methods.%20Fundus%20autofluorescence%2C%20optical%0Acoherence%20tomography%20%28OCT%29%2C%20and%20OCT%20angiography%20images%20from%20328%20eyes%20of%2059%0AAmyloidPET%20positive%20subjects%20and%20108%20AmyloidPET%20negative%20subjects%20were%20used%20for%0Aclassification.%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20were%20trained%0Ato%20generate%20synthetic%20images%20and%20unimodal%20CNNs%20were%20pretrained%20on%20synthetic%0Adata%20and%20finetuned%20on%20real%20data%20or%20trained%20solely%20on%20real%20data.%20Multimodal%0Aclassifiers%20were%20developed%20to%20combine%20predictions%20of%20the%20four%20unimodal%20CNNs%0Awith%20patient%20metadata.%20Class%20activation%20maps%20of%20the%20unimodal%20classifiers%0Aprovided%20insight%20into%20the%20network%27s%20attention%20to%20inputs.%20Results.%20DDPMs%0Agenerated%20diverse%2C%20realistic%20images%20without%20memorization.%20Pretraining%20unimodal%0ACNNs%20with%20synthetic%20data%20improved%20AUPR%20at%20most%20from%200.350%20to%200.579.%20Integration%0Aof%20metadata%20in%20multimodal%20CNNs%20improved%20AUPR%20from%200.486%20to%200.634%2C%20which%20was%20the%0Abest%20overall%20best%20classifier.%20Class%20activation%20maps%20highlighted%20relevant%0Aretinal%20regions%20which%20correlated%20with%20AD.%20Conclusion.%20Our%20method%20for%20generating%0Aand%20leveraging%20synthetic%20data%20has%20the%20potential%20to%20improve%20AmyloidPET%0Aprediction%20from%20multimodal%20retinal%20imaging.%20A%20DDPM%20can%20generate%20realistic%20and%0Aunique%20multimodal%20synthetic%20retinal%20images.%20Our%20best%20performing%20unimodal%20and%0Amultimodal%20classifiers%20were%20not%20pretrained%20on%20synthetic%20data%2C%20however%0Apretraining%20with%20synthetic%20data%20slightly%20improved%20classification%20performance%0Afor%20two%20out%20of%20the%20four%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18247v1&entry.124074799=Read"},
{"title": "Symbolic Learning Enables Self-Evolving Agents", "author": "Wangchunshu Zhou and Yixin Ou and Shengwei Ding and Long Li and Jialong Wu and Tiannan Wang and Jiamin Chen and Shuai Wang and Xiaohua Xu and Ningyu Zhang and Huajun Chen and Yuchen Eleanor Jiang", "abstract": "  The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".\n", "link": "http://arxiv.org/abs/2406.18532v1", "date": "2024-06-26", "relevancy": 2.1441, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5416}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5379}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symbolic%20Learning%20Enables%20Self-Evolving%20Agents&body=Title%3A%20Symbolic%20Learning%20Enables%20Self-Evolving%20Agents%0AAuthor%3A%20Wangchunshu%20Zhou%20and%20Yixin%20Ou%20and%20Shengwei%20Ding%20and%20Long%20Li%20and%20Jialong%20Wu%20and%20Tiannan%20Wang%20and%20Jiamin%20Chen%20and%20Shuai%20Wang%20and%20Xiaohua%20Xu%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen%20and%20Yuchen%20Eleanor%20Jiang%0AAbstract%3A%20%20%20The%20AI%20community%20has%20been%20exploring%20a%20pathway%20to%20artificial%20general%0Aintelligence%20%28AGI%29%20by%20developing%20%22language%20agents%22%2C%20which%20are%20complex%20large%0Alanguage%20models%20%28LLMs%29%20pipelines%20involving%20both%20prompting%20techniques%20and%20tool%0Ausage%20methods.%20While%20language%20agents%20have%20demonstrated%20impressive%20capabilities%0Afor%20many%20real-world%20tasks%2C%20a%20fundamental%20limitation%20of%20current%20language%20agents%0Aresearch%20is%20that%20they%20are%20model-centric%2C%20or%20engineering-centric.%20That%27s%20to%20say%2C%0Athe%20progress%20on%20prompts%2C%20tools%2C%20and%20pipelines%20of%20language%20agents%20requires%0Asubstantial%20manual%20engineering%20efforts%20from%20human%20experts%20rather%20than%0Aautomatically%20learning%20from%20data.%20We%20believe%20the%20transition%20from%20model-centric%2C%0Aor%20engineering-centric%2C%20to%20data-centric%2C%20i.e.%2C%20the%20ability%20of%20language%20agents%0Ato%20autonomously%20learn%20and%20evolve%20in%20environments%2C%20is%20the%20key%20for%20them%20to%0Apossibly%20achieve%20AGI.%0A%20%20In%20this%20work%2C%20we%20introduce%20agent%20symbolic%20learning%2C%20a%20systematic%20framework%0Athat%20enables%20language%20agents%20to%20optimize%20themselves%20on%20their%20own%20in%20a%0Adata-centric%20way%20using%20symbolic%20optimizers.%20Specifically%2C%20we%20consider%20agents%20as%0Asymbolic%20networks%20where%20learnable%20weights%20are%20defined%20by%20prompts%2C%20tools%2C%20and%0Athe%20way%20they%20are%20stacked%20together.%20Agent%20symbolic%20learning%20is%20designed%20to%0Aoptimize%20the%20symbolic%20network%20within%20language%20agents%20by%20mimicking%20two%0Afundamental%20algorithms%20in%20connectionist%20learning%3A%20back-propagation%20and%20gradient%0Adescent.%20Instead%20of%20dealing%20with%20numeric%20weights%2C%20agent%20symbolic%20learning%20works%0Awith%20natural%20language%20simulacrums%20of%20weights%2C%20loss%2C%20and%20gradients.%20We%20conduct%0Aproof-of-concept%20experiments%20on%20both%20standard%20benchmarks%20and%20complex%20real-world%0Atasks%20and%20show%20that%20agent%20symbolic%20learning%20enables%20language%20agents%20to%20update%0Athemselves%20after%20being%20created%20and%20deployed%20in%20the%20wild%2C%20resulting%20in%0A%22self-evolving%20agents%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymbolic%2520Learning%2520Enables%2520Self-Evolving%2520Agents%26entry.906535625%3DWangchunshu%2520Zhou%2520and%2520Yixin%2520Ou%2520and%2520Shengwei%2520Ding%2520and%2520Long%2520Li%2520and%2520Jialong%2520Wu%2520and%2520Tiannan%2520Wang%2520and%2520Jiamin%2520Chen%2520and%2520Shuai%2520Wang%2520and%2520Xiaohua%2520Xu%2520and%2520Ningyu%2520Zhang%2520and%2520Huajun%2520Chen%2520and%2520Yuchen%2520Eleanor%2520Jiang%26entry.1292438233%3D%2520%2520The%2520AI%2520community%2520has%2520been%2520exploring%2520a%2520pathway%2520to%2520artificial%2520general%250Aintelligence%2520%2528AGI%2529%2520by%2520developing%2520%2522language%2520agents%2522%252C%2520which%2520are%2520complex%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520pipelines%2520involving%2520both%2520prompting%2520techniques%2520and%2520tool%250Ausage%2520methods.%2520While%2520language%2520agents%2520have%2520demonstrated%2520impressive%2520capabilities%250Afor%2520many%2520real-world%2520tasks%252C%2520a%2520fundamental%2520limitation%2520of%2520current%2520language%2520agents%250Aresearch%2520is%2520that%2520they%2520are%2520model-centric%252C%2520or%2520engineering-centric.%2520That%2527s%2520to%2520say%252C%250Athe%2520progress%2520on%2520prompts%252C%2520tools%252C%2520and%2520pipelines%2520of%2520language%2520agents%2520requires%250Asubstantial%2520manual%2520engineering%2520efforts%2520from%2520human%2520experts%2520rather%2520than%250Aautomatically%2520learning%2520from%2520data.%2520We%2520believe%2520the%2520transition%2520from%2520model-centric%252C%250Aor%2520engineering-centric%252C%2520to%2520data-centric%252C%2520i.e.%252C%2520the%2520ability%2520of%2520language%2520agents%250Ato%2520autonomously%2520learn%2520and%2520evolve%2520in%2520environments%252C%2520is%2520the%2520key%2520for%2520them%2520to%250Apossibly%2520achieve%2520AGI.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520agent%2520symbolic%2520learning%252C%2520a%2520systematic%2520framework%250Athat%2520enables%2520language%2520agents%2520to%2520optimize%2520themselves%2520on%2520their%2520own%2520in%2520a%250Adata-centric%2520way%2520using%2520symbolic%2520optimizers.%2520Specifically%252C%2520we%2520consider%2520agents%2520as%250Asymbolic%2520networks%2520where%2520learnable%2520weights%2520are%2520defined%2520by%2520prompts%252C%2520tools%252C%2520and%250Athe%2520way%2520they%2520are%2520stacked%2520together.%2520Agent%2520symbolic%2520learning%2520is%2520designed%2520to%250Aoptimize%2520the%2520symbolic%2520network%2520within%2520language%2520agents%2520by%2520mimicking%2520two%250Afundamental%2520algorithms%2520in%2520connectionist%2520learning%253A%2520back-propagation%2520and%2520gradient%250Adescent.%2520Instead%2520of%2520dealing%2520with%2520numeric%2520weights%252C%2520agent%2520symbolic%2520learning%2520works%250Awith%2520natural%2520language%2520simulacrums%2520of%2520weights%252C%2520loss%252C%2520and%2520gradients.%2520We%2520conduct%250Aproof-of-concept%2520experiments%2520on%2520both%2520standard%2520benchmarks%2520and%2520complex%2520real-world%250Atasks%2520and%2520show%2520that%2520agent%2520symbolic%2520learning%2520enables%2520language%2520agents%2520to%2520update%250Athemselves%2520after%2520being%2520created%2520and%2520deployed%2520in%2520the%2520wild%252C%2520resulting%2520in%250A%2522self-evolving%2520agents%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolic%20Learning%20Enables%20Self-Evolving%20Agents&entry.906535625=Wangchunshu%20Zhou%20and%20Yixin%20Ou%20and%20Shengwei%20Ding%20and%20Long%20Li%20and%20Jialong%20Wu%20and%20Tiannan%20Wang%20and%20Jiamin%20Chen%20and%20Shuai%20Wang%20and%20Xiaohua%20Xu%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen%20and%20Yuchen%20Eleanor%20Jiang&entry.1292438233=%20%20The%20AI%20community%20has%20been%20exploring%20a%20pathway%20to%20artificial%20general%0Aintelligence%20%28AGI%29%20by%20developing%20%22language%20agents%22%2C%20which%20are%20complex%20large%0Alanguage%20models%20%28LLMs%29%20pipelines%20involving%20both%20prompting%20techniques%20and%20tool%0Ausage%20methods.%20While%20language%20agents%20have%20demonstrated%20impressive%20capabilities%0Afor%20many%20real-world%20tasks%2C%20a%20fundamental%20limitation%20of%20current%20language%20agents%0Aresearch%20is%20that%20they%20are%20model-centric%2C%20or%20engineering-centric.%20That%27s%20to%20say%2C%0Athe%20progress%20on%20prompts%2C%20tools%2C%20and%20pipelines%20of%20language%20agents%20requires%0Asubstantial%20manual%20engineering%20efforts%20from%20human%20experts%20rather%20than%0Aautomatically%20learning%20from%20data.%20We%20believe%20the%20transition%20from%20model-centric%2C%0Aor%20engineering-centric%2C%20to%20data-centric%2C%20i.e.%2C%20the%20ability%20of%20language%20agents%0Ato%20autonomously%20learn%20and%20evolve%20in%20environments%2C%20is%20the%20key%20for%20them%20to%0Apossibly%20achieve%20AGI.%0A%20%20In%20this%20work%2C%20we%20introduce%20agent%20symbolic%20learning%2C%20a%20systematic%20framework%0Athat%20enables%20language%20agents%20to%20optimize%20themselves%20on%20their%20own%20in%20a%0Adata-centric%20way%20using%20symbolic%20optimizers.%20Specifically%2C%20we%20consider%20agents%20as%0Asymbolic%20networks%20where%20learnable%20weights%20are%20defined%20by%20prompts%2C%20tools%2C%20and%0Athe%20way%20they%20are%20stacked%20together.%20Agent%20symbolic%20learning%20is%20designed%20to%0Aoptimize%20the%20symbolic%20network%20within%20language%20agents%20by%20mimicking%20two%0Afundamental%20algorithms%20in%20connectionist%20learning%3A%20back-propagation%20and%20gradient%0Adescent.%20Instead%20of%20dealing%20with%20numeric%20weights%2C%20agent%20symbolic%20learning%20works%0Awith%20natural%20language%20simulacrums%20of%20weights%2C%20loss%2C%20and%20gradients.%20We%20conduct%0Aproof-of-concept%20experiments%20on%20both%20standard%20benchmarks%20and%20complex%20real-world%0Atasks%20and%20show%20that%20agent%20symbolic%20learning%20enables%20language%20agents%20to%20update%0Athemselves%20after%20being%20created%20and%20deployed%20in%20the%20wild%2C%20resulting%20in%0A%22self-evolving%20agents%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18532v1&entry.124074799=Read"},
{"title": "An unsupervised approach towards promptable defect segmentation in\n  laser-based additive manufacturing by Segment Anything", "author": "Israt Zarin Era and Imtiaz Ahmed and Zhichao Liu and Srinjoy Das", "abstract": "  Foundation models are currently driving a paradigm shift in computer vision\ntasks for various fields including biology, astronomy, and robotics among\nothers, leveraging user-generated prompts to enhance their performance. In the\nLaser Additive Manufacturing (LAM) domain, accurate image-based defect\nsegmentation is imperative to ensure product quality and facilitate real-time\nprocess control. However, such tasks are often characterized by multiple\nchallenges including the absence of labels and the requirement for low latency\ninference among others. Porosity is a very common defect in LAM due to lack of\nfusion, entrapped gas, and keyholes, directly affecting mechanical properties\nlike tensile strength, stiffness, and hardness, thereby compromising the\nquality of the final product. To address these issues, we construct a framework\nfor image segmentation using a state-of-the-art Vision Transformer (ViT) based\nFoundation model (Segment Anything Model) with a novel multi-point prompt\ngeneration scheme using unsupervised clustering. Utilizing our framework we\nperform porosity segmentation in a case study of laser-based powder bed fusion\n(L-PBF) and obtain high accuracy without using any labeled data to guide the\nprompt tuning process. By capitalizing on lightweight foundation model\ninference combined with unsupervised prompt generation, we envision\nconstructing a real-time anomaly detection pipeline that could revolutionize\ncurrent laser additive manufacturing processes, thereby facilitating the shift\ntowards Industry 4.0 and promoting defect-free production along with\noperational efficiency.\n", "link": "http://arxiv.org/abs/2312.04063v3", "date": "2024-06-26", "relevancy": 2.1371, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5377}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5326}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20unsupervised%20approach%20towards%20promptable%20defect%20segmentation%20in%0A%20%20laser-based%20additive%20manufacturing%20by%20Segment%20Anything&body=Title%3A%20An%20unsupervised%20approach%20towards%20promptable%20defect%20segmentation%20in%0A%20%20laser-based%20additive%20manufacturing%20by%20Segment%20Anything%0AAuthor%3A%20Israt%20Zarin%20Era%20and%20Imtiaz%20Ahmed%20and%20Zhichao%20Liu%20and%20Srinjoy%20Das%0AAbstract%3A%20%20%20Foundation%20models%20are%20currently%20driving%20a%20paradigm%20shift%20in%20computer%20vision%0Atasks%20for%20various%20fields%20including%20biology%2C%20astronomy%2C%20and%20robotics%20among%0Aothers%2C%20leveraging%20user-generated%20prompts%20to%20enhance%20their%20performance.%20In%20the%0ALaser%20Additive%20Manufacturing%20%28LAM%29%20domain%2C%20accurate%20image-based%20defect%0Asegmentation%20is%20imperative%20to%20ensure%20product%20quality%20and%20facilitate%20real-time%0Aprocess%20control.%20However%2C%20such%20tasks%20are%20often%20characterized%20by%20multiple%0Achallenges%20including%20the%20absence%20of%20labels%20and%20the%20requirement%20for%20low%20latency%0Ainference%20among%20others.%20Porosity%20is%20a%20very%20common%20defect%20in%20LAM%20due%20to%20lack%20of%0Afusion%2C%20entrapped%20gas%2C%20and%20keyholes%2C%20directly%20affecting%20mechanical%20properties%0Alike%20tensile%20strength%2C%20stiffness%2C%20and%20hardness%2C%20thereby%20compromising%20the%0Aquality%20of%20the%20final%20product.%20To%20address%20these%20issues%2C%20we%20construct%20a%20framework%0Afor%20image%20segmentation%20using%20a%20state-of-the-art%20Vision%20Transformer%20%28ViT%29%20based%0AFoundation%20model%20%28Segment%20Anything%20Model%29%20with%20a%20novel%20multi-point%20prompt%0Ageneration%20scheme%20using%20unsupervised%20clustering.%20Utilizing%20our%20framework%20we%0Aperform%20porosity%20segmentation%20in%20a%20case%20study%20of%20laser-based%20powder%20bed%20fusion%0A%28L-PBF%29%20and%20obtain%20high%20accuracy%20without%20using%20any%20labeled%20data%20to%20guide%20the%0Aprompt%20tuning%20process.%20By%20capitalizing%20on%20lightweight%20foundation%20model%0Ainference%20combined%20with%20unsupervised%20prompt%20generation%2C%20we%20envision%0Aconstructing%20a%20real-time%20anomaly%20detection%20pipeline%20that%20could%20revolutionize%0Acurrent%20laser%20additive%20manufacturing%20processes%2C%20thereby%20facilitating%20the%20shift%0Atowards%20Industry%204.0%20and%20promoting%20defect-free%20production%20along%20with%0Aoperational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04063v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520unsupervised%2520approach%2520towards%2520promptable%2520defect%2520segmentation%2520in%250A%2520%2520laser-based%2520additive%2520manufacturing%2520by%2520Segment%2520Anything%26entry.906535625%3DIsrat%2520Zarin%2520Era%2520and%2520Imtiaz%2520Ahmed%2520and%2520Zhichao%2520Liu%2520and%2520Srinjoy%2520Das%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520currently%2520driving%2520a%2520paradigm%2520shift%2520in%2520computer%2520vision%250Atasks%2520for%2520various%2520fields%2520including%2520biology%252C%2520astronomy%252C%2520and%2520robotics%2520among%250Aothers%252C%2520leveraging%2520user-generated%2520prompts%2520to%2520enhance%2520their%2520performance.%2520In%2520the%250ALaser%2520Additive%2520Manufacturing%2520%2528LAM%2529%2520domain%252C%2520accurate%2520image-based%2520defect%250Asegmentation%2520is%2520imperative%2520to%2520ensure%2520product%2520quality%2520and%2520facilitate%2520real-time%250Aprocess%2520control.%2520However%252C%2520such%2520tasks%2520are%2520often%2520characterized%2520by%2520multiple%250Achallenges%2520including%2520the%2520absence%2520of%2520labels%2520and%2520the%2520requirement%2520for%2520low%2520latency%250Ainference%2520among%2520others.%2520Porosity%2520is%2520a%2520very%2520common%2520defect%2520in%2520LAM%2520due%2520to%2520lack%2520of%250Afusion%252C%2520entrapped%2520gas%252C%2520and%2520keyholes%252C%2520directly%2520affecting%2520mechanical%2520properties%250Alike%2520tensile%2520strength%252C%2520stiffness%252C%2520and%2520hardness%252C%2520thereby%2520compromising%2520the%250Aquality%2520of%2520the%2520final%2520product.%2520To%2520address%2520these%2520issues%252C%2520we%2520construct%2520a%2520framework%250Afor%2520image%2520segmentation%2520using%2520a%2520state-of-the-art%2520Vision%2520Transformer%2520%2528ViT%2529%2520based%250AFoundation%2520model%2520%2528Segment%2520Anything%2520Model%2529%2520with%2520a%2520novel%2520multi-point%2520prompt%250Ageneration%2520scheme%2520using%2520unsupervised%2520clustering.%2520Utilizing%2520our%2520framework%2520we%250Aperform%2520porosity%2520segmentation%2520in%2520a%2520case%2520study%2520of%2520laser-based%2520powder%2520bed%2520fusion%250A%2528L-PBF%2529%2520and%2520obtain%2520high%2520accuracy%2520without%2520using%2520any%2520labeled%2520data%2520to%2520guide%2520the%250Aprompt%2520tuning%2520process.%2520By%2520capitalizing%2520on%2520lightweight%2520foundation%2520model%250Ainference%2520combined%2520with%2520unsupervised%2520prompt%2520generation%252C%2520we%2520envision%250Aconstructing%2520a%2520real-time%2520anomaly%2520detection%2520pipeline%2520that%2520could%2520revolutionize%250Acurrent%2520laser%2520additive%2520manufacturing%2520processes%252C%2520thereby%2520facilitating%2520the%2520shift%250Atowards%2520Industry%25204.0%2520and%2520promoting%2520defect-free%2520production%2520along%2520with%250Aoperational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04063v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20unsupervised%20approach%20towards%20promptable%20defect%20segmentation%20in%0A%20%20laser-based%20additive%20manufacturing%20by%20Segment%20Anything&entry.906535625=Israt%20Zarin%20Era%20and%20Imtiaz%20Ahmed%20and%20Zhichao%20Liu%20and%20Srinjoy%20Das&entry.1292438233=%20%20Foundation%20models%20are%20currently%20driving%20a%20paradigm%20shift%20in%20computer%20vision%0Atasks%20for%20various%20fields%20including%20biology%2C%20astronomy%2C%20and%20robotics%20among%0Aothers%2C%20leveraging%20user-generated%20prompts%20to%20enhance%20their%20performance.%20In%20the%0ALaser%20Additive%20Manufacturing%20%28LAM%29%20domain%2C%20accurate%20image-based%20defect%0Asegmentation%20is%20imperative%20to%20ensure%20product%20quality%20and%20facilitate%20real-time%0Aprocess%20control.%20However%2C%20such%20tasks%20are%20often%20characterized%20by%20multiple%0Achallenges%20including%20the%20absence%20of%20labels%20and%20the%20requirement%20for%20low%20latency%0Ainference%20among%20others.%20Porosity%20is%20a%20very%20common%20defect%20in%20LAM%20due%20to%20lack%20of%0Afusion%2C%20entrapped%20gas%2C%20and%20keyholes%2C%20directly%20affecting%20mechanical%20properties%0Alike%20tensile%20strength%2C%20stiffness%2C%20and%20hardness%2C%20thereby%20compromising%20the%0Aquality%20of%20the%20final%20product.%20To%20address%20these%20issues%2C%20we%20construct%20a%20framework%0Afor%20image%20segmentation%20using%20a%20state-of-the-art%20Vision%20Transformer%20%28ViT%29%20based%0AFoundation%20model%20%28Segment%20Anything%20Model%29%20with%20a%20novel%20multi-point%20prompt%0Ageneration%20scheme%20using%20unsupervised%20clustering.%20Utilizing%20our%20framework%20we%0Aperform%20porosity%20segmentation%20in%20a%20case%20study%20of%20laser-based%20powder%20bed%20fusion%0A%28L-PBF%29%20and%20obtain%20high%20accuracy%20without%20using%20any%20labeled%20data%20to%20guide%20the%0Aprompt%20tuning%20process.%20By%20capitalizing%20on%20lightweight%20foundation%20model%0Ainference%20combined%20with%20unsupervised%20prompt%20generation%2C%20we%20envision%0Aconstructing%20a%20real-time%20anomaly%20detection%20pipeline%20that%20could%20revolutionize%0Acurrent%20laser%20additive%20manufacturing%20processes%2C%20thereby%20facilitating%20the%20shift%0Atowards%20Industry%204.0%20and%20promoting%20defect-free%20production%20along%20with%0Aoperational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04063v3&entry.124074799=Read"},
{"title": "ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State\n  Space Model", "author": "Hongruixuan Chen and Jian Song and Chengxi Han and Junshi Xia and Naoto Yokoya", "abstract": "  Convolutional neural networks (CNN) and Transformers have made impressive\nprogress in the field of remote sensing change detection (CD). However, both\narchitectures have inherent shortcomings: CNN are constrained by a limited\nreceptive field that may hinder their ability to capture broader spatial\ncontexts, while Transformers are computationally intensive, making them costly\nto train and deploy on large datasets. Recently, the Mamba architecture, based\non state space models, has shown remarkable performance in a series of natural\nlanguage processing tasks, which can effectively compensate for the\nshortcomings of the above two architectures. In this paper, we explore for the\nfirst time the potential of the Mamba architecture for remote sensing CD tasks.\nWe tailor the corresponding frameworks, called MambaBCD, MambaSCD, and\nMambaBDA, for binary change detection (BCD), semantic change detection (SCD),\nand building damage assessment (BDA), respectively. All three frameworks adopt\nthe cutting-edge Visual Mamba architecture as the encoder, which allows full\nlearning of global spatial contextual information from the input images. For\nthe change decoder, which is available in all three architectures, we propose\nthree spatio-temporal relationship modeling mechanisms, which can be naturally\ncombined with the Mamba architecture and fully utilize its attribute to achieve\nspatio-temporal interaction of multi-temporal features, thereby obtaining\naccurate change information. On five benchmark datasets, our proposed\nframeworks outperform current CNN- and Transformer-based approaches without\nusing any complex training strategies or tricks, fully demonstrating the\npotential of the Mamba architecture in CD tasks. Further experiments show that\nour architecture is quite robust to degraded data. The source code will be\navailable in https://github.com/ChenHongruixuan/MambaCD\n", "link": "http://arxiv.org/abs/2404.03425v5", "date": "2024-06-26", "relevancy": 2.1304, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model&body=Title%3A%20ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model%0AAuthor%3A%20Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Chengxi%20Han%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNN%29%20and%20Transformers%20have%20made%20impressive%0Aprogress%20in%20the%20field%20of%20remote%20sensing%20change%20detection%20%28CD%29.%20However%2C%20both%0Aarchitectures%20have%20inherent%20shortcomings%3A%20CNN%20are%20constrained%20by%20a%20limited%0Areceptive%20field%20that%20may%20hinder%20their%20ability%20to%20capture%20broader%20spatial%0Acontexts%2C%20while%20Transformers%20are%20computationally%20intensive%2C%20making%20them%20costly%0Ato%20train%20and%20deploy%20on%20large%20datasets.%20Recently%2C%20the%20Mamba%20architecture%2C%20based%0Aon%20state%20space%20models%2C%20has%20shown%20remarkable%20performance%20in%20a%20series%20of%20natural%0Alanguage%20processing%20tasks%2C%20which%20can%20effectively%20compensate%20for%20the%0Ashortcomings%20of%20the%20above%20two%20architectures.%20In%20this%20paper%2C%20we%20explore%20for%20the%0Afirst%20time%20the%20potential%20of%20the%20Mamba%20architecture%20for%20remote%20sensing%20CD%20tasks.%0AWe%20tailor%20the%20corresponding%20frameworks%2C%20called%20MambaBCD%2C%20MambaSCD%2C%20and%0AMambaBDA%2C%20for%20binary%20change%20detection%20%28BCD%29%2C%20semantic%20change%20detection%20%28SCD%29%2C%0Aand%20building%20damage%20assessment%20%28BDA%29%2C%20respectively.%20All%20three%20frameworks%20adopt%0Athe%20cutting-edge%20Visual%20Mamba%20architecture%20as%20the%20encoder%2C%20which%20allows%20full%0Alearning%20of%20global%20spatial%20contextual%20information%20from%20the%20input%20images.%20For%0Athe%20change%20decoder%2C%20which%20is%20available%20in%20all%20three%20architectures%2C%20we%20propose%0Athree%20spatio-temporal%20relationship%20modeling%20mechanisms%2C%20which%20can%20be%20naturally%0Acombined%20with%20the%20Mamba%20architecture%20and%20fully%20utilize%20its%20attribute%20to%20achieve%0Aspatio-temporal%20interaction%20of%20multi-temporal%20features%2C%20thereby%20obtaining%0Aaccurate%20change%20information.%20On%20five%20benchmark%20datasets%2C%20our%20proposed%0Aframeworks%20outperform%20current%20CNN-%20and%20Transformer-based%20approaches%20without%0Ausing%20any%20complex%20training%20strategies%20or%20tricks%2C%20fully%20demonstrating%20the%0Apotential%20of%20the%20Mamba%20architecture%20in%20CD%20tasks.%20Further%20experiments%20show%20that%0Aour%20architecture%20is%20quite%20robust%20to%20degraded%20data.%20The%20source%20code%20will%20be%0Aavailable%20in%20https%3A//github.com/ChenHongruixuan/MambaCD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03425v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChangeMamba%253A%2520Remote%2520Sensing%2520Change%2520Detection%2520with%2520Spatio-Temporal%2520State%250A%2520%2520Space%2520Model%26entry.906535625%3DHongruixuan%2520Chen%2520and%2520Jian%2520Song%2520and%2520Chengxi%2520Han%2520and%2520Junshi%2520Xia%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNN%2529%2520and%2520Transformers%2520have%2520made%2520impressive%250Aprogress%2520in%2520the%2520field%2520of%2520remote%2520sensing%2520change%2520detection%2520%2528CD%2529.%2520However%252C%2520both%250Aarchitectures%2520have%2520inherent%2520shortcomings%253A%2520CNN%2520are%2520constrained%2520by%2520a%2520limited%250Areceptive%2520field%2520that%2520may%2520hinder%2520their%2520ability%2520to%2520capture%2520broader%2520spatial%250Acontexts%252C%2520while%2520Transformers%2520are%2520computationally%2520intensive%252C%2520making%2520them%2520costly%250Ato%2520train%2520and%2520deploy%2520on%2520large%2520datasets.%2520Recently%252C%2520the%2520Mamba%2520architecture%252C%2520based%250Aon%2520state%2520space%2520models%252C%2520has%2520shown%2520remarkable%2520performance%2520in%2520a%2520series%2520of%2520natural%250Alanguage%2520processing%2520tasks%252C%2520which%2520can%2520effectively%2520compensate%2520for%2520the%250Ashortcomings%2520of%2520the%2520above%2520two%2520architectures.%2520In%2520this%2520paper%252C%2520we%2520explore%2520for%2520the%250Afirst%2520time%2520the%2520potential%2520of%2520the%2520Mamba%2520architecture%2520for%2520remote%2520sensing%2520CD%2520tasks.%250AWe%2520tailor%2520the%2520corresponding%2520frameworks%252C%2520called%2520MambaBCD%252C%2520MambaSCD%252C%2520and%250AMambaBDA%252C%2520for%2520binary%2520change%2520detection%2520%2528BCD%2529%252C%2520semantic%2520change%2520detection%2520%2528SCD%2529%252C%250Aand%2520building%2520damage%2520assessment%2520%2528BDA%2529%252C%2520respectively.%2520All%2520three%2520frameworks%2520adopt%250Athe%2520cutting-edge%2520Visual%2520Mamba%2520architecture%2520as%2520the%2520encoder%252C%2520which%2520allows%2520full%250Alearning%2520of%2520global%2520spatial%2520contextual%2520information%2520from%2520the%2520input%2520images.%2520For%250Athe%2520change%2520decoder%252C%2520which%2520is%2520available%2520in%2520all%2520three%2520architectures%252C%2520we%2520propose%250Athree%2520spatio-temporal%2520relationship%2520modeling%2520mechanisms%252C%2520which%2520can%2520be%2520naturally%250Acombined%2520with%2520the%2520Mamba%2520architecture%2520and%2520fully%2520utilize%2520its%2520attribute%2520to%2520achieve%250Aspatio-temporal%2520interaction%2520of%2520multi-temporal%2520features%252C%2520thereby%2520obtaining%250Aaccurate%2520change%2520information.%2520On%2520five%2520benchmark%2520datasets%252C%2520our%2520proposed%250Aframeworks%2520outperform%2520current%2520CNN-%2520and%2520Transformer-based%2520approaches%2520without%250Ausing%2520any%2520complex%2520training%2520strategies%2520or%2520tricks%252C%2520fully%2520demonstrating%2520the%250Apotential%2520of%2520the%2520Mamba%2520architecture%2520in%2520CD%2520tasks.%2520Further%2520experiments%2520show%2520that%250Aour%2520architecture%2520is%2520quite%2520robust%2520to%2520degraded%2520data.%2520The%2520source%2520code%2520will%2520be%250Aavailable%2520in%2520https%253A//github.com/ChenHongruixuan/MambaCD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03425v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model&entry.906535625=Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Chengxi%20Han%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNN%29%20and%20Transformers%20have%20made%20impressive%0Aprogress%20in%20the%20field%20of%20remote%20sensing%20change%20detection%20%28CD%29.%20However%2C%20both%0Aarchitectures%20have%20inherent%20shortcomings%3A%20CNN%20are%20constrained%20by%20a%20limited%0Areceptive%20field%20that%20may%20hinder%20their%20ability%20to%20capture%20broader%20spatial%0Acontexts%2C%20while%20Transformers%20are%20computationally%20intensive%2C%20making%20them%20costly%0Ato%20train%20and%20deploy%20on%20large%20datasets.%20Recently%2C%20the%20Mamba%20architecture%2C%20based%0Aon%20state%20space%20models%2C%20has%20shown%20remarkable%20performance%20in%20a%20series%20of%20natural%0Alanguage%20processing%20tasks%2C%20which%20can%20effectively%20compensate%20for%20the%0Ashortcomings%20of%20the%20above%20two%20architectures.%20In%20this%20paper%2C%20we%20explore%20for%20the%0Afirst%20time%20the%20potential%20of%20the%20Mamba%20architecture%20for%20remote%20sensing%20CD%20tasks.%0AWe%20tailor%20the%20corresponding%20frameworks%2C%20called%20MambaBCD%2C%20MambaSCD%2C%20and%0AMambaBDA%2C%20for%20binary%20change%20detection%20%28BCD%29%2C%20semantic%20change%20detection%20%28SCD%29%2C%0Aand%20building%20damage%20assessment%20%28BDA%29%2C%20respectively.%20All%20three%20frameworks%20adopt%0Athe%20cutting-edge%20Visual%20Mamba%20architecture%20as%20the%20encoder%2C%20which%20allows%20full%0Alearning%20of%20global%20spatial%20contextual%20information%20from%20the%20input%20images.%20For%0Athe%20change%20decoder%2C%20which%20is%20available%20in%20all%20three%20architectures%2C%20we%20propose%0Athree%20spatio-temporal%20relationship%20modeling%20mechanisms%2C%20which%20can%20be%20naturally%0Acombined%20with%20the%20Mamba%20architecture%20and%20fully%20utilize%20its%20attribute%20to%20achieve%0Aspatio-temporal%20interaction%20of%20multi-temporal%20features%2C%20thereby%20obtaining%0Aaccurate%20change%20information.%20On%20five%20benchmark%20datasets%2C%20our%20proposed%0Aframeworks%20outperform%20current%20CNN-%20and%20Transformer-based%20approaches%20without%0Ausing%20any%20complex%20training%20strategies%20or%20tricks%2C%20fully%20demonstrating%20the%0Apotential%20of%20the%20Mamba%20architecture%20in%20CD%20tasks.%20Further%20experiments%20show%20that%0Aour%20architecture%20is%20quite%20robust%20to%20degraded%20data.%20The%20source%20code%20will%20be%0Aavailable%20in%20https%3A//github.com/ChenHongruixuan/MambaCD%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03425v5&entry.124074799=Read"},
{"title": "ObjFormer: Learning Land-Cover Changes From Paired OSM Data and Optical\n  High-Resolution Imagery via Object-Guided Transformer", "author": "Hongruixuan Chen and Cuiling Lan and Jian Song and Clifford Broni-Bediako and Junshi Xia and Naoto Yokoya", "abstract": "  Optical high-resolution imagery and OSM data are two important data sources\nof change detection (CD). Previous related studies focus on utilizing the\ninformation in OSM data to aid the CD on optical high-resolution images. This\npaper pioneers the direct detection of land-cover changes utilizing paired OSM\ndata and optical imagery, thereby expanding the scope of CD tasks. To this end,\nwe propose an object-guided Transformer (ObjFormer) by naturally combining the\nobject-based image analysis (OBIA) technique with the advanced vision\nTransformer architecture. This combination can significantly reduce the\ncomputational overhead in the self-attention module without adding extra\nparameters or layers. ObjFormer has a hierarchical pseudo-siamese encoder\nconsisting of object-guided self-attention modules that extracts multi-level\nheterogeneous features from OSM data and optical images; a decoder consisting\nof object-guided cross-attention modules can recover land-cover changes from\nthe extracted heterogeneous features. Beyond basic binary change detection,\nthis paper raises a new semi-supervised semantic change detection task that\ndoes not require any manually annotated land-cover labels to train semantic\nchange detectors. Two lightweight semantic decoders are added to ObjFormer to\naccomplish this task efficiently. A converse cross-entropy loss is designed to\nfully utilize negative samples, contributing to the great performance\nimprovement in this task. A large-scale benchmark dataset called OpenMapCD\ncontaining 1,287 samples covering 40 regions on six continents is constructed\nto conduct detailed experiments. The results show the effectiveness of our\nmethods in this new kind of CD task. Additionally, case studies in Japanese\ncities demonstrate the framework's generalizability and practical potential.\nThe OpenMapCD and source code are available in\nhttps://github.com/ChenHongruixuan/ObjFormer\n", "link": "http://arxiv.org/abs/2310.02674v3", "date": "2024-06-26", "relevancy": 2.1281, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5396}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5352}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ObjFormer%3A%20Learning%20Land-Cover%20Changes%20From%20Paired%20OSM%20Data%20and%20Optical%0A%20%20High-Resolution%20Imagery%20via%20Object-Guided%20Transformer&body=Title%3A%20ObjFormer%3A%20Learning%20Land-Cover%20Changes%20From%20Paired%20OSM%20Data%20and%20Optical%0A%20%20High-Resolution%20Imagery%20via%20Object-Guided%20Transformer%0AAuthor%3A%20Hongruixuan%20Chen%20and%20Cuiling%20Lan%20and%20Jian%20Song%20and%20Clifford%20Broni-Bediako%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Optical%20high-resolution%20imagery%20and%20OSM%20data%20are%20two%20important%20data%20sources%0Aof%20change%20detection%20%28CD%29.%20Previous%20related%20studies%20focus%20on%20utilizing%20the%0Ainformation%20in%20OSM%20data%20to%20aid%20the%20CD%20on%20optical%20high-resolution%20images.%20This%0Apaper%20pioneers%20the%20direct%20detection%20of%20land-cover%20changes%20utilizing%20paired%20OSM%0Adata%20and%20optical%20imagery%2C%20thereby%20expanding%20the%20scope%20of%20CD%20tasks.%20To%20this%20end%2C%0Awe%20propose%20an%20object-guided%20Transformer%20%28ObjFormer%29%20by%20naturally%20combining%20the%0Aobject-based%20image%20analysis%20%28OBIA%29%20technique%20with%20the%20advanced%20vision%0ATransformer%20architecture.%20This%20combination%20can%20significantly%20reduce%20the%0Acomputational%20overhead%20in%20the%20self-attention%20module%20without%20adding%20extra%0Aparameters%20or%20layers.%20ObjFormer%20has%20a%20hierarchical%20pseudo-siamese%20encoder%0Aconsisting%20of%20object-guided%20self-attention%20modules%20that%20extracts%20multi-level%0Aheterogeneous%20features%20from%20OSM%20data%20and%20optical%20images%3B%20a%20decoder%20consisting%0Aof%20object-guided%20cross-attention%20modules%20can%20recover%20land-cover%20changes%20from%0Athe%20extracted%20heterogeneous%20features.%20Beyond%20basic%20binary%20change%20detection%2C%0Athis%20paper%20raises%20a%20new%20semi-supervised%20semantic%20change%20detection%20task%20that%0Adoes%20not%20require%20any%20manually%20annotated%20land-cover%20labels%20to%20train%20semantic%0Achange%20detectors.%20Two%20lightweight%20semantic%20decoders%20are%20added%20to%20ObjFormer%20to%0Aaccomplish%20this%20task%20efficiently.%20A%20converse%20cross-entropy%20loss%20is%20designed%20to%0Afully%20utilize%20negative%20samples%2C%20contributing%20to%20the%20great%20performance%0Aimprovement%20in%20this%20task.%20A%20large-scale%20benchmark%20dataset%20called%20OpenMapCD%0Acontaining%201%2C287%20samples%20covering%2040%20regions%20on%20six%20continents%20is%20constructed%0Ato%20conduct%20detailed%20experiments.%20The%20results%20show%20the%20effectiveness%20of%20our%0Amethods%20in%20this%20new%20kind%20of%20CD%20task.%20Additionally%2C%20case%20studies%20in%20Japanese%0Acities%20demonstrate%20the%20framework%27s%20generalizability%20and%20practical%20potential.%0AThe%20OpenMapCD%20and%20source%20code%20are%20available%20in%0Ahttps%3A//github.com/ChenHongruixuan/ObjFormer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02674v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjFormer%253A%2520Learning%2520Land-Cover%2520Changes%2520From%2520Paired%2520OSM%2520Data%2520and%2520Optical%250A%2520%2520High-Resolution%2520Imagery%2520via%2520Object-Guided%2520Transformer%26entry.906535625%3DHongruixuan%2520Chen%2520and%2520Cuiling%2520Lan%2520and%2520Jian%2520Song%2520and%2520Clifford%2520Broni-Bediako%2520and%2520Junshi%2520Xia%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Optical%2520high-resolution%2520imagery%2520and%2520OSM%2520data%2520are%2520two%2520important%2520data%2520sources%250Aof%2520change%2520detection%2520%2528CD%2529.%2520Previous%2520related%2520studies%2520focus%2520on%2520utilizing%2520the%250Ainformation%2520in%2520OSM%2520data%2520to%2520aid%2520the%2520CD%2520on%2520optical%2520high-resolution%2520images.%2520This%250Apaper%2520pioneers%2520the%2520direct%2520detection%2520of%2520land-cover%2520changes%2520utilizing%2520paired%2520OSM%250Adata%2520and%2520optical%2520imagery%252C%2520thereby%2520expanding%2520the%2520scope%2520of%2520CD%2520tasks.%2520To%2520this%2520end%252C%250Awe%2520propose%2520an%2520object-guided%2520Transformer%2520%2528ObjFormer%2529%2520by%2520naturally%2520combining%2520the%250Aobject-based%2520image%2520analysis%2520%2528OBIA%2529%2520technique%2520with%2520the%2520advanced%2520vision%250ATransformer%2520architecture.%2520This%2520combination%2520can%2520significantly%2520reduce%2520the%250Acomputational%2520overhead%2520in%2520the%2520self-attention%2520module%2520without%2520adding%2520extra%250Aparameters%2520or%2520layers.%2520ObjFormer%2520has%2520a%2520hierarchical%2520pseudo-siamese%2520encoder%250Aconsisting%2520of%2520object-guided%2520self-attention%2520modules%2520that%2520extracts%2520multi-level%250Aheterogeneous%2520features%2520from%2520OSM%2520data%2520and%2520optical%2520images%253B%2520a%2520decoder%2520consisting%250Aof%2520object-guided%2520cross-attention%2520modules%2520can%2520recover%2520land-cover%2520changes%2520from%250Athe%2520extracted%2520heterogeneous%2520features.%2520Beyond%2520basic%2520binary%2520change%2520detection%252C%250Athis%2520paper%2520raises%2520a%2520new%2520semi-supervised%2520semantic%2520change%2520detection%2520task%2520that%250Adoes%2520not%2520require%2520any%2520manually%2520annotated%2520land-cover%2520labels%2520to%2520train%2520semantic%250Achange%2520detectors.%2520Two%2520lightweight%2520semantic%2520decoders%2520are%2520added%2520to%2520ObjFormer%2520to%250Aaccomplish%2520this%2520task%2520efficiently.%2520A%2520converse%2520cross-entropy%2520loss%2520is%2520designed%2520to%250Afully%2520utilize%2520negative%2520samples%252C%2520contributing%2520to%2520the%2520great%2520performance%250Aimprovement%2520in%2520this%2520task.%2520A%2520large-scale%2520benchmark%2520dataset%2520called%2520OpenMapCD%250Acontaining%25201%252C287%2520samples%2520covering%252040%2520regions%2520on%2520six%2520continents%2520is%2520constructed%250Ato%2520conduct%2520detailed%2520experiments.%2520The%2520results%2520show%2520the%2520effectiveness%2520of%2520our%250Amethods%2520in%2520this%2520new%2520kind%2520of%2520CD%2520task.%2520Additionally%252C%2520case%2520studies%2520in%2520Japanese%250Acities%2520demonstrate%2520the%2520framework%2527s%2520generalizability%2520and%2520practical%2520potential.%250AThe%2520OpenMapCD%2520and%2520source%2520code%2520are%2520available%2520in%250Ahttps%253A//github.com/ChenHongruixuan/ObjFormer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02674v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObjFormer%3A%20Learning%20Land-Cover%20Changes%20From%20Paired%20OSM%20Data%20and%20Optical%0A%20%20High-Resolution%20Imagery%20via%20Object-Guided%20Transformer&entry.906535625=Hongruixuan%20Chen%20and%20Cuiling%20Lan%20and%20Jian%20Song%20and%20Clifford%20Broni-Bediako%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Optical%20high-resolution%20imagery%20and%20OSM%20data%20are%20two%20important%20data%20sources%0Aof%20change%20detection%20%28CD%29.%20Previous%20related%20studies%20focus%20on%20utilizing%20the%0Ainformation%20in%20OSM%20data%20to%20aid%20the%20CD%20on%20optical%20high-resolution%20images.%20This%0Apaper%20pioneers%20the%20direct%20detection%20of%20land-cover%20changes%20utilizing%20paired%20OSM%0Adata%20and%20optical%20imagery%2C%20thereby%20expanding%20the%20scope%20of%20CD%20tasks.%20To%20this%20end%2C%0Awe%20propose%20an%20object-guided%20Transformer%20%28ObjFormer%29%20by%20naturally%20combining%20the%0Aobject-based%20image%20analysis%20%28OBIA%29%20technique%20with%20the%20advanced%20vision%0ATransformer%20architecture.%20This%20combination%20can%20significantly%20reduce%20the%0Acomputational%20overhead%20in%20the%20self-attention%20module%20without%20adding%20extra%0Aparameters%20or%20layers.%20ObjFormer%20has%20a%20hierarchical%20pseudo-siamese%20encoder%0Aconsisting%20of%20object-guided%20self-attention%20modules%20that%20extracts%20multi-level%0Aheterogeneous%20features%20from%20OSM%20data%20and%20optical%20images%3B%20a%20decoder%20consisting%0Aof%20object-guided%20cross-attention%20modules%20can%20recover%20land-cover%20changes%20from%0Athe%20extracted%20heterogeneous%20features.%20Beyond%20basic%20binary%20change%20detection%2C%0Athis%20paper%20raises%20a%20new%20semi-supervised%20semantic%20change%20detection%20task%20that%0Adoes%20not%20require%20any%20manually%20annotated%20land-cover%20labels%20to%20train%20semantic%0Achange%20detectors.%20Two%20lightweight%20semantic%20decoders%20are%20added%20to%20ObjFormer%20to%0Aaccomplish%20this%20task%20efficiently.%20A%20converse%20cross-entropy%20loss%20is%20designed%20to%0Afully%20utilize%20negative%20samples%2C%20contributing%20to%20the%20great%20performance%0Aimprovement%20in%20this%20task.%20A%20large-scale%20benchmark%20dataset%20called%20OpenMapCD%0Acontaining%201%2C287%20samples%20covering%2040%20regions%20on%20six%20continents%20is%20constructed%0Ato%20conduct%20detailed%20experiments.%20The%20results%20show%20the%20effectiveness%20of%20our%0Amethods%20in%20this%20new%20kind%20of%20CD%20task.%20Additionally%2C%20case%20studies%20in%20Japanese%0Acities%20demonstrate%20the%20framework%27s%20generalizability%20and%20practical%20potential.%0AThe%20OpenMapCD%20and%20source%20code%20are%20available%20in%0Ahttps%3A//github.com/ChenHongruixuan/ObjFormer%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02674v3&entry.124074799=Read"},
{"title": "Facial Image Feature Analysis and its Specialization for Fr\u00e9chet\n  Distance and Neighborhoods", "author": "Doruk Cetin and Benedikt Schesch and Petar Stamenkovic and Niko Benjamin Huber and Fabio Z\u00fcnd and Majed El Helou", "abstract": "  Assessing distances between images and image datasets is a fundamental task\nin vision-based research. It is a challenging open problem in the literature\nand despite the criticism it receives, the most ubiquitous method remains the\nFr\\'echet Inception Distance. The Inception network is trained on a specific\nlabeled dataset, ImageNet, which has caused the core of its criticism in the\nmost recent research. Improvements were shown by moving to self-supervision\nlearning over ImageNet, leaving the training data domain as an open question.\nWe make that last leap and provide the first analysis on domain-specific\nfeature training and its effects on feature distance, on the widely-researched\nfacial image domain. We provide our findings and insights on this domain\nspecialization for Fr\\'echet distance and image neighborhoods, supported by\nextensive experiments and in-depth user studies.\n", "link": "http://arxiv.org/abs/2406.18430v1", "date": "2024-06-26", "relevancy": 2.1185, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5523}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.518}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facial%20Image%20Feature%20Analysis%20and%20its%20Specialization%20for%20Fr%C3%A9chet%0A%20%20Distance%20and%20Neighborhoods&body=Title%3A%20Facial%20Image%20Feature%20Analysis%20and%20its%20Specialization%20for%20Fr%C3%A9chet%0A%20%20Distance%20and%20Neighborhoods%0AAuthor%3A%20Doruk%20Cetin%20and%20Benedikt%20Schesch%20and%20Petar%20Stamenkovic%20and%20Niko%20Benjamin%20Huber%20and%20Fabio%20Z%C3%BCnd%20and%20Majed%20El%20Helou%0AAbstract%3A%20%20%20Assessing%20distances%20between%20images%20and%20image%20datasets%20is%20a%20fundamental%20task%0Ain%20vision-based%20research.%20It%20is%20a%20challenging%20open%20problem%20in%20the%20literature%0Aand%20despite%20the%20criticism%20it%20receives%2C%20the%20most%20ubiquitous%20method%20remains%20the%0AFr%5C%27echet%20Inception%20Distance.%20The%20Inception%20network%20is%20trained%20on%20a%20specific%0Alabeled%20dataset%2C%20ImageNet%2C%20which%20has%20caused%20the%20core%20of%20its%20criticism%20in%20the%0Amost%20recent%20research.%20Improvements%20were%20shown%20by%20moving%20to%20self-supervision%0Alearning%20over%20ImageNet%2C%20leaving%20the%20training%20data%20domain%20as%20an%20open%20question.%0AWe%20make%20that%20last%20leap%20and%20provide%20the%20first%20analysis%20on%20domain-specific%0Afeature%20training%20and%20its%20effects%20on%20feature%20distance%2C%20on%20the%20widely-researched%0Afacial%20image%20domain.%20We%20provide%20our%20findings%20and%20insights%20on%20this%20domain%0Aspecialization%20for%20Fr%5C%27echet%20distance%20and%20image%20neighborhoods%2C%20supported%20by%0Aextensive%20experiments%20and%20in-depth%20user%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacial%2520Image%2520Feature%2520Analysis%2520and%2520its%2520Specialization%2520for%2520Fr%25C3%25A9chet%250A%2520%2520Distance%2520and%2520Neighborhoods%26entry.906535625%3DDoruk%2520Cetin%2520and%2520Benedikt%2520Schesch%2520and%2520Petar%2520Stamenkovic%2520and%2520Niko%2520Benjamin%2520Huber%2520and%2520Fabio%2520Z%25C3%25BCnd%2520and%2520Majed%2520El%2520Helou%26entry.1292438233%3D%2520%2520Assessing%2520distances%2520between%2520images%2520and%2520image%2520datasets%2520is%2520a%2520fundamental%2520task%250Ain%2520vision-based%2520research.%2520It%2520is%2520a%2520challenging%2520open%2520problem%2520in%2520the%2520literature%250Aand%2520despite%2520the%2520criticism%2520it%2520receives%252C%2520the%2520most%2520ubiquitous%2520method%2520remains%2520the%250AFr%255C%2527echet%2520Inception%2520Distance.%2520The%2520Inception%2520network%2520is%2520trained%2520on%2520a%2520specific%250Alabeled%2520dataset%252C%2520ImageNet%252C%2520which%2520has%2520caused%2520the%2520core%2520of%2520its%2520criticism%2520in%2520the%250Amost%2520recent%2520research.%2520Improvements%2520were%2520shown%2520by%2520moving%2520to%2520self-supervision%250Alearning%2520over%2520ImageNet%252C%2520leaving%2520the%2520training%2520data%2520domain%2520as%2520an%2520open%2520question.%250AWe%2520make%2520that%2520last%2520leap%2520and%2520provide%2520the%2520first%2520analysis%2520on%2520domain-specific%250Afeature%2520training%2520and%2520its%2520effects%2520on%2520feature%2520distance%252C%2520on%2520the%2520widely-researched%250Afacial%2520image%2520domain.%2520We%2520provide%2520our%2520findings%2520and%2520insights%2520on%2520this%2520domain%250Aspecialization%2520for%2520Fr%255C%2527echet%2520distance%2520and%2520image%2520neighborhoods%252C%2520supported%2520by%250Aextensive%2520experiments%2520and%2520in-depth%2520user%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facial%20Image%20Feature%20Analysis%20and%20its%20Specialization%20for%20Fr%C3%A9chet%0A%20%20Distance%20and%20Neighborhoods&entry.906535625=Doruk%20Cetin%20and%20Benedikt%20Schesch%20and%20Petar%20Stamenkovic%20and%20Niko%20Benjamin%20Huber%20and%20Fabio%20Z%C3%BCnd%20and%20Majed%20El%20Helou&entry.1292438233=%20%20Assessing%20distances%20between%20images%20and%20image%20datasets%20is%20a%20fundamental%20task%0Ain%20vision-based%20research.%20It%20is%20a%20challenging%20open%20problem%20in%20the%20literature%0Aand%20despite%20the%20criticism%20it%20receives%2C%20the%20most%20ubiquitous%20method%20remains%20the%0AFr%5C%27echet%20Inception%20Distance.%20The%20Inception%20network%20is%20trained%20on%20a%20specific%0Alabeled%20dataset%2C%20ImageNet%2C%20which%20has%20caused%20the%20core%20of%20its%20criticism%20in%20the%0Amost%20recent%20research.%20Improvements%20were%20shown%20by%20moving%20to%20self-supervision%0Alearning%20over%20ImageNet%2C%20leaving%20the%20training%20data%20domain%20as%20an%20open%20question.%0AWe%20make%20that%20last%20leap%20and%20provide%20the%20first%20analysis%20on%20domain-specific%0Afeature%20training%20and%20its%20effects%20on%20feature%20distance%2C%20on%20the%20widely-researched%0Afacial%20image%20domain.%20We%20provide%20our%20findings%20and%20insights%20on%20this%20domain%0Aspecialization%20for%20Fr%5C%27echet%20distance%20and%20image%20neighborhoods%2C%20supported%20by%0Aextensive%20experiments%20and%20in-depth%20user%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18430v1&entry.124074799=Read"},
{"title": "BASS: Batched Attention-optimized Speculative Sampling", "author": "Haifeng Qian and Sujan Kumar Gonugondla and Sungsoo Ha and Mingyue Shang and Sanjay Krishna Gouda and Ramesh Nallapati and Sudipta Sengupta and Xiaofei Ma and Anoop Deoras", "abstract": "  Speculative decoding has emerged as a powerful method to improve latency and\nthroughput in hosting large language models. However, most existing\nimplementations focus on generating a single sequence. Real-world generative AI\napplications often require multiple responses and how to perform speculative\ndecoding in a batched setting while preserving its latency benefits poses\nnon-trivial challenges. This paper describes a system of batched speculative\ndecoding that sets a new state of the art in multi-sequence generation latency\nand that demonstrates superior GPU utilization as well as quality of\ngenerations within a time budget. For example, for a 7.8B-size model on a\nsingle A100 GPU and with a batch size of 8, each sequence is generated at an\naverage speed of 5.8ms per token, the overall throughput being 1.1K tokens per\nsecond. These results represent state-of-the-art latency and a 2.15X speed-up\nover optimized regular decoding. Within a time budget that regular decoding\ndoes not finish, our system is able to generate sequences with HumanEval\nPass@First of 43% and Pass@All of 61%, far exceeding what's feasible with\nsingle-sequence speculative decoding. Our peak GPU utilization during decoding\nreaches as high as 15.8%, more than 3X the highest of that of regular decoding\nand around 10X of single-sequence speculative decoding.\n", "link": "http://arxiv.org/abs/2404.15778v2", "date": "2024-06-26", "relevancy": 2.1093, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.564}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5276}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BASS%3A%20Batched%20Attention-optimized%20Speculative%20Sampling&body=Title%3A%20BASS%3A%20Batched%20Attention-optimized%20Speculative%20Sampling%0AAuthor%3A%20Haifeng%20Qian%20and%20Sujan%20Kumar%20Gonugondla%20and%20Sungsoo%20Ha%20and%20Mingyue%20Shang%20and%20Sanjay%20Krishna%20Gouda%20and%20Ramesh%20Nallapati%20and%20Sudipta%20Sengupta%20and%20Xiaofei%20Ma%20and%20Anoop%20Deoras%0AAbstract%3A%20%20%20Speculative%20decoding%20has%20emerged%20as%20a%20powerful%20method%20to%20improve%20latency%20and%0Athroughput%20in%20hosting%20large%20language%20models.%20However%2C%20most%20existing%0Aimplementations%20focus%20on%20generating%20a%20single%20sequence.%20Real-world%20generative%20AI%0Aapplications%20often%20require%20multiple%20responses%20and%20how%20to%20perform%20speculative%0Adecoding%20in%20a%20batched%20setting%20while%20preserving%20its%20latency%20benefits%20poses%0Anon-trivial%20challenges.%20This%20paper%20describes%20a%20system%20of%20batched%20speculative%0Adecoding%20that%20sets%20a%20new%20state%20of%20the%20art%20in%20multi-sequence%20generation%20latency%0Aand%20that%20demonstrates%20superior%20GPU%20utilization%20as%20well%20as%20quality%20of%0Agenerations%20within%20a%20time%20budget.%20For%20example%2C%20for%20a%207.8B-size%20model%20on%20a%0Asingle%20A100%20GPU%20and%20with%20a%20batch%20size%20of%208%2C%20each%20sequence%20is%20generated%20at%20an%0Aaverage%20speed%20of%205.8ms%20per%20token%2C%20the%20overall%20throughput%20being%201.1K%20tokens%20per%0Asecond.%20These%20results%20represent%20state-of-the-art%20latency%20and%20a%202.15X%20speed-up%0Aover%20optimized%20regular%20decoding.%20Within%20a%20time%20budget%20that%20regular%20decoding%0Adoes%20not%20finish%2C%20our%20system%20is%20able%20to%20generate%20sequences%20with%20HumanEval%0APass%40First%20of%2043%25%20and%20Pass%40All%20of%2061%25%2C%20far%20exceeding%20what%27s%20feasible%20with%0Asingle-sequence%20speculative%20decoding.%20Our%20peak%20GPU%20utilization%20during%20decoding%0Areaches%20as%20high%20as%2015.8%25%2C%20more%20than%203X%20the%20highest%20of%20that%20of%20regular%20decoding%0Aand%20around%2010X%20of%20single-sequence%20speculative%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15778v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBASS%253A%2520Batched%2520Attention-optimized%2520Speculative%2520Sampling%26entry.906535625%3DHaifeng%2520Qian%2520and%2520Sujan%2520Kumar%2520Gonugondla%2520and%2520Sungsoo%2520Ha%2520and%2520Mingyue%2520Shang%2520and%2520Sanjay%2520Krishna%2520Gouda%2520and%2520Ramesh%2520Nallapati%2520and%2520Sudipta%2520Sengupta%2520and%2520Xiaofei%2520Ma%2520and%2520Anoop%2520Deoras%26entry.1292438233%3D%2520%2520Speculative%2520decoding%2520has%2520emerged%2520as%2520a%2520powerful%2520method%2520to%2520improve%2520latency%2520and%250Athroughput%2520in%2520hosting%2520large%2520language%2520models.%2520However%252C%2520most%2520existing%250Aimplementations%2520focus%2520on%2520generating%2520a%2520single%2520sequence.%2520Real-world%2520generative%2520AI%250Aapplications%2520often%2520require%2520multiple%2520responses%2520and%2520how%2520to%2520perform%2520speculative%250Adecoding%2520in%2520a%2520batched%2520setting%2520while%2520preserving%2520its%2520latency%2520benefits%2520poses%250Anon-trivial%2520challenges.%2520This%2520paper%2520describes%2520a%2520system%2520of%2520batched%2520speculative%250Adecoding%2520that%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520in%2520multi-sequence%2520generation%2520latency%250Aand%2520that%2520demonstrates%2520superior%2520GPU%2520utilization%2520as%2520well%2520as%2520quality%2520of%250Agenerations%2520within%2520a%2520time%2520budget.%2520For%2520example%252C%2520for%2520a%25207.8B-size%2520model%2520on%2520a%250Asingle%2520A100%2520GPU%2520and%2520with%2520a%2520batch%2520size%2520of%25208%252C%2520each%2520sequence%2520is%2520generated%2520at%2520an%250Aaverage%2520speed%2520of%25205.8ms%2520per%2520token%252C%2520the%2520overall%2520throughput%2520being%25201.1K%2520tokens%2520per%250Asecond.%2520These%2520results%2520represent%2520state-of-the-art%2520latency%2520and%2520a%25202.15X%2520speed-up%250Aover%2520optimized%2520regular%2520decoding.%2520Within%2520a%2520time%2520budget%2520that%2520regular%2520decoding%250Adoes%2520not%2520finish%252C%2520our%2520system%2520is%2520able%2520to%2520generate%2520sequences%2520with%2520HumanEval%250APass%2540First%2520of%252043%2525%2520and%2520Pass%2540All%2520of%252061%2525%252C%2520far%2520exceeding%2520what%2527s%2520feasible%2520with%250Asingle-sequence%2520speculative%2520decoding.%2520Our%2520peak%2520GPU%2520utilization%2520during%2520decoding%250Areaches%2520as%2520high%2520as%252015.8%2525%252C%2520more%2520than%25203X%2520the%2520highest%2520of%2520that%2520of%2520regular%2520decoding%250Aand%2520around%252010X%2520of%2520single-sequence%2520speculative%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15778v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BASS%3A%20Batched%20Attention-optimized%20Speculative%20Sampling&entry.906535625=Haifeng%20Qian%20and%20Sujan%20Kumar%20Gonugondla%20and%20Sungsoo%20Ha%20and%20Mingyue%20Shang%20and%20Sanjay%20Krishna%20Gouda%20and%20Ramesh%20Nallapati%20and%20Sudipta%20Sengupta%20and%20Xiaofei%20Ma%20and%20Anoop%20Deoras&entry.1292438233=%20%20Speculative%20decoding%20has%20emerged%20as%20a%20powerful%20method%20to%20improve%20latency%20and%0Athroughput%20in%20hosting%20large%20language%20models.%20However%2C%20most%20existing%0Aimplementations%20focus%20on%20generating%20a%20single%20sequence.%20Real-world%20generative%20AI%0Aapplications%20often%20require%20multiple%20responses%20and%20how%20to%20perform%20speculative%0Adecoding%20in%20a%20batched%20setting%20while%20preserving%20its%20latency%20benefits%20poses%0Anon-trivial%20challenges.%20This%20paper%20describes%20a%20system%20of%20batched%20speculative%0Adecoding%20that%20sets%20a%20new%20state%20of%20the%20art%20in%20multi-sequence%20generation%20latency%0Aand%20that%20demonstrates%20superior%20GPU%20utilization%20as%20well%20as%20quality%20of%0Agenerations%20within%20a%20time%20budget.%20For%20example%2C%20for%20a%207.8B-size%20model%20on%20a%0Asingle%20A100%20GPU%20and%20with%20a%20batch%20size%20of%208%2C%20each%20sequence%20is%20generated%20at%20an%0Aaverage%20speed%20of%205.8ms%20per%20token%2C%20the%20overall%20throughput%20being%201.1K%20tokens%20per%0Asecond.%20These%20results%20represent%20state-of-the-art%20latency%20and%20a%202.15X%20speed-up%0Aover%20optimized%20regular%20decoding.%20Within%20a%20time%20budget%20that%20regular%20decoding%0Adoes%20not%20finish%2C%20our%20system%20is%20able%20to%20generate%20sequences%20with%20HumanEval%0APass%40First%20of%2043%25%20and%20Pass%40All%20of%2061%25%2C%20far%20exceeding%20what%27s%20feasible%20with%0Asingle-sequence%20speculative%20decoding.%20Our%20peak%20GPU%20utilization%20during%20decoding%0Areaches%20as%20high%20as%2015.8%25%2C%20more%20than%203X%20the%20highest%20of%20that%20of%20regular%20decoding%0Aand%20around%2010X%20of%20single-sequence%20speculative%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15778v2&entry.124074799=Read"},
{"title": "Single-Model Attribution of Generative Models Through Final-Layer\n  Inversion", "author": "Mike Laszkiewicz and Jonas Ricker and Johannes Lederer and Asja Fischer", "abstract": "  Recent breakthroughs in generative modeling have sparked interest in\npractical single-model attribution. Such methods predict whether a sample was\ngenerated by a specific generator or not, for instance, to prove intellectual\nproperty theft. However, previous works are either limited to the closed-world\nsetting or require undesirable changes to the generative model. We address\nthese shortcomings by, first, viewing single-model attribution through the lens\nof anomaly detection. Arising from this change of perspective, we propose\nFLIPAD, a new approach for single-model attribution in the open-world setting\nbased on final-layer inversion and anomaly detection. We show that the utilized\nfinal-layer inversion can be reduced to a convex lasso optimization problem,\nmaking our approach theoretically sound and computationally efficient. The\ntheoretical findings are accompanied by an experimental study demonstrating the\neffectiveness of our approach and its flexibility to various domains.\n", "link": "http://arxiv.org/abs/2306.06210v5", "date": "2024-06-26", "relevancy": 2.1, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5335}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5296}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-Model%20Attribution%20of%20Generative%20Models%20Through%20Final-Layer%0A%20%20Inversion&body=Title%3A%20Single-Model%20Attribution%20of%20Generative%20Models%20Through%20Final-Layer%0A%20%20Inversion%0AAuthor%3A%20Mike%20Laszkiewicz%20and%20Jonas%20Ricker%20and%20Johannes%20Lederer%20and%20Asja%20Fischer%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20generative%20modeling%20have%20sparked%20interest%20in%0Apractical%20single-model%20attribution.%20Such%20methods%20predict%20whether%20a%20sample%20was%0Agenerated%20by%20a%20specific%20generator%20or%20not%2C%20for%20instance%2C%20to%20prove%20intellectual%0Aproperty%20theft.%20However%2C%20previous%20works%20are%20either%20limited%20to%20the%20closed-world%0Asetting%20or%20require%20undesirable%20changes%20to%20the%20generative%20model.%20We%20address%0Athese%20shortcomings%20by%2C%20first%2C%20viewing%20single-model%20attribution%20through%20the%20lens%0Aof%20anomaly%20detection.%20Arising%20from%20this%20change%20of%20perspective%2C%20we%20propose%0AFLIPAD%2C%20a%20new%20approach%20for%20single-model%20attribution%20in%20the%20open-world%20setting%0Abased%20on%20final-layer%20inversion%20and%20anomaly%20detection.%20We%20show%20that%20the%20utilized%0Afinal-layer%20inversion%20can%20be%20reduced%20to%20a%20convex%20lasso%20optimization%20problem%2C%0Amaking%20our%20approach%20theoretically%20sound%20and%20computationally%20efficient.%20The%0Atheoretical%20findings%20are%20accompanied%20by%20an%20experimental%20study%20demonstrating%20the%0Aeffectiveness%20of%20our%20approach%20and%20its%20flexibility%20to%20various%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06210v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-Model%2520Attribution%2520of%2520Generative%2520Models%2520Through%2520Final-Layer%250A%2520%2520Inversion%26entry.906535625%3DMike%2520Laszkiewicz%2520and%2520Jonas%2520Ricker%2520and%2520Johannes%2520Lederer%2520and%2520Asja%2520Fischer%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520generative%2520modeling%2520have%2520sparked%2520interest%2520in%250Apractical%2520single-model%2520attribution.%2520Such%2520methods%2520predict%2520whether%2520a%2520sample%2520was%250Agenerated%2520by%2520a%2520specific%2520generator%2520or%2520not%252C%2520for%2520instance%252C%2520to%2520prove%2520intellectual%250Aproperty%2520theft.%2520However%252C%2520previous%2520works%2520are%2520either%2520limited%2520to%2520the%2520closed-world%250Asetting%2520or%2520require%2520undesirable%2520changes%2520to%2520the%2520generative%2520model.%2520We%2520address%250Athese%2520shortcomings%2520by%252C%2520first%252C%2520viewing%2520single-model%2520attribution%2520through%2520the%2520lens%250Aof%2520anomaly%2520detection.%2520Arising%2520from%2520this%2520change%2520of%2520perspective%252C%2520we%2520propose%250AFLIPAD%252C%2520a%2520new%2520approach%2520for%2520single-model%2520attribution%2520in%2520the%2520open-world%2520setting%250Abased%2520on%2520final-layer%2520inversion%2520and%2520anomaly%2520detection.%2520We%2520show%2520that%2520the%2520utilized%250Afinal-layer%2520inversion%2520can%2520be%2520reduced%2520to%2520a%2520convex%2520lasso%2520optimization%2520problem%252C%250Amaking%2520our%2520approach%2520theoretically%2520sound%2520and%2520computationally%2520efficient.%2520The%250Atheoretical%2520findings%2520are%2520accompanied%2520by%2520an%2520experimental%2520study%2520demonstrating%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520and%2520its%2520flexibility%2520to%2520various%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06210v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Model%20Attribution%20of%20Generative%20Models%20Through%20Final-Layer%0A%20%20Inversion&entry.906535625=Mike%20Laszkiewicz%20and%20Jonas%20Ricker%20and%20Johannes%20Lederer%20and%20Asja%20Fischer&entry.1292438233=%20%20Recent%20breakthroughs%20in%20generative%20modeling%20have%20sparked%20interest%20in%0Apractical%20single-model%20attribution.%20Such%20methods%20predict%20whether%20a%20sample%20was%0Agenerated%20by%20a%20specific%20generator%20or%20not%2C%20for%20instance%2C%20to%20prove%20intellectual%0Aproperty%20theft.%20However%2C%20previous%20works%20are%20either%20limited%20to%20the%20closed-world%0Asetting%20or%20require%20undesirable%20changes%20to%20the%20generative%20model.%20We%20address%0Athese%20shortcomings%20by%2C%20first%2C%20viewing%20single-model%20attribution%20through%20the%20lens%0Aof%20anomaly%20detection.%20Arising%20from%20this%20change%20of%20perspective%2C%20we%20propose%0AFLIPAD%2C%20a%20new%20approach%20for%20single-model%20attribution%20in%20the%20open-world%20setting%0Abased%20on%20final-layer%20inversion%20and%20anomaly%20detection.%20We%20show%20that%20the%20utilized%0Afinal-layer%20inversion%20can%20be%20reduced%20to%20a%20convex%20lasso%20optimization%20problem%2C%0Amaking%20our%20approach%20theoretically%20sound%20and%20computationally%20efficient.%20The%0Atheoretical%20findings%20are%20accompanied%20by%20an%20experimental%20study%20demonstrating%20the%0Aeffectiveness%20of%20our%20approach%20and%20its%20flexibility%20to%20various%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06210v5&entry.124074799=Read"},
{"title": "Continuous Sign Language Recognition Using Intra-inter Gloss Attention", "author": "Hossein Ranjbar and Alireza Taheri", "abstract": "  Many continuous sign language recognition (CSLR) studies adopt\ntransformer-based architectures for sequence modeling due to their powerful\ncapacity for capturing global contexts. Nevertheless, vanilla self-attention,\nwhich serves as the core module of the transformer, calculates a weighted\naverage over all time steps; therefore, the local temporal semantics of sign\nvideos may not be fully exploited. In this study, we introduce a novel module\nin sign language recognition studies, called intra-inter gloss attention\nmodule, to leverage the relationships among frames within glosses and the\nsemantic and grammatical dependencies between glosses in the video. In the\nintra-gloss attention module, the video is divided into equally sized chunks\nand a self-attention mechanism is applied within each chunk. This localized\nself-attention significantly reduces complexity and eliminates noise introduced\nby considering non-relative frames. In the inter-gloss attention module, we\nfirst aggregate the chunk-level features within each gloss chunk by average\npooling along the temporal dimension. Subsequently, multi-head self-attention\nis applied to all chunk-level features. Given the non-significance of the\nsigner-environment interaction, we utilize segmentation to remove the\nbackground of the videos. This enables the proposed model to direct its focus\ntoward the signer. Experimental results on the PHOENIX-2014 benchmark dataset\ndemonstrate that our method can effectively extract sign language features in\nan end-to-end manner without any prior knowledge, improve the accuracy of CSLR,\nand achieve the word error rate (WER) of 20.4 on the test set which is a\ncompetitive result compare to the state-of-the-art which uses additional\nsupervisions.\n", "link": "http://arxiv.org/abs/2406.18333v1", "date": "2024-06-26", "relevancy": 2.0914, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5513}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5222}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Sign%20Language%20Recognition%20Using%20Intra-inter%20Gloss%20Attention&body=Title%3A%20Continuous%20Sign%20Language%20Recognition%20Using%20Intra-inter%20Gloss%20Attention%0AAuthor%3A%20Hossein%20Ranjbar%20and%20Alireza%20Taheri%0AAbstract%3A%20%20%20Many%20continuous%20sign%20language%20recognition%20%28CSLR%29%20studies%20adopt%0Atransformer-based%20architectures%20for%20sequence%20modeling%20due%20to%20their%20powerful%0Acapacity%20for%20capturing%20global%20contexts.%20Nevertheless%2C%20vanilla%20self-attention%2C%0Awhich%20serves%20as%20the%20core%20module%20of%20the%20transformer%2C%20calculates%20a%20weighted%0Aaverage%20over%20all%20time%20steps%3B%20therefore%2C%20the%20local%20temporal%20semantics%20of%20sign%0Avideos%20may%20not%20be%20fully%20exploited.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20module%0Ain%20sign%20language%20recognition%20studies%2C%20called%20intra-inter%20gloss%20attention%0Amodule%2C%20to%20leverage%20the%20relationships%20among%20frames%20within%20glosses%20and%20the%0Asemantic%20and%20grammatical%20dependencies%20between%20glosses%20in%20the%20video.%20In%20the%0Aintra-gloss%20attention%20module%2C%20the%20video%20is%20divided%20into%20equally%20sized%20chunks%0Aand%20a%20self-attention%20mechanism%20is%20applied%20within%20each%20chunk.%20This%20localized%0Aself-attention%20significantly%20reduces%20complexity%20and%20eliminates%20noise%20introduced%0Aby%20considering%20non-relative%20frames.%20In%20the%20inter-gloss%20attention%20module%2C%20we%0Afirst%20aggregate%20the%20chunk-level%20features%20within%20each%20gloss%20chunk%20by%20average%0Apooling%20along%20the%20temporal%20dimension.%20Subsequently%2C%20multi-head%20self-attention%0Ais%20applied%20to%20all%20chunk-level%20features.%20Given%20the%20non-significance%20of%20the%0Asigner-environment%20interaction%2C%20we%20utilize%20segmentation%20to%20remove%20the%0Abackground%20of%20the%20videos.%20This%20enables%20the%20proposed%20model%20to%20direct%20its%20focus%0Atoward%20the%20signer.%20Experimental%20results%20on%20the%20PHOENIX-2014%20benchmark%20dataset%0Ademonstrate%20that%20our%20method%20can%20effectively%20extract%20sign%20language%20features%20in%0Aan%20end-to-end%20manner%20without%20any%20prior%20knowledge%2C%20improve%20the%20accuracy%20of%20CSLR%2C%0Aand%20achieve%20the%20word%20error%20rate%20%28WER%29%20of%2020.4%20on%20the%20test%20set%20which%20is%20a%0Acompetitive%20result%20compare%20to%20the%20state-of-the-art%20which%20uses%20additional%0Asupervisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Sign%2520Language%2520Recognition%2520Using%2520Intra-inter%2520Gloss%2520Attention%26entry.906535625%3DHossein%2520Ranjbar%2520and%2520Alireza%2520Taheri%26entry.1292438233%3D%2520%2520Many%2520continuous%2520sign%2520language%2520recognition%2520%2528CSLR%2529%2520studies%2520adopt%250Atransformer-based%2520architectures%2520for%2520sequence%2520modeling%2520due%2520to%2520their%2520powerful%250Acapacity%2520for%2520capturing%2520global%2520contexts.%2520Nevertheless%252C%2520vanilla%2520self-attention%252C%250Awhich%2520serves%2520as%2520the%2520core%2520module%2520of%2520the%2520transformer%252C%2520calculates%2520a%2520weighted%250Aaverage%2520over%2520all%2520time%2520steps%253B%2520therefore%252C%2520the%2520local%2520temporal%2520semantics%2520of%2520sign%250Avideos%2520may%2520not%2520be%2520fully%2520exploited.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520module%250Ain%2520sign%2520language%2520recognition%2520studies%252C%2520called%2520intra-inter%2520gloss%2520attention%250Amodule%252C%2520to%2520leverage%2520the%2520relationships%2520among%2520frames%2520within%2520glosses%2520and%2520the%250Asemantic%2520and%2520grammatical%2520dependencies%2520between%2520glosses%2520in%2520the%2520video.%2520In%2520the%250Aintra-gloss%2520attention%2520module%252C%2520the%2520video%2520is%2520divided%2520into%2520equally%2520sized%2520chunks%250Aand%2520a%2520self-attention%2520mechanism%2520is%2520applied%2520within%2520each%2520chunk.%2520This%2520localized%250Aself-attention%2520significantly%2520reduces%2520complexity%2520and%2520eliminates%2520noise%2520introduced%250Aby%2520considering%2520non-relative%2520frames.%2520In%2520the%2520inter-gloss%2520attention%2520module%252C%2520we%250Afirst%2520aggregate%2520the%2520chunk-level%2520features%2520within%2520each%2520gloss%2520chunk%2520by%2520average%250Apooling%2520along%2520the%2520temporal%2520dimension.%2520Subsequently%252C%2520multi-head%2520self-attention%250Ais%2520applied%2520to%2520all%2520chunk-level%2520features.%2520Given%2520the%2520non-significance%2520of%2520the%250Asigner-environment%2520interaction%252C%2520we%2520utilize%2520segmentation%2520to%2520remove%2520the%250Abackground%2520of%2520the%2520videos.%2520This%2520enables%2520the%2520proposed%2520model%2520to%2520direct%2520its%2520focus%250Atoward%2520the%2520signer.%2520Experimental%2520results%2520on%2520the%2520PHOENIX-2014%2520benchmark%2520dataset%250Ademonstrate%2520that%2520our%2520method%2520can%2520effectively%2520extract%2520sign%2520language%2520features%2520in%250Aan%2520end-to-end%2520manner%2520without%2520any%2520prior%2520knowledge%252C%2520improve%2520the%2520accuracy%2520of%2520CSLR%252C%250Aand%2520achieve%2520the%2520word%2520error%2520rate%2520%2528WER%2529%2520of%252020.4%2520on%2520the%2520test%2520set%2520which%2520is%2520a%250Acompetitive%2520result%2520compare%2520to%2520the%2520state-of-the-art%2520which%2520uses%2520additional%250Asupervisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Sign%20Language%20Recognition%20Using%20Intra-inter%20Gloss%20Attention&entry.906535625=Hossein%20Ranjbar%20and%20Alireza%20Taheri&entry.1292438233=%20%20Many%20continuous%20sign%20language%20recognition%20%28CSLR%29%20studies%20adopt%0Atransformer-based%20architectures%20for%20sequence%20modeling%20due%20to%20their%20powerful%0Acapacity%20for%20capturing%20global%20contexts.%20Nevertheless%2C%20vanilla%20self-attention%2C%0Awhich%20serves%20as%20the%20core%20module%20of%20the%20transformer%2C%20calculates%20a%20weighted%0Aaverage%20over%20all%20time%20steps%3B%20therefore%2C%20the%20local%20temporal%20semantics%20of%20sign%0Avideos%20may%20not%20be%20fully%20exploited.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20module%0Ain%20sign%20language%20recognition%20studies%2C%20called%20intra-inter%20gloss%20attention%0Amodule%2C%20to%20leverage%20the%20relationships%20among%20frames%20within%20glosses%20and%20the%0Asemantic%20and%20grammatical%20dependencies%20between%20glosses%20in%20the%20video.%20In%20the%0Aintra-gloss%20attention%20module%2C%20the%20video%20is%20divided%20into%20equally%20sized%20chunks%0Aand%20a%20self-attention%20mechanism%20is%20applied%20within%20each%20chunk.%20This%20localized%0Aself-attention%20significantly%20reduces%20complexity%20and%20eliminates%20noise%20introduced%0Aby%20considering%20non-relative%20frames.%20In%20the%20inter-gloss%20attention%20module%2C%20we%0Afirst%20aggregate%20the%20chunk-level%20features%20within%20each%20gloss%20chunk%20by%20average%0Apooling%20along%20the%20temporal%20dimension.%20Subsequently%2C%20multi-head%20self-attention%0Ais%20applied%20to%20all%20chunk-level%20features.%20Given%20the%20non-significance%20of%20the%0Asigner-environment%20interaction%2C%20we%20utilize%20segmentation%20to%20remove%20the%0Abackground%20of%20the%20videos.%20This%20enables%20the%20proposed%20model%20to%20direct%20its%20focus%0Atoward%20the%20signer.%20Experimental%20results%20on%20the%20PHOENIX-2014%20benchmark%20dataset%0Ademonstrate%20that%20our%20method%20can%20effectively%20extract%20sign%20language%20features%20in%0Aan%20end-to-end%20manner%20without%20any%20prior%20knowledge%2C%20improve%20the%20accuracy%20of%20CSLR%2C%0Aand%20achieve%20the%20word%20error%20rate%20%28WER%29%20of%2020.4%20on%20the%20test%20set%20which%20is%20a%0Acompetitive%20result%20compare%20to%20the%20state-of-the-art%20which%20uses%20additional%0Asupervisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18333v1&entry.124074799=Read"},
{"title": "On Reducing Activity with Distillation and Regularization for Energy\n  Efficient Spiking Neural Networks", "author": "Thomas Louis and Benoit Miramond and Alain Pegatoquet and Adrien Girard", "abstract": "  Interest in spiking neural networks (SNNs) has been growing steadily,\npromising an energy-efficient alternative to formal neural networks (FNNs),\ncommonly known as artificial neural networks (ANNs). Despite increasing\ninterest, especially for Edge applications, these event-driven neural networks\nsuffered from their difficulty to be trained compared to FNNs. To alleviate\nthis problem, a number of innovative methods have been developed to provide\nperformance more or less equivalent to that of FNNs. However, the spiking\nactivity of a network during inference is usually not considered. While SNNs\nmay usually have performance comparable to that of FNNs, it is often at the\ncost of an increase of the network's activity, thus limiting the benefit of\nusing them as a more energy-efficient solution.\n  In this paper, we propose to leverage Knowledge Distillation (KD) for SNNs\ntraining with surrogate gradient descent in order to optimize the trade-off\nbetween performance and spiking activity. Then, after understanding why KD led\nto an increase in sparsity, we also explored Activations regularization and\nproposed a novel method with Logits Regularization. These approaches, validated\non several datasets, clearly show a reduction in network spiking activity\n(-26.73% on GSC and -14.32% on CIFAR-10) while preserving accuracy.\n", "link": "http://arxiv.org/abs/2406.18350v1", "date": "2024-06-26", "relevancy": 2.079, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5469}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5084}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Reducing%20Activity%20with%20Distillation%20and%20Regularization%20for%20Energy%0A%20%20Efficient%20Spiking%20Neural%20Networks&body=Title%3A%20On%20Reducing%20Activity%20with%20Distillation%20and%20Regularization%20for%20Energy%0A%20%20Efficient%20Spiking%20Neural%20Networks%0AAuthor%3A%20Thomas%20Louis%20and%20Benoit%20Miramond%20and%20Alain%20Pegatoquet%20and%20Adrien%20Girard%0AAbstract%3A%20%20%20Interest%20in%20spiking%20neural%20networks%20%28SNNs%29%20has%20been%20growing%20steadily%2C%0Apromising%20an%20energy-efficient%20alternative%20to%20formal%20neural%20networks%20%28FNNs%29%2C%0Acommonly%20known%20as%20artificial%20neural%20networks%20%28ANNs%29.%20Despite%20increasing%0Ainterest%2C%20especially%20for%20Edge%20applications%2C%20these%20event-driven%20neural%20networks%0Asuffered%20from%20their%20difficulty%20to%20be%20trained%20compared%20to%20FNNs.%20To%20alleviate%0Athis%20problem%2C%20a%20number%20of%20innovative%20methods%20have%20been%20developed%20to%20provide%0Aperformance%20more%20or%20less%20equivalent%20to%20that%20of%20FNNs.%20However%2C%20the%20spiking%0Aactivity%20of%20a%20network%20during%20inference%20is%20usually%20not%20considered.%20While%20SNNs%0Amay%20usually%20have%20performance%20comparable%20to%20that%20of%20FNNs%2C%20it%20is%20often%20at%20the%0Acost%20of%20an%20increase%20of%20the%20network%27s%20activity%2C%20thus%20limiting%20the%20benefit%20of%0Ausing%20them%20as%20a%20more%20energy-efficient%20solution.%0A%20%20In%20this%20paper%2C%20we%20propose%20to%20leverage%20Knowledge%20Distillation%20%28KD%29%20for%20SNNs%0Atraining%20with%20surrogate%20gradient%20descent%20in%20order%20to%20optimize%20the%20trade-off%0Abetween%20performance%20and%20spiking%20activity.%20Then%2C%20after%20understanding%20why%20KD%20led%0Ato%20an%20increase%20in%20sparsity%2C%20we%20also%20explored%20Activations%20regularization%20and%0Aproposed%20a%20novel%20method%20with%20Logits%20Regularization.%20These%20approaches%2C%20validated%0Aon%20several%20datasets%2C%20clearly%20show%20a%20reduction%20in%20network%20spiking%20activity%0A%28-26.73%25%20on%20GSC%20and%20-14.32%25%20on%20CIFAR-10%29%20while%20preserving%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Reducing%2520Activity%2520with%2520Distillation%2520and%2520Regularization%2520for%2520Energy%250A%2520%2520Efficient%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DThomas%2520Louis%2520and%2520Benoit%2520Miramond%2520and%2520Alain%2520Pegatoquet%2520and%2520Adrien%2520Girard%26entry.1292438233%3D%2520%2520Interest%2520in%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%2520has%2520been%2520growing%2520steadily%252C%250Apromising%2520an%2520energy-efficient%2520alternative%2520to%2520formal%2520neural%2520networks%2520%2528FNNs%2529%252C%250Acommonly%2520known%2520as%2520artificial%2520neural%2520networks%2520%2528ANNs%2529.%2520Despite%2520increasing%250Ainterest%252C%2520especially%2520for%2520Edge%2520applications%252C%2520these%2520event-driven%2520neural%2520networks%250Asuffered%2520from%2520their%2520difficulty%2520to%2520be%2520trained%2520compared%2520to%2520FNNs.%2520To%2520alleviate%250Athis%2520problem%252C%2520a%2520number%2520of%2520innovative%2520methods%2520have%2520been%2520developed%2520to%2520provide%250Aperformance%2520more%2520or%2520less%2520equivalent%2520to%2520that%2520of%2520FNNs.%2520However%252C%2520the%2520spiking%250Aactivity%2520of%2520a%2520network%2520during%2520inference%2520is%2520usually%2520not%2520considered.%2520While%2520SNNs%250Amay%2520usually%2520have%2520performance%2520comparable%2520to%2520that%2520of%2520FNNs%252C%2520it%2520is%2520often%2520at%2520the%250Acost%2520of%2520an%2520increase%2520of%2520the%2520network%2527s%2520activity%252C%2520thus%2520limiting%2520the%2520benefit%2520of%250Ausing%2520them%2520as%2520a%2520more%2520energy-efficient%2520solution.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520leverage%2520Knowledge%2520Distillation%2520%2528KD%2529%2520for%2520SNNs%250Atraining%2520with%2520surrogate%2520gradient%2520descent%2520in%2520order%2520to%2520optimize%2520the%2520trade-off%250Abetween%2520performance%2520and%2520spiking%2520activity.%2520Then%252C%2520after%2520understanding%2520why%2520KD%2520led%250Ato%2520an%2520increase%2520in%2520sparsity%252C%2520we%2520also%2520explored%2520Activations%2520regularization%2520and%250Aproposed%2520a%2520novel%2520method%2520with%2520Logits%2520Regularization.%2520These%2520approaches%252C%2520validated%250Aon%2520several%2520datasets%252C%2520clearly%2520show%2520a%2520reduction%2520in%2520network%2520spiking%2520activity%250A%2528-26.73%2525%2520on%2520GSC%2520and%2520-14.32%2525%2520on%2520CIFAR-10%2529%2520while%2520preserving%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Reducing%20Activity%20with%20Distillation%20and%20Regularization%20for%20Energy%0A%20%20Efficient%20Spiking%20Neural%20Networks&entry.906535625=Thomas%20Louis%20and%20Benoit%20Miramond%20and%20Alain%20Pegatoquet%20and%20Adrien%20Girard&entry.1292438233=%20%20Interest%20in%20spiking%20neural%20networks%20%28SNNs%29%20has%20been%20growing%20steadily%2C%0Apromising%20an%20energy-efficient%20alternative%20to%20formal%20neural%20networks%20%28FNNs%29%2C%0Acommonly%20known%20as%20artificial%20neural%20networks%20%28ANNs%29.%20Despite%20increasing%0Ainterest%2C%20especially%20for%20Edge%20applications%2C%20these%20event-driven%20neural%20networks%0Asuffered%20from%20their%20difficulty%20to%20be%20trained%20compared%20to%20FNNs.%20To%20alleviate%0Athis%20problem%2C%20a%20number%20of%20innovative%20methods%20have%20been%20developed%20to%20provide%0Aperformance%20more%20or%20less%20equivalent%20to%20that%20of%20FNNs.%20However%2C%20the%20spiking%0Aactivity%20of%20a%20network%20during%20inference%20is%20usually%20not%20considered.%20While%20SNNs%0Amay%20usually%20have%20performance%20comparable%20to%20that%20of%20FNNs%2C%20it%20is%20often%20at%20the%0Acost%20of%20an%20increase%20of%20the%20network%27s%20activity%2C%20thus%20limiting%20the%20benefit%20of%0Ausing%20them%20as%20a%20more%20energy-efficient%20solution.%0A%20%20In%20this%20paper%2C%20we%20propose%20to%20leverage%20Knowledge%20Distillation%20%28KD%29%20for%20SNNs%0Atraining%20with%20surrogate%20gradient%20descent%20in%20order%20to%20optimize%20the%20trade-off%0Abetween%20performance%20and%20spiking%20activity.%20Then%2C%20after%20understanding%20why%20KD%20led%0Ato%20an%20increase%20in%20sparsity%2C%20we%20also%20explored%20Activations%20regularization%20and%0Aproposed%20a%20novel%20method%20with%20Logits%20Regularization.%20These%20approaches%2C%20validated%0Aon%20several%20datasets%2C%20clearly%20show%20a%20reduction%20in%20network%20spiking%20activity%0A%28-26.73%25%20on%20GSC%20and%20-14.32%25%20on%20CIFAR-10%29%20while%20preserving%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18350v1&entry.124074799=Read"},
{"title": "DiffuseHigh: Training-free Progressive High-Resolution Image Synthesis\n  through Structure Guidance", "author": "Younghyun Kim and Geunmin Hwang and Eunbyung Park", "abstract": "  Recent surge in large-scale generative models has spurred the development of\nvast fields in computer vision. In particular, text-to-image diffusion models\nhave garnered widespread adoption across diverse domain due to their potential\nfor high-fidelity image generation. Nonetheless, existing large-scale diffusion\nmodels are confined to generate images of up to 1K resolution, which is far\nfrom meeting the demands of contemporary commercial applications. Directly\nsampling higher-resolution images often yields results marred by artifacts such\nas object repetition and distorted shapes. Addressing the aforementioned issues\ntypically necessitates training or fine-tuning models on higher resolution\ndatasets. However, this undertaking poses a formidable challenge due to the\ndifficulty in collecting large-scale high-resolution contents and substantial\ncomputational resources. While several preceding works have proposed\nalternatives, they often fail to produce convincing results. In this work, we\nprobe the generative ability of diffusion models at higher resolution beyond\nits original capability and propose a novel progressive approach that fully\nutilizes generated low-resolution image to guide the generation of higher\nresolution image. Our method obviates the need for additional training or\nfine-tuning which significantly lowers the burden of computational costs.\nExtensive experiments and results validate the efficiency and efficacy of our\nmethod.\n", "link": "http://arxiv.org/abs/2406.18459v1", "date": "2024-06-26", "relevancy": 2.0765, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7055}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6914}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffuseHigh%3A%20Training-free%20Progressive%20High-Resolution%20Image%20Synthesis%0A%20%20through%20Structure%20Guidance&body=Title%3A%20DiffuseHigh%3A%20Training-free%20Progressive%20High-Resolution%20Image%20Synthesis%0A%20%20through%20Structure%20Guidance%0AAuthor%3A%20Younghyun%20Kim%20and%20Geunmin%20Hwang%20and%20Eunbyung%20Park%0AAbstract%3A%20%20%20Recent%20surge%20in%20large-scale%20generative%20models%20has%20spurred%20the%20development%20of%0Avast%20fields%20in%20computer%20vision.%20In%20particular%2C%20text-to-image%20diffusion%20models%0Ahave%20garnered%20widespread%20adoption%20across%20diverse%20domain%20due%20to%20their%20potential%0Afor%20high-fidelity%20image%20generation.%20Nonetheless%2C%20existing%20large-scale%20diffusion%0Amodels%20are%20confined%20to%20generate%20images%20of%20up%20to%201K%20resolution%2C%20which%20is%20far%0Afrom%20meeting%20the%20demands%20of%20contemporary%20commercial%20applications.%20Directly%0Asampling%20higher-resolution%20images%20often%20yields%20results%20marred%20by%20artifacts%20such%0Aas%20object%20repetition%20and%20distorted%20shapes.%20Addressing%20the%20aforementioned%20issues%0Atypically%20necessitates%20training%20or%20fine-tuning%20models%20on%20higher%20resolution%0Adatasets.%20However%2C%20this%20undertaking%20poses%20a%20formidable%20challenge%20due%20to%20the%0Adifficulty%20in%20collecting%20large-scale%20high-resolution%20contents%20and%20substantial%0Acomputational%20resources.%20While%20several%20preceding%20works%20have%20proposed%0Aalternatives%2C%20they%20often%20fail%20to%20produce%20convincing%20results.%20In%20this%20work%2C%20we%0Aprobe%20the%20generative%20ability%20of%20diffusion%20models%20at%20higher%20resolution%20beyond%0Aits%20original%20capability%20and%20propose%20a%20novel%20progressive%20approach%20that%20fully%0Autilizes%20generated%20low-resolution%20image%20to%20guide%20the%20generation%20of%20higher%0Aresolution%20image.%20Our%20method%20obviates%20the%20need%20for%20additional%20training%20or%0Afine-tuning%20which%20significantly%20lowers%20the%20burden%20of%20computational%20costs.%0AExtensive%20experiments%20and%20results%20validate%20the%20efficiency%20and%20efficacy%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffuseHigh%253A%2520Training-free%2520Progressive%2520High-Resolution%2520Image%2520Synthesis%250A%2520%2520through%2520Structure%2520Guidance%26entry.906535625%3DYounghyun%2520Kim%2520and%2520Geunmin%2520Hwang%2520and%2520Eunbyung%2520Park%26entry.1292438233%3D%2520%2520Recent%2520surge%2520in%2520large-scale%2520generative%2520models%2520has%2520spurred%2520the%2520development%2520of%250Avast%2520fields%2520in%2520computer%2520vision.%2520In%2520particular%252C%2520text-to-image%2520diffusion%2520models%250Ahave%2520garnered%2520widespread%2520adoption%2520across%2520diverse%2520domain%2520due%2520to%2520their%2520potential%250Afor%2520high-fidelity%2520image%2520generation.%2520Nonetheless%252C%2520existing%2520large-scale%2520diffusion%250Amodels%2520are%2520confined%2520to%2520generate%2520images%2520of%2520up%2520to%25201K%2520resolution%252C%2520which%2520is%2520far%250Afrom%2520meeting%2520the%2520demands%2520of%2520contemporary%2520commercial%2520applications.%2520Directly%250Asampling%2520higher-resolution%2520images%2520often%2520yields%2520results%2520marred%2520by%2520artifacts%2520such%250Aas%2520object%2520repetition%2520and%2520distorted%2520shapes.%2520Addressing%2520the%2520aforementioned%2520issues%250Atypically%2520necessitates%2520training%2520or%2520fine-tuning%2520models%2520on%2520higher%2520resolution%250Adatasets.%2520However%252C%2520this%2520undertaking%2520poses%2520a%2520formidable%2520challenge%2520due%2520to%2520the%250Adifficulty%2520in%2520collecting%2520large-scale%2520high-resolution%2520contents%2520and%2520substantial%250Acomputational%2520resources.%2520While%2520several%2520preceding%2520works%2520have%2520proposed%250Aalternatives%252C%2520they%2520often%2520fail%2520to%2520produce%2520convincing%2520results.%2520In%2520this%2520work%252C%2520we%250Aprobe%2520the%2520generative%2520ability%2520of%2520diffusion%2520models%2520at%2520higher%2520resolution%2520beyond%250Aits%2520original%2520capability%2520and%2520propose%2520a%2520novel%2520progressive%2520approach%2520that%2520fully%250Autilizes%2520generated%2520low-resolution%2520image%2520to%2520guide%2520the%2520generation%2520of%2520higher%250Aresolution%2520image.%2520Our%2520method%2520obviates%2520the%2520need%2520for%2520additional%2520training%2520or%250Afine-tuning%2520which%2520significantly%2520lowers%2520the%2520burden%2520of%2520computational%2520costs.%250AExtensive%2520experiments%2520and%2520results%2520validate%2520the%2520efficiency%2520and%2520efficacy%2520of%2520our%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffuseHigh%3A%20Training-free%20Progressive%20High-Resolution%20Image%20Synthesis%0A%20%20through%20Structure%20Guidance&entry.906535625=Younghyun%20Kim%20and%20Geunmin%20Hwang%20and%20Eunbyung%20Park&entry.1292438233=%20%20Recent%20surge%20in%20large-scale%20generative%20models%20has%20spurred%20the%20development%20of%0Avast%20fields%20in%20computer%20vision.%20In%20particular%2C%20text-to-image%20diffusion%20models%0Ahave%20garnered%20widespread%20adoption%20across%20diverse%20domain%20due%20to%20their%20potential%0Afor%20high-fidelity%20image%20generation.%20Nonetheless%2C%20existing%20large-scale%20diffusion%0Amodels%20are%20confined%20to%20generate%20images%20of%20up%20to%201K%20resolution%2C%20which%20is%20far%0Afrom%20meeting%20the%20demands%20of%20contemporary%20commercial%20applications.%20Directly%0Asampling%20higher-resolution%20images%20often%20yields%20results%20marred%20by%20artifacts%20such%0Aas%20object%20repetition%20and%20distorted%20shapes.%20Addressing%20the%20aforementioned%20issues%0Atypically%20necessitates%20training%20or%20fine-tuning%20models%20on%20higher%20resolution%0Adatasets.%20However%2C%20this%20undertaking%20poses%20a%20formidable%20challenge%20due%20to%20the%0Adifficulty%20in%20collecting%20large-scale%20high-resolution%20contents%20and%20substantial%0Acomputational%20resources.%20While%20several%20preceding%20works%20have%20proposed%0Aalternatives%2C%20they%20often%20fail%20to%20produce%20convincing%20results.%20In%20this%20work%2C%20we%0Aprobe%20the%20generative%20ability%20of%20diffusion%20models%20at%20higher%20resolution%20beyond%0Aits%20original%20capability%20and%20propose%20a%20novel%20progressive%20approach%20that%20fully%0Autilizes%20generated%20low-resolution%20image%20to%20guide%20the%20generation%20of%20higher%0Aresolution%20image.%20Our%20method%20obviates%20the%20need%20for%20additional%20training%20or%0Afine-tuning%20which%20significantly%20lowers%20the%20burden%20of%20computational%20costs.%0AExtensive%20experiments%20and%20results%20validate%20the%20efficiency%20and%20efficacy%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18459v1&entry.124074799=Read"},
{"title": "MedMNIST-C: Comprehensive benchmark and improved classifier robustness\n  by simulating realistic image corruptions", "author": "Francesco Di Salvo and Sebastian Doerrich and Christian Ledig", "abstract": "  The integration of neural-network-based systems into clinical practice is\nlimited by challenges related to domain generalization and robustness. The\ncomputer vision community established benchmarks such as ImageNet-C as a\nfundamental prerequisite to measure progress towards those challenges. Similar\ndatasets are largely absent in the medical imaging community which lacks a\ncomprehensive benchmark that spans across imaging modalities and applications.\nTo address this gap, we create and open-source MedMNIST-C, a benchmark dataset\nbased on the MedMNIST+ collection covering 12 datasets and 9 imaging\nmodalities. We simulate task and modality-specific image corruptions of varying\nseverity to comprehensively evaluate the robustness of established algorithms\nagainst real-world artifacts and distribution shifts. We further provide\nquantitative evidence that our simple-to-use artificial corruptions allow for\nhighly performant, lightweight data augmentation to enhance model robustness.\nUnlike traditional, generic augmentation strategies, our approach leverages\ndomain knowledge, exhibiting significantly higher robustness when compared to\nwidely adopted methods. By introducing MedMNIST-C and open-sourcing the\ncorresponding library allowing for targeted data augmentations, we contribute\nto the development of increasingly robust methods tailored to the challenges of\nmedical imaging. The code is available at\nhttps://github.com/francescodisalvo05/medmnistc-api}{github.com/francescodisalvo05/medmnistc-api .\n", "link": "http://arxiv.org/abs/2406.17536v2", "date": "2024-06-26", "relevancy": 2.0478, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5373}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5151}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedMNIST-C%3A%20Comprehensive%20benchmark%20and%20improved%20classifier%20robustness%0A%20%20by%20simulating%20realistic%20image%20corruptions&body=Title%3A%20MedMNIST-C%3A%20Comprehensive%20benchmark%20and%20improved%20classifier%20robustness%0A%20%20by%20simulating%20realistic%20image%20corruptions%0AAuthor%3A%20Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Christian%20Ledig%0AAbstract%3A%20%20%20The%20integration%20of%20neural-network-based%20systems%20into%20clinical%20practice%20is%0Alimited%20by%20challenges%20related%20to%20domain%20generalization%20and%20robustness.%20The%0Acomputer%20vision%20community%20established%20benchmarks%20such%20as%20ImageNet-C%20as%20a%0Afundamental%20prerequisite%20to%20measure%20progress%20towards%20those%20challenges.%20Similar%0Adatasets%20are%20largely%20absent%20in%20the%20medical%20imaging%20community%20which%20lacks%20a%0Acomprehensive%20benchmark%20that%20spans%20across%20imaging%20modalities%20and%20applications.%0ATo%20address%20this%20gap%2C%20we%20create%20and%20open-source%20MedMNIST-C%2C%20a%20benchmark%20dataset%0Abased%20on%20the%20MedMNIST%2B%20collection%20covering%2012%20datasets%20and%209%20imaging%0Amodalities.%20We%20simulate%20task%20and%20modality-specific%20image%20corruptions%20of%20varying%0Aseverity%20to%20comprehensively%20evaluate%20the%20robustness%20of%20established%20algorithms%0Aagainst%20real-world%20artifacts%20and%20distribution%20shifts.%20We%20further%20provide%0Aquantitative%20evidence%20that%20our%20simple-to-use%20artificial%20corruptions%20allow%20for%0Ahighly%20performant%2C%20lightweight%20data%20augmentation%20to%20enhance%20model%20robustness.%0AUnlike%20traditional%2C%20generic%20augmentation%20strategies%2C%20our%20approach%20leverages%0Adomain%20knowledge%2C%20exhibiting%20significantly%20higher%20robustness%20when%20compared%20to%0Awidely%20adopted%20methods.%20By%20introducing%20MedMNIST-C%20and%20open-sourcing%20the%0Acorresponding%20library%20allowing%20for%20targeted%20data%20augmentations%2C%20we%20contribute%0Ato%20the%20development%20of%20increasingly%20robust%20methods%20tailored%20to%20the%20challenges%20of%0Amedical%20imaging.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/medmnistc-api%7D%7Bgithub.com/francescodisalvo05/medmnistc-api%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17536v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedMNIST-C%253A%2520Comprehensive%2520benchmark%2520and%2520improved%2520classifier%2520robustness%250A%2520%2520by%2520simulating%2520realistic%2520image%2520corruptions%26entry.906535625%3DFrancesco%2520Di%2520Salvo%2520and%2520Sebastian%2520Doerrich%2520and%2520Christian%2520Ledig%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520neural-network-based%2520systems%2520into%2520clinical%2520practice%2520is%250Alimited%2520by%2520challenges%2520related%2520to%2520domain%2520generalization%2520and%2520robustness.%2520The%250Acomputer%2520vision%2520community%2520established%2520benchmarks%2520such%2520as%2520ImageNet-C%2520as%2520a%250Afundamental%2520prerequisite%2520to%2520measure%2520progress%2520towards%2520those%2520challenges.%2520Similar%250Adatasets%2520are%2520largely%2520absent%2520in%2520the%2520medical%2520imaging%2520community%2520which%2520lacks%2520a%250Acomprehensive%2520benchmark%2520that%2520spans%2520across%2520imaging%2520modalities%2520and%2520applications.%250ATo%2520address%2520this%2520gap%252C%2520we%2520create%2520and%2520open-source%2520MedMNIST-C%252C%2520a%2520benchmark%2520dataset%250Abased%2520on%2520the%2520MedMNIST%252B%2520collection%2520covering%252012%2520datasets%2520and%25209%2520imaging%250Amodalities.%2520We%2520simulate%2520task%2520and%2520modality-specific%2520image%2520corruptions%2520of%2520varying%250Aseverity%2520to%2520comprehensively%2520evaluate%2520the%2520robustness%2520of%2520established%2520algorithms%250Aagainst%2520real-world%2520artifacts%2520and%2520distribution%2520shifts.%2520We%2520further%2520provide%250Aquantitative%2520evidence%2520that%2520our%2520simple-to-use%2520artificial%2520corruptions%2520allow%2520for%250Ahighly%2520performant%252C%2520lightweight%2520data%2520augmentation%2520to%2520enhance%2520model%2520robustness.%250AUnlike%2520traditional%252C%2520generic%2520augmentation%2520strategies%252C%2520our%2520approach%2520leverages%250Adomain%2520knowledge%252C%2520exhibiting%2520significantly%2520higher%2520robustness%2520when%2520compared%2520to%250Awidely%2520adopted%2520methods.%2520By%2520introducing%2520MedMNIST-C%2520and%2520open-sourcing%2520the%250Acorresponding%2520library%2520allowing%2520for%2520targeted%2520data%2520augmentations%252C%2520we%2520contribute%250Ato%2520the%2520development%2520of%2520increasingly%2520robust%2520methods%2520tailored%2520to%2520the%2520challenges%2520of%250Amedical%2520imaging.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/francescodisalvo05/medmnistc-api%257D%257Bgithub.com/francescodisalvo05/medmnistc-api%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17536v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedMNIST-C%3A%20Comprehensive%20benchmark%20and%20improved%20classifier%20robustness%0A%20%20by%20simulating%20realistic%20image%20corruptions&entry.906535625=Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Christian%20Ledig&entry.1292438233=%20%20The%20integration%20of%20neural-network-based%20systems%20into%20clinical%20practice%20is%0Alimited%20by%20challenges%20related%20to%20domain%20generalization%20and%20robustness.%20The%0Acomputer%20vision%20community%20established%20benchmarks%20such%20as%20ImageNet-C%20as%20a%0Afundamental%20prerequisite%20to%20measure%20progress%20towards%20those%20challenges.%20Similar%0Adatasets%20are%20largely%20absent%20in%20the%20medical%20imaging%20community%20which%20lacks%20a%0Acomprehensive%20benchmark%20that%20spans%20across%20imaging%20modalities%20and%20applications.%0ATo%20address%20this%20gap%2C%20we%20create%20and%20open-source%20MedMNIST-C%2C%20a%20benchmark%20dataset%0Abased%20on%20the%20MedMNIST%2B%20collection%20covering%2012%20datasets%20and%209%20imaging%0Amodalities.%20We%20simulate%20task%20and%20modality-specific%20image%20corruptions%20of%20varying%0Aseverity%20to%20comprehensively%20evaluate%20the%20robustness%20of%20established%20algorithms%0Aagainst%20real-world%20artifacts%20and%20distribution%20shifts.%20We%20further%20provide%0Aquantitative%20evidence%20that%20our%20simple-to-use%20artificial%20corruptions%20allow%20for%0Ahighly%20performant%2C%20lightweight%20data%20augmentation%20to%20enhance%20model%20robustness.%0AUnlike%20traditional%2C%20generic%20augmentation%20strategies%2C%20our%20approach%20leverages%0Adomain%20knowledge%2C%20exhibiting%20significantly%20higher%20robustness%20when%20compared%20to%0Awidely%20adopted%20methods.%20By%20introducing%20MedMNIST-C%20and%20open-sourcing%20the%0Acorresponding%20library%20allowing%20for%20targeted%20data%20augmentations%2C%20we%20contribute%0Ato%20the%20development%20of%20increasingly%20robust%20methods%20tailored%20to%20the%20challenges%20of%0Amedical%20imaging.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/medmnistc-api%7D%7Bgithub.com/francescodisalvo05/medmnistc-api%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17536v2&entry.124074799=Read"},
{"title": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in\n  Deep Robust Classifiers", "author": "Jonas Ngnaw\u00e9 and Sabyasachi Sahoo and Yann Pequignot and Fr\u00e9d\u00e9ric Precioso and Christian Gagn\u00e9", "abstract": "  Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate strong margin consistency\nwith a strong correlation between their input space margins and the logit\nmargins. Then, we show that we can effectively use the logit margin to\nconfidently detect brittle decisions with such models and accurately estimate\nrobust accuracy on an arbitrarily large test set by estimating the input\nmargins only on a small subset. Finally, we address cases where the model is\nnot sufficiently margin-consistent by learning a pseudo-margin from the feature\nrepresentation. Our findings highlight the potential of leveraging deep\nrepresentations to efficiently assess adversarial vulnerability in deployment\nscenarios.\n", "link": "http://arxiv.org/abs/2406.18451v1", "date": "2024-06-26", "relevancy": 2.0299, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Brittle%20Decisions%20for%20Free%3A%20Leveraging%20Margin%20Consistency%20in%0A%20%20Deep%20Robust%20Classifiers&body=Title%3A%20Detecting%20Brittle%20Decisions%20for%20Free%3A%20Leveraging%20Margin%20Consistency%20in%0A%20%20Deep%20Robust%20Classifiers%0AAuthor%3A%20Jonas%20Ngnaw%C3%A9%20and%20Sabyasachi%20Sahoo%20and%20Yann%20Pequignot%20and%20Fr%C3%A9d%C3%A9ric%20Precioso%20and%20Christian%20Gagn%C3%A9%0AAbstract%3A%20%20%20Despite%20extensive%20research%20on%20adversarial%20training%20strategies%20to%20improve%0Arobustness%2C%20the%20decisions%20of%20even%20the%20most%20robust%20deep%20learning%20models%20can%0Astill%20be%20quite%20sensitive%20to%20imperceptible%20perturbations%2C%20creating%20serious%20risks%0Awhen%20deploying%20them%20for%20high-stakes%20real-world%20applications.%20While%20detecting%0Asuch%20cases%20may%20be%20critical%2C%20evaluating%20a%20model%27s%20vulnerability%20at%20a%0Aper-instance%20level%20using%20adversarial%20attacks%20is%20computationally%20too%20intensive%0Aand%20unsuitable%20for%20real-time%20deployment%20scenarios.%20The%20input%20space%20margin%20is%0Athe%20exact%20score%20to%20detect%20non-robust%20samples%20and%20is%20intractable%20for%20deep%20neural%0Anetworks.%20This%20paper%20introduces%20the%20concept%20of%20margin%20consistency%20--%20a%20property%0Athat%20links%20the%20input%20space%20margins%20and%20the%20logit%20margins%20in%20robust%20models%20--%0Afor%20efficient%20detection%20of%20vulnerable%20samples.%20First%2C%20we%20establish%20that%20margin%0Aconsistency%20is%20a%20necessary%20and%20sufficient%20condition%20to%20use%20a%20model%27s%20logit%0Amargin%20as%20a%20score%20for%20identifying%20non-robust%20samples.%20Next%2C%20through%0Acomprehensive%20empirical%20analysis%20of%20various%20robustly%20trained%20models%20on%20CIFAR10%0Aand%20CIFAR100%20datasets%2C%20we%20show%20that%20they%20indicate%20strong%20margin%20consistency%0Awith%20a%20strong%20correlation%20between%20their%20input%20space%20margins%20and%20the%20logit%0Amargins.%20Then%2C%20we%20show%20that%20we%20can%20effectively%20use%20the%20logit%20margin%20to%0Aconfidently%20detect%20brittle%20decisions%20with%20such%20models%20and%20accurately%20estimate%0Arobust%20accuracy%20on%20an%20arbitrarily%20large%20test%20set%20by%20estimating%20the%20input%0Amargins%20only%20on%20a%20small%20subset.%20Finally%2C%20we%20address%20cases%20where%20the%20model%20is%0Anot%20sufficiently%20margin-consistent%20by%20learning%20a%20pseudo-margin%20from%20the%20feature%0Arepresentation.%20Our%20findings%20highlight%20the%20potential%20of%20leveraging%20deep%0Arepresentations%20to%20efficiently%20assess%20adversarial%20vulnerability%20in%20deployment%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Brittle%2520Decisions%2520for%2520Free%253A%2520Leveraging%2520Margin%2520Consistency%2520in%250A%2520%2520Deep%2520Robust%2520Classifiers%26entry.906535625%3DJonas%2520Ngnaw%25C3%25A9%2520and%2520Sabyasachi%2520Sahoo%2520and%2520Yann%2520Pequignot%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Precioso%2520and%2520Christian%2520Gagn%25C3%25A9%26entry.1292438233%3D%2520%2520Despite%2520extensive%2520research%2520on%2520adversarial%2520training%2520strategies%2520to%2520improve%250Arobustness%252C%2520the%2520decisions%2520of%2520even%2520the%2520most%2520robust%2520deep%2520learning%2520models%2520can%250Astill%2520be%2520quite%2520sensitive%2520to%2520imperceptible%2520perturbations%252C%2520creating%2520serious%2520risks%250Awhen%2520deploying%2520them%2520for%2520high-stakes%2520real-world%2520applications.%2520While%2520detecting%250Asuch%2520cases%2520may%2520be%2520critical%252C%2520evaluating%2520a%2520model%2527s%2520vulnerability%2520at%2520a%250Aper-instance%2520level%2520using%2520adversarial%2520attacks%2520is%2520computationally%2520too%2520intensive%250Aand%2520unsuitable%2520for%2520real-time%2520deployment%2520scenarios.%2520The%2520input%2520space%2520margin%2520is%250Athe%2520exact%2520score%2520to%2520detect%2520non-robust%2520samples%2520and%2520is%2520intractable%2520for%2520deep%2520neural%250Anetworks.%2520This%2520paper%2520introduces%2520the%2520concept%2520of%2520margin%2520consistency%2520--%2520a%2520property%250Athat%2520links%2520the%2520input%2520space%2520margins%2520and%2520the%2520logit%2520margins%2520in%2520robust%2520models%2520--%250Afor%2520efficient%2520detection%2520of%2520vulnerable%2520samples.%2520First%252C%2520we%2520establish%2520that%2520margin%250Aconsistency%2520is%2520a%2520necessary%2520and%2520sufficient%2520condition%2520to%2520use%2520a%2520model%2527s%2520logit%250Amargin%2520as%2520a%2520score%2520for%2520identifying%2520non-robust%2520samples.%2520Next%252C%2520through%250Acomprehensive%2520empirical%2520analysis%2520of%2520various%2520robustly%2520trained%2520models%2520on%2520CIFAR10%250Aand%2520CIFAR100%2520datasets%252C%2520we%2520show%2520that%2520they%2520indicate%2520strong%2520margin%2520consistency%250Awith%2520a%2520strong%2520correlation%2520between%2520their%2520input%2520space%2520margins%2520and%2520the%2520logit%250Amargins.%2520Then%252C%2520we%2520show%2520that%2520we%2520can%2520effectively%2520use%2520the%2520logit%2520margin%2520to%250Aconfidently%2520detect%2520brittle%2520decisions%2520with%2520such%2520models%2520and%2520accurately%2520estimate%250Arobust%2520accuracy%2520on%2520an%2520arbitrarily%2520large%2520test%2520set%2520by%2520estimating%2520the%2520input%250Amargins%2520only%2520on%2520a%2520small%2520subset.%2520Finally%252C%2520we%2520address%2520cases%2520where%2520the%2520model%2520is%250Anot%2520sufficiently%2520margin-consistent%2520by%2520learning%2520a%2520pseudo-margin%2520from%2520the%2520feature%250Arepresentation.%2520Our%2520findings%2520highlight%2520the%2520potential%2520of%2520leveraging%2520deep%250Arepresentations%2520to%2520efficiently%2520assess%2520adversarial%2520vulnerability%2520in%2520deployment%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Brittle%20Decisions%20for%20Free%3A%20Leveraging%20Margin%20Consistency%20in%0A%20%20Deep%20Robust%20Classifiers&entry.906535625=Jonas%20Ngnaw%C3%A9%20and%20Sabyasachi%20Sahoo%20and%20Yann%20Pequignot%20and%20Fr%C3%A9d%C3%A9ric%20Precioso%20and%20Christian%20Gagn%C3%A9&entry.1292438233=%20%20Despite%20extensive%20research%20on%20adversarial%20training%20strategies%20to%20improve%0Arobustness%2C%20the%20decisions%20of%20even%20the%20most%20robust%20deep%20learning%20models%20can%0Astill%20be%20quite%20sensitive%20to%20imperceptible%20perturbations%2C%20creating%20serious%20risks%0Awhen%20deploying%20them%20for%20high-stakes%20real-world%20applications.%20While%20detecting%0Asuch%20cases%20may%20be%20critical%2C%20evaluating%20a%20model%27s%20vulnerability%20at%20a%0Aper-instance%20level%20using%20adversarial%20attacks%20is%20computationally%20too%20intensive%0Aand%20unsuitable%20for%20real-time%20deployment%20scenarios.%20The%20input%20space%20margin%20is%0Athe%20exact%20score%20to%20detect%20non-robust%20samples%20and%20is%20intractable%20for%20deep%20neural%0Anetworks.%20This%20paper%20introduces%20the%20concept%20of%20margin%20consistency%20--%20a%20property%0Athat%20links%20the%20input%20space%20margins%20and%20the%20logit%20margins%20in%20robust%20models%20--%0Afor%20efficient%20detection%20of%20vulnerable%20samples.%20First%2C%20we%20establish%20that%20margin%0Aconsistency%20is%20a%20necessary%20and%20sufficient%20condition%20to%20use%20a%20model%27s%20logit%0Amargin%20as%20a%20score%20for%20identifying%20non-robust%20samples.%20Next%2C%20through%0Acomprehensive%20empirical%20analysis%20of%20various%20robustly%20trained%20models%20on%20CIFAR10%0Aand%20CIFAR100%20datasets%2C%20we%20show%20that%20they%20indicate%20strong%20margin%20consistency%0Awith%20a%20strong%20correlation%20between%20their%20input%20space%20margins%20and%20the%20logit%0Amargins.%20Then%2C%20we%20show%20that%20we%20can%20effectively%20use%20the%20logit%20margin%20to%0Aconfidently%20detect%20brittle%20decisions%20with%20such%20models%20and%20accurately%20estimate%0Arobust%20accuracy%20on%20an%20arbitrarily%20large%20test%20set%20by%20estimating%20the%20input%0Amargins%20only%20on%20a%20small%20subset.%20Finally%2C%20we%20address%20cases%20where%20the%20model%20is%0Anot%20sufficiently%20margin-consistent%20by%20learning%20a%20pseudo-margin%20from%20the%20feature%0Arepresentation.%20Our%20findings%20highlight%20the%20potential%20of%20leveraging%20deep%0Arepresentations%20to%20efficiently%20assess%20adversarial%20vulnerability%20in%20deployment%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18451v1&entry.124074799=Read"},
{"title": "EmT: A Novel Transformer for Generalized Cross-subject EEG Emotion\n  Recognition", "author": "Yi Ding and Chengxuan Tong and Shuailei Zhang and Muyun Jiang and Yong Li and Kevin Lim Jun Liang and Cuntai Guan", "abstract": "  Integrating prior knowledge of neurophysiology into neural network\narchitecture enhances the performance of emotion decoding. While numerous\ntechniques emphasize learning spatial and short-term temporal patterns, there\nhas been limited emphasis on capturing the vital long-term contextual\ninformation associated with emotional cognitive processes. In order to address\nthis discrepancy, we introduce a novel transformer model called emotion\ntransformer (EmT). EmT is designed to excel in both generalized cross-subject\nEEG emotion classification and regression tasks. In EmT, EEG signals are\ntransformed into a temporal graph format, creating a sequence of EEG feature\ngraphs using a temporal graph construction module (TGC). A novel residual\nmulti-view pyramid GCN module (RMPG) is then proposed to learn dynamic graph\nrepresentations for each EEG feature graph within the series, and the learned\nrepresentations of each graph are fused into one token. Furthermore, we design\na temporal contextual transformer module (TCT) with two types of token mixers\nto learn the temporal contextual information. Finally, the task-specific output\nmodule (TSO) generates the desired outputs. Experiments on four publicly\navailable datasets show that EmT achieves higher results than the baseline\nmethods for both EEG emotion classification and regression tasks. The code is\navailable at https://github.com/yi-ding-cs/EmT.\n", "link": "http://arxiv.org/abs/2406.18345v1", "date": "2024-06-26", "relevancy": 2.0165, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5104}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5067}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmT%3A%20A%20Novel%20Transformer%20for%20Generalized%20Cross-subject%20EEG%20Emotion%0A%20%20Recognition&body=Title%3A%20EmT%3A%20A%20Novel%20Transformer%20for%20Generalized%20Cross-subject%20EEG%20Emotion%0A%20%20Recognition%0AAuthor%3A%20Yi%20Ding%20and%20Chengxuan%20Tong%20and%20Shuailei%20Zhang%20and%20Muyun%20Jiang%20and%20Yong%20Li%20and%20Kevin%20Lim%20Jun%20Liang%20and%20Cuntai%20Guan%0AAbstract%3A%20%20%20Integrating%20prior%20knowledge%20of%20neurophysiology%20into%20neural%20network%0Aarchitecture%20enhances%20the%20performance%20of%20emotion%20decoding.%20While%20numerous%0Atechniques%20emphasize%20learning%20spatial%20and%20short-term%20temporal%20patterns%2C%20there%0Ahas%20been%20limited%20emphasis%20on%20capturing%20the%20vital%20long-term%20contextual%0Ainformation%20associated%20with%20emotional%20cognitive%20processes.%20In%20order%20to%20address%0Athis%20discrepancy%2C%20we%20introduce%20a%20novel%20transformer%20model%20called%20emotion%0Atransformer%20%28EmT%29.%20EmT%20is%20designed%20to%20excel%20in%20both%20generalized%20cross-subject%0AEEG%20emotion%20classification%20and%20regression%20tasks.%20In%20EmT%2C%20EEG%20signals%20are%0Atransformed%20into%20a%20temporal%20graph%20format%2C%20creating%20a%20sequence%20of%20EEG%20feature%0Agraphs%20using%20a%20temporal%20graph%20construction%20module%20%28TGC%29.%20A%20novel%20residual%0Amulti-view%20pyramid%20GCN%20module%20%28RMPG%29%20is%20then%20proposed%20to%20learn%20dynamic%20graph%0Arepresentations%20for%20each%20EEG%20feature%20graph%20within%20the%20series%2C%20and%20the%20learned%0Arepresentations%20of%20each%20graph%20are%20fused%20into%20one%20token.%20Furthermore%2C%20we%20design%0Aa%20temporal%20contextual%20transformer%20module%20%28TCT%29%20with%20two%20types%20of%20token%20mixers%0Ato%20learn%20the%20temporal%20contextual%20information.%20Finally%2C%20the%20task-specific%20output%0Amodule%20%28TSO%29%20generates%20the%20desired%20outputs.%20Experiments%20on%20four%20publicly%0Aavailable%20datasets%20show%20that%20EmT%20achieves%20higher%20results%20than%20the%20baseline%0Amethods%20for%20both%20EEG%20emotion%20classification%20and%20regression%20tasks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/yi-ding-cs/EmT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmT%253A%2520A%2520Novel%2520Transformer%2520for%2520Generalized%2520Cross-subject%2520EEG%2520Emotion%250A%2520%2520Recognition%26entry.906535625%3DYi%2520Ding%2520and%2520Chengxuan%2520Tong%2520and%2520Shuailei%2520Zhang%2520and%2520Muyun%2520Jiang%2520and%2520Yong%2520Li%2520and%2520Kevin%2520Lim%2520Jun%2520Liang%2520and%2520Cuntai%2520Guan%26entry.1292438233%3D%2520%2520Integrating%2520prior%2520knowledge%2520of%2520neurophysiology%2520into%2520neural%2520network%250Aarchitecture%2520enhances%2520the%2520performance%2520of%2520emotion%2520decoding.%2520While%2520numerous%250Atechniques%2520emphasize%2520learning%2520spatial%2520and%2520short-term%2520temporal%2520patterns%252C%2520there%250Ahas%2520been%2520limited%2520emphasis%2520on%2520capturing%2520the%2520vital%2520long-term%2520contextual%250Ainformation%2520associated%2520with%2520emotional%2520cognitive%2520processes.%2520In%2520order%2520to%2520address%250Athis%2520discrepancy%252C%2520we%2520introduce%2520a%2520novel%2520transformer%2520model%2520called%2520emotion%250Atransformer%2520%2528EmT%2529.%2520EmT%2520is%2520designed%2520to%2520excel%2520in%2520both%2520generalized%2520cross-subject%250AEEG%2520emotion%2520classification%2520and%2520regression%2520tasks.%2520In%2520EmT%252C%2520EEG%2520signals%2520are%250Atransformed%2520into%2520a%2520temporal%2520graph%2520format%252C%2520creating%2520a%2520sequence%2520of%2520EEG%2520feature%250Agraphs%2520using%2520a%2520temporal%2520graph%2520construction%2520module%2520%2528TGC%2529.%2520A%2520novel%2520residual%250Amulti-view%2520pyramid%2520GCN%2520module%2520%2528RMPG%2529%2520is%2520then%2520proposed%2520to%2520learn%2520dynamic%2520graph%250Arepresentations%2520for%2520each%2520EEG%2520feature%2520graph%2520within%2520the%2520series%252C%2520and%2520the%2520learned%250Arepresentations%2520of%2520each%2520graph%2520are%2520fused%2520into%2520one%2520token.%2520Furthermore%252C%2520we%2520design%250Aa%2520temporal%2520contextual%2520transformer%2520module%2520%2528TCT%2529%2520with%2520two%2520types%2520of%2520token%2520mixers%250Ato%2520learn%2520the%2520temporal%2520contextual%2520information.%2520Finally%252C%2520the%2520task-specific%2520output%250Amodule%2520%2528TSO%2529%2520generates%2520the%2520desired%2520outputs.%2520Experiments%2520on%2520four%2520publicly%250Aavailable%2520datasets%2520show%2520that%2520EmT%2520achieves%2520higher%2520results%2520than%2520the%2520baseline%250Amethods%2520for%2520both%2520EEG%2520emotion%2520classification%2520and%2520regression%2520tasks.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/yi-ding-cs/EmT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmT%3A%20A%20Novel%20Transformer%20for%20Generalized%20Cross-subject%20EEG%20Emotion%0A%20%20Recognition&entry.906535625=Yi%20Ding%20and%20Chengxuan%20Tong%20and%20Shuailei%20Zhang%20and%20Muyun%20Jiang%20and%20Yong%20Li%20and%20Kevin%20Lim%20Jun%20Liang%20and%20Cuntai%20Guan&entry.1292438233=%20%20Integrating%20prior%20knowledge%20of%20neurophysiology%20into%20neural%20network%0Aarchitecture%20enhances%20the%20performance%20of%20emotion%20decoding.%20While%20numerous%0Atechniques%20emphasize%20learning%20spatial%20and%20short-term%20temporal%20patterns%2C%20there%0Ahas%20been%20limited%20emphasis%20on%20capturing%20the%20vital%20long-term%20contextual%0Ainformation%20associated%20with%20emotional%20cognitive%20processes.%20In%20order%20to%20address%0Athis%20discrepancy%2C%20we%20introduce%20a%20novel%20transformer%20model%20called%20emotion%0Atransformer%20%28EmT%29.%20EmT%20is%20designed%20to%20excel%20in%20both%20generalized%20cross-subject%0AEEG%20emotion%20classification%20and%20regression%20tasks.%20In%20EmT%2C%20EEG%20signals%20are%0Atransformed%20into%20a%20temporal%20graph%20format%2C%20creating%20a%20sequence%20of%20EEG%20feature%0Agraphs%20using%20a%20temporal%20graph%20construction%20module%20%28TGC%29.%20A%20novel%20residual%0Amulti-view%20pyramid%20GCN%20module%20%28RMPG%29%20is%20then%20proposed%20to%20learn%20dynamic%20graph%0Arepresentations%20for%20each%20EEG%20feature%20graph%20within%20the%20series%2C%20and%20the%20learned%0Arepresentations%20of%20each%20graph%20are%20fused%20into%20one%20token.%20Furthermore%2C%20we%20design%0Aa%20temporal%20contextual%20transformer%20module%20%28TCT%29%20with%20two%20types%20of%20token%20mixers%0Ato%20learn%20the%20temporal%20contextual%20information.%20Finally%2C%20the%20task-specific%20output%0Amodule%20%28TSO%29%20generates%20the%20desired%20outputs.%20Experiments%20on%20four%20publicly%0Aavailable%20datasets%20show%20that%20EmT%20achieves%20higher%20results%20than%20the%20baseline%0Amethods%20for%20both%20EEG%20emotion%20classification%20and%20regression%20tasks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/yi-ding-cs/EmT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18345v1&entry.124074799=Read"},
{"title": "Trade-off between Gradient Measurement Efficiency and Expressivity in\n  Deep Quantum Neural Networks", "author": "Koki Chinzei and Shinichiro Yamano and Quoc Hoan Tran and Yasuhiro Endo and Hirotaka Oshima", "abstract": "  Quantum neural networks (QNNs) require an efficient training algorithm to\nachieve practical quantum advantages. A promising approach is the use of\ngradient-based optimization algorithms, where gradients are estimated through\nquantum measurements. However, it is generally difficult to efficiently measure\ngradients in QNNs because the quantum state collapses upon measurement. In this\nwork, we prove a general trade-off between gradient measurement efficiency and\nexpressivity in a wide class of deep QNNs, elucidating the theoretical limits\nand possibilities of efficient gradient estimation. This trade-off implies that\na more expressive QNN requires a higher measurement cost in gradient\nestimation, whereas we can increase gradient measurement efficiency by reducing\nthe QNN expressivity to suit a given task. We further propose a general QNN\nansatz called the stabilizer-logical product ansatz (SLPA), which can reach the\nupper limit of the trade-off inequality by leveraging the symmetric structure\nof the quantum circuit. In learning an unknown symmetric function, the SLPA\ndrastically reduces the quantum resources required for training while\nmaintaining accuracy and trainability compared to a well-designed symmetric\ncircuit based on the parameter-shift method. Our results not only reveal a\ntheoretical understanding of efficient training in QNNs but also provide a\nstandard and broadly applicable efficient QNN design.\n", "link": "http://arxiv.org/abs/2406.18316v1", "date": "2024-06-26", "relevancy": 2.0114, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5121}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5067}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trade-off%20between%20Gradient%20Measurement%20Efficiency%20and%20Expressivity%20in%0A%20%20Deep%20Quantum%20Neural%20Networks&body=Title%3A%20Trade-off%20between%20Gradient%20Measurement%20Efficiency%20and%20Expressivity%20in%0A%20%20Deep%20Quantum%20Neural%20Networks%0AAuthor%3A%20Koki%20Chinzei%20and%20Shinichiro%20Yamano%20and%20Quoc%20Hoan%20Tran%20and%20Yasuhiro%20Endo%20and%20Hirotaka%20Oshima%0AAbstract%3A%20%20%20Quantum%20neural%20networks%20%28QNNs%29%20require%20an%20efficient%20training%20algorithm%20to%0Aachieve%20practical%20quantum%20advantages.%20A%20promising%20approach%20is%20the%20use%20of%0Agradient-based%20optimization%20algorithms%2C%20where%20gradients%20are%20estimated%20through%0Aquantum%20measurements.%20However%2C%20it%20is%20generally%20difficult%20to%20efficiently%20measure%0Agradients%20in%20QNNs%20because%20the%20quantum%20state%20collapses%20upon%20measurement.%20In%20this%0Awork%2C%20we%20prove%20a%20general%20trade-off%20between%20gradient%20measurement%20efficiency%20and%0Aexpressivity%20in%20a%20wide%20class%20of%20deep%20QNNs%2C%20elucidating%20the%20theoretical%20limits%0Aand%20possibilities%20of%20efficient%20gradient%20estimation.%20This%20trade-off%20implies%20that%0Aa%20more%20expressive%20QNN%20requires%20a%20higher%20measurement%20cost%20in%20gradient%0Aestimation%2C%20whereas%20we%20can%20increase%20gradient%20measurement%20efficiency%20by%20reducing%0Athe%20QNN%20expressivity%20to%20suit%20a%20given%20task.%20We%20further%20propose%20a%20general%20QNN%0Aansatz%20called%20the%20stabilizer-logical%20product%20ansatz%20%28SLPA%29%2C%20which%20can%20reach%20the%0Aupper%20limit%20of%20the%20trade-off%20inequality%20by%20leveraging%20the%20symmetric%20structure%0Aof%20the%20quantum%20circuit.%20In%20learning%20an%20unknown%20symmetric%20function%2C%20the%20SLPA%0Adrastically%20reduces%20the%20quantum%20resources%20required%20for%20training%20while%0Amaintaining%20accuracy%20and%20trainability%20compared%20to%20a%20well-designed%20symmetric%0Acircuit%20based%20on%20the%20parameter-shift%20method.%20Our%20results%20not%20only%20reveal%20a%0Atheoretical%20understanding%20of%20efficient%20training%20in%20QNNs%20but%20also%20provide%20a%0Astandard%20and%20broadly%20applicable%20efficient%20QNN%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrade-off%2520between%2520Gradient%2520Measurement%2520Efficiency%2520and%2520Expressivity%2520in%250A%2520%2520Deep%2520Quantum%2520Neural%2520Networks%26entry.906535625%3DKoki%2520Chinzei%2520and%2520Shinichiro%2520Yamano%2520and%2520Quoc%2520Hoan%2520Tran%2520and%2520Yasuhiro%2520Endo%2520and%2520Hirotaka%2520Oshima%26entry.1292438233%3D%2520%2520Quantum%2520neural%2520networks%2520%2528QNNs%2529%2520require%2520an%2520efficient%2520training%2520algorithm%2520to%250Aachieve%2520practical%2520quantum%2520advantages.%2520A%2520promising%2520approach%2520is%2520the%2520use%2520of%250Agradient-based%2520optimization%2520algorithms%252C%2520where%2520gradients%2520are%2520estimated%2520through%250Aquantum%2520measurements.%2520However%252C%2520it%2520is%2520generally%2520difficult%2520to%2520efficiently%2520measure%250Agradients%2520in%2520QNNs%2520because%2520the%2520quantum%2520state%2520collapses%2520upon%2520measurement.%2520In%2520this%250Awork%252C%2520we%2520prove%2520a%2520general%2520trade-off%2520between%2520gradient%2520measurement%2520efficiency%2520and%250Aexpressivity%2520in%2520a%2520wide%2520class%2520of%2520deep%2520QNNs%252C%2520elucidating%2520the%2520theoretical%2520limits%250Aand%2520possibilities%2520of%2520efficient%2520gradient%2520estimation.%2520This%2520trade-off%2520implies%2520that%250Aa%2520more%2520expressive%2520QNN%2520requires%2520a%2520higher%2520measurement%2520cost%2520in%2520gradient%250Aestimation%252C%2520whereas%2520we%2520can%2520increase%2520gradient%2520measurement%2520efficiency%2520by%2520reducing%250Athe%2520QNN%2520expressivity%2520to%2520suit%2520a%2520given%2520task.%2520We%2520further%2520propose%2520a%2520general%2520QNN%250Aansatz%2520called%2520the%2520stabilizer-logical%2520product%2520ansatz%2520%2528SLPA%2529%252C%2520which%2520can%2520reach%2520the%250Aupper%2520limit%2520of%2520the%2520trade-off%2520inequality%2520by%2520leveraging%2520the%2520symmetric%2520structure%250Aof%2520the%2520quantum%2520circuit.%2520In%2520learning%2520an%2520unknown%2520symmetric%2520function%252C%2520the%2520SLPA%250Adrastically%2520reduces%2520the%2520quantum%2520resources%2520required%2520for%2520training%2520while%250Amaintaining%2520accuracy%2520and%2520trainability%2520compared%2520to%2520a%2520well-designed%2520symmetric%250Acircuit%2520based%2520on%2520the%2520parameter-shift%2520method.%2520Our%2520results%2520not%2520only%2520reveal%2520a%250Atheoretical%2520understanding%2520of%2520efficient%2520training%2520in%2520QNNs%2520but%2520also%2520provide%2520a%250Astandard%2520and%2520broadly%2520applicable%2520efficient%2520QNN%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trade-off%20between%20Gradient%20Measurement%20Efficiency%20and%20Expressivity%20in%0A%20%20Deep%20Quantum%20Neural%20Networks&entry.906535625=Koki%20Chinzei%20and%20Shinichiro%20Yamano%20and%20Quoc%20Hoan%20Tran%20and%20Yasuhiro%20Endo%20and%20Hirotaka%20Oshima&entry.1292438233=%20%20Quantum%20neural%20networks%20%28QNNs%29%20require%20an%20efficient%20training%20algorithm%20to%0Aachieve%20practical%20quantum%20advantages.%20A%20promising%20approach%20is%20the%20use%20of%0Agradient-based%20optimization%20algorithms%2C%20where%20gradients%20are%20estimated%20through%0Aquantum%20measurements.%20However%2C%20it%20is%20generally%20difficult%20to%20efficiently%20measure%0Agradients%20in%20QNNs%20because%20the%20quantum%20state%20collapses%20upon%20measurement.%20In%20this%0Awork%2C%20we%20prove%20a%20general%20trade-off%20between%20gradient%20measurement%20efficiency%20and%0Aexpressivity%20in%20a%20wide%20class%20of%20deep%20QNNs%2C%20elucidating%20the%20theoretical%20limits%0Aand%20possibilities%20of%20efficient%20gradient%20estimation.%20This%20trade-off%20implies%20that%0Aa%20more%20expressive%20QNN%20requires%20a%20higher%20measurement%20cost%20in%20gradient%0Aestimation%2C%20whereas%20we%20can%20increase%20gradient%20measurement%20efficiency%20by%20reducing%0Athe%20QNN%20expressivity%20to%20suit%20a%20given%20task.%20We%20further%20propose%20a%20general%20QNN%0Aansatz%20called%20the%20stabilizer-logical%20product%20ansatz%20%28SLPA%29%2C%20which%20can%20reach%20the%0Aupper%20limit%20of%20the%20trade-off%20inequality%20by%20leveraging%20the%20symmetric%20structure%0Aof%20the%20quantum%20circuit.%20In%20learning%20an%20unknown%20symmetric%20function%2C%20the%20SLPA%0Adrastically%20reduces%20the%20quantum%20resources%20required%20for%20training%20while%0Amaintaining%20accuracy%20and%20trainability%20compared%20to%20a%20well-designed%20symmetric%0Acircuit%20based%20on%20the%20parameter-shift%20method.%20Our%20results%20not%20only%20reveal%20a%0Atheoretical%20understanding%20of%20efficient%20training%20in%20QNNs%20but%20also%20provide%20a%0Astandard%20and%20broadly%20applicable%20efficient%20QNN%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18316v1&entry.124074799=Read"},
{"title": "MatchTime: Towards Automatic Soccer Game Commentary Generation", "author": "Jiayuan Rao and Haoning Wu and Chang Liu and Yanfeng Wang and Weidi Xie", "abstract": "  Soccer is a globally popular sport with a vast audience, in this paper, we\nconsider constructing an automatic soccer game commentary model to improve the\naudiences' viewing experience. In general, we make the following contributions:\nFirst, observing the prevalent video-text misalignment in existing datasets, we\nmanually annotate timestamps for 49 matches, establishing a more robust\nbenchmark for soccer game commentary generation, termed as\nSN-Caption-test-align; Second, we propose a multi-modal temporal alignment\npipeline to automatically correct and filter the existing dataset at scale,\ncreating a higher-quality soccer game commentary dataset for training, denoted\nas MatchTime; Third, based on our curated dataset, we train an automatic\ncommentary generation model, named MatchVoice. Extensive experiments and\nablation studies have demonstrated the effectiveness of our alignment pipeline,\nand training model on the curated datasets achieves state-of-the-art\nperformance for commentary generation, showcasing that better alignment can\nlead to significant performance improvements in downstream tasks.\n", "link": "http://arxiv.org/abs/2406.18530v1", "date": "2024-06-26", "relevancy": 1.9992, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5164}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4999}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatchTime%3A%20Towards%20Automatic%20Soccer%20Game%20Commentary%20Generation&body=Title%3A%20MatchTime%3A%20Towards%20Automatic%20Soccer%20Game%20Commentary%20Generation%0AAuthor%3A%20Jiayuan%20Rao%20and%20Haoning%20Wu%20and%20Chang%20Liu%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%0AAbstract%3A%20%20%20Soccer%20is%20a%20globally%20popular%20sport%20with%20a%20vast%20audience%2C%20in%20this%20paper%2C%20we%0Aconsider%20constructing%20an%20automatic%20soccer%20game%20commentary%20model%20to%20improve%20the%0Aaudiences%27%20viewing%20experience.%20In%20general%2C%20we%20make%20the%20following%20contributions%3A%0AFirst%2C%20observing%20the%20prevalent%20video-text%20misalignment%20in%20existing%20datasets%2C%20we%0Amanually%20annotate%20timestamps%20for%2049%20matches%2C%20establishing%20a%20more%20robust%0Abenchmark%20for%20soccer%20game%20commentary%20generation%2C%20termed%20as%0ASN-Caption-test-align%3B%20Second%2C%20we%20propose%20a%20multi-modal%20temporal%20alignment%0Apipeline%20to%20automatically%20correct%20and%20filter%20the%20existing%20dataset%20at%20scale%2C%0Acreating%20a%20higher-quality%20soccer%20game%20commentary%20dataset%20for%20training%2C%20denoted%0Aas%20MatchTime%3B%20Third%2C%20based%20on%20our%20curated%20dataset%2C%20we%20train%20an%20automatic%0Acommentary%20generation%20model%2C%20named%20MatchVoice.%20Extensive%20experiments%20and%0Aablation%20studies%20have%20demonstrated%20the%20effectiveness%20of%20our%20alignment%20pipeline%2C%0Aand%20training%20model%20on%20the%20curated%20datasets%20achieves%20state-of-the-art%0Aperformance%20for%20commentary%20generation%2C%20showcasing%20that%20better%20alignment%20can%0Alead%20to%20significant%20performance%20improvements%20in%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatchTime%253A%2520Towards%2520Automatic%2520Soccer%2520Game%2520Commentary%2520Generation%26entry.906535625%3DJiayuan%2520Rao%2520and%2520Haoning%2520Wu%2520and%2520Chang%2520Liu%2520and%2520Yanfeng%2520Wang%2520and%2520Weidi%2520Xie%26entry.1292438233%3D%2520%2520Soccer%2520is%2520a%2520globally%2520popular%2520sport%2520with%2520a%2520vast%2520audience%252C%2520in%2520this%2520paper%252C%2520we%250Aconsider%2520constructing%2520an%2520automatic%2520soccer%2520game%2520commentary%2520model%2520to%2520improve%2520the%250Aaudiences%2527%2520viewing%2520experience.%2520In%2520general%252C%2520we%2520make%2520the%2520following%2520contributions%253A%250AFirst%252C%2520observing%2520the%2520prevalent%2520video-text%2520misalignment%2520in%2520existing%2520datasets%252C%2520we%250Amanually%2520annotate%2520timestamps%2520for%252049%2520matches%252C%2520establishing%2520a%2520more%2520robust%250Abenchmark%2520for%2520soccer%2520game%2520commentary%2520generation%252C%2520termed%2520as%250ASN-Caption-test-align%253B%2520Second%252C%2520we%2520propose%2520a%2520multi-modal%2520temporal%2520alignment%250Apipeline%2520to%2520automatically%2520correct%2520and%2520filter%2520the%2520existing%2520dataset%2520at%2520scale%252C%250Acreating%2520a%2520higher-quality%2520soccer%2520game%2520commentary%2520dataset%2520for%2520training%252C%2520denoted%250Aas%2520MatchTime%253B%2520Third%252C%2520based%2520on%2520our%2520curated%2520dataset%252C%2520we%2520train%2520an%2520automatic%250Acommentary%2520generation%2520model%252C%2520named%2520MatchVoice.%2520Extensive%2520experiments%2520and%250Aablation%2520studies%2520have%2520demonstrated%2520the%2520effectiveness%2520of%2520our%2520alignment%2520pipeline%252C%250Aand%2520training%2520model%2520on%2520the%2520curated%2520datasets%2520achieves%2520state-of-the-art%250Aperformance%2520for%2520commentary%2520generation%252C%2520showcasing%2520that%2520better%2520alignment%2520can%250Alead%2520to%2520significant%2520performance%2520improvements%2520in%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatchTime%3A%20Towards%20Automatic%20Soccer%20Game%20Commentary%20Generation&entry.906535625=Jiayuan%20Rao%20and%20Haoning%20Wu%20and%20Chang%20Liu%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie&entry.1292438233=%20%20Soccer%20is%20a%20globally%20popular%20sport%20with%20a%20vast%20audience%2C%20in%20this%20paper%2C%20we%0Aconsider%20constructing%20an%20automatic%20soccer%20game%20commentary%20model%20to%20improve%20the%0Aaudiences%27%20viewing%20experience.%20In%20general%2C%20we%20make%20the%20following%20contributions%3A%0AFirst%2C%20observing%20the%20prevalent%20video-text%20misalignment%20in%20existing%20datasets%2C%20we%0Amanually%20annotate%20timestamps%20for%2049%20matches%2C%20establishing%20a%20more%20robust%0Abenchmark%20for%20soccer%20game%20commentary%20generation%2C%20termed%20as%0ASN-Caption-test-align%3B%20Second%2C%20we%20propose%20a%20multi-modal%20temporal%20alignment%0Apipeline%20to%20automatically%20correct%20and%20filter%20the%20existing%20dataset%20at%20scale%2C%0Acreating%20a%20higher-quality%20soccer%20game%20commentary%20dataset%20for%20training%2C%20denoted%0Aas%20MatchTime%3B%20Third%2C%20based%20on%20our%20curated%20dataset%2C%20we%20train%20an%20automatic%0Acommentary%20generation%20model%2C%20named%20MatchVoice.%20Extensive%20experiments%20and%0Aablation%20studies%20have%20demonstrated%20the%20effectiveness%20of%20our%20alignment%20pipeline%2C%0Aand%20training%20model%20on%20the%20curated%20datasets%20achieves%20state-of-the-art%0Aperformance%20for%20commentary%20generation%2C%20showcasing%20that%20better%20alignment%20can%0Alead%20to%20significant%20performance%20improvements%20in%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18530v1&entry.124074799=Read"},
{"title": "Graph Neural Networks for Emulation of Finite-Element Ice Dynamics in\n  Greenland and Antarctic Ice Sheets", "author": "Younghyun Koo and Maryam Rahnemoonfar", "abstract": "  Although numerical models provide accurate solutions for ice sheet dynamics\nbased on physics laws, they accompany intensified computational demands to\nsolve partial differential equations. In recent years, convolutional neural\nnetworks (CNNs) have been widely used as statistical emulators for those\nnumerical models. However, since CNNs operate on regular grids, they cannot\nrepresent the refined meshes and computational efficiency of finite-element\nnumerical models. Therefore, instead of CNNs, this study adopts an equivariant\ngraph convolutional network (EGCN) as an emulator for the ice sheet dynamics\nmodeling. EGCN reproduces ice thickness and velocity changes in the Helheim\nGlacier, Greenland, and Pine Island Glacier, Antarctica, with 260 times and 44\ntimes faster computation time, respectively. Compared to the traditional CNN\nand graph convolutional network, EGCN shows outstanding accuracy in thickness\nprediction near fast ice streams by preserving the equivariance to the\ntranslation and rotation of graphs.\n", "link": "http://arxiv.org/abs/2406.18423v1", "date": "2024-06-26", "relevancy": 1.9953, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5204}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4841}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20for%20Emulation%20of%20Finite-Element%20Ice%20Dynamics%20in%0A%20%20Greenland%20and%20Antarctic%20Ice%20Sheets&body=Title%3A%20Graph%20Neural%20Networks%20for%20Emulation%20of%20Finite-Element%20Ice%20Dynamics%20in%0A%20%20Greenland%20and%20Antarctic%20Ice%20Sheets%0AAuthor%3A%20Younghyun%20Koo%20and%20Maryam%20Rahnemoonfar%0AAbstract%3A%20%20%20Although%20numerical%20models%20provide%20accurate%20solutions%20for%20ice%20sheet%20dynamics%0Abased%20on%20physics%20laws%2C%20they%20accompany%20intensified%20computational%20demands%20to%0Asolve%20partial%20differential%20equations.%20In%20recent%20years%2C%20convolutional%20neural%0Anetworks%20%28CNNs%29%20have%20been%20widely%20used%20as%20statistical%20emulators%20for%20those%0Anumerical%20models.%20However%2C%20since%20CNNs%20operate%20on%20regular%20grids%2C%20they%20cannot%0Arepresent%20the%20refined%20meshes%20and%20computational%20efficiency%20of%20finite-element%0Anumerical%20models.%20Therefore%2C%20instead%20of%20CNNs%2C%20this%20study%20adopts%20an%20equivariant%0Agraph%20convolutional%20network%20%28EGCN%29%20as%20an%20emulator%20for%20the%20ice%20sheet%20dynamics%0Amodeling.%20EGCN%20reproduces%20ice%20thickness%20and%20velocity%20changes%20in%20the%20Helheim%0AGlacier%2C%20Greenland%2C%20and%20Pine%20Island%20Glacier%2C%20Antarctica%2C%20with%20260%20times%20and%2044%0Atimes%20faster%20computation%20time%2C%20respectively.%20Compared%20to%20the%20traditional%20CNN%0Aand%20graph%20convolutional%20network%2C%20EGCN%20shows%20outstanding%20accuracy%20in%20thickness%0Aprediction%20near%20fast%20ice%20streams%20by%20preserving%20the%20equivariance%20to%20the%0Atranslation%20and%20rotation%20of%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520for%2520Emulation%2520of%2520Finite-Element%2520Ice%2520Dynamics%2520in%250A%2520%2520Greenland%2520and%2520Antarctic%2520Ice%2520Sheets%26entry.906535625%3DYounghyun%2520Koo%2520and%2520Maryam%2520Rahnemoonfar%26entry.1292438233%3D%2520%2520Although%2520numerical%2520models%2520provide%2520accurate%2520solutions%2520for%2520ice%2520sheet%2520dynamics%250Abased%2520on%2520physics%2520laws%252C%2520they%2520accompany%2520intensified%2520computational%2520demands%2520to%250Asolve%2520partial%2520differential%2520equations.%2520In%2520recent%2520years%252C%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%2520have%2520been%2520widely%2520used%2520as%2520statistical%2520emulators%2520for%2520those%250Anumerical%2520models.%2520However%252C%2520since%2520CNNs%2520operate%2520on%2520regular%2520grids%252C%2520they%2520cannot%250Arepresent%2520the%2520refined%2520meshes%2520and%2520computational%2520efficiency%2520of%2520finite-element%250Anumerical%2520models.%2520Therefore%252C%2520instead%2520of%2520CNNs%252C%2520this%2520study%2520adopts%2520an%2520equivariant%250Agraph%2520convolutional%2520network%2520%2528EGCN%2529%2520as%2520an%2520emulator%2520for%2520the%2520ice%2520sheet%2520dynamics%250Amodeling.%2520EGCN%2520reproduces%2520ice%2520thickness%2520and%2520velocity%2520changes%2520in%2520the%2520Helheim%250AGlacier%252C%2520Greenland%252C%2520and%2520Pine%2520Island%2520Glacier%252C%2520Antarctica%252C%2520with%2520260%2520times%2520and%252044%250Atimes%2520faster%2520computation%2520time%252C%2520respectively.%2520Compared%2520to%2520the%2520traditional%2520CNN%250Aand%2520graph%2520convolutional%2520network%252C%2520EGCN%2520shows%2520outstanding%2520accuracy%2520in%2520thickness%250Aprediction%2520near%2520fast%2520ice%2520streams%2520by%2520preserving%2520the%2520equivariance%2520to%2520the%250Atranslation%2520and%2520rotation%2520of%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20for%20Emulation%20of%20Finite-Element%20Ice%20Dynamics%20in%0A%20%20Greenland%20and%20Antarctic%20Ice%20Sheets&entry.906535625=Younghyun%20Koo%20and%20Maryam%20Rahnemoonfar&entry.1292438233=%20%20Although%20numerical%20models%20provide%20accurate%20solutions%20for%20ice%20sheet%20dynamics%0Abased%20on%20physics%20laws%2C%20they%20accompany%20intensified%20computational%20demands%20to%0Asolve%20partial%20differential%20equations.%20In%20recent%20years%2C%20convolutional%20neural%0Anetworks%20%28CNNs%29%20have%20been%20widely%20used%20as%20statistical%20emulators%20for%20those%0Anumerical%20models.%20However%2C%20since%20CNNs%20operate%20on%20regular%20grids%2C%20they%20cannot%0Arepresent%20the%20refined%20meshes%20and%20computational%20efficiency%20of%20finite-element%0Anumerical%20models.%20Therefore%2C%20instead%20of%20CNNs%2C%20this%20study%20adopts%20an%20equivariant%0Agraph%20convolutional%20network%20%28EGCN%29%20as%20an%20emulator%20for%20the%20ice%20sheet%20dynamics%0Amodeling.%20EGCN%20reproduces%20ice%20thickness%20and%20velocity%20changes%20in%20the%20Helheim%0AGlacier%2C%20Greenland%2C%20and%20Pine%20Island%20Glacier%2C%20Antarctica%2C%20with%20260%20times%20and%2044%0Atimes%20faster%20computation%20time%2C%20respectively.%20Compared%20to%20the%20traditional%20CNN%0Aand%20graph%20convolutional%20network%2C%20EGCN%20shows%20outstanding%20accuracy%20in%20thickness%0Aprediction%20near%20fast%20ice%20streams%20by%20preserving%20the%20equivariance%20to%20the%0Atranslation%20and%20rotation%20of%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18423v1&entry.124074799=Read"},
{"title": "An Information Theoretic Perspective on Conformal Prediction", "author": "Alvaro H. C. Correia and Fabio Valerio Massoli and Christos Louizos and Arash Behboodi", "abstract": "  Conformal Prediction (CP) is a distribution-free uncertainty estimation\nframework that constructs prediction sets guaranteed to contain the true answer\nwith a user-specified probability. Intuitively, the size of the prediction set\nencodes a general notion of uncertainty, with larger sets associated with\nhigher degrees of uncertainty. In this work, we leverage information theory to\nconnect conformal prediction to other notions of uncertainty. More precisely,\nwe prove three different ways to upper bound the intrinsic uncertainty, as\ndescribed by the conditional entropy of the target variable given the inputs,\nby combining CP with information theoretical inequalities. Moreover, we\ndemonstrate two direct and useful applications of such connection between\nconformal prediction and information theory: (i) more principled and effective\nconformal training objectives that generalize previous approaches and enable\nend-to-end training of machine learning models from scratch, and (ii) a natural\nmechanism to incorporate side information into conformal prediction. We\nempirically validate both applications in centralized and federated learning\nsettings, showing our theoretical results translate to lower inefficiency\n(average prediction set size) for popular CP methods.\n", "link": "http://arxiv.org/abs/2405.02140v2", "date": "2024-06-26", "relevancy": 1.9949, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4948}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Information%20Theoretic%20Perspective%20on%20Conformal%20Prediction&body=Title%3A%20An%20Information%20Theoretic%20Perspective%20on%20Conformal%20Prediction%0AAuthor%3A%20Alvaro%20H.%20C.%20Correia%20and%20Fabio%20Valerio%20Massoli%20and%20Christos%20Louizos%20and%20Arash%20Behboodi%0AAbstract%3A%20%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20distribution-free%20uncertainty%20estimation%0Aframework%20that%20constructs%20prediction%20sets%20guaranteed%20to%20contain%20the%20true%20answer%0Awith%20a%20user-specified%20probability.%20Intuitively%2C%20the%20size%20of%20the%20prediction%20set%0Aencodes%20a%20general%20notion%20of%20uncertainty%2C%20with%20larger%20sets%20associated%20with%0Ahigher%20degrees%20of%20uncertainty.%20In%20this%20work%2C%20we%20leverage%20information%20theory%20to%0Aconnect%20conformal%20prediction%20to%20other%20notions%20of%20uncertainty.%20More%20precisely%2C%0Awe%20prove%20three%20different%20ways%20to%20upper%20bound%20the%20intrinsic%20uncertainty%2C%20as%0Adescribed%20by%20the%20conditional%20entropy%20of%20the%20target%20variable%20given%20the%20inputs%2C%0Aby%20combining%20CP%20with%20information%20theoretical%20inequalities.%20Moreover%2C%20we%0Ademonstrate%20two%20direct%20and%20useful%20applications%20of%20such%20connection%20between%0Aconformal%20prediction%20and%20information%20theory%3A%20%28i%29%20more%20principled%20and%20effective%0Aconformal%20training%20objectives%20that%20generalize%20previous%20approaches%20and%20enable%0Aend-to-end%20training%20of%20machine%20learning%20models%20from%20scratch%2C%20and%20%28ii%29%20a%20natural%0Amechanism%20to%20incorporate%20side%20information%20into%20conformal%20prediction.%20We%0Aempirically%20validate%20both%20applications%20in%20centralized%20and%20federated%20learning%0Asettings%2C%20showing%20our%20theoretical%20results%20translate%20to%20lower%20inefficiency%0A%28average%20prediction%20set%20size%29%20for%20popular%20CP%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Information%2520Theoretic%2520Perspective%2520on%2520Conformal%2520Prediction%26entry.906535625%3DAlvaro%2520H.%2520C.%2520Correia%2520and%2520Fabio%2520Valerio%2520Massoli%2520and%2520Christos%2520Louizos%2520and%2520Arash%2520Behboodi%26entry.1292438233%3D%2520%2520Conformal%2520Prediction%2520%2528CP%2529%2520is%2520a%2520distribution-free%2520uncertainty%2520estimation%250Aframework%2520that%2520constructs%2520prediction%2520sets%2520guaranteed%2520to%2520contain%2520the%2520true%2520answer%250Awith%2520a%2520user-specified%2520probability.%2520Intuitively%252C%2520the%2520size%2520of%2520the%2520prediction%2520set%250Aencodes%2520a%2520general%2520notion%2520of%2520uncertainty%252C%2520with%2520larger%2520sets%2520associated%2520with%250Ahigher%2520degrees%2520of%2520uncertainty.%2520In%2520this%2520work%252C%2520we%2520leverage%2520information%2520theory%2520to%250Aconnect%2520conformal%2520prediction%2520to%2520other%2520notions%2520of%2520uncertainty.%2520More%2520precisely%252C%250Awe%2520prove%2520three%2520different%2520ways%2520to%2520upper%2520bound%2520the%2520intrinsic%2520uncertainty%252C%2520as%250Adescribed%2520by%2520the%2520conditional%2520entropy%2520of%2520the%2520target%2520variable%2520given%2520the%2520inputs%252C%250Aby%2520combining%2520CP%2520with%2520information%2520theoretical%2520inequalities.%2520Moreover%252C%2520we%250Ademonstrate%2520two%2520direct%2520and%2520useful%2520applications%2520of%2520such%2520connection%2520between%250Aconformal%2520prediction%2520and%2520information%2520theory%253A%2520%2528i%2529%2520more%2520principled%2520and%2520effective%250Aconformal%2520training%2520objectives%2520that%2520generalize%2520previous%2520approaches%2520and%2520enable%250Aend-to-end%2520training%2520of%2520machine%2520learning%2520models%2520from%2520scratch%252C%2520and%2520%2528ii%2529%2520a%2520natural%250Amechanism%2520to%2520incorporate%2520side%2520information%2520into%2520conformal%2520prediction.%2520We%250Aempirically%2520validate%2520both%2520applications%2520in%2520centralized%2520and%2520federated%2520learning%250Asettings%252C%2520showing%2520our%2520theoretical%2520results%2520translate%2520to%2520lower%2520inefficiency%250A%2528average%2520prediction%2520set%2520size%2529%2520for%2520popular%2520CP%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Information%20Theoretic%20Perspective%20on%20Conformal%20Prediction&entry.906535625=Alvaro%20H.%20C.%20Correia%20and%20Fabio%20Valerio%20Massoli%20and%20Christos%20Louizos%20and%20Arash%20Behboodi&entry.1292438233=%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20distribution-free%20uncertainty%20estimation%0Aframework%20that%20constructs%20prediction%20sets%20guaranteed%20to%20contain%20the%20true%20answer%0Awith%20a%20user-specified%20probability.%20Intuitively%2C%20the%20size%20of%20the%20prediction%20set%0Aencodes%20a%20general%20notion%20of%20uncertainty%2C%20with%20larger%20sets%20associated%20with%0Ahigher%20degrees%20of%20uncertainty.%20In%20this%20work%2C%20we%20leverage%20information%20theory%20to%0Aconnect%20conformal%20prediction%20to%20other%20notions%20of%20uncertainty.%20More%20precisely%2C%0Awe%20prove%20three%20different%20ways%20to%20upper%20bound%20the%20intrinsic%20uncertainty%2C%20as%0Adescribed%20by%20the%20conditional%20entropy%20of%20the%20target%20variable%20given%20the%20inputs%2C%0Aby%20combining%20CP%20with%20information%20theoretical%20inequalities.%20Moreover%2C%20we%0Ademonstrate%20two%20direct%20and%20useful%20applications%20of%20such%20connection%20between%0Aconformal%20prediction%20and%20information%20theory%3A%20%28i%29%20more%20principled%20and%20effective%0Aconformal%20training%20objectives%20that%20generalize%20previous%20approaches%20and%20enable%0Aend-to-end%20training%20of%20machine%20learning%20models%20from%20scratch%2C%20and%20%28ii%29%20a%20natural%0Amechanism%20to%20incorporate%20side%20information%20into%20conformal%20prediction.%20We%0Aempirically%20validate%20both%20applications%20in%20centralized%20and%20federated%20learning%0Asettings%2C%20showing%20our%20theoretical%20results%20translate%20to%20lower%20inefficiency%0A%28average%20prediction%20set%20size%29%20for%20popular%20CP%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02140v2&entry.124074799=Read"},
{"title": "Unlocking the Potential of Operations Research for Multi-Graph Matching", "author": "Max Kahl and Sebastian Stricker and Lisa Hutschenreiter and Florian Bernard and Bogdan Savchynskyy", "abstract": "  We consider the incomplete multi-graph matching problem, which is a\ngeneralization of the NP-hard quadratic assignment problem for matching\nmultiple finite sets. Multi-graph matching plays a central role in computer\nvision, e.g., for matching images or shapes, so that a number of dedicated\noptimization techniques have been proposed. While the closely related NP-hard\nmulti-dimensional assignment problem (MDAP) has been studied for decades in the\noperations research community, it only considers complete matchings and has a\ndifferent cost structure. We bridge this gap and transfer well-known\napproximation algorithms for the MDAP to incomplete multi-graph matching. To\nthis end, we revisit respective algorithms, adapt them to incomplete\nmulti-graph matching, and propose their extended and parallelized versions. Our\nexperimental validation shows that our new method substantially outperforms the\nprevious state of the art in terms of objective and runtime. Our algorithm\nmatches, for example, 29 images with more than 500 keypoints each in less than\ntwo minutes, whereas the fastest considered competitor requires at least half\nan hour while producing far worse results.\n", "link": "http://arxiv.org/abs/2406.18215v1", "date": "2024-06-26", "relevancy": 1.9849, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5211}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4806}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20the%20Potential%20of%20Operations%20Research%20for%20Multi-Graph%20Matching&body=Title%3A%20Unlocking%20the%20Potential%20of%20Operations%20Research%20for%20Multi-Graph%20Matching%0AAuthor%3A%20Max%20Kahl%20and%20Sebastian%20Stricker%20and%20Lisa%20Hutschenreiter%20and%20Florian%20Bernard%20and%20Bogdan%20Savchynskyy%0AAbstract%3A%20%20%20We%20consider%20the%20incomplete%20multi-graph%20matching%20problem%2C%20which%20is%20a%0Ageneralization%20of%20the%20NP-hard%20quadratic%20assignment%20problem%20for%20matching%0Amultiple%20finite%20sets.%20Multi-graph%20matching%20plays%20a%20central%20role%20in%20computer%0Avision%2C%20e.g.%2C%20for%20matching%20images%20or%20shapes%2C%20so%20that%20a%20number%20of%20dedicated%0Aoptimization%20techniques%20have%20been%20proposed.%20While%20the%20closely%20related%20NP-hard%0Amulti-dimensional%20assignment%20problem%20%28MDAP%29%20has%20been%20studied%20for%20decades%20in%20the%0Aoperations%20research%20community%2C%20it%20only%20considers%20complete%20matchings%20and%20has%20a%0Adifferent%20cost%20structure.%20We%20bridge%20this%20gap%20and%20transfer%20well-known%0Aapproximation%20algorithms%20for%20the%20MDAP%20to%20incomplete%20multi-graph%20matching.%20To%0Athis%20end%2C%20we%20revisit%20respective%20algorithms%2C%20adapt%20them%20to%20incomplete%0Amulti-graph%20matching%2C%20and%20propose%20their%20extended%20and%20parallelized%20versions.%20Our%0Aexperimental%20validation%20shows%20that%20our%20new%20method%20substantially%20outperforms%20the%0Aprevious%20state%20of%20the%20art%20in%20terms%20of%20objective%20and%20runtime.%20Our%20algorithm%0Amatches%2C%20for%20example%2C%2029%20images%20with%20more%20than%20500%20keypoints%20each%20in%20less%20than%0Atwo%20minutes%2C%20whereas%20the%20fastest%20considered%20competitor%20requires%20at%20least%20half%0Aan%20hour%20while%20producing%20far%20worse%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520the%2520Potential%2520of%2520Operations%2520Research%2520for%2520Multi-Graph%2520Matching%26entry.906535625%3DMax%2520Kahl%2520and%2520Sebastian%2520Stricker%2520and%2520Lisa%2520Hutschenreiter%2520and%2520Florian%2520Bernard%2520and%2520Bogdan%2520Savchynskyy%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520incomplete%2520multi-graph%2520matching%2520problem%252C%2520which%2520is%2520a%250Ageneralization%2520of%2520the%2520NP-hard%2520quadratic%2520assignment%2520problem%2520for%2520matching%250Amultiple%2520finite%2520sets.%2520Multi-graph%2520matching%2520plays%2520a%2520central%2520role%2520in%2520computer%250Avision%252C%2520e.g.%252C%2520for%2520matching%2520images%2520or%2520shapes%252C%2520so%2520that%2520a%2520number%2520of%2520dedicated%250Aoptimization%2520techniques%2520have%2520been%2520proposed.%2520While%2520the%2520closely%2520related%2520NP-hard%250Amulti-dimensional%2520assignment%2520problem%2520%2528MDAP%2529%2520has%2520been%2520studied%2520for%2520decades%2520in%2520the%250Aoperations%2520research%2520community%252C%2520it%2520only%2520considers%2520complete%2520matchings%2520and%2520has%2520a%250Adifferent%2520cost%2520structure.%2520We%2520bridge%2520this%2520gap%2520and%2520transfer%2520well-known%250Aapproximation%2520algorithms%2520for%2520the%2520MDAP%2520to%2520incomplete%2520multi-graph%2520matching.%2520To%250Athis%2520end%252C%2520we%2520revisit%2520respective%2520algorithms%252C%2520adapt%2520them%2520to%2520incomplete%250Amulti-graph%2520matching%252C%2520and%2520propose%2520their%2520extended%2520and%2520parallelized%2520versions.%2520Our%250Aexperimental%2520validation%2520shows%2520that%2520our%2520new%2520method%2520substantially%2520outperforms%2520the%250Aprevious%2520state%2520of%2520the%2520art%2520in%2520terms%2520of%2520objective%2520and%2520runtime.%2520Our%2520algorithm%250Amatches%252C%2520for%2520example%252C%252029%2520images%2520with%2520more%2520than%2520500%2520keypoints%2520each%2520in%2520less%2520than%250Atwo%2520minutes%252C%2520whereas%2520the%2520fastest%2520considered%2520competitor%2520requires%2520at%2520least%2520half%250Aan%2520hour%2520while%2520producing%2520far%2520worse%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20the%20Potential%20of%20Operations%20Research%20for%20Multi-Graph%20Matching&entry.906535625=Max%20Kahl%20and%20Sebastian%20Stricker%20and%20Lisa%20Hutschenreiter%20and%20Florian%20Bernard%20and%20Bogdan%20Savchynskyy&entry.1292438233=%20%20We%20consider%20the%20incomplete%20multi-graph%20matching%20problem%2C%20which%20is%20a%0Ageneralization%20of%20the%20NP-hard%20quadratic%20assignment%20problem%20for%20matching%0Amultiple%20finite%20sets.%20Multi-graph%20matching%20plays%20a%20central%20role%20in%20computer%0Avision%2C%20e.g.%2C%20for%20matching%20images%20or%20shapes%2C%20so%20that%20a%20number%20of%20dedicated%0Aoptimization%20techniques%20have%20been%20proposed.%20While%20the%20closely%20related%20NP-hard%0Amulti-dimensional%20assignment%20problem%20%28MDAP%29%20has%20been%20studied%20for%20decades%20in%20the%0Aoperations%20research%20community%2C%20it%20only%20considers%20complete%20matchings%20and%20has%20a%0Adifferent%20cost%20structure.%20We%20bridge%20this%20gap%20and%20transfer%20well-known%0Aapproximation%20algorithms%20for%20the%20MDAP%20to%20incomplete%20multi-graph%20matching.%20To%0Athis%20end%2C%20we%20revisit%20respective%20algorithms%2C%20adapt%20them%20to%20incomplete%0Amulti-graph%20matching%2C%20and%20propose%20their%20extended%20and%20parallelized%20versions.%20Our%0Aexperimental%20validation%20shows%20that%20our%20new%20method%20substantially%20outperforms%20the%0Aprevious%20state%20of%20the%20art%20in%20terms%20of%20objective%20and%20runtime.%20Our%20algorithm%0Amatches%2C%20for%20example%2C%2029%20images%20with%20more%20than%20500%20keypoints%20each%20in%20less%20than%0Atwo%20minutes%2C%20whereas%20the%20fastest%20considered%20competitor%20requires%20at%20least%20half%0Aan%20hour%20while%20producing%20far%20worse%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18215v1&entry.124074799=Read"},
{"title": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and\n  Explainability is Complicated", "author": "Jiazhou Ji and Ruizhe Li and Shujun Li and Jie Guo and Weidong Qiu and Zheng Huang and Chiyu Chen and Xiaoyu Jiang and Xinru Lu", "abstract": "  As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power.\n", "link": "http://arxiv.org/abs/2406.18259v1", "date": "2024-06-26", "relevancy": 1.9741, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.546}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4879}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Machine-Generated%20Texts%3A%20Not%20Just%20%22AI%20vs%20Humans%22%20and%0A%20%20Explainability%20is%20Complicated&body=Title%3A%20Detecting%20Machine-Generated%20Texts%3A%20Not%20Just%20%22AI%20vs%20Humans%22%20and%0A%20%20Explainability%20is%20Complicated%0AAuthor%3A%20Jiazhou%20Ji%20and%20Ruizhe%20Li%20and%20Shujun%20Li%20and%20Jie%20Guo%20and%20Weidong%20Qiu%20and%20Zheng%20Huang%20and%20Chiyu%20Chen%20and%20Xiaoyu%20Jiang%20and%20Xinru%20Lu%0AAbstract%3A%20%20%20As%20LLMs%20rapidly%20advance%2C%20increasing%20concerns%20arise%20regarding%20risks%20about%0Aactual%20authorship%20of%20texts%20we%20see%20online%20and%20in%20real%20world.%20The%20task%20of%0Adistinguishing%20LLM-authored%20texts%20is%20complicated%20by%20the%20nuanced%20and%20overlapping%0Abehaviors%20of%20both%20machines%20and%20humans.%20In%20this%20paper%2C%20we%20challenge%20the%20current%0Apractice%20of%20considering%20LLM-generated%20text%20detection%20a%20binary%20classification%0Atask%20of%20differentiating%20human%20from%20AI.%20Instead%2C%20we%20introduce%20a%20novel%20ternary%0Atext%20classification%20scheme%2C%20adding%20an%20%22undecided%22%20category%20for%20texts%20that%20could%0Abe%20attributed%20to%20either%20source%2C%20and%20we%20show%20that%20this%20new%20category%20is%20crucial%0Ato%20understand%20how%20to%20make%20the%20detection%20result%20more%20explainable%20to%20lay%20users.%0AThis%20research%20shifts%20the%20paradigm%20from%20merely%20classifying%20to%20explaining%0Amachine-generated%20texts%2C%20emphasizing%20need%20for%20detectors%20to%20provide%20clear%20and%0Aunderstandable%20explanations%20to%20users.%20Our%20study%20involves%20creating%20four%20new%0Adatasets%20comprised%20of%20texts%20from%20various%20LLMs%20and%20human%20authors.%20Based%20on%20new%0Adatasets%2C%20we%20performed%20binary%20classification%20tests%20to%20ascertain%20the%20most%0Aeffective%20SOTA%20detection%20methods%20and%20identified%20SOTA%20LLMs%20capable%20of%20producing%0Aharder-to-detect%20texts.%20We%20constructed%20a%20new%20dataset%20of%20texts%20generated%20by%20two%0Atop-performing%20LLMs%20and%20human%20authors%2C%20and%20asked%20three%20human%20annotators%20to%0Aproduce%20ternary%20labels%20with%20explanation%20notes.%20This%20dataset%20was%20used%20to%0Ainvestigate%20how%20three%20top-performing%20SOTA%20detectors%20behave%20in%20new%20ternary%0Aclassification%20context.%20Our%20results%20highlight%20why%20%22undecided%22%20category%20is%20much%0Aneeded%20from%20the%20viewpoint%20of%20explainability.%20Additionally%2C%20we%20conducted%20an%0Aanalysis%20of%20explainability%20of%20the%20three%20best-performing%20detectors%20and%20the%0Aexplanation%20notes%20of%20the%20human%20annotators%2C%20revealing%20insights%20about%20the%0Acomplexity%20of%20explainable%20detection%20of%20machine-generated%20texts.%20Finally%2C%20we%0Apropose%20guidelines%20for%20developing%20future%20detection%20systems%20with%20improved%0Aexplanatory%20power.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Machine-Generated%2520Texts%253A%2520Not%2520Just%2520%2522AI%2520vs%2520Humans%2522%2520and%250A%2520%2520Explainability%2520is%2520Complicated%26entry.906535625%3DJiazhou%2520Ji%2520and%2520Ruizhe%2520Li%2520and%2520Shujun%2520Li%2520and%2520Jie%2520Guo%2520and%2520Weidong%2520Qiu%2520and%2520Zheng%2520Huang%2520and%2520Chiyu%2520Chen%2520and%2520Xiaoyu%2520Jiang%2520and%2520Xinru%2520Lu%26entry.1292438233%3D%2520%2520As%2520LLMs%2520rapidly%2520advance%252C%2520increasing%2520concerns%2520arise%2520regarding%2520risks%2520about%250Aactual%2520authorship%2520of%2520texts%2520we%2520see%2520online%2520and%2520in%2520real%2520world.%2520The%2520task%2520of%250Adistinguishing%2520LLM-authored%2520texts%2520is%2520complicated%2520by%2520the%2520nuanced%2520and%2520overlapping%250Abehaviors%2520of%2520both%2520machines%2520and%2520humans.%2520In%2520this%2520paper%252C%2520we%2520challenge%2520the%2520current%250Apractice%2520of%2520considering%2520LLM-generated%2520text%2520detection%2520a%2520binary%2520classification%250Atask%2520of%2520differentiating%2520human%2520from%2520AI.%2520Instead%252C%2520we%2520introduce%2520a%2520novel%2520ternary%250Atext%2520classification%2520scheme%252C%2520adding%2520an%2520%2522undecided%2522%2520category%2520for%2520texts%2520that%2520could%250Abe%2520attributed%2520to%2520either%2520source%252C%2520and%2520we%2520show%2520that%2520this%2520new%2520category%2520is%2520crucial%250Ato%2520understand%2520how%2520to%2520make%2520the%2520detection%2520result%2520more%2520explainable%2520to%2520lay%2520users.%250AThis%2520research%2520shifts%2520the%2520paradigm%2520from%2520merely%2520classifying%2520to%2520explaining%250Amachine-generated%2520texts%252C%2520emphasizing%2520need%2520for%2520detectors%2520to%2520provide%2520clear%2520and%250Aunderstandable%2520explanations%2520to%2520users.%2520Our%2520study%2520involves%2520creating%2520four%2520new%250Adatasets%2520comprised%2520of%2520texts%2520from%2520various%2520LLMs%2520and%2520human%2520authors.%2520Based%2520on%2520new%250Adatasets%252C%2520we%2520performed%2520binary%2520classification%2520tests%2520to%2520ascertain%2520the%2520most%250Aeffective%2520SOTA%2520detection%2520methods%2520and%2520identified%2520SOTA%2520LLMs%2520capable%2520of%2520producing%250Aharder-to-detect%2520texts.%2520We%2520constructed%2520a%2520new%2520dataset%2520of%2520texts%2520generated%2520by%2520two%250Atop-performing%2520LLMs%2520and%2520human%2520authors%252C%2520and%2520asked%2520three%2520human%2520annotators%2520to%250Aproduce%2520ternary%2520labels%2520with%2520explanation%2520notes.%2520This%2520dataset%2520was%2520used%2520to%250Ainvestigate%2520how%2520three%2520top-performing%2520SOTA%2520detectors%2520behave%2520in%2520new%2520ternary%250Aclassification%2520context.%2520Our%2520results%2520highlight%2520why%2520%2522undecided%2522%2520category%2520is%2520much%250Aneeded%2520from%2520the%2520viewpoint%2520of%2520explainability.%2520Additionally%252C%2520we%2520conducted%2520an%250Aanalysis%2520of%2520explainability%2520of%2520the%2520three%2520best-performing%2520detectors%2520and%2520the%250Aexplanation%2520notes%2520of%2520the%2520human%2520annotators%252C%2520revealing%2520insights%2520about%2520the%250Acomplexity%2520of%2520explainable%2520detection%2520of%2520machine-generated%2520texts.%2520Finally%252C%2520we%250Apropose%2520guidelines%2520for%2520developing%2520future%2520detection%2520systems%2520with%2520improved%250Aexplanatory%2520power.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Machine-Generated%20Texts%3A%20Not%20Just%20%22AI%20vs%20Humans%22%20and%0A%20%20Explainability%20is%20Complicated&entry.906535625=Jiazhou%20Ji%20and%20Ruizhe%20Li%20and%20Shujun%20Li%20and%20Jie%20Guo%20and%20Weidong%20Qiu%20and%20Zheng%20Huang%20and%20Chiyu%20Chen%20and%20Xiaoyu%20Jiang%20and%20Xinru%20Lu&entry.1292438233=%20%20As%20LLMs%20rapidly%20advance%2C%20increasing%20concerns%20arise%20regarding%20risks%20about%0Aactual%20authorship%20of%20texts%20we%20see%20online%20and%20in%20real%20world.%20The%20task%20of%0Adistinguishing%20LLM-authored%20texts%20is%20complicated%20by%20the%20nuanced%20and%20overlapping%0Abehaviors%20of%20both%20machines%20and%20humans.%20In%20this%20paper%2C%20we%20challenge%20the%20current%0Apractice%20of%20considering%20LLM-generated%20text%20detection%20a%20binary%20classification%0Atask%20of%20differentiating%20human%20from%20AI.%20Instead%2C%20we%20introduce%20a%20novel%20ternary%0Atext%20classification%20scheme%2C%20adding%20an%20%22undecided%22%20category%20for%20texts%20that%20could%0Abe%20attributed%20to%20either%20source%2C%20and%20we%20show%20that%20this%20new%20category%20is%20crucial%0Ato%20understand%20how%20to%20make%20the%20detection%20result%20more%20explainable%20to%20lay%20users.%0AThis%20research%20shifts%20the%20paradigm%20from%20merely%20classifying%20to%20explaining%0Amachine-generated%20texts%2C%20emphasizing%20need%20for%20detectors%20to%20provide%20clear%20and%0Aunderstandable%20explanations%20to%20users.%20Our%20study%20involves%20creating%20four%20new%0Adatasets%20comprised%20of%20texts%20from%20various%20LLMs%20and%20human%20authors.%20Based%20on%20new%0Adatasets%2C%20we%20performed%20binary%20classification%20tests%20to%20ascertain%20the%20most%0Aeffective%20SOTA%20detection%20methods%20and%20identified%20SOTA%20LLMs%20capable%20of%20producing%0Aharder-to-detect%20texts.%20We%20constructed%20a%20new%20dataset%20of%20texts%20generated%20by%20two%0Atop-performing%20LLMs%20and%20human%20authors%2C%20and%20asked%20three%20human%20annotators%20to%0Aproduce%20ternary%20labels%20with%20explanation%20notes.%20This%20dataset%20was%20used%20to%0Ainvestigate%20how%20three%20top-performing%20SOTA%20detectors%20behave%20in%20new%20ternary%0Aclassification%20context.%20Our%20results%20highlight%20why%20%22undecided%22%20category%20is%20much%0Aneeded%20from%20the%20viewpoint%20of%20explainability.%20Additionally%2C%20we%20conducted%20an%0Aanalysis%20of%20explainability%20of%20the%20three%20best-performing%20detectors%20and%20the%0Aexplanation%20notes%20of%20the%20human%20annotators%2C%20revealing%20insights%20about%20the%0Acomplexity%20of%20explainable%20detection%20of%20machine-generated%20texts.%20Finally%2C%20we%0Apropose%20guidelines%20for%20developing%20future%20detection%20systems%20with%20improved%0Aexplanatory%20power.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18259v1&entry.124074799=Read"},
{"title": "Confident Natural Policy Gradient for Local Planning in\n  $q_\u03c0$-realizable Constrained MDPs", "author": "Tian Tian and Lin F. Yang and Csaba Szepesv\u00e1ri", "abstract": "  The constrained Markov decision process (CMDP) framework emerges as an\nimportant reinforcement learning approach for imposing safety or other critical\nobjectives while maximizing cumulative reward. However, the current\nunderstanding of how to learn efficiently in a CMDP environment with a\npotentially infinite number of states remains under investigation, particularly\nwhen function approximation is applied to the value functions. In this paper,\nwe address the learning problem given linear function approximation with\n$q_{\\pi}$-realizability, where the value functions of all policies are linearly\nrepresentable with a known feature map, a setting known to be more general and\nchallenging than other linear settings. Utilizing a local-access model, we\npropose a novel primal-dual algorithm that, after $\\tilde{O}(\\text{poly}(d)\n\\epsilon^{-3})$ queries, outputs with high probability a policy that strictly\nsatisfies the constraints while nearly optimizing the value with respect to a\nreward function. Here, $d$ is the feature dimension and $\\epsilon > 0$ is a\ngiven error. The algorithm relies on a carefully crafted off-policy evaluation\nprocedure to evaluate the policy using historical data, which informs policy\nupdates through policy gradients and conserves samples. To our knowledge, this\nis the first result achieving polynomial sample complexity for CMDP in the\n$q_{\\pi}$-realizable setting.\n", "link": "http://arxiv.org/abs/2406.18529v1", "date": "2024-06-26", "relevancy": 1.9723, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5085}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confident%20Natural%20Policy%20Gradient%20for%20Local%20Planning%20in%0A%20%20%24q_%CF%80%24-realizable%20Constrained%20MDPs&body=Title%3A%20Confident%20Natural%20Policy%20Gradient%20for%20Local%20Planning%20in%0A%20%20%24q_%CF%80%24-realizable%20Constrained%20MDPs%0AAuthor%3A%20Tian%20Tian%20and%20Lin%20F.%20Yang%20and%20Csaba%20Szepesv%C3%A1ri%0AAbstract%3A%20%20%20The%20constrained%20Markov%20decision%20process%20%28CMDP%29%20framework%20emerges%20as%20an%0Aimportant%20reinforcement%20learning%20approach%20for%20imposing%20safety%20or%20other%20critical%0Aobjectives%20while%20maximizing%20cumulative%20reward.%20However%2C%20the%20current%0Aunderstanding%20of%20how%20to%20learn%20efficiently%20in%20a%20CMDP%20environment%20with%20a%0Apotentially%20infinite%20number%20of%20states%20remains%20under%20investigation%2C%20particularly%0Awhen%20function%20approximation%20is%20applied%20to%20the%20value%20functions.%20In%20this%20paper%2C%0Awe%20address%20the%20learning%20problem%20given%20linear%20function%20approximation%20with%0A%24q_%7B%5Cpi%7D%24-realizability%2C%20where%20the%20value%20functions%20of%20all%20policies%20are%20linearly%0Arepresentable%20with%20a%20known%20feature%20map%2C%20a%20setting%20known%20to%20be%20more%20general%20and%0Achallenging%20than%20other%20linear%20settings.%20Utilizing%20a%20local-access%20model%2C%20we%0Apropose%20a%20novel%20primal-dual%20algorithm%20that%2C%20after%20%24%5Ctilde%7BO%7D%28%5Ctext%7Bpoly%7D%28d%29%0A%5Cepsilon%5E%7B-3%7D%29%24%20queries%2C%20outputs%20with%20high%20probability%20a%20policy%20that%20strictly%0Asatisfies%20the%20constraints%20while%20nearly%20optimizing%20the%20value%20with%20respect%20to%20a%0Areward%20function.%20Here%2C%20%24d%24%20is%20the%20feature%20dimension%20and%20%24%5Cepsilon%20%3E%200%24%20is%20a%0Agiven%20error.%20The%20algorithm%20relies%20on%20a%20carefully%20crafted%20off-policy%20evaluation%0Aprocedure%20to%20evaluate%20the%20policy%20using%20historical%20data%2C%20which%20informs%20policy%0Aupdates%20through%20policy%20gradients%20and%20conserves%20samples.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20result%20achieving%20polynomial%20sample%20complexity%20for%20CMDP%20in%20the%0A%24q_%7B%5Cpi%7D%24-realizable%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfident%2520Natural%2520Policy%2520Gradient%2520for%2520Local%2520Planning%2520in%250A%2520%2520%2524q_%25CF%2580%2524-realizable%2520Constrained%2520MDPs%26entry.906535625%3DTian%2520Tian%2520and%2520Lin%2520F.%2520Yang%2520and%2520Csaba%2520Szepesv%25C3%25A1ri%26entry.1292438233%3D%2520%2520The%2520constrained%2520Markov%2520decision%2520process%2520%2528CMDP%2529%2520framework%2520emerges%2520as%2520an%250Aimportant%2520reinforcement%2520learning%2520approach%2520for%2520imposing%2520safety%2520or%2520other%2520critical%250Aobjectives%2520while%2520maximizing%2520cumulative%2520reward.%2520However%252C%2520the%2520current%250Aunderstanding%2520of%2520how%2520to%2520learn%2520efficiently%2520in%2520a%2520CMDP%2520environment%2520with%2520a%250Apotentially%2520infinite%2520number%2520of%2520states%2520remains%2520under%2520investigation%252C%2520particularly%250Awhen%2520function%2520approximation%2520is%2520applied%2520to%2520the%2520value%2520functions.%2520In%2520this%2520paper%252C%250Awe%2520address%2520the%2520learning%2520problem%2520given%2520linear%2520function%2520approximation%2520with%250A%2524q_%257B%255Cpi%257D%2524-realizability%252C%2520where%2520the%2520value%2520functions%2520of%2520all%2520policies%2520are%2520linearly%250Arepresentable%2520with%2520a%2520known%2520feature%2520map%252C%2520a%2520setting%2520known%2520to%2520be%2520more%2520general%2520and%250Achallenging%2520than%2520other%2520linear%2520settings.%2520Utilizing%2520a%2520local-access%2520model%252C%2520we%250Apropose%2520a%2520novel%2520primal-dual%2520algorithm%2520that%252C%2520after%2520%2524%255Ctilde%257BO%257D%2528%255Ctext%257Bpoly%257D%2528d%2529%250A%255Cepsilon%255E%257B-3%257D%2529%2524%2520queries%252C%2520outputs%2520with%2520high%2520probability%2520a%2520policy%2520that%2520strictly%250Asatisfies%2520the%2520constraints%2520while%2520nearly%2520optimizing%2520the%2520value%2520with%2520respect%2520to%2520a%250Areward%2520function.%2520Here%252C%2520%2524d%2524%2520is%2520the%2520feature%2520dimension%2520and%2520%2524%255Cepsilon%2520%253E%25200%2524%2520is%2520a%250Agiven%2520error.%2520The%2520algorithm%2520relies%2520on%2520a%2520carefully%2520crafted%2520off-policy%2520evaluation%250Aprocedure%2520to%2520evaluate%2520the%2520policy%2520using%2520historical%2520data%252C%2520which%2520informs%2520policy%250Aupdates%2520through%2520policy%2520gradients%2520and%2520conserves%2520samples.%2520To%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520result%2520achieving%2520polynomial%2520sample%2520complexity%2520for%2520CMDP%2520in%2520the%250A%2524q_%257B%255Cpi%257D%2524-realizable%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confident%20Natural%20Policy%20Gradient%20for%20Local%20Planning%20in%0A%20%20%24q_%CF%80%24-realizable%20Constrained%20MDPs&entry.906535625=Tian%20Tian%20and%20Lin%20F.%20Yang%20and%20Csaba%20Szepesv%C3%A1ri&entry.1292438233=%20%20The%20constrained%20Markov%20decision%20process%20%28CMDP%29%20framework%20emerges%20as%20an%0Aimportant%20reinforcement%20learning%20approach%20for%20imposing%20safety%20or%20other%20critical%0Aobjectives%20while%20maximizing%20cumulative%20reward.%20However%2C%20the%20current%0Aunderstanding%20of%20how%20to%20learn%20efficiently%20in%20a%20CMDP%20environment%20with%20a%0Apotentially%20infinite%20number%20of%20states%20remains%20under%20investigation%2C%20particularly%0Awhen%20function%20approximation%20is%20applied%20to%20the%20value%20functions.%20In%20this%20paper%2C%0Awe%20address%20the%20learning%20problem%20given%20linear%20function%20approximation%20with%0A%24q_%7B%5Cpi%7D%24-realizability%2C%20where%20the%20value%20functions%20of%20all%20policies%20are%20linearly%0Arepresentable%20with%20a%20known%20feature%20map%2C%20a%20setting%20known%20to%20be%20more%20general%20and%0Achallenging%20than%20other%20linear%20settings.%20Utilizing%20a%20local-access%20model%2C%20we%0Apropose%20a%20novel%20primal-dual%20algorithm%20that%2C%20after%20%24%5Ctilde%7BO%7D%28%5Ctext%7Bpoly%7D%28d%29%0A%5Cepsilon%5E%7B-3%7D%29%24%20queries%2C%20outputs%20with%20high%20probability%20a%20policy%20that%20strictly%0Asatisfies%20the%20constraints%20while%20nearly%20optimizing%20the%20value%20with%20respect%20to%20a%0Areward%20function.%20Here%2C%20%24d%24%20is%20the%20feature%20dimension%20and%20%24%5Cepsilon%20%3E%200%24%20is%20a%0Agiven%20error.%20The%20algorithm%20relies%20on%20a%20carefully%20crafted%20off-policy%20evaluation%0Aprocedure%20to%20evaluate%20the%20policy%20using%20historical%20data%2C%20which%20informs%20policy%0Aupdates%20through%20policy%20gradients%20and%20conserves%20samples.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20result%20achieving%20polynomial%20sample%20complexity%20for%20CMDP%20in%20the%0A%24q_%7B%5Cpi%7D%24-realizable%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18529v1&entry.124074799=Read"},
{"title": "Robustness to Subpopulation Shift with Domain Label Noise via\n  Regularized Annotation of Domains", "author": "Nathan Stromberg and Rohan Ayyagari and Monica Welfert and Sanmi Koyejo and Richard Nock and Lalitha Sankar", "abstract": "  Existing methods for last layer retraining that aim to optimize worst-group\naccuracy (WGA) rely heavily on well-annotated groups in the training data. We\nshow, both in theory and practice, that annotation-based data augmentations\nusing either downsampling or upweighting for WGA are susceptible to domain\nannotation noise, and in high-noise regimes approach the WGA of a model trained\nwith vanilla empirical risk minimization. We introduce Regularized Annotation\nof Domains (RAD) in order to train robust last layer classifiers without the\nneed for explicit domain annotations. Our results show that RAD is competitive\nwith other recently proposed domain annotation-free techniques. Most\nimportantly, RAD outperforms state-of-the-art annotation-reliant methods even\nwith only 5% noise in the training data for several publicly available\ndatasets.\n", "link": "http://arxiv.org/abs/2402.11039v2", "date": "2024-06-26", "relevancy": 1.9705, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5058}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4901}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20to%20Subpopulation%20Shift%20with%20Domain%20Label%20Noise%20via%0A%20%20Regularized%20Annotation%20of%20Domains&body=Title%3A%20Robustness%20to%20Subpopulation%20Shift%20with%20Domain%20Label%20Noise%20via%0A%20%20Regularized%20Annotation%20of%20Domains%0AAuthor%3A%20Nathan%20Stromberg%20and%20Rohan%20Ayyagari%20and%20Monica%20Welfert%20and%20Sanmi%20Koyejo%20and%20Richard%20Nock%20and%20Lalitha%20Sankar%0AAbstract%3A%20%20%20Existing%20methods%20for%20last%20layer%20retraining%20that%20aim%20to%20optimize%20worst-group%0Aaccuracy%20%28WGA%29%20rely%20heavily%20on%20well-annotated%20groups%20in%20the%20training%20data.%20We%0Ashow%2C%20both%20in%20theory%20and%20practice%2C%20that%20annotation-based%20data%20augmentations%0Ausing%20either%20downsampling%20or%20upweighting%20for%20WGA%20are%20susceptible%20to%20domain%0Aannotation%20noise%2C%20and%20in%20high-noise%20regimes%20approach%20the%20WGA%20of%20a%20model%20trained%0Awith%20vanilla%20empirical%20risk%20minimization.%20We%20introduce%20Regularized%20Annotation%0Aof%20Domains%20%28RAD%29%20in%20order%20to%20train%20robust%20last%20layer%20classifiers%20without%20the%0Aneed%20for%20explicit%20domain%20annotations.%20Our%20results%20show%20that%20RAD%20is%20competitive%0Awith%20other%20recently%20proposed%20domain%20annotation-free%20techniques.%20Most%0Aimportantly%2C%20RAD%20outperforms%20state-of-the-art%20annotation-reliant%20methods%20even%0Awith%20only%205%25%20noise%20in%20the%20training%20data%20for%20several%20publicly%20available%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520to%2520Subpopulation%2520Shift%2520with%2520Domain%2520Label%2520Noise%2520via%250A%2520%2520Regularized%2520Annotation%2520of%2520Domains%26entry.906535625%3DNathan%2520Stromberg%2520and%2520Rohan%2520Ayyagari%2520and%2520Monica%2520Welfert%2520and%2520Sanmi%2520Koyejo%2520and%2520Richard%2520Nock%2520and%2520Lalitha%2520Sankar%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520last%2520layer%2520retraining%2520that%2520aim%2520to%2520optimize%2520worst-group%250Aaccuracy%2520%2528WGA%2529%2520rely%2520heavily%2520on%2520well-annotated%2520groups%2520in%2520the%2520training%2520data.%2520We%250Ashow%252C%2520both%2520in%2520theory%2520and%2520practice%252C%2520that%2520annotation-based%2520data%2520augmentations%250Ausing%2520either%2520downsampling%2520or%2520upweighting%2520for%2520WGA%2520are%2520susceptible%2520to%2520domain%250Aannotation%2520noise%252C%2520and%2520in%2520high-noise%2520regimes%2520approach%2520the%2520WGA%2520of%2520a%2520model%2520trained%250Awith%2520vanilla%2520empirical%2520risk%2520minimization.%2520We%2520introduce%2520Regularized%2520Annotation%250Aof%2520Domains%2520%2528RAD%2529%2520in%2520order%2520to%2520train%2520robust%2520last%2520layer%2520classifiers%2520without%2520the%250Aneed%2520for%2520explicit%2520domain%2520annotations.%2520Our%2520results%2520show%2520that%2520RAD%2520is%2520competitive%250Awith%2520other%2520recently%2520proposed%2520domain%2520annotation-free%2520techniques.%2520Most%250Aimportantly%252C%2520RAD%2520outperforms%2520state-of-the-art%2520annotation-reliant%2520methods%2520even%250Awith%2520only%25205%2525%2520noise%2520in%2520the%2520training%2520data%2520for%2520several%2520publicly%2520available%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20to%20Subpopulation%20Shift%20with%20Domain%20Label%20Noise%20via%0A%20%20Regularized%20Annotation%20of%20Domains&entry.906535625=Nathan%20Stromberg%20and%20Rohan%20Ayyagari%20and%20Monica%20Welfert%20and%20Sanmi%20Koyejo%20and%20Richard%20Nock%20and%20Lalitha%20Sankar&entry.1292438233=%20%20Existing%20methods%20for%20last%20layer%20retraining%20that%20aim%20to%20optimize%20worst-group%0Aaccuracy%20%28WGA%29%20rely%20heavily%20on%20well-annotated%20groups%20in%20the%20training%20data.%20We%0Ashow%2C%20both%20in%20theory%20and%20practice%2C%20that%20annotation-based%20data%20augmentations%0Ausing%20either%20downsampling%20or%20upweighting%20for%20WGA%20are%20susceptible%20to%20domain%0Aannotation%20noise%2C%20and%20in%20high-noise%20regimes%20approach%20the%20WGA%20of%20a%20model%20trained%0Awith%20vanilla%20empirical%20risk%20minimization.%20We%20introduce%20Regularized%20Annotation%0Aof%20Domains%20%28RAD%29%20in%20order%20to%20train%20robust%20last%20layer%20classifiers%20without%20the%0Aneed%20for%20explicit%20domain%20annotations.%20Our%20results%20show%20that%20RAD%20is%20competitive%0Awith%20other%20recently%20proposed%20domain%20annotation-free%20techniques.%20Most%0Aimportantly%2C%20RAD%20outperforms%20state-of-the-art%20annotation-reliant%20methods%20even%0Awith%20only%205%25%20noise%20in%20the%20training%20data%20for%20several%20publicly%20available%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11039v2&entry.124074799=Read"},
{"title": "Jina CLIP: Your CLIP Model Is Also Your Text Retriever", "author": "Andreas Koukounas and Georgios Mastrapas and Michael G\u00fcnther and Bo Wang and Scott Martens and Isabelle Mohr and Saba Sturua and Mohammad Kalim Akram and Joan Fontanals Mart\u00ednez and Saahil Ognawala and Susana Guzman and Maximilian Werk and Nan Wang and Han Xiao", "abstract": "  Contrastive Language-Image Pretraining (CLIP) is widely used to train models\nto align images and texts in a common embedding space by mapping them to\nfixed-sized vectors. These models are key to multimodal information retrieval\nand related tasks. However, CLIP models generally underperform in text-only\ntasks compared to specialized text models. This creates inefficiencies for\ninformation retrieval systems that keep separate embeddings and models for\ntext-only and multimodal tasks. We propose a novel, multi-task contrastive\ntraining method to address this issue, which we use to train the jina-clip-v1\nmodel to achieve the state-of-the-art performance on both text-image and\ntext-text retrieval tasks.\n", "link": "http://arxiv.org/abs/2405.20204v2", "date": "2024-06-26", "relevancy": 1.9627, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.552}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4583}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jina%20CLIP%3A%20Your%20CLIP%20Model%20Is%20Also%20Your%20Text%20Retriever&body=Title%3A%20Jina%20CLIP%3A%20Your%20CLIP%20Model%20Is%20Also%20Your%20Text%20Retriever%0AAuthor%3A%20Andreas%20Koukounas%20and%20Georgios%20Mastrapas%20and%20Michael%20G%C3%BCnther%20and%20Bo%20Wang%20and%20Scott%20Martens%20and%20Isabelle%20Mohr%20and%20Saba%20Sturua%20and%20Mohammad%20Kalim%20Akram%20and%20Joan%20Fontanals%20Mart%C3%ADnez%20and%20Saahil%20Ognawala%20and%20Susana%20Guzman%20and%20Maximilian%20Werk%20and%20Nan%20Wang%20and%20Han%20Xiao%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20is%20widely%20used%20to%20train%20models%0Ato%20align%20images%20and%20texts%20in%20a%20common%20embedding%20space%20by%20mapping%20them%20to%0Afixed-sized%20vectors.%20These%20models%20are%20key%20to%20multimodal%20information%20retrieval%0Aand%20related%20tasks.%20However%2C%20CLIP%20models%20generally%20underperform%20in%20text-only%0Atasks%20compared%20to%20specialized%20text%20models.%20This%20creates%20inefficiencies%20for%0Ainformation%20retrieval%20systems%20that%20keep%20separate%20embeddings%20and%20models%20for%0Atext-only%20and%20multimodal%20tasks.%20We%20propose%20a%20novel%2C%20multi-task%20contrastive%0Atraining%20method%20to%20address%20this%20issue%2C%20which%20we%20use%20to%20train%20the%20jina-clip-v1%0Amodel%20to%20achieve%20the%20state-of-the-art%20performance%20on%20both%20text-image%20and%0Atext-text%20retrieval%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJina%2520CLIP%253A%2520Your%2520CLIP%2520Model%2520Is%2520Also%2520Your%2520Text%2520Retriever%26entry.906535625%3DAndreas%2520Koukounas%2520and%2520Georgios%2520Mastrapas%2520and%2520Michael%2520G%25C3%25BCnther%2520and%2520Bo%2520Wang%2520and%2520Scott%2520Martens%2520and%2520Isabelle%2520Mohr%2520and%2520Saba%2520Sturua%2520and%2520Mohammad%2520Kalim%2520Akram%2520and%2520Joan%2520Fontanals%2520Mart%25C3%25ADnez%2520and%2520Saahil%2520Ognawala%2520and%2520Susana%2520Guzman%2520and%2520Maximilian%2520Werk%2520and%2520Nan%2520Wang%2520and%2520Han%2520Xiao%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520is%2520widely%2520used%2520to%2520train%2520models%250Ato%2520align%2520images%2520and%2520texts%2520in%2520a%2520common%2520embedding%2520space%2520by%2520mapping%2520them%2520to%250Afixed-sized%2520vectors.%2520These%2520models%2520are%2520key%2520to%2520multimodal%2520information%2520retrieval%250Aand%2520related%2520tasks.%2520However%252C%2520CLIP%2520models%2520generally%2520underperform%2520in%2520text-only%250Atasks%2520compared%2520to%2520specialized%2520text%2520models.%2520This%2520creates%2520inefficiencies%2520for%250Ainformation%2520retrieval%2520systems%2520that%2520keep%2520separate%2520embeddings%2520and%2520models%2520for%250Atext-only%2520and%2520multimodal%2520tasks.%2520We%2520propose%2520a%2520novel%252C%2520multi-task%2520contrastive%250Atraining%2520method%2520to%2520address%2520this%2520issue%252C%2520which%2520we%2520use%2520to%2520train%2520the%2520jina-clip-v1%250Amodel%2520to%2520achieve%2520the%2520state-of-the-art%2520performance%2520on%2520both%2520text-image%2520and%250Atext-text%2520retrieval%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jina%20CLIP%3A%20Your%20CLIP%20Model%20Is%20Also%20Your%20Text%20Retriever&entry.906535625=Andreas%20Koukounas%20and%20Georgios%20Mastrapas%20and%20Michael%20G%C3%BCnther%20and%20Bo%20Wang%20and%20Scott%20Martens%20and%20Isabelle%20Mohr%20and%20Saba%20Sturua%20and%20Mohammad%20Kalim%20Akram%20and%20Joan%20Fontanals%20Mart%C3%ADnez%20and%20Saahil%20Ognawala%20and%20Susana%20Guzman%20and%20Maximilian%20Werk%20and%20Nan%20Wang%20and%20Han%20Xiao&entry.1292438233=%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20is%20widely%20used%20to%20train%20models%0Ato%20align%20images%20and%20texts%20in%20a%20common%20embedding%20space%20by%20mapping%20them%20to%0Afixed-sized%20vectors.%20These%20models%20are%20key%20to%20multimodal%20information%20retrieval%0Aand%20related%20tasks.%20However%2C%20CLIP%20models%20generally%20underperform%20in%20text-only%0Atasks%20compared%20to%20specialized%20text%20models.%20This%20creates%20inefficiencies%20for%0Ainformation%20retrieval%20systems%20that%20keep%20separate%20embeddings%20and%20models%20for%0Atext-only%20and%20multimodal%20tasks.%20We%20propose%20a%20novel%2C%20multi-task%20contrastive%0Atraining%20method%20to%20address%20this%20issue%2C%20which%20we%20use%20to%20train%20the%20jina-clip-v1%0Amodel%20to%20achieve%20the%20state-of-the-art%20performance%20on%20both%20text-image%20and%0Atext-text%20retrieval%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20204v2&entry.124074799=Read"},
{"title": "Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with\n  1-to-K Contrastive Learning", "author": "Zhijie Nie and Richong Zhang and Zhangchi Feng and Hailang Huang and Xudong Liu", "abstract": "  Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search,\nwhich aims to break the barriers between modality and language simultaneously\nand achieves image-text retrieval in the multi-lingual scenario with a single\nmodel. In recent years, excellent progress has been made based on cross-lingual\ncross-modal pre-training; particularly, the methods based on contrastive\nlearning on large-scale data have significantly improved retrieval tasks.\nHowever, these methods directly follow the existing pre-training methods in the\ncross-lingual or cross-modal domain, leading to two problems of inconsistency\nin CCR: The methods with cross-lingual style suffer from the intra-modal error\npropagation, resulting in inconsistent recall performance across languages in\nthe whole dataset. The methods with cross-modal style suffer from the\ninter-modal optimization direction bias, resulting in inconsistent rank across\nlanguages within each instance, which cannot be reflected by Recall@K. To solve\nthese problems, we propose a simple but effective 1-to-K contrastive learning\nmethod, which treats each language equally and eliminates error propagation and\noptimization bias. In addition, we propose a new evaluation metric, Mean Rank\nVariance (MRV), to reflect the rank inconsistency across languages within each\ninstance. Extensive experiments on four CCR datasets show that our method\nimproves both recall rates and MRV with smaller-scale pre-trained data,\nachieving the new state-of-art.\n", "link": "http://arxiv.org/abs/2406.18254v1", "date": "2024-06-26", "relevancy": 1.9616, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4647}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Consistency%20in%20Cross-Lingual%20Cross-Modal%20Retrieval%20with%0A%20%201-to-K%20Contrastive%20Learning&body=Title%3A%20Improving%20the%20Consistency%20in%20Cross-Lingual%20Cross-Modal%20Retrieval%20with%0A%20%201-to-K%20Contrastive%20Learning%0AAuthor%3A%20Zhijie%20Nie%20and%20Richong%20Zhang%20and%20Zhangchi%20Feng%20and%20Hailang%20Huang%20and%20Xudong%20Liu%0AAbstract%3A%20%20%20Cross-lingual%20Cross-modal%20Retrieval%20%28CCR%29%20is%20an%20essential%20task%20in%20web%20search%2C%0Awhich%20aims%20to%20break%20the%20barriers%20between%20modality%20and%20language%20simultaneously%0Aand%20achieves%20image-text%20retrieval%20in%20the%20multi-lingual%20scenario%20with%20a%20single%0Amodel.%20In%20recent%20years%2C%20excellent%20progress%20has%20been%20made%20based%20on%20cross-lingual%0Across-modal%20pre-training%3B%20particularly%2C%20the%20methods%20based%20on%20contrastive%0Alearning%20on%20large-scale%20data%20have%20significantly%20improved%20retrieval%20tasks.%0AHowever%2C%20these%20methods%20directly%20follow%20the%20existing%20pre-training%20methods%20in%20the%0Across-lingual%20or%20cross-modal%20domain%2C%20leading%20to%20two%20problems%20of%20inconsistency%0Ain%20CCR%3A%20The%20methods%20with%20cross-lingual%20style%20suffer%20from%20the%20intra-modal%20error%0Apropagation%2C%20resulting%20in%20inconsistent%20recall%20performance%20across%20languages%20in%0Athe%20whole%20dataset.%20The%20methods%20with%20cross-modal%20style%20suffer%20from%20the%0Ainter-modal%20optimization%20direction%20bias%2C%20resulting%20in%20inconsistent%20rank%20across%0Alanguages%20within%20each%20instance%2C%20which%20cannot%20be%20reflected%20by%20Recall%40K.%20To%20solve%0Athese%20problems%2C%20we%20propose%20a%20simple%20but%20effective%201-to-K%20contrastive%20learning%0Amethod%2C%20which%20treats%20each%20language%20equally%20and%20eliminates%20error%20propagation%20and%0Aoptimization%20bias.%20In%20addition%2C%20we%20propose%20a%20new%20evaluation%20metric%2C%20Mean%20Rank%0AVariance%20%28MRV%29%2C%20to%20reflect%20the%20rank%20inconsistency%20across%20languages%20within%20each%0Ainstance.%20Extensive%20experiments%20on%20four%20CCR%20datasets%20show%20that%20our%20method%0Aimproves%20both%20recall%20rates%20and%20MRV%20with%20smaller-scale%20pre-trained%20data%2C%0Aachieving%20the%20new%20state-of-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Consistency%2520in%2520Cross-Lingual%2520Cross-Modal%2520Retrieval%2520with%250A%2520%25201-to-K%2520Contrastive%2520Learning%26entry.906535625%3DZhijie%2520Nie%2520and%2520Richong%2520Zhang%2520and%2520Zhangchi%2520Feng%2520and%2520Hailang%2520Huang%2520and%2520Xudong%2520Liu%26entry.1292438233%3D%2520%2520Cross-lingual%2520Cross-modal%2520Retrieval%2520%2528CCR%2529%2520is%2520an%2520essential%2520task%2520in%2520web%2520search%252C%250Awhich%2520aims%2520to%2520break%2520the%2520barriers%2520between%2520modality%2520and%2520language%2520simultaneously%250Aand%2520achieves%2520image-text%2520retrieval%2520in%2520the%2520multi-lingual%2520scenario%2520with%2520a%2520single%250Amodel.%2520In%2520recent%2520years%252C%2520excellent%2520progress%2520has%2520been%2520made%2520based%2520on%2520cross-lingual%250Across-modal%2520pre-training%253B%2520particularly%252C%2520the%2520methods%2520based%2520on%2520contrastive%250Alearning%2520on%2520large-scale%2520data%2520have%2520significantly%2520improved%2520retrieval%2520tasks.%250AHowever%252C%2520these%2520methods%2520directly%2520follow%2520the%2520existing%2520pre-training%2520methods%2520in%2520the%250Across-lingual%2520or%2520cross-modal%2520domain%252C%2520leading%2520to%2520two%2520problems%2520of%2520inconsistency%250Ain%2520CCR%253A%2520The%2520methods%2520with%2520cross-lingual%2520style%2520suffer%2520from%2520the%2520intra-modal%2520error%250Apropagation%252C%2520resulting%2520in%2520inconsistent%2520recall%2520performance%2520across%2520languages%2520in%250Athe%2520whole%2520dataset.%2520The%2520methods%2520with%2520cross-modal%2520style%2520suffer%2520from%2520the%250Ainter-modal%2520optimization%2520direction%2520bias%252C%2520resulting%2520in%2520inconsistent%2520rank%2520across%250Alanguages%2520within%2520each%2520instance%252C%2520which%2520cannot%2520be%2520reflected%2520by%2520Recall%2540K.%2520To%2520solve%250Athese%2520problems%252C%2520we%2520propose%2520a%2520simple%2520but%2520effective%25201-to-K%2520contrastive%2520learning%250Amethod%252C%2520which%2520treats%2520each%2520language%2520equally%2520and%2520eliminates%2520error%2520propagation%2520and%250Aoptimization%2520bias.%2520In%2520addition%252C%2520we%2520propose%2520a%2520new%2520evaluation%2520metric%252C%2520Mean%2520Rank%250AVariance%2520%2528MRV%2529%252C%2520to%2520reflect%2520the%2520rank%2520inconsistency%2520across%2520languages%2520within%2520each%250Ainstance.%2520Extensive%2520experiments%2520on%2520four%2520CCR%2520datasets%2520show%2520that%2520our%2520method%250Aimproves%2520both%2520recall%2520rates%2520and%2520MRV%2520with%2520smaller-scale%2520pre-trained%2520data%252C%250Aachieving%2520the%2520new%2520state-of-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Consistency%20in%20Cross-Lingual%20Cross-Modal%20Retrieval%20with%0A%20%201-to-K%20Contrastive%20Learning&entry.906535625=Zhijie%20Nie%20and%20Richong%20Zhang%20and%20Zhangchi%20Feng%20and%20Hailang%20Huang%20and%20Xudong%20Liu&entry.1292438233=%20%20Cross-lingual%20Cross-modal%20Retrieval%20%28CCR%29%20is%20an%20essential%20task%20in%20web%20search%2C%0Awhich%20aims%20to%20break%20the%20barriers%20between%20modality%20and%20language%20simultaneously%0Aand%20achieves%20image-text%20retrieval%20in%20the%20multi-lingual%20scenario%20with%20a%20single%0Amodel.%20In%20recent%20years%2C%20excellent%20progress%20has%20been%20made%20based%20on%20cross-lingual%0Across-modal%20pre-training%3B%20particularly%2C%20the%20methods%20based%20on%20contrastive%0Alearning%20on%20large-scale%20data%20have%20significantly%20improved%20retrieval%20tasks.%0AHowever%2C%20these%20methods%20directly%20follow%20the%20existing%20pre-training%20methods%20in%20the%0Across-lingual%20or%20cross-modal%20domain%2C%20leading%20to%20two%20problems%20of%20inconsistency%0Ain%20CCR%3A%20The%20methods%20with%20cross-lingual%20style%20suffer%20from%20the%20intra-modal%20error%0Apropagation%2C%20resulting%20in%20inconsistent%20recall%20performance%20across%20languages%20in%0Athe%20whole%20dataset.%20The%20methods%20with%20cross-modal%20style%20suffer%20from%20the%0Ainter-modal%20optimization%20direction%20bias%2C%20resulting%20in%20inconsistent%20rank%20across%0Alanguages%20within%20each%20instance%2C%20which%20cannot%20be%20reflected%20by%20Recall%40K.%20To%20solve%0Athese%20problems%2C%20we%20propose%20a%20simple%20but%20effective%201-to-K%20contrastive%20learning%0Amethod%2C%20which%20treats%20each%20language%20equally%20and%20eliminates%20error%20propagation%20and%0Aoptimization%20bias.%20In%20addition%2C%20we%20propose%20a%20new%20evaluation%20metric%2C%20Mean%20Rank%0AVariance%20%28MRV%29%2C%20to%20reflect%20the%20rank%20inconsistency%20across%20languages%20within%20each%0Ainstance.%20Extensive%20experiments%20on%20four%20CCR%20datasets%20show%20that%20our%20method%0Aimproves%20both%20recall%20rates%20and%20MRV%20with%20smaller-scale%20pre-trained%20data%2C%0Aachieving%20the%20new%20state-of-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18254v1&entry.124074799=Read"},
{"title": "Differential error feedback for communication-efficient decentralized\n  learning", "author": "Roula Nassif and Stefan Vlaski and Marco Carpentiero and Vincenzo Matta and Ali H. Sayed", "abstract": "  Communication-constrained algorithms for decentralized learning and\noptimization rely on local updates coupled with the exchange of compressed\nsignals. In this context, differential quantization is an effective technique\nto mitigate the negative impact of compression by leveraging correlations\nbetween successive iterates. In addition, the use of error feedback, which\nconsists of incorporating the compression error into subsequent steps, is a\npowerful mechanism to compensate for the bias caused by the compression. Under\nerror feedback, performance guarantees in the literature have so far focused on\nalgorithms employing a fusion center or a special class of contractive\ncompressors that cannot be implemented with a finite number of bits. In this\nwork, we propose a new decentralized communication-efficient learning approach\nthat blends differential quantization with error feedback. The approach is\nspecifically tailored for decentralized learning problems where agents have\nindividual risk functions to minimize subject to subspace constraints that\nrequire the minimizers across the network to lie in low-dimensional subspaces.\nThis constrained formulation includes consensus or single-task optimization as\nspecial cases, and allows for more general task relatedness models such as\nmultitask smoothness and coupled optimization. We show that, under some general\nconditions on the compression noise, and for sufficiently small step-sizes\n$\\mu$, the resulting communication-efficient strategy is stable both in terms\nof mean-square error and average bit rate: by reducing $\\mu$, it is possible to\nkeep the estimation errors small (on the order of $\\mu$) without increasing\nindefinitely the bit rate as $\\mu\\rightarrow 0$. The results establish that, in\nthe small step-size regime and with a finite number of bits, it is possible to\nattain the performance achievable in the absence of compression.\n", "link": "http://arxiv.org/abs/2406.18418v1", "date": "2024-06-26", "relevancy": 1.9512, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4946}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4864}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differential%20error%20feedback%20for%20communication-efficient%20decentralized%0A%20%20learning&body=Title%3A%20Differential%20error%20feedback%20for%20communication-efficient%20decentralized%0A%20%20learning%0AAuthor%3A%20Roula%20Nassif%20and%20Stefan%20Vlaski%20and%20Marco%20Carpentiero%20and%20Vincenzo%20Matta%20and%20Ali%20H.%20Sayed%0AAbstract%3A%20%20%20Communication-constrained%20algorithms%20for%20decentralized%20learning%20and%0Aoptimization%20rely%20on%20local%20updates%20coupled%20with%20the%20exchange%20of%20compressed%0Asignals.%20In%20this%20context%2C%20differential%20quantization%20is%20an%20effective%20technique%0Ato%20mitigate%20the%20negative%20impact%20of%20compression%20by%20leveraging%20correlations%0Abetween%20successive%20iterates.%20In%20addition%2C%20the%20use%20of%20error%20feedback%2C%20which%0Aconsists%20of%20incorporating%20the%20compression%20error%20into%20subsequent%20steps%2C%20is%20a%0Apowerful%20mechanism%20to%20compensate%20for%20the%20bias%20caused%20by%20the%20compression.%20Under%0Aerror%20feedback%2C%20performance%20guarantees%20in%20the%20literature%20have%20so%20far%20focused%20on%0Aalgorithms%20employing%20a%20fusion%20center%20or%20a%20special%20class%20of%20contractive%0Acompressors%20that%20cannot%20be%20implemented%20with%20a%20finite%20number%20of%20bits.%20In%20this%0Awork%2C%20we%20propose%20a%20new%20decentralized%20communication-efficient%20learning%20approach%0Athat%20blends%20differential%20quantization%20with%20error%20feedback.%20The%20approach%20is%0Aspecifically%20tailored%20for%20decentralized%20learning%20problems%20where%20agents%20have%0Aindividual%20risk%20functions%20to%20minimize%20subject%20to%20subspace%20constraints%20that%0Arequire%20the%20minimizers%20across%20the%20network%20to%20lie%20in%20low-dimensional%20subspaces.%0AThis%20constrained%20formulation%20includes%20consensus%20or%20single-task%20optimization%20as%0Aspecial%20cases%2C%20and%20allows%20for%20more%20general%20task%20relatedness%20models%20such%20as%0Amultitask%20smoothness%20and%20coupled%20optimization.%20We%20show%20that%2C%20under%20some%20general%0Aconditions%20on%20the%20compression%20noise%2C%20and%20for%20sufficiently%20small%20step-sizes%0A%24%5Cmu%24%2C%20the%20resulting%20communication-efficient%20strategy%20is%20stable%20both%20in%20terms%0Aof%20mean-square%20error%20and%20average%20bit%20rate%3A%20by%20reducing%20%24%5Cmu%24%2C%20it%20is%20possible%20to%0Akeep%20the%20estimation%20errors%20small%20%28on%20the%20order%20of%20%24%5Cmu%24%29%20without%20increasing%0Aindefinitely%20the%20bit%20rate%20as%20%24%5Cmu%5Crightarrow%200%24.%20The%20results%20establish%20that%2C%20in%0Athe%20small%20step-size%20regime%20and%20with%20a%20finite%20number%20of%20bits%2C%20it%20is%20possible%20to%0Aattain%20the%20performance%20achievable%20in%20the%20absence%20of%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferential%2520error%2520feedback%2520for%2520communication-efficient%2520decentralized%250A%2520%2520learning%26entry.906535625%3DRoula%2520Nassif%2520and%2520Stefan%2520Vlaski%2520and%2520Marco%2520Carpentiero%2520and%2520Vincenzo%2520Matta%2520and%2520Ali%2520H.%2520Sayed%26entry.1292438233%3D%2520%2520Communication-constrained%2520algorithms%2520for%2520decentralized%2520learning%2520and%250Aoptimization%2520rely%2520on%2520local%2520updates%2520coupled%2520with%2520the%2520exchange%2520of%2520compressed%250Asignals.%2520In%2520this%2520context%252C%2520differential%2520quantization%2520is%2520an%2520effective%2520technique%250Ato%2520mitigate%2520the%2520negative%2520impact%2520of%2520compression%2520by%2520leveraging%2520correlations%250Abetween%2520successive%2520iterates.%2520In%2520addition%252C%2520the%2520use%2520of%2520error%2520feedback%252C%2520which%250Aconsists%2520of%2520incorporating%2520the%2520compression%2520error%2520into%2520subsequent%2520steps%252C%2520is%2520a%250Apowerful%2520mechanism%2520to%2520compensate%2520for%2520the%2520bias%2520caused%2520by%2520the%2520compression.%2520Under%250Aerror%2520feedback%252C%2520performance%2520guarantees%2520in%2520the%2520literature%2520have%2520so%2520far%2520focused%2520on%250Aalgorithms%2520employing%2520a%2520fusion%2520center%2520or%2520a%2520special%2520class%2520of%2520contractive%250Acompressors%2520that%2520cannot%2520be%2520implemented%2520with%2520a%2520finite%2520number%2520of%2520bits.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520new%2520decentralized%2520communication-efficient%2520learning%2520approach%250Athat%2520blends%2520differential%2520quantization%2520with%2520error%2520feedback.%2520The%2520approach%2520is%250Aspecifically%2520tailored%2520for%2520decentralized%2520learning%2520problems%2520where%2520agents%2520have%250Aindividual%2520risk%2520functions%2520to%2520minimize%2520subject%2520to%2520subspace%2520constraints%2520that%250Arequire%2520the%2520minimizers%2520across%2520the%2520network%2520to%2520lie%2520in%2520low-dimensional%2520subspaces.%250AThis%2520constrained%2520formulation%2520includes%2520consensus%2520or%2520single-task%2520optimization%2520as%250Aspecial%2520cases%252C%2520and%2520allows%2520for%2520more%2520general%2520task%2520relatedness%2520models%2520such%2520as%250Amultitask%2520smoothness%2520and%2520coupled%2520optimization.%2520We%2520show%2520that%252C%2520under%2520some%2520general%250Aconditions%2520on%2520the%2520compression%2520noise%252C%2520and%2520for%2520sufficiently%2520small%2520step-sizes%250A%2524%255Cmu%2524%252C%2520the%2520resulting%2520communication-efficient%2520strategy%2520is%2520stable%2520both%2520in%2520terms%250Aof%2520mean-square%2520error%2520and%2520average%2520bit%2520rate%253A%2520by%2520reducing%2520%2524%255Cmu%2524%252C%2520it%2520is%2520possible%2520to%250Akeep%2520the%2520estimation%2520errors%2520small%2520%2528on%2520the%2520order%2520of%2520%2524%255Cmu%2524%2529%2520without%2520increasing%250Aindefinitely%2520the%2520bit%2520rate%2520as%2520%2524%255Cmu%255Crightarrow%25200%2524.%2520The%2520results%2520establish%2520that%252C%2520in%250Athe%2520small%2520step-size%2520regime%2520and%2520with%2520a%2520finite%2520number%2520of%2520bits%252C%2520it%2520is%2520possible%2520to%250Aattain%2520the%2520performance%2520achievable%2520in%2520the%2520absence%2520of%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differential%20error%20feedback%20for%20communication-efficient%20decentralized%0A%20%20learning&entry.906535625=Roula%20Nassif%20and%20Stefan%20Vlaski%20and%20Marco%20Carpentiero%20and%20Vincenzo%20Matta%20and%20Ali%20H.%20Sayed&entry.1292438233=%20%20Communication-constrained%20algorithms%20for%20decentralized%20learning%20and%0Aoptimization%20rely%20on%20local%20updates%20coupled%20with%20the%20exchange%20of%20compressed%0Asignals.%20In%20this%20context%2C%20differential%20quantization%20is%20an%20effective%20technique%0Ato%20mitigate%20the%20negative%20impact%20of%20compression%20by%20leveraging%20correlations%0Abetween%20successive%20iterates.%20In%20addition%2C%20the%20use%20of%20error%20feedback%2C%20which%0Aconsists%20of%20incorporating%20the%20compression%20error%20into%20subsequent%20steps%2C%20is%20a%0Apowerful%20mechanism%20to%20compensate%20for%20the%20bias%20caused%20by%20the%20compression.%20Under%0Aerror%20feedback%2C%20performance%20guarantees%20in%20the%20literature%20have%20so%20far%20focused%20on%0Aalgorithms%20employing%20a%20fusion%20center%20or%20a%20special%20class%20of%20contractive%0Acompressors%20that%20cannot%20be%20implemented%20with%20a%20finite%20number%20of%20bits.%20In%20this%0Awork%2C%20we%20propose%20a%20new%20decentralized%20communication-efficient%20learning%20approach%0Athat%20blends%20differential%20quantization%20with%20error%20feedback.%20The%20approach%20is%0Aspecifically%20tailored%20for%20decentralized%20learning%20problems%20where%20agents%20have%0Aindividual%20risk%20functions%20to%20minimize%20subject%20to%20subspace%20constraints%20that%0Arequire%20the%20minimizers%20across%20the%20network%20to%20lie%20in%20low-dimensional%20subspaces.%0AThis%20constrained%20formulation%20includes%20consensus%20or%20single-task%20optimization%20as%0Aspecial%20cases%2C%20and%20allows%20for%20more%20general%20task%20relatedness%20models%20such%20as%0Amultitask%20smoothness%20and%20coupled%20optimization.%20We%20show%20that%2C%20under%20some%20general%0Aconditions%20on%20the%20compression%20noise%2C%20and%20for%20sufficiently%20small%20step-sizes%0A%24%5Cmu%24%2C%20the%20resulting%20communication-efficient%20strategy%20is%20stable%20both%20in%20terms%0Aof%20mean-square%20error%20and%20average%20bit%20rate%3A%20by%20reducing%20%24%5Cmu%24%2C%20it%20is%20possible%20to%0Akeep%20the%20estimation%20errors%20small%20%28on%20the%20order%20of%20%24%5Cmu%24%29%20without%20increasing%0Aindefinitely%20the%20bit%20rate%20as%20%24%5Cmu%5Crightarrow%200%24.%20The%20results%20establish%20that%2C%20in%0Athe%20small%20step-size%20regime%20and%20with%20a%20finite%20number%20of%20bits%2C%20it%20is%20possible%20to%0Aattain%20the%20performance%20achievable%20in%20the%20absence%20of%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18418v1&entry.124074799=Read"},
{"title": "UniRec: A Dual Enhancement of Uniformity and Frequency in Sequential\n  Recommendations", "author": "Yang Liu and Yitong Wang and Chenyue Feng", "abstract": "  Representation learning in sequential recommendation is critical for\naccurately modeling user interaction patterns and improving recommendation\nprecision. However, existing approaches predominantly emphasize item-to-item\ntransitions, often neglecting the time intervals between interactions, which\nare closely related to behavior pattern changes. Additionally, broader\ninteraction attributes, such as item frequency, are frequently overlooked. We\nfound that both sequences with more uniform time intervals and items with\nhigher frequency yield better prediction performance. Conversely, non-uniform\nsequences exacerbate user interest drift and less-frequent items are difficult\nto model due to sparse sampling, presenting unique challenges inadequately\naddressed by current methods. In this paper, we propose UniRec, a novel\nbidirectional enhancement sequential recommendation method. UniRec leverages\nsequence uniformity and item frequency to enhance performance, particularly\nimproving the representation of non-uniform sequences and less-frequent items.\nThese two branches mutually reinforce each other, driving comprehensive\nperformance optimization in complex sequential recommendation scenarios.\nAdditionally, we present a multidimensional time module to further enhance\nadaptability. To the best of our knowledge, UniRec is the first method to\nutilize the characteristics of uniformity and frequency for feature\naugmentation. Comparing with eleven advanced models across four datasets, we\ndemonstrate that UniRec outperforms SOTA models significantly. The code is\navailable at https://github.com/Linxi000/UniRec.\n", "link": "http://arxiv.org/abs/2406.18470v1", "date": "2024-06-26", "relevancy": 1.9419, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4868}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4862}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniRec%3A%20A%20Dual%20Enhancement%20of%20Uniformity%20and%20Frequency%20in%20Sequential%0A%20%20Recommendations&body=Title%3A%20UniRec%3A%20A%20Dual%20Enhancement%20of%20Uniformity%20and%20Frequency%20in%20Sequential%0A%20%20Recommendations%0AAuthor%3A%20Yang%20Liu%20and%20Yitong%20Wang%20and%20Chenyue%20Feng%0AAbstract%3A%20%20%20Representation%20learning%20in%20sequential%20recommendation%20is%20critical%20for%0Aaccurately%20modeling%20user%20interaction%20patterns%20and%20improving%20recommendation%0Aprecision.%20However%2C%20existing%20approaches%20predominantly%20emphasize%20item-to-item%0Atransitions%2C%20often%20neglecting%20the%20time%20intervals%20between%20interactions%2C%20which%0Aare%20closely%20related%20to%20behavior%20pattern%20changes.%20Additionally%2C%20broader%0Ainteraction%20attributes%2C%20such%20as%20item%20frequency%2C%20are%20frequently%20overlooked.%20We%0Afound%20that%20both%20sequences%20with%20more%20uniform%20time%20intervals%20and%20items%20with%0Ahigher%20frequency%20yield%20better%20prediction%20performance.%20Conversely%2C%20non-uniform%0Asequences%20exacerbate%20user%20interest%20drift%20and%20less-frequent%20items%20are%20difficult%0Ato%20model%20due%20to%20sparse%20sampling%2C%20presenting%20unique%20challenges%20inadequately%0Aaddressed%20by%20current%20methods.%20In%20this%20paper%2C%20we%20propose%20UniRec%2C%20a%20novel%0Abidirectional%20enhancement%20sequential%20recommendation%20method.%20UniRec%20leverages%0Asequence%20uniformity%20and%20item%20frequency%20to%20enhance%20performance%2C%20particularly%0Aimproving%20the%20representation%20of%20non-uniform%20sequences%20and%20less-frequent%20items.%0AThese%20two%20branches%20mutually%20reinforce%20each%20other%2C%20driving%20comprehensive%0Aperformance%20optimization%20in%20complex%20sequential%20recommendation%20scenarios.%0AAdditionally%2C%20we%20present%20a%20multidimensional%20time%20module%20to%20further%20enhance%0Aadaptability.%20To%20the%20best%20of%20our%20knowledge%2C%20UniRec%20is%20the%20first%20method%20to%0Autilize%20the%20characteristics%20of%20uniformity%20and%20frequency%20for%20feature%0Aaugmentation.%20Comparing%20with%20eleven%20advanced%20models%20across%20four%20datasets%2C%20we%0Ademonstrate%20that%20UniRec%20outperforms%20SOTA%20models%20significantly.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/Linxi000/UniRec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniRec%253A%2520A%2520Dual%2520Enhancement%2520of%2520Uniformity%2520and%2520Frequency%2520in%2520Sequential%250A%2520%2520Recommendations%26entry.906535625%3DYang%2520Liu%2520and%2520Yitong%2520Wang%2520and%2520Chenyue%2520Feng%26entry.1292438233%3D%2520%2520Representation%2520learning%2520in%2520sequential%2520recommendation%2520is%2520critical%2520for%250Aaccurately%2520modeling%2520user%2520interaction%2520patterns%2520and%2520improving%2520recommendation%250Aprecision.%2520However%252C%2520existing%2520approaches%2520predominantly%2520emphasize%2520item-to-item%250Atransitions%252C%2520often%2520neglecting%2520the%2520time%2520intervals%2520between%2520interactions%252C%2520which%250Aare%2520closely%2520related%2520to%2520behavior%2520pattern%2520changes.%2520Additionally%252C%2520broader%250Ainteraction%2520attributes%252C%2520such%2520as%2520item%2520frequency%252C%2520are%2520frequently%2520overlooked.%2520We%250Afound%2520that%2520both%2520sequences%2520with%2520more%2520uniform%2520time%2520intervals%2520and%2520items%2520with%250Ahigher%2520frequency%2520yield%2520better%2520prediction%2520performance.%2520Conversely%252C%2520non-uniform%250Asequences%2520exacerbate%2520user%2520interest%2520drift%2520and%2520less-frequent%2520items%2520are%2520difficult%250Ato%2520model%2520due%2520to%2520sparse%2520sampling%252C%2520presenting%2520unique%2520challenges%2520inadequately%250Aaddressed%2520by%2520current%2520methods.%2520In%2520this%2520paper%252C%2520we%2520propose%2520UniRec%252C%2520a%2520novel%250Abidirectional%2520enhancement%2520sequential%2520recommendation%2520method.%2520UniRec%2520leverages%250Asequence%2520uniformity%2520and%2520item%2520frequency%2520to%2520enhance%2520performance%252C%2520particularly%250Aimproving%2520the%2520representation%2520of%2520non-uniform%2520sequences%2520and%2520less-frequent%2520items.%250AThese%2520two%2520branches%2520mutually%2520reinforce%2520each%2520other%252C%2520driving%2520comprehensive%250Aperformance%2520optimization%2520in%2520complex%2520sequential%2520recommendation%2520scenarios.%250AAdditionally%252C%2520we%2520present%2520a%2520multidimensional%2520time%2520module%2520to%2520further%2520enhance%250Aadaptability.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520UniRec%2520is%2520the%2520first%2520method%2520to%250Autilize%2520the%2520characteristics%2520of%2520uniformity%2520and%2520frequency%2520for%2520feature%250Aaugmentation.%2520Comparing%2520with%2520eleven%2520advanced%2520models%2520across%2520four%2520datasets%252C%2520we%250Ademonstrate%2520that%2520UniRec%2520outperforms%2520SOTA%2520models%2520significantly.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/Linxi000/UniRec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniRec%3A%20A%20Dual%20Enhancement%20of%20Uniformity%20and%20Frequency%20in%20Sequential%0A%20%20Recommendations&entry.906535625=Yang%20Liu%20and%20Yitong%20Wang%20and%20Chenyue%20Feng&entry.1292438233=%20%20Representation%20learning%20in%20sequential%20recommendation%20is%20critical%20for%0Aaccurately%20modeling%20user%20interaction%20patterns%20and%20improving%20recommendation%0Aprecision.%20However%2C%20existing%20approaches%20predominantly%20emphasize%20item-to-item%0Atransitions%2C%20often%20neglecting%20the%20time%20intervals%20between%20interactions%2C%20which%0Aare%20closely%20related%20to%20behavior%20pattern%20changes.%20Additionally%2C%20broader%0Ainteraction%20attributes%2C%20such%20as%20item%20frequency%2C%20are%20frequently%20overlooked.%20We%0Afound%20that%20both%20sequences%20with%20more%20uniform%20time%20intervals%20and%20items%20with%0Ahigher%20frequency%20yield%20better%20prediction%20performance.%20Conversely%2C%20non-uniform%0Asequences%20exacerbate%20user%20interest%20drift%20and%20less-frequent%20items%20are%20difficult%0Ato%20model%20due%20to%20sparse%20sampling%2C%20presenting%20unique%20challenges%20inadequately%0Aaddressed%20by%20current%20methods.%20In%20this%20paper%2C%20we%20propose%20UniRec%2C%20a%20novel%0Abidirectional%20enhancement%20sequential%20recommendation%20method.%20UniRec%20leverages%0Asequence%20uniformity%20and%20item%20frequency%20to%20enhance%20performance%2C%20particularly%0Aimproving%20the%20representation%20of%20non-uniform%20sequences%20and%20less-frequent%20items.%0AThese%20two%20branches%20mutually%20reinforce%20each%20other%2C%20driving%20comprehensive%0Aperformance%20optimization%20in%20complex%20sequential%20recommendation%20scenarios.%0AAdditionally%2C%20we%20present%20a%20multidimensional%20time%20module%20to%20further%20enhance%0Aadaptability.%20To%20the%20best%20of%20our%20knowledge%2C%20UniRec%20is%20the%20first%20method%20to%0Autilize%20the%20characteristics%20of%20uniformity%20and%20frequency%20for%20feature%0Aaugmentation.%20Comparing%20with%20eleven%20advanced%20models%20across%20four%20datasets%2C%20we%0Ademonstrate%20that%20UniRec%20outperforms%20SOTA%20models%20significantly.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/Linxi000/UniRec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18470v1&entry.124074799=Read"},
{"title": "ReLU Neural Networks with Linear Layers are Biased Towards Single- and\n  Multi-Index Models", "author": "Suzanna Parkinson and Greg Ongie and Rebecca Willett", "abstract": "  Neural networks often operate in the overparameterized regime, in which there\nare far more parameters than training samples, allowing the training data to be\nfit perfectly. That is, training the network effectively learns an\ninterpolating function, and properties of the interpolant affect predictions\nthe network will make on new samples. This manuscript explores how properties\nof such functions learned by neural networks of depth greater than two layers.\nOur framework considers a family of networks of varying depths that all have\nthe same capacity but different representation costs. The representation cost\nof a function induced by a neural network architecture is the minimum sum of\nsquared weights needed for the network to represent the function; it reflects\nthe function space bias associated with the architecture. Our results show that\nadding additional linear layers to the input side of a shallow ReLU network\nyields a representation cost favoring functions with low mixed variation - that\nis, it has limited variation in directions orthogonal to a low-dimensional\nsubspace and can be well approximated by a single- or multi-index model. Such\nfunctions may be represented by the composition of a function with low\ntwo-layer representation cost and a low-rank linear operator. Our experiments\nconfirm this behavior in standard network training regimes. They additionally\nshow that linear layers can improve generalization and the learned network is\nwell-aligned with the true latent low-dimensional linear subspace when data is\ngenerated using a multi-index model.\n", "link": "http://arxiv.org/abs/2305.15598v3", "date": "2024-06-26", "relevancy": 1.9244, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5052}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReLU%20Neural%20Networks%20with%20Linear%20Layers%20are%20Biased%20Towards%20Single-%20and%0A%20%20Multi-Index%20Models&body=Title%3A%20ReLU%20Neural%20Networks%20with%20Linear%20Layers%20are%20Biased%20Towards%20Single-%20and%0A%20%20Multi-Index%20Models%0AAuthor%3A%20Suzanna%20Parkinson%20and%20Greg%20Ongie%20and%20Rebecca%20Willett%0AAbstract%3A%20%20%20Neural%20networks%20often%20operate%20in%20the%20overparameterized%20regime%2C%20in%20which%20there%0Aare%20far%20more%20parameters%20than%20training%20samples%2C%20allowing%20the%20training%20data%20to%20be%0Afit%20perfectly.%20That%20is%2C%20training%20the%20network%20effectively%20learns%20an%0Ainterpolating%20function%2C%20and%20properties%20of%20the%20interpolant%20affect%20predictions%0Athe%20network%20will%20make%20on%20new%20samples.%20This%20manuscript%20explores%20how%20properties%0Aof%20such%20functions%20learned%20by%20neural%20networks%20of%20depth%20greater%20than%20two%20layers.%0AOur%20framework%20considers%20a%20family%20of%20networks%20of%20varying%20depths%20that%20all%20have%0Athe%20same%20capacity%20but%20different%20representation%20costs.%20The%20representation%20cost%0Aof%20a%20function%20induced%20by%20a%20neural%20network%20architecture%20is%20the%20minimum%20sum%20of%0Asquared%20weights%20needed%20for%20the%20network%20to%20represent%20the%20function%3B%20it%20reflects%0Athe%20function%20space%20bias%20associated%20with%20the%20architecture.%20Our%20results%20show%20that%0Aadding%20additional%20linear%20layers%20to%20the%20input%20side%20of%20a%20shallow%20ReLU%20network%0Ayields%20a%20representation%20cost%20favoring%20functions%20with%20low%20mixed%20variation%20-%20that%0Ais%2C%20it%20has%20limited%20variation%20in%20directions%20orthogonal%20to%20a%20low-dimensional%0Asubspace%20and%20can%20be%20well%20approximated%20by%20a%20single-%20or%20multi-index%20model.%20Such%0Afunctions%20may%20be%20represented%20by%20the%20composition%20of%20a%20function%20with%20low%0Atwo-layer%20representation%20cost%20and%20a%20low-rank%20linear%20operator.%20Our%20experiments%0Aconfirm%20this%20behavior%20in%20standard%20network%20training%20regimes.%20They%20additionally%0Ashow%20that%20linear%20layers%20can%20improve%20generalization%20and%20the%20learned%20network%20is%0Awell-aligned%20with%20the%20true%20latent%20low-dimensional%20linear%20subspace%20when%20data%20is%0Agenerated%20using%20a%20multi-index%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15598v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReLU%2520Neural%2520Networks%2520with%2520Linear%2520Layers%2520are%2520Biased%2520Towards%2520Single-%2520and%250A%2520%2520Multi-Index%2520Models%26entry.906535625%3DSuzanna%2520Parkinson%2520and%2520Greg%2520Ongie%2520and%2520Rebecca%2520Willett%26entry.1292438233%3D%2520%2520Neural%2520networks%2520often%2520operate%2520in%2520the%2520overparameterized%2520regime%252C%2520in%2520which%2520there%250Aare%2520far%2520more%2520parameters%2520than%2520training%2520samples%252C%2520allowing%2520the%2520training%2520data%2520to%2520be%250Afit%2520perfectly.%2520That%2520is%252C%2520training%2520the%2520network%2520effectively%2520learns%2520an%250Ainterpolating%2520function%252C%2520and%2520properties%2520of%2520the%2520interpolant%2520affect%2520predictions%250Athe%2520network%2520will%2520make%2520on%2520new%2520samples.%2520This%2520manuscript%2520explores%2520how%2520properties%250Aof%2520such%2520functions%2520learned%2520by%2520neural%2520networks%2520of%2520depth%2520greater%2520than%2520two%2520layers.%250AOur%2520framework%2520considers%2520a%2520family%2520of%2520networks%2520of%2520varying%2520depths%2520that%2520all%2520have%250Athe%2520same%2520capacity%2520but%2520different%2520representation%2520costs.%2520The%2520representation%2520cost%250Aof%2520a%2520function%2520induced%2520by%2520a%2520neural%2520network%2520architecture%2520is%2520the%2520minimum%2520sum%2520of%250Asquared%2520weights%2520needed%2520for%2520the%2520network%2520to%2520represent%2520the%2520function%253B%2520it%2520reflects%250Athe%2520function%2520space%2520bias%2520associated%2520with%2520the%2520architecture.%2520Our%2520results%2520show%2520that%250Aadding%2520additional%2520linear%2520layers%2520to%2520the%2520input%2520side%2520of%2520a%2520shallow%2520ReLU%2520network%250Ayields%2520a%2520representation%2520cost%2520favoring%2520functions%2520with%2520low%2520mixed%2520variation%2520-%2520that%250Ais%252C%2520it%2520has%2520limited%2520variation%2520in%2520directions%2520orthogonal%2520to%2520a%2520low-dimensional%250Asubspace%2520and%2520can%2520be%2520well%2520approximated%2520by%2520a%2520single-%2520or%2520multi-index%2520model.%2520Such%250Afunctions%2520may%2520be%2520represented%2520by%2520the%2520composition%2520of%2520a%2520function%2520with%2520low%250Atwo-layer%2520representation%2520cost%2520and%2520a%2520low-rank%2520linear%2520operator.%2520Our%2520experiments%250Aconfirm%2520this%2520behavior%2520in%2520standard%2520network%2520training%2520regimes.%2520They%2520additionally%250Ashow%2520that%2520linear%2520layers%2520can%2520improve%2520generalization%2520and%2520the%2520learned%2520network%2520is%250Awell-aligned%2520with%2520the%2520true%2520latent%2520low-dimensional%2520linear%2520subspace%2520when%2520data%2520is%250Agenerated%2520using%2520a%2520multi-index%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15598v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReLU%20Neural%20Networks%20with%20Linear%20Layers%20are%20Biased%20Towards%20Single-%20and%0A%20%20Multi-Index%20Models&entry.906535625=Suzanna%20Parkinson%20and%20Greg%20Ongie%20and%20Rebecca%20Willett&entry.1292438233=%20%20Neural%20networks%20often%20operate%20in%20the%20overparameterized%20regime%2C%20in%20which%20there%0Aare%20far%20more%20parameters%20than%20training%20samples%2C%20allowing%20the%20training%20data%20to%20be%0Afit%20perfectly.%20That%20is%2C%20training%20the%20network%20effectively%20learns%20an%0Ainterpolating%20function%2C%20and%20properties%20of%20the%20interpolant%20affect%20predictions%0Athe%20network%20will%20make%20on%20new%20samples.%20This%20manuscript%20explores%20how%20properties%0Aof%20such%20functions%20learned%20by%20neural%20networks%20of%20depth%20greater%20than%20two%20layers.%0AOur%20framework%20considers%20a%20family%20of%20networks%20of%20varying%20depths%20that%20all%20have%0Athe%20same%20capacity%20but%20different%20representation%20costs.%20The%20representation%20cost%0Aof%20a%20function%20induced%20by%20a%20neural%20network%20architecture%20is%20the%20minimum%20sum%20of%0Asquared%20weights%20needed%20for%20the%20network%20to%20represent%20the%20function%3B%20it%20reflects%0Athe%20function%20space%20bias%20associated%20with%20the%20architecture.%20Our%20results%20show%20that%0Aadding%20additional%20linear%20layers%20to%20the%20input%20side%20of%20a%20shallow%20ReLU%20network%0Ayields%20a%20representation%20cost%20favoring%20functions%20with%20low%20mixed%20variation%20-%20that%0Ais%2C%20it%20has%20limited%20variation%20in%20directions%20orthogonal%20to%20a%20low-dimensional%0Asubspace%20and%20can%20be%20well%20approximated%20by%20a%20single-%20or%20multi-index%20model.%20Such%0Afunctions%20may%20be%20represented%20by%20the%20composition%20of%20a%20function%20with%20low%0Atwo-layer%20representation%20cost%20and%20a%20low-rank%20linear%20operator.%20Our%20experiments%0Aconfirm%20this%20behavior%20in%20standard%20network%20training%20regimes.%20They%20additionally%0Ashow%20that%20linear%20layers%20can%20improve%20generalization%20and%20the%20learned%20network%20is%0Awell-aligned%20with%20the%20true%20latent%20low-dimensional%20linear%20subspace%20when%20data%20is%0Agenerated%20using%20a%20multi-index%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15598v3&entry.124074799=Read"},
{"title": "Are AI-Generated Text Detectors Robust to Adversarial Perturbations?", "author": "Guanhua Huang and Yuchen Zhang and Zhe Li and Yongjian You and Mingze Wang and Zhouwang Yang", "abstract": "  The widespread use of large language models (LLMs) has sparked concerns about\nthe potential misuse of AI-generated text, as these models can produce content\nthat closely resembles human-generated text. Current detectors for AI-generated\ntext (AIGT) lack robustness against adversarial perturbations, with even minor\nchanges in characters or words causing a reversal in distinguishing between\nhuman-created and AI-generated text. This paper investigates the robustness of\nexisting AIGT detection methods and introduces a novel detector, the Siamese\nCalibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction\nnetwork to add and remove noise from text, extracting a semantic representation\nthat is robust to local perturbations. We also propose a siamese calibration\ntechnique to train the model to make equally confidence predictions under\ndifferent noise, which improves the model's robustness against adversarial\nperturbations. Experiments on four publicly available datasets show that the\nSCRN outperforms all baseline methods, achieving 6.5\\%-18.25\\% absolute\naccuracy improvement over the best baseline method under adversarial attacks.\nMoreover, it exhibits superior generalizability in cross-domain, cross-genre,\nand mixed-source scenarios. The code is available at\n\\url{https://github.com/CarlanLark/Robust-AIGC-Detector}.\n", "link": "http://arxiv.org/abs/2406.01179v2", "date": "2024-06-26", "relevancy": 1.9235, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4841}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4807}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20AI-Generated%20Text%20Detectors%20Robust%20to%20Adversarial%20Perturbations%3F&body=Title%3A%20Are%20AI-Generated%20Text%20Detectors%20Robust%20to%20Adversarial%20Perturbations%3F%0AAuthor%3A%20Guanhua%20Huang%20and%20Yuchen%20Zhang%20and%20Zhe%20Li%20and%20Yongjian%20You%20and%20Mingze%20Wang%20and%20Zhouwang%20Yang%0AAbstract%3A%20%20%20The%20widespread%20use%20of%20large%20language%20models%20%28LLMs%29%20has%20sparked%20concerns%20about%0Athe%20potential%20misuse%20of%20AI-generated%20text%2C%20as%20these%20models%20can%20produce%20content%0Athat%20closely%20resembles%20human-generated%20text.%20Current%20detectors%20for%20AI-generated%0Atext%20%28AIGT%29%20lack%20robustness%20against%20adversarial%20perturbations%2C%20with%20even%20minor%0Achanges%20in%20characters%20or%20words%20causing%20a%20reversal%20in%20distinguishing%20between%0Ahuman-created%20and%20AI-generated%20text.%20This%20paper%20investigates%20the%20robustness%20of%0Aexisting%20AIGT%20detection%20methods%20and%20introduces%20a%20novel%20detector%2C%20the%20Siamese%0ACalibrated%20Reconstruction%20Network%20%28SCRN%29.%20The%20SCRN%20employs%20a%20reconstruction%0Anetwork%20to%20add%20and%20remove%20noise%20from%20text%2C%20extracting%20a%20semantic%20representation%0Athat%20is%20robust%20to%20local%20perturbations.%20We%20also%20propose%20a%20siamese%20calibration%0Atechnique%20to%20train%20the%20model%20to%20make%20equally%20confidence%20predictions%20under%0Adifferent%20noise%2C%20which%20improves%20the%20model%27s%20robustness%20against%20adversarial%0Aperturbations.%20Experiments%20on%20four%20publicly%20available%20datasets%20show%20that%20the%0ASCRN%20outperforms%20all%20baseline%20methods%2C%20achieving%206.5%5C%25-18.25%5C%25%20absolute%0Aaccuracy%20improvement%20over%20the%20best%20baseline%20method%20under%20adversarial%20attacks.%0AMoreover%2C%20it%20exhibits%20superior%20generalizability%20in%20cross-domain%2C%20cross-genre%2C%0Aand%20mixed-source%20scenarios.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/CarlanLark/Robust-AIGC-Detector%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520AI-Generated%2520Text%2520Detectors%2520Robust%2520to%2520Adversarial%2520Perturbations%253F%26entry.906535625%3DGuanhua%2520Huang%2520and%2520Yuchen%2520Zhang%2520and%2520Zhe%2520Li%2520and%2520Yongjian%2520You%2520and%2520Mingze%2520Wang%2520and%2520Zhouwang%2520Yang%26entry.1292438233%3D%2520%2520The%2520widespread%2520use%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520sparked%2520concerns%2520about%250Athe%2520potential%2520misuse%2520of%2520AI-generated%2520text%252C%2520as%2520these%2520models%2520can%2520produce%2520content%250Athat%2520closely%2520resembles%2520human-generated%2520text.%2520Current%2520detectors%2520for%2520AI-generated%250Atext%2520%2528AIGT%2529%2520lack%2520robustness%2520against%2520adversarial%2520perturbations%252C%2520with%2520even%2520minor%250Achanges%2520in%2520characters%2520or%2520words%2520causing%2520a%2520reversal%2520in%2520distinguishing%2520between%250Ahuman-created%2520and%2520AI-generated%2520text.%2520This%2520paper%2520investigates%2520the%2520robustness%2520of%250Aexisting%2520AIGT%2520detection%2520methods%2520and%2520introduces%2520a%2520novel%2520detector%252C%2520the%2520Siamese%250ACalibrated%2520Reconstruction%2520Network%2520%2528SCRN%2529.%2520The%2520SCRN%2520employs%2520a%2520reconstruction%250Anetwork%2520to%2520add%2520and%2520remove%2520noise%2520from%2520text%252C%2520extracting%2520a%2520semantic%2520representation%250Athat%2520is%2520robust%2520to%2520local%2520perturbations.%2520We%2520also%2520propose%2520a%2520siamese%2520calibration%250Atechnique%2520to%2520train%2520the%2520model%2520to%2520make%2520equally%2520confidence%2520predictions%2520under%250Adifferent%2520noise%252C%2520which%2520improves%2520the%2520model%2527s%2520robustness%2520against%2520adversarial%250Aperturbations.%2520Experiments%2520on%2520four%2520publicly%2520available%2520datasets%2520show%2520that%2520the%250ASCRN%2520outperforms%2520all%2520baseline%2520methods%252C%2520achieving%25206.5%255C%2525-18.25%255C%2525%2520absolute%250Aaccuracy%2520improvement%2520over%2520the%2520best%2520baseline%2520method%2520under%2520adversarial%2520attacks.%250AMoreover%252C%2520it%2520exhibits%2520superior%2520generalizability%2520in%2520cross-domain%252C%2520cross-genre%252C%250Aand%2520mixed-source%2520scenarios.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/CarlanLark/Robust-AIGC-Detector%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20AI-Generated%20Text%20Detectors%20Robust%20to%20Adversarial%20Perturbations%3F&entry.906535625=Guanhua%20Huang%20and%20Yuchen%20Zhang%20and%20Zhe%20Li%20and%20Yongjian%20You%20and%20Mingze%20Wang%20and%20Zhouwang%20Yang&entry.1292438233=%20%20The%20widespread%20use%20of%20large%20language%20models%20%28LLMs%29%20has%20sparked%20concerns%20about%0Athe%20potential%20misuse%20of%20AI-generated%20text%2C%20as%20these%20models%20can%20produce%20content%0Athat%20closely%20resembles%20human-generated%20text.%20Current%20detectors%20for%20AI-generated%0Atext%20%28AIGT%29%20lack%20robustness%20against%20adversarial%20perturbations%2C%20with%20even%20minor%0Achanges%20in%20characters%20or%20words%20causing%20a%20reversal%20in%20distinguishing%20between%0Ahuman-created%20and%20AI-generated%20text.%20This%20paper%20investigates%20the%20robustness%20of%0Aexisting%20AIGT%20detection%20methods%20and%20introduces%20a%20novel%20detector%2C%20the%20Siamese%0ACalibrated%20Reconstruction%20Network%20%28SCRN%29.%20The%20SCRN%20employs%20a%20reconstruction%0Anetwork%20to%20add%20and%20remove%20noise%20from%20text%2C%20extracting%20a%20semantic%20representation%0Athat%20is%20robust%20to%20local%20perturbations.%20We%20also%20propose%20a%20siamese%20calibration%0Atechnique%20to%20train%20the%20model%20to%20make%20equally%20confidence%20predictions%20under%0Adifferent%20noise%2C%20which%20improves%20the%20model%27s%20robustness%20against%20adversarial%0Aperturbations.%20Experiments%20on%20four%20publicly%20available%20datasets%20show%20that%20the%0ASCRN%20outperforms%20all%20baseline%20methods%2C%20achieving%206.5%5C%25-18.25%5C%25%20absolute%0Aaccuracy%20improvement%20over%20the%20best%20baseline%20method%20under%20adversarial%20attacks.%0AMoreover%2C%20it%20exhibits%20superior%20generalizability%20in%20cross-domain%2C%20cross-genre%2C%0Aand%20mixed-source%20scenarios.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/CarlanLark/Robust-AIGC-Detector%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01179v2&entry.124074799=Read"},
{"title": "Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain\n  Human-Machine Conversation", "author": "Ahmed Njifenjou and Virgile Sucal and Bassam Jabaian and Fabrice Lef\u00e8vre", "abstract": "  Recently, various methods have been proposed to create open-domain\nconversational agents with Large Language Models (LLMs). These models are able\nto answer user queries, but in a one-way Q&A format rather than a true\nconversation. Fine-tuning on particular datasets is the usual way to modify\ntheir style to increase conversational ability, but this is expensive and\nusually only available in a few languages. In this study, we explore role-play\nzero-shot prompting as an efficient and cost-effective solution for open-domain\nconversation, using capable multilingual LLMs (Beeching et al., 2023) trained\nto obey instructions. We design a prompting system that, when combined with an\ninstruction-following model - here Vicuna (Chiang et al., 2023) - produces\nconversational agents that match and even surpass fine-tuned models in human\nevaluation in French in two different tasks.\n", "link": "http://arxiv.org/abs/2406.18460v1", "date": "2024-06-26", "relevancy": 1.9199, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4937}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4925}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Role-Play%20Zero-Shot%20Prompting%20with%20Large%20Language%20Models%20for%20Open-Domain%0A%20%20Human-Machine%20Conversation&body=Title%3A%20Role-Play%20Zero-Shot%20Prompting%20with%20Large%20Language%20Models%20for%20Open-Domain%0A%20%20Human-Machine%20Conversation%0AAuthor%3A%20Ahmed%20Njifenjou%20and%20Virgile%20Sucal%20and%20Bassam%20Jabaian%20and%20Fabrice%20Lef%C3%A8vre%0AAbstract%3A%20%20%20Recently%2C%20various%20methods%20have%20been%20proposed%20to%20create%20open-domain%0Aconversational%20agents%20with%20Large%20Language%20Models%20%28LLMs%29.%20These%20models%20are%20able%0Ato%20answer%20user%20queries%2C%20but%20in%20a%20one-way%20Q%26A%20format%20rather%20than%20a%20true%0Aconversation.%20Fine-tuning%20on%20particular%20datasets%20is%20the%20usual%20way%20to%20modify%0Atheir%20style%20to%20increase%20conversational%20ability%2C%20but%20this%20is%20expensive%20and%0Ausually%20only%20available%20in%20a%20few%20languages.%20In%20this%20study%2C%20we%20explore%20role-play%0Azero-shot%20prompting%20as%20an%20efficient%20and%20cost-effective%20solution%20for%20open-domain%0Aconversation%2C%20using%20capable%20multilingual%20LLMs%20%28Beeching%20et%20al.%2C%202023%29%20trained%0Ato%20obey%20instructions.%20We%20design%20a%20prompting%20system%20that%2C%20when%20combined%20with%20an%0Ainstruction-following%20model%20-%20here%20Vicuna%20%28Chiang%20et%20al.%2C%202023%29%20-%20produces%0Aconversational%20agents%20that%20match%20and%20even%20surpass%20fine-tuned%20models%20in%20human%0Aevaluation%20in%20French%20in%20two%20different%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRole-Play%2520Zero-Shot%2520Prompting%2520with%2520Large%2520Language%2520Models%2520for%2520Open-Domain%250A%2520%2520Human-Machine%2520Conversation%26entry.906535625%3DAhmed%2520Njifenjou%2520and%2520Virgile%2520Sucal%2520and%2520Bassam%2520Jabaian%2520and%2520Fabrice%2520Lef%25C3%25A8vre%26entry.1292438233%3D%2520%2520Recently%252C%2520various%2520methods%2520have%2520been%2520proposed%2520to%2520create%2520open-domain%250Aconversational%2520agents%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520These%2520models%2520are%2520able%250Ato%2520answer%2520user%2520queries%252C%2520but%2520in%2520a%2520one-way%2520Q%2526A%2520format%2520rather%2520than%2520a%2520true%250Aconversation.%2520Fine-tuning%2520on%2520particular%2520datasets%2520is%2520the%2520usual%2520way%2520to%2520modify%250Atheir%2520style%2520to%2520increase%2520conversational%2520ability%252C%2520but%2520this%2520is%2520expensive%2520and%250Ausually%2520only%2520available%2520in%2520a%2520few%2520languages.%2520In%2520this%2520study%252C%2520we%2520explore%2520role-play%250Azero-shot%2520prompting%2520as%2520an%2520efficient%2520and%2520cost-effective%2520solution%2520for%2520open-domain%250Aconversation%252C%2520using%2520capable%2520multilingual%2520LLMs%2520%2528Beeching%2520et%2520al.%252C%25202023%2529%2520trained%250Ato%2520obey%2520instructions.%2520We%2520design%2520a%2520prompting%2520system%2520that%252C%2520when%2520combined%2520with%2520an%250Ainstruction-following%2520model%2520-%2520here%2520Vicuna%2520%2528Chiang%2520et%2520al.%252C%25202023%2529%2520-%2520produces%250Aconversational%2520agents%2520that%2520match%2520and%2520even%2520surpass%2520fine-tuned%2520models%2520in%2520human%250Aevaluation%2520in%2520French%2520in%2520two%2520different%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Role-Play%20Zero-Shot%20Prompting%20with%20Large%20Language%20Models%20for%20Open-Domain%0A%20%20Human-Machine%20Conversation&entry.906535625=Ahmed%20Njifenjou%20and%20Virgile%20Sucal%20and%20Bassam%20Jabaian%20and%20Fabrice%20Lef%C3%A8vre&entry.1292438233=%20%20Recently%2C%20various%20methods%20have%20been%20proposed%20to%20create%20open-domain%0Aconversational%20agents%20with%20Large%20Language%20Models%20%28LLMs%29.%20These%20models%20are%20able%0Ato%20answer%20user%20queries%2C%20but%20in%20a%20one-way%20Q%26A%20format%20rather%20than%20a%20true%0Aconversation.%20Fine-tuning%20on%20particular%20datasets%20is%20the%20usual%20way%20to%20modify%0Atheir%20style%20to%20increase%20conversational%20ability%2C%20but%20this%20is%20expensive%20and%0Ausually%20only%20available%20in%20a%20few%20languages.%20In%20this%20study%2C%20we%20explore%20role-play%0Azero-shot%20prompting%20as%20an%20efficient%20and%20cost-effective%20solution%20for%20open-domain%0Aconversation%2C%20using%20capable%20multilingual%20LLMs%20%28Beeching%20et%20al.%2C%202023%29%20trained%0Ato%20obey%20instructions.%20We%20design%20a%20prompting%20system%20that%2C%20when%20combined%20with%20an%0Ainstruction-following%20model%20-%20here%20Vicuna%20%28Chiang%20et%20al.%2C%202023%29%20-%20produces%0Aconversational%20agents%20that%20match%20and%20even%20surpass%20fine-tuned%20models%20in%20human%0Aevaluation%20in%20French%20in%20two%20different%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18460v1&entry.124074799=Read"},
{"title": "IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying\n  and Reweighting Context-Aware Neurons", "author": "Dan Shi and Renren Jin and Tianhao Shen and Weilong Dong and Xinwei Wu and Deyi Xiong", "abstract": "  It is widely acknowledged that large language models (LLMs) encode a vast\nreservoir of knowledge after being trained on mass data. Recent studies\ndisclose knowledge conflicts in LLM generation, wherein outdated or incorrect\nparametric knowledge (i.e., encoded knowledge) contradicts new knowledge\nprovided in the context. To mitigate such knowledge conflicts, we propose a\nnovel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to\ncapitalize on neurons that are crucial in processing contextual cues.\nSpecifically, IRCAN first identifies neurons that significantly contribute to\ncontext processing, utilizing a context-aware attribution score derived from\nintegrated gradients. Subsequently, the identified context-aware neurons are\nstrengthened via reweighting. In doing so, we steer LLMs to generate\ncontext-sensitive outputs with respect to the new knowledge provided in the\ncontext. Extensive experiments conducted across a variety of models and tasks\ndemonstrate that IRCAN not only achieves remarkable improvements in handling\nknowledge conflicts but also offers a scalable, plug-andplay solution that can\nbe integrated seamlessly with existing models.\n", "link": "http://arxiv.org/abs/2406.18406v1", "date": "2024-06-26", "relevancy": 1.9153, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4925}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4799}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRCAN%3A%20Mitigating%20Knowledge%20Conflicts%20in%20LLM%20Generation%20via%20Identifying%0A%20%20and%20Reweighting%20Context-Aware%20Neurons&body=Title%3A%20IRCAN%3A%20Mitigating%20Knowledge%20Conflicts%20in%20LLM%20Generation%20via%20Identifying%0A%20%20and%20Reweighting%20Context-Aware%20Neurons%0AAuthor%3A%20Dan%20Shi%20and%20Renren%20Jin%20and%20Tianhao%20Shen%20and%20Weilong%20Dong%20and%20Xinwei%20Wu%20and%20Deyi%20Xiong%0AAbstract%3A%20%20%20It%20is%20widely%20acknowledged%20that%20large%20language%20models%20%28LLMs%29%20encode%20a%20vast%0Areservoir%20of%20knowledge%20after%20being%20trained%20on%20mass%20data.%20Recent%20studies%0Adisclose%20knowledge%20conflicts%20in%20LLM%20generation%2C%20wherein%20outdated%20or%20incorrect%0Aparametric%20knowledge%20%28i.e.%2C%20encoded%20knowledge%29%20contradicts%20new%20knowledge%0Aprovided%20in%20the%20context.%20To%20mitigate%20such%20knowledge%20conflicts%2C%20we%20propose%20a%0Anovel%20framework%2C%20IRCAN%20%28Identifying%20and%20Reweighting%20Context-Aware%20Neurons%29%20to%0Acapitalize%20on%20neurons%20that%20are%20crucial%20in%20processing%20contextual%20cues.%0ASpecifically%2C%20IRCAN%20first%20identifies%20neurons%20that%20significantly%20contribute%20to%0Acontext%20processing%2C%20utilizing%20a%20context-aware%20attribution%20score%20derived%20from%0Aintegrated%20gradients.%20Subsequently%2C%20the%20identified%20context-aware%20neurons%20are%0Astrengthened%20via%20reweighting.%20In%20doing%20so%2C%20we%20steer%20LLMs%20to%20generate%0Acontext-sensitive%20outputs%20with%20respect%20to%20the%20new%20knowledge%20provided%20in%20the%0Acontext.%20Extensive%20experiments%20conducted%20across%20a%20variety%20of%20models%20and%20tasks%0Ademonstrate%20that%20IRCAN%20not%20only%20achieves%20remarkable%20improvements%20in%20handling%0Aknowledge%20conflicts%20but%20also%20offers%20a%20scalable%2C%20plug-andplay%20solution%20that%20can%0Abe%20integrated%20seamlessly%20with%20existing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRCAN%253A%2520Mitigating%2520Knowledge%2520Conflicts%2520in%2520LLM%2520Generation%2520via%2520Identifying%250A%2520%2520and%2520Reweighting%2520Context-Aware%2520Neurons%26entry.906535625%3DDan%2520Shi%2520and%2520Renren%2520Jin%2520and%2520Tianhao%2520Shen%2520and%2520Weilong%2520Dong%2520and%2520Xinwei%2520Wu%2520and%2520Deyi%2520Xiong%26entry.1292438233%3D%2520%2520It%2520is%2520widely%2520acknowledged%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520encode%2520a%2520vast%250Areservoir%2520of%2520knowledge%2520after%2520being%2520trained%2520on%2520mass%2520data.%2520Recent%2520studies%250Adisclose%2520knowledge%2520conflicts%2520in%2520LLM%2520generation%252C%2520wherein%2520outdated%2520or%2520incorrect%250Aparametric%2520knowledge%2520%2528i.e.%252C%2520encoded%2520knowledge%2529%2520contradicts%2520new%2520knowledge%250Aprovided%2520in%2520the%2520context.%2520To%2520mitigate%2520such%2520knowledge%2520conflicts%252C%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520IRCAN%2520%2528Identifying%2520and%2520Reweighting%2520Context-Aware%2520Neurons%2529%2520to%250Acapitalize%2520on%2520neurons%2520that%2520are%2520crucial%2520in%2520processing%2520contextual%2520cues.%250ASpecifically%252C%2520IRCAN%2520first%2520identifies%2520neurons%2520that%2520significantly%2520contribute%2520to%250Acontext%2520processing%252C%2520utilizing%2520a%2520context-aware%2520attribution%2520score%2520derived%2520from%250Aintegrated%2520gradients.%2520Subsequently%252C%2520the%2520identified%2520context-aware%2520neurons%2520are%250Astrengthened%2520via%2520reweighting.%2520In%2520doing%2520so%252C%2520we%2520steer%2520LLMs%2520to%2520generate%250Acontext-sensitive%2520outputs%2520with%2520respect%2520to%2520the%2520new%2520knowledge%2520provided%2520in%2520the%250Acontext.%2520Extensive%2520experiments%2520conducted%2520across%2520a%2520variety%2520of%2520models%2520and%2520tasks%250Ademonstrate%2520that%2520IRCAN%2520not%2520only%2520achieves%2520remarkable%2520improvements%2520in%2520handling%250Aknowledge%2520conflicts%2520but%2520also%2520offers%2520a%2520scalable%252C%2520plug-andplay%2520solution%2520that%2520can%250Abe%2520integrated%2520seamlessly%2520with%2520existing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRCAN%3A%20Mitigating%20Knowledge%20Conflicts%20in%20LLM%20Generation%20via%20Identifying%0A%20%20and%20Reweighting%20Context-Aware%20Neurons&entry.906535625=Dan%20Shi%20and%20Renren%20Jin%20and%20Tianhao%20Shen%20and%20Weilong%20Dong%20and%20Xinwei%20Wu%20and%20Deyi%20Xiong&entry.1292438233=%20%20It%20is%20widely%20acknowledged%20that%20large%20language%20models%20%28LLMs%29%20encode%20a%20vast%0Areservoir%20of%20knowledge%20after%20being%20trained%20on%20mass%20data.%20Recent%20studies%0Adisclose%20knowledge%20conflicts%20in%20LLM%20generation%2C%20wherein%20outdated%20or%20incorrect%0Aparametric%20knowledge%20%28i.e.%2C%20encoded%20knowledge%29%20contradicts%20new%20knowledge%0Aprovided%20in%20the%20context.%20To%20mitigate%20such%20knowledge%20conflicts%2C%20we%20propose%20a%0Anovel%20framework%2C%20IRCAN%20%28Identifying%20and%20Reweighting%20Context-Aware%20Neurons%29%20to%0Acapitalize%20on%20neurons%20that%20are%20crucial%20in%20processing%20contextual%20cues.%0ASpecifically%2C%20IRCAN%20first%20identifies%20neurons%20that%20significantly%20contribute%20to%0Acontext%20processing%2C%20utilizing%20a%20context-aware%20attribution%20score%20derived%20from%0Aintegrated%20gradients.%20Subsequently%2C%20the%20identified%20context-aware%20neurons%20are%0Astrengthened%20via%20reweighting.%20In%20doing%20so%2C%20we%20steer%20LLMs%20to%20generate%0Acontext-sensitive%20outputs%20with%20respect%20to%20the%20new%20knowledge%20provided%20in%20the%0Acontext.%20Extensive%20experiments%20conducted%20across%20a%20variety%20of%20models%20and%20tasks%0Ademonstrate%20that%20IRCAN%20not%20only%20achieves%20remarkable%20improvements%20in%20handling%0Aknowledge%20conflicts%20but%20also%20offers%20a%20scalable%2C%20plug-andplay%20solution%20that%20can%0Abe%20integrated%20seamlessly%20with%20existing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18406v1&entry.124074799=Read"},
{"title": "The Challenges of the Nonlinear Regime for Physics-Informed Neural\n  Networks", "author": "Andrea Bonfanti and Giuseppe Bruno and Cristina Cipriani", "abstract": "  The Neural Tangent Kernel (NTK) viewpoint is widely employed to analyze the\ntraining dynamics of overparameterized Physics-Informed Neural Networks\n(PINNs). However, unlike the case of linear Partial Differential Equations\n(PDEs), we show how the NTK perspective falls short in the nonlinear scenario.\nSpecifically, we establish that the NTK yields a random matrix at\ninitialization that is not constant during training, contrary to conventional\nbelief. Another significant difference from the linear regime is that, even in\nthe idealistic infinite-width limit, the Hessian does not vanish and hence it\ncannot be disregarded during training. This motivates the adoption of\nsecond-order optimization methods. We explore the convergence guarantees of\nsuch methods in both linear and nonlinear cases, addressing challenges such as\nspectral bias and slow convergence. Every theoretical result is supported by\nnumerical examples with both linear and nonlinear PDEs, and we highlight the\nbenefits of second-order methods in benchmark test cases.\n", "link": "http://arxiv.org/abs/2402.03864v2", "date": "2024-06-26", "relevancy": 1.9137, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4981}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4839}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Challenges%20of%20the%20Nonlinear%20Regime%20for%20Physics-Informed%20Neural%0A%20%20Networks&body=Title%3A%20The%20Challenges%20of%20the%20Nonlinear%20Regime%20for%20Physics-Informed%20Neural%0A%20%20Networks%0AAuthor%3A%20Andrea%20Bonfanti%20and%20Giuseppe%20Bruno%20and%20Cristina%20Cipriani%0AAbstract%3A%20%20%20The%20Neural%20Tangent%20Kernel%20%28NTK%29%20viewpoint%20is%20widely%20employed%20to%20analyze%20the%0Atraining%20dynamics%20of%20overparameterized%20Physics-Informed%20Neural%20Networks%0A%28PINNs%29.%20However%2C%20unlike%20the%20case%20of%20linear%20Partial%20Differential%20Equations%0A%28PDEs%29%2C%20we%20show%20how%20the%20NTK%20perspective%20falls%20short%20in%20the%20nonlinear%20scenario.%0ASpecifically%2C%20we%20establish%20that%20the%20NTK%20yields%20a%20random%20matrix%20at%0Ainitialization%20that%20is%20not%20constant%20during%20training%2C%20contrary%20to%20conventional%0Abelief.%20Another%20significant%20difference%20from%20the%20linear%20regime%20is%20that%2C%20even%20in%0Athe%20idealistic%20infinite-width%20limit%2C%20the%20Hessian%20does%20not%20vanish%20and%20hence%20it%0Acannot%20be%20disregarded%20during%20training.%20This%20motivates%20the%20adoption%20of%0Asecond-order%20optimization%20methods.%20We%20explore%20the%20convergence%20guarantees%20of%0Asuch%20methods%20in%20both%20linear%20and%20nonlinear%20cases%2C%20addressing%20challenges%20such%20as%0Aspectral%20bias%20and%20slow%20convergence.%20Every%20theoretical%20result%20is%20supported%20by%0Anumerical%20examples%20with%20both%20linear%20and%20nonlinear%20PDEs%2C%20and%20we%20highlight%20the%0Abenefits%20of%20second-order%20methods%20in%20benchmark%20test%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Challenges%2520of%2520the%2520Nonlinear%2520Regime%2520for%2520Physics-Informed%2520Neural%250A%2520%2520Networks%26entry.906535625%3DAndrea%2520Bonfanti%2520and%2520Giuseppe%2520Bruno%2520and%2520Cristina%2520Cipriani%26entry.1292438233%3D%2520%2520The%2520Neural%2520Tangent%2520Kernel%2520%2528NTK%2529%2520viewpoint%2520is%2520widely%2520employed%2520to%2520analyze%2520the%250Atraining%2520dynamics%2520of%2520overparameterized%2520Physics-Informed%2520Neural%2520Networks%250A%2528PINNs%2529.%2520However%252C%2520unlike%2520the%2520case%2520of%2520linear%2520Partial%2520Differential%2520Equations%250A%2528PDEs%2529%252C%2520we%2520show%2520how%2520the%2520NTK%2520perspective%2520falls%2520short%2520in%2520the%2520nonlinear%2520scenario.%250ASpecifically%252C%2520we%2520establish%2520that%2520the%2520NTK%2520yields%2520a%2520random%2520matrix%2520at%250Ainitialization%2520that%2520is%2520not%2520constant%2520during%2520training%252C%2520contrary%2520to%2520conventional%250Abelief.%2520Another%2520significant%2520difference%2520from%2520the%2520linear%2520regime%2520is%2520that%252C%2520even%2520in%250Athe%2520idealistic%2520infinite-width%2520limit%252C%2520the%2520Hessian%2520does%2520not%2520vanish%2520and%2520hence%2520it%250Acannot%2520be%2520disregarded%2520during%2520training.%2520This%2520motivates%2520the%2520adoption%2520of%250Asecond-order%2520optimization%2520methods.%2520We%2520explore%2520the%2520convergence%2520guarantees%2520of%250Asuch%2520methods%2520in%2520both%2520linear%2520and%2520nonlinear%2520cases%252C%2520addressing%2520challenges%2520such%2520as%250Aspectral%2520bias%2520and%2520slow%2520convergence.%2520Every%2520theoretical%2520result%2520is%2520supported%2520by%250Anumerical%2520examples%2520with%2520both%2520linear%2520and%2520nonlinear%2520PDEs%252C%2520and%2520we%2520highlight%2520the%250Abenefits%2520of%2520second-order%2520methods%2520in%2520benchmark%2520test%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Challenges%20of%20the%20Nonlinear%20Regime%20for%20Physics-Informed%20Neural%0A%20%20Networks&entry.906535625=Andrea%20Bonfanti%20and%20Giuseppe%20Bruno%20and%20Cristina%20Cipriani&entry.1292438233=%20%20The%20Neural%20Tangent%20Kernel%20%28NTK%29%20viewpoint%20is%20widely%20employed%20to%20analyze%20the%0Atraining%20dynamics%20of%20overparameterized%20Physics-Informed%20Neural%20Networks%0A%28PINNs%29.%20However%2C%20unlike%20the%20case%20of%20linear%20Partial%20Differential%20Equations%0A%28PDEs%29%2C%20we%20show%20how%20the%20NTK%20perspective%20falls%20short%20in%20the%20nonlinear%20scenario.%0ASpecifically%2C%20we%20establish%20that%20the%20NTK%20yields%20a%20random%20matrix%20at%0Ainitialization%20that%20is%20not%20constant%20during%20training%2C%20contrary%20to%20conventional%0Abelief.%20Another%20significant%20difference%20from%20the%20linear%20regime%20is%20that%2C%20even%20in%0Athe%20idealistic%20infinite-width%20limit%2C%20the%20Hessian%20does%20not%20vanish%20and%20hence%20it%0Acannot%20be%20disregarded%20during%20training.%20This%20motivates%20the%20adoption%20of%0Asecond-order%20optimization%20methods.%20We%20explore%20the%20convergence%20guarantees%20of%0Asuch%20methods%20in%20both%20linear%20and%20nonlinear%20cases%2C%20addressing%20challenges%20such%20as%0Aspectral%20bias%20and%20slow%20convergence.%20Every%20theoretical%20result%20is%20supported%20by%0Anumerical%20examples%20with%20both%20linear%20and%20nonlinear%20PDEs%2C%20and%20we%20highlight%20the%0Abenefits%20of%20second-order%20methods%20in%20benchmark%20test%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03864v2&entry.124074799=Read"},
{"title": "An Autotuning-based Optimization Framework for Mixed-kernel SVM\n  Classifications in Smart Pixel Datasets and Heterojunction Transistors", "author": "Xingfu Wu and Tupendra Oli and ustin H. Qian and Valerie Taylor and Mark C. Hersam and Vinod K. Sangwan", "abstract": "  Support Vector Machine (SVM) is a state-of-the-art classification method\nwidely used in science and engineering due to its high accuracy, its ability to\ndeal with high dimensional data, and its flexibility in modeling diverse\nsources of data. In this paper, we propose an autotuning-based optimization\nframework to quantify the ranges of hyperparameters in SVMs to identify their\noptimal choices, and apply the framework to two SVMs with the mixed-kernel\nbetween Sigmoid and Gaussian kernels for smart pixel datasets in high energy\nphysics (HEP) and mixed-kernel heterojunction transistors (MKH). Our\nexperimental results show that the optimal selection of hyperparameters in the\nSVMs and the kernels greatly varies for different applications and datasets,\nand choosing their optimal choices is critical for a high classification\naccuracy of the mixed kernel SVMs. Uninformed choices of hyperparameters C and\ncoef0 in the mixed-kernel SVMs result in severely low accuracy, and the\nproposed framework effectively quantifies the proper ranges for the\nhyperparameters in the SVMs to identify their optimal choices to achieve the\nhighest accuracy 94.6\\% for the HEP application and the highest average\naccuracy 97.2\\% with far less tuning time for the MKH application.\n", "link": "http://arxiv.org/abs/2406.18445v1", "date": "2024-06-26", "relevancy": 1.9128, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4944}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4815}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Autotuning-based%20Optimization%20Framework%20for%20Mixed-kernel%20SVM%0A%20%20Classifications%20in%20Smart%20Pixel%20Datasets%20and%20Heterojunction%20Transistors&body=Title%3A%20An%20Autotuning-based%20Optimization%20Framework%20for%20Mixed-kernel%20SVM%0A%20%20Classifications%20in%20Smart%20Pixel%20Datasets%20and%20Heterojunction%20Transistors%0AAuthor%3A%20Xingfu%20Wu%20and%20Tupendra%20Oli%20and%20ustin%20H.%20Qian%20and%20Valerie%20Taylor%20and%20Mark%20C.%20Hersam%20and%20Vinod%20K.%20Sangwan%0AAbstract%3A%20%20%20Support%20Vector%20Machine%20%28SVM%29%20is%20a%20state-of-the-art%20classification%20method%0Awidely%20used%20in%20science%20and%20engineering%20due%20to%20its%20high%20accuracy%2C%20its%20ability%20to%0Adeal%20with%20high%20dimensional%20data%2C%20and%20its%20flexibility%20in%20modeling%20diverse%0Asources%20of%20data.%20In%20this%20paper%2C%20we%20propose%20an%20autotuning-based%20optimization%0Aframework%20to%20quantify%20the%20ranges%20of%20hyperparameters%20in%20SVMs%20to%20identify%20their%0Aoptimal%20choices%2C%20and%20apply%20the%20framework%20to%20two%20SVMs%20with%20the%20mixed-kernel%0Abetween%20Sigmoid%20and%20Gaussian%20kernels%20for%20smart%20pixel%20datasets%20in%20high%20energy%0Aphysics%20%28HEP%29%20and%20mixed-kernel%20heterojunction%20transistors%20%28MKH%29.%20Our%0Aexperimental%20results%20show%20that%20the%20optimal%20selection%20of%20hyperparameters%20in%20the%0ASVMs%20and%20the%20kernels%20greatly%20varies%20for%20different%20applications%20and%20datasets%2C%0Aand%20choosing%20their%20optimal%20choices%20is%20critical%20for%20a%20high%20classification%0Aaccuracy%20of%20the%20mixed%20kernel%20SVMs.%20Uninformed%20choices%20of%20hyperparameters%20C%20and%0Acoef0%20in%20the%20mixed-kernel%20SVMs%20result%20in%20severely%20low%20accuracy%2C%20and%20the%0Aproposed%20framework%20effectively%20quantifies%20the%20proper%20ranges%20for%20the%0Ahyperparameters%20in%20the%20SVMs%20to%20identify%20their%20optimal%20choices%20to%20achieve%20the%0Ahighest%20accuracy%2094.6%5C%25%20for%20the%20HEP%20application%20and%20the%20highest%20average%0Aaccuracy%2097.2%5C%25%20with%20far%20less%20tuning%20time%20for%20the%20MKH%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Autotuning-based%2520Optimization%2520Framework%2520for%2520Mixed-kernel%2520SVM%250A%2520%2520Classifications%2520in%2520Smart%2520Pixel%2520Datasets%2520and%2520Heterojunction%2520Transistors%26entry.906535625%3DXingfu%2520Wu%2520and%2520Tupendra%2520Oli%2520and%2520ustin%2520H.%2520Qian%2520and%2520Valerie%2520Taylor%2520and%2520Mark%2520C.%2520Hersam%2520and%2520Vinod%2520K.%2520Sangwan%26entry.1292438233%3D%2520%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%2520is%2520a%2520state-of-the-art%2520classification%2520method%250Awidely%2520used%2520in%2520science%2520and%2520engineering%2520due%2520to%2520its%2520high%2520accuracy%252C%2520its%2520ability%2520to%250Adeal%2520with%2520high%2520dimensional%2520data%252C%2520and%2520its%2520flexibility%2520in%2520modeling%2520diverse%250Asources%2520of%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520autotuning-based%2520optimization%250Aframework%2520to%2520quantify%2520the%2520ranges%2520of%2520hyperparameters%2520in%2520SVMs%2520to%2520identify%2520their%250Aoptimal%2520choices%252C%2520and%2520apply%2520the%2520framework%2520to%2520two%2520SVMs%2520with%2520the%2520mixed-kernel%250Abetween%2520Sigmoid%2520and%2520Gaussian%2520kernels%2520for%2520smart%2520pixel%2520datasets%2520in%2520high%2520energy%250Aphysics%2520%2528HEP%2529%2520and%2520mixed-kernel%2520heterojunction%2520transistors%2520%2528MKH%2529.%2520Our%250Aexperimental%2520results%2520show%2520that%2520the%2520optimal%2520selection%2520of%2520hyperparameters%2520in%2520the%250ASVMs%2520and%2520the%2520kernels%2520greatly%2520varies%2520for%2520different%2520applications%2520and%2520datasets%252C%250Aand%2520choosing%2520their%2520optimal%2520choices%2520is%2520critical%2520for%2520a%2520high%2520classification%250Aaccuracy%2520of%2520the%2520mixed%2520kernel%2520SVMs.%2520Uninformed%2520choices%2520of%2520hyperparameters%2520C%2520and%250Acoef0%2520in%2520the%2520mixed-kernel%2520SVMs%2520result%2520in%2520severely%2520low%2520accuracy%252C%2520and%2520the%250Aproposed%2520framework%2520effectively%2520quantifies%2520the%2520proper%2520ranges%2520for%2520the%250Ahyperparameters%2520in%2520the%2520SVMs%2520to%2520identify%2520their%2520optimal%2520choices%2520to%2520achieve%2520the%250Ahighest%2520accuracy%252094.6%255C%2525%2520for%2520the%2520HEP%2520application%2520and%2520the%2520highest%2520average%250Aaccuracy%252097.2%255C%2525%2520with%2520far%2520less%2520tuning%2520time%2520for%2520the%2520MKH%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Autotuning-based%20Optimization%20Framework%20for%20Mixed-kernel%20SVM%0A%20%20Classifications%20in%20Smart%20Pixel%20Datasets%20and%20Heterojunction%20Transistors&entry.906535625=Xingfu%20Wu%20and%20Tupendra%20Oli%20and%20ustin%20H.%20Qian%20and%20Valerie%20Taylor%20and%20Mark%20C.%20Hersam%20and%20Vinod%20K.%20Sangwan&entry.1292438233=%20%20Support%20Vector%20Machine%20%28SVM%29%20is%20a%20state-of-the-art%20classification%20method%0Awidely%20used%20in%20science%20and%20engineering%20due%20to%20its%20high%20accuracy%2C%20its%20ability%20to%0Adeal%20with%20high%20dimensional%20data%2C%20and%20its%20flexibility%20in%20modeling%20diverse%0Asources%20of%20data.%20In%20this%20paper%2C%20we%20propose%20an%20autotuning-based%20optimization%0Aframework%20to%20quantify%20the%20ranges%20of%20hyperparameters%20in%20SVMs%20to%20identify%20their%0Aoptimal%20choices%2C%20and%20apply%20the%20framework%20to%20two%20SVMs%20with%20the%20mixed-kernel%0Abetween%20Sigmoid%20and%20Gaussian%20kernels%20for%20smart%20pixel%20datasets%20in%20high%20energy%0Aphysics%20%28HEP%29%20and%20mixed-kernel%20heterojunction%20transistors%20%28MKH%29.%20Our%0Aexperimental%20results%20show%20that%20the%20optimal%20selection%20of%20hyperparameters%20in%20the%0ASVMs%20and%20the%20kernels%20greatly%20varies%20for%20different%20applications%20and%20datasets%2C%0Aand%20choosing%20their%20optimal%20choices%20is%20critical%20for%20a%20high%20classification%0Aaccuracy%20of%20the%20mixed%20kernel%20SVMs.%20Uninformed%20choices%20of%20hyperparameters%20C%20and%0Acoef0%20in%20the%20mixed-kernel%20SVMs%20result%20in%20severely%20low%20accuracy%2C%20and%20the%0Aproposed%20framework%20effectively%20quantifies%20the%20proper%20ranges%20for%20the%0Ahyperparameters%20in%20the%20SVMs%20to%20identify%20their%20optimal%20choices%20to%20achieve%20the%0Ahighest%20accuracy%2094.6%5C%25%20for%20the%20HEP%20application%20and%20the%20highest%20average%0Aaccuracy%2097.2%5C%25%20with%20far%20less%20tuning%20time%20for%20the%20MKH%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18445v1&entry.124074799=Read"},
{"title": "Adam-mini: Use Fewer Learning Rates To Gain More", "author": "Yushun Zhang and Congliang Chen and Ziniu Li and Tian Ding and Chenwei Wu and Yinyu Ye and Zhi-Quan Luo and Ruoyu Sun", "abstract": "  We propose Adam-mini, an optimizer that achieves on-par or better performance\nthan AdamW with 45% to 50% less memory footprint. Adam-mini reduces memory by\ncutting down the learning rate resources in Adam (i.e., $1/\\sqrt{v}$). We find\nthat $\\geq$ 90% of these learning rates in $v$ could be harmlessly removed if\nwe (1) carefully partition the parameters into blocks following our proposed\nprinciple on Hessian structure; (2) assign a single but good learning rate to\neach parameter block. We further find that, for each of these parameter blocks,\nthere exists a single high-quality learning rate that can outperform Adam,\nprovided that sufficient resources are available to search it out. We then\nprovide one cost-effective way to find good learning rates and propose\nAdam-mini. Empirically, we verify that Adam-mini performs on par or better than\nAdamW on various language models sized from 125M to 7B for pre-training,\nsupervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini\nalso alleviates communication overheads among GPUs and CPUs, thereby increasing\nthroughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamW\nwhen pre-training Llama2-7B on $2\\times$ A800-80GB GPUs, which saves 33%\nwall-clock time for pre-training.\n", "link": "http://arxiv.org/abs/2406.16793v3", "date": "2024-06-26", "relevancy": 1.9, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4962}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More&body=Title%3A%20Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More%0AAuthor%3A%20Yushun%20Zhang%20and%20Congliang%20Chen%20and%20Ziniu%20Li%20and%20Tian%20Ding%20and%20Chenwei%20Wu%20and%20Yinyu%20Ye%20and%20Zhi-Quan%20Luo%20and%20Ruoyu%20Sun%0AAbstract%3A%20%20%20We%20propose%20Adam-mini%2C%20an%20optimizer%20that%20achieves%20on-par%20or%20better%20performance%0Athan%20AdamW%20with%2045%25%20to%2050%25%20less%20memory%20footprint.%20Adam-mini%20reduces%20memory%20by%0Acutting%20down%20the%20learning%20rate%20resources%20in%20Adam%20%28i.e.%2C%20%241/%5Csqrt%7Bv%7D%24%29.%20We%20find%0Athat%20%24%5Cgeq%24%2090%25%20of%20these%20learning%20rates%20in%20%24v%24%20could%20be%20harmlessly%20removed%20if%0Awe%20%281%29%20carefully%20partition%20the%20parameters%20into%20blocks%20following%20our%20proposed%0Aprinciple%20on%20Hessian%20structure%3B%20%282%29%20assign%20a%20single%20but%20good%20learning%20rate%20to%0Aeach%20parameter%20block.%20We%20further%20find%20that%2C%20for%20each%20of%20these%20parameter%20blocks%2C%0Athere%20exists%20a%20single%20high-quality%20learning%20rate%20that%20can%20outperform%20Adam%2C%0Aprovided%20that%20sufficient%20resources%20are%20available%20to%20search%20it%20out.%20We%20then%0Aprovide%20one%20cost-effective%20way%20to%20find%20good%20learning%20rates%20and%20propose%0AAdam-mini.%20Empirically%2C%20we%20verify%20that%20Adam-mini%20performs%20on%20par%20or%20better%20than%0AAdamW%20on%20various%20language%20models%20sized%20from%20125M%20to%207B%20for%20pre-training%2C%0Asupervised%20fine-tuning%2C%20and%20RLHF.%20The%20reduced%20memory%20footprint%20of%20Adam-mini%0Aalso%20alleviates%20communication%20overheads%20among%20GPUs%20and%20CPUs%2C%20thereby%20increasing%0Athroughput.%20For%20instance%2C%20Adam-mini%20achieves%2049.6%25%20higher%20throughput%20than%20AdamW%0Awhen%20pre-training%20Llama2-7B%20on%20%242%5Ctimes%24%20A800-80GB%20GPUs%2C%20which%20saves%2033%25%0Awall-clock%20time%20for%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16793v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdam-mini%253A%2520Use%2520Fewer%2520Learning%2520Rates%2520To%2520Gain%2520More%26entry.906535625%3DYushun%2520Zhang%2520and%2520Congliang%2520Chen%2520and%2520Ziniu%2520Li%2520and%2520Tian%2520Ding%2520and%2520Chenwei%2520Wu%2520and%2520Yinyu%2520Ye%2520and%2520Zhi-Quan%2520Luo%2520and%2520Ruoyu%2520Sun%26entry.1292438233%3D%2520%2520We%2520propose%2520Adam-mini%252C%2520an%2520optimizer%2520that%2520achieves%2520on-par%2520or%2520better%2520performance%250Athan%2520AdamW%2520with%252045%2525%2520to%252050%2525%2520less%2520memory%2520footprint.%2520Adam-mini%2520reduces%2520memory%2520by%250Acutting%2520down%2520the%2520learning%2520rate%2520resources%2520in%2520Adam%2520%2528i.e.%252C%2520%25241/%255Csqrt%257Bv%257D%2524%2529.%2520We%2520find%250Athat%2520%2524%255Cgeq%2524%252090%2525%2520of%2520these%2520learning%2520rates%2520in%2520%2524v%2524%2520could%2520be%2520harmlessly%2520removed%2520if%250Awe%2520%25281%2529%2520carefully%2520partition%2520the%2520parameters%2520into%2520blocks%2520following%2520our%2520proposed%250Aprinciple%2520on%2520Hessian%2520structure%253B%2520%25282%2529%2520assign%2520a%2520single%2520but%2520good%2520learning%2520rate%2520to%250Aeach%2520parameter%2520block.%2520We%2520further%2520find%2520that%252C%2520for%2520each%2520of%2520these%2520parameter%2520blocks%252C%250Athere%2520exists%2520a%2520single%2520high-quality%2520learning%2520rate%2520that%2520can%2520outperform%2520Adam%252C%250Aprovided%2520that%2520sufficient%2520resources%2520are%2520available%2520to%2520search%2520it%2520out.%2520We%2520then%250Aprovide%2520one%2520cost-effective%2520way%2520to%2520find%2520good%2520learning%2520rates%2520and%2520propose%250AAdam-mini.%2520Empirically%252C%2520we%2520verify%2520that%2520Adam-mini%2520performs%2520on%2520par%2520or%2520better%2520than%250AAdamW%2520on%2520various%2520language%2520models%2520sized%2520from%2520125M%2520to%25207B%2520for%2520pre-training%252C%250Asupervised%2520fine-tuning%252C%2520and%2520RLHF.%2520The%2520reduced%2520memory%2520footprint%2520of%2520Adam-mini%250Aalso%2520alleviates%2520communication%2520overheads%2520among%2520GPUs%2520and%2520CPUs%252C%2520thereby%2520increasing%250Athroughput.%2520For%2520instance%252C%2520Adam-mini%2520achieves%252049.6%2525%2520higher%2520throughput%2520than%2520AdamW%250Awhen%2520pre-training%2520Llama2-7B%2520on%2520%25242%255Ctimes%2524%2520A800-80GB%2520GPUs%252C%2520which%2520saves%252033%2525%250Awall-clock%2520time%2520for%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16793v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More&entry.906535625=Yushun%20Zhang%20and%20Congliang%20Chen%20and%20Ziniu%20Li%20and%20Tian%20Ding%20and%20Chenwei%20Wu%20and%20Yinyu%20Ye%20and%20Zhi-Quan%20Luo%20and%20Ruoyu%20Sun&entry.1292438233=%20%20We%20propose%20Adam-mini%2C%20an%20optimizer%20that%20achieves%20on-par%20or%20better%20performance%0Athan%20AdamW%20with%2045%25%20to%2050%25%20less%20memory%20footprint.%20Adam-mini%20reduces%20memory%20by%0Acutting%20down%20the%20learning%20rate%20resources%20in%20Adam%20%28i.e.%2C%20%241/%5Csqrt%7Bv%7D%24%29.%20We%20find%0Athat%20%24%5Cgeq%24%2090%25%20of%20these%20learning%20rates%20in%20%24v%24%20could%20be%20harmlessly%20removed%20if%0Awe%20%281%29%20carefully%20partition%20the%20parameters%20into%20blocks%20following%20our%20proposed%0Aprinciple%20on%20Hessian%20structure%3B%20%282%29%20assign%20a%20single%20but%20good%20learning%20rate%20to%0Aeach%20parameter%20block.%20We%20further%20find%20that%2C%20for%20each%20of%20these%20parameter%20blocks%2C%0Athere%20exists%20a%20single%20high-quality%20learning%20rate%20that%20can%20outperform%20Adam%2C%0Aprovided%20that%20sufficient%20resources%20are%20available%20to%20search%20it%20out.%20We%20then%0Aprovide%20one%20cost-effective%20way%20to%20find%20good%20learning%20rates%20and%20propose%0AAdam-mini.%20Empirically%2C%20we%20verify%20that%20Adam-mini%20performs%20on%20par%20or%20better%20than%0AAdamW%20on%20various%20language%20models%20sized%20from%20125M%20to%207B%20for%20pre-training%2C%0Asupervised%20fine-tuning%2C%20and%20RLHF.%20The%20reduced%20memory%20footprint%20of%20Adam-mini%0Aalso%20alleviates%20communication%20overheads%20among%20GPUs%20and%20CPUs%2C%20thereby%20increasing%0Athroughput.%20For%20instance%2C%20Adam-mini%20achieves%2049.6%25%20higher%20throughput%20than%20AdamW%0Awhen%20pre-training%20Llama2-7B%20on%20%242%5Ctimes%24%20A800-80GB%20GPUs%2C%20which%20saves%2033%25%0Awall-clock%20time%20for%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16793v3&entry.124074799=Read"},
{"title": "AI-native Memory: A Pathway from LLMs Towards AGI", "author": "Jingbo Shang and Zai Zheng and Xiang Ying and Felix Tao and Mindverse Team", "abstract": "  Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.\n", "link": "http://arxiv.org/abs/2406.18312v1", "date": "2024-06-26", "relevancy": 1.8785, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4976}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4716}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-native%20Memory%3A%20A%20Pathway%20from%20LLMs%20Towards%20AGI&body=Title%3A%20AI-native%20Memory%3A%20A%20Pathway%20from%20LLMs%20Towards%20AGI%0AAuthor%3A%20Jingbo%20Shang%20and%20Zai%20Zheng%20and%20Xiang%20Ying%20and%20Felix%20Tao%20and%20Mindverse%20Team%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20the%20world%20with%20the%20sparks%20of%0Aartificial%20general%20intelligence%20%28AGI%29.%20One%20opinion%2C%20especially%20from%20some%0Astartups%20working%20on%20LLMs%2C%20argues%20that%20an%20LLM%20with%20nearly%20unlimited%20context%0Alength%20can%20realize%20AGI.%20However%2C%20they%20might%20be%20too%20optimistic%20about%20the%0Along-context%20capability%20of%20%28existing%29%20LLMs%20--%20%281%29%20Recent%20literature%20has%20shown%0Athat%20their%20effective%20context%20length%20is%20significantly%20smaller%20than%20their%20claimed%0Acontext%20length%3B%20and%20%282%29%20Our%20reasoning-in-a-haystack%20experiments%20further%0Ademonstrate%20that%20simultaneously%20finding%20the%20relevant%20information%20from%20a%20long%0Acontext%20and%20conducting%20%28simple%29%20reasoning%20is%20nearly%20impossible.%20In%20this%20paper%2C%0Awe%20envision%20a%20pathway%20from%20LLMs%20to%20AGI%20through%20the%20integration%20of%0A%5Cemph%7Bmemory%7D.%20We%20believe%20that%20AGI%20should%20be%20a%20system%20where%20LLMs%20serve%20as%20core%0Aprocessors.%20In%20addition%20to%20raw%20data%2C%20the%20memory%20in%20this%20system%20would%20store%20a%0Alarge%20number%20of%20important%20conclusions%20derived%20from%20reasoning%20processes.%0ACompared%20with%20retrieval-augmented%20generation%20%28RAG%29%20that%20merely%20processing%20raw%0Adata%2C%20this%20approach%20not%20only%20connects%20semantically%20related%20information%20closer%2C%0Abut%20also%20simplifies%20complex%20inferences%20at%20the%20time%20of%20querying.%20As%20an%0Aintermediate%20stage%2C%20the%20memory%20will%20likely%20be%20in%20the%20form%20of%20natural%20language%0Adescriptions%2C%20which%20can%20be%20directly%20consumed%20by%20users%20too.%20Ultimately%2C%20every%0Aagent/person%20should%20have%20its%20own%20large%20personal%20model%2C%20a%20deep%20neural%20network%0Amodel%20%28thus%20%5Cemph%7BAI-native%7D%29%20that%20parameterizes%20and%20compresses%20all%20types%20of%0Amemory%2C%20even%20the%20ones%20cannot%20be%20described%20by%20natural%20languages.%20Finally%2C%20we%0Adiscuss%20the%20significant%20potential%20of%20AI-native%20memory%20as%20the%20transformative%0Ainfrastructure%20for%20%28proactive%29%20engagement%2C%20personalization%2C%20distribution%2C%20and%0Asocial%20in%20the%20AGI%20era%2C%20as%20well%20as%20the%20incurred%20privacy%20and%20security%20challenges%0Awith%20preliminary%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-native%2520Memory%253A%2520A%2520Pathway%2520from%2520LLMs%2520Towards%2520AGI%26entry.906535625%3DJingbo%2520Shang%2520and%2520Zai%2520Zheng%2520and%2520Xiang%2520Ying%2520and%2520Felix%2520Tao%2520and%2520Mindverse%2520Team%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520the%2520world%2520with%2520the%2520sparks%2520of%250Aartificial%2520general%2520intelligence%2520%2528AGI%2529.%2520One%2520opinion%252C%2520especially%2520from%2520some%250Astartups%2520working%2520on%2520LLMs%252C%2520argues%2520that%2520an%2520LLM%2520with%2520nearly%2520unlimited%2520context%250Alength%2520can%2520realize%2520AGI.%2520However%252C%2520they%2520might%2520be%2520too%2520optimistic%2520about%2520the%250Along-context%2520capability%2520of%2520%2528existing%2529%2520LLMs%2520--%2520%25281%2529%2520Recent%2520literature%2520has%2520shown%250Athat%2520their%2520effective%2520context%2520length%2520is%2520significantly%2520smaller%2520than%2520their%2520claimed%250Acontext%2520length%253B%2520and%2520%25282%2529%2520Our%2520reasoning-in-a-haystack%2520experiments%2520further%250Ademonstrate%2520that%2520simultaneously%2520finding%2520the%2520relevant%2520information%2520from%2520a%2520long%250Acontext%2520and%2520conducting%2520%2528simple%2529%2520reasoning%2520is%2520nearly%2520impossible.%2520In%2520this%2520paper%252C%250Awe%2520envision%2520a%2520pathway%2520from%2520LLMs%2520to%2520AGI%2520through%2520the%2520integration%2520of%250A%255Cemph%257Bmemory%257D.%2520We%2520believe%2520that%2520AGI%2520should%2520be%2520a%2520system%2520where%2520LLMs%2520serve%2520as%2520core%250Aprocessors.%2520In%2520addition%2520to%2520raw%2520data%252C%2520the%2520memory%2520in%2520this%2520system%2520would%2520store%2520a%250Alarge%2520number%2520of%2520important%2520conclusions%2520derived%2520from%2520reasoning%2520processes.%250ACompared%2520with%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520that%2520merely%2520processing%2520raw%250Adata%252C%2520this%2520approach%2520not%2520only%2520connects%2520semantically%2520related%2520information%2520closer%252C%250Abut%2520also%2520simplifies%2520complex%2520inferences%2520at%2520the%2520time%2520of%2520querying.%2520As%2520an%250Aintermediate%2520stage%252C%2520the%2520memory%2520will%2520likely%2520be%2520in%2520the%2520form%2520of%2520natural%2520language%250Adescriptions%252C%2520which%2520can%2520be%2520directly%2520consumed%2520by%2520users%2520too.%2520Ultimately%252C%2520every%250Aagent/person%2520should%2520have%2520its%2520own%2520large%2520personal%2520model%252C%2520a%2520deep%2520neural%2520network%250Amodel%2520%2528thus%2520%255Cemph%257BAI-native%257D%2529%2520that%2520parameterizes%2520and%2520compresses%2520all%2520types%2520of%250Amemory%252C%2520even%2520the%2520ones%2520cannot%2520be%2520described%2520by%2520natural%2520languages.%2520Finally%252C%2520we%250Adiscuss%2520the%2520significant%2520potential%2520of%2520AI-native%2520memory%2520as%2520the%2520transformative%250Ainfrastructure%2520for%2520%2528proactive%2529%2520engagement%252C%2520personalization%252C%2520distribution%252C%2520and%250Asocial%2520in%2520the%2520AGI%2520era%252C%2520as%2520well%2520as%2520the%2520incurred%2520privacy%2520and%2520security%2520challenges%250Awith%2520preliminary%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-native%20Memory%3A%20A%20Pathway%20from%20LLMs%20Towards%20AGI&entry.906535625=Jingbo%20Shang%20and%20Zai%20Zheng%20and%20Xiang%20Ying%20and%20Felix%20Tao%20and%20Mindverse%20Team&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20the%20world%20with%20the%20sparks%20of%0Aartificial%20general%20intelligence%20%28AGI%29.%20One%20opinion%2C%20especially%20from%20some%0Astartups%20working%20on%20LLMs%2C%20argues%20that%20an%20LLM%20with%20nearly%20unlimited%20context%0Alength%20can%20realize%20AGI.%20However%2C%20they%20might%20be%20too%20optimistic%20about%20the%0Along-context%20capability%20of%20%28existing%29%20LLMs%20--%20%281%29%20Recent%20literature%20has%20shown%0Athat%20their%20effective%20context%20length%20is%20significantly%20smaller%20than%20their%20claimed%0Acontext%20length%3B%20and%20%282%29%20Our%20reasoning-in-a-haystack%20experiments%20further%0Ademonstrate%20that%20simultaneously%20finding%20the%20relevant%20information%20from%20a%20long%0Acontext%20and%20conducting%20%28simple%29%20reasoning%20is%20nearly%20impossible.%20In%20this%20paper%2C%0Awe%20envision%20a%20pathway%20from%20LLMs%20to%20AGI%20through%20the%20integration%20of%0A%5Cemph%7Bmemory%7D.%20We%20believe%20that%20AGI%20should%20be%20a%20system%20where%20LLMs%20serve%20as%20core%0Aprocessors.%20In%20addition%20to%20raw%20data%2C%20the%20memory%20in%20this%20system%20would%20store%20a%0Alarge%20number%20of%20important%20conclusions%20derived%20from%20reasoning%20processes.%0ACompared%20with%20retrieval-augmented%20generation%20%28RAG%29%20that%20merely%20processing%20raw%0Adata%2C%20this%20approach%20not%20only%20connects%20semantically%20related%20information%20closer%2C%0Abut%20also%20simplifies%20complex%20inferences%20at%20the%20time%20of%20querying.%20As%20an%0Aintermediate%20stage%2C%20the%20memory%20will%20likely%20be%20in%20the%20form%20of%20natural%20language%0Adescriptions%2C%20which%20can%20be%20directly%20consumed%20by%20users%20too.%20Ultimately%2C%20every%0Aagent/person%20should%20have%20its%20own%20large%20personal%20model%2C%20a%20deep%20neural%20network%0Amodel%20%28thus%20%5Cemph%7BAI-native%7D%29%20that%20parameterizes%20and%20compresses%20all%20types%20of%0Amemory%2C%20even%20the%20ones%20cannot%20be%20described%20by%20natural%20languages.%20Finally%2C%20we%0Adiscuss%20the%20significant%20potential%20of%20AI-native%20memory%20as%20the%20transformative%0Ainfrastructure%20for%20%28proactive%29%20engagement%2C%20personalization%2C%20distribution%2C%20and%0Asocial%20in%20the%20AGI%20era%2C%20as%20well%20as%20the%20incurred%20privacy%20and%20security%20challenges%0Awith%20preliminary%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18312v1&entry.124074799=Read"},
{"title": "MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large\n  Language Models Using Odyssey Math Data", "author": "Meng Fang and Xiangpeng Wan and Fei Lu and Fei Xing and Kai Zou", "abstract": "  Large language models (LLMs) have significantly advanced natural language\nunderstanding and demonstrated strong problem-solving abilities. Despite these\nsuccesses, most LLMs still struggle with solving mathematical problems due to\nthe intricate reasoning required. This paper investigates the mathematical\nproblem-solving capabilities of LLMs using the newly developed \"MathOdyssey\"\ndataset. The dataset includes diverse mathematical problems at high school and\nuniversity levels, created by experts from notable institutions to rigorously\ntest LLMs in advanced problem-solving scenarios and cover a wider range of\nsubject areas. By providing the MathOdyssey dataset as a resource to the AI\ncommunity, we aim to contribute to the understanding and improvement of AI\ncapabilities in complex mathematical problem-solving. We conduct benchmarking\non open-source models, such as Llama-3 and DBRX-Instruct, and closed-source\nmodels from the GPT series and Gemini models. Our results indicate that while\nLLMs perform well on routine and moderately difficult tasks, they face\nsignificant challenges with Olympiad-level problems and complex\nuniversity-level questions. Our analysis shows a narrowing performance gap\nbetween open-source and closed-source models, yet substantial challenges\nremain, particularly with the most demanding problems. This study highlights\nthe ongoing need for research to enhance the mathematical reasoning of LLMs.\nThe dataset, results, and code are publicly available.\n", "link": "http://arxiv.org/abs/2406.18321v1", "date": "2024-06-26", "relevancy": 1.8667, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4713}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MathOdyssey%3A%20Benchmarking%20Mathematical%20Problem-Solving%20Skills%20in%20Large%0A%20%20Language%20Models%20Using%20Odyssey%20Math%20Data&body=Title%3A%20MathOdyssey%3A%20Benchmarking%20Mathematical%20Problem-Solving%20Skills%20in%20Large%0A%20%20Language%20Models%20Using%20Odyssey%20Math%20Data%0AAuthor%3A%20Meng%20Fang%20and%20Xiangpeng%20Wan%20and%20Fei%20Lu%20and%20Fei%20Xing%20and%20Kai%20Zou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20natural%20language%0Aunderstanding%20and%20demonstrated%20strong%20problem-solving%20abilities.%20Despite%20these%0Asuccesses%2C%20most%20LLMs%20still%20struggle%20with%20solving%20mathematical%20problems%20due%20to%0Athe%20intricate%20reasoning%20required.%20This%20paper%20investigates%20the%20mathematical%0Aproblem-solving%20capabilities%20of%20LLMs%20using%20the%20newly%20developed%20%22MathOdyssey%22%0Adataset.%20The%20dataset%20includes%20diverse%20mathematical%20problems%20at%20high%20school%20and%0Auniversity%20levels%2C%20created%20by%20experts%20from%20notable%20institutions%20to%20rigorously%0Atest%20LLMs%20in%20advanced%20problem-solving%20scenarios%20and%20cover%20a%20wider%20range%20of%0Asubject%20areas.%20By%20providing%20the%20MathOdyssey%20dataset%20as%20a%20resource%20to%20the%20AI%0Acommunity%2C%20we%20aim%20to%20contribute%20to%20the%20understanding%20and%20improvement%20of%20AI%0Acapabilities%20in%20complex%20mathematical%20problem-solving.%20We%20conduct%20benchmarking%0Aon%20open-source%20models%2C%20such%20as%20Llama-3%20and%20DBRX-Instruct%2C%20and%20closed-source%0Amodels%20from%20the%20GPT%20series%20and%20Gemini%20models.%20Our%20results%20indicate%20that%20while%0ALLMs%20perform%20well%20on%20routine%20and%20moderately%20difficult%20tasks%2C%20they%20face%0Asignificant%20challenges%20with%20Olympiad-level%20problems%20and%20complex%0Auniversity-level%20questions.%20Our%20analysis%20shows%20a%20narrowing%20performance%20gap%0Abetween%20open-source%20and%20closed-source%20models%2C%20yet%20substantial%20challenges%0Aremain%2C%20particularly%20with%20the%20most%20demanding%20problems.%20This%20study%20highlights%0Athe%20ongoing%20need%20for%20research%20to%20enhance%20the%20mathematical%20reasoning%20of%20LLMs.%0AThe%20dataset%2C%20results%2C%20and%20code%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathOdyssey%253A%2520Benchmarking%2520Mathematical%2520Problem-Solving%2520Skills%2520in%2520Large%250A%2520%2520Language%2520Models%2520Using%2520Odyssey%2520Math%2520Data%26entry.906535625%3DMeng%2520Fang%2520and%2520Xiangpeng%2520Wan%2520and%2520Fei%2520Lu%2520and%2520Fei%2520Xing%2520and%2520Kai%2520Zou%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520advanced%2520natural%2520language%250Aunderstanding%2520and%2520demonstrated%2520strong%2520problem-solving%2520abilities.%2520Despite%2520these%250Asuccesses%252C%2520most%2520LLMs%2520still%2520struggle%2520with%2520solving%2520mathematical%2520problems%2520due%2520to%250Athe%2520intricate%2520reasoning%2520required.%2520This%2520paper%2520investigates%2520the%2520mathematical%250Aproblem-solving%2520capabilities%2520of%2520LLMs%2520using%2520the%2520newly%2520developed%2520%2522MathOdyssey%2522%250Adataset.%2520The%2520dataset%2520includes%2520diverse%2520mathematical%2520problems%2520at%2520high%2520school%2520and%250Auniversity%2520levels%252C%2520created%2520by%2520experts%2520from%2520notable%2520institutions%2520to%2520rigorously%250Atest%2520LLMs%2520in%2520advanced%2520problem-solving%2520scenarios%2520and%2520cover%2520a%2520wider%2520range%2520of%250Asubject%2520areas.%2520By%2520providing%2520the%2520MathOdyssey%2520dataset%2520as%2520a%2520resource%2520to%2520the%2520AI%250Acommunity%252C%2520we%2520aim%2520to%2520contribute%2520to%2520the%2520understanding%2520and%2520improvement%2520of%2520AI%250Acapabilities%2520in%2520complex%2520mathematical%2520problem-solving.%2520We%2520conduct%2520benchmarking%250Aon%2520open-source%2520models%252C%2520such%2520as%2520Llama-3%2520and%2520DBRX-Instruct%252C%2520and%2520closed-source%250Amodels%2520from%2520the%2520GPT%2520series%2520and%2520Gemini%2520models.%2520Our%2520results%2520indicate%2520that%2520while%250ALLMs%2520perform%2520well%2520on%2520routine%2520and%2520moderately%2520difficult%2520tasks%252C%2520they%2520face%250Asignificant%2520challenges%2520with%2520Olympiad-level%2520problems%2520and%2520complex%250Auniversity-level%2520questions.%2520Our%2520analysis%2520shows%2520a%2520narrowing%2520performance%2520gap%250Abetween%2520open-source%2520and%2520closed-source%2520models%252C%2520yet%2520substantial%2520challenges%250Aremain%252C%2520particularly%2520with%2520the%2520most%2520demanding%2520problems.%2520This%2520study%2520highlights%250Athe%2520ongoing%2520need%2520for%2520research%2520to%2520enhance%2520the%2520mathematical%2520reasoning%2520of%2520LLMs.%250AThe%2520dataset%252C%2520results%252C%2520and%2520code%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MathOdyssey%3A%20Benchmarking%20Mathematical%20Problem-Solving%20Skills%20in%20Large%0A%20%20Language%20Models%20Using%20Odyssey%20Math%20Data&entry.906535625=Meng%20Fang%20and%20Xiangpeng%20Wan%20and%20Fei%20Lu%20and%20Fei%20Xing%20and%20Kai%20Zou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20natural%20language%0Aunderstanding%20and%20demonstrated%20strong%20problem-solving%20abilities.%20Despite%20these%0Asuccesses%2C%20most%20LLMs%20still%20struggle%20with%20solving%20mathematical%20problems%20due%20to%0Athe%20intricate%20reasoning%20required.%20This%20paper%20investigates%20the%20mathematical%0Aproblem-solving%20capabilities%20of%20LLMs%20using%20the%20newly%20developed%20%22MathOdyssey%22%0Adataset.%20The%20dataset%20includes%20diverse%20mathematical%20problems%20at%20high%20school%20and%0Auniversity%20levels%2C%20created%20by%20experts%20from%20notable%20institutions%20to%20rigorously%0Atest%20LLMs%20in%20advanced%20problem-solving%20scenarios%20and%20cover%20a%20wider%20range%20of%0Asubject%20areas.%20By%20providing%20the%20MathOdyssey%20dataset%20as%20a%20resource%20to%20the%20AI%0Acommunity%2C%20we%20aim%20to%20contribute%20to%20the%20understanding%20and%20improvement%20of%20AI%0Acapabilities%20in%20complex%20mathematical%20problem-solving.%20We%20conduct%20benchmarking%0Aon%20open-source%20models%2C%20such%20as%20Llama-3%20and%20DBRX-Instruct%2C%20and%20closed-source%0Amodels%20from%20the%20GPT%20series%20and%20Gemini%20models.%20Our%20results%20indicate%20that%20while%0ALLMs%20perform%20well%20on%20routine%20and%20moderately%20difficult%20tasks%2C%20they%20face%0Asignificant%20challenges%20with%20Olympiad-level%20problems%20and%20complex%0Auniversity-level%20questions.%20Our%20analysis%20shows%20a%20narrowing%20performance%20gap%0Abetween%20open-source%20and%20closed-source%20models%2C%20yet%20substantial%20challenges%0Aremain%2C%20particularly%20with%20the%20most%20demanding%20problems.%20This%20study%20highlights%0Athe%20ongoing%20need%20for%20research%20to%20enhance%20the%20mathematical%20reasoning%20of%20LLMs.%0AThe%20dataset%2C%20results%2C%20and%20code%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18321v1&entry.124074799=Read"},
{"title": "Early Classification of Time Series: Taxonomy and Benchmark", "author": "Aur\u00e9lien Renault and Alexis Bondu and Antoine Cornu\u00e9jols and Vincent Lemaire", "abstract": "  In many situations, the measurements of a studied phenomenon are provided\nsequentially, and the prediction of its class needs to be made as early as\npossible so as not to incur too high a time penalty, but not too early and risk\npaying the cost of misclassification. This problem has been particularly\nstudied in the case of time series, and is known as Early Classification of\nTime Series (ECTS). Although it has been the subject of a growing body of\nliterature, there is still a lack of a systematic, shared evaluation protocol\nto compare the relative merits of the various existing methods. This document\nbegins by situating these methods within a principle-based taxonomy. It defines\ndimensions for organizing their evaluation, and then reports the results of a\nvery extensive set of experiments along these dimensions involving nine\nstate-of-the art ECTS algorithms. In addition, these and other experiments can\nbe carried out using an open-source library in which most of the existing ECTS\nalgorithms have been implemented (see \\url{https://github.com/ML-EDM/ml_edm}).\n", "link": "http://arxiv.org/abs/2406.18332v1", "date": "2024-06-26", "relevancy": 1.8664, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4981}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.449}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Classification%20of%20Time%20Series%3A%20Taxonomy%20and%20Benchmark&body=Title%3A%20Early%20Classification%20of%20Time%20Series%3A%20Taxonomy%20and%20Benchmark%0AAuthor%3A%20Aur%C3%A9lien%20Renault%20and%20Alexis%20Bondu%20and%20Antoine%20Cornu%C3%A9jols%20and%20Vincent%20Lemaire%0AAbstract%3A%20%20%20In%20many%20situations%2C%20the%20measurements%20of%20a%20studied%20phenomenon%20are%20provided%0Asequentially%2C%20and%20the%20prediction%20of%20its%20class%20needs%20to%20be%20made%20as%20early%20as%0Apossible%20so%20as%20not%20to%20incur%20too%20high%20a%20time%20penalty%2C%20but%20not%20too%20early%20and%20risk%0Apaying%20the%20cost%20of%20misclassification.%20This%20problem%20has%20been%20particularly%0Astudied%20in%20the%20case%20of%20time%20series%2C%20and%20is%20known%20as%20Early%20Classification%20of%0ATime%20Series%20%28ECTS%29.%20Although%20it%20has%20been%20the%20subject%20of%20a%20growing%20body%20of%0Aliterature%2C%20there%20is%20still%20a%20lack%20of%20a%20systematic%2C%20shared%20evaluation%20protocol%0Ato%20compare%20the%20relative%20merits%20of%20the%20various%20existing%20methods.%20This%20document%0Abegins%20by%20situating%20these%20methods%20within%20a%20principle-based%20taxonomy.%20It%20defines%0Adimensions%20for%20organizing%20their%20evaluation%2C%20and%20then%20reports%20the%20results%20of%20a%0Avery%20extensive%20set%20of%20experiments%20along%20these%20dimensions%20involving%20nine%0Astate-of-the%20art%20ECTS%20algorithms.%20In%20addition%2C%20these%20and%20other%20experiments%20can%0Abe%20carried%20out%20using%20an%20open-source%20library%20in%20which%20most%20of%20the%20existing%20ECTS%0Aalgorithms%20have%20been%20implemented%20%28see%20%5Curl%7Bhttps%3A//github.com/ML-EDM/ml_edm%7D%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Classification%2520of%2520Time%2520Series%253A%2520Taxonomy%2520and%2520Benchmark%26entry.906535625%3DAur%25C3%25A9lien%2520Renault%2520and%2520Alexis%2520Bondu%2520and%2520Antoine%2520Cornu%25C3%25A9jols%2520and%2520Vincent%2520Lemaire%26entry.1292438233%3D%2520%2520In%2520many%2520situations%252C%2520the%2520measurements%2520of%2520a%2520studied%2520phenomenon%2520are%2520provided%250Asequentially%252C%2520and%2520the%2520prediction%2520of%2520its%2520class%2520needs%2520to%2520be%2520made%2520as%2520early%2520as%250Apossible%2520so%2520as%2520not%2520to%2520incur%2520too%2520high%2520a%2520time%2520penalty%252C%2520but%2520not%2520too%2520early%2520and%2520risk%250Apaying%2520the%2520cost%2520of%2520misclassification.%2520This%2520problem%2520has%2520been%2520particularly%250Astudied%2520in%2520the%2520case%2520of%2520time%2520series%252C%2520and%2520is%2520known%2520as%2520Early%2520Classification%2520of%250ATime%2520Series%2520%2528ECTS%2529.%2520Although%2520it%2520has%2520been%2520the%2520subject%2520of%2520a%2520growing%2520body%2520of%250Aliterature%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520a%2520systematic%252C%2520shared%2520evaluation%2520protocol%250Ato%2520compare%2520the%2520relative%2520merits%2520of%2520the%2520various%2520existing%2520methods.%2520This%2520document%250Abegins%2520by%2520situating%2520these%2520methods%2520within%2520a%2520principle-based%2520taxonomy.%2520It%2520defines%250Adimensions%2520for%2520organizing%2520their%2520evaluation%252C%2520and%2520then%2520reports%2520the%2520results%2520of%2520a%250Avery%2520extensive%2520set%2520of%2520experiments%2520along%2520these%2520dimensions%2520involving%2520nine%250Astate-of-the%2520art%2520ECTS%2520algorithms.%2520In%2520addition%252C%2520these%2520and%2520other%2520experiments%2520can%250Abe%2520carried%2520out%2520using%2520an%2520open-source%2520library%2520in%2520which%2520most%2520of%2520the%2520existing%2520ECTS%250Aalgorithms%2520have%2520been%2520implemented%2520%2528see%2520%255Curl%257Bhttps%253A//github.com/ML-EDM/ml_edm%257D%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Classification%20of%20Time%20Series%3A%20Taxonomy%20and%20Benchmark&entry.906535625=Aur%C3%A9lien%20Renault%20and%20Alexis%20Bondu%20and%20Antoine%20Cornu%C3%A9jols%20and%20Vincent%20Lemaire&entry.1292438233=%20%20In%20many%20situations%2C%20the%20measurements%20of%20a%20studied%20phenomenon%20are%20provided%0Asequentially%2C%20and%20the%20prediction%20of%20its%20class%20needs%20to%20be%20made%20as%20early%20as%0Apossible%20so%20as%20not%20to%20incur%20too%20high%20a%20time%20penalty%2C%20but%20not%20too%20early%20and%20risk%0Apaying%20the%20cost%20of%20misclassification.%20This%20problem%20has%20been%20particularly%0Astudied%20in%20the%20case%20of%20time%20series%2C%20and%20is%20known%20as%20Early%20Classification%20of%0ATime%20Series%20%28ECTS%29.%20Although%20it%20has%20been%20the%20subject%20of%20a%20growing%20body%20of%0Aliterature%2C%20there%20is%20still%20a%20lack%20of%20a%20systematic%2C%20shared%20evaluation%20protocol%0Ato%20compare%20the%20relative%20merits%20of%20the%20various%20existing%20methods.%20This%20document%0Abegins%20by%20situating%20these%20methods%20within%20a%20principle-based%20taxonomy.%20It%20defines%0Adimensions%20for%20organizing%20their%20evaluation%2C%20and%20then%20reports%20the%20results%20of%20a%0Avery%20extensive%20set%20of%20experiments%20along%20these%20dimensions%20involving%20nine%0Astate-of-the%20art%20ECTS%20algorithms.%20In%20addition%2C%20these%20and%20other%20experiments%20can%0Abe%20carried%20out%20using%20an%20open-source%20library%20in%20which%20most%20of%20the%20existing%20ECTS%0Aalgorithms%20have%20been%20implemented%20%28see%20%5Curl%7Bhttps%3A//github.com/ML-EDM/ml_edm%7D%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18332v1&entry.124074799=Read"},
{"title": "Enhancing Federated Learning with Adaptive Differential Privacy and\n  Priority-Based Aggregation", "author": "Mahtab Talaei and Iman Izadi", "abstract": "  Federated learning (FL), a novel branch of distributed machine learning (ML),\ndevelops global models through a private procedure without direct access to\nlocal datasets. However, it is still possible to access the model updates\n(gradient updates of deep neural networks) transferred between clients and\nservers, potentially revealing sensitive local information to adversaries using\nmodel inversion attacks. Differential privacy (DP) offers a promising approach\nto addressing this issue by adding noise to the parameters. On the other hand,\nheterogeneities in data structure, storage, communication, and computational\ncapabilities of devices can cause convergence problems and delays in developing\nthe global model. A personalized weighted averaging of local parameters based\non the resources of each device can yield a better aggregated model in each\nround. In this paper, to efficiently preserve privacy, we propose a\npersonalized DP framework that injects noise based on clients' relative impact\nfactors and aggregates parameters while considering heterogeneities and\nadjusting properties. To fulfill the DP requirements, we first analyze the\nconvergence boundary of the FL algorithm when impact factors are personalized\nand fixed throughout the learning process. We then further study the\nconvergence property considering time-varying (adaptive) impact factors.\n", "link": "http://arxiv.org/abs/2406.18491v1", "date": "2024-06-26", "relevancy": 1.8661, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4696}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4651}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Federated%20Learning%20with%20Adaptive%20Differential%20Privacy%20and%0A%20%20Priority-Based%20Aggregation&body=Title%3A%20Enhancing%20Federated%20Learning%20with%20Adaptive%20Differential%20Privacy%20and%0A%20%20Priority-Based%20Aggregation%0AAuthor%3A%20Mahtab%20Talaei%20and%20Iman%20Izadi%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%2C%20a%20novel%20branch%20of%20distributed%20machine%20learning%20%28ML%29%2C%0Adevelops%20global%20models%20through%20a%20private%20procedure%20without%20direct%20access%20to%0Alocal%20datasets.%20However%2C%20it%20is%20still%20possible%20to%20access%20the%20model%20updates%0A%28gradient%20updates%20of%20deep%20neural%20networks%29%20transferred%20between%20clients%20and%0Aservers%2C%20potentially%20revealing%20sensitive%20local%20information%20to%20adversaries%20using%0Amodel%20inversion%20attacks.%20Differential%20privacy%20%28DP%29%20offers%20a%20promising%20approach%0Ato%20addressing%20this%20issue%20by%20adding%20noise%20to%20the%20parameters.%20On%20the%20other%20hand%2C%0Aheterogeneities%20in%20data%20structure%2C%20storage%2C%20communication%2C%20and%20computational%0Acapabilities%20of%20devices%20can%20cause%20convergence%20problems%20and%20delays%20in%20developing%0Athe%20global%20model.%20A%20personalized%20weighted%20averaging%20of%20local%20parameters%20based%0Aon%20the%20resources%20of%20each%20device%20can%20yield%20a%20better%20aggregated%20model%20in%20each%0Around.%20In%20this%20paper%2C%20to%20efficiently%20preserve%20privacy%2C%20we%20propose%20a%0Apersonalized%20DP%20framework%20that%20injects%20noise%20based%20on%20clients%27%20relative%20impact%0Afactors%20and%20aggregates%20parameters%20while%20considering%20heterogeneities%20and%0Aadjusting%20properties.%20To%20fulfill%20the%20DP%20requirements%2C%20we%20first%20analyze%20the%0Aconvergence%20boundary%20of%20the%20FL%20algorithm%20when%20impact%20factors%20are%20personalized%0Aand%20fixed%20throughout%20the%20learning%20process.%20We%20then%20further%20study%20the%0Aconvergence%20property%20considering%20time-varying%20%28adaptive%29%20impact%20factors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Federated%2520Learning%2520with%2520Adaptive%2520Differential%2520Privacy%2520and%250A%2520%2520Priority-Based%2520Aggregation%26entry.906535625%3DMahtab%2520Talaei%2520and%2520Iman%2520Izadi%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%252C%2520a%2520novel%2520branch%2520of%2520distributed%2520machine%2520learning%2520%2528ML%2529%252C%250Adevelops%2520global%2520models%2520through%2520a%2520private%2520procedure%2520without%2520direct%2520access%2520to%250Alocal%2520datasets.%2520However%252C%2520it%2520is%2520still%2520possible%2520to%2520access%2520the%2520model%2520updates%250A%2528gradient%2520updates%2520of%2520deep%2520neural%2520networks%2529%2520transferred%2520between%2520clients%2520and%250Aservers%252C%2520potentially%2520revealing%2520sensitive%2520local%2520information%2520to%2520adversaries%2520using%250Amodel%2520inversion%2520attacks.%2520Differential%2520privacy%2520%2528DP%2529%2520offers%2520a%2520promising%2520approach%250Ato%2520addressing%2520this%2520issue%2520by%2520adding%2520noise%2520to%2520the%2520parameters.%2520On%2520the%2520other%2520hand%252C%250Aheterogeneities%2520in%2520data%2520structure%252C%2520storage%252C%2520communication%252C%2520and%2520computational%250Acapabilities%2520of%2520devices%2520can%2520cause%2520convergence%2520problems%2520and%2520delays%2520in%2520developing%250Athe%2520global%2520model.%2520A%2520personalized%2520weighted%2520averaging%2520of%2520local%2520parameters%2520based%250Aon%2520the%2520resources%2520of%2520each%2520device%2520can%2520yield%2520a%2520better%2520aggregated%2520model%2520in%2520each%250Around.%2520In%2520this%2520paper%252C%2520to%2520efficiently%2520preserve%2520privacy%252C%2520we%2520propose%2520a%250Apersonalized%2520DP%2520framework%2520that%2520injects%2520noise%2520based%2520on%2520clients%2527%2520relative%2520impact%250Afactors%2520and%2520aggregates%2520parameters%2520while%2520considering%2520heterogeneities%2520and%250Aadjusting%2520properties.%2520To%2520fulfill%2520the%2520DP%2520requirements%252C%2520we%2520first%2520analyze%2520the%250Aconvergence%2520boundary%2520of%2520the%2520FL%2520algorithm%2520when%2520impact%2520factors%2520are%2520personalized%250Aand%2520fixed%2520throughout%2520the%2520learning%2520process.%2520We%2520then%2520further%2520study%2520the%250Aconvergence%2520property%2520considering%2520time-varying%2520%2528adaptive%2529%2520impact%2520factors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Federated%20Learning%20with%20Adaptive%20Differential%20Privacy%20and%0A%20%20Priority-Based%20Aggregation&entry.906535625=Mahtab%20Talaei%20and%20Iman%20Izadi&entry.1292438233=%20%20Federated%20learning%20%28FL%29%2C%20a%20novel%20branch%20of%20distributed%20machine%20learning%20%28ML%29%2C%0Adevelops%20global%20models%20through%20a%20private%20procedure%20without%20direct%20access%20to%0Alocal%20datasets.%20However%2C%20it%20is%20still%20possible%20to%20access%20the%20model%20updates%0A%28gradient%20updates%20of%20deep%20neural%20networks%29%20transferred%20between%20clients%20and%0Aservers%2C%20potentially%20revealing%20sensitive%20local%20information%20to%20adversaries%20using%0Amodel%20inversion%20attacks.%20Differential%20privacy%20%28DP%29%20offers%20a%20promising%20approach%0Ato%20addressing%20this%20issue%20by%20adding%20noise%20to%20the%20parameters.%20On%20the%20other%20hand%2C%0Aheterogeneities%20in%20data%20structure%2C%20storage%2C%20communication%2C%20and%20computational%0Acapabilities%20of%20devices%20can%20cause%20convergence%20problems%20and%20delays%20in%20developing%0Athe%20global%20model.%20A%20personalized%20weighted%20averaging%20of%20local%20parameters%20based%0Aon%20the%20resources%20of%20each%20device%20can%20yield%20a%20better%20aggregated%20model%20in%20each%0Around.%20In%20this%20paper%2C%20to%20efficiently%20preserve%20privacy%2C%20we%20propose%20a%0Apersonalized%20DP%20framework%20that%20injects%20noise%20based%20on%20clients%27%20relative%20impact%0Afactors%20and%20aggregates%20parameters%20while%20considering%20heterogeneities%20and%0Aadjusting%20properties.%20To%20fulfill%20the%20DP%20requirements%2C%20we%20first%20analyze%20the%0Aconvergence%20boundary%20of%20the%20FL%20algorithm%20when%20impact%20factors%20are%20personalized%0Aand%20fixed%20throughout%20the%20learning%20process.%20We%20then%20further%20study%20the%0Aconvergence%20property%20considering%20time-varying%20%28adaptive%29%20impact%20factors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18491v1&entry.124074799=Read"},
{"title": "Scaling and renormalization in high-dimensional regression", "author": "Alexander Atanasov and Jacob A. Zavatone-Veth and Cengiz Pehlevan", "abstract": "  This paper presents a succinct derivation of the training and generalization\nperformance of a variety of high-dimensional ridge regression models using the\nbasic tools of random matrix theory and free probability. We provide an\nintroduction and review of recent results on these topics, aimed at readers\nwith backgrounds in physics and deep learning. Analytic formulas for the\ntraining and generalization errors are obtained in a few lines of algebra\ndirectly from the properties of the $S$-transform of free probability. This\nallows for a straightforward identification of the sources of power-law scaling\nin model performance. We compute the generalization error of a broad class of\nrandom feature models. We find that in all models, the $S$-transform\ncorresponds to the train-test generalization gap, and yields an analogue of the\ngeneralized-cross-validation estimator. Using these techniques, we derive\nfine-grained bias-variance decompositions for a very general class of random\nfeature models with structured covariates. These novel results allow us to\ndiscover a scaling regime for random feature models where the variance due to\nthe features limits performance in the overparameterized setting. We also\ndemonstrate how anisotropic weight structure in random feature models can limit\nperformance and lead to nontrivial exponents for finite-width corrections in\nthe overparameterized setting. Our results extend and provide a unifying\nperspective on earlier models of neural scaling laws.\n", "link": "http://arxiv.org/abs/2405.00592v3", "date": "2024-06-26", "relevancy": 1.8639, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5445}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4541}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20and%20renormalization%20in%20high-dimensional%20regression&body=Title%3A%20Scaling%20and%20renormalization%20in%20high-dimensional%20regression%0AAuthor%3A%20Alexander%20Atanasov%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20succinct%20derivation%20of%20the%20training%20and%20generalization%0Aperformance%20of%20a%20variety%20of%20high-dimensional%20ridge%20regression%20models%20using%20the%0Abasic%20tools%20of%20random%20matrix%20theory%20and%20free%20probability.%20We%20provide%20an%0Aintroduction%20and%20review%20of%20recent%20results%20on%20these%20topics%2C%20aimed%20at%20readers%0Awith%20backgrounds%20in%20physics%20and%20deep%20learning.%20Analytic%20formulas%20for%20the%0Atraining%20and%20generalization%20errors%20are%20obtained%20in%20a%20few%20lines%20of%20algebra%0Adirectly%20from%20the%20properties%20of%20the%20%24S%24-transform%20of%20free%20probability.%20This%0Aallows%20for%20a%20straightforward%20identification%20of%20the%20sources%20of%20power-law%20scaling%0Ain%20model%20performance.%20We%20compute%20the%20generalization%20error%20of%20a%20broad%20class%20of%0Arandom%20feature%20models.%20We%20find%20that%20in%20all%20models%2C%20the%20%24S%24-transform%0Acorresponds%20to%20the%20train-test%20generalization%20gap%2C%20and%20yields%20an%20analogue%20of%20the%0Ageneralized-cross-validation%20estimator.%20Using%20these%20techniques%2C%20we%20derive%0Afine-grained%20bias-variance%20decompositions%20for%20a%20very%20general%20class%20of%20random%0Afeature%20models%20with%20structured%20covariates.%20These%20novel%20results%20allow%20us%20to%0Adiscover%20a%20scaling%20regime%20for%20random%20feature%20models%20where%20the%20variance%20due%20to%0Athe%20features%20limits%20performance%20in%20the%20overparameterized%20setting.%20We%20also%0Ademonstrate%20how%20anisotropic%20weight%20structure%20in%20random%20feature%20models%20can%20limit%0Aperformance%20and%20lead%20to%20nontrivial%20exponents%20for%20finite-width%20corrections%20in%0Athe%20overparameterized%20setting.%20Our%20results%20extend%20and%20provide%20a%20unifying%0Aperspective%20on%20earlier%20models%20of%20neural%20scaling%20laws.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00592v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520and%2520renormalization%2520in%2520high-dimensional%2520regression%26entry.906535625%3DAlexander%2520Atanasov%2520and%2520Jacob%2520A.%2520Zavatone-Veth%2520and%2520Cengiz%2520Pehlevan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520succinct%2520derivation%2520of%2520the%2520training%2520and%2520generalization%250Aperformance%2520of%2520a%2520variety%2520of%2520high-dimensional%2520ridge%2520regression%2520models%2520using%2520the%250Abasic%2520tools%2520of%2520random%2520matrix%2520theory%2520and%2520free%2520probability.%2520We%2520provide%2520an%250Aintroduction%2520and%2520review%2520of%2520recent%2520results%2520on%2520these%2520topics%252C%2520aimed%2520at%2520readers%250Awith%2520backgrounds%2520in%2520physics%2520and%2520deep%2520learning.%2520Analytic%2520formulas%2520for%2520the%250Atraining%2520and%2520generalization%2520errors%2520are%2520obtained%2520in%2520a%2520few%2520lines%2520of%2520algebra%250Adirectly%2520from%2520the%2520properties%2520of%2520the%2520%2524S%2524-transform%2520of%2520free%2520probability.%2520This%250Aallows%2520for%2520a%2520straightforward%2520identification%2520of%2520the%2520sources%2520of%2520power-law%2520scaling%250Ain%2520model%2520performance.%2520We%2520compute%2520the%2520generalization%2520error%2520of%2520a%2520broad%2520class%2520of%250Arandom%2520feature%2520models.%2520We%2520find%2520that%2520in%2520all%2520models%252C%2520the%2520%2524S%2524-transform%250Acorresponds%2520to%2520the%2520train-test%2520generalization%2520gap%252C%2520and%2520yields%2520an%2520analogue%2520of%2520the%250Ageneralized-cross-validation%2520estimator.%2520Using%2520these%2520techniques%252C%2520we%2520derive%250Afine-grained%2520bias-variance%2520decompositions%2520for%2520a%2520very%2520general%2520class%2520of%2520random%250Afeature%2520models%2520with%2520structured%2520covariates.%2520These%2520novel%2520results%2520allow%2520us%2520to%250Adiscover%2520a%2520scaling%2520regime%2520for%2520random%2520feature%2520models%2520where%2520the%2520variance%2520due%2520to%250Athe%2520features%2520limits%2520performance%2520in%2520the%2520overparameterized%2520setting.%2520We%2520also%250Ademonstrate%2520how%2520anisotropic%2520weight%2520structure%2520in%2520random%2520feature%2520models%2520can%2520limit%250Aperformance%2520and%2520lead%2520to%2520nontrivial%2520exponents%2520for%2520finite-width%2520corrections%2520in%250Athe%2520overparameterized%2520setting.%2520Our%2520results%2520extend%2520and%2520provide%2520a%2520unifying%250Aperspective%2520on%2520earlier%2520models%2520of%2520neural%2520scaling%2520laws.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00592v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20and%20renormalization%20in%20high-dimensional%20regression&entry.906535625=Alexander%20Atanasov%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20This%20paper%20presents%20a%20succinct%20derivation%20of%20the%20training%20and%20generalization%0Aperformance%20of%20a%20variety%20of%20high-dimensional%20ridge%20regression%20models%20using%20the%0Abasic%20tools%20of%20random%20matrix%20theory%20and%20free%20probability.%20We%20provide%20an%0Aintroduction%20and%20review%20of%20recent%20results%20on%20these%20topics%2C%20aimed%20at%20readers%0Awith%20backgrounds%20in%20physics%20and%20deep%20learning.%20Analytic%20formulas%20for%20the%0Atraining%20and%20generalization%20errors%20are%20obtained%20in%20a%20few%20lines%20of%20algebra%0Adirectly%20from%20the%20properties%20of%20the%20%24S%24-transform%20of%20free%20probability.%20This%0Aallows%20for%20a%20straightforward%20identification%20of%20the%20sources%20of%20power-law%20scaling%0Ain%20model%20performance.%20We%20compute%20the%20generalization%20error%20of%20a%20broad%20class%20of%0Arandom%20feature%20models.%20We%20find%20that%20in%20all%20models%2C%20the%20%24S%24-transform%0Acorresponds%20to%20the%20train-test%20generalization%20gap%2C%20and%20yields%20an%20analogue%20of%20the%0Ageneralized-cross-validation%20estimator.%20Using%20these%20techniques%2C%20we%20derive%0Afine-grained%20bias-variance%20decompositions%20for%20a%20very%20general%20class%20of%20random%0Afeature%20models%20with%20structured%20covariates.%20These%20novel%20results%20allow%20us%20to%0Adiscover%20a%20scaling%20regime%20for%20random%20feature%20models%20where%20the%20variance%20due%20to%0Athe%20features%20limits%20performance%20in%20the%20overparameterized%20setting.%20We%20also%0Ademonstrate%20how%20anisotropic%20weight%20structure%20in%20random%20feature%20models%20can%20limit%0Aperformance%20and%20lead%20to%20nontrivial%20exponents%20for%20finite-width%20corrections%20in%0Athe%20overparameterized%20setting.%20Our%20results%20extend%20and%20provide%20a%20unifying%0Aperspective%20on%20earlier%20models%20of%20neural%20scaling%20laws.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00592v3&entry.124074799=Read"},
{"title": "Invertible Consistency Distillation for Text-Guided Image Editing in\n  Around 7 Steps", "author": "Nikita Starodubcev and Mikhail Khoroshikh and Artem Babenko and Dmitry Baranchuk", "abstract": "  Diffusion distillation represents a highly promising direction for achieving\nfaithful text-to-image generation in a few sampling steps. However, despite\nrecent successes, existing distilled models still do not provide the full\nspectrum of diffusion abilities, such as real image inversion, which enables\nmany precise image manipulation methods. This work aims to enrich distilled\ntext-to-image diffusion models with the ability to effectively encode real\nimages into their latent space. To this end, we introduce invertible\nConsistency Distillation (iCD), a generalized consistency distillation\nframework that facilitates both high-quality image synthesis and accurate image\nencoding in only 3-4 inference steps. Though the inversion problem for\ntext-to-image diffusion models gets exacerbated by high classifier-free\nguidance scales, we notice that dynamic guidance significantly reduces\nreconstruction errors without noticeable degradation in generation performance.\nAs a result, we demonstrate that iCD equipped with dynamic guidance may serve\nas a highly effective tool for zero-shot text-guided image editing, competing\nwith more expensive state-of-the-art alternatives.\n", "link": "http://arxiv.org/abs/2406.14539v2", "date": "2024-06-26", "relevancy": 1.8629, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6524}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6189}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invertible%20Consistency%20Distillation%20for%20Text-Guided%20Image%20Editing%20in%0A%20%20Around%207%20Steps&body=Title%3A%20Invertible%20Consistency%20Distillation%20for%20Text-Guided%20Image%20Editing%20in%0A%20%20Around%207%20Steps%0AAuthor%3A%20Nikita%20Starodubcev%20and%20Mikhail%20Khoroshikh%20and%20Artem%20Babenko%20and%20Dmitry%20Baranchuk%0AAbstract%3A%20%20%20Diffusion%20distillation%20represents%20a%20highly%20promising%20direction%20for%20achieving%0Afaithful%20text-to-image%20generation%20in%20a%20few%20sampling%20steps.%20However%2C%20despite%0Arecent%20successes%2C%20existing%20distilled%20models%20still%20do%20not%20provide%20the%20full%0Aspectrum%20of%20diffusion%20abilities%2C%20such%20as%20real%20image%20inversion%2C%20which%20enables%0Amany%20precise%20image%20manipulation%20methods.%20This%20work%20aims%20to%20enrich%20distilled%0Atext-to-image%20diffusion%20models%20with%20the%20ability%20to%20effectively%20encode%20real%0Aimages%20into%20their%20latent%20space.%20To%20this%20end%2C%20we%20introduce%20invertible%0AConsistency%20Distillation%20%28iCD%29%2C%20a%20generalized%20consistency%20distillation%0Aframework%20that%20facilitates%20both%20high-quality%20image%20synthesis%20and%20accurate%20image%0Aencoding%20in%20only%203-4%20inference%20steps.%20Though%20the%20inversion%20problem%20for%0Atext-to-image%20diffusion%20models%20gets%20exacerbated%20by%20high%20classifier-free%0Aguidance%20scales%2C%20we%20notice%20that%20dynamic%20guidance%20significantly%20reduces%0Areconstruction%20errors%20without%20noticeable%20degradation%20in%20generation%20performance.%0AAs%20a%20result%2C%20we%20demonstrate%20that%20iCD%20equipped%20with%20dynamic%20guidance%20may%20serve%0Aas%20a%20highly%20effective%20tool%20for%20zero-shot%20text-guided%20image%20editing%2C%20competing%0Awith%20more%20expensive%20state-of-the-art%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvertible%2520Consistency%2520Distillation%2520for%2520Text-Guided%2520Image%2520Editing%2520in%250A%2520%2520Around%25207%2520Steps%26entry.906535625%3DNikita%2520Starodubcev%2520and%2520Mikhail%2520Khoroshikh%2520and%2520Artem%2520Babenko%2520and%2520Dmitry%2520Baranchuk%26entry.1292438233%3D%2520%2520Diffusion%2520distillation%2520represents%2520a%2520highly%2520promising%2520direction%2520for%2520achieving%250Afaithful%2520text-to-image%2520generation%2520in%2520a%2520few%2520sampling%2520steps.%2520However%252C%2520despite%250Arecent%2520successes%252C%2520existing%2520distilled%2520models%2520still%2520do%2520not%2520provide%2520the%2520full%250Aspectrum%2520of%2520diffusion%2520abilities%252C%2520such%2520as%2520real%2520image%2520inversion%252C%2520which%2520enables%250Amany%2520precise%2520image%2520manipulation%2520methods.%2520This%2520work%2520aims%2520to%2520enrich%2520distilled%250Atext-to-image%2520diffusion%2520models%2520with%2520the%2520ability%2520to%2520effectively%2520encode%2520real%250Aimages%2520into%2520their%2520latent%2520space.%2520To%2520this%2520end%252C%2520we%2520introduce%2520invertible%250AConsistency%2520Distillation%2520%2528iCD%2529%252C%2520a%2520generalized%2520consistency%2520distillation%250Aframework%2520that%2520facilitates%2520both%2520high-quality%2520image%2520synthesis%2520and%2520accurate%2520image%250Aencoding%2520in%2520only%25203-4%2520inference%2520steps.%2520Though%2520the%2520inversion%2520problem%2520for%250Atext-to-image%2520diffusion%2520models%2520gets%2520exacerbated%2520by%2520high%2520classifier-free%250Aguidance%2520scales%252C%2520we%2520notice%2520that%2520dynamic%2520guidance%2520significantly%2520reduces%250Areconstruction%2520errors%2520without%2520noticeable%2520degradation%2520in%2520generation%2520performance.%250AAs%2520a%2520result%252C%2520we%2520demonstrate%2520that%2520iCD%2520equipped%2520with%2520dynamic%2520guidance%2520may%2520serve%250Aas%2520a%2520highly%2520effective%2520tool%2520for%2520zero-shot%2520text-guided%2520image%2520editing%252C%2520competing%250Awith%2520more%2520expensive%2520state-of-the-art%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invertible%20Consistency%20Distillation%20for%20Text-Guided%20Image%20Editing%20in%0A%20%20Around%207%20Steps&entry.906535625=Nikita%20Starodubcev%20and%20Mikhail%20Khoroshikh%20and%20Artem%20Babenko%20and%20Dmitry%20Baranchuk&entry.1292438233=%20%20Diffusion%20distillation%20represents%20a%20highly%20promising%20direction%20for%20achieving%0Afaithful%20text-to-image%20generation%20in%20a%20few%20sampling%20steps.%20However%2C%20despite%0Arecent%20successes%2C%20existing%20distilled%20models%20still%20do%20not%20provide%20the%20full%0Aspectrum%20of%20diffusion%20abilities%2C%20such%20as%20real%20image%20inversion%2C%20which%20enables%0Amany%20precise%20image%20manipulation%20methods.%20This%20work%20aims%20to%20enrich%20distilled%0Atext-to-image%20diffusion%20models%20with%20the%20ability%20to%20effectively%20encode%20real%0Aimages%20into%20their%20latent%20space.%20To%20this%20end%2C%20we%20introduce%20invertible%0AConsistency%20Distillation%20%28iCD%29%2C%20a%20generalized%20consistency%20distillation%0Aframework%20that%20facilitates%20both%20high-quality%20image%20synthesis%20and%20accurate%20image%0Aencoding%20in%20only%203-4%20inference%20steps.%20Though%20the%20inversion%20problem%20for%0Atext-to-image%20diffusion%20models%20gets%20exacerbated%20by%20high%20classifier-free%0Aguidance%20scales%2C%20we%20notice%20that%20dynamic%20guidance%20significantly%20reduces%0Areconstruction%20errors%20without%20noticeable%20degradation%20in%20generation%20performance.%0AAs%20a%20result%2C%20we%20demonstrate%20that%20iCD%20equipped%20with%20dynamic%20guidance%20may%20serve%0Aas%20a%20highly%20effective%20tool%20for%20zero-shot%20text-guided%20image%20editing%2C%20competing%0Awith%20more%20expensive%20state-of-the-art%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14539v2&entry.124074799=Read"},
{"title": "PaCoST: Paired Confidence Significance Testing for Benchmark\n  Contamination Detection in Large Language Models", "author": "Huixuan Zhang and Yun Lin and Xiaojun Wan", "abstract": "  Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods.\n", "link": "http://arxiv.org/abs/2406.18326v1", "date": "2024-06-26", "relevancy": 1.8578, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4786}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4629}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaCoST%3A%20Paired%20Confidence%20Significance%20Testing%20for%20Benchmark%0A%20%20Contamination%20Detection%20in%20Large%20Language%20Models&body=Title%3A%20PaCoST%3A%20Paired%20Confidence%20Significance%20Testing%20for%20Benchmark%0A%20%20Contamination%20Detection%20in%20Large%20Language%20Models%0AAuthor%3A%20Huixuan%20Zhang%20and%20Yun%20Lin%20and%20Xiaojun%20Wan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20known%20to%20be%20trained%20on%20vast%20amounts%20of%20data%2C%0Awhich%20may%20unintentionally%20or%20intentionally%20include%20data%20from%20commonly%20used%0Abenchmarks.%20This%20inclusion%20can%20lead%20to%20cheatingly%20high%20scores%20on%20model%0Aleaderboards%2C%20yet%20result%20in%20disappointing%20performance%20in%20real-world%0Aapplications.%20To%20address%20this%20benchmark%20contamination%20problem%2C%20we%20first%20propose%0Aa%20set%20of%20requirements%20that%20practical%20contamination%20detection%20methods%20should%0Afollow.%20Following%20these%20proposed%20requirements%2C%20we%20introduce%20PaCoST%2C%20a%20Paired%0AConfidence%20Significance%20Testing%20to%20effectively%20detect%20benchmark%20contamination%0Ain%20LLMs.%20Our%20method%20constructs%20a%20counterpart%20for%20each%20piece%20of%20data%20with%20the%0Asame%20distribution%2C%20and%20performs%20statistical%20analysis%20of%20the%20corresponding%0Aconfidence%20to%20test%20whether%20the%20model%20is%20significantly%20more%20confident%20under%20the%0Aoriginal%20benchmark.%20We%20validate%20the%20effectiveness%20of%20PaCoST%20and%20apply%20it%20on%0Apopular%20open-source%20models%20and%20benchmarks.%20We%20find%20that%20almost%20all%20models%20and%0Abenchmarks%20we%20tested%20are%20suspected%20contaminated%20more%20or%20less.%20We%20finally%20call%0Afor%20new%20LLM%20evaluation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaCoST%253A%2520Paired%2520Confidence%2520Significance%2520Testing%2520for%2520Benchmark%250A%2520%2520Contamination%2520Detection%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DHuixuan%2520Zhang%2520and%2520Yun%2520Lin%2520and%2520Xiaojun%2520Wan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520known%2520to%2520be%2520trained%2520on%2520vast%2520amounts%2520of%2520data%252C%250Awhich%2520may%2520unintentionally%2520or%2520intentionally%2520include%2520data%2520from%2520commonly%2520used%250Abenchmarks.%2520This%2520inclusion%2520can%2520lead%2520to%2520cheatingly%2520high%2520scores%2520on%2520model%250Aleaderboards%252C%2520yet%2520result%2520in%2520disappointing%2520performance%2520in%2520real-world%250Aapplications.%2520To%2520address%2520this%2520benchmark%2520contamination%2520problem%252C%2520we%2520first%2520propose%250Aa%2520set%2520of%2520requirements%2520that%2520practical%2520contamination%2520detection%2520methods%2520should%250Afollow.%2520Following%2520these%2520proposed%2520requirements%252C%2520we%2520introduce%2520PaCoST%252C%2520a%2520Paired%250AConfidence%2520Significance%2520Testing%2520to%2520effectively%2520detect%2520benchmark%2520contamination%250Ain%2520LLMs.%2520Our%2520method%2520constructs%2520a%2520counterpart%2520for%2520each%2520piece%2520of%2520data%2520with%2520the%250Asame%2520distribution%252C%2520and%2520performs%2520statistical%2520analysis%2520of%2520the%2520corresponding%250Aconfidence%2520to%2520test%2520whether%2520the%2520model%2520is%2520significantly%2520more%2520confident%2520under%2520the%250Aoriginal%2520benchmark.%2520We%2520validate%2520the%2520effectiveness%2520of%2520PaCoST%2520and%2520apply%2520it%2520on%250Apopular%2520open-source%2520models%2520and%2520benchmarks.%2520We%2520find%2520that%2520almost%2520all%2520models%2520and%250Abenchmarks%2520we%2520tested%2520are%2520suspected%2520contaminated%2520more%2520or%2520less.%2520We%2520finally%2520call%250Afor%2520new%2520LLM%2520evaluation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaCoST%3A%20Paired%20Confidence%20Significance%20Testing%20for%20Benchmark%0A%20%20Contamination%20Detection%20in%20Large%20Language%20Models&entry.906535625=Huixuan%20Zhang%20and%20Yun%20Lin%20and%20Xiaojun%20Wan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20known%20to%20be%20trained%20on%20vast%20amounts%20of%20data%2C%0Awhich%20may%20unintentionally%20or%20intentionally%20include%20data%20from%20commonly%20used%0Abenchmarks.%20This%20inclusion%20can%20lead%20to%20cheatingly%20high%20scores%20on%20model%0Aleaderboards%2C%20yet%20result%20in%20disappointing%20performance%20in%20real-world%0Aapplications.%20To%20address%20this%20benchmark%20contamination%20problem%2C%20we%20first%20propose%0Aa%20set%20of%20requirements%20that%20practical%20contamination%20detection%20methods%20should%0Afollow.%20Following%20these%20proposed%20requirements%2C%20we%20introduce%20PaCoST%2C%20a%20Paired%0AConfidence%20Significance%20Testing%20to%20effectively%20detect%20benchmark%20contamination%0Ain%20LLMs.%20Our%20method%20constructs%20a%20counterpart%20for%20each%20piece%20of%20data%20with%20the%0Asame%20distribution%2C%20and%20performs%20statistical%20analysis%20of%20the%20corresponding%0Aconfidence%20to%20test%20whether%20the%20model%20is%20significantly%20more%20confident%20under%20the%0Aoriginal%20benchmark.%20We%20validate%20the%20effectiveness%20of%20PaCoST%20and%20apply%20it%20on%0Apopular%20open-source%20models%20and%20benchmarks.%20We%20find%20that%20almost%20all%20models%20and%0Abenchmarks%20we%20tested%20are%20suspected%20contaminated%20more%20or%20less.%20We%20finally%20call%0Afor%20new%20LLM%20evaluation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18326v1&entry.124074799=Read"},
{"title": "Robotic Exploration through Semantic Topometric Mapping", "author": "Scott Fredriksson and Akshit Saradagi and George Nikolakopoulos", "abstract": "  In this article, we introduce a novel strategy for robotic exploration in\nunknown environments using a semantic topometric map. As it will be presented,\nthe semantic topometric map is generated by segmenting the grid map of the\ncurrently explored parts of the environment into regions, such as\nintersections, pathways, dead-ends, and unexplored frontiers, which constitute\nthe structural semantics of an environment. The proposed exploration strategy\nleverages metric information of the frontier, such as distance and angle to the\nfrontier, similar to existing frameworks, with the key difference being the\nadditional utilization of structural semantic information, such as properties\nof the intersections leading to frontiers. The algorithm for generating\nsemantic topometric mapping utilized by the proposed method is lightweight,\nresulting in the method's online execution being both rapid and computationally\nefficient. Moreover, the proposed framework can be applied to both structured\nand unstructured indoor and outdoor environments, which enhances the\nversatility of the proposed exploration algorithm. We validate our exploration\nstrategy and demonstrate the utility of structural semantics in exploration in\ntwo complex indoor environments by utilizing a Turtlebot3 as the robotic agent.\nCompared to traditional frontier-based methods, our findings indicate that the\nproposed approach leads to faster exploration and requires less computation\ntime.\n", "link": "http://arxiv.org/abs/2406.18381v1", "date": "2024-06-26", "relevancy": 1.8471, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6871}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6424}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20Exploration%20through%20Semantic%20Topometric%20Mapping&body=Title%3A%20Robotic%20Exploration%20through%20Semantic%20Topometric%20Mapping%0AAuthor%3A%20Scott%20Fredriksson%20and%20Akshit%20Saradagi%20and%20George%20Nikolakopoulos%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20introduce%20a%20novel%20strategy%20for%20robotic%20exploration%20in%0Aunknown%20environments%20using%20a%20semantic%20topometric%20map.%20As%20it%20will%20be%20presented%2C%0Athe%20semantic%20topometric%20map%20is%20generated%20by%20segmenting%20the%20grid%20map%20of%20the%0Acurrently%20explored%20parts%20of%20the%20environment%20into%20regions%2C%20such%20as%0Aintersections%2C%20pathways%2C%20dead-ends%2C%20and%20unexplored%20frontiers%2C%20which%20constitute%0Athe%20structural%20semantics%20of%20an%20environment.%20The%20proposed%20exploration%20strategy%0Aleverages%20metric%20information%20of%20the%20frontier%2C%20such%20as%20distance%20and%20angle%20to%20the%0Afrontier%2C%20similar%20to%20existing%20frameworks%2C%20with%20the%20key%20difference%20being%20the%0Aadditional%20utilization%20of%20structural%20semantic%20information%2C%20such%20as%20properties%0Aof%20the%20intersections%20leading%20to%20frontiers.%20The%20algorithm%20for%20generating%0Asemantic%20topometric%20mapping%20utilized%20by%20the%20proposed%20method%20is%20lightweight%2C%0Aresulting%20in%20the%20method%27s%20online%20execution%20being%20both%20rapid%20and%20computationally%0Aefficient.%20Moreover%2C%20the%20proposed%20framework%20can%20be%20applied%20to%20both%20structured%0Aand%20unstructured%20indoor%20and%20outdoor%20environments%2C%20which%20enhances%20the%0Aversatility%20of%20the%20proposed%20exploration%20algorithm.%20We%20validate%20our%20exploration%0Astrategy%20and%20demonstrate%20the%20utility%20of%20structural%20semantics%20in%20exploration%20in%0Atwo%20complex%20indoor%20environments%20by%20utilizing%20a%20Turtlebot3%20as%20the%20robotic%20agent.%0ACompared%20to%20traditional%20frontier-based%20methods%2C%20our%20findings%20indicate%20that%20the%0Aproposed%20approach%20leads%20to%20faster%20exploration%20and%20requires%20less%20computation%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520Exploration%2520through%2520Semantic%2520Topometric%2520Mapping%26entry.906535625%3DScott%2520Fredriksson%2520and%2520Akshit%2520Saradagi%2520and%2520George%2520Nikolakopoulos%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520introduce%2520a%2520novel%2520strategy%2520for%2520robotic%2520exploration%2520in%250Aunknown%2520environments%2520using%2520a%2520semantic%2520topometric%2520map.%2520As%2520it%2520will%2520be%2520presented%252C%250Athe%2520semantic%2520topometric%2520map%2520is%2520generated%2520by%2520segmenting%2520the%2520grid%2520map%2520of%2520the%250Acurrently%2520explored%2520parts%2520of%2520the%2520environment%2520into%2520regions%252C%2520such%2520as%250Aintersections%252C%2520pathways%252C%2520dead-ends%252C%2520and%2520unexplored%2520frontiers%252C%2520which%2520constitute%250Athe%2520structural%2520semantics%2520of%2520an%2520environment.%2520The%2520proposed%2520exploration%2520strategy%250Aleverages%2520metric%2520information%2520of%2520the%2520frontier%252C%2520such%2520as%2520distance%2520and%2520angle%2520to%2520the%250Afrontier%252C%2520similar%2520to%2520existing%2520frameworks%252C%2520with%2520the%2520key%2520difference%2520being%2520the%250Aadditional%2520utilization%2520of%2520structural%2520semantic%2520information%252C%2520such%2520as%2520properties%250Aof%2520the%2520intersections%2520leading%2520to%2520frontiers.%2520The%2520algorithm%2520for%2520generating%250Asemantic%2520topometric%2520mapping%2520utilized%2520by%2520the%2520proposed%2520method%2520is%2520lightweight%252C%250Aresulting%2520in%2520the%2520method%2527s%2520online%2520execution%2520being%2520both%2520rapid%2520and%2520computationally%250Aefficient.%2520Moreover%252C%2520the%2520proposed%2520framework%2520can%2520be%2520applied%2520to%2520both%2520structured%250Aand%2520unstructured%2520indoor%2520and%2520outdoor%2520environments%252C%2520which%2520enhances%2520the%250Aversatility%2520of%2520the%2520proposed%2520exploration%2520algorithm.%2520We%2520validate%2520our%2520exploration%250Astrategy%2520and%2520demonstrate%2520the%2520utility%2520of%2520structural%2520semantics%2520in%2520exploration%2520in%250Atwo%2520complex%2520indoor%2520environments%2520by%2520utilizing%2520a%2520Turtlebot3%2520as%2520the%2520robotic%2520agent.%250ACompared%2520to%2520traditional%2520frontier-based%2520methods%252C%2520our%2520findings%2520indicate%2520that%2520the%250Aproposed%2520approach%2520leads%2520to%2520faster%2520exploration%2520and%2520requires%2520less%2520computation%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20Exploration%20through%20Semantic%20Topometric%20Mapping&entry.906535625=Scott%20Fredriksson%20and%20Akshit%20Saradagi%20and%20George%20Nikolakopoulos&entry.1292438233=%20%20In%20this%20article%2C%20we%20introduce%20a%20novel%20strategy%20for%20robotic%20exploration%20in%0Aunknown%20environments%20using%20a%20semantic%20topometric%20map.%20As%20it%20will%20be%20presented%2C%0Athe%20semantic%20topometric%20map%20is%20generated%20by%20segmenting%20the%20grid%20map%20of%20the%0Acurrently%20explored%20parts%20of%20the%20environment%20into%20regions%2C%20such%20as%0Aintersections%2C%20pathways%2C%20dead-ends%2C%20and%20unexplored%20frontiers%2C%20which%20constitute%0Athe%20structural%20semantics%20of%20an%20environment.%20The%20proposed%20exploration%20strategy%0Aleverages%20metric%20information%20of%20the%20frontier%2C%20such%20as%20distance%20and%20angle%20to%20the%0Afrontier%2C%20similar%20to%20existing%20frameworks%2C%20with%20the%20key%20difference%20being%20the%0Aadditional%20utilization%20of%20structural%20semantic%20information%2C%20such%20as%20properties%0Aof%20the%20intersections%20leading%20to%20frontiers.%20The%20algorithm%20for%20generating%0Asemantic%20topometric%20mapping%20utilized%20by%20the%20proposed%20method%20is%20lightweight%2C%0Aresulting%20in%20the%20method%27s%20online%20execution%20being%20both%20rapid%20and%20computationally%0Aefficient.%20Moreover%2C%20the%20proposed%20framework%20can%20be%20applied%20to%20both%20structured%0Aand%20unstructured%20indoor%20and%20outdoor%20environments%2C%20which%20enhances%20the%0Aversatility%20of%20the%20proposed%20exploration%20algorithm.%20We%20validate%20our%20exploration%0Astrategy%20and%20demonstrate%20the%20utility%20of%20structural%20semantics%20in%20exploration%20in%0Atwo%20complex%20indoor%20environments%20by%20utilizing%20a%20Turtlebot3%20as%20the%20robotic%20agent.%0ACompared%20to%20traditional%20frontier-based%20methods%2C%20our%20findings%20indicate%20that%20the%0Aproposed%20approach%20leads%20to%20faster%20exploration%20and%20requires%20less%20computation%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18381v1&entry.124074799=Read"},
{"title": "Autoencoder-based Anomaly Detection System for Online Data Quality\n  Monitoring of the CMS Electromagnetic Calorimeter", "author": " The CMS ECAL Collaboration", "abstract": "  The CMS detector is a general-purpose apparatus that detects high-energy\ncollisions produced at the LHC. Online Data Quality Monitoring of the CMS\nelectromagnetic calorimeter is a vital operational tool that allows detector\nexperts to quickly identify, localize, and diagnose a broad range of detector\nissues that could affect the quality of physics data. A real-time\nautoencoder-based anomaly detection system using semi-supervised machine\nlearning is presented enabling the detection of anomalies in the CMS\nelectromagnetic calorimeter data. A novel method is introduced which maximizes\nthe anomaly detection performance by exploiting the time-dependent evolution of\nanomalies as well as spatial variations in the detector response. The\nautoencoder-based system is able to efficiently detect anomalies, while\nmaintaining a very low false discovery rate. The performance of the system is\nvalidated with anomalies found in 2018 and 2022 LHC collision data.\nAdditionally, the first results from deploying the autoencoder-based system in\nthe CMS online Data Quality Monitoring workflow during the beginning of Run 3\nof the LHC are presented, showing its ability to detect issues missed by the\nexisting system.\n", "link": "http://arxiv.org/abs/2309.10157v2", "date": "2024-06-26", "relevancy": 1.8412, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5085}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4549}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoencoder-based%20Anomaly%20Detection%20System%20for%20Online%20Data%20Quality%0A%20%20Monitoring%20of%20the%20CMS%20Electromagnetic%20Calorimeter&body=Title%3A%20Autoencoder-based%20Anomaly%20Detection%20System%20for%20Online%20Data%20Quality%0A%20%20Monitoring%20of%20the%20CMS%20Electromagnetic%20Calorimeter%0AAuthor%3A%20%20The%20CMS%20ECAL%20Collaboration%0AAbstract%3A%20%20%20The%20CMS%20detector%20is%20a%20general-purpose%20apparatus%20that%20detects%20high-energy%0Acollisions%20produced%20at%20the%20LHC.%20Online%20Data%20Quality%20Monitoring%20of%20the%20CMS%0Aelectromagnetic%20calorimeter%20is%20a%20vital%20operational%20tool%20that%20allows%20detector%0Aexperts%20to%20quickly%20identify%2C%20localize%2C%20and%20diagnose%20a%20broad%20range%20of%20detector%0Aissues%20that%20could%20affect%20the%20quality%20of%20physics%20data.%20A%20real-time%0Aautoencoder-based%20anomaly%20detection%20system%20using%20semi-supervised%20machine%0Alearning%20is%20presented%20enabling%20the%20detection%20of%20anomalies%20in%20the%20CMS%0Aelectromagnetic%20calorimeter%20data.%20A%20novel%20method%20is%20introduced%20which%20maximizes%0Athe%20anomaly%20detection%20performance%20by%20exploiting%20the%20time-dependent%20evolution%20of%0Aanomalies%20as%20well%20as%20spatial%20variations%20in%20the%20detector%20response.%20The%0Aautoencoder-based%20system%20is%20able%20to%20efficiently%20detect%20anomalies%2C%20while%0Amaintaining%20a%20very%20low%20false%20discovery%20rate.%20The%20performance%20of%20the%20system%20is%0Avalidated%20with%20anomalies%20found%20in%202018%20and%202022%20LHC%20collision%20data.%0AAdditionally%2C%20the%20first%20results%20from%20deploying%20the%20autoencoder-based%20system%20in%0Athe%20CMS%20online%20Data%20Quality%20Monitoring%20workflow%20during%20the%20beginning%20of%20Run%203%0Aof%20the%20LHC%20are%20presented%2C%20showing%20its%20ability%20to%20detect%20issues%20missed%20by%20the%0Aexisting%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoencoder-based%2520Anomaly%2520Detection%2520System%2520for%2520Online%2520Data%2520Quality%250A%2520%2520Monitoring%2520of%2520the%2520CMS%2520Electromagnetic%2520Calorimeter%26entry.906535625%3D%2520The%2520CMS%2520ECAL%2520Collaboration%26entry.1292438233%3D%2520%2520The%2520CMS%2520detector%2520is%2520a%2520general-purpose%2520apparatus%2520that%2520detects%2520high-energy%250Acollisions%2520produced%2520at%2520the%2520LHC.%2520Online%2520Data%2520Quality%2520Monitoring%2520of%2520the%2520CMS%250Aelectromagnetic%2520calorimeter%2520is%2520a%2520vital%2520operational%2520tool%2520that%2520allows%2520detector%250Aexperts%2520to%2520quickly%2520identify%252C%2520localize%252C%2520and%2520diagnose%2520a%2520broad%2520range%2520of%2520detector%250Aissues%2520that%2520could%2520affect%2520the%2520quality%2520of%2520physics%2520data.%2520A%2520real-time%250Aautoencoder-based%2520anomaly%2520detection%2520system%2520using%2520semi-supervised%2520machine%250Alearning%2520is%2520presented%2520enabling%2520the%2520detection%2520of%2520anomalies%2520in%2520the%2520CMS%250Aelectromagnetic%2520calorimeter%2520data.%2520A%2520novel%2520method%2520is%2520introduced%2520which%2520maximizes%250Athe%2520anomaly%2520detection%2520performance%2520by%2520exploiting%2520the%2520time-dependent%2520evolution%2520of%250Aanomalies%2520as%2520well%2520as%2520spatial%2520variations%2520in%2520the%2520detector%2520response.%2520The%250Aautoencoder-based%2520system%2520is%2520able%2520to%2520efficiently%2520detect%2520anomalies%252C%2520while%250Amaintaining%2520a%2520very%2520low%2520false%2520discovery%2520rate.%2520The%2520performance%2520of%2520the%2520system%2520is%250Avalidated%2520with%2520anomalies%2520found%2520in%25202018%2520and%25202022%2520LHC%2520collision%2520data.%250AAdditionally%252C%2520the%2520first%2520results%2520from%2520deploying%2520the%2520autoencoder-based%2520system%2520in%250Athe%2520CMS%2520online%2520Data%2520Quality%2520Monitoring%2520workflow%2520during%2520the%2520beginning%2520of%2520Run%25203%250Aof%2520the%2520LHC%2520are%2520presented%252C%2520showing%2520its%2520ability%2520to%2520detect%2520issues%2520missed%2520by%2520the%250Aexisting%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoencoder-based%20Anomaly%20Detection%20System%20for%20Online%20Data%20Quality%0A%20%20Monitoring%20of%20the%20CMS%20Electromagnetic%20Calorimeter&entry.906535625=%20The%20CMS%20ECAL%20Collaboration&entry.1292438233=%20%20The%20CMS%20detector%20is%20a%20general-purpose%20apparatus%20that%20detects%20high-energy%0Acollisions%20produced%20at%20the%20LHC.%20Online%20Data%20Quality%20Monitoring%20of%20the%20CMS%0Aelectromagnetic%20calorimeter%20is%20a%20vital%20operational%20tool%20that%20allows%20detector%0Aexperts%20to%20quickly%20identify%2C%20localize%2C%20and%20diagnose%20a%20broad%20range%20of%20detector%0Aissues%20that%20could%20affect%20the%20quality%20of%20physics%20data.%20A%20real-time%0Aautoencoder-based%20anomaly%20detection%20system%20using%20semi-supervised%20machine%0Alearning%20is%20presented%20enabling%20the%20detection%20of%20anomalies%20in%20the%20CMS%0Aelectromagnetic%20calorimeter%20data.%20A%20novel%20method%20is%20introduced%20which%20maximizes%0Athe%20anomaly%20detection%20performance%20by%20exploiting%20the%20time-dependent%20evolution%20of%0Aanomalies%20as%20well%20as%20spatial%20variations%20in%20the%20detector%20response.%20The%0Aautoencoder-based%20system%20is%20able%20to%20efficiently%20detect%20anomalies%2C%20while%0Amaintaining%20a%20very%20low%20false%20discovery%20rate.%20The%20performance%20of%20the%20system%20is%0Avalidated%20with%20anomalies%20found%20in%202018%20and%202022%20LHC%20collision%20data.%0AAdditionally%2C%20the%20first%20results%20from%20deploying%20the%20autoencoder-based%20system%20in%0Athe%20CMS%20online%20Data%20Quality%20Monitoring%20workflow%20during%20the%20beginning%20of%20Run%203%0Aof%20the%20LHC%20are%20presented%2C%20showing%20its%20ability%20to%20detect%20issues%20missed%20by%20the%0Aexisting%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10157v2&entry.124074799=Read"},
{"title": "Large Language Model Enhanced Clustering for News Event Detection", "author": "Adane Nega Tarekegn", "abstract": "  The news landscape is continuously evolving, with an ever-increasing volume\nof information from around the world. Automated event detection within this\nvast data repository is essential for monitoring, identifying, and categorizing\nsignificant news occurrences across diverse platforms. This paper presents an\nevent detection framework that leverages Large Language Models (LLMs) combined\nwith clustering analysis to detect news events from the Global Database of\nEvents, Language, and Tone (GDELT). The framework enhances event clustering\nthrough both pre-event detection tasks (keyword extraction and text embedding)\nand post-event detection tasks (event summarization and topic labeling). We\nalso evaluate the impact of various textual embeddings on the quality of\nclustering outcomes, ensuring robust news categorization. Additionally, we\nintroduce a novel Cluster Stability Assessment Index (CSAI) to assess the\nvalidity and robustness of clustering results. CSAI utilizes latent feature\nvectors to provide a new way of measuring clustering quality. Our experiments\nindicate that combining LLM embeddings with clustering algorithms yields the\nbest results, demonstrating greater robustness in terms of CSAI scores.\nMoreover, post-event detection tasks generate meaningful insights, facilitating\neffective interpretation of event clustering results. Overall, our experimental\nresults indicate that the proposed framework offers valuable insights and could\nenhance the accuracy and depth of news reporting.\n", "link": "http://arxiv.org/abs/2406.10552v2", "date": "2024-06-26", "relevancy": 1.8286, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4863}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4533}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Enhanced%20Clustering%20for%20News%20Event%20Detection&body=Title%3A%20Large%20Language%20Model%20Enhanced%20Clustering%20for%20News%20Event%20Detection%0AAuthor%3A%20Adane%20Nega%20Tarekegn%0AAbstract%3A%20%20%20The%20news%20landscape%20is%20continuously%20evolving%2C%20with%20an%20ever-increasing%20volume%0Aof%20information%20from%20around%20the%20world.%20Automated%20event%20detection%20within%20this%0Avast%20data%20repository%20is%20essential%20for%20monitoring%2C%20identifying%2C%20and%20categorizing%0Asignificant%20news%20occurrences%20across%20diverse%20platforms.%20This%20paper%20presents%20an%0Aevent%20detection%20framework%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20combined%0Awith%20clustering%20analysis%20to%20detect%20news%20events%20from%20the%20Global%20Database%20of%0AEvents%2C%20Language%2C%20and%20Tone%20%28GDELT%29.%20The%20framework%20enhances%20event%20clustering%0Athrough%20both%20pre-event%20detection%20tasks%20%28keyword%20extraction%20and%20text%20embedding%29%0Aand%20post-event%20detection%20tasks%20%28event%20summarization%20and%20topic%20labeling%29.%20We%0Aalso%20evaluate%20the%20impact%20of%20various%20textual%20embeddings%20on%20the%20quality%20of%0Aclustering%20outcomes%2C%20ensuring%20robust%20news%20categorization.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20Cluster%20Stability%20Assessment%20Index%20%28CSAI%29%20to%20assess%20the%0Avalidity%20and%20robustness%20of%20clustering%20results.%20CSAI%20utilizes%20latent%20feature%0Avectors%20to%20provide%20a%20new%20way%20of%20measuring%20clustering%20quality.%20Our%20experiments%0Aindicate%20that%20combining%20LLM%20embeddings%20with%20clustering%20algorithms%20yields%20the%0Abest%20results%2C%20demonstrating%20greater%20robustness%20in%20terms%20of%20CSAI%20scores.%0AMoreover%2C%20post-event%20detection%20tasks%20generate%20meaningful%20insights%2C%20facilitating%0Aeffective%20interpretation%20of%20event%20clustering%20results.%20Overall%2C%20our%20experimental%0Aresults%20indicate%20that%20the%20proposed%20framework%20offers%20valuable%20insights%20and%20could%0Aenhance%20the%20accuracy%20and%20depth%20of%20news%20reporting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10552v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520Enhanced%2520Clustering%2520for%2520News%2520Event%2520Detection%26entry.906535625%3DAdane%2520Nega%2520Tarekegn%26entry.1292438233%3D%2520%2520The%2520news%2520landscape%2520is%2520continuously%2520evolving%252C%2520with%2520an%2520ever-increasing%2520volume%250Aof%2520information%2520from%2520around%2520the%2520world.%2520Automated%2520event%2520detection%2520within%2520this%250Avast%2520data%2520repository%2520is%2520essential%2520for%2520monitoring%252C%2520identifying%252C%2520and%2520categorizing%250Asignificant%2520news%2520occurrences%2520across%2520diverse%2520platforms.%2520This%2520paper%2520presents%2520an%250Aevent%2520detection%2520framework%2520that%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520combined%250Awith%2520clustering%2520analysis%2520to%2520detect%2520news%2520events%2520from%2520the%2520Global%2520Database%2520of%250AEvents%252C%2520Language%252C%2520and%2520Tone%2520%2528GDELT%2529.%2520The%2520framework%2520enhances%2520event%2520clustering%250Athrough%2520both%2520pre-event%2520detection%2520tasks%2520%2528keyword%2520extraction%2520and%2520text%2520embedding%2529%250Aand%2520post-event%2520detection%2520tasks%2520%2528event%2520summarization%2520and%2520topic%2520labeling%2529.%2520We%250Aalso%2520evaluate%2520the%2520impact%2520of%2520various%2520textual%2520embeddings%2520on%2520the%2520quality%2520of%250Aclustering%2520outcomes%252C%2520ensuring%2520robust%2520news%2520categorization.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520novel%2520Cluster%2520Stability%2520Assessment%2520Index%2520%2528CSAI%2529%2520to%2520assess%2520the%250Avalidity%2520and%2520robustness%2520of%2520clustering%2520results.%2520CSAI%2520utilizes%2520latent%2520feature%250Avectors%2520to%2520provide%2520a%2520new%2520way%2520of%2520measuring%2520clustering%2520quality.%2520Our%2520experiments%250Aindicate%2520that%2520combining%2520LLM%2520embeddings%2520with%2520clustering%2520algorithms%2520yields%2520the%250Abest%2520results%252C%2520demonstrating%2520greater%2520robustness%2520in%2520terms%2520of%2520CSAI%2520scores.%250AMoreover%252C%2520post-event%2520detection%2520tasks%2520generate%2520meaningful%2520insights%252C%2520facilitating%250Aeffective%2520interpretation%2520of%2520event%2520clustering%2520results.%2520Overall%252C%2520our%2520experimental%250Aresults%2520indicate%2520that%2520the%2520proposed%2520framework%2520offers%2520valuable%2520insights%2520and%2520could%250Aenhance%2520the%2520accuracy%2520and%2520depth%2520of%2520news%2520reporting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10552v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Enhanced%20Clustering%20for%20News%20Event%20Detection&entry.906535625=Adane%20Nega%20Tarekegn&entry.1292438233=%20%20The%20news%20landscape%20is%20continuously%20evolving%2C%20with%20an%20ever-increasing%20volume%0Aof%20information%20from%20around%20the%20world.%20Automated%20event%20detection%20within%20this%0Avast%20data%20repository%20is%20essential%20for%20monitoring%2C%20identifying%2C%20and%20categorizing%0Asignificant%20news%20occurrences%20across%20diverse%20platforms.%20This%20paper%20presents%20an%0Aevent%20detection%20framework%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20combined%0Awith%20clustering%20analysis%20to%20detect%20news%20events%20from%20the%20Global%20Database%20of%0AEvents%2C%20Language%2C%20and%20Tone%20%28GDELT%29.%20The%20framework%20enhances%20event%20clustering%0Athrough%20both%20pre-event%20detection%20tasks%20%28keyword%20extraction%20and%20text%20embedding%29%0Aand%20post-event%20detection%20tasks%20%28event%20summarization%20and%20topic%20labeling%29.%20We%0Aalso%20evaluate%20the%20impact%20of%20various%20textual%20embeddings%20on%20the%20quality%20of%0Aclustering%20outcomes%2C%20ensuring%20robust%20news%20categorization.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20Cluster%20Stability%20Assessment%20Index%20%28CSAI%29%20to%20assess%20the%0Avalidity%20and%20robustness%20of%20clustering%20results.%20CSAI%20utilizes%20latent%20feature%0Avectors%20to%20provide%20a%20new%20way%20of%20measuring%20clustering%20quality.%20Our%20experiments%0Aindicate%20that%20combining%20LLM%20embeddings%20with%20clustering%20algorithms%20yields%20the%0Abest%20results%2C%20demonstrating%20greater%20robustness%20in%20terms%20of%20CSAI%20scores.%0AMoreover%2C%20post-event%20detection%20tasks%20generate%20meaningful%20insights%2C%20facilitating%0Aeffective%20interpretation%20of%20event%20clustering%20results.%20Overall%2C%20our%20experimental%0Aresults%20indicate%20that%20the%20proposed%20framework%20offers%20valuable%20insights%20and%20could%0Aenhance%20the%20accuracy%20and%20depth%20of%20news%20reporting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10552v2&entry.124074799=Read"},
{"title": "Sensorless model-based tension control for a cable-driven exosuit", "author": "Elena Bardi and Adrian Esser and Peter Wolf and Marta Gandolla and Emilia Ambrosini and Alessandra Pedrocchi and Robert Riener", "abstract": "  Cable-driven exosuits have the potential to support individuals with motor\ndisabilities across the continuum of care. When supporting a limb with a cable,\nforce sensors are often used to measure tension. However, force sensors add\ncost, complexity, and distal components. This paper presents a design and\ncontrol approach to remove the force sensor from an upper limb cable-driven\nexosuit. A mechanical design for the exosuit was developed to maximize passive\ntransparency. Then, a data-driven friction identification was conducted on a\nmannequin test bench to design a model-based tension controller. Seventeen\nhealthy participants raised and lowered their right arms to evaluate tension\ntracking, movement quality, and muscular effort. Questionnaires on discomfort,\nphysical exertion, and fatigue were collected. The proposed strategy allowed\ntracking the desired assistive torque with an RMSE of 0.71 Nm (18%) at 50%\ngravity support. During the raising phase, the EMG signals of the anterior\ndeltoid, trapezius, and pectoralis major were reduced on average compared to\nthe no-suit condition by 30%, 38%, and 38%, respectively. The posterior deltoid\nactivity was increased by 32% during lowering. Position tracking was not\nsignificantly altered, whereas movement smoothness significantly decreased.\nThis work demonstrates the feasibility and effectiveness of removing the force\nsensor from a cable-driven exosuit. A significant increase in discomfort in the\nlower neck and right shoulder indicated that the ergonomics of the suit could\nbe improved. Overall this work paves the way towards simpler and more\naffordable exosuits.\n", "link": "http://arxiv.org/abs/2406.18412v1", "date": "2024-06-26", "relevancy": 1.8263, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4594}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sensorless%20model-based%20tension%20control%20for%20a%20cable-driven%20exosuit&body=Title%3A%20Sensorless%20model-based%20tension%20control%20for%20a%20cable-driven%20exosuit%0AAuthor%3A%20Elena%20Bardi%20and%20Adrian%20Esser%20and%20Peter%20Wolf%20and%20Marta%20Gandolla%20and%20Emilia%20Ambrosini%20and%20Alessandra%20Pedrocchi%20and%20Robert%20Riener%0AAbstract%3A%20%20%20Cable-driven%20exosuits%20have%20the%20potential%20to%20support%20individuals%20with%20motor%0Adisabilities%20across%20the%20continuum%20of%20care.%20When%20supporting%20a%20limb%20with%20a%20cable%2C%0Aforce%20sensors%20are%20often%20used%20to%20measure%20tension.%20However%2C%20force%20sensors%20add%0Acost%2C%20complexity%2C%20and%20distal%20components.%20This%20paper%20presents%20a%20design%20and%0Acontrol%20approach%20to%20remove%20the%20force%20sensor%20from%20an%20upper%20limb%20cable-driven%0Aexosuit.%20A%20mechanical%20design%20for%20the%20exosuit%20was%20developed%20to%20maximize%20passive%0Atransparency.%20Then%2C%20a%20data-driven%20friction%20identification%20was%20conducted%20on%20a%0Amannequin%20test%20bench%20to%20design%20a%20model-based%20tension%20controller.%20Seventeen%0Ahealthy%20participants%20raised%20and%20lowered%20their%20right%20arms%20to%20evaluate%20tension%0Atracking%2C%20movement%20quality%2C%20and%20muscular%20effort.%20Questionnaires%20on%20discomfort%2C%0Aphysical%20exertion%2C%20and%20fatigue%20were%20collected.%20The%20proposed%20strategy%20allowed%0Atracking%20the%20desired%20assistive%20torque%20with%20an%20RMSE%20of%200.71%20Nm%20%2818%25%29%20at%2050%25%0Agravity%20support.%20During%20the%20raising%20phase%2C%20the%20EMG%20signals%20of%20the%20anterior%0Adeltoid%2C%20trapezius%2C%20and%20pectoralis%20major%20were%20reduced%20on%20average%20compared%20to%0Athe%20no-suit%20condition%20by%2030%25%2C%2038%25%2C%20and%2038%25%2C%20respectively.%20The%20posterior%20deltoid%0Aactivity%20was%20increased%20by%2032%25%20during%20lowering.%20Position%20tracking%20was%20not%0Asignificantly%20altered%2C%20whereas%20movement%20smoothness%20significantly%20decreased.%0AThis%20work%20demonstrates%20the%20feasibility%20and%20effectiveness%20of%20removing%20the%20force%0Asensor%20from%20a%20cable-driven%20exosuit.%20A%20significant%20increase%20in%20discomfort%20in%20the%0Alower%20neck%20and%20right%20shoulder%20indicated%20that%20the%20ergonomics%20of%20the%20suit%20could%0Abe%20improved.%20Overall%20this%20work%20paves%20the%20way%20towards%20simpler%20and%20more%0Aaffordable%20exosuits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSensorless%2520model-based%2520tension%2520control%2520for%2520a%2520cable-driven%2520exosuit%26entry.906535625%3DElena%2520Bardi%2520and%2520Adrian%2520Esser%2520and%2520Peter%2520Wolf%2520and%2520Marta%2520Gandolla%2520and%2520Emilia%2520Ambrosini%2520and%2520Alessandra%2520Pedrocchi%2520and%2520Robert%2520Riener%26entry.1292438233%3D%2520%2520Cable-driven%2520exosuits%2520have%2520the%2520potential%2520to%2520support%2520individuals%2520with%2520motor%250Adisabilities%2520across%2520the%2520continuum%2520of%2520care.%2520When%2520supporting%2520a%2520limb%2520with%2520a%2520cable%252C%250Aforce%2520sensors%2520are%2520often%2520used%2520to%2520measure%2520tension.%2520However%252C%2520force%2520sensors%2520add%250Acost%252C%2520complexity%252C%2520and%2520distal%2520components.%2520This%2520paper%2520presents%2520a%2520design%2520and%250Acontrol%2520approach%2520to%2520remove%2520the%2520force%2520sensor%2520from%2520an%2520upper%2520limb%2520cable-driven%250Aexosuit.%2520A%2520mechanical%2520design%2520for%2520the%2520exosuit%2520was%2520developed%2520to%2520maximize%2520passive%250Atransparency.%2520Then%252C%2520a%2520data-driven%2520friction%2520identification%2520was%2520conducted%2520on%2520a%250Amannequin%2520test%2520bench%2520to%2520design%2520a%2520model-based%2520tension%2520controller.%2520Seventeen%250Ahealthy%2520participants%2520raised%2520and%2520lowered%2520their%2520right%2520arms%2520to%2520evaluate%2520tension%250Atracking%252C%2520movement%2520quality%252C%2520and%2520muscular%2520effort.%2520Questionnaires%2520on%2520discomfort%252C%250Aphysical%2520exertion%252C%2520and%2520fatigue%2520were%2520collected.%2520The%2520proposed%2520strategy%2520allowed%250Atracking%2520the%2520desired%2520assistive%2520torque%2520with%2520an%2520RMSE%2520of%25200.71%2520Nm%2520%252818%2525%2529%2520at%252050%2525%250Agravity%2520support.%2520During%2520the%2520raising%2520phase%252C%2520the%2520EMG%2520signals%2520of%2520the%2520anterior%250Adeltoid%252C%2520trapezius%252C%2520and%2520pectoralis%2520major%2520were%2520reduced%2520on%2520average%2520compared%2520to%250Athe%2520no-suit%2520condition%2520by%252030%2525%252C%252038%2525%252C%2520and%252038%2525%252C%2520respectively.%2520The%2520posterior%2520deltoid%250Aactivity%2520was%2520increased%2520by%252032%2525%2520during%2520lowering.%2520Position%2520tracking%2520was%2520not%250Asignificantly%2520altered%252C%2520whereas%2520movement%2520smoothness%2520significantly%2520decreased.%250AThis%2520work%2520demonstrates%2520the%2520feasibility%2520and%2520effectiveness%2520of%2520removing%2520the%2520force%250Asensor%2520from%2520a%2520cable-driven%2520exosuit.%2520A%2520significant%2520increase%2520in%2520discomfort%2520in%2520the%250Alower%2520neck%2520and%2520right%2520shoulder%2520indicated%2520that%2520the%2520ergonomics%2520of%2520the%2520suit%2520could%250Abe%2520improved.%2520Overall%2520this%2520work%2520paves%2520the%2520way%2520towards%2520simpler%2520and%2520more%250Aaffordable%2520exosuits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sensorless%20model-based%20tension%20control%20for%20a%20cable-driven%20exosuit&entry.906535625=Elena%20Bardi%20and%20Adrian%20Esser%20and%20Peter%20Wolf%20and%20Marta%20Gandolla%20and%20Emilia%20Ambrosini%20and%20Alessandra%20Pedrocchi%20and%20Robert%20Riener&entry.1292438233=%20%20Cable-driven%20exosuits%20have%20the%20potential%20to%20support%20individuals%20with%20motor%0Adisabilities%20across%20the%20continuum%20of%20care.%20When%20supporting%20a%20limb%20with%20a%20cable%2C%0Aforce%20sensors%20are%20often%20used%20to%20measure%20tension.%20However%2C%20force%20sensors%20add%0Acost%2C%20complexity%2C%20and%20distal%20components.%20This%20paper%20presents%20a%20design%20and%0Acontrol%20approach%20to%20remove%20the%20force%20sensor%20from%20an%20upper%20limb%20cable-driven%0Aexosuit.%20A%20mechanical%20design%20for%20the%20exosuit%20was%20developed%20to%20maximize%20passive%0Atransparency.%20Then%2C%20a%20data-driven%20friction%20identification%20was%20conducted%20on%20a%0Amannequin%20test%20bench%20to%20design%20a%20model-based%20tension%20controller.%20Seventeen%0Ahealthy%20participants%20raised%20and%20lowered%20their%20right%20arms%20to%20evaluate%20tension%0Atracking%2C%20movement%20quality%2C%20and%20muscular%20effort.%20Questionnaires%20on%20discomfort%2C%0Aphysical%20exertion%2C%20and%20fatigue%20were%20collected.%20The%20proposed%20strategy%20allowed%0Atracking%20the%20desired%20assistive%20torque%20with%20an%20RMSE%20of%200.71%20Nm%20%2818%25%29%20at%2050%25%0Agravity%20support.%20During%20the%20raising%20phase%2C%20the%20EMG%20signals%20of%20the%20anterior%0Adeltoid%2C%20trapezius%2C%20and%20pectoralis%20major%20were%20reduced%20on%20average%20compared%20to%0Athe%20no-suit%20condition%20by%2030%25%2C%2038%25%2C%20and%2038%25%2C%20respectively.%20The%20posterior%20deltoid%0Aactivity%20was%20increased%20by%2032%25%20during%20lowering.%20Position%20tracking%20was%20not%0Asignificantly%20altered%2C%20whereas%20movement%20smoothness%20significantly%20decreased.%0AThis%20work%20demonstrates%20the%20feasibility%20and%20effectiveness%20of%20removing%20the%20force%0Asensor%20from%20a%20cable-driven%20exosuit.%20A%20significant%20increase%20in%20discomfort%20in%20the%0Alower%20neck%20and%20right%20shoulder%20indicated%20that%20the%20ergonomics%20of%20the%20suit%20could%0Abe%20improved.%20Overall%20this%20work%20paves%20the%20way%20towards%20simpler%20and%20more%0Aaffordable%20exosuits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18412v1&entry.124074799=Read"},
{"title": "Enhancing Data Privacy in Large Language Models through Private\n  Association Editing", "author": "Davide Venditti and Elena Sofia Ruzzetti and Giancarlo A. Xompero and Cristina Giannone and Andrea Favalli and Raniero Romagnoli and Fabio Massimo Zanzotto", "abstract": "  Large Language Models (LLMs) are powerful tools with extensive applications,\nbut their tendency to memorize private information raises significant concerns\nas private data leakage can easily happen. In this paper, we introduce Private\nAssociation Editing (PAE), a novel defense approach for private data leakage.\nPAE is designed to effectively remove Personally Identifiable Information (PII)\nwithout retraining the model. Our approach consists of a four-step procedure:\ndetecting memorized PII, applying PAE cards to mitigate memorization of private\ndata, verifying resilience to targeted data extraction (TDE) attacks, and\nensuring consistency in the post-edit LLMs. The versatility and efficiency of\nPAE, which allows for batch modifications, significantly enhance data privacy\nin LLMs. Experimental results demonstrate the effectiveness of PAE in\nmitigating private data leakage. We believe PAE will serve as a critical tool\nin the ongoing effort to protect data privacy in LLMs, encouraging the\ndevelopment of safer models for real-world applications.\n", "link": "http://arxiv.org/abs/2406.18221v1", "date": "2024-06-26", "relevancy": 1.8217, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4977}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4535}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Data%20Privacy%20in%20Large%20Language%20Models%20through%20Private%0A%20%20Association%20Editing&body=Title%3A%20Enhancing%20Data%20Privacy%20in%20Large%20Language%20Models%20through%20Private%0A%20%20Association%20Editing%0AAuthor%3A%20Davide%20Venditti%20and%20Elena%20Sofia%20Ruzzetti%20and%20Giancarlo%20A.%20Xompero%20and%20Cristina%20Giannone%20and%20Andrea%20Favalli%20and%20Raniero%20Romagnoli%20and%20Fabio%20Massimo%20Zanzotto%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20powerful%20tools%20with%20extensive%20applications%2C%0Abut%20their%20tendency%20to%20memorize%20private%20information%20raises%20significant%20concerns%0Aas%20private%20data%20leakage%20can%20easily%20happen.%20In%20this%20paper%2C%20we%20introduce%20Private%0AAssociation%20Editing%20%28PAE%29%2C%20a%20novel%20defense%20approach%20for%20private%20data%20leakage.%0APAE%20is%20designed%20to%20effectively%20remove%20Personally%20Identifiable%20Information%20%28PII%29%0Awithout%20retraining%20the%20model.%20Our%20approach%20consists%20of%20a%20four-step%20procedure%3A%0Adetecting%20memorized%20PII%2C%20applying%20PAE%20cards%20to%20mitigate%20memorization%20of%20private%0Adata%2C%20verifying%20resilience%20to%20targeted%20data%20extraction%20%28TDE%29%20attacks%2C%20and%0Aensuring%20consistency%20in%20the%20post-edit%20LLMs.%20The%20versatility%20and%20efficiency%20of%0APAE%2C%20which%20allows%20for%20batch%20modifications%2C%20significantly%20enhance%20data%20privacy%0Ain%20LLMs.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20PAE%20in%0Amitigating%20private%20data%20leakage.%20We%20believe%20PAE%20will%20serve%20as%20a%20critical%20tool%0Ain%20the%20ongoing%20effort%20to%20protect%20data%20privacy%20in%20LLMs%2C%20encouraging%20the%0Adevelopment%20of%20safer%20models%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Data%2520Privacy%2520in%2520Large%2520Language%2520Models%2520through%2520Private%250A%2520%2520Association%2520Editing%26entry.906535625%3DDavide%2520Venditti%2520and%2520Elena%2520Sofia%2520Ruzzetti%2520and%2520Giancarlo%2520A.%2520Xompero%2520and%2520Cristina%2520Giannone%2520and%2520Andrea%2520Favalli%2520and%2520Raniero%2520Romagnoli%2520and%2520Fabio%2520Massimo%2520Zanzotto%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520powerful%2520tools%2520with%2520extensive%2520applications%252C%250Abut%2520their%2520tendency%2520to%2520memorize%2520private%2520information%2520raises%2520significant%2520concerns%250Aas%2520private%2520data%2520leakage%2520can%2520easily%2520happen.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Private%250AAssociation%2520Editing%2520%2528PAE%2529%252C%2520a%2520novel%2520defense%2520approach%2520for%2520private%2520data%2520leakage.%250APAE%2520is%2520designed%2520to%2520effectively%2520remove%2520Personally%2520Identifiable%2520Information%2520%2528PII%2529%250Awithout%2520retraining%2520the%2520model.%2520Our%2520approach%2520consists%2520of%2520a%2520four-step%2520procedure%253A%250Adetecting%2520memorized%2520PII%252C%2520applying%2520PAE%2520cards%2520to%2520mitigate%2520memorization%2520of%2520private%250Adata%252C%2520verifying%2520resilience%2520to%2520targeted%2520data%2520extraction%2520%2528TDE%2529%2520attacks%252C%2520and%250Aensuring%2520consistency%2520in%2520the%2520post-edit%2520LLMs.%2520The%2520versatility%2520and%2520efficiency%2520of%250APAE%252C%2520which%2520allows%2520for%2520batch%2520modifications%252C%2520significantly%2520enhance%2520data%2520privacy%250Ain%2520LLMs.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520PAE%2520in%250Amitigating%2520private%2520data%2520leakage.%2520We%2520believe%2520PAE%2520will%2520serve%2520as%2520a%2520critical%2520tool%250Ain%2520the%2520ongoing%2520effort%2520to%2520protect%2520data%2520privacy%2520in%2520LLMs%252C%2520encouraging%2520the%250Adevelopment%2520of%2520safer%2520models%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Data%20Privacy%20in%20Large%20Language%20Models%20through%20Private%0A%20%20Association%20Editing&entry.906535625=Davide%20Venditti%20and%20Elena%20Sofia%20Ruzzetti%20and%20Giancarlo%20A.%20Xompero%20and%20Cristina%20Giannone%20and%20Andrea%20Favalli%20and%20Raniero%20Romagnoli%20and%20Fabio%20Massimo%20Zanzotto&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20powerful%20tools%20with%20extensive%20applications%2C%0Abut%20their%20tendency%20to%20memorize%20private%20information%20raises%20significant%20concerns%0Aas%20private%20data%20leakage%20can%20easily%20happen.%20In%20this%20paper%2C%20we%20introduce%20Private%0AAssociation%20Editing%20%28PAE%29%2C%20a%20novel%20defense%20approach%20for%20private%20data%20leakage.%0APAE%20is%20designed%20to%20effectively%20remove%20Personally%20Identifiable%20Information%20%28PII%29%0Awithout%20retraining%20the%20model.%20Our%20approach%20consists%20of%20a%20four-step%20procedure%3A%0Adetecting%20memorized%20PII%2C%20applying%20PAE%20cards%20to%20mitigate%20memorization%20of%20private%0Adata%2C%20verifying%20resilience%20to%20targeted%20data%20extraction%20%28TDE%29%20attacks%2C%20and%0Aensuring%20consistency%20in%20the%20post-edit%20LLMs.%20The%20versatility%20and%20efficiency%20of%0APAE%2C%20which%20allows%20for%20batch%20modifications%2C%20significantly%20enhance%20data%20privacy%0Ain%20LLMs.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20PAE%20in%0Amitigating%20private%20data%20leakage.%20We%20believe%20PAE%20will%20serve%20as%20a%20critical%20tool%0Ain%20the%20ongoing%20effort%20to%20protect%20data%20privacy%20in%20LLMs%2C%20encouraging%20the%0Adevelopment%20of%20safer%20models%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18221v1&entry.124074799=Read"},
{"title": "CoDA: Interactive Segmentation and Morphological Analysis of Dendroid\n  Structures Exemplified on Stony Cold-Water Corals", "author": "Kira Schmitt and J\u00fcrgen Titschack and Daniel Baum", "abstract": "  Herein, we present CoDA, the Coral Dendroid structure Analyzer, a visual\nanalytics suite that allows for the first time to investigate the ontogenetic\nmorphological development of complex dendroid coral colonies, exemplified on\nthree important framework-forming dendroid cold-water corals: Lophelia pertusa\n(Linnaeus, 1758), Madrepora oculata (Linnaeus, 1758), and Goniocorella dumosa\n(Alcock, 1902). Input to CoDA is an initial instance segmentation of the coral\npolyp cavities (calices), from which it estimates the skeleton tree of the\ncolony and extracts classical morphological measurements and advanced shape\nfeatures of the individual corallites. CoDA also works as a proofreading and\nerror correction tool by helping to identify wrong parts in the skeleton tree\nand providing tools to quickly correct these errors. The final skeleton tree\nenables the derivation of additional information about the calices/corallite\ninstances that otherwise could not be obtained, including their ontogenetic\ngeneration and branching patterns - the basis of a fully quantitative\nstatistical analysis of the coral colony morphology. Part of CoDA is CoDAGraph,\na feature-rich link-and-brush user interface for visualizing the extracted\nfeatures and 2D graph layouts of the skeleton tree, enabling the real-time\nexploration of complex coral colonies and their building blocks, the individual\ncorallites and branches.\n  In the future, we expect CoDA to greatly facilitate the analysis of large\nstony corals of different species and morphotypes, as well as other dendroid\nstructures, enabling new insights into the influence of genetic and\nenvironmental factors on their ontogenetic morphological development.\n", "link": "http://arxiv.org/abs/2406.18236v1", "date": "2024-06-26", "relevancy": 1.8182, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4587}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4587}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoDA%3A%20Interactive%20Segmentation%20and%20Morphological%20Analysis%20of%20Dendroid%0A%20%20Structures%20Exemplified%20on%20Stony%20Cold-Water%20Corals&body=Title%3A%20CoDA%3A%20Interactive%20Segmentation%20and%20Morphological%20Analysis%20of%20Dendroid%0A%20%20Structures%20Exemplified%20on%20Stony%20Cold-Water%20Corals%0AAuthor%3A%20Kira%20Schmitt%20and%20J%C3%BCrgen%20Titschack%20and%20Daniel%20Baum%0AAbstract%3A%20%20%20Herein%2C%20we%20present%20CoDA%2C%20the%20Coral%20Dendroid%20structure%20Analyzer%2C%20a%20visual%0Aanalytics%20suite%20that%20allows%20for%20the%20first%20time%20to%20investigate%20the%20ontogenetic%0Amorphological%20development%20of%20complex%20dendroid%20coral%20colonies%2C%20exemplified%20on%0Athree%20important%20framework-forming%20dendroid%20cold-water%20corals%3A%20Lophelia%20pertusa%0A%28Linnaeus%2C%201758%29%2C%20Madrepora%20oculata%20%28Linnaeus%2C%201758%29%2C%20and%20Goniocorella%20dumosa%0A%28Alcock%2C%201902%29.%20Input%20to%20CoDA%20is%20an%20initial%20instance%20segmentation%20of%20the%20coral%0Apolyp%20cavities%20%28calices%29%2C%20from%20which%20it%20estimates%20the%20skeleton%20tree%20of%20the%0Acolony%20and%20extracts%20classical%20morphological%20measurements%20and%20advanced%20shape%0Afeatures%20of%20the%20individual%20corallites.%20CoDA%20also%20works%20as%20a%20proofreading%20and%0Aerror%20correction%20tool%20by%20helping%20to%20identify%20wrong%20parts%20in%20the%20skeleton%20tree%0Aand%20providing%20tools%20to%20quickly%20correct%20these%20errors.%20The%20final%20skeleton%20tree%0Aenables%20the%20derivation%20of%20additional%20information%20about%20the%20calices/corallite%0Ainstances%20that%20otherwise%20could%20not%20be%20obtained%2C%20including%20their%20ontogenetic%0Ageneration%20and%20branching%20patterns%20-%20the%20basis%20of%20a%20fully%20quantitative%0Astatistical%20analysis%20of%20the%20coral%20colony%20morphology.%20Part%20of%20CoDA%20is%20CoDAGraph%2C%0Aa%20feature-rich%20link-and-brush%20user%20interface%20for%20visualizing%20the%20extracted%0Afeatures%20and%202D%20graph%20layouts%20of%20the%20skeleton%20tree%2C%20enabling%20the%20real-time%0Aexploration%20of%20complex%20coral%20colonies%20and%20their%20building%20blocks%2C%20the%20individual%0Acorallites%20and%20branches.%0A%20%20In%20the%20future%2C%20we%20expect%20CoDA%20to%20greatly%20facilitate%20the%20analysis%20of%20large%0Astony%20corals%20of%20different%20species%20and%20morphotypes%2C%20as%20well%20as%20other%20dendroid%0Astructures%2C%20enabling%20new%20insights%20into%20the%20influence%20of%20genetic%20and%0Aenvironmental%20factors%20on%20their%20ontogenetic%20morphological%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoDA%253A%2520Interactive%2520Segmentation%2520and%2520Morphological%2520Analysis%2520of%2520Dendroid%250A%2520%2520Structures%2520Exemplified%2520on%2520Stony%2520Cold-Water%2520Corals%26entry.906535625%3DKira%2520Schmitt%2520and%2520J%25C3%25BCrgen%2520Titschack%2520and%2520Daniel%2520Baum%26entry.1292438233%3D%2520%2520Herein%252C%2520we%2520present%2520CoDA%252C%2520the%2520Coral%2520Dendroid%2520structure%2520Analyzer%252C%2520a%2520visual%250Aanalytics%2520suite%2520that%2520allows%2520for%2520the%2520first%2520time%2520to%2520investigate%2520the%2520ontogenetic%250Amorphological%2520development%2520of%2520complex%2520dendroid%2520coral%2520colonies%252C%2520exemplified%2520on%250Athree%2520important%2520framework-forming%2520dendroid%2520cold-water%2520corals%253A%2520Lophelia%2520pertusa%250A%2528Linnaeus%252C%25201758%2529%252C%2520Madrepora%2520oculata%2520%2528Linnaeus%252C%25201758%2529%252C%2520and%2520Goniocorella%2520dumosa%250A%2528Alcock%252C%25201902%2529.%2520Input%2520to%2520CoDA%2520is%2520an%2520initial%2520instance%2520segmentation%2520of%2520the%2520coral%250Apolyp%2520cavities%2520%2528calices%2529%252C%2520from%2520which%2520it%2520estimates%2520the%2520skeleton%2520tree%2520of%2520the%250Acolony%2520and%2520extracts%2520classical%2520morphological%2520measurements%2520and%2520advanced%2520shape%250Afeatures%2520of%2520the%2520individual%2520corallites.%2520CoDA%2520also%2520works%2520as%2520a%2520proofreading%2520and%250Aerror%2520correction%2520tool%2520by%2520helping%2520to%2520identify%2520wrong%2520parts%2520in%2520the%2520skeleton%2520tree%250Aand%2520providing%2520tools%2520to%2520quickly%2520correct%2520these%2520errors.%2520The%2520final%2520skeleton%2520tree%250Aenables%2520the%2520derivation%2520of%2520additional%2520information%2520about%2520the%2520calices/corallite%250Ainstances%2520that%2520otherwise%2520could%2520not%2520be%2520obtained%252C%2520including%2520their%2520ontogenetic%250Ageneration%2520and%2520branching%2520patterns%2520-%2520the%2520basis%2520of%2520a%2520fully%2520quantitative%250Astatistical%2520analysis%2520of%2520the%2520coral%2520colony%2520morphology.%2520Part%2520of%2520CoDA%2520is%2520CoDAGraph%252C%250Aa%2520feature-rich%2520link-and-brush%2520user%2520interface%2520for%2520visualizing%2520the%2520extracted%250Afeatures%2520and%25202D%2520graph%2520layouts%2520of%2520the%2520skeleton%2520tree%252C%2520enabling%2520the%2520real-time%250Aexploration%2520of%2520complex%2520coral%2520colonies%2520and%2520their%2520building%2520blocks%252C%2520the%2520individual%250Acorallites%2520and%2520branches.%250A%2520%2520In%2520the%2520future%252C%2520we%2520expect%2520CoDA%2520to%2520greatly%2520facilitate%2520the%2520analysis%2520of%2520large%250Astony%2520corals%2520of%2520different%2520species%2520and%2520morphotypes%252C%2520as%2520well%2520as%2520other%2520dendroid%250Astructures%252C%2520enabling%2520new%2520insights%2520into%2520the%2520influence%2520of%2520genetic%2520and%250Aenvironmental%2520factors%2520on%2520their%2520ontogenetic%2520morphological%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoDA%3A%20Interactive%20Segmentation%20and%20Morphological%20Analysis%20of%20Dendroid%0A%20%20Structures%20Exemplified%20on%20Stony%20Cold-Water%20Corals&entry.906535625=Kira%20Schmitt%20and%20J%C3%BCrgen%20Titschack%20and%20Daniel%20Baum&entry.1292438233=%20%20Herein%2C%20we%20present%20CoDA%2C%20the%20Coral%20Dendroid%20structure%20Analyzer%2C%20a%20visual%0Aanalytics%20suite%20that%20allows%20for%20the%20first%20time%20to%20investigate%20the%20ontogenetic%0Amorphological%20development%20of%20complex%20dendroid%20coral%20colonies%2C%20exemplified%20on%0Athree%20important%20framework-forming%20dendroid%20cold-water%20corals%3A%20Lophelia%20pertusa%0A%28Linnaeus%2C%201758%29%2C%20Madrepora%20oculata%20%28Linnaeus%2C%201758%29%2C%20and%20Goniocorella%20dumosa%0A%28Alcock%2C%201902%29.%20Input%20to%20CoDA%20is%20an%20initial%20instance%20segmentation%20of%20the%20coral%0Apolyp%20cavities%20%28calices%29%2C%20from%20which%20it%20estimates%20the%20skeleton%20tree%20of%20the%0Acolony%20and%20extracts%20classical%20morphological%20measurements%20and%20advanced%20shape%0Afeatures%20of%20the%20individual%20corallites.%20CoDA%20also%20works%20as%20a%20proofreading%20and%0Aerror%20correction%20tool%20by%20helping%20to%20identify%20wrong%20parts%20in%20the%20skeleton%20tree%0Aand%20providing%20tools%20to%20quickly%20correct%20these%20errors.%20The%20final%20skeleton%20tree%0Aenables%20the%20derivation%20of%20additional%20information%20about%20the%20calices/corallite%0Ainstances%20that%20otherwise%20could%20not%20be%20obtained%2C%20including%20their%20ontogenetic%0Ageneration%20and%20branching%20patterns%20-%20the%20basis%20of%20a%20fully%20quantitative%0Astatistical%20analysis%20of%20the%20coral%20colony%20morphology.%20Part%20of%20CoDA%20is%20CoDAGraph%2C%0Aa%20feature-rich%20link-and-brush%20user%20interface%20for%20visualizing%20the%20extracted%0Afeatures%20and%202D%20graph%20layouts%20of%20the%20skeleton%20tree%2C%20enabling%20the%20real-time%0Aexploration%20of%20complex%20coral%20colonies%20and%20their%20building%20blocks%2C%20the%20individual%0Acorallites%20and%20branches.%0A%20%20In%20the%20future%2C%20we%20expect%20CoDA%20to%20greatly%20facilitate%20the%20analysis%20of%20large%0Astony%20corals%20of%20different%20species%20and%20morphotypes%2C%20as%20well%20as%20other%20dendroid%0Astructures%2C%20enabling%20new%20insights%20into%20the%20influence%20of%20genetic%20and%0Aenvironmental%20factors%20on%20their%20ontogenetic%20morphological%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18236v1&entry.124074799=Read"},
{"title": "Second Maximum of a Gaussian Random Field and Exact (t-)Spacing test", "author": "Aza\u00efs Jean-Marc and Dalmao Federico and De Castro Yohann", "abstract": "  In this article, we introduce the novel concept of the second maximum of a\nGaussian random field on a Riemannian submanifold. This second maximum serves\nas a powerful tool for characterizing the distribution of the maximum. By\nutilizing an ad-hoc Kac Rice formula, we derive the explicit form of the\nmaximum's distribution, conditioned on the second maximum and some regressed\ncomponent of the Riemannian Hessian. This approach results in an exact test,\nbased on the evaluation of spacing between these maxima, which we refer to as\nthe spacing test.\n  We investigate the applicability of this test in detecting sparse\nalternatives within Gaussian symmetric tensors, continuous sparse\ndeconvolution, and two-layered neural networks with smooth rectifiers. Our\ntheoretical results are supported by numerical experiments, which illustrate\nthe calibration and power of the proposed tests. More generally, this test can\nbe applied to any Gaussian random field on a Riemannian manifold, and we\nprovide a general framework for the application of the spacing test in\ncontinuous sparse kernel regression.\n  Furthermore, when the variance-covariance function of the Gaussian random\nfield is known up to a scaling factor, we derive an exact Studentized version\nof our test, coined the $t$-spacing test. This test is perfectly calibrated\nunder the null hypothesis and has high power for detecting sparse alternatives.\n", "link": "http://arxiv.org/abs/2406.18397v1", "date": "2024-06-26", "relevancy": 1.8102, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4716}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4594}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Second%20Maximum%20of%20a%20Gaussian%20Random%20Field%20and%20Exact%20%28t-%29Spacing%20test&body=Title%3A%20Second%20Maximum%20of%20a%20Gaussian%20Random%20Field%20and%20Exact%20%28t-%29Spacing%20test%0AAuthor%3A%20Aza%C3%AFs%20Jean-Marc%20and%20Dalmao%20Federico%20and%20De%20Castro%20Yohann%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20introduce%20the%20novel%20concept%20of%20the%20second%20maximum%20of%20a%0AGaussian%20random%20field%20on%20a%20Riemannian%20submanifold.%20This%20second%20maximum%20serves%0Aas%20a%20powerful%20tool%20for%20characterizing%20the%20distribution%20of%20the%20maximum.%20By%0Autilizing%20an%20ad-hoc%20Kac%20Rice%20formula%2C%20we%20derive%20the%20explicit%20form%20of%20the%0Amaximum%27s%20distribution%2C%20conditioned%20on%20the%20second%20maximum%20and%20some%20regressed%0Acomponent%20of%20the%20Riemannian%20Hessian.%20This%20approach%20results%20in%20an%20exact%20test%2C%0Abased%20on%20the%20evaluation%20of%20spacing%20between%20these%20maxima%2C%20which%20we%20refer%20to%20as%0Athe%20spacing%20test.%0A%20%20We%20investigate%20the%20applicability%20of%20this%20test%20in%20detecting%20sparse%0Aalternatives%20within%20Gaussian%20symmetric%20tensors%2C%20continuous%20sparse%0Adeconvolution%2C%20and%20two-layered%20neural%20networks%20with%20smooth%20rectifiers.%20Our%0Atheoretical%20results%20are%20supported%20by%20numerical%20experiments%2C%20which%20illustrate%0Athe%20calibration%20and%20power%20of%20the%20proposed%20tests.%20More%20generally%2C%20this%20test%20can%0Abe%20applied%20to%20any%20Gaussian%20random%20field%20on%20a%20Riemannian%20manifold%2C%20and%20we%0Aprovide%20a%20general%20framework%20for%20the%20application%20of%20the%20spacing%20test%20in%0Acontinuous%20sparse%20kernel%20regression.%0A%20%20Furthermore%2C%20when%20the%20variance-covariance%20function%20of%20the%20Gaussian%20random%0Afield%20is%20known%20up%20to%20a%20scaling%20factor%2C%20we%20derive%20an%20exact%20Studentized%20version%0Aof%20our%20test%2C%20coined%20the%20%24t%24-spacing%20test.%20This%20test%20is%20perfectly%20calibrated%0Aunder%20the%20null%20hypothesis%20and%20has%20high%20power%20for%20detecting%20sparse%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecond%2520Maximum%2520of%2520a%2520Gaussian%2520Random%2520Field%2520and%2520Exact%2520%2528t-%2529Spacing%2520test%26entry.906535625%3DAza%25C3%25AFs%2520Jean-Marc%2520and%2520Dalmao%2520Federico%2520and%2520De%2520Castro%2520Yohann%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520introduce%2520the%2520novel%2520concept%2520of%2520the%2520second%2520maximum%2520of%2520a%250AGaussian%2520random%2520field%2520on%2520a%2520Riemannian%2520submanifold.%2520This%2520second%2520maximum%2520serves%250Aas%2520a%2520powerful%2520tool%2520for%2520characterizing%2520the%2520distribution%2520of%2520the%2520maximum.%2520By%250Autilizing%2520an%2520ad-hoc%2520Kac%2520Rice%2520formula%252C%2520we%2520derive%2520the%2520explicit%2520form%2520of%2520the%250Amaximum%2527s%2520distribution%252C%2520conditioned%2520on%2520the%2520second%2520maximum%2520and%2520some%2520regressed%250Acomponent%2520of%2520the%2520Riemannian%2520Hessian.%2520This%2520approach%2520results%2520in%2520an%2520exact%2520test%252C%250Abased%2520on%2520the%2520evaluation%2520of%2520spacing%2520between%2520these%2520maxima%252C%2520which%2520we%2520refer%2520to%2520as%250Athe%2520spacing%2520test.%250A%2520%2520We%2520investigate%2520the%2520applicability%2520of%2520this%2520test%2520in%2520detecting%2520sparse%250Aalternatives%2520within%2520Gaussian%2520symmetric%2520tensors%252C%2520continuous%2520sparse%250Adeconvolution%252C%2520and%2520two-layered%2520neural%2520networks%2520with%2520smooth%2520rectifiers.%2520Our%250Atheoretical%2520results%2520are%2520supported%2520by%2520numerical%2520experiments%252C%2520which%2520illustrate%250Athe%2520calibration%2520and%2520power%2520of%2520the%2520proposed%2520tests.%2520More%2520generally%252C%2520this%2520test%2520can%250Abe%2520applied%2520to%2520any%2520Gaussian%2520random%2520field%2520on%2520a%2520Riemannian%2520manifold%252C%2520and%2520we%250Aprovide%2520a%2520general%2520framework%2520for%2520the%2520application%2520of%2520the%2520spacing%2520test%2520in%250Acontinuous%2520sparse%2520kernel%2520regression.%250A%2520%2520Furthermore%252C%2520when%2520the%2520variance-covariance%2520function%2520of%2520the%2520Gaussian%2520random%250Afield%2520is%2520known%2520up%2520to%2520a%2520scaling%2520factor%252C%2520we%2520derive%2520an%2520exact%2520Studentized%2520version%250Aof%2520our%2520test%252C%2520coined%2520the%2520%2524t%2524-spacing%2520test.%2520This%2520test%2520is%2520perfectly%2520calibrated%250Aunder%2520the%2520null%2520hypothesis%2520and%2520has%2520high%2520power%2520for%2520detecting%2520sparse%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Second%20Maximum%20of%20a%20Gaussian%20Random%20Field%20and%20Exact%20%28t-%29Spacing%20test&entry.906535625=Aza%C3%AFs%20Jean-Marc%20and%20Dalmao%20Federico%20and%20De%20Castro%20Yohann&entry.1292438233=%20%20In%20this%20article%2C%20we%20introduce%20the%20novel%20concept%20of%20the%20second%20maximum%20of%20a%0AGaussian%20random%20field%20on%20a%20Riemannian%20submanifold.%20This%20second%20maximum%20serves%0Aas%20a%20powerful%20tool%20for%20characterizing%20the%20distribution%20of%20the%20maximum.%20By%0Autilizing%20an%20ad-hoc%20Kac%20Rice%20formula%2C%20we%20derive%20the%20explicit%20form%20of%20the%0Amaximum%27s%20distribution%2C%20conditioned%20on%20the%20second%20maximum%20and%20some%20regressed%0Acomponent%20of%20the%20Riemannian%20Hessian.%20This%20approach%20results%20in%20an%20exact%20test%2C%0Abased%20on%20the%20evaluation%20of%20spacing%20between%20these%20maxima%2C%20which%20we%20refer%20to%20as%0Athe%20spacing%20test.%0A%20%20We%20investigate%20the%20applicability%20of%20this%20test%20in%20detecting%20sparse%0Aalternatives%20within%20Gaussian%20symmetric%20tensors%2C%20continuous%20sparse%0Adeconvolution%2C%20and%20two-layered%20neural%20networks%20with%20smooth%20rectifiers.%20Our%0Atheoretical%20results%20are%20supported%20by%20numerical%20experiments%2C%20which%20illustrate%0Athe%20calibration%20and%20power%20of%20the%20proposed%20tests.%20More%20generally%2C%20this%20test%20can%0Abe%20applied%20to%20any%20Gaussian%20random%20field%20on%20a%20Riemannian%20manifold%2C%20and%20we%0Aprovide%20a%20general%20framework%20for%20the%20application%20of%20the%20spacing%20test%20in%0Acontinuous%20sparse%20kernel%20regression.%0A%20%20Furthermore%2C%20when%20the%20variance-covariance%20function%20of%20the%20Gaussian%20random%0Afield%20is%20known%20up%20to%20a%20scaling%20factor%2C%20we%20derive%20an%20exact%20Studentized%20version%0Aof%20our%20test%2C%20coined%20the%20%24t%24-spacing%20test.%20This%20test%20is%20perfectly%20calibrated%0Aunder%20the%20null%20hypothesis%20and%20has%20high%20power%20for%20detecting%20sparse%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18397v1&entry.124074799=Read"},
{"title": "Benchmarking mortality risk prediction from electrocardiograms", "author": "Platon Lukyanenko and Joshua Mayourian and Mingxuan Liu and John K. Triedman and Sunil J. Ghelani and William G. La Cava", "abstract": "  Several recent high-impact studies leverage large hospital-owned\nelectrocardiographic (ECG) databases to model and predict patient mortality.\nMIMIC-IV, released September 2023, is the first comparable public dataset and\nincludes 800,000 ECGs from a U.S. hospital system. Previously, the largest\npublic ECG dataset was Code-15, containing 345,000 ECGs collected during\nroutine care in Brazil. These datasets now provide an excellent resource for a\nbroader audience to explore ECG survival modeling. Here, we benchmark survival\nmodel performance on Code-15 and MIMIC-IV with two neural network\narchitectures, compare four deep survival modeling approaches to Cox\nregressions trained on classifier outputs, and evaluate performance at one to\nten years. Our results yield AUROC and concordance scores comparable to past\nwork (circa 0.8) and reasonable AUPRC scores (MIMIC-IV: 0.4-0.5, Code-15:\n0.05-0.13) considering the fraction of ECG samples linked to a mortality\n(MIMIC-IV: 27\\%, Code-15: 4\\%). When evaluating models on the opposite dataset,\nAUROC and concordance values drop by 0.1-0.15, which may be due to cohort\ndifferences. All code and results are made public.\n", "link": "http://arxiv.org/abs/2406.17002v2", "date": "2024-06-26", "relevancy": 1.6301, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4252}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4164}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20mortality%20risk%20prediction%20from%20electrocardiograms&body=Title%3A%20Benchmarking%20mortality%20risk%20prediction%20from%20electrocardiograms%0AAuthor%3A%20Platon%20Lukyanenko%20and%20Joshua%20Mayourian%20and%20Mingxuan%20Liu%20and%20John%20K.%20Triedman%20and%20Sunil%20J.%20Ghelani%20and%20William%20G.%20La%20Cava%0AAbstract%3A%20%20%20Several%20recent%20high-impact%20studies%20leverage%20large%20hospital-owned%0Aelectrocardiographic%20%28ECG%29%20databases%20to%20model%20and%20predict%20patient%20mortality.%0AMIMIC-IV%2C%20released%20September%202023%2C%20is%20the%20first%20comparable%20public%20dataset%20and%0Aincludes%20800%2C000%20ECGs%20from%20a%20U.S.%20hospital%20system.%20Previously%2C%20the%20largest%0Apublic%20ECG%20dataset%20was%20Code-15%2C%20containing%20345%2C000%20ECGs%20collected%20during%0Aroutine%20care%20in%20Brazil.%20These%20datasets%20now%20provide%20an%20excellent%20resource%20for%20a%0Abroader%20audience%20to%20explore%20ECG%20survival%20modeling.%20Here%2C%20we%20benchmark%20survival%0Amodel%20performance%20on%20Code-15%20and%20MIMIC-IV%20with%20two%20neural%20network%0Aarchitectures%2C%20compare%20four%20deep%20survival%20modeling%20approaches%20to%20Cox%0Aregressions%20trained%20on%20classifier%20outputs%2C%20and%20evaluate%20performance%20at%20one%20to%0Aten%20years.%20Our%20results%20yield%20AUROC%20and%20concordance%20scores%20comparable%20to%20past%0Awork%20%28circa%200.8%29%20and%20reasonable%20AUPRC%20scores%20%28MIMIC-IV%3A%200.4-0.5%2C%20Code-15%3A%0A0.05-0.13%29%20considering%20the%20fraction%20of%20ECG%20samples%20linked%20to%20a%20mortality%0A%28MIMIC-IV%3A%2027%5C%25%2C%20Code-15%3A%204%5C%25%29.%20When%20evaluating%20models%20on%20the%20opposite%20dataset%2C%0AAUROC%20and%20concordance%20values%20drop%20by%200.1-0.15%2C%20which%20may%20be%20due%20to%20cohort%0Adifferences.%20All%20code%20and%20results%20are%20made%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520mortality%2520risk%2520prediction%2520from%2520electrocardiograms%26entry.906535625%3DPlaton%2520Lukyanenko%2520and%2520Joshua%2520Mayourian%2520and%2520Mingxuan%2520Liu%2520and%2520John%2520K.%2520Triedman%2520and%2520Sunil%2520J.%2520Ghelani%2520and%2520William%2520G.%2520La%2520Cava%26entry.1292438233%3D%2520%2520Several%2520recent%2520high-impact%2520studies%2520leverage%2520large%2520hospital-owned%250Aelectrocardiographic%2520%2528ECG%2529%2520databases%2520to%2520model%2520and%2520predict%2520patient%2520mortality.%250AMIMIC-IV%252C%2520released%2520September%25202023%252C%2520is%2520the%2520first%2520comparable%2520public%2520dataset%2520and%250Aincludes%2520800%252C000%2520ECGs%2520from%2520a%2520U.S.%2520hospital%2520system.%2520Previously%252C%2520the%2520largest%250Apublic%2520ECG%2520dataset%2520was%2520Code-15%252C%2520containing%2520345%252C000%2520ECGs%2520collected%2520during%250Aroutine%2520care%2520in%2520Brazil.%2520These%2520datasets%2520now%2520provide%2520an%2520excellent%2520resource%2520for%2520a%250Abroader%2520audience%2520to%2520explore%2520ECG%2520survival%2520modeling.%2520Here%252C%2520we%2520benchmark%2520survival%250Amodel%2520performance%2520on%2520Code-15%2520and%2520MIMIC-IV%2520with%2520two%2520neural%2520network%250Aarchitectures%252C%2520compare%2520four%2520deep%2520survival%2520modeling%2520approaches%2520to%2520Cox%250Aregressions%2520trained%2520on%2520classifier%2520outputs%252C%2520and%2520evaluate%2520performance%2520at%2520one%2520to%250Aten%2520years.%2520Our%2520results%2520yield%2520AUROC%2520and%2520concordance%2520scores%2520comparable%2520to%2520past%250Awork%2520%2528circa%25200.8%2529%2520and%2520reasonable%2520AUPRC%2520scores%2520%2528MIMIC-IV%253A%25200.4-0.5%252C%2520Code-15%253A%250A0.05-0.13%2529%2520considering%2520the%2520fraction%2520of%2520ECG%2520samples%2520linked%2520to%2520a%2520mortality%250A%2528MIMIC-IV%253A%252027%255C%2525%252C%2520Code-15%253A%25204%255C%2525%2529.%2520When%2520evaluating%2520models%2520on%2520the%2520opposite%2520dataset%252C%250AAUROC%2520and%2520concordance%2520values%2520drop%2520by%25200.1-0.15%252C%2520which%2520may%2520be%2520due%2520to%2520cohort%250Adifferences.%2520All%2520code%2520and%2520results%2520are%2520made%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20mortality%20risk%20prediction%20from%20electrocardiograms&entry.906535625=Platon%20Lukyanenko%20and%20Joshua%20Mayourian%20and%20Mingxuan%20Liu%20and%20John%20K.%20Triedman%20and%20Sunil%20J.%20Ghelani%20and%20William%20G.%20La%20Cava&entry.1292438233=%20%20Several%20recent%20high-impact%20studies%20leverage%20large%20hospital-owned%0Aelectrocardiographic%20%28ECG%29%20databases%20to%20model%20and%20predict%20patient%20mortality.%0AMIMIC-IV%2C%20released%20September%202023%2C%20is%20the%20first%20comparable%20public%20dataset%20and%0Aincludes%20800%2C000%20ECGs%20from%20a%20U.S.%20hospital%20system.%20Previously%2C%20the%20largest%0Apublic%20ECG%20dataset%20was%20Code-15%2C%20containing%20345%2C000%20ECGs%20collected%20during%0Aroutine%20care%20in%20Brazil.%20These%20datasets%20now%20provide%20an%20excellent%20resource%20for%20a%0Abroader%20audience%20to%20explore%20ECG%20survival%20modeling.%20Here%2C%20we%20benchmark%20survival%0Amodel%20performance%20on%20Code-15%20and%20MIMIC-IV%20with%20two%20neural%20network%0Aarchitectures%2C%20compare%20four%20deep%20survival%20modeling%20approaches%20to%20Cox%0Aregressions%20trained%20on%20classifier%20outputs%2C%20and%20evaluate%20performance%20at%20one%20to%0Aten%20years.%20Our%20results%20yield%20AUROC%20and%20concordance%20scores%20comparable%20to%20past%0Awork%20%28circa%200.8%29%20and%20reasonable%20AUPRC%20scores%20%28MIMIC-IV%3A%200.4-0.5%2C%20Code-15%3A%0A0.05-0.13%29%20considering%20the%20fraction%20of%20ECG%20samples%20linked%20to%20a%20mortality%0A%28MIMIC-IV%3A%2027%5C%25%2C%20Code-15%3A%204%5C%25%29.%20When%20evaluating%20models%20on%20the%20opposite%20dataset%2C%0AAUROC%20and%20concordance%20values%20drop%20by%200.1-0.15%2C%20which%20may%20be%20due%20to%20cohort%0Adifferences.%20All%20code%20and%20results%20are%20made%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17002v2&entry.124074799=Read"},
{"title": "Advancing Robotic Surgery: Affordable Kinesthetic and Tactile Feedback\n  Solutions for Endotrainers", "author": "Bharath Rajiv Nair and Aravinthkumar T. and B. Vinod", "abstract": "  The proliferation of robot-assisted minimally invasive surgery highlights the\nneed for advanced training tools such as cost-effective robotic endotrainers.\nCurrent surgical robots often lack haptic feedback, which is crucial for\nproviding surgeons with a real-time sense of touch. This absence can impact the\nsurgeon's ability to perform delicate operations effectively. To enhance\nsurgical training and address this deficiency, we have integrated a\ncost-effective haptic feedback system into a robotic endotrainer. This system\nincorporates both kinesthetic (force) and tactile feedback, improving the\nfidelity of surgical simulations and enabling more precise control during\noperations. Our system incorporates an innovative, cost-effective Force/Torque\nsensor utilizing optoelectronic technology, specifically designed to accurately\ndetect forces and moments exerted on surgical tools with a 95% accuracy,\nproviding essential kinesthetic feedback. Additionally, we implemented a\ntactile feedback mechanism that informs the surgeon of the gripping forces\nbetween the tool's tip and the tissue. This dual feedback system enhances the\nfidelity of training simulations and the execution of robotic surgeries,\npromoting broader adoption and safer practices.\n", "link": "http://arxiv.org/abs/2406.18229v1", "date": "2024-06-26", "relevancy": 1.5846, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.54}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5143}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Robotic%20Surgery%3A%20Affordable%20Kinesthetic%20and%20Tactile%20Feedback%0A%20%20Solutions%20for%20Endotrainers&body=Title%3A%20Advancing%20Robotic%20Surgery%3A%20Affordable%20Kinesthetic%20and%20Tactile%20Feedback%0A%20%20Solutions%20for%20Endotrainers%0AAuthor%3A%20Bharath%20Rajiv%20Nair%20and%20Aravinthkumar%20T.%20and%20B.%20Vinod%0AAbstract%3A%20%20%20The%20proliferation%20of%20robot-assisted%20minimally%20invasive%20surgery%20highlights%20the%0Aneed%20for%20advanced%20training%20tools%20such%20as%20cost-effective%20robotic%20endotrainers.%0ACurrent%20surgical%20robots%20often%20lack%20haptic%20feedback%2C%20which%20is%20crucial%20for%0Aproviding%20surgeons%20with%20a%20real-time%20sense%20of%20touch.%20This%20absence%20can%20impact%20the%0Asurgeon%27s%20ability%20to%20perform%20delicate%20operations%20effectively.%20To%20enhance%0Asurgical%20training%20and%20address%20this%20deficiency%2C%20we%20have%20integrated%20a%0Acost-effective%20haptic%20feedback%20system%20into%20a%20robotic%20endotrainer.%20This%20system%0Aincorporates%20both%20kinesthetic%20%28force%29%20and%20tactile%20feedback%2C%20improving%20the%0Afidelity%20of%20surgical%20simulations%20and%20enabling%20more%20precise%20control%20during%0Aoperations.%20Our%20system%20incorporates%20an%20innovative%2C%20cost-effective%20Force/Torque%0Asensor%20utilizing%20optoelectronic%20technology%2C%20specifically%20designed%20to%20accurately%0Adetect%20forces%20and%20moments%20exerted%20on%20surgical%20tools%20with%20a%2095%25%20accuracy%2C%0Aproviding%20essential%20kinesthetic%20feedback.%20Additionally%2C%20we%20implemented%20a%0Atactile%20feedback%20mechanism%20that%20informs%20the%20surgeon%20of%20the%20gripping%20forces%0Abetween%20the%20tool%27s%20tip%20and%20the%20tissue.%20This%20dual%20feedback%20system%20enhances%20the%0Afidelity%20of%20training%20simulations%20and%20the%20execution%20of%20robotic%20surgeries%2C%0Apromoting%20broader%20adoption%20and%20safer%20practices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Robotic%2520Surgery%253A%2520Affordable%2520Kinesthetic%2520and%2520Tactile%2520Feedback%250A%2520%2520Solutions%2520for%2520Endotrainers%26entry.906535625%3DBharath%2520Rajiv%2520Nair%2520and%2520Aravinthkumar%2520T.%2520and%2520B.%2520Vinod%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520robot-assisted%2520minimally%2520invasive%2520surgery%2520highlights%2520the%250Aneed%2520for%2520advanced%2520training%2520tools%2520such%2520as%2520cost-effective%2520robotic%2520endotrainers.%250ACurrent%2520surgical%2520robots%2520often%2520lack%2520haptic%2520feedback%252C%2520which%2520is%2520crucial%2520for%250Aproviding%2520surgeons%2520with%2520a%2520real-time%2520sense%2520of%2520touch.%2520This%2520absence%2520can%2520impact%2520the%250Asurgeon%2527s%2520ability%2520to%2520perform%2520delicate%2520operations%2520effectively.%2520To%2520enhance%250Asurgical%2520training%2520and%2520address%2520this%2520deficiency%252C%2520we%2520have%2520integrated%2520a%250Acost-effective%2520haptic%2520feedback%2520system%2520into%2520a%2520robotic%2520endotrainer.%2520This%2520system%250Aincorporates%2520both%2520kinesthetic%2520%2528force%2529%2520and%2520tactile%2520feedback%252C%2520improving%2520the%250Afidelity%2520of%2520surgical%2520simulations%2520and%2520enabling%2520more%2520precise%2520control%2520during%250Aoperations.%2520Our%2520system%2520incorporates%2520an%2520innovative%252C%2520cost-effective%2520Force/Torque%250Asensor%2520utilizing%2520optoelectronic%2520technology%252C%2520specifically%2520designed%2520to%2520accurately%250Adetect%2520forces%2520and%2520moments%2520exerted%2520on%2520surgical%2520tools%2520with%2520a%252095%2525%2520accuracy%252C%250Aproviding%2520essential%2520kinesthetic%2520feedback.%2520Additionally%252C%2520we%2520implemented%2520a%250Atactile%2520feedback%2520mechanism%2520that%2520informs%2520the%2520surgeon%2520of%2520the%2520gripping%2520forces%250Abetween%2520the%2520tool%2527s%2520tip%2520and%2520the%2520tissue.%2520This%2520dual%2520feedback%2520system%2520enhances%2520the%250Afidelity%2520of%2520training%2520simulations%2520and%2520the%2520execution%2520of%2520robotic%2520surgeries%252C%250Apromoting%2520broader%2520adoption%2520and%2520safer%2520practices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Robotic%20Surgery%3A%20Affordable%20Kinesthetic%20and%20Tactile%20Feedback%0A%20%20Solutions%20for%20Endotrainers&entry.906535625=Bharath%20Rajiv%20Nair%20and%20Aravinthkumar%20T.%20and%20B.%20Vinod&entry.1292438233=%20%20The%20proliferation%20of%20robot-assisted%20minimally%20invasive%20surgery%20highlights%20the%0Aneed%20for%20advanced%20training%20tools%20such%20as%20cost-effective%20robotic%20endotrainers.%0ACurrent%20surgical%20robots%20often%20lack%20haptic%20feedback%2C%20which%20is%20crucial%20for%0Aproviding%20surgeons%20with%20a%20real-time%20sense%20of%20touch.%20This%20absence%20can%20impact%20the%0Asurgeon%27s%20ability%20to%20perform%20delicate%20operations%20effectively.%20To%20enhance%0Asurgical%20training%20and%20address%20this%20deficiency%2C%20we%20have%20integrated%20a%0Acost-effective%20haptic%20feedback%20system%20into%20a%20robotic%20endotrainer.%20This%20system%0Aincorporates%20both%20kinesthetic%20%28force%29%20and%20tactile%20feedback%2C%20improving%20the%0Afidelity%20of%20surgical%20simulations%20and%20enabling%20more%20precise%20control%20during%0Aoperations.%20Our%20system%20incorporates%20an%20innovative%2C%20cost-effective%20Force/Torque%0Asensor%20utilizing%20optoelectronic%20technology%2C%20specifically%20designed%20to%20accurately%0Adetect%20forces%20and%20moments%20exerted%20on%20surgical%20tools%20with%20a%2095%25%20accuracy%2C%0Aproviding%20essential%20kinesthetic%20feedback.%20Additionally%2C%20we%20implemented%20a%0Atactile%20feedback%20mechanism%20that%20informs%20the%20surgeon%20of%20the%20gripping%20forces%0Abetween%20the%20tool%27s%20tip%20and%20the%20tissue.%20This%20dual%20feedback%20system%20enhances%20the%0Afidelity%20of%20training%20simulations%20and%20the%20execution%20of%20robotic%20surgeries%2C%0Apromoting%20broader%20adoption%20and%20safer%20practices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18229v1&entry.124074799=Read"},
{"title": "LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language\n  Models", "author": "Michele Brienza and Emanuele Musumeci and Vincenzo Suriani and Daniele Affinita and Andrea Pennisi and Daniele Nardi and Domenico Daniele Bloisi", "abstract": "  The deployment of robots into human scenarios necessitates advanced planning\nstrategies, particularly when we ask robots to operate in dynamic, unstructured\nenvironments. RoboCup offers the chance to deploy robots in one of those\nscenarios, a human-shaped game represented by a soccer match. In such\nscenarios, robots must operate using predefined behaviors that can fail in\nunpredictable conditions. This paper introduces a novel application of Large\nLanguage Models (LLMs) to address the challenge of generating actionable plans\nin such settings, specifically within the context of the RoboCup Standard\nPlatform League (SPL) competitions where robots are required to autonomously\nexecute soccer strategies that emerge from the interactions of individual\nagents. In particular, we propose a multi-role approach leveraging the\ncapabilities of LLMs to generate and refine plans for a robotic soccer team.\nThe potential of the proposed method is demonstrated through an experimental\nevaluation,carried out simulating multiple matches where robots with\nAI-generated plans play against robots running human-built code.\n", "link": "http://arxiv.org/abs/2406.18285v1", "date": "2024-06-26", "relevancy": 1.5783, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5995}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5498}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLCoach%3A%20Generating%20Robot%20Soccer%20Plans%20using%20Multi-Role%20Large%20Language%0A%20%20Models&body=Title%3A%20LLCoach%3A%20Generating%20Robot%20Soccer%20Plans%20using%20Multi-Role%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Michele%20Brienza%20and%20Emanuele%20Musumeci%20and%20Vincenzo%20Suriani%20and%20Daniele%20Affinita%20and%20Andrea%20Pennisi%20and%20Daniele%20Nardi%20and%20Domenico%20Daniele%20Bloisi%0AAbstract%3A%20%20%20The%20deployment%20of%20robots%20into%20human%20scenarios%20necessitates%20advanced%20planning%0Astrategies%2C%20particularly%20when%20we%20ask%20robots%20to%20operate%20in%20dynamic%2C%20unstructured%0Aenvironments.%20RoboCup%20offers%20the%20chance%20to%20deploy%20robots%20in%20one%20of%20those%0Ascenarios%2C%20a%20human-shaped%20game%20represented%20by%20a%20soccer%20match.%20In%20such%0Ascenarios%2C%20robots%20must%20operate%20using%20predefined%20behaviors%20that%20can%20fail%20in%0Aunpredictable%20conditions.%20This%20paper%20introduces%20a%20novel%20application%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20address%20the%20challenge%20of%20generating%20actionable%20plans%0Ain%20such%20settings%2C%20specifically%20within%20the%20context%20of%20the%20RoboCup%20Standard%0APlatform%20League%20%28SPL%29%20competitions%20where%20robots%20are%20required%20to%20autonomously%0Aexecute%20soccer%20strategies%20that%20emerge%20from%20the%20interactions%20of%20individual%0Aagents.%20In%20particular%2C%20we%20propose%20a%20multi-role%20approach%20leveraging%20the%0Acapabilities%20of%20LLMs%20to%20generate%20and%20refine%20plans%20for%20a%20robotic%20soccer%20team.%0AThe%20potential%20of%20the%20proposed%20method%20is%20demonstrated%20through%20an%20experimental%0Aevaluation%2Ccarried%20out%20simulating%20multiple%20matches%20where%20robots%20with%0AAI-generated%20plans%20play%20against%20robots%20running%20human-built%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLCoach%253A%2520Generating%2520Robot%2520Soccer%2520Plans%2520using%2520Multi-Role%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DMichele%2520Brienza%2520and%2520Emanuele%2520Musumeci%2520and%2520Vincenzo%2520Suriani%2520and%2520Daniele%2520Affinita%2520and%2520Andrea%2520Pennisi%2520and%2520Daniele%2520Nardi%2520and%2520Domenico%2520Daniele%2520Bloisi%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520robots%2520into%2520human%2520scenarios%2520necessitates%2520advanced%2520planning%250Astrategies%252C%2520particularly%2520when%2520we%2520ask%2520robots%2520to%2520operate%2520in%2520dynamic%252C%2520unstructured%250Aenvironments.%2520RoboCup%2520offers%2520the%2520chance%2520to%2520deploy%2520robots%2520in%2520one%2520of%2520those%250Ascenarios%252C%2520a%2520human-shaped%2520game%2520represented%2520by%2520a%2520soccer%2520match.%2520In%2520such%250Ascenarios%252C%2520robots%2520must%2520operate%2520using%2520predefined%2520behaviors%2520that%2520can%2520fail%2520in%250Aunpredictable%2520conditions.%2520This%2520paper%2520introduces%2520a%2520novel%2520application%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520address%2520the%2520challenge%2520of%2520generating%2520actionable%2520plans%250Ain%2520such%2520settings%252C%2520specifically%2520within%2520the%2520context%2520of%2520the%2520RoboCup%2520Standard%250APlatform%2520League%2520%2528SPL%2529%2520competitions%2520where%2520robots%2520are%2520required%2520to%2520autonomously%250Aexecute%2520soccer%2520strategies%2520that%2520emerge%2520from%2520the%2520interactions%2520of%2520individual%250Aagents.%2520In%2520particular%252C%2520we%2520propose%2520a%2520multi-role%2520approach%2520leveraging%2520the%250Acapabilities%2520of%2520LLMs%2520to%2520generate%2520and%2520refine%2520plans%2520for%2520a%2520robotic%2520soccer%2520team.%250AThe%2520potential%2520of%2520the%2520proposed%2520method%2520is%2520demonstrated%2520through%2520an%2520experimental%250Aevaluation%252Ccarried%2520out%2520simulating%2520multiple%2520matches%2520where%2520robots%2520with%250AAI-generated%2520plans%2520play%2520against%2520robots%2520running%2520human-built%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLCoach%3A%20Generating%20Robot%20Soccer%20Plans%20using%20Multi-Role%20Large%20Language%0A%20%20Models&entry.906535625=Michele%20Brienza%20and%20Emanuele%20Musumeci%20and%20Vincenzo%20Suriani%20and%20Daniele%20Affinita%20and%20Andrea%20Pennisi%20and%20Daniele%20Nardi%20and%20Domenico%20Daniele%20Bloisi&entry.1292438233=%20%20The%20deployment%20of%20robots%20into%20human%20scenarios%20necessitates%20advanced%20planning%0Astrategies%2C%20particularly%20when%20we%20ask%20robots%20to%20operate%20in%20dynamic%2C%20unstructured%0Aenvironments.%20RoboCup%20offers%20the%20chance%20to%20deploy%20robots%20in%20one%20of%20those%0Ascenarios%2C%20a%20human-shaped%20game%20represented%20by%20a%20soccer%20match.%20In%20such%0Ascenarios%2C%20robots%20must%20operate%20using%20predefined%20behaviors%20that%20can%20fail%20in%0Aunpredictable%20conditions.%20This%20paper%20introduces%20a%20novel%20application%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20address%20the%20challenge%20of%20generating%20actionable%20plans%0Ain%20such%20settings%2C%20specifically%20within%20the%20context%20of%20the%20RoboCup%20Standard%0APlatform%20League%20%28SPL%29%20competitions%20where%20robots%20are%20required%20to%20autonomously%0Aexecute%20soccer%20strategies%20that%20emerge%20from%20the%20interactions%20of%20individual%0Aagents.%20In%20particular%2C%20we%20propose%20a%20multi-role%20approach%20leveraging%20the%0Acapabilities%20of%20LLMs%20to%20generate%20and%20refine%20plans%20for%20a%20robotic%20soccer%20team.%0AThe%20potential%20of%20the%20proposed%20method%20is%20demonstrated%20through%20an%20experimental%0Aevaluation%2Ccarried%20out%20simulating%20multiple%20matches%20where%20robots%20with%0AAI-generated%20plans%20play%20against%20robots%20running%20human-built%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18285v1&entry.124074799=Read"},
{"title": "Do LLMs dream of elephants (when told not to)? Latent concept\n  association and associative memory in transformers", "author": "Yibo Jiang and Goutham Rajendran and Pradeep Ravikumar and Bryon Aragam", "abstract": "  Large Language Models (LLMs) have the capacity to store and recall facts.\nThrough experimentation with open-source models, we observe that this ability\nto retrieve facts can be easily manipulated by changing contexts, even without\naltering their factual meanings. These findings highlight that LLMs might\nbehave like an associative memory model where certain tokens in the contexts\nserve as clues to retrieving facts. We mathematically explore this property by\nstudying how transformers, the building blocks of LLMs, can complete such\nmemory tasks. We study a simple latent concept association problem with a\none-layer transformer and we show theoretically and empirically that the\ntransformer gathers information using self-attention and uses the value matrix\nfor associative memory.\n", "link": "http://arxiv.org/abs/2406.18400v1", "date": "2024-06-26", "relevancy": 1.7911, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4511}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4456}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20dream%20of%20elephants%20%28when%20told%20not%20to%29%3F%20Latent%20concept%0A%20%20association%20and%20associative%20memory%20in%20transformers&body=Title%3A%20Do%20LLMs%20dream%20of%20elephants%20%28when%20told%20not%20to%29%3F%20Latent%20concept%0A%20%20association%20and%20associative%20memory%20in%20transformers%0AAuthor%3A%20Yibo%20Jiang%20and%20Goutham%20Rajendran%20and%20Pradeep%20Ravikumar%20and%20Bryon%20Aragam%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20the%20capacity%20to%20store%20and%20recall%20facts.%0AThrough%20experimentation%20with%20open-source%20models%2C%20we%20observe%20that%20this%20ability%0Ato%20retrieve%20facts%20can%20be%20easily%20manipulated%20by%20changing%20contexts%2C%20even%20without%0Aaltering%20their%20factual%20meanings.%20These%20findings%20highlight%20that%20LLMs%20might%0Abehave%20like%20an%20associative%20memory%20model%20where%20certain%20tokens%20in%20the%20contexts%0Aserve%20as%20clues%20to%20retrieving%20facts.%20We%20mathematically%20explore%20this%20property%20by%0Astudying%20how%20transformers%2C%20the%20building%20blocks%20of%20LLMs%2C%20can%20complete%20such%0Amemory%20tasks.%20We%20study%20a%20simple%20latent%20concept%20association%20problem%20with%20a%0Aone-layer%20transformer%20and%20we%20show%20theoretically%20and%20empirically%20that%20the%0Atransformer%20gathers%20information%20using%20self-attention%20and%20uses%20the%20value%20matrix%0Afor%20associative%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520dream%2520of%2520elephants%2520%2528when%2520told%2520not%2520to%2529%253F%2520Latent%2520concept%250A%2520%2520association%2520and%2520associative%2520memory%2520in%2520transformers%26entry.906535625%3DYibo%2520Jiang%2520and%2520Goutham%2520Rajendran%2520and%2520Pradeep%2520Ravikumar%2520and%2520Bryon%2520Aragam%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520the%2520capacity%2520to%2520store%2520and%2520recall%2520facts.%250AThrough%2520experimentation%2520with%2520open-source%2520models%252C%2520we%2520observe%2520that%2520this%2520ability%250Ato%2520retrieve%2520facts%2520can%2520be%2520easily%2520manipulated%2520by%2520changing%2520contexts%252C%2520even%2520without%250Aaltering%2520their%2520factual%2520meanings.%2520These%2520findings%2520highlight%2520that%2520LLMs%2520might%250Abehave%2520like%2520an%2520associative%2520memory%2520model%2520where%2520certain%2520tokens%2520in%2520the%2520contexts%250Aserve%2520as%2520clues%2520to%2520retrieving%2520facts.%2520We%2520mathematically%2520explore%2520this%2520property%2520by%250Astudying%2520how%2520transformers%252C%2520the%2520building%2520blocks%2520of%2520LLMs%252C%2520can%2520complete%2520such%250Amemory%2520tasks.%2520We%2520study%2520a%2520simple%2520latent%2520concept%2520association%2520problem%2520with%2520a%250Aone-layer%2520transformer%2520and%2520we%2520show%2520theoretically%2520and%2520empirically%2520that%2520the%250Atransformer%2520gathers%2520information%2520using%2520self-attention%2520and%2520uses%2520the%2520value%2520matrix%250Afor%2520associative%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20dream%20of%20elephants%20%28when%20told%20not%20to%29%3F%20Latent%20concept%0A%20%20association%20and%20associative%20memory%20in%20transformers&entry.906535625=Yibo%20Jiang%20and%20Goutham%20Rajendran%20and%20Pradeep%20Ravikumar%20and%20Bryon%20Aragam&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20the%20capacity%20to%20store%20and%20recall%20facts.%0AThrough%20experimentation%20with%20open-source%20models%2C%20we%20observe%20that%20this%20ability%0Ato%20retrieve%20facts%20can%20be%20easily%20manipulated%20by%20changing%20contexts%2C%20even%20without%0Aaltering%20their%20factual%20meanings.%20These%20findings%20highlight%20that%20LLMs%20might%0Abehave%20like%20an%20associative%20memory%20model%20where%20certain%20tokens%20in%20the%20contexts%0Aserve%20as%20clues%20to%20retrieving%20facts.%20We%20mathematically%20explore%20this%20property%20by%0Astudying%20how%20transformers%2C%20the%20building%20blocks%20of%20LLMs%2C%20can%20complete%20such%0Amemory%20tasks.%20We%20study%20a%20simple%20latent%20concept%20association%20problem%20with%20a%0Aone-layer%20transformer%20and%20we%20show%20theoretically%20and%20empirically%20that%20the%0Atransformer%20gathers%20information%20using%20self-attention%20and%20uses%20the%20value%20matrix%0Afor%20associative%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18400v1&entry.124074799=Read"},
{"title": "Mixture of Experts in a Mixture of RL settings", "author": "Timon Willi and Johan Obando-Ceron and Jakob Foerster and Karolina Dziugaite and Pablo Samuel Castro", "abstract": "  Mixtures of Experts (MoEs) have gained prominence in (self-)supervised\nlearning due to their enhanced inference efficiency, adaptability to\ndistributed training, and modularity. Previous research has illustrated that\nMoEs can significantly boost Deep Reinforcement Learning (DRL) performance by\nexpanding the network's parameter count while reducing dormant neurons, thereby\nenhancing the model's learning capacity and ability to deal with\nnon-stationarity. In this work, we shed more light on MoEs' ability to deal\nwith non-stationarity and investigate MoEs in DRL settings with \"amplified\"\nnon-stationarity via multi-task training, providing further evidence that MoEs\nimprove learning capacity. In contrast to previous work, our multi-task results\nallow us to better understand the underlying causes for the beneficial effect\nof MoE in DRL training, the impact of the various MoE components, and insights\ninto how best to incorporate them in actor-critic-based DRL networks. Finally,\nwe also confirm results from previous work.\n", "link": "http://arxiv.org/abs/2406.18420v1", "date": "2024-06-26", "relevancy": 1.0065, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5087}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5015}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Experts%20in%20a%20Mixture%20of%20RL%20settings&body=Title%3A%20Mixture%20of%20Experts%20in%20a%20Mixture%20of%20RL%20settings%0AAuthor%3A%20Timon%20Willi%20and%20Johan%20Obando-Ceron%20and%20Jakob%20Foerster%20and%20Karolina%20Dziugaite%20and%20Pablo%20Samuel%20Castro%0AAbstract%3A%20%20%20Mixtures%20of%20Experts%20%28MoEs%29%20have%20gained%20prominence%20in%20%28self-%29supervised%0Alearning%20due%20to%20their%20enhanced%20inference%20efficiency%2C%20adaptability%20to%0Adistributed%20training%2C%20and%20modularity.%20Previous%20research%20has%20illustrated%20that%0AMoEs%20can%20significantly%20boost%20Deep%20Reinforcement%20Learning%20%28DRL%29%20performance%20by%0Aexpanding%20the%20network%27s%20parameter%20count%20while%20reducing%20dormant%20neurons%2C%20thereby%0Aenhancing%20the%20model%27s%20learning%20capacity%20and%20ability%20to%20deal%20with%0Anon-stationarity.%20In%20this%20work%2C%20we%20shed%20more%20light%20on%20MoEs%27%20ability%20to%20deal%0Awith%20non-stationarity%20and%20investigate%20MoEs%20in%20DRL%20settings%20with%20%22amplified%22%0Anon-stationarity%20via%20multi-task%20training%2C%20providing%20further%20evidence%20that%20MoEs%0Aimprove%20learning%20capacity.%20In%20contrast%20to%20previous%20work%2C%20our%20multi-task%20results%0Aallow%20us%20to%20better%20understand%20the%20underlying%20causes%20for%20the%20beneficial%20effect%0Aof%20MoE%20in%20DRL%20training%2C%20the%20impact%20of%20the%20various%20MoE%20components%2C%20and%20insights%0Ainto%20how%20best%20to%20incorporate%20them%20in%20actor-critic-based%20DRL%20networks.%20Finally%2C%0Awe%20also%20confirm%20results%20from%20previous%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Experts%2520in%2520a%2520Mixture%2520of%2520RL%2520settings%26entry.906535625%3DTimon%2520Willi%2520and%2520Johan%2520Obando-Ceron%2520and%2520Jakob%2520Foerster%2520and%2520Karolina%2520Dziugaite%2520and%2520Pablo%2520Samuel%2520Castro%26entry.1292438233%3D%2520%2520Mixtures%2520of%2520Experts%2520%2528MoEs%2529%2520have%2520gained%2520prominence%2520in%2520%2528self-%2529supervised%250Alearning%2520due%2520to%2520their%2520enhanced%2520inference%2520efficiency%252C%2520adaptability%2520to%250Adistributed%2520training%252C%2520and%2520modularity.%2520Previous%2520research%2520has%2520illustrated%2520that%250AMoEs%2520can%2520significantly%2520boost%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520performance%2520by%250Aexpanding%2520the%2520network%2527s%2520parameter%2520count%2520while%2520reducing%2520dormant%2520neurons%252C%2520thereby%250Aenhancing%2520the%2520model%2527s%2520learning%2520capacity%2520and%2520ability%2520to%2520deal%2520with%250Anon-stationarity.%2520In%2520this%2520work%252C%2520we%2520shed%2520more%2520light%2520on%2520MoEs%2527%2520ability%2520to%2520deal%250Awith%2520non-stationarity%2520and%2520investigate%2520MoEs%2520in%2520DRL%2520settings%2520with%2520%2522amplified%2522%250Anon-stationarity%2520via%2520multi-task%2520training%252C%2520providing%2520further%2520evidence%2520that%2520MoEs%250Aimprove%2520learning%2520capacity.%2520In%2520contrast%2520to%2520previous%2520work%252C%2520our%2520multi-task%2520results%250Aallow%2520us%2520to%2520better%2520understand%2520the%2520underlying%2520causes%2520for%2520the%2520beneficial%2520effect%250Aof%2520MoE%2520in%2520DRL%2520training%252C%2520the%2520impact%2520of%2520the%2520various%2520MoE%2520components%252C%2520and%2520insights%250Ainto%2520how%2520best%2520to%2520incorporate%2520them%2520in%2520actor-critic-based%2520DRL%2520networks.%2520Finally%252C%250Awe%2520also%2520confirm%2520results%2520from%2520previous%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Experts%20in%20a%20Mixture%20of%20RL%20settings&entry.906535625=Timon%20Willi%20and%20Johan%20Obando-Ceron%20and%20Jakob%20Foerster%20and%20Karolina%20Dziugaite%20and%20Pablo%20Samuel%20Castro&entry.1292438233=%20%20Mixtures%20of%20Experts%20%28MoEs%29%20have%20gained%20prominence%20in%20%28self-%29supervised%0Alearning%20due%20to%20their%20enhanced%20inference%20efficiency%2C%20adaptability%20to%0Adistributed%20training%2C%20and%20modularity.%20Previous%20research%20has%20illustrated%20that%0AMoEs%20can%20significantly%20boost%20Deep%20Reinforcement%20Learning%20%28DRL%29%20performance%20by%0Aexpanding%20the%20network%27s%20parameter%20count%20while%20reducing%20dormant%20neurons%2C%20thereby%0Aenhancing%20the%20model%27s%20learning%20capacity%20and%20ability%20to%20deal%20with%0Anon-stationarity.%20In%20this%20work%2C%20we%20shed%20more%20light%20on%20MoEs%27%20ability%20to%20deal%0Awith%20non-stationarity%20and%20investigate%20MoEs%20in%20DRL%20settings%20with%20%22amplified%22%0Anon-stationarity%20via%20multi-task%20training%2C%20providing%20further%20evidence%20that%20MoEs%0Aimprove%20learning%20capacity.%20In%20contrast%20to%20previous%20work%2C%20our%20multi-task%20results%0Aallow%20us%20to%20better%20understand%20the%20underlying%20causes%20for%20the%20beneficial%20effect%0Aof%20MoE%20in%20DRL%20training%2C%20the%20impact%20of%20the%20various%20MoE%20components%2C%20and%20insights%0Ainto%20how%20best%20to%20incorporate%20them%20in%20actor-critic-based%20DRL%20networks.%20Finally%2C%0Awe%20also%20confirm%20results%20from%20previous%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18420v1&entry.124074799=Read"},
{"title": "DiarizationLM: Speaker Diarization Post-Processing with Large Language\n  Models", "author": "Quan Wang and Yiling Huang and Guanlong Zhao and Evan Clark and Wei Xia and Hank Liao", "abstract": "  In this paper, we introduce DiarizationLM, a framework to leverage large\nlanguage models (LLM) to post-process the outputs from a speaker diarization\nsystem. Various goals can be achieved with the proposed framework, such as\nimproving the readability of the diarized transcript, or reducing the word\ndiarization error rate (WDER). In this framework, the outputs of the automatic\nspeech recognition (ASR) and speaker diarization systems are represented as a\ncompact textual format, which is included in the prompt to an optionally\nfinetuned LLM. The outputs of the LLM can be used as the refined diarization\nresults with the desired enhancement. As a post-processing step, this framework\ncan be easily applied to any off-the-shelf ASR and speaker diarization systems\nwithout retraining existing components. Our experiments show that a finetuned\nPaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone\nconversation dataset, and rel. 44.9% on the Callhome English dataset.\n", "link": "http://arxiv.org/abs/2401.03506v6", "date": "2024-06-26", "relevancy": 1.3415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4601}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4396}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiarizationLM%3A%20Speaker%20Diarization%20Post-Processing%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20DiarizationLM%3A%20Speaker%20Diarization%20Post-Processing%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Quan%20Wang%20and%20Yiling%20Huang%20and%20Guanlong%20Zhao%20and%20Evan%20Clark%20and%20Wei%20Xia%20and%20Hank%20Liao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20DiarizationLM%2C%20a%20framework%20to%20leverage%20large%0Alanguage%20models%20%28LLM%29%20to%20post-process%20the%20outputs%20from%20a%20speaker%20diarization%0Asystem.%20Various%20goals%20can%20be%20achieved%20with%20the%20proposed%20framework%2C%20such%20as%0Aimproving%20the%20readability%20of%20the%20diarized%20transcript%2C%20or%20reducing%20the%20word%0Adiarization%20error%20rate%20%28WDER%29.%20In%20this%20framework%2C%20the%20outputs%20of%20the%20automatic%0Aspeech%20recognition%20%28ASR%29%20and%20speaker%20diarization%20systems%20are%20represented%20as%20a%0Acompact%20textual%20format%2C%20which%20is%20included%20in%20the%20prompt%20to%20an%20optionally%0Afinetuned%20LLM.%20The%20outputs%20of%20the%20LLM%20can%20be%20used%20as%20the%20refined%20diarization%0Aresults%20with%20the%20desired%20enhancement.%20As%20a%20post-processing%20step%2C%20this%20framework%0Acan%20be%20easily%20applied%20to%20any%20off-the-shelf%20ASR%20and%20speaker%20diarization%20systems%0Awithout%20retraining%20existing%20components.%20Our%20experiments%20show%20that%20a%20finetuned%0APaLM%202-S%20model%20can%20reduce%20the%20WDER%20by%20rel.%2055.5%25%20on%20the%20Fisher%20telephone%0Aconversation%20dataset%2C%20and%20rel.%2044.9%25%20on%20the%20Callhome%20English%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03506v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiarizationLM%253A%2520Speaker%2520Diarization%2520Post-Processing%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DQuan%2520Wang%2520and%2520Yiling%2520Huang%2520and%2520Guanlong%2520Zhao%2520and%2520Evan%2520Clark%2520and%2520Wei%2520Xia%2520and%2520Hank%2520Liao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DiarizationLM%252C%2520a%2520framework%2520to%2520leverage%2520large%250Alanguage%2520models%2520%2528LLM%2529%2520to%2520post-process%2520the%2520outputs%2520from%2520a%2520speaker%2520diarization%250Asystem.%2520Various%2520goals%2520can%2520be%2520achieved%2520with%2520the%2520proposed%2520framework%252C%2520such%2520as%250Aimproving%2520the%2520readability%2520of%2520the%2520diarized%2520transcript%252C%2520or%2520reducing%2520the%2520word%250Adiarization%2520error%2520rate%2520%2528WDER%2529.%2520In%2520this%2520framework%252C%2520the%2520outputs%2520of%2520the%2520automatic%250Aspeech%2520recognition%2520%2528ASR%2529%2520and%2520speaker%2520diarization%2520systems%2520are%2520represented%2520as%2520a%250Acompact%2520textual%2520format%252C%2520which%2520is%2520included%2520in%2520the%2520prompt%2520to%2520an%2520optionally%250Afinetuned%2520LLM.%2520The%2520outputs%2520of%2520the%2520LLM%2520can%2520be%2520used%2520as%2520the%2520refined%2520diarization%250Aresults%2520with%2520the%2520desired%2520enhancement.%2520As%2520a%2520post-processing%2520step%252C%2520this%2520framework%250Acan%2520be%2520easily%2520applied%2520to%2520any%2520off-the-shelf%2520ASR%2520and%2520speaker%2520diarization%2520systems%250Awithout%2520retraining%2520existing%2520components.%2520Our%2520experiments%2520show%2520that%2520a%2520finetuned%250APaLM%25202-S%2520model%2520can%2520reduce%2520the%2520WDER%2520by%2520rel.%252055.5%2525%2520on%2520the%2520Fisher%2520telephone%250Aconversation%2520dataset%252C%2520and%2520rel.%252044.9%2525%2520on%2520the%2520Callhome%2520English%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03506v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiarizationLM%3A%20Speaker%20Diarization%20Post-Processing%20with%20Large%20Language%0A%20%20Models&entry.906535625=Quan%20Wang%20and%20Yiling%20Huang%20and%20Guanlong%20Zhao%20and%20Evan%20Clark%20and%20Wei%20Xia%20and%20Hank%20Liao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20DiarizationLM%2C%20a%20framework%20to%20leverage%20large%0Alanguage%20models%20%28LLM%29%20to%20post-process%20the%20outputs%20from%20a%20speaker%20diarization%0Asystem.%20Various%20goals%20can%20be%20achieved%20with%20the%20proposed%20framework%2C%20such%20as%0Aimproving%20the%20readability%20of%20the%20diarized%20transcript%2C%20or%20reducing%20the%20word%0Adiarization%20error%20rate%20%28WDER%29.%20In%20this%20framework%2C%20the%20outputs%20of%20the%20automatic%0Aspeech%20recognition%20%28ASR%29%20and%20speaker%20diarization%20systems%20are%20represented%20as%20a%0Acompact%20textual%20format%2C%20which%20is%20included%20in%20the%20prompt%20to%20an%20optionally%0Afinetuned%20LLM.%20The%20outputs%20of%20the%20LLM%20can%20be%20used%20as%20the%20refined%20diarization%0Aresults%20with%20the%20desired%20enhancement.%20As%20a%20post-processing%20step%2C%20this%20framework%0Acan%20be%20easily%20applied%20to%20any%20off-the-shelf%20ASR%20and%20speaker%20diarization%20systems%0Awithout%20retraining%20existing%20components.%20Our%20experiments%20show%20that%20a%20finetuned%0APaLM%202-S%20model%20can%20reduce%20the%20WDER%20by%20rel.%2055.5%25%20on%20the%20Fisher%20telephone%0Aconversation%20dataset%2C%20and%20rel.%2044.9%25%20on%20the%20Callhome%20English%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03506v6&entry.124074799=Read"},
{"title": "Situational Awareness Matters in 3D Vision Language Reasoning", "author": "Yunze Man and Liang-Yan Gui and Yu-Xiong Wang", "abstract": "  Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.\n", "link": "http://arxiv.org/abs/2406.07544v2", "date": "2024-06-26", "relevancy": 1.6925, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6077}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5809}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Situational%20Awareness%20Matters%20in%203D%20Vision%20Language%20Reasoning&body=Title%3A%20Situational%20Awareness%20Matters%20in%203D%20Vision%20Language%20Reasoning%0AAuthor%3A%20Yunze%20Man%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20Being%20able%20to%20carry%20out%20complicated%20vision%20language%20reasoning%20tasks%20in%203D%0Aspace%20represents%20a%20significant%20milestone%20in%20developing%20household%20robots%20and%0Ahuman-centered%20embodied%20AI.%20In%20this%20work%2C%20we%20demonstrate%20that%20a%20critical%20and%0Adistinct%20challenge%20in%203D%20vision%20language%20reasoning%20is%20situational%20awareness%2C%0Awhich%20incorporates%20two%20key%20components%3A%20%281%29%20The%20autonomous%20agent%20grounds%20its%0Aself-location%20based%20on%20a%20language%20prompt.%20%282%29%20The%20agent%20answers%20open-ended%0Aquestions%20from%20the%20perspective%20of%20its%20calculated%20position.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20SIG3D%2C%20an%20end-to-end%20Situation-Grounded%20model%20for%203D%0Avision%20language%20reasoning.%20We%20tokenize%20the%203D%20scene%20into%20sparse%20voxel%0Arepresentation%20and%20propose%20a%20language-grounded%20situation%20estimator%2C%20followed%20by%0Aa%20situated%20question%20answering%20module.%20Experiments%20on%20the%20SQA3D%20and%20ScanQA%0Adatasets%20show%20that%20SIG3D%20outperforms%20state-of-the-art%20models%20in%20situation%0Aestimation%20and%20question%20answering%20by%20a%20large%20margin%20%28e.g.%2C%20an%20enhancement%20of%0Aover%2030%25%20on%20situation%20estimation%20accuracy%29.%20Subsequent%20analysis%20corroborates%0Aour%20architectural%20design%20choices%2C%20explores%20the%20distinct%20functions%20of%20visual%20and%0Atextual%20tokens%2C%20and%20highlights%20the%20importance%20of%20situational%20awareness%20in%20the%0Adomain%20of%203D%20question%20answering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSituational%2520Awareness%2520Matters%2520in%25203D%2520Vision%2520Language%2520Reasoning%26entry.906535625%3DYunze%2520Man%2520and%2520Liang-Yan%2520Gui%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520Being%2520able%2520to%2520carry%2520out%2520complicated%2520vision%2520language%2520reasoning%2520tasks%2520in%25203D%250Aspace%2520represents%2520a%2520significant%2520milestone%2520in%2520developing%2520household%2520robots%2520and%250Ahuman-centered%2520embodied%2520AI.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520a%2520critical%2520and%250Adistinct%2520challenge%2520in%25203D%2520vision%2520language%2520reasoning%2520is%2520situational%2520awareness%252C%250Awhich%2520incorporates%2520two%2520key%2520components%253A%2520%25281%2529%2520The%2520autonomous%2520agent%2520grounds%2520its%250Aself-location%2520based%2520on%2520a%2520language%2520prompt.%2520%25282%2529%2520The%2520agent%2520answers%2520open-ended%250Aquestions%2520from%2520the%2520perspective%2520of%2520its%2520calculated%2520position.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520introduce%2520SIG3D%252C%2520an%2520end-to-end%2520Situation-Grounded%2520model%2520for%25203D%250Avision%2520language%2520reasoning.%2520We%2520tokenize%2520the%25203D%2520scene%2520into%2520sparse%2520voxel%250Arepresentation%2520and%2520propose%2520a%2520language-grounded%2520situation%2520estimator%252C%2520followed%2520by%250Aa%2520situated%2520question%2520answering%2520module.%2520Experiments%2520on%2520the%2520SQA3D%2520and%2520ScanQA%250Adatasets%2520show%2520that%2520SIG3D%2520outperforms%2520state-of-the-art%2520models%2520in%2520situation%250Aestimation%2520and%2520question%2520answering%2520by%2520a%2520large%2520margin%2520%2528e.g.%252C%2520an%2520enhancement%2520of%250Aover%252030%2525%2520on%2520situation%2520estimation%2520accuracy%2529.%2520Subsequent%2520analysis%2520corroborates%250Aour%2520architectural%2520design%2520choices%252C%2520explores%2520the%2520distinct%2520functions%2520of%2520visual%2520and%250Atextual%2520tokens%252C%2520and%2520highlights%2520the%2520importance%2520of%2520situational%2520awareness%2520in%2520the%250Adomain%2520of%25203D%2520question%2520answering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Situational%20Awareness%20Matters%20in%203D%20Vision%20Language%20Reasoning&entry.906535625=Yunze%20Man%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20Being%20able%20to%20carry%20out%20complicated%20vision%20language%20reasoning%20tasks%20in%203D%0Aspace%20represents%20a%20significant%20milestone%20in%20developing%20household%20robots%20and%0Ahuman-centered%20embodied%20AI.%20In%20this%20work%2C%20we%20demonstrate%20that%20a%20critical%20and%0Adistinct%20challenge%20in%203D%20vision%20language%20reasoning%20is%20situational%20awareness%2C%0Awhich%20incorporates%20two%20key%20components%3A%20%281%29%20The%20autonomous%20agent%20grounds%20its%0Aself-location%20based%20on%20a%20language%20prompt.%20%282%29%20The%20agent%20answers%20open-ended%0Aquestions%20from%20the%20perspective%20of%20its%20calculated%20position.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20SIG3D%2C%20an%20end-to-end%20Situation-Grounded%20model%20for%203D%0Avision%20language%20reasoning.%20We%20tokenize%20the%203D%20scene%20into%20sparse%20voxel%0Arepresentation%20and%20propose%20a%20language-grounded%20situation%20estimator%2C%20followed%20by%0Aa%20situated%20question%20answering%20module.%20Experiments%20on%20the%20SQA3D%20and%20ScanQA%0Adatasets%20show%20that%20SIG3D%20outperforms%20state-of-the-art%20models%20in%20situation%0Aestimation%20and%20question%20answering%20by%20a%20large%20margin%20%28e.g.%2C%20an%20enhancement%20of%0Aover%2030%25%20on%20situation%20estimation%20accuracy%29.%20Subsequent%20analysis%20corroborates%0Aour%20architectural%20design%20choices%2C%20explores%20the%20distinct%20functions%20of%20visual%20and%0Atextual%20tokens%2C%20and%20highlights%20the%20importance%20of%20situational%20awareness%20in%20the%0Adomain%20of%203D%20question%20answering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07544v2&entry.124074799=Read"},
{"title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language\n  Model", "author": "Kenneth Li and Oam Patel and Fernanda Vi\u00e9gas and Hanspeter Pfister and Martin Wattenberg", "abstract": "  We introduce Inference-Time Intervention (ITI), a technique designed to\nenhance the \"truthfulness\" of large language models (LLMs). ITI operates by\nshifting model activations during inference, following a set of directions\nacross a limited number of attention heads. This intervention significantly\nimproves the performance of LLaMA models on the TruthfulQA benchmark. On an\ninstruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from\n32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and\ndemonstrate how to balance it by tuning the intervention strength. ITI is\nminimally invasive and computationally inexpensive. Moreover, the technique is\ndata efficient: while approaches like RLHF require extensive annotations, ITI\nlocates truthful directions using only few hundred examples. Our findings\nsuggest that LLMs may have an internal representation of the likelihood of\nsomething being true, even as they produce falsehoods on the surface.\n", "link": "http://arxiv.org/abs/2306.03341v6", "date": "2024-06-26", "relevancy": 1.7583, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4798}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4389}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-Time%20Intervention%3A%20Eliciting%20Truthful%20Answers%20from%20a%20Language%0A%20%20Model&body=Title%3A%20Inference-Time%20Intervention%3A%20Eliciting%20Truthful%20Answers%20from%20a%20Language%0A%20%20Model%0AAuthor%3A%20Kenneth%20Li%20and%20Oam%20Patel%20and%20Fernanda%20Vi%C3%A9gas%20and%20Hanspeter%20Pfister%20and%20Martin%20Wattenberg%0AAbstract%3A%20%20%20We%20introduce%20Inference-Time%20Intervention%20%28ITI%29%2C%20a%20technique%20designed%20to%0Aenhance%20the%20%22truthfulness%22%20of%20large%20language%20models%20%28LLMs%29.%20ITI%20operates%20by%0Ashifting%20model%20activations%20during%20inference%2C%20following%20a%20set%20of%20directions%0Aacross%20a%20limited%20number%20of%20attention%20heads.%20This%20intervention%20significantly%0Aimproves%20the%20performance%20of%20LLaMA%20models%20on%20the%20TruthfulQA%20benchmark.%20On%20an%0Ainstruction-finetuned%20LLaMA%20called%20Alpaca%2C%20ITI%20improves%20its%20truthfulness%20from%0A32.5%25%20to%2065.1%25.%20We%20identify%20a%20tradeoff%20between%20truthfulness%20and%20helpfulness%20and%0Ademonstrate%20how%20to%20balance%20it%20by%20tuning%20the%20intervention%20strength.%20ITI%20is%0Aminimally%20invasive%20and%20computationally%20inexpensive.%20Moreover%2C%20the%20technique%20is%0Adata%20efficient%3A%20while%20approaches%20like%20RLHF%20require%20extensive%20annotations%2C%20ITI%0Alocates%20truthful%20directions%20using%20only%20few%20hundred%20examples.%20Our%20findings%0Asuggest%20that%20LLMs%20may%20have%20an%20internal%20representation%20of%20the%20likelihood%20of%0Asomething%20being%20true%2C%20even%20as%20they%20produce%20falsehoods%20on%20the%20surface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.03341v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-Time%2520Intervention%253A%2520Eliciting%2520Truthful%2520Answers%2520from%2520a%2520Language%250A%2520%2520Model%26entry.906535625%3DKenneth%2520Li%2520and%2520Oam%2520Patel%2520and%2520Fernanda%2520Vi%25C3%25A9gas%2520and%2520Hanspeter%2520Pfister%2520and%2520Martin%2520Wattenberg%26entry.1292438233%3D%2520%2520We%2520introduce%2520Inference-Time%2520Intervention%2520%2528ITI%2529%252C%2520a%2520technique%2520designed%2520to%250Aenhance%2520the%2520%2522truthfulness%2522%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520ITI%2520operates%2520by%250Ashifting%2520model%2520activations%2520during%2520inference%252C%2520following%2520a%2520set%2520of%2520directions%250Aacross%2520a%2520limited%2520number%2520of%2520attention%2520heads.%2520This%2520intervention%2520significantly%250Aimproves%2520the%2520performance%2520of%2520LLaMA%2520models%2520on%2520the%2520TruthfulQA%2520benchmark.%2520On%2520an%250Ainstruction-finetuned%2520LLaMA%2520called%2520Alpaca%252C%2520ITI%2520improves%2520its%2520truthfulness%2520from%250A32.5%2525%2520to%252065.1%2525.%2520We%2520identify%2520a%2520tradeoff%2520between%2520truthfulness%2520and%2520helpfulness%2520and%250Ademonstrate%2520how%2520to%2520balance%2520it%2520by%2520tuning%2520the%2520intervention%2520strength.%2520ITI%2520is%250Aminimally%2520invasive%2520and%2520computationally%2520inexpensive.%2520Moreover%252C%2520the%2520technique%2520is%250Adata%2520efficient%253A%2520while%2520approaches%2520like%2520RLHF%2520require%2520extensive%2520annotations%252C%2520ITI%250Alocates%2520truthful%2520directions%2520using%2520only%2520few%2520hundred%2520examples.%2520Our%2520findings%250Asuggest%2520that%2520LLMs%2520may%2520have%2520an%2520internal%2520representation%2520of%2520the%2520likelihood%2520of%250Asomething%2520being%2520true%252C%2520even%2520as%2520they%2520produce%2520falsehoods%2520on%2520the%2520surface.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.03341v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-Time%20Intervention%3A%20Eliciting%20Truthful%20Answers%20from%20a%20Language%0A%20%20Model&entry.906535625=Kenneth%20Li%20and%20Oam%20Patel%20and%20Fernanda%20Vi%C3%A9gas%20and%20Hanspeter%20Pfister%20and%20Martin%20Wattenberg&entry.1292438233=%20%20We%20introduce%20Inference-Time%20Intervention%20%28ITI%29%2C%20a%20technique%20designed%20to%0Aenhance%20the%20%22truthfulness%22%20of%20large%20language%20models%20%28LLMs%29.%20ITI%20operates%20by%0Ashifting%20model%20activations%20during%20inference%2C%20following%20a%20set%20of%20directions%0Aacross%20a%20limited%20number%20of%20attention%20heads.%20This%20intervention%20significantly%0Aimproves%20the%20performance%20of%20LLaMA%20models%20on%20the%20TruthfulQA%20benchmark.%20On%20an%0Ainstruction-finetuned%20LLaMA%20called%20Alpaca%2C%20ITI%20improves%20its%20truthfulness%20from%0A32.5%25%20to%2065.1%25.%20We%20identify%20a%20tradeoff%20between%20truthfulness%20and%20helpfulness%20and%0Ademonstrate%20how%20to%20balance%20it%20by%20tuning%20the%20intervention%20strength.%20ITI%20is%0Aminimally%20invasive%20and%20computationally%20inexpensive.%20Moreover%2C%20the%20technique%20is%0Adata%20efficient%3A%20while%20approaches%20like%20RLHF%20require%20extensive%20annotations%2C%20ITI%0Alocates%20truthful%20directions%20using%20only%20few%20hundred%20examples.%20Our%20findings%0Asuggest%20that%20LLMs%20may%20have%20an%20internal%20representation%20of%20the%20likelihood%20of%0Asomething%20being%20true%2C%20even%20as%20they%20produce%20falsehoods%20on%20the%20surface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.03341v6&entry.124074799=Read"},
{"title": "Normalizing Flows for Conformal Regression", "author": "Nicolo Colombo", "abstract": "  Conformal Prediction (CP) algorithms estimate the uncertainty of a prediction\nmodel by calibrating its outputs on labeled data. The same calibration scheme\nusually applies to any model and data without modifications. The obtained\nprediction intervals are valid by construction but could be inefficient, i.e.\nunnecessarily big, if the prediction errors are not uniformly distributed over\nthe input space.\n  We present a general scheme to localize the intervals by training the\ncalibration process. The standard prediction error is replaced by an optimized\ndistance metric that depends explicitly on the object attributes. Learning the\noptimal metric is equivalent to training a Normalizing Flow that acts on the\njoint distribution of the errors and the inputs. Unlike the Error Reweighting\nCP algorithm of Papadopoulos et al. (2008), the framework allows estimating the\ngap between nominal and empirical conditional validity. The approach is\ncompatible with existing locally-adaptive CP strategies based on re-weighting\nthe calibration samples and applies to any point-prediction model without\nretraining.\n", "link": "http://arxiv.org/abs/2406.03346v2", "date": "2024-06-26", "relevancy": 1.4967, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5407}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4926}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normalizing%20Flows%20for%20Conformal%20Regression&body=Title%3A%20Normalizing%20Flows%20for%20Conformal%20Regression%0AAuthor%3A%20Nicolo%20Colombo%0AAbstract%3A%20%20%20Conformal%20Prediction%20%28CP%29%20algorithms%20estimate%20the%20uncertainty%20of%20a%20prediction%0Amodel%20by%20calibrating%20its%20outputs%20on%20labeled%20data.%20The%20same%20calibration%20scheme%0Ausually%20applies%20to%20any%20model%20and%20data%20without%20modifications.%20The%20obtained%0Aprediction%20intervals%20are%20valid%20by%20construction%20but%20could%20be%20inefficient%2C%20i.e.%0Aunnecessarily%20big%2C%20if%20the%20prediction%20errors%20are%20not%20uniformly%20distributed%20over%0Athe%20input%20space.%0A%20%20We%20present%20a%20general%20scheme%20to%20localize%20the%20intervals%20by%20training%20the%0Acalibration%20process.%20The%20standard%20prediction%20error%20is%20replaced%20by%20an%20optimized%0Adistance%20metric%20that%20depends%20explicitly%20on%20the%20object%20attributes.%20Learning%20the%0Aoptimal%20metric%20is%20equivalent%20to%20training%20a%20Normalizing%20Flow%20that%20acts%20on%20the%0Ajoint%20distribution%20of%20the%20errors%20and%20the%20inputs.%20Unlike%20the%20Error%20Reweighting%0ACP%20algorithm%20of%20Papadopoulos%20et%20al.%20%282008%29%2C%20the%20framework%20allows%20estimating%20the%0Agap%20between%20nominal%20and%20empirical%20conditional%20validity.%20The%20approach%20is%0Acompatible%20with%20existing%20locally-adaptive%20CP%20strategies%20based%20on%20re-weighting%0Athe%20calibration%20samples%20and%20applies%20to%20any%20point-prediction%20model%20without%0Aretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalizing%2520Flows%2520for%2520Conformal%2520Regression%26entry.906535625%3DNicolo%2520Colombo%26entry.1292438233%3D%2520%2520Conformal%2520Prediction%2520%2528CP%2529%2520algorithms%2520estimate%2520the%2520uncertainty%2520of%2520a%2520prediction%250Amodel%2520by%2520calibrating%2520its%2520outputs%2520on%2520labeled%2520data.%2520The%2520same%2520calibration%2520scheme%250Ausually%2520applies%2520to%2520any%2520model%2520and%2520data%2520without%2520modifications.%2520The%2520obtained%250Aprediction%2520intervals%2520are%2520valid%2520by%2520construction%2520but%2520could%2520be%2520inefficient%252C%2520i.e.%250Aunnecessarily%2520big%252C%2520if%2520the%2520prediction%2520errors%2520are%2520not%2520uniformly%2520distributed%2520over%250Athe%2520input%2520space.%250A%2520%2520We%2520present%2520a%2520general%2520scheme%2520to%2520localize%2520the%2520intervals%2520by%2520training%2520the%250Acalibration%2520process.%2520The%2520standard%2520prediction%2520error%2520is%2520replaced%2520by%2520an%2520optimized%250Adistance%2520metric%2520that%2520depends%2520explicitly%2520on%2520the%2520object%2520attributes.%2520Learning%2520the%250Aoptimal%2520metric%2520is%2520equivalent%2520to%2520training%2520a%2520Normalizing%2520Flow%2520that%2520acts%2520on%2520the%250Ajoint%2520distribution%2520of%2520the%2520errors%2520and%2520the%2520inputs.%2520Unlike%2520the%2520Error%2520Reweighting%250ACP%2520algorithm%2520of%2520Papadopoulos%2520et%2520al.%2520%25282008%2529%252C%2520the%2520framework%2520allows%2520estimating%2520the%250Agap%2520between%2520nominal%2520and%2520empirical%2520conditional%2520validity.%2520The%2520approach%2520is%250Acompatible%2520with%2520existing%2520locally-adaptive%2520CP%2520strategies%2520based%2520on%2520re-weighting%250Athe%2520calibration%2520samples%2520and%2520applies%2520to%2520any%2520point-prediction%2520model%2520without%250Aretraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalizing%20Flows%20for%20Conformal%20Regression&entry.906535625=Nicolo%20Colombo&entry.1292438233=%20%20Conformal%20Prediction%20%28CP%29%20algorithms%20estimate%20the%20uncertainty%20of%20a%20prediction%0Amodel%20by%20calibrating%20its%20outputs%20on%20labeled%20data.%20The%20same%20calibration%20scheme%0Ausually%20applies%20to%20any%20model%20and%20data%20without%20modifications.%20The%20obtained%0Aprediction%20intervals%20are%20valid%20by%20construction%20but%20could%20be%20inefficient%2C%20i.e.%0Aunnecessarily%20big%2C%20if%20the%20prediction%20errors%20are%20not%20uniformly%20distributed%20over%0Athe%20input%20space.%0A%20%20We%20present%20a%20general%20scheme%20to%20localize%20the%20intervals%20by%20training%20the%0Acalibration%20process.%20The%20standard%20prediction%20error%20is%20replaced%20by%20an%20optimized%0Adistance%20metric%20that%20depends%20explicitly%20on%20the%20object%20attributes.%20Learning%20the%0Aoptimal%20metric%20is%20equivalent%20to%20training%20a%20Normalizing%20Flow%20that%20acts%20on%20the%0Ajoint%20distribution%20of%20the%20errors%20and%20the%20inputs.%20Unlike%20the%20Error%20Reweighting%0ACP%20algorithm%20of%20Papadopoulos%20et%20al.%20%282008%29%2C%20the%20framework%20allows%20estimating%20the%0Agap%20between%20nominal%20and%20empirical%20conditional%20validity.%20The%20approach%20is%0Acompatible%20with%20existing%20locally-adaptive%20CP%20strategies%20based%20on%20re-weighting%0Athe%20calibration%20samples%20and%20applies%20to%20any%20point-prediction%20model%20without%0Aretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03346v2&entry.124074799=Read"},
{"title": "Adversarial Search Engine Optimization for Large Language Models", "author": "Fredrik Nestaas and Edoardo Debenedetti and Florian Tram\u00e8r", "abstract": "  Large Language Models (LLMs) are increasingly used in applications where the\nmodel selects from competing third-party content, such as in LLM-powered search\nengines or chatbot plugins. In this paper, we introduce Preference Manipulation\nAttacks, a new class of attacks that manipulate an LLM's selections to favor\nthe attacker. We demonstrate that carefully crafted website content or plugin\ndocumentations can trick an LLM to promote the attacker products and discredit\ncompetitors, thereby increasing user traffic and monetization. We show this\nleads to a prisoner's dilemma, where all parties are incentivized to launch\nattacks, but the collective effect degrades the LLM's outputs for everyone. We\ndemonstrate our attacks on production LLM search engines (Bing and Perplexity)\nand plugin APIs (for GPT-4 and Claude). As LLMs are increasingly used to rank\nthird-party content, we expect Preference Manipulation Attacks to emerge as a\nsignificant threat.\n", "link": "http://arxiv.org/abs/2406.18382v1", "date": "2024-06-26", "relevancy": 1.7811, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4374}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Search%20Engine%20Optimization%20for%20Large%20Language%20Models&body=Title%3A%20Adversarial%20Search%20Engine%20Optimization%20for%20Large%20Language%20Models%0AAuthor%3A%20Fredrik%20Nestaas%20and%20Edoardo%20Debenedetti%20and%20Florian%20Tram%C3%A8r%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20applications%20where%20the%0Amodel%20selects%20from%20competing%20third-party%20content%2C%20such%20as%20in%20LLM-powered%20search%0Aengines%20or%20chatbot%20plugins.%20In%20this%20paper%2C%20we%20introduce%20Preference%20Manipulation%0AAttacks%2C%20a%20new%20class%20of%20attacks%20that%20manipulate%20an%20LLM%27s%20selections%20to%20favor%0Athe%20attacker.%20We%20demonstrate%20that%20carefully%20crafted%20website%20content%20or%20plugin%0Adocumentations%20can%20trick%20an%20LLM%20to%20promote%20the%20attacker%20products%20and%20discredit%0Acompetitors%2C%20thereby%20increasing%20user%20traffic%20and%20monetization.%20We%20show%20this%0Aleads%20to%20a%20prisoner%27s%20dilemma%2C%20where%20all%20parties%20are%20incentivized%20to%20launch%0Aattacks%2C%20but%20the%20collective%20effect%20degrades%20the%20LLM%27s%20outputs%20for%20everyone.%20We%0Ademonstrate%20our%20attacks%20on%20production%20LLM%20search%20engines%20%28Bing%20and%20Perplexity%29%0Aand%20plugin%20APIs%20%28for%20GPT-4%20and%20Claude%29.%20As%20LLMs%20are%20increasingly%20used%20to%20rank%0Athird-party%20content%2C%20we%20expect%20Preference%20Manipulation%20Attacks%20to%20emerge%20as%20a%0Asignificant%20threat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Search%2520Engine%2520Optimization%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DFredrik%2520Nestaas%2520and%2520Edoardo%2520Debenedetti%2520and%2520Florian%2520Tram%25C3%25A8r%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520in%2520applications%2520where%2520the%250Amodel%2520selects%2520from%2520competing%2520third-party%2520content%252C%2520such%2520as%2520in%2520LLM-powered%2520search%250Aengines%2520or%2520chatbot%2520plugins.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Preference%2520Manipulation%250AAttacks%252C%2520a%2520new%2520class%2520of%2520attacks%2520that%2520manipulate%2520an%2520LLM%2527s%2520selections%2520to%2520favor%250Athe%2520attacker.%2520We%2520demonstrate%2520that%2520carefully%2520crafted%2520website%2520content%2520or%2520plugin%250Adocumentations%2520can%2520trick%2520an%2520LLM%2520to%2520promote%2520the%2520attacker%2520products%2520and%2520discredit%250Acompetitors%252C%2520thereby%2520increasing%2520user%2520traffic%2520and%2520monetization.%2520We%2520show%2520this%250Aleads%2520to%2520a%2520prisoner%2527s%2520dilemma%252C%2520where%2520all%2520parties%2520are%2520incentivized%2520to%2520launch%250Aattacks%252C%2520but%2520the%2520collective%2520effect%2520degrades%2520the%2520LLM%2527s%2520outputs%2520for%2520everyone.%2520We%250Ademonstrate%2520our%2520attacks%2520on%2520production%2520LLM%2520search%2520engines%2520%2528Bing%2520and%2520Perplexity%2529%250Aand%2520plugin%2520APIs%2520%2528for%2520GPT-4%2520and%2520Claude%2529.%2520As%2520LLMs%2520are%2520increasingly%2520used%2520to%2520rank%250Athird-party%2520content%252C%2520we%2520expect%2520Preference%2520Manipulation%2520Attacks%2520to%2520emerge%2520as%2520a%250Asignificant%2520threat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Search%20Engine%20Optimization%20for%20Large%20Language%20Models&entry.906535625=Fredrik%20Nestaas%20and%20Edoardo%20Debenedetti%20and%20Florian%20Tram%C3%A8r&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20applications%20where%20the%0Amodel%20selects%20from%20competing%20third-party%20content%2C%20such%20as%20in%20LLM-powered%20search%0Aengines%20or%20chatbot%20plugins.%20In%20this%20paper%2C%20we%20introduce%20Preference%20Manipulation%0AAttacks%2C%20a%20new%20class%20of%20attacks%20that%20manipulate%20an%20LLM%27s%20selections%20to%20favor%0Athe%20attacker.%20We%20demonstrate%20that%20carefully%20crafted%20website%20content%20or%20plugin%0Adocumentations%20can%20trick%20an%20LLM%20to%20promote%20the%20attacker%20products%20and%20discredit%0Acompetitors%2C%20thereby%20increasing%20user%20traffic%20and%20monetization.%20We%20show%20this%0Aleads%20to%20a%20prisoner%27s%20dilemma%2C%20where%20all%20parties%20are%20incentivized%20to%20launch%0Aattacks%2C%20but%20the%20collective%20effect%20degrades%20the%20LLM%27s%20outputs%20for%20everyone.%20We%0Ademonstrate%20our%20attacks%20on%20production%20LLM%20search%20engines%20%28Bing%20and%20Perplexity%29%0Aand%20plugin%20APIs%20%28for%20GPT-4%20and%20Claude%29.%20As%20LLMs%20are%20increasingly%20used%20to%20rank%0Athird-party%20content%2C%20we%20expect%20Preference%20Manipulation%20Attacks%20to%20emerge%20as%20a%0Asignificant%20threat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18382v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


