<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "G3DR: Generative 3D Reconstruction in ImageNet", "author": "Pradyumna Reddy and Ismail Elezi and Jiankang Deng", "abstract": "  We introduce a novel 3D generative method, Generative 3D Reconstruction\n(G3DR) in ImageNet, capable of generating diverse and high-quality 3D objects\nfrom single images, addressing the limitations of existing methods. At the\nheart of our framework is a novel depth regularization technique that enables\nthe generation of scenes with high-geometric fidelity. G3DR also leverages a\npretrained language-vision model, such as CLIP, to enable reconstruction in\nnovel views and improve the visual realism of generations. Additionally, G3DR\ndesigns a simple but effective sampling procedure to further improve the\nquality of generations. G3DR offers diverse and efficient 3D asset generation\nbased on class or text conditioning. Despite its simplicity, G3DR is able to\nbeat state-of-theart methods, improving over them by up to 22% in perceptual\nmetrics and 90% in geometry scores, while needing only half of the training\ntime. Code is available at https://github.com/preddy5/G3DR\n", "link": "http://arxiv.org/abs/2403.00939v1", "date": "2024-03-01", "relevancy": 2.9224, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5978}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5974}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5583}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G3DR%3A%20Generative%203D%20Reconstruction%20in%20ImageNet&entry.906535625=Pradyumna%20Reddy%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng&entry.1292438233=%20%20We%20introduce%20a%20novel%203D%20generative%20method%2C%20Generative%203D%20Reconstruction%0A%28G3DR%29%20in%20ImageNet%2C%20capable%20of%20generating%20diverse%20and%20high-quality%203D%20objects%0Afrom%20single%20images%2C%20addressing%20the%20limitations%20of%20existing%20methods.%20At%20the%0Aheart%20of%20our%20framework%20is%20a%20novel%20depth%20regularization%20technique%20that%20enables%0Athe%20generation%20of%20scenes%20with%20high-geometric%20fidelity.%20G3DR%20also%20leverages%20a%0Apretrained%20language-vision%20model%2C%20such%20as%20CLIP%2C%20to%20enable%20reconstruction%20in%0Anovel%20views%20and%20improve%20the%20visual%20realism%20of%20generations.%20Additionally%2C%20G3DR%0Adesigns%20a%20simple%20but%20effective%20sampling%20procedure%20to%20further%20improve%20the%0Aquality%20of%20generations.%20G3DR%20offers%20diverse%20and%20efficient%203D%20asset%20generation%0Abased%20on%20class%20or%20text%20conditioning.%20Despite%20its%20simplicity%2C%20G3DR%20is%20able%20to%0Abeat%20state-of-theart%20methods%2C%20improving%20over%20them%20by%20up%20to%2022%25%20in%20perceptual%0Ametrics%20and%2090%25%20in%20geometry%20scores%2C%20while%20needing%20only%20half%20of%20the%20training%0Atime.%20Code%20is%20available%20at%20https%3A//github.com/preddy5/G3DR%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00939v1&entry.124074799=Read"},
{"title": "Continuous Pose for Monocular Cameras in Neural Implicit Representation", "author": "Qi Ma and Danda Pani Paudel and Ajad Chhatkuli and Luc Van Gool", "abstract": "  In this paper, we showcase the effectiveness of optimizing monocular camera\nposes as a continuous function of time. The camera poses are represented using\nan implicit neural function which maps the given time to the corresponding\ncamera pose. The mapped camera poses are then used for the downstream tasks\nwhere joint camera pose optimization is also required. While doing so, the\nnetwork parameters -- that implicitly represent camera poses -- are optimized.\nWe exploit the proposed method in four diverse experimental settings, namely,\n(1) NeRF from noisy poses; (2) NeRF from asynchronous Events; (3) Visual\nSimultaneous Localization and Mapping (vSLAM); and (4) vSLAM with IMUs. In all\nfour settings, the proposed method performs significantly better than the\ncompared baselines and the state-of-the-art methods. Additionally, using the\nassumption of continuous motion, changes in pose may actually live in a\nmanifold that has lower than 6 degrees of freedom (DOF) is also realized. We\ncall this low DOF motion representation as the \\emph{intrinsic motion} and use\nthe approach in vSLAM settings, showing impressive camera tracking performance.\n", "link": "http://arxiv.org/abs/2311.17119v3", "date": "2024-03-02", "relevancy": 2.8865, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5861}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5845}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5613}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Pose%20for%20Monocular%20Cameras%20in%20Neural%20Implicit%20Representation&entry.906535625=Qi%20Ma%20and%20Danda%20Pani%20Paudel%20and%20Ajad%20Chhatkuli%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20In%20this%20paper%2C%20we%20showcase%20the%20effectiveness%20of%20optimizing%20monocular%20camera%0Aposes%20as%20a%20continuous%20function%20of%20time.%20The%20camera%20poses%20are%20represented%20using%0Aan%20implicit%20neural%20function%20which%20maps%20the%20given%20time%20to%20the%20corresponding%0Acamera%20pose.%20The%20mapped%20camera%20poses%20are%20then%20used%20for%20the%20downstream%20tasks%0Awhere%20joint%20camera%20pose%20optimization%20is%20also%20required.%20While%20doing%20so%2C%20the%0Anetwork%20parameters%20--%20that%20implicitly%20represent%20camera%20poses%20--%20are%20optimized.%0AWe%20exploit%20the%20proposed%20method%20in%20four%20diverse%20experimental%20settings%2C%20namely%2C%0A%281%29%20NeRF%20from%20noisy%20poses%3B%20%282%29%20NeRF%20from%20asynchronous%20Events%3B%20%283%29%20Visual%0ASimultaneous%20Localization%20and%20Mapping%20%28vSLAM%29%3B%20and%20%284%29%20vSLAM%20with%20IMUs.%20In%20all%0Afour%20settings%2C%20the%20proposed%20method%20performs%20significantly%20better%20than%20the%0Acompared%20baselines%20and%20the%20state-of-the-art%20methods.%20Additionally%2C%20using%20the%0Aassumption%20of%20continuous%20motion%2C%20changes%20in%20pose%20may%20actually%20live%20in%20a%0Amanifold%20that%20has%20lower%20than%206%20degrees%20of%20freedom%20%28DOF%29%20is%20also%20realized.%20We%0Acall%20this%20low%20DOF%20motion%20representation%20as%20the%20%5Cemph%7Bintrinsic%20motion%7D%20and%20use%0Athe%20approach%20in%20vSLAM%20settings%2C%20showing%20impressive%20camera%20tracking%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17119v3&entry.124074799=Read"},
{"title": "CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly\n  Detection", "author": "Xuhai Chen and Jiangning Zhang and Guanzhong Tian and Haoyang He and Wuhao Zhang and Yabiao Wang and Chengjie Wang and Yong Liu", "abstract": "  This paper considers zero-shot Anomaly Detection (AD), performing AD without\nreference images of the test objects. We propose a framework called CLIP-AD to\nleverage the zero-shot capabilities of the large vision-language model CLIP.\nFirstly, we reinterpret the text prompts design from a distributional\nperspective and propose a Representative Vector Selection (RVS) paradigm to\nobtain improved text features. Secondly, we note opposite predictions and\nirrelevant highlights in the direct computation of the anomaly maps. To address\nthese issues, we introduce a Staged Dual-Path model (SDP) that leverages\nfeatures from various levels and applies architecture and feature surgery.\nLastly, delving deeply into the two phenomena, we point out that the image and\ntext features are not aligned in the joint embedding space. Thus, we introduce\na fine-tuning strategy by adding linear layers and construct an extended model\nSDP+, further enhancing the performance. Abundant experiments demonstrate the\neffectiveness of our approach, e.g., on MVTec-AD, SDP outperforms the SOTA\nWinCLIP by +4.2/+10.7 in segmentation metrics F1-max/PRO, while SDP+ achieves\n+8.3/+20.5 improvements.\n", "link": "http://arxiv.org/abs/2311.00453v2", "date": "2024-03-02", "relevancy": 2.8353, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6011}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5529}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5472}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-AD%3A%20A%20Language-Guided%20Staged%20Dual-Path%20Model%20for%20Zero-shot%20Anomaly%0A%20%20Detection&entry.906535625=Xuhai%20Chen%20and%20Jiangning%20Zhang%20and%20Guanzhong%20Tian%20and%20Haoyang%20He%20and%20Wuhao%20Zhang%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Yong%20Liu&entry.1292438233=%20%20This%20paper%20considers%20zero-shot%20Anomaly%20Detection%20%28AD%29%2C%20performing%20AD%20without%0Areference%20images%20of%20the%20test%20objects.%20We%20propose%20a%20framework%20called%20CLIP-AD%20to%0Aleverage%20the%20zero-shot%20capabilities%20of%20the%20large%20vision-language%20model%20CLIP.%0AFirstly%2C%20we%20reinterpret%20the%20text%20prompts%20design%20from%20a%20distributional%0Aperspective%20and%20propose%20a%20Representative%20Vector%20Selection%20%28RVS%29%20paradigm%20to%0Aobtain%20improved%20text%20features.%20Secondly%2C%20we%20note%20opposite%20predictions%20and%0Airrelevant%20highlights%20in%20the%20direct%20computation%20of%20the%20anomaly%20maps.%20To%20address%0Athese%20issues%2C%20we%20introduce%20a%20Staged%20Dual-Path%20model%20%28SDP%29%20that%20leverages%0Afeatures%20from%20various%20levels%20and%20applies%20architecture%20and%20feature%20surgery.%0ALastly%2C%20delving%20deeply%20into%20the%20two%20phenomena%2C%20we%20point%20out%20that%20the%20image%20and%0Atext%20features%20are%20not%20aligned%20in%20the%20joint%20embedding%20space.%20Thus%2C%20we%20introduce%0Aa%20fine-tuning%20strategy%20by%20adding%20linear%20layers%20and%20construct%20an%20extended%20model%0ASDP%2B%2C%20further%20enhancing%20the%20performance.%20Abundant%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%2C%20e.g.%2C%20on%20MVTec-AD%2C%20SDP%20outperforms%20the%20SOTA%0AWinCLIP%20by%20%2B4.2/%2B10.7%20in%20segmentation%20metrics%20F1-max/PRO%2C%20while%20SDP%2B%20achieves%0A%2B8.3/%2B20.5%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00453v2&entry.124074799=Read"},
{"title": "GCD-DDPM: A Generative Change Detection Model Based on\n  Difference-Feature Guided DDPM", "author": "Yihan Wen and Xianping Ma and Xiaokang Zhang and Man-On Pun", "abstract": "  Deep learning (DL)-based methods have recently shown great promise in\nbitemporal change detection (CD). Existing discriminative methods based on\nConvolutional Neural Networks (CNNs) and Transformers rely on discriminative\nrepresentation learning for change recognition while struggling with exploring\nlocal and long-range contextual dependencies. As a result, it is still\nchallenging to obtain fine-grained and robust CD maps in diverse ground scenes.\nTo cope with this challenge, this work proposes a generative change detection\nmodel called GCD-DDPM to directly generate CD maps by exploiting the Denoising\nDiffusion Probabilistic Model (DDPM), instead of classifying each pixel into\nchanged or unchanged categories. Furthermore, the Difference Conditional\nEncoder (DCE), is designed to guide the generation of CD maps by exploiting\nmulti-level difference features. Leveraging the variational inference (VI)\nprocedure, GCD-DDPM can adaptively re-calibrate the CD results through an\niterative inference process, while accurately distinguishing subtle and\nirregular changes in diverse scenes. Finally, a Noise Suppression-based\nSemantic Enhancer (NSSE) is specifically designed to mitigate noise in the\ncurrent step's change-aware feature representations from the CD Encoder. This\nrefinement, serving as an attention map, can guide subsequent iterations while\nenhancing CD accuracy. Extensive experiments on four high-resolution CD\ndatasets confirm the superior performance of the proposed GCD-DDPM. The code\nfor this work will be available at https://github.com/udrs/GCD.\n", "link": "http://arxiv.org/abs/2306.03424v4", "date": "2024-03-02", "relevancy": 2.8159, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5675}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5648}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5572}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCD-DDPM%3A%20A%20Generative%20Change%20Detection%20Model%20Based%20on%0A%20%20Difference-Feature%20Guided%20DDPM&entry.906535625=Yihan%20Wen%20and%20Xianping%20Ma%20and%20Xiaokang%20Zhang%20and%20Man-On%20Pun&entry.1292438233=%20%20Deep%20learning%20%28DL%29-based%20methods%20have%20recently%20shown%20great%20promise%20in%0Abitemporal%20change%20detection%20%28CD%29.%20Existing%20discriminative%20methods%20based%20on%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformers%20rely%20on%20discriminative%0Arepresentation%20learning%20for%20change%20recognition%20while%20struggling%20with%20exploring%0Alocal%20and%20long-range%20contextual%20dependencies.%20As%20a%20result%2C%20it%20is%20still%0Achallenging%20to%20obtain%20fine-grained%20and%20robust%20CD%20maps%20in%20diverse%20ground%20scenes.%0ATo%20cope%20with%20this%20challenge%2C%20this%20work%20proposes%20a%20generative%20change%20detection%0Amodel%20called%20GCD-DDPM%20to%20directly%20generate%20CD%20maps%20by%20exploiting%20the%20Denoising%0ADiffusion%20Probabilistic%20Model%20%28DDPM%29%2C%20instead%20of%20classifying%20each%20pixel%20into%0Achanged%20or%20unchanged%20categories.%20Furthermore%2C%20the%20Difference%20Conditional%0AEncoder%20%28DCE%29%2C%20is%20designed%20to%20guide%20the%20generation%20of%20CD%20maps%20by%20exploiting%0Amulti-level%20difference%20features.%20Leveraging%20the%20variational%20inference%20%28VI%29%0Aprocedure%2C%20GCD-DDPM%20can%20adaptively%20re-calibrate%20the%20CD%20results%20through%20an%0Aiterative%20inference%20process%2C%20while%20accurately%20distinguishing%20subtle%20and%0Airregular%20changes%20in%20diverse%20scenes.%20Finally%2C%20a%20Noise%20Suppression-based%0ASemantic%20Enhancer%20%28NSSE%29%20is%20specifically%20designed%20to%20mitigate%20noise%20in%20the%0Acurrent%20step%27s%20change-aware%20feature%20representations%20from%20the%20CD%20Encoder.%20This%0Arefinement%2C%20serving%20as%20an%20attention%20map%2C%20can%20guide%20subsequent%20iterations%20while%0Aenhancing%20CD%20accuracy.%20Extensive%20experiments%20on%20four%20high-resolution%20CD%0Adatasets%20confirm%20the%20superior%20performance%20of%20the%20proposed%20GCD-DDPM.%20The%20code%0Afor%20this%20work%20will%20be%20available%20at%20https%3A//github.com/udrs/GCD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.03424v4&entry.124074799=Read"},
{"title": "LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR\n  Perception", "author": "Zixiang Zhou and Dongqiangzi Ye and Weijia Chen and Yufei Xie and Yu Wang and Panqu Wang and Hassan Foroosh", "abstract": "  There is a recent trend in the LiDAR perception field towards unifying\nmultiple tasks in a single strong network with improved performance, as opposed\nto using separate networks for each task. In this paper, we introduce a new\nLiDAR multi-task learning paradigm based on the transformer. The proposed\nLiDARFormer utilizes cross-space global contextual feature information and\nexploits cross-task synergy to boost the performance of LiDAR perception tasks\nacross multiple large-scale datasets and benchmarks. Our novel\ntransformer-based framework includes a cross-space transformer module that\nlearns attentive features between the 2D dense Bird's Eye View (BEV) and 3D\nsparse voxel feature maps. Additionally, we propose a transformer decoder for\nthe segmentation task to dynamically adjust the learned features by leveraging\nthe categorical feature representations. Furthermore, we combine the\nsegmentation and detection features in a shared transformer decoder with\ncross-task attention layers to enhance and integrate the object-level and\nclass-level features. LiDARFormer is evaluated on the large-scale nuScenes and\nthe Waymo Open datasets for both 3D detection and semantic segmentation tasks,\nand it outperforms all previously published methods on both tasks. Notably,\nLiDARFormer achieves the state-of-the-art performance of 76.4% L2 mAPH and\n74.3% NDS on the challenging Waymo and nuScenes detection benchmarks for a\nsingle model LiDAR-only method.\n", "link": "http://arxiv.org/abs/2303.12194v2", "date": "2024-03-02", "relevancy": 2.8013, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5803}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5679}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5326}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDARFormer%3A%20A%20Unified%20Transformer-based%20Multi-task%20Network%20for%20LiDAR%0A%20%20Perception&entry.906535625=Zixiang%20Zhou%20and%20Dongqiangzi%20Ye%20and%20Weijia%20Chen%20and%20Yufei%20Xie%20and%20Yu%20Wang%20and%20Panqu%20Wang%20and%20Hassan%20Foroosh&entry.1292438233=%20%20There%20is%20a%20recent%20trend%20in%20the%20LiDAR%20perception%20field%20towards%20unifying%0Amultiple%20tasks%20in%20a%20single%20strong%20network%20with%20improved%20performance%2C%20as%20opposed%0Ato%20using%20separate%20networks%20for%20each%20task.%20In%20this%20paper%2C%20we%20introduce%20a%20new%0ALiDAR%20multi-task%20learning%20paradigm%20based%20on%20the%20transformer.%20The%20proposed%0ALiDARFormer%20utilizes%20cross-space%20global%20contextual%20feature%20information%20and%0Aexploits%20cross-task%20synergy%20to%20boost%20the%20performance%20of%20LiDAR%20perception%20tasks%0Aacross%20multiple%20large-scale%20datasets%20and%20benchmarks.%20Our%20novel%0Atransformer-based%20framework%20includes%20a%20cross-space%20transformer%20module%20that%0Alearns%20attentive%20features%20between%20the%202D%20dense%20Bird%27s%20Eye%20View%20%28BEV%29%20and%203D%0Asparse%20voxel%20feature%20maps.%20Additionally%2C%20we%20propose%20a%20transformer%20decoder%20for%0Athe%20segmentation%20task%20to%20dynamically%20adjust%20the%20learned%20features%20by%20leveraging%0Athe%20categorical%20feature%20representations.%20Furthermore%2C%20we%20combine%20the%0Asegmentation%20and%20detection%20features%20in%20a%20shared%20transformer%20decoder%20with%0Across-task%20attention%20layers%20to%20enhance%20and%20integrate%20the%20object-level%20and%0Aclass-level%20features.%20LiDARFormer%20is%20evaluated%20on%20the%20large-scale%20nuScenes%20and%0Athe%20Waymo%20Open%20datasets%20for%20both%203D%20detection%20and%20semantic%20segmentation%20tasks%2C%0Aand%20it%20outperforms%20all%20previously%20published%20methods%20on%20both%20tasks.%20Notably%2C%0ALiDARFormer%20achieves%20the%20state-of-the-art%20performance%20of%2076.4%25%20L2%20mAPH%20and%0A74.3%25%20NDS%20on%20the%20challenging%20Waymo%20and%20nuScenes%20detection%20benchmarks%20for%20a%0Asingle%20model%20LiDAR-only%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.12194v2&entry.124074799=Read"},
{"title": "Point Could Mamba: Point Cloud Learning via State Space Model", "author": "Tao Zhang and Xiangtai Li and Haobo Yuan and Shunping Ji and Shuicheng Yan", "abstract": "  In this work, for the first time, we demonstrate that Mamba-based point cloud\nmethods can outperform point-based methods. Mamba exhibits strong global\nmodeling capabilities and linear computational complexity, making it highly\nattractive for point cloud analysis. To enable more effective processing of 3-D\npoint cloud data by Mamba, we propose a novel Consistent Traverse Serialization\nto convert point clouds into 1-D point sequences while ensuring that\nneighboring points in the sequence are also spatially adjacent. Consistent\nTraverse Serialization yields six variants by permuting the order of x, y, and\nz coordinates, and the synergistic use of these variants aids Mamba in\ncomprehensively observing point cloud data. Furthermore, to assist Mamba in\nhandling point sequences with different orders more effectively, we introduce\npoint prompts to inform Mamba of the sequence's arrangement rules. Finally, we\npropose positional encoding based on spatial coordinate mapping to inject\npositional information into point cloud sequences better. Based on these\nimprovements, we construct a point cloud network named Point Cloud Mamba, which\ncombines local and global modeling. Point Cloud Mamba surpasses the SOTA\npoint-based method PointNeXt and achieves new SOTA performance on the\nScanObjectNN, ModelNet40, and ShapeNetPart datasets.\n", "link": "http://arxiv.org/abs/2403.00762v1", "date": "2024-03-01", "relevancy": 2.7656, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5632}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5524}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5438}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Could%20Mamba%3A%20Point%20Cloud%20Learning%20via%20State%20Space%20Model&entry.906535625=Tao%20Zhang%20and%20Xiangtai%20Li%20and%20Haobo%20Yuan%20and%20Shunping%20Ji%20and%20Shuicheng%20Yan&entry.1292438233=%20%20In%20this%20work%2C%20for%20the%20first%20time%2C%20we%20demonstrate%20that%20Mamba-based%20point%20cloud%0Amethods%20can%20outperform%20point-based%20methods.%20Mamba%20exhibits%20strong%20global%0Amodeling%20capabilities%20and%20linear%20computational%20complexity%2C%20making%20it%20highly%0Aattractive%20for%20point%20cloud%20analysis.%20To%20enable%20more%20effective%20processing%20of%203-D%0Apoint%20cloud%20data%20by%20Mamba%2C%20we%20propose%20a%20novel%20Consistent%20Traverse%20Serialization%0Ato%20convert%20point%20clouds%20into%201-D%20point%20sequences%20while%20ensuring%20that%0Aneighboring%20points%20in%20the%20sequence%20are%20also%20spatially%20adjacent.%20Consistent%0ATraverse%20Serialization%20yields%20six%20variants%20by%20permuting%20the%20order%20of%20x%2C%20y%2C%20and%0Az%20coordinates%2C%20and%20the%20synergistic%20use%20of%20these%20variants%20aids%20Mamba%20in%0Acomprehensively%20observing%20point%20cloud%20data.%20Furthermore%2C%20to%20assist%20Mamba%20in%0Ahandling%20point%20sequences%20with%20different%20orders%20more%20effectively%2C%20we%20introduce%0Apoint%20prompts%20to%20inform%20Mamba%20of%20the%20sequence%27s%20arrangement%20rules.%20Finally%2C%20we%0Apropose%20positional%20encoding%20based%20on%20spatial%20coordinate%20mapping%20to%20inject%0Apositional%20information%20into%20point%20cloud%20sequences%20better.%20Based%20on%20these%0Aimprovements%2C%20we%20construct%20a%20point%20cloud%20network%20named%20Point%20Cloud%20Mamba%2C%20which%0Acombines%20local%20and%20global%20modeling.%20Point%20Cloud%20Mamba%20surpasses%20the%20SOTA%0Apoint-based%20method%20PointNeXt%20and%20achieves%20new%20SOTA%20performance%20on%20the%0AScanObjectNN%2C%20ModelNet40%2C%20and%20ShapeNetPart%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00762v1&entry.124074799=Read"},
{"title": "MultIOD: Rehearsal-free Multihead Incremental Object Detector", "author": "Eden Belouadah and Arnaud Dapogny and Kevin Bailly", "abstract": "  Class-Incremental learning (CIL) refers to the ability of artificial agents\nto integrate new classes as they appear in a stream. It is particularly\ninteresting in evolving environments where agents have limited access to memory\nand computational resources. The main challenge of incremental learning is\ncatastrophic forgetting, the inability of neural networks to retain past\nknowledge when learning a new one. Unfortunately, most existing\nclass-incremental methods for object detection are applied to two-stage\nalgorithms such as Faster-RCNN, and rely on rehearsal memory to retain past\nknowledge. We argue that those are not realistic, and more effort should be\ndedicated to anchor-free and rehearsal-free object detection. In this context,\nwe propose MultIOD, a class-incremental object detector based on CenterNet. Our\nmain contributions are: (1) we propose a multihead feature pyramid and\nmultihead detection architecture to efficiently separate class representations,\n(2) we employ transfer learning between classes learned initially and those\nlearned incrementally to tackle catastrophic forgetting, and (3) we use a\nclass-wise non-max-suppression as a post-processing technique to remove\nredundant boxes. Results show that our method outperforms a range of\nstate-of-the-art methods on two Pascal VOC datasets, while reducing memory\nfootprint by more than half.\n", "link": "http://arxiv.org/abs/2309.05334v2", "date": "2024-03-02", "relevancy": 2.76, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.565}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.55}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.541}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultIOD%3A%20Rehearsal-free%20Multihead%20Incremental%20Object%20Detector&entry.906535625=Eden%20Belouadah%20and%20Arnaud%20Dapogny%20and%20Kevin%20Bailly&entry.1292438233=%20%20Class-Incremental%20learning%20%28CIL%29%20refers%20to%20the%20ability%20of%20artificial%20agents%0Ato%20integrate%20new%20classes%20as%20they%20appear%20in%20a%20stream.%20It%20is%20particularly%0Ainteresting%20in%20evolving%20environments%20where%20agents%20have%20limited%20access%20to%20memory%0Aand%20computational%20resources.%20The%20main%20challenge%20of%20incremental%20learning%20is%0Acatastrophic%20forgetting%2C%20the%20inability%20of%20neural%20networks%20to%20retain%20past%0Aknowledge%20when%20learning%20a%20new%20one.%20Unfortunately%2C%20most%20existing%0Aclass-incremental%20methods%20for%20object%20detection%20are%20applied%20to%20two-stage%0Aalgorithms%20such%20as%20Faster-RCNN%2C%20and%20rely%20on%20rehearsal%20memory%20to%20retain%20past%0Aknowledge.%20We%20argue%20that%20those%20are%20not%20realistic%2C%20and%20more%20effort%20should%20be%0Adedicated%20to%20anchor-free%20and%20rehearsal-free%20object%20detection.%20In%20this%20context%2C%0Awe%20propose%20MultIOD%2C%20a%20class-incremental%20object%20detector%20based%20on%20CenterNet.%20Our%0Amain%20contributions%20are%3A%20%281%29%20we%20propose%20a%20multihead%20feature%20pyramid%20and%0Amultihead%20detection%20architecture%20to%20efficiently%20separate%20class%20representations%2C%0A%282%29%20we%20employ%20transfer%20learning%20between%20classes%20learned%20initially%20and%20those%0Alearned%20incrementally%20to%20tackle%20catastrophic%20forgetting%2C%20and%20%283%29%20we%20use%20a%0Aclass-wise%20non-max-suppression%20as%20a%20post-processing%20technique%20to%20remove%0Aredundant%20boxes.%20Results%20show%20that%20our%20method%20outperforms%20a%20range%20of%0Astate-of-the-art%20methods%20on%20two%20Pascal%20VOC%20datasets%2C%20while%20reducing%20memory%0Afootprint%20by%20more%20than%20half.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05334v2&entry.124074799=Read"},
{"title": "Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised\n  Semantic Segmentation", "author": "Lian Xu and Mohammed Bennamoun and Farid Boussaid and Wanli Ouyang and Ferdous Sohel and Dan Xu", "abstract": "  Most existing weakly supervised semantic segmentation (WSSS) methods rely on\nClass Activation Mapping (CAM) to extract coarse class-specific localization\nmaps using image-level labels. Prior works have commonly used an off-line\nheuristic thresholding process that combines the CAM maps with off-the-shelf\nsaliency maps produced by a general pre-trained saliency model to produce more\naccurate pseudo-segmentation labels. We propose AuxSegNet+, a weakly supervised\nauxiliary learning framework to explore the rich information from these\nsaliency maps and the significant inter-task correlation between saliency\ndetection and semantic segmentation. In the proposed AuxSegNet+, saliency\ndetection and multi-label image classification are used as auxiliary tasks to\nimprove the primary task of semantic segmentation with only image-level\nground-truth labels. We also propose a cross-task affinity learning mechanism\nto learn pixel-level affinities from the saliency and segmentation feature\nmaps. In particular, we propose a cross-task dual-affinity learning module to\nlearn both pairwise and unary affinities, which are used to enhance the\ntask-specific features and predictions by aggregating both query-dependent and\nquery-independent global context for both saliency detection and semantic\nsegmentation. The learned cross-task pairwise affinity can also be used to\nrefine and propagate CAM maps to provide better pseudo labels for both tasks.\nIterative improvement of segmentation performance is enabled by cross-task\naffinity learning and pseudo-label updating. Extensive experiments demonstrate\nthe effectiveness of the proposed approach with new state-of-the-art WSSS\nresults on the challenging PASCAL VOC and MS COCO benchmarks.\n", "link": "http://arxiv.org/abs/2403.01156v1", "date": "2024-03-02", "relevancy": 2.7484, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5547}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5543}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5401}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auxiliary%20Tasks%20Enhanced%20Dual-affinity%20Learning%20for%20Weakly%20Supervised%0A%20%20Semantic%20Segmentation&entry.906535625=Lian%20Xu%20and%20Mohammed%20Bennamoun%20and%20Farid%20Boussaid%20and%20Wanli%20Ouyang%20and%20Ferdous%20Sohel%20and%20Dan%20Xu&entry.1292438233=%20%20Most%20existing%20weakly%20supervised%20semantic%20segmentation%20%28WSSS%29%20methods%20rely%20on%0AClass%20Activation%20Mapping%20%28CAM%29%20to%20extract%20coarse%20class-specific%20localization%0Amaps%20using%20image-level%20labels.%20Prior%20works%20have%20commonly%20used%20an%20off-line%0Aheuristic%20thresholding%20process%20that%20combines%20the%20CAM%20maps%20with%20off-the-shelf%0Asaliency%20maps%20produced%20by%20a%20general%20pre-trained%20saliency%20model%20to%20produce%20more%0Aaccurate%20pseudo-segmentation%20labels.%20We%20propose%20AuxSegNet%2B%2C%20a%20weakly%20supervised%0Aauxiliary%20learning%20framework%20to%20explore%20the%20rich%20information%20from%20these%0Asaliency%20maps%20and%20the%20significant%20inter-task%20correlation%20between%20saliency%0Adetection%20and%20semantic%20segmentation.%20In%20the%20proposed%20AuxSegNet%2B%2C%20saliency%0Adetection%20and%20multi-label%20image%20classification%20are%20used%20as%20auxiliary%20tasks%20to%0Aimprove%20the%20primary%20task%20of%20semantic%20segmentation%20with%20only%20image-level%0Aground-truth%20labels.%20We%20also%20propose%20a%20cross-task%20affinity%20learning%20mechanism%0Ato%20learn%20pixel-level%20affinities%20from%20the%20saliency%20and%20segmentation%20feature%0Amaps.%20In%20particular%2C%20we%20propose%20a%20cross-task%20dual-affinity%20learning%20module%20to%0Alearn%20both%20pairwise%20and%20unary%20affinities%2C%20which%20are%20used%20to%20enhance%20the%0Atask-specific%20features%20and%20predictions%20by%20aggregating%20both%20query-dependent%20and%0Aquery-independent%20global%20context%20for%20both%20saliency%20detection%20and%20semantic%0Asegmentation.%20The%20learned%20cross-task%20pairwise%20affinity%20can%20also%20be%20used%20to%0Arefine%20and%20propagate%20CAM%20maps%20to%20provide%20better%20pseudo%20labels%20for%20both%20tasks.%0AIterative%20improvement%20of%20segmentation%20performance%20is%20enabled%20by%20cross-task%0Aaffinity%20learning%20and%20pseudo-label%20updating.%20Extensive%20experiments%20demonstrate%0Athe%20effectiveness%20of%20the%20proposed%20approach%20with%20new%20state-of-the-art%20WSSS%0Aresults%20on%20the%20challenging%20PASCAL%20VOC%20and%20MS%20COCO%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01156v1&entry.124074799=Read"},
{"title": "Advancing Generative Model Evaluation: A Novel Algorithm for Realistic\n  Image Synthesis and Comparison in OCR System", "author": "Majid Memari and Khaled R. Ahmed and Shahram Rahimi and Noorbakhsh Amiri Golilarz", "abstract": "  This research addresses a critical challenge in the field of generative\nmodels, particularly in the generation and evaluation of synthetic images.\nGiven the inherent complexity of generative models and the absence of a\nstandardized procedure for their comparison, our study introduces a pioneering\nalgorithm to objectively assess the realism of synthetic images. This approach\nsignificantly enhances the evaluation methodology by refining the Fr\\'echet\nInception Distance (FID) score, allowing for a more precise and subjective\nassessment of image quality. Our algorithm is particularly tailored to address\nthe challenges in generating and evaluating realistic images of Arabic\nhandwritten digits, a task that has traditionally been near-impossible due to\nthe subjective nature of realism in image generation. By providing a systematic\nand objective framework, our method not only enables the comparison of\ndifferent generative models but also paves the way for improvements in their\ndesign and output. This breakthrough in evaluation and comparison is crucial\nfor advancing the field of OCR, especially for scripts that present unique\ncomplexities, and sets a new standard in the generation and assessment of\nhigh-quality synthetic images.\n", "link": "http://arxiv.org/abs/2402.17204v3", "date": "2024-03-01", "relevancy": 2.7356, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.565}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5439}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5324}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Generative%20Model%20Evaluation%3A%20A%20Novel%20Algorithm%20for%20Realistic%0A%20%20Image%20Synthesis%20and%20Comparison%20in%20OCR%20System&entry.906535625=Majid%20Memari%20and%20Khaled%20R.%20Ahmed%20and%20Shahram%20Rahimi%20and%20Noorbakhsh%20Amiri%20Golilarz&entry.1292438233=%20%20This%20research%20addresses%20a%20critical%20challenge%20in%20the%20field%20of%20generative%0Amodels%2C%20particularly%20in%20the%20generation%20and%20evaluation%20of%20synthetic%20images.%0AGiven%20the%20inherent%20complexity%20of%20generative%20models%20and%20the%20absence%20of%20a%0Astandardized%20procedure%20for%20their%20comparison%2C%20our%20study%20introduces%20a%20pioneering%0Aalgorithm%20to%20objectively%20assess%20the%20realism%20of%20synthetic%20images.%20This%20approach%0Asignificantly%20enhances%20the%20evaluation%20methodology%20by%20refining%20the%20Fr%5C%27echet%0AInception%20Distance%20%28FID%29%20score%2C%20allowing%20for%20a%20more%20precise%20and%20subjective%0Aassessment%20of%20image%20quality.%20Our%20algorithm%20is%20particularly%20tailored%20to%20address%0Athe%20challenges%20in%20generating%20and%20evaluating%20realistic%20images%20of%20Arabic%0Ahandwritten%20digits%2C%20a%20task%20that%20has%20traditionally%20been%20near-impossible%20due%20to%0Athe%20subjective%20nature%20of%20realism%20in%20image%20generation.%20By%20providing%20a%20systematic%0Aand%20objective%20framework%2C%20our%20method%20not%20only%20enables%20the%20comparison%20of%0Adifferent%20generative%20models%20but%20also%20paves%20the%20way%20for%20improvements%20in%20their%0Adesign%20and%20output.%20This%20breakthrough%20in%20evaluation%20and%20comparison%20is%20crucial%0Afor%20advancing%20the%20field%20of%20OCR%2C%20especially%20for%20scripts%20that%20present%20unique%0Acomplexities%2C%20and%20sets%20a%20new%20standard%20in%20the%20generation%20and%20assessment%20of%0Ahigh-quality%20synthetic%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17204v3&entry.124074799=Read"},
{"title": "BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning\n  of SAM", "author": "Li Zhang and Youwei Liang and Ruiyi Zhang and Pengtao Xie", "abstract": "  The Segment Anything Model (SAM), a foundation model pretrained on millions\nof images and segmentation masks, has significantly advanced semantic\nsegmentation, a fundamental task in computer vision. Despite its strengths, SAM\nencounters two major challenges. Firstly, it struggles with segmenting specific\nobjects autonomously, as it relies on users to manually input prompts like\npoints or bounding boxes to identify targeted objects. Secondly, SAM faces\nchallenges in excelling at specific downstream tasks, like medical imaging, due\nto a disparity between the distribution of its pretraining data, which\npredominantly consists of general-domain images, and the data used in\ndownstream tasks. Current solutions to these problems, which involve finetuning\nSAM, often lead to overfitting, a notable issue in scenarios with very limited\ndata, like in medical imaging. To overcome these limitations, we introduce\nBLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach\nallows for automatic image segmentation without the need for manual prompts, by\noptimizing a learnable prompt embedding. Furthermore, it significantly reduces\nthe risk of overfitting by training the model's weight parameters and the\nprompt embedding on two separate subsets of the training dataset, each at a\ndifferent level of optimization. We apply BLO-SAM to diverse semantic\nsegmentation tasks in general and medical domains. The results demonstrate\nBLO-SAM's superior performance over various state-of-the-art image semantic\nsegmentation methods.\n", "link": "http://arxiv.org/abs/2402.16338v3", "date": "2024-03-02", "relevancy": 2.716, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.55}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5194}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLO-SAM%3A%20Bi-level%20Optimization%20Based%20Overfitting-Preventing%20Finetuning%0A%20%20of%20SAM&entry.906535625=Li%20Zhang%20and%20Youwei%20Liang%20and%20Ruiyi%20Zhang%20and%20Pengtao%20Xie&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20foundation%20model%20pretrained%20on%20millions%0Aof%20images%20and%20segmentation%20masks%2C%20has%20significantly%20advanced%20semantic%0Asegmentation%2C%20a%20fundamental%20task%20in%20computer%20vision.%20Despite%20its%20strengths%2C%20SAM%0Aencounters%20two%20major%20challenges.%20Firstly%2C%20it%20struggles%20with%20segmenting%20specific%0Aobjects%20autonomously%2C%20as%20it%20relies%20on%20users%20to%20manually%20input%20prompts%20like%0Apoints%20or%20bounding%20boxes%20to%20identify%20targeted%20objects.%20Secondly%2C%20SAM%20faces%0Achallenges%20in%20excelling%20at%20specific%20downstream%20tasks%2C%20like%20medical%20imaging%2C%20due%0Ato%20a%20disparity%20between%20the%20distribution%20of%20its%20pretraining%20data%2C%20which%0Apredominantly%20consists%20of%20general-domain%20images%2C%20and%20the%20data%20used%20in%0Adownstream%20tasks.%20Current%20solutions%20to%20these%20problems%2C%20which%20involve%20finetuning%0ASAM%2C%20often%20lead%20to%20overfitting%2C%20a%20notable%20issue%20in%20scenarios%20with%20very%20limited%0Adata%2C%20like%20in%20medical%20imaging.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0ABLO-SAM%2C%20which%20finetunes%20SAM%20based%20on%20bi-level%20optimization%20%28BLO%29.%20Our%20approach%0Aallows%20for%20automatic%20image%20segmentation%20without%20the%20need%20for%20manual%20prompts%2C%20by%0Aoptimizing%20a%20learnable%20prompt%20embedding.%20Furthermore%2C%20it%20significantly%20reduces%0Athe%20risk%20of%20overfitting%20by%20training%20the%20model%27s%20weight%20parameters%20and%20the%0Aprompt%20embedding%20on%20two%20separate%20subsets%20of%20the%20training%20dataset%2C%20each%20at%20a%0Adifferent%20level%20of%20optimization.%20We%20apply%20BLO-SAM%20to%20diverse%20semantic%0Asegmentation%20tasks%20in%20general%20and%20medical%20domains.%20The%20results%20demonstrate%0ABLO-SAM%27s%20superior%20performance%20over%20various%20state-of-the-art%20image%20semantic%0Asegmentation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16338v3&entry.124074799=Read"},
{"title": "ELA: Efficient Local Attention for Deep Convolutional Neural Networks", "author": "Wei Xu and Yi Wan", "abstract": "  The attention mechanism has gained significant recognition in the field of\ncomputer vision due to its ability to effectively enhance the performance of\ndeep neural networks. However, existing methods often struggle to effectively\nutilize spatial information or, if they do, they come at the cost of reducing\nchannel dimensions or increasing the complexity of neural networks. In order to\naddress these limitations, this paper introduces an Efficient Local Attention\n(ELA) method that achieves substantial performance improvements with a simple\nstructure. By analyzing the limitations of the Coordinate Attention method, we\nidentify the lack of generalization ability in Batch Normalization, the adverse\neffects of dimension reduction on channel attention, and the complexity of\nattention generation process. To overcome these challenges, we propose the\nincorporation of 1D convolution and Group Normalization feature enhancement\ntechniques. This approach enables accurate localization of regions of interest\nby efficiently encoding two 1D positional feature maps without the need for\ndimension reduction, while allowing for a lightweight implementation. We\ncarefully design three hyperparameters in ELA, resulting in four different\nversions: ELA-T, ELA-B, ELA-S, and ELA-L, to cater to the specific requirements\nof different visual tasks such as image classification, object detection and\nsementic segmentation. ELA can be seamlessly integrated into deep CNN networks\nsuch as ResNet, MobileNet, and DeepLab. Extensive evaluations on the ImageNet,\nMSCOCO, and Pascal VOC datasets demonstrate the superiority of the proposed ELA\nmodule over current state-of-the-art methods in all three aforementioned visual\ntasks.\n", "link": "http://arxiv.org/abs/2403.01123v1", "date": "2024-03-02", "relevancy": 2.7023, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5687}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5387}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.514}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELA%3A%20Efficient%20Local%20Attention%20for%20Deep%20Convolutional%20Neural%20Networks&entry.906535625=Wei%20Xu%20and%20Yi%20Wan&entry.1292438233=%20%20The%20attention%20mechanism%20has%20gained%20significant%20recognition%20in%20the%20field%20of%0Acomputer%20vision%20due%20to%20its%20ability%20to%20effectively%20enhance%20the%20performance%20of%0Adeep%20neural%20networks.%20However%2C%20existing%20methods%20often%20struggle%20to%20effectively%0Autilize%20spatial%20information%20or%2C%20if%20they%20do%2C%20they%20come%20at%20the%20cost%20of%20reducing%0Achannel%20dimensions%20or%20increasing%20the%20complexity%20of%20neural%20networks.%20In%20order%20to%0Aaddress%20these%20limitations%2C%20this%20paper%20introduces%20an%20Efficient%20Local%20Attention%0A%28ELA%29%20method%20that%20achieves%20substantial%20performance%20improvements%20with%20a%20simple%0Astructure.%20By%20analyzing%20the%20limitations%20of%20the%20Coordinate%20Attention%20method%2C%20we%0Aidentify%20the%20lack%20of%20generalization%20ability%20in%20Batch%20Normalization%2C%20the%20adverse%0Aeffects%20of%20dimension%20reduction%20on%20channel%20attention%2C%20and%20the%20complexity%20of%0Aattention%20generation%20process.%20To%20overcome%20these%20challenges%2C%20we%20propose%20the%0Aincorporation%20of%201D%20convolution%20and%20Group%20Normalization%20feature%20enhancement%0Atechniques.%20This%20approach%20enables%20accurate%20localization%20of%20regions%20of%20interest%0Aby%20efficiently%20encoding%20two%201D%20positional%20feature%20maps%20without%20the%20need%20for%0Adimension%20reduction%2C%20while%20allowing%20for%20a%20lightweight%20implementation.%20We%0Acarefully%20design%20three%20hyperparameters%20in%20ELA%2C%20resulting%20in%20four%20different%0Aversions%3A%20ELA-T%2C%20ELA-B%2C%20ELA-S%2C%20and%20ELA-L%2C%20to%20cater%20to%20the%20specific%20requirements%0Aof%20different%20visual%20tasks%20such%20as%20image%20classification%2C%20object%20detection%20and%0Asementic%20segmentation.%20ELA%20can%20be%20seamlessly%20integrated%20into%20deep%20CNN%20networks%0Asuch%20as%20ResNet%2C%20MobileNet%2C%20and%20DeepLab.%20Extensive%20evaluations%20on%20the%20ImageNet%2C%0AMSCOCO%2C%20and%20Pascal%20VOC%20datasets%20demonstrate%20the%20superiority%20of%20the%20proposed%20ELA%0Amodule%20over%20current%20state-of-the-art%20methods%20in%20all%20three%20aforementioned%20visual%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01123v1&entry.124074799=Read"},
{"title": "Beyond Night Visibility: Adaptive Multi-Scale Fusion of Infrared and\n  Visible Images", "author": "Shufan Pei and Junhong Lin and Wenxi Liu and Tiesong Zhao and Chia-Wen Lin", "abstract": "  In addition to low light, night images suffer degradation from light effects\n(e.g., glare, floodlight, etc). However, existing nighttime visibility\nenhancement methods generally focus on low-light regions, which neglects, or\neven amplifies the light effects. To address this issue, we propose an Adaptive\nMulti-scale Fusion network (AMFusion) with infrared and visible images, which\ndesigns fusion rules according to different illumination regions. First, we\nseparately fuse spatial and semantic features from infrared and visible images,\nwhere the former are used for the adjustment of light distribution and the\nlatter are used for the improvement of detection accuracy. Thereby, we obtain\nan image free of low light and light effects, which improves the performance of\nnighttime object detection. Second, we utilize detection features extracted by\na pre-trained backbone that guide the fusion of semantic features. Hereby, we\ndesign a Detection-guided Semantic Fusion Module (DSFM) to bridge the domain\ngap between detection and semantic features. Third, we propose a new\nillumination loss to constrain fusion image with normal light intensity.\nExperimental results demonstrate the superiority of AMFusion with better visual\nquality and detection accuracy. The source code will be released after the peer\nreview process.\n", "link": "http://arxiv.org/abs/2403.01083v1", "date": "2024-03-02", "relevancy": 2.7001, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5421}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5394}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Night%20Visibility%3A%20Adaptive%20Multi-Scale%20Fusion%20of%20Infrared%20and%0A%20%20Visible%20Images&entry.906535625=Shufan%20Pei%20and%20Junhong%20Lin%20and%20Wenxi%20Liu%20and%20Tiesong%20Zhao%20and%20Chia-Wen%20Lin&entry.1292438233=%20%20In%20addition%20to%20low%20light%2C%20night%20images%20suffer%20degradation%20from%20light%20effects%0A%28e.g.%2C%20glare%2C%20floodlight%2C%20etc%29.%20However%2C%20existing%20nighttime%20visibility%0Aenhancement%20methods%20generally%20focus%20on%20low-light%20regions%2C%20which%20neglects%2C%20or%0Aeven%20amplifies%20the%20light%20effects.%20To%20address%20this%20issue%2C%20we%20propose%20an%20Adaptive%0AMulti-scale%20Fusion%20network%20%28AMFusion%29%20with%20infrared%20and%20visible%20images%2C%20which%0Adesigns%20fusion%20rules%20according%20to%20different%20illumination%20regions.%20First%2C%20we%0Aseparately%20fuse%20spatial%20and%20semantic%20features%20from%20infrared%20and%20visible%20images%2C%0Awhere%20the%20former%20are%20used%20for%20the%20adjustment%20of%20light%20distribution%20and%20the%0Alatter%20are%20used%20for%20the%20improvement%20of%20detection%20accuracy.%20Thereby%2C%20we%20obtain%0Aan%20image%20free%20of%20low%20light%20and%20light%20effects%2C%20which%20improves%20the%20performance%20of%0Anighttime%20object%20detection.%20Second%2C%20we%20utilize%20detection%20features%20extracted%20by%0Aa%20pre-trained%20backbone%20that%20guide%20the%20fusion%20of%20semantic%20features.%20Hereby%2C%20we%0Adesign%20a%20Detection-guided%20Semantic%20Fusion%20Module%20%28DSFM%29%20to%20bridge%20the%20domain%0Agap%20between%20detection%20and%20semantic%20features.%20Third%2C%20we%20propose%20a%20new%0Aillumination%20loss%20to%20constrain%20fusion%20image%20with%20normal%20light%20intensity.%0AExperimental%20results%20demonstrate%20the%20superiority%20of%20AMFusion%20with%20better%20visual%0Aquality%20and%20detection%20accuracy.%20The%20source%20code%20will%20be%20released%20after%20the%20peer%0Areview%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01083v1&entry.124074799=Read"},
{"title": "Learning and Leveraging World Models in Visual Representation Learning", "author": "Quentin Garrido and Mahmoud Assran and Nicolas Ballas and Adrien Bardes and Laurent Najman and Yann LeCun", "abstract": "  Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising\nself-supervised approach that learns by leveraging a world model. While\npreviously limited to predicting missing parts of an input, we explore how to\ngeneralize the JEPA prediction task to a broader set of corruptions. We\nintroduce Image World Models, an approach that goes beyond masked image\nmodeling and learns to predict the effect of global photometric transformations\nin latent space. We study the recipe of learning performant IWMs and show that\nit relies on three key aspects: conditioning, prediction difficulty, and\ncapacity. Additionally, we show that the predictive world model learned by IWM\ncan be adapted through finetuning to solve diverse tasks; a fine-tuned IWM\nworld model matches or surpasses the performance of previous self-supervised\nmethods. Finally, we show that learning with an IWM allows one to control the\nabstraction level of the learned representations, learning invariant\nrepresentations such as contrastive methods, or equivariant representations\nsuch as masked image modelling.\n", "link": "http://arxiv.org/abs/2403.00504v1", "date": "2024-03-01", "relevancy": 2.676, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5703}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.52}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5153}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20and%20Leveraging%20World%20Models%20in%20Visual%20Representation%20Learning&entry.906535625=Quentin%20Garrido%20and%20Mahmoud%20Assran%20and%20Nicolas%20Ballas%20and%20Adrien%20Bardes%20and%20Laurent%20Najman%20and%20Yann%20LeCun&entry.1292438233=%20%20Joint-Embedding%20Predictive%20Architecture%20%28JEPA%29%20has%20emerged%20as%20a%20promising%0Aself-supervised%20approach%20that%20learns%20by%20leveraging%20a%20world%20model.%20While%0Apreviously%20limited%20to%20predicting%20missing%20parts%20of%20an%20input%2C%20we%20explore%20how%20to%0Ageneralize%20the%20JEPA%20prediction%20task%20to%20a%20broader%20set%20of%20corruptions.%20We%0Aintroduce%20Image%20World%20Models%2C%20an%20approach%20that%20goes%20beyond%20masked%20image%0Amodeling%20and%20learns%20to%20predict%20the%20effect%20of%20global%20photometric%20transformations%0Ain%20latent%20space.%20We%20study%20the%20recipe%20of%20learning%20performant%20IWMs%20and%20show%20that%0Ait%20relies%20on%20three%20key%20aspects%3A%20conditioning%2C%20prediction%20difficulty%2C%20and%0Acapacity.%20Additionally%2C%20we%20show%20that%20the%20predictive%20world%20model%20learned%20by%20IWM%0Acan%20be%20adapted%20through%20finetuning%20to%20solve%20diverse%20tasks%3B%20a%20fine-tuned%20IWM%0Aworld%20model%20matches%20or%20surpasses%20the%20performance%20of%20previous%20self-supervised%0Amethods.%20Finally%2C%20we%20show%20that%20learning%20with%20an%20IWM%20allows%20one%20to%20control%20the%0Aabstraction%20level%20of%20the%20learned%20representations%2C%20learning%20invariant%0Arepresentations%20such%20as%20contrastive%20methods%2C%20or%20equivariant%20representations%0Asuch%20as%20masked%20image%20modelling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00504v1&entry.124074799=Read"},
{"title": "Run-time Introspection of 2D Object Detection in Automated Driving\n  Systems Using Learning Representations", "author": "Hakan Yekta Yatbaz and Mehrdad Dianati and Konstantinos Koufos and Roger Woodman", "abstract": "  Reliable detection of various objects and road users in the surrounding\nenvironment is crucial for the safe operation of automated driving systems\n(ADS). Despite recent progresses in developing highly accurate object detectors\nbased on Deep Neural Networks (DNNs), they still remain prone to detection\nerrors, which can lead to fatal consequences in safety-critical applications\nsuch as ADS. An effective remedy to this problem is to equip the system with\nrun-time monitoring, named as introspection in the context of autonomous\nsystems. Motivated by this, we introduce a novel introspection solution, which\noperates at the frame level for DNN-based 2D object detection and leverages\nneural network activation patterns. The proposed approach pre-processes the\nneural activation patterns of the object detector's backbone using several\ndifferent modes. To provide extensive comparative analysis and fair comparison,\nwe also adapt and implement several state-of-the-art (SOTA) introspection\nmechanisms for error detection in 2D object detection, using one-stage and\ntwo-stage object detectors evaluated on KITTI and BDD datasets. We compare the\nperformance of the proposed solution in terms of error detection, adaptability\nto dataset shift, and, computational and memory resource requirements. Our\nperformance evaluation shows that the proposed introspection solution\noutperforms SOTA methods, achieving an absolute reduction in the missed error\nratio of 9% to 17% in the BDD dataset.\n", "link": "http://arxiv.org/abs/2403.01172v1", "date": "2024-03-02", "relevancy": 2.6717, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5357}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5343}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.533}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Run-time%20Introspection%20of%202D%20Object%20Detection%20in%20Automated%20Driving%0A%20%20Systems%20Using%20Learning%20Representations&entry.906535625=Hakan%20Yekta%20Yatbaz%20and%20Mehrdad%20Dianati%20and%20Konstantinos%20Koufos%20and%20Roger%20Woodman&entry.1292438233=%20%20Reliable%20detection%20of%20various%20objects%20and%20road%20users%20in%20the%20surrounding%0Aenvironment%20is%20crucial%20for%20the%20safe%20operation%20of%20automated%20driving%20systems%0A%28ADS%29.%20Despite%20recent%20progresses%20in%20developing%20highly%20accurate%20object%20detectors%0Abased%20on%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20they%20still%20remain%20prone%20to%20detection%0Aerrors%2C%20which%20can%20lead%20to%20fatal%20consequences%20in%20safety-critical%20applications%0Asuch%20as%20ADS.%20An%20effective%20remedy%20to%20this%20problem%20is%20to%20equip%20the%20system%20with%0Arun-time%20monitoring%2C%20named%20as%20introspection%20in%20the%20context%20of%20autonomous%0Asystems.%20Motivated%20by%20this%2C%20we%20introduce%20a%20novel%20introspection%20solution%2C%20which%0Aoperates%20at%20the%20frame%20level%20for%20DNN-based%202D%20object%20detection%20and%20leverages%0Aneural%20network%20activation%20patterns.%20The%20proposed%20approach%20pre-processes%20the%0Aneural%20activation%20patterns%20of%20the%20object%20detector%27s%20backbone%20using%20several%0Adifferent%20modes.%20To%20provide%20extensive%20comparative%20analysis%20and%20fair%20comparison%2C%0Awe%20also%20adapt%20and%20implement%20several%20state-of-the-art%20%28SOTA%29%20introspection%0Amechanisms%20for%20error%20detection%20in%202D%20object%20detection%2C%20using%20one-stage%20and%0Atwo-stage%20object%20detectors%20evaluated%20on%20KITTI%20and%20BDD%20datasets.%20We%20compare%20the%0Aperformance%20of%20the%20proposed%20solution%20in%20terms%20of%20error%20detection%2C%20adaptability%0Ato%20dataset%20shift%2C%20and%2C%20computational%20and%20memory%20resource%20requirements.%20Our%0Aperformance%20evaluation%20shows%20that%20the%20proposed%20introspection%20solution%0Aoutperforms%20SOTA%20methods%2C%20achieving%20an%20absolute%20reduction%20in%20the%20missed%20error%0Aratio%20of%209%25%20to%2017%25%20in%20the%20BDD%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01172v1&entry.124074799=Read"},
{"title": "Adversarial AutoMixup", "author": "Huafeng Qin and Xin Jin and Yun Jiang and Mounim A. El-Yacoubi and Xinbo Gao", "abstract": "  Data mixing augmentation has been widely applied to improve the\ngeneralization ability of deep neural networks. Recently, offline data mixing\naugmentation, e.g. handcrafted and saliency information-based mixup, has been\ngradually replaced by automatic mixing approaches. Through minimizing two\nsub-tasks, namely, mixed sample generation and mixup classification in an\nend-to-end way, AutoMix significantly improves accuracy on image classification\ntasks. However, as the optimization objective is consistent for the two\nsub-tasks, this approach is prone to generating consistent instead of diverse\nmixed samples, which results in overfitting for target task training. In this\npaper, we propose AdAutomixup, an adversarial automatic mixup augmentation\napproach that generates challenging samples to train a robust classifier for\nimage classification, by alternatively optimizing the classifier and the mixup\nsample generator. AdAutomixup comprises two modules, a mixed example generator,\nand a target classifier. The mixed sample generator aims to produce hard mixed\nexamples to challenge the target classifier, while the target classifier's aim\nis to learn robust features from hard mixed examples to improve generalization.\nTo prevent the collapse of the inherent meanings of images, we further\nintroduce an exponential moving average (EMA) teacher and cosine similarity to\ntrain AdAutomixup in an end-to-end way. Extensive experiments on seven image\nbenchmarks consistently prove that our approach outperforms the state of the\nart in various classification scenarios. The source code is available at\nhttps://github.com/JinXins/Adversarial-AutoMixup.\n", "link": "http://arxiv.org/abs/2312.11954v2", "date": "2024-03-01", "relevancy": 2.6698, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5989}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5075}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4955}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20AutoMixup&entry.906535625=Huafeng%20Qin%20and%20Xin%20Jin%20and%20Yun%20Jiang%20and%20Mounim%20A.%20El-Yacoubi%20and%20Xinbo%20Gao&entry.1292438233=%20%20Data%20mixing%20augmentation%20has%20been%20widely%20applied%20to%20improve%20the%0Ageneralization%20ability%20of%20deep%20neural%20networks.%20Recently%2C%20offline%20data%20mixing%0Aaugmentation%2C%20e.g.%20handcrafted%20and%20saliency%20information-based%20mixup%2C%20has%20been%0Agradually%20replaced%20by%20automatic%20mixing%20approaches.%20Through%20minimizing%20two%0Asub-tasks%2C%20namely%2C%20mixed%20sample%20generation%20and%20mixup%20classification%20in%20an%0Aend-to-end%20way%2C%20AutoMix%20significantly%20improves%20accuracy%20on%20image%20classification%0Atasks.%20However%2C%20as%20the%20optimization%20objective%20is%20consistent%20for%20the%20two%0Asub-tasks%2C%20this%20approach%20is%20prone%20to%20generating%20consistent%20instead%20of%20diverse%0Amixed%20samples%2C%20which%20results%20in%20overfitting%20for%20target%20task%20training.%20In%20this%0Apaper%2C%20we%20propose%20AdAutomixup%2C%20an%20adversarial%20automatic%20mixup%20augmentation%0Aapproach%20that%20generates%20challenging%20samples%20to%20train%20a%20robust%20classifier%20for%0Aimage%20classification%2C%20by%20alternatively%20optimizing%20the%20classifier%20and%20the%20mixup%0Asample%20generator.%20AdAutomixup%20comprises%20two%20modules%2C%20a%20mixed%20example%20generator%2C%0Aand%20a%20target%20classifier.%20The%20mixed%20sample%20generator%20aims%20to%20produce%20hard%20mixed%0Aexamples%20to%20challenge%20the%20target%20classifier%2C%20while%20the%20target%20classifier%27s%20aim%0Ais%20to%20learn%20robust%20features%20from%20hard%20mixed%20examples%20to%20improve%20generalization.%0ATo%20prevent%20the%20collapse%20of%20the%20inherent%20meanings%20of%20images%2C%20we%20further%0Aintroduce%20an%20exponential%20moving%20average%20%28EMA%29%20teacher%20and%20cosine%20similarity%20to%0Atrain%20AdAutomixup%20in%20an%20end-to-end%20way.%20Extensive%20experiments%20on%20seven%20image%0Abenchmarks%20consistently%20prove%20that%20our%20approach%20outperforms%20the%20state%20of%20the%0Aart%20in%20various%20classification%20scenarios.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/JinXins/Adversarial-AutoMixup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11954v2&entry.124074799=Read"},
{"title": "Top-Down Framework for Weakly-supervised Grounded Image Captioning", "author": "Chen Cai and Suchen Wang and Kim-hui Yap and Yi Wang", "abstract": "  Weakly-supervised grounded image captioning (WSGIC) aims to generate the\ncaption and ground (localize) predicted object words in the input image without\nusing bounding box supervision. Recent two-stage solutions mostly apply a\nbottom-up pipeline: (1) encode the input image into multiple region features\nusing an object detector; (2) leverage region features for captioning and\ngrounding. However, utilizing independent proposals produced by object\ndetectors tends to make the subsequent grounded captioner overfitted in finding\nthe correct object words, overlooking the relation between objects, and\nselecting incompatible proposal regions for grounding. To address these issues,\nwe propose a one-stage weakly-supervised grounded captioner that directly takes\nthe RGB image as input to perform captioning and grounding at the top-down\nimage level. Specifically, we encode the image into visual token\nrepresentations and propose a Recurrent Grounding Module (RGM) in the decoder\nto obtain precise Visual Language Attention Maps (VLAMs), which recognize the\nspatial locations of the objects. In addition, we explicitly inject a relation\nmodule into our one-stage framework to encourage relation understanding through\nmulti-label classification. This relation semantics served as contextual\ninformation facilitating the prediction of relation and object words in the\ncaption. We observe that the relation semantic not only assists the grounded\ncaptioner in generating a more accurate caption but also improves the grounding\nperformance. We validate the effectiveness of our proposed method on two\nchallenging datasets (Flick30k Entities captioning and MSCOCO captioning). The\nexperimental results demonstrate that our method achieves state-of-the-art\ngrounding performance.\n", "link": "http://arxiv.org/abs/2306.07490v3", "date": "2024-03-02", "relevancy": 2.6571, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5411}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5307}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5225}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Top-Down%20Framework%20for%20Weakly-supervised%20Grounded%20Image%20Captioning&entry.906535625=Chen%20Cai%20and%20Suchen%20Wang%20and%20Kim-hui%20Yap%20and%20Yi%20Wang&entry.1292438233=%20%20Weakly-supervised%20grounded%20image%20captioning%20%28WSGIC%29%20aims%20to%20generate%20the%0Acaption%20and%20ground%20%28localize%29%20predicted%20object%20words%20in%20the%20input%20image%20without%0Ausing%20bounding%20box%20supervision.%20Recent%20two-stage%20solutions%20mostly%20apply%20a%0Abottom-up%20pipeline%3A%20%281%29%20encode%20the%20input%20image%20into%20multiple%20region%20features%0Ausing%20an%20object%20detector%3B%20%282%29%20leverage%20region%20features%20for%20captioning%20and%0Agrounding.%20However%2C%20utilizing%20independent%20proposals%20produced%20by%20object%0Adetectors%20tends%20to%20make%20the%20subsequent%20grounded%20captioner%20overfitted%20in%20finding%0Athe%20correct%20object%20words%2C%20overlooking%20the%20relation%20between%20objects%2C%20and%0Aselecting%20incompatible%20proposal%20regions%20for%20grounding.%20To%20address%20these%20issues%2C%0Awe%20propose%20a%20one-stage%20weakly-supervised%20grounded%20captioner%20that%20directly%20takes%0Athe%20RGB%20image%20as%20input%20to%20perform%20captioning%20and%20grounding%20at%20the%20top-down%0Aimage%20level.%20Specifically%2C%20we%20encode%20the%20image%20into%20visual%20token%0Arepresentations%20and%20propose%20a%20Recurrent%20Grounding%20Module%20%28RGM%29%20in%20the%20decoder%0Ato%20obtain%20precise%20Visual%20Language%20Attention%20Maps%20%28VLAMs%29%2C%20which%20recognize%20the%0Aspatial%20locations%20of%20the%20objects.%20In%20addition%2C%20we%20explicitly%20inject%20a%20relation%0Amodule%20into%20our%20one-stage%20framework%20to%20encourage%20relation%20understanding%20through%0Amulti-label%20classification.%20This%20relation%20semantics%20served%20as%20contextual%0Ainformation%20facilitating%20the%20prediction%20of%20relation%20and%20object%20words%20in%20the%0Acaption.%20We%20observe%20that%20the%20relation%20semantic%20not%20only%20assists%20the%20grounded%0Acaptioner%20in%20generating%20a%20more%20accurate%20caption%20but%20also%20improves%20the%20grounding%0Aperformance.%20We%20validate%20the%20effectiveness%20of%20our%20proposed%20method%20on%20two%0Achallenging%20datasets%20%28Flick30k%20Entities%20captioning%20and%20MSCOCO%20captioning%29.%20The%0Aexperimental%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Agrounding%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07490v3&entry.124074799=Read"},
{"title": "Grid-based Fast and Structural Visual Odometry", "author": "Zhang Zhihe", "abstract": "  In the field of Simultaneous Localization and Mapping (SLAM), researchers\nhave always pursued better performance in terms of accuracy and time cost.\nTraditional algorithms typically rely on fundamental geometric elements in\nimages to establish connections between frames. However, these elements suffer\nfrom disadvantages such as uneven distribution and slow extraction. In\naddition, geometry elements like lines have not been fully utilized in the\nprocess of pose estimation. To address these challenges, we propose GFS-VO, a\ngrid-based RGB-D visual odometry algorithm that maximizes the utilization of\nboth point and line features. Our algorithm incorporates fast line extraction\nand a stable line homogenization scheme to improve feature processing. To fully\nleverage hidden elements in the scene, we introduce Manhattan Axes (MA) to\nprovide constraints between local map and current frame. Additionally, we have\ndesigned an algorithm based on breadth-first search for extracting plane normal\nvectors. To evaluate the performance of GFS-VO, we conducted extensive\nexperiments. The results demonstrate that our proposed algorithm exhibits\nsignificant improvements in both time cost and accuracy compared to existing\napproaches.\n", "link": "http://arxiv.org/abs/2403.01110v1", "date": "2024-03-02", "relevancy": 2.6462, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5404}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5401}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5073}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grid-based%20Fast%20and%20Structural%20Visual%20Odometry&entry.906535625=Zhang%20Zhihe&entry.1292438233=%20%20In%20the%20field%20of%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%2C%20researchers%0Ahave%20always%20pursued%20better%20performance%20in%20terms%20of%20accuracy%20and%20time%20cost.%0ATraditional%20algorithms%20typically%20rely%20on%20fundamental%20geometric%20elements%20in%0Aimages%20to%20establish%20connections%20between%20frames.%20However%2C%20these%20elements%20suffer%0Afrom%20disadvantages%20such%20as%20uneven%20distribution%20and%20slow%20extraction.%20In%0Aaddition%2C%20geometry%20elements%20like%20lines%20have%20not%20been%20fully%20utilized%20in%20the%0Aprocess%20of%20pose%20estimation.%20To%20address%20these%20challenges%2C%20we%20propose%20GFS-VO%2C%20a%0Agrid-based%20RGB-D%20visual%20odometry%20algorithm%20that%20maximizes%20the%20utilization%20of%0Aboth%20point%20and%20line%20features.%20Our%20algorithm%20incorporates%20fast%20line%20extraction%0Aand%20a%20stable%20line%20homogenization%20scheme%20to%20improve%20feature%20processing.%20To%20fully%0Aleverage%20hidden%20elements%20in%20the%20scene%2C%20we%20introduce%20Manhattan%20Axes%20%28MA%29%20to%0Aprovide%20constraints%20between%20local%20map%20and%20current%20frame.%20Additionally%2C%20we%20have%0Adesigned%20an%20algorithm%20based%20on%20breadth-first%20search%20for%20extracting%20plane%20normal%0Avectors.%20To%20evaluate%20the%20performance%20of%20GFS-VO%2C%20we%20conducted%20extensive%0Aexperiments.%20The%20results%20demonstrate%20that%20our%20proposed%20algorithm%20exhibits%0Asignificant%20improvements%20in%20both%20time%20cost%20and%20accuracy%20compared%20to%20existing%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01110v1&entry.124074799=Read"},
{"title": "On the Road to Portability: Compressing End-to-End Motion Planner for\n  Autonomous Driving", "author": "Kaituo Feng and Changsheng Li and Dongchun Ren and Ye Yuan and Guoren Wang", "abstract": "  End-to-end motion planning models equipped with deep neural networks have\nshown great potential for enabling full autonomous driving. However, the\noversized neural networks render them impractical for deployment on\nresource-constrained systems, which unavoidably requires more computational\ntime and resources during reference.To handle this, knowledge distillation\noffers a promising approach that compresses models by enabling a smaller\nstudent model to learn from a larger teacher model. Nevertheless, how to apply\nknowledge distillation to compress motion planners has not been explored so\nfar. In this paper, we propose PlanKD, the first knowledge distillation\nframework tailored for compressing end-to-end motion planners. First,\nconsidering that driving scenes are inherently complex, often containing\nplanning-irrelevant or even noisy information, transferring such information is\nnot beneficial for the student planner. Thus, we design an information\nbottleneck based strategy to only distill planning-relevant information, rather\nthan transfer all information indiscriminately. Second, different waypoints in\nan output planned trajectory may hold varying degrees of importance for motion\nplanning, where a slight deviation in certain crucial waypoints might lead to a\ncollision. Therefore, we devise a safety-aware waypoint-attentive distillation\nmodule that assigns adaptive weights to different waypoints based on the\nimportance, to encourage the student to accurately mimic more crucial\nwaypoints, thereby improving overall safety. Experiments demonstrate that our\nPlanKD can boost the performance of smaller planners by a large margin, and\nsignificantly reduce their reference time.\n", "link": "http://arxiv.org/abs/2403.01238v1", "date": "2024-03-02", "relevancy": 2.6427, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5401}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5289}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5166}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Road%20to%20Portability%3A%20Compressing%20End-to-End%20Motion%20Planner%20for%0A%20%20Autonomous%20Driving&entry.906535625=Kaituo%20Feng%20and%20Changsheng%20Li%20and%20Dongchun%20Ren%20and%20Ye%20Yuan%20and%20Guoren%20Wang&entry.1292438233=%20%20End-to-end%20motion%20planning%20models%20equipped%20with%20deep%20neural%20networks%20have%0Ashown%20great%20potential%20for%20enabling%20full%20autonomous%20driving.%20However%2C%20the%0Aoversized%20neural%20networks%20render%20them%20impractical%20for%20deployment%20on%0Aresource-constrained%20systems%2C%20which%20unavoidably%20requires%20more%20computational%0Atime%20and%20resources%20during%20reference.To%20handle%20this%2C%20knowledge%20distillation%0Aoffers%20a%20promising%20approach%20that%20compresses%20models%20by%20enabling%20a%20smaller%0Astudent%20model%20to%20learn%20from%20a%20larger%20teacher%20model.%20Nevertheless%2C%20how%20to%20apply%0Aknowledge%20distillation%20to%20compress%20motion%20planners%20has%20not%20been%20explored%20so%0Afar.%20In%20this%20paper%2C%20we%20propose%20PlanKD%2C%20the%20first%20knowledge%20distillation%0Aframework%20tailored%20for%20compressing%20end-to-end%20motion%20planners.%20First%2C%0Aconsidering%20that%20driving%20scenes%20are%20inherently%20complex%2C%20often%20containing%0Aplanning-irrelevant%20or%20even%20noisy%20information%2C%20transferring%20such%20information%20is%0Anot%20beneficial%20for%20the%20student%20planner.%20Thus%2C%20we%20design%20an%20information%0Abottleneck%20based%20strategy%20to%20only%20distill%20planning-relevant%20information%2C%20rather%0Athan%20transfer%20all%20information%20indiscriminately.%20Second%2C%20different%20waypoints%20in%0Aan%20output%20planned%20trajectory%20may%20hold%20varying%20degrees%20of%20importance%20for%20motion%0Aplanning%2C%20where%20a%20slight%20deviation%20in%20certain%20crucial%20waypoints%20might%20lead%20to%20a%0Acollision.%20Therefore%2C%20we%20devise%20a%20safety-aware%20waypoint-attentive%20distillation%0Amodule%20that%20assigns%20adaptive%20weights%20to%20different%20waypoints%20based%20on%20the%0Aimportance%2C%20to%20encourage%20the%20student%20to%20accurately%20mimic%20more%20crucial%0Awaypoints%2C%20thereby%20improving%20overall%20safety.%20Experiments%20demonstrate%20that%20our%0APlanKD%20can%20boost%20the%20performance%20of%20smaller%20planners%20by%20a%20large%20margin%2C%20and%0Asignificantly%20reduce%20their%20reference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01238v1&entry.124074799=Read"},
{"title": "Beyond Single-Model Views for Deep Learning: Optimization versus\n  Generalizability of Stochastic Optimization Algorithms", "author": "Toki Tahmid Inan and Mingrui Liu and Amarda Shehu", "abstract": "  Despite an extensive body of literature on deep learning optimization, our\ncurrent understanding of what makes an optimization algorithm effective is\nfragmented. In particular, we do not understand well whether enhanced\noptimization translates to improved generalizability. Current research\noverlooks the inherent stochastic nature of stochastic gradient descent (SGD)\nand its variants, resulting in a lack of comprehensive benchmarking and insight\ninto their statistical performance. This paper aims to address this gap by\nadopting a novel approach. Rather than solely evaluating the endpoint of\nindividual optimization trajectories, we draw from an ensemble of trajectories\nto estimate the stationary distribution of stochastic optimizers. Our\ninvestigation encompasses a wide array of techniques, including SGD and its\nvariants, flat-minima optimizers, and new algorithms we propose under the Basin\nHopping framework. Through our evaluation, which encompasses synthetic\nfunctions with known minima and real-world problems in computer vision and\nnatural language processing, we emphasize fair benchmarking under a statistical\nframework, comparing stationary distributions and establishing statistical\nsignificance. Our study uncovers several key findings regarding the\nrelationship between training loss and hold-out accuracy, as well as the\ncomparable performance of SGD, noise-enabled variants, and novel optimizers\nutilizing the BH framework. Notably, these algorithms demonstrate performance\non par with flat-minima optimizers like SAM, albeit with half the gradient\nevaluations. We anticipate that our work will catalyze further exploration in\ndeep learning optimization, encouraging a shift away from single-model\napproaches towards methodologies that acknowledge and leverage the stochastic\nnature of optimizers.\n", "link": "http://arxiv.org/abs/2403.00574v1", "date": "2024-03-01", "relevancy": 2.6418, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5533}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5269}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5049}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Single-Model%20Views%20for%20Deep%20Learning%3A%20Optimization%20versus%0A%20%20Generalizability%20of%20Stochastic%20Optimization%20Algorithms&entry.906535625=Toki%20Tahmid%20Inan%20and%20Mingrui%20Liu%20and%20Amarda%20Shehu&entry.1292438233=%20%20Despite%20an%20extensive%20body%20of%20literature%20on%20deep%20learning%20optimization%2C%20our%0Acurrent%20understanding%20of%20what%20makes%20an%20optimization%20algorithm%20effective%20is%0Afragmented.%20In%20particular%2C%20we%20do%20not%20understand%20well%20whether%20enhanced%0Aoptimization%20translates%20to%20improved%20generalizability.%20Current%20research%0Aoverlooks%20the%20inherent%20stochastic%20nature%20of%20stochastic%20gradient%20descent%20%28SGD%29%0Aand%20its%20variants%2C%20resulting%20in%20a%20lack%20of%20comprehensive%20benchmarking%20and%20insight%0Ainto%20their%20statistical%20performance.%20This%20paper%20aims%20to%20address%20this%20gap%20by%0Aadopting%20a%20novel%20approach.%20Rather%20than%20solely%20evaluating%20the%20endpoint%20of%0Aindividual%20optimization%20trajectories%2C%20we%20draw%20from%20an%20ensemble%20of%20trajectories%0Ato%20estimate%20the%20stationary%20distribution%20of%20stochastic%20optimizers.%20Our%0Ainvestigation%20encompasses%20a%20wide%20array%20of%20techniques%2C%20including%20SGD%20and%20its%0Avariants%2C%20flat-minima%20optimizers%2C%20and%20new%20algorithms%20we%20propose%20under%20the%20Basin%0AHopping%20framework.%20Through%20our%20evaluation%2C%20which%20encompasses%20synthetic%0Afunctions%20with%20known%20minima%20and%20real-world%20problems%20in%20computer%20vision%20and%0Anatural%20language%20processing%2C%20we%20emphasize%20fair%20benchmarking%20under%20a%20statistical%0Aframework%2C%20comparing%20stationary%20distributions%20and%20establishing%20statistical%0Asignificance.%20Our%20study%20uncovers%20several%20key%20findings%20regarding%20the%0Arelationship%20between%20training%20loss%20and%20hold-out%20accuracy%2C%20as%20well%20as%20the%0Acomparable%20performance%20of%20SGD%2C%20noise-enabled%20variants%2C%20and%20novel%20optimizers%0Autilizing%20the%20BH%20framework.%20Notably%2C%20these%20algorithms%20demonstrate%20performance%0Aon%20par%20with%20flat-minima%20optimizers%20like%20SAM%2C%20albeit%20with%20half%20the%20gradient%0Aevaluations.%20We%20anticipate%20that%20our%20work%20will%20catalyze%20further%20exploration%20in%0Adeep%20learning%20optimization%2C%20encouraging%20a%20shift%20away%20from%20single-model%0Aapproaches%20towards%20methodologies%20that%20acknowledge%20and%20leverage%20the%20stochastic%0Anature%20of%20optimizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00574v1&entry.124074799=Read"},
{"title": "Rethinking Few-shot 3D Point Cloud Semantic Segmentation", "author": "Zhaochong An and Guolei Sun and Yun Liu and Fayao Liu and Zongwei Wu and Dan Wang and Luc Van Gool and Serge Belongie", "abstract": "  This paper revisits few-shot 3D point cloud semantic segmentation (FS-PCS),\nwith a focus on two significant issues in the state-of-the-art: foreground\nleakage and sparse point distribution. The former arises from non-uniform point\nsampling, allowing models to distinguish the density disparities between\nforeground and background for easier segmentation. The latter results from\nsampling only 2,048 points, limiting semantic information and deviating from\nthe real-world practice. To address these issues, we introduce a standardized\nFS-PCS setting, upon which a new benchmark is built. Moreover, we propose a\nnovel FS-PCS model. While previous methods are based on feature optimization by\nmainly refining support features to enhance prototypes, our method is based on\ncorrelation optimization, referred to as Correlation Optimization Segmentation\n(COSeg). Specifically, we compute Class-specific Multi-prototypical Correlation\n(CMC) for each query point, representing its correlations to category\nprototypes. Then, we propose the Hyper Correlation Augmentation (HCA) module to\nenhance CMC. Furthermore, tackling the inherent property of few-shot training\nto incur base susceptibility for models, we propose to learn non-parametric\nprototypes for the base classes during training. The learned base prototypes\nare used to calibrate correlations for the background class through a Base\nPrototypes Calibration (BPC) module. Experiments on popular datasets\ndemonstrate the superiority of COSeg over existing methods. The code is\navailable at: https://github.com/ZhaochongAn/COSeg\n", "link": "http://arxiv.org/abs/2403.00592v1", "date": "2024-03-01", "relevancy": 2.6323, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5292}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5255}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5247}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Few-shot%203D%20Point%20Cloud%20Semantic%20Segmentation&entry.906535625=Zhaochong%20An%20and%20Guolei%20Sun%20and%20Yun%20Liu%20and%20Fayao%20Liu%20and%20Zongwei%20Wu%20and%20Dan%20Wang%20and%20Luc%20Van%20Gool%20and%20Serge%20Belongie&entry.1292438233=%20%20This%20paper%20revisits%20few-shot%203D%20point%20cloud%20semantic%20segmentation%20%28FS-PCS%29%2C%0Awith%20a%20focus%20on%20two%20significant%20issues%20in%20the%20state-of-the-art%3A%20foreground%0Aleakage%20and%20sparse%20point%20distribution.%20The%20former%20arises%20from%20non-uniform%20point%0Asampling%2C%20allowing%20models%20to%20distinguish%20the%20density%20disparities%20between%0Aforeground%20and%20background%20for%20easier%20segmentation.%20The%20latter%20results%20from%0Asampling%20only%202%2C048%20points%2C%20limiting%20semantic%20information%20and%20deviating%20from%0Athe%20real-world%20practice.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20standardized%0AFS-PCS%20setting%2C%20upon%20which%20a%20new%20benchmark%20is%20built.%20Moreover%2C%20we%20propose%20a%0Anovel%20FS-PCS%20model.%20While%20previous%20methods%20are%20based%20on%20feature%20optimization%20by%0Amainly%20refining%20support%20features%20to%20enhance%20prototypes%2C%20our%20method%20is%20based%20on%0Acorrelation%20optimization%2C%20referred%20to%20as%20Correlation%20Optimization%20Segmentation%0A%28COSeg%29.%20Specifically%2C%20we%20compute%20Class-specific%20Multi-prototypical%20Correlation%0A%28CMC%29%20for%20each%20query%20point%2C%20representing%20its%20correlations%20to%20category%0Aprototypes.%20Then%2C%20we%20propose%20the%20Hyper%20Correlation%20Augmentation%20%28HCA%29%20module%20to%0Aenhance%20CMC.%20Furthermore%2C%20tackling%20the%20inherent%20property%20of%20few-shot%20training%0Ato%20incur%20base%20susceptibility%20for%20models%2C%20we%20propose%20to%20learn%20non-parametric%0Aprototypes%20for%20the%20base%20classes%20during%20training.%20The%20learned%20base%20prototypes%0Aare%20used%20to%20calibrate%20correlations%20for%20the%20background%20class%20through%20a%20Base%0APrototypes%20Calibration%20%28BPC%29%20module.%20Experiments%20on%20popular%20datasets%0Ademonstrate%20the%20superiority%20of%20COSeg%20over%20existing%20methods.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/ZhaochongAn/COSeg%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00592v1&entry.124074799=Read"},
{"title": "Fine-tuning with Very Large Dropout", "author": "Jianyu Zhang and L\u00e9on Bottou", "abstract": "  It is impossible today to pretend that the practice of machine learning is\ncompatible with the idea that training and testing data follow the same\ndistribution. Several authors have recently used ensemble techniques to show\nhow scenarios involving multiple data distributions are best served by\nrepresentations that are both richer than those obtained by regularizing for\nthe best in-distribution performance, and richer than those obtained under the\ninfluence of the implicit sparsity bias of common stochastic gradient\nprocedures.\n  This contribution investigates the use of very high dropout rates instead of\nensembles to obtain such rich representations. Although training a deep network\nfrom scratch using such dropout rates is virtually impossible, fine-tuning a\nlarge pre-trained model under such conditions is not only possible but also\nachieves out-of-distribution performances that exceed those of both ensembles\nand weight averaging methods such as model soups. This result has practical\nsignificance because the importance of the fine-tuning scenario has\nconsiderably grown in recent years. This result also provides interesting\ninsights on the nature of rich representations and on the intrinsically linear\nnature of fine-tuning a large network using a comparatively small dataset.\n", "link": "http://arxiv.org/abs/2403.00946v1", "date": "2024-03-01", "relevancy": 2.628, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5595}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.54}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4773}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20with%20Very%20Large%20Dropout&entry.906535625=Jianyu%20Zhang%20and%20L%C3%A9on%20Bottou&entry.1292438233=%20%20It%20is%20impossible%20today%20to%20pretend%20that%20the%20practice%20of%20machine%20learning%20is%0Acompatible%20with%20the%20idea%20that%20training%20and%20testing%20data%20follow%20the%20same%0Adistribution.%20Several%20authors%20have%20recently%20used%20ensemble%20techniques%20to%20show%0Ahow%20scenarios%20involving%20multiple%20data%20distributions%20are%20best%20served%20by%0Arepresentations%20that%20are%20both%20richer%20than%20those%20obtained%20by%20regularizing%20for%0Athe%20best%20in-distribution%20performance%2C%20and%20richer%20than%20those%20obtained%20under%20the%0Ainfluence%20of%20the%20implicit%20sparsity%20bias%20of%20common%20stochastic%20gradient%0Aprocedures.%0A%20%20This%20contribution%20investigates%20the%20use%20of%20very%20high%20dropout%20rates%20instead%20of%0Aensembles%20to%20obtain%20such%20rich%20representations.%20Although%20training%20a%20deep%20network%0Afrom%20scratch%20using%20such%20dropout%20rates%20is%20virtually%20impossible%2C%20fine-tuning%20a%0Alarge%20pre-trained%20model%20under%20such%20conditions%20is%20not%20only%20possible%20but%20also%0Aachieves%20out-of-distribution%20performances%20that%20exceed%20those%20of%20both%20ensembles%0Aand%20weight%20averaging%20methods%20such%20as%20model%20soups.%20This%20result%20has%20practical%0Asignificance%20because%20the%20importance%20of%20the%20fine-tuning%20scenario%20has%0Aconsiderably%20grown%20in%20recent%20years.%20This%20result%20also%20provides%20interesting%0Ainsights%20on%20the%20nature%20of%20rich%20representations%20and%20on%20the%20intrinsically%20linear%0Anature%20of%20fine-tuning%20a%20large%20network%20using%20a%20comparatively%20small%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00946v1&entry.124074799=Read"},
{"title": "Boosting Box-supervised Instance Segmentation with Pseudo Depth", "author": "Xinyi Yu and Ling Yan and Pengtao Jiang and Hao Chen and Bo Li and Lin Yuanbo Wu and Linlin Ou", "abstract": "  The realm of Weakly Supervised Instance Segmentation (WSIS) under box\nsupervision has garnered substantial attention, showcasing remarkable\nadvancements in recent years. However, the limitations of box supervision\nbecome apparent in its inability to furnish effective information for\ndistinguishing foreground from background within the specified target box. This\nresearch addresses this challenge by introducing pseudo-depth maps into the\ntraining process of the instance segmentation network, thereby boosting its\nperformance by capturing depth differences between instances. These\npseudo-depth maps are generated using a readily available depth predictor and\nare not necessary during the inference stage. To enable the network to discern\ndepth features when predicting masks, we integrate a depth prediction layer\ninto the mask prediction head. This innovative approach empowers the network to\nsimultaneously predict masks and depth, enhancing its ability to capture\nnuanced depth-related information during the instance segmentation process. We\nfurther utilize the mask generated in the training process as supervision to\ndistinguish the foreground from the background. When selecting the best mask\nfor each box through the Hungarian algorithm, we use depth consistency as one\ncalculation cost item. The proposed method achieves significant improvements on\nCityscapes and COCO dataset.\n", "link": "http://arxiv.org/abs/2403.01214v1", "date": "2024-03-02", "relevancy": 2.6276, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5335}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.531}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.512}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Box-supervised%20Instance%20Segmentation%20with%20Pseudo%20Depth&entry.906535625=Xinyi%20Yu%20and%20Ling%20Yan%20and%20Pengtao%20Jiang%20and%20Hao%20Chen%20and%20Bo%20Li%20and%20Lin%20Yuanbo%20Wu%20and%20Linlin%20Ou&entry.1292438233=%20%20The%20realm%20of%20Weakly%20Supervised%20Instance%20Segmentation%20%28WSIS%29%20under%20box%0Asupervision%20has%20garnered%20substantial%20attention%2C%20showcasing%20remarkable%0Aadvancements%20in%20recent%20years.%20However%2C%20the%20limitations%20of%20box%20supervision%0Abecome%20apparent%20in%20its%20inability%20to%20furnish%20effective%20information%20for%0Adistinguishing%20foreground%20from%20background%20within%20the%20specified%20target%20box.%20This%0Aresearch%20addresses%20this%20challenge%20by%20introducing%20pseudo-depth%20maps%20into%20the%0Atraining%20process%20of%20the%20instance%20segmentation%20network%2C%20thereby%20boosting%20its%0Aperformance%20by%20capturing%20depth%20differences%20between%20instances.%20These%0Apseudo-depth%20maps%20are%20generated%20using%20a%20readily%20available%20depth%20predictor%20and%0Aare%20not%20necessary%20during%20the%20inference%20stage.%20To%20enable%20the%20network%20to%20discern%0Adepth%20features%20when%20predicting%20masks%2C%20we%20integrate%20a%20depth%20prediction%20layer%0Ainto%20the%20mask%20prediction%20head.%20This%20innovative%20approach%20empowers%20the%20network%20to%0Asimultaneously%20predict%20masks%20and%20depth%2C%20enhancing%20its%20ability%20to%20capture%0Anuanced%20depth-related%20information%20during%20the%20instance%20segmentation%20process.%20We%0Afurther%20utilize%20the%20mask%20generated%20in%20the%20training%20process%20as%20supervision%20to%0Adistinguish%20the%20foreground%20from%20the%20background.%20When%20selecting%20the%20best%20mask%0Afor%20each%20box%20through%20the%20Hungarian%20algorithm%2C%20we%20use%20depth%20consistency%20as%20one%0Acalculation%20cost%20item.%20The%20proposed%20method%20achieves%20significant%20improvements%20on%0ACityscapes%20and%20COCO%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01214v1&entry.124074799=Read"},
{"title": "Decodable and Sample Invariant Continuous Object Encoder", "author": "Dehao Yuan and Furong Huang and Cornelia Ferm\u00fcller and Yiannis Aloimonos", "abstract": "  We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of a\ncontinuous object (e.g. a function), HDFE produces an explicit vector\nrepresentation of the given object, invariant to the sample distribution and\ndensity. Sample distribution and density invariance enables HDFE to\nconsistently encode continuous objects regardless of their sampling, and\ntherefore allows neural networks to receive continuous objects as inputs for\nmachine learning tasks, such as classification and regression. Besides, HDFE\ndoes not require any training and is proved to map the object into an organized\nembedding space, which facilitates the training of the downstream tasks. In\naddition, the encoding is decodable, which enables neural networks to regress\ncontinuous objects by regressing their encodings. Therefore, HDFE serves as an\ninterface for processing continuous objects.\n  We apply HDFE to function-to-function mapping, where vanilla HDFE achieves\ncompetitive performance as the state-of-the-art algorithm. We apply HDFE to\npoint cloud surface normal estimation, where a simple replacement from PointNet\nto HDFE leads to immediate 12% and 15% error reductions in two benchmarks. In\naddition, by integrating HDFE into the PointNet-based SOTA network, we improve\nthe SOTA baseline by 2.5% and 1.7% in the same benchmarks.\n", "link": "http://arxiv.org/abs/2311.00187v3", "date": "2024-03-02", "relevancy": 2.6135, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5715}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4995}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4971}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decodable%20and%20Sample%20Invariant%20Continuous%20Object%20Encoder&entry.906535625=Dehao%20Yuan%20and%20Furong%20Huang%20and%20Cornelia%20Ferm%C3%BCller%20and%20Yiannis%20Aloimonos&entry.1292438233=%20%20We%20propose%20Hyper-Dimensional%20Function%20Encoding%20%28HDFE%29.%20Given%20samples%20of%20a%0Acontinuous%20object%20%28e.g.%20a%20function%29%2C%20HDFE%20produces%20an%20explicit%20vector%0Arepresentation%20of%20the%20given%20object%2C%20invariant%20to%20the%20sample%20distribution%20and%0Adensity.%20Sample%20distribution%20and%20density%20invariance%20enables%20HDFE%20to%0Aconsistently%20encode%20continuous%20objects%20regardless%20of%20their%20sampling%2C%20and%0Atherefore%20allows%20neural%20networks%20to%20receive%20continuous%20objects%20as%20inputs%20for%0Amachine%20learning%20tasks%2C%20such%20as%20classification%20and%20regression.%20Besides%2C%20HDFE%0Adoes%20not%20require%20any%20training%20and%20is%20proved%20to%20map%20the%20object%20into%20an%20organized%0Aembedding%20space%2C%20which%20facilitates%20the%20training%20of%20the%20downstream%20tasks.%20In%0Aaddition%2C%20the%20encoding%20is%20decodable%2C%20which%20enables%20neural%20networks%20to%20regress%0Acontinuous%20objects%20by%20regressing%20their%20encodings.%20Therefore%2C%20HDFE%20serves%20as%20an%0Ainterface%20for%20processing%20continuous%20objects.%0A%20%20We%20apply%20HDFE%20to%20function-to-function%20mapping%2C%20where%20vanilla%20HDFE%20achieves%0Acompetitive%20performance%20as%20the%20state-of-the-art%20algorithm.%20We%20apply%20HDFE%20to%0Apoint%20cloud%20surface%20normal%20estimation%2C%20where%20a%20simple%20replacement%20from%20PointNet%0Ato%20HDFE%20leads%20to%20immediate%2012%25%20and%2015%25%20error%20reductions%20in%20two%20benchmarks.%20In%0Aaddition%2C%20by%20integrating%20HDFE%20into%20the%20PointNet-based%20SOTA%20network%2C%20we%20improve%0Athe%20SOTA%20baseline%20by%202.5%25%20and%201.7%25%20in%20the%20same%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00187v3&entry.124074799=Read"},
{"title": "Few-Shot Panoptic Segmentation With Foundation Models", "author": "Markus K\u00e4ppeler and K\u00fcrsat Petek and Niclas V\u00f6disch and Wolfram Burgard and Abhinav Valada", "abstract": "  Current state-of-the-art methods for panoptic segmentation require an immense\namount of annotated training data that is both arduous and expensive to obtain\nposing a significant challenge for their widespread adoption. Concurrently,\nrecent breakthroughs in visual representation learning have sparked a paradigm\nshift leading to the advent of large foundation models that can be trained with\ncompletely unlabeled images. In this work, we propose to leverage such\ntask-agnostic image features to enable few-shot panoptic segmentation by\npresenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In\ndetail, our method combines a DINOv2 backbone with lightweight network heads\nfor semantic segmentation and boundary estimation. We show that our approach,\nalbeit being trained with only ten annotated images, predicts high-quality\npseudo-labels that can be used with any existing panoptic segmentation method.\nNotably, we demonstrate that SPINO achieves competitive results compared to\nfully supervised baselines while using less than 0.3% of the ground truth\nlabels, paving the way for learning complex visual recognition tasks leveraging\nfoundation models. To illustrate its general applicability, we further deploy\nSPINO on real-world robotic vision systems for both outdoor and indoor\nenvironments. To foster future research, we make the code and trained models\npublicly available at http://spino.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2309.10726v3", "date": "2024-03-01", "relevancy": 2.5925, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5469}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Panoptic%20Segmentation%20With%20Foundation%20Models&entry.906535625=Markus%20K%C3%A4ppeler%20and%20K%C3%BCrsat%20Petek%20and%20Niclas%20V%C3%B6disch%20and%20Wolfram%20Burgard%20and%20Abhinav%20Valada&entry.1292438233=%20%20Current%20state-of-the-art%20methods%20for%20panoptic%20segmentation%20require%20an%20immense%0Aamount%20of%20annotated%20training%20data%20that%20is%20both%20arduous%20and%20expensive%20to%20obtain%0Aposing%20a%20significant%20challenge%20for%20their%20widespread%20adoption.%20Concurrently%2C%0Arecent%20breakthroughs%20in%20visual%20representation%20learning%20have%20sparked%20a%20paradigm%0Ashift%20leading%20to%20the%20advent%20of%20large%20foundation%20models%20that%20can%20be%20trained%20with%0Acompletely%20unlabeled%20images.%20In%20this%20work%2C%20we%20propose%20to%20leverage%20such%0Atask-agnostic%20image%20features%20to%20enable%20few-shot%20panoptic%20segmentation%20by%0Apresenting%20Segmenting%20Panoptic%20Information%20with%20Nearly%200%20labels%20%28SPINO%29.%20In%0Adetail%2C%20our%20method%20combines%20a%20DINOv2%20backbone%20with%20lightweight%20network%20heads%0Afor%20semantic%20segmentation%20and%20boundary%20estimation.%20We%20show%20that%20our%20approach%2C%0Aalbeit%20being%20trained%20with%20only%20ten%20annotated%20images%2C%20predicts%20high-quality%0Apseudo-labels%20that%20can%20be%20used%20with%20any%20existing%20panoptic%20segmentation%20method.%0ANotably%2C%20we%20demonstrate%20that%20SPINO%20achieves%20competitive%20results%20compared%20to%0Afully%20supervised%20baselines%20while%20using%20less%20than%200.3%25%20of%20the%20ground%20truth%0Alabels%2C%20paving%20the%20way%20for%20learning%20complex%20visual%20recognition%20tasks%20leveraging%0Afoundation%20models.%20To%20illustrate%20its%20general%20applicability%2C%20we%20further%20deploy%0ASPINO%20on%20real-world%20robotic%20vision%20systems%20for%20both%20outdoor%20and%20indoor%0Aenvironments.%20To%20foster%20future%20research%2C%20we%20make%20the%20code%20and%20trained%20models%0Apublicly%20available%20at%20http%3A//spino.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10726v3&entry.124074799=Read"},
{"title": "SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for\n  Indoor Localization", "author": "Danish Gufran and Saideep Tiku and Sudeep Pasricha", "abstract": "  Indoor localization is a critical task in many embedded applications, such as\nasset tracking, emergency response, and realtime navigation. In this article,\nwe propose a novel fingerprintingbased framework for indoor localization called\nSANGRIA that uses stacked autoencoder neural networks with gradient boosted\ntrees. Our approach is designed to overcome the device heterogeneity challenge\nthat can create uncertainty in wireless signal measurements across embedded\ndevices used for localization. We compare SANGRIA to several state-of-the-art\nframeworks and demonstrate 42.96% lower average localization error across\ndiverse indoor locales and heterogeneous devices.\n", "link": "http://arxiv.org/abs/2403.01348v1", "date": "2024-03-03", "relevancy": 2.5871, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5315}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4823}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SANGRIA%3A%20Stacked%20Autoencoder%20Neural%20Networks%20with%20Gradient%20Boosting%20for%0A%20%20Indoor%20Localization&entry.906535625=Danish%20Gufran%20and%20Saideep%20Tiku%20and%20Sudeep%20Pasricha&entry.1292438233=%20%20Indoor%20localization%20is%20a%20critical%20task%20in%20many%20embedded%20applications%2C%20such%20as%0Aasset%20tracking%2C%20emergency%20response%2C%20and%20realtime%20navigation.%20In%20this%20article%2C%0Awe%20propose%20a%20novel%20fingerprintingbased%20framework%20for%20indoor%20localization%20called%0ASANGRIA%20that%20uses%20stacked%20autoencoder%20neural%20networks%20with%20gradient%20boosted%0Atrees.%20Our%20approach%20is%20designed%20to%20overcome%20the%20device%20heterogeneity%20challenge%0Athat%20can%20create%20uncertainty%20in%20wireless%20signal%20measurements%20across%20embedded%0Adevices%20used%20for%20localization.%20We%20compare%20SANGRIA%20to%20several%20state-of-the-art%0Aframeworks%20and%20demonstrate%2042.96%25%20lower%20average%20localization%20error%20across%0Adiverse%20indoor%20locales%20and%20heterogeneous%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01348v1&entry.124074799=Read"},
{"title": "Unpaired Image-to-Image Translation via Neural Schr\u00f6dinger Bridge", "author": "Beomsu Kim and Gihyun Kwon and Kwanyoung Kim and Jong Chul Ye", "abstract": "  Diffusion models are a powerful class of generative models which simulate\nstochastic differential equations (SDEs) to generate data from noise. While\ndiffusion models have achieved remarkable progress, they have limitations in\nunpaired image-to-image (I2I) translation tasks due to the Gaussian prior\nassumption. Schr\\\"{o}dinger Bridge (SB), which learns an SDE to translate\nbetween two arbitrary distributions, have risen as an attractive solution to\nthis problem. Yet, to our best knowledge, none of SB models so far have been\nsuccessful at unpaired translation between high-resolution images. In this\nwork, we propose Unpaired Neural Schr\\\"{o}dinger Bridge (UNSB), which expresses\nthe SB problem as a sequence of adversarial learning problems. This allows us\nto incorporate advanced discriminators and regularization to learn a SB between\nunpaired data. We show that UNSB is scalable and successfully solves various\nunpaired I2I translation tasks. Code: \\url{https://github.com/cyclomon/UNSB}\n", "link": "http://arxiv.org/abs/2305.15086v3", "date": "2024-03-02", "relevancy": 2.5865, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5277}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5122}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.512}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unpaired%20Image-to-Image%20Translation%20via%20Neural%20Schr%C3%B6dinger%20Bridge&entry.906535625=Beomsu%20Kim%20and%20Gihyun%20Kwon%20and%20Kwanyoung%20Kim%20and%20Jong%20Chul%20Ye&entry.1292438233=%20%20Diffusion%20models%20are%20a%20powerful%20class%20of%20generative%20models%20which%20simulate%0Astochastic%20differential%20equations%20%28SDEs%29%20to%20generate%20data%20from%20noise.%20While%0Adiffusion%20models%20have%20achieved%20remarkable%20progress%2C%20they%20have%20limitations%20in%0Aunpaired%20image-to-image%20%28I2I%29%20translation%20tasks%20due%20to%20the%20Gaussian%20prior%0Aassumption.%20Schr%5C%22%7Bo%7Ddinger%20Bridge%20%28SB%29%2C%20which%20learns%20an%20SDE%20to%20translate%0Abetween%20two%20arbitrary%20distributions%2C%20have%20risen%20as%20an%20attractive%20solution%20to%0Athis%20problem.%20Yet%2C%20to%20our%20best%20knowledge%2C%20none%20of%20SB%20models%20so%20far%20have%20been%0Asuccessful%20at%20unpaired%20translation%20between%20high-resolution%20images.%20In%20this%0Awork%2C%20we%20propose%20Unpaired%20Neural%20Schr%5C%22%7Bo%7Ddinger%20Bridge%20%28UNSB%29%2C%20which%20expresses%0Athe%20SB%20problem%20as%20a%20sequence%20of%20adversarial%20learning%20problems.%20This%20allows%20us%0Ato%20incorporate%20advanced%20discriminators%20and%20regularization%20to%20learn%20a%20SB%20between%0Aunpaired%20data.%20We%20show%20that%20UNSB%20is%20scalable%20and%20successfully%20solves%20various%0Aunpaired%20I2I%20translation%20tasks.%20Code%3A%20%5Curl%7Bhttps%3A//github.com/cyclomon/UNSB%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15086v3&entry.124074799=Read"},
{"title": "Edge-guided Low-light Image Enhancement with Inertial Bregman\n  Alternating Linearized Minimization", "author": "Chaoyan Huang and Zhongming Wu and Tieyong Zeng", "abstract": "  Prior-based methods for low-light image enhancement often face challenges in\nextracting available prior information from dim images. To overcome this\nlimitation, we introduce a simple yet effective Retinex model with the proposed\nedge extraction prior. More specifically, we design an edge extraction network\nto capture the fine edge features from the low-light image directly. Building\nupon the Retinex theory, we decompose the low-light image into its illumination\nand reflectance components and introduce an edge-guided Retinex model for\nenhancing low-light images. To solve the proposed model, we propose a novel\ninertial Bregman alternating linearized minimization algorithm. This algorithm\naddresses the optimization problem associated with the edge-guided Retinex\nmodel, enabling effective enhancement of low-light images. Through rigorous\ntheoretical analysis, we establish the convergence properties of the algorithm.\nBesides, we prove that the proposed algorithm converges to a stationary point\nof the problem through nonconvex optimization theory. Furthermore, extensive\nexperiments are conducted on multiple real-world low-light image datasets to\ndemonstrate the efficiency and superiority of the proposed scheme.\n", "link": "http://arxiv.org/abs/2403.01142v1", "date": "2024-03-02", "relevancy": 2.5827, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5378}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5083}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5036}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-guided%20Low-light%20Image%20Enhancement%20with%20Inertial%20Bregman%0A%20%20Alternating%20Linearized%20Minimization&entry.906535625=Chaoyan%20Huang%20and%20Zhongming%20Wu%20and%20Tieyong%20Zeng&entry.1292438233=%20%20Prior-based%20methods%20for%20low-light%20image%20enhancement%20often%20face%20challenges%20in%0Aextracting%20available%20prior%20information%20from%20dim%20images.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20a%20simple%20yet%20effective%20Retinex%20model%20with%20the%20proposed%0Aedge%20extraction%20prior.%20More%20specifically%2C%20we%20design%20an%20edge%20extraction%20network%0Ato%20capture%20the%20fine%20edge%20features%20from%20the%20low-light%20image%20directly.%20Building%0Aupon%20the%20Retinex%20theory%2C%20we%20decompose%20the%20low-light%20image%20into%20its%20illumination%0Aand%20reflectance%20components%20and%20introduce%20an%20edge-guided%20Retinex%20model%20for%0Aenhancing%20low-light%20images.%20To%20solve%20the%20proposed%20model%2C%20we%20propose%20a%20novel%0Ainertial%20Bregman%20alternating%20linearized%20minimization%20algorithm.%20This%20algorithm%0Aaddresses%20the%20optimization%20problem%20associated%20with%20the%20edge-guided%20Retinex%0Amodel%2C%20enabling%20effective%20enhancement%20of%20low-light%20images.%20Through%20rigorous%0Atheoretical%20analysis%2C%20we%20establish%20the%20convergence%20properties%20of%20the%20algorithm.%0ABesides%2C%20we%20prove%20that%20the%20proposed%20algorithm%20converges%20to%20a%20stationary%20point%0Aof%20the%20problem%20through%20nonconvex%20optimization%20theory.%20Furthermore%2C%20extensive%0Aexperiments%20are%20conducted%20on%20multiple%20real-world%20low-light%20image%20datasets%20to%0Ademonstrate%20the%20efficiency%20and%20superiority%20of%20the%20proposed%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01142v1&entry.124074799=Read"},
{"title": "Out-of-Distribution Detection using Neural Activation Prior", "author": "Weilin Wan and Weizhong Zhang and Cheng Jin", "abstract": "  Out-of-distribution detection is a crucial technique for deploying machine\nlearning models in the real world to handle the unseen scenarios. In this\npaper, we propose a simple but effective Neural Activation Prior (NAP) for\nout-of-distribution detection (OOD). Our neural activation prior is based on a\nkey observation that, for a channel before the global pooling layer of a fully\ntrained neural network, the probability of a few of its neurons being activated\nwith a larger response by an in-distribution (ID) sample is significantly\nhigher than that by an OOD sample. An intuitive explanation is each channel in\na model fully trained on ID dataset would play a role in detecting a certain\npattern in the samples within the ID dataset, and a few neurons can be\nactivated with a large response when the pattern is detected in an input\nsample. Thus, a new scoring function based on this prior is proposed to\nhighlight the role of these strongly activated neurons in OOD detection. This\napproach is plug-and-play and does not lead to any performance degradation on\nin-distribution data classification and requires no extra training or\nstatistics from training or external datasets. Notice that previous methods\nprimarily rely on post-global-pooling features of the neural networks, while\nthe within-channel distribution information we leverage would be discarded by\nthe global pooling operator. Consequently, our method is orthogonal to existing\napproaches and can be effectively combined with them in various applications.\nExperimental results show that our method achieves the state-of-the-art\nperformance on CIFAR-10, CIFAR-100 and ImageNet datasets, which demonstrates\nthe power of the proposed prior.\n", "link": "http://arxiv.org/abs/2402.18162v2", "date": "2024-03-01", "relevancy": 2.5711, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5473}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5104}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4849}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-Distribution%20Detection%20using%20Neural%20Activation%20Prior&entry.906535625=Weilin%20Wan%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin&entry.1292438233=%20%20Out-of-distribution%20detection%20is%20a%20crucial%20technique%20for%20deploying%20machine%0Alearning%20models%20in%20the%20real%20world%20to%20handle%20the%20unseen%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20a%20simple%20but%20effective%20Neural%20Activation%20Prior%20%28NAP%29%20for%0Aout-of-distribution%20detection%20%28OOD%29.%20Our%20neural%20activation%20prior%20is%20based%20on%20a%0Akey%20observation%20that%2C%20for%20a%20channel%20before%20the%20global%20pooling%20layer%20of%20a%20fully%0Atrained%20neural%20network%2C%20the%20probability%20of%20a%20few%20of%20its%20neurons%20being%20activated%0Awith%20a%20larger%20response%20by%20an%20in-distribution%20%28ID%29%20sample%20is%20significantly%0Ahigher%20than%20that%20by%20an%20OOD%20sample.%20An%20intuitive%20explanation%20is%20each%20channel%20in%0Aa%20model%20fully%20trained%20on%20ID%20dataset%20would%20play%20a%20role%20in%20detecting%20a%20certain%0Apattern%20in%20the%20samples%20within%20the%20ID%20dataset%2C%20and%20a%20few%20neurons%20can%20be%0Aactivated%20with%20a%20large%20response%20when%20the%20pattern%20is%20detected%20in%20an%20input%0Asample.%20Thus%2C%20a%20new%20scoring%20function%20based%20on%20this%20prior%20is%20proposed%20to%0Ahighlight%20the%20role%20of%20these%20strongly%20activated%20neurons%20in%20OOD%20detection.%20This%0Aapproach%20is%20plug-and-play%20and%20does%20not%20lead%20to%20any%20performance%20degradation%20on%0Ain-distribution%20data%20classification%20and%20requires%20no%20extra%20training%20or%0Astatistics%20from%20training%20or%20external%20datasets.%20Notice%20that%20previous%20methods%0Aprimarily%20rely%20on%20post-global-pooling%20features%20of%20the%20neural%20networks%2C%20while%0Athe%20within-channel%20distribution%20information%20we%20leverage%20would%20be%20discarded%20by%0Athe%20global%20pooling%20operator.%20Consequently%2C%20our%20method%20is%20orthogonal%20to%20existing%0Aapproaches%20and%20can%20be%20effectively%20combined%20with%20them%20in%20various%20applications.%0AExperimental%20results%20show%20that%20our%20method%20achieves%20the%20state-of-the-art%0Aperformance%20on%20CIFAR-10%2C%20CIFAR-100%20and%20ImageNet%20datasets%2C%20which%20demonstrates%0Athe%20power%20of%20the%20proposed%20prior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18162v2&entry.124074799=Read"},
{"title": "Fusion is Not Enough: Single Modal Attacks on Fusion Models for 3D\n  Object Detection", "author": "Zhiyuan Cheng and Hongjun Choi and James Liang and Shiwei Feng and Guanhong Tao and Dongfang Liu and Michael Zuzak and Xiangyu Zhang", "abstract": "  Multi-sensor fusion (MSF) is widely used in autonomous vehicles (AVs) for\nperception, particularly for 3D object detection with camera and LiDAR sensors.\nThe purpose of fusion is to capitalize on the advantages of each modality while\nminimizing its weaknesses. Advanced deep neural network (DNN)-based fusion\ntechniques have demonstrated the exceptional and industry-leading performance.\nDue to the redundant information in multiple modalities, MSF is also recognized\nas a general defence strategy against adversarial attacks. In this paper, we\nattack fusion models from the camera modality that is considered to be of\nlesser importance in fusion but is more affordable for attackers. We argue that\nthe weakest link of fusion models depends on their most vulnerable modality,\nand propose an attack framework that targets advanced camera-LiDAR fusion-based\n3D object detection models through camera-only adversarial attacks. Our\napproach employs a two-stage optimization-based strategy that first thoroughly\nevaluates vulnerable image areas under adversarial attacks, and then applies\ndedicated attack strategies for different fusion models to generate deployable\npatches. The evaluations with six advanced camera-LiDAR fusion models and one\ncamera-only model indicate that our attacks successfully compromise all of\nthem. Our approach can either decrease the mean average precision (mAP) of\ndetection performance from 0.824 to 0.353, or degrade the detection score of a\ntarget object from 0.728 to 0.156, demonstrating the efficacy of our proposed\nattack framework. Code is available.\n", "link": "http://arxiv.org/abs/2304.14614v3", "date": "2024-03-02", "relevancy": 2.5666, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5221}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5173}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5005}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20is%20Not%20Enough%3A%20Single%20Modal%20Attacks%20on%20Fusion%20Models%20for%203D%0A%20%20Object%20Detection&entry.906535625=Zhiyuan%20Cheng%20and%20Hongjun%20Choi%20and%20James%20Liang%20and%20Shiwei%20Feng%20and%20Guanhong%20Tao%20and%20Dongfang%20Liu%20and%20Michael%20Zuzak%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20Multi-sensor%20fusion%20%28MSF%29%20is%20widely%20used%20in%20autonomous%20vehicles%20%28AVs%29%20for%0Aperception%2C%20particularly%20for%203D%20object%20detection%20with%20camera%20and%20LiDAR%20sensors.%0AThe%20purpose%20of%20fusion%20is%20to%20capitalize%20on%20the%20advantages%20of%20each%20modality%20while%0Aminimizing%20its%20weaknesses.%20Advanced%20deep%20neural%20network%20%28DNN%29-based%20fusion%0Atechniques%20have%20demonstrated%20the%20exceptional%20and%20industry-leading%20performance.%0ADue%20to%20the%20redundant%20information%20in%20multiple%20modalities%2C%20MSF%20is%20also%20recognized%0Aas%20a%20general%20defence%20strategy%20against%20adversarial%20attacks.%20In%20this%20paper%2C%20we%0Aattack%20fusion%20models%20from%20the%20camera%20modality%20that%20is%20considered%20to%20be%20of%0Alesser%20importance%20in%20fusion%20but%20is%20more%20affordable%20for%20attackers.%20We%20argue%20that%0Athe%20weakest%20link%20of%20fusion%20models%20depends%20on%20their%20most%20vulnerable%20modality%2C%0Aand%20propose%20an%20attack%20framework%20that%20targets%20advanced%20camera-LiDAR%20fusion-based%0A3D%20object%20detection%20models%20through%20camera-only%20adversarial%20attacks.%20Our%0Aapproach%20employs%20a%20two-stage%20optimization-based%20strategy%20that%20first%20thoroughly%0Aevaluates%20vulnerable%20image%20areas%20under%20adversarial%20attacks%2C%20and%20then%20applies%0Adedicated%20attack%20strategies%20for%20different%20fusion%20models%20to%20generate%20deployable%0Apatches.%20The%20evaluations%20with%20six%20advanced%20camera-LiDAR%20fusion%20models%20and%20one%0Acamera-only%20model%20indicate%20that%20our%20attacks%20successfully%20compromise%20all%20of%0Athem.%20Our%20approach%20can%20either%20decrease%20the%20mean%20average%20precision%20%28mAP%29%20of%0Adetection%20performance%20from%200.824%20to%200.353%2C%20or%20degrade%20the%20detection%20score%20of%20a%0Atarget%20object%20from%200.728%20to%200.156%2C%20demonstrating%20the%20efficacy%20of%20our%20proposed%0Aattack%20framework.%20Code%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.14614v3&entry.124074799=Read"},
{"title": "Enhancing Retinal Vascular Structure Segmentation in Images With a Novel\n  Design Two-Path Interactive Fusion Module Model", "author": "Rui Yang and Shunpu Zhang", "abstract": "  Precision in identifying and differentiating micro and macro blood vessels in\nthe retina is crucial for the diagnosis of retinal diseases, although it poses\na significant challenge. Current autoencoding-based segmentation approaches\nencounter limitations as they are constrained by the encoder and undergo a\nreduction in resolution during the encoding stage. The inability to recover\nlost information in the decoding phase further impedes these approaches.\nConsequently, their capacity to extract the retinal microvascular structure is\nrestricted. To address this issue, we introduce Swin-Res-Net, a specialized\nmodule designed to enhance the precision of retinal vessel segmentation.\nSwin-Res-Net utilizes the Swin transformer which uses shifted windows with\ndisplacement for partitioning, to reduce network complexity and accelerate\nmodel convergence. Additionally, the model incorporates interactive fusion with\na functional module in the Res2Net architecture. The Res2Net leverages\nmulti-scale techniques to enlarge the receptive field of the convolutional\nkernel, enabling the extraction of additional semantic information from the\nimage. This combination creates a new module that enhances the localization and\nseparation of micro vessels in the retina. To improve the efficiency of\nprocessing vascular information, we've added a module to eliminate redundant\ninformation between the encoding and decoding steps.\n  Our proposed architecture produces outstanding results, either meeting or\nsurpassing those of other published models. The AUC reflects significant\nenhancements, achieving values of 0.9956, 0.9931, and 0.9946 in pixel-wise\nsegmentation of retinal vessels across three widely utilized datasets:\nCHASE-DB1, DRIVE, and STARE, respectively. Moreover, Swin-Res-Net outperforms\nalternative architectures, demonstrating superior performance in both IOU and\nF1 measure metrics.\n", "link": "http://arxiv.org/abs/2403.01362v1", "date": "2024-03-03", "relevancy": 2.5614, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.543}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4998}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.494}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Retinal%20Vascular%20Structure%20Segmentation%20in%20Images%20With%20a%20Novel%0A%20%20Design%20Two-Path%20Interactive%20Fusion%20Module%20Model&entry.906535625=Rui%20Yang%20and%20Shunpu%20Zhang&entry.1292438233=%20%20Precision%20in%20identifying%20and%20differentiating%20micro%20and%20macro%20blood%20vessels%20in%0Athe%20retina%20is%20crucial%20for%20the%20diagnosis%20of%20retinal%20diseases%2C%20although%20it%20poses%0Aa%20significant%20challenge.%20Current%20autoencoding-based%20segmentation%20approaches%0Aencounter%20limitations%20as%20they%20are%20constrained%20by%20the%20encoder%20and%20undergo%20a%0Areduction%20in%20resolution%20during%20the%20encoding%20stage.%20The%20inability%20to%20recover%0Alost%20information%20in%20the%20decoding%20phase%20further%20impedes%20these%20approaches.%0AConsequently%2C%20their%20capacity%20to%20extract%20the%20retinal%20microvascular%20structure%20is%0Arestricted.%20To%20address%20this%20issue%2C%20we%20introduce%20Swin-Res-Net%2C%20a%20specialized%0Amodule%20designed%20to%20enhance%20the%20precision%20of%20retinal%20vessel%20segmentation.%0ASwin-Res-Net%20utilizes%20the%20Swin%20transformer%20which%20uses%20shifted%20windows%20with%0Adisplacement%20for%20partitioning%2C%20to%20reduce%20network%20complexity%20and%20accelerate%0Amodel%20convergence.%20Additionally%2C%20the%20model%20incorporates%20interactive%20fusion%20with%0Aa%20functional%20module%20in%20the%20Res2Net%20architecture.%20The%20Res2Net%20leverages%0Amulti-scale%20techniques%20to%20enlarge%20the%20receptive%20field%20of%20the%20convolutional%0Akernel%2C%20enabling%20the%20extraction%20of%20additional%20semantic%20information%20from%20the%0Aimage.%20This%20combination%20creates%20a%20new%20module%20that%20enhances%20the%20localization%20and%0Aseparation%20of%20micro%20vessels%20in%20the%20retina.%20To%20improve%20the%20efficiency%20of%0Aprocessing%20vascular%20information%2C%20we%27ve%20added%20a%20module%20to%20eliminate%20redundant%0Ainformation%20between%20the%20encoding%20and%20decoding%20steps.%0A%20%20Our%20proposed%20architecture%20produces%20outstanding%20results%2C%20either%20meeting%20or%0Asurpassing%20those%20of%20other%20published%20models.%20The%20AUC%20reflects%20significant%0Aenhancements%2C%20achieving%20values%20of%200.9956%2C%200.9931%2C%20and%200.9946%20in%20pixel-wise%0Asegmentation%20of%20retinal%20vessels%20across%20three%20widely%20utilized%20datasets%3A%0ACHASE-DB1%2C%20DRIVE%2C%20and%20STARE%2C%20respectively.%20Moreover%2C%20Swin-Res-Net%20outperforms%0Aalternative%20architectures%2C%20demonstrating%20superior%20performance%20in%20both%20IOU%20and%0AF1%20measure%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01362v1&entry.124074799=Read"},
{"title": "Generalized User Representations for Transfer Learning", "author": "Ghazal Fazelnia and Sanket Gupta and Claire Keum and Mark Koh and Ian Anderson and Mounia Lalmas", "abstract": "  We present a novel framework for user representation in large-scale\nrecommender systems, aiming at effectively representing diverse user taste in a\ngeneralized manner. Our approach employs a two-stage methodology combining\nrepresentation learning and transfer learning. The representation learning\nmodel uses an autoencoder that compresses various user features into a\nrepresentation space. In the second stage, downstream task-specific models\nleverage user representations via transfer learning instead of curating user\nfeatures individually. We further augment this methodology on the\nrepresentation's input features to increase flexibility and enable reaction to\nuser events, including new user experiences, in Near-Real Time. Additionally,\nwe propose a novel solution to manage deployment of this framework in\nproduction models, allowing downstream models to work independently. We\nvalidate the performance of our framework through rigorous offline and online\nexperiments within a large-scale system, showcasing its remarkable efficacy\nacross multiple evaluation tasks. Finally, we show how the proposed framework\ncan significantly reduce infrastructure costs compared to alternative\napproaches.\n", "link": "http://arxiv.org/abs/2403.00584v1", "date": "2024-03-01", "relevancy": 2.5525, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5103}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5102}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20User%20Representations%20for%20Transfer%20Learning&entry.906535625=Ghazal%20Fazelnia%20and%20Sanket%20Gupta%20and%20Claire%20Keum%20and%20Mark%20Koh%20and%20Ian%20Anderson%20and%20Mounia%20Lalmas&entry.1292438233=%20%20We%20present%20a%20novel%20framework%20for%20user%20representation%20in%20large-scale%0Arecommender%20systems%2C%20aiming%20at%20effectively%20representing%20diverse%20user%20taste%20in%20a%0Ageneralized%20manner.%20Our%20approach%20employs%20a%20two-stage%20methodology%20combining%0Arepresentation%20learning%20and%20transfer%20learning.%20The%20representation%20learning%0Amodel%20uses%20an%20autoencoder%20that%20compresses%20various%20user%20features%20into%20a%0Arepresentation%20space.%20In%20the%20second%20stage%2C%20downstream%20task-specific%20models%0Aleverage%20user%20representations%20via%20transfer%20learning%20instead%20of%20curating%20user%0Afeatures%20individually.%20We%20further%20augment%20this%20methodology%20on%20the%0Arepresentation%27s%20input%20features%20to%20increase%20flexibility%20and%20enable%20reaction%20to%0Auser%20events%2C%20including%20new%20user%20experiences%2C%20in%20Near-Real%20Time.%20Additionally%2C%0Awe%20propose%20a%20novel%20solution%20to%20manage%20deployment%20of%20this%20framework%20in%0Aproduction%20models%2C%20allowing%20downstream%20models%20to%20work%20independently.%20We%0Avalidate%20the%20performance%20of%20our%20framework%20through%20rigorous%20offline%20and%20online%0Aexperiments%20within%20a%20large-scale%20system%2C%20showcasing%20its%20remarkable%20efficacy%0Aacross%20multiple%20evaluation%20tasks.%20Finally%2C%20we%20show%20how%20the%20proposed%20framework%0Acan%20significantly%20reduce%20infrastructure%20costs%20compared%20to%20alternative%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00584v1&entry.124074799=Read"},
{"title": "A Hybrid Model for Traffic Incident Detection based on Generative\n  Adversarial Networks and Transformer Model", "author": "Xinying Lu and Doudou Zhang and Jianli Xiao", "abstract": "  In addition to enhancing traffic safety and facilitating prompt emergency\nresponse, traffic incident detection plays an indispensable role in intelligent\ntransportation systems by providing real-time traffic status information. This\nenables the realization of intelligent traffic control and management. Previous\nresearch has identified that apart from employing advanced algorithmic models,\nthe effectiveness of detection is also significantly influenced by challenges\nrelated to acquiring large datasets and addressing dataset imbalances. A hybrid\nmodel combining transformer and generative adversarial networks (GANs) is\nproposed to address these challenges. Experiments are conducted on four real\ndatasets to validate the superiority of the transformer in traffic incident\ndetection. Additionally, GANs are utilized to expand the dataset and achieve a\nbalanced ratio of 1:4, 2:3, and 1:1. The proposed model is evaluated against\nthe baseline model. The results demonstrate that the proposed model enhances\nthe dataset size, balances the dataset, and improves the performance of traffic\nincident detection in various aspects.\n", "link": "http://arxiv.org/abs/2403.01147v1", "date": "2024-03-02", "relevancy": 2.5465, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5293}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5154}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4832}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Model%20for%20Traffic%20Incident%20Detection%20based%20on%20Generative%0A%20%20Adversarial%20Networks%20and%20Transformer%20Model&entry.906535625=Xinying%20Lu%20and%20Doudou%20Zhang%20and%20Jianli%20Xiao&entry.1292438233=%20%20In%20addition%20to%20enhancing%20traffic%20safety%20and%20facilitating%20prompt%20emergency%0Aresponse%2C%20traffic%20incident%20detection%20plays%20an%20indispensable%20role%20in%20intelligent%0Atransportation%20systems%20by%20providing%20real-time%20traffic%20status%20information.%20This%0Aenables%20the%20realization%20of%20intelligent%20traffic%20control%20and%20management.%20Previous%0Aresearch%20has%20identified%20that%20apart%20from%20employing%20advanced%20algorithmic%20models%2C%0Athe%20effectiveness%20of%20detection%20is%20also%20significantly%20influenced%20by%20challenges%0Arelated%20to%20acquiring%20large%20datasets%20and%20addressing%20dataset%20imbalances.%20A%20hybrid%0Amodel%20combining%20transformer%20and%20generative%20adversarial%20networks%20%28GANs%29%20is%0Aproposed%20to%20address%20these%20challenges.%20Experiments%20are%20conducted%20on%20four%20real%0Adatasets%20to%20validate%20the%20superiority%20of%20the%20transformer%20in%20traffic%20incident%0Adetection.%20Additionally%2C%20GANs%20are%20utilized%20to%20expand%20the%20dataset%20and%20achieve%20a%0Abalanced%20ratio%20of%201%3A4%2C%202%3A3%2C%20and%201%3A1.%20The%20proposed%20model%20is%20evaluated%20against%0Athe%20baseline%20model.%20The%20results%20demonstrate%20that%20the%20proposed%20model%20enhances%0Athe%20dataset%20size%2C%20balances%20the%20dataset%2C%20and%20improves%20the%20performance%20of%20traffic%0Aincident%20detection%20in%20various%20aspects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01147v1&entry.124074799=Read"},
{"title": "Removal and Selection: Improving RGB-Infrared Object Detection via\n  Coarse-to-Fine Fusion", "author": "Tianyi Zhao and Maoxun Yuan and Xingxing Wei", "abstract": "  Object detection in visible (RGB) and infrared (IR) images has been widely\napplied in recent years. Leveraging the complementary characteristics of RGB\nand IR images, the object detector provides reliable and robust object\nlocalization from day to night. Existing fusion strategies directly inject RGB\nand IR images into convolution neural networks, leading to inferior detection\nperformance. Since the RGB and IR features have modality-specific noise, these\nstrategies will worsen the fused features along with the propagation. Inspired\nby the mechanism of human brain processing multimodal information, this work\nintroduces a new coarse-to-fine perspective to purify and fuse two modality\nfeatures. Specifically, following this perspective, we design a Redundant\nSpectrum Removal module to coarsely remove interfering information within each\nmodality and a Dynamic Feature Selection module to finely select the desired\nfeatures for feature fusion. To verify the effectiveness of the coarse-to-fine\nfusion strategy, we construct a new object detector called Removal and\nSelection Detector (RSDet). Extensive experiments on three RGB-IR object\ndetection datasets verify the superior performance of our method.\n", "link": "http://arxiv.org/abs/2401.10731v3", "date": "2024-03-02", "relevancy": 2.5458, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5179}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5154}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4942}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Removal%20and%20Selection%3A%20Improving%20RGB-Infrared%20Object%20Detection%20via%0A%20%20Coarse-to-Fine%20Fusion&entry.906535625=Tianyi%20Zhao%20and%20Maoxun%20Yuan%20and%20Xingxing%20Wei&entry.1292438233=%20%20Object%20detection%20in%20visible%20%28RGB%29%20and%20infrared%20%28IR%29%20images%20has%20been%20widely%0Aapplied%20in%20recent%20years.%20Leveraging%20the%20complementary%20characteristics%20of%20RGB%0Aand%20IR%20images%2C%20the%20object%20detector%20provides%20reliable%20and%20robust%20object%0Alocalization%20from%20day%20to%20night.%20Existing%20fusion%20strategies%20directly%20inject%20RGB%0Aand%20IR%20images%20into%20convolution%20neural%20networks%2C%20leading%20to%20inferior%20detection%0Aperformance.%20Since%20the%20RGB%20and%20IR%20features%20have%20modality-specific%20noise%2C%20these%0Astrategies%20will%20worsen%20the%20fused%20features%20along%20with%20the%20propagation.%20Inspired%0Aby%20the%20mechanism%20of%20human%20brain%20processing%20multimodal%20information%2C%20this%20work%0Aintroduces%20a%20new%20coarse-to-fine%20perspective%20to%20purify%20and%20fuse%20two%20modality%0Afeatures.%20Specifically%2C%20following%20this%20perspective%2C%20we%20design%20a%20Redundant%0ASpectrum%20Removal%20module%20to%20coarsely%20remove%20interfering%20information%20within%20each%0Amodality%20and%20a%20Dynamic%20Feature%20Selection%20module%20to%20finely%20select%20the%20desired%0Afeatures%20for%20feature%20fusion.%20To%20verify%20the%20effectiveness%20of%20the%20coarse-to-fine%0Afusion%20strategy%2C%20we%20construct%20a%20new%20object%20detector%20called%20Removal%20and%0ASelection%20Detector%20%28RSDet%29.%20Extensive%20experiments%20on%20three%20RGB-IR%20object%0Adetection%20datasets%20verify%20the%20superior%20performance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10731v3&entry.124074799=Read"},
{"title": "Emergence of Latent Binary Encoding in Deep Neural Network Classifiers", "author": "Luigi Sbail\u00f2 and Luca Ghiringhelli", "abstract": "  We investigate the emergence of binary encoding within the latent space of\ndeep-neural-network classifiers. Such binary encoding is induced by the\nintegration of a linear penultimate layer, which employs during training a loss\nfunction specifically designed to compress the latent representations. As a\nresult of a trade-off between compression and information retention, the\nnetwork learns to assume only one of two possible values for each dimension in\nthe latent space. The binary encoding is provoked by the collapse of all\nrepresentations of the same class to the same point, which corresponds to the\nvertex of a hypercube, thereby creating the encoding. We demonstrate that the\nemergence of binary encoding significantly enhances robustness, reliability and\naccuracy of the network.\n", "link": "http://arxiv.org/abs/2310.08224v3", "date": "2024-03-01", "relevancy": 2.5373, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.537}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4948}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4906}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20Latent%20Binary%20Encoding%20in%20Deep%20Neural%20Network%20Classifiers&entry.906535625=Luigi%20Sbail%C3%B2%20and%20Luca%20Ghiringhelli&entry.1292438233=%20%20We%20investigate%20the%20emergence%20of%20binary%20encoding%20within%20the%20latent%20space%20of%0Adeep-neural-network%20classifiers.%20Such%20binary%20encoding%20is%20induced%20by%20the%0Aintegration%20of%20a%20linear%20penultimate%20layer%2C%20which%20employs%20during%20training%20a%20loss%0Afunction%20specifically%20designed%20to%20compress%20the%20latent%20representations.%20As%20a%0Aresult%20of%20a%20trade-off%20between%20compression%20and%20information%20retention%2C%20the%0Anetwork%20learns%20to%20assume%20only%20one%20of%20two%20possible%20values%20for%20each%20dimension%20in%0Athe%20latent%20space.%20The%20binary%20encoding%20is%20provoked%20by%20the%20collapse%20of%20all%0Arepresentations%20of%20the%20same%20class%20to%20the%20same%20point%2C%20which%20corresponds%20to%20the%0Avertex%20of%20a%20hypercube%2C%20thereby%20creating%20the%20encoding.%20We%20demonstrate%20that%20the%0Aemergence%20of%20binary%20encoding%20significantly%20enhances%20robustness%2C%20reliability%20and%0Aaccuracy%20of%20the%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08224v3&entry.124074799=Read"},
{"title": "Improving Explicit Spatial Relationships in Text-to-Image Generation\n  through an Automatically Derived Dataset", "author": "Ander Salaberria and Gorka Azkune and Oier Lopez de Lacalle and Aitor Soroa and Eneko Agirre and Frank Keller", "abstract": "  Existing work has observed that current text-to-image systems do not\naccurately reflect explicit spatial relations between objects such as 'left of'\nor 'below'. We hypothesize that this is because explicit spatial relations\nrarely appear in the image captions used to train these models. We propose an\nautomatic method that, given existing images, generates synthetic captions that\ncontain 14 explicit spatial relations. We introduce the Spatial Relation for\nGeneration (SR4G) dataset, which contains 9.9 millions image-caption pairs for\ntraining, and more than 60 thousand captions for evaluation. In order to test\ngeneralization we also provide an 'unseen' split, where the set of objects in\nthe train and test captions are disjoint. SR4G is the first dataset that can be\nused to spatially fine-tune text-to-image systems. We show that fine-tuning two\ndifferent Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9\npoints improvements in the VISOR metric. The improvement holds in the 'unseen'\nsplit, showing that SD$_{SR4G}$ is able to generalize to unseen objects.\nSD$_{SR4G}$ improves the state-of-the-art with fewer parameters, and avoids\ncomplex architectures. Our analysis shows that improvement is consistent for\nall relations. The dataset and the code will be publicly available.\n", "link": "http://arxiv.org/abs/2403.00587v1", "date": "2024-03-01", "relevancy": 2.5351, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5287}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4989}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4934}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Explicit%20Spatial%20Relationships%20in%20Text-to-Image%20Generation%0A%20%20through%20an%20Automatically%20Derived%20Dataset&entry.906535625=Ander%20Salaberria%20and%20Gorka%20Azkune%20and%20Oier%20Lopez%20de%20Lacalle%20and%20Aitor%20Soroa%20and%20Eneko%20Agirre%20and%20Frank%20Keller&entry.1292438233=%20%20Existing%20work%20has%20observed%20that%20current%20text-to-image%20systems%20do%20not%0Aaccurately%20reflect%20explicit%20spatial%20relations%20between%20objects%20such%20as%20%27left%20of%27%0Aor%20%27below%27.%20We%20hypothesize%20that%20this%20is%20because%20explicit%20spatial%20relations%0Ararely%20appear%20in%20the%20image%20captions%20used%20to%20train%20these%20models.%20We%20propose%20an%0Aautomatic%20method%20that%2C%20given%20existing%20images%2C%20generates%20synthetic%20captions%20that%0Acontain%2014%20explicit%20spatial%20relations.%20We%20introduce%20the%20Spatial%20Relation%20for%0AGeneration%20%28SR4G%29%20dataset%2C%20which%20contains%209.9%20millions%20image-caption%20pairs%20for%0Atraining%2C%20and%20more%20than%2060%20thousand%20captions%20for%20evaluation.%20In%20order%20to%20test%0Ageneralization%20we%20also%20provide%20an%20%27unseen%27%20split%2C%20where%20the%20set%20of%20objects%20in%0Athe%20train%20and%20test%20captions%20are%20disjoint.%20SR4G%20is%20the%20first%20dataset%20that%20can%20be%0Aused%20to%20spatially%20fine-tune%20text-to-image%20systems.%20We%20show%20that%20fine-tuning%20two%0Adifferent%20Stable%20Diffusion%20models%20%28denoted%20as%20SD%24_%7BSR4G%7D%24%29%20yields%20up%20to%209%0Apoints%20improvements%20in%20the%20VISOR%20metric.%20The%20improvement%20holds%20in%20the%20%27unseen%27%0Asplit%2C%20showing%20that%20SD%24_%7BSR4G%7D%24%20is%20able%20to%20generalize%20to%20unseen%20objects.%0ASD%24_%7BSR4G%7D%24%20improves%20the%20state-of-the-art%20with%20fewer%20parameters%2C%20and%20avoids%0Acomplex%20architectures.%20Our%20analysis%20shows%20that%20improvement%20is%20consistent%20for%0Aall%20relations.%20The%20dataset%20and%20the%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00587v1&entry.124074799=Read"},
{"title": "OpenGraph: Towards Open Graph Foundation Models", "author": "Lianghao Xia and Ben Kao and Chao Huang", "abstract": "  Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains.\n", "link": "http://arxiv.org/abs/2403.01121v1", "date": "2024-03-02", "relevancy": 2.5343, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5441}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4764}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenGraph%3A%20Towards%20Open%20Graph%20Foundation%20Models&entry.906535625=Lianghao%20Xia%20and%20Ben%20Kao%20and%20Chao%20Huang&entry.1292438233=%20%20Graph%20learning%20has%20become%20indispensable%20for%20interpreting%20and%20harnessing%0Arelational%20data%20in%20diverse%20fields%2C%20ranging%20from%20recommendation%20systems%20to%0Asocial%20network%20analysis.%20In%20this%20context%2C%20a%20variety%20of%20GNNs%20have%20emerged%20as%0Apromising%20methodologies%20for%20encoding%20the%20structural%20information%20of%20graphs.%20By%0Aeffectively%20capturing%20the%20graph%27s%20underlying%20structure%2C%20these%20GNNs%20have%20shown%0Agreat%20potential%20in%20enhancing%20performance%20in%20graph%20learning%20tasks%2C%20such%20as%20link%0Aprediction%20and%20node%20classification.%20However%2C%20despite%20their%20successes%2C%20a%0Asignificant%20challenge%20persists%3A%20these%20advanced%20methods%20often%20face%20difficulties%0Ain%20generalizing%20to%20unseen%20graph%20data%20that%20significantly%20differs%20from%20the%0Atraining%20instances.%20In%20this%20work%2C%20our%20aim%20is%20to%20advance%20the%20graph%20learning%0Aparadigm%20by%20developing%20a%20general%20graph%20foundation%20model.%20This%20model%20is%20designed%0Ato%20understand%20the%20complex%20topological%20patterns%20present%20in%20diverse%20graph%20data%2C%0Aenabling%20it%20to%20excel%20in%20zero-shot%20graph%20learning%20tasks%20across%20different%0Adownstream%20datasets.%20To%20achieve%20this%20goal%2C%20we%20address%20several%20key%20technical%0Achallenges%20in%20our%20OpenGraph%20model.%20Firstly%2C%20we%20propose%20a%20unified%20graph%0Atokenizer%20to%20adapt%20our%20graph%20model%20to%20generalize%20well%20on%20unseen%20graph%20data%2C%0Aeven%20when%20the%20underlying%20graph%20properties%20differ%20significantly%20from%20those%0Aencountered%20during%20training.%20Secondly%2C%20we%20develop%20a%20scalable%20graph%20transformer%0Aas%20the%20foundational%20encoder%2C%20which%20effectively%20captures%20node-wise%20dependencies%0Awithin%20the%20global%20topological%20context.%20Thirdly%2C%20we%20introduce%20a%20data%0Aaugmentation%20mechanism%20enhanced%20by%20a%20LLM%20to%20alleviate%20the%20limitations%20of%20data%0Ascarcity%20in%20real-world%20scenarios.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20framework.%20By%20adapting%20our%20OpenGraph%20to%20new%20graph%0Acharacteristics%20and%20comprehending%20the%20nuances%20of%20diverse%20graphs%2C%20our%20approach%0Aachieves%20remarkable%20zero-shot%20graph%20learning%20performance%20across%20various%0Asettings%20and%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01121v1&entry.124074799=Read"},
{"title": "Distributional Dataset Distillation with Subtask Decomposition", "author": "Tian Qin and Zhiwei Deng and David Alvarez-Melis", "abstract": "  What does a neural network learn when training from a task-specific dataset?\nSynthesizing this knowledge is the central idea behind Dataset Distillation,\nwhich recent work has shown can be used to compress large datasets into a small\nset of input-label pairs ($\\textit{prototypes}$) that capture essential aspects\nof the original dataset. In this paper, we make the key observation that\nexisting methods distilling into explicit prototypes are very often suboptimal,\nincurring in unexpected storage cost from distilled labels. In response, we\npropose $\\textit{Distributional Dataset Distillation}$ (D3), which encodes the\ndata using minimal sufficient per-class statistics and paired with a decoder,\nwe distill dataset into a compact distributional representation that is more\nmemory-efficient compared to prototype-based methods. To scale up the process\nof learning these representations, we propose $\\textit{Federated\ndistillation}$, which decomposes the dataset into subsets, distills them in\nparallel using sub-task experts and then re-aggregates them. We thoroughly\nevaluate our algorithm on a three-dimensional metric and show that our method\nachieves state-of-the-art results on TinyImageNet and ImageNet-1K.\nSpecifically, we outperform the prior art by $6.9\\%$ on ImageNet-1K under the\nstorage budget of 2 images per class.\n", "link": "http://arxiv.org/abs/2403.00999v1", "date": "2024-03-01", "relevancy": 2.5246, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5435}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4983}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.473}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributional%20Dataset%20Distillation%20with%20Subtask%20Decomposition&entry.906535625=Tian%20Qin%20and%20Zhiwei%20Deng%20and%20David%20Alvarez-Melis&entry.1292438233=%20%20What%20does%20a%20neural%20network%20learn%20when%20training%20from%20a%20task-specific%20dataset%3F%0ASynthesizing%20this%20knowledge%20is%20the%20central%20idea%20behind%20Dataset%20Distillation%2C%0Awhich%20recent%20work%20has%20shown%20can%20be%20used%20to%20compress%20large%20datasets%20into%20a%20small%0Aset%20of%20input-label%20pairs%20%28%24%5Ctextit%7Bprototypes%7D%24%29%20that%20capture%20essential%20aspects%0Aof%20the%20original%20dataset.%20In%20this%20paper%2C%20we%20make%20the%20key%20observation%20that%0Aexisting%20methods%20distilling%20into%20explicit%20prototypes%20are%20very%20often%20suboptimal%2C%0Aincurring%20in%20unexpected%20storage%20cost%20from%20distilled%20labels.%20In%20response%2C%20we%0Apropose%20%24%5Ctextit%7BDistributional%20Dataset%20Distillation%7D%24%20%28D3%29%2C%20which%20encodes%20the%0Adata%20using%20minimal%20sufficient%20per-class%20statistics%20and%20paired%20with%20a%20decoder%2C%0Awe%20distill%20dataset%20into%20a%20compact%20distributional%20representation%20that%20is%20more%0Amemory-efficient%20compared%20to%20prototype-based%20methods.%20To%20scale%20up%20the%20process%0Aof%20learning%20these%20representations%2C%20we%20propose%20%24%5Ctextit%7BFederated%0Adistillation%7D%24%2C%20which%20decomposes%20the%20dataset%20into%20subsets%2C%20distills%20them%20in%0Aparallel%20using%20sub-task%20experts%20and%20then%20re-aggregates%20them.%20We%20thoroughly%0Aevaluate%20our%20algorithm%20on%20a%20three-dimensional%20metric%20and%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20results%20on%20TinyImageNet%20and%20ImageNet-1K.%0ASpecifically%2C%20we%20outperform%20the%20prior%20art%20by%20%246.9%5C%25%24%20on%20ImageNet-1K%20under%20the%0Astorage%20budget%20of%202%20images%20per%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00999v1&entry.124074799=Read"},
{"title": "Take the Bull by the Horns: Hard Sample-Reweighted Continual Training\n  Improves LLM Generalization", "author": "Xuxi Chen and Zhendong Wang and Daouda Sow and Junjie Yang and Tianlong Chen and Yingbin Liang and Mingyuan Zhou and Zhangyang Wang", "abstract": "  In the rapidly advancing arena of large language models (LLMs), a key\nchallenge is to enhance their capabilities amid a looming shortage of\nhigh-quality training data. Our study starts from an empirical strategy for the\nlight continual training of LLMs using their original pre-training data sets,\nwith a specific focus on selective retention of samples that incur moderately\nhigh losses. These samples are deemed informative and beneficial for model\nrefinement, contrasting with the highest-loss samples, which would be discarded\ndue to their correlation with data noise and complexity. We then formalize this\nstrategy into a principled framework of Instance-Reweighted Distributionally\nRobust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the\ntraining focus on informative samples through an instance reweighting\nmechanism, streamlined by a closed-form solution for straightforward\nintegration into established training protocols. Through rigorous\nexperimentation with various models and datasets, our findings indicate that\nour sample-targeted methods significantly improve LLM performance across\nmultiple benchmarks, in both continual pre-training and instruction tuning\nscenarios. Our codes are available at\nhttps://github.com/VITA-Group/HardFocusTraining.\n", "link": "http://arxiv.org/abs/2402.14270v2", "date": "2024-03-01", "relevancy": 2.5222, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5402}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4945}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4786}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Take%20the%20Bull%20by%20the%20Horns%3A%20Hard%20Sample-Reweighted%20Continual%20Training%0A%20%20Improves%20LLM%20Generalization&entry.906535625=Xuxi%20Chen%20and%20Zhendong%20Wang%20and%20Daouda%20Sow%20and%20Junjie%20Yang%20and%20Tianlong%20Chen%20and%20Yingbin%20Liang%20and%20Mingyuan%20Zhou%20and%20Zhangyang%20Wang&entry.1292438233=%20%20In%20the%20rapidly%20advancing%20arena%20of%20large%20language%20models%20%28LLMs%29%2C%20a%20key%0Achallenge%20is%20to%20enhance%20their%20capabilities%20amid%20a%20looming%20shortage%20of%0Ahigh-quality%20training%20data.%20Our%20study%20starts%20from%20an%20empirical%20strategy%20for%20the%0Alight%20continual%20training%20of%20LLMs%20using%20their%20original%20pre-training%20data%20sets%2C%0Awith%20a%20specific%20focus%20on%20selective%20retention%20of%20samples%20that%20incur%20moderately%0Ahigh%20losses.%20These%20samples%20are%20deemed%20informative%20and%20beneficial%20for%20model%0Arefinement%2C%20contrasting%20with%20the%20highest-loss%20samples%2C%20which%20would%20be%20discarded%0Adue%20to%20their%20correlation%20with%20data%20noise%20and%20complexity.%20We%20then%20formalize%20this%0Astrategy%20into%20a%20principled%20framework%20of%20Instance-Reweighted%20Distributionally%0ARobust%20Optimization%20%28IR-DRO%29.%20IR-DRO%20is%20designed%20to%20dynamically%20prioritize%20the%0Atraining%20focus%20on%20informative%20samples%20through%20an%20instance%20reweighting%0Amechanism%2C%20streamlined%20by%20a%20closed-form%20solution%20for%20straightforward%0Aintegration%20into%20established%20training%20protocols.%20Through%20rigorous%0Aexperimentation%20with%20various%20models%20and%20datasets%2C%20our%20findings%20indicate%20that%0Aour%20sample-targeted%20methods%20significantly%20improve%20LLM%20performance%20across%0Amultiple%20benchmarks%2C%20in%20both%20continual%20pre-training%20and%20instruction%20tuning%0Ascenarios.%20Our%20codes%20are%20available%20at%0Ahttps%3A//github.com/VITA-Group/HardFocusTraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14270v2&entry.124074799=Read"},
{"title": "Pairwise Alignment Improves Graph Domain Adaptation", "author": "Shikun Liu and Deyu Zou and Han Zhao and Pan Li", "abstract": "  Graph-based methods, pivotal for label inference over interconnected objects\nin many real-world applications, often encounter generalization challenges, if\nthe graph used for model training differs significantly from the graph used for\ntesting. This work delves into Graph Domain Adaptation (GDA) to address the\nunique complexities of distribution shifts over graph data, where\ninterconnected data points experience shifts in features, labels, and in\nparticular, connecting patterns. We propose a novel, theoretically principled\nmethod, Pairwise Alignment (Pair-Align) to counter graph structure shift by\nmitigating conditional structure shift (CSS) and label shift (LS). Pair-Align\nuses edge weights to recalibrate the influence among neighboring nodes to\nhandle CSS and adjusts the classification loss with label weights to handle LS.\nOur method demonstrates superior performance in real-world applications,\nincluding node classification with region shift in social networks, and the\npileup mitigation task in particle colliding experiments. For the first\napplication, we also curate the largest dataset by far for GDA studies. Our\nmethod shows strong performance in synthetic and other existing benchmark\ndatasets.\n", "link": "http://arxiv.org/abs/2403.01092v1", "date": "2024-03-02", "relevancy": 2.5145, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5336}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.477}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pairwise%20Alignment%20Improves%20Graph%20Domain%20Adaptation&entry.906535625=Shikun%20Liu%20and%20Deyu%20Zou%20and%20Han%20Zhao%20and%20Pan%20Li&entry.1292438233=%20%20Graph-based%20methods%2C%20pivotal%20for%20label%20inference%20over%20interconnected%20objects%0Ain%20many%20real-world%20applications%2C%20often%20encounter%20generalization%20challenges%2C%20if%0Athe%20graph%20used%20for%20model%20training%20differs%20significantly%20from%20the%20graph%20used%20for%0Atesting.%20This%20work%20delves%20into%20Graph%20Domain%20Adaptation%20%28GDA%29%20to%20address%20the%0Aunique%20complexities%20of%20distribution%20shifts%20over%20graph%20data%2C%20where%0Ainterconnected%20data%20points%20experience%20shifts%20in%20features%2C%20labels%2C%20and%20in%0Aparticular%2C%20connecting%20patterns.%20We%20propose%20a%20novel%2C%20theoretically%20principled%0Amethod%2C%20Pairwise%20Alignment%20%28Pair-Align%29%20to%20counter%20graph%20structure%20shift%20by%0Amitigating%20conditional%20structure%20shift%20%28CSS%29%20and%20label%20shift%20%28LS%29.%20Pair-Align%0Auses%20edge%20weights%20to%20recalibrate%20the%20influence%20among%20neighboring%20nodes%20to%0Ahandle%20CSS%20and%20adjusts%20the%20classification%20loss%20with%20label%20weights%20to%20handle%20LS.%0AOur%20method%20demonstrates%20superior%20performance%20in%20real-world%20applications%2C%0Aincluding%20node%20classification%20with%20region%20shift%20in%20social%20networks%2C%20and%20the%0Apileup%20mitigation%20task%20in%20particle%20colliding%20experiments.%20For%20the%20first%0Aapplication%2C%20we%20also%20curate%20the%20largest%20dataset%20by%20far%20for%20GDA%20studies.%20Our%0Amethod%20shows%20strong%20performance%20in%20synthetic%20and%20other%20existing%20benchmark%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01092v1&entry.124074799=Read"},
{"title": "Single-image camera calibration with model-free distortion correction", "author": "Katia Genovese", "abstract": "  Camera calibration is a process of paramount importance in computer vision\napplications that require accurate quantitative measurements. The popular\nmethod developed by Zhang relies on the use of a large number of images of a\nplanar grid of fiducial points captured in multiple poses. Although flexible\nand easy to implement, Zhang's method has some limitations. The simultaneous\noptimization of the entire parameter set, including the coefficients of a\npredefined distortion model, may result in poor distortion correction at the\nimage boundaries or in miscalculation of the intrinsic parameters, even with a\nreasonably small reprojection error. Indeed, applications involving image\nstitching (e.g. multi-camera systems) require accurate mapping of distortion up\nto the outermost regions of the image. Moreover, intrinsic parameters affect\nthe accuracy of camera pose estimation, which is fundamental for applications\nsuch as vision servoing in robot navigation and automated assembly. This paper\nproposes a method for estimating the complete set of calibration parameters\nfrom a single image of a planar speckle pattern covering the entire sensor. The\ncorrespondence between image points and physical points on the calibration\ntarget is obtained using Digital Image Correlation. The effective focal length\nand the extrinsic parameters are calculated separately after a prior evaluation\nof the principal point. At the end of the procedure, a dense and uniform\nmodel-free distortion map is obtained over the entire image. Synthetic data\nwith different noise levels were used to test the feasibility of the proposed\nmethod and to compare its metrological performance with Zhang's method.\nReal-world tests demonstrate the potential of the developed method to reveal\naspects of the image formation that are hidden by averaging over multiple\nimages.\n", "link": "http://arxiv.org/abs/2403.01263v1", "date": "2024-03-02", "relevancy": 2.5117, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5306}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4633}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-image%20camera%20calibration%20with%20model-free%20distortion%20correction&entry.906535625=Katia%20Genovese&entry.1292438233=%20%20Camera%20calibration%20is%20a%20process%20of%20paramount%20importance%20in%20computer%20vision%0Aapplications%20that%20require%20accurate%20quantitative%20measurements.%20The%20popular%0Amethod%20developed%20by%20Zhang%20relies%20on%20the%20use%20of%20a%20large%20number%20of%20images%20of%20a%0Aplanar%20grid%20of%20fiducial%20points%20captured%20in%20multiple%20poses.%20Although%20flexible%0Aand%20easy%20to%20implement%2C%20Zhang%27s%20method%20has%20some%20limitations.%20The%20simultaneous%0Aoptimization%20of%20the%20entire%20parameter%20set%2C%20including%20the%20coefficients%20of%20a%0Apredefined%20distortion%20model%2C%20may%20result%20in%20poor%20distortion%20correction%20at%20the%0Aimage%20boundaries%20or%20in%20miscalculation%20of%20the%20intrinsic%20parameters%2C%20even%20with%20a%0Areasonably%20small%20reprojection%20error.%20Indeed%2C%20applications%20involving%20image%0Astitching%20%28e.g.%20multi-camera%20systems%29%20require%20accurate%20mapping%20of%20distortion%20up%0Ato%20the%20outermost%20regions%20of%20the%20image.%20Moreover%2C%20intrinsic%20parameters%20affect%0Athe%20accuracy%20of%20camera%20pose%20estimation%2C%20which%20is%20fundamental%20for%20applications%0Asuch%20as%20vision%20servoing%20in%20robot%20navigation%20and%20automated%20assembly.%20This%20paper%0Aproposes%20a%20method%20for%20estimating%20the%20complete%20set%20of%20calibration%20parameters%0Afrom%20a%20single%20image%20of%20a%20planar%20speckle%20pattern%20covering%20the%20entire%20sensor.%20The%0Acorrespondence%20between%20image%20points%20and%20physical%20points%20on%20the%20calibration%0Atarget%20is%20obtained%20using%20Digital%20Image%20Correlation.%20The%20effective%20focal%20length%0Aand%20the%20extrinsic%20parameters%20are%20calculated%20separately%20after%20a%20prior%20evaluation%0Aof%20the%20principal%20point.%20At%20the%20end%20of%20the%20procedure%2C%20a%20dense%20and%20uniform%0Amodel-free%20distortion%20map%20is%20obtained%20over%20the%20entire%20image.%20Synthetic%20data%0Awith%20different%20noise%20levels%20were%20used%20to%20test%20the%20feasibility%20of%20the%20proposed%0Amethod%20and%20to%20compare%20its%20metrological%20performance%20with%20Zhang%27s%20method.%0AReal-world%20tests%20demonstrate%20the%20potential%20of%20the%20developed%20method%20to%20reveal%0Aaspects%20of%20the%20image%20formation%20that%20are%20hidden%20by%20averaging%20over%20multiple%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01263v1&entry.124074799=Read"},
{"title": "DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions", "author": "Guangrun Wang and Changlin Li and Liuchun Yuan and Jiefeng Peng and Xiaoyu Xian and Xiaodan Liang and Xiaojun Chang and Liang Lin", "abstract": "  Neural Architecture Search (NAS), aiming at automatically designing neural\narchitectures by machines, has been considered a key step toward automatic\nmachine learning. One notable NAS branch is the weight-sharing NAS, which\nsignificantly improves search efficiency and allows NAS algorithms to run on\nordinary computers. Despite receiving high expectations, this category of\nmethods suffers from low search effectiveness. By employing a generalization\nboundedness tool, we demonstrate that the devil behind this drawback is the\nuntrustworthy architecture rating with the oversized search space of the\npossible architectures. Addressing this problem, we modularize a large search\nspace into blocks with small search spaces and develop a family of models with\nthe distilling neural architecture (DNA) techniques. These proposed models,\nnamely a DNA family, are capable of resolving multiple dilemmas of the\nweight-sharing NAS, such as scalability, efficiency, and multi-modal\ncompatibility. Our proposed DNA models can rate all architecture candidates, as\nopposed to previous works that can only access a subsearch space using\nheuristic algorithms. Moreover, under a certain computational complexity\nconstraint, our method can seek architectures with different depths and widths.\nExtensive experimental evaluations show that our models achieve\nstate-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobile\nconvolutional network and a small vision transformer, respectively.\nAdditionally, we provide in-depth empirical analysis and insights into neural\narchitecture ratings. Codes available: \\url{https://github.com/changlin31/DNA}.\n", "link": "http://arxiv.org/abs/2403.01326v1", "date": "2024-03-02", "relevancy": 2.5001, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5332}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4881}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4788}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNA%20Family%3A%20Boosting%20Weight-Sharing%20NAS%20with%20Block-Wise%20Supervisions&entry.906535625=Guangrun%20Wang%20and%20Changlin%20Li%20and%20Liuchun%20Yuan%20and%20Jiefeng%20Peng%20and%20Xiaoyu%20Xian%20and%20Xiaodan%20Liang%20and%20Xiaojun%20Chang%20and%20Liang%20Lin&entry.1292438233=%20%20Neural%20Architecture%20Search%20%28NAS%29%2C%20aiming%20at%20automatically%20designing%20neural%0Aarchitectures%20by%20machines%2C%20has%20been%20considered%20a%20key%20step%20toward%20automatic%0Amachine%20learning.%20One%20notable%20NAS%20branch%20is%20the%20weight-sharing%20NAS%2C%20which%0Asignificantly%20improves%20search%20efficiency%20and%20allows%20NAS%20algorithms%20to%20run%20on%0Aordinary%20computers.%20Despite%20receiving%20high%20expectations%2C%20this%20category%20of%0Amethods%20suffers%20from%20low%20search%20effectiveness.%20By%20employing%20a%20generalization%0Aboundedness%20tool%2C%20we%20demonstrate%20that%20the%20devil%20behind%20this%20drawback%20is%20the%0Auntrustworthy%20architecture%20rating%20with%20the%20oversized%20search%20space%20of%20the%0Apossible%20architectures.%20Addressing%20this%20problem%2C%20we%20modularize%20a%20large%20search%0Aspace%20into%20blocks%20with%20small%20search%20spaces%20and%20develop%20a%20family%20of%20models%20with%0Athe%20distilling%20neural%20architecture%20%28DNA%29%20techniques.%20These%20proposed%20models%2C%0Anamely%20a%20DNA%20family%2C%20are%20capable%20of%20resolving%20multiple%20dilemmas%20of%20the%0Aweight-sharing%20NAS%2C%20such%20as%20scalability%2C%20efficiency%2C%20and%20multi-modal%0Acompatibility.%20Our%20proposed%20DNA%20models%20can%20rate%20all%20architecture%20candidates%2C%20as%0Aopposed%20to%20previous%20works%20that%20can%20only%20access%20a%20subsearch%20space%20using%0Aheuristic%20algorithms.%20Moreover%2C%20under%20a%20certain%20computational%20complexity%0Aconstraint%2C%20our%20method%20can%20seek%20architectures%20with%20different%20depths%20and%20widths.%0AExtensive%20experimental%20evaluations%20show%20that%20our%20models%20achieve%0Astate-of-the-art%20top-1%20accuracy%20of%2078.9%25%20and%2083.6%25%20on%20ImageNet%20for%20a%20mobile%0Aconvolutional%20network%20and%20a%20small%20vision%20transformer%2C%20respectively.%0AAdditionally%2C%20we%20provide%20in-depth%20empirical%20analysis%20and%20insights%20into%20neural%0Aarchitecture%20ratings.%20Codes%20available%3A%20%5Curl%7Bhttps%3A//github.com/changlin31/DNA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01326v1&entry.124074799=Read"},
{"title": "Tri-Modal Motion Retrieval by Learning a Joint Embedding Space", "author": "Kangning Yin and Shihao Zou and Yuxuan Ge and Zheng Tian", "abstract": "  Information retrieval is an ever-evolving and crucial research domain. The\nsubstantial demand for high-quality human motion data especially in online\nacquirement has led to a surge in human motion research works. Prior works have\nmainly concentrated on dual-modality learning, such as text and motion tasks,\nbut three-modality learning has been rarely explored. Intuitively, an extra\nintroduced modality can enrich a model's application scenario, and more\nimportantly, an adequate choice of the extra modality can also act as an\nintermediary and enhance the alignment between the other two disparate\nmodalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion\nalignment), a novel framework for three-modality learning integrating\nhuman-centric videos as an additional modality, thereby effectively bridging\nthe gap between text and motion. Moreover, our approach leverages a specially\ndesigned attention mechanism to foster enhanced alignment and synergistic\neffects among text, video, and motion modalities. Empirically, our results on\nthe HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art\nperformance in various motion-related cross-modal retrieval tasks, including\ntext-to-motion, motion-to-text, video-to-motion and motion-to-video.\n", "link": "http://arxiv.org/abs/2403.00691v1", "date": "2024-03-01", "relevancy": 2.4914, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6566}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5993}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5974}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tri-Modal%20Motion%20Retrieval%20by%20Learning%20a%20Joint%20Embedding%20Space&entry.906535625=Kangning%20Yin%20and%20Shihao%20Zou%20and%20Yuxuan%20Ge%20and%20Zheng%20Tian&entry.1292438233=%20%20Information%20retrieval%20is%20an%20ever-evolving%20and%20crucial%20research%20domain.%20The%0Asubstantial%20demand%20for%20high-quality%20human%20motion%20data%20especially%20in%20online%0Aacquirement%20has%20led%20to%20a%20surge%20in%20human%20motion%20research%20works.%20Prior%20works%20have%0Amainly%20concentrated%20on%20dual-modality%20learning%2C%20such%20as%20text%20and%20motion%20tasks%2C%0Abut%20three-modality%20learning%20has%20been%20rarely%20explored.%20Intuitively%2C%20an%20extra%0Aintroduced%20modality%20can%20enrich%20a%20model%27s%20application%20scenario%2C%20and%20more%0Aimportantly%2C%20an%20adequate%20choice%20of%20the%20extra%20modality%20can%20also%20act%20as%20an%0Aintermediary%20and%20enhance%20the%20alignment%20between%20the%20other%20two%20disparate%0Amodalities.%20In%20this%20work%2C%20we%20introduce%20LAVIMO%20%28LAnguage-VIdeo-MOtion%0Aalignment%29%2C%20a%20novel%20framework%20for%20three-modality%20learning%20integrating%0Ahuman-centric%20videos%20as%20an%20additional%20modality%2C%20thereby%20effectively%20bridging%0Athe%20gap%20between%20text%20and%20motion.%20Moreover%2C%20our%20approach%20leverages%20a%20specially%0Adesigned%20attention%20mechanism%20to%20foster%20enhanced%20alignment%20and%20synergistic%0Aeffects%20among%20text%2C%20video%2C%20and%20motion%20modalities.%20Empirically%2C%20our%20results%20on%0Athe%20HumanML3D%20and%20KIT-ML%20datasets%20show%20that%20LAVIMO%20achieves%20state-of-the-art%0Aperformance%20in%20various%20motion-related%20cross-modal%20retrieval%20tasks%2C%20including%0Atext-to-motion%2C%20motion-to-text%2C%20video-to-motion%20and%20motion-to-video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00691v1&entry.124074799=Read"},
{"title": "Learning Causal Features for Incremental Object Detection", "author": "Zhenwei He and Lei Zhang", "abstract": "  Object detection limits its recognizable categories during the training\nphase, in which it can not cover all objects of interest for users. To satisfy\nthe practical necessity, the incremental learning ability of the detector\nbecomes a critical factor for real-world applications. Unfortunately, neural\nnetworks unavoidably meet catastrophic forgetting problem when it is\nimplemented on a new task. To this end, many incremental object detection\nmodels preserve the knowledge of previous tasks by replaying samples or\ndistillation from previous models. However, they ignore an important factor\nthat the performance of the model mostly depends on its feature. These models\ntry to rouse the memory of the neural network with previous samples but not to\nprevent forgetting. To this end, in this paper, we propose an incremental\ncausal object detection (ICOD) model by learning causal features, which can\nadapt to more tasks. Traditional object detection models, unavoidably depend on\nthe data-bias or data-specific features to get the detection results, which can\nnot adapt to the new task. When the model meets the requirements of incremental\nlearning, the data-bias information is not beneficial to the new task, and the\nincremental learning may eliminate these features and lead to forgetting. To\nthis end, our ICOD is introduced to learn the causal features, rather than the\ndata-bias features when training the detector. Thus, when the model is\nimplemented to a new task, the causal features of the old task can aid the\nincremental learning process to alleviate the catastrophic forgetting problem.\nWe conduct our model on several experiments, which shows a causal feature\nwithout data-bias can make the model adapt to new tasks better.\n\\keywords{Object detection, incremental learning, causal feature.\n", "link": "http://arxiv.org/abs/2403.00591v1", "date": "2024-03-01", "relevancy": 2.4901, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5017}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4978}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4946}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Causal%20Features%20for%20Incremental%20Object%20Detection&entry.906535625=Zhenwei%20He%20and%20Lei%20Zhang&entry.1292438233=%20%20Object%20detection%20limits%20its%20recognizable%20categories%20during%20the%20training%0Aphase%2C%20in%20which%20it%20can%20not%20cover%20all%20objects%20of%20interest%20for%20users.%20To%20satisfy%0Athe%20practical%20necessity%2C%20the%20incremental%20learning%20ability%20of%20the%20detector%0Abecomes%20a%20critical%20factor%20for%20real-world%20applications.%20Unfortunately%2C%20neural%0Anetworks%20unavoidably%20meet%20catastrophic%20forgetting%20problem%20when%20it%20is%0Aimplemented%20on%20a%20new%20task.%20To%20this%20end%2C%20many%20incremental%20object%20detection%0Amodels%20preserve%20the%20knowledge%20of%20previous%20tasks%20by%20replaying%20samples%20or%0Adistillation%20from%20previous%20models.%20However%2C%20they%20ignore%20an%20important%20factor%0Athat%20the%20performance%20of%20the%20model%20mostly%20depends%20on%20its%20feature.%20These%20models%0Atry%20to%20rouse%20the%20memory%20of%20the%20neural%20network%20with%20previous%20samples%20but%20not%20to%0Aprevent%20forgetting.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20propose%20an%20incremental%0Acausal%20object%20detection%20%28ICOD%29%20model%20by%20learning%20causal%20features%2C%20which%20can%0Aadapt%20to%20more%20tasks.%20Traditional%20object%20detection%20models%2C%20unavoidably%20depend%20on%0Athe%20data-bias%20or%20data-specific%20features%20to%20get%20the%20detection%20results%2C%20which%20can%0Anot%20adapt%20to%20the%20new%20task.%20When%20the%20model%20meets%20the%20requirements%20of%20incremental%0Alearning%2C%20the%20data-bias%20information%20is%20not%20beneficial%20to%20the%20new%20task%2C%20and%20the%0Aincremental%20learning%20may%20eliminate%20these%20features%20and%20lead%20to%20forgetting.%20To%0Athis%20end%2C%20our%20ICOD%20is%20introduced%20to%20learn%20the%20causal%20features%2C%20rather%20than%20the%0Adata-bias%20features%20when%20training%20the%20detector.%20Thus%2C%20when%20the%20model%20is%0Aimplemented%20to%20a%20new%20task%2C%20the%20causal%20features%20of%20the%20old%20task%20can%20aid%20the%0Aincremental%20learning%20process%20to%20alleviate%20the%20catastrophic%20forgetting%20problem.%0AWe%20conduct%20our%20model%20on%20several%20experiments%2C%20which%20shows%20a%20causal%20feature%0Awithout%20data-bias%20can%20make%20the%20model%20adapt%20to%20new%20tasks%20better.%0A%5Ckeywords%7BObject%20detection%2C%20incremental%20learning%2C%20causal%20feature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00591v1&entry.124074799=Read"},
{"title": "Seeing the World through Your Eyes", "author": "Hadi Alzayer and Kevin Zhang and Brandon Feng and Christopher Metzler and Jia-Bin Huang", "abstract": "  The reflective nature of the human eye is an underappreciated source of\ninformation about what the world around us looks like. By imaging the eyes of a\nmoving person, we can collect multiple views of a scene outside the camera's\ndirect line of sight through the reflections in the eyes. In this paper, we\nreconstruct a 3D scene beyond the camera's line of sight using portrait images\ncontaining eye reflections. This task is challenging due to 1) the difficulty\nof accurately estimating eye poses and 2) the entangled appearance of the eye\niris and the scene reflections. Our method jointly refines the cornea poses,\nthe radiance field depicting the scene, and the observer's eye iris texture. We\nfurther propose a simple regularization prior on the iris texture pattern to\nimprove reconstruction quality. Through various experiments on synthetic and\nreal-world captures featuring people with varied eye colors, we demonstrate the\nfeasibility of our approach to recover 3D scenes using eye reflections.\n", "link": "http://arxiv.org/abs/2306.09348v2", "date": "2024-03-02", "relevancy": 2.4891, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5098}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4974}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4863}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20the%20World%20through%20Your%20Eyes&entry.906535625=Hadi%20Alzayer%20and%20Kevin%20Zhang%20and%20Brandon%20Feng%20and%20Christopher%20Metzler%20and%20Jia-Bin%20Huang&entry.1292438233=%20%20The%20reflective%20nature%20of%20the%20human%20eye%20is%20an%20underappreciated%20source%20of%0Ainformation%20about%20what%20the%20world%20around%20us%20looks%20like.%20By%20imaging%20the%20eyes%20of%20a%0Amoving%20person%2C%20we%20can%20collect%20multiple%20views%20of%20a%20scene%20outside%20the%20camera%27s%0Adirect%20line%20of%20sight%20through%20the%20reflections%20in%20the%20eyes.%20In%20this%20paper%2C%20we%0Areconstruct%20a%203D%20scene%20beyond%20the%20camera%27s%20line%20of%20sight%20using%20portrait%20images%0Acontaining%20eye%20reflections.%20This%20task%20is%20challenging%20due%20to%201%29%20the%20difficulty%0Aof%20accurately%20estimating%20eye%20poses%20and%202%29%20the%20entangled%20appearance%20of%20the%20eye%0Airis%20and%20the%20scene%20reflections.%20Our%20method%20jointly%20refines%20the%20cornea%20poses%2C%0Athe%20radiance%20field%20depicting%20the%20scene%2C%20and%20the%20observer%27s%20eye%20iris%20texture.%20We%0Afurther%20propose%20a%20simple%20regularization%20prior%20on%20the%20iris%20texture%20pattern%20to%0Aimprove%20reconstruction%20quality.%20Through%20various%20experiments%20on%20synthetic%20and%0Areal-world%20captures%20featuring%20people%20with%20varied%20eye%20colors%2C%20we%20demonstrate%20the%0Afeasibility%20of%20our%20approach%20to%20recover%203D%20scenes%20using%20eye%20reflections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09348v2&entry.124074799=Read"},
{"title": "Mixer is more than just a model", "author": "Qingfeng Ji and Yuxin Wang and Letong Sun", "abstract": "  Recently, MLP structures have regained popularity, with MLP-Mixer standing\nout as a prominent example. In the field of computer vision, MLP-Mixer is noted\nfor its ability to extract data information from both channel and token\nperspectives, effectively acting as a fusion of channel and token information.\nIndeed, Mixer represents a paradigm for information extraction that amalgamates\nchannel and token information. The essence of Mixer lies in its ability to\nblend information from diverse perspectives, epitomizing the true concept of\n\"mixing\" in the realm of neural network architectures. Beyond channel and token\nconsiderations, it is possible to create more tailored mixers from various\nperspectives to better suit specific task requirements. This study focuses on\nthe domain of audio recognition, introducing a novel model named Audio\nSpectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates\ninsights from both time and frequency domains. Experimental results demonstrate\nthat ASM-RH is particularly well-suited for audio data and yields promising\noutcomes across multiple classification tasks. The models and optimal weights\nfiles will be published.\n", "link": "http://arxiv.org/abs/2402.18007v2", "date": "2024-03-02", "relevancy": 2.4777, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4892}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4841}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixer%20is%20more%20than%20just%20a%20model&entry.906535625=Qingfeng%20Ji%20and%20Yuxin%20Wang%20and%20Letong%20Sun&entry.1292438233=%20%20Recently%2C%20MLP%20structures%20have%20regained%20popularity%2C%20with%20MLP-Mixer%20standing%0Aout%20as%20a%20prominent%20example.%20In%20the%20field%20of%20computer%20vision%2C%20MLP-Mixer%20is%20noted%0Afor%20its%20ability%20to%20extract%20data%20information%20from%20both%20channel%20and%20token%0Aperspectives%2C%20effectively%20acting%20as%20a%20fusion%20of%20channel%20and%20token%20information.%0AIndeed%2C%20Mixer%20represents%20a%20paradigm%20for%20information%20extraction%20that%20amalgamates%0Achannel%20and%20token%20information.%20The%20essence%20of%20Mixer%20lies%20in%20its%20ability%20to%0Ablend%20information%20from%20diverse%20perspectives%2C%20epitomizing%20the%20true%20concept%20of%0A%22mixing%22%20in%20the%20realm%20of%20neural%20network%20architectures.%20Beyond%20channel%20and%20token%0Aconsiderations%2C%20it%20is%20possible%20to%20create%20more%20tailored%20mixers%20from%20various%0Aperspectives%20to%20better%20suit%20specific%20task%20requirements.%20This%20study%20focuses%20on%0Athe%20domain%20of%20audio%20recognition%2C%20introducing%20a%20novel%20model%20named%20Audio%0ASpectrogram%20Mixer%20with%20Roll-Time%20and%20Hermit%20FFT%20%28ASM-RH%29%20that%20incorporates%0Ainsights%20from%20both%20time%20and%20frequency%20domains.%20Experimental%20results%20demonstrate%0Athat%20ASM-RH%20is%20particularly%20well-suited%20for%20audio%20data%20and%20yields%20promising%0Aoutcomes%20across%20multiple%20classification%20tasks.%20The%20models%20and%20optimal%20weights%0Afiles%20will%20be%20published.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18007v2&entry.124074799=Read"},
{"title": "High-Speed Detector For Low-Powered Devices In Aerial Grasping", "author": "Ashish Kumar and Laxmidhar Behera", "abstract": "  Autonomous aerial harvesting is a highly complex problem because it requires\nnumerous interdisciplinary algorithms to be executed on mini low-powered\ncomputing devices. Object detection is one such algorithm that is\ncompute-hungry. In this context, we make the following contributions: (i) Fast\nFruit Detector (FFD), a resource-efficient, single-stage, and\npostprocessing-free object detector based on our novel latent object\nrepresentation (LOR) module, query assignment, and prediction strategy. FFD\nachieves 100FPS@FP32 precision on the latest 10W NVIDIA Jetson-NX embedded\ndevice while co-existing with other time-critical sub-systems such as control,\ngrasping, SLAM, a major achievement of this work. (ii) a method to generate\nvast amounts of training data without exhaustive manual labelling of fruit\nimages since they consist of a large number of instances, which increases the\nlabelling cost and time. (iii) an open-source fruit detection dataset having\nplenty of very small-sized instances that are difficult to detect. Our\nexhaustive evaluations on our and MinneApple dataset show that FFD, being only\na single-scale detector, is more accurate than many representative detectors,\ne.g. FFD is better than single-scale Faster-RCNN by 10.7AP, multi-scale\nFaster-RCNN by 2.3AP, and better than latest single-scale YOLO-v8 by 8AP and\nmulti-scale YOLO-v8 by 0.3 while being considerably faster.\n", "link": "http://arxiv.org/abs/2402.14591v2", "date": "2024-03-01", "relevancy": 2.4771, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5005}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4966}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4892}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Speed%20Detector%20For%20Low-Powered%20Devices%20In%20Aerial%20Grasping&entry.906535625=Ashish%20Kumar%20and%20Laxmidhar%20Behera&entry.1292438233=%20%20Autonomous%20aerial%20harvesting%20is%20a%20highly%20complex%20problem%20because%20it%20requires%0Anumerous%20interdisciplinary%20algorithms%20to%20be%20executed%20on%20mini%20low-powered%0Acomputing%20devices.%20Object%20detection%20is%20one%20such%20algorithm%20that%20is%0Acompute-hungry.%20In%20this%20context%2C%20we%20make%20the%20following%20contributions%3A%20%28i%29%20Fast%0AFruit%20Detector%20%28FFD%29%2C%20a%20resource-efficient%2C%20single-stage%2C%20and%0Apostprocessing-free%20object%20detector%20based%20on%20our%20novel%20latent%20object%0Arepresentation%20%28LOR%29%20module%2C%20query%20assignment%2C%20and%20prediction%20strategy.%20FFD%0Aachieves%20100FPS%40FP32%20precision%20on%20the%20latest%2010W%20NVIDIA%20Jetson-NX%20embedded%0Adevice%20while%20co-existing%20with%20other%20time-critical%20sub-systems%20such%20as%20control%2C%0Agrasping%2C%20SLAM%2C%20a%20major%20achievement%20of%20this%20work.%20%28ii%29%20a%20method%20to%20generate%0Avast%20amounts%20of%20training%20data%20without%20exhaustive%20manual%20labelling%20of%20fruit%0Aimages%20since%20they%20consist%20of%20a%20large%20number%20of%20instances%2C%20which%20increases%20the%0Alabelling%20cost%20and%20time.%20%28iii%29%20an%20open-source%20fruit%20detection%20dataset%20having%0Aplenty%20of%20very%20small-sized%20instances%20that%20are%20difficult%20to%20detect.%20Our%0Aexhaustive%20evaluations%20on%20our%20and%20MinneApple%20dataset%20show%20that%20FFD%2C%20being%20only%0Aa%20single-scale%20detector%2C%20is%20more%20accurate%20than%20many%20representative%20detectors%2C%0Ae.g.%20FFD%20is%20better%20than%20single-scale%20Faster-RCNN%20by%2010.7AP%2C%20multi-scale%0AFaster-RCNN%20by%202.3AP%2C%20and%20better%20than%20latest%20single-scale%20YOLO-v8%20by%208AP%20and%0Amulti-scale%20YOLO-v8%20by%200.3%20while%20being%20considerably%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14591v2&entry.124074799=Read"},
{"title": "Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient\n  adaptive algorithms for neural networks", "author": "Dong-Young Lim and Sotirios Sabanis", "abstract": "  We present a new class of Langevin based algorithms, which overcomes many of\nthe known shortcomings of popular adaptive optimizers that are currently used\nfor the fine tuning of deep learning models. Its underpinning theory relies on\nrecent advances of Euler's polygonal approximations for stochastic differential\nequations (SDEs) with monotone coefficients. As a result, it inherits the\nstability properties of tamed algorithms, while it addresses other known\nissues, e.g. vanishing gradients in neural networks. In particular, we provide\na nonasymptotic analysis and full theoretical guarantees for the convergence\nproperties of an algorithm of this novel class, which we named TH$\\varepsilon$O\nPOULA (or, simply, TheoPouLa). Finally, several experiments are presented with\ndifferent types of deep learning models, which show the superior performance of\nTheoPouLa over many popular adaptive optimization algorithms.\n", "link": "http://arxiv.org/abs/2105.13937v3", "date": "2024-03-02", "relevancy": 2.4639, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4899}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4894}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polygonal%20Unadjusted%20Langevin%20Algorithms%3A%20Creating%20stable%20and%20efficient%0A%20%20adaptive%20algorithms%20for%20neural%20networks&entry.906535625=Dong-Young%20Lim%20and%20Sotirios%20Sabanis&entry.1292438233=%20%20We%20present%20a%20new%20class%20of%20Langevin%20based%20algorithms%2C%20which%20overcomes%20many%20of%0Athe%20known%20shortcomings%20of%20popular%20adaptive%20optimizers%20that%20are%20currently%20used%0Afor%20the%20fine%20tuning%20of%20deep%20learning%20models.%20Its%20underpinning%20theory%20relies%20on%0Arecent%20advances%20of%20Euler%27s%20polygonal%20approximations%20for%20stochastic%20differential%0Aequations%20%28SDEs%29%20with%20monotone%20coefficients.%20As%20a%20result%2C%20it%20inherits%20the%0Astability%20properties%20of%20tamed%20algorithms%2C%20while%20it%20addresses%20other%20known%0Aissues%2C%20e.g.%20vanishing%20gradients%20in%20neural%20networks.%20In%20particular%2C%20we%20provide%0Aa%20nonasymptotic%20analysis%20and%20full%20theoretical%20guarantees%20for%20the%20convergence%0Aproperties%20of%20an%20algorithm%20of%20this%20novel%20class%2C%20which%20we%20named%20TH%24%5Cvarepsilon%24O%0APOULA%20%28or%2C%20simply%2C%20TheoPouLa%29.%20Finally%2C%20several%20experiments%20are%20presented%20with%0Adifferent%20types%20of%20deep%20learning%20models%2C%20which%20show%20the%20superior%20performance%20of%0ATheoPouLa%20over%20many%20popular%20adaptive%20optimization%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2105.13937v3&entry.124074799=Read"},
{"title": "Toward Autonomous Cooperation in Heterogeneous Nanosatellite\n  Constellations Using Dynamic Graph Neural Networks", "author": "Guillem Casadesus-Vila and Joan-Adria Ruiz-de-Azua and Eduard Alarcon", "abstract": "  The upcoming landscape of Earth Observation missions will defined by\nnetworked heterogeneous nanosatellite constellations required to meet strict\nmission requirements, such as revisit times and spatial resolution. However,\nscheduling satellite communications in these satellite networks through\nefficiently creating a global satellite Contact Plan (CP) is a complex task,\nwith current solutions requiring ground-based coordination or being limited by\nonboard computational resources. The paper proposes a novel approach to\novercome these challenges by modeling the constellations and CP as dynamic\nnetworks and employing graph-based techniques. The proposed method utilizes a\nstate-of-the-art dynamic graph neural network to evaluate the performance of a\ngiven CP and update it using a heuristic algorithm based on simulated\nannealing. The trained neural network can predict the network delay with a mean\nabsolute error of 3.6 minutes. Simulation results show that the proposed method\ncan successfully design a contact plan for large satellite networks, improving\nthe delay by 29.1%, similar to a traditional approach, while performing the\nobjective evaluations 20x faster.\n", "link": "http://arxiv.org/abs/2403.00692v2", "date": "2024-03-04", "relevancy": 2.4607, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4997}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4959}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4808}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Autonomous%20Cooperation%20in%20Heterogeneous%20Nanosatellite%0A%20%20Constellations%20Using%20Dynamic%20Graph%20Neural%20Networks&entry.906535625=Guillem%20Casadesus-Vila%20and%20Joan-Adria%20Ruiz-de-Azua%20and%20Eduard%20Alarcon&entry.1292438233=%20%20The%20upcoming%20landscape%20of%20Earth%20Observation%20missions%20will%20defined%20by%0Anetworked%20heterogeneous%20nanosatellite%20constellations%20required%20to%20meet%20strict%0Amission%20requirements%2C%20such%20as%20revisit%20times%20and%20spatial%20resolution.%20However%2C%0Ascheduling%20satellite%20communications%20in%20these%20satellite%20networks%20through%0Aefficiently%20creating%20a%20global%20satellite%20Contact%20Plan%20%28CP%29%20is%20a%20complex%20task%2C%0Awith%20current%20solutions%20requiring%20ground-based%20coordination%20or%20being%20limited%20by%0Aonboard%20computational%20resources.%20The%20paper%20proposes%20a%20novel%20approach%20to%0Aovercome%20these%20challenges%20by%20modeling%20the%20constellations%20and%20CP%20as%20dynamic%0Anetworks%20and%20employing%20graph-based%20techniques.%20The%20proposed%20method%20utilizes%20a%0Astate-of-the-art%20dynamic%20graph%20neural%20network%20to%20evaluate%20the%20performance%20of%20a%0Agiven%20CP%20and%20update%20it%20using%20a%20heuristic%20algorithm%20based%20on%20simulated%0Aannealing.%20The%20trained%20neural%20network%20can%20predict%20the%20network%20delay%20with%20a%20mean%0Aabsolute%20error%20of%203.6%20minutes.%20Simulation%20results%20show%20that%20the%20proposed%20method%0Acan%20successfully%20design%20a%20contact%20plan%20for%20large%20satellite%20networks%2C%20improving%0Athe%20delay%20by%2029.1%25%2C%20similar%20to%20a%20traditional%20approach%2C%20while%20performing%20the%0Aobjective%20evaluations%2020x%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00692v2&entry.124074799=Read"},
{"title": "Improve Cost Efficiency of Active Learning over Noisy Dataset", "author": "Zan-Kai Chong and Hiroyuki Ohsaki and Bryan Ng", "abstract": "  Active learning is a learning strategy whereby the machine learning algorithm\nactively identifies and labels data points to optimize its learning. This\nstrategy is particularly effective in domains where an abundance of unlabeled\ndata exists, but the cost of labeling these data points is prohibitively\nexpensive. In this paper, we consider cases of binary classification, where\nacquiring a positive instance incurs a significantly higher cost compared to\nthat of negative instances. For example, in the financial industry, such as in\nmoney-lending businesses, a defaulted loan constitutes a positive event leading\nto substantial financial loss. To address this issue, we propose a shifted\nnormal distribution sampling function that samples from a wider range than\ntypical uncertainty sampling. Our simulation underscores that our proposed\nsampling function limits both noisy and positive label selection, delivering\nbetween 20% and 32% improved cost efficiency over different test datasets.\n", "link": "http://arxiv.org/abs/2403.01346v1", "date": "2024-03-02", "relevancy": 2.4562, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5069}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5035}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4634}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improve%20Cost%20Efficiency%20of%20Active%20Learning%20over%20Noisy%20Dataset&entry.906535625=Zan-Kai%20Chong%20and%20Hiroyuki%20Ohsaki%20and%20Bryan%20Ng&entry.1292438233=%20%20Active%20learning%20is%20a%20learning%20strategy%20whereby%20the%20machine%20learning%20algorithm%0Aactively%20identifies%20and%20labels%20data%20points%20to%20optimize%20its%20learning.%20This%0Astrategy%20is%20particularly%20effective%20in%20domains%20where%20an%20abundance%20of%20unlabeled%0Adata%20exists%2C%20but%20the%20cost%20of%20labeling%20these%20data%20points%20is%20prohibitively%0Aexpensive.%20In%20this%20paper%2C%20we%20consider%20cases%20of%20binary%20classification%2C%20where%0Aacquiring%20a%20positive%20instance%20incurs%20a%20significantly%20higher%20cost%20compared%20to%0Athat%20of%20negative%20instances.%20For%20example%2C%20in%20the%20financial%20industry%2C%20such%20as%20in%0Amoney-lending%20businesses%2C%20a%20defaulted%20loan%20constitutes%20a%20positive%20event%20leading%0Ato%20substantial%20financial%20loss.%20To%20address%20this%20issue%2C%20we%20propose%20a%20shifted%0Anormal%20distribution%20sampling%20function%20that%20samples%20from%20a%20wider%20range%20than%0Atypical%20uncertainty%20sampling.%20Our%20simulation%20underscores%20that%20our%20proposed%0Asampling%20function%20limits%20both%20noisy%20and%20positive%20label%20selection%2C%20delivering%0Abetween%2020%25%20and%2032%25%20improved%20cost%20efficiency%20over%20different%20test%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01346v1&entry.124074799=Read"},
{"title": "Assisted Learning for Organizations with Limited Imbalanced Data", "author": "Cheng Chen and Jiaying Zhou and Jie Ding and Yi Zhou", "abstract": "  In the era of big data, many big organizations are integrating machine\nlearning into their work pipelines to facilitate data analysis. However, the\nperformance of their trained models is often restricted by limited and\nimbalanced data available to them. In this work, we develop an assisted\nlearning framework for assisting organizations to improve their learning\nperformance. The organizations have sufficient computation resources but are\nsubject to stringent data-sharing and collaboration policies. Their limited\nimbalanced data often cause biased inference and sub-optimal decision-making.\nIn assisted learning, an organizational learner purchases assistance service\nfrom an external service provider and aims to enhance its model performance\nwithin only a few assistance rounds. We develop effective stochastic training\nalgorithms for both assisted deep learning and assisted reinforcement learning.\nDifferent from existing distributed algorithms that need to frequently transmit\ngradients or models, our framework allows the learner to only occasionally\nshare information with the service provider, but still obtain a model that\nachieves near-oracle performance as if all the data were centralized.\n", "link": "http://arxiv.org/abs/2109.09307v4", "date": "2024-03-02", "relevancy": 2.4556, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4981}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4892}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.486}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assisted%20Learning%20for%20Organizations%20with%20Limited%20Imbalanced%20Data&entry.906535625=Cheng%20Chen%20and%20Jiaying%20Zhou%20and%20Jie%20Ding%20and%20Yi%20Zhou&entry.1292438233=%20%20In%20the%20era%20of%20big%20data%2C%20many%20big%20organizations%20are%20integrating%20machine%0Alearning%20into%20their%20work%20pipelines%20to%20facilitate%20data%20analysis.%20However%2C%20the%0Aperformance%20of%20their%20trained%20models%20is%20often%20restricted%20by%20limited%20and%0Aimbalanced%20data%20available%20to%20them.%20In%20this%20work%2C%20we%20develop%20an%20assisted%0Alearning%20framework%20for%20assisting%20organizations%20to%20improve%20their%20learning%0Aperformance.%20The%20organizations%20have%20sufficient%20computation%20resources%20but%20are%0Asubject%20to%20stringent%20data-sharing%20and%20collaboration%20policies.%20Their%20limited%0Aimbalanced%20data%20often%20cause%20biased%20inference%20and%20sub-optimal%20decision-making.%0AIn%20assisted%20learning%2C%20an%20organizational%20learner%20purchases%20assistance%20service%0Afrom%20an%20external%20service%20provider%20and%20aims%20to%20enhance%20its%20model%20performance%0Awithin%20only%20a%20few%20assistance%20rounds.%20We%20develop%20effective%20stochastic%20training%0Aalgorithms%20for%20both%20assisted%20deep%20learning%20and%20assisted%20reinforcement%20learning.%0ADifferent%20from%20existing%20distributed%20algorithms%20that%20need%20to%20frequently%20transmit%0Agradients%20or%20models%2C%20our%20framework%20allows%20the%20learner%20to%20only%20occasionally%0Ashare%20information%20with%20the%20service%20provider%2C%20but%20still%20obtain%20a%20model%20that%0Aachieves%20near-oracle%20performance%20as%20if%20all%20the%20data%20were%20centralized.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.09307v4&entry.124074799=Read"},
{"title": "Teaching MLP More Graph Information: A Three-stage Multitask Knowledge\n  Distillation Framework", "author": "Junxian Li and Bin Shi and Erfei Cui and Hua Wei and Qinghua Zheng", "abstract": "  We study the challenging problem for inference tasks on large-scale graph\ndatasets of Graph Neural Networks: huge time and memory consumption, and try to\novercome it by reducing reliance on graph structure. Even though distilling\ngraph knowledge to student MLP is an excellent idea, it faces two major\nproblems of positional information loss and low generalization. To solve the\nproblems, we propose a new three-stage multitask distillation framework. In\ndetail, we use Positional Encoding to capture positional information. Also, we\nintroduce Neural Heat Kernels responsible for graph data processing in GNN and\nutilize hidden layer outputs matching for better performance of student MLP's\nhidden layers. To the best of our knowledge, it is the first work to include\nhidden layer distillation for student MLP on graphs and to combine graph\nPositional Encoding with MLP. We test its performance and robustness with\nseveral settings and draw the conclusion that our work can outperform well with\ngood stability.\n", "link": "http://arxiv.org/abs/2403.01079v1", "date": "2024-03-02", "relevancy": 2.4544, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5061}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4948}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4717}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20MLP%20More%20Graph%20Information%3A%20A%20Three-stage%20Multitask%20Knowledge%0A%20%20Distillation%20Framework&entry.906535625=Junxian%20Li%20and%20Bin%20Shi%20and%20Erfei%20Cui%20and%20Hua%20Wei%20and%20Qinghua%20Zheng&entry.1292438233=%20%20We%20study%20the%20challenging%20problem%20for%20inference%20tasks%20on%20large-scale%20graph%0Adatasets%20of%20Graph%20Neural%20Networks%3A%20huge%20time%20and%20memory%20consumption%2C%20and%20try%20to%0Aovercome%20it%20by%20reducing%20reliance%20on%20graph%20structure.%20Even%20though%20distilling%0Agraph%20knowledge%20to%20student%20MLP%20is%20an%20excellent%20idea%2C%20it%20faces%20two%20major%0Aproblems%20of%20positional%20information%20loss%20and%20low%20generalization.%20To%20solve%20the%0Aproblems%2C%20we%20propose%20a%20new%20three-stage%20multitask%20distillation%20framework.%20In%0Adetail%2C%20we%20use%20Positional%20Encoding%20to%20capture%20positional%20information.%20Also%2C%20we%0Aintroduce%20Neural%20Heat%20Kernels%20responsible%20for%20graph%20data%20processing%20in%20GNN%20and%0Autilize%20hidden%20layer%20outputs%20matching%20for%20better%20performance%20of%20student%20MLP%27s%0Ahidden%20layers.%20To%20the%20best%20of%20our%20knowledge%2C%20it%20is%20the%20first%20work%20to%20include%0Ahidden%20layer%20distillation%20for%20student%20MLP%20on%20graphs%20and%20to%20combine%20graph%0APositional%20Encoding%20with%20MLP.%20We%20test%20its%20performance%20and%20robustness%20with%0Aseveral%20settings%20and%20draw%20the%20conclusion%20that%20our%20work%20can%20outperform%20well%20with%0Agood%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01079v1&entry.124074799=Read"},
{"title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for\n  Recognizing Low-Quality Images", "author": "Jinsung Jeon and Hyundong Jin and Jonghyun Choi and Sanghyun Hong and Dongeun Lee and Kookjin Lee and Noseong Park", "abstract": "  A standard practice in developing image recognition models is to train a\nmodel on a specific image resolution and then deploy it. However, in real-world\ninference, models often encounter images different from the training sets in\nresolution and/or subject to natural variations such as weather changes, noise\ntypes and compression artifacts. While traditional solutions involve training\nmultiple models for different resolutions or input variations, these methods\nare computationally expensive and thus do not scale in practice. To this end,\nwe propose a novel neural network model, parallel-structured and all-component\nFourier neural operator (PAC-FNO), that addresses the problem. Unlike\nconventional feed-forward neural networks, PAC-FNO operates in the frequency\ndomain, allowing it to handle images of varying resolutions within a single\nmodel. We also propose a two-stage algorithm for training PAC-FNO with a\nminimal modification to the original, downstream model. Moreover, the proposed\nPAC-FNO is ready to work with existing image recognition models. Extensively\nevaluating methods with seven image recognition benchmarks, we show that the\nproposed PAC-FNO improves the performance of existing baseline models on images\nwith various resolutions by up to 77.1% and various types of natural variations\nin the images at inference.\n", "link": "http://arxiv.org/abs/2402.12721v3", "date": "2024-03-05", "relevancy": 2.4539, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.501}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4984}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.473}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAC-FNO%3A%20Parallel-Structured%20All-Component%20Fourier%20Neural%20Operators%20for%0A%20%20Recognizing%20Low-Quality%20Images&entry.906535625=Jinsung%20Jeon%20and%20Hyundong%20Jin%20and%20Jonghyun%20Choi%20and%20Sanghyun%20Hong%20and%20Dongeun%20Lee%20and%20Kookjin%20Lee%20and%20Noseong%20Park&entry.1292438233=%20%20A%20standard%20practice%20in%20developing%20image%20recognition%20models%20is%20to%20train%20a%0Amodel%20on%20a%20specific%20image%20resolution%20and%20then%20deploy%20it.%20However%2C%20in%20real-world%0Ainference%2C%20models%20often%20encounter%20images%20different%20from%20the%20training%20sets%20in%0Aresolution%20and/or%20subject%20to%20natural%20variations%20such%20as%20weather%20changes%2C%20noise%0Atypes%20and%20compression%20artifacts.%20While%20traditional%20solutions%20involve%20training%0Amultiple%20models%20for%20different%20resolutions%20or%20input%20variations%2C%20these%20methods%0Aare%20computationally%20expensive%20and%20thus%20do%20not%20scale%20in%20practice.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20neural%20network%20model%2C%20parallel-structured%20and%20all-component%0AFourier%20neural%20operator%20%28PAC-FNO%29%2C%20that%20addresses%20the%20problem.%20Unlike%0Aconventional%20feed-forward%20neural%20networks%2C%20PAC-FNO%20operates%20in%20the%20frequency%0Adomain%2C%20allowing%20it%20to%20handle%20images%20of%20varying%20resolutions%20within%20a%20single%0Amodel.%20We%20also%20propose%20a%20two-stage%20algorithm%20for%20training%20PAC-FNO%20with%20a%0Aminimal%20modification%20to%20the%20original%2C%20downstream%20model.%20Moreover%2C%20the%20proposed%0APAC-FNO%20is%20ready%20to%20work%20with%20existing%20image%20recognition%20models.%20Extensively%0Aevaluating%20methods%20with%20seven%20image%20recognition%20benchmarks%2C%20we%20show%20that%20the%0Aproposed%20PAC-FNO%20improves%20the%20performance%20of%20existing%20baseline%20models%20on%20images%0Awith%20various%20resolutions%20by%20up%20to%2077.1%25%20and%20various%20types%20of%20natural%20variations%0Ain%20the%20images%20at%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12721v3&entry.124074799=Read"},
{"title": "Influence of Camera-LiDAR Configuration on 3D Object Detection for\n  Autonomous Driving", "author": "Ye Li and Hanjiang Hu and Zuxin Liu and Xiaohao Xu and Xiaonan Huang and Ding Zhao", "abstract": "  Cameras and LiDARs are both important sensors for autonomous driving, playing\ncritical roles in 3D object detection. Camera-LiDAR Fusion has been a prevalent\nsolution for robust and accurate driving perception. In contrast to the vast\nmajority of existing arts that focus on how to improve the performance of 3D\ntarget detection through cross-modal schemes, deep learning algorithms, and\ntraining tricks, we devote attention to the impact of sensor configurations on\nthe performance of learning-based methods. To achieve this, we propose a\nunified information-theoretic surrogate metric for camera and LiDAR evaluation\nbased on the proposed sensor perception model. We also design an accelerated\nhigh-quality framework for data acquisition, model training, and performance\nevaluation that functions with the CARLA simulator. To show the correlation\nbetween detection performance and our surrogate metrics, We conduct experiments\nusing several camera-LiDAR placements and parameters inspired by self-driving\ncompanies and research institutions. Extensive experimental results of\nrepresentative algorithms on nuScenes dataset validate the effectiveness of our\nsurrogate metric, demonstrating that sensor configurations significantly impact\npoint-cloud-image fusion based detection models, which contribute up to 30%\ndiscrepancy in terms of the average precision.\n", "link": "http://arxiv.org/abs/2310.05245v2", "date": "2024-03-02", "relevancy": 2.447, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6229}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6119}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5836}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influence%20of%20Camera-LiDAR%20Configuration%20on%203D%20Object%20Detection%20for%0A%20%20Autonomous%20Driving&entry.906535625=Ye%20Li%20and%20Hanjiang%20Hu%20and%20Zuxin%20Liu%20and%20Xiaohao%20Xu%20and%20Xiaonan%20Huang%20and%20Ding%20Zhao&entry.1292438233=%20%20Cameras%20and%20LiDARs%20are%20both%20important%20sensors%20for%20autonomous%20driving%2C%20playing%0Acritical%20roles%20in%203D%20object%20detection.%20Camera-LiDAR%20Fusion%20has%20been%20a%20prevalent%0Asolution%20for%20robust%20and%20accurate%20driving%20perception.%20In%20contrast%20to%20the%20vast%0Amajority%20of%20existing%20arts%20that%20focus%20on%20how%20to%20improve%20the%20performance%20of%203D%0Atarget%20detection%20through%20cross-modal%20schemes%2C%20deep%20learning%20algorithms%2C%20and%0Atraining%20tricks%2C%20we%20devote%20attention%20to%20the%20impact%20of%20sensor%20configurations%20on%0Athe%20performance%20of%20learning-based%20methods.%20To%20achieve%20this%2C%20we%20propose%20a%0Aunified%20information-theoretic%20surrogate%20metric%20for%20camera%20and%20LiDAR%20evaluation%0Abased%20on%20the%20proposed%20sensor%20perception%20model.%20We%20also%20design%20an%20accelerated%0Ahigh-quality%20framework%20for%20data%20acquisition%2C%20model%20training%2C%20and%20performance%0Aevaluation%20that%20functions%20with%20the%20CARLA%20simulator.%20To%20show%20the%20correlation%0Abetween%20detection%20performance%20and%20our%20surrogate%20metrics%2C%20We%20conduct%20experiments%0Ausing%20several%20camera-LiDAR%20placements%20and%20parameters%20inspired%20by%20self-driving%0Acompanies%20and%20research%20institutions.%20Extensive%20experimental%20results%20of%0Arepresentative%20algorithms%20on%20nuScenes%20dataset%20validate%20the%20effectiveness%20of%20our%0Asurrogate%20metric%2C%20demonstrating%20that%20sensor%20configurations%20significantly%20impact%0Apoint-cloud-image%20fusion%20based%20detection%20models%2C%20which%20contribute%20up%20to%2030%25%0Adiscrepancy%20in%20terms%20of%20the%20average%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05245v2&entry.124074799=Read"},
{"title": "Deep Temporal Graph Clustering", "author": "Meng Liu and Yue Liu and Ke Liang and Wenxuan Tu and Siwei Wang and Sihang Zhou and Xinwang Liu", "abstract": "  Deep graph clustering has recently received significant attention due to its\nability to enhance the representation learning capabilities of models in\nunsupervised scenarios. Nevertheless, deep clustering for temporal graphs,\nwhich could capture crucial dynamic interaction information, has not been fully\nexplored. It means that in many clustering-oriented real-world scenarios,\ntemporal graphs can only be processed as static graphs. This not only causes\nthe loss of dynamic information but also triggers huge computational\nconsumption. To solve the problem, we propose a general framework for deep\nTemporal Graph Clustering called TGC, which introduces deep clustering\ntechniques to suit the interaction sequence-based batch-processing pattern of\ntemporal graphs. In addition, we discuss differences between temporal graph\nclustering and static graph clustering from several levels. To verify the\nsuperiority of the proposed framework TGC, we conduct extensive experiments.\nThe experimental results show that temporal graph clustering enables more\nflexibility in finding a balance between time and space requirements, and our\nframework can effectively improve the performance of existing temporal graph\nlearning methods. The code is released:\nhttps://github.com/MGitHubL/Deep-Temporal-Graph-Clustering.\n", "link": "http://arxiv.org/abs/2305.10738v2", "date": "2024-03-02", "relevancy": 2.4437, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5176}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4878}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4608}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Temporal%20Graph%20Clustering&entry.906535625=Meng%20Liu%20and%20Yue%20Liu%20and%20Ke%20Liang%20and%20Wenxuan%20Tu%20and%20Siwei%20Wang%20and%20Sihang%20Zhou%20and%20Xinwang%20Liu&entry.1292438233=%20%20Deep%20graph%20clustering%20has%20recently%20received%20significant%20attention%20due%20to%20its%0Aability%20to%20enhance%20the%20representation%20learning%20capabilities%20of%20models%20in%0Aunsupervised%20scenarios.%20Nevertheless%2C%20deep%20clustering%20for%20temporal%20graphs%2C%0Awhich%20could%20capture%20crucial%20dynamic%20interaction%20information%2C%20has%20not%20been%20fully%0Aexplored.%20It%20means%20that%20in%20many%20clustering-oriented%20real-world%20scenarios%2C%0Atemporal%20graphs%20can%20only%20be%20processed%20as%20static%20graphs.%20This%20not%20only%20causes%0Athe%20loss%20of%20dynamic%20information%20but%20also%20triggers%20huge%20computational%0Aconsumption.%20To%20solve%20the%20problem%2C%20we%20propose%20a%20general%20framework%20for%20deep%0ATemporal%20Graph%20Clustering%20called%20TGC%2C%20which%20introduces%20deep%20clustering%0Atechniques%20to%20suit%20the%20interaction%20sequence-based%20batch-processing%20pattern%20of%0Atemporal%20graphs.%20In%20addition%2C%20we%20discuss%20differences%20between%20temporal%20graph%0Aclustering%20and%20static%20graph%20clustering%20from%20several%20levels.%20To%20verify%20the%0Asuperiority%20of%20the%20proposed%20framework%20TGC%2C%20we%20conduct%20extensive%20experiments.%0AThe%20experimental%20results%20show%20that%20temporal%20graph%20clustering%20enables%20more%0Aflexibility%20in%20finding%20a%20balance%20between%20time%20and%20space%20requirements%2C%20and%20our%0Aframework%20can%20effectively%20improve%20the%20performance%20of%20existing%20temporal%20graph%0Alearning%20methods.%20The%20code%20is%20released%3A%0Ahttps%3A//github.com/MGitHubL/Deep-Temporal-Graph-Clustering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10738v2&entry.124074799=Read"},
{"title": "Tree-Regularized Tabular Embeddings", "author": "Xuan Li and Yun Wang and Bo Li", "abstract": "  Tabular neural network (NN) has attracted remarkable attentions and its\nrecent advances have gradually narrowed the performance gap with respect to\ntree-based models on many public datasets. While the mainstreams focus on\ncalibrating NN to fit tabular data, we emphasize the importance of homogeneous\nembeddings and alternately concentrate on regularizing tabular inputs through\nsupervised pretraining. Specifically, we extend a recent work (DeepTLF) and\nutilize the structure of pretrained tree ensembles to transform raw variables\ninto a single vector (T2V), or an array of tokens (T2T). Without loss of space\nefficiency, these binarized embeddings can be consumed by canonical tabular NN\nwith fully-connected or attention-based building blocks. Through quantitative\nexperiments on 88 OpenML datasets with binary classification task, we validated\nthat the proposed tree-regularized representation not only tapers the\ndifference with respect to tree-based models, but also achieves on-par and\nbetter performance when compared with advanced NN models. Most importantly, it\npossesses better robustness and can be easily scaled and generalized as\nstandalone encoder for tabular modality. Codes:\nhttps://github.com/milanlx/tree-regularized-embedding.\n", "link": "http://arxiv.org/abs/2403.00963v1", "date": "2024-03-01", "relevancy": 2.4399, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4975}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.496}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4704}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree-Regularized%20Tabular%20Embeddings&entry.906535625=Xuan%20Li%20and%20Yun%20Wang%20and%20Bo%20Li&entry.1292438233=%20%20Tabular%20neural%20network%20%28NN%29%20has%20attracted%20remarkable%20attentions%20and%20its%0Arecent%20advances%20have%20gradually%20narrowed%20the%20performance%20gap%20with%20respect%20to%0Atree-based%20models%20on%20many%20public%20datasets.%20While%20the%20mainstreams%20focus%20on%0Acalibrating%20NN%20to%20fit%20tabular%20data%2C%20we%20emphasize%20the%20importance%20of%20homogeneous%0Aembeddings%20and%20alternately%20concentrate%20on%20regularizing%20tabular%20inputs%20through%0Asupervised%20pretraining.%20Specifically%2C%20we%20extend%20a%20recent%20work%20%28DeepTLF%29%20and%0Autilize%20the%20structure%20of%20pretrained%20tree%20ensembles%20to%20transform%20raw%20variables%0Ainto%20a%20single%20vector%20%28T2V%29%2C%20or%20an%20array%20of%20tokens%20%28T2T%29.%20Without%20loss%20of%20space%0Aefficiency%2C%20these%20binarized%20embeddings%20can%20be%20consumed%20by%20canonical%20tabular%20NN%0Awith%20fully-connected%20or%20attention-based%20building%20blocks.%20Through%20quantitative%0Aexperiments%20on%2088%20OpenML%20datasets%20with%20binary%20classification%20task%2C%20we%20validated%0Athat%20the%20proposed%20tree-regularized%20representation%20not%20only%20tapers%20the%0Adifference%20with%20respect%20to%20tree-based%20models%2C%20but%20also%20achieves%20on-par%20and%0Abetter%20performance%20when%20compared%20with%20advanced%20NN%20models.%20Most%20importantly%2C%20it%0Apossesses%20better%20robustness%20and%20can%20be%20easily%20scaled%20and%20generalized%20as%0Astandalone%20encoder%20for%20tabular%20modality.%20Codes%3A%0Ahttps%3A//github.com/milanlx/tree-regularized-embedding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00963v1&entry.124074799=Read"},
{"title": "Less is More: Hop-Wise Graph Attention for Scalable and Generalizable\n  Learning on Circuits", "author": "Chenhui Deng and Zichao Yue and Cunxi Yu and Gokce Sarar and Ryan Carey and Rajeev Jain and Zhiru Zhang", "abstract": "  While graph neural networks (GNNs) have gained popularity for learning\ncircuit representations in various electronic design automation (EDA) tasks,\nthey face challenges in scalability when applied to large graphs and exhibit\nlimited generalizability to new designs. These limitations make them less\npractical for addressing large-scale, complex circuit problems. In this work we\npropose HOGA, a novel attention-based model for learning circuit\nrepresentations in a scalable and generalizable manner. HOGA first computes\nhop-wise features per node prior to model training. Subsequently, the hop-wise\nfeatures are solely used to produce node representations through a gated\nself-attention module, which adaptively learns important features among\ndifferent hops without involving the graph topology. As a result, HOGA is\nadaptive to various structures across different circuits and can be efficiently\ntrained in a distributed manner. To demonstrate the efficacy of HOGA, we\nconsider two representative EDA tasks: quality of results (QoR) prediction and\nfunctional reasoning. Our experimental results indicate that (1) HOGA reduces\nestimation error over conventional GNNs by 46.76% for predicting QoR after\nlogic synthesis; (2) HOGA improves 10.0% reasoning accuracy over GNNs for\nidentifying functional blocks on unseen gate-level netlists after complex\ntechnology mapping; (3) The training time for HOGA almost linearly decreases\nwith an increase in computing resources.\n", "link": "http://arxiv.org/abs/2403.01317v1", "date": "2024-03-02", "relevancy": 2.4371, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5005}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4965}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4652}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Hop-Wise%20Graph%20Attention%20for%20Scalable%20and%20Generalizable%0A%20%20Learning%20on%20Circuits&entry.906535625=Chenhui%20Deng%20and%20Zichao%20Yue%20and%20Cunxi%20Yu%20and%20Gokce%20Sarar%20and%20Ryan%20Carey%20and%20Rajeev%20Jain%20and%20Zhiru%20Zhang&entry.1292438233=%20%20While%20graph%20neural%20networks%20%28GNNs%29%20have%20gained%20popularity%20for%20learning%0Acircuit%20representations%20in%20various%20electronic%20design%20automation%20%28EDA%29%20tasks%2C%0Athey%20face%20challenges%20in%20scalability%20when%20applied%20to%20large%20graphs%20and%20exhibit%0Alimited%20generalizability%20to%20new%20designs.%20These%20limitations%20make%20them%20less%0Apractical%20for%20addressing%20large-scale%2C%20complex%20circuit%20problems.%20In%20this%20work%20we%0Apropose%20HOGA%2C%20a%20novel%20attention-based%20model%20for%20learning%20circuit%0Arepresentations%20in%20a%20scalable%20and%20generalizable%20manner.%20HOGA%20first%20computes%0Ahop-wise%20features%20per%20node%20prior%20to%20model%20training.%20Subsequently%2C%20the%20hop-wise%0Afeatures%20are%20solely%20used%20to%20produce%20node%20representations%20through%20a%20gated%0Aself-attention%20module%2C%20which%20adaptively%20learns%20important%20features%20among%0Adifferent%20hops%20without%20involving%20the%20graph%20topology.%20As%20a%20result%2C%20HOGA%20is%0Aadaptive%20to%20various%20structures%20across%20different%20circuits%20and%20can%20be%20efficiently%0Atrained%20in%20a%20distributed%20manner.%20To%20demonstrate%20the%20efficacy%20of%20HOGA%2C%20we%0Aconsider%20two%20representative%20EDA%20tasks%3A%20quality%20of%20results%20%28QoR%29%20prediction%20and%0Afunctional%20reasoning.%20Our%20experimental%20results%20indicate%20that%20%281%29%20HOGA%20reduces%0Aestimation%20error%20over%20conventional%20GNNs%20by%2046.76%25%20for%20predicting%20QoR%20after%0Alogic%20synthesis%3B%20%282%29%20HOGA%20improves%2010.0%25%20reasoning%20accuracy%20over%20GNNs%20for%0Aidentifying%20functional%20blocks%20on%20unseen%20gate-level%20netlists%20after%20complex%0Atechnology%20mapping%3B%20%283%29%20The%20training%20time%20for%20HOGA%20almost%20linearly%20decreases%0Awith%20an%20increase%20in%20computing%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01317v1&entry.124074799=Read"},
{"title": "Neural radiance fields-based holography [Invited]", "author": "Minsung Kang and Fan Wang and Kai Kumano and Tomoyoshi Ito and Tomoyoshi Shimobaba", "abstract": "  This study presents a novel approach for generating holograms based on the\nneural radiance fields (NeRF) technique. Generating three-dimensional (3D) data\nis difficult in hologram computation. NeRF is a state-of-the-art technique for\n3D light-field reconstruction from 2D images based on volume rendering. The\nNeRF can rapidly predict new-view images that do not include a training\ndataset. In this study, we constructed a rendering pipeline directly from a 3D\nlight field generated from 2D images by NeRF for hologram generation using deep\nneural networks within a reasonable time. The pipeline comprises three main\ncomponents: the NeRF, a depth predictor, and a hologram generator, all\nconstructed using deep neural networks. The pipeline does not include any\nphysical calculations. The predicted holograms of a 3D scene viewed from any\ndirection were computed using the proposed pipeline. The simulation and\nexperimental results are presented.\n", "link": "http://arxiv.org/abs/2403.01137v1", "date": "2024-03-02", "relevancy": 2.428, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.512}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4749}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4699}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20radiance%20fields-based%20holography%20%5BInvited%5D&entry.906535625=Minsung%20Kang%20and%20Fan%20Wang%20and%20Kai%20Kumano%20and%20Tomoyoshi%20Ito%20and%20Tomoyoshi%20Shimobaba&entry.1292438233=%20%20This%20study%20presents%20a%20novel%20approach%20for%20generating%20holograms%20based%20on%20the%0Aneural%20radiance%20fields%20%28NeRF%29%20technique.%20Generating%20three-dimensional%20%283D%29%20data%0Ais%20difficult%20in%20hologram%20computation.%20NeRF%20is%20a%20state-of-the-art%20technique%20for%0A3D%20light-field%20reconstruction%20from%202D%20images%20based%20on%20volume%20rendering.%20The%0ANeRF%20can%20rapidly%20predict%20new-view%20images%20that%20do%20not%20include%20a%20training%0Adataset.%20In%20this%20study%2C%20we%20constructed%20a%20rendering%20pipeline%20directly%20from%20a%203D%0Alight%20field%20generated%20from%202D%20images%20by%20NeRF%20for%20hologram%20generation%20using%20deep%0Aneural%20networks%20within%20a%20reasonable%20time.%20The%20pipeline%20comprises%20three%20main%0Acomponents%3A%20the%20NeRF%2C%20a%20depth%20predictor%2C%20and%20a%20hologram%20generator%2C%20all%0Aconstructed%20using%20deep%20neural%20networks.%20The%20pipeline%20does%20not%20include%20any%0Aphysical%20calculations.%20The%20predicted%20holograms%20of%20a%203D%20scene%20viewed%20from%20any%0Adirection%20were%20computed%20using%20the%20proposed%20pipeline.%20The%20simulation%20and%0Aexperimental%20results%20are%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01137v1&entry.124074799=Read"},
{"title": "NoMAD-Attention: Efficient LLM Inference on CPUs Through\n  Multiply-add-free Attention", "author": "Tianyi Zhang and Jonah Wonkyu Yi and Bowen Yao and Zhaozhuo Xu and Anshumali Shrivastava", "abstract": "  Large language model inference on Central Processing Units (CPU) is\nchallenging due to the vast quantities of expensive Multiply-Add (MAD) matrix\noperations in the attention computations. In this paper, we argue that there is\na rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers,\nwhich allow for ultra-low-latency lookups in batch. We leverage this unique\ncapability of CPUs to propose NoMAD-Attention, an efficient attention algorithm\nthat replaces MAD operations with in-register lookups. Through hardware-aware\nalgorithmic designs, NoMAD-Attention achieves the computation of attention\nscores using repeated fast accesses to SIMD registers despite their highly\nlimited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based\nLLMs without model finetuning. Empirical evaluations demonstrate that\nNoMAD-Attention maintains the quality of the original LLMs well, and speeds up\nthe 4-bit quantized LLaMA-7B-based model by up to 2$\\times$ at 16k context\nlength. Our results are reproducible at\nhttps://github.com/tonyzhang617/nomad-dist.\n", "link": "http://arxiv.org/abs/2403.01273v1", "date": "2024-03-02", "relevancy": 2.4244, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5198}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4801}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4548}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoMAD-Attention%3A%20Efficient%20LLM%20Inference%20on%20CPUs%20Through%0A%20%20Multiply-add-free%20Attention&entry.906535625=Tianyi%20Zhang%20and%20Jonah%20Wonkyu%20Yi%20and%20Bowen%20Yao%20and%20Zhaozhuo%20Xu%20and%20Anshumali%20Shrivastava&entry.1292438233=%20%20Large%20language%20model%20inference%20on%20Central%20Processing%20Units%20%28CPU%29%20is%0Achallenging%20due%20to%20the%20vast%20quantities%20of%20expensive%20Multiply-Add%20%28MAD%29%20matrix%0Aoperations%20in%20the%20attention%20computations.%20In%20this%20paper%2C%20we%20argue%20that%20there%20is%0Aa%20rare%20gem%20in%20modern%20CPUs%2C%20Single-Instruction-Multiple-Data%20%28SIMD%29%20registers%2C%0Awhich%20allow%20for%20ultra-low-latency%20lookups%20in%20batch.%20We%20leverage%20this%20unique%0Acapability%20of%20CPUs%20to%20propose%20NoMAD-Attention%2C%20an%20efficient%20attention%20algorithm%0Athat%20replaces%20MAD%20operations%20with%20in-register%20lookups.%20Through%20hardware-aware%0Aalgorithmic%20designs%2C%20NoMAD-Attention%20achieves%20the%20computation%20of%20attention%0Ascores%20using%20repeated%20fast%20accesses%20to%20SIMD%20registers%20despite%20their%20highly%0Alimited%20sizes.%20Moreover%2C%20NoMAD-Attention%20works%20with%20pre-trained%20attention-based%0ALLMs%20without%20model%20finetuning.%20Empirical%20evaluations%20demonstrate%20that%0ANoMAD-Attention%20maintains%20the%20quality%20of%20the%20original%20LLMs%20well%2C%20and%20speeds%20up%0Athe%204-bit%20quantized%20LLaMA-7B-based%20model%20by%20up%20to%202%24%5Ctimes%24%20at%2016k%20context%0Alength.%20Our%20results%20are%20reproducible%20at%0Ahttps%3A//github.com/tonyzhang617/nomad-dist.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01273v1&entry.124074799=Read"},
{"title": "Protect and Extend -- Using GANs for Synthetic Data Generation of\n  Time-Series Medical Records", "author": "Navid Ashrafi and Vera Schmitt and Robert P. Spang and Sebastian M\u00f6ller and Jan-Niklas Voigt-Antons", "abstract": "  Preservation of private user data is of paramount importance for high Quality\nof Experience (QoE) and acceptability, particularly with services treating\nsensitive data, such as IT-based health services. Whereas anonymization\ntechniques were shown to be prone to data re-identification, synthetic data\ngeneration has gradually replaced anonymization since it is relatively less\ntime and resource-consuming and more robust to data leakage. Generative\nAdversarial Networks (GANs) have been used for generating synthetic datasets,\nespecially GAN frameworks adhering to the differential privacy phenomena. This\nresearch compares state-of-the-art GAN-based models for synthetic data\ngeneration to generate time-series synthetic medical records of dementia\npatients which can be distributed without privacy concerns. Predictive\nmodeling, autocorrelation, and distribution analysis are used to assess the\nQuality of Generating (QoG) of the generated data. The privacy preservation of\nthe respective models is assessed by applying membership inference attacks to\ndetermine potential data leakage risks. Our experiments indicate the\nsuperiority of the privacy-preserving GAN (PPGAN) model over other models\nregarding privacy preservation while maintaining an acceptable level of QoG.\nThe presented results can support better data protection for medical use cases\nin the future.\n", "link": "http://arxiv.org/abs/2402.14042v2", "date": "2024-03-01", "relevancy": 2.4222, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5034}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4852}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4647}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protect%20and%20Extend%20--%20Using%20GANs%20for%20Synthetic%20Data%20Generation%20of%0A%20%20Time-Series%20Medical%20Records&entry.906535625=Navid%20Ashrafi%20and%20Vera%20Schmitt%20and%20Robert%20P.%20Spang%20and%20Sebastian%20M%C3%B6ller%20and%20Jan-Niklas%20Voigt-Antons&entry.1292438233=%20%20Preservation%20of%20private%20user%20data%20is%20of%20paramount%20importance%20for%20high%20Quality%0Aof%20Experience%20%28QoE%29%20and%20acceptability%2C%20particularly%20with%20services%20treating%0Asensitive%20data%2C%20such%20as%20IT-based%20health%20services.%20Whereas%20anonymization%0Atechniques%20were%20shown%20to%20be%20prone%20to%20data%20re-identification%2C%20synthetic%20data%0Ageneration%20has%20gradually%20replaced%20anonymization%20since%20it%20is%20relatively%20less%0Atime%20and%20resource-consuming%20and%20more%20robust%20to%20data%20leakage.%20Generative%0AAdversarial%20Networks%20%28GANs%29%20have%20been%20used%20for%20generating%20synthetic%20datasets%2C%0Aespecially%20GAN%20frameworks%20adhering%20to%20the%20differential%20privacy%20phenomena.%20This%0Aresearch%20compares%20state-of-the-art%20GAN-based%20models%20for%20synthetic%20data%0Ageneration%20to%20generate%20time-series%20synthetic%20medical%20records%20of%20dementia%0Apatients%20which%20can%20be%20distributed%20without%20privacy%20concerns.%20Predictive%0Amodeling%2C%20autocorrelation%2C%20and%20distribution%20analysis%20are%20used%20to%20assess%20the%0AQuality%20of%20Generating%20%28QoG%29%20of%20the%20generated%20data.%20The%20privacy%20preservation%20of%0Athe%20respective%20models%20is%20assessed%20by%20applying%20membership%20inference%20attacks%20to%0Adetermine%20potential%20data%20leakage%20risks.%20Our%20experiments%20indicate%20the%0Asuperiority%20of%20the%20privacy-preserving%20GAN%20%28PPGAN%29%20model%20over%20other%20models%0Aregarding%20privacy%20preservation%20while%20maintaining%20an%20acceptable%20level%20of%20QoG.%0AThe%20presented%20results%20can%20support%20better%20data%20protection%20for%20medical%20use%20cases%0Ain%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14042v2&entry.124074799=Read"},
{"title": "AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence\n  Inference", "author": "Xuanlei Zhao and Shenggan Cheng and Guangyang Lu and Jiarui Fang and Haotian Zhou and Bin Jia and Ziming Liu and Yang You", "abstract": "  Large deep learning models have achieved impressive performance across a\nrange of applications. However, their large memory requirements, including\nparameter memory and activation memory, have become a significant challenge for\ntheir practical serving. While existing methods mainly address parameter\nmemory, the importance of activation memory has been overlooked. Especially for\nlong input sequences, activation memory is expected to experience a significant\nexponential growth as the length of sequences increases. In this approach, we\npropose AutoChunk, an automatic and adaptive compiler system that efficiently\nreduces activation memory for long sequence inference by chunk strategies. The\nproposed system generates chunk plans by optimizing through multiple stages. In\neach stage, the chunk search pass explores all possible chunk candidates and\nthe chunk selection pass identifies the optimal one. At runtime, AutoChunk\nemploys code generation to automatically apply chunk strategies. The\nexperiments demonstrate that AutoChunk can reduce over 80\\% of activation\nmemory while maintaining speed loss within 10%, extend max sequence length by\n3.2x to 11.7x, and outperform state-of-the-art methods by a large margin.\n", "link": "http://arxiv.org/abs/2401.10652v2", "date": "2024-03-02", "relevancy": 2.4214, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4905}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4841}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4783}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoChunk%3A%20Automated%20Activation%20Chunk%20for%20Memory-Efficient%20Long%20Sequence%0A%20%20Inference&entry.906535625=Xuanlei%20Zhao%20and%20Shenggan%20Cheng%20and%20Guangyang%20Lu%20and%20Jiarui%20Fang%20and%20Haotian%20Zhou%20and%20Bin%20Jia%20and%20Ziming%20Liu%20and%20Yang%20You&entry.1292438233=%20%20Large%20deep%20learning%20models%20have%20achieved%20impressive%20performance%20across%20a%0Arange%20of%20applications.%20However%2C%20their%20large%20memory%20requirements%2C%20including%0Aparameter%20memory%20and%20activation%20memory%2C%20have%20become%20a%20significant%20challenge%20for%0Atheir%20practical%20serving.%20While%20existing%20methods%20mainly%20address%20parameter%0Amemory%2C%20the%20importance%20of%20activation%20memory%20has%20been%20overlooked.%20Especially%20for%0Along%20input%20sequences%2C%20activation%20memory%20is%20expected%20to%20experience%20a%20significant%0Aexponential%20growth%20as%20the%20length%20of%20sequences%20increases.%20In%20this%20approach%2C%20we%0Apropose%20AutoChunk%2C%20an%20automatic%20and%20adaptive%20compiler%20system%20that%20efficiently%0Areduces%20activation%20memory%20for%20long%20sequence%20inference%20by%20chunk%20strategies.%20The%0Aproposed%20system%20generates%20chunk%20plans%20by%20optimizing%20through%20multiple%20stages.%20In%0Aeach%20stage%2C%20the%20chunk%20search%20pass%20explores%20all%20possible%20chunk%20candidates%20and%0Athe%20chunk%20selection%20pass%20identifies%20the%20optimal%20one.%20At%20runtime%2C%20AutoChunk%0Aemploys%20code%20generation%20to%20automatically%20apply%20chunk%20strategies.%20The%0Aexperiments%20demonstrate%20that%20AutoChunk%20can%20reduce%20over%2080%5C%25%20of%20activation%0Amemory%20while%20maintaining%20speed%20loss%20within%2010%25%2C%20extend%20max%20sequence%20length%20by%0A3.2x%20to%2011.7x%2C%20and%20outperform%20state-of-the-art%20methods%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10652v2&entry.124074799=Read"},
{"title": "Escaping mediocrity: how two-layer networks learn hard generalized\n  linear models with SGD", "author": "Luca Arnaboldi and Florent Krzakala and Bruno Loureiro and Ludovic Stephan", "abstract": "  This study explores the sample complexity for two-layer neural networks to\nlearn a generalized linear target function under Stochastic Gradient Descent\n(SGD), focusing on the challenging regime where many flat directions are\npresent at initialization. It is well-established that in this scenario $n=O(d\n\\log d)$ samples are typically needed. However, we provide precise results\nconcerning the pre-factors in high-dimensional contexts and for varying widths.\nNotably, our findings suggest that overparameterization can only enhance\nconvergence by a constant factor within this problem class. These insights are\ngrounded in the reduction of SGD dynamics to a stochastic process in lower\ndimensions, where escaping mediocrity equates to calculating an exit time. Yet,\nwe demonstrate that a deterministic approximation of this process adequately\nrepresents the escape time, implying that the role of stochasticity may be\nminimal in this scenario.\n", "link": "http://arxiv.org/abs/2305.18502v2", "date": "2024-03-01", "relevancy": 2.4193, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5125}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4797}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4593}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Escaping%20mediocrity%3A%20how%20two-layer%20networks%20learn%20hard%20generalized%0A%20%20linear%20models%20with%20SGD&entry.906535625=Luca%20Arnaboldi%20and%20Florent%20Krzakala%20and%20Bruno%20Loureiro%20and%20Ludovic%20Stephan&entry.1292438233=%20%20This%20study%20explores%20the%20sample%20complexity%20for%20two-layer%20neural%20networks%20to%0Alearn%20a%20generalized%20linear%20target%20function%20under%20Stochastic%20Gradient%20Descent%0A%28SGD%29%2C%20focusing%20on%20the%20challenging%20regime%20where%20many%20flat%20directions%20are%0Apresent%20at%20initialization.%20It%20is%20well-established%20that%20in%20this%20scenario%20%24n%3DO%28d%0A%5Clog%20d%29%24%20samples%20are%20typically%20needed.%20However%2C%20we%20provide%20precise%20results%0Aconcerning%20the%20pre-factors%20in%20high-dimensional%20contexts%20and%20for%20varying%20widths.%0ANotably%2C%20our%20findings%20suggest%20that%20overparameterization%20can%20only%20enhance%0Aconvergence%20by%20a%20constant%20factor%20within%20this%20problem%20class.%20These%20insights%20are%0Agrounded%20in%20the%20reduction%20of%20SGD%20dynamics%20to%20a%20stochastic%20process%20in%20lower%0Adimensions%2C%20where%20escaping%20mediocrity%20equates%20to%20calculating%20an%20exit%20time.%20Yet%2C%0Awe%20demonstrate%20that%20a%20deterministic%20approximation%20of%20this%20process%20adequately%0Arepresents%20the%20escape%20time%2C%20implying%20that%20the%20role%20of%20stochasticity%20may%20be%0Aminimal%20in%20this%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.18502v2&entry.124074799=Read"},
{"title": "Memorization Capacity of Multi-Head Attention in Transformers", "author": "Sadegh Mahdavi and Renjie Liao and Christos Thrampoulidis", "abstract": "  Transformers have become the go-to architecture for language and vision\ntasks, yet their theoretical properties, especially memorization capacity,\nremain elusive. This paper investigates the memorization abilities of\nmulti-head attention mechanisms, examining how many example sequences they can\nmemorize, as a function of the number of heads and sequence length. Motivated\nby experimental findings on vision transformers, we introduce novel assumptions\nabout the linear independence of input data, distinct from the commonly used\ngeneral-position assumption. Under these assumptions, we demonstrate that an\nattention layer with $H$ heads, dimension $d$, and context size $n < d$,\nfeaturing $\\Theta(Hd^2)$ parameters, can memorize $\\Omega(Hn)$ examples. Our\nanalysis sheds light on how different attention heads handle various example\nsequences, aided by the softmax operator's saturation property. We validate our\nfindings through experiments on synthetic data.\n", "link": "http://arxiv.org/abs/2306.02010v3", "date": "2024-03-02", "relevancy": 2.4182, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4854}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.432}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorization%20Capacity%20of%20Multi-Head%20Attention%20in%20Transformers&entry.906535625=Sadegh%20Mahdavi%20and%20Renjie%20Liao%20and%20Christos%20Thrampoulidis&entry.1292438233=%20%20Transformers%20have%20become%20the%20go-to%20architecture%20for%20language%20and%20vision%0Atasks%2C%20yet%20their%20theoretical%20properties%2C%20especially%20memorization%20capacity%2C%0Aremain%20elusive.%20This%20paper%20investigates%20the%20memorization%20abilities%20of%0Amulti-head%20attention%20mechanisms%2C%20examining%20how%20many%20example%20sequences%20they%20can%0Amemorize%2C%20as%20a%20function%20of%20the%20number%20of%20heads%20and%20sequence%20length.%20Motivated%0Aby%20experimental%20findings%20on%20vision%20transformers%2C%20we%20introduce%20novel%20assumptions%0Aabout%20the%20linear%20independence%20of%20input%20data%2C%20distinct%20from%20the%20commonly%20used%0Ageneral-position%20assumption.%20Under%20these%20assumptions%2C%20we%20demonstrate%20that%20an%0Aattention%20layer%20with%20%24H%24%20heads%2C%20dimension%20%24d%24%2C%20and%20context%20size%20%24n%20%3C%20d%24%2C%0Afeaturing%20%24%5CTheta%28Hd%5E2%29%24%20parameters%2C%20can%20memorize%20%24%5COmega%28Hn%29%24%20examples.%20Our%0Aanalysis%20sheds%20light%20on%20how%20different%20attention%20heads%20handle%20various%20example%0Asequences%2C%20aided%20by%20the%20softmax%20operator%27s%20saturation%20property.%20We%20validate%20our%0Afindings%20through%20experiments%20on%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02010v3&entry.124074799=Read"},
{"title": "Machine Learning Training Optimization using the Barycentric Correction\n  Procedure", "author": "Sofia Ramos-Pulido and Neil Hernandez-Gress and Hector G. Ceballos-Cancino", "abstract": "  Machine learning (ML) algorithms are predictively competitive algorithms with\nmany human-impact applications. However, the issue of long execution time\nremains unsolved in the literature for high-dimensional spaces. This study\nproposes combining ML algorithms with an efficient methodology known as the\nbarycentric correction procedure (BCP) to address this issue. This study uses\nsynthetic data and an educational dataset from a private university to show the\nbenefits of the proposed method. It was found that this combination provides\nsignificant benefits related to time in synthetic and real data without losing\naccuracy when the number of instances and dimensions increases. Additionally,\nfor high-dimensional spaces, it was proved that BCP and linear support vector\nclassification (LinearSVC), after an estimated feature map for the gaussian\nradial basis function (RBF) kernel, were unfeasible in terms of computational\ntime and accuracy.\n", "link": "http://arxiv.org/abs/2403.00542v1", "date": "2024-03-01", "relevancy": 2.4085, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4871}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4812}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4767}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Training%20Optimization%20using%20the%20Barycentric%20Correction%0A%20%20Procedure&entry.906535625=Sofia%20Ramos-Pulido%20and%20Neil%20Hernandez-Gress%20and%20Hector%20G.%20Ceballos-Cancino&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20algorithms%20are%20predictively%20competitive%20algorithms%20with%0Amany%20human-impact%20applications.%20However%2C%20the%20issue%20of%20long%20execution%20time%0Aremains%20unsolved%20in%20the%20literature%20for%20high-dimensional%20spaces.%20This%20study%0Aproposes%20combining%20ML%20algorithms%20with%20an%20efficient%20methodology%20known%20as%20the%0Abarycentric%20correction%20procedure%20%28BCP%29%20to%20address%20this%20issue.%20This%20study%20uses%0Asynthetic%20data%20and%20an%20educational%20dataset%20from%20a%20private%20university%20to%20show%20the%0Abenefits%20of%20the%20proposed%20method.%20It%20was%20found%20that%20this%20combination%20provides%0Asignificant%20benefits%20related%20to%20time%20in%20synthetic%20and%20real%20data%20without%20losing%0Aaccuracy%20when%20the%20number%20of%20instances%20and%20dimensions%20increases.%20Additionally%2C%0Afor%20high-dimensional%20spaces%2C%20it%20was%20proved%20that%20BCP%20and%20linear%20support%20vector%0Aclassification%20%28LinearSVC%29%2C%20after%20an%20estimated%20feature%20map%20for%20the%20gaussian%0Aradial%20basis%20function%20%28RBF%29%20kernel%2C%20were%20unfeasible%20in%20terms%20of%20computational%0Atime%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00542v1&entry.124074799=Read"},
{"title": "GAMMA: Generalizable Articulation Modeling and Manipulation for\n  Articulated Objects", "author": "Qiaojun Yu and Junbo Wang and Wenhai Liu and Ce Hao and Liu Liu and Lin Shao and Weiming Wang and Cewu Lu", "abstract": "  Articulated objects like cabinets and doors are widespread in daily life.\nHowever, directly manipulating 3D articulated objects is challenging because\nthey have diverse geometrical shapes, semantic categories, and kinetic\nconstraints. Prior works mostly focused on recognizing and manipulating\narticulated objects with specific joint types. They can either estimate the\njoint parameters or distinguish suitable grasp poses to facilitate trajectory\nplanning. Although these approaches have succeeded in certain types of\narticulated objects, they lack generalizability to unseen objects, which\nsignificantly impedes their application in broader scenarios. In this paper, we\npropose a novel framework of Generalizable Articulation Modeling and\nManipulating for Articulated Objects (GAMMA), which learns both articulation\nmodeling and grasp pose affordance from diverse articulated objects with\ndifferent categories. In addition, GAMMA adopts adaptive manipulation to\niteratively reduce the modeling errors and enhance manipulation performance. We\ntrain GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive\nexperiments in SAPIEN simulation and real-world Franka robot. Results show that\nGAMMA significantly outperforms SOTA articulation modeling and manipulation\nalgorithms in unseen and cross-category articulated objects. We will\nopen-source all codes and datasets in both simulation and real robots for\nreproduction in the final version. Images and videos are published on the\nproject website at: http://sites.google.com/view/gamma-articulation\n", "link": "http://arxiv.org/abs/2309.16264v3", "date": "2024-03-01", "relevancy": 2.4038, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6358}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5895}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5707}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAMMA%3A%20Generalizable%20Articulation%20Modeling%20and%20Manipulation%20for%0A%20%20Articulated%20Objects&entry.906535625=Qiaojun%20Yu%20and%20Junbo%20Wang%20and%20Wenhai%20Liu%20and%20Ce%20Hao%20and%20Liu%20Liu%20and%20Lin%20Shao%20and%20Weiming%20Wang%20and%20Cewu%20Lu&entry.1292438233=%20%20Articulated%20objects%20like%20cabinets%20and%20doors%20are%20widespread%20in%20daily%20life.%0AHowever%2C%20directly%20manipulating%203D%20articulated%20objects%20is%20challenging%20because%0Athey%20have%20diverse%20geometrical%20shapes%2C%20semantic%20categories%2C%20and%20kinetic%0Aconstraints.%20Prior%20works%20mostly%20focused%20on%20recognizing%20and%20manipulating%0Aarticulated%20objects%20with%20specific%20joint%20types.%20They%20can%20either%20estimate%20the%0Ajoint%20parameters%20or%20distinguish%20suitable%20grasp%20poses%20to%20facilitate%20trajectory%0Aplanning.%20Although%20these%20approaches%20have%20succeeded%20in%20certain%20types%20of%0Aarticulated%20objects%2C%20they%20lack%20generalizability%20to%20unseen%20objects%2C%20which%0Asignificantly%20impedes%20their%20application%20in%20broader%20scenarios.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20framework%20of%20Generalizable%20Articulation%20Modeling%20and%0AManipulating%20for%20Articulated%20Objects%20%28GAMMA%29%2C%20which%20learns%20both%20articulation%0Amodeling%20and%20grasp%20pose%20affordance%20from%20diverse%20articulated%20objects%20with%0Adifferent%20categories.%20In%20addition%2C%20GAMMA%20adopts%20adaptive%20manipulation%20to%0Aiteratively%20reduce%20the%20modeling%20errors%20and%20enhance%20manipulation%20performance.%20We%0Atrain%20GAMMA%20with%20the%20PartNet-Mobility%20dataset%20and%20evaluate%20with%20comprehensive%0Aexperiments%20in%20SAPIEN%20simulation%20and%20real-world%20Franka%20robot.%20Results%20show%20that%0AGAMMA%20significantly%20outperforms%20SOTA%20articulation%20modeling%20and%20manipulation%0Aalgorithms%20in%20unseen%20and%20cross-category%20articulated%20objects.%20We%20will%0Aopen-source%20all%20codes%20and%20datasets%20in%20both%20simulation%20and%20real%20robots%20for%0Areproduction%20in%20the%20final%20version.%20Images%20and%20videos%20are%20published%20on%20the%0Aproject%20website%20at%3A%20http%3A//sites.google.com/view/gamma-articulation%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16264v3&entry.124074799=Read"},
{"title": "Feature Alignment: Rethinking Efficient Active Learning via Proxy in the\n  Context of Pre-trained Models", "author": "Ziting Wen and Oscar Pizarro and Stefan Williams", "abstract": "  Fine-tuning the pre-trained model with active learning holds promise for\nreducing annotation costs. However, this combination introduces significant\ncomputational costs, particularly with the growing scale of pre-trained models.\nRecent research has proposed proxy-based active learning, which pre-computes\nfeatures to reduce computational costs. Yet, this approach often incurs a\nsignificant loss in active learning performance, which may even outweigh the\ncomputational cost savings. In this paper, we argue the performance drop stems\nnot only from pre-computed features' inability to distinguish between\ncategories of labeled samples, resulting in the selection of redundant samples\nbut also from the tendency to compromise valuable pre-trained information when\nfine-tuning with samples selected through the proxy model. To address this\nissue, we propose a novel method called aligned selection via proxy to update\npre-computed features while selecting a proper training method to inherit\nvaluable pre-training information. Extensive experiments validate that our\nmethod significantly improves the total cost of efficient active learning while\nmaintaining computational efficiency.\n", "link": "http://arxiv.org/abs/2403.01101v1", "date": "2024-03-02", "relevancy": 2.3969, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4821}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4769}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Alignment%3A%20Rethinking%20Efficient%20Active%20Learning%20via%20Proxy%20in%20the%0A%20%20Context%20of%20Pre-trained%20Models&entry.906535625=Ziting%20Wen%20and%20Oscar%20Pizarro%20and%20Stefan%20Williams&entry.1292438233=%20%20Fine-tuning%20the%20pre-trained%20model%20with%20active%20learning%20holds%20promise%20for%0Areducing%20annotation%20costs.%20However%2C%20this%20combination%20introduces%20significant%0Acomputational%20costs%2C%20particularly%20with%20the%20growing%20scale%20of%20pre-trained%20models.%0ARecent%20research%20has%20proposed%20proxy-based%20active%20learning%2C%20which%20pre-computes%0Afeatures%20to%20reduce%20computational%20costs.%20Yet%2C%20this%20approach%20often%20incurs%20a%0Asignificant%20loss%20in%20active%20learning%20performance%2C%20which%20may%20even%20outweigh%20the%0Acomputational%20cost%20savings.%20In%20this%20paper%2C%20we%20argue%20the%20performance%20drop%20stems%0Anot%20only%20from%20pre-computed%20features%27%20inability%20to%20distinguish%20between%0Acategories%20of%20labeled%20samples%2C%20resulting%20in%20the%20selection%20of%20redundant%20samples%0Abut%20also%20from%20the%20tendency%20to%20compromise%20valuable%20pre-trained%20information%20when%0Afine-tuning%20with%20samples%20selected%20through%20the%20proxy%20model.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20novel%20method%20called%20aligned%20selection%20via%20proxy%20to%20update%0Apre-computed%20features%20while%20selecting%20a%20proper%20training%20method%20to%20inherit%0Avaluable%20pre-training%20information.%20Extensive%20experiments%20validate%20that%20our%0Amethod%20significantly%20improves%20the%20total%20cost%20of%20efficient%20active%20learning%20while%0Amaintaining%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01101v1&entry.124074799=Read"},
{"title": "Imagine, Initialize, and Explore: An Effective Exploration Method in\n  Multi-Agent Reinforcement Learning", "author": "Zeyang Liu and Lipeng Wan and Xinrui Yang and Zhuoran Chen and Xingyu Chen and Xuguang Lan", "abstract": "  Effective exploration is crucial to discovering optimal strategies for\nmulti-agent reinforcement learning (MARL) in complex coordination tasks.\nExisting methods mainly utilize intrinsic rewards to enable committed\nexploration or use role-based learning for decomposing joint action spaces\ninstead of directly conducting a collective search in the entire\naction-observation space. However, they often face challenges obtaining\nspecific joint action sequences to reach successful states in long-horizon\ntasks. To address this limitation, we propose Imagine, Initialize, and Explore\n(IIE), a novel method that offers a promising solution for efficient\nmulti-agent exploration in complex scenarios. IIE employs a transformer model\nto imagine how the agents reach a critical state that can influence each\nother's transition functions. Then, we initialize the environment at this state\nusing a simulator before the exploration phase. We formulate the imagination as\na sequence modeling problem, where the states, observations, prompts, actions,\nand rewards are predicted autoregressively. The prompt consists of\ntimestep-to-go, return-to-go, influence value, and one-shot demonstration,\nspecifying the desired state and trajectory as well as guiding the action\ngeneration. By initializing agents at the critical states, IIE significantly\nincreases the likelihood of discovering potentially important under-explored\nregions. Despite its simplicity, empirical results demonstrate that our method\noutperforms multi-agent exploration baselines on the StarCraft Multi-Agent\nChallenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved\nperformance in the sparse-reward SMAC tasks and produces more effective\ncurricula over the initialized states than other generative methods, such as\nCVAE-GAN and diffusion models.\n", "link": "http://arxiv.org/abs/2402.17978v2", "date": "2024-03-01", "relevancy": 2.3968, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6512}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6231}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5544}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imagine%2C%20Initialize%2C%20and%20Explore%3A%20An%20Effective%20Exploration%20Method%20in%0A%20%20Multi-Agent%20Reinforcement%20Learning&entry.906535625=Zeyang%20Liu%20and%20Lipeng%20Wan%20and%20Xinrui%20Yang%20and%20Zhuoran%20Chen%20and%20Xingyu%20Chen%20and%20Xuguang%20Lan&entry.1292438233=%20%20Effective%20exploration%20is%20crucial%20to%20discovering%20optimal%20strategies%20for%0Amulti-agent%20reinforcement%20learning%20%28MARL%29%20in%20complex%20coordination%20tasks.%0AExisting%20methods%20mainly%20utilize%20intrinsic%20rewards%20to%20enable%20committed%0Aexploration%20or%20use%20role-based%20learning%20for%20decomposing%20joint%20action%20spaces%0Ainstead%20of%20directly%20conducting%20a%20collective%20search%20in%20the%20entire%0Aaction-observation%20space.%20However%2C%20they%20often%20face%20challenges%20obtaining%0Aspecific%20joint%20action%20sequences%20to%20reach%20successful%20states%20in%20long-horizon%0Atasks.%20To%20address%20this%20limitation%2C%20we%20propose%20Imagine%2C%20Initialize%2C%20and%20Explore%0A%28IIE%29%2C%20a%20novel%20method%20that%20offers%20a%20promising%20solution%20for%20efficient%0Amulti-agent%20exploration%20in%20complex%20scenarios.%20IIE%20employs%20a%20transformer%20model%0Ato%20imagine%20how%20the%20agents%20reach%20a%20critical%20state%20that%20can%20influence%20each%0Aother%27s%20transition%20functions.%20Then%2C%20we%20initialize%20the%20environment%20at%20this%20state%0Ausing%20a%20simulator%20before%20the%20exploration%20phase.%20We%20formulate%20the%20imagination%20as%0Aa%20sequence%20modeling%20problem%2C%20where%20the%20states%2C%20observations%2C%20prompts%2C%20actions%2C%0Aand%20rewards%20are%20predicted%20autoregressively.%20The%20prompt%20consists%20of%0Atimestep-to-go%2C%20return-to-go%2C%20influence%20value%2C%20and%20one-shot%20demonstration%2C%0Aspecifying%20the%20desired%20state%20and%20trajectory%20as%20well%20as%20guiding%20the%20action%0Ageneration.%20By%20initializing%20agents%20at%20the%20critical%20states%2C%20IIE%20significantly%0Aincreases%20the%20likelihood%20of%20discovering%20potentially%20important%20under-explored%0Aregions.%20Despite%20its%20simplicity%2C%20empirical%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20multi-agent%20exploration%20baselines%20on%20the%20StarCraft%20Multi-Agent%0AChallenge%20%28SMAC%29%20and%20SMACv2%20environments.%20Particularly%2C%20IIE%20shows%20improved%0Aperformance%20in%20the%20sparse-reward%20SMAC%20tasks%20and%20produces%20more%20effective%0Acurricula%20over%20the%20initialized%20states%20than%20other%20generative%20methods%2C%20such%20as%0ACVAE-GAN%20and%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17978v2&entry.124074799=Read"},
{"title": "Improving Android Malware Detection Through Data Augmentation Using\n  Wasserstein Generative Adversarial Networks", "author": "Kawana Stalin and Mikias Berhanu Mekoya", "abstract": "  Generative Adversarial Networks (GANs) have demonstrated their versatility\nacross various applications, including data augmentation and malware detection.\nThis research explores the effectiveness of utilizing GAN-generated data to\ntrain a model for the detection of Android malware. Given the considerable\nstorage requirements of Android applications, the study proposes a method to\nsynthetically represent data using GANs, thereby reducing storage demands. The\nproposed methodology involves creating image representations of features\nextracted from an existing dataset. A GAN model is then employed to generate a\nmore extensive dataset consisting of realistic synthetic grayscale images.\nSubsequently, this synthetic dataset is utilized to train a Convolutional\nNeural Network (CNN) designed to identify previously unseen Android malware\napplications. The study includes a comparative analysis of the CNN's\nperformance when trained on real images versus synthetic images generated by\nthe GAN. Furthermore, the research explores variations in performance between\nthe Wasserstein Generative Adversarial Network (WGAN) and the Deep\nConvolutional Generative Adversarial Network (DCGAN). The investigation extends\nto studying the impact of image size and malware obfuscation on the\nclassification model's effectiveness. The data augmentation approach\nimplemented in this study resulted in a notable performance enhancement of the\nclassification model, ranging from 1.5% to 7%, depending on the dataset. The\nhighest achieved F1 score reached 0.975.\n  Keywords--Generative Adversarial Networks, Android Malware, Data\nAugmentation, Wasserstein Generative Adversarial Network\n", "link": "http://arxiv.org/abs/2403.00890v2", "date": "2024-03-05", "relevancy": 2.3881, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4922}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4777}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.463}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Android%20Malware%20Detection%20Through%20Data%20Augmentation%20Using%0A%20%20Wasserstein%20Generative%20Adversarial%20Networks&entry.906535625=Kawana%20Stalin%20and%20Mikias%20Berhanu%20Mekoya&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20demonstrated%20their%20versatility%0Aacross%20various%20applications%2C%20including%20data%20augmentation%20and%20malware%20detection.%0AThis%20research%20explores%20the%20effectiveness%20of%20utilizing%20GAN-generated%20data%20to%0Atrain%20a%20model%20for%20the%20detection%20of%20Android%20malware.%20Given%20the%20considerable%0Astorage%20requirements%20of%20Android%20applications%2C%20the%20study%20proposes%20a%20method%20to%0Asynthetically%20represent%20data%20using%20GANs%2C%20thereby%20reducing%20storage%20demands.%20The%0Aproposed%20methodology%20involves%20creating%20image%20representations%20of%20features%0Aextracted%20from%20an%20existing%20dataset.%20A%20GAN%20model%20is%20then%20employed%20to%20generate%20a%0Amore%20extensive%20dataset%20consisting%20of%20realistic%20synthetic%20grayscale%20images.%0ASubsequently%2C%20this%20synthetic%20dataset%20is%20utilized%20to%20train%20a%20Convolutional%0ANeural%20Network%20%28CNN%29%20designed%20to%20identify%20previously%20unseen%20Android%20malware%0Aapplications.%20The%20study%20includes%20a%20comparative%20analysis%20of%20the%20CNN%27s%0Aperformance%20when%20trained%20on%20real%20images%20versus%20synthetic%20images%20generated%20by%0Athe%20GAN.%20Furthermore%2C%20the%20research%20explores%20variations%20in%20performance%20between%0Athe%20Wasserstein%20Generative%20Adversarial%20Network%20%28WGAN%29%20and%20the%20Deep%0AConvolutional%20Generative%20Adversarial%20Network%20%28DCGAN%29.%20The%20investigation%20extends%0Ato%20studying%20the%20impact%20of%20image%20size%20and%20malware%20obfuscation%20on%20the%0Aclassification%20model%27s%20effectiveness.%20The%20data%20augmentation%20approach%0Aimplemented%20in%20this%20study%20resulted%20in%20a%20notable%20performance%20enhancement%20of%20the%0Aclassification%20model%2C%20ranging%20from%201.5%25%20to%207%25%2C%20depending%20on%20the%20dataset.%20The%0Ahighest%20achieved%20F1%20score%20reached%200.975.%0A%20%20Keywords--Generative%20Adversarial%20Networks%2C%20Android%20Malware%2C%20Data%0AAugmentation%2C%20Wasserstein%20Generative%20Adversarial%20Network%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00890v2&entry.124074799=Read"},
{"title": "ARIA: On the Interaction Between Architectures, Initialization and\n  Aggregation Methods for Federated Visual Classification", "author": "Vasilis Siomos and Sergio Naval-Marimont and Jonathan Passerat-Palmbach and Giacomo Tarroni", "abstract": "  Federated Learning (FL) is a collaborative training paradigm that allows for\nprivacy-preserving learning of cross-institutional models by eliminating the\nexchange of sensitive data and instead relying on the exchange of model\nparameters between the clients and a server. Despite individual studies on how\nclient models are aggregated, and, more recently, on the benefits of ImageNet\npre-training, there is a lack of understanding of the effect the architecture\nchosen for the federation has, and of how the aforementioned elements\ninterconnect. To this end, we conduct the first joint\nARchitecture-Initialization-Aggregation study and benchmark ARIAs across a\nrange of medical image classification tasks. We find that, contrary to current\npractices, ARIA elements have to be chosen together to achieve the best\npossible performance. Our results also shed light on good choices for each\nelement depending on the task, the effect of normalisation layers, and the\nutility of SSL pre-training, pointing to potential directions for designing\nFL-specific architectures and training pipelines.\n", "link": "http://arxiv.org/abs/2311.14625v2", "date": "2024-03-01", "relevancy": 2.3789, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4887}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.477}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4616}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARIA%3A%20On%20the%20Interaction%20Between%20Architectures%2C%20Initialization%20and%0A%20%20Aggregation%20Methods%20for%20Federated%20Visual%20Classification&entry.906535625=Vasilis%20Siomos%20and%20Sergio%20Naval-Marimont%20and%20Jonathan%20Passerat-Palmbach%20and%20Giacomo%20Tarroni&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20collaborative%20training%20paradigm%20that%20allows%20for%0Aprivacy-preserving%20learning%20of%20cross-institutional%20models%20by%20eliminating%20the%0Aexchange%20of%20sensitive%20data%20and%20instead%20relying%20on%20the%20exchange%20of%20model%0Aparameters%20between%20the%20clients%20and%20a%20server.%20Despite%20individual%20studies%20on%20how%0Aclient%20models%20are%20aggregated%2C%20and%2C%20more%20recently%2C%20on%20the%20benefits%20of%20ImageNet%0Apre-training%2C%20there%20is%20a%20lack%20of%20understanding%20of%20the%20effect%20the%20architecture%0Achosen%20for%20the%20federation%20has%2C%20and%20of%20how%20the%20aforementioned%20elements%0Ainterconnect.%20To%20this%20end%2C%20we%20conduct%20the%20first%20joint%0AARchitecture-Initialization-Aggregation%20study%20and%20benchmark%20ARIAs%20across%20a%0Arange%20of%20medical%20image%20classification%20tasks.%20We%20find%20that%2C%20contrary%20to%20current%0Apractices%2C%20ARIA%20elements%20have%20to%20be%20chosen%20together%20to%20achieve%20the%20best%0Apossible%20performance.%20Our%20results%20also%20shed%20light%20on%20good%20choices%20for%20each%0Aelement%20depending%20on%20the%20task%2C%20the%20effect%20of%20normalisation%20layers%2C%20and%20the%0Autility%20of%20SSL%20pre-training%2C%20pointing%20to%20potential%20directions%20for%20designing%0AFL-specific%20architectures%20and%20training%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14625v2&entry.124074799=Read"},
{"title": "Approximately optimal domain adaptation with Fisher's Linear\n  Discriminant", "author": "Hayden S. Helm and Ashwin De Silva and Joshua T. Vogelstein and Carey E. Priebe and Weiwei Yang", "abstract": "  We propose a class of models based on Fisher's Linear Discriminant (FLD) in\nthe context of domain adaptation. The class is the convex combination of two\nhypotheses: i) an average hypothesis representing previously seen source tasks\nand ii) a hypothesis trained on a new target task. For a particular generative\nsetting we derive the optimal convex combination of the two models under 0-1\nloss, propose a computable approximation, and study the effect of various\nparameter settings on the relative risks between the optimal hypothesis,\nhypothesis i), and hypothesis ii). We demonstrate the effectiveness of the\nproposed optimal classifier in the context of EEG- and ECG-based classification\nsettings and argue that the optimal classifier can be computed without access\nto direct information from any of the individual source tasks. We conclude by\ndiscussing further applications, limitations, and possible future directions.\n", "link": "http://arxiv.org/abs/2302.14186v3", "date": "2024-03-01", "relevancy": 2.3778, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4999}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4747}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4521}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximately%20optimal%20domain%20adaptation%20with%20Fisher%27s%20Linear%0A%20%20Discriminant&entry.906535625=Hayden%20S.%20Helm%20and%20Ashwin%20De%20Silva%20and%20Joshua%20T.%20Vogelstein%20and%20Carey%20E.%20Priebe%20and%20Weiwei%20Yang&entry.1292438233=%20%20We%20propose%20a%20class%20of%20models%20based%20on%20Fisher%27s%20Linear%20Discriminant%20%28FLD%29%20in%0Athe%20context%20of%20domain%20adaptation.%20The%20class%20is%20the%20convex%20combination%20of%20two%0Ahypotheses%3A%20i%29%20an%20average%20hypothesis%20representing%20previously%20seen%20source%20tasks%0Aand%20ii%29%20a%20hypothesis%20trained%20on%20a%20new%20target%20task.%20For%20a%20particular%20generative%0Asetting%20we%20derive%20the%20optimal%20convex%20combination%20of%20the%20two%20models%20under%200-1%0Aloss%2C%20propose%20a%20computable%20approximation%2C%20and%20study%20the%20effect%20of%20various%0Aparameter%20settings%20on%20the%20relative%20risks%20between%20the%20optimal%20hypothesis%2C%0Ahypothesis%20i%29%2C%20and%20hypothesis%20ii%29.%20We%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20optimal%20classifier%20in%20the%20context%20of%20EEG-%20and%20ECG-based%20classification%0Asettings%20and%20argue%20that%20the%20optimal%20classifier%20can%20be%20computed%20without%20access%0Ato%20direct%20information%20from%20any%20of%20the%20individual%20source%20tasks.%20We%20conclude%20by%0Adiscussing%20further%20applications%2C%20limitations%2C%20and%20possible%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.14186v3&entry.124074799=Read"},
{"title": "Imitation Learning Datasets: A Toolkit For Creating Datasets, Training\n  Agents and Benchmarking", "author": "Nathan Gavenski and Michael Luck and Odinaldo Rodrigues", "abstract": "  Imitation learning field requires expert data to train agents in a task. Most\noften, this learning approach suffers from the absence of available data, which\nresults in techniques being tested on its dataset. Creating datasets is a\ncumbersome process requiring researchers to train expert agents from scratch,\nrecord their interactions and test each benchmark method with newly created\ndata. Moreover, creating new datasets for each new technique results in a lack\nof consistency in the evaluation process since each dataset can drastically\nvary in state and action distribution. In response, this work aims to address\nthese issues by creating Imitation Learning Datasets, a toolkit that allows\nfor: (i) curated expert policies with multithreaded support for faster dataset\ncreation; (ii) readily available datasets and techniques with precise\nmeasurements; and (iii) sharing implementations of common imitation learning\ntechniques. Demonstration link:\nhttps://nathangavenski.github.io/#/il-datasets-video\n", "link": "http://arxiv.org/abs/2403.00550v1", "date": "2024-03-01", "relevancy": 2.3707, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4838}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4792}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4594}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20Learning%20Datasets%3A%20A%20Toolkit%20For%20Creating%20Datasets%2C%20Training%0A%20%20Agents%20and%20Benchmarking&entry.906535625=Nathan%20Gavenski%20and%20Michael%20Luck%20and%20Odinaldo%20Rodrigues&entry.1292438233=%20%20Imitation%20learning%20field%20requires%20expert%20data%20to%20train%20agents%20in%20a%20task.%20Most%0Aoften%2C%20this%20learning%20approach%20suffers%20from%20the%20absence%20of%20available%20data%2C%20which%0Aresults%20in%20techniques%20being%20tested%20on%20its%20dataset.%20Creating%20datasets%20is%20a%0Acumbersome%20process%20requiring%20researchers%20to%20train%20expert%20agents%20from%20scratch%2C%0Arecord%20their%20interactions%20and%20test%20each%20benchmark%20method%20with%20newly%20created%0Adata.%20Moreover%2C%20creating%20new%20datasets%20for%20each%20new%20technique%20results%20in%20a%20lack%0Aof%20consistency%20in%20the%20evaluation%20process%20since%20each%20dataset%20can%20drastically%0Avary%20in%20state%20and%20action%20distribution.%20In%20response%2C%20this%20work%20aims%20to%20address%0Athese%20issues%20by%20creating%20Imitation%20Learning%20Datasets%2C%20a%20toolkit%20that%20allows%0Afor%3A%20%28i%29%20curated%20expert%20policies%20with%20multithreaded%20support%20for%20faster%20dataset%0Acreation%3B%20%28ii%29%20readily%20available%20datasets%20and%20techniques%20with%20precise%0Ameasurements%3B%20and%20%28iii%29%20sharing%20implementations%20of%20common%20imitation%20learning%0Atechniques.%20Demonstration%20link%3A%0Ahttps%3A//nathangavenski.github.io/%23/il-datasets-video%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00550v1&entry.124074799=Read"},
{"title": "Optimal Budgeted Rejection Sampling for Generative Models", "author": "Alexandre Verine and Muni Sreenivas Pydi and Benjamin Negrevergne and Yann Chevaleyre", "abstract": "  Rejection sampling methods have recently been proposed to improve the\nperformance of discriminator-based generative models. However, these methods\nare only optimal under an unlimited sampling budget, and are usually applied to\na generator trained independently of the rejection procedure. We first propose\nan Optimal Budgeted Rejection Sampling (OBRS) scheme that is provably optimal\nwith respect to \\textit{any} $f$-divergence between the true distribution and\nthe post-rejection distribution, for a given sampling budget. Second, we\npropose an end-to-end method that incorporates the sampling scheme into the\ntraining procedure to further enhance the model's overall performance. Through\nexperiments and supporting theory, we show that the proposed methods are\neffective in significantly improving the quality and diversity of the samples.\n", "link": "http://arxiv.org/abs/2311.00460v2", "date": "2024-03-01", "relevancy": 2.3638, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4879}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4729}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4574}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Budgeted%20Rejection%20Sampling%20for%20Generative%20Models&entry.906535625=Alexandre%20Verine%20and%20Muni%20Sreenivas%20Pydi%20and%20Benjamin%20Negrevergne%20and%20Yann%20Chevaleyre&entry.1292438233=%20%20Rejection%20sampling%20methods%20have%20recently%20been%20proposed%20to%20improve%20the%0Aperformance%20of%20discriminator-based%20generative%20models.%20However%2C%20these%20methods%0Aare%20only%20optimal%20under%20an%20unlimited%20sampling%20budget%2C%20and%20are%20usually%20applied%20to%0Aa%20generator%20trained%20independently%20of%20the%20rejection%20procedure.%20We%20first%20propose%0Aan%20Optimal%20Budgeted%20Rejection%20Sampling%20%28OBRS%29%20scheme%20that%20is%20provably%20optimal%0Awith%20respect%20to%20%5Ctextit%7Bany%7D%20%24f%24-divergence%20between%20the%20true%20distribution%20and%0Athe%20post-rejection%20distribution%2C%20for%20a%20given%20sampling%20budget.%20Second%2C%20we%0Apropose%20an%20end-to-end%20method%20that%20incorporates%20the%20sampling%20scheme%20into%20the%0Atraining%20procedure%20to%20further%20enhance%20the%20model%27s%20overall%20performance.%20Through%0Aexperiments%20and%20supporting%20theory%2C%20we%20show%20that%20the%20proposed%20methods%20are%0Aeffective%20in%20significantly%20improving%20the%20quality%20and%20diversity%20of%20the%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00460v2&entry.124074799=Read"},
{"title": "LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network", "author": "Dongqiangzi Ye and Yufei Xie and Weijia Chen and Zixiang Zhou and Lingting Ge and Hassan Foroosh", "abstract": "  Due to the difficulty of acquiring large-scale 3D human keypoint annotation,\nprevious methods for 3D human pose estimation (HPE) have often relied on 2D\nimage features and sequential 2D annotations. Furthermore, the training of\nthese networks typically assumes the prediction of a human bounding box and the\naccurate alignment of 3D point clouds with 2D images, making direct application\nin real-world scenarios challenging. In this paper, we present the 1st\nframework for end-to-end 3D human pose estimation, named LPFormer, which uses\nonly LiDAR as its input along with its corresponding 3D annotations. LPFormer\nconsists of two stages: firstly, it identifies the human bounding box and\nextracts multi-level feature representations, and secondly, it utilizes a\ntransformer-based network to predict human keypoints based on these features.\nOur method demonstrates that 3D HPE can be seamlessly integrated into a strong\nLiDAR perception network and benefit from the features extracted by the\nnetwork. Experimental results on the Waymo Open Dataset demonstrate the\nstate-of-the-art performance, and improvements even compared to previous\nmulti-modal solutions.\n", "link": "http://arxiv.org/abs/2306.12525v2", "date": "2024-03-02", "relevancy": 2.36, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6333}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5747}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5529}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LPFormer%3A%20LiDAR%20Pose%20Estimation%20Transformer%20with%20Multi-Task%20Network&entry.906535625=Dongqiangzi%20Ye%20and%20Yufei%20Xie%20and%20Weijia%20Chen%20and%20Zixiang%20Zhou%20and%20Lingting%20Ge%20and%20Hassan%20Foroosh&entry.1292438233=%20%20Due%20to%20the%20difficulty%20of%20acquiring%20large-scale%203D%20human%20keypoint%20annotation%2C%0Aprevious%20methods%20for%203D%20human%20pose%20estimation%20%28HPE%29%20have%20often%20relied%20on%202D%0Aimage%20features%20and%20sequential%202D%20annotations.%20Furthermore%2C%20the%20training%20of%0Athese%20networks%20typically%20assumes%20the%20prediction%20of%20a%20human%20bounding%20box%20and%20the%0Aaccurate%20alignment%20of%203D%20point%20clouds%20with%202D%20images%2C%20making%20direct%20application%0Ain%20real-world%20scenarios%20challenging.%20In%20this%20paper%2C%20we%20present%20the%201st%0Aframework%20for%20end-to-end%203D%20human%20pose%20estimation%2C%20named%20LPFormer%2C%20which%20uses%0Aonly%20LiDAR%20as%20its%20input%20along%20with%20its%20corresponding%203D%20annotations.%20LPFormer%0Aconsists%20of%20two%20stages%3A%20firstly%2C%20it%20identifies%20the%20human%20bounding%20box%20and%0Aextracts%20multi-level%20feature%20representations%2C%20and%20secondly%2C%20it%20utilizes%20a%0Atransformer-based%20network%20to%20predict%20human%20keypoints%20based%20on%20these%20features.%0AOur%20method%20demonstrates%20that%203D%20HPE%20can%20be%20seamlessly%20integrated%20into%20a%20strong%0ALiDAR%20perception%20network%20and%20benefit%20from%20the%20features%20extracted%20by%20the%0Anetwork.%20Experimental%20results%20on%20the%20Waymo%20Open%20Dataset%20demonstrate%20the%0Astate-of-the-art%20performance%2C%20and%20improvements%20even%20compared%20to%20previous%0Amulti-modal%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.12525v2&entry.124074799=Read"},
{"title": "Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level\n  Tasks", "author": "Zhili Wang and Shimin Di and Lei Chen and Xiaofang Zhou", "abstract": "  Recently, graph neural networks (GNNs) have shown its unprecedented success\nin many graph-related tasks. However, GNNs face the label scarcity issue as\nother neural networks do. Thus, recent efforts try to pre-train GNNs on a\nlarge-scale unlabeled graph and adapt the knowledge from the unlabeled graph to\nthe target downstream task. The adaptation is generally achieved by fine-tuning\nthe pre-trained GNNs with a limited number of labeled data. Despite the\nimportance of fine-tuning, current GNNs pre-training works often ignore\ndesigning a good fine-tuning strategy to better leverage transferred knowledge\nand improve the performance on downstream tasks. Only few works start to\ninvestigate a better fine-tuning strategy for pre-trained GNNs. But their\ndesigns either have strong assumptions or overlook the data-aware issue for\nvarious downstream datasets. Therefore, we aim to design a better fine-tuning\nstrategy for pre-trained GNNs to improve the model performance in this paper.\nGiven a pre-trained GNN, we propose to search to fine-tune pre-trained graph\nneural networks for graph-level tasks (S2PGNN), which adaptively design a\nsuitable fine-tuning framework for the given labeled data on the downstream\ntask. To ensure the improvement brought by searching fine-tuning strategy, we\ncarefully summarize a proper search space of fine-tuning framework that is\nsuitable for GNNs. The empirical studies show that S2PGNN can be implemented on\nthe top of 10 famous pre-trained GNNs and consistently improve their\nperformance. Besides, S2PGNN achieves better performance than existing\nfine-tuning strategies within and outside the GNN area. Our code is publicly\navailable at \\url{https://anonymous.4open.science/r/code_icde2024-A9CB/}.\n", "link": "http://arxiv.org/abs/2308.06960v2", "date": "2024-03-01", "relevancy": 2.349, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4823}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4751}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.452}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search%20to%20Fine-tune%20Pre-trained%20Graph%20Neural%20Networks%20for%20Graph-level%0A%20%20Tasks&entry.906535625=Zhili%20Wang%20and%20Shimin%20Di%20and%20Lei%20Chen%20and%20Xiaofang%20Zhou&entry.1292438233=%20%20Recently%2C%20graph%20neural%20networks%20%28GNNs%29%20have%20shown%20its%20unprecedented%20success%0Ain%20many%20graph-related%20tasks.%20However%2C%20GNNs%20face%20the%20label%20scarcity%20issue%20as%0Aother%20neural%20networks%20do.%20Thus%2C%20recent%20efforts%20try%20to%20pre-train%20GNNs%20on%20a%0Alarge-scale%20unlabeled%20graph%20and%20adapt%20the%20knowledge%20from%20the%20unlabeled%20graph%20to%0Athe%20target%20downstream%20task.%20The%20adaptation%20is%20generally%20achieved%20by%20fine-tuning%0Athe%20pre-trained%20GNNs%20with%20a%20limited%20number%20of%20labeled%20data.%20Despite%20the%0Aimportance%20of%20fine-tuning%2C%20current%20GNNs%20pre-training%20works%20often%20ignore%0Adesigning%20a%20good%20fine-tuning%20strategy%20to%20better%20leverage%20transferred%20knowledge%0Aand%20improve%20the%20performance%20on%20downstream%20tasks.%20Only%20few%20works%20start%20to%0Ainvestigate%20a%20better%20fine-tuning%20strategy%20for%20pre-trained%20GNNs.%20But%20their%0Adesigns%20either%20have%20strong%20assumptions%20or%20overlook%20the%20data-aware%20issue%20for%0Avarious%20downstream%20datasets.%20Therefore%2C%20we%20aim%20to%20design%20a%20better%20fine-tuning%0Astrategy%20for%20pre-trained%20GNNs%20to%20improve%20the%20model%20performance%20in%20this%20paper.%0AGiven%20a%20pre-trained%20GNN%2C%20we%20propose%20to%20search%20to%20fine-tune%20pre-trained%20graph%0Aneural%20networks%20for%20graph-level%20tasks%20%28S2PGNN%29%2C%20which%20adaptively%20design%20a%0Asuitable%20fine-tuning%20framework%20for%20the%20given%20labeled%20data%20on%20the%20downstream%0Atask.%20To%20ensure%20the%20improvement%20brought%20by%20searching%20fine-tuning%20strategy%2C%20we%0Acarefully%20summarize%20a%20proper%20search%20space%20of%20fine-tuning%20framework%20that%20is%0Asuitable%20for%20GNNs.%20The%20empirical%20studies%20show%20that%20S2PGNN%20can%20be%20implemented%20on%0Athe%20top%20of%2010%20famous%20pre-trained%20GNNs%20and%20consistently%20improve%20their%0Aperformance.%20Besides%2C%20S2PGNN%20achieves%20better%20performance%20than%20existing%0Afine-tuning%20strategies%20within%20and%20outside%20the%20GNN%20area.%20Our%20code%20is%20publicly%0Aavailable%20at%20%5Curl%7Bhttps%3A//anonymous.4open.science/r/code_icde2024-A9CB/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.06960v2&entry.124074799=Read"},
{"title": "Making Images Real Again: A Comprehensive Survey on Deep Image\n  Composition", "author": "Li Niu and Wenyan Cong and Liu Liu and Yan Hong and Bo Zhang and Jing Liang and Liqing Zhang", "abstract": "  As a common image editing operation, image composition aims to combine the\nforeground from one image and another background image, resulting in a\ncomposite image. However, there are many issues that could make the composite\nimages unrealistic. These issues can be summarized as the inconsistency between\nforeground and background, which includes appearance inconsistency (e.g.,\nincompatible illumination), geometry inconsistency (e.g., unreasonable size),\nand semantic inconsistency (e.g., mismatched semantic context). Image\ncomposition task could be decomposed into multiple sub-tasks, in which each\nsub-task targets at one or more issues. Specifically, object placement aims to\nfind reasonable scale, location, and shape for the foreground. Image blending\naims to address the unnatural boundary between foreground and background. Image\nharmonization aims to adjust the illumination statistics of foreground. Shadow\ngeneration aims to generate plausible shadow for the foreground. These\nsub-tasks can be executed sequentially or parallelly to acquire realistic\ncomposite images. To the best of our knowledge, there is no previous survey on\nimage composition. In this paper, we conduct comprehensive survey over the\nsub-tasks and combinatorial task of image composition. For each one, we\nsummarize the existing methods, available datasets, and common evaluation\nmetrics. Datasets and codes for image composition are summarized at\nhttps://github.com/bcmi/Awesome-Image-Composition. We have also contributed the\nfirst image composition toolbox: libcom https://github.com/bcmi/libcom, which\nassembles 10+ image composition related functions (e.g., image blending, image\nharmonization, object placement, shadow generation, generative composition).\nThe ultimate goal of this toolbox is solving all the problems related to image\ncomposition with simple `import libcom'.\n", "link": "http://arxiv.org/abs/2106.14490v4", "date": "2024-03-02", "relevancy": 2.3489, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.49}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4688}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4505}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Making%20Images%20Real%20Again%3A%20A%20Comprehensive%20Survey%20on%20Deep%20Image%0A%20%20Composition&entry.906535625=Li%20Niu%20and%20Wenyan%20Cong%20and%20Liu%20Liu%20and%20Yan%20Hong%20and%20Bo%20Zhang%20and%20Jing%20Liang%20and%20Liqing%20Zhang&entry.1292438233=%20%20As%20a%20common%20image%20editing%20operation%2C%20image%20composition%20aims%20to%20combine%20the%0Aforeground%20from%20one%20image%20and%20another%20background%20image%2C%20resulting%20in%20a%0Acomposite%20image.%20However%2C%20there%20are%20many%20issues%20that%20could%20make%20the%20composite%0Aimages%20unrealistic.%20These%20issues%20can%20be%20summarized%20as%20the%20inconsistency%20between%0Aforeground%20and%20background%2C%20which%20includes%20appearance%20inconsistency%20%28e.g.%2C%0Aincompatible%20illumination%29%2C%20geometry%20inconsistency%20%28e.g.%2C%20unreasonable%20size%29%2C%0Aand%20semantic%20inconsistency%20%28e.g.%2C%20mismatched%20semantic%20context%29.%20Image%0Acomposition%20task%20could%20be%20decomposed%20into%20multiple%20sub-tasks%2C%20in%20which%20each%0Asub-task%20targets%20at%20one%20or%20more%20issues.%20Specifically%2C%20object%20placement%20aims%20to%0Afind%20reasonable%20scale%2C%20location%2C%20and%20shape%20for%20the%20foreground.%20Image%20blending%0Aaims%20to%20address%20the%20unnatural%20boundary%20between%20foreground%20and%20background.%20Image%0Aharmonization%20aims%20to%20adjust%20the%20illumination%20statistics%20of%20foreground.%20Shadow%0Ageneration%20aims%20to%20generate%20plausible%20shadow%20for%20the%20foreground.%20These%0Asub-tasks%20can%20be%20executed%20sequentially%20or%20parallelly%20to%20acquire%20realistic%0Acomposite%20images.%20To%20the%20best%20of%20our%20knowledge%2C%20there%20is%20no%20previous%20survey%20on%0Aimage%20composition.%20In%20this%20paper%2C%20we%20conduct%20comprehensive%20survey%20over%20the%0Asub-tasks%20and%20combinatorial%20task%20of%20image%20composition.%20For%20each%20one%2C%20we%0Asummarize%20the%20existing%20methods%2C%20available%20datasets%2C%20and%20common%20evaluation%0Ametrics.%20Datasets%20and%20codes%20for%20image%20composition%20are%20summarized%20at%0Ahttps%3A//github.com/bcmi/Awesome-Image-Composition.%20We%20have%20also%20contributed%20the%0Afirst%20image%20composition%20toolbox%3A%20libcom%20https%3A//github.com/bcmi/libcom%2C%20which%0Aassembles%2010%2B%20image%20composition%20related%20functions%20%28e.g.%2C%20image%20blending%2C%20image%0Aharmonization%2C%20object%20placement%2C%20shadow%20generation%2C%20generative%20composition%29.%0AThe%20ultimate%20goal%20of%20this%20toolbox%20is%20solving%20all%20the%20problems%20related%20to%20image%0Acomposition%20with%20simple%20%60import%20libcom%27.%0A&entry.1838667208=http%3A//arxiv.org/abs/2106.14490v4&entry.124074799=Read"},
{"title": "TCIG: Two-Stage Controlled Image Generation with Quality Enhancement\n  through Diffusion", "author": "Salaheldin Mohamed", "abstract": "  In recent years, significant progress has been made in the development of\ntext-to-image generation models. However, these models still face limitations\nwhen it comes to achieving full controllability during the generation process.\nOften, specific training or the use of limited models is required, and even\nthen, they have certain restrictions. To address these challenges, A two-stage\nmethod that effectively combines controllability and high quality in the\ngeneration of images is proposed. This approach leverages the expertise of\npre-trained models to achieve precise control over the generated images, while\nalso harnessing the power of diffusion models to achieve state-of-the-art\nquality. By separating controllability from high quality, This method achieves\noutstanding results. It is compatible with both latent and image space\ndiffusion models, ensuring versatility and flexibility. Moreover, This approach\nconsistently produces comparable outcomes to the current state-of-the-art\nmethods in the field. Overall, This proposed method represents a significant\nadvancement in text-to-image generation, enabling improved controllability\nwithout compromising on the quality of the generated images.\n", "link": "http://arxiv.org/abs/2403.01212v1", "date": "2024-03-02", "relevancy": 2.3407, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6032}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5728}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5711}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TCIG%3A%20Two-Stage%20Controlled%20Image%20Generation%20with%20Quality%20Enhancement%0A%20%20through%20Diffusion&entry.906535625=Salaheldin%20Mohamed&entry.1292438233=%20%20In%20recent%20years%2C%20significant%20progress%20has%20been%20made%20in%20the%20development%20of%0Atext-to-image%20generation%20models.%20However%2C%20these%20models%20still%20face%20limitations%0Awhen%20it%20comes%20to%20achieving%20full%20controllability%20during%20the%20generation%20process.%0AOften%2C%20specific%20training%20or%20the%20use%20of%20limited%20models%20is%20required%2C%20and%20even%0Athen%2C%20they%20have%20certain%20restrictions.%20To%20address%20these%20challenges%2C%20A%20two-stage%0Amethod%20that%20effectively%20combines%20controllability%20and%20high%20quality%20in%20the%0Ageneration%20of%20images%20is%20proposed.%20This%20approach%20leverages%20the%20expertise%20of%0Apre-trained%20models%20to%20achieve%20precise%20control%20over%20the%20generated%20images%2C%20while%0Aalso%20harnessing%20the%20power%20of%20diffusion%20models%20to%20achieve%20state-of-the-art%0Aquality.%20By%20separating%20controllability%20from%20high%20quality%2C%20This%20method%20achieves%0Aoutstanding%20results.%20It%20is%20compatible%20with%20both%20latent%20and%20image%20space%0Adiffusion%20models%2C%20ensuring%20versatility%20and%20flexibility.%20Moreover%2C%20This%20approach%0Aconsistently%20produces%20comparable%20outcomes%20to%20the%20current%20state-of-the-art%0Amethods%20in%20the%20field.%20Overall%2C%20This%20proposed%20method%20represents%20a%20significant%0Aadvancement%20in%20text-to-image%20generation%2C%20enabling%20improved%20controllability%0Awithout%20compromising%20on%20the%20quality%20of%20the%20generated%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01212v1&entry.124074799=Read"},
{"title": "Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning", "author": "Shuo Yang and Zirui Shang and Yongqi Wang and Derong Deng and Hongwei Chen and Qiyuan Cheng and Xinxiao Wu", "abstract": "  This paper proposes a novel framework for multi-label image recognition\nwithout any training data, called data-free framework, which uses knowledge of\npre-trained Large Language Model (LLM) to learn prompts to adapt pretrained\nVision-Language Model (VLM) like CLIP to multilabel classification. Through\nasking LLM by well-designed questions, we acquire comprehensive knowledge about\ncharacteristics and contexts of objects, which provides valuable text\ndescriptions for learning prompts. Then we propose a hierarchical prompt\nlearning method by taking the multi-label dependency into consideration,\nwherein a subset of category-specific prompt tokens are shared when the\ncorresponding objects exhibit similar attributes or are more likely to\nco-occur. Benefiting from the remarkable alignment between visual and\nlinguistic semantics of CLIP, the hierarchical prompts learned from text\ndescriptions are applied to perform classification of images during inference.\nOur framework presents a new way to explore the synergies between multiple\npre-trained models for novel category recognition. Extensive experiments on\nthree public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that our\nmethod achieves better results than the state-of-the-art methods, especially\noutperforming the zero-shot multi-label recognition methods by 4.7% in mAP on\nMS-COCO.\n", "link": "http://arxiv.org/abs/2403.01209v1", "date": "2024-03-02", "relevancy": 2.3403, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5599}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5553}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-free%20Multi-label%20Image%20Recognition%20via%20LLM-powered%20Prompt%20Tuning&entry.906535625=Shuo%20Yang%20and%20Zirui%20Shang%20and%20Yongqi%20Wang%20and%20Derong%20Deng%20and%20Hongwei%20Chen%20and%20Qiyuan%20Cheng%20and%20Xinxiao%20Wu&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20framework%20for%20multi-label%20image%20recognition%0Awithout%20any%20training%20data%2C%20called%20data-free%20framework%2C%20which%20uses%20knowledge%20of%0Apre-trained%20Large%20Language%20Model%20%28LLM%29%20to%20learn%20prompts%20to%20adapt%20pretrained%0AVision-Language%20Model%20%28VLM%29%20like%20CLIP%20to%20multilabel%20classification.%20Through%0Aasking%20LLM%20by%20well-designed%20questions%2C%20we%20acquire%20comprehensive%20knowledge%20about%0Acharacteristics%20and%20contexts%20of%20objects%2C%20which%20provides%20valuable%20text%0Adescriptions%20for%20learning%20prompts.%20Then%20we%20propose%20a%20hierarchical%20prompt%0Alearning%20method%20by%20taking%20the%20multi-label%20dependency%20into%20consideration%2C%0Awherein%20a%20subset%20of%20category-specific%20prompt%20tokens%20are%20shared%20when%20the%0Acorresponding%20objects%20exhibit%20similar%20attributes%20or%20are%20more%20likely%20to%0Aco-occur.%20Benefiting%20from%20the%20remarkable%20alignment%20between%20visual%20and%0Alinguistic%20semantics%20of%20CLIP%2C%20the%20hierarchical%20prompts%20learned%20from%20text%0Adescriptions%20are%20applied%20to%20perform%20classification%20of%20images%20during%20inference.%0AOur%20framework%20presents%20a%20new%20way%20to%20explore%20the%20synergies%20between%20multiple%0Apre-trained%20models%20for%20novel%20category%20recognition.%20Extensive%20experiments%20on%0Athree%20public%20datasets%20%28MS-COCO%2C%20VOC2007%2C%20and%20NUS-WIDE%29%20demonstrate%20that%20our%0Amethod%20achieves%20better%20results%20than%20the%20state-of-the-art%20methods%2C%20especially%0Aoutperforming%20the%20zero-shot%20multi-label%20recognition%20methods%20by%204.7%25%20in%20mAP%20on%0AMS-COCO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01209v1&entry.124074799=Read"},
{"title": "Mitigating Catastrophic Forgetting in Large Language Models with\n  Self-Synthesized Rehearsal", "author": "Jianheng Huang and Leyang Cui and Ante Wang and Chengyi Yang and Xinting Liao and Linfeng Song and Junfeng Yao and Jinsong Su", "abstract": "  Large language models (LLMs) suffer from catastrophic forgetting during\ncontinual learning. Conventional rehearsal-based methods rely on previous\ntraining data to retain the model's ability, which may not be feasible in\nreal-world applications. When conducting continual learning based on a\npublicly-released LLM checkpoint, the availability of the original training\ndata may be non-existent. To address this challenge, we propose a framework\ncalled Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic\ninstances for rehearsal. Concretely, we first employ the base LLM for\nin-context learning to generate synthetic instances. Subsequently, we utilize\nthe latest LLM to refine the instance outputs based on the synthetic inputs,\npreserving its acquired ability. Finally, we select diverse high-quality\nsynthetic instances for rehearsal in future stages. Experimental results\ndemonstrate that SSR achieves superior or comparable performance compared to\nconventional rehearsal-based approaches while being more data-efficient.\nBesides, SSR effectively preserves the generalization capabilities of LLMs in\ngeneral domains.\n", "link": "http://arxiv.org/abs/2403.01244v1", "date": "2024-03-02", "relevancy": 2.3397, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4788}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4679}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4572}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Catastrophic%20Forgetting%20in%20Large%20Language%20Models%20with%0A%20%20Self-Synthesized%20Rehearsal&entry.906535625=Jianheng%20Huang%20and%20Leyang%20Cui%20and%20Ante%20Wang%20and%20Chengyi%20Yang%20and%20Xinting%20Liao%20and%20Linfeng%20Song%20and%20Junfeng%20Yao%20and%20Jinsong%20Su&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20suffer%20from%20catastrophic%20forgetting%20during%0Acontinual%20learning.%20Conventional%20rehearsal-based%20methods%20rely%20on%20previous%0Atraining%20data%20to%20retain%20the%20model%27s%20ability%2C%20which%20may%20not%20be%20feasible%20in%0Areal-world%20applications.%20When%20conducting%20continual%20learning%20based%20on%20a%0Apublicly-released%20LLM%20checkpoint%2C%20the%20availability%20of%20the%20original%20training%0Adata%20may%20be%20non-existent.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20framework%0Acalled%20Self-Synthesized%20Rehearsal%20%28SSR%29%20that%20uses%20the%20LLM%20to%20generate%20synthetic%0Ainstances%20for%20rehearsal.%20Concretely%2C%20we%20first%20employ%20the%20base%20LLM%20for%0Ain-context%20learning%20to%20generate%20synthetic%20instances.%20Subsequently%2C%20we%20utilize%0Athe%20latest%20LLM%20to%20refine%20the%20instance%20outputs%20based%20on%20the%20synthetic%20inputs%2C%0Apreserving%20its%20acquired%20ability.%20Finally%2C%20we%20select%20diverse%20high-quality%0Asynthetic%20instances%20for%20rehearsal%20in%20future%20stages.%20Experimental%20results%0Ademonstrate%20that%20SSR%20achieves%20superior%20or%20comparable%20performance%20compared%20to%0Aconventional%20rehearsal-based%20approaches%20while%20being%20more%20data-efficient.%0ABesides%2C%20SSR%20effectively%20preserves%20the%20generalization%20capabilities%20of%20LLMs%20in%0Ageneral%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01244v1&entry.124074799=Read"},
{"title": "Polynormer: Polynomial-Expressive Graph Transformer in Linear Time", "author": "Chenhui Deng and Zichao Yue and Zhiru Zhang", "abstract": "  Graph transformers (GTs) have emerged as a promising architecture that is\ntheoretically more expressive than message-passing graph neural networks\n(GNNs). However, typical GT models have at least quadratic complexity and thus\ncannot scale to large graphs. While there are several linear GTs recently\nproposed, they still lag behind GNN counterparts on several popular graph\ndatasets, which poses a critical concern on their practical expressivity. To\nbalance the trade-off between expressivity and scalability of GTs, we propose\nPolynormer, a polynomial-expressive GT model with linear complexity. Polynormer\nis built upon a novel base model that learns a high-degree polynomial on input\nfeatures. To enable the base model permutation equivariant, we integrate it\nwith graph topology and node features separately, resulting in local and global\nequivariant attention models. Consequently, Polynormer adopts a linear\nlocal-to-global attention scheme to learn high-degree equivariant polynomials\nwhose coefficients are controlled by attention scores. Polynormer has been\nevaluated on $13$ homophilic and heterophilic datasets, including large graphs\nwith millions of nodes. Our extensive experiment results show that Polynormer\noutperforms state-of-the-art GNN and GT baselines on most datasets, even\nwithout the use of nonlinear activation functions.\n", "link": "http://arxiv.org/abs/2403.01232v1", "date": "2024-03-02", "relevancy": 2.3394, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4754}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4688}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4594}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polynormer%3A%20Polynomial-Expressive%20Graph%20Transformer%20in%20Linear%20Time&entry.906535625=Chenhui%20Deng%20and%20Zichao%20Yue%20and%20Zhiru%20Zhang&entry.1292438233=%20%20Graph%20transformers%20%28GTs%29%20have%20emerged%20as%20a%20promising%20architecture%20that%20is%0Atheoretically%20more%20expressive%20than%20message-passing%20graph%20neural%20networks%0A%28GNNs%29.%20However%2C%20typical%20GT%20models%20have%20at%20least%20quadratic%20complexity%20and%20thus%0Acannot%20scale%20to%20large%20graphs.%20While%20there%20are%20several%20linear%20GTs%20recently%0Aproposed%2C%20they%20still%20lag%20behind%20GNN%20counterparts%20on%20several%20popular%20graph%0Adatasets%2C%20which%20poses%20a%20critical%20concern%20on%20their%20practical%20expressivity.%20To%0Abalance%20the%20trade-off%20between%20expressivity%20and%20scalability%20of%20GTs%2C%20we%20propose%0APolynormer%2C%20a%20polynomial-expressive%20GT%20model%20with%20linear%20complexity.%20Polynormer%0Ais%20built%20upon%20a%20novel%20base%20model%20that%20learns%20a%20high-degree%20polynomial%20on%20input%0Afeatures.%20To%20enable%20the%20base%20model%20permutation%20equivariant%2C%20we%20integrate%20it%0Awith%20graph%20topology%20and%20node%20features%20separately%2C%20resulting%20in%20local%20and%20global%0Aequivariant%20attention%20models.%20Consequently%2C%20Polynormer%20adopts%20a%20linear%0Alocal-to-global%20attention%20scheme%20to%20learn%20high-degree%20equivariant%20polynomials%0Awhose%20coefficients%20are%20controlled%20by%20attention%20scores.%20Polynormer%20has%20been%0Aevaluated%20on%20%2413%24%20homophilic%20and%20heterophilic%20datasets%2C%20including%20large%20graphs%0Awith%20millions%20of%20nodes.%20Our%20extensive%20experiment%20results%20show%20that%20Polynormer%0Aoutperforms%20state-of-the-art%20GNN%20and%20GT%20baselines%20on%20most%20datasets%2C%20even%0Awithout%20the%20use%20of%20nonlinear%20activation%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01232v1&entry.124074799=Read"},
{"title": "Federated Domain Generalization: A Survey", "author": "Ying Li and Xingwei Wang and Rongfei Zeng and Praveen Kumar Donta and Ilir Murturi and Min Huang and Schahram Dustdar", "abstract": "  Machine learning typically relies on the assumption that training and testing\ndistributions are identical and that data is centrally stored for training and\ntesting. However, in real-world scenarios, distributions may differ\nsignificantly and data is often distributed across different devices,\norganizations, or edge nodes. Consequently, it is imperative to develop models\nthat can effectively generalize to unseen distributions where data is\ndistributed across different domains. In response to this challenge, there has\nbeen a surge of interest in federated domain generalization (FDG) in recent\nyears. FDG combines the strengths of federated learning (FL) and domain\ngeneralization (DG) techniques to enable multiple source domains to\ncollaboratively learn a model capable of directly generalizing to unseen\ndomains while preserving data privacy. However, generalizing the federated\nmodel under domain shifts is a technically challenging problem that has\nreceived scant attention in the research area so far. This paper presents the\nfirst survey of recent advances in this area. Initially, we discuss the\ndevelopment process from traditional machine learning to domain adaptation and\ndomain generalization, leading to FDG as well as provide the corresponding\nformal definition. Then, we categorize recent methodologies into four classes:\nfederated domain alignment, data manipulation, learning strategies, and\naggregation optimization, and present suitable algorithms in detail for each\ncategory. Next, we introduce commonly used datasets, applications, evaluations,\nand benchmarks. Finally, we conclude this survey by providing some potential\nresearch topics for the future.\n", "link": "http://arxiv.org/abs/2306.01334v2", "date": "2024-03-01", "relevancy": 2.3369, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5047}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4512}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4462}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Domain%20Generalization%3A%20A%20Survey&entry.906535625=Ying%20Li%20and%20Xingwei%20Wang%20and%20Rongfei%20Zeng%20and%20Praveen%20Kumar%20Donta%20and%20Ilir%20Murturi%20and%20Min%20Huang%20and%20Schahram%20Dustdar&entry.1292438233=%20%20Machine%20learning%20typically%20relies%20on%20the%20assumption%20that%20training%20and%20testing%0Adistributions%20are%20identical%20and%20that%20data%20is%20centrally%20stored%20for%20training%20and%0Atesting.%20However%2C%20in%20real-world%20scenarios%2C%20distributions%20may%20differ%0Asignificantly%20and%20data%20is%20often%20distributed%20across%20different%20devices%2C%0Aorganizations%2C%20or%20edge%20nodes.%20Consequently%2C%20it%20is%20imperative%20to%20develop%20models%0Athat%20can%20effectively%20generalize%20to%20unseen%20distributions%20where%20data%20is%0Adistributed%20across%20different%20domains.%20In%20response%20to%20this%20challenge%2C%20there%20has%0Abeen%20a%20surge%20of%20interest%20in%20federated%20domain%20generalization%20%28FDG%29%20in%20recent%0Ayears.%20FDG%20combines%20the%20strengths%20of%20federated%20learning%20%28FL%29%20and%20domain%0Ageneralization%20%28DG%29%20techniques%20to%20enable%20multiple%20source%20domains%20to%0Acollaboratively%20learn%20a%20model%20capable%20of%20directly%20generalizing%20to%20unseen%0Adomains%20while%20preserving%20data%20privacy.%20However%2C%20generalizing%20the%20federated%0Amodel%20under%20domain%20shifts%20is%20a%20technically%20challenging%20problem%20that%20has%0Areceived%20scant%20attention%20in%20the%20research%20area%20so%20far.%20This%20paper%20presents%20the%0Afirst%20survey%20of%20recent%20advances%20in%20this%20area.%20Initially%2C%20we%20discuss%20the%0Adevelopment%20process%20from%20traditional%20machine%20learning%20to%20domain%20adaptation%20and%0Adomain%20generalization%2C%20leading%20to%20FDG%20as%20well%20as%20provide%20the%20corresponding%0Aformal%20definition.%20Then%2C%20we%20categorize%20recent%20methodologies%20into%20four%20classes%3A%0Afederated%20domain%20alignment%2C%20data%20manipulation%2C%20learning%20strategies%2C%20and%0Aaggregation%20optimization%2C%20and%20present%20suitable%20algorithms%20in%20detail%20for%20each%0Acategory.%20Next%2C%20we%20introduce%20commonly%20used%20datasets%2C%20applications%2C%20evaluations%2C%0Aand%20benchmarks.%20Finally%2C%20we%20conclude%20this%20survey%20by%20providing%20some%20potential%0Aresearch%20topics%20for%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.01334v2&entry.124074799=Read"},
{"title": "DoRA: Weight-Decomposed Low-Rank Adaptation", "author": "Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen", "abstract": "  Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and\nits variants have gained considerable popularity because of avoiding additional\ninference costs. However, there still often exists an accuracy gap between\nthese methods and full fine-tuning (FT). In this work, we first introduce a\nnovel weight decomposition analysis to investigate the inherent differences\nbetween FT and LoRA. Aiming to resemble the learning capacity of FT from the\nfindings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA\ndecomposes the pre-trained weight into two components, magnitude and direction,\nfor fine-tuning, specifically employing LoRA for directional updates to\nefficiently minimize the number of trainable parameters. By employing DoRA, we\nenhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. DoRA consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding.\n", "link": "http://arxiv.org/abs/2402.09353v3", "date": "2024-03-05", "relevancy": 2.3368, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4825}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4595}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DoRA%3A%20Weight-Decomposed%20Low-Rank%20Adaptation&entry.906535625=Shih-Yang%20Liu%20and%20Chien-Yi%20Wang%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%20and%20Yu-Chiang%20Frank%20Wang%20and%20Kwang-Ting%20Cheng%20and%20Min-Hung%20Chen&entry.1292438233=%20%20Among%20the%20widely%20used%20parameter-efficient%20finetuning%20%28PEFT%29%20methods%2C%20LoRA%20and%0Aits%20variants%20have%20gained%20considerable%20popularity%20because%20of%20avoiding%20additional%0Ainference%20costs.%20However%2C%20there%20still%20often%20exists%20an%20accuracy%20gap%20between%0Athese%20methods%20and%20full%20fine-tuning%20%28FT%29.%20In%20this%20work%2C%20we%20first%20introduce%20a%0Anovel%20weight%20decomposition%20analysis%20to%20investigate%20the%20inherent%20differences%0Abetween%20FT%20and%20LoRA.%20Aiming%20to%20resemble%20the%20learning%20capacity%20of%20FT%20from%20the%0Afindings%2C%20we%20propose%20Weight-Decomposed%20LowRank%20Adaptation%20%28DoRA%29.%20DoRA%0Adecomposes%20the%20pre-trained%20weight%20into%20two%20components%2C%20magnitude%20and%20direction%2C%0Afor%20fine-tuning%2C%20specifically%20employing%20LoRA%20for%20directional%20updates%20to%0Aefficiently%20minimize%20the%20number%20of%20trainable%20parameters.%20By%20employing%20DoRA%2C%20we%0Aenhance%20both%20the%20learning%20capacity%20and%20training%20stability%20of%20LoRA%20while%0Aavoiding%20any%20additional%20inference%20overhead.%20DoRA%20consistently%20outperforms%20LoRA%0Aon%20fine-tuning%20LLaMA%2C%20LLaVA%2C%20and%20VL-BART%20on%20various%20downstream%20tasks%2C%20such%20as%0Acommonsense%20reasoning%2C%20visual%20instruction%20tuning%2C%20and%20image/video-text%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09353v3&entry.124074799=Read"},
{"title": "A critical look at the evaluation of GNNs under heterophily: Are we\n  really making progress?", "author": "Oleg Platonov and Denis Kuznedelev and Michael Diskin and Artem Babenko and Liudmila Prokhorenkova", "abstract": "  Node classification is a classical graph machine learning task on which Graph\nNeural Networks (GNNs) have recently achieved strong results. However, it is\noften believed that standard GNNs only work well for homophilous graphs, i.e.,\ngraphs where edges tend to connect nodes of the same class. Graphs without this\nproperty are called heterophilous, and it is typically assumed that specialized\nmethods are required to achieve strong performance on such graphs. In this\nwork, we challenge this assumption. First, we show that the standard datasets\nused for evaluating heterophily-specific models have serious drawbacks, making\nresults obtained by using them unreliable. The most significant of these\ndrawbacks is the presence of a large number of duplicate nodes in the datasets\nSquirrel and Chameleon, which leads to train-test data leakage. We show that\nremoving duplicate nodes strongly affects GNN performance on these datasets.\nThen, we propose a set of heterophilous graphs of varying properties that we\nbelieve can serve as a better benchmark for evaluating the performance of GNNs\nunder heterophily. We show that standard GNNs achieve strong results on these\nheterophilous graphs, almost always outperforming specialized models. Our\ndatasets and the code for reproducing our experiments are available at\nhttps://github.com/yandex-research/heterophilous-graphs\n", "link": "http://arxiv.org/abs/2302.11640v2", "date": "2024-03-02", "relevancy": 2.3344, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5015}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4584}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4408}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20critical%20look%20at%20the%20evaluation%20of%20GNNs%20under%20heterophily%3A%20Are%20we%0A%20%20really%20making%20progress%3F&entry.906535625=Oleg%20Platonov%20and%20Denis%20Kuznedelev%20and%20Michael%20Diskin%20and%20Artem%20Babenko%20and%20Liudmila%20Prokhorenkova&entry.1292438233=%20%20Node%20classification%20is%20a%20classical%20graph%20machine%20learning%20task%20on%20which%20Graph%0ANeural%20Networks%20%28GNNs%29%20have%20recently%20achieved%20strong%20results.%20However%2C%20it%20is%0Aoften%20believed%20that%20standard%20GNNs%20only%20work%20well%20for%20homophilous%20graphs%2C%20i.e.%2C%0Agraphs%20where%20edges%20tend%20to%20connect%20nodes%20of%20the%20same%20class.%20Graphs%20without%20this%0Aproperty%20are%20called%20heterophilous%2C%20and%20it%20is%20typically%20assumed%20that%20specialized%0Amethods%20are%20required%20to%20achieve%20strong%20performance%20on%20such%20graphs.%20In%20this%0Awork%2C%20we%20challenge%20this%20assumption.%20First%2C%20we%20show%20that%20the%20standard%20datasets%0Aused%20for%20evaluating%20heterophily-specific%20models%20have%20serious%20drawbacks%2C%20making%0Aresults%20obtained%20by%20using%20them%20unreliable.%20The%20most%20significant%20of%20these%0Adrawbacks%20is%20the%20presence%20of%20a%20large%20number%20of%20duplicate%20nodes%20in%20the%20datasets%0ASquirrel%20and%20Chameleon%2C%20which%20leads%20to%20train-test%20data%20leakage.%20We%20show%20that%0Aremoving%20duplicate%20nodes%20strongly%20affects%20GNN%20performance%20on%20these%20datasets.%0AThen%2C%20we%20propose%20a%20set%20of%20heterophilous%20graphs%20of%20varying%20properties%20that%20we%0Abelieve%20can%20serve%20as%20a%20better%20benchmark%20for%20evaluating%20the%20performance%20of%20GNNs%0Aunder%20heterophily.%20We%20show%20that%20standard%20GNNs%20achieve%20strong%20results%20on%20these%0Aheterophilous%20graphs%2C%20almost%20always%20outperforming%20specialized%20models.%20Our%0Adatasets%20and%20the%20code%20for%20reproducing%20our%20experiments%20are%20available%20at%0Ahttps%3A//github.com/yandex-research/heterophilous-graphs%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.11640v2&entry.124074799=Read"},
{"title": "DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with\n  Diffusion Autoencoder", "author": "Chenpeng Du and Qi Chen and Xie Chen and Kai Yu", "abstract": "  While recent research has made significant progress in speech-driven talking\nface generation, the quality of the generated video still lags behind that of\nreal recordings. One reason for this is the use of handcrafted intermediate\nrepresentations like facial landmarks and 3DMM coefficients, which are designed\nbased on human knowledge and are insufficient to precisely describe facial\nmovements. Additionally, these methods require an external pretrained model for\nextracting these representations, whose performance sets an upper bound on\ntalking face generation. To address these limitations, we propose a novel\nmethod called DAE-Talker that leverages data-driven latent representations\nobtained from a diffusion autoencoder (DAE). DAE contains an image encoder that\nencodes an image into a latent vector and a DDIM image decoder that\nreconstructs the image from it. We train our DAE on talking face video frames\nand then extract their latent representations as the training target for a\nConformer-based speech2latent model. This allows DAE-Talker to synthesize full\nvideo frames and produce natural head movements that align with the content of\nspeech, rather than relying on a predetermined head pose from a template video.\nWe also introduce pose modelling in speech2latent for pose controllability.\nAdditionally, we propose a novel method for generating continuous video frames\nwith the DDIM image decoder trained on individual frames, eliminating the need\nfor modelling the joint distribution of consecutive frames directly. Our\nexperiments show that DAE-Talker outperforms existing popular methods in\nlip-sync, video fidelity, and pose naturalness. We also conduct ablation\nstudies to analyze the effectiveness of the proposed techniques and demonstrate\nthe pose controllability of DAE-Talker.\n", "link": "http://arxiv.org/abs/2303.17550v5", "date": "2024-03-01", "relevancy": 2.334, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6232}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6067}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5345}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAE-Talker%3A%20High%20Fidelity%20Speech-Driven%20Talking%20Face%20Generation%20with%0A%20%20Diffusion%20Autoencoder&entry.906535625=Chenpeng%20Du%20and%20Qi%20Chen%20and%20Xie%20Chen%20and%20Kai%20Yu&entry.1292438233=%20%20While%20recent%20research%20has%20made%20significant%20progress%20in%20speech-driven%20talking%0Aface%20generation%2C%20the%20quality%20of%20the%20generated%20video%20still%20lags%20behind%20that%20of%0Areal%20recordings.%20One%20reason%20for%20this%20is%20the%20use%20of%20handcrafted%20intermediate%0Arepresentations%20like%20facial%20landmarks%20and%203DMM%20coefficients%2C%20which%20are%20designed%0Abased%20on%20human%20knowledge%20and%20are%20insufficient%20to%20precisely%20describe%20facial%0Amovements.%20Additionally%2C%20these%20methods%20require%20an%20external%20pretrained%20model%20for%0Aextracting%20these%20representations%2C%20whose%20performance%20sets%20an%20upper%20bound%20on%0Atalking%20face%20generation.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Amethod%20called%20DAE-Talker%20that%20leverages%20data-driven%20latent%20representations%0Aobtained%20from%20a%20diffusion%20autoencoder%20%28DAE%29.%20DAE%20contains%20an%20image%20encoder%20that%0Aencodes%20an%20image%20into%20a%20latent%20vector%20and%20a%20DDIM%20image%20decoder%20that%0Areconstructs%20the%20image%20from%20it.%20We%20train%20our%20DAE%20on%20talking%20face%20video%20frames%0Aand%20then%20extract%20their%20latent%20representations%20as%20the%20training%20target%20for%20a%0AConformer-based%20speech2latent%20model.%20This%20allows%20DAE-Talker%20to%20synthesize%20full%0Avideo%20frames%20and%20produce%20natural%20head%20movements%20that%20align%20with%20the%20content%20of%0Aspeech%2C%20rather%20than%20relying%20on%20a%20predetermined%20head%20pose%20from%20a%20template%20video.%0AWe%20also%20introduce%20pose%20modelling%20in%20speech2latent%20for%20pose%20controllability.%0AAdditionally%2C%20we%20propose%20a%20novel%20method%20for%20generating%20continuous%20video%20frames%0Awith%20the%20DDIM%20image%20decoder%20trained%20on%20individual%20frames%2C%20eliminating%20the%20need%0Afor%20modelling%20the%20joint%20distribution%20of%20consecutive%20frames%20directly.%20Our%0Aexperiments%20show%20that%20DAE-Talker%20outperforms%20existing%20popular%20methods%20in%0Alip-sync%2C%20video%20fidelity%2C%20and%20pose%20naturalness.%20We%20also%20conduct%20ablation%0Astudies%20to%20analyze%20the%20effectiveness%20of%20the%20proposed%20techniques%20and%20demonstrate%0Athe%20pose%20controllability%20of%20DAE-Talker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.17550v5&entry.124074799=Read"},
{"title": "RealCustom: Narrowing Real Text Word for Real-Time Open-Domain\n  Text-to-Image Customization", "author": "Mengqi Huang and Zhendong Mao and Mingcong Liu and Qian He and Yongdong Zhang", "abstract": "  Text-to-image customization, which aims to synthesize text-driven images for\nthe given subjects, has recently revolutionized content creation. Existing\nworks follow the pseudo-word paradigm, i.e., represent the given subjects as\npseudo-words and then compose them with the given text. However, the inherent\nentangled influence scope of pseudo-words with the given text results in a\ndual-optimum paradox, i.e., the similarity of the given subjects and the\ncontrollability of the given text could not be optimal simultaneously. We\npresent RealCustom that, for the first time, disentangles similarity from\ncontrollability by precisely limiting subject influence to relevant parts only,\nachieved by gradually narrowing real text word from its general connotation to\nthe specific subject and using its cross-attention to distinguish relevance.\nSpecifically, RealCustom introduces a novel \"train-inference\" decoupled\nframework: (1) during training, RealCustom learns general alignment between\nvisual conditions to original textual conditions by a novel adaptive scoring\nmodule to adaptively modulate influence quantity; (2) during inference, a novel\nadaptive mask guidance strategy is proposed to iteratively update the influence\nscope and influence quantity of the given subjects to gradually narrow the\ngeneration of the real text word. Comprehensive experiments demonstrate the\nsuperior real-time customization ability of RealCustom in the open domain,\nachieving both unprecedented similarity of the given subjects and\ncontrollability of the given text for the first time. The project page is\nhttps://corleone-huang.github.io/realcustom/.\n", "link": "http://arxiv.org/abs/2403.00483v1", "date": "2024-03-01", "relevancy": 2.3322, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6571}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6001}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5364}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealCustom%3A%20Narrowing%20Real%20Text%20Word%20for%20Real-Time%20Open-Domain%0A%20%20Text-to-Image%20Customization&entry.906535625=Mengqi%20Huang%20and%20Zhendong%20Mao%20and%20Mingcong%20Liu%20and%20Qian%20He%20and%20Yongdong%20Zhang&entry.1292438233=%20%20Text-to-image%20customization%2C%20which%20aims%20to%20synthesize%20text-driven%20images%20for%0Athe%20given%20subjects%2C%20has%20recently%20revolutionized%20content%20creation.%20Existing%0Aworks%20follow%20the%20pseudo-word%20paradigm%2C%20i.e.%2C%20represent%20the%20given%20subjects%20as%0Apseudo-words%20and%20then%20compose%20them%20with%20the%20given%20text.%20However%2C%20the%20inherent%0Aentangled%20influence%20scope%20of%20pseudo-words%20with%20the%20given%20text%20results%20in%20a%0Adual-optimum%20paradox%2C%20i.e.%2C%20the%20similarity%20of%20the%20given%20subjects%20and%20the%0Acontrollability%20of%20the%20given%20text%20could%20not%20be%20optimal%20simultaneously.%20We%0Apresent%20RealCustom%20that%2C%20for%20the%20first%20time%2C%20disentangles%20similarity%20from%0Acontrollability%20by%20precisely%20limiting%20subject%20influence%20to%20relevant%20parts%20only%2C%0Aachieved%20by%20gradually%20narrowing%20real%20text%20word%20from%20its%20general%20connotation%20to%0Athe%20specific%20subject%20and%20using%20its%20cross-attention%20to%20distinguish%20relevance.%0ASpecifically%2C%20RealCustom%20introduces%20a%20novel%20%22train-inference%22%20decoupled%0Aframework%3A%20%281%29%20during%20training%2C%20RealCustom%20learns%20general%20alignment%20between%0Avisual%20conditions%20to%20original%20textual%20conditions%20by%20a%20novel%20adaptive%20scoring%0Amodule%20to%20adaptively%20modulate%20influence%20quantity%3B%20%282%29%20during%20inference%2C%20a%20novel%0Aadaptive%20mask%20guidance%20strategy%20is%20proposed%20to%20iteratively%20update%20the%20influence%0Ascope%20and%20influence%20quantity%20of%20the%20given%20subjects%20to%20gradually%20narrow%20the%0Ageneration%20of%20the%20real%20text%20word.%20Comprehensive%20experiments%20demonstrate%20the%0Asuperior%20real-time%20customization%20ability%20of%20RealCustom%20in%20the%20open%20domain%2C%0Aachieving%20both%20unprecedented%20similarity%20of%20the%20given%20subjects%20and%0Acontrollability%20of%20the%20given%20text%20for%20the%20first%20time.%20The%20project%20page%20is%0Ahttps%3A//corleone-huang.github.io/realcustom/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00483v1&entry.124074799=Read"},
{"title": "A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex\n  Lasso Models with Reflection Features", "author": "Emi Zeger and Yifei Wang and Aaron Mishkin and Tolga Ergen and Emmanuel Cand\u00e8s and Mert Pilanci", "abstract": "  We prove that training neural networks on 1-D data is equivalent to solving a\nconvex Lasso problem with a fixed, explicitly defined dictionary matrix of\nfeatures. The specific dictionary depends on the activation and depth. We\nconsider 2-layer networks with piecewise linear activations, deep narrow ReLU\nnetworks with up to 4 layers, and rectangular and tree networks with sign\nactivation and arbitrary depth. Interestingly in ReLU networks, a fourth layer\ncreates features that represent reflections of training data about themselves.\nThe Lasso representation sheds insight to globally optimal networks and the\nsolution landscape.\n", "link": "http://arxiv.org/abs/2403.01046v1", "date": "2024-03-02", "relevancy": 2.3265, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.484}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.455}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Library%20of%20Mirrors%3A%20Deep%20Neural%20Nets%20in%20Low%20Dimensions%20are%20Convex%0A%20%20Lasso%20Models%20with%20Reflection%20Features&entry.906535625=Emi%20Zeger%20and%20Yifei%20Wang%20and%20Aaron%20Mishkin%20and%20Tolga%20Ergen%20and%20Emmanuel%20Cand%C3%A8s%20and%20Mert%20Pilanci&entry.1292438233=%20%20We%20prove%20that%20training%20neural%20networks%20on%201-D%20data%20is%20equivalent%20to%20solving%20a%0Aconvex%20Lasso%20problem%20with%20a%20fixed%2C%20explicitly%20defined%20dictionary%20matrix%20of%0Afeatures.%20The%20specific%20dictionary%20depends%20on%20the%20activation%20and%20depth.%20We%0Aconsider%202-layer%20networks%20with%20piecewise%20linear%20activations%2C%20deep%20narrow%20ReLU%0Anetworks%20with%20up%20to%204%20layers%2C%20and%20rectangular%20and%20tree%20networks%20with%20sign%0Aactivation%20and%20arbitrary%20depth.%20Interestingly%20in%20ReLU%20networks%2C%20a%20fourth%20layer%0Acreates%20features%20that%20represent%20reflections%20of%20training%20data%20about%20themselves.%0AThe%20Lasso%20representation%20sheds%20insight%20to%20globally%20optimal%20networks%20and%20the%0Asolution%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01046v1&entry.124074799=Read"},
{"title": "Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor", "author": "Junlin Song and Antoine Richard and Miguel Olivares-Mendez", "abstract": "  In robotics, motion capture systems have been widely used to measure the\naccuracy of localization algorithms. Moreover, this infrastructure can also be\nused for other computer vision tasks, such as the evaluation of Visual\n(-Inertial) SLAM dynamic initialization, multi-object tracking, or automatic\nannotation. Yet, to work optimally, these functionalities require having\naccurate and reliable spatial-temporal calibration parameters between the\ncamera and the global pose sensor. In this study, we provide two novel\nsolutions to estimate these calibration parameters. Firstly, we design an\noffline target-based method with high accuracy and consistency.\nSpatial-temporal parameters, camera intrinsic, and trajectory are optimized\nsimultaneously. Then, we propose an online target-less method, eliminating the\nneed for a calibration target and enabling the estimation of time-varying\nspatial-temporal parameters. Additionally, we perform detailed observability\nanalysis for the target-less method. Our theoretical findings regarding\nobservability are validated by simulation experiments and provide explainable\nguidelines for calibration. Finally, the accuracy and consistency of two\nproposed methods are evaluated with hand-held real-world datasets where\ntraditional hand-eye calibration method do not work.\n", "link": "http://arxiv.org/abs/2403.00976v1", "date": "2024-03-01", "relevancy": 2.3238, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5921}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5758}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5718}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Spatial-Temporal%20Calibration%20for%20Camera%20and%20Global%20Pose%20Sensor&entry.906535625=Junlin%20Song%20and%20Antoine%20Richard%20and%20Miguel%20Olivares-Mendez&entry.1292438233=%20%20In%20robotics%2C%20motion%20capture%20systems%20have%20been%20widely%20used%20to%20measure%20the%0Aaccuracy%20of%20localization%20algorithms.%20Moreover%2C%20this%20infrastructure%20can%20also%20be%0Aused%20for%20other%20computer%20vision%20tasks%2C%20such%20as%20the%20evaluation%20of%20Visual%0A%28-Inertial%29%20SLAM%20dynamic%20initialization%2C%20multi-object%20tracking%2C%20or%20automatic%0Aannotation.%20Yet%2C%20to%20work%20optimally%2C%20these%20functionalities%20require%20having%0Aaccurate%20and%20reliable%20spatial-temporal%20calibration%20parameters%20between%20the%0Acamera%20and%20the%20global%20pose%20sensor.%20In%20this%20study%2C%20we%20provide%20two%20novel%0Asolutions%20to%20estimate%20these%20calibration%20parameters.%20Firstly%2C%20we%20design%20an%0Aoffline%20target-based%20method%20with%20high%20accuracy%20and%20consistency.%0ASpatial-temporal%20parameters%2C%20camera%20intrinsic%2C%20and%20trajectory%20are%20optimized%0Asimultaneously.%20Then%2C%20we%20propose%20an%20online%20target-less%20method%2C%20eliminating%20the%0Aneed%20for%20a%20calibration%20target%20and%20enabling%20the%20estimation%20of%20time-varying%0Aspatial-temporal%20parameters.%20Additionally%2C%20we%20perform%20detailed%20observability%0Aanalysis%20for%20the%20target-less%20method.%20Our%20theoretical%20findings%20regarding%0Aobservability%20are%20validated%20by%20simulation%20experiments%20and%20provide%20explainable%0Aguidelines%20for%20calibration.%20Finally%2C%20the%20accuracy%20and%20consistency%20of%20two%0Aproposed%20methods%20are%20evaluated%20with%20hand-held%20real-world%20datasets%20where%0Atraditional%20hand-eye%20calibration%20method%20do%20not%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00976v1&entry.124074799=Read"},
{"title": "SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM", "author": "Mingrui Li and Shuhong Liu and Heng Zhou and Guohao Zhu and Na Cheng and Hongyu Wang", "abstract": "  Semantic understanding plays a crucial role in Dense Simultaneous\nLocalization and Mapping (SLAM). Recent advancements that integrate Gaussian\nSplatting into SLAM systems have demonstrated its effectiveness in generating\nhigh-quality renderings. Building on this progress, we propose SGS-SLAM which\nprovides precise 3D semantic segmentation alongside high-fidelity\nreconstructions. Specifically, we propose to employ multi-channel optimization\nduring the mapping process, integrating appearance, geometric, and semantic\nconstraints with key-frame optimization to enhance reconstruction quality.\nExtensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\nperformance in camera pose estimation, map reconstruction, and semantic\nsegmentation. It outperforms existing methods by a large margin meanwhile\npreserving real-time rendering ability.\n", "link": "http://arxiv.org/abs/2402.03246v3", "date": "2024-03-02", "relevancy": 2.3189, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.617}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5474}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGS-SLAM%3A%20Semantic%20Gaussian%20Splatting%20For%20Neural%20Dense%20SLAM&entry.906535625=Mingrui%20Li%20and%20Shuhong%20Liu%20and%20Heng%20Zhou%20and%20Guohao%20Zhu%20and%20Na%20Cheng%20and%20Hongyu%20Wang&entry.1292438233=%20%20Semantic%20understanding%20plays%20a%20crucial%20role%20in%20Dense%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29.%20Recent%20advancements%20that%20integrate%20Gaussian%0ASplatting%20into%20SLAM%20systems%20have%20demonstrated%20its%20effectiveness%20in%20generating%0Ahigh-quality%20renderings.%20Building%20on%20this%20progress%2C%20we%20propose%20SGS-SLAM%20which%0Aprovides%20precise%203D%20semantic%20segmentation%20alongside%20high-fidelity%0Areconstructions.%20Specifically%2C%20we%20propose%20to%20employ%20multi-channel%20optimization%0Aduring%20the%20mapping%20process%2C%20integrating%20appearance%2C%20geometric%2C%20and%20semantic%0Aconstraints%20with%20key-frame%20optimization%20to%20enhance%20reconstruction%20quality.%0AExtensive%20experiments%20demonstrate%20that%20SGS-SLAM%20delivers%20state-of-the-art%0Aperformance%20in%20camera%20pose%20estimation%2C%20map%20reconstruction%2C%20and%20semantic%0Asegmentation.%20It%20outperforms%20existing%20methods%20by%20a%20large%20margin%20meanwhile%0Apreserving%20real-time%20rendering%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03246v3&entry.124074799=Read"},
{"title": "Contributing Dimension Structure of Deep Feature for Coreset Selection", "author": "Zhijing Wan and Zhixiang Wang and Yuran Wang and Zheng Wang and Hongyuan Zhu and Shin'ichi Satoh", "abstract": "  Coreset selection seeks to choose a subset of crucial training samples for\nefficient learning. It has gained traction in deep learning, particularly with\nthe surge in training dataset sizes. Sample selection hinges on two main\naspects: a sample's representation in enhancing performance and the role of\nsample diversity in averting overfitting. Existing methods typically measure\nboth the representation and diversity of data based on similarity metrics, such\nas L2-norm. They have capably tackled representation via distribution matching\nguided by the similarities of features, gradients, or other information between\ndata. However, the results of effectively diverse sample selection are mired in\nsub-optimality. This is because the similarity metrics usually simply aggregate\ndimension similarities without acknowledging disparities among the dimensions\nthat significantly contribute to the final similarity. As a result, they fall\nshort of adequately capturing diversity. To address this, we propose a\nfeature-based diversity constraint, compelling the chosen subset to exhibit\nmaximum diversity. Our key lies in the introduction of a novel Contributing\nDimension Structure (CDS) metric. Different from similarity metrics that\nmeasure the overall similarity of high-dimensional features, our CDS metric\nconsiders not only the reduction of redundancy in feature dimensions, but also\nthe difference between dimensions that contribute significantly to the final\nsimilarity. We reveal that existing methods tend to favor samples with similar\nCDS, leading to a reduced variety of CDS types within the coreset and\nsubsequently hindering model performance. In response, we enhance the\nperformance of five classical selection methods by integrating the CDS\nconstraint. Our experiments on three datasets demonstrate the general\neffectiveness of the proposed method in boosting existing methods.\n", "link": "http://arxiv.org/abs/2401.16193v2", "date": "2024-03-02", "relevancy": 2.3168, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4662}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4657}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4583}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contributing%20Dimension%20Structure%20of%20Deep%20Feature%20for%20Coreset%20Selection&entry.906535625=Zhijing%20Wan%20and%20Zhixiang%20Wang%20and%20Yuran%20Wang%20and%20Zheng%20Wang%20and%20Hongyuan%20Zhu%20and%20Shin%27ichi%20Satoh&entry.1292438233=%20%20Coreset%20selection%20seeks%20to%20choose%20a%20subset%20of%20crucial%20training%20samples%20for%0Aefficient%20learning.%20It%20has%20gained%20traction%20in%20deep%20learning%2C%20particularly%20with%0Athe%20surge%20in%20training%20dataset%20sizes.%20Sample%20selection%20hinges%20on%20two%20main%0Aaspects%3A%20a%20sample%27s%20representation%20in%20enhancing%20performance%20and%20the%20role%20of%0Asample%20diversity%20in%20averting%20overfitting.%20Existing%20methods%20typically%20measure%0Aboth%20the%20representation%20and%20diversity%20of%20data%20based%20on%20similarity%20metrics%2C%20such%0Aas%20L2-norm.%20They%20have%20capably%20tackled%20representation%20via%20distribution%20matching%0Aguided%20by%20the%20similarities%20of%20features%2C%20gradients%2C%20or%20other%20information%20between%0Adata.%20However%2C%20the%20results%20of%20effectively%20diverse%20sample%20selection%20are%20mired%20in%0Asub-optimality.%20This%20is%20because%20the%20similarity%20metrics%20usually%20simply%20aggregate%0Adimension%20similarities%20without%20acknowledging%20disparities%20among%20the%20dimensions%0Athat%20significantly%20contribute%20to%20the%20final%20similarity.%20As%20a%20result%2C%20they%20fall%0Ashort%20of%20adequately%20capturing%20diversity.%20To%20address%20this%2C%20we%20propose%20a%0Afeature-based%20diversity%20constraint%2C%20compelling%20the%20chosen%20subset%20to%20exhibit%0Amaximum%20diversity.%20Our%20key%20lies%20in%20the%20introduction%20of%20a%20novel%20Contributing%0ADimension%20Structure%20%28CDS%29%20metric.%20Different%20from%20similarity%20metrics%20that%0Ameasure%20the%20overall%20similarity%20of%20high-dimensional%20features%2C%20our%20CDS%20metric%0Aconsiders%20not%20only%20the%20reduction%20of%20redundancy%20in%20feature%20dimensions%2C%20but%20also%0Athe%20difference%20between%20dimensions%20that%20contribute%20significantly%20to%20the%20final%0Asimilarity.%20We%20reveal%20that%20existing%20methods%20tend%20to%20favor%20samples%20with%20similar%0ACDS%2C%20leading%20to%20a%20reduced%20variety%20of%20CDS%20types%20within%20the%20coreset%20and%0Asubsequently%20hindering%20model%20performance.%20In%20response%2C%20we%20enhance%20the%0Aperformance%20of%20five%20classical%20selection%20methods%20by%20integrating%20the%20CDS%0Aconstraint.%20Our%20experiments%20on%20three%20datasets%20demonstrate%20the%20general%0Aeffectiveness%20of%20the%20proposed%20method%20in%20boosting%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16193v2&entry.124074799=Read"},
{"title": "Flattening Singular Values of Factorized Convolution for Medical Images", "author": "Zexin Feng and Na Zeng and Jiansheng Fang and Xingyue Wang and Xiaoxi Lu and Heng Meng and Jiang Liu", "abstract": "  Convolutional neural networks (CNNs) have long been the paradigm of choice\nfor robust medical image processing (MIP). Therefore, it is crucial to\neffectively and efficiently deploy CNNs on devices with different computing\ncapabilities to support computer-aided diagnosis. Many methods employ\nfactorized convolutional layers to alleviate the burden of limited\ncomputational resources at the expense of expressiveness. To this end, given\nweak medical image-driven CNN model optimization, a Singular value equalization\ngeneralizer-induced Factorized Convolution (SFConv) is proposed to improve the\nexpressive power of factorized convolutions in MIP models. We first decompose\nthe weight matrix of convolutional filters into two low-rank matrices to\nachieve model reduction. Then minimize the KL divergence between the two\nlow-rank weight matrices and the uniform distribution, thereby reducing the\nnumber of singular value directions with significant variance. Extensive\nexperiments on fundus and OCTA datasets demonstrate that our SFConv yields\ncompetitive expressiveness over vanilla convolutions while reducing complexity.\n", "link": "http://arxiv.org/abs/2403.00606v1", "date": "2024-03-01", "relevancy": 2.3046, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4773}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4576}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4478}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flattening%20Singular%20Values%20of%20Factorized%20Convolution%20for%20Medical%20Images&entry.906535625=Zexin%20Feng%20and%20Na%20Zeng%20and%20Jiansheng%20Fang%20and%20Xingyue%20Wang%20and%20Xiaoxi%20Lu%20and%20Heng%20Meng%20and%20Jiang%20Liu&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20have%20long%20been%20the%20paradigm%20of%20choice%0Afor%20robust%20medical%20image%20processing%20%28MIP%29.%20Therefore%2C%20it%20is%20crucial%20to%0Aeffectively%20and%20efficiently%20deploy%20CNNs%20on%20devices%20with%20different%20computing%0Acapabilities%20to%20support%20computer-aided%20diagnosis.%20Many%20methods%20employ%0Afactorized%20convolutional%20layers%20to%20alleviate%20the%20burden%20of%20limited%0Acomputational%20resources%20at%20the%20expense%20of%20expressiveness.%20To%20this%20end%2C%20given%0Aweak%20medical%20image-driven%20CNN%20model%20optimization%2C%20a%20Singular%20value%20equalization%0Ageneralizer-induced%20Factorized%20Convolution%20%28SFConv%29%20is%20proposed%20to%20improve%20the%0Aexpressive%20power%20of%20factorized%20convolutions%20in%20MIP%20models.%20We%20first%20decompose%0Athe%20weight%20matrix%20of%20convolutional%20filters%20into%20two%20low-rank%20matrices%20to%0Aachieve%20model%20reduction.%20Then%20minimize%20the%20KL%20divergence%20between%20the%20two%0Alow-rank%20weight%20matrices%20and%20the%20uniform%20distribution%2C%20thereby%20reducing%20the%0Anumber%20of%20singular%20value%20directions%20with%20significant%20variance.%20Extensive%0Aexperiments%20on%20fundus%20and%20OCTA%20datasets%20demonstrate%20that%20our%20SFConv%20yields%0Acompetitive%20expressiveness%20over%20vanilla%20convolutions%20while%20reducing%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00606v1&entry.124074799=Read"},
{"title": "Dynamic 3D Point Cloud Sequences as 2D Videos", "author": "Yiming Zeng and Junhui Hou and Qijian Zhang and Siyu Ren and Wenping Wang", "abstract": "  Dynamic 3D point cloud sequences serve as one of the most common and\npractical representation modalities of dynamic real-world environments.\nHowever, their unstructured nature in both spatial and temporal domains poses\nsignificant challenges to effective and efficient processing. Existing deep\npoint cloud sequence modeling approaches imitate the mature 2D video learning\nmechanisms by developing complex spatio-temporal point neighbor grouping and\nfeature aggregation schemes, often resulting in methods lacking effectiveness,\nefficiency, and expressive power. In this paper, we propose a novel generic\nrepresentation called \\textit{Structured Point Cloud Videos} (SPCVs).\nIntuitively, by leveraging the fact that 3D geometric shapes are essentially 2D\nmanifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatial\nsmoothness and temporal consistency, where the pixel values correspond to the\n3D coordinates of points. The structured nature of our SPCV representation\nallows for the seamless adaptation of well-established 2D image/video\ntechniques, enabling efficient and effective processing and analysis of 3D\npoint cloud sequences. To achieve such re-organization, we design a\nself-supervised learning pipeline that is geometrically regularized and driven\nby self-reconstructive and deformation field learning objectives. Additionally,\nwe construct SPCV-based frameworks for both low-level and high-level 3D point\ncloud sequence processing and analysis tasks, including action recognition,\ntemporal interpolation, and compression. Extensive experiments demonstrate the\nversatility and superiority of the proposed SPCV, which has the potential to\noffer new possibilities for deep learning on unstructured 3D point cloud\nsequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.\n", "link": "http://arxiv.org/abs/2403.01129v1", "date": "2024-03-02", "relevancy": 2.3042, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5997}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.563}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5496}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%203D%20Point%20Cloud%20Sequences%20as%202D%20Videos&entry.906535625=Yiming%20Zeng%20and%20Junhui%20Hou%20and%20Qijian%20Zhang%20and%20Siyu%20Ren%20and%20Wenping%20Wang&entry.1292438233=%20%20Dynamic%203D%20point%20cloud%20sequences%20serve%20as%20one%20of%20the%20most%20common%20and%0Apractical%20representation%20modalities%20of%20dynamic%20real-world%20environments.%0AHowever%2C%20their%20unstructured%20nature%20in%20both%20spatial%20and%20temporal%20domains%20poses%0Asignificant%20challenges%20to%20effective%20and%20efficient%20processing.%20Existing%20deep%0Apoint%20cloud%20sequence%20modeling%20approaches%20imitate%20the%20mature%202D%20video%20learning%0Amechanisms%20by%20developing%20complex%20spatio-temporal%20point%20neighbor%20grouping%20and%0Afeature%20aggregation%20schemes%2C%20often%20resulting%20in%20methods%20lacking%20effectiveness%2C%0Aefficiency%2C%20and%20expressive%20power.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20generic%0Arepresentation%20called%20%5Ctextit%7BStructured%20Point%20Cloud%20Videos%7D%20%28SPCVs%29.%0AIntuitively%2C%20by%20leveraging%20the%20fact%20that%203D%20geometric%20shapes%20are%20essentially%202D%0Amanifolds%2C%20SPCV%20re-organizes%20a%20point%20cloud%20sequence%20as%20a%202D%20video%20with%20spatial%0Asmoothness%20and%20temporal%20consistency%2C%20where%20the%20pixel%20values%20correspond%20to%20the%0A3D%20coordinates%20of%20points.%20The%20structured%20nature%20of%20our%20SPCV%20representation%0Aallows%20for%20the%20seamless%20adaptation%20of%20well-established%202D%20image/video%0Atechniques%2C%20enabling%20efficient%20and%20effective%20processing%20and%20analysis%20of%203D%0Apoint%20cloud%20sequences.%20To%20achieve%20such%20re-organization%2C%20we%20design%20a%0Aself-supervised%20learning%20pipeline%20that%20is%20geometrically%20regularized%20and%20driven%0Aby%20self-reconstructive%20and%20deformation%20field%20learning%20objectives.%20Additionally%2C%0Awe%20construct%20SPCV-based%20frameworks%20for%20both%20low-level%20and%20high-level%203D%20point%0Acloud%20sequence%20processing%20and%20analysis%20tasks%2C%20including%20action%20recognition%2C%0Atemporal%20interpolation%2C%20and%20compression.%20Extensive%20experiments%20demonstrate%20the%0Aversatility%20and%20superiority%20of%20the%20proposed%20SPCV%2C%20which%20has%20the%20potential%20to%0Aoffer%20new%20possibilities%20for%20deep%20learning%20on%20unstructured%203D%20point%20cloud%0Asequences.%20Code%20will%20be%20released%20at%20https%3A//github.com/ZENGYIMING-EAMON/SPCV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01129v1&entry.124074799=Read"},
{"title": "Subhomogeneous Deep Equilibrium Models", "author": "Pietro Sittoni and Francesco Tudisco", "abstract": "  Implicit-depth neural networks have grown as powerful alternatives to\ntraditional networks in various applications in recent years. However, these\nmodels often lack guarantees of existence and uniqueness, raising stability,\nperformance, and reproducibility issues. In this paper, we present a new\nanalysis of the existence and uniqueness of fixed points for implicit-depth\nneural networks based on the concept of subhomogeneous operators and the\nnonlinear Perron-Frobenius theory. Compared to previous similar analyses, our\ntheory allows for weaker assumptions on the parameter matrices, thus yielding a\nmore flexible framework for well-defined implicit networks. We illustrate the\nperformance of the resulting subhomogeneous networks on feed-forward,\nconvolutional, and graph neural network examples.\n", "link": "http://arxiv.org/abs/2403.00720v1", "date": "2024-03-01", "relevancy": 2.3041, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4823}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.459}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4412}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subhomogeneous%20Deep%20Equilibrium%20Models&entry.906535625=Pietro%20Sittoni%20and%20Francesco%20Tudisco&entry.1292438233=%20%20Implicit-depth%20neural%20networks%20have%20grown%20as%20powerful%20alternatives%20to%0Atraditional%20networks%20in%20various%20applications%20in%20recent%20years.%20However%2C%20these%0Amodels%20often%20lack%20guarantees%20of%20existence%20and%20uniqueness%2C%20raising%20stability%2C%0Aperformance%2C%20and%20reproducibility%20issues.%20In%20this%20paper%2C%20we%20present%20a%20new%0Aanalysis%20of%20the%20existence%20and%20uniqueness%20of%20fixed%20points%20for%20implicit-depth%0Aneural%20networks%20based%20on%20the%20concept%20of%20subhomogeneous%20operators%20and%20the%0Anonlinear%20Perron-Frobenius%20theory.%20Compared%20to%20previous%20similar%20analyses%2C%20our%0Atheory%20allows%20for%20weaker%20assumptions%20on%20the%20parameter%20matrices%2C%20thus%20yielding%20a%0Amore%20flexible%20framework%20for%20well-defined%20implicit%20networks.%20We%20illustrate%20the%0Aperformance%20of%20the%20resulting%20subhomogeneous%20networks%20on%20feed-forward%2C%0Aconvolutional%2C%20and%20graph%20neural%20network%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00720v1&entry.124074799=Read"},
{"title": "Prediction of Cellular Identities from Trajectory and Cell Fate\n  Information", "author": "Baiyang Dai and Jiamin Yang and Hari Shroff and Patrick La Riviere", "abstract": "  Determining cell identities in imaging sequences is an important yet\nchallenging task. The conventional method for cell identification is via cell\ntracking, which is complex and can be time-consuming. In this study, we propose\nan innovative approach to cell identification during early $\\textit{C.\nelegans}$ embryogenesis using machine learning. Cell identification during\n$\\textit{C. elegans}$ embryogenesis would provide insights into neural\ndevelopment with implications for higher organisms including humans. We\nemployed random forest, MLP, and LSTM models, and tested cell classification\naccuracy on 3D time-lapse confocal datasets spanning the first 4 hours of\nembryogenesis. By leveraging a small number of spatial-temporal features of\nindividual cells, including cell trajectory and cell fate information, our\nmodels achieve an accuracy of over 91%, even with limited data. We also\ndetermine the most important feature contributions and can interpret these\nfeatures in the context of biological knowledge. Our research demonstrates the\nsuccess of predicting cell identities in time-lapse imaging sequences directly\nfrom simple spatio-temporal features.\n", "link": "http://arxiv.org/abs/2401.06182v2", "date": "2024-03-02", "relevancy": 1.9364, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5122}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4821}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4749}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20of%20Cellular%20Identities%20from%20Trajectory%20and%20Cell%20Fate%0A%20%20Information&entry.906535625=Baiyang%20Dai%20and%20Jiamin%20Yang%20and%20Hari%20Shroff%20and%20Patrick%20La%20Riviere&entry.1292438233=%20%20Determining%20cell%20identities%20in%20imaging%20sequences%20is%20an%20important%20yet%0Achallenging%20task.%20The%20conventional%20method%20for%20cell%20identification%20is%20via%20cell%0Atracking%2C%20which%20is%20complex%20and%20can%20be%20time-consuming.%20In%20this%20study%2C%20we%20propose%0Aan%20innovative%20approach%20to%20cell%20identification%20during%20early%20%24%5Ctextit%7BC.%0Aelegans%7D%24%20embryogenesis%20using%20machine%20learning.%20Cell%20identification%20during%0A%24%5Ctextit%7BC.%20elegans%7D%24%20embryogenesis%20would%20provide%20insights%20into%20neural%0Adevelopment%20with%20implications%20for%20higher%20organisms%20including%20humans.%20We%0Aemployed%20random%20forest%2C%20MLP%2C%20and%20LSTM%20models%2C%20and%20tested%20cell%20classification%0Aaccuracy%20on%203D%20time-lapse%20confocal%20datasets%20spanning%20the%20first%204%20hours%20of%0Aembryogenesis.%20By%20leveraging%20a%20small%20number%20of%20spatial-temporal%20features%20of%0Aindividual%20cells%2C%20including%20cell%20trajectory%20and%20cell%20fate%20information%2C%20our%0Amodels%20achieve%20an%20accuracy%20of%20over%2091%25%2C%20even%20with%20limited%20data.%20We%20also%0Adetermine%20the%20most%20important%20feature%20contributions%20and%20can%20interpret%20these%0Afeatures%20in%20the%20context%20of%20biological%20knowledge.%20Our%20research%20demonstrates%20the%0Asuccess%20of%20predicting%20cell%20identities%20in%20time-lapse%20imaging%20sequences%20directly%0Afrom%20simple%20spatio-temporal%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06182v2&entry.124074799=Read"},
{"title": "Path Tracking using Echoes in an Unknown Environment: the Issue of\n  Symmetries and How to Break Them", "author": "Mireille Boutin and Gregor Kemper", "abstract": "  This paper deals with the problem of reconstructing the path of a vehicle in\nan unknown environment consisting of planar structures using sound. Many\nsystems in the literature do this by using a loudspeaker and microphones\nmounted on a vehicle. Symmetries in the environment lead to solution\nambiguities for such systems. We propose to resolve this issue by placing the\nloudspeaker at a fixed location in the environment rather than on the vehicle.\nThe question of whether this will remove ambiguities regardless of the\nenvironment geometry leads to a question about breaking symmetries that can be\nphrased in purely mathematical terms. We solve this question in the affirmative\nif the geometry is in dimension three or bigger, and give counterexamples in\ndimension two. Excluding the rare situations where the counterexamples arise,\nwe also give an affirmative answer in dimension two. Our results lead to a\nsimple path reconstruction algorithm for a vehicle carrying four microphones\nnavigating within an environment in which a loudspeaker at a fixed position\nemits short bursts of sounds. This algorithm could be combined with other\nmethods from the literature to construct a path tracking system for vehicles\nnavigating within a potentially symmetric environment.\n", "link": "http://arxiv.org/abs/2403.00698v1", "date": "2024-03-01", "relevancy": 1.6844, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4358}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4107}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4103}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Path%20Tracking%20using%20Echoes%20in%20an%20Unknown%20Environment%3A%20the%20Issue%20of%0A%20%20Symmetries%20and%20How%20to%20Break%20Them&entry.906535625=Mireille%20Boutin%20and%20Gregor%20Kemper&entry.1292438233=%20%20This%20paper%20deals%20with%20the%20problem%20of%20reconstructing%20the%20path%20of%20a%20vehicle%20in%0Aan%20unknown%20environment%20consisting%20of%20planar%20structures%20using%20sound.%20Many%0Asystems%20in%20the%20literature%20do%20this%20by%20using%20a%20loudspeaker%20and%20microphones%0Amounted%20on%20a%20vehicle.%20Symmetries%20in%20the%20environment%20lead%20to%20solution%0Aambiguities%20for%20such%20systems.%20We%20propose%20to%20resolve%20this%20issue%20by%20placing%20the%0Aloudspeaker%20at%20a%20fixed%20location%20in%20the%20environment%20rather%20than%20on%20the%20vehicle.%0AThe%20question%20of%20whether%20this%20will%20remove%20ambiguities%20regardless%20of%20the%0Aenvironment%20geometry%20leads%20to%20a%20question%20about%20breaking%20symmetries%20that%20can%20be%0Aphrased%20in%20purely%20mathematical%20terms.%20We%20solve%20this%20question%20in%20the%20affirmative%0Aif%20the%20geometry%20is%20in%20dimension%20three%20or%20bigger%2C%20and%20give%20counterexamples%20in%0Adimension%20two.%20Excluding%20the%20rare%20situations%20where%20the%20counterexamples%20arise%2C%0Awe%20also%20give%20an%20affirmative%20answer%20in%20dimension%20two.%20Our%20results%20lead%20to%20a%0Asimple%20path%20reconstruction%20algorithm%20for%20a%20vehicle%20carrying%20four%20microphones%0Anavigating%20within%20an%20environment%20in%20which%20a%20loudspeaker%20at%20a%20fixed%20position%0Aemits%20short%20bursts%20of%20sounds.%20This%20algorithm%20could%20be%20combined%20with%20other%0Amethods%20from%20the%20literature%20to%20construct%20a%20path%20tracking%20system%20for%20vehicles%0Anavigating%20within%20a%20potentially%20symmetric%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00698v1&entry.124074799=Read"},
{"title": "Adversarial Testing for Visual Grounding via Image-Aware Property\n  Reduction", "author": "Zhiyuan Chang and Mingyang Li and Junjie Wang and Cheng Li and Boyu Wu and Fanjiang Xu and Qing Wang", "abstract": "  Due to the advantages of fusing information from various modalities,\nmultimodal learning is gaining increasing attention. Being a fundamental task\nof multimodal learning, Visual Grounding (VG), aims to locate objects in images\nthrough natural language expressions. Ensuring the quality of VG models\npresents significant challenges due to the complex nature of the task. In the\nblack box scenario, existing adversarial testing techniques often fail to fully\nexploit the potential of both modalities of information. They typically apply\nperturbations based solely on either the image or text information,\ndisregarding the crucial correlation between the two modalities, which would\nlead to failures in test oracles or an inability to effectively challenge VG\nmodels. To this end, we propose PEELING, a text perturbation approach via\nimage-aware property reduction for adversarial testing of the VG model. The\ncore idea is to reduce the property-related information in the original\nexpression meanwhile ensuring the reduced expression can still uniquely\ndescribe the original object in the image. To achieve this, PEELING first\nconducts the object and properties extraction and recombination to generate\ncandidate property reduction expressions. It then selects the satisfied\nexpressions that accurately describe the original object while ensuring no\nother objects in the image fulfill the expression, through querying the image\nwith a visual understanding technique. We evaluate PEELING on the\nstate-of-the-art VG model, i.e. OFA-VG, involving three commonly used datasets.\nResults show that the adversarial tests generated by PEELING achieves 21.4% in\nMultiModal Impact score (MMI), and outperforms state-of-the-art baselines for\nimages and texts by 8.2%--15.1%.\n", "link": "http://arxiv.org/abs/2403.01118v1", "date": "2024-03-02", "relevancy": 2.0506, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5222}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5185}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5007}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Testing%20for%20Visual%20Grounding%20via%20Image-Aware%20Property%0A%20%20Reduction&entry.906535625=Zhiyuan%20Chang%20and%20Mingyang%20Li%20and%20Junjie%20Wang%20and%20Cheng%20Li%20and%20Boyu%20Wu%20and%20Fanjiang%20Xu%20and%20Qing%20Wang&entry.1292438233=%20%20Due%20to%20the%20advantages%20of%20fusing%20information%20from%20various%20modalities%2C%0Amultimodal%20learning%20is%20gaining%20increasing%20attention.%20Being%20a%20fundamental%20task%0Aof%20multimodal%20learning%2C%20Visual%20Grounding%20%28VG%29%2C%20aims%20to%20locate%20objects%20in%20images%0Athrough%20natural%20language%20expressions.%20Ensuring%20the%20quality%20of%20VG%20models%0Apresents%20significant%20challenges%20due%20to%20the%20complex%20nature%20of%20the%20task.%20In%20the%0Ablack%20box%20scenario%2C%20existing%20adversarial%20testing%20techniques%20often%20fail%20to%20fully%0Aexploit%20the%20potential%20of%20both%20modalities%20of%20information.%20They%20typically%20apply%0Aperturbations%20based%20solely%20on%20either%20the%20image%20or%20text%20information%2C%0Adisregarding%20the%20crucial%20correlation%20between%20the%20two%20modalities%2C%20which%20would%0Alead%20to%20failures%20in%20test%20oracles%20or%20an%20inability%20to%20effectively%20challenge%20VG%0Amodels.%20To%20this%20end%2C%20we%20propose%20PEELING%2C%20a%20text%20perturbation%20approach%20via%0Aimage-aware%20property%20reduction%20for%20adversarial%20testing%20of%20the%20VG%20model.%20The%0Acore%20idea%20is%20to%20reduce%20the%20property-related%20information%20in%20the%20original%0Aexpression%20meanwhile%20ensuring%20the%20reduced%20expression%20can%20still%20uniquely%0Adescribe%20the%20original%20object%20in%20the%20image.%20To%20achieve%20this%2C%20PEELING%20first%0Aconducts%20the%20object%20and%20properties%20extraction%20and%20recombination%20to%20generate%0Acandidate%20property%20reduction%20expressions.%20It%20then%20selects%20the%20satisfied%0Aexpressions%20that%20accurately%20describe%20the%20original%20object%20while%20ensuring%20no%0Aother%20objects%20in%20the%20image%20fulfill%20the%20expression%2C%20through%20querying%20the%20image%0Awith%20a%20visual%20understanding%20technique.%20We%20evaluate%20PEELING%20on%20the%0Astate-of-the-art%20VG%20model%2C%20i.e.%20OFA-VG%2C%20involving%20three%20commonly%20used%20datasets.%0AResults%20show%20that%20the%20adversarial%20tests%20generated%20by%20PEELING%20achieves%2021.4%25%20in%0AMultiModal%20Impact%20score%20%28MMI%29%2C%20and%20outperforms%20state-of-the-art%20baselines%20for%0Aimages%20and%20texts%20by%208.2%25--15.1%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01118v1&entry.124074799=Read"},
{"title": "Leveraging Image Augmentation for Object Manipulation: Towards\n  Interpretable Controllability in Object-Centric Learning", "author": "Jinwoo Kim and Janghyuk Choi and Jaehyun Kang and Changyeon Lee and Ho-Jin Choi and Seon Joo Kim", "abstract": "  The binding problem in artificial neural networks is actively explored with\nthe goal of achieving human-level recognition skills through the comprehension\nof the world in terms of symbol-like entities. Especially in the field of\ncomputer vision, object-centric learning (OCL) is extensively researched to\nbetter understand complex scenes by acquiring object representations or slots.\nWhile recent studies in OCL have made strides with complex images or videos,\nthe interpretability and interactivity over object representation remain\nlargely uncharted, still holding promise in the field of OCL. In this paper, we\nintroduce a novel method, Slot Attention with Image Augmentation (SlotAug), to\nexplore the possibility of learning interpretable controllability over slots in\na self-supervised manner by utilizing an image augmentation strategy. We also\ndevise the concept of sustainability in controllable slots by introducing\niterative and reversible controls over slots with two proposed submethods:\nAuxiliary Identity Manipulation and Slot Consistency Loss. Extensive empirical\nstudies and theoretical validation confirm the effectiveness of our approach,\noffering a novel capability for interpretable and sustainable control of object\nrepresentations.\n", "link": "http://arxiv.org/abs/2310.08929v4", "date": "2024-03-02", "relevancy": 2.1369, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.544}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5291}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5226}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Image%20Augmentation%20for%20Object%20Manipulation%3A%20Towards%0A%20%20Interpretable%20Controllability%20in%20Object-Centric%20Learning&entry.906535625=Jinwoo%20Kim%20and%20Janghyuk%20Choi%20and%20Jaehyun%20Kang%20and%20Changyeon%20Lee%20and%20Ho-Jin%20Choi%20and%20Seon%20Joo%20Kim&entry.1292438233=%20%20The%20binding%20problem%20in%20artificial%20neural%20networks%20is%20actively%20explored%20with%0Athe%20goal%20of%20achieving%20human-level%20recognition%20skills%20through%20the%20comprehension%0Aof%20the%20world%20in%20terms%20of%20symbol-like%20entities.%20Especially%20in%20the%20field%20of%0Acomputer%20vision%2C%20object-centric%20learning%20%28OCL%29%20is%20extensively%20researched%20to%0Abetter%20understand%20complex%20scenes%20by%20acquiring%20object%20representations%20or%20slots.%0AWhile%20recent%20studies%20in%20OCL%20have%20made%20strides%20with%20complex%20images%20or%20videos%2C%0Athe%20interpretability%20and%20interactivity%20over%20object%20representation%20remain%0Alargely%20uncharted%2C%20still%20holding%20promise%20in%20the%20field%20of%20OCL.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20method%2C%20Slot%20Attention%20with%20Image%20Augmentation%20%28SlotAug%29%2C%20to%0Aexplore%20the%20possibility%20of%20learning%20interpretable%20controllability%20over%20slots%20in%0Aa%20self-supervised%20manner%20by%20utilizing%20an%20image%20augmentation%20strategy.%20We%20also%0Adevise%20the%20concept%20of%20sustainability%20in%20controllable%20slots%20by%20introducing%0Aiterative%20and%20reversible%20controls%20over%20slots%20with%20two%20proposed%20submethods%3A%0AAuxiliary%20Identity%20Manipulation%20and%20Slot%20Consistency%20Loss.%20Extensive%20empirical%0Astudies%20and%20theoretical%20validation%20confirm%20the%20effectiveness%20of%20our%20approach%2C%0Aoffering%20a%20novel%20capability%20for%20interpretable%20and%20sustainable%20control%20of%20object%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08929v4&entry.124074799=Read"},
{"title": "Enhancing Group Fairness in Online Settings Using Oblique Decision\n  Forests", "author": "Somnath Basu Roy Chowdhury and Nicholas Monath and Ahmad Beirami and Rahul Kidambi and Avinava Dubey and Amr Ahmed and Snigdha Chaturvedi", "abstract": "  Fairness, especially group fairness, is an important consideration in the\ncontext of machine learning systems. The most commonly adopted group\nfairness-enhancing techniques are in-processing methods that rely on a mixture\nof a fairness objective (e.g., demographic parity) and a task-specific\nobjective (e.g., cross-entropy) during the training process. However, when data\narrives in an online fashion -- one instance at a time -- optimizing such\nfairness objectives poses several challenges. In particular, group fairness\nobjectives are defined using expectations of predictions across different\ndemographic groups. In the online setting, where the algorithm has access to a\nsingle instance at a time, estimating the group fairness objective requires\nadditional storage and significantly more computation (e.g., forward/backward\npasses) than the task-specific objective at every time step. In this paper, we\npropose Aranyani, an ensemble of oblique decision trees, to make fair decisions\nin online settings. The hierarchical tree structure of Aranyani enables\nparameter isolation and allows us to efficiently compute the fairness gradients\nusing aggregate statistics of previous decisions, eliminating the need for\nadditional storage and forward/backward passes. We also present an efficient\nframework to train Aranyani and theoretically analyze several of its\nproperties. We conduct empirical evaluations on 5 publicly available benchmarks\n(including vision and language datasets) to show that Aranyani achieves a\nbetter accuracy-fairness trade-off compared to baseline approaches.\n", "link": "http://arxiv.org/abs/2310.11401v2", "date": "2024-03-01", "relevancy": 1.8614, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4849}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4638}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4591}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Group%20Fairness%20in%20Online%20Settings%20Using%20Oblique%20Decision%0A%20%20Forests&entry.906535625=Somnath%20Basu%20Roy%20Chowdhury%20and%20Nicholas%20Monath%20and%20Ahmad%20Beirami%20and%20Rahul%20Kidambi%20and%20Avinava%20Dubey%20and%20Amr%20Ahmed%20and%20Snigdha%20Chaturvedi&entry.1292438233=%20%20Fairness%2C%20especially%20group%20fairness%2C%20is%20an%20important%20consideration%20in%20the%0Acontext%20of%20machine%20learning%20systems.%20The%20most%20commonly%20adopted%20group%0Afairness-enhancing%20techniques%20are%20in-processing%20methods%20that%20rely%20on%20a%20mixture%0Aof%20a%20fairness%20objective%20%28e.g.%2C%20demographic%20parity%29%20and%20a%20task-specific%0Aobjective%20%28e.g.%2C%20cross-entropy%29%20during%20the%20training%20process.%20However%2C%20when%20data%0Aarrives%20in%20an%20online%20fashion%20--%20one%20instance%20at%20a%20time%20--%20optimizing%20such%0Afairness%20objectives%20poses%20several%20challenges.%20In%20particular%2C%20group%20fairness%0Aobjectives%20are%20defined%20using%20expectations%20of%20predictions%20across%20different%0Ademographic%20groups.%20In%20the%20online%20setting%2C%20where%20the%20algorithm%20has%20access%20to%20a%0Asingle%20instance%20at%20a%20time%2C%20estimating%20the%20group%20fairness%20objective%20requires%0Aadditional%20storage%20and%20significantly%20more%20computation%20%28e.g.%2C%20forward/backward%0Apasses%29%20than%20the%20task-specific%20objective%20at%20every%20time%20step.%20In%20this%20paper%2C%20we%0Apropose%20Aranyani%2C%20an%20ensemble%20of%20oblique%20decision%20trees%2C%20to%20make%20fair%20decisions%0Ain%20online%20settings.%20The%20hierarchical%20tree%20structure%20of%20Aranyani%20enables%0Aparameter%20isolation%20and%20allows%20us%20to%20efficiently%20compute%20the%20fairness%20gradients%0Ausing%20aggregate%20statistics%20of%20previous%20decisions%2C%20eliminating%20the%20need%20for%0Aadditional%20storage%20and%20forward/backward%20passes.%20We%20also%20present%20an%20efficient%0Aframework%20to%20train%20Aranyani%20and%20theoretically%20analyze%20several%20of%20its%0Aproperties.%20We%20conduct%20empirical%20evaluations%20on%205%20publicly%20available%20benchmarks%0A%28including%20vision%20and%20language%20datasets%29%20to%20show%20that%20Aranyani%20achieves%20a%0Abetter%20accuracy-fairness%20trade-off%20compared%20to%20baseline%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11401v2&entry.124074799=Read"},
{"title": "High-Dimensional Tail Index Regression: with An Application to Text\n  Analyses of Viral Posts in Social Media", "author": "Yuya Sasaki and Jing Tao and Yulong Wang", "abstract": "  Motivated by the empirical power law of the distributions of credits (e.g.,\nthe number of \"likes\") of viral posts in social media, we introduce the\nhigh-dimensional tail index regression and methods of estimation and inference\nfor its parameters. We propose a regularized estimator, establish its\nconsistency, and derive its convergence rate. To conduct inference, we propose\nto debias the regularized estimate, and establish the asymptotic normality of\nthe debiased estimator. Simulation studies support our theory. These methods\nare applied to text analyses of viral posts in X (formerly Twitter) concerning\nLGBTQ+.\n", "link": "http://arxiv.org/abs/2403.01318v1", "date": "2024-03-02", "relevancy": 1.8466, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4799}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4667}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4493}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Dimensional%20Tail%20Index%20Regression%3A%20with%20An%20Application%20to%20Text%0A%20%20Analyses%20of%20Viral%20Posts%20in%20Social%20Media&entry.906535625=Yuya%20Sasaki%20and%20Jing%20Tao%20and%20Yulong%20Wang&entry.1292438233=%20%20Motivated%20by%20the%20empirical%20power%20law%20of%20the%20distributions%20of%20credits%20%28e.g.%2C%0Athe%20number%20of%20%22likes%22%29%20of%20viral%20posts%20in%20social%20media%2C%20we%20introduce%20the%0Ahigh-dimensional%20tail%20index%20regression%20and%20methods%20of%20estimation%20and%20inference%0Afor%20its%20parameters.%20We%20propose%20a%20regularized%20estimator%2C%20establish%20its%0Aconsistency%2C%20and%20derive%20its%20convergence%20rate.%20To%20conduct%20inference%2C%20we%20propose%0Ato%20debias%20the%20regularized%20estimate%2C%20and%20establish%20the%20asymptotic%20normality%20of%0Athe%20debiased%20estimator.%20Simulation%20studies%20support%20our%20theory.%20These%20methods%0Aare%20applied%20to%20text%20analyses%20of%20viral%20posts%20in%20X%20%28formerly%20Twitter%29%20concerning%0ALGBTQ%2B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01318v1&entry.124074799=Read"},
{"title": "Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment", "author": "Luyao Wang and Pengnian Qi and Xigang Bao and Chunlai Zhou and Biao Qin", "abstract": "  Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween two multi-modal knowledge graphs for integration. Unfortunately, prior\narts have attempted to improve the interaction and fusion of multi-modal\ninformation, which have overlooked the influence of modal-specific noise and\nthe usage of labeled and unlabeled data in semi-supervised settings. In this\nwork, we introduce a Pseudo-label Calibration Multi-modal Entity Alignment\n(PCMEA) in a semi-supervised way. Specifically, in order to generate holistic\nentity representations, we first devise various embedding modules and attention\nmechanisms to extract visual, structural, relational, and attribute features.\nDifferent from the prior direct fusion methods, we next propose to exploit\nmutual information maximization to filter the modal-specific noise and to\naugment modal-invariant commonality. Then, we combine pseudo-label calibration\nwith momentum-based contrastive learning to make full use of the labeled and\nunlabeled data, which improves the quality of pseudo-label and pulls aligned\nentities closer. Finally, extensive experiments on two MMEA datasets\ndemonstrate the effectiveness of our PCMEA, which yields state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2403.01203v1", "date": "2024-03-02", "relevancy": 1.6846, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5686}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Label%20Calibration%20Semi-supervised%20Multi-Modal%20Entity%20Alignment&entry.906535625=Luyao%20Wang%20and%20Pengnian%20Qi%20and%20Xigang%20Bao%20and%20Chunlai%20Zhou%20and%20Biao%20Qin&entry.1292438233=%20%20Multi-modal%20entity%20alignment%20%28MMEA%29%20aims%20to%20identify%20equivalent%20entities%0Abetween%20two%20multi-modal%20knowledge%20graphs%20for%20integration.%20Unfortunately%2C%20prior%0Aarts%20have%20attempted%20to%20improve%20the%20interaction%20and%20fusion%20of%20multi-modal%0Ainformation%2C%20which%20have%20overlooked%20the%20influence%20of%20modal-specific%20noise%20and%0Athe%20usage%20of%20labeled%20and%20unlabeled%20data%20in%20semi-supervised%20settings.%20In%20this%0Awork%2C%20we%20introduce%20a%20Pseudo-label%20Calibration%20Multi-modal%20Entity%20Alignment%0A%28PCMEA%29%20in%20a%20semi-supervised%20way.%20Specifically%2C%20in%20order%20to%20generate%20holistic%0Aentity%20representations%2C%20we%20first%20devise%20various%20embedding%20modules%20and%20attention%0Amechanisms%20to%20extract%20visual%2C%20structural%2C%20relational%2C%20and%20attribute%20features.%0ADifferent%20from%20the%20prior%20direct%20fusion%20methods%2C%20we%20next%20propose%20to%20exploit%0Amutual%20information%20maximization%20to%20filter%20the%20modal-specific%20noise%20and%20to%0Aaugment%20modal-invariant%20commonality.%20Then%2C%20we%20combine%20pseudo-label%20calibration%0Awith%20momentum-based%20contrastive%20learning%20to%20make%20full%20use%20of%20the%20labeled%20and%0Aunlabeled%20data%2C%20which%20improves%20the%20quality%20of%20pseudo-label%20and%20pulls%20aligned%0Aentities%20closer.%20Finally%2C%20extensive%20experiments%20on%20two%20MMEA%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20PCMEA%2C%20which%20yields%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01203v1&entry.124074799=Read"},
{"title": "Relaxometry Guided Quantitative Cardiac Magnetic Resonance Image\n  Reconstruction", "author": "Yidong Zhao and Yi Zhang and Qian Tao", "abstract": "  Deep learning-based methods have achieved prestigious performance for\nmagnetic resonance imaging (MRI) reconstruction, enabling fast imaging for many\nclinical applications. Previous methods employ convolutional networks to learn\nthe image prior as the regularization term. In quantitative MRI, the physical\nmodel of nuclear magnetic resonance relaxometry is known, providing additional\nprior knowledge for image reconstruction. However, traditional reconstruction\nnetworks are limited to learning the spatial domain prior knowledge, ignoring\nthe relaxometry prior. Therefore, we propose a relaxometry-guided quantitative\nMRI reconstruction framework to learn the spatial prior from data and the\nrelaxometry prior from MRI physics. Additionally, we also evaluated the\nperformance of two popular reconstruction backbones, namely, recurrent\nvariational networks (RVN) and variational networks (VN) with U- Net.\nExperiments demonstrate that the proposed method achieves highly promising\nresults in quantitative MRI reconstruction.\n", "link": "http://arxiv.org/abs/2403.00549v1", "date": "2024-03-01", "relevancy": 1.9533, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5095}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4834}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4691}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relaxometry%20Guided%20Quantitative%20Cardiac%20Magnetic%20Resonance%20Image%0A%20%20Reconstruction&entry.906535625=Yidong%20Zhao%20and%20Yi%20Zhang%20and%20Qian%20Tao&entry.1292438233=%20%20Deep%20learning-based%20methods%20have%20achieved%20prestigious%20performance%20for%0Amagnetic%20resonance%20imaging%20%28MRI%29%20reconstruction%2C%20enabling%20fast%20imaging%20for%20many%0Aclinical%20applications.%20Previous%20methods%20employ%20convolutional%20networks%20to%20learn%0Athe%20image%20prior%20as%20the%20regularization%20term.%20In%20quantitative%20MRI%2C%20the%20physical%0Amodel%20of%20nuclear%20magnetic%20resonance%20relaxometry%20is%20known%2C%20providing%20additional%0Aprior%20knowledge%20for%20image%20reconstruction.%20However%2C%20traditional%20reconstruction%0Anetworks%20are%20limited%20to%20learning%20the%20spatial%20domain%20prior%20knowledge%2C%20ignoring%0Athe%20relaxometry%20prior.%20Therefore%2C%20we%20propose%20a%20relaxometry-guided%20quantitative%0AMRI%20reconstruction%20framework%20to%20learn%20the%20spatial%20prior%20from%20data%20and%20the%0Arelaxometry%20prior%20from%20MRI%20physics.%20Additionally%2C%20we%20also%20evaluated%20the%0Aperformance%20of%20two%20popular%20reconstruction%20backbones%2C%20namely%2C%20recurrent%0Avariational%20networks%20%28RVN%29%20and%20variational%20networks%20%28VN%29%20with%20U-%20Net.%0AExperiments%20demonstrate%20that%20the%20proposed%20method%20achieves%20highly%20promising%0Aresults%20in%20quantitative%20MRI%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00549v1&entry.124074799=Read"},
{"title": "API Is Enough: Conformal Prediction for Large Language Models Without\n  Logit-Access", "author": "Jiayuan Su and Jing Luo and Hongwei Wang and Lu Cheng", "abstract": "  This study aims to address the pervasive challenge of quantifying uncertainty\nin large language models (LLMs) without logit-access. Conformal Prediction\n(CP), known for its model-agnostic and distribution-free features, is a desired\napproach for various LLMs and data distributions. However, existing CP methods\nfor LLMs typically assume access to the logits, which are unavailable for some\nAPI-only LLMs. In addition, logits are known to be miscalibrated, potentially\nleading to degraded CP performance. To tackle these challenges, we introduce a\nnovel CP method that (1) is tailored for API-only LLMs without logit-access;\n(2) minimizes the size of prediction sets; and (3) ensures a statistical\nguarantee of the user-defined coverage. The core idea of this approach is to\nformulate nonconformity measures using both coarse-grained (i.e., sample\nfrequency) and fine-grained uncertainty notions (e.g., semantic similarity).\nExperimental results on both close-ended and open-ended Question Answering\ntasks show our approach can mostly outperform the logit-based CP baselines.\n", "link": "http://arxiv.org/abs/2403.01216v1", "date": "2024-03-02", "relevancy": 2.1162, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5358}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5324}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.523}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=API%20Is%20Enough%3A%20Conformal%20Prediction%20for%20Large%20Language%20Models%20Without%0A%20%20Logit-Access&entry.906535625=Jiayuan%20Su%20and%20Jing%20Luo%20and%20Hongwei%20Wang%20and%20Lu%20Cheng&entry.1292438233=%20%20This%20study%20aims%20to%20address%20the%20pervasive%20challenge%20of%20quantifying%20uncertainty%0Ain%20large%20language%20models%20%28LLMs%29%20without%20logit-access.%20Conformal%20Prediction%0A%28CP%29%2C%20known%20for%20its%20model-agnostic%20and%20distribution-free%20features%2C%20is%20a%20desired%0Aapproach%20for%20various%20LLMs%20and%20data%20distributions.%20However%2C%20existing%20CP%20methods%0Afor%20LLMs%20typically%20assume%20access%20to%20the%20logits%2C%20which%20are%20unavailable%20for%20some%0AAPI-only%20LLMs.%20In%20addition%2C%20logits%20are%20known%20to%20be%20miscalibrated%2C%20potentially%0Aleading%20to%20degraded%20CP%20performance.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20a%0Anovel%20CP%20method%20that%20%281%29%20is%20tailored%20for%20API-only%20LLMs%20without%20logit-access%3B%0A%282%29%20minimizes%20the%20size%20of%20prediction%20sets%3B%20and%20%283%29%20ensures%20a%20statistical%0Aguarantee%20of%20the%20user-defined%20coverage.%20The%20core%20idea%20of%20this%20approach%20is%20to%0Aformulate%20nonconformity%20measures%20using%20both%20coarse-grained%20%28i.e.%2C%20sample%0Afrequency%29%20and%20fine-grained%20uncertainty%20notions%20%28e.g.%2C%20semantic%20similarity%29.%0AExperimental%20results%20on%20both%20close-ended%20and%20open-ended%20Question%20Answering%0Atasks%20show%20our%20approach%20can%20mostly%20outperform%20the%20logit-based%20CP%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01216v1&entry.124074799=Read"},
{"title": "Automated Continuous Force-Torque Sensor Bias Estimation", "author": "Philippe Nadeau and Miguel Rogel Garcia and Emmett Wise and Jonathan Kelly", "abstract": "  Six axis force-torque sensors are commonly attached to the wrist of serial\nrobots to measure the external forces and torques acting on the robot's\nend-effector. These measurements are used for load identification, contact\ndetection, and human-robot interaction amongst other applications. Typically,\nthe measurements obtained from the force-torque sensor are more accurate than\nestimates computed from joint torque readings, as the former is independent of\nthe robot's dynamic and kinematic models. However, the force-torque sensor\nmeasurements are affected by a bias that drifts over time, caused by the\ncompounding effects of temperature changes, mechanical stresses, and other\nfactors. In this work, we present a pipeline that continuously estimates the\nbias and the drift of the bias of a force-torque sensor attached to the wrist\nof a robot. The first component of the pipeline is a Kalman filter that\nestimates the kinematic state (position, velocity, and acceleration) of the\nrobot's joints. The second component is a kinematic model that maps the\njoint-space kinematics to the task-space kinematics of the force-torque sensor.\nFinally, the third component is a Kalman filter that estimates the bias and the\ndrift of the bias of the force-torque sensor assuming that the inertial\nparameters of the gripper attached to the distal end of the force-torque sensor\nare known with certainty.\n", "link": "http://arxiv.org/abs/2403.01068v1", "date": "2024-03-02", "relevancy": 1.972, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5599}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4926}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4667}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Continuous%20Force-Torque%20Sensor%20Bias%20Estimation&entry.906535625=Philippe%20Nadeau%20and%20Miguel%20Rogel%20Garcia%20and%20Emmett%20Wise%20and%20Jonathan%20Kelly&entry.1292438233=%20%20Six%20axis%20force-torque%20sensors%20are%20commonly%20attached%20to%20the%20wrist%20of%20serial%0Arobots%20to%20measure%20the%20external%20forces%20and%20torques%20acting%20on%20the%20robot%27s%0Aend-effector.%20These%20measurements%20are%20used%20for%20load%20identification%2C%20contact%0Adetection%2C%20and%20human-robot%20interaction%20amongst%20other%20applications.%20Typically%2C%0Athe%20measurements%20obtained%20from%20the%20force-torque%20sensor%20are%20more%20accurate%20than%0Aestimates%20computed%20from%20joint%20torque%20readings%2C%20as%20the%20former%20is%20independent%20of%0Athe%20robot%27s%20dynamic%20and%20kinematic%20models.%20However%2C%20the%20force-torque%20sensor%0Ameasurements%20are%20affected%20by%20a%20bias%20that%20drifts%20over%20time%2C%20caused%20by%20the%0Acompounding%20effects%20of%20temperature%20changes%2C%20mechanical%20stresses%2C%20and%20other%0Afactors.%20In%20this%20work%2C%20we%20present%20a%20pipeline%20that%20continuously%20estimates%20the%0Abias%20and%20the%20drift%20of%20the%20bias%20of%20a%20force-torque%20sensor%20attached%20to%20the%20wrist%0Aof%20a%20robot.%20The%20first%20component%20of%20the%20pipeline%20is%20a%20Kalman%20filter%20that%0Aestimates%20the%20kinematic%20state%20%28position%2C%20velocity%2C%20and%20acceleration%29%20of%20the%0Arobot%27s%20joints.%20The%20second%20component%20is%20a%20kinematic%20model%20that%20maps%20the%0Ajoint-space%20kinematics%20to%20the%20task-space%20kinematics%20of%20the%20force-torque%20sensor.%0AFinally%2C%20the%20third%20component%20is%20a%20Kalman%20filter%20that%20estimates%20the%20bias%20and%20the%0Adrift%20of%20the%20bias%20of%20the%20force-torque%20sensor%20assuming%20that%20the%20inertial%0Aparameters%20of%20the%20gripper%20attached%20to%20the%20distal%20end%20of%20the%20force-torque%20sensor%0Aare%20known%20with%20certainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01068v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


