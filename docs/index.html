<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Self-supervised Photographic Image Layout Representation Learning", "author": "Zhaoran Zhao and Peng Lu and Xujun Peng and Wenhao Guo", "abstract": "  In the domain of image layout representation learning, the critical process\nof translating image layouts into succinct vector forms is increasingly\nsignificant across diverse applications, such as image retrieval, manipulation,\nand generation. Most approaches in this area heavily rely on costly labeled\ndatasets and notably lack in adapting their modeling and learning methods to\nthe specific nuances of photographic image layouts. This shortfall makes the\nlearning process for photographic image layouts suboptimal. In our research, we\ndirectly address these challenges. We innovate by defining basic layout\nprimitives that encapsulate various levels of layout information and by mapping\nthese, along with their interconnections, onto a heterogeneous graph structure.\nThis graph is meticulously engineered to capture the intricate layout\ninformation within the pixel domain explicitly. Advancing further, we introduce\nnovel pretext tasks coupled with customized loss functions, strategically\ndesigned for effective self-supervised learning of these layout graphs.\nBuilding on this foundation, we develop an autoencoder-based network\narchitecture skilled in compressing these heterogeneous layout graphs into\nprecise, dimensionally-reduced layout representations. Additionally, we\nintroduce the LODB dataset, which features a broader range of layout categories\nand richer semantics, serving as a comprehensive benchmark for evaluating the\neffectiveness of layout representation learning methods. Our extensive\nexperimentation on this dataset demonstrates the superior performance of our\napproach in the realm of photographic image layout representation learning.\n", "link": "http://arxiv.org/abs/2403.03740v1", "date": "2024-03-06", "relevancy": 2.7954, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5663}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5617}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5492}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Photographic%20Image%20Layout%20Representation%20Learning&entry.906535625=Zhaoran%20Zhao%20and%20Peng%20Lu%20and%20Xujun%20Peng%20and%20Wenhao%20Guo&entry.1292438233=%20%20In%20the%20domain%20of%20image%20layout%20representation%20learning%2C%20the%20critical%20process%0Aof%20translating%20image%20layouts%20into%20succinct%20vector%20forms%20is%20increasingly%0Asignificant%20across%20diverse%20applications%2C%20such%20as%20image%20retrieval%2C%20manipulation%2C%0Aand%20generation.%20Most%20approaches%20in%20this%20area%20heavily%20rely%20on%20costly%20labeled%0Adatasets%20and%20notably%20lack%20in%20adapting%20their%20modeling%20and%20learning%20methods%20to%0Athe%20specific%20nuances%20of%20photographic%20image%20layouts.%20This%20shortfall%20makes%20the%0Alearning%20process%20for%20photographic%20image%20layouts%20suboptimal.%20In%20our%20research%2C%20we%0Adirectly%20address%20these%20challenges.%20We%20innovate%20by%20defining%20basic%20layout%0Aprimitives%20that%20encapsulate%20various%20levels%20of%20layout%20information%20and%20by%20mapping%0Athese%2C%20along%20with%20their%20interconnections%2C%20onto%20a%20heterogeneous%20graph%20structure.%0AThis%20graph%20is%20meticulously%20engineered%20to%20capture%20the%20intricate%20layout%0Ainformation%20within%20the%20pixel%20domain%20explicitly.%20Advancing%20further%2C%20we%20introduce%0Anovel%20pretext%20tasks%20coupled%20with%20customized%20loss%20functions%2C%20strategically%0Adesigned%20for%20effective%20self-supervised%20learning%20of%20these%20layout%20graphs.%0ABuilding%20on%20this%20foundation%2C%20we%20develop%20an%20autoencoder-based%20network%0Aarchitecture%20skilled%20in%20compressing%20these%20heterogeneous%20layout%20graphs%20into%0Aprecise%2C%20dimensionally-reduced%20layout%20representations.%20Additionally%2C%20we%0Aintroduce%20the%20LODB%20dataset%2C%20which%20features%20a%20broader%20range%20of%20layout%20categories%0Aand%20richer%20semantics%2C%20serving%20as%20a%20comprehensive%20benchmark%20for%20evaluating%20the%0Aeffectiveness%20of%20layout%20representation%20learning%20methods.%20Our%20extensive%0Aexperimentation%20on%20this%20dataset%20demonstrates%20the%20superior%20performance%20of%20our%0Aapproach%20in%20the%20realm%20of%20photographic%20image%20layout%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03740v1&entry.124074799=Read"},
{"title": "Bilateral Reference for High-Resolution Dichotomous Image Segmentation", "author": "Peng Zheng and Dehong Gao and Deng-Ping Fan and Li Liu and Jorma Laaksonen and Wanli Ouyang and Nicu Sebe", "abstract": "  We introduce a novel bilateral reference framework (BiRefNet) for\nhigh-resolution dichotomous image segmentation (DIS). It comprises two\nessential components: the localization module (LM) and the reconstruction\nmodule (RM) with our proposed bilateral reference (BiRef). The LM aids in\nobject localization using global semantic information. Within the RM, we\nutilize BiRef for the reconstruction process, where hierarchical patches of\nimages provide the source reference and gradient maps serve as the target\nreference. These components collaborate to generate the final predicted maps.\nWe also introduce auxiliary gradient supervision to enhance focus on regions\nwith finer details. Furthermore, we outline practical training strategies\ntailored for DIS to improve map quality and training process. To validate the\ngeneral applicability of our approach, we conduct extensive experiments on four\ntasks to evince that BiRefNet exhibits remarkable performance, outperforming\ntask-specific cutting-edge methods across all benchmarks. Our codes are\navailable at https://github.com/ZhengPeng7/BiRefNet.\n", "link": "http://arxiv.org/abs/2401.03407v2", "date": "2024-03-06", "relevancy": 2.7433, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5244}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5066}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bilateral%20Reference%20for%20High-Resolution%20Dichotomous%20Image%20Segmentation&entry.906535625=Peng%20Zheng%20and%20Dehong%20Gao%20and%20Deng-Ping%20Fan%20and%20Li%20Liu%20and%20Jorma%20Laaksonen%20and%20Wanli%20Ouyang%20and%20Nicu%20Sebe&entry.1292438233=%20%20We%20introduce%20a%20novel%20bilateral%20reference%20framework%20%28BiRefNet%29%20for%0Ahigh-resolution%20dichotomous%20image%20segmentation%20%28DIS%29.%20It%20comprises%20two%0Aessential%20components%3A%20the%20localization%20module%20%28LM%29%20and%20the%20reconstruction%0Amodule%20%28RM%29%20with%20our%20proposed%20bilateral%20reference%20%28BiRef%29.%20The%20LM%20aids%20in%0Aobject%20localization%20using%20global%20semantic%20information.%20Within%20the%20RM%2C%20we%0Autilize%20BiRef%20for%20the%20reconstruction%20process%2C%20where%20hierarchical%20patches%20of%0Aimages%20provide%20the%20source%20reference%20and%20gradient%20maps%20serve%20as%20the%20target%0Areference.%20These%20components%20collaborate%20to%20generate%20the%20final%20predicted%20maps.%0AWe%20also%20introduce%20auxiliary%20gradient%20supervision%20to%20enhance%20focus%20on%20regions%0Awith%20finer%20details.%20Furthermore%2C%20we%20outline%20practical%20training%20strategies%0Atailored%20for%20DIS%20to%20improve%20map%20quality%20and%20training%20process.%20To%20validate%20the%0Ageneral%20applicability%20of%20our%20approach%2C%20we%20conduct%20extensive%20experiments%20on%20four%0Atasks%20to%20evince%20that%20BiRefNet%20exhibits%20remarkable%20performance%2C%20outperforming%0Atask-specific%20cutting-edge%20methods%20across%20all%20benchmarks.%20Our%20codes%20are%0Aavailable%20at%20https%3A//github.com/ZhengPeng7/BiRefNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03407v2&entry.124074799=Read"},
{"title": "Out-of-Distribution Detection using Neural Activation Prior", "author": "Weilin Wan and Weizhong Zhang and Cheng Jin", "abstract": "  Out-of-distribution detection (OOD) is a crucial technique for deploying\nmachine learning models in the real world to handle the unseen scenarios. In\nthis paper, we first propose a simple yet effective Neural Activation Prior\n(NAP) for OOD detection. Our neural activation prior is based on a key\nobservation that, for a channel before the global pooling layer of a fully\ntrained neural network, the probability of a few neurons being activated with a\nlarge response by an in-distribution (ID) sample is significantly higher than\nthat by an OOD sample. An intuitive explanation is that for a model fully\ntrained on ID dataset, each channel would play a role in detecting a certain\npattern in the ID dataset, and a few neurons can be activated with a large\nresponse when the pattern is detected in an input sample. Then, a new scoring\nfunction based on this prior is proposed to highlight the role of these\nstrongly activated neurons in OOD detection. Our approach is plug-and-play and\ndoes not lead to any performance degradation on ID data classification and\nrequires no extra training or statistics from training or external datasets.\nNotice that previous methods primarily rely on post-global-pooling features of\nthe neural networks, while the within-channel distribution information we\nleverage would be discarded by the global pooling operator. Consequently, our\nmethod is orthogonal to existing approaches and can be effectively combined\nwith them in various applications. Experimental results show that our method\nachieves the state-of-the-art performance on CIFAR benchmark and ImageNet\ndataset, which demonstrates the power of the proposed prior. Finally, we extend\nour method to Transformers and the experimental findings indicate that NAP can\nalso significantly enhance the performance of OOD detection on Transformers,\nthereby demonstrating the broad applicability of this prior knowledge.\n", "link": "http://arxiv.org/abs/2402.18162v3", "date": "2024-03-06", "relevancy": 2.5941, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5449}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5217}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4898}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-Distribution%20Detection%20using%20Neural%20Activation%20Prior&entry.906535625=Weilin%20Wan%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin&entry.1292438233=%20%20Out-of-distribution%20detection%20%28OOD%29%20is%20a%20crucial%20technique%20for%20deploying%0Amachine%20learning%20models%20in%20the%20real%20world%20to%20handle%20the%20unseen%20scenarios.%20In%0Athis%20paper%2C%20we%20first%20propose%20a%20simple%20yet%20effective%20Neural%20Activation%20Prior%0A%28NAP%29%20for%20OOD%20detection.%20Our%20neural%20activation%20prior%20is%20based%20on%20a%20key%0Aobservation%20that%2C%20for%20a%20channel%20before%20the%20global%20pooling%20layer%20of%20a%20fully%0Atrained%20neural%20network%2C%20the%20probability%20of%20a%20few%20neurons%20being%20activated%20with%20a%0Alarge%20response%20by%20an%20in-distribution%20%28ID%29%20sample%20is%20significantly%20higher%20than%0Athat%20by%20an%20OOD%20sample.%20An%20intuitive%20explanation%20is%20that%20for%20a%20model%20fully%0Atrained%20on%20ID%20dataset%2C%20each%20channel%20would%20play%20a%20role%20in%20detecting%20a%20certain%0Apattern%20in%20the%20ID%20dataset%2C%20and%20a%20few%20neurons%20can%20be%20activated%20with%20a%20large%0Aresponse%20when%20the%20pattern%20is%20detected%20in%20an%20input%20sample.%20Then%2C%20a%20new%20scoring%0Afunction%20based%20on%20this%20prior%20is%20proposed%20to%20highlight%20the%20role%20of%20these%0Astrongly%20activated%20neurons%20in%20OOD%20detection.%20Our%20approach%20is%20plug-and-play%20and%0Adoes%20not%20lead%20to%20any%20performance%20degradation%20on%20ID%20data%20classification%20and%0Arequires%20no%20extra%20training%20or%20statistics%20from%20training%20or%20external%20datasets.%0ANotice%20that%20previous%20methods%20primarily%20rely%20on%20post-global-pooling%20features%20of%0Athe%20neural%20networks%2C%20while%20the%20within-channel%20distribution%20information%20we%0Aleverage%20would%20be%20discarded%20by%20the%20global%20pooling%20operator.%20Consequently%2C%20our%0Amethod%20is%20orthogonal%20to%20existing%20approaches%20and%20can%20be%20effectively%20combined%0Awith%20them%20in%20various%20applications.%20Experimental%20results%20show%20that%20our%20method%0Aachieves%20the%20state-of-the-art%20performance%20on%20CIFAR%20benchmark%20and%20ImageNet%0Adataset%2C%20which%20demonstrates%20the%20power%20of%20the%20proposed%20prior.%20Finally%2C%20we%20extend%0Aour%20method%20to%20Transformers%20and%20the%20experimental%20findings%20indicate%20that%20NAP%20can%0Aalso%20significantly%20enhance%20the%20performance%20of%20OOD%20detection%20on%20Transformers%2C%0Athereby%20demonstrating%20the%20broad%20applicability%20of%20this%20prior%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18162v3&entry.124074799=Read"},
{"title": "Improving Adversarial Attacks on Latent Diffusion Model", "author": "Boyang Zheng and Chumeng Liang and Xiaoyu Wu and Yan Liu", "abstract": "  Adversarial attacks on Latent Diffusion Model (LDM), the state-of-the-art\nimage generative model, have been adopted as effective protection against\nmalicious finetuning of LDM on unauthorized images. We show that these attacks\nadd an extra error to the score function of adversarial examples predicted by\nLDM. LDM finetuned on these adversarial examples learns to lower the error by a\nbias, from which the model is attacked and predicts the score function with\nbiases.\n  Based on the dynamics, we propose to improve the adversarial attack on LDM by\nAttacking with Consistent score-function Errors (ACE). ACE unifies the pattern\nof the extra error added to the predicted score function. This induces the\nfinetuned LDM to learn the same pattern as a bias in predicting the score\nfunction. We then introduce a well-crafted pattern to improve the attack. Our\nmethod outperforms state-of-the-art methods in adversarial attacks on LDM.\n", "link": "http://arxiv.org/abs/2310.04687v3", "date": "2024-03-06", "relevancy": 2.5899, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5281}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5129}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5129}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Adversarial%20Attacks%20on%20Latent%20Diffusion%20Model&entry.906535625=Boyang%20Zheng%20and%20Chumeng%20Liang%20and%20Xiaoyu%20Wu%20and%20Yan%20Liu&entry.1292438233=%20%20Adversarial%20attacks%20on%20Latent%20Diffusion%20Model%20%28LDM%29%2C%20the%20state-of-the-art%0Aimage%20generative%20model%2C%20have%20been%20adopted%20as%20effective%20protection%20against%0Amalicious%20finetuning%20of%20LDM%20on%20unauthorized%20images.%20We%20show%20that%20these%20attacks%0Aadd%20an%20extra%20error%20to%20the%20score%20function%20of%20adversarial%20examples%20predicted%20by%0ALDM.%20LDM%20finetuned%20on%20these%20adversarial%20examples%20learns%20to%20lower%20the%20error%20by%20a%0Abias%2C%20from%20which%20the%20model%20is%20attacked%20and%20predicts%20the%20score%20function%20with%0Abiases.%0A%20%20Based%20on%20the%20dynamics%2C%20we%20propose%20to%20improve%20the%20adversarial%20attack%20on%20LDM%20by%0AAttacking%20with%20Consistent%20score-function%20Errors%20%28ACE%29.%20ACE%20unifies%20the%20pattern%0Aof%20the%20extra%20error%20added%20to%20the%20predicted%20score%20function.%20This%20induces%20the%0Afinetuned%20LDM%20to%20learn%20the%20same%20pattern%20as%20a%20bias%20in%20predicting%20the%20score%0Afunction.%20We%20then%20introduce%20a%20well-crafted%20pattern%20to%20improve%20the%20attack.%20Our%0Amethod%20outperforms%20state-of-the-art%20methods%20in%20adversarial%20attacks%20on%20LDM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04687v3&entry.124074799=Read"},
{"title": "Q&A Prompts: Discovering Rich Visual Clues through Mining\n  Question-Answer Prompts for VQA requiring Diverse World Knowledge", "author": "Haibi Wang and Weifeng Ge", "abstract": "  With the breakthrough of multi-modal large language models, answering complex\nvisual questions that demand advanced reasoning abilities and world knowledge\nhas become a much more important testbed for developing AI models than ever.\nHowever, equipping AI models with robust cross-modality reasoning ability\nremains challenging since the cognition scheme of humans has not been\nunderstood systematically. In this paper, we believe that if we can collect\nvisual clues in the given image as much as possible, we will recognize the\nimage more accurately, understand the question better, recall relevant\nknowledge more easily, and finally reason out the answer. We discover these\nrich visual clues by mining question-answer pairs in images and sending them\ninto multi-modal large language models as prompts. We call the proposed method\nQ&A Prompts. Specifically, we first use the image-answer pairs and the\ncorresponding questions in the training set as inputs and outputs to train a\nvisual question generation model. Then, we use an image tagging model to\nidentify various instances and send packaged image-tag pairs into the visual\nquestion generation model to generate relevant questions with the extracted\nimage tags as answers. Finally, we encode these generated question-answer pairs\nas prompts with a visual-aware prompting module and send them into pre-trained\nmulti-modal large language models to reason out the final answers. Experimental\nresults show that, compared with state-of-the-art methods, our Q&A Prompts\nachieves substantial improvements on the challenging visual question answering\ndatasets requiring reasoning over diverse world knowledge, such as OK-VQA and\nA-OKVQA.\n", "link": "http://arxiv.org/abs/2401.10712v2", "date": "2024-03-06", "relevancy": 2.5684, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5162}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5073}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q%26A%20Prompts%3A%20Discovering%20Rich%20Visual%20Clues%20through%20Mining%0A%20%20Question-Answer%20Prompts%20for%20VQA%20requiring%20Diverse%20World%20Knowledge&entry.906535625=Haibi%20Wang%20and%20Weifeng%20Ge&entry.1292438233=%20%20With%20the%20breakthrough%20of%20multi-modal%20large%20language%20models%2C%20answering%20complex%0Avisual%20questions%20that%20demand%20advanced%20reasoning%20abilities%20and%20world%20knowledge%0Ahas%20become%20a%20much%20more%20important%20testbed%20for%20developing%20AI%20models%20than%20ever.%0AHowever%2C%20equipping%20AI%20models%20with%20robust%20cross-modality%20reasoning%20ability%0Aremains%20challenging%20since%20the%20cognition%20scheme%20of%20humans%20has%20not%20been%0Aunderstood%20systematically.%20In%20this%20paper%2C%20we%20believe%20that%20if%20we%20can%20collect%0Avisual%20clues%20in%20the%20given%20image%20as%20much%20as%20possible%2C%20we%20will%20recognize%20the%0Aimage%20more%20accurately%2C%20understand%20the%20question%20better%2C%20recall%20relevant%0Aknowledge%20more%20easily%2C%20and%20finally%20reason%20out%20the%20answer.%20We%20discover%20these%0Arich%20visual%20clues%20by%20mining%20question-answer%20pairs%20in%20images%20and%20sending%20them%0Ainto%20multi-modal%20large%20language%20models%20as%20prompts.%20We%20call%20the%20proposed%20method%0AQ%26A%20Prompts.%20Specifically%2C%20we%20first%20use%20the%20image-answer%20pairs%20and%20the%0Acorresponding%20questions%20in%20the%20training%20set%20as%20inputs%20and%20outputs%20to%20train%20a%0Avisual%20question%20generation%20model.%20Then%2C%20we%20use%20an%20image%20tagging%20model%20to%0Aidentify%20various%20instances%20and%20send%20packaged%20image-tag%20pairs%20into%20the%20visual%0Aquestion%20generation%20model%20to%20generate%20relevant%20questions%20with%20the%20extracted%0Aimage%20tags%20as%20answers.%20Finally%2C%20we%20encode%20these%20generated%20question-answer%20pairs%0Aas%20prompts%20with%20a%20visual-aware%20prompting%20module%20and%20send%20them%20into%20pre-trained%0Amulti-modal%20large%20language%20models%20to%20reason%20out%20the%20final%20answers.%20Experimental%0Aresults%20show%20that%2C%20compared%20with%20state-of-the-art%20methods%2C%20our%20Q%26A%20Prompts%0Aachieves%20substantial%20improvements%20on%20the%20challenging%20visual%20question%20answering%0Adatasets%20requiring%20reasoning%20over%20diverse%20world%20knowledge%2C%20such%20as%20OK-VQA%20and%0AA-OKVQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10712v2&entry.124074799=Read"},
{"title": "A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain\n  Recommendation", "author": "Li Wang and Lei Sang and Quangui Zhang and Qiang Wu and Min Xu", "abstract": "  Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in\na target domain with sparse data by leveraging rich information in a source\ndomain, thereby addressing the data-sparsity problem. Some existing CDR methods\nhighlight the advantages of extracting domain-common and domain-specific\nfeatures to learn comprehensive user and item representations. However, these\nmethods can't effectively disentangle these components as they often rely on\nsimple user-item historical interaction information (such as ratings, clicks,\nand browsing), neglecting the rich multi-modal features. Additionally, they\ndon't protect user-sensitive data from potential leakage during knowledge\ntransfer between domains. To address these challenges, we propose a\nPrivacy-Preserving Framework with Multi-Modal Data for Cross-Domain\nRecommendation, called P2M2-CDR. Specifically, we first design a multi-modal\ndisentangled encoder that utilizes multi-modal information to disentangle more\ninformative domain-common and domain-specific embeddings. Furthermore, we\nintroduce a privacy-preserving decoder to mitigate user privacy leakage during\nknowledge transfer. Local differential privacy (LDP) is utilized to obfuscate\nthe disentangled embeddings before inter-domain exchange, thereby enhancing\nprivacy protection. To ensure both consistency and differentiation among these\nobfuscated disentangled embeddings, we incorporate contrastive learning-based\ndomain-inter and domain-intra losses. Extensive Experiments conducted on four\nreal-world datasets demonstrate that P2M2-CDR outperforms other\nstate-of-the-art single-domain and cross-domain baselines.\n", "link": "http://arxiv.org/abs/2403.03600v1", "date": "2024-03-06", "relevancy": 2.5337, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.52}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5001}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5001}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Privacy-Preserving%20Framework%20with%20Multi-Modal%20Data%20for%20Cross-Domain%0A%20%20Recommendation&entry.906535625=Li%20Wang%20and%20Lei%20Sang%20and%20Quangui%20Zhang%20and%20Qiang%20Wu%20and%20Min%20Xu&entry.1292438233=%20%20Cross-domain%20recommendation%20%28CDR%29%20aims%20to%20enhance%20recommendation%20accuracy%20in%0Aa%20target%20domain%20with%20sparse%20data%20by%20leveraging%20rich%20information%20in%20a%20source%0Adomain%2C%20thereby%20addressing%20the%20data-sparsity%20problem.%20Some%20existing%20CDR%20methods%0Ahighlight%20the%20advantages%20of%20extracting%20domain-common%20and%20domain-specific%0Afeatures%20to%20learn%20comprehensive%20user%20and%20item%20representations.%20However%2C%20these%0Amethods%20can%27t%20effectively%20disentangle%20these%20components%20as%20they%20often%20rely%20on%0Asimple%20user-item%20historical%20interaction%20information%20%28such%20as%20ratings%2C%20clicks%2C%0Aand%20browsing%29%2C%20neglecting%20the%20rich%20multi-modal%20features.%20Additionally%2C%20they%0Adon%27t%20protect%20user-sensitive%20data%20from%20potential%20leakage%20during%20knowledge%0Atransfer%20between%20domains.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0APrivacy-Preserving%20Framework%20with%20Multi-Modal%20Data%20for%20Cross-Domain%0ARecommendation%2C%20called%20P2M2-CDR.%20Specifically%2C%20we%20first%20design%20a%20multi-modal%0Adisentangled%20encoder%20that%20utilizes%20multi-modal%20information%20to%20disentangle%20more%0Ainformative%20domain-common%20and%20domain-specific%20embeddings.%20Furthermore%2C%20we%0Aintroduce%20a%20privacy-preserving%20decoder%20to%20mitigate%20user%20privacy%20leakage%20during%0Aknowledge%20transfer.%20Local%20differential%20privacy%20%28LDP%29%20is%20utilized%20to%20obfuscate%0Athe%20disentangled%20embeddings%20before%20inter-domain%20exchange%2C%20thereby%20enhancing%0Aprivacy%20protection.%20To%20ensure%20both%20consistency%20and%20differentiation%20among%20these%0Aobfuscated%20disentangled%20embeddings%2C%20we%20incorporate%20contrastive%20learning-based%0Adomain-inter%20and%20domain-intra%20losses.%20Extensive%20Experiments%20conducted%20on%20four%0Areal-world%20datasets%20demonstrate%20that%20P2M2-CDR%20outperforms%20other%0Astate-of-the-art%20single-domain%20and%20cross-domain%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03600v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning for Dynamic Algorithm Selection: A\n  Proof-of-Principle Study on Differential Evolution", "author": "Hongshu Guo and Yining Ma and Zeyuan Ma and Jiacheng Chen and Xinglin Zhang and Zhiguang Cao and Jun Zhang and Yue-Jiao Gong", "abstract": "  Evolutionary algorithms, such as Differential Evolution, excel in solving\nreal-parameter optimization challenges. However, the effectiveness of a single\nalgorithm varies across different problem instances, necessitating considerable\nefforts in algorithm selection or configuration. This paper aims to address the\nlimitation by leveraging the complementary strengths of a group of algorithms\nand dynamically scheduling them throughout the optimization progress for\nspecific problems. We propose a deep reinforcement learning-based dynamic\nalgorithm selection framework to accomplish this task. Our approach models the\ndynamic algorithm selection a Markov Decision Process, training an agent in a\npolicy gradient manner to select the most suitable algorithm according to the\nfeatures observed during the optimization process. To empower the agent with\nthe necessary information, our framework incorporates a thoughtful design of\nlandscape and algorithmic features. Meanwhile, we employ a sophisticated deep\nneural network model to infer the optimal action, ensuring informed algorithm\nselections. Additionally, an algorithm context restoration mechanism is\nembedded to facilitate smooth switching among different algorithms. These\nmechanisms together enable our framework to seamlessly select and switch\nalgorithms in a dynamic online fashion. Notably, the proposed framework is\nsimple and generic, offering potential improvements across a broad spectrum of\nevolutionary algorithms. As a proof-of-principle study, we apply this framework\nto a group of Differential Evolution algorithms. The experimental results\nshowcase the remarkable effectiveness of the proposed framework, not only\nenhancing the overall optimization performance but also demonstrating favorable\ngeneralization ability across different problem classes.\n", "link": "http://arxiv.org/abs/2403.02131v2", "date": "2024-03-06", "relevancy": 2.5165, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5453}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4914}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4733}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20for%20Dynamic%20Algorithm%20Selection%3A%20A%0A%20%20Proof-of-Principle%20Study%20on%20Differential%20Evolution&entry.906535625=Hongshu%20Guo%20and%20Yining%20Ma%20and%20Zeyuan%20Ma%20and%20Jiacheng%20Chen%20and%20Xinglin%20Zhang%20and%20Zhiguang%20Cao%20and%20Jun%20Zhang%20and%20Yue-Jiao%20Gong&entry.1292438233=%20%20Evolutionary%20algorithms%2C%20such%20as%20Differential%20Evolution%2C%20excel%20in%20solving%0Areal-parameter%20optimization%20challenges.%20However%2C%20the%20effectiveness%20of%20a%20single%0Aalgorithm%20varies%20across%20different%20problem%20instances%2C%20necessitating%20considerable%0Aefforts%20in%20algorithm%20selection%20or%20configuration.%20This%20paper%20aims%20to%20address%20the%0Alimitation%20by%20leveraging%20the%20complementary%20strengths%20of%20a%20group%20of%20algorithms%0Aand%20dynamically%20scheduling%20them%20throughout%20the%20optimization%20progress%20for%0Aspecific%20problems.%20We%20propose%20a%20deep%20reinforcement%20learning-based%20dynamic%0Aalgorithm%20selection%20framework%20to%20accomplish%20this%20task.%20Our%20approach%20models%20the%0Adynamic%20algorithm%20selection%20a%20Markov%20Decision%20Process%2C%20training%20an%20agent%20in%20a%0Apolicy%20gradient%20manner%20to%20select%20the%20most%20suitable%20algorithm%20according%20to%20the%0Afeatures%20observed%20during%20the%20optimization%20process.%20To%20empower%20the%20agent%20with%0Athe%20necessary%20information%2C%20our%20framework%20incorporates%20a%20thoughtful%20design%20of%0Alandscape%20and%20algorithmic%20features.%20Meanwhile%2C%20we%20employ%20a%20sophisticated%20deep%0Aneural%20network%20model%20to%20infer%20the%20optimal%20action%2C%20ensuring%20informed%20algorithm%0Aselections.%20Additionally%2C%20an%20algorithm%20context%20restoration%20mechanism%20is%0Aembedded%20to%20facilitate%20smooth%20switching%20among%20different%20algorithms.%20These%0Amechanisms%20together%20enable%20our%20framework%20to%20seamlessly%20select%20and%20switch%0Aalgorithms%20in%20a%20dynamic%20online%20fashion.%20Notably%2C%20the%20proposed%20framework%20is%0Asimple%20and%20generic%2C%20offering%20potential%20improvements%20across%20a%20broad%20spectrum%20of%0Aevolutionary%20algorithms.%20As%20a%20proof-of-principle%20study%2C%20we%20apply%20this%20framework%0Ato%20a%20group%20of%20Differential%20Evolution%20algorithms.%20The%20experimental%20results%0Ashowcase%20the%20remarkable%20effectiveness%20of%20the%20proposed%20framework%2C%20not%20only%0Aenhancing%20the%20overall%20optimization%20performance%20but%20also%20demonstrating%20favorable%0Ageneralization%20ability%20across%20different%20problem%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02131v2&entry.124074799=Read"},
{"title": "UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video\n  Diffusion Models via Training-Free Unified Attention Control", "author": "Xuweiyi Chen and Tian Xia and Sihan Xu", "abstract": "  Video Diffusion Models have been developed for video generation, usually\nintegrating text and image conditioning to enhance control over the generated\ncontent. Despite the progress, ensuring consistency across frames remains a\nchallenge, particularly when using text prompts as control conditions. To\naddress this problem, we introduce UniCtrl, a novel, plug-and-play method that\nis universally applicable to improve the spatiotemporal consistency and motion\ndiversity of videos generated by text-to-video models without additional\ntraining. UniCtrl ensures semantic consistency across different frames through\ncross-frame self-attention control, and meanwhile, enhances the motion quality\nand spatiotemporal consistency through motion injection and spatiotemporal\nsynchronization. Our experimental results demonstrate UniCtrl's efficacy in\nenhancing various text-to-video models, confirming its effectiveness and\nuniversality.\n", "link": "http://arxiv.org/abs/2403.02332v3", "date": "2024-03-06", "relevancy": 2.5146, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7063}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6305}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5957}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniCtrl%3A%20Improving%20the%20Spatiotemporal%20Consistency%20of%20Text-to-Video%0A%20%20Diffusion%20Models%20via%20Training-Free%20Unified%20Attention%20Control&entry.906535625=Xuweiyi%20Chen%20and%20Tian%20Xia%20and%20Sihan%20Xu&entry.1292438233=%20%20Video%20Diffusion%20Models%20have%20been%20developed%20for%20video%20generation%2C%20usually%0Aintegrating%20text%20and%20image%20conditioning%20to%20enhance%20control%20over%20the%20generated%0Acontent.%20Despite%20the%20progress%2C%20ensuring%20consistency%20across%20frames%20remains%20a%0Achallenge%2C%20particularly%20when%20using%20text%20prompts%20as%20control%20conditions.%20To%0Aaddress%20this%20problem%2C%20we%20introduce%20UniCtrl%2C%20a%20novel%2C%20plug-and-play%20method%20that%0Ais%20universally%20applicable%20to%20improve%20the%20spatiotemporal%20consistency%20and%20motion%0Adiversity%20of%20videos%20generated%20by%20text-to-video%20models%20without%20additional%0Atraining.%20UniCtrl%20ensures%20semantic%20consistency%20across%20different%20frames%20through%0Across-frame%20self-attention%20control%2C%20and%20meanwhile%2C%20enhances%20the%20motion%20quality%0Aand%20spatiotemporal%20consistency%20through%20motion%20injection%20and%20spatiotemporal%0Asynchronization.%20Our%20experimental%20results%20demonstrate%20UniCtrl%27s%20efficacy%20in%0Aenhancing%20various%20text-to-video%20models%2C%20confirming%20its%20effectiveness%20and%0Auniversality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02332v3&entry.124074799=Read"},
{"title": "Feature Selection as Deep Sequential Generative Learning", "author": "Wangyang Ying and Dongjie Wang and Haifeng Chen and Yanjie Fu", "abstract": "  Feature selection aims to identify the most pattern-discriminative feature\nsubset. In prior literature, filter (e.g., backward elimination) and embedded\n(e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding)\nand tie to specific models, thus, hard to generalize; wrapper methods search a\nfeature subset in a huge discrete space and is computationally costly. To\ntransform the way of feature selection, we regard a selected feature subset as\na selection decision token sequence and reformulate feature selection as a deep\nsequential generative learning task that distills feature knowledge and\ngenerates decision sequences. Our method includes three steps: (1) We develop a\ndeep variational transformer model over a joint of sequential reconstruction,\nvariational, and performance evaluator losses. Our model can distill feature\nselection knowledge and learn a continuous embedding space to map feature\nselection decision sequences into embedding vectors associated with utility\nscores. (2) We leverage the trained feature subset utility evaluator as a\ngradient provider to guide the identification of the optimal feature subset\nembedding;(3) We decode the optimal feature subset embedding to\nautoregressively generate the best feature selection decision sequence with\nautostop. Extensive experimental results show this generative perspective is\neffective and generic, without large discrete search space and expert-specific\nhyperparameters.\n", "link": "http://arxiv.org/abs/2403.03838v1", "date": "2024-03-06", "relevancy": 2.5016, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5242}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5096}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4672}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Selection%20as%20Deep%20Sequential%20Generative%20Learning&entry.906535625=Wangyang%20Ying%20and%20Dongjie%20Wang%20and%20Haifeng%20Chen%20and%20Yanjie%20Fu&entry.1292438233=%20%20Feature%20selection%20aims%20to%20identify%20the%20most%20pattern-discriminative%20feature%0Asubset.%20In%20prior%20literature%2C%20filter%20%28e.g.%2C%20backward%20elimination%29%20and%20embedded%0A%28e.g.%2C%20Lasso%29%20methods%20have%20hyperparameters%20%28e.g.%2C%20top-K%2C%20score%20thresholding%29%0Aand%20tie%20to%20specific%20models%2C%20thus%2C%20hard%20to%20generalize%3B%20wrapper%20methods%20search%20a%0Afeature%20subset%20in%20a%20huge%20discrete%20space%20and%20is%20computationally%20costly.%20To%0Atransform%20the%20way%20of%20feature%20selection%2C%20we%20regard%20a%20selected%20feature%20subset%20as%0Aa%20selection%20decision%20token%20sequence%20and%20reformulate%20feature%20selection%20as%20a%20deep%0Asequential%20generative%20learning%20task%20that%20distills%20feature%20knowledge%20and%0Agenerates%20decision%20sequences.%20Our%20method%20includes%20three%20steps%3A%20%281%29%20We%20develop%20a%0Adeep%20variational%20transformer%20model%20over%20a%20joint%20of%20sequential%20reconstruction%2C%0Avariational%2C%20and%20performance%20evaluator%20losses.%20Our%20model%20can%20distill%20feature%0Aselection%20knowledge%20and%20learn%20a%20continuous%20embedding%20space%20to%20map%20feature%0Aselection%20decision%20sequences%20into%20embedding%20vectors%20associated%20with%20utility%0Ascores.%20%282%29%20We%20leverage%20the%20trained%20feature%20subset%20utility%20evaluator%20as%20a%0Agradient%20provider%20to%20guide%20the%20identification%20of%20the%20optimal%20feature%20subset%0Aembedding%3B%283%29%20We%20decode%20the%20optimal%20feature%20subset%20embedding%20to%0Aautoregressively%20generate%20the%20best%20feature%20selection%20decision%20sequence%20with%0Aautostop.%20Extensive%20experimental%20results%20show%20this%20generative%20perspective%20is%0Aeffective%20and%20generic%2C%20without%20large%20discrete%20search%20space%20and%20expert-specific%0Ahyperparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03838v1&entry.124074799=Read"},
{"title": "GUIDE: Guidance-based Incremental Learning with Diffusion Models", "author": "Bartosz Cywi\u0144ski and Kamil Deja and Tomasz Trzci\u0144ski and Bart\u0142omiej Twardowski and \u0141ukasz Kuci\u0144ski", "abstract": "  We introduce GUIDE, a novel continual learning approach that directs\ndiffusion models to rehearse samples at risk of being forgotten. Existing\ngenerative strategies combat catastrophic forgetting by randomly sampling\nrehearsal examples from a generative model. Such an approach contradicts\nbuffer-based approaches where sampling strategy plays an important role. We\npropose to bridge this gap by integrating diffusion models with classifier\nguidance techniques to produce rehearsal examples specifically targeting\ninformation forgotten by a continuously trained model. This approach enables\nthe generation of samples from preceding task distributions, which are more\nlikely to be misclassified in the context of recently encountered classes. Our\nexperimental results show that GUIDE significantly reduces catastrophic\nforgetting, outperforming conventional random sampling approaches and\nsurpassing recent state-of-the-art methods in continual learning with\ngenerative replay.\n", "link": "http://arxiv.org/abs/2403.03938v1", "date": "2024-03-06", "relevancy": 2.4957, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5186}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4939}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4849}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUIDE%3A%20Guidance-based%20Incremental%20Learning%20with%20Diffusion%20Models&entry.906535625=Bartosz%20Cywi%C5%84ski%20and%20Kamil%20Deja%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski%20and%20%C5%81ukasz%20Kuci%C5%84ski&entry.1292438233=%20%20We%20introduce%20GUIDE%2C%20a%20novel%20continual%20learning%20approach%20that%20directs%0Adiffusion%20models%20to%20rehearse%20samples%20at%20risk%20of%20being%20forgotten.%20Existing%0Agenerative%20strategies%20combat%20catastrophic%20forgetting%20by%20randomly%20sampling%0Arehearsal%20examples%20from%20a%20generative%20model.%20Such%20an%20approach%20contradicts%0Abuffer-based%20approaches%20where%20sampling%20strategy%20plays%20an%20important%20role.%20We%0Apropose%20to%20bridge%20this%20gap%20by%20integrating%20diffusion%20models%20with%20classifier%0Aguidance%20techniques%20to%20produce%20rehearsal%20examples%20specifically%20targeting%0Ainformation%20forgotten%20by%20a%20continuously%20trained%20model.%20This%20approach%20enables%0Athe%20generation%20of%20samples%20from%20preceding%20task%20distributions%2C%20which%20are%20more%0Alikely%20to%20be%20misclassified%20in%20the%20context%20of%20recently%20encountered%20classes.%20Our%0Aexperimental%20results%20show%20that%20GUIDE%20significantly%20reduces%20catastrophic%0Aforgetting%2C%20outperforming%20conventional%20random%20sampling%20approaches%20and%0Asurpassing%20recent%20state-of-the-art%20methods%20in%20continual%20learning%20with%0Agenerative%20replay.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03938v1&entry.124074799=Read"},
{"title": "MolNexTR: A Generalized Deep Learning Model for Molecular Image\n  Recognition", "author": "Yufan Chen and Ching Ting Leung and Yong Huang and Jianwei Sun and Hao Chen and Hanyu Gao", "abstract": "  In the field of chemical structure recognition, the task of converting\nmolecular images into graph structures and SMILES string stands as a\nsignificant challenge, primarily due to the varied drawing styles and\nconventions prevalent in chemical literature. To bridge this gap, we proposed\nMolNexTR, a novel image-to-graph deep learning model that collaborates to fuse\nthe strengths of ConvNext, a powerful Convolutional Neural Network variant, and\nVision-TRansformer. This integration facilitates a more nuanced extraction of\nboth local and global features from molecular images. MolNexTR can predict\natoms and bonds simultaneously and understand their layout rules. It also\nexcels at flexibly integrating symbolic chemistry principles to discern\nchirality and decipher abbreviated structures. We further incorporate a series\nof advanced algorithms, including improved data augmentation module, image\ncontamination module, and a post-processing module to get the final SMILES\noutput. These modules synergistically enhance the model's robustness against\nthe diverse styles of molecular imagery found in real literature. In our test\nsets, MolNexTR has demonstrated superior performance, achieving an accuracy\nrate of 81-97%, marking a significant advancement in the domain of molecular\nstructure recognition. Scientific contribution: MolNexTR is a novel\nimage-to-graph model that incorporates a unique dual-stream encoder to extract\ncomplex molecular image features, and combines chemical rules to predict atoms\nand bonds while understanding atom and bond layout rules. In addition, it\nemploys a series of novel augmentation algorithms to significantly enhance the\nrobustness and performance of the model.\n", "link": "http://arxiv.org/abs/2403.03691v1", "date": "2024-03-06", "relevancy": 2.4837, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4911}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4907}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MolNexTR%3A%20A%20Generalized%20Deep%20Learning%20Model%20for%20Molecular%20Image%0A%20%20Recognition&entry.906535625=Yufan%20Chen%20and%20Ching%20Ting%20Leung%20and%20Yong%20Huang%20and%20Jianwei%20Sun%20and%20Hao%20Chen%20and%20Hanyu%20Gao&entry.1292438233=%20%20In%20the%20field%20of%20chemical%20structure%20recognition%2C%20the%20task%20of%20converting%0Amolecular%20images%20into%20graph%20structures%20and%20SMILES%20string%20stands%20as%20a%0Asignificant%20challenge%2C%20primarily%20due%20to%20the%20varied%20drawing%20styles%20and%0Aconventions%20prevalent%20in%20chemical%20literature.%20To%20bridge%20this%20gap%2C%20we%20proposed%0AMolNexTR%2C%20a%20novel%20image-to-graph%20deep%20learning%20model%20that%20collaborates%20to%20fuse%0Athe%20strengths%20of%20ConvNext%2C%20a%20powerful%20Convolutional%20Neural%20Network%20variant%2C%20and%0AVision-TRansformer.%20This%20integration%20facilitates%20a%20more%20nuanced%20extraction%20of%0Aboth%20local%20and%20global%20features%20from%20molecular%20images.%20MolNexTR%20can%20predict%0Aatoms%20and%20bonds%20simultaneously%20and%20understand%20their%20layout%20rules.%20It%20also%0Aexcels%20at%20flexibly%20integrating%20symbolic%20chemistry%20principles%20to%20discern%0Achirality%20and%20decipher%20abbreviated%20structures.%20We%20further%20incorporate%20a%20series%0Aof%20advanced%20algorithms%2C%20including%20improved%20data%20augmentation%20module%2C%20image%0Acontamination%20module%2C%20and%20a%20post-processing%20module%20to%20get%20the%20final%20SMILES%0Aoutput.%20These%20modules%20synergistically%20enhance%20the%20model%27s%20robustness%20against%0Athe%20diverse%20styles%20of%20molecular%20imagery%20found%20in%20real%20literature.%20In%20our%20test%0Asets%2C%20MolNexTR%20has%20demonstrated%20superior%20performance%2C%20achieving%20an%20accuracy%0Arate%20of%2081-97%25%2C%20marking%20a%20significant%20advancement%20in%20the%20domain%20of%20molecular%0Astructure%20recognition.%20Scientific%20contribution%3A%20MolNexTR%20is%20a%20novel%0Aimage-to-graph%20model%20that%20incorporates%20a%20unique%20dual-stream%20encoder%20to%20extract%0Acomplex%20molecular%20image%20features%2C%20and%20combines%20chemical%20rules%20to%20predict%20atoms%0Aand%20bonds%20while%20understanding%20atom%20and%20bond%20layout%20rules.%20In%20addition%2C%20it%0Aemploys%20a%20series%20of%20novel%20augmentation%20algorithms%20to%20significantly%20enhance%20the%0Arobustness%20and%20performance%20of%20the%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03691v1&entry.124074799=Read"},
{"title": "Simplified PCNet with Robustness", "author": "Bingheng Li and Xuanting Xie and Haoxiang Lei and Ruiyi Fang and Zhao Kang", "abstract": "  Graph Neural Networks (GNNs) have garnered significant attention for their\nsuccess in learning the representation of homophilic or heterophilic graphs.\nHowever, they cannot generalize well to real-world graphs with different levels\nof homophily. In response, the Possion-Charlier Network (PCNet)\n\\cite{li2024pc}, the previous work, allows graph representation to be learned\nfrom heterophily to homophily. Although PCNet alleviates the heterophily issue,\nthere remain some challenges in further improving the efficacy and efficiency.\nIn this paper, we simplify PCNet and enhance its robustness. We first extend\nthe filter order to continuous values and reduce its parameters. Two variants\nwith adaptive neighborhood sizes are implemented. Theoretical analysis shows\nour model's robustness to graph structure perturbations or adversarial attacks.\nWe validate our approach through semi-supervised learning tasks on various\ndatasets representing both homophilic and heterophilic graphs.\n", "link": "http://arxiv.org/abs/2403.03676v1", "date": "2024-03-06", "relevancy": 2.4808, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5212}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.485}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4822}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplified%20PCNet%20with%20Robustness&entry.906535625=Bingheng%20Li%20and%20Xuanting%20Xie%20and%20Haoxiang%20Lei%20and%20Ruiyi%20Fang%20and%20Zhao%20Kang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20garnered%20significant%20attention%20for%20their%0Asuccess%20in%20learning%20the%20representation%20of%20homophilic%20or%20heterophilic%20graphs.%0AHowever%2C%20they%20cannot%20generalize%20well%20to%20real-world%20graphs%20with%20different%20levels%0Aof%20homophily.%20In%20response%2C%20the%20Possion-Charlier%20Network%20%28PCNet%29%0A%5Ccite%7Bli2024pc%7D%2C%20the%20previous%20work%2C%20allows%20graph%20representation%20to%20be%20learned%0Afrom%20heterophily%20to%20homophily.%20Although%20PCNet%20alleviates%20the%20heterophily%20issue%2C%0Athere%20remain%20some%20challenges%20in%20further%20improving%20the%20efficacy%20and%20efficiency.%0AIn%20this%20paper%2C%20we%20simplify%20PCNet%20and%20enhance%20its%20robustness.%20We%20first%20extend%0Athe%20filter%20order%20to%20continuous%20values%20and%20reduce%20its%20parameters.%20Two%20variants%0Awith%20adaptive%20neighborhood%20sizes%20are%20implemented.%20Theoretical%20analysis%20shows%0Aour%20model%27s%20robustness%20to%20graph%20structure%20perturbations%20or%20adversarial%20attacks.%0AWe%20validate%20our%20approach%20through%20semi-supervised%20learning%20tasks%20on%20various%0Adatasets%20representing%20both%20homophilic%20and%20heterophilic%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03676v1&entry.124074799=Read"},
{"title": "CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D\n  Object Detection", "author": "Gyusam Chang and Wonseok Roh and Sujin Jang and Dongwook Lee and Daehyun Ji and Gyeongrok Oh and Jinsun Park and Jinkyu Kim and Sangpil Kim", "abstract": "  Recent LiDAR-based 3D Object Detection (3DOD) methods show promising results,\nbut they often do not generalize well to target domains outside the source (or\ntraining) data distribution. To reduce such domain gaps and thus to make 3DOD\nmodels more generalizable, we introduce a novel unsupervised domain adaptation\n(UDA) method, called CMDA, which (i) leverages visual semantic cues from an\nimage modality (i.e., camera images) as an effective semantic bridge to close\nthe domain gap in the cross-modal Bird's Eye View (BEV) representations.\nFurther, (ii) we also introduce a self-training-based learning strategy,\nwherein a model is adversarially trained to generate domain-invariant features,\nwhich disrupt the discrimination of whether a feature instance comes from a\nsource or an unseen target domain. Overall, our CMDA framework guides the 3DOD\nmodel to generate highly informative and domain-adaptive features for novel\ndata distributions. In our extensive experiments with large-scale benchmarks,\nsuch as nuScenes, Waymo, and KITTI, those mentioned above provide significant\nperformance gains for UDA tasks, achieving state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2403.03721v1", "date": "2024-03-06", "relevancy": 2.4807, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6338}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6127}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6095}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMDA%3A%20Cross-Modal%20and%20Domain%20Adversarial%20Adaptation%20for%20LiDAR-Based%203D%0A%20%20Object%20Detection&entry.906535625=Gyusam%20Chang%20and%20Wonseok%20Roh%20and%20Sujin%20Jang%20and%20Dongwook%20Lee%20and%20Daehyun%20Ji%20and%20Gyeongrok%20Oh%20and%20Jinsun%20Park%20and%20Jinkyu%20Kim%20and%20Sangpil%20Kim&entry.1292438233=%20%20Recent%20LiDAR-based%203D%20Object%20Detection%20%283DOD%29%20methods%20show%20promising%20results%2C%0Abut%20they%20often%20do%20not%20generalize%20well%20to%20target%20domains%20outside%20the%20source%20%28or%0Atraining%29%20data%20distribution.%20To%20reduce%20such%20domain%20gaps%20and%20thus%20to%20make%203DOD%0Amodels%20more%20generalizable%2C%20we%20introduce%20a%20novel%20unsupervised%20domain%20adaptation%0A%28UDA%29%20method%2C%20called%20CMDA%2C%20which%20%28i%29%20leverages%20visual%20semantic%20cues%20from%20an%0Aimage%20modality%20%28i.e.%2C%20camera%20images%29%20as%20an%20effective%20semantic%20bridge%20to%20close%0Athe%20domain%20gap%20in%20the%20cross-modal%20Bird%27s%20Eye%20View%20%28BEV%29%20representations.%0AFurther%2C%20%28ii%29%20we%20also%20introduce%20a%20self-training-based%20learning%20strategy%2C%0Awherein%20a%20model%20is%20adversarially%20trained%20to%20generate%20domain-invariant%20features%2C%0Awhich%20disrupt%20the%20discrimination%20of%20whether%20a%20feature%20instance%20comes%20from%20a%0Asource%20or%20an%20unseen%20target%20domain.%20Overall%2C%20our%20CMDA%20framework%20guides%20the%203DOD%0Amodel%20to%20generate%20highly%20informative%20and%20domain-adaptive%20features%20for%20novel%0Adata%20distributions.%20In%20our%20extensive%20experiments%20with%20large-scale%20benchmarks%2C%0Asuch%20as%20nuScenes%2C%20Waymo%2C%20and%20KITTI%2C%20those%20mentioned%20above%20provide%20significant%0Aperformance%20gains%20for%20UDA%20tasks%2C%20achieving%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03721v1&entry.124074799=Read"},
{"title": "Learning Invariant Representations of Graph Neural Networks via Cluster\n  Generalization", "author": "Donglin Xia and Xiao Wang and Nian Liu and Chuan Shi", "abstract": "  Graph neural networks (GNNs) have become increasingly popular in modeling\ngraph-structured data due to their ability to learn node representations by\naggregating local structure information. However, it is widely acknowledged\nthat the test graph structure may differ from the training graph structure,\nresulting in a structure shift. In this paper, we experimentally find that the\nperformance of GNNs drops significantly when the structure shift happens,\nsuggesting that the learned models may be biased towards specific structure\npatterns. To address this challenge, we propose the Cluster Information\nTransfer (CIT) mechanism (Code available at\nhttps://github.com/BUPT-GAMMA/CITGNN), which can learn invariant\nrepresentations for GNNs, thereby improving their generalization ability to\nvarious and unknown test graphs with structure shift. The CIT mechanism\nachieves this by combining different cluster information with the nodes while\npreserving their cluster-independent information. By generating nodes across\ndifferent clusters, the mechanism significantly enhances the diversity of the\nnodes and helps GNNs learn the invariant representations. We provide a\ntheoretical analysis of the CIT mechanism, showing that the impact of changing\nclusters during structure shift can be mitigated after transfer. Additionally,\nthe proposed mechanism is a plug-in that can be easily used to improve existing\nGNNs. We comprehensively evaluate our proposed method on three typical\nstructure shift scenarios, demonstrating its effectiveness in enhancing GNNs'\nperformance.\n", "link": "http://arxiv.org/abs/2403.03599v1", "date": "2024-03-06", "relevancy": 2.4788, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5217}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4875}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Invariant%20Representations%20of%20Graph%20Neural%20Networks%20via%20Cluster%0A%20%20Generalization&entry.906535625=Donglin%20Xia%20and%20Xiao%20Wang%20and%20Nian%20Liu%20and%20Chuan%20Shi&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20increasingly%20popular%20in%20modeling%0Agraph-structured%20data%20due%20to%20their%20ability%20to%20learn%20node%20representations%20by%0Aaggregating%20local%20structure%20information.%20However%2C%20it%20is%20widely%20acknowledged%0Athat%20the%20test%20graph%20structure%20may%20differ%20from%20the%20training%20graph%20structure%2C%0Aresulting%20in%20a%20structure%20shift.%20In%20this%20paper%2C%20we%20experimentally%20find%20that%20the%0Aperformance%20of%20GNNs%20drops%20significantly%20when%20the%20structure%20shift%20happens%2C%0Asuggesting%20that%20the%20learned%20models%20may%20be%20biased%20towards%20specific%20structure%0Apatterns.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Cluster%20Information%0ATransfer%20%28CIT%29%20mechanism%20%28Code%20available%20at%0Ahttps%3A//github.com/BUPT-GAMMA/CITGNN%29%2C%20which%20can%20learn%20invariant%0Arepresentations%20for%20GNNs%2C%20thereby%20improving%20their%20generalization%20ability%20to%0Avarious%20and%20unknown%20test%20graphs%20with%20structure%20shift.%20The%20CIT%20mechanism%0Aachieves%20this%20by%20combining%20different%20cluster%20information%20with%20the%20nodes%20while%0Apreserving%20their%20cluster-independent%20information.%20By%20generating%20nodes%20across%0Adifferent%20clusters%2C%20the%20mechanism%20significantly%20enhances%20the%20diversity%20of%20the%0Anodes%20and%20helps%20GNNs%20learn%20the%20invariant%20representations.%20We%20provide%20a%0Atheoretical%20analysis%20of%20the%20CIT%20mechanism%2C%20showing%20that%20the%20impact%20of%20changing%0Aclusters%20during%20structure%20shift%20can%20be%20mitigated%20after%20transfer.%20Additionally%2C%0Athe%20proposed%20mechanism%20is%20a%20plug-in%20that%20can%20be%20easily%20used%20to%20improve%20existing%0AGNNs.%20We%20comprehensively%20evaluate%20our%20proposed%20method%20on%20three%20typical%0Astructure%20shift%20scenarios%2C%20demonstrating%20its%20effectiveness%20in%20enhancing%20GNNs%27%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03599v1&entry.124074799=Read"},
{"title": "VQGraph: Rethinking Graph Representation Space for Bridging GNNs and\n  MLPs", "author": "Ling Yang and Ye Tian and Minkai Xu and Zhongyi Liu and Shenda Hong and Wei Qu and Wentao Zhang and Bin Cui and Muhan Zhang and Jure Leskovec", "abstract": "  GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn\ncomputationally-efficient multi-layer perceptron (student MLP) on graph data by\nmimicking the output representations of teacher GNN. Existing methods mainly\nmake the MLP to mimic the GNN predictions over a few class labels. However, the\nclass space may not be expressive enough for covering numerous diverse local\ngraph structures, thus limiting the performance of knowledge transfer from GNN\nto MLP. To address this issue, we propose to learn a new powerful graph\nrepresentation space by directly labeling nodes' diverse local structures for\nGNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn\na structure-aware tokenizer on graph data that can encode each node's local\nsubstructure as a discrete code. The discrete codes constitute a codebook as a\nnew graph representation space that is able to identify different local graph\nstructures of nodes with the corresponding code indices. Then, based on the\nlearned codebook, we propose a new distillation target, namely soft code\nassignments, to directly transfer the structural knowledge of each node from\nGNN to MLP. The resulting framework VQGraph achieves new state-of-the-art\nperformance on GNN-to-MLP distillation in both transductive and inductive\nsettings across seven graph datasets. We show that VQGraph with better\nperformance infers faster than GNNs by 828x, and also achieves accuracy\nimprovement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average,\nrespectively. Code: https://github.com/YangLing0818/VQGraph.\n", "link": "http://arxiv.org/abs/2308.02117v3", "date": "2024-03-06", "relevancy": 2.456, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5045}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4916}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4775}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQGraph%3A%20Rethinking%20Graph%20Representation%20Space%20for%20Bridging%20GNNs%20and%0A%20%20MLPs&entry.906535625=Ling%20Yang%20and%20Ye%20Tian%20and%20Minkai%20Xu%20and%20Zhongyi%20Liu%20and%20Shenda%20Hong%20and%20Wei%20Qu%20and%20Wentao%20Zhang%20and%20Bin%20Cui%20and%20Muhan%20Zhang%20and%20Jure%20Leskovec&entry.1292438233=%20%20GNN-to-MLP%20distillation%20aims%20to%20utilize%20knowledge%20distillation%20%28KD%29%20to%20learn%0Acomputationally-efficient%20multi-layer%20perceptron%20%28student%20MLP%29%20on%20graph%20data%20by%0Amimicking%20the%20output%20representations%20of%20teacher%20GNN.%20Existing%20methods%20mainly%0Amake%20the%20MLP%20to%20mimic%20the%20GNN%20predictions%20over%20a%20few%20class%20labels.%20However%2C%20the%0Aclass%20space%20may%20not%20be%20expressive%20enough%20for%20covering%20numerous%20diverse%20local%0Agraph%20structures%2C%20thus%20limiting%20the%20performance%20of%20knowledge%20transfer%20from%20GNN%0Ato%20MLP.%20To%20address%20this%20issue%2C%20we%20propose%20to%20learn%20a%20new%20powerful%20graph%0Arepresentation%20space%20by%20directly%20labeling%20nodes%27%20diverse%20local%20structures%20for%0AGNN-to-MLP%20distillation.%20Specifically%2C%20we%20propose%20a%20variant%20of%20VQ-VAE%20to%20learn%0Aa%20structure-aware%20tokenizer%20on%20graph%20data%20that%20can%20encode%20each%20node%27s%20local%0Asubstructure%20as%20a%20discrete%20code.%20The%20discrete%20codes%20constitute%20a%20codebook%20as%20a%0Anew%20graph%20representation%20space%20that%20is%20able%20to%20identify%20different%20local%20graph%0Astructures%20of%20nodes%20with%20the%20corresponding%20code%20indices.%20Then%2C%20based%20on%20the%0Alearned%20codebook%2C%20we%20propose%20a%20new%20distillation%20target%2C%20namely%20soft%20code%0Aassignments%2C%20to%20directly%20transfer%20the%20structural%20knowledge%20of%20each%20node%20from%0AGNN%20to%20MLP.%20The%20resulting%20framework%20VQGraph%20achieves%20new%20state-of-the-art%0Aperformance%20on%20GNN-to-MLP%20distillation%20in%20both%20transductive%20and%20inductive%0Asettings%20across%20seven%20graph%20datasets.%20We%20show%20that%20VQGraph%20with%20better%0Aperformance%20infers%20faster%20than%20GNNs%20by%20828x%2C%20and%20also%20achieves%20accuracy%0Aimprovement%20over%20GNNs%20and%20stand-alone%20MLPs%20by%203.90%25%20and%2028.05%25%20on%20average%2C%0Arespectively.%20Code%3A%20https%3A//github.com/YangLing0818/VQGraph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.02117v3&entry.124074799=Read"},
{"title": "Fair Text-to-Image Diffusion via Fair Mapping", "author": "Jia Li and Lijie Hu and Jingfeng Zhang and Tianhang Zheng and Hua Zhang and Di Wang", "abstract": "  In this paper, we address the limitations of existing text-to-image diffusion\nmodels in generating demographically fair results when given human-related\ndescriptions. These models often struggle to disentangle the target language\ncontext from sociocultural biases, resulting in biased image generation. To\novercome this challenge, we propose Fair Mapping, a flexible, model-agnostic,\nand lightweight approach that modifies a pre-trained text-to-image diffusion\nmodel by controlling the prompt to achieve fair image generation. One key\nadvantage of our approach is its high efficiency. It only requires updating an\nadditional linear network with few parameters at a low computational cost. By\ndeveloping a linear network that maps conditioning embeddings into a debiased\nspace, we enable the generation of relatively balanced demographic results\nbased on the specified text condition. With comprehensive experiments on face\nimage generation, we show that our method significantly improves image\ngeneration fairness with almost the same image quality compared to conventional\ndiffusion models when prompted with descriptions related to humans. By\neffectively addressing the issue of implicit language bias, our method produces\nmore fair and diverse image outputs.\n", "link": "http://arxiv.org/abs/2311.17695v2", "date": "2024-03-06", "relevancy": 2.4505, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6278}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6143}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6049}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Text-to-Image%20Diffusion%20via%20Fair%20Mapping&entry.906535625=Jia%20Li%20and%20Lijie%20Hu%20and%20Jingfeng%20Zhang%20and%20Tianhang%20Zheng%20and%20Hua%20Zhang%20and%20Di%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20limitations%20of%20existing%20text-to-image%20diffusion%0Amodels%20in%20generating%20demographically%20fair%20results%20when%20given%20human-related%0Adescriptions.%20These%20models%20often%20struggle%20to%20disentangle%20the%20target%20language%0Acontext%20from%20sociocultural%20biases%2C%20resulting%20in%20biased%20image%20generation.%20To%0Aovercome%20this%20challenge%2C%20we%20propose%20Fair%20Mapping%2C%20a%20flexible%2C%20model-agnostic%2C%0Aand%20lightweight%20approach%20that%20modifies%20a%20pre-trained%20text-to-image%20diffusion%0Amodel%20by%20controlling%20the%20prompt%20to%20achieve%20fair%20image%20generation.%20One%20key%0Aadvantage%20of%20our%20approach%20is%20its%20high%20efficiency.%20It%20only%20requires%20updating%20an%0Aadditional%20linear%20network%20with%20few%20parameters%20at%20a%20low%20computational%20cost.%20By%0Adeveloping%20a%20linear%20network%20that%20maps%20conditioning%20embeddings%20into%20a%20debiased%0Aspace%2C%20we%20enable%20the%20generation%20of%20relatively%20balanced%20demographic%20results%0Abased%20on%20the%20specified%20text%20condition.%20With%20comprehensive%20experiments%20on%20face%0Aimage%20generation%2C%20we%20show%20that%20our%20method%20significantly%20improves%20image%0Ageneration%20fairness%20with%20almost%20the%20same%20image%20quality%20compared%20to%20conventional%0Adiffusion%20models%20when%20prompted%20with%20descriptions%20related%20to%20humans.%20By%0Aeffectively%20addressing%20the%20issue%20of%20implicit%20language%20bias%2C%20our%20method%20produces%0Amore%20fair%20and%20diverse%20image%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17695v2&entry.124074799=Read"},
{"title": "PanDepth: Joint Panoptic Segmentation and Depth Completion", "author": "Juan Lagos and Esa Rahtu", "abstract": "  Understanding 3D environments semantically is pivotal in autonomous driving\napplications where multiple computer vision tasks are involved. Multi-task\nmodels provide different types of outputs for a given scene, yielding a more\nholistic representation while keeping the computational cost low. We propose a\nmulti-task model for panoptic segmentation and depth completion using RGB\nimages and sparse depth maps. Our model successfully predicts fully dense depth\nmaps and performs semantic segmentation, instance segmentation, and panoptic\nsegmentation for every input frame. Extensive experiments were done on the\nVirtual KITTI 2 dataset and we demonstrate that our model solves multiple\ntasks, without a significant increase in computational cost, while keeping high\naccuracy performance. Code is available at\nhttps://github.com/juanb09111/PanDepth.git\n", "link": "http://arxiv.org/abs/2212.14180v2", "date": "2024-03-06", "relevancy": 2.448, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6524}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5856}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5772}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanDepth%3A%20Joint%20Panoptic%20Segmentation%20and%20Depth%20Completion&entry.906535625=Juan%20Lagos%20and%20Esa%20Rahtu&entry.1292438233=%20%20Understanding%203D%20environments%20semantically%20is%20pivotal%20in%20autonomous%20driving%0Aapplications%20where%20multiple%20computer%20vision%20tasks%20are%20involved.%20Multi-task%0Amodels%20provide%20different%20types%20of%20outputs%20for%20a%20given%20scene%2C%20yielding%20a%20more%0Aholistic%20representation%20while%20keeping%20the%20computational%20cost%20low.%20We%20propose%20a%0Amulti-task%20model%20for%20panoptic%20segmentation%20and%20depth%20completion%20using%20RGB%0Aimages%20and%20sparse%20depth%20maps.%20Our%20model%20successfully%20predicts%20fully%20dense%20depth%0Amaps%20and%20performs%20semantic%20segmentation%2C%20instance%20segmentation%2C%20and%20panoptic%0Asegmentation%20for%20every%20input%20frame.%20Extensive%20experiments%20were%20done%20on%20the%0AVirtual%20KITTI%202%20dataset%20and%20we%20demonstrate%20that%20our%20model%20solves%20multiple%0Atasks%2C%20without%20a%20significant%20increase%20in%20computational%20cost%2C%20while%20keeping%20high%0Aaccuracy%20performance.%20Code%20is%20available%20at%0Ahttps%3A//github.com/juanb09111/PanDepth.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.14180v2&entry.124074799=Read"},
{"title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction", "author": "Zekun Qi and Runpei Dong and Shaochen Zhang and Haoran Geng and Chunrui Han and Zheng Ge and He Wang and Li Yi and Kaisheng Ma", "abstract": "  This paper presents ShapeLLM, the first 3D Multimodal Large Language Model\n(LLM) designed for embodied interaction, exploring a universal 3D object\nunderstanding with 3D point clouds and languages. ShapeLLM is built upon an\nimproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view\nimage distillation for enhanced geometry understanding. By utilizing ReCon++ as\nthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed\ninstruction-following data and tested on our newly human-curated evaluation\nbenchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance\nin 3D geometry understanding and language-unified 3D interaction tasks, such as\nembodied visual grounding.\n", "link": "http://arxiv.org/abs/2402.17766v2", "date": "2024-03-06", "relevancy": 2.4261, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6599}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6008}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5554}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeLLM%3A%20Universal%203D%20Object%20Understanding%20for%20Embodied%20Interaction&entry.906535625=Zekun%20Qi%20and%20Runpei%20Dong%20and%20Shaochen%20Zhang%20and%20Haoran%20Geng%20and%20Chunrui%20Han%20and%20Zheng%20Ge%20and%20He%20Wang%20and%20Li%20Yi%20and%20Kaisheng%20Ma&entry.1292438233=%20%20This%20paper%20presents%20ShapeLLM%2C%20the%20first%203D%20Multimodal%20Large%20Language%20Model%0A%28LLM%29%20designed%20for%20embodied%20interaction%2C%20exploring%20a%20universal%203D%20object%0Aunderstanding%20with%203D%20point%20clouds%20and%20languages.%20ShapeLLM%20is%20built%20upon%20an%0Aimproved%203D%20encoder%20by%20extending%20ReCon%20to%20ReCon%2B%2B%20that%20benefits%20from%20multi-view%0Aimage%20distillation%20for%20enhanced%20geometry%20understanding.%20By%20utilizing%20ReCon%2B%2B%20as%0Athe%203D%20point%20cloud%20input%20encoder%20for%20LLMs%2C%20ShapeLLM%20is%20trained%20on%20constructed%0Ainstruction-following%20data%20and%20tested%20on%20our%20newly%20human-curated%20evaluation%0Abenchmark%2C%203D%20MM-Vet.%20ReCon%2B%2B%20and%20ShapeLLM%20achieve%20state-of-the-art%20performance%0Ain%203D%20geometry%20understanding%20and%20language-unified%203D%20interaction%20tasks%2C%20such%20as%0Aembodied%20visual%20grounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17766v2&entry.124074799=Read"},
{"title": "Decoupled Vertical Federated Learning for Practical Training on\n  Vertically Partitioned Data", "author": "Avi Amalanshu and Yash Sirvi and David I. Inouye", "abstract": "  Vertical Federated Learning (VFL) is an emergent distributed machine learning\nparadigm wherein owners of disjoint features of a common set of entities\ncollaborate to learn a global model without sharing data. In VFL, a host client\nowns data labels for each entity and learns a final representation based on\nintermediate local representations from all guest clients. Therefore, the host\nis a single point of failure and label feedback can be used by malicious guest\nclients to infer private features. Requiring all participants to remain active\nand trustworthy throughout the entire training process is generally impractical\nand altogether infeasible outside of controlled environments. We propose\nDecoupled VFL (DVFL), a blockwise learning approach to VFL. By training each\nmodel on its own objective, DVFL allows for decentralized aggregation and\nisolation between feature learning and label supervision. With these\nproperties, DVFL is fault tolerant and secure. We implement DVFL to train split\nneural networks and show that model performance is comparable to VFL on a\nvariety of classification datasets.\n", "link": "http://arxiv.org/abs/2403.03871v1", "date": "2024-03-06", "relevancy": 2.4188, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5046}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4801}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4666}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20Vertical%20Federated%20Learning%20for%20Practical%20Training%20on%0A%20%20Vertically%20Partitioned%20Data&entry.906535625=Avi%20Amalanshu%20and%20Yash%20Sirvi%20and%20David%20I.%20Inouye&entry.1292438233=%20%20Vertical%20Federated%20Learning%20%28VFL%29%20is%20an%20emergent%20distributed%20machine%20learning%0Aparadigm%20wherein%20owners%20of%20disjoint%20features%20of%20a%20common%20set%20of%20entities%0Acollaborate%20to%20learn%20a%20global%20model%20without%20sharing%20data.%20In%20VFL%2C%20a%20host%20client%0Aowns%20data%20labels%20for%20each%20entity%20and%20learns%20a%20final%20representation%20based%20on%0Aintermediate%20local%20representations%20from%20all%20guest%20clients.%20Therefore%2C%20the%20host%0Ais%20a%20single%20point%20of%20failure%20and%20label%20feedback%20can%20be%20used%20by%20malicious%20guest%0Aclients%20to%20infer%20private%20features.%20Requiring%20all%20participants%20to%20remain%20active%0Aand%20trustworthy%20throughout%20the%20entire%20training%20process%20is%20generally%20impractical%0Aand%20altogether%20infeasible%20outside%20of%20controlled%20environments.%20We%20propose%0ADecoupled%20VFL%20%28DVFL%29%2C%20a%20blockwise%20learning%20approach%20to%20VFL.%20By%20training%20each%0Amodel%20on%20its%20own%20objective%2C%20DVFL%20allows%20for%20decentralized%20aggregation%20and%0Aisolation%20between%20feature%20learning%20and%20label%20supervision.%20With%20these%0Aproperties%2C%20DVFL%20is%20fault%20tolerant%20and%20secure.%20We%20implement%20DVFL%20to%20train%20split%0Aneural%20networks%20and%20show%20that%20model%20performance%20is%20comparable%20to%20VFL%20on%20a%0Avariety%20of%20classification%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03871v1&entry.124074799=Read"},
{"title": "Unifying Generation and Compression: Ultra-low bitrate Image Coding Via\n  Multi-stage Transformer", "author": "Naifu Xue and Qi Mao and Zijian Wang and Yuan Zhang and Siwei Ma", "abstract": "  Recent progress in generative compression technology has significantly\nimproved the perceptual quality of compressed data. However, these advancements\nprimarily focus on producing high-frequency details, often overlooking the\nability of generative models to capture the prior distribution of image\ncontent, thus impeding further bitrate reduction in extreme compression\nscenarios (<0.05 bpp). Motivated by the capabilities of predictive language\nmodels for lossless compression, this paper introduces a novel Unified Image\nGeneration-Compression (UIGC) paradigm, merging the processes of generation and\ncompression. A key feature of the UIGC framework is the adoption of\nvector-quantized (VQ) image models for tokenization, alongside a multi-stage\ntransformer designed to exploit spatial contextual information for modeling the\nprior distribution. As such, the dual-purpose framework effectively utilizes\nthe learned prior for entropy estimation and assists in the regeneration of\nlost tokens. Extensive experiments demonstrate the superiority of the proposed\nUIGC framework over existing codecs in perceptual quality and human perception,\nparticularly in ultra-low bitrate scenarios (<=0.03 bpp), pioneering a new\ndirection in generative compression.\n", "link": "http://arxiv.org/abs/2403.03736v1", "date": "2024-03-06", "relevancy": 2.4177, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6127}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5991}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.597}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Generation%20and%20Compression%3A%20Ultra-low%20bitrate%20Image%20Coding%20Via%0A%20%20Multi-stage%20Transformer&entry.906535625=Naifu%20Xue%20and%20Qi%20Mao%20and%20Zijian%20Wang%20and%20Yuan%20Zhang%20and%20Siwei%20Ma&entry.1292438233=%20%20Recent%20progress%20in%20generative%20compression%20technology%20has%20significantly%0Aimproved%20the%20perceptual%20quality%20of%20compressed%20data.%20However%2C%20these%20advancements%0Aprimarily%20focus%20on%20producing%20high-frequency%20details%2C%20often%20overlooking%20the%0Aability%20of%20generative%20models%20to%20capture%20the%20prior%20distribution%20of%20image%0Acontent%2C%20thus%20impeding%20further%20bitrate%20reduction%20in%20extreme%20compression%0Ascenarios%20%28%3C0.05%20bpp%29.%20Motivated%20by%20the%20capabilities%20of%20predictive%20language%0Amodels%20for%20lossless%20compression%2C%20this%20paper%20introduces%20a%20novel%20Unified%20Image%0AGeneration-Compression%20%28UIGC%29%20paradigm%2C%20merging%20the%20processes%20of%20generation%20and%0Acompression.%20A%20key%20feature%20of%20the%20UIGC%20framework%20is%20the%20adoption%20of%0Avector-quantized%20%28VQ%29%20image%20models%20for%20tokenization%2C%20alongside%20a%20multi-stage%0Atransformer%20designed%20to%20exploit%20spatial%20contextual%20information%20for%20modeling%20the%0Aprior%20distribution.%20As%20such%2C%20the%20dual-purpose%20framework%20effectively%20utilizes%0Athe%20learned%20prior%20for%20entropy%20estimation%20and%20assists%20in%20the%20regeneration%20of%0Alost%20tokens.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20the%20proposed%0AUIGC%20framework%20over%20existing%20codecs%20in%20perceptual%20quality%20and%20human%20perception%2C%0Aparticularly%20in%20ultra-low%20bitrate%20scenarios%20%28%3C%3D0.03%20bpp%29%2C%20pioneering%20a%20new%0Adirection%20in%20generative%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03736v1&entry.124074799=Read"},
{"title": "Learning 3D object-centric representation through prediction", "author": "John Day and Tushar Arora and Jirui Liu and Li Erran Li and Ming Bo Cai", "abstract": "  As part of human core knowledge, the representation of objects is the\nbuilding block of mental representation that supports high-level concepts and\nsymbolic reasoning. While humans develop the ability of perceiving objects\nsituated in 3D environments without supervision, models that learn the same set\nof abilities with similar constraints faced by human infants are lacking.\nTowards this end, we developed a novel network architecture that simultaneously\nlearns to 1) segment objects from discrete images, 2) infer their 3D locations,\nand 3) perceive depth, all while using only information directly available to\nthe brain as training data, namely: sequences of images and self-motion. The\ncore idea is treating objects as latent causes of visual input which the brain\nuses to make efficient predictions of future scenes. This results in object\nrepresentations being learned as an essential byproduct of learning to predict.\n", "link": "http://arxiv.org/abs/2403.03730v1", "date": "2024-03-06", "relevancy": 2.4165, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6122}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6018}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5898}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%203D%20object-centric%20representation%20through%20prediction&entry.906535625=John%20Day%20and%20Tushar%20Arora%20and%20Jirui%20Liu%20and%20Li%20Erran%20Li%20and%20Ming%20Bo%20Cai&entry.1292438233=%20%20As%20part%20of%20human%20core%20knowledge%2C%20the%20representation%20of%20objects%20is%20the%0Abuilding%20block%20of%20mental%20representation%20that%20supports%20high-level%20concepts%20and%0Asymbolic%20reasoning.%20While%20humans%20develop%20the%20ability%20of%20perceiving%20objects%0Asituated%20in%203D%20environments%20without%20supervision%2C%20models%20that%20learn%20the%20same%20set%0Aof%20abilities%20with%20similar%20constraints%20faced%20by%20human%20infants%20are%20lacking.%0ATowards%20this%20end%2C%20we%20developed%20a%20novel%20network%20architecture%20that%20simultaneously%0Alearns%20to%201%29%20segment%20objects%20from%20discrete%20images%2C%202%29%20infer%20their%203D%20locations%2C%0Aand%203%29%20perceive%20depth%2C%20all%20while%20using%20only%20information%20directly%20available%20to%0Athe%20brain%20as%20training%20data%2C%20namely%3A%20sequences%20of%20images%20and%20self-motion.%20The%0Acore%20idea%20is%20treating%20objects%20as%20latent%20causes%20of%20visual%20input%20which%20the%20brain%0Auses%20to%20make%20efficient%20predictions%20of%20future%20scenes.%20This%20results%20in%20object%0Arepresentations%20being%20learned%20as%20an%20essential%20byproduct%20of%20learning%20to%20predict.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03730v1&entry.124074799=Read"},
{"title": "A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network", "author": "Ruichen Ma and Guanchao Qiao and Yian Liu and Liwei Meng and Ning Ning and Yang Liu and Shaogang Hu", "abstract": "  Binary neural networks utilize 1-bit quantized weights and activations to\nreduce both the model's storage demands and computational burden. However,\nadvanced binary architectures still incorporate millions of inefficient and\nnonhardware-friendly full-precision multiplication operations. A&B BNN is\nproposed to directly remove part of the multiplication operations in a\ntraditional BNN and replace the rest with an equal number of bit operations,\nintroducing the mask layer and the quantized RPReLU structure based on the\nnormalizer-free network architecture. The mask layer can be removed during\ninference by leveraging the intrinsic characteristics of BNN with\nstraightforward mathematical transformations to avoid the associated\nmultiplication operations. The quantized RPReLU structure enables more\nefficient bit operations by constraining its slope to be integer powers of 2.\nExperimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10,\nCIFAR-100, and ImageNet datasets, respectively, which are competitive with the\nstate-of-the-art. Ablation studies have verified the efficacy of the quantized\nRPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to\nusing a fixed slope RLeakyReLU. The proposed add&bit-operation-only BNN offers\nan innovative approach for hardware-friendly network architecture.\n", "link": "http://arxiv.org/abs/2403.03739v1", "date": "2024-03-06", "relevancy": 2.413, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5309}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4723}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4446}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%26B%20BNN%3A%20Add%26Bit-Operation-Only%20Hardware-Friendly%20Binary%20Neural%20Network&entry.906535625=Ruichen%20Ma%20and%20Guanchao%20Qiao%20and%20Yian%20Liu%20and%20Liwei%20Meng%20and%20Ning%20Ning%20and%20Yang%20Liu%20and%20Shaogang%20Hu&entry.1292438233=%20%20Binary%20neural%20networks%20utilize%201-bit%20quantized%20weights%20and%20activations%20to%0Areduce%20both%20the%20model%27s%20storage%20demands%20and%20computational%20burden.%20However%2C%0Aadvanced%20binary%20architectures%20still%20incorporate%20millions%20of%20inefficient%20and%0Anonhardware-friendly%20full-precision%20multiplication%20operations.%20A%26B%20BNN%20is%0Aproposed%20to%20directly%20remove%20part%20of%20the%20multiplication%20operations%20in%20a%0Atraditional%20BNN%20and%20replace%20the%20rest%20with%20an%20equal%20number%20of%20bit%20operations%2C%0Aintroducing%20the%20mask%20layer%20and%20the%20quantized%20RPReLU%20structure%20based%20on%20the%0Anormalizer-free%20network%20architecture.%20The%20mask%20layer%20can%20be%20removed%20during%0Ainference%20by%20leveraging%20the%20intrinsic%20characteristics%20of%20BNN%20with%0Astraightforward%20mathematical%20transformations%20to%20avoid%20the%20associated%0Amultiplication%20operations.%20The%20quantized%20RPReLU%20structure%20enables%20more%0Aefficient%20bit%20operations%20by%20constraining%20its%20slope%20to%20be%20integer%20powers%20of%202.%0AExperimental%20results%20achieved%2092.30%25%2C%2069.35%25%2C%20and%2066.89%25%20on%20the%20CIFAR-10%2C%0ACIFAR-100%2C%20and%20ImageNet%20datasets%2C%20respectively%2C%20which%20are%20competitive%20with%20the%0Astate-of-the-art.%20Ablation%20studies%20have%20verified%20the%20efficacy%20of%20the%20quantized%0ARPReLU%20structure%2C%20leading%20to%20a%201.14%25%20enhancement%20on%20the%20ImageNet%20compared%20to%0Ausing%20a%20fixed%20slope%20RLeakyReLU.%20The%20proposed%20add%26bit-operation-only%20BNN%20offers%0Aan%20innovative%20approach%20for%20hardware-friendly%20network%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03739v1&entry.124074799=Read"},
{"title": "SNI-SLAM: Semantic Neural Implicit SLAM", "author": "Siting Zhu and Guangming Wang and Hermann Blum and Jiuming Liu and Liang Song and Marc Pollefeys and Hesheng Wang", "abstract": "  We propose SNI-SLAM, a semantic SLAM system utilizing neural implicit\nrepresentation, that simultaneously performs accurate semantic mapping,\nhigh-quality surface reconstruction, and robust camera tracking. In this\nsystem, we introduce hierarchical semantic representation to allow multi-level\nsemantic comprehension for top-down structured semantic mapping of the scene.\nIn addition, to fully utilize the correlation between multiple attributes of\nthe environment, we integrate appearance, geometry and semantic features\nthrough cross-attention for feature collaboration. This strategy enables a more\nmultifaceted understanding of the environment, thereby allowing SNI-SLAM to\nremain robust even when single attribute is defective. Then, we design an\ninternal fusion-based decoder to obtain semantic, RGB, Truncated Signed\nDistance Field (TSDF) values from multi-level features for accurate decoding.\nFurthermore, we propose a feature loss to update the scene representation at\nthe feature level. Compared with low-level losses such as RGB loss and depth\nloss, our feature loss is capable of guiding the network optimization on a\nhigher-level. Our SNI-SLAM method demonstrates superior performance over all\nrecent NeRF-based SLAM methods in terms of mapping and tracking accuracy on\nReplica and ScanNet datasets, while also showing excellent capabilities in\naccurate semantic segmentation and real-time semantic mapping.\n", "link": "http://arxiv.org/abs/2311.11016v2", "date": "2024-03-06", "relevancy": 2.3696, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6209}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5726}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5705}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SNI-SLAM%3A%20Semantic%20Neural%20Implicit%20SLAM&entry.906535625=Siting%20Zhu%20and%20Guangming%20Wang%20and%20Hermann%20Blum%20and%20Jiuming%20Liu%20and%20Liang%20Song%20and%20Marc%20Pollefeys%20and%20Hesheng%20Wang&entry.1292438233=%20%20We%20propose%20SNI-SLAM%2C%20a%20semantic%20SLAM%20system%20utilizing%20neural%20implicit%0Arepresentation%2C%20that%20simultaneously%20performs%20accurate%20semantic%20mapping%2C%0Ahigh-quality%20surface%20reconstruction%2C%20and%20robust%20camera%20tracking.%20In%20this%0Asystem%2C%20we%20introduce%20hierarchical%20semantic%20representation%20to%20allow%20multi-level%0Asemantic%20comprehension%20for%20top-down%20structured%20semantic%20mapping%20of%20the%20scene.%0AIn%20addition%2C%20to%20fully%20utilize%20the%20correlation%20between%20multiple%20attributes%20of%0Athe%20environment%2C%20we%20integrate%20appearance%2C%20geometry%20and%20semantic%20features%0Athrough%20cross-attention%20for%20feature%20collaboration.%20This%20strategy%20enables%20a%20more%0Amultifaceted%20understanding%20of%20the%20environment%2C%20thereby%20allowing%20SNI-SLAM%20to%0Aremain%20robust%20even%20when%20single%20attribute%20is%20defective.%20Then%2C%20we%20design%20an%0Ainternal%20fusion-based%20decoder%20to%20obtain%20semantic%2C%20RGB%2C%20Truncated%20Signed%0ADistance%20Field%20%28TSDF%29%20values%20from%20multi-level%20features%20for%20accurate%20decoding.%0AFurthermore%2C%20we%20propose%20a%20feature%20loss%20to%20update%20the%20scene%20representation%20at%0Athe%20feature%20level.%20Compared%20with%20low-level%20losses%20such%20as%20RGB%20loss%20and%20depth%0Aloss%2C%20our%20feature%20loss%20is%20capable%20of%20guiding%20the%20network%20optimization%20on%20a%0Ahigher-level.%20Our%20SNI-SLAM%20method%20demonstrates%20superior%20performance%20over%20all%0Arecent%20NeRF-based%20SLAM%20methods%20in%20terms%20of%20mapping%20and%20tracking%20accuracy%20on%0AReplica%20and%20ScanNet%20datasets%2C%20while%20also%20showing%20excellent%20capabilities%20in%0Aaccurate%20semantic%20segmentation%20and%20real-time%20semantic%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11016v2&entry.124074799=Read"},
{"title": "High-Fidelity Image Compression with Score-based Generative Models", "author": "Emiel Hoogeboom and Eirikur Agustsson and Fabian Mentzer and Luca Versari and George Toderici and Lucas Theis", "abstract": "  Despite the tremendous success of diffusion generative models in\ntext-to-image generation, replicating this success in the domain of image\ncompression has proven difficult. In this paper, we demonstrate that diffusion\ncan significantly improve perceptual quality at a given bit-rate, outperforming\nstate-of-the-art approaches PO-ELIC and HiFiC as measured by FID score. This is\nachieved using a simple but theoretically motivated two-stage approach\ncombining an autoencoder targeting MSE followed by a further score-based\ndecoder. However, as we will show, implementation details matter and the\noptimal design decisions can differ greatly from typical text-to-image models.\n", "link": "http://arxiv.org/abs/2305.18231v2", "date": "2024-03-06", "relevancy": 2.3629, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6235}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5971}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5712}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Fidelity%20Image%20Compression%20with%20Score-based%20Generative%20Models&entry.906535625=Emiel%20Hoogeboom%20and%20Eirikur%20Agustsson%20and%20Fabian%20Mentzer%20and%20Luca%20Versari%20and%20George%20Toderici%20and%20Lucas%20Theis&entry.1292438233=%20%20Despite%20the%20tremendous%20success%20of%20diffusion%20generative%20models%20in%0Atext-to-image%20generation%2C%20replicating%20this%20success%20in%20the%20domain%20of%20image%0Acompression%20has%20proven%20difficult.%20In%20this%20paper%2C%20we%20demonstrate%20that%20diffusion%0Acan%20significantly%20improve%20perceptual%20quality%20at%20a%20given%20bit-rate%2C%20outperforming%0Astate-of-the-art%20approaches%20PO-ELIC%20and%20HiFiC%20as%20measured%20by%20FID%20score.%20This%20is%0Aachieved%20using%20a%20simple%20but%20theoretically%20motivated%20two-stage%20approach%0Acombining%20an%20autoencoder%20targeting%20MSE%20followed%20by%20a%20further%20score-based%0Adecoder.%20However%2C%20as%20we%20will%20show%2C%20implementation%20details%20matter%20and%20the%0Aoptimal%20design%20decisions%20can%20differ%20greatly%20from%20typical%20text-to-image%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.18231v2&entry.124074799=Read"},
{"title": "3D Object Visibility Prediction in Autonomous Driving", "author": "Chuanyu Luo and Nuo Cheng and Ren Zhong and Haipeng Jiang and Wenyu Chen and Aoli Wang and Pu Li", "abstract": "  With the rapid advancement of hardware and software technologies, research in\nautonomous driving has seen significant growth. The prevailing framework for\nmulti-sensor autonomous driving encompasses sensor installation, perception,\npath planning, decision-making, and motion control. At the perception phase, a\ncommon approach involves utilizing neural networks to infer 3D bounding box\n(Bbox) attributes from raw sensor data, including classification, size, and\norientation. In this paper, we present a novel attribute and its corresponding\nalgorithm: 3D object visibility. By incorporating multi-task learning, the\nintroduction of this attribute, visibility, negligibly affects the model's\neffectiveness and efficiency. Our proposal of this attribute and its\ncomputational strategy aims to expand the capabilities for downstream tasks,\nthereby enhancing the safety and reliability of real-time autonomous driving in\nreal-world scenarios.\n", "link": "http://arxiv.org/abs/2403.03681v1", "date": "2024-03-06", "relevancy": 2.3479, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6029}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5898}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5778}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Object%20Visibility%20Prediction%20in%20Autonomous%20Driving&entry.906535625=Chuanyu%20Luo%20and%20Nuo%20Cheng%20and%20Ren%20Zhong%20and%20Haipeng%20Jiang%20and%20Wenyu%20Chen%20and%20Aoli%20Wang%20and%20Pu%20Li&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20hardware%20and%20software%20technologies%2C%20research%20in%0Aautonomous%20driving%20has%20seen%20significant%20growth.%20The%20prevailing%20framework%20for%0Amulti-sensor%20autonomous%20driving%20encompasses%20sensor%20installation%2C%20perception%2C%0Apath%20planning%2C%20decision-making%2C%20and%20motion%20control.%20At%20the%20perception%20phase%2C%20a%0Acommon%20approach%20involves%20utilizing%20neural%20networks%20to%20infer%203D%20bounding%20box%0A%28Bbox%29%20attributes%20from%20raw%20sensor%20data%2C%20including%20classification%2C%20size%2C%20and%0Aorientation.%20In%20this%20paper%2C%20we%20present%20a%20novel%20attribute%20and%20its%20corresponding%0Aalgorithm%3A%203D%20object%20visibility.%20By%20incorporating%20multi-task%20learning%2C%20the%0Aintroduction%20of%20this%20attribute%2C%20visibility%2C%20negligibly%20affects%20the%20model%27s%0Aeffectiveness%20and%20efficiency.%20Our%20proposal%20of%20this%20attribute%20and%20its%0Acomputational%20strategy%20aims%20to%20expand%20the%20capabilities%20for%20downstream%20tasks%2C%0Athereby%20enhancing%20the%20safety%20and%20reliability%20of%20real-time%20autonomous%20driving%20in%0Areal-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03681v1&entry.124074799=Read"},
{"title": "Interpretable Stereotype Identification through Reasoning", "author": "Jacob-Junqi Tian and Omkar Dige and David Emerson and Faiza Khan Khattak", "abstract": "  Given that language models are trained on vast datasets that may contain\ninherent biases, there is a potential danger of inadvertently perpetuating\nsystemic discrimination. Consequently, it becomes essential to examine and\naddress biases in language models, integrating fairness into their development\nto ensure these models are equitable and free from bias. In this work, we\ndemonstrate the importance of reasoning in zero-shot stereotype identification\nbased on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from\n13B to 33B, we show that the performance gain from reasoning significantly\nexceeds the gain from scaling up. Our findings suggest that reasoning could be\na key factor that enables LLMs to trescend the scaling law on out-of-domain\ntasks such as stereotype identification. Additionally, through a qualitative\nanalysis of select reasoning traces, we highlight how reasoning enhances not\njust accuracy but also the interpretability of the decision.\n", "link": "http://arxiv.org/abs/2308.00071v2", "date": "2024-03-06", "relevancy": 2.3431, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4798}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4684}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4576}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Stereotype%20Identification%20through%20Reasoning&entry.906535625=Jacob-Junqi%20Tian%20and%20Omkar%20Dige%20and%20David%20Emerson%20and%20Faiza%20Khan%20Khattak&entry.1292438233=%20%20Given%20that%20language%20models%20are%20trained%20on%20vast%20datasets%20that%20may%20contain%0Ainherent%20biases%2C%20there%20is%20a%20potential%20danger%20of%20inadvertently%20perpetuating%0Asystemic%20discrimination.%20Consequently%2C%20it%20becomes%20essential%20to%20examine%20and%0Aaddress%20biases%20in%20language%20models%2C%20integrating%20fairness%20into%20their%20development%0Ato%20ensure%20these%20models%20are%20equitable%20and%20free%20from%20bias.%20In%20this%20work%2C%20we%0Ademonstrate%20the%20importance%20of%20reasoning%20in%20zero-shot%20stereotype%20identification%0Abased%20on%20Vicuna-13B-v1.3.%20While%20we%20do%20observe%20improved%20accuracy%20by%20scaling%20from%0A13B%20to%2033B%2C%20we%20show%20that%20the%20performance%20gain%20from%20reasoning%20significantly%0Aexceeds%20the%20gain%20from%20scaling%20up.%20Our%20findings%20suggest%20that%20reasoning%20could%20be%0Aa%20key%20factor%20that%20enables%20LLMs%20to%20trescend%20the%20scaling%20law%20on%20out-of-domain%0Atasks%20such%20as%20stereotype%20identification.%20Additionally%2C%20through%20a%20qualitative%0Aanalysis%20of%20select%20reasoning%20traces%2C%20we%20highlight%20how%20reasoning%20enhances%20not%0Ajust%20accuracy%20but%20also%20the%20interpretability%20of%20the%20decision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.00071v2&entry.124074799=Read"},
{"title": "Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm", "author": "Yanqi Qiao and Dazhuang Liu and Rui Wang and Kaitai Liang", "abstract": "  While convolutional neural networks (CNNs) have achieved success in computer\nvision tasks, it is vulnerable to backdoor attacks. Such attacks could mislead\nthe victim model to make attacker-chosen prediction with a specific trigger\npattern. Until now, the trigger injection of existing attacks is mainly limited\nto spatial domain. Recent works take advantage of perceptual properties of\nplanting specific patterns in the frequency domain, which only reflect\nindistinguishable pixel-wise perturbations in pixel domain. However, in the\nblack-box setup, the inaccessibility of training process often renders more\ncomplex trigger designs. Existing frequency attacks simply handcraft the\nmagnitude of spectrum, introducing anomaly frequency disparities between clean\nand poisoned data and taking risks of being removed by image processing\noperations (such as lossy compression and filtering). In this paper, we propose\na robust low-frequency black-box backdoor attack (LFBA), which minimally\nperturbs low-frequency components of frequency spectrum and maintains the\nperceptual similarity in spatial space simultaneously. The key insight of our\nattack restrict the search for the optimal trigger to low-frequency region that\ncan achieve high attack effectiveness, robustness against image transformation\ndefenses and stealthiness in dual space. We utilize simulated annealing (SA), a\nform of evolutionary algorithm, to optimize the properties of frequency trigger\nincluding the number of manipulated frequency bands and the perturbation of\neach frequency component, without relying on the knowledge from the victim\nclassifier. Extensive experiments on real-world datasets verify the\neffectiveness and robustness of LFBA against image processing operations and\nthe state-of-the-art backdoor defenses, as well as its inherent stealthiness in\nboth spatial and frequency space, making it resilient against frequency\ninspection.\n", "link": "http://arxiv.org/abs/2402.15653v2", "date": "2024-03-06", "relevancy": 2.3329, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4845}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4579}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4573}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Frequency%20Black-Box%20Backdoor%20Attack%20via%20Evolutionary%20Algorithm&entry.906535625=Yanqi%20Qiao%20and%20Dazhuang%20Liu%20and%20Rui%20Wang%20and%20Kaitai%20Liang&entry.1292438233=%20%20While%20convolutional%20neural%20networks%20%28CNNs%29%20have%20achieved%20success%20in%20computer%0Avision%20tasks%2C%20it%20is%20vulnerable%20to%20backdoor%20attacks.%20Such%20attacks%20could%20mislead%0Athe%20victim%20model%20to%20make%20attacker-chosen%20prediction%20with%20a%20specific%20trigger%0Apattern.%20Until%20now%2C%20the%20trigger%20injection%20of%20existing%20attacks%20is%20mainly%20limited%0Ato%20spatial%20domain.%20Recent%20works%20take%20advantage%20of%20perceptual%20properties%20of%0Aplanting%20specific%20patterns%20in%20the%20frequency%20domain%2C%20which%20only%20reflect%0Aindistinguishable%20pixel-wise%20perturbations%20in%20pixel%20domain.%20However%2C%20in%20the%0Ablack-box%20setup%2C%20the%20inaccessibility%20of%20training%20process%20often%20renders%20more%0Acomplex%20trigger%20designs.%20Existing%20frequency%20attacks%20simply%20handcraft%20the%0Amagnitude%20of%20spectrum%2C%20introducing%20anomaly%20frequency%20disparities%20between%20clean%0Aand%20poisoned%20data%20and%20taking%20risks%20of%20being%20removed%20by%20image%20processing%0Aoperations%20%28such%20as%20lossy%20compression%20and%20filtering%29.%20In%20this%20paper%2C%20we%20propose%0Aa%20robust%20low-frequency%20black-box%20backdoor%20attack%20%28LFBA%29%2C%20which%20minimally%0Aperturbs%20low-frequency%20components%20of%20frequency%20spectrum%20and%20maintains%20the%0Aperceptual%20similarity%20in%20spatial%20space%20simultaneously.%20The%20key%20insight%20of%20our%0Aattack%20restrict%20the%20search%20for%20the%20optimal%20trigger%20to%20low-frequency%20region%20that%0Acan%20achieve%20high%20attack%20effectiveness%2C%20robustness%20against%20image%20transformation%0Adefenses%20and%20stealthiness%20in%20dual%20space.%20We%20utilize%20simulated%20annealing%20%28SA%29%2C%20a%0Aform%20of%20evolutionary%20algorithm%2C%20to%20optimize%20the%20properties%20of%20frequency%20trigger%0Aincluding%20the%20number%20of%20manipulated%20frequency%20bands%20and%20the%20perturbation%20of%0Aeach%20frequency%20component%2C%20without%20relying%20on%20the%20knowledge%20from%20the%20victim%0Aclassifier.%20Extensive%20experiments%20on%20real-world%20datasets%20verify%20the%0Aeffectiveness%20and%20robustness%20of%20LFBA%20against%20image%20processing%20operations%20and%0Athe%20state-of-the-art%20backdoor%20defenses%2C%20as%20well%20as%20its%20inherent%20stealthiness%20in%0Aboth%20spatial%20and%20frequency%20space%2C%20making%20it%20resilient%20against%20frequency%0Ainspection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15653v2&entry.124074799=Read"},
{"title": "SUPClust: Active Learning at the Boundaries", "author": "Yuta Ono and Till Aczel and Benjamin Estermann and Roger Wattenhofer", "abstract": "  Active learning is a machine learning paradigm designed to optimize model\nperformance in a setting where labeled data is expensive to acquire. In this\nwork, we propose a novel active learning method called SUPClust that seeks to\nidentify points at the decision boundary between classes. By targeting these\npoints, SUPClust aims to gather information that is most informative for\nrefining the model's prediction of complex decision regions. We demonstrate\nexperimentally that labeling these points leads to strong model performance.\nThis improvement is observed even in scenarios characterized by strong class\nimbalance.\n", "link": "http://arxiv.org/abs/2403.03741v1", "date": "2024-03-06", "relevancy": 2.3269, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4728}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4657}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4576}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUPClust%3A%20Active%20Learning%20at%20the%20Boundaries&entry.906535625=Yuta%20Ono%20and%20Till%20Aczel%20and%20Benjamin%20Estermann%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20Active%20learning%20is%20a%20machine%20learning%20paradigm%20designed%20to%20optimize%20model%0Aperformance%20in%20a%20setting%20where%20labeled%20data%20is%20expensive%20to%20acquire.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20active%20learning%20method%20called%20SUPClust%20that%20seeks%20to%0Aidentify%20points%20at%20the%20decision%20boundary%20between%20classes.%20By%20targeting%20these%0Apoints%2C%20SUPClust%20aims%20to%20gather%20information%20that%20is%20most%20informative%20for%0Arefining%20the%20model%27s%20prediction%20of%20complex%20decision%20regions.%20We%20demonstrate%0Aexperimentally%20that%20labeling%20these%20points%20leads%20to%20strong%20model%20performance.%0AThis%20improvement%20is%20observed%20even%20in%20scenarios%20characterized%20by%20strong%20class%0Aimbalance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03741v1&entry.124074799=Read"},
{"title": "Multi-Grained Cross-modal Alignment for Learning Open-vocabulary\n  Semantic Segmentation from Text Supervision", "author": "Yajie Liu and Pu Ge and Qingjie Liu and Di Huang", "abstract": "  Recently, learning open-vocabulary semantic segmentation from text\nsupervision has achieved promising downstream performance. Nevertheless,\ncurrent approaches encounter an alignment granularity gap owing to the absence\nof dense annotations, wherein they learn coarse image/region-text alignment\nduring training yet perform group/pixel-level predictions at inference. Such\ndiscrepancy leads to suboptimal learning efficiency and inferior zero-shot\nsegmentation results. In this paper, we introduce a Multi-Grained Cross-modal\nAlignment (MGCA) framework, which explicitly learns pixel-level alignment along\nwith object- and region-level alignment to bridge the granularity gap without\nany dense annotations. Specifically, MGCA ingeniously constructs pseudo\nmulti-granular semantic correspondences upon image-text pairs and collaborates\nwith hard sampling strategies to facilitate fine-grained cross-modal\ncontrastive learning. Further, we point out the defects of existing group and\npixel prediction units in downstream segmentation and develop an adaptive\nsemantic unit which effectively mitigates their dilemmas including under- and\nover-segmentation. Training solely on CC3M, our method achieves significant\nadvancements over state-of-the-art methods, demonstrating its effectiveness and\nefficiency.\n", "link": "http://arxiv.org/abs/2403.03707v1", "date": "2024-03-06", "relevancy": 2.3093, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6036}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5666}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5385}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Grained%20Cross-modal%20Alignment%20for%20Learning%20Open-vocabulary%0A%20%20Semantic%20Segmentation%20from%20Text%20Supervision&entry.906535625=Yajie%20Liu%20and%20Pu%20Ge%20and%20Qingjie%20Liu%20and%20Di%20Huang&entry.1292438233=%20%20Recently%2C%20learning%20open-vocabulary%20semantic%20segmentation%20from%20text%0Asupervision%20has%20achieved%20promising%20downstream%20performance.%20Nevertheless%2C%0Acurrent%20approaches%20encounter%20an%20alignment%20granularity%20gap%20owing%20to%20the%20absence%0Aof%20dense%20annotations%2C%20wherein%20they%20learn%20coarse%20image/region-text%20alignment%0Aduring%20training%20yet%20perform%20group/pixel-level%20predictions%20at%20inference.%20Such%0Adiscrepancy%20leads%20to%20suboptimal%20learning%20efficiency%20and%20inferior%20zero-shot%0Asegmentation%20results.%20In%20this%20paper%2C%20we%20introduce%20a%20Multi-Grained%20Cross-modal%0AAlignment%20%28MGCA%29%20framework%2C%20which%20explicitly%20learns%20pixel-level%20alignment%20along%0Awith%20object-%20and%20region-level%20alignment%20to%20bridge%20the%20granularity%20gap%20without%0Aany%20dense%20annotations.%20Specifically%2C%20MGCA%20ingeniously%20constructs%20pseudo%0Amulti-granular%20semantic%20correspondences%20upon%20image-text%20pairs%20and%20collaborates%0Awith%20hard%20sampling%20strategies%20to%20facilitate%20fine-grained%20cross-modal%0Acontrastive%20learning.%20Further%2C%20we%20point%20out%20the%20defects%20of%20existing%20group%20and%0Apixel%20prediction%20units%20in%20downstream%20segmentation%20and%20develop%20an%20adaptive%0Asemantic%20unit%20which%20effectively%20mitigates%20their%20dilemmas%20including%20under-%20and%0Aover-segmentation.%20Training%20solely%20on%20CC3M%2C%20our%20method%20achieves%20significant%0Aadvancements%20over%20state-of-the-art%20methods%2C%20demonstrating%20its%20effectiveness%20and%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03707v1&entry.124074799=Read"},
{"title": "3D Diffusion Policy", "author": "Yanjie Ze and Gu Zhang and Kangning Zhang and Chenyuan Hu and Muhan Wang and Huazhe Xu", "abstract": "  Imitation learning provides an efficient way to teach robots dexterous\nskills; however, learning complex skills robustly and generalizablely usually\nconsumes large amounts of human demonstrations. To tackle this challenging\nproblem, we present 3D Diffusion Policy (DP3), a novel visual imitation\nlearning approach that incorporates the power of 3D visual representations into\ndiffusion policies, a class of conditional action generative models. The core\ndesign of DP3 is the utilization of a compact 3D visual representation,\nextracted from sparse point clouds with an efficient point encoder. In our\nexperiments involving 72 simulation tasks, DP3 successfully handles most tasks\nwith just 10 demonstrations and surpasses baselines with a 55.3% relative\nimprovement. In 4 real robot tasks, DP3 demonstrates precise control with a\nhigh success rate of 85%, given only 40 demonstrations of each task, and shows\nexcellent generalization abilities in diverse aspects, including space,\nviewpoint, appearance, and instance. Interestingly, in real robot experiments,\nDP3 rarely violates safety requirements, in contrast to baseline methods which\nfrequently do, necessitating human intervention. Our extensive evaluation\nhighlights the critical importance of 3D representations in real-world robot\nlearning. Videos, code, and data are available on\nhttps://3d-diffusion-policy.github.io .\n", "link": "http://arxiv.org/abs/2403.03954v1", "date": "2024-03-06", "relevancy": 2.2998, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5793}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5758}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5703}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Diffusion%20Policy&entry.906535625=Yanjie%20Ze%20and%20Gu%20Zhang%20and%20Kangning%20Zhang%20and%20Chenyuan%20Hu%20and%20Muhan%20Wang%20and%20Huazhe%20Xu&entry.1292438233=%20%20Imitation%20learning%20provides%20an%20efficient%20way%20to%20teach%20robots%20dexterous%0Askills%3B%20however%2C%20learning%20complex%20skills%20robustly%20and%20generalizablely%20usually%0Aconsumes%20large%20amounts%20of%20human%20demonstrations.%20To%20tackle%20this%20challenging%0Aproblem%2C%20we%20present%203D%20Diffusion%20Policy%20%28DP3%29%2C%20a%20novel%20visual%20imitation%0Alearning%20approach%20that%20incorporates%20the%20power%20of%203D%20visual%20representations%20into%0Adiffusion%20policies%2C%20a%20class%20of%20conditional%20action%20generative%20models.%20The%20core%0Adesign%20of%20DP3%20is%20the%20utilization%20of%20a%20compact%203D%20visual%20representation%2C%0Aextracted%20from%20sparse%20point%20clouds%20with%20an%20efficient%20point%20encoder.%20In%20our%0Aexperiments%20involving%2072%20simulation%20tasks%2C%20DP3%20successfully%20handles%20most%20tasks%0Awith%20just%2010%20demonstrations%20and%20surpasses%20baselines%20with%20a%2055.3%25%20relative%0Aimprovement.%20In%204%20real%20robot%20tasks%2C%20DP3%20demonstrates%20precise%20control%20with%20a%0Ahigh%20success%20rate%20of%2085%25%2C%20given%20only%2040%20demonstrations%20of%20each%20task%2C%20and%20shows%0Aexcellent%20generalization%20abilities%20in%20diverse%20aspects%2C%20including%20space%2C%0Aviewpoint%2C%20appearance%2C%20and%20instance.%20Interestingly%2C%20in%20real%20robot%20experiments%2C%0ADP3%20rarely%20violates%20safety%20requirements%2C%20in%20contrast%20to%20baseline%20methods%20which%0Afrequently%20do%2C%20necessitating%20human%20intervention.%20Our%20extensive%20evaluation%0Ahighlights%20the%20critical%20importance%20of%203D%20representations%20in%20real-world%20robot%0Alearning.%20Videos%2C%20code%2C%20and%20data%20are%20available%20on%0Ahttps%3A//3d-diffusion-policy.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03954v1&entry.124074799=Read"},
{"title": "Dual-IMU State Estimation for Relative Localization of Two Mobile Agents", "author": "Wenqian Lai and Ruonan Guo and Kejian J. Wu", "abstract": "  In this paper, we address the problem of relative localization of two mobile\nagents. Specifically, we consider the Dual-IMU system, where each agent is\nequipped with one IMU, and employs relative pose observations between them.\nPrevious works, however, typically assumed known ego motion and ignored biases\nof the IMUs. Instead, we study the most general case of unknown biases for both\nIMUs. Besides the derivation of dynamic model equations of the proposed system,\nwe focus on the observability analysis, for the observability under general\nmotion and the unobservable directions arising from various special motions.\nThrough numerical simulations, we validate our key observability findings and\nexamine their impact on the estimation accuracy and consistency. Finally, the\nsystem is implemented to achieve effective relative localization of an HMD with\nrespect to a vehicle moving in the real world.\n", "link": "http://arxiv.org/abs/2402.18394v3", "date": "2024-03-06", "relevancy": 2.272, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5703}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5681}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.567}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-IMU%20State%20Estimation%20for%20Relative%20Localization%20of%20Two%20Mobile%20Agents&entry.906535625=Wenqian%20Lai%20and%20Ruonan%20Guo%20and%20Kejian%20J.%20Wu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20relative%20localization%20of%20two%20mobile%0Aagents.%20Specifically%2C%20we%20consider%20the%20Dual-IMU%20system%2C%20where%20each%20agent%20is%0Aequipped%20with%20one%20IMU%2C%20and%20employs%20relative%20pose%20observations%20between%20them.%0APrevious%20works%2C%20however%2C%20typically%20assumed%20known%20ego%20motion%20and%20ignored%20biases%0Aof%20the%20IMUs.%20Instead%2C%20we%20study%20the%20most%20general%20case%20of%20unknown%20biases%20for%20both%0AIMUs.%20Besides%20the%20derivation%20of%20dynamic%20model%20equations%20of%20the%20proposed%20system%2C%0Awe%20focus%20on%20the%20observability%20analysis%2C%20for%20the%20observability%20under%20general%0Amotion%20and%20the%20unobservable%20directions%20arising%20from%20various%20special%20motions.%0AThrough%20numerical%20simulations%2C%20we%20validate%20our%20key%20observability%20findings%20and%0Aexamine%20their%20impact%20on%20the%20estimation%20accuracy%20and%20consistency.%20Finally%2C%20the%0Asystem%20is%20implemented%20to%20achieve%20effective%20relative%20localization%20of%20an%20HMD%20with%0Arespect%20to%20a%20vehicle%20moving%20in%20the%20real%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18394v3&entry.124074799=Read"},
{"title": "SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation", "author": "Jiehong Lin and Lihua Liu and Dekun Lu and Kui Jia", "abstract": "  Zero-shot 6D object pose estimation involves the detection of novel objects\nwith their 6D poses in cluttered scenes, presenting significant challenges for\nmodel generalizability. Fortunately, the recent Segment Anything Model (SAM)\nhas showcased remarkable zero-shot transfer performance, which provides a\npromising solution to tackle this task. Motivated by this, we introduce SAM-6D,\na novel framework designed to realize the task through two steps, including\ninstance segmentation and pose estimation. Given the target objects, SAM-6D\nemploys two dedicated sub-networks, namely Instance Segmentation Model (ISM)\nand Pose Estimation Model (PEM), to perform these steps on cluttered RGB-D\nimages. ISM takes SAM as an advanced starting point to generate all possible\nobject proposals and selectively preserves valid ones through meticulously\ncrafted object matching scores in terms of semantics, appearance and geometry.\nBy treating pose estimation as a partial-to-partial point matching problem, PEM\nperforms a two-stage point matching process featuring a novel design of\nbackground tokens to construct dense 3D-3D correspondence, ultimately yielding\nthe pose estimates. Without bells and whistles, SAM-6D outperforms the existing\nmethods on the seven core datasets of the BOP Benchmark for both instance\nsegmentation and pose estimation of novel objects.\n", "link": "http://arxiv.org/abs/2311.15707v2", "date": "2024-03-06", "relevancy": 2.2287, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5515}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5242}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-6D%3A%20Segment%20Anything%20Model%20Meets%20Zero-Shot%206D%20Object%20Pose%20Estimation&entry.906535625=Jiehong%20Lin%20and%20Lihua%20Liu%20and%20Dekun%20Lu%20and%20Kui%20Jia&entry.1292438233=%20%20Zero-shot%206D%20object%20pose%20estimation%20involves%20the%20detection%20of%20novel%20objects%0Awith%20their%206D%20poses%20in%20cluttered%20scenes%2C%20presenting%20significant%20challenges%20for%0Amodel%20generalizability.%20Fortunately%2C%20the%20recent%20Segment%20Anything%20Model%20%28SAM%29%0Ahas%20showcased%20remarkable%20zero-shot%20transfer%20performance%2C%20which%20provides%20a%0Apromising%20solution%20to%20tackle%20this%20task.%20Motivated%20by%20this%2C%20we%20introduce%20SAM-6D%2C%0Aa%20novel%20framework%20designed%20to%20realize%20the%20task%20through%20two%20steps%2C%20including%0Ainstance%20segmentation%20and%20pose%20estimation.%20Given%20the%20target%20objects%2C%20SAM-6D%0Aemploys%20two%20dedicated%20sub-networks%2C%20namely%20Instance%20Segmentation%20Model%20%28ISM%29%0Aand%20Pose%20Estimation%20Model%20%28PEM%29%2C%20to%20perform%20these%20steps%20on%20cluttered%20RGB-D%0Aimages.%20ISM%20takes%20SAM%20as%20an%20advanced%20starting%20point%20to%20generate%20all%20possible%0Aobject%20proposals%20and%20selectively%20preserves%20valid%20ones%20through%20meticulously%0Acrafted%20object%20matching%20scores%20in%20terms%20of%20semantics%2C%20appearance%20and%20geometry.%0ABy%20treating%20pose%20estimation%20as%20a%20partial-to-partial%20point%20matching%20problem%2C%20PEM%0Aperforms%20a%20two-stage%20point%20matching%20process%20featuring%20a%20novel%20design%20of%0Abackground%20tokens%20to%20construct%20dense%203D-3D%20correspondence%2C%20ultimately%20yielding%0Athe%20pose%20estimates.%20Without%20bells%20and%20whistles%2C%20SAM-6D%20outperforms%20the%20existing%0Amethods%20on%20the%20seven%20core%20datasets%20of%20the%20BOP%20Benchmark%20for%20both%20instance%0Asegmentation%20and%20pose%20estimation%20of%20novel%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15707v2&entry.124074799=Read"},
{"title": "DECap: Towards Generalized Explicit Caption Editing via Diffusion\n  Mechanism", "author": "Zhen Wang and Xinyun Jiang and Jun Xiao and Tao Chen and Long Chen", "abstract": "  Explicit Caption Editing (ECE) -- refining reference image captions through a\nsequence of explicit edit operations (e.g., KEEP, DETELE) -- has raised\nsignificant attention due to its explainable and human-like nature. After\ntraining with carefully designed reference and ground-truth caption pairs,\nstate-of-the-art ECE models exhibit limited generalization ability beyond the\noriginal training data distribution, i.e., they are tailored to refine content\ndetails only in in-domain samples but fail to correct errors in out-of-domain\nsamples. To this end, we propose a new Diffusion-based Explicit Caption editing\nmethod: DECap. Specifically, we reformulate the ECE task as a denoising process\nunder the diffusion mechanism, and introduce innovative edit-based noising and\ndenoising processes. Thanks to this design, the noising process can help to\neliminate the need for meticulous paired data selection by directly introducing\nword-level noises for training, learning diverse distribution over input\nreference caption. The denoising process involves the explicit predictions of\nedit operations and corresponding content words, refining reference captions\nthrough iterative step-wise editing. To further efficiently implement our\ndiffusion process and improve the inference speed, DECap discards the prevalent\nmulti-stage design and directly generates edit operations and content words\nsimultaneously. Extensive ablations have demonstrated the strong generalization\nability of DECap in various scenarios. More interestingly, it even shows great\npotential in improving the quality and controllability of caption generation.\n", "link": "http://arxiv.org/abs/2311.14920v2", "date": "2024-03-06", "relevancy": 2.2243, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5807}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.562}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5291}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DECap%3A%20Towards%20Generalized%20Explicit%20Caption%20Editing%20via%20Diffusion%0A%20%20Mechanism&entry.906535625=Zhen%20Wang%20and%20Xinyun%20Jiang%20and%20Jun%20Xiao%20and%20Tao%20Chen%20and%20Long%20Chen&entry.1292438233=%20%20Explicit%20Caption%20Editing%20%28ECE%29%20--%20refining%20reference%20image%20captions%20through%20a%0Asequence%20of%20explicit%20edit%20operations%20%28e.g.%2C%20KEEP%2C%20DETELE%29%20--%20has%20raised%0Asignificant%20attention%20due%20to%20its%20explainable%20and%20human-like%20nature.%20After%0Atraining%20with%20carefully%20designed%20reference%20and%20ground-truth%20caption%20pairs%2C%0Astate-of-the-art%20ECE%20models%20exhibit%20limited%20generalization%20ability%20beyond%20the%0Aoriginal%20training%20data%20distribution%2C%20i.e.%2C%20they%20are%20tailored%20to%20refine%20content%0Adetails%20only%20in%20in-domain%20samples%20but%20fail%20to%20correct%20errors%20in%20out-of-domain%0Asamples.%20To%20this%20end%2C%20we%20propose%20a%20new%20Diffusion-based%20Explicit%20Caption%20editing%0Amethod%3A%20DECap.%20Specifically%2C%20we%20reformulate%20the%20ECE%20task%20as%20a%20denoising%20process%0Aunder%20the%20diffusion%20mechanism%2C%20and%20introduce%20innovative%20edit-based%20noising%20and%0Adenoising%20processes.%20Thanks%20to%20this%20design%2C%20the%20noising%20process%20can%20help%20to%0Aeliminate%20the%20need%20for%20meticulous%20paired%20data%20selection%20by%20directly%20introducing%0Aword-level%20noises%20for%20training%2C%20learning%20diverse%20distribution%20over%20input%0Areference%20caption.%20The%20denoising%20process%20involves%20the%20explicit%20predictions%20of%0Aedit%20operations%20and%20corresponding%20content%20words%2C%20refining%20reference%20captions%0Athrough%20iterative%20step-wise%20editing.%20To%20further%20efficiently%20implement%20our%0Adiffusion%20process%20and%20improve%20the%20inference%20speed%2C%20DECap%20discards%20the%20prevalent%0Amulti-stage%20design%20and%20directly%20generates%20edit%20operations%20and%20content%20words%0Asimultaneously.%20Extensive%20ablations%20have%20demonstrated%20the%20strong%20generalization%0Aability%20of%20DECap%20in%20various%20scenarios.%20More%20interestingly%2C%20it%20even%20shows%20great%0Apotential%20in%20improving%20the%20quality%20and%20controllability%20of%20caption%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14920v2&entry.124074799=Read"},
{"title": "CDC: A Simple Framework for Complex Data Clustering", "author": "Zhao Kang and Xuanting Xie and Bingheng Li and Erlin Pan", "abstract": "  In today's data-driven digital era, the amount as well as complexity, such as\nmulti-view, non-Euclidean, and multi-relational, of the collected data are\ngrowing exponentially or even faster. Clustering, which unsupervisely extracts\nvalid knowledge from data, is extremely useful in practice. However, existing\nmethods are independently developed to handle one particular challenge at the\nexpense of the others. In this work, we propose a simple but effective\nframework for complex data clustering (CDC) that can efficiently process\ndifferent types of data with linear complexity. We first utilize graph\nfiltering to fuse geometry structure and attribute information. We then reduce\nthe complexity with high-quality anchors that are adaptively learned via a\nnovel similarity-preserving regularizer. We illustrate the cluster-ability of\nour proposed method theoretically and experimentally. In particular, we deploy\nCDC to graph data of size 111M.\n", "link": "http://arxiv.org/abs/2403.03670v1", "date": "2024-03-06", "relevancy": 2.2228, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4653}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4428}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4256}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDC%3A%20A%20Simple%20Framework%20for%20Complex%20Data%20Clustering&entry.906535625=Zhao%20Kang%20and%20Xuanting%20Xie%20and%20Bingheng%20Li%20and%20Erlin%20Pan&entry.1292438233=%20%20In%20today%27s%20data-driven%20digital%20era%2C%20the%20amount%20as%20well%20as%20complexity%2C%20such%20as%0Amulti-view%2C%20non-Euclidean%2C%20and%20multi-relational%2C%20of%20the%20collected%20data%20are%0Agrowing%20exponentially%20or%20even%20faster.%20Clustering%2C%20which%20unsupervisely%20extracts%0Avalid%20knowledge%20from%20data%2C%20is%20extremely%20useful%20in%20practice.%20However%2C%20existing%0Amethods%20are%20independently%20developed%20to%20handle%20one%20particular%20challenge%20at%20the%0Aexpense%20of%20the%20others.%20In%20this%20work%2C%20we%20propose%20a%20simple%20but%20effective%0Aframework%20for%20complex%20data%20clustering%20%28CDC%29%20that%20can%20efficiently%20process%0Adifferent%20types%20of%20data%20with%20linear%20complexity.%20We%20first%20utilize%20graph%0Afiltering%20to%20fuse%20geometry%20structure%20and%20attribute%20information.%20We%20then%20reduce%0Athe%20complexity%20with%20high-quality%20anchors%20that%20are%20adaptively%20learned%20via%20a%0Anovel%20similarity-preserving%20regularizer.%20We%20illustrate%20the%20cluster-ability%20of%0Aour%20proposed%20method%20theoretically%20and%20experimentally.%20In%20particular%2C%20we%20deploy%0ACDC%20to%20graph%20data%20of%20size%20111M.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03670v1&entry.124074799=Read"},
{"title": "ECAP: Extensive Cut-and-Paste Augmentation for Unsupervised Domain\n  Adaptive Semantic Segmentation", "author": "Erik Brorsson and Knut \u00c5kesson and Lennart Svensson and Kristofer Bengtsson", "abstract": "  We consider unsupervised domain adaptation (UDA) for semantic segmentation in\nwhich the model is trained on a labeled source dataset and adapted to an\nunlabeled target dataset. Unfortunately, current self-training methods are\nsusceptible to misclassified pseudo-labels resulting from erroneous\npredictions. Since certain classes are typically associated with less reliable\npredictions in UDA, reducing the impact of such pseudo-labels without skewing\nthe training towards some classes is notoriously difficult. To this end, we\npropose an extensive cut-and-paste strategy (ECAP) to leverage reliable\npseudo-labels through data augmentation. Specifically, ECAP maintains a memory\nbank of pseudo-labeled target samples throughout training and cut-and-pastes\nthe most confident ones onto the current training batch. We implement ECAP on\ntop of the recent method MIC and boost its performance on two synthetic-to-real\ndomain adaptation benchmarks. Notably, MIC+ECAP reaches an unprecedented\nperformance of 69.1 mIoU on the Synthia->Cityscapes benchmark. Our code is\navailable at https://github.com/ErikBrorsson/ECAP.\n", "link": "http://arxiv.org/abs/2403.03854v1", "date": "2024-03-06", "relevancy": 2.2221, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5625}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5487}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECAP%3A%20Extensive%20Cut-and-Paste%20Augmentation%20for%20Unsupervised%20Domain%0A%20%20Adaptive%20Semantic%20Segmentation&entry.906535625=Erik%20Brorsson%20and%20Knut%20%C3%85kesson%20and%20Lennart%20Svensson%20and%20Kristofer%20Bengtsson&entry.1292438233=%20%20We%20consider%20unsupervised%20domain%20adaptation%20%28UDA%29%20for%20semantic%20segmentation%20in%0Awhich%20the%20model%20is%20trained%20on%20a%20labeled%20source%20dataset%20and%20adapted%20to%20an%0Aunlabeled%20target%20dataset.%20Unfortunately%2C%20current%20self-training%20methods%20are%0Asusceptible%20to%20misclassified%20pseudo-labels%20resulting%20from%20erroneous%0Apredictions.%20Since%20certain%20classes%20are%20typically%20associated%20with%20less%20reliable%0Apredictions%20in%20UDA%2C%20reducing%20the%20impact%20of%20such%20pseudo-labels%20without%20skewing%0Athe%20training%20towards%20some%20classes%20is%20notoriously%20difficult.%20To%20this%20end%2C%20we%0Apropose%20an%20extensive%20cut-and-paste%20strategy%20%28ECAP%29%20to%20leverage%20reliable%0Apseudo-labels%20through%20data%20augmentation.%20Specifically%2C%20ECAP%20maintains%20a%20memory%0Abank%20of%20pseudo-labeled%20target%20samples%20throughout%20training%20and%20cut-and-pastes%0Athe%20most%20confident%20ones%20onto%20the%20current%20training%20batch.%20We%20implement%20ECAP%20on%0Atop%20of%20the%20recent%20method%20MIC%20and%20boost%20its%20performance%20on%20two%20synthetic-to-real%0Adomain%20adaptation%20benchmarks.%20Notably%2C%20MIC%2BECAP%20reaches%20an%20unprecedented%0Aperformance%20of%2069.1%20mIoU%20on%20the%20Synthia-%3ECityscapes%20benchmark.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/ErikBrorsson/ECAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03854v1&entry.124074799=Read"},
{"title": "RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud", "author": "Zhijun Pan and Fangqiang Ding and Hantao Zhong and Chris Xiaoxuan Lu", "abstract": "  Mobile autonomy relies on the precise perception of dynamic environments.\nRobustly tracking moving objects in 3D world thus plays a pivotal role for\napplications like trajectory prediction, obstacle avoidance, and path planning.\nWhile most current methods utilize LiDARs or cameras for Multiple Object\nTracking (MOT), the capabilities of 4D imaging radars remain largely\nunexplored. Recognizing the challenges posed by radar noise and point sparsity\nin 4D radar data, we introduce RaTrack, an innovative solution tailored for\nradar-based tracking. Bypassing the typical reliance on specific object types\nand 3D bounding boxes, our method focuses on motion segmentation and\nclustering, enriched by a motion estimation module. Evaluated on the\nView-of-Delft dataset, RaTrack showcases superior tracking precision of moving\nobjects, largely surpassing the performance of the state of the art. We release\nour code and model at https://github.com/LJacksonPan/RaTrack.\n", "link": "http://arxiv.org/abs/2309.09737v6", "date": "2024-03-06", "relevancy": 2.2147, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5653}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5579}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaTrack%3A%20Moving%20Object%20Detection%20and%20Tracking%20with%204D%20Radar%20Point%20Cloud&entry.906535625=Zhijun%20Pan%20and%20Fangqiang%20Ding%20and%20Hantao%20Zhong%20and%20Chris%20Xiaoxuan%20Lu&entry.1292438233=%20%20Mobile%20autonomy%20relies%20on%20the%20precise%20perception%20of%20dynamic%20environments.%0ARobustly%20tracking%20moving%20objects%20in%203D%20world%20thus%20plays%20a%20pivotal%20role%20for%0Aapplications%20like%20trajectory%20prediction%2C%20obstacle%20avoidance%2C%20and%20path%20planning.%0AWhile%20most%20current%20methods%20utilize%20LiDARs%20or%20cameras%20for%20Multiple%20Object%0ATracking%20%28MOT%29%2C%20the%20capabilities%20of%204D%20imaging%20radars%20remain%20largely%0Aunexplored.%20Recognizing%20the%20challenges%20posed%20by%20radar%20noise%20and%20point%20sparsity%0Ain%204D%20radar%20data%2C%20we%20introduce%20RaTrack%2C%20an%20innovative%20solution%20tailored%20for%0Aradar-based%20tracking.%20Bypassing%20the%20typical%20reliance%20on%20specific%20object%20types%0Aand%203D%20bounding%20boxes%2C%20our%20method%20focuses%20on%20motion%20segmentation%20and%0Aclustering%2C%20enriched%20by%20a%20motion%20estimation%20module.%20Evaluated%20on%20the%0AView-of-Delft%20dataset%2C%20RaTrack%20showcases%20superior%20tracking%20precision%20of%20moving%0Aobjects%2C%20largely%20surpassing%20the%20performance%20of%20the%20state%20of%20the%20art.%20We%20release%0Aour%20code%20and%20model%20at%20https%3A//github.com/LJacksonPan/RaTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09737v6&entry.124074799=Read"},
{"title": "MeaCap: Memory-Augmented Zero-shot Image Captioning", "author": "Zequn Zeng and Yan Xie and Hao Zhang and Chiyu Chen and Zhengjue Wang and Bo Chen", "abstract": "  Zero-shot image captioning (IC) without well-paired image-text data can be\ndivided into two categories, training-free and text-only-training. Generally,\nthese two types of methods realize zero-shot IC by integrating pretrained\nvision-language models like CLIP for image-text similarity evaluation and a\npre-trained language model (LM) for caption generation. The main difference\nbetween them is whether using a textual corpus to train the LM. Though\nachieving attractive performance w.r.t. some metrics, existing methods often\nexhibit some common drawbacks. Training-free methods tend to produce\nhallucinations, while text-only-training often lose generalization capability.\nTo move forward, in this paper, we propose a novel Memory-Augmented zero-shot\nimage Captioning framework (MeaCap). Specifically, equipped with a textual\nmemory, we introduce a retrieve-then-filter module to get key concepts that are\nhighly related to the image. By deploying our proposed memory-augmented\nvisual-related fusion score in a keywords-to-sentence LM, MeaCap can generate\nconcept-centered captions that keep high consistency with the image with fewer\nhallucinations and more world-knowledge. The framework of MeaCap achieves the\nstate-of-the-art performance on a series of zero-shot IC settings. Our code is\navailable at https://github.com/joeyz0z/MeaCap.\n", "link": "http://arxiv.org/abs/2403.03715v1", "date": "2024-03-06", "relevancy": 2.2116, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5757}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5521}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4979}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeaCap%3A%20Memory-Augmented%20Zero-shot%20Image%20Captioning&entry.906535625=Zequn%20Zeng%20and%20Yan%20Xie%20and%20Hao%20Zhang%20and%20Chiyu%20Chen%20and%20Zhengjue%20Wang%20and%20Bo%20Chen&entry.1292438233=%20%20Zero-shot%20image%20captioning%20%28IC%29%20without%20well-paired%20image-text%20data%20can%20be%0Adivided%20into%20two%20categories%2C%20training-free%20and%20text-only-training.%20Generally%2C%0Athese%20two%20types%20of%20methods%20realize%20zero-shot%20IC%20by%20integrating%20pretrained%0Avision-language%20models%20like%20CLIP%20for%20image-text%20similarity%20evaluation%20and%20a%0Apre-trained%20language%20model%20%28LM%29%20for%20caption%20generation.%20The%20main%20difference%0Abetween%20them%20is%20whether%20using%20a%20textual%20corpus%20to%20train%20the%20LM.%20Though%0Aachieving%20attractive%20performance%20w.r.t.%20some%20metrics%2C%20existing%20methods%20often%0Aexhibit%20some%20common%20drawbacks.%20Training-free%20methods%20tend%20to%20produce%0Ahallucinations%2C%20while%20text-only-training%20often%20lose%20generalization%20capability.%0ATo%20move%20forward%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20Memory-Augmented%20zero-shot%0Aimage%20Captioning%20framework%20%28MeaCap%29.%20Specifically%2C%20equipped%20with%20a%20textual%0Amemory%2C%20we%20introduce%20a%20retrieve-then-filter%20module%20to%20get%20key%20concepts%20that%20are%0Ahighly%20related%20to%20the%20image.%20By%20deploying%20our%20proposed%20memory-augmented%0Avisual-related%20fusion%20score%20in%20a%20keywords-to-sentence%20LM%2C%20MeaCap%20can%20generate%0Aconcept-centered%20captions%20that%20keep%20high%20consistency%20with%20the%20image%20with%20fewer%0Ahallucinations%20and%20more%20world-knowledge.%20The%20framework%20of%20MeaCap%20achieves%20the%0Astate-of-the-art%20performance%20on%20a%20series%20of%20zero-shot%20IC%20settings.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/joeyz0z/MeaCap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03715v1&entry.124074799=Read"},
{"title": "Harnessing Meta-Learning for Improving Full-Frame Video Stabilization", "author": "Muhammad Kashif Ali and Eun Woo Im and Dongjin Kim and Tae Hyun Kim", "abstract": "  Video stabilization is a longstanding computer vision problem, particularly\npixel-level synthesis solutions for video stabilization which synthesize full\nframes add to the complexity of this task. These techniques aim to stabilize\nvideos by synthesizing full frames while enhancing the stability of the\nconsidered video. This intensifies the complexity of the task due to the\ndistinct mix of unique motion profiles and visual content present in each video\nsequence, making robust generalization with fixed parameters difficult. In our\nstudy, we introduce a novel approach to enhance the performance of pixel-level\nsynthesis solutions for video stabilization by adapting these models to\nindividual input video sequences. The proposed adaptation exploits low-level\nvisual cues accessible during test-time to improve both the stability and\nquality of resulting videos. We highlight the efficacy of our methodology of\n\"test-time adaptation\" through simple fine-tuning of one of these models,\nfollowed by significant stability gain via the integration of meta-learning\ntechniques. Notably, significant improvement is achieved with only a single\nadaptation step. The versatility of the proposed algorithm is demonstrated by\nconsistently improving the performance of various pixel-level synthesis models\nfor video stabilization in real-world scenarios.\n", "link": "http://arxiv.org/abs/2403.03662v1", "date": "2024-03-06", "relevancy": 2.2105, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5708}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.564}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.534}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Meta-Learning%20for%20Improving%20Full-Frame%20Video%20Stabilization&entry.906535625=Muhammad%20Kashif%20Ali%20and%20Eun%20Woo%20Im%20and%20Dongjin%20Kim%20and%20Tae%20Hyun%20Kim&entry.1292438233=%20%20Video%20stabilization%20is%20a%20longstanding%20computer%20vision%20problem%2C%20particularly%0Apixel-level%20synthesis%20solutions%20for%20video%20stabilization%20which%20synthesize%20full%0Aframes%20add%20to%20the%20complexity%20of%20this%20task.%20These%20techniques%20aim%20to%20stabilize%0Avideos%20by%20synthesizing%20full%20frames%20while%20enhancing%20the%20stability%20of%20the%0Aconsidered%20video.%20This%20intensifies%20the%20complexity%20of%20the%20task%20due%20to%20the%0Adistinct%20mix%20of%20unique%20motion%20profiles%20and%20visual%20content%20present%20in%20each%20video%0Asequence%2C%20making%20robust%20generalization%20with%20fixed%20parameters%20difficult.%20In%20our%0Astudy%2C%20we%20introduce%20a%20novel%20approach%20to%20enhance%20the%20performance%20of%20pixel-level%0Asynthesis%20solutions%20for%20video%20stabilization%20by%20adapting%20these%20models%20to%0Aindividual%20input%20video%20sequences.%20The%20proposed%20adaptation%20exploits%20low-level%0Avisual%20cues%20accessible%20during%20test-time%20to%20improve%20both%20the%20stability%20and%0Aquality%20of%20resulting%20videos.%20We%20highlight%20the%20efficacy%20of%20our%20methodology%20of%0A%22test-time%20adaptation%22%20through%20simple%20fine-tuning%20of%20one%20of%20these%20models%2C%0Afollowed%20by%20significant%20stability%20gain%20via%20the%20integration%20of%20meta-learning%0Atechniques.%20Notably%2C%20significant%20improvement%20is%20achieved%20with%20only%20a%20single%0Aadaptation%20step.%20The%20versatility%20of%20the%20proposed%20algorithm%20is%20demonstrated%20by%0Aconsistently%20improving%20the%20performance%20of%20various%20pixel-level%20synthesis%20models%0Afor%20video%20stabilization%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03662v1&entry.124074799=Read"},
{"title": "Bridging Diversity and Uncertainty in Active learning with\n  Self-Supervised Pre-Training", "author": "Paul Doucet and Benjamin Estermann and Till Aczel and Roger Wattenhofer", "abstract": "  This study addresses the integration of diversity-based and uncertainty-based\nsampling strategies in active learning, particularly within the context of\nself-supervised pre-trained models. We introduce a straightforward heuristic\ncalled TCM that mitigates the cold start problem while maintaining strong\nperformance across various data levels. By initially applying TypiClust for\ndiversity sampling and subsequently transitioning to uncertainty sampling with\nMargin, our approach effectively combines the strengths of both strategies. Our\nexperiments demonstrate that TCM consistently outperforms existing methods\nacross various datasets in both low and high data regimes.\n", "link": "http://arxiv.org/abs/2403.03728v1", "date": "2024-03-06", "relevancy": 2.1826, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5537}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.545}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.527}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Diversity%20and%20Uncertainty%20in%20Active%20learning%20with%0A%20%20Self-Supervised%20Pre-Training&entry.906535625=Paul%20Doucet%20and%20Benjamin%20Estermann%20and%20Till%20Aczel%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20This%20study%20addresses%20the%20integration%20of%20diversity-based%20and%20uncertainty-based%0Asampling%20strategies%20in%20active%20learning%2C%20particularly%20within%20the%20context%20of%0Aself-supervised%20pre-trained%20models.%20We%20introduce%20a%20straightforward%20heuristic%0Acalled%20TCM%20that%20mitigates%20the%20cold%20start%20problem%20while%20maintaining%20strong%0Aperformance%20across%20various%20data%20levels.%20By%20initially%20applying%20TypiClust%20for%0Adiversity%20sampling%20and%20subsequently%20transitioning%20to%20uncertainty%20sampling%20with%0AMargin%2C%20our%20approach%20effectively%20combines%20the%20strengths%20of%20both%20strategies.%20Our%0Aexperiments%20demonstrate%20that%20TCM%20consistently%20outperforms%20existing%20methods%0Aacross%20various%20datasets%20in%20both%20low%20and%20high%20data%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03728v1&entry.124074799=Read"},
{"title": "Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection\n  from Remote Sensing Imagery", "author": "Wei Zhang and Miaoxin Cai and Tong Zhang and Guoqiang Lei and Yin Zhuang and Xuerui Mao", "abstract": "  Ship detection needs to identify ship locations from remote sensing (RS)\nscenes. However, due to different imaging payloads, various appearances of\nships, and complicated background interference from the bird's eye view, it is\ndifficult to set up a unified paradigm for achieving multi-source ship\ndetection. Therefore, in this article, considering that the large language\nmodels (LLMs) emerge the powerful generalization ability, a novel unified\nvisual-language model called Popeye is proposed for multi-source ship detection\nfrom RS imagery. First, to bridge the interpretation gap between multi-source\nimages for ship detection, a novel image-instruction-answer way is designed to\nintegrate the various ship detection ways (e.g., horizontal bounding box (HBB),\noriented bounding box (OBB)) into a unified labeling paradigm. Then, in view of\nthis, a cross-modal image interpretation method is developed for the proposed\nPopeye to enhance interactive comprehension ability between visual and language\ncontent, which can be easily migrated into any multi-source ship detection\ntask. Subsequently, owing to objective domain differences, a knowledge adaption\nmechanism is designed to adapt the pre-trained visual-language knowledge from\nthe nature scene into the RS domain for multi-source ship detection. In\naddition, the segment anything model (SAM) is also seamlessly integrated into\nthe proposed Popeye to achieve pixel-level ship segmentation without additional\ntraining costs. Finally, extensive experiments are conducted on the newly\nconstructed instruction dataset named MMShip, and the results indicate that the\nproposed Popeye outperforms current specialist, open-vocabulary, and other\nvisual-language models for zero-shot multi-source ship detection.\n", "link": "http://arxiv.org/abs/2403.03790v1", "date": "2024-03-06", "relevancy": 2.1799, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5626}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5421}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5082}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Popeye%3A%20A%20Unified%20Visual-Language%20Model%20for%20Multi-Source%20Ship%20Detection%0A%20%20from%20Remote%20Sensing%20Imagery&entry.906535625=Wei%20Zhang%20and%20Miaoxin%20Cai%20and%20Tong%20Zhang%20and%20Guoqiang%20Lei%20and%20Yin%20Zhuang%20and%20Xuerui%20Mao&entry.1292438233=%20%20Ship%20detection%20needs%20to%20identify%20ship%20locations%20from%20remote%20sensing%20%28RS%29%0Ascenes.%20However%2C%20due%20to%20different%20imaging%20payloads%2C%20various%20appearances%20of%0Aships%2C%20and%20complicated%20background%20interference%20from%20the%20bird%27s%20eye%20view%2C%20it%20is%0Adifficult%20to%20set%20up%20a%20unified%20paradigm%20for%20achieving%20multi-source%20ship%0Adetection.%20Therefore%2C%20in%20this%20article%2C%20considering%20that%20the%20large%20language%0Amodels%20%28LLMs%29%20emerge%20the%20powerful%20generalization%20ability%2C%20a%20novel%20unified%0Avisual-language%20model%20called%20Popeye%20is%20proposed%20for%20multi-source%20ship%20detection%0Afrom%20RS%20imagery.%20First%2C%20to%20bridge%20the%20interpretation%20gap%20between%20multi-source%0Aimages%20for%20ship%20detection%2C%20a%20novel%20image-instruction-answer%20way%20is%20designed%20to%0Aintegrate%20the%20various%20ship%20detection%20ways%20%28e.g.%2C%20horizontal%20bounding%20box%20%28HBB%29%2C%0Aoriented%20bounding%20box%20%28OBB%29%29%20into%20a%20unified%20labeling%20paradigm.%20Then%2C%20in%20view%20of%0Athis%2C%20a%20cross-modal%20image%20interpretation%20method%20is%20developed%20for%20the%20proposed%0APopeye%20to%20enhance%20interactive%20comprehension%20ability%20between%20visual%20and%20language%0Acontent%2C%20which%20can%20be%20easily%20migrated%20into%20any%20multi-source%20ship%20detection%0Atask.%20Subsequently%2C%20owing%20to%20objective%20domain%20differences%2C%20a%20knowledge%20adaption%0Amechanism%20is%20designed%20to%20adapt%20the%20pre-trained%20visual-language%20knowledge%20from%0Athe%20nature%20scene%20into%20the%20RS%20domain%20for%20multi-source%20ship%20detection.%20In%0Aaddition%2C%20the%20segment%20anything%20model%20%28SAM%29%20is%20also%20seamlessly%20integrated%20into%0Athe%20proposed%20Popeye%20to%20achieve%20pixel-level%20ship%20segmentation%20without%20additional%0Atraining%20costs.%20Finally%2C%20extensive%20experiments%20are%20conducted%20on%20the%20newly%0Aconstructed%20instruction%20dataset%20named%20MMShip%2C%20and%20the%20results%20indicate%20that%20the%0Aproposed%20Popeye%20outperforms%20current%20specialist%2C%20open-vocabulary%2C%20and%20other%0Avisual-language%20models%20for%20zero-shot%20multi-source%20ship%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03790v1&entry.124074799=Read"},
{"title": "Neural Exec: Learning (and Learning from) Execution Triggers for Prompt\n  Injection Attacks", "author": "Dario Pasquini and Martin Strohmeier and Carmela Troncoso", "abstract": "  We introduce a new family of prompt injection attacks, termed Neural Exec.\nUnlike known attacks that rely on handcrafted strings (e.g., \"Ignore previous\ninstructions and...\"), we show that it is possible to conceptualize the\ncreation of execution triggers as a differentiable search problem and use\nlearning-based methods to autonomously generate them.\n  Our results demonstrate that a motivated adversary can forge triggers that\nare not only drastically more effective than current handcrafted ones but also\nexhibit inherent flexibility in shape, properties, and functionality. In this\ndirection, we show that an attacker can design and generate Neural Execs\ncapable of persisting through multi-stage preprocessing pipelines, such as in\nthe case of Retrieval-Augmented Generation (RAG)-based applications. More\ncritically, our findings show that attackers can produce triggers that deviate\nmarkedly in form and shape from any known attack, sidestepping existing\nblacklist-based detection and sanitation approaches.\n", "link": "http://arxiv.org/abs/2403.03792v1", "date": "2024-03-06", "relevancy": 2.1742, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4602}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4468}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3976}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Exec%3A%20Learning%20%28and%20Learning%20from%29%20Execution%20Triggers%20for%20Prompt%0A%20%20Injection%20Attacks&entry.906535625=Dario%20Pasquini%20and%20Martin%20Strohmeier%20and%20Carmela%20Troncoso&entry.1292438233=%20%20We%20introduce%20a%20new%20family%20of%20prompt%20injection%20attacks%2C%20termed%20Neural%20Exec.%0AUnlike%20known%20attacks%20that%20rely%20on%20handcrafted%20strings%20%28e.g.%2C%20%22Ignore%20previous%0Ainstructions%20and...%22%29%2C%20we%20show%20that%20it%20is%20possible%20to%20conceptualize%20the%0Acreation%20of%20execution%20triggers%20as%20a%20differentiable%20search%20problem%20and%20use%0Alearning-based%20methods%20to%20autonomously%20generate%20them.%0A%20%20Our%20results%20demonstrate%20that%20a%20motivated%20adversary%20can%20forge%20triggers%20that%0Aare%20not%20only%20drastically%20more%20effective%20than%20current%20handcrafted%20ones%20but%20also%0Aexhibit%20inherent%20flexibility%20in%20shape%2C%20properties%2C%20and%20functionality.%20In%20this%0Adirection%2C%20we%20show%20that%20an%20attacker%20can%20design%20and%20generate%20Neural%20Execs%0Acapable%20of%20persisting%20through%20multi-stage%20preprocessing%20pipelines%2C%20such%20as%20in%0Athe%20case%20of%20Retrieval-Augmented%20Generation%20%28RAG%29-based%20applications.%20More%0Acritically%2C%20our%20findings%20show%20that%20attackers%20can%20produce%20triggers%20that%20deviate%0Amarkedly%20in%20form%20and%20shape%20from%20any%20known%20attack%2C%20sidestepping%20existing%0Ablacklist-based%20detection%20and%20sanitation%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03792v1&entry.124074799=Read"},
{"title": "SemSegDepth: A Combined Model for Semantic Segmentation and Depth\n  Completion", "author": "Juan Pablo Lagos and Esa Rahtu", "abstract": "  Holistic scene understanding is pivotal for the performance of autonomous\nmachines. In this paper we propose a new end-to-end model for performing\nsemantic segmentation and depth completion jointly. The vast majority of recent\napproaches have developed semantic segmentation and depth completion as\nindependent tasks. Our approach relies on RGB and sparse depth as inputs to our\nmodel and produces a dense depth map and the corresponding semantic\nsegmentation image. It consists of a feature extractor, a depth completion\nbranch, a semantic segmentation branch and a joint branch which further\nprocesses semantic and depth information altogether. The experiments done on\nVirtual KITTI 2 dataset, demonstrate and provide further evidence, that\ncombining both tasks, semantic segmentation and depth completion, in a\nmulti-task network can effectively improve the performance of each task. Code\nis available at https://github.com/juanb09111/semantic depth.\n", "link": "http://arxiv.org/abs/2209.00381v2", "date": "2024-03-06", "relevancy": 2.1709, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5587}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5398}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5392}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemSegDepth%3A%20A%20Combined%20Model%20for%20Semantic%20Segmentation%20and%20Depth%0A%20%20Completion&entry.906535625=Juan%20Pablo%20Lagos%20and%20Esa%20Rahtu&entry.1292438233=%20%20Holistic%20scene%20understanding%20is%20pivotal%20for%20the%20performance%20of%20autonomous%0Amachines.%20In%20this%20paper%20we%20propose%20a%20new%20end-to-end%20model%20for%20performing%0Asemantic%20segmentation%20and%20depth%20completion%20jointly.%20The%20vast%20majority%20of%20recent%0Aapproaches%20have%20developed%20semantic%20segmentation%20and%20depth%20completion%20as%0Aindependent%20tasks.%20Our%20approach%20relies%20on%20RGB%20and%20sparse%20depth%20as%20inputs%20to%20our%0Amodel%20and%20produces%20a%20dense%20depth%20map%20and%20the%20corresponding%20semantic%0Asegmentation%20image.%20It%20consists%20of%20a%20feature%20extractor%2C%20a%20depth%20completion%0Abranch%2C%20a%20semantic%20segmentation%20branch%20and%20a%20joint%20branch%20which%20further%0Aprocesses%20semantic%20and%20depth%20information%20altogether.%20The%20experiments%20done%20on%0AVirtual%20KITTI%202%20dataset%2C%20demonstrate%20and%20provide%20further%20evidence%2C%20that%0Acombining%20both%20tasks%2C%20semantic%20segmentation%20and%20depth%20completion%2C%20in%20a%0Amulti-task%20network%20can%20effectively%20improve%20the%20performance%20of%20each%20task.%20Code%0Ais%20available%20at%20https%3A//github.com/juanb09111/semantic%20depth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.00381v2&entry.124074799=Read"},
{"title": "Self and Mixed Supervision to Improve Training Labels for Multi-Class\n  Medical Image Segmentation", "author": "Jianfei Liu and Christopher Parnell and Ronald M. Summers", "abstract": "  Accurate training labels are a key component for multi-class medical image\nsegmentation. Their annotation is costly and time-consuming because it requires\ndomain expertise. This work aims to develop a dual-branch network and\nautomatically improve training labels for multi-class image segmentation.\nTransfer learning is used to train the network and improve inaccurate weak\nlabels sequentially. The dual-branch network is first trained by weak labels\nalone to initialize model parameters. After the network is stabilized, the\nshared encoder is frozen, and strong and weak decoders are fine-tuned by strong\nand weak labels together. The accuracy of weak labels is iteratively improved\nin the fine-tuning process. The proposed method was applied to a three-class\nsegmentation of muscle, subcutaneous and visceral adipose tissue on abdominal\nCT scans. Validation results on 11 patients showed that the accuracy of\ntraining labels was statistically significantly improved, with the Dice\nsimilarity coefficient of muscle, subcutaneous and visceral adipose tissue\nincreased from 74.2% to 91.5%, 91.2% to 95.6%, and 77.6% to 88.5%, respectively\n(p<0.05). In comparison with our earlier method, the label accuracy was also\nsignificantly improved (p<0.05). These experimental results suggested that the\ncombination of the dual-branch network and transfer learning is an efficient\nmeans to improve training labels for multi-class segmentation.\n", "link": "http://arxiv.org/abs/2403.03882v1", "date": "2024-03-06", "relevancy": 2.1597, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5595}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5222}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self%20and%20Mixed%20Supervision%20to%20Improve%20Training%20Labels%20for%20Multi-Class%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Jianfei%20Liu%20and%20Christopher%20Parnell%20and%20Ronald%20M.%20Summers&entry.1292438233=%20%20Accurate%20training%20labels%20are%20a%20key%20component%20for%20multi-class%20medical%20image%0Asegmentation.%20Their%20annotation%20is%20costly%20and%20time-consuming%20because%20it%20requires%0Adomain%20expertise.%20This%20work%20aims%20to%20develop%20a%20dual-branch%20network%20and%0Aautomatically%20improve%20training%20labels%20for%20multi-class%20image%20segmentation.%0ATransfer%20learning%20is%20used%20to%20train%20the%20network%20and%20improve%20inaccurate%20weak%0Alabels%20sequentially.%20The%20dual-branch%20network%20is%20first%20trained%20by%20weak%20labels%0Aalone%20to%20initialize%20model%20parameters.%20After%20the%20network%20is%20stabilized%2C%20the%0Ashared%20encoder%20is%20frozen%2C%20and%20strong%20and%20weak%20decoders%20are%20fine-tuned%20by%20strong%0Aand%20weak%20labels%20together.%20The%20accuracy%20of%20weak%20labels%20is%20iteratively%20improved%0Ain%20the%20fine-tuning%20process.%20The%20proposed%20method%20was%20applied%20to%20a%20three-class%0Asegmentation%20of%20muscle%2C%20subcutaneous%20and%20visceral%20adipose%20tissue%20on%20abdominal%0ACT%20scans.%20Validation%20results%20on%2011%20patients%20showed%20that%20the%20accuracy%20of%0Atraining%20labels%20was%20statistically%20significantly%20improved%2C%20with%20the%20Dice%0Asimilarity%20coefficient%20of%20muscle%2C%20subcutaneous%20and%20visceral%20adipose%20tissue%0Aincreased%20from%2074.2%25%20to%2091.5%25%2C%2091.2%25%20to%2095.6%25%2C%20and%2077.6%25%20to%2088.5%25%2C%20respectively%0A%28p%3C0.05%29.%20In%20comparison%20with%20our%20earlier%20method%2C%20the%20label%20accuracy%20was%20also%0Asignificantly%20improved%20%28p%3C0.05%29.%20These%20experimental%20results%20suggested%20that%20the%0Acombination%20of%20the%20dual-branch%20network%20and%20transfer%20learning%20is%20an%20efficient%0Ameans%20to%20improve%20training%20labels%20for%20multi-class%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03882v1&entry.124074799=Read"},
{"title": "Directional Texture Editing for 3D Models", "author": "Shengqi Liu and Zhuo Chen and Jingnan Gao and Yichao Yan and Wenhan Zhu and Jiangjing Lyu and Xiaokang Yang", "abstract": "  Texture editing is a crucial task in 3D modeling that allows users to\nautomatically manipulate the surface materials of 3D models. However, the\ninherent complexity of 3D models and the ambiguous text description lead to the\nchallenge in this task. To address this challenge, we propose ITEM3D, a\n\\textbf{T}exture \\textbf{E}diting \\textbf{M}odel designed for automatic\n\\textbf{3D} object editing according to the text \\textbf{I}nstructions.\nLeveraging the diffusion models and the differentiable rendering, ITEM3D takes\nthe rendered images as the bridge of text and 3D representation, and further\noptimizes the disentangled texture and environment map. Previous methods\nadopted the absolute editing direction namely score distillation sampling (SDS)\nas the optimization objective, which unfortunately results in the noisy\nappearance and text inconsistency. To solve the problem caused by the ambiguous\ntext, we introduce a relative editing direction, an optimization objective\ndefined by the noise difference between the source and target texts, to release\nthe semantic ambiguity between the texts and images. Additionally, we gradually\nadjust the direction during optimization to further address the unexpected\ndeviation in the texture domain. Qualitative and quantitative experiments show\nthat our ITEM3D outperforms the state-of-the-art methods on various 3D objects.\nWe also perform text-guided relighting to show explicit control over lighting.\nOur project page: https://shengqiliu1.github.io/ITEM3D.\n", "link": "http://arxiv.org/abs/2309.14872v4", "date": "2024-03-06", "relevancy": 2.1496, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5681}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5255}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Directional%20Texture%20Editing%20for%203D%20Models&entry.906535625=Shengqi%20Liu%20and%20Zhuo%20Chen%20and%20Jingnan%20Gao%20and%20Yichao%20Yan%20and%20Wenhan%20Zhu%20and%20Jiangjing%20Lyu%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Texture%20editing%20is%20a%20crucial%20task%20in%203D%20modeling%20that%20allows%20users%20to%0Aautomatically%20manipulate%20the%20surface%20materials%20of%203D%20models.%20However%2C%20the%0Ainherent%20complexity%20of%203D%20models%20and%20the%20ambiguous%20text%20description%20lead%20to%20the%0Achallenge%20in%20this%20task.%20To%20address%20this%20challenge%2C%20we%20propose%20ITEM3D%2C%20a%0A%5Ctextbf%7BT%7Dexture%20%5Ctextbf%7BE%7Dditing%20%5Ctextbf%7BM%7Dodel%20designed%20for%20automatic%0A%5Ctextbf%7B3D%7D%20object%20editing%20according%20to%20the%20text%20%5Ctextbf%7BI%7Dnstructions.%0ALeveraging%20the%20diffusion%20models%20and%20the%20differentiable%20rendering%2C%20ITEM3D%20takes%0Athe%20rendered%20images%20as%20the%20bridge%20of%20text%20and%203D%20representation%2C%20and%20further%0Aoptimizes%20the%20disentangled%20texture%20and%20environment%20map.%20Previous%20methods%0Aadopted%20the%20absolute%20editing%20direction%20namely%20score%20distillation%20sampling%20%28SDS%29%0Aas%20the%20optimization%20objective%2C%20which%20unfortunately%20results%20in%20the%20noisy%0Aappearance%20and%20text%20inconsistency.%20To%20solve%20the%20problem%20caused%20by%20the%20ambiguous%0Atext%2C%20we%20introduce%20a%20relative%20editing%20direction%2C%20an%20optimization%20objective%0Adefined%20by%20the%20noise%20difference%20between%20the%20source%20and%20target%20texts%2C%20to%20release%0Athe%20semantic%20ambiguity%20between%20the%20texts%20and%20images.%20Additionally%2C%20we%20gradually%0Aadjust%20the%20direction%20during%20optimization%20to%20further%20address%20the%20unexpected%0Adeviation%20in%20the%20texture%20domain.%20Qualitative%20and%20quantitative%20experiments%20show%0Athat%20our%20ITEM3D%20outperforms%20the%20state-of-the-art%20methods%20on%20various%203D%20objects.%0AWe%20also%20perform%20text-guided%20relighting%20to%20show%20explicit%20control%20over%20lighting.%0AOur%20project%20page%3A%20https%3A//shengqiliu1.github.io/ITEM3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14872v4&entry.124074799=Read"},
{"title": "GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D\n  Scene Understanding", "author": "Zi-Ting Chou and Sheng-Yu Huang and I-Jieh Liu and Yu-Chiang Frank Wang", "abstract": "  Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance\nFields (NeRF) have emerged as a popular research topic in 3D vision. In this\nwork, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),\nwhich uniquely takes image semantics into the synthesis process so that both\nnovel view images and the associated semantic maps can be produced for unseen\nscenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and\nDepth-Guided Visual rendering. The former is able to observe multi-view image\ninputs to extract semantic and geometry features from a scene. Guided by the\nresulting image geometry information, the latter performs both image and\nsemantic rendering with improved performances. Our experiments not only confirm\nthat GSNeRF performs favorably against prior works on both novel-view image and\nsemantic segmentation synthesis but the effectiveness of our sampling strategy\nfor visual rendering is further verified.\n", "link": "http://arxiv.org/abs/2403.03608v1", "date": "2024-03-06", "relevancy": 2.1351, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5456}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5304}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5233}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSNeRF%3A%20Generalizable%20Semantic%20Neural%20Radiance%20Fields%20with%20Enhanced%203D%0A%20%20Scene%20Understanding&entry.906535625=Zi-Ting%20Chou%20and%20Sheng-Yu%20Huang%20and%20I-Jieh%20Liu%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=%20%20Utilizing%20multi-view%20inputs%20to%20synthesize%20novel-view%20images%2C%20Neural%20Radiance%0AFields%20%28NeRF%29%20have%20emerged%20as%20a%20popular%20research%20topic%20in%203D%20vision.%20In%20this%0Awork%2C%20we%20introduce%20a%20Generalizable%20Semantic%20Neural%20Radiance%20Field%20%28GSNeRF%29%2C%0Awhich%20uniquely%20takes%20image%20semantics%20into%20the%20synthesis%20process%20so%20that%20both%0Anovel%20view%20images%20and%20the%20associated%20semantic%20maps%20can%20be%20produced%20for%20unseen%0Ascenes.%20Our%20GSNeRF%20is%20composed%20of%20two%20stages%3A%20Semantic%20Geo-Reasoning%20and%0ADepth-Guided%20Visual%20rendering.%20The%20former%20is%20able%20to%20observe%20multi-view%20image%0Ainputs%20to%20extract%20semantic%20and%20geometry%20features%20from%20a%20scene.%20Guided%20by%20the%0Aresulting%20image%20geometry%20information%2C%20the%20latter%20performs%20both%20image%20and%0Asemantic%20rendering%20with%20improved%20performances.%20Our%20experiments%20not%20only%20confirm%0Athat%20GSNeRF%20performs%20favorably%20against%20prior%20works%20on%20both%20novel-view%20image%20and%0Asemantic%20segmentation%20synthesis%20but%20the%20effectiveness%20of%20our%20sampling%20strategy%0Afor%20visual%20rendering%20is%20further%20verified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03608v1&entry.124074799=Read"},
{"title": "Towards Controllable Time Series Generation", "author": "Yifan Bao and Yihao Ang and Qiang Huang and Anthony K. H. Tung and Zhiyong Huang", "abstract": "  Time Series Generation (TSG) has emerged as a pivotal technique in\nsynthesizing data that accurately mirrors real-world time series, becoming\nindispensable in numerous applications. Despite significant advancements in\nTSG, its efficacy frequently hinges on having large training datasets. This\ndependency presents a substantial challenge in data-scarce scenarios,\nespecially when dealing with rare or unique conditions. To confront these\nchallenges, we explore a new problem of Controllable Time Series Generation\n(CTSG), aiming to produce synthetic time series that can adapt to various\nexternal conditions, thereby tackling the data scarcity issue.\n  In this paper, we propose \\textbf{C}ontrollable \\textbf{T}ime \\textbf{S}eries\n(\\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key\nfeature of \\textsf{CTS} is that it decouples the mapping process from standard\nVAE training, enabling precise learning of a complex interplay between latent\nfeatures and external conditions. Moreover, we develop a comprehensive\nevaluation scheme for CTSG. Extensive experiments across three real-world time\nseries datasets showcase \\textsf{CTS}'s exceptional capabilities in generating\nhigh-quality, controllable outputs. This underscores its adeptness in\nseamlessly integrating latent features with external conditions. Extending\n\\textsf{CTS} to the image domain highlights its remarkable potential for\nexplainability and further reinforces its versatility across different\nmodalities.\n", "link": "http://arxiv.org/abs/2403.03698v1", "date": "2024-03-06", "relevancy": 2.1265, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5485}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5256}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5044}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Controllable%20Time%20Series%20Generation&entry.906535625=Yifan%20Bao%20and%20Yihao%20Ang%20and%20Qiang%20Huang%20and%20Anthony%20K.%20H.%20Tung%20and%20Zhiyong%20Huang&entry.1292438233=%20%20Time%20Series%20Generation%20%28TSG%29%20has%20emerged%20as%20a%20pivotal%20technique%20in%0Asynthesizing%20data%20that%20accurately%20mirrors%20real-world%20time%20series%2C%20becoming%0Aindispensable%20in%20numerous%20applications.%20Despite%20significant%20advancements%20in%0ATSG%2C%20its%20efficacy%20frequently%20hinges%20on%20having%20large%20training%20datasets.%20This%0Adependency%20presents%20a%20substantial%20challenge%20in%20data-scarce%20scenarios%2C%0Aespecially%20when%20dealing%20with%20rare%20or%20unique%20conditions.%20To%20confront%20these%0Achallenges%2C%20we%20explore%20a%20new%20problem%20of%20Controllable%20Time%20Series%20Generation%0A%28CTSG%29%2C%20aiming%20to%20produce%20synthetic%20time%20series%20that%20can%20adapt%20to%20various%0Aexternal%20conditions%2C%20thereby%20tackling%20the%20data%20scarcity%20issue.%0A%20%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BC%7Dontrollable%20%5Ctextbf%7BT%7Dime%20%5Ctextbf%7BS%7Deries%0A%28%5Ctextsf%7BCTS%7D%29%2C%20an%20innovative%20VAE-agnostic%20framework%20tailored%20for%20CTSG.%20A%20key%0Afeature%20of%20%5Ctextsf%7BCTS%7D%20is%20that%20it%20decouples%20the%20mapping%20process%20from%20standard%0AVAE%20training%2C%20enabling%20precise%20learning%20of%20a%20complex%20interplay%20between%20latent%0Afeatures%20and%20external%20conditions.%20Moreover%2C%20we%20develop%20a%20comprehensive%0Aevaluation%20scheme%20for%20CTSG.%20Extensive%20experiments%20across%20three%20real-world%20time%0Aseries%20datasets%20showcase%20%5Ctextsf%7BCTS%7D%27s%20exceptional%20capabilities%20in%20generating%0Ahigh-quality%2C%20controllable%20outputs.%20This%20underscores%20its%20adeptness%20in%0Aseamlessly%20integrating%20latent%20features%20with%20external%20conditions.%20Extending%0A%5Ctextsf%7BCTS%7D%20to%20the%20image%20domain%20highlights%20its%20remarkable%20potential%20for%0Aexplainability%20and%20further%20reinforces%20its%20versatility%20across%20different%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03698v1&entry.124074799=Read"},
{"title": "Causal Prototype-inspired Contrast Adaptation for Unsupervised Domain\n  Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery", "author": "Jingru Zhu and Ya Guo and Geng Sun and Liang Hong and Jie Chen", "abstract": "  Semantic segmentation of high-resolution remote sensing imagery (HRSI)\nsuffers from the domain shift, resulting in poor performance of the model in\nanother unseen domain. Unsupervised domain adaptive (UDA) semantic segmentation\naims to adapt the semantic segmentation model trained on the labeled source\ndomain to an unlabeled target domain. However, the existing UDA semantic\nsegmentation models tend to align pixels or features based on statistical\ninformation related to labels in source and target domain data, and make\npredictions accordingly, which leads to uncertainty and fragility of prediction\nresults. In this paper, we propose a causal prototype-inspired contrast\nadaptation (CPCA) method to explore the invariant causal mechanisms between\ndifferent HRSIs domains and their semantic labels. It firstly disentangles\ncausal features and bias features from the source and target domain images\nthrough a causal feature disentanglement module. Then, a causal prototypical\ncontrast module is used to learn domain invariant causal features. To further\nde-correlate causal and bias features, a causal intervention module is\nintroduced to intervene on the bias features to generate counterfactual\nunbiased samples. By forcing the causal features to meet the principles of\nseparability, invariance and intervention, CPCA can simulate the causal factors\nof source and target domains, and make decisions on the target domain based on\nthe causal features, which can observe improved generalization ability.\nExtensive experiments under three cross-domain tasks indicate that CPCA is\nremarkably superior to the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.03704v1", "date": "2024-03-06", "relevancy": 2.1221, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5411}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5322}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5247}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Prototype-inspired%20Contrast%20Adaptation%20for%20Unsupervised%20Domain%0A%20%20Adaptive%20Semantic%20Segmentation%20of%20High-resolution%20Remote%20Sensing%20Imagery&entry.906535625=Jingru%20Zhu%20and%20Ya%20Guo%20and%20Geng%20Sun%20and%20Liang%20Hong%20and%20Jie%20Chen&entry.1292438233=%20%20Semantic%20segmentation%20of%20high-resolution%20remote%20sensing%20imagery%20%28HRSI%29%0Asuffers%20from%20the%20domain%20shift%2C%20resulting%20in%20poor%20performance%20of%20the%20model%20in%0Aanother%20unseen%20domain.%20Unsupervised%20domain%20adaptive%20%28UDA%29%20semantic%20segmentation%0Aaims%20to%20adapt%20the%20semantic%20segmentation%20model%20trained%20on%20the%20labeled%20source%0Adomain%20to%20an%20unlabeled%20target%20domain.%20However%2C%20the%20existing%20UDA%20semantic%0Asegmentation%20models%20tend%20to%20align%20pixels%20or%20features%20based%20on%20statistical%0Ainformation%20related%20to%20labels%20in%20source%20and%20target%20domain%20data%2C%20and%20make%0Apredictions%20accordingly%2C%20which%20leads%20to%20uncertainty%20and%20fragility%20of%20prediction%0Aresults.%20In%20this%20paper%2C%20we%20propose%20a%20causal%20prototype-inspired%20contrast%0Aadaptation%20%28CPCA%29%20method%20to%20explore%20the%20invariant%20causal%20mechanisms%20between%0Adifferent%20HRSIs%20domains%20and%20their%20semantic%20labels.%20It%20firstly%20disentangles%0Acausal%20features%20and%20bias%20features%20from%20the%20source%20and%20target%20domain%20images%0Athrough%20a%20causal%20feature%20disentanglement%20module.%20Then%2C%20a%20causal%20prototypical%0Acontrast%20module%20is%20used%20to%20learn%20domain%20invariant%20causal%20features.%20To%20further%0Ade-correlate%20causal%20and%20bias%20features%2C%20a%20causal%20intervention%20module%20is%0Aintroduced%20to%20intervene%20on%20the%20bias%20features%20to%20generate%20counterfactual%0Aunbiased%20samples.%20By%20forcing%20the%20causal%20features%20to%20meet%20the%20principles%20of%0Aseparability%2C%20invariance%20and%20intervention%2C%20CPCA%20can%20simulate%20the%20causal%20factors%0Aof%20source%20and%20target%20domains%2C%20and%20make%20decisions%20on%20the%20target%20domain%20based%20on%0Athe%20causal%20features%2C%20which%20can%20observe%20improved%20generalization%20ability.%0AExtensive%20experiments%20under%20three%20cross-domain%20tasks%20indicate%20that%20CPCA%20is%0Aremarkably%20superior%20to%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03704v1&entry.124074799=Read"},
{"title": "Leveraging Ensemble Diversity for Robust Self-Training in the Presence\n  of Sample Selection Bias", "author": "Ambroise Odonnat and Vasilii Feofanov and Ievgen Redko", "abstract": "  Self-training is a well-known approach for semi-supervised learning. It\nconsists of iteratively assigning pseudo-labels to unlabeled data for which the\nmodel is confident and treating them as labeled examples. For neural networks,\nsoftmax prediction probabilities are often used as a confidence measure,\nalthough they are known to be overconfident, even for wrong predictions. This\nphenomenon is particularly intensified in the presence of sample selection\nbias, i.e., when data labeling is subject to some constraint. To address this\nissue, we propose a novel confidence measure, called $\\mathcal{T}$-similarity,\nbuilt upon the prediction diversity of an ensemble of linear classifiers. We\nprovide the theoretical analysis of our approach by studying stationary points\nand describing the relationship between the diversity of the individual members\nand their performance. We empirically demonstrate the benefit of our confidence\nmeasure for three different pseudo-labeling policies on classification datasets\nof various data modalities. The code is available at\nhttps://github.com/ambroiseodt/tsim.\n", "link": "http://arxiv.org/abs/2310.14814v3", "date": "2024-03-06", "relevancy": 2.1181, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5671}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5048}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5019}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Ensemble%20Diversity%20for%20Robust%20Self-Training%20in%20the%20Presence%0A%20%20of%20Sample%20Selection%20Bias&entry.906535625=Ambroise%20Odonnat%20and%20Vasilii%20Feofanov%20and%20Ievgen%20Redko&entry.1292438233=%20%20Self-training%20is%20a%20well-known%20approach%20for%20semi-supervised%20learning.%20It%0Aconsists%20of%20iteratively%20assigning%20pseudo-labels%20to%20unlabeled%20data%20for%20which%20the%0Amodel%20is%20confident%20and%20treating%20them%20as%20labeled%20examples.%20For%20neural%20networks%2C%0Asoftmax%20prediction%20probabilities%20are%20often%20used%20as%20a%20confidence%20measure%2C%0Aalthough%20they%20are%20known%20to%20be%20overconfident%2C%20even%20for%20wrong%20predictions.%20This%0Aphenomenon%20is%20particularly%20intensified%20in%20the%20presence%20of%20sample%20selection%0Abias%2C%20i.e.%2C%20when%20data%20labeling%20is%20subject%20to%20some%20constraint.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20novel%20confidence%20measure%2C%20called%20%24%5Cmathcal%7BT%7D%24-similarity%2C%0Abuilt%20upon%20the%20prediction%20diversity%20of%20an%20ensemble%20of%20linear%20classifiers.%20We%0Aprovide%20the%20theoretical%20analysis%20of%20our%20approach%20by%20studying%20stationary%20points%0Aand%20describing%20the%20relationship%20between%20the%20diversity%20of%20the%20individual%20members%0Aand%20their%20performance.%20We%20empirically%20demonstrate%20the%20benefit%20of%20our%20confidence%0Ameasure%20for%20three%20different%20pseudo-labeling%20policies%20on%20classification%20datasets%0Aof%20various%20data%20modalities.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ambroiseodt/tsim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14814v3&entry.124074799=Read"},
{"title": "Causal-Story: Local Causal Attention Utilizing Parameter-Efficient\n  Tuning For Visual Story Synthesis", "author": "Tianyi Song and Jiuxin Cao and Kun Wang and Bo Liu and Xiaofeng Zhang", "abstract": "  The excellent text-to-image synthesis capability of diffusion models has\ndriven progress in synthesizing coherent visual stories. The current\nstate-of-the-art method combines the features of historical captions,\nhistorical frames, and the current captions as conditions for generating the\ncurrent frame. However, this method treats each historical frame and caption as\nthe same contribution. It connects them in order with equal weights, ignoring\nthat not all historical conditions are associated with the generation of the\ncurrent frame. To address this issue, we propose Causal-Story. This model\nincorporates a local causal attention mechanism that considers the causal\nrelationship between previous captions, frames, and current captions. By\nassigning weights based on this relationship, Causal-Story generates the\ncurrent frame, thereby improving the global consistency of story generation. We\nevaluated our model on the PororoSV and FlintstonesSV datasets and obtained\nstate-of-the-art FID scores, and the generated frames also demonstrate better\nstorytelling in visuals.\n", "link": "http://arxiv.org/abs/2309.09553v4", "date": "2024-03-06", "relevancy": 2.1149, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5469}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5362}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.514}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal-Story%3A%20Local%20Causal%20Attention%20Utilizing%20Parameter-Efficient%0A%20%20Tuning%20For%20Visual%20Story%20Synthesis&entry.906535625=Tianyi%20Song%20and%20Jiuxin%20Cao%20and%20Kun%20Wang%20and%20Bo%20Liu%20and%20Xiaofeng%20Zhang&entry.1292438233=%20%20The%20excellent%20text-to-image%20synthesis%20capability%20of%20diffusion%20models%20has%0Adriven%20progress%20in%20synthesizing%20coherent%20visual%20stories.%20The%20current%0Astate-of-the-art%20method%20combines%20the%20features%20of%20historical%20captions%2C%0Ahistorical%20frames%2C%20and%20the%20current%20captions%20as%20conditions%20for%20generating%20the%0Acurrent%20frame.%20However%2C%20this%20method%20treats%20each%20historical%20frame%20and%20caption%20as%0Athe%20same%20contribution.%20It%20connects%20them%20in%20order%20with%20equal%20weights%2C%20ignoring%0Athat%20not%20all%20historical%20conditions%20are%20associated%20with%20the%20generation%20of%20the%0Acurrent%20frame.%20To%20address%20this%20issue%2C%20we%20propose%20Causal-Story.%20This%20model%0Aincorporates%20a%20local%20causal%20attention%20mechanism%20that%20considers%20the%20causal%0Arelationship%20between%20previous%20captions%2C%20frames%2C%20and%20current%20captions.%20By%0Aassigning%20weights%20based%20on%20this%20relationship%2C%20Causal-Story%20generates%20the%0Acurrent%20frame%2C%20thereby%20improving%20the%20global%20consistency%20of%20story%20generation.%20We%0Aevaluated%20our%20model%20on%20the%20PororoSV%20and%20FlintstonesSV%20datasets%20and%20obtained%0Astate-of-the-art%20FID%20scores%2C%20and%20the%20generated%20frames%20also%20demonstrate%20better%0Astorytelling%20in%20visuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09553v4&entry.124074799=Read"},
{"title": "Provable Filter for Real-world Graph Clustering", "author": "Xuanting Xie and Erlin Pan and Zhao Kang and Wenyu Chen and Bingheng Li", "abstract": "  Graph clustering, an important unsupervised problem, has been shown to be\nmore resistant to advances in Graph Neural Networks (GNNs). In addition, almost\nall clustering methods focus on homophilic graphs and ignore heterophily. This\nsignificantly limits their applicability in practice, since real-world graphs\nexhibit a structural disparity and cannot simply be classified as homophily and\nheterophily. Thus, a principled way to handle practical graphs is urgently\nneeded. To fill this gap, we provide a novel solution with theoretical support.\nInterestingly, we find that most homophilic and heterophilic edges can be\ncorrectly identified on the basis of neighbor information. Motivated by this\nfinding, we construct two graphs that are highly homophilic and heterophilic,\nrespectively. They are used to build low-pass and high-pass filters to capture\nholistic information. Important features are further enhanced by the\nsqueeze-and-excitation block. We validate our approach through extensive\nexperiments on both homophilic and heterophilic graphs. Empirical results\ndemonstrate the superiority of our method compared to state-of-the-art\nclustering methods.\n", "link": "http://arxiv.org/abs/2403.03666v1", "date": "2024-03-06", "relevancy": 2.1075, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4321}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4185}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4139}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Filter%20for%20Real-world%20Graph%20Clustering&entry.906535625=Xuanting%20Xie%20and%20Erlin%20Pan%20and%20Zhao%20Kang%20and%20Wenyu%20Chen%20and%20Bingheng%20Li&entry.1292438233=%20%20Graph%20clustering%2C%20an%20important%20unsupervised%20problem%2C%20has%20been%20shown%20to%20be%0Amore%20resistant%20to%20advances%20in%20Graph%20Neural%20Networks%20%28GNNs%29.%20In%20addition%2C%20almost%0Aall%20clustering%20methods%20focus%20on%20homophilic%20graphs%20and%20ignore%20heterophily.%20This%0Asignificantly%20limits%20their%20applicability%20in%20practice%2C%20since%20real-world%20graphs%0Aexhibit%20a%20structural%20disparity%20and%20cannot%20simply%20be%20classified%20as%20homophily%20and%0Aheterophily.%20Thus%2C%20a%20principled%20way%20to%20handle%20practical%20graphs%20is%20urgently%0Aneeded.%20To%20fill%20this%20gap%2C%20we%20provide%20a%20novel%20solution%20with%20theoretical%20support.%0AInterestingly%2C%20we%20find%20that%20most%20homophilic%20and%20heterophilic%20edges%20can%20be%0Acorrectly%20identified%20on%20the%20basis%20of%20neighbor%20information.%20Motivated%20by%20this%0Afinding%2C%20we%20construct%20two%20graphs%20that%20are%20highly%20homophilic%20and%20heterophilic%2C%0Arespectively.%20They%20are%20used%20to%20build%20low-pass%20and%20high-pass%20filters%20to%20capture%0Aholistic%20information.%20Important%20features%20are%20further%20enhanced%20by%20the%0Asqueeze-and-excitation%20block.%20We%20validate%20our%20approach%20through%20extensive%0Aexperiments%20on%20both%20homophilic%20and%20heterophilic%20graphs.%20Empirical%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20compared%20to%20state-of-the-art%0Aclustering%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03666v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning with Task-Adaptive Retrieval via\n  Hypernetwork", "author": "Yonggang Jin and Chenxu Wang and Tianyu Zheng and Liuyu Xiang and Yaodong Yang and Junge Zhang and Jie Fu and Zhaofeng He", "abstract": "  Deep reinforcement learning algorithms are usually impeded by sampling\ninefficiency, heavily depending on multiple interactions with the environment\nto acquire accurate decision-making capabilities. In contrast, humans rely on\ntheir hippocampus to retrieve relevant information from past experiences of\nrelevant tasks, which guides their decision-making when learning a new task,\nrather than exclusively depending on environmental interactions. Nevertheless,\ndesigning a hippocampus-like module for an agent to incorporate past\nexperiences into established reinforcement learning algorithms presents two\nchallenges. The first challenge involves selecting the most relevant past\nexperiences for the current task, and the second challenge is integrating such\nexperiences into the decision network. To address these challenges, we propose\na novel method that utilizes a retrieval network based on task-conditioned\nhypernetwork, which adapts the retrieval network's parameters depending on the\ntask. At the same time, a dynamic modification mechanism enhances the\ncollaborative efforts between the retrieval and decision networks. We evaluate\nthe proposed method across various tasks within a multitask scenario in the\nMinigrid environment. The experimental results demonstrate that our proposed\nmethod significantly outperforms strong baselines.\n", "link": "http://arxiv.org/abs/2306.10698v6", "date": "2024-03-06", "relevancy": 2.1038, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5444}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5237}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5084}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20with%20Task-Adaptive%20Retrieval%20via%0A%20%20Hypernetwork&entry.906535625=Yonggang%20Jin%20and%20Chenxu%20Wang%20and%20Tianyu%20Zheng%20and%20Liuyu%20Xiang%20and%20Yaodong%20Yang%20and%20Junge%20Zhang%20and%20Jie%20Fu%20and%20Zhaofeng%20He&entry.1292438233=%20%20Deep%20reinforcement%20learning%20algorithms%20are%20usually%20impeded%20by%20sampling%0Ainefficiency%2C%20heavily%20depending%20on%20multiple%20interactions%20with%20the%20environment%0Ato%20acquire%20accurate%20decision-making%20capabilities.%20In%20contrast%2C%20humans%20rely%20on%0Atheir%20hippocampus%20to%20retrieve%20relevant%20information%20from%20past%20experiences%20of%0Arelevant%20tasks%2C%20which%20guides%20their%20decision-making%20when%20learning%20a%20new%20task%2C%0Arather%20than%20exclusively%20depending%20on%20environmental%20interactions.%20Nevertheless%2C%0Adesigning%20a%20hippocampus-like%20module%20for%20an%20agent%20to%20incorporate%20past%0Aexperiences%20into%20established%20reinforcement%20learning%20algorithms%20presents%20two%0Achallenges.%20The%20first%20challenge%20involves%20selecting%20the%20most%20relevant%20past%0Aexperiences%20for%20the%20current%20task%2C%20and%20the%20second%20challenge%20is%20integrating%20such%0Aexperiences%20into%20the%20decision%20network.%20To%20address%20these%20challenges%2C%20we%20propose%0Aa%20novel%20method%20that%20utilizes%20a%20retrieval%20network%20based%20on%20task-conditioned%0Ahypernetwork%2C%20which%20adapts%20the%20retrieval%20network%27s%20parameters%20depending%20on%20the%0Atask.%20At%20the%20same%20time%2C%20a%20dynamic%20modification%20mechanism%20enhances%20the%0Acollaborative%20efforts%20between%20the%20retrieval%20and%20decision%20networks.%20We%20evaluate%0Athe%20proposed%20method%20across%20various%20tasks%20within%20a%20multitask%20scenario%20in%20the%0AMinigrid%20environment.%20The%20experimental%20results%20demonstrate%20that%20our%20proposed%0Amethod%20significantly%20outperforms%20strong%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10698v6&entry.124074799=Read"},
{"title": "Dual-modal Dynamic Traceback Learning for Medical Report Generation", "author": "Shuchang Ye and Mingyuan Meng and Mingjian Li and Dagan Feng and Jinman Kim", "abstract": "  With increasing reliance on medical imaging in clinical practices, automated\nreport generation from medical images is in great demand. Existing report\ngeneration methods typically adopt an encoder-decoder deep learning framework\nto build a uni-directional image-to-report mapping. However, such a framework\nignores the bi-directional mutual associations between images and reports, thus\nincurring difficulties in associating the intrinsic medical meanings between\nthem. Recent generative representation learning methods have demonstrated the\nbenefits of dual-modal learning from both image and text modalities. However,\nthese methods exhibit two major drawbacks for medical report generation: 1)\nthey tend to capture morphological information and have difficulties in\ncapturing subtle pathological semantic information, and 2) they predict masked\ntext rely on both unmasked images and text, inevitably degrading performance\nwhen inference is based solely on images. In this study, we propose a new\nreport generation framework with dual-modal dynamic traceback learning (DTrace)\nto overcome the two identified drawbacks and enable dual-modal learning for\nmedical report generation. To achieve this, our DTrace introduces a traceback\nmechanism to control the semantic validity of generated content via\nself-assessment. Further, our DTrace introduces a dynamic learning strategy to\nadapt to various proportions of image and text input, enabling report\ngeneration without reliance on textual input during inference. Extensive\nexperiments on two well-benchmarked datasets (IU-Xray and MIMIC-CXR) show that\nour DTrace outperforms state-of-the-art medical report generation methods.\n", "link": "http://arxiv.org/abs/2401.13267v2", "date": "2024-03-06", "relevancy": 2.089, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5261}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5115}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-modal%20Dynamic%20Traceback%20Learning%20for%20Medical%20Report%20Generation&entry.906535625=Shuchang%20Ye%20and%20Mingyuan%20Meng%20and%20Mingjian%20Li%20and%20Dagan%20Feng%20and%20Jinman%20Kim&entry.1292438233=%20%20With%20increasing%20reliance%20on%20medical%20imaging%20in%20clinical%20practices%2C%20automated%0Areport%20generation%20from%20medical%20images%20is%20in%20great%20demand.%20Existing%20report%0Ageneration%20methods%20typically%20adopt%20an%20encoder-decoder%20deep%20learning%20framework%0Ato%20build%20a%20uni-directional%20image-to-report%20mapping.%20However%2C%20such%20a%20framework%0Aignores%20the%20bi-directional%20mutual%20associations%20between%20images%20and%20reports%2C%20thus%0Aincurring%20difficulties%20in%20associating%20the%20intrinsic%20medical%20meanings%20between%0Athem.%20Recent%20generative%20representation%20learning%20methods%20have%20demonstrated%20the%0Abenefits%20of%20dual-modal%20learning%20from%20both%20image%20and%20text%20modalities.%20However%2C%0Athese%20methods%20exhibit%20two%20major%20drawbacks%20for%20medical%20report%20generation%3A%201%29%0Athey%20tend%20to%20capture%20morphological%20information%20and%20have%20difficulties%20in%0Acapturing%20subtle%20pathological%20semantic%20information%2C%20and%202%29%20they%20predict%20masked%0Atext%20rely%20on%20both%20unmasked%20images%20and%20text%2C%20inevitably%20degrading%20performance%0Awhen%20inference%20is%20based%20solely%20on%20images.%20In%20this%20study%2C%20we%20propose%20a%20new%0Areport%20generation%20framework%20with%20dual-modal%20dynamic%20traceback%20learning%20%28DTrace%29%0Ato%20overcome%20the%20two%20identified%20drawbacks%20and%20enable%20dual-modal%20learning%20for%0Amedical%20report%20generation.%20To%20achieve%20this%2C%20our%20DTrace%20introduces%20a%20traceback%0Amechanism%20to%20control%20the%20semantic%20validity%20of%20generated%20content%20via%0Aself-assessment.%20Further%2C%20our%20DTrace%20introduces%20a%20dynamic%20learning%20strategy%20to%0Aadapt%20to%20various%20proportions%20of%20image%20and%20text%20input%2C%20enabling%20report%0Ageneration%20without%20reliance%20on%20textual%20input%20during%20inference.%20Extensive%0Aexperiments%20on%20two%20well-benchmarked%20datasets%20%28IU-Xray%20and%20MIMIC-CXR%29%20show%20that%0Aour%20DTrace%20outperforms%20state-of-the-art%20medical%20report%20generation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13267v2&entry.124074799=Read"},
{"title": "Residual Multi-Fidelity Neural Network Computing", "author": "Owen Davis and Mohammad Motamed and Raul Tempone", "abstract": "  In this work, we consider the general problem of constructing a neural\nnetwork surrogate model using multi-fidelity information. Motivated by rigorous\nerror and complexity estimates for ReLU neural networks, given an inexpensive\nlow-fidelity and an expensive high-fidelity computational model, we present a\nresidual multi-fidelity computational framework that formulates the correlation\nbetween models as a residual function, a possibly non-linear mapping between 1)\nthe shared input space of the models together with the low-fidelity model\noutput and 2) the discrepancy between the two model outputs. To accomplish\nthis, we train two neural networks to work in concert. The first network learns\nthe residual function on a small set of high-fidelity and low-fidelity data.\nOnce trained, this network is used to generate additional synthetic\nhigh-fidelity data, which is used in the training of a second network. This\nsecond network, once trained, acts as our surrogate for the high-fidelity\nquantity of interest. We present three numerical examples to demonstrate the\npower of the proposed framework. In particular, we show that dramatic savings\nin computational cost may be achieved when the output predictions are desired\nto be accurate within small tolerances.\n", "link": "http://arxiv.org/abs/2310.03572v2", "date": "2024-03-06", "relevancy": 2.0828, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5299}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5184}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5125}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Multi-Fidelity%20Neural%20Network%20Computing&entry.906535625=Owen%20Davis%20and%20Mohammad%20Motamed%20and%20Raul%20Tempone&entry.1292438233=%20%20In%20this%20work%2C%20we%20consider%20the%20general%20problem%20of%20constructing%20a%20neural%0Anetwork%20surrogate%20model%20using%20multi-fidelity%20information.%20Motivated%20by%20rigorous%0Aerror%20and%20complexity%20estimates%20for%20ReLU%20neural%20networks%2C%20given%20an%20inexpensive%0Alow-fidelity%20and%20an%20expensive%20high-fidelity%20computational%20model%2C%20we%20present%20a%0Aresidual%20multi-fidelity%20computational%20framework%20that%20formulates%20the%20correlation%0Abetween%20models%20as%20a%20residual%20function%2C%20a%20possibly%20non-linear%20mapping%20between%201%29%0Athe%20shared%20input%20space%20of%20the%20models%20together%20with%20the%20low-fidelity%20model%0Aoutput%20and%202%29%20the%20discrepancy%20between%20the%20two%20model%20outputs.%20To%20accomplish%0Athis%2C%20we%20train%20two%20neural%20networks%20to%20work%20in%20concert.%20The%20first%20network%20learns%0Athe%20residual%20function%20on%20a%20small%20set%20of%20high-fidelity%20and%20low-fidelity%20data.%0AOnce%20trained%2C%20this%20network%20is%20used%20to%20generate%20additional%20synthetic%0Ahigh-fidelity%20data%2C%20which%20is%20used%20in%20the%20training%20of%20a%20second%20network.%20This%0Asecond%20network%2C%20once%20trained%2C%20acts%20as%20our%20surrogate%20for%20the%20high-fidelity%0Aquantity%20of%20interest.%20We%20present%20three%20numerical%20examples%20to%20demonstrate%20the%0Apower%20of%20the%20proposed%20framework.%20In%20particular%2C%20we%20show%20that%20dramatic%20savings%0Ain%20computational%20cost%20may%20be%20achieved%20when%20the%20output%20predictions%20are%20desired%0Ato%20be%20accurate%20within%20small%20tolerances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03572v2&entry.124074799=Read"},
{"title": "R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction", "author": "Chenhuan Li and Meihua Xiao and zehuan li and Fangping Chen and Shanshan Qiao and Dingli Wang and Mengxi Gao and Siyi Zhang", "abstract": "  Recently, vision transformers have performed well in various computer vision\ntasks, including voxel 3D reconstruction. However, the windows of the vision\ntransformer are not multi-scale, and there is no connection between the\nwindows, which limits the accuracy of voxel 3D reconstruction. Therefore, we\npropose a voxel 3D reconstruction network based on shifted window attention. To\nthe best of our knowledge, this is the first work to apply shifted window\nattention to voxel 3D reconstruction. Experimental results on ShapeNet verify\nour method achieves SOTA accuracy in single-view reconstruction.\n", "link": "http://arxiv.org/abs/2312.02725v3", "date": "2024-03-06", "relevancy": 2.0795, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5466}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5053}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4896}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R3D-SWIN%3AUse%20Shifted%20Window%20Attention%20for%20Single-View%203D%20Reconstruction&entry.906535625=Chenhuan%20Li%20and%20Meihua%20Xiao%20and%20zehuan%20li%20and%20Fangping%20Chen%20and%20Shanshan%20Qiao%20and%20Dingli%20Wang%20and%20Mengxi%20Gao%20and%20Siyi%20Zhang&entry.1292438233=%20%20Recently%2C%20vision%20transformers%20have%20performed%20well%20in%20various%20computer%20vision%0Atasks%2C%20including%20voxel%203D%20reconstruction.%20However%2C%20the%20windows%20of%20the%20vision%0Atransformer%20are%20not%20multi-scale%2C%20and%20there%20is%20no%20connection%20between%20the%0Awindows%2C%20which%20limits%20the%20accuracy%20of%20voxel%203D%20reconstruction.%20Therefore%2C%20we%0Apropose%20a%20voxel%203D%20reconstruction%20network%20based%20on%20shifted%20window%20attention.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20apply%20shifted%20window%0Aattention%20to%20voxel%203D%20reconstruction.%20Experimental%20results%20on%20ShapeNet%20verify%0Aour%20method%20achieves%20SOTA%20accuracy%20in%20single-view%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02725v3&entry.124074799=Read"},
{"title": "Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining", "author": "Jiarun Liu and Hao Yang and Hong-Yu Zhou and Yan Xi and Lequan Yu and Yizhou Yu and Yong Liang and Guangming Shi and Shaoting Zhang and Hairong Zheng and Shanshan Wang", "abstract": "  Accurate medical image segmentation demands the integration of multi-scale\ninformation, spanning from local features to global dependencies. However, it\nis challenging for existing methods to model long-range global information,\nwhere convolutional neural networks (CNNs) are constrained by their local\nreceptive fields, and vision transformers (ViTs) suffer from high quadratic\ncomplexity of their attention mechanism. Recently, Mamba-based models have\ngained great attention for their impressive ability in long sequence modeling.\nSeveral studies have demonstrated that these models can outperform popular\nvision models in various tasks, offering higher accuracy, lower memory\nconsumption, and less computational burden. However, existing Mamba-based\nmodels are mostly trained from scratch and do not explore the power of\npretraining, which has been proven to be quite effective for data-efficient\nmedical image analysis. This paper introduces a novel Mamba-based model,\nSwin-UMamba, designed specifically for medical image segmentation tasks,\nleveraging the advantages of ImageNet-based pretraining. Our experimental\nresults reveal the vital role of ImageNet-based training in enhancing the\nperformance of Mamba-based models. Swin-UMamba demonstrates superior\nperformance with a large margin compared to CNNs, ViTs, and latest Mamba-based\nmodels. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba\noutperforms its closest counterpart U-Mamba_Enc by an average score of 2.72%.\n", "link": "http://arxiv.org/abs/2402.03302v2", "date": "2024-03-06", "relevancy": 2.0456, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4884}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swin-UMamba%3A%20Mamba-based%20UNet%20with%20ImageNet-based%20pretraining&entry.906535625=Jiarun%20Liu%20and%20Hao%20Yang%20and%20Hong-Yu%20Zhou%20and%20Yan%20Xi%20and%20Lequan%20Yu%20and%20Yizhou%20Yu%20and%20Yong%20Liang%20and%20Guangming%20Shi%20and%20Shaoting%20Zhang%20and%20Hairong%20Zheng%20and%20Shanshan%20Wang&entry.1292438233=%20%20Accurate%20medical%20image%20segmentation%20demands%20the%20integration%20of%20multi-scale%0Ainformation%2C%20spanning%20from%20local%20features%20to%20global%20dependencies.%20However%2C%20it%0Ais%20challenging%20for%20existing%20methods%20to%20model%20long-range%20global%20information%2C%0Awhere%20convolutional%20neural%20networks%20%28CNNs%29%20are%20constrained%20by%20their%20local%0Areceptive%20fields%2C%20and%20vision%20transformers%20%28ViTs%29%20suffer%20from%20high%20quadratic%0Acomplexity%20of%20their%20attention%20mechanism.%20Recently%2C%20Mamba-based%20models%20have%0Agained%20great%20attention%20for%20their%20impressive%20ability%20in%20long%20sequence%20modeling.%0ASeveral%20studies%20have%20demonstrated%20that%20these%20models%20can%20outperform%20popular%0Avision%20models%20in%20various%20tasks%2C%20offering%20higher%20accuracy%2C%20lower%20memory%0Aconsumption%2C%20and%20less%20computational%20burden.%20However%2C%20existing%20Mamba-based%0Amodels%20are%20mostly%20trained%20from%20scratch%20and%20do%20not%20explore%20the%20power%20of%0Apretraining%2C%20which%20has%20been%20proven%20to%20be%20quite%20effective%20for%20data-efficient%0Amedical%20image%20analysis.%20This%20paper%20introduces%20a%20novel%20Mamba-based%20model%2C%0ASwin-UMamba%2C%20designed%20specifically%20for%20medical%20image%20segmentation%20tasks%2C%0Aleveraging%20the%20advantages%20of%20ImageNet-based%20pretraining.%20Our%20experimental%0Aresults%20reveal%20the%20vital%20role%20of%20ImageNet-based%20training%20in%20enhancing%20the%0Aperformance%20of%20Mamba-based%20models.%20Swin-UMamba%20demonstrates%20superior%0Aperformance%20with%20a%20large%20margin%20compared%20to%20CNNs%2C%20ViTs%2C%20and%20latest%20Mamba-based%0Amodels.%20Notably%2C%20on%20AbdomenMRI%2C%20Encoscopy%2C%20and%20Microscopy%20datasets%2C%20Swin-UMamba%0Aoutperforms%20its%20closest%20counterpart%20U-Mamba_Enc%20by%20an%20average%20score%20of%202.72%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03302v2&entry.124074799=Read"},
{"title": "Learning to Decode Collaboratively with Multiple Language Models", "author": "Shannon Zejiang Shen and Hunter Lang and Bailin Wang and Yoon Kim and David Sontag", "abstract": "  We propose a method to teach multiple large language models (LLM) to\ncollaborate by interleaving their generations at the token level. We model the\ndecision of which LLM generates the next token as a latent variable. By\noptimizing the marginal likelihood of a training set under our latent variable\nmodel, the base LLM automatically learns when to generate itself and when to\ncall on one of the ``assistant'' language models to generate, all without\ndirect supervision. Token-level collaboration during decoding allows for a\nfusion of each model's expertise in a manner tailored to the specific task at\nhand. Our collaborative decoding is especially useful in cross-domain settings\nwhere a generalist base LLM learns to invoke domain expert models. On\ninstruction-following, domain-specific QA, and reasoning tasks, we show that\nthe performance of the joint system exceeds that of the individual models.\nThrough qualitative analysis of the learned latent decisions, we show models\ntrained with our method exhibit several interesting collaboration patterns,\ne.g., template-filling. Our code is available at\nhttps://github.com/clinicalml/co-llm.\n", "link": "http://arxiv.org/abs/2403.03870v1", "date": "2024-03-06", "relevancy": 2.0424, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5471}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4985}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4789}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Decode%20Collaboratively%20with%20Multiple%20Language%20Models&entry.906535625=Shannon%20Zejiang%20Shen%20and%20Hunter%20Lang%20and%20Bailin%20Wang%20and%20Yoon%20Kim%20and%20David%20Sontag&entry.1292438233=%20%20We%20propose%20a%20method%20to%20teach%20multiple%20large%20language%20models%20%28LLM%29%20to%0Acollaborate%20by%20interleaving%20their%20generations%20at%20the%20token%20level.%20We%20model%20the%0Adecision%20of%20which%20LLM%20generates%20the%20next%20token%20as%20a%20latent%20variable.%20By%0Aoptimizing%20the%20marginal%20likelihood%20of%20a%20training%20set%20under%20our%20latent%20variable%0Amodel%2C%20the%20base%20LLM%20automatically%20learns%20when%20to%20generate%20itself%20and%20when%20to%0Acall%20on%20one%20of%20the%20%60%60assistant%27%27%20language%20models%20to%20generate%2C%20all%20without%0Adirect%20supervision.%20Token-level%20collaboration%20during%20decoding%20allows%20for%20a%0Afusion%20of%20each%20model%27s%20expertise%20in%20a%20manner%20tailored%20to%20the%20specific%20task%20at%0Ahand.%20Our%20collaborative%20decoding%20is%20especially%20useful%20in%20cross-domain%20settings%0Awhere%20a%20generalist%20base%20LLM%20learns%20to%20invoke%20domain%20expert%20models.%20On%0Ainstruction-following%2C%20domain-specific%20QA%2C%20and%20reasoning%20tasks%2C%20we%20show%20that%0Athe%20performance%20of%20the%20joint%20system%20exceeds%20that%20of%20the%20individual%20models.%0AThrough%20qualitative%20analysis%20of%20the%20learned%20latent%20decisions%2C%20we%20show%20models%0Atrained%20with%20our%20method%20exhibit%20several%20interesting%20collaboration%20patterns%2C%0Ae.g.%2C%20template-filling.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/clinicalml/co-llm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03870v1&entry.124074799=Read"},
{"title": "Towards Implicit Prompt For Text-To-Image Models", "author": "Yue Yang and Yuqi lin and Hong Liu and Wenqi Shao and Runjian Chen and Hailong Shang and Yu Wang and Yu Qiao and Kaipeng Zhang and Ping Luo", "abstract": "  Recent text-to-image (T2I) models have had great success, and many benchmarks\nhave been proposed to evaluate their performance and safety. However, they only\nconsider explicit prompts while neglecting implicit prompts (hint at a target\nwithout explicitly mentioning it). These prompts may get rid of safety\nconstraints and pose potential threats to the applications of these models.\nThis position paper highlights the current state of T2I models toward implicit\nprompts. We present a benchmark named ImplicitBench and conduct an\ninvestigation on the performance and impacts of implicit prompts with popular\nT2I models. Specifically, we design and collect more than 2,000 implicit\nprompts of three aspects: General Symbols, Celebrity Privacy, and\nNot-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models'\ncapabilities under these implicit prompts. Experiment results show that (1) T2I\nmodels are able to accurately create various target symbols indicated by\nimplicit prompts; (2) Implicit prompts bring potential risks of privacy leakage\nfor T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can\nbe bypassed with implicit prompts. We call for increased attention to the\npotential and risks of implicit prompts in the T2I community and further\ninvestigation into the capabilities and impacts of implicit prompts, advocating\nfor a balanced approach that harnesses their benefits while mitigating their\nrisks.\n", "link": "http://arxiv.org/abs/2403.02118v2", "date": "2024-03-06", "relevancy": 2.0299, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5286}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5122}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4844}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Implicit%20Prompt%20For%20Text-To-Image%20Models&entry.906535625=Yue%20Yang%20and%20Yuqi%20lin%20and%20Hong%20Liu%20and%20Wenqi%20Shao%20and%20Runjian%20Chen%20and%20Hailong%20Shang%20and%20Yu%20Wang%20and%20Yu%20Qiao%20and%20Kaipeng%20Zhang%20and%20Ping%20Luo&entry.1292438233=%20%20Recent%20text-to-image%20%28T2I%29%20models%20have%20had%20great%20success%2C%20and%20many%20benchmarks%0Ahave%20been%20proposed%20to%20evaluate%20their%20performance%20and%20safety.%20However%2C%20they%20only%0Aconsider%20explicit%20prompts%20while%20neglecting%20implicit%20prompts%20%28hint%20at%20a%20target%0Awithout%20explicitly%20mentioning%20it%29.%20These%20prompts%20may%20get%20rid%20of%20safety%0Aconstraints%20and%20pose%20potential%20threats%20to%20the%20applications%20of%20these%20models.%0AThis%20position%20paper%20highlights%20the%20current%20state%20of%20T2I%20models%20toward%20implicit%0Aprompts.%20We%20present%20a%20benchmark%20named%20ImplicitBench%20and%20conduct%20an%0Ainvestigation%20on%20the%20performance%20and%20impacts%20of%20implicit%20prompts%20with%20popular%0AT2I%20models.%20Specifically%2C%20we%20design%20and%20collect%20more%20than%202%2C000%20implicit%0Aprompts%20of%20three%20aspects%3A%20General%20Symbols%2C%20Celebrity%20Privacy%2C%20and%0ANot-Safe-For-Work%20%28NSFW%29%20Issues%2C%20and%20evaluate%20six%20well-known%20T2I%20models%27%0Acapabilities%20under%20these%20implicit%20prompts.%20Experiment%20results%20show%20that%20%281%29%20T2I%0Amodels%20are%20able%20to%20accurately%20create%20various%20target%20symbols%20indicated%20by%0Aimplicit%20prompts%3B%20%282%29%20Implicit%20prompts%20bring%20potential%20risks%20of%20privacy%20leakage%0Afor%20T2I%20models.%20%283%29%20Constraints%20of%20NSFW%20in%20most%20of%20the%20evaluated%20T2I%20models%20can%0Abe%20bypassed%20with%20implicit%20prompts.%20We%20call%20for%20increased%20attention%20to%20the%0Apotential%20and%20risks%20of%20implicit%20prompts%20in%20the%20T2I%20community%20and%20further%0Ainvestigation%20into%20the%20capabilities%20and%20impacts%20of%20implicit%20prompts%2C%20advocating%0Afor%20a%20balanced%20approach%20that%20harnesses%20their%20benefits%20while%20mitigating%20their%0Arisks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02118v2&entry.124074799=Read"},
{"title": "A Precision Drone Landing System using Visual and IR Fiducial Markers\n  and a Multi-Payload Camera", "author": "Joshua Springer and Gylfi \u00de\u00f3r Gu\u00f0mundsson and Marcel Kyas", "abstract": "  We propose a method for autonomous precision drone landing with fiducial\nmarkers and a gimbal-mounted, multi-payload camera with wide-angle, zoom, and\nIR sensors. The method has minimal data requirements; it depends primarily on\nthe direction from the drone to the landing pad, enabling it to switch\ndynamically between the camera's different sensors and zoom factors, and\nminimizing auxiliary sensor requirements. It eliminates the need for data such\nas altitude above ground level, straight-line distance to the landing pad,\nfiducial marker size, and 6 DoF marker pose (of which the orientation is\nproblematic). We leverage the zoom and wide-angle cameras, as well as visual\nApril Tag fiducial markers to conduct successful precision landings from much\nlonger distances than in previous work (168m horizontal distance, 102m\naltitude). We use two types of April Tags in the IR spectrum - active and\npassive - for precision landing both at daytime and nighttime, instead of\nsimple IR beacons used in most previous work. The active IR landing pad is\nheated; the novel, passive one is unpowered, at ambient temperature, and\ndepends on its high reflectivity and an IR differential between the ground and\nthe sky. Finally, we propose a high-level control policy to manage initial\nsearch for the landing pad and subsequent searches if it is lost - not\naddressed in previous work. The method demonstrates successful landings with\nthe landing skids at least touching the landing pad, achieving an average error\nof 0.19m. It also demonstrates successful recovery and landing when the landing\npad is temporarily obscured.\n", "link": "http://arxiv.org/abs/2403.03806v1", "date": "2024-03-06", "relevancy": 2.0293, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5353}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5105}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.478}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Precision%20Drone%20Landing%20System%20using%20Visual%20and%20IR%20Fiducial%20Markers%0A%20%20and%20a%20Multi-Payload%20Camera&entry.906535625=Joshua%20Springer%20and%20Gylfi%20%C3%9E%C3%B3r%20Gu%C3%B0mundsson%20and%20Marcel%20Kyas&entry.1292438233=%20%20We%20propose%20a%20method%20for%20autonomous%20precision%20drone%20landing%20with%20fiducial%0Amarkers%20and%20a%20gimbal-mounted%2C%20multi-payload%20camera%20with%20wide-angle%2C%20zoom%2C%20and%0AIR%20sensors.%20The%20method%20has%20minimal%20data%20requirements%3B%20it%20depends%20primarily%20on%0Athe%20direction%20from%20the%20drone%20to%20the%20landing%20pad%2C%20enabling%20it%20to%20switch%0Adynamically%20between%20the%20camera%27s%20different%20sensors%20and%20zoom%20factors%2C%20and%0Aminimizing%20auxiliary%20sensor%20requirements.%20It%20eliminates%20the%20need%20for%20data%20such%0Aas%20altitude%20above%20ground%20level%2C%20straight-line%20distance%20to%20the%20landing%20pad%2C%0Afiducial%20marker%20size%2C%20and%206%20DoF%20marker%20pose%20%28of%20which%20the%20orientation%20is%0Aproblematic%29.%20We%20leverage%20the%20zoom%20and%20wide-angle%20cameras%2C%20as%20well%20as%20visual%0AApril%20Tag%20fiducial%20markers%20to%20conduct%20successful%20precision%20landings%20from%20much%0Alonger%20distances%20than%20in%20previous%20work%20%28168m%20horizontal%20distance%2C%20102m%0Aaltitude%29.%20We%20use%20two%20types%20of%20April%20Tags%20in%20the%20IR%20spectrum%20-%20active%20and%0Apassive%20-%20for%20precision%20landing%20both%20at%20daytime%20and%20nighttime%2C%20instead%20of%0Asimple%20IR%20beacons%20used%20in%20most%20previous%20work.%20The%20active%20IR%20landing%20pad%20is%0Aheated%3B%20the%20novel%2C%20passive%20one%20is%20unpowered%2C%20at%20ambient%20temperature%2C%20and%0Adepends%20on%20its%20high%20reflectivity%20and%20an%20IR%20differential%20between%20the%20ground%20and%0Athe%20sky.%20Finally%2C%20we%20propose%20a%20high-level%20control%20policy%20to%20manage%20initial%0Asearch%20for%20the%20landing%20pad%20and%20subsequent%20searches%20if%20it%20is%20lost%20-%20not%0Aaddressed%20in%20previous%20work.%20The%20method%20demonstrates%20successful%20landings%20with%0Athe%20landing%20skids%20at%20least%20touching%20the%20landing%20pad%2C%20achieving%20an%20average%20error%0Aof%200.19m.%20It%20also%20demonstrates%20successful%20recovery%20and%20landing%20when%20the%20landing%0Apad%20is%20temporarily%20obscured.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03806v1&entry.124074799=Read"},
{"title": "Towards Concept-based Interpretability of Skin Lesion Diagnosis using\n  Vision-Language Models", "author": "Cristiano Patr\u00edcio and Lu\u00eds F. Teixeira and Jo\u00e3o C. Neves", "abstract": "  Concept-based models naturally lend themselves to the development of\ninherently interpretable skin lesion diagnosis, as medical experts make\ndecisions based on a set of visual patterns of the lesion. Nevertheless, the\ndevelopment of these models depends on the existence of concept-annotated\ndatasets, whose availability is scarce due to the specialized knowledge and\nexpertise required in the annotation process. In this work, we show that\nvision-language models can be used to alleviate the dependence on a large\nnumber of concept-annotated samples. In particular, we propose an embedding\nlearning strategy to adapt CLIP to the downstream task of skin lesion\nclassification using concept-based descriptions as textual embeddings. Our\nexperiments reveal that vision-language models not only attain better accuracy\nwhen using concepts as textual embeddings, but also require a smaller number of\nconcept-annotated samples to attain comparable performance to approaches\nspecifically devised for automatic concept generation.\n", "link": "http://arxiv.org/abs/2311.14339v2", "date": "2024-03-06", "relevancy": 2.0286, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5109}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5085}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Concept-based%20Interpretability%20of%20Skin%20Lesion%20Diagnosis%20using%0A%20%20Vision-Language%20Models&entry.906535625=Cristiano%20Patr%C3%ADcio%20and%20Lu%C3%ADs%20F.%20Teixeira%20and%20Jo%C3%A3o%20C.%20Neves&entry.1292438233=%20%20Concept-based%20models%20naturally%20lend%20themselves%20to%20the%20development%20of%0Ainherently%20interpretable%20skin%20lesion%20diagnosis%2C%20as%20medical%20experts%20make%0Adecisions%20based%20on%20a%20set%20of%20visual%20patterns%20of%20the%20lesion.%20Nevertheless%2C%20the%0Adevelopment%20of%20these%20models%20depends%20on%20the%20existence%20of%20concept-annotated%0Adatasets%2C%20whose%20availability%20is%20scarce%20due%20to%20the%20specialized%20knowledge%20and%0Aexpertise%20required%20in%20the%20annotation%20process.%20In%20this%20work%2C%20we%20show%20that%0Avision-language%20models%20can%20be%20used%20to%20alleviate%20the%20dependence%20on%20a%20large%0Anumber%20of%20concept-annotated%20samples.%20In%20particular%2C%20we%20propose%20an%20embedding%0Alearning%20strategy%20to%20adapt%20CLIP%20to%20the%20downstream%20task%20of%20skin%20lesion%0Aclassification%20using%20concept-based%20descriptions%20as%20textual%20embeddings.%20Our%0Aexperiments%20reveal%20that%20vision-language%20models%20not%20only%20attain%20better%20accuracy%0Awhen%20using%20concepts%20as%20textual%20embeddings%2C%20but%20also%20require%20a%20smaller%20number%20of%0Aconcept-annotated%20samples%20to%20attain%20comparable%20performance%20to%20approaches%0Aspecifically%20devised%20for%20automatic%20concept%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14339v2&entry.124074799=Read"},
{"title": "Structure-Preserving Transformers for Sequences of SPD Matrices", "author": "Mathieu Seraphim and Alexis Lechervy and Florian Yger and Luc Brun and Olivier Etard", "abstract": "  In recent years, Transformer-based auto-attention mechanisms have been\nsuccessfully applied to the analysis of a variety of context-reliant data\ntypes, from texts to images and beyond, including data from non-Euclidean\ngeometries. In this paper, we present such a mechanism, designed to classify\nsequences of Symmetric Positive Definite matrices while preserving their\nRiemannian geometry throughout the analysis. We apply our method to automatic\nsleep staging on timeseries of EEG-derived covariance matrices from a standard\ndataset, obtaining high levels of stage-wise performance.\n", "link": "http://arxiv.org/abs/2309.07579v5", "date": "2024-03-06", "relevancy": 2.0157, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5115}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4796}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Preserving%20Transformers%20for%20Sequences%20of%20SPD%20Matrices&entry.906535625=Mathieu%20Seraphim%20and%20Alexis%20Lechervy%20and%20Florian%20Yger%20and%20Luc%20Brun%20and%20Olivier%20Etard&entry.1292438233=%20%20In%20recent%20years%2C%20Transformer-based%20auto-attention%20mechanisms%20have%20been%0Asuccessfully%20applied%20to%20the%20analysis%20of%20a%20variety%20of%20context-reliant%20data%0Atypes%2C%20from%20texts%20to%20images%20and%20beyond%2C%20including%20data%20from%20non-Euclidean%0Ageometries.%20In%20this%20paper%2C%20we%20present%20such%20a%20mechanism%2C%20designed%20to%20classify%0Asequences%20of%20Symmetric%20Positive%20Definite%20matrices%20while%20preserving%20their%0ARiemannian%20geometry%20throughout%20the%20analysis.%20We%20apply%20our%20method%20to%20automatic%0Asleep%20staging%20on%20timeseries%20of%20EEG-derived%20covariance%20matrices%20from%20a%20standard%0Adataset%2C%20obtaining%20high%20levels%20of%20stage-wise%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07579v5&entry.124074799=Read"},
{"title": "Multimodal Large Language Models to Support Real-World Fact-Checking", "author": "Jiahui Geng and Yova Kementchedjhieva and Preslav Nakov and Iryna Gurevych", "abstract": "  Multimodal large language models (MLLMs) carry the potential to support\nhumans in processing vast amounts of information. While MLLMs are already being\nused as a fact-checking tool, their abilities and limitations in this regard\nare understudied. Here is aim to bridge this gap. In particular, we propose a\nframework for systematically assessing the capacity of current multimodal\nmodels to facilitate real-world fact-checking. Our methodology is\nevidence-free, leveraging only these models' intrinsic knowledge and reasoning\ncapabilities. By designing prompts that extract models' predictions,\nexplanations, and confidence levels, we delve into research questions\nconcerning model accuracy, robustness, and reasons for failure. We empirically\nfind that (1) GPT-4V exhibits superior performance in identifying malicious and\nmisleading multimodal claims, with the ability to explain the unreasonable\naspects and underlying motives, and (2) existing open-source models exhibit\nstrong biases and are highly sensitive to the prompt. Our study offers insights\ninto combating false multimodal information and building secure, trustworthy\nmultimodal models. To the best of our knowledge, we are the first to evaluate\nMLLMs for real-world fact-checking.\n", "link": "http://arxiv.org/abs/2403.03627v1", "date": "2024-03-06", "relevancy": 2.0051, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5418}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4959}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4904}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Large%20Language%20Models%20to%20Support%20Real-World%20Fact-Checking&entry.906535625=Jiahui%20Geng%20and%20Yova%20Kementchedjhieva%20and%20Preslav%20Nakov%20and%20Iryna%20Gurevych&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20carry%20the%20potential%20to%20support%0Ahumans%20in%20processing%20vast%20amounts%20of%20information.%20While%20MLLMs%20are%20already%20being%0Aused%20as%20a%20fact-checking%20tool%2C%20their%20abilities%20and%20limitations%20in%20this%20regard%0Aare%20understudied.%20Here%20is%20aim%20to%20bridge%20this%20gap.%20In%20particular%2C%20we%20propose%20a%0Aframework%20for%20systematically%20assessing%20the%20capacity%20of%20current%20multimodal%0Amodels%20to%20facilitate%20real-world%20fact-checking.%20Our%20methodology%20is%0Aevidence-free%2C%20leveraging%20only%20these%20models%27%20intrinsic%20knowledge%20and%20reasoning%0Acapabilities.%20By%20designing%20prompts%20that%20extract%20models%27%20predictions%2C%0Aexplanations%2C%20and%20confidence%20levels%2C%20we%20delve%20into%20research%20questions%0Aconcerning%20model%20accuracy%2C%20robustness%2C%20and%20reasons%20for%20failure.%20We%20empirically%0Afind%20that%20%281%29%20GPT-4V%20exhibits%20superior%20performance%20in%20identifying%20malicious%20and%0Amisleading%20multimodal%20claims%2C%20with%20the%20ability%20to%20explain%20the%20unreasonable%0Aaspects%20and%20underlying%20motives%2C%20and%20%282%29%20existing%20open-source%20models%20exhibit%0Astrong%20biases%20and%20are%20highly%20sensitive%20to%20the%20prompt.%20Our%20study%20offers%20insights%0Ainto%20combating%20false%20multimodal%20information%20and%20building%20secure%2C%20trustworthy%0Amultimodal%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20evaluate%0AMLLMs%20for%20real-world%20fact-checking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03627v1&entry.124074799=Read"},
{"title": "AllSpark: Reborn Labeled Features from Unlabeled in Transformer for\n  Semi-Supervised Semantic Segmentation", "author": "Haonan Wang and Qixiang Zhang and Yi Li and Xiaomeng Li", "abstract": "  Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate\nthe burden of time-consuming pixel-level manual labeling, which leverages\nlimited labeled data along with larger amounts of unlabeled data. Current\nstate-of-the-art methods train the labeled data with ground truths and\nunlabeled data with pseudo labels. However, the two training flows are\nseparate, which allows labeled data to dominate the training process, resulting\nin low-quality pseudo labels and, consequently, sub-optimal results. To\nalleviate this issue, we present AllSpark, which reborns the labeled features\nfrom unlabeled ones with the channel-wise cross-attention mechanism. We further\nintroduce a Semantic Memory along with a Channel Semantic Grouping strategy to\nensure that unlabeled features adequately represent labeled features. The\nAllSpark shed new light on the architecture level designs of SSSS rather than\nframework level, which avoids increasingly complicated training pipeline\ndesigns. It can also be regarded as a flexible bottleneck module that can be\nseamlessly integrated into a general transformer-based segmentation model. The\nproposed AllSpark outperforms existing methods across all evaluation protocols\non Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and\nmodel weights are available at: https://github.com/xmed-lab/AllSpark.\n", "link": "http://arxiv.org/abs/2403.01818v2", "date": "2024-03-06", "relevancy": 1.9899, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.481}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AllSpark%3A%20Reborn%20Labeled%20Features%20from%20Unlabeled%20in%20Transformer%20for%0A%20%20Semi-Supervised%20Semantic%20Segmentation&entry.906535625=Haonan%20Wang%20and%20Qixiang%20Zhang%20and%20Yi%20Li%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Semi-supervised%20semantic%20segmentation%20%28SSSS%29%20has%20been%20proposed%20to%20alleviate%0Athe%20burden%20of%20time-consuming%20pixel-level%20manual%20labeling%2C%20which%20leverages%0Alimited%20labeled%20data%20along%20with%20larger%20amounts%20of%20unlabeled%20data.%20Current%0Astate-of-the-art%20methods%20train%20the%20labeled%20data%20with%20ground%20truths%20and%0Aunlabeled%20data%20with%20pseudo%20labels.%20However%2C%20the%20two%20training%20flows%20are%0Aseparate%2C%20which%20allows%20labeled%20data%20to%20dominate%20the%20training%20process%2C%20resulting%0Ain%20low-quality%20pseudo%20labels%20and%2C%20consequently%2C%20sub-optimal%20results.%20To%0Aalleviate%20this%20issue%2C%20we%20present%20AllSpark%2C%20which%20reborns%20the%20labeled%20features%0Afrom%20unlabeled%20ones%20with%20the%20channel-wise%20cross-attention%20mechanism.%20We%20further%0Aintroduce%20a%20Semantic%20Memory%20along%20with%20a%20Channel%20Semantic%20Grouping%20strategy%20to%0Aensure%20that%20unlabeled%20features%20adequately%20represent%20labeled%20features.%20The%0AAllSpark%20shed%20new%20light%20on%20the%20architecture%20level%20designs%20of%20SSSS%20rather%20than%0Aframework%20level%2C%20which%20avoids%20increasingly%20complicated%20training%20pipeline%0Adesigns.%20It%20can%20also%20be%20regarded%20as%20a%20flexible%20bottleneck%20module%20that%20can%20be%0Aseamlessly%20integrated%20into%20a%20general%20transformer-based%20segmentation%20model.%20The%0Aproposed%20AllSpark%20outperforms%20existing%20methods%20across%20all%20evaluation%20protocols%0Aon%20Pascal%2C%20Cityscapes%20and%20COCO%20benchmarks%20without%20bells-and-whistles.%20Code%20and%0Amodel%20weights%20are%20available%20at%3A%20https%3A//github.com/xmed-lab/AllSpark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01818v2&entry.124074799=Read"},
{"title": "Robust Quantification of Percent Emphysema on CT via Domain Attention:\n  the Multi-Ethnic Study of Atherosclerosis (MESA) Lung Study", "author": "Xuzhe Zhang and Elsa D. Angelini and Eric A. Hoffman and Karol E. Watson and Benjamin M. Smith and R. Graham Barr and Andrew F. Laine", "abstract": "  Robust quantification of pulmonary emphysema on computed tomography (CT)\nremains challenging for large-scale research studies that involve scans from\ndifferent scanner types and for translation to clinical scans. Existing studies\nhave explored several directions to tackle this challenge, including density\ncorrection, noise filtering, regression, hidden Markov measure field (HMMF)\nmodel-based segmentation, and volume-adjusted lung density. Despite some\npromising results, previous studies either required a tedious workflow or\nlimited opportunities for downstream emphysema subtyping, limiting efficient\nadaptation on a large-scale study. To alleviate this dilemma, we developed an\nend-to-end deep learning framework based on an existing HMMF segmentation\nframework. We first demonstrate that a regular UNet cannot replicate the\nexisting HMMF results because of the lack of scanner priors. We then design a\nnovel domain attention block to fuse image feature with quantitative scanner\npriors which significantly improves the results.\n", "link": "http://arxiv.org/abs/2402.18383v2", "date": "2024-03-06", "relevancy": 1.9736, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5368}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4927}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4768}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Quantification%20of%20Percent%20Emphysema%20on%20CT%20via%20Domain%20Attention%3A%0A%20%20the%20Multi-Ethnic%20Study%20of%20Atherosclerosis%20%28MESA%29%20Lung%20Study&entry.906535625=Xuzhe%20Zhang%20and%20Elsa%20D.%20Angelini%20and%20Eric%20A.%20Hoffman%20and%20Karol%20E.%20Watson%20and%20Benjamin%20M.%20Smith%20and%20R.%20Graham%20Barr%20and%20Andrew%20F.%20Laine&entry.1292438233=%20%20Robust%20quantification%20of%20pulmonary%20emphysema%20on%20computed%20tomography%20%28CT%29%0Aremains%20challenging%20for%20large-scale%20research%20studies%20that%20involve%20scans%20from%0Adifferent%20scanner%20types%20and%20for%20translation%20to%20clinical%20scans.%20Existing%20studies%0Ahave%20explored%20several%20directions%20to%20tackle%20this%20challenge%2C%20including%20density%0Acorrection%2C%20noise%20filtering%2C%20regression%2C%20hidden%20Markov%20measure%20field%20%28HMMF%29%0Amodel-based%20segmentation%2C%20and%20volume-adjusted%20lung%20density.%20Despite%20some%0Apromising%20results%2C%20previous%20studies%20either%20required%20a%20tedious%20workflow%20or%0Alimited%20opportunities%20for%20downstream%20emphysema%20subtyping%2C%20limiting%20efficient%0Aadaptation%20on%20a%20large-scale%20study.%20To%20alleviate%20this%20dilemma%2C%20we%20developed%20an%0Aend-to-end%20deep%20learning%20framework%20based%20on%20an%20existing%20HMMF%20segmentation%0Aframework.%20We%20first%20demonstrate%20that%20a%20regular%20UNet%20cannot%20replicate%20the%0Aexisting%20HMMF%20results%20because%20of%20the%20lack%20of%20scanner%20priors.%20We%20then%20design%20a%0Anovel%20domain%20attention%20block%20to%20fuse%20image%20feature%20with%20quantitative%20scanner%0Apriors%20which%20significantly%20improves%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18383v2&entry.124074799=Read"},
{"title": "ENOT: Expectile Regularization for Fast and Accurate Training of Neural\n  Optimal Transport", "author": "Nazar Buzun and Maksim Bobrin and Dmitry V. Dylov", "abstract": "  We present a new extension for Neural Optimal Transport (NOT) training\nprocedure, capable of accurately and efficiently estimating optimal\ntransportation plan via specific regularisation on conjugate potentials. The\nmain bottleneck of existing NOT solvers is associated with the procedure of\nfinding a near-exact approximation of the conjugate operator (i.e., the\nc-transform), which is done either by optimizing over maximin objectives or by\nthe computationally-intensive fine-tuning of the initial approximated\nprediction. We resolve both issues by proposing a new, theoretically justified\nloss in the form of expectile regularization that enforces binding conditions\non the learning dual potentials. Such a regularization provides the upper bound\nestimation over the distribution of possible conjugate potentials and makes the\nlearning stable, eliminating the need for additional extensive finetuning. We\nformally justify the efficiency of our method, called Expectile-Regularised\nNeural Optimal Transport (ENOT). ENOT outperforms previous state-of-the-art\napproaches on the Wasserstein-2 benchmark tasks by a large margin (up to a\n3-fold improvement in quality and up to a 10-fold improvement in runtime).\n", "link": "http://arxiv.org/abs/2403.03777v1", "date": "2024-03-06", "relevancy": 1.9663, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5201}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4714}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4707}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENOT%3A%20Expectile%20Regularization%20for%20Fast%20and%20Accurate%20Training%20of%20Neural%0A%20%20Optimal%20Transport&entry.906535625=Nazar%20Buzun%20and%20Maksim%20Bobrin%20and%20Dmitry%20V.%20Dylov&entry.1292438233=%20%20We%20present%20a%20new%20extension%20for%20Neural%20Optimal%20Transport%20%28NOT%29%20training%0Aprocedure%2C%20capable%20of%20accurately%20and%20efficiently%20estimating%20optimal%0Atransportation%20plan%20via%20specific%20regularisation%20on%20conjugate%20potentials.%20The%0Amain%20bottleneck%20of%20existing%20NOT%20solvers%20is%20associated%20with%20the%20procedure%20of%0Afinding%20a%20near-exact%20approximation%20of%20the%20conjugate%20operator%20%28i.e.%2C%20the%0Ac-transform%29%2C%20which%20is%20done%20either%20by%20optimizing%20over%20maximin%20objectives%20or%20by%0Athe%20computationally-intensive%20fine-tuning%20of%20the%20initial%20approximated%0Aprediction.%20We%20resolve%20both%20issues%20by%20proposing%20a%20new%2C%20theoretically%20justified%0Aloss%20in%20the%20form%20of%20expectile%20regularization%20that%20enforces%20binding%20conditions%0Aon%20the%20learning%20dual%20potentials.%20Such%20a%20regularization%20provides%20the%20upper%20bound%0Aestimation%20over%20the%20distribution%20of%20possible%20conjugate%20potentials%20and%20makes%20the%0Alearning%20stable%2C%20eliminating%20the%20need%20for%20additional%20extensive%20finetuning.%20We%0Aformally%20justify%20the%20efficiency%20of%20our%20method%2C%20called%20Expectile-Regularised%0ANeural%20Optimal%20Transport%20%28ENOT%29.%20ENOT%20outperforms%20previous%20state-of-the-art%0Aapproaches%20on%20the%20Wasserstein-2%20benchmark%20tasks%20by%20a%20large%20margin%20%28up%20to%20a%0A3-fold%20improvement%20in%20quality%20and%20up%20to%20a%2010-fold%20improvement%20in%20runtime%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03777v1&entry.124074799=Read"},
{"title": "Neural Koopman prior for data assimilation", "author": "Anthony Frion and Lucas Drumetz and Mauro Dalla Mura and Guillaume Tochon and Abdeldjalil A\u00efssa El Bey", "abstract": "  With the increasing availability of large scale datasets, computational power\nand tools like automatic differentiation and expressive neural network\narchitectures, sequential data are now often treated in a data-driven way, with\na dynamical model trained from the observation data. While neural networks are\noften seen as uninterpretable black-box architectures, they can still benefit\nfrom physical priors on the data and from mathematical knowledge. In this\npaper, we use a neural network architecture which leverages the long-known\nKoopman operator theory to embed dynamical systems in latent spaces where their\ndynamics can be described linearly, enabling a number of appealing features. We\nintroduce methods that enable to train such a model for long-term continuous\nreconstruction, even in difficult contexts where the data comes in\nirregularly-sampled time series. The potential for self-supervised learning is\nalso demonstrated, as we show the promising use of trained dynamical models as\npriors for variational data assimilation techniques, with applications to e.g.\ntime series interpolation and forecasting.\n", "link": "http://arxiv.org/abs/2309.05317v2", "date": "2024-03-06", "relevancy": 1.9607, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5254}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4866}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4797}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Koopman%20prior%20for%20data%20assimilation&entry.906535625=Anthony%20Frion%20and%20Lucas%20Drumetz%20and%20Mauro%20Dalla%20Mura%20and%20Guillaume%20Tochon%20and%20Abdeldjalil%20A%C3%AFssa%20El%20Bey&entry.1292438233=%20%20With%20the%20increasing%20availability%20of%20large%20scale%20datasets%2C%20computational%20power%0Aand%20tools%20like%20automatic%20differentiation%20and%20expressive%20neural%20network%0Aarchitectures%2C%20sequential%20data%20are%20now%20often%20treated%20in%20a%20data-driven%20way%2C%20with%0Aa%20dynamical%20model%20trained%20from%20the%20observation%20data.%20While%20neural%20networks%20are%0Aoften%20seen%20as%20uninterpretable%20black-box%20architectures%2C%20they%20can%20still%20benefit%0Afrom%20physical%20priors%20on%20the%20data%20and%20from%20mathematical%20knowledge.%20In%20this%0Apaper%2C%20we%20use%20a%20neural%20network%20architecture%20which%20leverages%20the%20long-known%0AKoopman%20operator%20theory%20to%20embed%20dynamical%20systems%20in%20latent%20spaces%20where%20their%0Adynamics%20can%20be%20described%20linearly%2C%20enabling%20a%20number%20of%20appealing%20features.%20We%0Aintroduce%20methods%20that%20enable%20to%20train%20such%20a%20model%20for%20long-term%20continuous%0Areconstruction%2C%20even%20in%20difficult%20contexts%20where%20the%20data%20comes%20in%0Airregularly-sampled%20time%20series.%20The%20potential%20for%20self-supervised%20learning%20is%0Aalso%20demonstrated%2C%20as%20we%20show%20the%20promising%20use%20of%20trained%20dynamical%20models%20as%0Apriors%20for%20variational%20data%20assimilation%20techniques%2C%20with%20applications%20to%20e.g.%0Atime%20series%20interpolation%20and%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05317v2&entry.124074799=Read"},
{"title": "Tackling Missing Values in Probabilistic Wind Power Forecasting: A\n  Generative Approach", "author": "Honglin Wen and Pierre Pinson and Jie Gu and Zhijian Jin", "abstract": "  Machine learning techniques have been successfully used in probabilistic wind\npower forecasting. However, the issue of missing values within datasets due to\nsensor failure, for instance, has been overlooked for a long time. Although it\nis natural to consider addressing this issue by imputing missing values before\nmodel estimation and forecasting, we suggest treating missing values and\nforecasting targets indifferently and predicting all unknown values\nsimultaneously based on observations. In this paper, we offer an efficient\nprobabilistic forecasting approach by estimating the joint distribution of\nfeatures and targets based on a generative model. It is free of preprocessing,\nand thus avoids introducing potential errors. Compared with the traditional\n\"impute, then predict\" pipeline, the proposed approach achieves better\nperformance in terms of continuous ranked probability score.\n", "link": "http://arxiv.org/abs/2403.03631v1", "date": "2024-03-06", "relevancy": 1.957, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.499}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4741}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20Missing%20Values%20in%20Probabilistic%20Wind%20Power%20Forecasting%3A%20A%0A%20%20Generative%20Approach&entry.906535625=Honglin%20Wen%20and%20Pierre%20Pinson%20and%20Jie%20Gu%20and%20Zhijian%20Jin&entry.1292438233=%20%20Machine%20learning%20techniques%20have%20been%20successfully%20used%20in%20probabilistic%20wind%0Apower%20forecasting.%20However%2C%20the%20issue%20of%20missing%20values%20within%20datasets%20due%20to%0Asensor%20failure%2C%20for%20instance%2C%20has%20been%20overlooked%20for%20a%20long%20time.%20Although%20it%0Ais%20natural%20to%20consider%20addressing%20this%20issue%20by%20imputing%20missing%20values%20before%0Amodel%20estimation%20and%20forecasting%2C%20we%20suggest%20treating%20missing%20values%20and%0Aforecasting%20targets%20indifferently%20and%20predicting%20all%20unknown%20values%0Asimultaneously%20based%20on%20observations.%20In%20this%20paper%2C%20we%20offer%20an%20efficient%0Aprobabilistic%20forecasting%20approach%20by%20estimating%20the%20joint%20distribution%20of%0Afeatures%20and%20targets%20based%20on%20a%20generative%20model.%20It%20is%20free%20of%20preprocessing%2C%0Aand%20thus%20avoids%20introducing%20potential%20errors.%20Compared%20with%20the%20traditional%0A%22impute%2C%20then%20predict%22%20pipeline%2C%20the%20proposed%20approach%20achieves%20better%0Aperformance%20in%20terms%20of%20continuous%20ranked%20probability%20score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03631v1&entry.124074799=Read"},
{"title": "Parameterized Projected Bellman Operator", "author": "Th\u00e9o Vincent and Alberto Maria Metelli and Boris Belousov and Jan Peters and Marcello Restelli and Carlo D'Eramo", "abstract": "  Approximate value iteration (AVI) is a family of algorithms for reinforcement\nlearning (RL) that aims to obtain an approximation of the optimal value\nfunction. Generally, AVI algorithms implement an iterated procedure where each\nstep consists of (i) an application of the Bellman operator and (ii) a\nprojection step into a considered function space. Notoriously, the Bellman\noperator leverages transition samples, which strongly determine its behavior,\nas uninformative samples can result in negligible updates or long detours,\nwhose detrimental effects are further exacerbated by the computationally\nintensive projection step. To address these issues, we propose a novel\nalternative approach based on learning an approximate version of the Bellman\noperator rather than estimating it through samples as in AVI approaches. This\nway, we are able to (i) generalize across transition samples and (ii) avoid the\ncomputationally intensive projection step. For this reason, we call our novel\noperator projected Bellman operator (PBO). We formulate an optimization problem\nto learn PBO for generic sequential decision-making problems, and we\ntheoretically analyze its properties in two representative classes of RL\nproblems. Furthermore, we theoretically study our approach under the lens of\nAVI and devise algorithmic implementations to learn PBO in offline and online\nsettings by leveraging neural network parameterizations. Finally, we\nempirically showcase the benefits of PBO w.r.t. the regular Bellman operator on\nseveral RL problems.\n", "link": "http://arxiv.org/abs/2312.12869v3", "date": "2024-03-06", "relevancy": 1.951, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5196}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4916}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4712}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameterized%20Projected%20Bellman%20Operator&entry.906535625=Th%C3%A9o%20Vincent%20and%20Alberto%20Maria%20Metelli%20and%20Boris%20Belousov%20and%20Jan%20Peters%20and%20Marcello%20Restelli%20and%20Carlo%20D%27Eramo&entry.1292438233=%20%20Approximate%20value%20iteration%20%28AVI%29%20is%20a%20family%20of%20algorithms%20for%20reinforcement%0Alearning%20%28RL%29%20that%20aims%20to%20obtain%20an%20approximation%20of%20the%20optimal%20value%0Afunction.%20Generally%2C%20AVI%20algorithms%20implement%20an%20iterated%20procedure%20where%20each%0Astep%20consists%20of%20%28i%29%20an%20application%20of%20the%20Bellman%20operator%20and%20%28ii%29%20a%0Aprojection%20step%20into%20a%20considered%20function%20space.%20Notoriously%2C%20the%20Bellman%0Aoperator%20leverages%20transition%20samples%2C%20which%20strongly%20determine%20its%20behavior%2C%0Aas%20uninformative%20samples%20can%20result%20in%20negligible%20updates%20or%20long%20detours%2C%0Awhose%20detrimental%20effects%20are%20further%20exacerbated%20by%20the%20computationally%0Aintensive%20projection%20step.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%0Aalternative%20approach%20based%20on%20learning%20an%20approximate%20version%20of%20the%20Bellman%0Aoperator%20rather%20than%20estimating%20it%20through%20samples%20as%20in%20AVI%20approaches.%20This%0Away%2C%20we%20are%20able%20to%20%28i%29%20generalize%20across%20transition%20samples%20and%20%28ii%29%20avoid%20the%0Acomputationally%20intensive%20projection%20step.%20For%20this%20reason%2C%20we%20call%20our%20novel%0Aoperator%20projected%20Bellman%20operator%20%28PBO%29.%20We%20formulate%20an%20optimization%20problem%0Ato%20learn%20PBO%20for%20generic%20sequential%20decision-making%20problems%2C%20and%20we%0Atheoretically%20analyze%20its%20properties%20in%20two%20representative%20classes%20of%20RL%0Aproblems.%20Furthermore%2C%20we%20theoretically%20study%20our%20approach%20under%20the%20lens%20of%0AAVI%20and%20devise%20algorithmic%20implementations%20to%20learn%20PBO%20in%20offline%20and%20online%0Asettings%20by%20leveraging%20neural%20network%20parameterizations.%20Finally%2C%20we%0Aempirically%20showcase%20the%20benefits%20of%20PBO%20w.r.t.%20the%20regular%20Bellman%20operator%20on%0Aseveral%20RL%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12869v3&entry.124074799=Read"},
{"title": "SPEAR:Exact Gradient Inversion of Batches in Federated Learning", "author": "Dimitar I. Dimitrov and Maximilian Baader and Mark Niklas M\u00fcller and Martin Vechev", "abstract": "  Federated learning is a popular framework for collaborative machine learning\nwhere multiple clients only share gradient updates on their local data with the\nserver and not the actual data. Unfortunately, it was recently shown that\ngradient inversion attacks can reconstruct this data from these shared\ngradients. Existing attacks enable exact reconstruction only for a batch size\nof $b=1$ in the important honest-but-curious setting, with larger batches\npermitting only approximate reconstruction. In this work, we propose \\emph{the\nfirst algorithm reconstructing whole batches with $b >1$ exactly}. This\napproach combines mathematical insights into the explicit low-rank structure of\ngradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced\ngradient sparsity to precisely filter out large numbers of incorrect samples,\nmaking a final reconstruction step tractable. We provide an efficient GPU\nimplementation for fully connected networks and show that it recovers batches\nof $b \\lesssim 25$ elements exactly while being tractable for large network\nwidths and depths.\n", "link": "http://arxiv.org/abs/2403.03945v1", "date": "2024-03-06", "relevancy": 1.9498, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4913}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4889}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.483}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPEAR%3AExact%20Gradient%20Inversion%20of%20Batches%20in%20Federated%20Learning&entry.906535625=Dimitar%20I.%20Dimitrov%20and%20Maximilian%20Baader%20and%20Mark%20Niklas%20M%C3%BCller%20and%20Martin%20Vechev&entry.1292438233=%20%20Federated%20learning%20is%20a%20popular%20framework%20for%20collaborative%20machine%20learning%0Awhere%20multiple%20clients%20only%20share%20gradient%20updates%20on%20their%20local%20data%20with%20the%0Aserver%20and%20not%20the%20actual%20data.%20Unfortunately%2C%20it%20was%20recently%20shown%20that%0Agradient%20inversion%20attacks%20can%20reconstruct%20this%20data%20from%20these%20shared%0Agradients.%20Existing%20attacks%20enable%20exact%20reconstruction%20only%20for%20a%20batch%20size%0Aof%20%24b%3D1%24%20in%20the%20important%20honest-but-curious%20setting%2C%20with%20larger%20batches%0Apermitting%20only%20approximate%20reconstruction.%20In%20this%20work%2C%20we%20propose%20%5Cemph%7Bthe%0Afirst%20algorithm%20reconstructing%20whole%20batches%20with%20%24b%20%3E1%24%20exactly%7D.%20This%0Aapproach%20combines%20mathematical%20insights%20into%20the%20explicit%20low-rank%20structure%20of%0Agradients%20with%20a%20sampling-based%20algorithm.%20Crucially%2C%20we%20leverage%20ReLU-induced%0Agradient%20sparsity%20to%20precisely%20filter%20out%20large%20numbers%20of%20incorrect%20samples%2C%0Amaking%20a%20final%20reconstruction%20step%20tractable.%20We%20provide%20an%20efficient%20GPU%0Aimplementation%20for%20fully%20connected%20networks%20and%20show%20that%20it%20recovers%20batches%0Aof%20%24b%20%5Clesssim%2025%24%20elements%20exactly%20while%20being%20tractable%20for%20large%20network%0Awidths%20and%20depths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03945v1&entry.124074799=Read"},
{"title": "The Heuristic Core: Understanding Subnetwork Generalization in\n  Pretrained Language Models", "author": "Adithya Bhaskar and Dan Friedman and Danqi Chen", "abstract": "  Prior work has found that pretrained language models (LMs) fine-tuned with\ndifferent random seeds can achieve similar in-domain performance but generalize\ndifferently on tests of syntactic generalization. In this work, we show that,\neven within a single model, we can find multiple subnetworks that perform\nsimilarly in-domain, but generalize vastly differently. To better understand\nthese phenomena, we investigate if they can be understood in terms of\n\"competing subnetworks\": the model initially represents a variety of distinct\nalgorithms, corresponding to different subnetworks, and generalization occurs\nwhen it ultimately converges to one. This explanation has been used to account\nfor generalization in simple algorithmic tasks. Instead of finding competing\nsubnetworks, we find that all subnetworks -- whether they generalize or not --\nshare a set of attention heads, which we refer to as the heuristic core.\nFurther analysis suggests that these attention heads emerge early in training\nand compute shallow, non-generalizing features. The model learns to generalize\nby incorporating additional attention heads, which depend on the outputs of the\n\"heuristic\" heads to compute higher-level features. Overall, our results offer\na more detailed picture of the mechanisms for syntactic generalization in\npretrained LMs.\n", "link": "http://arxiv.org/abs/2403.03942v1", "date": "2024-03-06", "relevancy": 1.9498, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4948}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4715}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Heuristic%20Core%3A%20Understanding%20Subnetwork%20Generalization%20in%0A%20%20Pretrained%20Language%20Models&entry.906535625=Adithya%20Bhaskar%20and%20Dan%20Friedman%20and%20Danqi%20Chen&entry.1292438233=%20%20Prior%20work%20has%20found%20that%20pretrained%20language%20models%20%28LMs%29%20fine-tuned%20with%0Adifferent%20random%20seeds%20can%20achieve%20similar%20in-domain%20performance%20but%20generalize%0Adifferently%20on%20tests%20of%20syntactic%20generalization.%20In%20this%20work%2C%20we%20show%20that%2C%0Aeven%20within%20a%20single%20model%2C%20we%20can%20find%20multiple%20subnetworks%20that%20perform%0Asimilarly%20in-domain%2C%20but%20generalize%20vastly%20differently.%20To%20better%20understand%0Athese%20phenomena%2C%20we%20investigate%20if%20they%20can%20be%20understood%20in%20terms%20of%0A%22competing%20subnetworks%22%3A%20the%20model%20initially%20represents%20a%20variety%20of%20distinct%0Aalgorithms%2C%20corresponding%20to%20different%20subnetworks%2C%20and%20generalization%20occurs%0Awhen%20it%20ultimately%20converges%20to%20one.%20This%20explanation%20has%20been%20used%20to%20account%0Afor%20generalization%20in%20simple%20algorithmic%20tasks.%20Instead%20of%20finding%20competing%0Asubnetworks%2C%20we%20find%20that%20all%20subnetworks%20--%20whether%20they%20generalize%20or%20not%20--%0Ashare%20a%20set%20of%20attention%20heads%2C%20which%20we%20refer%20to%20as%20the%20heuristic%20core.%0AFurther%20analysis%20suggests%20that%20these%20attention%20heads%20emerge%20early%20in%20training%0Aand%20compute%20shallow%2C%20non-generalizing%20features.%20The%20model%20learns%20to%20generalize%0Aby%20incorporating%20additional%20attention%20heads%2C%20which%20depend%20on%20the%20outputs%20of%20the%0A%22heuristic%22%20heads%20to%20compute%20higher-level%20features.%20Overall%2C%20our%20results%20offer%0Aa%20more%20detailed%20picture%20of%20the%20mechanisms%20for%20syntactic%20generalization%20in%0Apretrained%20LMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03942v1&entry.124074799=Read"},
{"title": "DART: Implicit Doppler Tomography for Radar Novel View Synthesis", "author": "Tianshu Huang and John Miller and Akarsh Prabhakara and Tao Jin and Tarana Laroia and Zico Kolter and Anthony Rowe", "abstract": "  Simulation is an invaluable tool for radio-frequency system designers that\nenables rapid prototyping of various algorithms for imaging, target detection,\nclassification, and tracking. However, simulating realistic radar scans is a\nchallenging task that requires an accurate model of the scene, radio frequency\nmaterial properties, and a corresponding radar synthesis function. Rather than\nspecifying these models explicitly, we propose DART - Doppler Aided Radar\nTomography, a Neural Radiance Field-inspired method which uses radar-specific\nphysics to create a reflectance and transmittance-based rendering pipeline for\nrange-Doppler images. We then evaluate DART by constructing a custom data\ncollection platform and collecting a novel radar dataset together with accurate\nposition and instantaneous velocity measurements from lidar-based localization.\nIn comparison to state-of-the-art baselines, DART synthesizes superior radar\nrange-Doppler images from novel views across all datasets and additionally can\nbe used to generate high quality tomographic images.\n", "link": "http://arxiv.org/abs/2403.03896v1", "date": "2024-03-06", "relevancy": 1.9488, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5031}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4933}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4747}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DART%3A%20Implicit%20Doppler%20Tomography%20for%20Radar%20Novel%20View%20Synthesis&entry.906535625=Tianshu%20Huang%20and%20John%20Miller%20and%20Akarsh%20Prabhakara%20and%20Tao%20Jin%20and%20Tarana%20Laroia%20and%20Zico%20Kolter%20and%20Anthony%20Rowe&entry.1292438233=%20%20Simulation%20is%20an%20invaluable%20tool%20for%20radio-frequency%20system%20designers%20that%0Aenables%20rapid%20prototyping%20of%20various%20algorithms%20for%20imaging%2C%20target%20detection%2C%0Aclassification%2C%20and%20tracking.%20However%2C%20simulating%20realistic%20radar%20scans%20is%20a%0Achallenging%20task%20that%20requires%20an%20accurate%20model%20of%20the%20scene%2C%20radio%20frequency%0Amaterial%20properties%2C%20and%20a%20corresponding%20radar%20synthesis%20function.%20Rather%20than%0Aspecifying%20these%20models%20explicitly%2C%20we%20propose%20DART%20-%20Doppler%20Aided%20Radar%0ATomography%2C%20a%20Neural%20Radiance%20Field-inspired%20method%20which%20uses%20radar-specific%0Aphysics%20to%20create%20a%20reflectance%20and%20transmittance-based%20rendering%20pipeline%20for%0Arange-Doppler%20images.%20We%20then%20evaluate%20DART%20by%20constructing%20a%20custom%20data%0Acollection%20platform%20and%20collecting%20a%20novel%20radar%20dataset%20together%20with%20accurate%0Aposition%20and%20instantaneous%20velocity%20measurements%20from%20lidar-based%20localization.%0AIn%20comparison%20to%20state-of-the-art%20baselines%2C%20DART%20synthesizes%20superior%20radar%0Arange-Doppler%20images%20from%20novel%20views%20across%20all%20datasets%20and%20additionally%20can%0Abe%20used%20to%20generate%20high%20quality%20tomographic%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03896v1&entry.124074799=Read"},
{"title": "On the Effectiveness of Distillation in Mitigating Backdoors in\n  Pre-trained Encoder", "author": "Tingxu Han and Shenghan Huang and Ziqi Ding and Weisong Sun and Yebo Feng and Chunrong Fang and Jun Li and Hanwei Qian and Cong Wu and Quanjun Zhang and Yang Liu and Zhenyu Chen", "abstract": "  In this paper, we study a defense against poisoned encoders in SSL called\ndistillation, which is a defense used in supervised learning originally.\nDistillation aims to distill knowledge from a given model (a.k.a the teacher\nnet) and transfer it to another (a.k.a the student net). Now, we use it to\ndistill benign knowledge from poisoned pre-trained encoders and transfer it to\na new encoder, resulting in a clean pre-trained encoder. In particular, we\nconduct an empirical study on the effectiveness and performance of distillation\nagainst poisoned encoders. Using two state-of-the-art backdoor attacks against\npre-trained image encoders and four commonly used image classification\ndatasets, our experimental results show that distillation can reduce attack\nsuccess rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy.\nMoreover, we investigate the impact of three core components of distillation on\nperformance: teacher net, student net, and distillation loss. By comparing 4\ndifferent teacher nets, 3 student nets, and 6 distillation losses, we find that\nfine-tuned teacher nets, warm-up-training-based student nets, and\nattention-based distillation loss perform best, respectively.\n", "link": "http://arxiv.org/abs/2403.03846v1", "date": "2024-03-06", "relevancy": 1.9399, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.499}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4873}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.477}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Effectiveness%20of%20Distillation%20in%20Mitigating%20Backdoors%20in%0A%20%20Pre-trained%20Encoder&entry.906535625=Tingxu%20Han%20and%20Shenghan%20Huang%20and%20Ziqi%20Ding%20and%20Weisong%20Sun%20and%20Yebo%20Feng%20and%20Chunrong%20Fang%20and%20Jun%20Li%20and%20Hanwei%20Qian%20and%20Cong%20Wu%20and%20Quanjun%20Zhang%20and%20Yang%20Liu%20and%20Zhenyu%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20a%20defense%20against%20poisoned%20encoders%20in%20SSL%20called%0Adistillation%2C%20which%20is%20a%20defense%20used%20in%20supervised%20learning%20originally.%0ADistillation%20aims%20to%20distill%20knowledge%20from%20a%20given%20model%20%28a.k.a%20the%20teacher%0Anet%29%20and%20transfer%20it%20to%20another%20%28a.k.a%20the%20student%20net%29.%20Now%2C%20we%20use%20it%20to%0Adistill%20benign%20knowledge%20from%20poisoned%20pre-trained%20encoders%20and%20transfer%20it%20to%0Aa%20new%20encoder%2C%20resulting%20in%20a%20clean%20pre-trained%20encoder.%20In%20particular%2C%20we%0Aconduct%20an%20empirical%20study%20on%20the%20effectiveness%20and%20performance%20of%20distillation%0Aagainst%20poisoned%20encoders.%20Using%20two%20state-of-the-art%20backdoor%20attacks%20against%0Apre-trained%20image%20encoders%20and%20four%20commonly%20used%20image%20classification%0Adatasets%2C%20our%20experimental%20results%20show%20that%20distillation%20can%20reduce%20attack%0Asuccess%20rate%20from%2080.87%25%20to%2027.51%25%20while%20suffering%20a%206.35%25%20loss%20in%20accuracy.%0AMoreover%2C%20we%20investigate%20the%20impact%20of%20three%20core%20components%20of%20distillation%20on%0Aperformance%3A%20teacher%20net%2C%20student%20net%2C%20and%20distillation%20loss.%20By%20comparing%204%0Adifferent%20teacher%20nets%2C%203%20student%20nets%2C%20and%206%20distillation%20losses%2C%20we%20find%20that%0Afine-tuned%20teacher%20nets%2C%20warm-up-training-based%20student%20nets%2C%20and%0Aattention-based%20distillation%20loss%20perform%20best%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03846v1&entry.124074799=Read"},
{"title": "Extreme Precipitation Nowcasting using Transformer-based Generative\n  Models", "author": "Cristian Meo and Ankush Roy and Mircea Lic\u0103 and Junzhe Yin and Zeineb Bou Che and Yanbo Wang and Ruben Imhoff and Remko Uijlenhoet and Justin Dauwels", "abstract": "  This paper presents an innovative approach to extreme precipitation\nnowcasting by employing Transformer-based generative models, namely\nNowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a\ncomprehensive dataset from the Royal Netherlands Meteorological Institute\n(KNMI), our study focuses on predicting short-term precipitation with high\naccuracy. We introduce a novel method for computing EVL without assuming fixed\nextreme representations, addressing the limitations of current models in\ncapturing extreme weather events. We present both qualitative and quantitative\nanalyses, demonstrating the superior performance of the proposed\nNowcastingGPT-EVL in generating accurate precipitation forecasts, especially\nwhen dealing with extreme precipitation events. The code is available at\n\\url{https://github.com/Cmeo97/NowcastingGPT}.\n", "link": "http://arxiv.org/abs/2403.03929v1", "date": "2024-03-06", "relevancy": 1.9392, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5271}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4819}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4708}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extreme%20Precipitation%20Nowcasting%20using%20Transformer-based%20Generative%0A%20%20Models&entry.906535625=Cristian%20Meo%20and%20Ankush%20Roy%20and%20Mircea%20Lic%C4%83%20and%20Junzhe%20Yin%20and%20Zeineb%20Bou%20Che%20and%20Yanbo%20Wang%20and%20Ruben%20Imhoff%20and%20Remko%20Uijlenhoet%20and%20Justin%20Dauwels&entry.1292438233=%20%20This%20paper%20presents%20an%20innovative%20approach%20to%20extreme%20precipitation%0Anowcasting%20by%20employing%20Transformer-based%20generative%20models%2C%20namely%0ANowcastingGPT%20with%20Extreme%20Value%20Loss%20%28EVL%29%20regularization.%20Leveraging%20a%0Acomprehensive%20dataset%20from%20the%20Royal%20Netherlands%20Meteorological%20Institute%0A%28KNMI%29%2C%20our%20study%20focuses%20on%20predicting%20short-term%20precipitation%20with%20high%0Aaccuracy.%20We%20introduce%20a%20novel%20method%20for%20computing%20EVL%20without%20assuming%20fixed%0Aextreme%20representations%2C%20addressing%20the%20limitations%20of%20current%20models%20in%0Acapturing%20extreme%20weather%20events.%20We%20present%20both%20qualitative%20and%20quantitative%0Aanalyses%2C%20demonstrating%20the%20superior%20performance%20of%20the%20proposed%0ANowcastingGPT-EVL%20in%20generating%20accurate%20precipitation%20forecasts%2C%20especially%0Awhen%20dealing%20with%20extreme%20precipitation%20events.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Cmeo97/NowcastingGPT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03929v1&entry.124074799=Read"},
{"title": "IRCoder: Intermediate Representations Make Language Models Robust\n  Multilingual Code Generators", "author": "Indraneil Paul and Jun Luo and Goran Glava\u0161 and Iryna Gurevych", "abstract": "  Code understanding and generation have fast become some of the most popular\napplications of language models (LMs). Nonetheless, research on multilingual\naspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual\ntransfer between different programming languages, language-specific data\naugmentation, and post-hoc LM adaptation, alongside exploitation of data\nsources other than the original textual content, has been much sparser than for\ntheir natural language counterparts. In particular, most mainstream Code-LMs\nhave been pre-trained on source code files alone. In this work, we investigate\nthe prospect of leveraging readily available compiler intermediate\nrepresentations - shared across programming languages - to improve the\nmultilingual capabilities of Code-LMs and facilitate cross-lingual transfer.\n  To this end, we first compile SLTrans, a parallel dataset consisting of\nnearly 4M self-contained source code files coupled with respective intermediate\nrepresentations. Next, starting from various base Code-LMs (ranging in size\nfrom 1.1B to 7.3B parameters), we carry out continued causal language modelling\ntraining on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2)\nalign the IR constructs with respective constructs of various programming\nlanguages. Our resulting models, dubbed IRCoder, display sizeable and\nconsistent gains across a wide variety of code generation tasks and metrics,\nincluding prompt robustness, multilingual code completion, code understanding,\nand instruction following.\n", "link": "http://arxiv.org/abs/2403.03894v1", "date": "2024-03-06", "relevancy": 1.939, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4878}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4843}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4839}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRCoder%3A%20Intermediate%20Representations%20Make%20Language%20Models%20Robust%0A%20%20Multilingual%20Code%20Generators&entry.906535625=Indraneil%20Paul%20and%20Jun%20Luo%20and%20Goran%20Glava%C5%A1%20and%20Iryna%20Gurevych&entry.1292438233=%20%20Code%20understanding%20and%20generation%20have%20fast%20become%20some%20of%20the%20most%20popular%0Aapplications%20of%20language%20models%20%28LMs%29.%20Nonetheless%2C%20research%20on%20multilingual%0Aaspects%20of%20Code-LMs%20%28i.e.%2C%20LMs%20for%20code%20generation%29%20such%20as%20cross-lingual%0Atransfer%20between%20different%20programming%20languages%2C%20language-specific%20data%0Aaugmentation%2C%20and%20post-hoc%20LM%20adaptation%2C%20alongside%20exploitation%20of%20data%0Asources%20other%20than%20the%20original%20textual%20content%2C%20has%20been%20much%20sparser%20than%20for%0Atheir%20natural%20language%20counterparts.%20In%20particular%2C%20most%20mainstream%20Code-LMs%0Ahave%20been%20pre-trained%20on%20source%20code%20files%20alone.%20In%20this%20work%2C%20we%20investigate%0Athe%20prospect%20of%20leveraging%20readily%20available%20compiler%20intermediate%0Arepresentations%20-%20shared%20across%20programming%20languages%20-%20to%20improve%20the%0Amultilingual%20capabilities%20of%20Code-LMs%20and%20facilitate%20cross-lingual%20transfer.%0A%20%20To%20this%20end%2C%20we%20first%20compile%20SLTrans%2C%20a%20parallel%20dataset%20consisting%20of%0Anearly%204M%20self-contained%20source%20code%20files%20coupled%20with%20respective%20intermediate%0Arepresentations.%20Next%2C%20starting%20from%20various%20base%20Code-LMs%20%28ranging%20in%20size%0Afrom%201.1B%20to%207.3B%20parameters%29%2C%20we%20carry%20out%20continued%20causal%20language%20modelling%0Atraining%20on%20SLTrans%2C%20forcing%20the%20Code-LMs%20to%20%281%29%20learn%20the%20IR%20language%20and%20%282%29%0Aalign%20the%20IR%20constructs%20with%20respective%20constructs%20of%20various%20programming%0Alanguages.%20Our%20resulting%20models%2C%20dubbed%20IRCoder%2C%20display%20sizeable%20and%0Aconsistent%20gains%20across%20a%20wide%20variety%20of%20code%20generation%20tasks%20and%20metrics%2C%0Aincluding%20prompt%20robustness%2C%20multilingual%20code%20completion%2C%20code%20understanding%2C%0Aand%20instruction%20following.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03894v1&entry.124074799=Read"},
{"title": "Temporal Enhanced Floating Car Observers", "author": "Jeremias Gerner and Klaus Bogenberger and Stefanie Schmidtner", "abstract": "  Floating Car Observers (FCOs) are an innovative method to collect traffic\ndata by deploying sensor-equipped vehicles to detect and locate other vehicles.\nWe demonstrate that even a small penetration rate of FCOs can identify a\nsignificant amount of vehicles at a given intersection. This is achieved\nthrough the emulation of detection within a microscopic traffic simulation.\nAdditionally, leveraging data from previous moments can enhance the detection\nof vehicles in the current frame. Our findings indicate that, with a 20-second\nobservation window, it is possible to recover up to 20\\% of vehicles that are\nnot visible by FCOs in the current timestep. To exploit this, we developed a\ndata-driven strategy, utilizing sequences of Bird's Eye View (BEV)\nrepresentations of detected vehicles and deep learning models. This approach\naims to bring currently undetected vehicles into view in the present moment,\nenhancing the currently detected vehicles. Results of different spatiotemporal\narchitectures show that up to 41\\% of the vehicles can be recovered into the\ncurrent timestep at their current position. This enhancement enriches the\ninformation initially available by the FCO, allowing an improved estimation of\ntraffic states and metrics (e.g. density and queue length) for improved\nimplementation of traffic management strategies.\n", "link": "http://arxiv.org/abs/2403.03825v1", "date": "2024-03-06", "relevancy": 1.9366, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5157}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4655}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.46}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Enhanced%20Floating%20Car%20Observers&entry.906535625=Jeremias%20Gerner%20and%20Klaus%20Bogenberger%20and%20Stefanie%20Schmidtner&entry.1292438233=%20%20Floating%20Car%20Observers%20%28FCOs%29%20are%20an%20innovative%20method%20to%20collect%20traffic%0Adata%20by%20deploying%20sensor-equipped%20vehicles%20to%20detect%20and%20locate%20other%20vehicles.%0AWe%20demonstrate%20that%20even%20a%20small%20penetration%20rate%20of%20FCOs%20can%20identify%20a%0Asignificant%20amount%20of%20vehicles%20at%20a%20given%20intersection.%20This%20is%20achieved%0Athrough%20the%20emulation%20of%20detection%20within%20a%20microscopic%20traffic%20simulation.%0AAdditionally%2C%20leveraging%20data%20from%20previous%20moments%20can%20enhance%20the%20detection%0Aof%20vehicles%20in%20the%20current%20frame.%20Our%20findings%20indicate%20that%2C%20with%20a%2020-second%0Aobservation%20window%2C%20it%20is%20possible%20to%20recover%20up%20to%2020%5C%25%20of%20vehicles%20that%20are%0Anot%20visible%20by%20FCOs%20in%20the%20current%20timestep.%20To%20exploit%20this%2C%20we%20developed%20a%0Adata-driven%20strategy%2C%20utilizing%20sequences%20of%20Bird%27s%20Eye%20View%20%28BEV%29%0Arepresentations%20of%20detected%20vehicles%20and%20deep%20learning%20models.%20This%20approach%0Aaims%20to%20bring%20currently%20undetected%20vehicles%20into%20view%20in%20the%20present%20moment%2C%0Aenhancing%20the%20currently%20detected%20vehicles.%20Results%20of%20different%20spatiotemporal%0Aarchitectures%20show%20that%20up%20to%2041%5C%25%20of%20the%20vehicles%20can%20be%20recovered%20into%20the%0Acurrent%20timestep%20at%20their%20current%20position.%20This%20enhancement%20enriches%20the%0Ainformation%20initially%20available%20by%20the%20FCO%2C%20allowing%20an%20improved%20estimation%20of%0Atraffic%20states%20and%20metrics%20%28e.g.%20density%20and%20queue%20length%29%20for%20improved%0Aimplementation%20of%20traffic%20management%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03825v1&entry.124074799=Read"},
{"title": "Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box\n  Simulators with Noise Parameters", "author": "John Joshua Miller and Simon Mak", "abstract": "  The optimization of a black-box simulator over control parameters\n$\\mathbf{x}$ arises in a myriad of scientific applications. In such\napplications, the simulator often takes the form\n$f(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\boldsymbol{\\theta}$ are parameters\nthat are uncertain in practice. Robust optimization aims to optimize the\nobjective $\\mathbb{E}[f(\\mathbf{x},\\boldsymbol{\\Theta})]$, where\n$\\boldsymbol{\\Theta} \\sim \\mathcal{P}$ is a random variable that models\nuncertainty on $\\boldsymbol{\\theta}$. For this, existing black-box methods\ntypically employ a two-stage approach for selecting the next point\n$(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\mathbf{x}$ and\n$\\boldsymbol{\\theta}$ are optimized separately via different acquisition\nfunctions. As such, these approaches do not employ a joint acquisition over\n$(\\mathbf{x},\\boldsymbol{\\theta})$, and thus may fail to fully exploit\ncontrol-to-noise interactions for effective robust optimization. To address\nthis, we propose a new Bayesian optimization method called Targeted Variance\nReduction (TVR). The TVR leverages a novel joint acquisition function over\n$(\\mathbf{x},\\boldsymbol{\\theta})$, which targets variance reduction on the\nobjective within the desired region of improvement. Under a Gaussian process\nsurrogate on $f$, the TVR acquisition can be evaluated in closed form, and\nreveals an insightful exploration-exploitation-precision trade-off for robust\nblack-box optimization. The TVR can further accommodate a broad class of\nnon-Gaussian distributions on $\\mathcal{P}$ via a careful integration of\nnormalizing flows. We demonstrate the improved performance of TVR over the\nstate-of-the-art in a suite of numerical experiments and an application to the\nrobust design of automobile brake discs under operational uncertainty.\n", "link": "http://arxiv.org/abs/2403.03816v1", "date": "2024-03-06", "relevancy": 1.9327, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4799}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4693}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Targeted%20Variance%20Reduction%3A%20Robust%20Bayesian%20Optimization%20of%20Black-Box%0A%20%20Simulators%20with%20Noise%20Parameters&entry.906535625=John%20Joshua%20Miller%20and%20Simon%20Mak&entry.1292438233=%20%20The%20optimization%20of%20a%20black-box%20simulator%20over%20control%20parameters%0A%24%5Cmathbf%7Bx%7D%24%20arises%20in%20a%20myriad%20of%20scientific%20applications.%20In%20such%0Aapplications%2C%20the%20simulator%20often%20takes%20the%20form%0A%24f%28%5Cmathbf%7Bx%7D%2C%5Cboldsymbol%7B%5Ctheta%7D%29%24%2C%20where%20%24%5Cboldsymbol%7B%5Ctheta%7D%24%20are%20parameters%0Athat%20are%20uncertain%20in%20practice.%20Robust%20optimization%20aims%20to%20optimize%20the%0Aobjective%20%24%5Cmathbb%7BE%7D%5Bf%28%5Cmathbf%7Bx%7D%2C%5Cboldsymbol%7B%5CTheta%7D%29%5D%24%2C%20where%0A%24%5Cboldsymbol%7B%5CTheta%7D%20%5Csim%20%5Cmathcal%7BP%7D%24%20is%20a%20random%20variable%20that%20models%0Auncertainty%20on%20%24%5Cboldsymbol%7B%5Ctheta%7D%24.%20For%20this%2C%20existing%20black-box%20methods%0Atypically%20employ%20a%20two-stage%20approach%20for%20selecting%20the%20next%20point%0A%24%28%5Cmathbf%7Bx%7D%2C%5Cboldsymbol%7B%5Ctheta%7D%29%24%2C%20where%20%24%5Cmathbf%7Bx%7D%24%20and%0A%24%5Cboldsymbol%7B%5Ctheta%7D%24%20are%20optimized%20separately%20via%20different%20acquisition%0Afunctions.%20As%20such%2C%20these%20approaches%20do%20not%20employ%20a%20joint%20acquisition%20over%0A%24%28%5Cmathbf%7Bx%7D%2C%5Cboldsymbol%7B%5Ctheta%7D%29%24%2C%20and%20thus%20may%20fail%20to%20fully%20exploit%0Acontrol-to-noise%20interactions%20for%20effective%20robust%20optimization.%20To%20address%0Athis%2C%20we%20propose%20a%20new%20Bayesian%20optimization%20method%20called%20Targeted%20Variance%0AReduction%20%28TVR%29.%20The%20TVR%20leverages%20a%20novel%20joint%20acquisition%20function%20over%0A%24%28%5Cmathbf%7Bx%7D%2C%5Cboldsymbol%7B%5Ctheta%7D%29%24%2C%20which%20targets%20variance%20reduction%20on%20the%0Aobjective%20within%20the%20desired%20region%20of%20improvement.%20Under%20a%20Gaussian%20process%0Asurrogate%20on%20%24f%24%2C%20the%20TVR%20acquisition%20can%20be%20evaluated%20in%20closed%20form%2C%20and%0Areveals%20an%20insightful%20exploration-exploitation-precision%20trade-off%20for%20robust%0Ablack-box%20optimization.%20The%20TVR%20can%20further%20accommodate%20a%20broad%20class%20of%0Anon-Gaussian%20distributions%20on%20%24%5Cmathcal%7BP%7D%24%20via%20a%20careful%20integration%20of%0Anormalizing%20flows.%20We%20demonstrate%20the%20improved%20performance%20of%20TVR%20over%20the%0Astate-of-the-art%20in%20a%20suite%20of%20numerical%20experiments%20and%20an%20application%20to%20the%0Arobust%20design%20of%20automobile%20brake%20discs%20under%20operational%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03816v1&entry.124074799=Read"},
{"title": "Generative Active Learning with Variational Autoencoder for Radiology\n  Data Generation in Veterinary Medicine", "author": "In-Gyu Lee and Jun-Young Oh and Hee-Jung Yu and Jae-Hwan Kim and Ki-Dong Eom and Ji-Hoon Jeong", "abstract": "  Recently, with increasing interest in pet healthcare, the demand for\ncomputer-aided diagnosis (CAD) systems in veterinary medicine has increased.\nThe development of veterinary CAD has stagnated due to a lack of sufficient\nradiology data. To overcome the challenge, we propose a generative active\nlearning framework based on a variational autoencoder. This approach aims to\nalleviate the scarcity of reliable data for CAD systems in veterinary medicine.\nThis study utilizes datasets comprising cardiomegaly radiograph data. After\nremoving annotations and standardizing images, we employed a framework for data\naugmentation, which consists of a data generation phase and a query phase for\nfiltering the generated data. The experimental results revealed that as the\ndata generated through this framework was added to the training data of the\ngenerative model, the frechet inception distance consistently decreased from\n84.14 to 50.75 on the radiograph. Subsequently, when the generated data were\nincorporated into the training of the classification model, the false positive\nof the confusion matrix also improved from 0.16 to 0.66 on the radiograph. The\nproposed framework has the potential to address the challenges of data scarcity\nin medical CAD, contributing to its advancement.\n", "link": "http://arxiv.org/abs/2403.03642v1", "date": "2024-03-06", "relevancy": 1.9107, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5119}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4717}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4699}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Active%20Learning%20with%20Variational%20Autoencoder%20for%20Radiology%0A%20%20Data%20Generation%20in%20Veterinary%20Medicine&entry.906535625=In-Gyu%20Lee%20and%20Jun-Young%20Oh%20and%20Hee-Jung%20Yu%20and%20Jae-Hwan%20Kim%20and%20Ki-Dong%20Eom%20and%20Ji-Hoon%20Jeong&entry.1292438233=%20%20Recently%2C%20with%20increasing%20interest%20in%20pet%20healthcare%2C%20the%20demand%20for%0Acomputer-aided%20diagnosis%20%28CAD%29%20systems%20in%20veterinary%20medicine%20has%20increased.%0AThe%20development%20of%20veterinary%20CAD%20has%20stagnated%20due%20to%20a%20lack%20of%20sufficient%0Aradiology%20data.%20To%20overcome%20the%20challenge%2C%20we%20propose%20a%20generative%20active%0Alearning%20framework%20based%20on%20a%20variational%20autoencoder.%20This%20approach%20aims%20to%0Aalleviate%20the%20scarcity%20of%20reliable%20data%20for%20CAD%20systems%20in%20veterinary%20medicine.%0AThis%20study%20utilizes%20datasets%20comprising%20cardiomegaly%20radiograph%20data.%20After%0Aremoving%20annotations%20and%20standardizing%20images%2C%20we%20employed%20a%20framework%20for%20data%0Aaugmentation%2C%20which%20consists%20of%20a%20data%20generation%20phase%20and%20a%20query%20phase%20for%0Afiltering%20the%20generated%20data.%20The%20experimental%20results%20revealed%20that%20as%20the%0Adata%20generated%20through%20this%20framework%20was%20added%20to%20the%20training%20data%20of%20the%0Agenerative%20model%2C%20the%20frechet%20inception%20distance%20consistently%20decreased%20from%0A84.14%20to%2050.75%20on%20the%20radiograph.%20Subsequently%2C%20when%20the%20generated%20data%20were%0Aincorporated%20into%20the%20training%20of%20the%20classification%20model%2C%20the%20false%20positive%0Aof%20the%20confusion%20matrix%20also%20improved%20from%200.16%20to%200.66%20on%20the%20radiograph.%20The%0Aproposed%20framework%20has%20the%20potential%20to%20address%20the%20challenges%20of%20data%20scarcity%0Ain%20medical%20CAD%2C%20contributing%20to%20its%20advancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03642v1&entry.124074799=Read"},
{"title": "Enhancing Instructional Quality: Leveraging Computer-Assisted Textual\n  Analysis to Generate In-Depth Insights from Educational Artifacts", "author": "Zewei Tian and Min Sun and Alex Liu and Shawon Sarkar and Jing Liu", "abstract": "  This paper explores the transformative potential of computer-assisted textual\nanalysis in enhancing instructional quality through in-depth insights from\neducational artifacts. We integrate Richard Elmore's Instructional Core\nFramework to examine how artificial intelligence (AI) and machine learning (ML)\nmethods, particularly natural language processing (NLP), can analyze\neducational content, teacher discourse, and student responses to foster\ninstructional improvement. Through a comprehensive review and case studies\nwithin the Instructional Core Framework, we identify key areas where AI/ML\nintegration offers significant advantages, including teacher coaching, student\nsupport, and content development. We unveil patterns that indicate AI/ML not\nonly streamlines administrative tasks but also introduces novel pathways for\npersonalized learning, providing actionable feedback for educators and\ncontributing to a richer understanding of instructional dynamics. This paper\nemphasizes the importance of aligning AI/ML technologies with pedagogical goals\nto realize their full potential in educational settings, advocating for a\nbalanced approach that considers ethical considerations, data quality, and the\nintegration of human expertise.\n", "link": "http://arxiv.org/abs/2403.03920v1", "date": "2024-03-06", "relevancy": 1.9093, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4842}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4763}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4756}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Instructional%20Quality%3A%20Leveraging%20Computer-Assisted%20Textual%0A%20%20Analysis%20to%20Generate%20In-Depth%20Insights%20from%20Educational%20Artifacts&entry.906535625=Zewei%20Tian%20and%20Min%20Sun%20and%20Alex%20Liu%20and%20Shawon%20Sarkar%20and%20Jing%20Liu&entry.1292438233=%20%20This%20paper%20explores%20the%20transformative%20potential%20of%20computer-assisted%20textual%0Aanalysis%20in%20enhancing%20instructional%20quality%20through%20in-depth%20insights%20from%0Aeducational%20artifacts.%20We%20integrate%20Richard%20Elmore%27s%20Instructional%20Core%0AFramework%20to%20examine%20how%20artificial%20intelligence%20%28AI%29%20and%20machine%20learning%20%28ML%29%0Amethods%2C%20particularly%20natural%20language%20processing%20%28NLP%29%2C%20can%20analyze%0Aeducational%20content%2C%20teacher%20discourse%2C%20and%20student%20responses%20to%20foster%0Ainstructional%20improvement.%20Through%20a%20comprehensive%20review%20and%20case%20studies%0Awithin%20the%20Instructional%20Core%20Framework%2C%20we%20identify%20key%20areas%20where%20AI/ML%0Aintegration%20offers%20significant%20advantages%2C%20including%20teacher%20coaching%2C%20student%0Asupport%2C%20and%20content%20development.%20We%20unveil%20patterns%20that%20indicate%20AI/ML%20not%0Aonly%20streamlines%20administrative%20tasks%20but%20also%20introduces%20novel%20pathways%20for%0Apersonalized%20learning%2C%20providing%20actionable%20feedback%20for%20educators%20and%0Acontributing%20to%20a%20richer%20understanding%20of%20instructional%20dynamics.%20This%20paper%0Aemphasizes%20the%20importance%20of%20aligning%20AI/ML%20technologies%20with%20pedagogical%20goals%0Ato%20realize%20their%20full%20potential%20in%20educational%20settings%2C%20advocating%20for%20a%0Abalanced%20approach%20that%20considers%20ethical%20considerations%2C%20data%20quality%2C%20and%20the%0Aintegration%20of%20human%20expertise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03920v1&entry.124074799=Read"},
{"title": "Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications", "author": "Arun Jambulapati and Syamantak Kumar and Jerry Li and Shourya Pandey and Ankit Pensia and Kevin Tian", "abstract": "  The $k$-principal component analysis ($k$-PCA) problem is a fundamental\nalgorithmic primitive that is widely-used in data analysis and dimensionality\nreduction applications. In statistical settings, the goal of $k$-PCA is to\nidentify a top eigenspace of the covariance matrix of a distribution, which we\nonly have implicit access to via samples. Motivated by these implicit settings,\nwe analyze black-box deflation methods as a framework for designing $k$-PCA\nalgorithms, where we model access to the unknown target matrix via a black-box\n$1$-PCA oracle which returns an approximate top eigenvector, under two popular\nnotions of approximation. Despite being arguably the most natural\nreduction-based approach to $k$-PCA algorithm design, such black-box methods,\nwhich recursively call a $1$-PCA oracle $k$ times, were previously\npoorly-understood.\n  Our main contribution is significantly sharper bounds on the approximation\nparameter degradation of deflation methods for $k$-PCA. For a quadratic form\nnotion of approximation we term ePCA (energy PCA), we show deflation methods\nsuffer no parameter loss. For an alternative well-studied approximation notion\nwe term cPCA (correlation PCA), we tightly characterize the parameter regimes\nwhere deflation methods are feasible. Moreover, we show that in all feasible\nregimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for\nany constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA\nalgorithms robust to dataset contamination, improving prior work both in sample\ncomplexity and approximation quality.\n", "link": "http://arxiv.org/abs/2403.03905v1", "date": "2024-03-06", "relevancy": 1.9087, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3988}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3756}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3708}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Black-Box%20%24k%24-to-%241%24-PCA%20Reductions%3A%20Theory%20and%20Applications&entry.906535625=Arun%20Jambulapati%20and%20Syamantak%20Kumar%20and%20Jerry%20Li%20and%20Shourya%20Pandey%20and%20Ankit%20Pensia%20and%20Kevin%20Tian&entry.1292438233=%20%20The%20%24k%24-principal%20component%20analysis%20%28%24k%24-PCA%29%20problem%20is%20a%20fundamental%0Aalgorithmic%20primitive%20that%20is%20widely-used%20in%20data%20analysis%20and%20dimensionality%0Areduction%20applications.%20In%20statistical%20settings%2C%20the%20goal%20of%20%24k%24-PCA%20is%20to%0Aidentify%20a%20top%20eigenspace%20of%20the%20covariance%20matrix%20of%20a%20distribution%2C%20which%20we%0Aonly%20have%20implicit%20access%20to%20via%20samples.%20Motivated%20by%20these%20implicit%20settings%2C%0Awe%20analyze%20black-box%20deflation%20methods%20as%20a%20framework%20for%20designing%20%24k%24-PCA%0Aalgorithms%2C%20where%20we%20model%20access%20to%20the%20unknown%20target%20matrix%20via%20a%20black-box%0A%241%24-PCA%20oracle%20which%20returns%20an%20approximate%20top%20eigenvector%2C%20under%20two%20popular%0Anotions%20of%20approximation.%20Despite%20being%20arguably%20the%20most%20natural%0Areduction-based%20approach%20to%20%24k%24-PCA%20algorithm%20design%2C%20such%20black-box%20methods%2C%0Awhich%20recursively%20call%20a%20%241%24-PCA%20oracle%20%24k%24%20times%2C%20were%20previously%0Apoorly-understood.%0A%20%20Our%20main%20contribution%20is%20significantly%20sharper%20bounds%20on%20the%20approximation%0Aparameter%20degradation%20of%20deflation%20methods%20for%20%24k%24-PCA.%20For%20a%20quadratic%20form%0Anotion%20of%20approximation%20we%20term%20ePCA%20%28energy%20PCA%29%2C%20we%20show%20deflation%20methods%0Asuffer%20no%20parameter%20loss.%20For%20an%20alternative%20well-studied%20approximation%20notion%0Awe%20term%20cPCA%20%28correlation%20PCA%29%2C%20we%20tightly%20characterize%20the%20parameter%20regimes%0Awhere%20deflation%20methods%20are%20feasible.%20Moreover%2C%20we%20show%20that%20in%20all%20feasible%0Aregimes%2C%20%24k%24-cPCA%20deflation%20algorithms%20suffer%20no%20asymptotic%20parameter%20loss%20for%0Aany%20constant%20%24k%24.%20We%20apply%20our%20framework%20to%20obtain%20state-of-the-art%20%24k%24-PCA%0Aalgorithms%20robust%20to%20dataset%20contamination%2C%20improving%20prior%20work%20both%20in%20sample%0Acomplexity%20and%20approximation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03905v1&entry.124074799=Read"},
{"title": "Neural Architecture Search using Particle Swarm and Ant Colony\n  Optimization", "author": "S\u00e9amus Lankford and Diarmuid Grimes", "abstract": "  Neural network models have a number of hyperparameters that must be chosen\nalong with their architecture. This can be a heavy burden on a novice user,\nchoosing which architecture and what values to assign to parameters. In most\ncases, default hyperparameters and architectures are used. Significant\nimprovements to model accuracy can be achieved through the evaluation of\nmultiple architectures. A process known as Neural Architecture Search (NAS) may\nbe applied to automatically evaluate a large number of such architectures. A\nsystem integrating open source tools for Neural Architecture Search (OpenNAS),\nin the classification of images, has been developed as part of this research.\nOpenNAS takes any dataset of grayscale, or RBG images, and generates\nConvolutional Neural Network (CNN) architectures based on a range of\nmetaheuristics using either an AutoKeras, a transfer learning or a Swarm\nIntelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony\nOptimization (ACO) are used as the SI algorithms. Furthermore, models developed\nthrough such metaheuristics may be combined using stacking ensembles. In the\ncontext of this paper, we focus on training and optimizing CNNs using the Swarm\nIntelligence (SI) components of OpenNAS. Two major types of SI algorithms,\nnamely PSO and ACO, are compared to see which is more effective in generating\nhigher model accuracies. It is shown, with our experimental design, that the\nPSO algorithm performs better than ACO. The performance improvement of PSO is\nmost notable with a more complex dataset. As a baseline, the performance of\nfine-tuned pre-trained models is also evaluated.\n", "link": "http://arxiv.org/abs/2403.03781v1", "date": "2024-03-06", "relevancy": 1.9069, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4998}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4611}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4599}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Architecture%20Search%20using%20Particle%20Swarm%20and%20Ant%20Colony%0A%20%20Optimization&entry.906535625=S%C3%A9amus%20Lankford%20and%20Diarmuid%20Grimes&entry.1292438233=%20%20Neural%20network%20models%20have%20a%20number%20of%20hyperparameters%20that%20must%20be%20chosen%0Aalong%20with%20their%20architecture.%20This%20can%20be%20a%20heavy%20burden%20on%20a%20novice%20user%2C%0Achoosing%20which%20architecture%20and%20what%20values%20to%20assign%20to%20parameters.%20In%20most%0Acases%2C%20default%20hyperparameters%20and%20architectures%20are%20used.%20Significant%0Aimprovements%20to%20model%20accuracy%20can%20be%20achieved%20through%20the%20evaluation%20of%0Amultiple%20architectures.%20A%20process%20known%20as%20Neural%20Architecture%20Search%20%28NAS%29%20may%0Abe%20applied%20to%20automatically%20evaluate%20a%20large%20number%20of%20such%20architectures.%20A%0Asystem%20integrating%20open%20source%20tools%20for%20Neural%20Architecture%20Search%20%28OpenNAS%29%2C%0Ain%20the%20classification%20of%20images%2C%20has%20been%20developed%20as%20part%20of%20this%20research.%0AOpenNAS%20takes%20any%20dataset%20of%20grayscale%2C%20or%20RBG%20images%2C%20and%20generates%0AConvolutional%20Neural%20Network%20%28CNN%29%20architectures%20based%20on%20a%20range%20of%0Ametaheuristics%20using%20either%20an%20AutoKeras%2C%20a%20transfer%20learning%20or%20a%20Swarm%0AIntelligence%20%28SI%29%20approach.%20Particle%20Swarm%20Optimization%20%28PSO%29%20and%20Ant%20Colony%0AOptimization%20%28ACO%29%20are%20used%20as%20the%20SI%20algorithms.%20Furthermore%2C%20models%20developed%0Athrough%20such%20metaheuristics%20may%20be%20combined%20using%20stacking%20ensembles.%20In%20the%0Acontext%20of%20this%20paper%2C%20we%20focus%20on%20training%20and%20optimizing%20CNNs%20using%20the%20Swarm%0AIntelligence%20%28SI%29%20components%20of%20OpenNAS.%20Two%20major%20types%20of%20SI%20algorithms%2C%0Anamely%20PSO%20and%20ACO%2C%20are%20compared%20to%20see%20which%20is%20more%20effective%20in%20generating%0Ahigher%20model%20accuracies.%20It%20is%20shown%2C%20with%20our%20experimental%20design%2C%20that%20the%0APSO%20algorithm%20performs%20better%20than%20ACO.%20The%20performance%20improvement%20of%20PSO%20is%0Amost%20notable%20with%20a%20more%20complex%20dataset.%20As%20a%20baseline%2C%20the%20performance%20of%0Afine-tuned%20pre-trained%20models%20is%20also%20evaluated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03781v1&entry.124074799=Read"},
{"title": "From Clicks to Security: Investigating Continuous Authentication via\n  Mouse Dynamics", "author": "Rushit Dave and Marcho Handoko and Ali Rashid and Cole Schoenbauer", "abstract": "  In the realm of computer security, the importance of efficient and reliable\nuser authentication methods has become increasingly critical. This paper\nexamines the potential of mouse movement dynamics as a consistent metric for\ncontinuous authentication. By analyzing user mouse movement patterns in two\ncontrasting gaming scenarios, \"Team Fortress\" and Poly Bridge we investigate\nthe distinctive behavioral patterns inherent in high-intensity and\nlow-intensity UI interactions. The study extends beyond conventional\nmethodologies by employing a range of machine learning models. These models are\ncarefully selected to assess their effectiveness in capturing and interpreting\nthe subtleties of user behavior as reflected in their mouse movements. This\nmultifaceted approach allows for a more nuanced and comprehensive understanding\nof user interaction patterns. Our findings reveal that mouse movement dynamics\ncan serve as a reliable indicator for continuous user authentication. The\ndiverse machine learning models employed in this study demonstrate competent\nperformance in user verification, marking an improvement over previous methods\nused in this field. This research contributes to the ongoing efforts to enhance\ncomputer security and highlights the potential of leveraging user behavior,\nspecifically mouse dynamics, in developing robust authentication systems.\n", "link": "http://arxiv.org/abs/2403.03828v1", "date": "2024-03-06", "relevancy": 1.899, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4829}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4625}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Clicks%20to%20Security%3A%20Investigating%20Continuous%20Authentication%20via%0A%20%20Mouse%20Dynamics&entry.906535625=Rushit%20Dave%20and%20Marcho%20Handoko%20and%20Ali%20Rashid%20and%20Cole%20Schoenbauer&entry.1292438233=%20%20In%20the%20realm%20of%20computer%20security%2C%20the%20importance%20of%20efficient%20and%20reliable%0Auser%20authentication%20methods%20has%20become%20increasingly%20critical.%20This%20paper%0Aexamines%20the%20potential%20of%20mouse%20movement%20dynamics%20as%20a%20consistent%20metric%20for%0Acontinuous%20authentication.%20By%20analyzing%20user%20mouse%20movement%20patterns%20in%20two%0Acontrasting%20gaming%20scenarios%2C%20%22Team%20Fortress%22%20and%20Poly%20Bridge%20we%20investigate%0Athe%20distinctive%20behavioral%20patterns%20inherent%20in%20high-intensity%20and%0Alow-intensity%20UI%20interactions.%20The%20study%20extends%20beyond%20conventional%0Amethodologies%20by%20employing%20a%20range%20of%20machine%20learning%20models.%20These%20models%20are%0Acarefully%20selected%20to%20assess%20their%20effectiveness%20in%20capturing%20and%20interpreting%0Athe%20subtleties%20of%20user%20behavior%20as%20reflected%20in%20their%20mouse%20movements.%20This%0Amultifaceted%20approach%20allows%20for%20a%20more%20nuanced%20and%20comprehensive%20understanding%0Aof%20user%20interaction%20patterns.%20Our%20findings%20reveal%20that%20mouse%20movement%20dynamics%0Acan%20serve%20as%20a%20reliable%20indicator%20for%20continuous%20user%20authentication.%20The%0Adiverse%20machine%20learning%20models%20employed%20in%20this%20study%20demonstrate%20competent%0Aperformance%20in%20user%20verification%2C%20marking%20an%20improvement%20over%20previous%20methods%0Aused%20in%20this%20field.%20This%20research%20contributes%20to%20the%20ongoing%20efforts%20to%20enhance%0Acomputer%20security%20and%20highlights%20the%20potential%20of%20leveraging%20user%20behavior%2C%0Aspecifically%20mouse%20dynamics%2C%20in%20developing%20robust%20authentication%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03828v1&entry.124074799=Read"},
{"title": "Towards Safe and Aligned Large Language Models for Medicine", "author": "Tessa Han and Aounon Kumar and Chirag Agarwal and Himabindu Lakkaraju", "abstract": "  The capabilities of large language models (LLMs) have been progressing at a\nbreathtaking speed, leaving even their own developers grappling with the depth\nof their potential and risks. While initial steps have been taken to evaluate\nthe safety and alignment of general-knowledge LLMs, exposing some weaknesses,\nto our knowledge, the safety and alignment of medical LLMs has not been\nevaluated despite their risks for personal health and safety, public health and\nsafety, and human rights. To this end, we carry out the first safety evaluation\nfor medical LLMs. Specifically, we set forth a definition of medical safety and\nalignment for medical artificial intelligence systems, develop a dataset of\nharmful medical questions to evaluate the medical safety and alignment of an\nLLM, evaluate both general and medical safety and alignment of medical LLMs,\ndemonstrate fine-tuning as an effective mitigation strategy, and discuss\nbroader, large-scale approaches used by the machine learning community to\ndevelop safe and aligned LLMs. We hope that this work casts light on the safety\nand alignment of medical LLMs and motivates future work to study it and develop\nadditional mitigation strategies, minimizing the risks of harm of LLMs in\nmedicine.\n", "link": "http://arxiv.org/abs/2403.03744v1", "date": "2024-03-06", "relevancy": 1.8886, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4649}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4478}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Safe%20and%20Aligned%20Large%20Language%20Models%20for%20Medicine&entry.906535625=Tessa%20Han%20and%20Aounon%20Kumar%20and%20Chirag%20Agarwal%20and%20Himabindu%20Lakkaraju&entry.1292438233=%20%20The%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20have%20been%20progressing%20at%20a%0Abreathtaking%20speed%2C%20leaving%20even%20their%20own%20developers%20grappling%20with%20the%20depth%0Aof%20their%20potential%20and%20risks.%20While%20initial%20steps%20have%20been%20taken%20to%20evaluate%0Athe%20safety%20and%20alignment%20of%20general-knowledge%20LLMs%2C%20exposing%20some%20weaknesses%2C%0Ato%20our%20knowledge%2C%20the%20safety%20and%20alignment%20of%20medical%20LLMs%20has%20not%20been%0Aevaluated%20despite%20their%20risks%20for%20personal%20health%20and%20safety%2C%20public%20health%20and%0Asafety%2C%20and%20human%20rights.%20To%20this%20end%2C%20we%20carry%20out%20the%20first%20safety%20evaluation%0Afor%20medical%20LLMs.%20Specifically%2C%20we%20set%20forth%20a%20definition%20of%20medical%20safety%20and%0Aalignment%20for%20medical%20artificial%20intelligence%20systems%2C%20develop%20a%20dataset%20of%0Aharmful%20medical%20questions%20to%20evaluate%20the%20medical%20safety%20and%20alignment%20of%20an%0ALLM%2C%20evaluate%20both%20general%20and%20medical%20safety%20and%20alignment%20of%20medical%20LLMs%2C%0Ademonstrate%20fine-tuning%20as%20an%20effective%20mitigation%20strategy%2C%20and%20discuss%0Abroader%2C%20large-scale%20approaches%20used%20by%20the%20machine%20learning%20community%20to%0Adevelop%20safe%20and%20aligned%20LLMs.%20We%20hope%20that%20this%20work%20casts%20light%20on%20the%20safety%0Aand%20alignment%20of%20medical%20LLMs%20and%20motivates%20future%20work%20to%20study%20it%20and%20develop%0Aadditional%20mitigation%20strategies%2C%20minimizing%20the%20risks%20of%20harm%20of%20LLMs%20in%0Amedicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03744v1&entry.124074799=Read"},
{"title": "Distribution-Free Statistical Dispersion Control for Societal\n  Applications", "author": "Zhun Deng and Thomas P. Zollo and Jake C. Snell and Toniann Pitassi and Richard Zemel", "abstract": "  Explicit finite-sample statistical guarantees on model performance are an\nimportant ingredient in responsible machine learning. Previous work has focused\nmainly on bounding either the expected loss of a predictor or the probability\nthat an individual prediction will incur a loss value in a specified range.\nHowever, for many high-stakes applications, it is crucial to understand and\ncontrol the dispersion of a loss distribution, or the extent to which different\nmembers of a population experience unequal effects of algorithmic decisions. We\ninitiate the study of distribution-free control of statistical dispersion\nmeasures with societal implications and propose a simple yet flexible framework\nthat allows us to handle a much richer class of statistical functionals beyond\nprevious work. Our methods are verified through experiments in toxic comment\ndetection, medical imaging, and film recommendation.\n", "link": "http://arxiv.org/abs/2309.13786v2", "date": "2024-03-06", "relevancy": 1.8831, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4917}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4562}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4548}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution-Free%20Statistical%20Dispersion%20Control%20for%20Societal%0A%20%20Applications&entry.906535625=Zhun%20Deng%20and%20Thomas%20P.%20Zollo%20and%20Jake%20C.%20Snell%20and%20Toniann%20Pitassi%20and%20Richard%20Zemel&entry.1292438233=%20%20Explicit%20finite-sample%20statistical%20guarantees%20on%20model%20performance%20are%20an%0Aimportant%20ingredient%20in%20responsible%20machine%20learning.%20Previous%20work%20has%20focused%0Amainly%20on%20bounding%20either%20the%20expected%20loss%20of%20a%20predictor%20or%20the%20probability%0Athat%20an%20individual%20prediction%20will%20incur%20a%20loss%20value%20in%20a%20specified%20range.%0AHowever%2C%20for%20many%20high-stakes%20applications%2C%20it%20is%20crucial%20to%20understand%20and%0Acontrol%20the%20dispersion%20of%20a%20loss%20distribution%2C%20or%20the%20extent%20to%20which%20different%0Amembers%20of%20a%20population%20experience%20unequal%20effects%20of%20algorithmic%20decisions.%20We%0Ainitiate%20the%20study%20of%20distribution-free%20control%20of%20statistical%20dispersion%0Ameasures%20with%20societal%20implications%20and%20propose%20a%20simple%20yet%20flexible%20framework%0Athat%20allows%20us%20to%20handle%20a%20much%20richer%20class%20of%20statistical%20functionals%20beyond%0Aprevious%20work.%20Our%20methods%20are%20verified%20through%20experiments%20in%20toxic%20comment%0Adetection%2C%20medical%20imaging%2C%20and%20film%20recommendation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.13786v2&entry.124074799=Read"},
{"title": "Robust Graph Structure Learning under Heterophily", "author": "Xuanting Xie and Zhao Kang and Wenyu Chen", "abstract": "  Graph is a fundamental mathematical structure in characterizing relations\nbetween different objects and has been widely used on various learning tasks.\nMost methods implicitly assume a given graph to be accurate and complete.\nHowever, real data is inevitably noisy and sparse, which will lead to inferior\nresults. Despite the remarkable success of recent graph representation learning\nmethods, they inherently presume that the graph is homophilic, and largely\noverlook heterophily, where most connected nodes are from different classes. In\nthis regard, we propose a novel robust graph structure learning method to\nachieve a high-quality graph from heterophilic data for downstream tasks. We\nfirst apply a high-pass filter to make each node more distinctive from its\nneighbors by encoding structure information into the node features. Then, we\nlearn a robust graph with an adaptive norm characterizing different levels of\nnoise. Afterwards, we propose a novel regularizer to further refine the graph\nstructure. Clustering and semi-supervised classification experiments on\nheterophilic graphs verify the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2403.03659v1", "date": "2024-03-06", "relevancy": 1.8809, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4802}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4683}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4501}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Graph%20Structure%20Learning%20under%20Heterophily&entry.906535625=Xuanting%20Xie%20and%20Zhao%20Kang%20and%20Wenyu%20Chen&entry.1292438233=%20%20Graph%20is%20a%20fundamental%20mathematical%20structure%20in%20characterizing%20relations%0Abetween%20different%20objects%20and%20has%20been%20widely%20used%20on%20various%20learning%20tasks.%0AMost%20methods%20implicitly%20assume%20a%20given%20graph%20to%20be%20accurate%20and%20complete.%0AHowever%2C%20real%20data%20is%20inevitably%20noisy%20and%20sparse%2C%20which%20will%20lead%20to%20inferior%0Aresults.%20Despite%20the%20remarkable%20success%20of%20recent%20graph%20representation%20learning%0Amethods%2C%20they%20inherently%20presume%20that%20the%20graph%20is%20homophilic%2C%20and%20largely%0Aoverlook%20heterophily%2C%20where%20most%20connected%20nodes%20are%20from%20different%20classes.%20In%0Athis%20regard%2C%20we%20propose%20a%20novel%20robust%20graph%20structure%20learning%20method%20to%0Aachieve%20a%20high-quality%20graph%20from%20heterophilic%20data%20for%20downstream%20tasks.%20We%0Afirst%20apply%20a%20high-pass%20filter%20to%20make%20each%20node%20more%20distinctive%20from%20its%0Aneighbors%20by%20encoding%20structure%20information%20into%20the%20node%20features.%20Then%2C%20we%0Alearn%20a%20robust%20graph%20with%20an%20adaptive%20norm%20characterizing%20different%20levels%20of%0Anoise.%20Afterwards%2C%20we%20propose%20a%20novel%20regularizer%20to%20further%20refine%20the%20graph%0Astructure.%20Clustering%20and%20semi-supervised%20classification%20experiments%20on%0Aheterophilic%20graphs%20verify%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03659v1&entry.124074799=Read"},
{"title": "Portraying the Need for Temporal Data in Flood Detection via Sentinel-1", "author": "Xavier Bou and Thibaud Ehret and Rafael Grompone von Gioi and Jeremy Anger", "abstract": "  Identifying flood affected areas in remote sensing data is a critical problem\nin earth observation to analyze flood impact and drive responses. While a\nnumber of methods have been proposed in the literature, there are two main\nlimitations in available flood detection datasets: (1) a lack of region\nvariability is commonly observed and/or (2) they require to distinguish\npermanent water bodies from flooded areas from a single image, which becomes an\nill-posed setup. Consequently, we extend the globally diverse MMFlood dataset\nto multi-date by providing one year of Sentinel-1 observations around each\nflood event. To our surprise, we notice that the definition of flooded pixels\nin MMFlood is inconsistent when observing the entire image sequence. Hence, we\nre-frame the flood detection task as a temporal anomaly detection problem,\nwhere anomalous water bodies are segmented from a Sentinel-1 temporal sequence.\nFrom this definition, we provide a simple method inspired by the popular video\nchange detector ViBe, results of which quantitatively align with the SAR image\ntime series, providing a reasonable baseline for future works.\n", "link": "http://arxiv.org/abs/2403.03671v1", "date": "2024-03-06", "relevancy": 1.8523, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4784}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4535}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4486}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Portraying%20the%20Need%20for%20Temporal%20Data%20in%20Flood%20Detection%20via%20Sentinel-1&entry.906535625=Xavier%20Bou%20and%20Thibaud%20Ehret%20and%20Rafael%20Grompone%20von%20Gioi%20and%20Jeremy%20Anger&entry.1292438233=%20%20Identifying%20flood%20affected%20areas%20in%20remote%20sensing%20data%20is%20a%20critical%20problem%0Ain%20earth%20observation%20to%20analyze%20flood%20impact%20and%20drive%20responses.%20While%20a%0Anumber%20of%20methods%20have%20been%20proposed%20in%20the%20literature%2C%20there%20are%20two%20main%0Alimitations%20in%20available%20flood%20detection%20datasets%3A%20%281%29%20a%20lack%20of%20region%0Avariability%20is%20commonly%20observed%20and/or%20%282%29%20they%20require%20to%20distinguish%0Apermanent%20water%20bodies%20from%20flooded%20areas%20from%20a%20single%20image%2C%20which%20becomes%20an%0Aill-posed%20setup.%20Consequently%2C%20we%20extend%20the%20globally%20diverse%20MMFlood%20dataset%0Ato%20multi-date%20by%20providing%20one%20year%20of%20Sentinel-1%20observations%20around%20each%0Aflood%20event.%20To%20our%20surprise%2C%20we%20notice%20that%20the%20definition%20of%20flooded%20pixels%0Ain%20MMFlood%20is%20inconsistent%20when%20observing%20the%20entire%20image%20sequence.%20Hence%2C%20we%0Are-frame%20the%20flood%20detection%20task%20as%20a%20temporal%20anomaly%20detection%20problem%2C%0Awhere%20anomalous%20water%20bodies%20are%20segmented%20from%20a%20Sentinel-1%20temporal%20sequence.%0AFrom%20this%20definition%2C%20we%20provide%20a%20simple%20method%20inspired%20by%20the%20popular%20video%0Achange%20detector%20ViBe%2C%20results%20of%20which%20quantitatively%20align%20with%20the%20SAR%20image%0Atime%20series%2C%20providing%20a%20reasonable%20baseline%20for%20future%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03671v1&entry.124074799=Read"},
{"title": "AI-Dentify: Deep learning for proximal caries detection on bitewing\n  x-ray -- HUNT4 Oral Health Study", "author": "Javier P\u00e9rez de Frutos and Ragnhild Holden Helland and Shreya Desai and Line Cathrine Nymoen and Thomas Lang\u00f8 and Theodor Remman and Abhijit Sen", "abstract": "  Background: Dental caries diagnosis requires the manual inspection of\ndiagnostic bitewing images of the patient, followed by a visual inspection and\nprobing of the identified dental pieces with potential lesions. Yet the use of\nartificial intelligence, and in particular deep-learning, has the potential to\naid in the diagnosis by providing a quick and informative analysis of the\nbitewing images.\n  Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were\nannotated individually by six different experts, and used to train three\ndifferent object detection deep-learning architectures: RetinaNet (ResNet50),\nYOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197\nimages, annotated jointly by the same six dentist, was used for evaluation. A\nfive-fold cross validation scheme was used to evaluate the performance of the\nAI models.\n  Results: he trained models show an increase in average precision and\nF1-score, and decrease of false negative rate, with respect to the dental\nclinicians. When compared against the dental clinicians, the YOLOv5 model shows\nthe largest improvement, reporting 0.647 mean average precision, 0.548 mean\nF1-score, and 0.149 mean false negative rate. Whereas the best annotators on\neach of these metrics reported 0.299, 0.495, and 0.164 respectively.\n  Conclusion: Deep-learning models have shown the potential to assist dental\nprofessionals in the diagnosis of caries. Yet, the task remains challenging due\nto the artifacts natural to the bitewing images.\n", "link": "http://arxiv.org/abs/2310.00354v2", "date": "2024-03-06", "relevancy": 1.8519, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5177}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4555}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4486}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Dentify%3A%20Deep%20learning%20for%20proximal%20caries%20detection%20on%20bitewing%0A%20%20x-ray%20--%20HUNT4%20Oral%20Health%20Study&entry.906535625=Javier%20P%C3%A9rez%20de%20Frutos%20and%20Ragnhild%20Holden%20Helland%20and%20Shreya%20Desai%20and%20Line%20Cathrine%20Nymoen%20and%20Thomas%20Lang%C3%B8%20and%20Theodor%20Remman%20and%20Abhijit%20Sen&entry.1292438233=%20%20Background%3A%20Dental%20caries%20diagnosis%20requires%20the%20manual%20inspection%20of%0Adiagnostic%20bitewing%20images%20of%20the%20patient%2C%20followed%20by%20a%20visual%20inspection%20and%0Aprobing%20of%20the%20identified%20dental%20pieces%20with%20potential%20lesions.%20Yet%20the%20use%20of%0Aartificial%20intelligence%2C%20and%20in%20particular%20deep-learning%2C%20has%20the%20potential%20to%0Aaid%20in%20the%20diagnosis%20by%20providing%20a%20quick%20and%20informative%20analysis%20of%20the%0Abitewing%20images.%0A%20%20Methods%3A%20A%20dataset%20of%2013%2C887%20bitewings%20from%20the%20HUNT4%20Oral%20Health%20Study%20were%0Aannotated%20individually%20by%20six%20different%20experts%2C%20and%20used%20to%20train%20three%0Adifferent%20object%20detection%20deep-learning%20architectures%3A%20RetinaNet%20%28ResNet50%29%2C%0AYOLOv5%20%28M%20size%29%2C%20and%20EfficientDet%20%28D0%20and%20D1%20sizes%29.%20A%20consensus%20dataset%20of%20197%0Aimages%2C%20annotated%20jointly%20by%20the%20same%20six%20dentist%2C%20was%20used%20for%20evaluation.%20A%0Afive-fold%20cross%20validation%20scheme%20was%20used%20to%20evaluate%20the%20performance%20of%20the%0AAI%20models.%0A%20%20Results%3A%20he%20trained%20models%20show%20an%20increase%20in%20average%20precision%20and%0AF1-score%2C%20and%20decrease%20of%20false%20negative%20rate%2C%20with%20respect%20to%20the%20dental%0Aclinicians.%20When%20compared%20against%20the%20dental%20clinicians%2C%20the%20YOLOv5%20model%20shows%0Athe%20largest%20improvement%2C%20reporting%200.647%20mean%20average%20precision%2C%200.548%20mean%0AF1-score%2C%20and%200.149%20mean%20false%20negative%20rate.%20Whereas%20the%20best%20annotators%20on%0Aeach%20of%20these%20metrics%20reported%200.299%2C%200.495%2C%20and%200.164%20respectively.%0A%20%20Conclusion%3A%20Deep-learning%20models%20have%20shown%20the%20potential%20to%20assist%20dental%0Aprofessionals%20in%20the%20diagnosis%20of%20caries.%20Yet%2C%20the%20task%20remains%20challenging%20due%0Ato%20the%20artifacts%20natural%20to%20the%20bitewing%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00354v2&entry.124074799=Read"},
{"title": "AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs", "author": "Victor Akinwande and J. Zico Kolter", "abstract": "  Existing causal discovery methods based on combinatorial optimization or\nsearch are slow, prohibiting their application on large-scale datasets. In\nresponse, more recent methods attempt to address this limitation by formulating\ncausal discovery as structure learning with continuous optimization but such\napproaches thus far provide no statistical guarantees. In this paper, we show\nthat by efficiently parallelizing existing causal discovery methods, we can in\nfact scale them to thousands of dimensions, making them practical for\nsubstantially larger-scale problems. In particular, we parallelize the LiNGAM\nmethod, which is quadratic in the number of variables, obtaining up to a\n32-fold speed-up on benchmark datasets when compared with existing sequential\nimplementations. Specifically, we focus on the causal ordering subprocedure in\nDirectLiNGAM and implement GPU kernels to accelerate it. This allows us to\napply DirectLiNGAM to causal inference on large-scale gene expression data with\ngenetic interventions yielding competitive results compared with specialized\ncontinuous optimization methods, and Var-LiNGAM for causal discovery on U.S.\nstock data.\n", "link": "http://arxiv.org/abs/2403.03772v1", "date": "2024-03-06", "relevancy": 1.8474, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4769}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4519}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4492}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AcceleratedLiNGAM%3A%20Learning%20Causal%20DAGs%20at%20the%20speed%20of%20GPUs&entry.906535625=Victor%20Akinwande%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Existing%20causal%20discovery%20methods%20based%20on%20combinatorial%20optimization%20or%0Asearch%20are%20slow%2C%20prohibiting%20their%20application%20on%20large-scale%20datasets.%20In%0Aresponse%2C%20more%20recent%20methods%20attempt%20to%20address%20this%20limitation%20by%20formulating%0Acausal%20discovery%20as%20structure%20learning%20with%20continuous%20optimization%20but%20such%0Aapproaches%20thus%20far%20provide%20no%20statistical%20guarantees.%20In%20this%20paper%2C%20we%20show%0Athat%20by%20efficiently%20parallelizing%20existing%20causal%20discovery%20methods%2C%20we%20can%20in%0Afact%20scale%20them%20to%20thousands%20of%20dimensions%2C%20making%20them%20practical%20for%0Asubstantially%20larger-scale%20problems.%20In%20particular%2C%20we%20parallelize%20the%20LiNGAM%0Amethod%2C%20which%20is%20quadratic%20in%20the%20number%20of%20variables%2C%20obtaining%20up%20to%20a%0A32-fold%20speed-up%20on%20benchmark%20datasets%20when%20compared%20with%20existing%20sequential%0Aimplementations.%20Specifically%2C%20we%20focus%20on%20the%20causal%20ordering%20subprocedure%20in%0ADirectLiNGAM%20and%20implement%20GPU%20kernels%20to%20accelerate%20it.%20This%20allows%20us%20to%0Aapply%20DirectLiNGAM%20to%20causal%20inference%20on%20large-scale%20gene%20expression%20data%20with%0Agenetic%20interventions%20yielding%20competitive%20results%20compared%20with%20specialized%0Acontinuous%20optimization%20methods%2C%20and%20Var-LiNGAM%20for%20causal%20discovery%20on%20U.S.%0Astock%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03772v1&entry.124074799=Read"},
{"title": "Conformal prediction for multi-dimensional time series by ellipsoidal\n  sets", "author": "Chen Xu and Hanyang Jiang and Yao Xie", "abstract": "  Conformal prediction (CP) has been a popular method for uncertainty\nquantification because it is distribution-free, model-agnostic, and\ntheoretically sound. For forecasting problems in supervised learning, most CP\nmethods focus on building prediction intervals for univariate responses. In\nthis work, we develop a sequential CP method called $\\texttt{MultiDimSPCI}$\nthat builds prediction regions for a multivariate response, especially in the\ncontext of multivariate time series, which are not exchangeable. Theoretically,\nwe estimate finite-sample high-probability bounds on the conditional coverage\ngap. Empirically, we demonstrate that $\\texttt{MultiDimSPCI}$ maintains valid\ncoverage on a wide range of multivariate time series while producing smaller\nprediction regions than CP and non-CP baselines.\n", "link": "http://arxiv.org/abs/2403.03850v1", "date": "2024-03-06", "relevancy": 1.8471, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5032}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4621}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4449}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20prediction%20for%20multi-dimensional%20time%20series%20by%20ellipsoidal%0A%20%20sets&entry.906535625=Chen%20Xu%20and%20Hanyang%20Jiang%20and%20Yao%20Xie&entry.1292438233=%20%20Conformal%20prediction%20%28CP%29%20has%20been%20a%20popular%20method%20for%20uncertainty%0Aquantification%20because%20it%20is%20distribution-free%2C%20model-agnostic%2C%20and%0Atheoretically%20sound.%20For%20forecasting%20problems%20in%20supervised%20learning%2C%20most%20CP%0Amethods%20focus%20on%20building%20prediction%20intervals%20for%20univariate%20responses.%20In%0Athis%20work%2C%20we%20develop%20a%20sequential%20CP%20method%20called%20%24%5Ctexttt%7BMultiDimSPCI%7D%24%0Athat%20builds%20prediction%20regions%20for%20a%20multivariate%20response%2C%20especially%20in%20the%0Acontext%20of%20multivariate%20time%20series%2C%20which%20are%20not%20exchangeable.%20Theoretically%2C%0Awe%20estimate%20finite-sample%20high-probability%20bounds%20on%20the%20conditional%20coverage%0Agap.%20Empirically%2C%20we%20demonstrate%20that%20%24%5Ctexttt%7BMultiDimSPCI%7D%24%20maintains%20valid%0Acoverage%20on%20a%20wide%20range%20of%20multivariate%20time%20series%20while%20producing%20smaller%0Aprediction%20regions%20than%20CP%20and%20non-CP%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03850v1&entry.124074799=Read"},
{"title": "Verification of Neural Networks' Global Robustness", "author": "Anan Kabaha and Dana Drachsler-Cohen", "abstract": "  Neural networks are successful in various applications but are also\nsusceptible to adversarial attacks. To show the safety of network classifiers,\nmany verifiers have been introduced to reason about the local robustness of a\ngiven input to a given perturbation. While successful, local robustness cannot\ngeneralize to unseen inputs. Several works analyze global robustness\nproperties, however, neither can provide a precise guarantee about the cases\nwhere a network classifier does not change its classification. In this work, we\npropose a new global robustness property for classifiers aiming at finding the\nminimal globally robust bound, which naturally extends the popular local\nrobustness property for classifiers. We introduce VHAGaR, an anytime verifier\nfor computing this bound. VHAGaR relies on three main ideas: encoding the\nproblem as a mixed-integer programming and pruning the search space by\nidentifying dependencies stemming from the perturbation or the network's\ncomputation and generalizing adversarial attacks to unknown inputs. We evaluate\nVHAGaR on several datasets and classifiers and show that, given a three hour\ntimeout, the average gap between the lower and upper bound on the minimal\nglobally robust bound computed by VHAGaR is 1.9, while the gap of an existing\nglobal robustness verifier is 154.7. Moreover, VHAGaR is 130.6x faster than\nthis verifier. Our results further indicate that leveraging dependencies and\nadversarial attacks makes VHAGaR 78.6x faster.\n", "link": "http://arxiv.org/abs/2402.19322v2", "date": "2024-03-06", "relevancy": 1.8301, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4629}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4596}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4388}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verification%20of%20Neural%20Networks%27%20Global%20Robustness&entry.906535625=Anan%20Kabaha%20and%20Dana%20Drachsler-Cohen&entry.1292438233=%20%20Neural%20networks%20are%20successful%20in%20various%20applications%20but%20are%20also%0Asusceptible%20to%20adversarial%20attacks.%20To%20show%20the%20safety%20of%20network%20classifiers%2C%0Amany%20verifiers%20have%20been%20introduced%20to%20reason%20about%20the%20local%20robustness%20of%20a%0Agiven%20input%20to%20a%20given%20perturbation.%20While%20successful%2C%20local%20robustness%20cannot%0Ageneralize%20to%20unseen%20inputs.%20Several%20works%20analyze%20global%20robustness%0Aproperties%2C%20however%2C%20neither%20can%20provide%20a%20precise%20guarantee%20about%20the%20cases%0Awhere%20a%20network%20classifier%20does%20not%20change%20its%20classification.%20In%20this%20work%2C%20we%0Apropose%20a%20new%20global%20robustness%20property%20for%20classifiers%20aiming%20at%20finding%20the%0Aminimal%20globally%20robust%20bound%2C%20which%20naturally%20extends%20the%20popular%20local%0Arobustness%20property%20for%20classifiers.%20We%20introduce%20VHAGaR%2C%20an%20anytime%20verifier%0Afor%20computing%20this%20bound.%20VHAGaR%20relies%20on%20three%20main%20ideas%3A%20encoding%20the%0Aproblem%20as%20a%20mixed-integer%20programming%20and%20pruning%20the%20search%20space%20by%0Aidentifying%20dependencies%20stemming%20from%20the%20perturbation%20or%20the%20network%27s%0Acomputation%20and%20generalizing%20adversarial%20attacks%20to%20unknown%20inputs.%20We%20evaluate%0AVHAGaR%20on%20several%20datasets%20and%20classifiers%20and%20show%20that%2C%20given%20a%20three%20hour%0Atimeout%2C%20the%20average%20gap%20between%20the%20lower%20and%20upper%20bound%20on%20the%20minimal%0Aglobally%20robust%20bound%20computed%20by%20VHAGaR%20is%201.9%2C%20while%20the%20gap%20of%20an%20existing%0Aglobal%20robustness%20verifier%20is%20154.7.%20Moreover%2C%20VHAGaR%20is%20130.6x%20faster%20than%0Athis%20verifier.%20Our%20results%20further%20indicate%20that%20leveraging%20dependencies%20and%0Aadversarial%20attacks%20makes%20VHAGaR%2078.6x%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19322v2&entry.124074799=Read"},
{"title": "Apollo: Lightweight Multilingual Medical LLMs towards Democratizing\n  Medical AI to 6B People", "author": "Xidong Wang and Nuo Chen and Junyin Chen and Yan Hu and Yidong Wang and Xiangbo Wu and Anningzhe Gao and Xiang Wan and Haizhou Li and Benyou Wang", "abstract": "  Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.\n", "link": "http://arxiv.org/abs/2403.03640v1", "date": "2024-03-06", "relevancy": 1.8225, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4663}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4315}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Apollo%3A%20Lightweight%20Multilingual%20Medical%20LLMs%20towards%20Democratizing%0A%20%20Medical%20AI%20to%206B%20People&entry.906535625=Xidong%20Wang%20and%20Nuo%20Chen%20and%20Junyin%20Chen%20and%20Yan%20Hu%20and%20Yidong%20Wang%20and%20Xiangbo%20Wu%20and%20Anningzhe%20Gao%20and%20Xiang%20Wan%20and%20Haizhou%20Li%20and%20Benyou%20Wang&entry.1292438233=%20%20Despite%20the%20vast%20repository%20of%20global%20medical%20knowledge%20predominantly%20being%0Ain%20English%2C%20local%20languages%20are%20crucial%20for%20delivering%20tailored%20healthcare%0Aservices%2C%20particularly%20in%20areas%20with%20limited%20medical%20resources.%20To%20extend%20the%0Areach%20of%20medical%20AI%20advancements%20to%20a%20broader%20population%2C%20we%20aim%20to%20develop%0Amedical%20LLMs%20across%20the%20six%20most%20widely%20spoken%20languages%2C%20encompassing%20a%20global%0Apopulation%20of%206.1%20billion.%20This%20effort%20culminates%20in%20the%20creation%20of%20the%0AApolloCorpora%20multilingual%20medical%20dataset%20and%20the%20XMedBench%20benchmark.%20In%20the%0Amultilingual%20medical%20benchmark%2C%20the%20released%20Apollo%20models%2C%20at%20various%0Arelatively-small%20sizes%20%28i.e.%2C%200.5B%2C%201.8B%2C%202B%2C%206B%2C%20and%207B%29%2C%20achieve%20the%20best%0Aperformance%20among%20models%20of%20equivalent%20size.%20Especially%2C%20Apollo-7B%20is%20the%0Astate-of-the-art%20multilingual%20medical%20LLMs%20up%20to%2070B.%20Additionally%2C%20these%20lite%0Amodels%20could%20be%20used%20to%20improve%20the%20multi-lingual%20medical%20capabilities%20of%0Alarger%20models%20without%20fine-tuning%20in%20a%20proxy-tuning%20fashion.%20We%20will%0Aopen-source%20training%20corpora%2C%20code%2C%20model%20weights%20and%20evaluation%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03640v1&entry.124074799=Read"},
{"title": "Graph neural network outputs are almost surely asymptotically constant", "author": "Sam Adam-Day and Michael Benedikt and \u0130smail \u0130lkan Ceylan and Ben Finkelshtein", "abstract": "  Graph neural networks (GNNs) are the predominant architectures for a variety\nof learning tasks on graphs. We present a new angle on the expressive power of\nGNNs by studying how the predictions of a GNN probabilistic classifier evolve\nas we apply it on larger graphs drawn from some random graph model. We show\nthat the output converges to a constant function, which upper-bounds what these\nclassifiers can express uniformly. This convergence phenomenon applies to a\nvery wide class of GNNs, including state of the art models, with aggregates\nincluding mean and the attention-based mechanism of graph transformers. Our\nresults apply to a broad class of random graph models, including the (sparse)\nErd\\H{o}s-R\\'enyi model and the stochastic block model. We empirically validate\nthese findings, observing that the convergence phenomenon already manifests\nitself on graphs of relatively modest size.\n", "link": "http://arxiv.org/abs/2403.03880v1", "date": "2024-03-06", "relevancy": 1.8218, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.478}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4559}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4327}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20neural%20network%20outputs%20are%20almost%20surely%20asymptotically%20constant&entry.906535625=Sam%20Adam-Day%20and%20Michael%20Benedikt%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Ben%20Finkelshtein&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20the%20predominant%20architectures%20for%20a%20variety%0Aof%20learning%20tasks%20on%20graphs.%20We%20present%20a%20new%20angle%20on%20the%20expressive%20power%20of%0AGNNs%20by%20studying%20how%20the%20predictions%20of%20a%20GNN%20probabilistic%20classifier%20evolve%0Aas%20we%20apply%20it%20on%20larger%20graphs%20drawn%20from%20some%20random%20graph%20model.%20We%20show%0Athat%20the%20output%20converges%20to%20a%20constant%20function%2C%20which%20upper-bounds%20what%20these%0Aclassifiers%20can%20express%20uniformly.%20This%20convergence%20phenomenon%20applies%20to%20a%0Avery%20wide%20class%20of%20GNNs%2C%20including%20state%20of%20the%20art%20models%2C%20with%20aggregates%0Aincluding%20mean%20and%20the%20attention-based%20mechanism%20of%20graph%20transformers.%20Our%0Aresults%20apply%20to%20a%20broad%20class%20of%20random%20graph%20models%2C%20including%20the%20%28sparse%29%0AErd%5CH%7Bo%7Ds-R%5C%27enyi%20model%20and%20the%20stochastic%20block%20model.%20We%20empirically%20validate%0Athese%20findings%2C%20observing%20that%20the%20convergence%20phenomenon%20already%20manifests%0Aitself%20on%20graphs%20of%20relatively%20modest%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03880v1&entry.124074799=Read"},
{"title": "Active Adaptive Experimental Design for Treatment Effect Estimation with\n  Covariate Choices", "author": "Masahiro Kato and Akihiro Oga and Wataru Komatsubara and Ryo Inokuchi", "abstract": "  This study designs an adaptive experiment for efficiently estimating average\ntreatment effect (ATEs). We consider an adaptive experiment where an\nexperimenter sequentially samples an experimental unit from a covariate density\ndecided by the experimenter and assigns a treatment. After assigning a\ntreatment, the experimenter observes the corresponding outcome immediately. At\nthe end of the experiment, the experimenter estimates an ATE using gathered\nsamples. The objective of the experimenter is to estimate the ATE with a\nsmaller asymptotic variance. Existing studies have designed experiments that\nadaptively optimize the propensity score (treatment-assignment probability). As\na generalization of such an approach, we propose a framework under which an\nexperimenter optimizes the covariate density, as well as the propensity score,\nand find that optimizing both covariate density and propensity score reduces\nthe asymptotic variance more than optimizing only the propensity score. Based\non this idea, in each round of our experiment, the experimenter optimizes the\ncovariate density and propensity score based on past observations. To design an\nadaptive experiment, we first derive the efficient covariate density and\npropensity score that minimizes the semiparametric efficiency bound, a lower\nbound for the asymptotic variance given a fixed covariate density and a fixed\npropensity score. Next, we design an adaptive experiment using the efficient\ncovariate density and propensity score sequentially estimated during the\nexperiment. Lastly, we propose an ATE estimator whose asymptotic variance\naligns with the minimized semiparametric efficiency bound.\n", "link": "http://arxiv.org/abs/2403.03589v1", "date": "2024-03-06", "relevancy": 1.2196, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4104}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4058}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.3975}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Adaptive%20Experimental%20Design%20for%20Treatment%20Effect%20Estimation%20with%0A%20%20Covariate%20Choices&entry.906535625=Masahiro%20Kato%20and%20Akihiro%20Oga%20and%20Wataru%20Komatsubara%20and%20Ryo%20Inokuchi&entry.1292438233=%20%20This%20study%20designs%20an%20adaptive%20experiment%20for%20efficiently%20estimating%20average%0Atreatment%20effect%20%28ATEs%29.%20We%20consider%20an%20adaptive%20experiment%20where%20an%0Aexperimenter%20sequentially%20samples%20an%20experimental%20unit%20from%20a%20covariate%20density%0Adecided%20by%20the%20experimenter%20and%20assigns%20a%20treatment.%20After%20assigning%20a%0Atreatment%2C%20the%20experimenter%20observes%20the%20corresponding%20outcome%20immediately.%20At%0Athe%20end%20of%20the%20experiment%2C%20the%20experimenter%20estimates%20an%20ATE%20using%20gathered%0Asamples.%20The%20objective%20of%20the%20experimenter%20is%20to%20estimate%20the%20ATE%20with%20a%0Asmaller%20asymptotic%20variance.%20Existing%20studies%20have%20designed%20experiments%20that%0Aadaptively%20optimize%20the%20propensity%20score%20%28treatment-assignment%20probability%29.%20As%0Aa%20generalization%20of%20such%20an%20approach%2C%20we%20propose%20a%20framework%20under%20which%20an%0Aexperimenter%20optimizes%20the%20covariate%20density%2C%20as%20well%20as%20the%20propensity%20score%2C%0Aand%20find%20that%20optimizing%20both%20covariate%20density%20and%20propensity%20score%20reduces%0Athe%20asymptotic%20variance%20more%20than%20optimizing%20only%20the%20propensity%20score.%20Based%0Aon%20this%20idea%2C%20in%20each%20round%20of%20our%20experiment%2C%20the%20experimenter%20optimizes%20the%0Acovariate%20density%20and%20propensity%20score%20based%20on%20past%20observations.%20To%20design%20an%0Aadaptive%20experiment%2C%20we%20first%20derive%20the%20efficient%20covariate%20density%20and%0Apropensity%20score%20that%20minimizes%20the%20semiparametric%20efficiency%20bound%2C%20a%20lower%0Abound%20for%20the%20asymptotic%20variance%20given%20a%20fixed%20covariate%20density%20and%20a%20fixed%0Apropensity%20score.%20Next%2C%20we%20design%20an%20adaptive%20experiment%20using%20the%20efficient%0Acovariate%20density%20and%20propensity%20score%20sequentially%20estimated%20during%20the%0Aexperiment.%20Lastly%2C%20we%20propose%20an%20ATE%20estimator%20whose%20asymptotic%20variance%0Aaligns%20with%20the%20minimized%20semiparametric%20efficiency%20bound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03589v1&entry.124074799=Read"},
{"title": "RouteExplainer: An Explanation Framework for Vehicle Routing Problem", "author": "Daisuke Kikuta and Hiroki Ikeuchi and Kengo Tajiri and Yuusuke Nakano", "abstract": "  The Vehicle Routing Problem (VRP) is a widely studied combinatorial\noptimization problem and has been applied to various practical problems. While\nthe explainability for VRP is significant for improving the reliability and\ninteractivity in practical VRP applications, it remains unexplored. In this\npaper, we propose RouteExplainer, a post-hoc explanation framework that\nexplains the influence of each edge in a generated route. Our framework\nrealizes this by rethinking a route as the sequence of actions and extending\ncounterfactual explanations based on the action influence model to VRP. To\nenhance the explanation, we additionally propose an edge classifier that infers\nthe intentions of each edge, a loss function to train the edge classifier, and\nexplanation-text generation by Large Language Models (LLMs). We quantitatively\nevaluate our edge classifier on four different VRPs. The results demonstrate\nits rapid computation while maintaining reasonable accuracy, thereby\nhighlighting its potential for deployment in practical applications. Moreover,\non the subject of a tourist route, we qualitatively evaluate explanations\ngenerated by our framework. This evaluation not only validates our framework\nbut also shows the synergy between explanation frameworks and LLMs. See\nhttps://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo.\n", "link": "http://arxiv.org/abs/2403.03585v1", "date": "2024-03-06", "relevancy": 1.3802, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5297}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4587}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4327}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RouteExplainer%3A%20An%20Explanation%20Framework%20for%20Vehicle%20Routing%20Problem&entry.906535625=Daisuke%20Kikuta%20and%20Hiroki%20Ikeuchi%20and%20Kengo%20Tajiri%20and%20Yuusuke%20Nakano&entry.1292438233=%20%20The%20Vehicle%20Routing%20Problem%20%28VRP%29%20is%20a%20widely%20studied%20combinatorial%0Aoptimization%20problem%20and%20has%20been%20applied%20to%20various%20practical%20problems.%20While%0Athe%20explainability%20for%20VRP%20is%20significant%20for%20improving%20the%20reliability%20and%0Ainteractivity%20in%20practical%20VRP%20applications%2C%20it%20remains%20unexplored.%20In%20this%0Apaper%2C%20we%20propose%20RouteExplainer%2C%20a%20post-hoc%20explanation%20framework%20that%0Aexplains%20the%20influence%20of%20each%20edge%20in%20a%20generated%20route.%20Our%20framework%0Arealizes%20this%20by%20rethinking%20a%20route%20as%20the%20sequence%20of%20actions%20and%20extending%0Acounterfactual%20explanations%20based%20on%20the%20action%20influence%20model%20to%20VRP.%20To%0Aenhance%20the%20explanation%2C%20we%20additionally%20propose%20an%20edge%20classifier%20that%20infers%0Athe%20intentions%20of%20each%20edge%2C%20a%20loss%20function%20to%20train%20the%20edge%20classifier%2C%20and%0Aexplanation-text%20generation%20by%20Large%20Language%20Models%20%28LLMs%29.%20We%20quantitatively%0Aevaluate%20our%20edge%20classifier%20on%20four%20different%20VRPs.%20The%20results%20demonstrate%0Aits%20rapid%20computation%20while%20maintaining%20reasonable%20accuracy%2C%20thereby%0Ahighlighting%20its%20potential%20for%20deployment%20in%20practical%20applications.%20Moreover%2C%0Aon%20the%20subject%20of%20a%20tourist%20route%2C%20we%20qualitatively%20evaluate%20explanations%0Agenerated%20by%20our%20framework.%20This%20evaluation%20not%20only%20validates%20our%20framework%0Abut%20also%20shows%20the%20synergy%20between%20explanation%20frameworks%20and%20LLMs.%20See%0Ahttps%3A//ntt-dkiku.github.io/xai-vrp%20for%20our%20code%2C%20datasets%2C%20models%2C%20and%20demo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03585v1&entry.124074799=Read"},
{"title": "Enhancing ASD detection accuracy: a combined approach of machine\n  learning and deep learning models with natural language processing", "author": "Sergio Rubio-Mart\u00edn and Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s and Mart\u00edn Bay\u00f3n-Guti\u00e9rrez and Natalia Prieto-Fern\u00e1ndez and Jos\u00e9 Alberto Ben\u00edtez-Andrades", "abstract": "  Purpose: Our study explored the use of artificial intelligence (AI) to\ndiagnose autism spectrum disorder (ASD). It focused on machine learning (ML)\nand deep learning (DL) to detect ASD from text inputs on social media,\naddressing challenges in traditional ASD diagnosis.\n  Methods: We used natural language processing (NLP), ML, and DL models\n(including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to\nanalyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A\nsubset of 90,000 tweets was used for model training and testing.\n  Results: Our AI models showed high accuracy, with an 88% success rate in\nidentifying texts from individuals with ASD.\n  Conclusion: The study demonstrates AI's potential in improving ASD diagnosis,\nespecially in children, highlighting the importance of early detection.\n", "link": "http://arxiv.org/abs/2403.03581v1", "date": "2024-03-06", "relevancy": 1.2888, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4369}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4294}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4227}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20ASD%20detection%20accuracy%3A%20a%20combined%20approach%20of%20machine%0A%20%20learning%20and%20deep%20learning%20models%20with%20natural%20language%20processing&entry.906535625=Sergio%20Rubio-Mart%C3%ADn%20and%20Mar%C3%ADa%20Teresa%20Garc%C3%ADa-Ord%C3%A1s%20and%20Mart%C3%ADn%20Bay%C3%B3n-Guti%C3%A9rrez%20and%20Natalia%20Prieto-Fern%C3%A1ndez%20and%20Jos%C3%A9%20Alberto%20Ben%C3%ADtez-Andrades&entry.1292438233=%20%20Purpose%3A%20Our%20study%20explored%20the%20use%20of%20artificial%20intelligence%20%28AI%29%20to%0Adiagnose%20autism%20spectrum%20disorder%20%28ASD%29.%20It%20focused%20on%20machine%20learning%20%28ML%29%0Aand%20deep%20learning%20%28DL%29%20to%20detect%20ASD%20from%20text%20inputs%20on%20social%20media%2C%0Aaddressing%20challenges%20in%20traditional%20ASD%20diagnosis.%0A%20%20Methods%3A%20We%20used%20natural%20language%20processing%20%28NLP%29%2C%20ML%2C%20and%20DL%20models%0A%28including%20decision%20trees%2C%20XGB%2C%20KNN%2C%20RNN%2C%20LSTM%2C%20Bi-LSTM%2C%20BERT%2C%20and%20BERTweet%29%20to%0Aanalyze%20404%2C627%20tweets%2C%20classifying%20them%20based%20on%20ASD%20or%20non-ASD%20authors.%20A%0Asubset%20of%2090%2C000%20tweets%20was%20used%20for%20model%20training%20and%20testing.%0A%20%20Results%3A%20Our%20AI%20models%20showed%20high%20accuracy%2C%20with%20an%2088%25%20success%20rate%20in%0Aidentifying%20texts%20from%20individuals%20with%20ASD.%0A%20%20Conclusion%3A%20The%20study%20demonstrates%20AI%27s%20potential%20in%20improving%20ASD%20diagnosis%2C%0Aespecially%20in%20children%2C%20highlighting%20the%20importance%20of%20early%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03581v1&entry.124074799=Read"},
{"title": "Parameterized quantum comb and simpler circuits for reversing unknown\n  qubit-unitary operations", "author": "Yin Mo and Lei Zhang and Yu-Ao Chen and Yingjian Liu and Tengxiang Lin and Xin Wang", "abstract": "  Quantum comb is an essential tool for characterizing complex quantum\nprotocols in quantum information processing. In this work, we introduce PQComb,\na framework leveraging parameterized quantum circuits to explore the\ncapabilities of quantum combs for general quantum process transformation tasks\nand beyond. By optimizing PQComb for time-reversal simulations of unknown\nunitary evolutions, we develop a simpler protocol for unknown qubit unitary\ninversion that reduces the ancilla qubit overhead from 6 to 3 compared to the\nexisting method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023]. This\ndemonstrates the utility of quantum comb structures and showcases PQComb's\npotential for solving complex quantum tasks. Our results pave the way for\nbroader PQComb applications in quantum computing and quantum information,\nemphasizing its versatility for tackling diverse problems in quantum machine\nlearning.\n", "link": "http://arxiv.org/abs/2403.03761v1", "date": "2024-03-06", "relevancy": 1.1286, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3852}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3765}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3664}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameterized%20quantum%20comb%20and%20simpler%20circuits%20for%20reversing%20unknown%0A%20%20qubit-unitary%20operations&entry.906535625=Yin%20Mo%20and%20Lei%20Zhang%20and%20Yu-Ao%20Chen%20and%20Yingjian%20Liu%20and%20Tengxiang%20Lin%20and%20Xin%20Wang&entry.1292438233=%20%20Quantum%20comb%20is%20an%20essential%20tool%20for%20characterizing%20complex%20quantum%0Aprotocols%20in%20quantum%20information%20processing.%20In%20this%20work%2C%20we%20introduce%20PQComb%2C%0Aa%20framework%20leveraging%20parameterized%20quantum%20circuits%20to%20explore%20the%0Acapabilities%20of%20quantum%20combs%20for%20general%20quantum%20process%20transformation%20tasks%0Aand%20beyond.%20By%20optimizing%20PQComb%20for%20time-reversal%20simulations%20of%20unknown%0Aunitary%20evolutions%2C%20we%20develop%20a%20simpler%20protocol%20for%20unknown%20qubit%20unitary%0Ainversion%20that%20reduces%20the%20ancilla%20qubit%20overhead%20from%206%20to%203%20compared%20to%20the%0Aexisting%20method%20in%20%5BYoshida%2C%20Soeda%2C%20Murao%2C%20PRL%20131%2C%20120602%2C%202023%5D.%20This%0Ademonstrates%20the%20utility%20of%20quantum%20comb%20structures%20and%20showcases%20PQComb%27s%0Apotential%20for%20solving%20complex%20quantum%20tasks.%20Our%20results%20pave%20the%20way%20for%0Abroader%20PQComb%20applications%20in%20quantum%20computing%20and%20quantum%20information%2C%0Aemphasizing%20its%20versatility%20for%20tackling%20diverse%20problems%20in%20quantum%20machine%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03761v1&entry.124074799=Read"},
{"title": "DeepEclipse: How to Break White-Box DNN-Watermarking Schemes", "author": "Alessandro Pegoraro and Carlotta Segna and Kavita Kumari and Ahmad-Reza Sadeghi", "abstract": "  Deep Learning (DL) models have become crucial in digital transformation, thus\nraising concerns about their intellectual property rights. Different\nwatermarking techniques have been developed to protect Deep Neural Networks\n(DNNs) from IP infringement, creating a competitive field for DNN watermarking\nand removal methods. The predominant watermarking schemes use white-box\ntechniques, which involve modifying weights by adding a unique signature to\nspecific DNN layers. On the other hand, existing attacks on white-box\nwatermarking usually require knowledge of the specific deployed watermarking\nscheme or access to the underlying data for further training and fine-tuning.\nWe propose DeepEclipse, a novel and unified framework designed to remove\nwhite-box watermarks. We present obfuscation techniques that significantly\ndiffer from the existing white-box watermarking removal schemes. DeepEclipse\ncan evade watermark detection without prior knowledge of the underlying\nwatermarking scheme, additional data, or training and fine-tuning. Our\nevaluation reveals that DeepEclipse excels in breaking multiple white-box\nwatermarking schemes, reducing watermark detection to random guessing while\nmaintaining a similar model accuracy as the original one. Our framework\nshowcases a promising solution to address the ongoing DNN watermark protection\nand removal challenges.\n", "link": "http://arxiv.org/abs/2403.03590v1", "date": "2024-03-06", "relevancy": 1.8148, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4668}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4513}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4509}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepEclipse%3A%20How%20to%20Break%20White-Box%20DNN-Watermarking%20Schemes&entry.906535625=Alessandro%20Pegoraro%20and%20Carlotta%20Segna%20and%20Kavita%20Kumari%20and%20Ahmad-Reza%20Sadeghi&entry.1292438233=%20%20Deep%20Learning%20%28DL%29%20models%20have%20become%20crucial%20in%20digital%20transformation%2C%20thus%0Araising%20concerns%20about%20their%20intellectual%20property%20rights.%20Different%0Awatermarking%20techniques%20have%20been%20developed%20to%20protect%20Deep%20Neural%20Networks%0A%28DNNs%29%20from%20IP%20infringement%2C%20creating%20a%20competitive%20field%20for%20DNN%20watermarking%0Aand%20removal%20methods.%20The%20predominant%20watermarking%20schemes%20use%20white-box%0Atechniques%2C%20which%20involve%20modifying%20weights%20by%20adding%20a%20unique%20signature%20to%0Aspecific%20DNN%20layers.%20On%20the%20other%20hand%2C%20existing%20attacks%20on%20white-box%0Awatermarking%20usually%20require%20knowledge%20of%20the%20specific%20deployed%20watermarking%0Ascheme%20or%20access%20to%20the%20underlying%20data%20for%20further%20training%20and%20fine-tuning.%0AWe%20propose%20DeepEclipse%2C%20a%20novel%20and%20unified%20framework%20designed%20to%20remove%0Awhite-box%20watermarks.%20We%20present%20obfuscation%20techniques%20that%20significantly%0Adiffer%20from%20the%20existing%20white-box%20watermarking%20removal%20schemes.%20DeepEclipse%0Acan%20evade%20watermark%20detection%20without%20prior%20knowledge%20of%20the%20underlying%0Awatermarking%20scheme%2C%20additional%20data%2C%20or%20training%20and%20fine-tuning.%20Our%0Aevaluation%20reveals%20that%20DeepEclipse%20excels%20in%20breaking%20multiple%20white-box%0Awatermarking%20schemes%2C%20reducing%20watermark%20detection%20to%20random%20guessing%20while%0Amaintaining%20a%20similar%20model%20accuracy%20as%20the%20original%20one.%20Our%20framework%0Ashowcases%20a%20promising%20solution%20to%20address%20the%20ongoing%20DNN%20watermark%20protection%0Aand%20removal%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03590v1&entry.124074799=Read"},
{"title": "Stop Regressing: Training Value Functions via Classification for\n  Scalable Deep RL", "author": "Jesse Farebrother and Jordi Orbay and Quan Vuong and Adrien Ali Ta\u00efga and Yevgen Chebotar and Ted Xiao and Alex Irpan and Sergey Levine and Pablo Samuel Castro and Aleksandra Faust and Aviral Kumar and Rishabh Agarwal", "abstract": "  Value functions are a central component of deep reinforcement learning (RL).\nThese functions, parameterized by neural networks, are trained using a mean\nsquared error regression objective to match bootstrapped target values.\nHowever, scaling value-based RL methods that use regression to large networks,\nsuch as high-capacity Transformers, has proven challenging. This difficulty is\nin stark contrast to supervised learning: by leveraging a cross-entropy\nclassification loss, supervised methods have scaled reliably to massive\nnetworks. Observing this discrepancy, in this paper, we investigate whether the\nscalability of deep RL can also be improved simply by using classification in\nplace of regression for training value functions. We demonstrate that value\nfunctions trained with categorical cross-entropy significantly improves\nperformance and scalability in a variety of domains. These include: single-task\nRL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale\nResNets, robotic manipulation with Q-transformers, playing Chess without\nsearch, and a language-agent Wordle task with high-capacity Transformers,\nachieving state-of-the-art results on these domains. Through careful analysis,\nwe show that the benefits of categorical cross-entropy primarily stem from its\nability to mitigate issues inherent to value-based RL, such as noisy targets\nand non-stationarity. Overall, we argue that a simple shift to training value\nfunctions with categorical cross-entropy can yield substantial improvements in\nthe scalability of deep RL at little-to-no cost.\n", "link": "http://arxiv.org/abs/2403.03950v1", "date": "2024-03-06", "relevancy": 1.4903, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5329}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4955}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4828}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stop%20Regressing%3A%20Training%20Value%20Functions%20via%20Classification%20for%0A%20%20Scalable%20Deep%20RL&entry.906535625=Jesse%20Farebrother%20and%20Jordi%20Orbay%20and%20Quan%20Vuong%20and%20Adrien%20Ali%20Ta%C3%AFga%20and%20Yevgen%20Chebotar%20and%20Ted%20Xiao%20and%20Alex%20Irpan%20and%20Sergey%20Levine%20and%20Pablo%20Samuel%20Castro%20and%20Aleksandra%20Faust%20and%20Aviral%20Kumar%20and%20Rishabh%20Agarwal&entry.1292438233=%20%20Value%20functions%20are%20a%20central%20component%20of%20deep%20reinforcement%20learning%20%28RL%29.%0AThese%20functions%2C%20parameterized%20by%20neural%20networks%2C%20are%20trained%20using%20a%20mean%0Asquared%20error%20regression%20objective%20to%20match%20bootstrapped%20target%20values.%0AHowever%2C%20scaling%20value-based%20RL%20methods%20that%20use%20regression%20to%20large%20networks%2C%0Asuch%20as%20high-capacity%20Transformers%2C%20has%20proven%20challenging.%20This%20difficulty%20is%0Ain%20stark%20contrast%20to%20supervised%20learning%3A%20by%20leveraging%20a%20cross-entropy%0Aclassification%20loss%2C%20supervised%20methods%20have%20scaled%20reliably%20to%20massive%0Anetworks.%20Observing%20this%20discrepancy%2C%20in%20this%20paper%2C%20we%20investigate%20whether%20the%0Ascalability%20of%20deep%20RL%20can%20also%20be%20improved%20simply%20by%20using%20classification%20in%0Aplace%20of%20regression%20for%20training%20value%20functions.%20We%20demonstrate%20that%20value%0Afunctions%20trained%20with%20categorical%20cross-entropy%20significantly%20improves%0Aperformance%20and%20scalability%20in%20a%20variety%20of%20domains.%20These%20include%3A%20single-task%0ARL%20on%20Atari%202600%20games%20with%20SoftMoEs%2C%20multi-task%20RL%20on%20Atari%20with%20large-scale%0AResNets%2C%20robotic%20manipulation%20with%20Q-transformers%2C%20playing%20Chess%20without%0Asearch%2C%20and%20a%20language-agent%20Wordle%20task%20with%20high-capacity%20Transformers%2C%0Aachieving%20state-of-the-art%20results%20on%20these%20domains.%20Through%20careful%20analysis%2C%0Awe%20show%20that%20the%20benefits%20of%20categorical%20cross-entropy%20primarily%20stem%20from%20its%0Aability%20to%20mitigate%20issues%20inherent%20to%20value-based%20RL%2C%20such%20as%20noisy%20targets%0Aand%20non-stationarity.%20Overall%2C%20we%20argue%20that%20a%20simple%20shift%20to%20training%20value%0Afunctions%20with%20categorical%20cross-entropy%20can%20yield%20substantial%20improvements%20in%0Athe%20scalability%20of%20deep%20RL%20at%20little-to-no%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03950v1&entry.124074799=Read"},
{"title": "German also Hallucinates! Inconsistency Detection in News Summaries with\n  the Absinth Dataset", "author": "Laura Mascarell and Ribin Chalumattu and Annette Rios", "abstract": "  The advent of Large Language Models (LLMs) has led to remarkable progress on\na wide range of natural language processing tasks. Despite the advances, these\nlarge-sized models still suffer from hallucinating information in their output,\nwhich poses a major issue in automatic text summarization, as we must guarantee\nthat the generated summary is consistent with the content of the source\ndocument. Previous research addresses the challenging task of detecting\nhallucinations in the output (i.e. inconsistency detection) in order to\nevaluate the faithfulness of the generated summaries. However, these works\nprimarily focus on English and recent multilingual approaches lack German data.\nThis work presents absinth, a manually annotated dataset for hallucination\ndetection in German news summarization and explores the capabilities of novel\nopen-source LLMs on this task in both fine-tuning and in-context learning\nsettings. We open-source and release the absinth dataset to foster further\nresearch on hallucination detection in German.\n", "link": "http://arxiv.org/abs/2403.03750v1", "date": "2024-03-06", "relevancy": 1.3662, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4776}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4317}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=German%20also%20Hallucinates%21%20Inconsistency%20Detection%20in%20News%20Summaries%20with%0A%20%20the%20Absinth%20Dataset&entry.906535625=Laura%20Mascarell%20and%20Ribin%20Chalumattu%20and%20Annette%20Rios&entry.1292438233=%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%20remarkable%20progress%20on%0Aa%20wide%20range%20of%20natural%20language%20processing%20tasks.%20Despite%20the%20advances%2C%20these%0Alarge-sized%20models%20still%20suffer%20from%20hallucinating%20information%20in%20their%20output%2C%0Awhich%20poses%20a%20major%20issue%20in%20automatic%20text%20summarization%2C%20as%20we%20must%20guarantee%0Athat%20the%20generated%20summary%20is%20consistent%20with%20the%20content%20of%20the%20source%0Adocument.%20Previous%20research%20addresses%20the%20challenging%20task%20of%20detecting%0Ahallucinations%20in%20the%20output%20%28i.e.%20inconsistency%20detection%29%20in%20order%20to%0Aevaluate%20the%20faithfulness%20of%20the%20generated%20summaries.%20However%2C%20these%20works%0Aprimarily%20focus%20on%20English%20and%20recent%20multilingual%20approaches%20lack%20German%20data.%0AThis%20work%20presents%20absinth%2C%20a%20manually%20annotated%20dataset%20for%20hallucination%0Adetection%20in%20German%20news%20summarization%20and%20explores%20the%20capabilities%20of%20novel%0Aopen-source%20LLMs%20on%20this%20task%20in%20both%20fine-tuning%20and%20in-context%20learning%0Asettings.%20We%20open-source%20and%20release%20the%20absinth%20dataset%20to%20foster%20further%0Aresearch%20on%20hallucination%20detection%20in%20German.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03750v1&entry.124074799=Read"},
{"title": "Learning Adversarial MDPs with Stochastic Hard Constraints", "author": "Francesco Emanuele Stradi and Matteo Castiglioni and Alberto Marchesi and Nicola Gatti", "abstract": "  We study online learning problems in constrained Markov decision processes\n(CMDPs) with adversarial losses and stochastic hard constraints. We consider\ntwo different scenarios. In the first one, we address general CMDPs, where we\ndesign an algorithm that attains sublinear regret and cumulative positive\nconstraints violation. In the second scenario, under the mild assumption that a\npolicy strictly satisfying the constraints exists and is known to the learner,\nwe design an algorithm that achieves sublinear regret while ensuring that the\nconstraints are satisfied at every episode with high probability. To the best\nof our knowledge, our work is the first to study CMDPs involving both\nadversarial losses and hard constraints. Indeed, previous works either focus on\nmuch weaker soft constraints--allowing for positive violation to cancel out\nnegative ones--or are restricted to stochastic losses. Thus, our algorithms can\ndeal with general non-stationary environments subject to requirements much\nstricter than those manageable with state-of-the-art algorithms. This enables\ntheir adoption in a much wider range of real-world applications, ranging from\nautonomous driving to online advertising and recommender systems.\n", "link": "http://arxiv.org/abs/2403.03672v1", "date": "2024-03-06", "relevancy": 1.7861, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4811}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4443}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4349}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Adversarial%20MDPs%20with%20Stochastic%20Hard%20Constraints&entry.906535625=Francesco%20Emanuele%20Stradi%20and%20Matteo%20Castiglioni%20and%20Alberto%20Marchesi%20and%20Nicola%20Gatti&entry.1292438233=%20%20We%20study%20online%20learning%20problems%20in%20constrained%20Markov%20decision%20processes%0A%28CMDPs%29%20with%20adversarial%20losses%20and%20stochastic%20hard%20constraints.%20We%20consider%0Atwo%20different%20scenarios.%20In%20the%20first%20one%2C%20we%20address%20general%20CMDPs%2C%20where%20we%0Adesign%20an%20algorithm%20that%20attains%20sublinear%20regret%20and%20cumulative%20positive%0Aconstraints%20violation.%20In%20the%20second%20scenario%2C%20under%20the%20mild%20assumption%20that%20a%0Apolicy%20strictly%20satisfying%20the%20constraints%20exists%20and%20is%20known%20to%20the%20learner%2C%0Awe%20design%20an%20algorithm%20that%20achieves%20sublinear%20regret%20while%20ensuring%20that%20the%0Aconstraints%20are%20satisfied%20at%20every%20episode%20with%20high%20probability.%20To%20the%20best%0Aof%20our%20knowledge%2C%20our%20work%20is%20the%20first%20to%20study%20CMDPs%20involving%20both%0Aadversarial%20losses%20and%20hard%20constraints.%20Indeed%2C%20previous%20works%20either%20focus%20on%0Amuch%20weaker%20soft%20constraints--allowing%20for%20positive%20violation%20to%20cancel%20out%0Anegative%20ones--or%20are%20restricted%20to%20stochastic%20losses.%20Thus%2C%20our%20algorithms%20can%0Adeal%20with%20general%20non-stationary%20environments%20subject%20to%20requirements%20much%0Astricter%20than%20those%20manageable%20with%20state-of-the-art%20algorithms.%20This%20enables%0Atheir%20adoption%20in%20a%20much%20wider%20range%20of%20real-world%20applications%2C%20ranging%20from%0Aautonomous%20driving%20to%20online%20advertising%20and%20recommender%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03672v1&entry.124074799=Read"},
{"title": "Transformer-based nowcasting of radar composites from satellite images\n  for severe weather", "author": "\u00c7a\u011flar K\u00fc\u00e7\u00fck and Apostolos Giannakos and Stefan Schneider and Alexander Jann", "abstract": "  Weather radar data are critical for nowcasting and an integral component of\nnumerical weather prediction models. While weather radar data provide valuable\ninformation at high resolution, their ground-based nature limits their\navailability, which impedes large-scale applications. In contrast,\nmeteorological satellites cover larger domains but with coarser resolution.\nHowever, with the rapid advancements in data-driven methodologies and modern\nsensors aboard geostationary satellites, new opportunities are emerging to\nbridge the gap between ground- and space-based observations, ultimately leading\nto more skillful weather prediction with high accuracy. Here, we present a\nTransformer-based model for nowcasting ground-based radar image sequences using\nsatellite data up to two hours lead time. Trained on a dataset reflecting\nsevere weather conditions, the model predicts radar fields occurring under\ndifferent weather phenomena and shows robustness against rapidly\ngrowing/decaying fields and complex field structures. Model interpretation\nreveals that the infrared channel centered at 10.3 $\\mu m$ (C13) contains\nskillful information for all weather conditions, while lightning data have the\nhighest relative feature importance in severe weather conditions, particularly\nin shorter lead times. The model can support precipitation nowcasting across\nlarge domains without an explicit need for radar towers, enhance numerical\nweather prediction and hydrological models, and provide radar proxy for\ndata-scarce regions. Moreover, the open-source framework facilitates progress\ntowards operational data-driven nowcasting.\n", "link": "http://arxiv.org/abs/2310.19515v2", "date": "2024-03-06", "relevancy": 1.3118, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4766}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4185}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-based%20nowcasting%20of%20radar%20composites%20from%20satellite%20images%0A%20%20for%20severe%20weather&entry.906535625=%C3%87a%C4%9Flar%20K%C3%BC%C3%A7%C3%BCk%20and%20Apostolos%20Giannakos%20and%20Stefan%20Schneider%20and%20Alexander%20Jann&entry.1292438233=%20%20Weather%20radar%20data%20are%20critical%20for%20nowcasting%20and%20an%20integral%20component%20of%0Anumerical%20weather%20prediction%20models.%20While%20weather%20radar%20data%20provide%20valuable%0Ainformation%20at%20high%20resolution%2C%20their%20ground-based%20nature%20limits%20their%0Aavailability%2C%20which%20impedes%20large-scale%20applications.%20In%20contrast%2C%0Ameteorological%20satellites%20cover%20larger%20domains%20but%20with%20coarser%20resolution.%0AHowever%2C%20with%20the%20rapid%20advancements%20in%20data-driven%20methodologies%20and%20modern%0Asensors%20aboard%20geostationary%20satellites%2C%20new%20opportunities%20are%20emerging%20to%0Abridge%20the%20gap%20between%20ground-%20and%20space-based%20observations%2C%20ultimately%20leading%0Ato%20more%20skillful%20weather%20prediction%20with%20high%20accuracy.%20Here%2C%20we%20present%20a%0ATransformer-based%20model%20for%20nowcasting%20ground-based%20radar%20image%20sequences%20using%0Asatellite%20data%20up%20to%20two%20hours%20lead%20time.%20Trained%20on%20a%20dataset%20reflecting%0Asevere%20weather%20conditions%2C%20the%20model%20predicts%20radar%20fields%20occurring%20under%0Adifferent%20weather%20phenomena%20and%20shows%20robustness%20against%20rapidly%0Agrowing/decaying%20fields%20and%20complex%20field%20structures.%20Model%20interpretation%0Areveals%20that%20the%20infrared%20channel%20centered%20at%2010.3%20%24%5Cmu%20m%24%20%28C13%29%20contains%0Askillful%20information%20for%20all%20weather%20conditions%2C%20while%20lightning%20data%20have%20the%0Ahighest%20relative%20feature%20importance%20in%20severe%20weather%20conditions%2C%20particularly%0Ain%20shorter%20lead%20times.%20The%20model%20can%20support%20precipitation%20nowcasting%20across%0Alarge%20domains%20without%20an%20explicit%20need%20for%20radar%20towers%2C%20enhance%20numerical%0Aweather%20prediction%20and%20hydrological%20models%2C%20and%20provide%20radar%20proxy%20for%0Adata-scarce%20regions.%20Moreover%2C%20the%20open-source%20framework%20facilitates%20progress%0Atowards%20operational%20data-driven%20nowcasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19515v2&entry.124074799=Read"},
{"title": "Wildest Dreams: Reproducible Research in Privacy-preserving Neural\n  Network Training", "author": "Tanveer Khan and Mindaugas Budzys and Khoa Nguyen and Antonis Michalas", "abstract": "  Machine Learning (ML), addresses a multitude of complex issues in multiple\ndisciplines, including social sciences, finance, and medical research. ML\nmodels require substantial computing power and are only as powerful as the data\nutilized. Due to high computational cost of ML methods, data scientists\nfrequently use Machine Learning-as-a-Service (MLaaS) to outsource computation\nto external servers. However, when working with private information, like\nfinancial data or health records, outsourcing the computation might result in\nprivacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have\nenabled ML training and inference over protected data through the use of\nPrivacy-Preserving Machine Learning (PPML). However, these techniques are still\nat a preliminary stage and their application in real-world situations is\ndemanding. In order to comprehend discrepancy between theoretical research\nsuggestions and actual applications, this work examines the past and present of\nPPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party\nComputation (SMPC) applied to ML. This work primarily focuses on the ML model's\ntraining phase, where maintaining user data privacy is of utmost importance. We\nprovide a solid theoretical background that eases the understanding of current\napproaches and their limitations. In addition, we present a SoK of the most\nrecent PPML frameworks for model training and provide a comprehensive\ncomparison in terms of the unique properties and performances on standard\nbenchmarks. Also, we reproduce the results for some of the papers and examine\nat what level existing works in the field provide support for open science. We\nbelieve our work serves as a valuable contribution by raising awareness about\nthe current gap between theoretical advancements and real-world applications in\nPPML, specifically regarding open-source availability, reproducibility, and\nusability.\n", "link": "http://arxiv.org/abs/2403.03592v1", "date": "2024-03-06", "relevancy": 1.4206, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.485}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4738}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4614}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wildest%20Dreams%3A%20Reproducible%20Research%20in%20Privacy-preserving%20Neural%0A%20%20Network%20Training&entry.906535625=Tanveer%20Khan%20and%20Mindaugas%20Budzys%20and%20Khoa%20Nguyen%20and%20Antonis%20Michalas&entry.1292438233=%20%20Machine%20Learning%20%28ML%29%2C%20addresses%20a%20multitude%20of%20complex%20issues%20in%20multiple%0Adisciplines%2C%20including%20social%20sciences%2C%20finance%2C%20and%20medical%20research.%20ML%0Amodels%20require%20substantial%20computing%20power%20and%20are%20only%20as%20powerful%20as%20the%20data%0Autilized.%20Due%20to%20high%20computational%20cost%20of%20ML%20methods%2C%20data%20scientists%0Afrequently%20use%20Machine%20Learning-as-a-Service%20%28MLaaS%29%20to%20outsource%20computation%0Ato%20external%20servers.%20However%2C%20when%20working%20with%20private%20information%2C%20like%0Afinancial%20data%20or%20health%20records%2C%20outsourcing%20the%20computation%20might%20result%20in%0Aprivacy%20issues.%20Recent%20advances%20in%20Privacy-Preserving%20Techniques%20%28PPTs%29%20have%0Aenabled%20ML%20training%20and%20inference%20over%20protected%20data%20through%20the%20use%20of%0APrivacy-Preserving%20Machine%20Learning%20%28PPML%29.%20However%2C%20these%20techniques%20are%20still%0Aat%20a%20preliminary%20stage%20and%20their%20application%20in%20real-world%20situations%20is%0Ademanding.%20In%20order%20to%20comprehend%20discrepancy%20between%20theoretical%20research%0Asuggestions%20and%20actual%20applications%2C%20this%20work%20examines%20the%20past%20and%20present%20of%0APPML%2C%20focusing%20on%20Homomorphic%20Encryption%20%28HE%29%20and%20Secure%20Multi-party%0AComputation%20%28SMPC%29%20applied%20to%20ML.%20This%20work%20primarily%20focuses%20on%20the%20ML%20model%27s%0Atraining%20phase%2C%20where%20maintaining%20user%20data%20privacy%20is%20of%20utmost%20importance.%20We%0Aprovide%20a%20solid%20theoretical%20background%20that%20eases%20the%20understanding%20of%20current%0Aapproaches%20and%20their%20limitations.%20In%20addition%2C%20we%20present%20a%20SoK%20of%20the%20most%0Arecent%20PPML%20frameworks%20for%20model%20training%20and%20provide%20a%20comprehensive%0Acomparison%20in%20terms%20of%20the%20unique%20properties%20and%20performances%20on%20standard%0Abenchmarks.%20Also%2C%20we%20reproduce%20the%20results%20for%20some%20of%20the%20papers%20and%20examine%0Aat%20what%20level%20existing%20works%20in%20the%20field%20provide%20support%20for%20open%20science.%20We%0Abelieve%20our%20work%20serves%20as%20a%20valuable%20contribution%20by%20raising%20awareness%20about%0Athe%20current%20gap%20between%20theoretical%20advancements%20and%20real-world%20applications%20in%0APPML%2C%20specifically%20regarding%20open-source%20availability%2C%20reproducibility%2C%20and%0Ausability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03592v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


