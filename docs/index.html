<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 1200px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A Self-supervised Pressure Map human keypoint Detection Approch:\n  Optimizing Generalization and Computational Efficiency Across Datasets", "author": "Chengzhang Yu and Xianjun Yang and Wenxia Bao and Shaonan Wang and Zhiming Yao", "abstract": "  In environments where RGB images are inadequate, pressure maps is a viable\nalternative, garnering scholarly attention. This study introduces a novel\nself-supervised pressure map keypoint detection (SPMKD) method, addressing the\ncurrent gap in specialized designs for human keypoint extraction from pressure\nmaps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model,\nwhich is a robust framework that integrates a lightweight encoder for precise\nhuman keypoint detection, a fuser for efficient gradient propagation, and a\ndecoder that transforms human keypoints into reconstructed pressure maps. This\nstructure is further enhanced by the Classification-to-Regression Weight\nTransfer (CRWT) method, which fine-tunes accuracy through initial\nclassification task training. This innovation not only enhances human keypoint\ngeneralization without manual annotations but also showcases remarkable\nefficiency and generalization, evidenced by a reduction to only $5.96\\%$ in\nFLOPs and $1.11\\%$ in parameter count compared to the baseline methods.\n", "link": "http://arxiv.org/abs/2402.14241v1", "date": "2024-02-22", "relevancy": 2.8999, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6053}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5882}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5464}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Self-supervised%20Pressure%20Map%20human%20keypoint%20Detection%20Approch%3A%0A%20%20Optimizing%20Generalization%20and%20Computational%20Efficiency%20Across%20Datasets&entry.906535625=Chengzhang%20Yu%20and%20Xianjun%20Yang%20and%20Wenxia%20Bao%20and%20Shaonan%20Wang%20and%20Zhiming%20Yao&entry.1292438233=%20%20In%20environments%20where%20RGB%20images%20are%20inadequate%2C%20pressure%20maps%20is%20a%20viable%0Aalternative%2C%20garnering%20scholarly%20attention.%20This%20study%20introduces%20a%20novel%0Aself-supervised%20pressure%20map%20keypoint%20detection%20%28SPMKD%29%20method%2C%20addressing%20the%0Acurrent%20gap%20in%20specialized%20designs%20for%20human%20keypoint%20extraction%20from%20pressure%0Amaps.%20Central%20to%20our%20contribution%20is%20the%20Encoder-Fuser-Decoder%20%28EFD%29%20model%2C%0Awhich%20is%20a%20robust%20framework%20that%20integrates%20a%20lightweight%20encoder%20for%20precise%0Ahuman%20keypoint%20detection%2C%20a%20fuser%20for%20efficient%20gradient%20propagation%2C%20and%20a%0Adecoder%20that%20transforms%20human%20keypoints%20into%20reconstructed%20pressure%20maps.%20This%0Astructure%20is%20further%20enhanced%20by%20the%20Classification-to-Regression%20Weight%0ATransfer%20%28CRWT%29%20method%2C%20which%20fine-tunes%20accuracy%20through%20initial%0Aclassification%20task%20training.%20This%20innovation%20not%20only%20enhances%20human%20keypoint%0Ageneralization%20without%20manual%20annotations%20but%20also%20showcases%20remarkable%0Aefficiency%20and%20generalization%2C%20evidenced%20by%20a%20reduction%20to%20only%20%245.96%5C%25%24%20in%0AFLOPs%20and%20%241.11%5C%25%24%20in%20parameter%20count%20compared%20to%20the%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14241v1&entry.124074799=Read"},
{"title": "Large-Scale Actionless Video Pre-Training via Discrete Diffusion for\n  Efficient Policy Learning", "author": "Haoran He and Chenjia Bai and Ling Pan and Weinan Zhang and Bin Zhao and Xuelong Li", "abstract": "  Learning a generalist embodied agent capable of completing multiple tasks\nposes challenges, primarily stemming from the scarcity of action-labeled\nrobotic datasets. In contrast, a vast amount of human videos exist, capturing\nintricate tasks and interactions with the physical world. Promising prospects\narise for utilizing actionless human videos for pre-training and transferring\nthe knowledge to facilitate robot policy learning through limited robot\ndemonstrations. In this paper, we introduce a novel framework that leverages a\nunified discrete diffusion to combine generative pre-training on human videos\nand policy fine-tuning on a small number of action-labeled robot videos. We\nstart by compressing both human and robot videos into unified video tokens. In\nthe pre-training stage, we employ a discrete diffusion model with a\nmask-and-replace diffusion strategy to predict future video tokens in the\nlatent space. In the fine-tuning stage, we harness the imagined future videos\nto guide low-level action learning trained on a limited set of robot data.\nExperiments demonstrate that our method generates high-fidelity future videos\nfor planning and enhances the fine-tuned policies compared to previous\nstate-of-the-art approaches with superior generalization ability. Our project\nwebsite is available at https://video-diff.github.io/.\n", "link": "http://arxiv.org/abs/2402.14407v1", "date": "2024-02-22", "relevancy": 2.8913, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5974}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5726}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5648}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Scale%20Actionless%20Video%20Pre-Training%20via%20Discrete%20Diffusion%20for%0A%20%20Efficient%20Policy%20Learning&entry.906535625=Haoran%20He%20and%20Chenjia%20Bai%20and%20Ling%20Pan%20and%20Weinan%20Zhang%20and%20Bin%20Zhao%20and%20Xuelong%20Li&entry.1292438233=%20%20Learning%20a%20generalist%20embodied%20agent%20capable%20of%20completing%20multiple%20tasks%0Aposes%20challenges%2C%20primarily%20stemming%20from%20the%20scarcity%20of%20action-labeled%0Arobotic%20datasets.%20In%20contrast%2C%20a%20vast%20amount%20of%20human%20videos%20exist%2C%20capturing%0Aintricate%20tasks%20and%20interactions%20with%20the%20physical%20world.%20Promising%20prospects%0Aarise%20for%20utilizing%20actionless%20human%20videos%20for%20pre-training%20and%20transferring%0Athe%20knowledge%20to%20facilitate%20robot%20policy%20learning%20through%20limited%20robot%0Ademonstrations.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20that%20leverages%20a%0Aunified%20discrete%20diffusion%20to%20combine%20generative%20pre-training%20on%20human%20videos%0Aand%20policy%20fine-tuning%20on%20a%20small%20number%20of%20action-labeled%20robot%20videos.%20We%0Astart%20by%20compressing%20both%20human%20and%20robot%20videos%20into%20unified%20video%20tokens.%20In%0Athe%20pre-training%20stage%2C%20we%20employ%20a%20discrete%20diffusion%20model%20with%20a%0Amask-and-replace%20diffusion%20strategy%20to%20predict%20future%20video%20tokens%20in%20the%0Alatent%20space.%20In%20the%20fine-tuning%20stage%2C%20we%20harness%20the%20imagined%20future%20videos%0Ato%20guide%20low-level%20action%20learning%20trained%20on%20a%20limited%20set%20of%20robot%20data.%0AExperiments%20demonstrate%20that%20our%20method%20generates%20high-fidelity%20future%20videos%0Afor%20planning%20and%20enhances%20the%20fine-tuned%20policies%20compared%20to%20previous%0Astate-of-the-art%20approaches%20with%20superior%20generalization%20ability.%20Our%20project%0Awebsite%20is%20available%20at%20https%3A//video-diff.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14407v1&entry.124074799=Read"},
{"title": "Secure Navigation using Landmark-based Localization in a GPS-denied\n  Environment", "author": "Ganesh Sapkota and Sanjay Madria", "abstract": "  In modern battlefield scenarios, the reliance on GPS for navigation can be a\ncritical vulnerability. Adversaries often employ tactics to deny or deceive GPS\nsignals, necessitating alternative methods for the localization and navigation\nof mobile troops. Range-free localization methods such as DV-HOP rely on\nradio-based anchors and their average hop distance which suffers from accuracy\nand stability in a dynamic and sparse network topology. Vision-based approaches\nlike SLAM and Visual Odometry use sensor fusion techniques for map generation\nand pose estimation that are more sophisticated and computationally expensive.\nThis paper proposes a novel framework that integrates landmark-based\nlocalization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the\nfuture state of moving entities along the battlefield. Our framework utilizes\nsafe trajectory information generated by the troop control center by\nconsidering identifiable landmarks and pre-defined hazard maps. It performs\npoint inclusion tests on the convex hull of the trajectory segments to ensure\nthe safety and survivability of a moving entity and determines the next point\nforward decisions. We present a simulated battlefield scenario for two\ndifferent approaches (with EKF and without EKF) that guide a moving entity\nthrough an obstacle and hazard-free path. Using the proposed method, we\nobserved a percent error of 6.51% lengthwise in safe trajectory estimation with\nan Average Displacement Error (ADE) of 2.97m and a Final Displacement Error\n(FDE) of 3.27m. The results demonstrate that our approach not only ensures the\nsafety of the mobile units by keeping them within the secure trajectory but\nalso enhances operational effectiveness by adapting to the evolving threat\nlandscape.\n", "link": "http://arxiv.org/abs/2402.14280v1", "date": "2024-02-22", "relevancy": 2.8346, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6132}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.551}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5366}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Secure%20Navigation%20using%20Landmark-based%20Localization%20in%20a%20GPS-denied%0A%20%20Environment&entry.906535625=Ganesh%20Sapkota%20and%20Sanjay%20Madria&entry.1292438233=%20%20In%20modern%20battlefield%20scenarios%2C%20the%20reliance%20on%20GPS%20for%20navigation%20can%20be%20a%0Acritical%20vulnerability.%20Adversaries%20often%20employ%20tactics%20to%20deny%20or%20deceive%20GPS%0Asignals%2C%20necessitating%20alternative%20methods%20for%20the%20localization%20and%20navigation%0Aof%20mobile%20troops.%20Range-free%20localization%20methods%20such%20as%20DV-HOP%20rely%20on%0Aradio-based%20anchors%20and%20their%20average%20hop%20distance%20which%20suffers%20from%20accuracy%0Aand%20stability%20in%20a%20dynamic%20and%20sparse%20network%20topology.%20Vision-based%20approaches%0Alike%20SLAM%20and%20Visual%20Odometry%20use%20sensor%20fusion%20techniques%20for%20map%20generation%0Aand%20pose%20estimation%20that%20are%20more%20sophisticated%20and%20computationally%20expensive.%0AThis%20paper%20proposes%20a%20novel%20framework%20that%20integrates%20landmark-based%0Alocalization%20%28LanBLoc%29%20with%20an%20Extended%20Kalman%20Filter%20%28EKF%29%20to%20predict%20the%0Afuture%20state%20of%20moving%20entities%20along%20the%20battlefield.%20Our%20framework%20utilizes%0Asafe%20trajectory%20information%20generated%20by%20the%20troop%20control%20center%20by%0Aconsidering%20identifiable%20landmarks%20and%20pre-defined%20hazard%20maps.%20It%20performs%0Apoint%20inclusion%20tests%20on%20the%20convex%20hull%20of%20the%20trajectory%20segments%20to%20ensure%0Athe%20safety%20and%20survivability%20of%20a%20moving%20entity%20and%20determines%20the%20next%20point%0Aforward%20decisions.%20We%20present%20a%20simulated%20battlefield%20scenario%20for%20two%0Adifferent%20approaches%20%28with%20EKF%20and%20without%20EKF%29%20that%20guide%20a%20moving%20entity%0Athrough%20an%20obstacle%20and%20hazard-free%20path.%20Using%20the%20proposed%20method%2C%20we%0Aobserved%20a%20percent%20error%20of%206.51%25%20lengthwise%20in%20safe%20trajectory%20estimation%20with%0Aan%20Average%20Displacement%20Error%20%28ADE%29%20of%202.97m%20and%20a%20Final%20Displacement%20Error%0A%28FDE%29%20of%203.27m.%20The%20results%20demonstrate%20that%20our%20approach%20not%20only%20ensures%20the%0Asafety%20of%20the%20mobile%20units%20by%20keeping%20them%20within%20the%20secure%20trajectory%20but%0Aalso%20enhances%20operational%20effectiveness%20by%20adapting%20to%20the%20evolving%20threat%0Alandscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14280v1&entry.124074799=Read"},
{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "author": "Yixuan Ren and Yang Zhou and Jimei Yang and Jing Shi and Difan Liu and Feng Liu and Mingi Kwon and Abhinav Shrivastava", "abstract": "  Image customization has been extensively studied in text-to-image (T2I)\ndiffusion models, leading to impressive outcomes and applications. With the\nemergence of text-to-video (T2V) diffusion models, its temporal counterpart,\nmotion customization, has not yet been well investigated. To address the\nchallenge of one-shot motion customization, we propose Customize-A-Video that\nmodels the motion from a single reference video and adapting it to new subjects\nand scenes with both spatial and temporal varieties. It leverages low-rank\nadaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V\ndiffusion model for specific motion modeling from the reference videos. To\ndisentangle the spatial and temporal information during the training pipeline,\nwe introduce a novel concept of appearance absorbers that detach the original\nappearance from the single reference video prior to motion learning. Our\nproposed method can be easily extended to various downstream tasks, including\ncustom video generation and editing, video appearance customization, and\nmultiple motion combination, in a plug-and-play fashion. Our project page can\nbe found at https://anonymous-314.github.io.\n", "link": "http://arxiv.org/abs/2402.14780v1", "date": "2024-02-22", "relevancy": 2.7826, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7215}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4836}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4645}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customize-A-Video%3A%20One-Shot%20Motion%20Customization%20of%20Text-to-Video%0A%20%20Diffusion%20Models&entry.906535625=Yixuan%20Ren%20and%20Yang%20Zhou%20and%20Jimei%20Yang%20and%20Jing%20Shi%20and%20Difan%20Liu%20and%20Feng%20Liu%20and%20Mingi%20Kwon%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Image%20customization%20has%20been%20extensively%20studied%20in%20text-to-image%20%28T2I%29%0Adiffusion%20models%2C%20leading%20to%20impressive%20outcomes%20and%20applications.%20With%20the%0Aemergence%20of%20text-to-video%20%28T2V%29%20diffusion%20models%2C%20its%20temporal%20counterpart%2C%0Amotion%20customization%2C%20has%20not%20yet%20been%20well%20investigated.%20To%20address%20the%0Achallenge%20of%20one-shot%20motion%20customization%2C%20we%20propose%20Customize-A-Video%20that%0Amodels%20the%20motion%20from%20a%20single%20reference%20video%20and%20adapting%20it%20to%20new%20subjects%0Aand%20scenes%20with%20both%20spatial%20and%20temporal%20varieties.%20It%20leverages%20low-rank%0Aadaptation%20%28LoRA%29%20on%20temporal%20attention%20layers%20to%20tailor%20the%20pre-trained%20T2V%0Adiffusion%20model%20for%20specific%20motion%20modeling%20from%20the%20reference%20videos.%20To%0Adisentangle%20the%20spatial%20and%20temporal%20information%20during%20the%20training%20pipeline%2C%0Awe%20introduce%20a%20novel%20concept%20of%20appearance%20absorbers%20that%20detach%20the%20original%0Aappearance%20from%20the%20single%20reference%20video%20prior%20to%20motion%20learning.%20Our%0Aproposed%20method%20can%20be%20easily%20extended%20to%20various%20downstream%20tasks%2C%20including%0Acustom%20video%20generation%20and%20editing%2C%20video%20appearance%20customization%2C%20and%0Amultiple%20motion%20combination%2C%20in%20a%20plug-and-play%20fashion.%20Our%20project%20page%20can%0Abe%20found%20at%20https%3A//anonymous-314.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14780v1&entry.124074799=Read"},
{"title": "Spatial Transform Decoupling for Oriented Object Detection", "author": "Hongtian Yu and Yunjie Tian and Qixiang Ye and Yunfan Liu", "abstract": "  Vision Transformers (ViTs) have achieved remarkable success in computer\nvision tasks. However, their potential in rotation-sensitive scenarios has not\nbeen fully explored, and this limitation may be inherently attributed to the\nlack of spatial invariance in the data-forwarding process. In this study, we\npresent a novel approach, termed Spatial Transform Decoupling (STD), providing\na simple-yet-effective solution for oriented object detection with ViTs. Built\nupon stacked ViT blocks, STD utilizes separate network branches to predict the\nposition, size, and angle of bounding boxes, effectively harnessing the spatial\ntransform potential of ViTs in a divide-and-conquer fashion. Moreover, by\naggregating cascaded activation masks (CAMs) computed upon the regressed\nparameters, STD gradually enhances features within regions of interest (RoIs),\nwhich complements the self-attention mechanism. Without bells and whistles, STD\nachieves state-of-the-art performance on the benchmark datasets including\nDOTA-v1.0 (82.24% mAP) and HRSC2016 (98.55% mAP), which demonstrates the\neffectiveness of the proposed method. Source code is available at\nhttps://github.com/yuhongtian17/Spatial-Transform-Decoupling.\n", "link": "http://arxiv.org/abs/2308.10561v2", "date": "2024-02-22", "relevancy": 2.7541, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5639}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5512}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5374}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Transform%20Decoupling%20for%20Oriented%20Object%20Detection&entry.906535625=Hongtian%20Yu%20and%20Yunjie%20Tian%20and%20Qixiang%20Ye%20and%20Yunfan%20Liu&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20remarkable%20success%20in%20computer%0Avision%20tasks.%20However%2C%20their%20potential%20in%20rotation-sensitive%20scenarios%20has%20not%0Abeen%20fully%20explored%2C%20and%20this%20limitation%20may%20be%20inherently%20attributed%20to%20the%0Alack%20of%20spatial%20invariance%20in%20the%20data-forwarding%20process.%20In%20this%20study%2C%20we%0Apresent%20a%20novel%20approach%2C%20termed%20Spatial%20Transform%20Decoupling%20%28STD%29%2C%20providing%0Aa%20simple-yet-effective%20solution%20for%20oriented%20object%20detection%20with%20ViTs.%20Built%0Aupon%20stacked%20ViT%20blocks%2C%20STD%20utilizes%20separate%20network%20branches%20to%20predict%20the%0Aposition%2C%20size%2C%20and%20angle%20of%20bounding%20boxes%2C%20effectively%20harnessing%20the%20spatial%0Atransform%20potential%20of%20ViTs%20in%20a%20divide-and-conquer%20fashion.%20Moreover%2C%20by%0Aaggregating%20cascaded%20activation%20masks%20%28CAMs%29%20computed%20upon%20the%20regressed%0Aparameters%2C%20STD%20gradually%20enhances%20features%20within%20regions%20of%20interest%20%28RoIs%29%2C%0Awhich%20complements%20the%20self-attention%20mechanism.%20Without%20bells%20and%20whistles%2C%20STD%0Aachieves%20state-of-the-art%20performance%20on%20the%20benchmark%20datasets%20including%0ADOTA-v1.0%20%2882.24%25%20mAP%29%20and%20HRSC2016%20%2898.55%25%20mAP%29%2C%20which%20demonstrates%20the%0Aeffectiveness%20of%20the%20proposed%20method.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/yuhongtian17/Spatial-Transform-Decoupling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10561v2&entry.124074799=Read"},
{"title": "WeakSAM: Segment Anything Meets Weakly-supervised Instance-level\n  Recognition", "author": "Lianghui Zhu and Junwei Zhou and Yan Liu and Xin Hao and Wenyu Liu and Xinggang Wang", "abstract": "  Weakly supervised visual recognition using inexact supervision is a critical\nyet challenging learning problem. It significantly reduces human labeling costs\nand traditionally relies on multi-instance learning and pseudo-labeling. This\npaper introduces WeakSAM and solves the weakly-supervised object detection\n(WSOD) and segmentation by utilizing the pre-learned world knowledge contained\nin a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM\naddresses two critical limitations in traditional WSOD retraining, i.e., pseudo\nground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT\ngeneration and Region of Interest (RoI) drop regularization. It also addresses\nthe SAM's problems of requiring prompts and category unawareness for automatic\nobject detection and segmentation. Our results indicate that WeakSAM\nsignificantly surpasses previous state-of-the-art methods in WSOD and WSIS\nbenchmarks with large margins, i.e. average improvements of 7.4% and 8.5%,\nrespectively. The code is available at \\url{https://github.com/hustvl/WeakSAM}.\n", "link": "http://arxiv.org/abs/2402.14812v1", "date": "2024-02-22", "relevancy": 2.7483, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5897}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5362}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5232}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeakSAM%3A%20Segment%20Anything%20Meets%20Weakly-supervised%20Instance-level%0A%20%20Recognition&entry.906535625=Lianghui%20Zhu%20and%20Junwei%20Zhou%20and%20Yan%20Liu%20and%20Xin%20Hao%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20Weakly%20supervised%20visual%20recognition%20using%20inexact%20supervision%20is%20a%20critical%0Ayet%20challenging%20learning%20problem.%20It%20significantly%20reduces%20human%20labeling%20costs%0Aand%20traditionally%20relies%20on%20multi-instance%20learning%20and%20pseudo-labeling.%20This%0Apaper%20introduces%20WeakSAM%20and%20solves%20the%20weakly-supervised%20object%20detection%0A%28WSOD%29%20and%20segmentation%20by%20utilizing%20the%20pre-learned%20world%20knowledge%20contained%0Ain%20a%20vision%20foundation%20model%2C%20i.e.%2C%20the%20Segment%20Anything%20Model%20%28SAM%29.%20WeakSAM%0Aaddresses%20two%20critical%20limitations%20in%20traditional%20WSOD%20retraining%2C%20i.e.%2C%20pseudo%0Aground%20truth%20%28PGT%29%20incompleteness%20and%20noisy%20PGT%20instances%2C%20through%20adaptive%20PGT%0Ageneration%20and%20Region%20of%20Interest%20%28RoI%29%20drop%20regularization.%20It%20also%20addresses%0Athe%20SAM%27s%20problems%20of%20requiring%20prompts%20and%20category%20unawareness%20for%20automatic%0Aobject%20detection%20and%20segmentation.%20Our%20results%20indicate%20that%20WeakSAM%0Asignificantly%20surpasses%20previous%20state-of-the-art%20methods%20in%20WSOD%20and%20WSIS%0Abenchmarks%20with%20large%20margins%2C%20i.e.%20average%20improvements%20of%207.4%25%20and%208.5%25%2C%0Arespectively.%20The%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/hustvl/WeakSAM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14812v1&entry.124074799=Read"},
{"title": "Towards Seamless Adaptation of Pre-trained Models for Visual Place\n  Recognition", "author": "Feng Lu and Lijun Zhang and Xiangyuan Lan and Shuting Dong and Yaowei Wang and Chun Yuan", "abstract": "  Recent studies show that vision models pre-trained in generic visual learning\ntasks with large-scale data can provide useful feature representations for a\nwide range of visual perception problems. However, few attempts have been made\nto exploit pre-trained foundation models in visual place recognition (VPR). Due\nto the inherent difference in training objectives and data between the tasks of\nmodel pre-training and VPR, how to bridge the gap and fully unleash the\ncapability of pre-trained models for VPR is still a key issue to address. To\nthis end, we propose a novel method to realize seamless adaptation of\npre-trained models for VPR. Specifically, to obtain both global and local\nfeatures that focus on salient landmarks for discriminating places, we design a\nhybrid adaptation method to achieve both global and local adaptation\nefficiently, in which only lightweight adapters are tuned without adjusting the\npre-trained model. Besides, to guide effective adaptation, we propose a mutual\nnearest neighbor local feature loss, which ensures proper dense local features\nare produced for local matching and avoids time-consuming spatial verification\nin re-ranking. Experimental results show that our method outperforms the\nstate-of-the-art methods with less training data and training time, and uses\nabout only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based\nspatial verification. It ranks 1st on the MSLS challenge leaderboard (at the\ntime of submission). The code is released at\nhttps://github.com/Lu-Feng/SelaVPR.\n", "link": "http://arxiv.org/abs/2402.14505v1", "date": "2024-02-22", "relevancy": 2.7326, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5467}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5373}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Seamless%20Adaptation%20of%20Pre-trained%20Models%20for%20Visual%20Place%0A%20%20Recognition&entry.906535625=Feng%20Lu%20and%20Lijun%20Zhang%20and%20Xiangyuan%20Lan%20and%20Shuting%20Dong%20and%20Yaowei%20Wang%20and%20Chun%20Yuan&entry.1292438233=%20%20Recent%20studies%20show%20that%20vision%20models%20pre-trained%20in%20generic%20visual%20learning%0Atasks%20with%20large-scale%20data%20can%20provide%20useful%20feature%20representations%20for%20a%0Awide%20range%20of%20visual%20perception%20problems.%20However%2C%20few%20attempts%20have%20been%20made%0Ato%20exploit%20pre-trained%20foundation%20models%20in%20visual%20place%20recognition%20%28VPR%29.%20Due%0Ato%20the%20inherent%20difference%20in%20training%20objectives%20and%20data%20between%20the%20tasks%20of%0Amodel%20pre-training%20and%20VPR%2C%20how%20to%20bridge%20the%20gap%20and%20fully%20unleash%20the%0Acapability%20of%20pre-trained%20models%20for%20VPR%20is%20still%20a%20key%20issue%20to%20address.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20method%20to%20realize%20seamless%20adaptation%20of%0Apre-trained%20models%20for%20VPR.%20Specifically%2C%20to%20obtain%20both%20global%20and%20local%0Afeatures%20that%20focus%20on%20salient%20landmarks%20for%20discriminating%20places%2C%20we%20design%20a%0Ahybrid%20adaptation%20method%20to%20achieve%20both%20global%20and%20local%20adaptation%0Aefficiently%2C%20in%20which%20only%20lightweight%20adapters%20are%20tuned%20without%20adjusting%20the%0Apre-trained%20model.%20Besides%2C%20to%20guide%20effective%20adaptation%2C%20we%20propose%20a%20mutual%0Anearest%20neighbor%20local%20feature%20loss%2C%20which%20ensures%20proper%20dense%20local%20features%0Aare%20produced%20for%20local%20matching%20and%20avoids%20time-consuming%20spatial%20verification%0Ain%20re-ranking.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20the%0Astate-of-the-art%20methods%20with%20less%20training%20data%20and%20training%20time%2C%20and%20uses%0Aabout%20only%203%25%20retrieval%20runtime%20of%20the%20two-stage%20VPR%20methods%20with%20RANSAC-based%0Aspatial%20verification.%20It%20ranks%201st%20on%20the%20MSLS%20challenge%20leaderboard%20%28at%20the%0Atime%20of%20submission%29.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/Lu-Feng/SelaVPR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14505v1&entry.124074799=Read"},
{"title": "Attention Disturbance and Dual-Path Constraint Network for Occluded\n  Person Re-identification", "author": "Jiaer Xia and Lei Tan and Pingyang Dai and Mingbo Zhao and Yongjian Wu and Liujuan Cao", "abstract": "  Occluded person re-identification (Re-ID) aims to address the potential\nocclusion problem when matching occluded or holistic pedestrians from different\ncamera views. Many methods use the background as artificial occlusion and rely\non attention networks to exclude noisy interference. However, the significant\ndiscrepancy between simple background occlusion and realistic occlusion can\nnegatively impact the generalization of the network. To address this issue, we\npropose a novel transformer-based Attention Disturbance and Dual-Path\nConstraint Network (ADP) to enhance the generalization of attention networks.\nFirstly, to imitate real-world obstacles, we introduce an Attention Disturbance\nMask (ADM) module that generates an offensive noise, which can distract\nattention like a realistic occluder, as a more complex form of occlusion.\nSecondly, to fully exploit these complex occluded images, we develop a\nDual-Path Constraint Module (DPC) that can obtain preferable supervision\ninformation from holistic images through dual-path interaction. With our\nproposed method, the network can effectively circumvent a wide variety of\nocclusions using the basic ViT baseline. Comprehensive experimental evaluations\nconducted on person re-ID benchmarks demonstrate the superiority of ADP over\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2303.10976v2", "date": "2024-02-22", "relevancy": 2.7075, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5528}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5415}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5302}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Disturbance%20and%20Dual-Path%20Constraint%20Network%20for%20Occluded%0A%20%20Person%20Re-identification&entry.906535625=Jiaer%20Xia%20and%20Lei%20Tan%20and%20Pingyang%20Dai%20and%20Mingbo%20Zhao%20and%20Yongjian%20Wu%20and%20Liujuan%20Cao&entry.1292438233=%20%20Occluded%20person%20re-identification%20%28Re-ID%29%20aims%20to%20address%20the%20potential%0Aocclusion%20problem%20when%20matching%20occluded%20or%20holistic%20pedestrians%20from%20different%0Acamera%20views.%20Many%20methods%20use%20the%20background%20as%20artificial%20occlusion%20and%20rely%0Aon%20attention%20networks%20to%20exclude%20noisy%20interference.%20However%2C%20the%20significant%0Adiscrepancy%20between%20simple%20background%20occlusion%20and%20realistic%20occlusion%20can%0Anegatively%20impact%20the%20generalization%20of%20the%20network.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20novel%20transformer-based%20Attention%20Disturbance%20and%20Dual-Path%0AConstraint%20Network%20%28ADP%29%20to%20enhance%20the%20generalization%20of%20attention%20networks.%0AFirstly%2C%20to%20imitate%20real-world%20obstacles%2C%20we%20introduce%20an%20Attention%20Disturbance%0AMask%20%28ADM%29%20module%20that%20generates%20an%20offensive%20noise%2C%20which%20can%20distract%0Aattention%20like%20a%20realistic%20occluder%2C%20as%20a%20more%20complex%20form%20of%20occlusion.%0ASecondly%2C%20to%20fully%20exploit%20these%20complex%20occluded%20images%2C%20we%20develop%20a%0ADual-Path%20Constraint%20Module%20%28DPC%29%20that%20can%20obtain%20preferable%20supervision%0Ainformation%20from%20holistic%20images%20through%20dual-path%20interaction.%20With%20our%0Aproposed%20method%2C%20the%20network%20can%20effectively%20circumvent%20a%20wide%20variety%20of%0Aocclusions%20using%20the%20basic%20ViT%20baseline.%20Comprehensive%20experimental%20evaluations%0Aconducted%20on%20person%20re-ID%20benchmarks%20demonstrate%20the%20superiority%20of%20ADP%20over%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10976v2&entry.124074799=Read"},
{"title": "Font Style Interpolation with Diffusion Models", "author": "Tetta Kondo and Shumpei Takezaki and Daichi Haraguchi and Seiichi Uchida", "abstract": "  Fonts have huge variations in their styles and give readers different\nimpressions. Therefore, generating new fonts is worthy of giving new\nimpressions to readers. In this paper, we employ diffusion models to generate\nnew font styles by interpolating a pair of reference fonts with different\nstyles. More specifically, we propose three different interpolation approaches,\nimage-blending, condition-blending, and noise-blending, with the diffusion\nmodels. We perform qualitative and quantitative experimental analyses to\nunderstand the style generation ability of the three approaches. According to\nexperimental results, three proposed approaches can generate not only expected\nfont styles but also somewhat serendipitous font styles. We also compare the\napproaches with a state-of-the-art style-conditional Latin-font generative\nnetwork model to confirm the validity of using the diffusion models for the\nstyle interpolation task.\n", "link": "http://arxiv.org/abs/2402.14311v1", "date": "2024-02-22", "relevancy": 2.1561, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4871}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4034}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4031}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Font%20Style%20Interpolation%20with%20Diffusion%20Models&entry.906535625=Tetta%20Kondo%20and%20Shumpei%20Takezaki%20and%20Daichi%20Haraguchi%20and%20Seiichi%20Uchida&entry.1292438233=%20%20Fonts%20have%20huge%20variations%20in%20their%20styles%20and%20give%20readers%20different%0Aimpressions.%20Therefore%2C%20generating%20new%20fonts%20is%20worthy%20of%20giving%20new%0Aimpressions%20to%20readers.%20In%20this%20paper%2C%20we%20employ%20diffusion%20models%20to%20generate%0Anew%20font%20styles%20by%20interpolating%20a%20pair%20of%20reference%20fonts%20with%20different%0Astyles.%20More%20specifically%2C%20we%20propose%20three%20different%20interpolation%20approaches%2C%0Aimage-blending%2C%20condition-blending%2C%20and%20noise-blending%2C%20with%20the%20diffusion%0Amodels.%20We%20perform%20qualitative%20and%20quantitative%20experimental%20analyses%20to%0Aunderstand%20the%20style%20generation%20ability%20of%20the%20three%20approaches.%20According%20to%0Aexperimental%20results%2C%20three%20proposed%20approaches%20can%20generate%20not%20only%20expected%0Afont%20styles%20but%20also%20somewhat%20serendipitous%20font%20styles.%20We%20also%20compare%20the%0Aapproaches%20with%20a%20state-of-the-art%20style-conditional%20Latin-font%20generative%0Anetwork%20model%20to%20confirm%20the%20validity%20of%20using%20the%20diffusion%20models%20for%20the%0Astyle%20interpolation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14311v1&entry.124074799=Read"},
{"title": "Multivariate Online Linear Regression for Hierarchical Forecasting", "author": "Massil Hihat and Guillaume Garrigos and Adeline Fermanian and Simon Bussy", "abstract": "  In this paper, we consider a deterministic online linear regression model\nwhere we allow the responses to be multivariate. To address this problem, we\nintroduce MultiVAW, a method that extends the well-known Vovk-Azoury-Warmuth\nalgorithm to the multivariate setting, and show that it also enjoys logarithmic\nregret in time. We apply our results to the online hierarchical forecasting\nproblem and recover an algorithm from this literature as a special case,\nallowing us to relax the hypotheses usually made for its analysis.\n", "link": "http://arxiv.org/abs/2402.14578v1", "date": "2024-02-22", "relevancy": 1.6688, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4758}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4102}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4008}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multivariate%20Online%20Linear%20Regression%20for%20Hierarchical%20Forecasting&entry.906535625=Massil%20Hihat%20and%20Guillaume%20Garrigos%20and%20Adeline%20Fermanian%20and%20Simon%20Bussy&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20a%20deterministic%20online%20linear%20regression%20model%0Awhere%20we%20allow%20the%20responses%20to%20be%20multivariate.%20To%20address%20this%20problem%2C%20we%0Aintroduce%20MultiVAW%2C%20a%20method%20that%20extends%20the%20well-known%20Vovk-Azoury-Warmuth%0Aalgorithm%20to%20the%20multivariate%20setting%2C%20and%20show%20that%20it%20also%20enjoys%20logarithmic%0Aregret%20in%20time.%20We%20apply%20our%20results%20to%20the%20online%20hierarchical%20forecasting%0Aproblem%20and%20recover%20an%20algorithm%20from%20this%20literature%20as%20a%20special%20case%2C%0Aallowing%20us%20to%20relax%20the%20hypotheses%20usually%20made%20for%20its%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14578v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


