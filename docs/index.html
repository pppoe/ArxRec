<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Behavior Generation with Latent Actions", "author": "Seungjae Lee and Yibin Wang and Haritheja Etukuru and H. Jin Kim and Nur Muhammad Mahi Shafiullah and Lerrel Pinto", "abstract": "  Generative modeling of complex behaviors from labeled datasets has been a\nlongstanding problem in decision making. Unlike language or image generation,\ndecision making requires modeling actions - continuous-valued vectors that are\nmultimodal in their distribution, potentially drawn from uncurated sources,\nwhere generation errors can compound in sequential prediction. A recent class\nof models called Behavior Transformers (BeT) addresses this by discretizing\nactions using k-means clustering to capture different modes. However, k-means\nstruggles to scale for high-dimensional action spaces or long sequences, and\nlacks gradient information, and thus BeT suffers in modeling long-range\nactions. In this work, we present Vector-Quantized Behavior Transformer\n(VQ-BeT), a versatile model for behavior generation that handles multimodal\naction prediction, conditional generation, and partial observations. VQ-BeT\naugments BeT by tokenizing continuous actions with a hierarchical vector\nquantization module. Across seven environments including simulated\nmanipulation, autonomous driving, and robotics, VQ-BeT improves on\nstate-of-the-art models such as BeT and Diffusion Policies. Importantly, we\ndemonstrate VQ-BeT's improved ability to capture behavior modes while\naccelerating inference speed 5x over Diffusion Policies. Videos and code can be\nfound https://sjlee.cc/vq-bet\n", "link": "http://arxiv.org/abs/2403.03181v1", "date": "2024-03-05", "relevancy": 2.8872, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6334}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5525}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5464}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behavior%20Generation%20with%20Latent%20Actions&entry.906535625=Seungjae%20Lee%20and%20Yibin%20Wang%20and%20Haritheja%20Etukuru%20and%20H.%20Jin%20Kim%20and%20Nur%20Muhammad%20Mahi%20Shafiullah%20and%20Lerrel%20Pinto&entry.1292438233=%20%20Generative%20modeling%20of%20complex%20behaviors%20from%20labeled%20datasets%20has%20been%20a%0Alongstanding%20problem%20in%20decision%20making.%20Unlike%20language%20or%20image%20generation%2C%0Adecision%20making%20requires%20modeling%20actions%20-%20continuous-valued%20vectors%20that%20are%0Amultimodal%20in%20their%20distribution%2C%20potentially%20drawn%20from%20uncurated%20sources%2C%0Awhere%20generation%20errors%20can%20compound%20in%20sequential%20prediction.%20A%20recent%20class%0Aof%20models%20called%20Behavior%20Transformers%20%28BeT%29%20addresses%20this%20by%20discretizing%0Aactions%20using%20k-means%20clustering%20to%20capture%20different%20modes.%20However%2C%20k-means%0Astruggles%20to%20scale%20for%20high-dimensional%20action%20spaces%20or%20long%20sequences%2C%20and%0Alacks%20gradient%20information%2C%20and%20thus%20BeT%20suffers%20in%20modeling%20long-range%0Aactions.%20In%20this%20work%2C%20we%20present%20Vector-Quantized%20Behavior%20Transformer%0A%28VQ-BeT%29%2C%20a%20versatile%20model%20for%20behavior%20generation%20that%20handles%20multimodal%0Aaction%20prediction%2C%20conditional%20generation%2C%20and%20partial%20observations.%20VQ-BeT%0Aaugments%20BeT%20by%20tokenizing%20continuous%20actions%20with%20a%20hierarchical%20vector%0Aquantization%20module.%20Across%20seven%20environments%20including%20simulated%0Amanipulation%2C%20autonomous%20driving%2C%20and%20robotics%2C%20VQ-BeT%20improves%20on%0Astate-of-the-art%20models%20such%20as%20BeT%20and%20Diffusion%20Policies.%20Importantly%2C%20we%0Ademonstrate%20VQ-BeT%27s%20improved%20ability%20to%20capture%20behavior%20modes%20while%0Aaccelerating%20inference%20speed%205x%20over%20Diffusion%20Policies.%20Videos%20and%20code%20can%20be%0Afound%20https%3A//sjlee.cc/vq-bet%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03181v1&entry.124074799=Read"},
{"title": "3D Human Pose Estimation Based on 2D-3D Consistency with Synchronized\n  Adversarial Training", "author": "Yicheng Deng and Cheng Sun and Yongqi Sun and Jiahui Zhu", "abstract": "  3D human pose estimation from a single image is still a challenging problem\ndespite the large amount of work that has been performed in this field.\nGenerally, most methods directly use neural networks and ignore certain\nconstraints (e.g., reprojection constraints, joint angle, and bone length\nconstraints). While a few methods consider these constraints but train the\nnetwork separately, they cannot effectively solve the depth ambiguity problem.\nIn this paper, we propose a GAN-based model for 3D human pose estimation, in\nwhich a reprojection network is employed to learn the mapping of the\ndistribution from 3D poses to 2D poses, and a discriminator is employed for\n2D-3D consistency discrimination. We adopt a novel strategy to synchronously\ntrain the generator, the reprojection network and the discriminator.\nFurthermore, inspired by the typical kinematic chain space (KCS) matrix, we\nintroduce a weighted KCS matrix and take it as one of the discriminator's\ninputs to impose joint angle and bone length constraints. The experimental\nresults on Human3.6M show that our method significantly outperforms\nstate-of-the-art methods in most cases.\n", "link": "http://arxiv.org/abs/2106.04274v4", "date": "2024-03-05", "relevancy": 2.8652, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5981}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5656}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5555}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human%20Pose%20Estimation%20Based%20on%202D-3D%20Consistency%20with%20Synchronized%0A%20%20Adversarial%20Training&entry.906535625=Yicheng%20Deng%20and%20Cheng%20Sun%20and%20Yongqi%20Sun%20and%20Jiahui%20Zhu&entry.1292438233=%20%203D%20human%20pose%20estimation%20from%20a%20single%20image%20is%20still%20a%20challenging%20problem%0Adespite%20the%20large%20amount%20of%20work%20that%20has%20been%20performed%20in%20this%20field.%0AGenerally%2C%20most%20methods%20directly%20use%20neural%20networks%20and%20ignore%20certain%0Aconstraints%20%28e.g.%2C%20reprojection%20constraints%2C%20joint%20angle%2C%20and%20bone%20length%0Aconstraints%29.%20While%20a%20few%20methods%20consider%20these%20constraints%20but%20train%20the%0Anetwork%20separately%2C%20they%20cannot%20effectively%20solve%20the%20depth%20ambiguity%20problem.%0AIn%20this%20paper%2C%20we%20propose%20a%20GAN-based%20model%20for%203D%20human%20pose%20estimation%2C%20in%0Awhich%20a%20reprojection%20network%20is%20employed%20to%20learn%20the%20mapping%20of%20the%0Adistribution%20from%203D%20poses%20to%202D%20poses%2C%20and%20a%20discriminator%20is%20employed%20for%0A2D-3D%20consistency%20discrimination.%20We%20adopt%20a%20novel%20strategy%20to%20synchronously%0Atrain%20the%20generator%2C%20the%20reprojection%20network%20and%20the%20discriminator.%0AFurthermore%2C%20inspired%20by%20the%20typical%20kinematic%20chain%20space%20%28KCS%29%20matrix%2C%20we%0Aintroduce%20a%20weighted%20KCS%20matrix%20and%20take%20it%20as%20one%20of%20the%20discriminator%27s%0Ainputs%20to%20impose%20joint%20angle%20and%20bone%20length%20constraints.%20The%20experimental%0Aresults%20on%20Human3.6M%20show%20that%20our%20method%20significantly%20outperforms%0Astate-of-the-art%20methods%20in%20most%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2106.04274v4&entry.124074799=Read"},
{"title": "Suppress and Rebalance: Towards Generalized Multi-Modal Face\n  Anti-Spoofing", "author": "Xun Lin and Shuai Wang and Rizhao Cai and Yizhong Liu and Ying Fu and Zitong Yu and Wenzhong Tang and Alex Kot", "abstract": "  Face Anti-Spoofing (FAS) is crucial for securing face recognition systems\nagainst presentation attacks. With advancements in sensor manufacture and\nmulti-modal learning techniques, many multi-modal FAS approaches have emerged.\nHowever, they face challenges in generalizing to unseen attacks and deployment\nconditions. These challenges arise from (1) modality unreliability, where some\nmodality sensors like depth and infrared undergo significant domain shifts in\nvarying environments, leading to the spread of unreliable information during\ncross-modal feature fusion, and (2) modality imbalance, where training overly\nrelies on a dominant modality hinders the convergence of others, reducing\neffectiveness against attack types that are indistinguishable sorely using the\ndominant modality. To address modality unreliability, we propose the\nUncertainty-Guided Cross-Adapter (U-Adapter) to recognize unreliably detected\nregions within each modality and suppress the impact of unreliable regions on\nother modalities. For modality imbalance, we propose a Rebalanced Modality\nGradient Modulation (ReGrad) strategy to rebalance the convergence speed of all\nmodalities by adaptively adjusting their gradients. Besides, we provide the\nfirst large-scale benchmark for evaluating multi-modal FAS performance under\ndomain generalization scenarios. Extensive experiments demonstrate that our\nmethod outperforms state-of-the-art methods. Source code and protocols will be\nreleased on https://github.com/OMGGGGG/mmdg.\n", "link": "http://arxiv.org/abs/2402.19298v2", "date": "2024-03-05", "relevancy": 2.8151, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5934}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5498}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5459}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Suppress%20and%20Rebalance%3A%20Towards%20Generalized%20Multi-Modal%20Face%0A%20%20Anti-Spoofing&entry.906535625=Xun%20Lin%20and%20Shuai%20Wang%20and%20Rizhao%20Cai%20and%20Yizhong%20Liu%20and%20Ying%20Fu%20and%20Zitong%20Yu%20and%20Wenzhong%20Tang%20and%20Alex%20Kot&entry.1292438233=%20%20Face%20Anti-Spoofing%20%28FAS%29%20is%20crucial%20for%20securing%20face%20recognition%20systems%0Aagainst%20presentation%20attacks.%20With%20advancements%20in%20sensor%20manufacture%20and%0Amulti-modal%20learning%20techniques%2C%20many%20multi-modal%20FAS%20approaches%20have%20emerged.%0AHowever%2C%20they%20face%20challenges%20in%20generalizing%20to%20unseen%20attacks%20and%20deployment%0Aconditions.%20These%20challenges%20arise%20from%20%281%29%20modality%20unreliability%2C%20where%20some%0Amodality%20sensors%20like%20depth%20and%20infrared%20undergo%20significant%20domain%20shifts%20in%0Avarying%20environments%2C%20leading%20to%20the%20spread%20of%20unreliable%20information%20during%0Across-modal%20feature%20fusion%2C%20and%20%282%29%20modality%20imbalance%2C%20where%20training%20overly%0Arelies%20on%20a%20dominant%20modality%20hinders%20the%20convergence%20of%20others%2C%20reducing%0Aeffectiveness%20against%20attack%20types%20that%20are%20indistinguishable%20sorely%20using%20the%0Adominant%20modality.%20To%20address%20modality%20unreliability%2C%20we%20propose%20the%0AUncertainty-Guided%20Cross-Adapter%20%28U-Adapter%29%20to%20recognize%20unreliably%20detected%0Aregions%20within%20each%20modality%20and%20suppress%20the%20impact%20of%20unreliable%20regions%20on%0Aother%20modalities.%20For%20modality%20imbalance%2C%20we%20propose%20a%20Rebalanced%20Modality%0AGradient%20Modulation%20%28ReGrad%29%20strategy%20to%20rebalance%20the%20convergence%20speed%20of%20all%0Amodalities%20by%20adaptively%20adjusting%20their%20gradients.%20Besides%2C%20we%20provide%20the%0Afirst%20large-scale%20benchmark%20for%20evaluating%20multi-modal%20FAS%20performance%20under%0Adomain%20generalization%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20outperforms%20state-of-the-art%20methods.%20Source%20code%20and%20protocols%20will%20be%0Areleased%20on%20https%3A//github.com/OMGGGGG/mmdg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19298v2&entry.124074799=Read"},
{"title": "MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for\n  Accelerating Vision-Language Transformer", "author": "Jianjian Cao and Peng Ye and Shengze Li and Chong Yu and Yansong Tang and Jiwen Lu and Tao Chen", "abstract": "  Vision-Language Transformers (VLTs) have shown great success recently, but\nare meanwhile accompanied by heavy computation costs, where a major reason can\nbe attributed to the large number of visual and language tokens. Existing token\npruning research for compressing VLTs mainly follows a single-modality-based\nscheme yet ignores the critical role of aligning different modalities for\nguiding the token pruning process, causing the important tokens for one\nmodality to be falsely pruned in another modality branch. Meanwhile, existing\nVLT pruning works also lack the flexibility to dynamically compress each layer\nbased on different input samples. To this end, we propose a novel framework\nnamed Multimodal Alignment-Guided Dynamic Token Pruning (MADTP) for\naccelerating various VLTs. Specifically, we first introduce a well-designed\nMulti-modality Alignment Guidance (MAG) module that can align features of the\nsame semantic concept from different modalities, to ensure the pruned tokens\nare less important for all modalities. We further design a novel Dynamic Token\nPruning (DTP) module, which can adaptively adjust the token compression ratio\nin each layer based on different input instances. Extensive experiments on\nvarious benchmarks demonstrate that MADTP significantly reduces the\ncomputational complexity of kinds of multimodal models while preserving\ncompetitive performance. Notably, when applied to the BLIP model in the NLVR2\ndataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance\ndegradation.\n", "link": "http://arxiv.org/abs/2403.02991v1", "date": "2024-03-05", "relevancy": 2.789, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5982}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5606}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5146}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MADTP%3A%20Multimodal%20Alignment-Guided%20Dynamic%20Token%20Pruning%20for%0A%20%20Accelerating%20Vision-Language%20Transformer&entry.906535625=Jianjian%20Cao%20and%20Peng%20Ye%20and%20Shengze%20Li%20and%20Chong%20Yu%20and%20Yansong%20Tang%20and%20Jiwen%20Lu%20and%20Tao%20Chen&entry.1292438233=%20%20Vision-Language%20Transformers%20%28VLTs%29%20have%20shown%20great%20success%20recently%2C%20but%0Aare%20meanwhile%20accompanied%20by%20heavy%20computation%20costs%2C%20where%20a%20major%20reason%20can%0Abe%20attributed%20to%20the%20large%20number%20of%20visual%20and%20language%20tokens.%20Existing%20token%0Apruning%20research%20for%20compressing%20VLTs%20mainly%20follows%20a%20single-modality-based%0Ascheme%20yet%20ignores%20the%20critical%20role%20of%20aligning%20different%20modalities%20for%0Aguiding%20the%20token%20pruning%20process%2C%20causing%20the%20important%20tokens%20for%20one%0Amodality%20to%20be%20falsely%20pruned%20in%20another%20modality%20branch.%20Meanwhile%2C%20existing%0AVLT%20pruning%20works%20also%20lack%20the%20flexibility%20to%20dynamically%20compress%20each%20layer%0Abased%20on%20different%20input%20samples.%20To%20this%20end%2C%20we%20propose%20a%20novel%20framework%0Anamed%20Multimodal%20Alignment-Guided%20Dynamic%20Token%20Pruning%20%28MADTP%29%20for%0Aaccelerating%20various%20VLTs.%20Specifically%2C%20we%20first%20introduce%20a%20well-designed%0AMulti-modality%20Alignment%20Guidance%20%28MAG%29%20module%20that%20can%20align%20features%20of%20the%0Asame%20semantic%20concept%20from%20different%20modalities%2C%20to%20ensure%20the%20pruned%20tokens%0Aare%20less%20important%20for%20all%20modalities.%20We%20further%20design%20a%20novel%20Dynamic%20Token%0APruning%20%28DTP%29%20module%2C%20which%20can%20adaptively%20adjust%20the%20token%20compression%20ratio%0Ain%20each%20layer%20based%20on%20different%20input%20instances.%20Extensive%20experiments%20on%0Avarious%20benchmarks%20demonstrate%20that%20MADTP%20significantly%20reduces%20the%0Acomputational%20complexity%20of%20kinds%20of%20multimodal%20models%20while%20preserving%0Acompetitive%20performance.%20Notably%2C%20when%20applied%20to%20the%20BLIP%20model%20in%20the%20NLVR2%0Adataset%2C%20MADTP%20can%20reduce%20the%20GFLOPs%20by%2080%25%20with%20less%20than%204%25%20performance%0Adegradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02991v1&entry.124074799=Read"},
{"title": "Recall-Oriented Continual Learning with Generative Adversarial\n  Meta-Model", "author": "Haneol Kang and Dong-Wan Choi", "abstract": "  The stability-plasticity dilemma is a major challenge in continual learning,\nas it involves balancing the conflicting objectives of maintaining performance\non previous tasks while learning new tasks. In this paper, we propose the\nrecall-oriented continual learning framework to address this challenge.\nInspired by the human brain's ability to separate the mechanisms responsible\nfor stability and plasticity, our framework consists of a two-level\narchitecture where an inference network effectively acquires new knowledge and\na generative network recalls past knowledge when necessary. In particular, to\nmaximize the stability of past knowledge, we investigate the complexity of\nknowledge depending on different representations, and thereby introducing\ngenerative adversarial meta-model (GAMM) that incrementally learns\ntask-specific parameters instead of input data samples of the task. Through our\nexperiments, we show that our framework not only effectively learns new\nknowledge without any disruption but also achieves high stability of previous\nknowledge in both task-aware and task-agnostic learning scenarios. Our code is\navailable at: https://github.com/bigdata-inha/recall-oriented-cl-framework.\n", "link": "http://arxiv.org/abs/2403.03082v1", "date": "2024-03-05", "relevancy": 2.6742, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.522}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recall-Oriented%20Continual%20Learning%20with%20Generative%20Adversarial%0A%20%20Meta-Model&entry.906535625=Haneol%20Kang%20and%20Dong-Wan%20Choi&entry.1292438233=%20%20The%20stability-plasticity%20dilemma%20is%20a%20major%20challenge%20in%20continual%20learning%2C%0Aas%20it%20involves%20balancing%20the%20conflicting%20objectives%20of%20maintaining%20performance%0Aon%20previous%20tasks%20while%20learning%20new%20tasks.%20In%20this%20paper%2C%20we%20propose%20the%0Arecall-oriented%20continual%20learning%20framework%20to%20address%20this%20challenge.%0AInspired%20by%20the%20human%20brain%27s%20ability%20to%20separate%20the%20mechanisms%20responsible%0Afor%20stability%20and%20plasticity%2C%20our%20framework%20consists%20of%20a%20two-level%0Aarchitecture%20where%20an%20inference%20network%20effectively%20acquires%20new%20knowledge%20and%0Aa%20generative%20network%20recalls%20past%20knowledge%20when%20necessary.%20In%20particular%2C%20to%0Amaximize%20the%20stability%20of%20past%20knowledge%2C%20we%20investigate%20the%20complexity%20of%0Aknowledge%20depending%20on%20different%20representations%2C%20and%20thereby%20introducing%0Agenerative%20adversarial%20meta-model%20%28GAMM%29%20that%20incrementally%20learns%0Atask-specific%20parameters%20instead%20of%20input%20data%20samples%20of%20the%20task.%20Through%20our%0Aexperiments%2C%20we%20show%20that%20our%20framework%20not%20only%20effectively%20learns%20new%0Aknowledge%20without%20any%20disruption%20but%20also%20achieves%20high%20stability%20of%20previous%0Aknowledge%20in%20both%20task-aware%20and%20task-agnostic%20learning%20scenarios.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/bigdata-inha/recall-oriented-cl-framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03082v1&entry.124074799=Read"},
{"title": "Design2Code: How Far Are We From Automating Front-End Engineering?", "author": "Chenglei Si and Yanzhe Zhang and Zhengyuan Yang and Ruibo Liu and Diyi Yang", "abstract": "  Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development, in which multimodal\nLLMs might directly convert visual designs into code implementations. In this\nwork, we formalize this as a Design2Code task and conduct comprehensive\nbenchmarking. Specifically, we manually curate a benchmark of 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations. We develop a suite of multimodal prompting\nmethods and show their effectiveness on GPT-4V and Gemini Pro Vision. We\nfurther finetune an open-source Design2Code-18B model that successfully matches\nthe performance of Gemini Pro Vision. Both human evaluation and automatic\nmetrics show that GPT-4V performs the best on this task compared to other\nmodels. Moreover, annotators think GPT-4V generated webpages can replace the\noriginal reference webpages in 49% of cases in terms of visual appearance and\ncontent; and perhaps surprisingly, in 64% of cases GPT-4V generated webpages\nare considered better than the original reference webpages. Our fine-grained\nbreak-down metrics indicate that open-source models mostly lag in recalling\nvisual elements from the input webpages and in generating correct layout\ndesigns, while aspects like text content and coloring can be drastically\nimproved with proper finetuning.\n", "link": "http://arxiv.org/abs/2403.03163v1", "date": "2024-03-05", "relevancy": 2.6707, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5614}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5257}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5153}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design2Code%3A%20How%20Far%20Are%20We%20From%20Automating%20Front-End%20Engineering%3F&entry.906535625=Chenglei%20Si%20and%20Yanzhe%20Zhang%20and%20Zhengyuan%20Yang%20and%20Ruibo%20Liu%20and%20Diyi%20Yang&entry.1292438233=%20%20Generative%20AI%20has%20made%20rapid%20advancements%20in%20recent%20years%2C%20achieving%0Aunprecedented%20capabilities%20in%20multimodal%20understanding%20and%20code%20generation.%0AThis%20can%20enable%20a%20new%20paradigm%20of%20front-end%20development%2C%20in%20which%20multimodal%0ALLMs%20might%20directly%20convert%20visual%20designs%20into%20code%20implementations.%20In%20this%0Awork%2C%20we%20formalize%20this%20as%20a%20Design2Code%20task%20and%20conduct%20comprehensive%0Abenchmarking.%20Specifically%2C%20we%20manually%20curate%20a%20benchmark%20of%20484%20diverse%0Areal-world%20webpages%20as%20test%20cases%20and%20develop%20a%20set%20of%20automatic%20evaluation%0Ametrics%20to%20assess%20how%20well%20current%20multimodal%20LLMs%20can%20generate%20the%20code%0Aimplementations%20that%20directly%20render%20into%20the%20given%20reference%20webpages%2C%20given%0Athe%20screenshots%20as%20input.%20We%20also%20complement%20automatic%20metrics%20with%0Acomprehensive%20human%20evaluations.%20We%20develop%20a%20suite%20of%20multimodal%20prompting%0Amethods%20and%20show%20their%20effectiveness%20on%20GPT-4V%20and%20Gemini%20Pro%20Vision.%20We%0Afurther%20finetune%20an%20open-source%20Design2Code-18B%20model%20that%20successfully%20matches%0Athe%20performance%20of%20Gemini%20Pro%20Vision.%20Both%20human%20evaluation%20and%20automatic%0Ametrics%20show%20that%20GPT-4V%20performs%20the%20best%20on%20this%20task%20compared%20to%20other%0Amodels.%20Moreover%2C%20annotators%20think%20GPT-4V%20generated%20webpages%20can%20replace%20the%0Aoriginal%20reference%20webpages%20in%2049%25%20of%20cases%20in%20terms%20of%20visual%20appearance%20and%0Acontent%3B%20and%20perhaps%20surprisingly%2C%20in%2064%25%20of%20cases%20GPT-4V%20generated%20webpages%0Aare%20considered%20better%20than%20the%20original%20reference%20webpages.%20Our%20fine-grained%0Abreak-down%20metrics%20indicate%20that%20open-source%20models%20mostly%20lag%20in%20recalling%0Avisual%20elements%20from%20the%20input%20webpages%20and%20in%20generating%20correct%20layout%0Adesigns%2C%20while%20aspects%20like%20text%20content%20and%20coloring%20can%20be%20drastically%0Aimproved%20with%20proper%20finetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03163v1&entry.124074799=Read"},
{"title": "Enhancing Long-Term Person Re-Identification Using Global, Local Body\n  Part, and Head Streams", "author": "Duy Tran Thanh and Yeejin Lee and Byeongkeun Kang", "abstract": "  This work addresses the task of long-term person re-identification.\nTypically, person re-identification assumes that people do not change their\nclothes, which limits its applications to short-term scenarios. To overcome\nthis limitation, we investigate long-term person re-identification, which\nconsiders both clothes-changing and clothes-consistent scenarios. In this\npaper, we propose a novel framework that effectively learns and utilizes both\nglobal and local information. The proposed framework consists of three streams:\nglobal, local body part, and head streams. The global and head streams encode\nidentity-relevant information from an entire image and a cropped image of the\nhead region, respectively. Both streams encode the most distinct, less\ndistinct, and average features using the combinations of adversarial erasing,\nmax pooling, and average pooling. The local body part stream extracts\nidentity-related information for each body part, allowing it to be compared\nwith the same body part from another image. Since body part annotations are not\navailable in re-identification datasets, pseudo-labels are generated using\nclustering. These labels are then utilized to train a body part segmentation\nhead in the local body part stream. The proposed framework is trained by\nbackpropagating the weighted summation of the identity classification loss, the\npair-based loss, and the pseudo body part segmentation loss. To demonstrate the\neffectiveness of the proposed method, we conducted experiments on three\npublicly available datasets (Celeb-reID, PRCC, and VC-Clothes). The\nexperimental results demonstrate that the proposed method outperforms the\nprevious state-of-the-art method.\n", "link": "http://arxiv.org/abs/2403.02892v1", "date": "2024-03-05", "relevancy": 2.6639, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5544}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5252}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5188}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Long-Term%20Person%20Re-Identification%20Using%20Global%2C%20Local%20Body%0A%20%20Part%2C%20and%20Head%20Streams&entry.906535625=Duy%20Tran%20Thanh%20and%20Yeejin%20Lee%20and%20Byeongkeun%20Kang&entry.1292438233=%20%20This%20work%20addresses%20the%20task%20of%20long-term%20person%20re-identification.%0ATypically%2C%20person%20re-identification%20assumes%20that%20people%20do%20not%20change%20their%0Aclothes%2C%20which%20limits%20its%20applications%20to%20short-term%20scenarios.%20To%20overcome%0Athis%20limitation%2C%20we%20investigate%20long-term%20person%20re-identification%2C%20which%0Aconsiders%20both%20clothes-changing%20and%20clothes-consistent%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20framework%20that%20effectively%20learns%20and%20utilizes%20both%0Aglobal%20and%20local%20information.%20The%20proposed%20framework%20consists%20of%20three%20streams%3A%0Aglobal%2C%20local%20body%20part%2C%20and%20head%20streams.%20The%20global%20and%20head%20streams%20encode%0Aidentity-relevant%20information%20from%20an%20entire%20image%20and%20a%20cropped%20image%20of%20the%0Ahead%20region%2C%20respectively.%20Both%20streams%20encode%20the%20most%20distinct%2C%20less%0Adistinct%2C%20and%20average%20features%20using%20the%20combinations%20of%20adversarial%20erasing%2C%0Amax%20pooling%2C%20and%20average%20pooling.%20The%20local%20body%20part%20stream%20extracts%0Aidentity-related%20information%20for%20each%20body%20part%2C%20allowing%20it%20to%20be%20compared%0Awith%20the%20same%20body%20part%20from%20another%20image.%20Since%20body%20part%20annotations%20are%20not%0Aavailable%20in%20re-identification%20datasets%2C%20pseudo-labels%20are%20generated%20using%0Aclustering.%20These%20labels%20are%20then%20utilized%20to%20train%20a%20body%20part%20segmentation%0Ahead%20in%20the%20local%20body%20part%20stream.%20The%20proposed%20framework%20is%20trained%20by%0Abackpropagating%20the%20weighted%20summation%20of%20the%20identity%20classification%20loss%2C%20the%0Apair-based%20loss%2C%20and%20the%20pseudo%20body%20part%20segmentation%20loss.%20To%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method%2C%20we%20conducted%20experiments%20on%20three%0Apublicly%20available%20datasets%20%28Celeb-reID%2C%20PRCC%2C%20and%20VC-Clothes%29.%20The%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20method%20outperforms%20the%0Aprevious%20state-of-the-art%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02892v1&entry.124074799=Read"},
{"title": "DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal\n  Forecasting", "author": "Hao Wu and Haomin Wen and Guibin Zhang and Yutong Xia and Kai Wang and Yuxuan Liang and Yu Zheng and Kun Wang", "abstract": "  The ever-increasing sensor service, though opening a precious path and\nproviding a deluge of earth system data for deep-learning-oriented earth\nscience, sadly introduce a daunting obstacle to their industrial level\ndeployment. Concretely, earth science systems rely heavily on the extensive\ndeployment of sensors, however, the data collection from sensors is constrained\nby complex geographical and social factors, making it challenging to achieve\ncomprehensive coverage and uniform deployment. To alleviate the obstacle,\ntraditional approaches to sensor deployment utilize specific algorithms to\ndesign and deploy sensors. These methods dynamically adjust the activation\ntimes of sensors to optimize the detection process across each sub-region.\nRegrettably, formulating an activation strategy generally based on historical\nobservations and geographic characteristics, which make the methods and\nresultant models were neither simple nor practical. Worse still, the complex\ntechnical design may ultimately lead to a model with weak generalizability. In\nthis paper, we introduce for the first time the concept of spatio-temporal data\ndynamic sparse training and are committed to adaptively, dynamically filtering\nimportant sensor distributions. To our knowledge, this is the first proposal\n(termed DynST) of an industry-level deployment optimization concept at the data\nlevel. However, due to the existence of the temporal dimension, pruning of\nspatio-temporal data may lead to conflicts at different timestamps. To achieve\nthis goal, we employ dynamic merge technology, along with ingenious dimensional\nmapping to mitigate potential impacts caused by the temporal aspect. During the\ntraining process, DynST utilize iterative pruning and sparse training,\nrepeatedly identifying and dynamically removing sensor perception areas that\ncontribute the least to future predictions.\n", "link": "http://arxiv.org/abs/2403.02914v1", "date": "2024-03-05", "relevancy": 2.6576, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5766}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5217}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4962}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynST%3A%20Dynamic%20Sparse%20Training%20for%20Resource-Constrained%20Spatio-Temporal%0A%20%20Forecasting&entry.906535625=Hao%20Wu%20and%20Haomin%20Wen%20and%20Guibin%20Zhang%20and%20Yutong%20Xia%20and%20Kai%20Wang%20and%20Yuxuan%20Liang%20and%20Yu%20Zheng%20and%20Kun%20Wang&entry.1292438233=%20%20The%20ever-increasing%20sensor%20service%2C%20though%20opening%20a%20precious%20path%20and%0Aproviding%20a%20deluge%20of%20earth%20system%20data%20for%20deep-learning-oriented%20earth%0Ascience%2C%20sadly%20introduce%20a%20daunting%20obstacle%20to%20their%20industrial%20level%0Adeployment.%20Concretely%2C%20earth%20science%20systems%20rely%20heavily%20on%20the%20extensive%0Adeployment%20of%20sensors%2C%20however%2C%20the%20data%20collection%20from%20sensors%20is%20constrained%0Aby%20complex%20geographical%20and%20social%20factors%2C%20making%20it%20challenging%20to%20achieve%0Acomprehensive%20coverage%20and%20uniform%20deployment.%20To%20alleviate%20the%20obstacle%2C%0Atraditional%20approaches%20to%20sensor%20deployment%20utilize%20specific%20algorithms%20to%0Adesign%20and%20deploy%20sensors.%20These%20methods%20dynamically%20adjust%20the%20activation%0Atimes%20of%20sensors%20to%20optimize%20the%20detection%20process%20across%20each%20sub-region.%0ARegrettably%2C%20formulating%20an%20activation%20strategy%20generally%20based%20on%20historical%0Aobservations%20and%20geographic%20characteristics%2C%20which%20make%20the%20methods%20and%0Aresultant%20models%20were%20neither%20simple%20nor%20practical.%20Worse%20still%2C%20the%20complex%0Atechnical%20design%20may%20ultimately%20lead%20to%20a%20model%20with%20weak%20generalizability.%20In%0Athis%20paper%2C%20we%20introduce%20for%20the%20first%20time%20the%20concept%20of%20spatio-temporal%20data%0Adynamic%20sparse%20training%20and%20are%20committed%20to%20adaptively%2C%20dynamically%20filtering%0Aimportant%20sensor%20distributions.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20proposal%0A%28termed%20DynST%29%20of%20an%20industry-level%20deployment%20optimization%20concept%20at%20the%20data%0Alevel.%20However%2C%20due%20to%20the%20existence%20of%20the%20temporal%20dimension%2C%20pruning%20of%0Aspatio-temporal%20data%20may%20lead%20to%20conflicts%20at%20different%20timestamps.%20To%20achieve%0Athis%20goal%2C%20we%20employ%20dynamic%20merge%20technology%2C%20along%20with%20ingenious%20dimensional%0Amapping%20to%20mitigate%20potential%20impacts%20caused%20by%20the%20temporal%20aspect.%20During%20the%0Atraining%20process%2C%20DynST%20utilize%20iterative%20pruning%20and%20sparse%20training%2C%0Arepeatedly%20identifying%20and%20dynamically%20removing%20sensor%20perception%20areas%20that%0Acontribute%20the%20least%20to%20future%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02914v1&entry.124074799=Read"},
{"title": "Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for\n  Low-Light Image Enhancement", "author": "Jinhong He and Minglong Xue and Zhipu Liu and Chengyun Song and Senming Zhong", "abstract": "  Diffusion model-based low-light image enhancement methods rely heavily on\npaired training data, leading to limited extensive application. Meanwhile,\nexisting unsupervised methods lack effective bridging capabilities for unknown\ndegradation. To address these limitations, we propose a novel zero-reference\nlighting estimation diffusion model for low-light image enhancement called\nZero-LED. It utilizes the stable convergence ability of diffusion models to\nbridge the gap between low-light domains and real normal-light domains and\nsuccessfully alleviates the dependence on pairwise training data via\nzero-reference learning. Specifically, we first design the initial optimization\nnetwork to preprocess the input image and implement bidirectional constraints\nbetween the diffusion model and the initial optimization network through\nmultiple objective functions. Subsequently, the degradation factors of the\nreal-world scene are optimized iteratively to achieve effective light\nenhancement. In addition, we explore a frequency-domain based and semantically\nguided appearance reconstruction module that encourages feature alignment of\nthe recovered image at a fine-grained level and satisfies subjective\nexpectations. Finally, extensive experiments demonstrate the superiority of our\napproach to other state-of-the-art methods and more significant generalization\ncapabilities. We will open the source code upon acceptance of the paper.\n", "link": "http://arxiv.org/abs/2403.02879v1", "date": "2024-03-05", "relevancy": 2.6312, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5457}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5167}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5163}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-LED%3A%20Zero-Reference%20Lighting%20Estimation%20Diffusion%20Model%20for%0A%20%20Low-Light%20Image%20Enhancement&entry.906535625=Jinhong%20He%20and%20Minglong%20Xue%20and%20Zhipu%20Liu%20and%20Chengyun%20Song%20and%20Senming%20Zhong&entry.1292438233=%20%20Diffusion%20model-based%20low-light%20image%20enhancement%20methods%20rely%20heavily%20on%0Apaired%20training%20data%2C%20leading%20to%20limited%20extensive%20application.%20Meanwhile%2C%0Aexisting%20unsupervised%20methods%20lack%20effective%20bridging%20capabilities%20for%20unknown%0Adegradation.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20zero-reference%0Alighting%20estimation%20diffusion%20model%20for%20low-light%20image%20enhancement%20called%0AZero-LED.%20It%20utilizes%20the%20stable%20convergence%20ability%20of%20diffusion%20models%20to%0Abridge%20the%20gap%20between%20low-light%20domains%20and%20real%20normal-light%20domains%20and%0Asuccessfully%20alleviates%20the%20dependence%20on%20pairwise%20training%20data%20via%0Azero-reference%20learning.%20Specifically%2C%20we%20first%20design%20the%20initial%20optimization%0Anetwork%20to%20preprocess%20the%20input%20image%20and%20implement%20bidirectional%20constraints%0Abetween%20the%20diffusion%20model%20and%20the%20initial%20optimization%20network%20through%0Amultiple%20objective%20functions.%20Subsequently%2C%20the%20degradation%20factors%20of%20the%0Areal-world%20scene%20are%20optimized%20iteratively%20to%20achieve%20effective%20light%0Aenhancement.%20In%20addition%2C%20we%20explore%20a%20frequency-domain%20based%20and%20semantically%0Aguided%20appearance%20reconstruction%20module%20that%20encourages%20feature%20alignment%20of%0Athe%20recovered%20image%20at%20a%20fine-grained%20level%20and%20satisfies%20subjective%0Aexpectations.%20Finally%2C%20extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%0Aapproach%20to%20other%20state-of-the-art%20methods%20and%20more%20significant%20generalization%0Acapabilities.%20We%20will%20open%20the%20source%20code%20upon%20acceptance%20of%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02879v1&entry.124074799=Read"},
{"title": "Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source\n  Localization", "author": "Yuxin Guo and Shijie Ma and Yuhao Zhao and Hu Su and Wei Zou", "abstract": "  Audio-Visual Source Localization (AVSL) is the task of identifying specific\nsounding objects in the scene given audio cues. In our work, we focus on\nsemi-supervised AVSL with pseudo-labeling. To address the issues with vanilla\nhard pseudo-labels including bias accumulation, noise sensitivity, and\ninstability, we propose a novel method named Cross Pseudo-Labeling (XPL),\nwherein two models learn from each other with the cross-refine mechanism to\navoid bias accumulation. We equip XPL with two effective components. Firstly,\nthe soft pseudo-labels with sharpening and pseudo-label exponential moving\naverage mechanisms enable models to achieve gradual self-improvement and ensure\nstable training. Secondly, the curriculum data selection module adaptively\nselects pseudo-labels with high quality during training to mitigate potential\nbias. Experimental results demonstrate that XPL significantly outperforms\nexisting methods, achieving state-of-the-art performance while effectively\nmitigating confirmation bias and ensuring training stability.\n", "link": "http://arxiv.org/abs/2403.03095v1", "date": "2024-03-05", "relevancy": 2.6274, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5329}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross%20Pseudo-Labeling%20for%20Semi-Supervised%20Audio-Visual%20Source%0A%20%20Localization&entry.906535625=Yuxin%20Guo%20and%20Shijie%20Ma%20and%20Yuhao%20Zhao%20and%20Hu%20Su%20and%20Wei%20Zou&entry.1292438233=%20%20Audio-Visual%20Source%20Localization%20%28AVSL%29%20is%20the%20task%20of%20identifying%20specific%0Asounding%20objects%20in%20the%20scene%20given%20audio%20cues.%20In%20our%20work%2C%20we%20focus%20on%0Asemi-supervised%20AVSL%20with%20pseudo-labeling.%20To%20address%20the%20issues%20with%20vanilla%0Ahard%20pseudo-labels%20including%20bias%20accumulation%2C%20noise%20sensitivity%2C%20and%0Ainstability%2C%20we%20propose%20a%20novel%20method%20named%20Cross%20Pseudo-Labeling%20%28XPL%29%2C%0Awherein%20two%20models%20learn%20from%20each%20other%20with%20the%20cross-refine%20mechanism%20to%0Aavoid%20bias%20accumulation.%20We%20equip%20XPL%20with%20two%20effective%20components.%20Firstly%2C%0Athe%20soft%20pseudo-labels%20with%20sharpening%20and%20pseudo-label%20exponential%20moving%0Aaverage%20mechanisms%20enable%20models%20to%20achieve%20gradual%20self-improvement%20and%20ensure%0Astable%20training.%20Secondly%2C%20the%20curriculum%20data%20selection%20module%20adaptively%0Aselects%20pseudo-labels%20with%20high%20quality%20during%20training%20to%20mitigate%20potential%0Abias.%20Experimental%20results%20demonstrate%20that%20XPL%20significantly%20outperforms%0Aexisting%20methods%2C%20achieving%20state-of-the-art%20performance%20while%20effectively%0Amitigating%20confirmation%20bias%20and%20ensuring%20training%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03095v1&entry.124074799=Read"},
{"title": "Zero-shot sketch-based remote sensing image retrieval based on\n  multi-level and attention-guided tokenization", "author": "Bo Yang and Chen Wang and Xiaoshuang Ma and Beiping Song and Zhuang Liu", "abstract": "  Effectively and efficiently retrieving images from remote sensing databases\nis a critical challenge in the realm of remote sensing big data. Utilizing\nhand-drawn sketches as retrieval inputs offers intuitive and user-friendly\nadvantages, yet the potential of multi-level feature integration from sketches\nremains underexplored, leading to suboptimal retrieval performance. To address\nthis gap, our study introduces a novel zero-shot, sketch-based retrieval method\nfor remote sensing images, leveraging multi-level feature extraction,\nself-attention-guided tokenization and filtering, and cross-modality attention\nupdate. This approach employs only vision information and does not require\nsemantic knowledge concerning the sketch and image. It starts by employing\nmulti-level self-attention guided feature extraction to tokenize the query\nsketches, as well as self-attention feature extraction to tokenize the\ncandidate images. It then employs cross-attention mechanisms to establish token\ncorrespondence between these two modalities, facilitating the computation of\nsketch-to-image similarity. Our method significantly outperforms existing\nsketch-based remote sensing image retrieval techniques, as evidenced by tests\non multiple datasets. Notably, it also exhibits robust zero-shot learning\ncapabilities and strong generalizability in handling unseen categories and\nnovel remote sensing data. The method's scalability can be further enhanced by\nthe pre-calculation of retrieval tokens for all candidate images in a database.\nThis research underscores the significant potential of multi-level,\nattention-guided tokenization in cross-modal remote sensing image retrieval.\nFor broader accessibility and research facilitation, we have made the code and\ndataset used in this study publicly available online. Code and dataset are\navailable at https://github.com/Snowstormfly/Cross-modal-retrieval-MLAGT.\n", "link": "http://arxiv.org/abs/2402.02141v2", "date": "2024-03-05", "relevancy": 2.6087, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5228}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5113}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20sketch-based%20remote%20sensing%20image%20retrieval%20based%20on%0A%20%20multi-level%20and%20attention-guided%20tokenization&entry.906535625=Bo%20Yang%20and%20Chen%20Wang%20and%20Xiaoshuang%20Ma%20and%20Beiping%20Song%20and%20Zhuang%20Liu&entry.1292438233=%20%20Effectively%20and%20efficiently%20retrieving%20images%20from%20remote%20sensing%20databases%0Ais%20a%20critical%20challenge%20in%20the%20realm%20of%20remote%20sensing%20big%20data.%20Utilizing%0Ahand-drawn%20sketches%20as%20retrieval%20inputs%20offers%20intuitive%20and%20user-friendly%0Aadvantages%2C%20yet%20the%20potential%20of%20multi-level%20feature%20integration%20from%20sketches%0Aremains%20underexplored%2C%20leading%20to%20suboptimal%20retrieval%20performance.%20To%20address%0Athis%20gap%2C%20our%20study%20introduces%20a%20novel%20zero-shot%2C%20sketch-based%20retrieval%20method%0Afor%20remote%20sensing%20images%2C%20leveraging%20multi-level%20feature%20extraction%2C%0Aself-attention-guided%20tokenization%20and%20filtering%2C%20and%20cross-modality%20attention%0Aupdate.%20This%20approach%20employs%20only%20vision%20information%20and%20does%20not%20require%0Asemantic%20knowledge%20concerning%20the%20sketch%20and%20image.%20It%20starts%20by%20employing%0Amulti-level%20self-attention%20guided%20feature%20extraction%20to%20tokenize%20the%20query%0Asketches%2C%20as%20well%20as%20self-attention%20feature%20extraction%20to%20tokenize%20the%0Acandidate%20images.%20It%20then%20employs%20cross-attention%20mechanisms%20to%20establish%20token%0Acorrespondence%20between%20these%20two%20modalities%2C%20facilitating%20the%20computation%20of%0Asketch-to-image%20similarity.%20Our%20method%20significantly%20outperforms%20existing%0Asketch-based%20remote%20sensing%20image%20retrieval%20techniques%2C%20as%20evidenced%20by%20tests%0Aon%20multiple%20datasets.%20Notably%2C%20it%20also%20exhibits%20robust%20zero-shot%20learning%0Acapabilities%20and%20strong%20generalizability%20in%20handling%20unseen%20categories%20and%0Anovel%20remote%20sensing%20data.%20The%20method%27s%20scalability%20can%20be%20further%20enhanced%20by%0Athe%20pre-calculation%20of%20retrieval%20tokens%20for%20all%20candidate%20images%20in%20a%20database.%0AThis%20research%20underscores%20the%20significant%20potential%20of%20multi-level%2C%0Aattention-guided%20tokenization%20in%20cross-modal%20remote%20sensing%20image%20retrieval.%0AFor%20broader%20accessibility%20and%20research%20facilitation%2C%20we%20have%20made%20the%20code%20and%0Adataset%20used%20in%20this%20study%20publicly%20available%20online.%20Code%20and%20dataset%20are%0Aavailable%20at%20https%3A//github.com/Snowstormfly/Cross-modal-retrieval-MLAGT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02141v2&entry.124074799=Read"},
{"title": "Codebook-enabled Generative End-to-end Semantic Communication Powered by\n  Transformer", "author": "Peigen Ye and Yaping Sun and Shumin Yao and Hao Chen and Xiaodong Xu and Shuguang Cui", "abstract": "  Codebook-based generative semantic communication attracts increasing\nattention, since only indices are required to be transmitted when the codebook\nis shared between transmitter and receiver. However, due to the fact that the\nsemantic relations among code vectors are not necessarily related to the\ndistance of the corresponding code indices, the performance of the\ncodebook-enabled semantic communication system is susceptible to the channel\nnoise. Thus, how to improve the system robustness against the noise requires\ncareful design. This paper proposes a robust codebook-assisted image semantic\ncommunication system, where semantic codec and codebook are first jointly\nconstructed, and then vector-to-index transformer is designed guided by the\ncodebook to eliminate the effects of channel noise, and achieve image\ngeneration. Thanks to the assistance of the high-quality codebook to the\nTransformer, the generated images at the receiver outperform those of the\ncompared methods in terms of visual perception. In the end, numerical results\nand generated images demonstrate the advantages of the generative semantic\ncommunication method over JPEG+LDPC and traditional joint source channel coding\n(JSCC) methods.\n", "link": "http://arxiv.org/abs/2402.16868v2", "date": "2024-03-05", "relevancy": 2.6067, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5644}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5176}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4819}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Codebook-enabled%20Generative%20End-to-end%20Semantic%20Communication%20Powered%20by%0A%20%20Transformer&entry.906535625=Peigen%20Ye%20and%20Yaping%20Sun%20and%20Shumin%20Yao%20and%20Hao%20Chen%20and%20Xiaodong%20Xu%20and%20Shuguang%20Cui&entry.1292438233=%20%20Codebook-based%20generative%20semantic%20communication%20attracts%20increasing%0Aattention%2C%20since%20only%20indices%20are%20required%20to%20be%20transmitted%20when%20the%20codebook%0Ais%20shared%20between%20transmitter%20and%20receiver.%20However%2C%20due%20to%20the%20fact%20that%20the%0Asemantic%20relations%20among%20code%20vectors%20are%20not%20necessarily%20related%20to%20the%0Adistance%20of%20the%20corresponding%20code%20indices%2C%20the%20performance%20of%20the%0Acodebook-enabled%20semantic%20communication%20system%20is%20susceptible%20to%20the%20channel%0Anoise.%20Thus%2C%20how%20to%20improve%20the%20system%20robustness%20against%20the%20noise%20requires%0Acareful%20design.%20This%20paper%20proposes%20a%20robust%20codebook-assisted%20image%20semantic%0Acommunication%20system%2C%20where%20semantic%20codec%20and%20codebook%20are%20first%20jointly%0Aconstructed%2C%20and%20then%20vector-to-index%20transformer%20is%20designed%20guided%20by%20the%0Acodebook%20to%20eliminate%20the%20effects%20of%20channel%20noise%2C%20and%20achieve%20image%0Ageneration.%20Thanks%20to%20the%20assistance%20of%20the%20high-quality%20codebook%20to%20the%0ATransformer%2C%20the%20generated%20images%20at%20the%20receiver%20outperform%20those%20of%20the%0Acompared%20methods%20in%20terms%20of%20visual%20perception.%20In%20the%20end%2C%20numerical%20results%0Aand%20generated%20images%20demonstrate%20the%20advantages%20of%20the%20generative%20semantic%0Acommunication%20method%20over%20JPEG%2BLDPC%20and%20traditional%20joint%20source%20channel%20coding%0A%28JSCC%29%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16868v2&entry.124074799=Read"},
{"title": "Optimising Event-Driven Spiking Neural Network with Regularisation and\n  Cutoff", "author": "Dengyu Wu and Gaojie Jin and Han Yu and Xinping Yi and Xiaowei Huang", "abstract": "  Spiking neural network (SNN), next generation of artificial neural network\n(ANN) that more closely mimic natural neural networks offers promising\nimprovements in computational efficiency. However, current SNN training\nmethodologies predominantly employ a fixed timestep approach, overlooking the\npotential of dynamic inference in SNN. In this paper, we strengthen the\nmarriage between SNN and event-driven processing with a proposal to consider\ncutoff in SNN, which can terminate SNN anytime during the inference to achieve\nefficient inference. Two novel optimisation techniques are presented to achieve\ninference efficient SNN: a Top-K cutoff and a regularisation. The Top-K cutoff\ntechnique optimises the inference of SNN, and the regularisation are proposed\nto affect the training and construct SNN with optimised performance for cutoff.\nWe conduct an extensive set of experiments on multiple benchmark frame-based\ndatsets, such as Cifar10/100, Tiny-ImageNet and event-based datasets, including\nCIFAR10-DVS, N-Caltech101 and DVS128 Gesture. The experimental results\ndemonstrate the effectiveness of our techniques in both ANN-to-SNN conversion\nand direct training, affirming their compatibility and potential benefits in\nenhancing accuracy and reducing inference timestep when integrated with\nexisting methods. Code available:\nhttps://github.com/Dengyu-Wu/SNN-Regularisation-Cutoff\n", "link": "http://arxiv.org/abs/2301.09522v3", "date": "2024-03-05", "relevancy": 2.5475, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6079}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4732}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4474}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimising%20Event-Driven%20Spiking%20Neural%20Network%20with%20Regularisation%20and%0A%20%20Cutoff&entry.906535625=Dengyu%20Wu%20and%20Gaojie%20Jin%20and%20Han%20Yu%20and%20Xinping%20Yi%20and%20Xiaowei%20Huang&entry.1292438233=%20%20Spiking%20neural%20network%20%28SNN%29%2C%20next%20generation%20of%20artificial%20neural%20network%0A%28ANN%29%20that%20more%20closely%20mimic%20natural%20neural%20networks%20offers%20promising%0Aimprovements%20in%20computational%20efficiency.%20However%2C%20current%20SNN%20training%0Amethodologies%20predominantly%20employ%20a%20fixed%20timestep%20approach%2C%20overlooking%20the%0Apotential%20of%20dynamic%20inference%20in%20SNN.%20In%20this%20paper%2C%20we%20strengthen%20the%0Amarriage%20between%20SNN%20and%20event-driven%20processing%20with%20a%20proposal%20to%20consider%0Acutoff%20in%20SNN%2C%20which%20can%20terminate%20SNN%20anytime%20during%20the%20inference%20to%20achieve%0Aefficient%20inference.%20Two%20novel%20optimisation%20techniques%20are%20presented%20to%20achieve%0Ainference%20efficient%20SNN%3A%20a%20Top-K%20cutoff%20and%20a%20regularisation.%20The%20Top-K%20cutoff%0Atechnique%20optimises%20the%20inference%20of%20SNN%2C%20and%20the%20regularisation%20are%20proposed%0Ato%20affect%20the%20training%20and%20construct%20SNN%20with%20optimised%20performance%20for%20cutoff.%0AWe%20conduct%20an%20extensive%20set%20of%20experiments%20on%20multiple%20benchmark%20frame-based%0Adatsets%2C%20such%20as%20Cifar10/100%2C%20Tiny-ImageNet%20and%20event-based%20datasets%2C%20including%0ACIFAR10-DVS%2C%20N-Caltech101%20and%20DVS128%20Gesture.%20The%20experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20techniques%20in%20both%20ANN-to-SNN%20conversion%0Aand%20direct%20training%2C%20affirming%20their%20compatibility%20and%20potential%20benefits%20in%0Aenhancing%20accuracy%20and%20reducing%20inference%20timestep%20when%20integrated%20with%0Aexisting%20methods.%20Code%20available%3A%0Ahttps%3A//github.com/Dengyu-Wu/SNN-Regularisation-Cutoff%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.09522v3&entry.124074799=Read"},
{"title": "A Generalized Neural Diffusion Framework on Graphs", "author": "Yibo Li and Xiao Wang and Hongrui Liu and Chuan Shi", "abstract": "  Recent studies reveal the connection between GNNs and the diffusion process,\nwhich motivates many diffusion-based GNNs to be proposed. However, since these\ntwo mechanisms are closely related, one fundamental question naturally arises:\nIs there a general diffusion framework that can formally unify these GNNs? The\nanswer to this question can not only deepen our understanding of the learning\nprocess of GNNs, but also may open a new door to design a broad new class of\nGNNs. In this paper, we propose a general diffusion equation framework with the\nfidelity term, which formally establishes the relationship between the\ndiffusion process with more GNNs. Meanwhile, with this framework, we identify\none characteristic of graph diffusion networks, i.e., the current neural\ndiffusion process only corresponds to the first-order diffusion equation.\nHowever, by an experimental investigation, we show that the labels of\nhigh-order neighbors actually exhibit monophily property, which induces the\nsimilarity based on labels among high-order neighbors without requiring the\nsimilarity among first-order neighbors. This discovery motives to design a new\nhigh-order neighbor-aware diffusion equation, and derive a new type of graph\ndiffusion network (HiD-Net) based on the framework. With the high-order\ndiffusion equation, HiD-Net is more robust against attacks and works on both\nhomophily and heterophily graphs. We not only theoretically analyze the\nrelation between HiD-Net with high-order random walk, but also provide a\ntheoretical convergence guarantee. Extensive experimental results well\ndemonstrate the effectiveness of HiD-Net over state-of-the-art graph diffusion\nnetworks.\n", "link": "http://arxiv.org/abs/2312.08616v4", "date": "2024-03-05", "relevancy": 2.5046, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5457}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4817}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4754}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generalized%20Neural%20Diffusion%20Framework%20on%20Graphs&entry.906535625=Yibo%20Li%20and%20Xiao%20Wang%20and%20Hongrui%20Liu%20and%20Chuan%20Shi&entry.1292438233=%20%20Recent%20studies%20reveal%20the%20connection%20between%20GNNs%20and%20the%20diffusion%20process%2C%0Awhich%20motivates%20many%20diffusion-based%20GNNs%20to%20be%20proposed.%20However%2C%20since%20these%0Atwo%20mechanisms%20are%20closely%20related%2C%20one%20fundamental%20question%20naturally%20arises%3A%0AIs%20there%20a%20general%20diffusion%20framework%20that%20can%20formally%20unify%20these%20GNNs%3F%20The%0Aanswer%20to%20this%20question%20can%20not%20only%20deepen%20our%20understanding%20of%20the%20learning%0Aprocess%20of%20GNNs%2C%20but%20also%20may%20open%20a%20new%20door%20to%20design%20a%20broad%20new%20class%20of%0AGNNs.%20In%20this%20paper%2C%20we%20propose%20a%20general%20diffusion%20equation%20framework%20with%20the%0Afidelity%20term%2C%20which%20formally%20establishes%20the%20relationship%20between%20the%0Adiffusion%20process%20with%20more%20GNNs.%20Meanwhile%2C%20with%20this%20framework%2C%20we%20identify%0Aone%20characteristic%20of%20graph%20diffusion%20networks%2C%20i.e.%2C%20the%20current%20neural%0Adiffusion%20process%20only%20corresponds%20to%20the%20first-order%20diffusion%20equation.%0AHowever%2C%20by%20an%20experimental%20investigation%2C%20we%20show%20that%20the%20labels%20of%0Ahigh-order%20neighbors%20actually%20exhibit%20monophily%20property%2C%20which%20induces%20the%0Asimilarity%20based%20on%20labels%20among%20high-order%20neighbors%20without%20requiring%20the%0Asimilarity%20among%20first-order%20neighbors.%20This%20discovery%20motives%20to%20design%20a%20new%0Ahigh-order%20neighbor-aware%20diffusion%20equation%2C%20and%20derive%20a%20new%20type%20of%20graph%0Adiffusion%20network%20%28HiD-Net%29%20based%20on%20the%20framework.%20With%20the%20high-order%0Adiffusion%20equation%2C%20HiD-Net%20is%20more%20robust%20against%20attacks%20and%20works%20on%20both%0Ahomophily%20and%20heterophily%20graphs.%20We%20not%20only%20theoretically%20analyze%20the%0Arelation%20between%20HiD-Net%20with%20high-order%20random%20walk%2C%20but%20also%20provide%20a%0Atheoretical%20convergence%20guarantee.%20Extensive%20experimental%20results%20well%0Ademonstrate%20the%20effectiveness%20of%20HiD-Net%20over%20state-of-the-art%20graph%20diffusion%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08616v4&entry.124074799=Read"},
{"title": "Zero-Shot Cross-Lingual Document-Level Event Causality Identification\n  with Heterogeneous Graph Contrastive Transfer Learning", "author": "Zhitao He and Pengfei Cao and Yubo Chen and Kang Liu and Zhiqiang Zhang and Mengshu Sun and Jun Zhao", "abstract": "  Event Causality Identification (ECI) refers to detect causal relations\nbetween events in texts. However, most existing studies focus on sentence-level\nECI with high-resource language, leaving more challenging document-level ECI\n(DECI) with low-resource languages under-explored. In this paper, we propose a\nHeterogeneous Graph Interaction Model with Multi-granularity Contrastive\nTransfer Learning (GIMC) for zero-shot cross-lingual document-level ECI.\nSpecifically, we introduce a heterogeneous graph interaction network to model\nthe long-distance dependencies between events that are scattered over document.\nThen, to improve cross-lingual transferability of causal knowledge learned from\nsource language, we propose a multi-granularity contrastive transfer learning\nmodule to align the causal representations across languages. Extensive\nexperiments show our framework outperforms previous state-of-the-art model by\n9.4% and 8.2% of average F1 score on monolingual and multilingual scenarios\nrespectively. Notably, in multilingual scenario, our zero-shot framework even\nexceeds GPT-3.5 with few-shot learning by 24.3% in overall performance.\n", "link": "http://arxiv.org/abs/2403.02893v1", "date": "2024-03-05", "relevancy": 2.4891, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5088}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4995}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Cross-Lingual%20Document-Level%20Event%20Causality%20Identification%0A%20%20with%20Heterogeneous%20Graph%20Contrastive%20Transfer%20Learning&entry.906535625=Zhitao%20He%20and%20Pengfei%20Cao%20and%20Yubo%20Chen%20and%20Kang%20Liu%20and%20Zhiqiang%20Zhang%20and%20Mengshu%20Sun%20and%20Jun%20Zhao&entry.1292438233=%20%20Event%20Causality%20Identification%20%28ECI%29%20refers%20to%20detect%20causal%20relations%0Abetween%20events%20in%20texts.%20However%2C%20most%20existing%20studies%20focus%20on%20sentence-level%0AECI%20with%20high-resource%20language%2C%20leaving%20more%20challenging%20document-level%20ECI%0A%28DECI%29%20with%20low-resource%20languages%20under-explored.%20In%20this%20paper%2C%20we%20propose%20a%0AHeterogeneous%20Graph%20Interaction%20Model%20with%20Multi-granularity%20Contrastive%0ATransfer%20Learning%20%28GIMC%29%20for%20zero-shot%20cross-lingual%20document-level%20ECI.%0ASpecifically%2C%20we%20introduce%20a%20heterogeneous%20graph%20interaction%20network%20to%20model%0Athe%20long-distance%20dependencies%20between%20events%20that%20are%20scattered%20over%20document.%0AThen%2C%20to%20improve%20cross-lingual%20transferability%20of%20causal%20knowledge%20learned%20from%0Asource%20language%2C%20we%20propose%20a%20multi-granularity%20contrastive%20transfer%20learning%0Amodule%20to%20align%20the%20causal%20representations%20across%20languages.%20Extensive%0Aexperiments%20show%20our%20framework%20outperforms%20previous%20state-of-the-art%20model%20by%0A9.4%25%20and%208.2%25%20of%20average%20F1%20score%20on%20monolingual%20and%20multilingual%20scenarios%0Arespectively.%20Notably%2C%20in%20multilingual%20scenario%2C%20our%20zero-shot%20framework%20even%0Aexceeds%20GPT-3.5%20with%20few-shot%20learning%20by%2024.3%25%20in%20overall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02893v1&entry.124074799=Read"},
{"title": "Neural Image Compression with Text-guided Encoding for both Pixel-level\n  and Perceptual Fidelity", "author": "Hagyeong Lee and Minkyu Kim and Jun-Hyuk Kim and Seungeon Kim and Dokwan Oh and Jaeho Lee", "abstract": "  Recent advances in text-guided image compression have shown great potential\nto enhance the perceptual quality of reconstructed images. These methods,\nhowever, tend to have significantly degraded pixel-wise fidelity, limiting\ntheir practicality. To fill this gap, we develop a new text-guided image\ncompression algorithm that achieves both high perceptual and pixel-wise\nfidelity. In particular, we propose a compression framework that leverages text\ninformation mainly by text-adaptive encoding and training with joint image-text\nloss. By doing so, we avoid decoding based on text-guided generative models --\nknown for high generative diversity -- and effectively utilize the semantic\ninformation of text at a global level. Experimental results on various datasets\nshow that our method can achieve high pixel-level and perceptual quality, with\neither human- or machine-generated captions. In particular, our method\noutperforms all baselines in terms of LPIPS, with some room for even more\nimprovements when we use more carefully generated captions.\n", "link": "http://arxiv.org/abs/2403.02944v1", "date": "2024-03-05", "relevancy": 2.467, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5055}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4878}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4869}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Image%20Compression%20with%20Text-guided%20Encoding%20for%20both%20Pixel-level%0A%20%20and%20Perceptual%20Fidelity&entry.906535625=Hagyeong%20Lee%20and%20Minkyu%20Kim%20and%20Jun-Hyuk%20Kim%20and%20Seungeon%20Kim%20and%20Dokwan%20Oh%20and%20Jaeho%20Lee&entry.1292438233=%20%20Recent%20advances%20in%20text-guided%20image%20compression%20have%20shown%20great%20potential%0Ato%20enhance%20the%20perceptual%20quality%20of%20reconstructed%20images.%20These%20methods%2C%0Ahowever%2C%20tend%20to%20have%20significantly%20degraded%20pixel-wise%20fidelity%2C%20limiting%0Atheir%20practicality.%20To%20fill%20this%20gap%2C%20we%20develop%20a%20new%20text-guided%20image%0Acompression%20algorithm%20that%20achieves%20both%20high%20perceptual%20and%20pixel-wise%0Afidelity.%20In%20particular%2C%20we%20propose%20a%20compression%20framework%20that%20leverages%20text%0Ainformation%20mainly%20by%20text-adaptive%20encoding%20and%20training%20with%20joint%20image-text%0Aloss.%20By%20doing%20so%2C%20we%20avoid%20decoding%20based%20on%20text-guided%20generative%20models%20--%0Aknown%20for%20high%20generative%20diversity%20--%20and%20effectively%20utilize%20the%20semantic%0Ainformation%20of%20text%20at%20a%20global%20level.%20Experimental%20results%20on%20various%20datasets%0Ashow%20that%20our%20method%20can%20achieve%20high%20pixel-level%20and%20perceptual%20quality%2C%20with%0Aeither%20human-%20or%20machine-generated%20captions.%20In%20particular%2C%20our%20method%0Aoutperforms%20all%20baselines%20in%20terms%20of%20LPIPS%2C%20with%20some%20room%20for%20even%20more%0Aimprovements%20when%20we%20use%20more%20carefully%20generated%20captions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02944v1&entry.124074799=Read"},
{"title": "Volumetric Semantically Consistent 3D Panoptic Mapping", "author": "Yang Miao and Iro Armeni and Marc Pollefeys and Daniel Barath", "abstract": "  We introduce an online 2D-to-3D semantic instance mapping algorithm aimed at\ngenerating comprehensive, accurate, and efficient semantic 3D maps suitable for\nautonomous agents in unstructured environments. The proposed approach is based\non a Voxel-TSDF representation used in recent algorithms. It introduces novel\nways of integrating semantic prediction confidence during mapping, producing\nsemantic and instance-consistent 3D regions. Further improvements are achieved\nby graph optimization-based semantic labeling and instance refinement. The\nproposed method achieves accuracy superior to the state of the art on public\nlarge-scale datasets, improving on a number of widely used metrics. We also\nhighlight a downfall in the evaluation of recent studies: using the ground\ntruth trajectory as input instead of a SLAM-estimated one substantially affects\nthe accuracy, creating a large gap between the reported results and the actual\nperformance on real-world data.\n", "link": "http://arxiv.org/abs/2309.14737v2", "date": "2024-03-05", "relevancy": 2.466, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6586}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5956}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5827}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Volumetric%20Semantically%20Consistent%203D%20Panoptic%20Mapping&entry.906535625=Yang%20Miao%20and%20Iro%20Armeni%20and%20Marc%20Pollefeys%20and%20Daniel%20Barath&entry.1292438233=%20%20We%20introduce%20an%20online%202D-to-3D%20semantic%20instance%20mapping%20algorithm%20aimed%20at%0Agenerating%20comprehensive%2C%20accurate%2C%20and%20efficient%20semantic%203D%20maps%20suitable%20for%0Aautonomous%20agents%20in%20unstructured%20environments.%20The%20proposed%20approach%20is%20based%0Aon%20a%20Voxel-TSDF%20representation%20used%20in%20recent%20algorithms.%20It%20introduces%20novel%0Aways%20of%20integrating%20semantic%20prediction%20confidence%20during%20mapping%2C%20producing%0Asemantic%20and%20instance-consistent%203D%20regions.%20Further%20improvements%20are%20achieved%0Aby%20graph%20optimization-based%20semantic%20labeling%20and%20instance%20refinement.%20The%0Aproposed%20method%20achieves%20accuracy%20superior%20to%20the%20state%20of%20the%20art%20on%20public%0Alarge-scale%20datasets%2C%20improving%20on%20a%20number%20of%20widely%20used%20metrics.%20We%20also%0Ahighlight%20a%20downfall%20in%20the%20evaluation%20of%20recent%20studies%3A%20using%20the%20ground%0Atruth%20trajectory%20as%20input%20instead%20of%20a%20SLAM-estimated%20one%20substantially%20affects%0Athe%20accuracy%2C%20creating%20a%20large%20gap%20between%20the%20reported%20results%20and%20the%20actual%0Aperformance%20on%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14737v2&entry.124074799=Read"},
{"title": "A Safety-Critical Framework for UGVs in Complex Environments: A\n  Data-Driven Discrepancy-Aware Approach", "author": "Skylar X. Wei and Lu Gan and Joel W. Burdick", "abstract": "  This work presents a novel data-driven multi-layered planning and control\nframework for the safe navigation of a class of unmanned ground vehicles (UGVs)\nin the presence of unknown stationary obstacles and additive modeling\nuncertainties. The foundation of this framework is a novel robust model\npredictive planner, designed to generate optimal collision-free trajectories\ngiven an occupancy grid map, and a paired ancillary controller, augmented to\nprovide robustness against model uncertainties extracted from learning data.\n  To tackle modeling discrepancies, we identify both matched (input\ndiscrepancies) and unmatched model residuals between the true and the nominal\nreduced-order models using closed-loop tracking errors as training data.\nUtilizing conformal prediction, we extract probabilistic upper bounds for the\nunknown model residuals, which serve to construct a robustifying ancillary\ncontroller. Further, we also determine maximum tracking discrepancies, also\nknown as the robust control invariance tube, under the augmented policy,\nformulating them as collision buffers. Employing a LiDAR-based occupancy map to\ncharacterize the environment, we construct a discrepancy-aware cost map that\nincorporates these collision buffers. This map is then integrated into a\nsampling-based model predictive path planner that generates optimal and safe\ntrajectories that can be robustly tracked by the augmented ancillary controller\nin the presence of model mismatches.\n  The effectiveness of the framework is experimentally validated for autonomous\nhigh-speed trajectory tracking in a cluttered environment with four different\nvehicle-terrain configurations. We also showcase the framework's versatility by\nreformulating it as a driver-assist program, providing collision avoidance\ncorrections based on user joystick commands.\n", "link": "http://arxiv.org/abs/2403.03215v1", "date": "2024-03-05", "relevancy": 2.4621, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6348}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.602}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6017}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Safety-Critical%20Framework%20for%20UGVs%20in%20Complex%20Environments%3A%20A%0A%20%20Data-Driven%20Discrepancy-Aware%20Approach&entry.906535625=Skylar%20X.%20Wei%20and%20Lu%20Gan%20and%20Joel%20W.%20Burdick&entry.1292438233=%20%20This%20work%20presents%20a%20novel%20data-driven%20multi-layered%20planning%20and%20control%0Aframework%20for%20the%20safe%20navigation%20of%20a%20class%20of%20unmanned%20ground%20vehicles%20%28UGVs%29%0Ain%20the%20presence%20of%20unknown%20stationary%20obstacles%20and%20additive%20modeling%0Auncertainties.%20The%20foundation%20of%20this%20framework%20is%20a%20novel%20robust%20model%0Apredictive%20planner%2C%20designed%20to%20generate%20optimal%20collision-free%20trajectories%0Agiven%20an%20occupancy%20grid%20map%2C%20and%20a%20paired%20ancillary%20controller%2C%20augmented%20to%0Aprovide%20robustness%20against%20model%20uncertainties%20extracted%20from%20learning%20data.%0A%20%20To%20tackle%20modeling%20discrepancies%2C%20we%20identify%20both%20matched%20%28input%0Adiscrepancies%29%20and%20unmatched%20model%20residuals%20between%20the%20true%20and%20the%20nominal%0Areduced-order%20models%20using%20closed-loop%20tracking%20errors%20as%20training%20data.%0AUtilizing%20conformal%20prediction%2C%20we%20extract%20probabilistic%20upper%20bounds%20for%20the%0Aunknown%20model%20residuals%2C%20which%20serve%20to%20construct%20a%20robustifying%20ancillary%0Acontroller.%20Further%2C%20we%20also%20determine%20maximum%20tracking%20discrepancies%2C%20also%0Aknown%20as%20the%20robust%20control%20invariance%20tube%2C%20under%20the%20augmented%20policy%2C%0Aformulating%20them%20as%20collision%20buffers.%20Employing%20a%20LiDAR-based%20occupancy%20map%20to%0Acharacterize%20the%20environment%2C%20we%20construct%20a%20discrepancy-aware%20cost%20map%20that%0Aincorporates%20these%20collision%20buffers.%20This%20map%20is%20then%20integrated%20into%20a%0Asampling-based%20model%20predictive%20path%20planner%20that%20generates%20optimal%20and%20safe%0Atrajectories%20that%20can%20be%20robustly%20tracked%20by%20the%20augmented%20ancillary%20controller%0Ain%20the%20presence%20of%20model%20mismatches.%0A%20%20The%20effectiveness%20of%20the%20framework%20is%20experimentally%20validated%20for%20autonomous%0Ahigh-speed%20trajectory%20tracking%20in%20a%20cluttered%20environment%20with%20four%20different%0Avehicle-terrain%20configurations.%20We%20also%20showcase%20the%20framework%27s%20versatility%20by%0Areformulating%20it%20as%20a%20driver-assist%20program%2C%20providing%20collision%20avoidance%0Acorrections%20based%20on%20user%20joystick%20commands.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03215v1&entry.124074799=Read"},
{"title": "Training-set-free two-stage deep learning for spectroscopic data\n  de-noising", "author": "Dongchen Huang and Junde Liu and Tian Qian and Hongming Weng", "abstract": "  De-noising is a prominent step in the spectra post-processing procedure.\nPrevious machine learning-based methods are fast but mostly based on supervised\nlearning and require a training set that may be typically expensive in real\nexperimental measurements. Unsupervised learning-based algorithms are slow and\nrequire many iterations to achieve convergence. Here, we bridge this gap by\nproposing a training-set-free two-stage deep learning method. We show that the\nfuzzy fixed input in previous methods can be improved by introducing an\nadaptive prior. Combined with more advanced optimization techniques, our\napproach can achieve five times acceleration compared to previous work.\nTheoretically, we study the landscape of a corresponding non-convex linear\nproblem, and our results indicates that this problem has benign geometry for\nfirst-order algorithms to converge.\n", "link": "http://arxiv.org/abs/2402.18830v2", "date": "2024-03-05", "relevancy": 2.4615, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5031}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4955}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4783}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-set-free%20two-stage%20deep%20learning%20for%20spectroscopic%20data%0A%20%20de-noising&entry.906535625=Dongchen%20Huang%20and%20Junde%20Liu%20and%20Tian%20Qian%20and%20Hongming%20Weng&entry.1292438233=%20%20De-noising%20is%20a%20prominent%20step%20in%20the%20spectra%20post-processing%20procedure.%0APrevious%20machine%20learning-based%20methods%20are%20fast%20but%20mostly%20based%20on%20supervised%0Alearning%20and%20require%20a%20training%20set%20that%20may%20be%20typically%20expensive%20in%20real%0Aexperimental%20measurements.%20Unsupervised%20learning-based%20algorithms%20are%20slow%20and%0Arequire%20many%20iterations%20to%20achieve%20convergence.%20Here%2C%20we%20bridge%20this%20gap%20by%0Aproposing%20a%20training-set-free%20two-stage%20deep%20learning%20method.%20We%20show%20that%20the%0Afuzzy%20fixed%20input%20in%20previous%20methods%20can%20be%20improved%20by%20introducing%20an%0Aadaptive%20prior.%20Combined%20with%20more%20advanced%20optimization%20techniques%2C%20our%0Aapproach%20can%20achieve%20five%20times%20acceleration%20compared%20to%20previous%20work.%0ATheoretically%2C%20we%20study%20the%20landscape%20of%20a%20corresponding%20non-convex%20linear%0Aproblem%2C%20and%20our%20results%20indicates%20that%20this%20problem%20has%20benign%20geometry%20for%0Afirst-order%20algorithms%20to%20converge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18830v2&entry.124074799=Read"},
{"title": "A Variational Approach for Joint Image Recovery and Feature Extraction\n  Based on Spatially-Varying Generalised Gaussian Models", "author": "Emilie Chouzenoux and Marie-Caroline Corbineau and Jean-Christophe Pesquet and Gabriele Scrivanti", "abstract": "  The joint problem of reconstruction / feature extraction is a challenging\ntask in image processing. It consists in performing, in a joint manner, the\nrestoration of an image and the extraction of its features. In this work, we\nfirstly propose a novel nonsmooth and non-convex variational formulation of the\nproblem. For this purpose, we introduce a versatile generalised Gaussian prior\nwhose parameters, including its exponent, are space-variant. Secondly, we\ndesign an alternating proximal-based optimisation algorithm that efficiently\nexploits the structure of the proposed non-convex objective function. We also\nanalyse the convergence of this algorithm. As shown in numerical experiments\nconducted on joint deblurring/segmentation tasks, the proposed method provides\nhigh-quality results.\n", "link": "http://arxiv.org/abs/2209.01375v3", "date": "2024-03-05", "relevancy": 2.4593, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5126}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4942}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4688}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Variational%20Approach%20for%20Joint%20Image%20Recovery%20and%20Feature%20Extraction%0A%20%20Based%20on%20Spatially-Varying%20Generalised%20Gaussian%20Models&entry.906535625=Emilie%20Chouzenoux%20and%20Marie-Caroline%20Corbineau%20and%20Jean-Christophe%20Pesquet%20and%20Gabriele%20Scrivanti&entry.1292438233=%20%20The%20joint%20problem%20of%20reconstruction%20/%20feature%20extraction%20is%20a%20challenging%0Atask%20in%20image%20processing.%20It%20consists%20in%20performing%2C%20in%20a%20joint%20manner%2C%20the%0Arestoration%20of%20an%20image%20and%20the%20extraction%20of%20its%20features.%20In%20this%20work%2C%20we%0Afirstly%20propose%20a%20novel%20nonsmooth%20and%20non-convex%20variational%20formulation%20of%20the%0Aproblem.%20For%20this%20purpose%2C%20we%20introduce%20a%20versatile%20generalised%20Gaussian%20prior%0Awhose%20parameters%2C%20including%20its%20exponent%2C%20are%20space-variant.%20Secondly%2C%20we%0Adesign%20an%20alternating%20proximal-based%20optimisation%20algorithm%20that%20efficiently%0Aexploits%20the%20structure%20of%20the%20proposed%20non-convex%20objective%20function.%20We%20also%0Aanalyse%20the%20convergence%20of%20this%20algorithm.%20As%20shown%20in%20numerical%20experiments%0Aconducted%20on%20joint%20deblurring/segmentation%20tasks%2C%20the%20proposed%20method%20provides%0Ahigh-quality%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.01375v3&entry.124074799=Read"},
{"title": "Leveraging Federated Learning and Edge Computing for Recommendation\n  Systems within Cloud Computing Networks", "author": "Yaqian Qi and Yaqian Qi and Xiangxiang Wang and Hanzhe Li and Jingxiao Tian", "abstract": "  To enable large-scale and efficient deployment of artificial intelligence\n(AI), the combination of AI and edge computing has spawned Edge Intelligence,\nwhich leverages the computing and communication capabilities of end devices and\nedge servers to process data closer to where it is generated. A key technology\nfor edge intelligence is the privacy-protecting machine learning paradigm known\nas Federated Learning (FL), which enables data owners to train models without\nhaving to transfer raw data to third-party servers. However, FL networks are\nexpected to involve thousands of heterogeneous distributed devices. As a\nresult, communication efficiency remains a key bottleneck. To reduce node\nfailures and device exits, a Hierarchical Federated Learning (HFL) framework is\nproposed, where a designated cluster leader supports the data owner through\nintermediate model aggregation. Therefore, based on the improvement of edge\nserver resource utilization, this paper can effectively make up for the\nlimitation of cache capacity. In order to mitigate the impact of soft clicks on\nthe quality of user experience (QoE), the authors model the user QoE as a\ncomprehensive system cost. To solve the formulaic problem, the authors propose\na decentralized caching algorithm with federated deep reinforcement learning\n(DRL) and federated learning (FL), where multiple agents learn and make\ndecisions independently\n", "link": "http://arxiv.org/abs/2403.03165v1", "date": "2024-03-05", "relevancy": 2.4508, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4976}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4965}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4764}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Federated%20Learning%20and%20Edge%20Computing%20for%20Recommendation%0A%20%20Systems%20within%20Cloud%20Computing%20Networks&entry.906535625=Yaqian%20Qi%20and%20Yaqian%20Qi%20and%20Xiangxiang%20Wang%20and%20Hanzhe%20Li%20and%20Jingxiao%20Tian&entry.1292438233=%20%20To%20enable%20large-scale%20and%20efficient%20deployment%20of%20artificial%20intelligence%0A%28AI%29%2C%20the%20combination%20of%20AI%20and%20edge%20computing%20has%20spawned%20Edge%20Intelligence%2C%0Awhich%20leverages%20the%20computing%20and%20communication%20capabilities%20of%20end%20devices%20and%0Aedge%20servers%20to%20process%20data%20closer%20to%20where%20it%20is%20generated.%20A%20key%20technology%0Afor%20edge%20intelligence%20is%20the%20privacy-protecting%20machine%20learning%20paradigm%20known%0Aas%20Federated%20Learning%20%28FL%29%2C%20which%20enables%20data%20owners%20to%20train%20models%20without%0Ahaving%20to%20transfer%20raw%20data%20to%20third-party%20servers.%20However%2C%20FL%20networks%20are%0Aexpected%20to%20involve%20thousands%20of%20heterogeneous%20distributed%20devices.%20As%20a%0Aresult%2C%20communication%20efficiency%20remains%20a%20key%20bottleneck.%20To%20reduce%20node%0Afailures%20and%20device%20exits%2C%20a%20Hierarchical%20Federated%20Learning%20%28HFL%29%20framework%20is%0Aproposed%2C%20where%20a%20designated%20cluster%20leader%20supports%20the%20data%20owner%20through%0Aintermediate%20model%20aggregation.%20Therefore%2C%20based%20on%20the%20improvement%20of%20edge%0Aserver%20resource%20utilization%2C%20this%20paper%20can%20effectively%20make%20up%20for%20the%0Alimitation%20of%20cache%20capacity.%20In%20order%20to%20mitigate%20the%20impact%20of%20soft%20clicks%20on%0Athe%20quality%20of%20user%20experience%20%28QoE%29%2C%20the%20authors%20model%20the%20user%20QoE%20as%20a%0Acomprehensive%20system%20cost.%20To%20solve%20the%20formulaic%20problem%2C%20the%20authors%20propose%0Aa%20decentralized%20caching%20algorithm%20with%20federated%20deep%20reinforcement%20learning%0A%28DRL%29%20and%20federated%20learning%20%28FL%29%2C%20where%20multiple%20agents%20learn%20and%20make%0Adecisions%20independently%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03165v1&entry.124074799=Read"},
{"title": "Enhancing Conceptual Understanding in Multimodal Contrastive Learning\n  through Hard Negative Samples", "author": "Philipp J. R\u00f6sch and Norbert Oswald and Michaela Geierhos and Jind\u0159ich Libovick\u00fd", "abstract": "  Current multimodal models leveraging contrastive learning often face\nlimitations in developing fine-grained conceptual understanding. This is due to\nrandom negative samples during pretraining, causing almost exclusively very\ndissimilar concepts to be compared in the loss function. Consequently, the\nmodels struggle with fine-grained semantic differences. To address this\nproblem, we introduce a novel pretraining method incorporating synthetic hard\nnegative text examples. The hard negatives permute terms corresponding to\nvisual concepts, leading to a more fine-grained visual and textual concept\nalignment. Further, we introduce InpaintCOCO, a new challenging dataset for\nassessing the fine-grained alignment of colors, objects, and sizes in\nvision-language models. We created the dataset using generative inpainting from\nCOCO images by changing the visual concepts so that the images no longer match\ntheir original captions. Our results show significant improvements in\nfine-grained concept understanding across a wide range of vision-language\ndatasets, including our InpaintCOCO dataset.\n", "link": "http://arxiv.org/abs/2403.02875v1", "date": "2024-03-05", "relevancy": 2.4504, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4696}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Conceptual%20Understanding%20in%20Multimodal%20Contrastive%20Learning%0A%20%20through%20Hard%20Negative%20Samples&entry.906535625=Philipp%20J.%20R%C3%B6sch%20and%20Norbert%20Oswald%20and%20Michaela%20Geierhos%20and%20Jind%C5%99ich%20Libovick%C3%BD&entry.1292438233=%20%20Current%20multimodal%20models%20leveraging%20contrastive%20learning%20often%20face%0Alimitations%20in%20developing%20fine-grained%20conceptual%20understanding.%20This%20is%20due%20to%0Arandom%20negative%20samples%20during%20pretraining%2C%20causing%20almost%20exclusively%20very%0Adissimilar%20concepts%20to%20be%20compared%20in%20the%20loss%20function.%20Consequently%2C%20the%0Amodels%20struggle%20with%20fine-grained%20semantic%20differences.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20a%20novel%20pretraining%20method%20incorporating%20synthetic%20hard%0Anegative%20text%20examples.%20The%20hard%20negatives%20permute%20terms%20corresponding%20to%0Avisual%20concepts%2C%20leading%20to%20a%20more%20fine-grained%20visual%20and%20textual%20concept%0Aalignment.%20Further%2C%20we%20introduce%20InpaintCOCO%2C%20a%20new%20challenging%20dataset%20for%0Aassessing%20the%20fine-grained%20alignment%20of%20colors%2C%20objects%2C%20and%20sizes%20in%0Avision-language%20models.%20We%20created%20the%20dataset%20using%20generative%20inpainting%20from%0ACOCO%20images%20by%20changing%20the%20visual%20concepts%20so%20that%20the%20images%20no%20longer%20match%0Atheir%20original%20captions.%20Our%20results%20show%20significant%20improvements%20in%0Afine-grained%20concept%20understanding%20across%20a%20wide%20range%20of%20vision-language%0Adatasets%2C%20including%20our%20InpaintCOCO%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02875v1&entry.124074799=Read"},
{"title": "MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual\n  Grounding", "author": "Chun-Peng Chang and Shaoxiang Wang and Alain Pagani and Didier Stricker", "abstract": "  3D visual grounding involves matching natural language descriptions with\ntheir corresponding objects in 3D spaces. Existing methods often face\nchallenges with accuracy in object recognition and struggle in interpreting\ncomplex linguistic queries, particularly with descriptions that involve\nmultiple anchors or are view-dependent. In response, we present the MiKASA\n(Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model\nintegrates a self-attention-based scene-aware object encoder and an original\nmulti-key-anchor technique, enhancing object recognition accuracy and the\nunderstanding of spatial relationships. Furthermore, MiKASA improves the\nexplainability of decision-making, facilitating error diagnosis. Our model\nachieves the highest overall accuracy in the Referit3D challenge for both the\nSr3D and Nr3D datasets, particularly excelling by a large margin in categories\nthat require viewpoint-dependent descriptions.\n  The source code and additional resources for this project are available on\nGitHub: https://github.com/birdy666/MiKASA-3DVG\n", "link": "http://arxiv.org/abs/2403.03077v1", "date": "2024-03-05", "relevancy": 2.4361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6418}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5919}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5698}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiKASA%3A%20Multi-Key-Anchor%20%26%20Scene-Aware%20Transformer%20for%203D%20Visual%0A%20%20Grounding&entry.906535625=Chun-Peng%20Chang%20and%20Shaoxiang%20Wang%20and%20Alain%20Pagani%20and%20Didier%20Stricker&entry.1292438233=%20%203D%20visual%20grounding%20involves%20matching%20natural%20language%20descriptions%20with%0Atheir%20corresponding%20objects%20in%203D%20spaces.%20Existing%20methods%20often%20face%0Achallenges%20with%20accuracy%20in%20object%20recognition%20and%20struggle%20in%20interpreting%0Acomplex%20linguistic%20queries%2C%20particularly%20with%20descriptions%20that%20involve%0Amultiple%20anchors%20or%20are%20view-dependent.%20In%20response%2C%20we%20present%20the%20MiKASA%0A%28Multi-Key-Anchor%20Scene-Aware%29%20Transformer.%20Our%20novel%20end-to-end%20trained%20model%0Aintegrates%20a%20self-attention-based%20scene-aware%20object%20encoder%20and%20an%20original%0Amulti-key-anchor%20technique%2C%20enhancing%20object%20recognition%20accuracy%20and%20the%0Aunderstanding%20of%20spatial%20relationships.%20Furthermore%2C%20MiKASA%20improves%20the%0Aexplainability%20of%20decision-making%2C%20facilitating%20error%20diagnosis.%20Our%20model%0Aachieves%20the%20highest%20overall%20accuracy%20in%20the%20Referit3D%20challenge%20for%20both%20the%0ASr3D%20and%20Nr3D%20datasets%2C%20particularly%20excelling%20by%20a%20large%20margin%20in%20categories%0Athat%20require%20viewpoint-dependent%20descriptions.%0A%20%20The%20source%20code%20and%20additional%20resources%20for%20this%20project%20are%20available%20on%0AGitHub%3A%20https%3A//github.com/birdy666/MiKASA-3DVG%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03077v1&entry.124074799=Read"},
{"title": "Generalizing Graph Neural Networks on Out-Of-Distribution Graphs", "author": "Shaohua Fan and Xiao Wang and Chuan Shi and Peng Cui and Bai Wang", "abstract": "  Graph Neural Networks (GNNs) are proposed without considering the agnostic\ndistribution shifts between training and testing graphs, inducing the\ndegeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD)\nsettings. The fundamental reason for such degeneration is that most GNNs are\ndeveloped based on the I.I.D hypothesis. In such a setting, GNNs tend to\nexploit subtle statistical correlations existing in the training set for\npredictions, even though it is a spurious correlation. However, such spurious\ncorrelations may change in testing environments, leading to the failure of\nGNNs. Therefore, eliminating the impact of spurious correlations is crucial for\nstable GNNs. To this end, we propose a general causal representation framework,\ncalled StableGNN. The main idea is to extract high-level representations from\ngraph data first and resort to the distinguishing ability of causal inference\nto help the model get rid of spurious correlations. Particularly, we exploit a\ngraph pooling layer to extract subgraph-based representations as high-level\nrepresentations. Furthermore, we propose a causal variable distinguishing\nregularizer to correct the biased training distribution. Hence, GNNs would\nconcentrate more on the stable correlations. Extensive experiments on both\nsynthetic and real-world OOD graph datasets well verify the effectiveness,\nflexibility and interpretability of the proposed framework.\n", "link": "http://arxiv.org/abs/2111.10657v3", "date": "2024-03-05", "relevancy": 2.4359, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5174}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4952}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.449}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20Graph%20Neural%20Networks%20on%20Out-Of-Distribution%20Graphs&entry.906535625=Shaohua%20Fan%20and%20Xiao%20Wang%20and%20Chuan%20Shi%20and%20Peng%20Cui%20and%20Bai%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20proposed%20without%20considering%20the%20agnostic%0Adistribution%20shifts%20between%20training%20and%20testing%20graphs%2C%20inducing%20the%0Adegeneration%20of%20the%20generalization%20ability%20of%20GNNs%20on%20Out-Of-Distribution%20%28OOD%29%0Asettings.%20The%20fundamental%20reason%20for%20such%20degeneration%20is%20that%20most%20GNNs%20are%0Adeveloped%20based%20on%20the%20I.I.D%20hypothesis.%20In%20such%20a%20setting%2C%20GNNs%20tend%20to%0Aexploit%20subtle%20statistical%20correlations%20existing%20in%20the%20training%20set%20for%0Apredictions%2C%20even%20though%20it%20is%20a%20spurious%20correlation.%20However%2C%20such%20spurious%0Acorrelations%20may%20change%20in%20testing%20environments%2C%20leading%20to%20the%20failure%20of%0AGNNs.%20Therefore%2C%20eliminating%20the%20impact%20of%20spurious%20correlations%20is%20crucial%20for%0Astable%20GNNs.%20To%20this%20end%2C%20we%20propose%20a%20general%20causal%20representation%20framework%2C%0Acalled%20StableGNN.%20The%20main%20idea%20is%20to%20extract%20high-level%20representations%20from%0Agraph%20data%20first%20and%20resort%20to%20the%20distinguishing%20ability%20of%20causal%20inference%0Ato%20help%20the%20model%20get%20rid%20of%20spurious%20correlations.%20Particularly%2C%20we%20exploit%20a%0Agraph%20pooling%20layer%20to%20extract%20subgraph-based%20representations%20as%20high-level%0Arepresentations.%20Furthermore%2C%20we%20propose%20a%20causal%20variable%20distinguishing%0Aregularizer%20to%20correct%20the%20biased%20training%20distribution.%20Hence%2C%20GNNs%20would%0Aconcentrate%20more%20on%20the%20stable%20correlations.%20Extensive%20experiments%20on%20both%0Asynthetic%20and%20real-world%20OOD%20graph%20datasets%20well%20verify%20the%20effectiveness%2C%0Aflexibility%20and%20interpretability%20of%20the%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2111.10657v3&entry.124074799=Read"},
{"title": "CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex\n  Theory for UAV Inspections", "author": "Zhen Yao and Jiawei Xu and Shuhang Hou and Mooi Choo Chuah", "abstract": "  Routine visual inspections of concrete structures are imperative for\nupholding the safety and integrity of critical infrastructure. Such visual\ninspections sometimes happen under low-light conditions, e.g., checking for\nbridge health. Crack segmentation under such conditions is challenging due to\nthe poor contrast between cracks and their surroundings. However, most deep\nlearning methods are designed for well-illuminated crack images and hence their\nperformance drops dramatically in low-light scenes. In addition, conventional\napproaches require many annotated low-light crack images which is\ntime-consuming. In this paper, we address these challenges by proposing\nCrackNex, a framework that utilizes reflectance information based on Retinex\nTheory to help the model learn a unified illumination-invariant representation.\nFurthermore, we utilize few-shot segmentation to solve the inefficient training\ndata problem. In CrackNex, both a support prototype and a reflectance prototype\nare extracted from the support set. Then, a prototype fusion module is designed\nto integrate the features from both prototypes. CrackNex outperforms the SOTA\nmethods on multiple datasets. Additionally, we present the first benchmark\ndataset, LCSD, for low-light crack segmentation. LCSD consists of 102\nwell-illuminated crack images and 41 low-light crack images. The dataset and\ncode are available at https://github.com/zy1296/CrackNex.\n", "link": "http://arxiv.org/abs/2403.03063v1", "date": "2024-03-05", "relevancy": 2.4335, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4964}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4882}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4754}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrackNex%3A%20a%20Few-shot%20Low-light%20Crack%20Segmentation%20Model%20Based%20on%20Retinex%0A%20%20Theory%20for%20UAV%20Inspections&entry.906535625=Zhen%20Yao%20and%20Jiawei%20Xu%20and%20Shuhang%20Hou%20and%20Mooi%20Choo%20Chuah&entry.1292438233=%20%20Routine%20visual%20inspections%20of%20concrete%20structures%20are%20imperative%20for%0Aupholding%20the%20safety%20and%20integrity%20of%20critical%20infrastructure.%20Such%20visual%0Ainspections%20sometimes%20happen%20under%20low-light%20conditions%2C%20e.g.%2C%20checking%20for%0Abridge%20health.%20Crack%20segmentation%20under%20such%20conditions%20is%20challenging%20due%20to%0Athe%20poor%20contrast%20between%20cracks%20and%20their%20surroundings.%20However%2C%20most%20deep%0Alearning%20methods%20are%20designed%20for%20well-illuminated%20crack%20images%20and%20hence%20their%0Aperformance%20drops%20dramatically%20in%20low-light%20scenes.%20In%20addition%2C%20conventional%0Aapproaches%20require%20many%20annotated%20low-light%20crack%20images%20which%20is%0Atime-consuming.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%20proposing%0ACrackNex%2C%20a%20framework%20that%20utilizes%20reflectance%20information%20based%20on%20Retinex%0ATheory%20to%20help%20the%20model%20learn%20a%20unified%20illumination-invariant%20representation.%0AFurthermore%2C%20we%20utilize%20few-shot%20segmentation%20to%20solve%20the%20inefficient%20training%0Adata%20problem.%20In%20CrackNex%2C%20both%20a%20support%20prototype%20and%20a%20reflectance%20prototype%0Aare%20extracted%20from%20the%20support%20set.%20Then%2C%20a%20prototype%20fusion%20module%20is%20designed%0Ato%20integrate%20the%20features%20from%20both%20prototypes.%20CrackNex%20outperforms%20the%20SOTA%0Amethods%20on%20multiple%20datasets.%20Additionally%2C%20we%20present%20the%20first%20benchmark%0Adataset%2C%20LCSD%2C%20for%20low-light%20crack%20segmentation.%20LCSD%20consists%20of%20102%0Awell-illuminated%20crack%20images%20and%2041%20low-light%20crack%20images.%20The%20dataset%20and%0Acode%20are%20available%20at%20https%3A//github.com/zy1296/CrackNex.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03063v1&entry.124074799=Read"},
{"title": "UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video\n  Diffusion Models via Training-Free Unified Attention Control", "author": "Xuweiyi Chen and Tian Xia and Sihan Xu", "abstract": "  Video Diffusion Models have been developed for video generation, usually\nintegrating text and image conditioning to enhance control over the generated\ncontent. Despite the progress, ensuring consistency across frames remains a\nchallenge, particularly when using text prompts as control conditions. To\naddress this problem, we introduce UniCtrl, a novel, plug-and-play method that\nis universally applicable to improve the spatiotemporal consistency and motion\ndiversity of videos generated by text-to-video models without additional\ntraining. UniCtrl ensures semantic consistency across different frames through\ncross-frame self-attention control, and meanwhile, enhances the motion quality\nand spatiotemporal consistency through motion injection and spatiotemporal\nsynchronization. Our experimental results demonstrate UniCtrl's efficacy in\nenhancing various text-to-video models, confirming its effectiveness and\nuniversality.\n", "link": "http://arxiv.org/abs/2403.02332v2", "date": "2024-03-05", "relevancy": 2.4176, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7063}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6306}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5375}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniCtrl%3A%20Improving%20the%20Spatiotemporal%20Consistency%20of%20Text-to-Video%0A%20%20Diffusion%20Models%20via%20Training-Free%20Unified%20Attention%20Control&entry.906535625=Xuweiyi%20Chen%20and%20Tian%20Xia%20and%20Sihan%20Xu&entry.1292438233=%20%20Video%20Diffusion%20Models%20have%20been%20developed%20for%20video%20generation%2C%20usually%0Aintegrating%20text%20and%20image%20conditioning%20to%20enhance%20control%20over%20the%20generated%0Acontent.%20Despite%20the%20progress%2C%20ensuring%20consistency%20across%20frames%20remains%20a%0Achallenge%2C%20particularly%20when%20using%20text%20prompts%20as%20control%20conditions.%20To%0Aaddress%20this%20problem%2C%20we%20introduce%20UniCtrl%2C%20a%20novel%2C%20plug-and-play%20method%20that%0Ais%20universally%20applicable%20to%20improve%20the%20spatiotemporal%20consistency%20and%20motion%0Adiversity%20of%20videos%20generated%20by%20text-to-video%20models%20without%20additional%0Atraining.%20UniCtrl%20ensures%20semantic%20consistency%20across%20different%20frames%20through%0Across-frame%20self-attention%20control%2C%20and%20meanwhile%2C%20enhances%20the%20motion%20quality%0Aand%20spatiotemporal%20consistency%20through%20motion%20injection%20and%20spatiotemporal%0Asynchronization.%20Our%20experimental%20results%20demonstrate%20UniCtrl%27s%20efficacy%20in%0Aenhancing%20various%20text-to-video%20models%2C%20confirming%20its%20effectiveness%20and%0Auniversality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02332v2&entry.124074799=Read"},
{"title": "Cross-Domain Image Conversion by CycleDM", "author": "Sho Shimotsumagari and Shumpei Takezaki and Daichi Haraguchi and Seiichi Uchida", "abstract": "  The purpose of this paper is to enable the conversion between machine-printed\ncharacter images (i.e., font images) and handwritten character images through\nmachine learning. For this purpose, we propose a novel unpaired image-to-image\ndomain conversion method, CycleDM, which incorporates the concept of CycleGAN\ninto the diffusion model. Specifically, CycleDM has two internal conversion\nmodels that bridge the denoising processes of two image domains. These\nconversion models are efficiently trained without explicit correspondence\nbetween the domains. By applying machine-printed and handwritten character\nimages to the two modalities, CycleDM realizes the conversion between them. Our\nexperiments for evaluating the converted images quantitatively and\nqualitatively found that ours performs better than other comparable approaches.\n", "link": "http://arxiv.org/abs/2403.02919v1", "date": "2024-03-05", "relevancy": 2.4122, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5091}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4667}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Image%20Conversion%20by%20CycleDM&entry.906535625=Sho%20Shimotsumagari%20and%20Shumpei%20Takezaki%20and%20Daichi%20Haraguchi%20and%20Seiichi%20Uchida&entry.1292438233=%20%20The%20purpose%20of%20this%20paper%20is%20to%20enable%20the%20conversion%20between%20machine-printed%0Acharacter%20images%20%28i.e.%2C%20font%20images%29%20and%20handwritten%20character%20images%20through%0Amachine%20learning.%20For%20this%20purpose%2C%20we%20propose%20a%20novel%20unpaired%20image-to-image%0Adomain%20conversion%20method%2C%20CycleDM%2C%20which%20incorporates%20the%20concept%20of%20CycleGAN%0Ainto%20the%20diffusion%20model.%20Specifically%2C%20CycleDM%20has%20two%20internal%20conversion%0Amodels%20that%20bridge%20the%20denoising%20processes%20of%20two%20image%20domains.%20These%0Aconversion%20models%20are%20efficiently%20trained%20without%20explicit%20correspondence%0Abetween%20the%20domains.%20By%20applying%20machine-printed%20and%20handwritten%20character%0Aimages%20to%20the%20two%20modalities%2C%20CycleDM%20realizes%20the%20conversion%20between%20them.%20Our%0Aexperiments%20for%20evaluating%20the%20converted%20images%20quantitatively%20and%0Aqualitatively%20found%20that%20ours%20performs%20better%20than%20other%20comparable%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02919v1&entry.124074799=Read"},
{"title": "Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion", "author": "Meng Zheng and Benjamin Planche and Xuan Gong and Fan Yang and Terrence Chen and Ziyan Wu", "abstract": "  3D patient body modeling is critical to the success of automated patient\npositioning for smart medical scanning and operating rooms. Existing CNN-based\nend-to-end patient modeling solutions typically require a) customized network\ndesigns demanding large amount of relevant training data, covering extensive\nrealistic clinical scenarios (e.g., patient covered by sheets), which leads to\nsuboptimal generalizability in practical deployment, b) expensive 3D human\nmodel annotations, i.e., requiring huge amount of manual effort, resulting in\nsystems that scale poorly. To address these issues, we propose a generic\nmodularized 3D patient modeling method consists of (a) a multi-modal keypoint\ndetection module with attentive fusion for 2D patient joint localization, to\nlearn complementary cross-modality patient body information, leading to\nimproved keypoint localization robustness and generalizability in a wide\nvariety of imaging (e.g., CT, MRI etc.) and clinical scenarios (e.g., heavy\nocclusions); and (b) a self-supervised 3D mesh regression module which does not\nrequire expensive 3D mesh parameter annotations to train, bringing immediate\ncost benefits for clinical deployment. We demonstrate the efficacy of the\nproposed method by extensive patient positioning experiments on both public and\nclinical data. Our evaluation results achieve superior patient positioning\nperformance across various imaging modalities in real clinical scenarios.\n", "link": "http://arxiv.org/abs/2403.03217v1", "date": "2024-03-05", "relevancy": 2.4112, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6008}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5572}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%203D%20Patient%20Modeling%20with%20Multi-modal%20Attentive%20Fusion&entry.906535625=Meng%20Zheng%20and%20Benjamin%20Planche%20and%20Xuan%20Gong%20and%20Fan%20Yang%20and%20Terrence%20Chen%20and%20Ziyan%20Wu&entry.1292438233=%20%203D%20patient%20body%20modeling%20is%20critical%20to%20the%20success%20of%20automated%20patient%0Apositioning%20for%20smart%20medical%20scanning%20and%20operating%20rooms.%20Existing%20CNN-based%0Aend-to-end%20patient%20modeling%20solutions%20typically%20require%20a%29%20customized%20network%0Adesigns%20demanding%20large%20amount%20of%20relevant%20training%20data%2C%20covering%20extensive%0Arealistic%20clinical%20scenarios%20%28e.g.%2C%20patient%20covered%20by%20sheets%29%2C%20which%20leads%20to%0Asuboptimal%20generalizability%20in%20practical%20deployment%2C%20b%29%20expensive%203D%20human%0Amodel%20annotations%2C%20i.e.%2C%20requiring%20huge%20amount%20of%20manual%20effort%2C%20resulting%20in%0Asystems%20that%20scale%20poorly.%20To%20address%20these%20issues%2C%20we%20propose%20a%20generic%0Amodularized%203D%20patient%20modeling%20method%20consists%20of%20%28a%29%20a%20multi-modal%20keypoint%0Adetection%20module%20with%20attentive%20fusion%20for%202D%20patient%20joint%20localization%2C%20to%0Alearn%20complementary%20cross-modality%20patient%20body%20information%2C%20leading%20to%0Aimproved%20keypoint%20localization%20robustness%20and%20generalizability%20in%20a%20wide%0Avariety%20of%20imaging%20%28e.g.%2C%20CT%2C%20MRI%20etc.%29%20and%20clinical%20scenarios%20%28e.g.%2C%20heavy%0Aocclusions%29%3B%20and%20%28b%29%20a%20self-supervised%203D%20mesh%20regression%20module%20which%20does%20not%0Arequire%20expensive%203D%20mesh%20parameter%20annotations%20to%20train%2C%20bringing%20immediate%0Acost%20benefits%20for%20clinical%20deployment.%20We%20demonstrate%20the%20efficacy%20of%20the%0Aproposed%20method%20by%20extensive%20patient%20positioning%20experiments%20on%20both%20public%20and%0Aclinical%20data.%20Our%20evaluation%20results%20achieve%20superior%20patient%20positioning%0Aperformance%20across%20various%20imaging%20modalities%20in%20real%20clinical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03217v1&entry.124074799=Read"},
{"title": "Improved LiDAR Odometry and Mapping using Deep Semantic Segmentation and\n  Novel Outliers Detection", "author": "Mohamed Afifi and Mohamed ElHelw", "abstract": "  Perception is a key element for enabling intelligent autonomous navigation.\nUnderstanding the semantics of the surrounding environment and accurate vehicle\npose estimation are essential capabilities for autonomous vehicles, including\nself-driving cars and mobile robots that perform complex tasks. Fast moving\nplatforms like self-driving cars impose a hard challenge for localization and\nmapping algorithms. In this work, we propose a novel framework for real-time\nLiDAR odometry and mapping based on LOAM architecture for fast moving\nplatforms. Our framework utilizes semantic information produced by a deep\nlearning model to improve point-to-line and point-to-plane matching between\nLiDAR scans and build a semantic map of the environment, leading to more\naccurate motion estimation using LiDAR data. We observe that including semantic\ninformation in the matching process introduces a new type of outlier matches to\nthe process, where matching occur between different objects of the same\nsemantic class. To this end, we propose a novel algorithm that explicitly\nidentifies and discards potential outliers in the matching process. In our\nexperiments, we study the effect of improving the matching process on the\nrobustness of LiDAR odometry against high speed motion. Our experimental\nevaluations on KITTI dataset demonstrate that utilizing semantic information\nand rejecting outliers significantly enhance the robustness of LiDAR odometry\nand mapping when there are large gaps between scan acquisition poses, which is\ntypical for fast moving platforms.\n", "link": "http://arxiv.org/abs/2403.03111v1", "date": "2024-03-05", "relevancy": 2.4084, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6414}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6049}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5617}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20LiDAR%20Odometry%20and%20Mapping%20using%20Deep%20Semantic%20Segmentation%20and%0A%20%20Novel%20Outliers%20Detection&entry.906535625=Mohamed%20Afifi%20and%20Mohamed%20ElHelw&entry.1292438233=%20%20Perception%20is%20a%20key%20element%20for%20enabling%20intelligent%20autonomous%20navigation.%0AUnderstanding%20the%20semantics%20of%20the%20surrounding%20environment%20and%20accurate%20vehicle%0Apose%20estimation%20are%20essential%20capabilities%20for%20autonomous%20vehicles%2C%20including%0Aself-driving%20cars%20and%20mobile%20robots%20that%20perform%20complex%20tasks.%20Fast%20moving%0Aplatforms%20like%20self-driving%20cars%20impose%20a%20hard%20challenge%20for%20localization%20and%0Amapping%20algorithms.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20for%20real-time%0ALiDAR%20odometry%20and%20mapping%20based%20on%20LOAM%20architecture%20for%20fast%20moving%0Aplatforms.%20Our%20framework%20utilizes%20semantic%20information%20produced%20by%20a%20deep%0Alearning%20model%20to%20improve%20point-to-line%20and%20point-to-plane%20matching%20between%0ALiDAR%20scans%20and%20build%20a%20semantic%20map%20of%20the%20environment%2C%20leading%20to%20more%0Aaccurate%20motion%20estimation%20using%20LiDAR%20data.%20We%20observe%20that%20including%20semantic%0Ainformation%20in%20the%20matching%20process%20introduces%20a%20new%20type%20of%20outlier%20matches%20to%0Athe%20process%2C%20where%20matching%20occur%20between%20different%20objects%20of%20the%20same%0Asemantic%20class.%20To%20this%20end%2C%20we%20propose%20a%20novel%20algorithm%20that%20explicitly%0Aidentifies%20and%20discards%20potential%20outliers%20in%20the%20matching%20process.%20In%20our%0Aexperiments%2C%20we%20study%20the%20effect%20of%20improving%20the%20matching%20process%20on%20the%0Arobustness%20of%20LiDAR%20odometry%20against%20high%20speed%20motion.%20Our%20experimental%0Aevaluations%20on%20KITTI%20dataset%20demonstrate%20that%20utilizing%20semantic%20information%0Aand%20rejecting%20outliers%20significantly%20enhance%20the%20robustness%20of%20LiDAR%20odometry%0Aand%20mapping%20when%20there%20are%20large%20gaps%20between%20scan%20acquisition%20poses%2C%20which%20is%0Atypical%20for%20fast%20moving%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03111v1&entry.124074799=Read"},
{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "author": "Weijie Li and Litong Gong and Yiran Zhu and Fanda Fan and Biao Wang and Tiezheng Ge and Bo Zheng", "abstract": "  Image-to-video (I2V) generation tasks always suffer from keeping high\nfidelity in the open domains. Traditional image animation techniques primarily\nfocus on specific domains such as faces or human poses, making them difficult\nto generalize to open domains. Several recent I2V frameworks based on diffusion\nmodels can generate dynamic content for open domain images but fail to maintain\nfidelity. We found that two main factors of low fidelity are the loss of image\ndetails and the noise prediction biases during the denoising process. To this\nend, we propose an effective method that can be applied to mainstream video\ndiffusion models. This method achieves high fidelity based on supplementing\nmore precise image information and noise rectification. Specifically, given a\nspecified image, our method first adds noise to the input image latent to keep\nmore details, then denoises the noisy latent with proper rectification to\nalleviate the noise prediction biases. Our method is tuning-free and\nplug-and-play. The experimental results demonstrate the effectiveness of our\napproach in improving the fidelity of generated videos. For more image-to-video\ngenerated results, please refer to the project website:\nhttps://noise-rectification.github.io.\n", "link": "http://arxiv.org/abs/2403.02827v1", "date": "2024-03-05", "relevancy": 2.3932, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6404}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6163}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.549}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning-Free%20Noise%20Rectification%20for%20High%20Fidelity%20Image-to-Video%0A%20%20Generation&entry.906535625=Weijie%20Li%20and%20Litong%20Gong%20and%20Yiran%20Zhu%20and%20Fanda%20Fan%20and%20Biao%20Wang%20and%20Tiezheng%20Ge%20and%20Bo%20Zheng&entry.1292438233=%20%20Image-to-video%20%28I2V%29%20generation%20tasks%20always%20suffer%20from%20keeping%20high%0Afidelity%20in%20the%20open%20domains.%20Traditional%20image%20animation%20techniques%20primarily%0Afocus%20on%20specific%20domains%20such%20as%20faces%20or%20human%20poses%2C%20making%20them%20difficult%0Ato%20generalize%20to%20open%20domains.%20Several%20recent%20I2V%20frameworks%20based%20on%20diffusion%0Amodels%20can%20generate%20dynamic%20content%20for%20open%20domain%20images%20but%20fail%20to%20maintain%0Afidelity.%20We%20found%20that%20two%20main%20factors%20of%20low%20fidelity%20are%20the%20loss%20of%20image%0Adetails%20and%20the%20noise%20prediction%20biases%20during%20the%20denoising%20process.%20To%20this%0Aend%2C%20we%20propose%20an%20effective%20method%20that%20can%20be%20applied%20to%20mainstream%20video%0Adiffusion%20models.%20This%20method%20achieves%20high%20fidelity%20based%20on%20supplementing%0Amore%20precise%20image%20information%20and%20noise%20rectification.%20Specifically%2C%20given%20a%0Aspecified%20image%2C%20our%20method%20first%20adds%20noise%20to%20the%20input%20image%20latent%20to%20keep%0Amore%20details%2C%20then%20denoises%20the%20noisy%20latent%20with%20proper%20rectification%20to%0Aalleviate%20the%20noise%20prediction%20biases.%20Our%20method%20is%20tuning-free%20and%0Aplug-and-play.%20The%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20in%20improving%20the%20fidelity%20of%20generated%20videos.%20For%20more%20image-to-video%0Agenerated%20results%2C%20please%20refer%20to%20the%20project%20website%3A%0Ahttps%3A//noise-rectification.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02827v1&entry.124074799=Read"},
{"title": "Improving Android Malware Detection Through Data Augmentation Using\n  Wasserstein Generative Adversarial Networks", "author": "Kawana Stalin and Mikias Berhanu Mekoya", "abstract": "  Generative Adversarial Networks (GANs) have demonstrated their versatility\nacross various applications, including data augmentation and malware detection.\nThis research explores the effectiveness of utilizing GAN-generated data to\ntrain a model for the detection of Android malware. Given the considerable\nstorage requirements of Android applications, the study proposes a method to\nsynthetically represent data using GANs, thereby reducing storage demands. The\nproposed methodology involves creating image representations of features\nextracted from an existing dataset. A GAN model is then employed to generate a\nmore extensive dataset consisting of realistic synthetic grayscale images.\nSubsequently, this synthetic dataset is utilized to train a Convolutional\nNeural Network (CNN) designed to identify previously unseen Android malware\napplications. The study includes a comparative analysis of the CNN's\nperformance when trained on real images versus synthetic images generated by\nthe GAN. Furthermore, the research explores variations in performance between\nthe Wasserstein Generative Adversarial Network (WGAN) and the Deep\nConvolutional Generative Adversarial Network (DCGAN). The investigation extends\nto studying the impact of image size and malware obfuscation on the\nclassification model's effectiveness. The data augmentation approach\nimplemented in this study resulted in a notable performance enhancement of the\nclassification model, ranging from 1.5% to 7%, depending on the dataset. The\nhighest achieved F1 score reached 0.975.\n  Keywords--Generative Adversarial Networks, Android Malware, Data\nAugmentation, Wasserstein Generative Adversarial Network\n", "link": "http://arxiv.org/abs/2403.00890v2", "date": "2024-03-05", "relevancy": 2.3881, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4922}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4777}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.463}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Android%20Malware%20Detection%20Through%20Data%20Augmentation%20Using%0A%20%20Wasserstein%20Generative%20Adversarial%20Networks&entry.906535625=Kawana%20Stalin%20and%20Mikias%20Berhanu%20Mekoya&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20demonstrated%20their%20versatility%0Aacross%20various%20applications%2C%20including%20data%20augmentation%20and%20malware%20detection.%0AThis%20research%20explores%20the%20effectiveness%20of%20utilizing%20GAN-generated%20data%20to%0Atrain%20a%20model%20for%20the%20detection%20of%20Android%20malware.%20Given%20the%20considerable%0Astorage%20requirements%20of%20Android%20applications%2C%20the%20study%20proposes%20a%20method%20to%0Asynthetically%20represent%20data%20using%20GANs%2C%20thereby%20reducing%20storage%20demands.%20The%0Aproposed%20methodology%20involves%20creating%20image%20representations%20of%20features%0Aextracted%20from%20an%20existing%20dataset.%20A%20GAN%20model%20is%20then%20employed%20to%20generate%20a%0Amore%20extensive%20dataset%20consisting%20of%20realistic%20synthetic%20grayscale%20images.%0ASubsequently%2C%20this%20synthetic%20dataset%20is%20utilized%20to%20train%20a%20Convolutional%0ANeural%20Network%20%28CNN%29%20designed%20to%20identify%20previously%20unseen%20Android%20malware%0Aapplications.%20The%20study%20includes%20a%20comparative%20analysis%20of%20the%20CNN%27s%0Aperformance%20when%20trained%20on%20real%20images%20versus%20synthetic%20images%20generated%20by%0Athe%20GAN.%20Furthermore%2C%20the%20research%20explores%20variations%20in%20performance%20between%0Athe%20Wasserstein%20Generative%20Adversarial%20Network%20%28WGAN%29%20and%20the%20Deep%0AConvolutional%20Generative%20Adversarial%20Network%20%28DCGAN%29.%20The%20investigation%20extends%0Ato%20studying%20the%20impact%20of%20image%20size%20and%20malware%20obfuscation%20on%20the%0Aclassification%20model%27s%20effectiveness.%20The%20data%20augmentation%20approach%0Aimplemented%20in%20this%20study%20resulted%20in%20a%20notable%20performance%20enhancement%20of%20the%0Aclassification%20model%2C%20ranging%20from%201.5%25%20to%207%25%2C%20depending%20on%20the%20dataset.%20The%0Ahighest%20achieved%20F1%20score%20reached%200.975.%0A%20%20Keywords--Generative%20Adversarial%20Networks%2C%20Android%20Malware%2C%20Data%0AAugmentation%2C%20Wasserstein%20Generative%20Adversarial%20Network%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00890v2&entry.124074799=Read"},
{"title": "Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time\n  Visual Scene Understanding", "author": "Christina Kassab and Matias Mattamala and Lintong Zhang and Maurice Fallon", "abstract": "  Versatile and adaptive semantic understanding would enable autonomous systems\nto comprehend and interact with their surroundings. Existing fixed-class models\nlimit the adaptability of indoor mobile and assistive autonomous systems. In\nthis work, we introduce LEXIS, a real-time indoor Simultaneous Localization and\nMapping (SLAM) system that harnesses the open-vocabulary nature of Large\nLanguage Models (LLMs) to create a unified approach to scene understanding and\nplace recognition. The approach first builds a topological SLAM graph of the\nenvironment (using visual-inertial odometry) and embeds Contrastive\nLanguage-Image Pretraining (CLIP) features in the graph nodes. We use this\nrepresentation for flexible room classification and segmentation, serving as a\nbasis for room-centric place recognition. This allows loop closure searches to\nbe directed towards semantically relevant places. Our proposed system is\nevaluated using both public, simulated data and real-world data, covering\noffice and home environments. It successfully categorizes rooms with varying\nlayouts and dimensions and outperforms the state-of-the-art (SOTA). For place\nrecognition and trajectory estimation tasks we achieve equivalent performance\nto the SOTA, all also utilizing the same pre-trained model. Lastly, we\ndemonstrate the system's potential for planning.\n", "link": "http://arxiv.org/abs/2309.15065v2", "date": "2024-03-05", "relevancy": 2.3634, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6028}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5986}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5783}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-EXtended%20Indoor%20SLAM%20%28LEXIS%29%3A%20A%20Versatile%20System%20for%20Real-time%0A%20%20Visual%20Scene%20Understanding&entry.906535625=Christina%20Kassab%20and%20Matias%20Mattamala%20and%20Lintong%20Zhang%20and%20Maurice%20Fallon&entry.1292438233=%20%20Versatile%20and%20adaptive%20semantic%20understanding%20would%20enable%20autonomous%20systems%0Ato%20comprehend%20and%20interact%20with%20their%20surroundings.%20Existing%20fixed-class%20models%0Alimit%20the%20adaptability%20of%20indoor%20mobile%20and%20assistive%20autonomous%20systems.%20In%0Athis%20work%2C%20we%20introduce%20LEXIS%2C%20a%20real-time%20indoor%20Simultaneous%20Localization%20and%0AMapping%20%28SLAM%29%20system%20that%20harnesses%20the%20open-vocabulary%20nature%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20create%20a%20unified%20approach%20to%20scene%20understanding%20and%0Aplace%20recognition.%20The%20approach%20first%20builds%20a%20topological%20SLAM%20graph%20of%20the%0Aenvironment%20%28using%20visual-inertial%20odometry%29%20and%20embeds%20Contrastive%0ALanguage-Image%20Pretraining%20%28CLIP%29%20features%20in%20the%20graph%20nodes.%20We%20use%20this%0Arepresentation%20for%20flexible%20room%20classification%20and%20segmentation%2C%20serving%20as%20a%0Abasis%20for%20room-centric%20place%20recognition.%20This%20allows%20loop%20closure%20searches%20to%0Abe%20directed%20towards%20semantically%20relevant%20places.%20Our%20proposed%20system%20is%0Aevaluated%20using%20both%20public%2C%20simulated%20data%20and%20real-world%20data%2C%20covering%0Aoffice%20and%20home%20environments.%20It%20successfully%20categorizes%20rooms%20with%20varying%0Alayouts%20and%20dimensions%20and%20outperforms%20the%20state-of-the-art%20%28SOTA%29.%20For%20place%0Arecognition%20and%20trajectory%20estimation%20tasks%20we%20achieve%20equivalent%20performance%0Ato%20the%20SOTA%2C%20all%20also%20utilizing%20the%20same%20pre-trained%20model.%20Lastly%2C%20we%0Ademonstrate%20the%20system%27s%20potential%20for%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15065v2&entry.124074799=Read"},
{"title": "A general approach to enhance the survivability of backdoor attacks by\n  decision path coupling", "author": "Yufei Zhao and Dingji Wang and Bihuan Chen and Ziqian Chen and Xin Peng", "abstract": "  Backdoor attacks have been one of the emerging security threats to deep\nneural networks (DNNs), leading to serious consequences. One of the mainstream\nbackdoor defenses is model reconstruction-based. Such defenses adopt model\nunlearning or pruning to eliminate backdoors. However, little attention has\nbeen paid to survive from such defenses. To bridge the gap, we propose Venom,\nthe first generic backdoor attack enhancer to improve the survivability of\nexisting backdoor attacks against model reconstruction-based defenses. We\nformalize Venom as a binary-task optimization problem. The first is the\noriginal backdoor attack task to preserve the original attack capability, while\nthe second is the attack enhancement task to improve the attack survivability.\nTo realize the second task, we propose attention imitation loss to force the\ndecision path of poisoned samples in backdoored models to couple with the\ncrucial decision path of benign samples, which makes backdoors difficult to\neliminate. Our extensive evaluation on two DNNs and three datasets has\ndemonstrated that Venom significantly improves the survivability of eight\nstate-of-the-art attacks against eight state-of-the-art defenses without\nimpacting the capability of the original attacks.\n", "link": "http://arxiv.org/abs/2403.02950v1", "date": "2024-03-05", "relevancy": 2.3623, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5037}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4749}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4388}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20general%20approach%20to%20enhance%20the%20survivability%20of%20backdoor%20attacks%20by%0A%20%20decision%20path%20coupling&entry.906535625=Yufei%20Zhao%20and%20Dingji%20Wang%20and%20Bihuan%20Chen%20and%20Ziqian%20Chen%20and%20Xin%20Peng&entry.1292438233=%20%20Backdoor%20attacks%20have%20been%20one%20of%20the%20emerging%20security%20threats%20to%20deep%0Aneural%20networks%20%28DNNs%29%2C%20leading%20to%20serious%20consequences.%20One%20of%20the%20mainstream%0Abackdoor%20defenses%20is%20model%20reconstruction-based.%20Such%20defenses%20adopt%20model%0Aunlearning%20or%20pruning%20to%20eliminate%20backdoors.%20However%2C%20little%20attention%20has%0Abeen%20paid%20to%20survive%20from%20such%20defenses.%20To%20bridge%20the%20gap%2C%20we%20propose%20Venom%2C%0Athe%20first%20generic%20backdoor%20attack%20enhancer%20to%20improve%20the%20survivability%20of%0Aexisting%20backdoor%20attacks%20against%20model%20reconstruction-based%20defenses.%20We%0Aformalize%20Venom%20as%20a%20binary-task%20optimization%20problem.%20The%20first%20is%20the%0Aoriginal%20backdoor%20attack%20task%20to%20preserve%20the%20original%20attack%20capability%2C%20while%0Athe%20second%20is%20the%20attack%20enhancement%20task%20to%20improve%20the%20attack%20survivability.%0ATo%20realize%20the%20second%20task%2C%20we%20propose%20attention%20imitation%20loss%20to%20force%20the%0Adecision%20path%20of%20poisoned%20samples%20in%20backdoored%20models%20to%20couple%20with%20the%0Acrucial%20decision%20path%20of%20benign%20samples%2C%20which%20makes%20backdoors%20difficult%20to%0Aeliminate.%20Our%20extensive%20evaluation%20on%20two%20DNNs%20and%20three%20datasets%20has%0Ademonstrated%20that%20Venom%20significantly%20improves%20the%20survivability%20of%20eight%0Astate-of-the-art%20attacks%20against%20eight%20state-of-the-art%20defenses%20without%0Aimpacting%20the%20capability%20of%20the%20original%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02950v1&entry.124074799=Read"},
{"title": "Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for\n  Audio-Visual Source Localization", "author": "Yuxin Guo and Shijie Ma and Hu Su and Zhiqing Wang and Yuhao Zhao and Wei Zou and Siyang Sun and Yun Zheng", "abstract": "  Audio-Visual Source Localization (AVSL) aims to locate sounding objects\nwithin video frames given the paired audio clips. Existing methods\npredominantly rely on self-supervised contrastive learning of audio-visual\ncorrespondence. Without any bounding-box annotations, they struggle to achieve\nprecise localization, especially for small objects, and suffer from blurry\nboundaries and false positives. Moreover, the naive semi-supervised method is\npoor in fully leveraging the information of abundant unlabeled data. In this\npaper, we propose a novel semi-supervised learning framework for AVSL, namely\nDual Mean-Teacher (DMT), comprising two teacher-student structures to\ncircumvent the confirmation bias issue. Specifically, two teachers, pre-trained\non limited labeled data, are employed to filter out noisy samples via the\nconsensus between their predictions, and then generate high-quality\npseudo-labels by intersecting their confidence maps. The sufficient utilization\nof both labeled and unlabeled data and the proposed unbiased framework enable\nDMT to outperform current state-of-the-art methods by a large margin, with CIoU\nof 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%,\n9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods\nrespectively, given only 3% positional-annotations. We also extend our\nframework to some existing AVSL methods and consistently boost their\nperformance.\n", "link": "http://arxiv.org/abs/2403.03145v1", "date": "2024-03-05", "relevancy": 2.3558, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6034}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5837}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5659}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Mean-Teacher%3A%20An%20Unbiased%20Semi-Supervised%20Framework%20for%0A%20%20Audio-Visual%20Source%20Localization&entry.906535625=Yuxin%20Guo%20and%20Shijie%20Ma%20and%20Hu%20Su%20and%20Zhiqing%20Wang%20and%20Yuhao%20Zhao%20and%20Wei%20Zou%20and%20Siyang%20Sun%20and%20Yun%20Zheng&entry.1292438233=%20%20Audio-Visual%20Source%20Localization%20%28AVSL%29%20aims%20to%20locate%20sounding%20objects%0Awithin%20video%20frames%20given%20the%20paired%20audio%20clips.%20Existing%20methods%0Apredominantly%20rely%20on%20self-supervised%20contrastive%20learning%20of%20audio-visual%0Acorrespondence.%20Without%20any%20bounding-box%20annotations%2C%20they%20struggle%20to%20achieve%0Aprecise%20localization%2C%20especially%20for%20small%20objects%2C%20and%20suffer%20from%20blurry%0Aboundaries%20and%20false%20positives.%20Moreover%2C%20the%20naive%20semi-supervised%20method%20is%0Apoor%20in%20fully%20leveraging%20the%20information%20of%20abundant%20unlabeled%20data.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20semi-supervised%20learning%20framework%20for%20AVSL%2C%20namely%0ADual%20Mean-Teacher%20%28DMT%29%2C%20comprising%20two%20teacher-student%20structures%20to%0Acircumvent%20the%20confirmation%20bias%20issue.%20Specifically%2C%20two%20teachers%2C%20pre-trained%0Aon%20limited%20labeled%20data%2C%20are%20employed%20to%20filter%20out%20noisy%20samples%20via%20the%0Aconsensus%20between%20their%20predictions%2C%20and%20then%20generate%20high-quality%0Apseudo-labels%20by%20intersecting%20their%20confidence%20maps.%20The%20sufficient%20utilization%0Aof%20both%20labeled%20and%20unlabeled%20data%20and%20the%20proposed%20unbiased%20framework%20enable%0ADMT%20to%20outperform%20current%20state-of-the-art%20methods%20by%20a%20large%20margin%2C%20with%20CIoU%0Aof%2090.4%25%20and%2048.8%25%20on%20Flickr-SoundNet%20and%20VGG-Sound%20Source%2C%20obtaining%208.9%25%2C%0A9.6%25%20and%204.6%25%2C%206.4%25%20improvements%20over%20self-%20and%20semi-supervised%20methods%0Arespectively%2C%20given%20only%203%25%20positional-annotations.%20We%20also%20extend%20our%0Aframework%20to%20some%20existing%20AVSL%20methods%20and%20consistently%20boost%20their%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03145v1&entry.124074799=Read"},
{"title": "Neural network relief: a pruning algorithm based on neural activity", "author": "Aleksandr Dekhovich and David M. J. Tax and Marcel H. F. Sluiter and Miguel A. Bessa", "abstract": "  Current deep neural networks (DNNs) are overparameterized and use most of\ntheir neuronal connections during inference for each task. The human brain,\nhowever, developed specialized regions for different tasks and performs\ninference with a small fraction of its neuronal connections. We propose an\niterative pruning strategy introducing a simple importance-score metric that\ndeactivates unimportant connections, tackling overparameterization in DNNs and\nmodulating the firing patterns. The aim is to find the smallest number of\nconnections that is still capable of solving a given task with comparable\naccuracy, i.e. a simpler subnetwork. We achieve comparable performance for\nLeNet architectures on MNIST, and significantly higher parameter compression\nthan state-of-the-art algorithms for VGG and ResNet architectures on\nCIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two\ndifferent optimizers considered -- Adam and SGD. The algorithm is not designed\nto minimize FLOPs when considering current hardware and software\nimplementations, although it performs reasonably when compared to the state of\nthe art.\n", "link": "http://arxiv.org/abs/2109.10795v3", "date": "2024-03-05", "relevancy": 2.3461, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5166}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4686}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4225}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20network%20relief%3A%20a%20pruning%20algorithm%20based%20on%20neural%20activity&entry.906535625=Aleksandr%20Dekhovich%20and%20David%20M.%20J.%20Tax%20and%20Marcel%20H.%20F.%20Sluiter%20and%20Miguel%20A.%20Bessa&entry.1292438233=%20%20Current%20deep%20neural%20networks%20%28DNNs%29%20are%20overparameterized%20and%20use%20most%20of%0Atheir%20neuronal%20connections%20during%20inference%20for%20each%20task.%20The%20human%20brain%2C%0Ahowever%2C%20developed%20specialized%20regions%20for%20different%20tasks%20and%20performs%0Ainference%20with%20a%20small%20fraction%20of%20its%20neuronal%20connections.%20We%20propose%20an%0Aiterative%20pruning%20strategy%20introducing%20a%20simple%20importance-score%20metric%20that%0Adeactivates%20unimportant%20connections%2C%20tackling%20overparameterization%20in%20DNNs%20and%0Amodulating%20the%20firing%20patterns.%20The%20aim%20is%20to%20find%20the%20smallest%20number%20of%0Aconnections%20that%20is%20still%20capable%20of%20solving%20a%20given%20task%20with%20comparable%0Aaccuracy%2C%20i.e.%20a%20simpler%20subnetwork.%20We%20achieve%20comparable%20performance%20for%0ALeNet%20architectures%20on%20MNIST%2C%20and%20significantly%20higher%20parameter%20compression%0Athan%20state-of-the-art%20algorithms%20for%20VGG%20and%20ResNet%20architectures%20on%0ACIFAR-10/100%20and%20Tiny-ImageNet.%20Our%20approach%20also%20performs%20well%20for%20the%20two%0Adifferent%20optimizers%20considered%20--%20Adam%20and%20SGD.%20The%20algorithm%20is%20not%20designed%0Ato%20minimize%20FLOPs%20when%20considering%20current%20hardware%20and%20software%0Aimplementations%2C%20although%20it%20performs%20reasonably%20when%20compared%20to%20the%20state%20of%0Athe%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.10795v3&entry.124074799=Read"},
{"title": "FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation", "author": "Chris Rockwell and Nilesh Kulkarni and Linyi Jin and Jeong Joon Park and Justin Johnson and David F. Fouhey", "abstract": "  Estimating relative camera poses between images has been a central problem in\ncomputer vision. Methods that find correspondences and solve for the\nfundamental matrix offer high precision in most cases. Conversely, methods\npredicting pose directly using neural networks are more robust to limited\noverlap and can infer absolute translation scale, but at the expense of reduced\nprecision. We show how to combine the best of both methods; our approach yields\nresults that are both precise and robust, while also accurately inferring\ntranslation scales. At the heart of our model lies a Transformer that (1)\nlearns to balance between solved and learned pose estimations, and (2) provides\na prior to guide a solver. A comprehensive analysis supports our design choices\nand demonstrates that our method adapts flexibly to various feature extractors\nand correspondence estimators, showing state-of-the-art performance in 6DoF\npose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free\nRelocalization.\n", "link": "http://arxiv.org/abs/2403.03221v1", "date": "2024-03-05", "relevancy": 2.3329, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5933}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5788}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5691}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAR%3A%20Flexible%2C%20Accurate%20and%20Robust%206DoF%20Relative%20Camera%20Pose%20Estimation&entry.906535625=Chris%20Rockwell%20and%20Nilesh%20Kulkarni%20and%20Linyi%20Jin%20and%20Jeong%20Joon%20Park%20and%20Justin%20Johnson%20and%20David%20F.%20Fouhey&entry.1292438233=%20%20Estimating%20relative%20camera%20poses%20between%20images%20has%20been%20a%20central%20problem%20in%0Acomputer%20vision.%20Methods%20that%20find%20correspondences%20and%20solve%20for%20the%0Afundamental%20matrix%20offer%20high%20precision%20in%20most%20cases.%20Conversely%2C%20methods%0Apredicting%20pose%20directly%20using%20neural%20networks%20are%20more%20robust%20to%20limited%0Aoverlap%20and%20can%20infer%20absolute%20translation%20scale%2C%20but%20at%20the%20expense%20of%20reduced%0Aprecision.%20We%20show%20how%20to%20combine%20the%20best%20of%20both%20methods%3B%20our%20approach%20yields%0Aresults%20that%20are%20both%20precise%20and%20robust%2C%20while%20also%20accurately%20inferring%0Atranslation%20scales.%20At%20the%20heart%20of%20our%20model%20lies%20a%20Transformer%20that%20%281%29%0Alearns%20to%20balance%20between%20solved%20and%20learned%20pose%20estimations%2C%20and%20%282%29%20provides%0Aa%20prior%20to%20guide%20a%20solver.%20A%20comprehensive%20analysis%20supports%20our%20design%20choices%0Aand%20demonstrates%20that%20our%20method%20adapts%20flexibly%20to%20various%20feature%20extractors%0Aand%20correspondence%20estimators%2C%20showing%20state-of-the-art%20performance%20in%206DoF%0Apose%20estimation%20on%20Matterport3D%2C%20InteriorNet%2C%20StreetLearn%2C%20and%20Map-free%0ARelocalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03221v1&entry.124074799=Read"},
{"title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image", "author": "Xijia Tao and Shuai Zhong and Lei Li and Qi Liu and Lingpeng Kong", "abstract": "  There has been an increasing interest in the alignment of large language\nmodels (LLMs) with human values. However, the safety issues of their\nintegration with a vision module, or vision language models (VLMs), remain\nrelatively underexplored. In this paper, we propose a novel jailbreaking attack\nagainst VLMs, aiming to bypass their safety barrier when a user inputs harmful\ninstructions. A scenario where our poisoned (image, text) data pairs are\nincluded in the training data is assumed. By replacing the original textual\ncaptions with malicious jailbreak prompts, our method can perform jailbreak\nattacks with the poisoned images. Moreover, we analyze the effect of poison\nratios and positions of trainable parameters on our attack's success rate. For\nevaluation, we design two metrics to quantify the success rate and the\nstealthiness of our attack. Together with a list of curated harmful\ninstructions, a benchmark for measuring attack efficacy is provided. We\ndemonstrate the efficacy of our attack by comparing it with baseline methods.\n", "link": "http://arxiv.org/abs/2403.02910v1", "date": "2024-03-05", "relevancy": 2.3116, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4773}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4534}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImgTrojan%3A%20Jailbreaking%20Vision-Language%20Models%20with%20ONE%20Image&entry.906535625=Xijia%20Tao%20and%20Shuai%20Zhong%20and%20Lei%20Li%20and%20Qi%20Liu%20and%20Lingpeng%20Kong&entry.1292438233=%20%20There%20has%20been%20an%20increasing%20interest%20in%20the%20alignment%20of%20large%20language%0Amodels%20%28LLMs%29%20with%20human%20values.%20However%2C%20the%20safety%20issues%20of%20their%0Aintegration%20with%20a%20vision%20module%2C%20or%20vision%20language%20models%20%28VLMs%29%2C%20remain%0Arelatively%20underexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20jailbreaking%20attack%0Aagainst%20VLMs%2C%20aiming%20to%20bypass%20their%20safety%20barrier%20when%20a%20user%20inputs%20harmful%0Ainstructions.%20A%20scenario%20where%20our%20poisoned%20%28image%2C%20text%29%20data%20pairs%20are%0Aincluded%20in%20the%20training%20data%20is%20assumed.%20By%20replacing%20the%20original%20textual%0Acaptions%20with%20malicious%20jailbreak%20prompts%2C%20our%20method%20can%20perform%20jailbreak%0Aattacks%20with%20the%20poisoned%20images.%20Moreover%2C%20we%20analyze%20the%20effect%20of%20poison%0Aratios%20and%20positions%20of%20trainable%20parameters%20on%20our%20attack%27s%20success%20rate.%20For%0Aevaluation%2C%20we%20design%20two%20metrics%20to%20quantify%20the%20success%20rate%20and%20the%0Astealthiness%20of%20our%20attack.%20Together%20with%20a%20list%20of%20curated%20harmful%0Ainstructions%2C%20a%20benchmark%20for%20measuring%20attack%20efficacy%20is%20provided.%20We%0Ademonstrate%20the%20efficacy%20of%20our%20attack%20by%20comparing%20it%20with%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02910v1&entry.124074799=Read"},
{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "author": "Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas M\u00fcller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Kyle Lacey and Alex Goodwin and Yannik Marek and Robin Rombach", "abstract": "  Diffusion models create data from noise by inverting the forward paths of\ndata towards noise and have emerged as a powerful generative modeling technique\nfor high-dimensional, perceptual data such as images and videos. Rectified flow\nis a recent generative model formulation that connects data and noise in a\nstraight line. Despite its better theoretical properties and conceptual\nsimplicity, it is not yet decisively established as standard practice. In this\nwork, we improve existing noise sampling techniques for training rectified flow\nmodels by biasing them towards perceptually relevant scales. Through a\nlarge-scale study, we demonstrate the superior performance of this approach\ncompared to established diffusion formulations for high-resolution\ntext-to-image synthesis. Additionally, we present a novel transformer-based\narchitecture for text-to-image generation that uses separate weights for the\ntwo modalities and enables a bidirectional flow of information between image\nand text tokens, improving text comprehension, typography, and human preference\nratings. We demonstrate that this architecture follows predictable scaling\ntrends and correlates lower validation loss to improved text-to-image synthesis\nas measured by various metrics and human evaluations. Our largest models\noutperform state-of-the-art models, and we will make our experimental data,\ncode, and model weights publicly available.\n", "link": "http://arxiv.org/abs/2403.03206v1", "date": "2024-03-05", "relevancy": 2.2997, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6075}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5636}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5468}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Rectified%20Flow%20Transformers%20for%20High-Resolution%20Image%20Synthesis&entry.906535625=Patrick%20Esser%20and%20Sumith%20Kulal%20and%20Andreas%20Blattmann%20and%20Rahim%20Entezari%20and%20Jonas%20M%C3%BCller%20and%20Harry%20Saini%20and%20Yam%20Levi%20and%20Dominik%20Lorenz%20and%20Axel%20Sauer%20and%20Frederic%20Boesel%20and%20Dustin%20Podell%20and%20Tim%20Dockhorn%20and%20Zion%20English%20and%20Kyle%20Lacey%20and%20Alex%20Goodwin%20and%20Yannik%20Marek%20and%20Robin%20Rombach&entry.1292438233=%20%20Diffusion%20models%20create%20data%20from%20noise%20by%20inverting%20the%20forward%20paths%20of%0Adata%20towards%20noise%20and%20have%20emerged%20as%20a%20powerful%20generative%20modeling%20technique%0Afor%20high-dimensional%2C%20perceptual%20data%20such%20as%20images%20and%20videos.%20Rectified%20flow%0Ais%20a%20recent%20generative%20model%20formulation%20that%20connects%20data%20and%20noise%20in%20a%0Astraight%20line.%20Despite%20its%20better%20theoretical%20properties%20and%20conceptual%0Asimplicity%2C%20it%20is%20not%20yet%20decisively%20established%20as%20standard%20practice.%20In%20this%0Awork%2C%20we%20improve%20existing%20noise%20sampling%20techniques%20for%20training%20rectified%20flow%0Amodels%20by%20biasing%20them%20towards%20perceptually%20relevant%20scales.%20Through%20a%0Alarge-scale%20study%2C%20we%20demonstrate%20the%20superior%20performance%20of%20this%20approach%0Acompared%20to%20established%20diffusion%20formulations%20for%20high-resolution%0Atext-to-image%20synthesis.%20Additionally%2C%20we%20present%20a%20novel%20transformer-based%0Aarchitecture%20for%20text-to-image%20generation%20that%20uses%20separate%20weights%20for%20the%0Atwo%20modalities%20and%20enables%20a%20bidirectional%20flow%20of%20information%20between%20image%0Aand%20text%20tokens%2C%20improving%20text%20comprehension%2C%20typography%2C%20and%20human%20preference%0Aratings.%20We%20demonstrate%20that%20this%20architecture%20follows%20predictable%20scaling%0Atrends%20and%20correlates%20lower%20validation%20loss%20to%20improved%20text-to-image%20synthesis%0Aas%20measured%20by%20various%20metrics%20and%20human%20evaluations.%20Our%20largest%20models%0Aoutperform%20state-of-the-art%20models%2C%20and%20we%20will%20make%20our%20experimental%20data%2C%0Acode%2C%20and%20model%20weights%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03206v1&entry.124074799=Read"},
{"title": "Simplicity in Complexity", "author": "Kevin Shen and Surabhi S Nath and Aenne Brielmann and Peter Dayan", "abstract": "  The complexity of visual stimuli plays an important role in many cognitive\nphenomena, including attention, engagement, memorability, time perception and\naesthetic evaluation. Despite its importance, complexity is poorly understood\nand ironically, previous models of image complexity have been quite\n\\textit{complex}. There have been many attempts to find handcrafted features\nthat explain complexity, but these features are usually dataset specific, and\nhence fail to generalise. On the other hand, more recent work has employed deep\nneural networks to predict complexity, but these models remain difficult to\ninterpret, and do not guide a theoretical understanding of the problem. Here we\npropose to model complexity using segment-based representations of images. We\nuse state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the\nnumber of segments at multiple granularities, and the number of classes in an\nimage respectively. We find that complexity is well-explained by a simple\nlinear model with these two features across six diverse image-sets of\nnaturalistic scene and art images. This suggests that the complexity of images\ncan be surprisingly simple.\n", "link": "http://arxiv.org/abs/2403.03134v1", "date": "2024-03-05", "relevancy": 2.2973, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4838}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4558}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4389}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplicity%20in%20Complexity&entry.906535625=Kevin%20Shen%20and%20Surabhi%20S%20Nath%20and%20Aenne%20Brielmann%20and%20Peter%20Dayan&entry.1292438233=%20%20The%20complexity%20of%20visual%20stimuli%20plays%20an%20important%20role%20in%20many%20cognitive%0Aphenomena%2C%20including%20attention%2C%20engagement%2C%20memorability%2C%20time%20perception%20and%0Aaesthetic%20evaluation.%20Despite%20its%20importance%2C%20complexity%20is%20poorly%20understood%0Aand%20ironically%2C%20previous%20models%20of%20image%20complexity%20have%20been%20quite%0A%5Ctextit%7Bcomplex%7D.%20There%20have%20been%20many%20attempts%20to%20find%20handcrafted%20features%0Athat%20explain%20complexity%2C%20but%20these%20features%20are%20usually%20dataset%20specific%2C%20and%0Ahence%20fail%20to%20generalise.%20On%20the%20other%20hand%2C%20more%20recent%20work%20has%20employed%20deep%0Aneural%20networks%20to%20predict%20complexity%2C%20but%20these%20models%20remain%20difficult%20to%0Ainterpret%2C%20and%20do%20not%20guide%20a%20theoretical%20understanding%20of%20the%20problem.%20Here%20we%0Apropose%20to%20model%20complexity%20using%20segment-based%20representations%20of%20images.%20We%0Ause%20state-of-the-art%20segmentation%20models%2C%20SAM%20and%20FC-CLIP%2C%20to%20quantify%20the%0Anumber%20of%20segments%20at%20multiple%20granularities%2C%20and%20the%20number%20of%20classes%20in%20an%0Aimage%20respectively.%20We%20find%20that%20complexity%20is%20well-explained%20by%20a%20simple%0Alinear%20model%20with%20these%20two%20features%20across%20six%20diverse%20image-sets%20of%0Anaturalistic%20scene%20and%20art%20images.%20This%20suggests%20that%20the%20complexity%20of%20images%0Acan%20be%20surprisingly%20simple.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03134v1&entry.124074799=Read"},
{"title": "GroundingGPT:Language Enhanced Multi-modal Grounding Model", "author": "Zhaowei Li and Qi Xu and Dong Zhang and Hang Song and Yiqing Cai and Qi Qi and Ran Zhou and Junting Pan and Zefeng Li and Van Tu Vu and Zhida Huang and Tao Wang", "abstract": "  Multi-modal large language models have demonstrated impressive performance\nacross various tasks in different modalities. However, existing multi-modal\nmodels primarily emphasize capturing global information within each modality\nwhile neglecting the importance of perceiving local information across\nmodalities. Consequently, these models lack the ability to effectively\nunderstand the fine-grained details of input data, limiting their performance\nin tasks that require a more nuanced understanding. To address this limitation,\nthere is a compelling need to develop models that enable fine-grained\nunderstanding across multiple modalities, thereby enhancing their applicability\nto a wide range of tasks. In this paper, we propose GroundingGPT, a language\nenhanced multi-modal grounding model. Beyond capturing global information like\nother multi-modal models, our proposed model excels at tasks demanding a\ndetailed understanding of local information within the input. It demonstrates\nprecise identification and localization of specific regions in images or\nmoments in videos. To achieve this objective, we design a diversified dataset\nconstruction pipeline, resulting in a multi-modal, multi-granularity dataset\nfor model training. The code, dataset, and demo of our model can be found at\nhttps: //github.com/lzw-lzw/GroundingGPT.\n", "link": "http://arxiv.org/abs/2401.06071v5", "date": "2024-03-05", "relevancy": 2.2738, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5735}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.555}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroundingGPT%3ALanguage%20Enhanced%20Multi-modal%20Grounding%20Model&entry.906535625=Zhaowei%20Li%20and%20Qi%20Xu%20and%20Dong%20Zhang%20and%20Hang%20Song%20and%20Yiqing%20Cai%20and%20Qi%20Qi%20and%20Ran%20Zhou%20and%20Junting%20Pan%20and%20Zefeng%20Li%20and%20Van%20Tu%20Vu%20and%20Zhida%20Huang%20and%20Tao%20Wang&entry.1292438233=%20%20Multi-modal%20large%20language%20models%20have%20demonstrated%20impressive%20performance%0Aacross%20various%20tasks%20in%20different%20modalities.%20However%2C%20existing%20multi-modal%0Amodels%20primarily%20emphasize%20capturing%20global%20information%20within%20each%20modality%0Awhile%20neglecting%20the%20importance%20of%20perceiving%20local%20information%20across%0Amodalities.%20Consequently%2C%20these%20models%20lack%20the%20ability%20to%20effectively%0Aunderstand%20the%20fine-grained%20details%20of%20input%20data%2C%20limiting%20their%20performance%0Ain%20tasks%20that%20require%20a%20more%20nuanced%20understanding.%20To%20address%20this%20limitation%2C%0Athere%20is%20a%20compelling%20need%20to%20develop%20models%20that%20enable%20fine-grained%0Aunderstanding%20across%20multiple%20modalities%2C%20thereby%20enhancing%20their%20applicability%0Ato%20a%20wide%20range%20of%20tasks.%20In%20this%20paper%2C%20we%20propose%20GroundingGPT%2C%20a%20language%0Aenhanced%20multi-modal%20grounding%20model.%20Beyond%20capturing%20global%20information%20like%0Aother%20multi-modal%20models%2C%20our%20proposed%20model%20excels%20at%20tasks%20demanding%20a%0Adetailed%20understanding%20of%20local%20information%20within%20the%20input.%20It%20demonstrates%0Aprecise%20identification%20and%20localization%20of%20specific%20regions%20in%20images%20or%0Amoments%20in%20videos.%20To%20achieve%20this%20objective%2C%20we%20design%20a%20diversified%20dataset%0Aconstruction%20pipeline%2C%20resulting%20in%20a%20multi-modal%2C%20multi-granularity%20dataset%0Afor%20model%20training.%20The%20code%2C%20dataset%2C%20and%20demo%20of%20our%20model%20can%20be%20found%20at%0Ahttps%3A%20//github.com/lzw-lzw/GroundingGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06071v5&entry.124074799=Read"},
{"title": "Rethinking Clustered Federated Learning in NOMA Enhanced Wireless\n  Networks", "author": "Yushen Lin and Kaidi Wang and Zhiguo Ding", "abstract": "  This study explores the benefits of integrating the novel clustered federated\nlearning (CFL) approach with non-orthogonal multiple access (NOMA) under\nnon-independent and identically distributed (non-IID) datasets, where multiple\ndevices participate in the aggregation with time limitations and a finite\nnumber of sub-channels. A detailed theoretical analysis of the generalization\ngap that measures the degree of non-IID in the data distribution is presented.\nFollowing that, solutions to address the challenges posed by non-IID conditions\nare proposed with the analysis of the properties. Specifically, users' data\ndistributions are parameterized as concentration parameters and grouped using\nspectral clustering, with Dirichlet distribution serving as the prior. The\ninvestigation into the generalization gap and convergence rate guides the\ndesign of sub-channel assignments through the matching-based algorithm, and the\npower allocation is achieved by Karush-Kuhn-Tucker (KKT) conditions with the\nderived closed-form solution. The extensive simulation results show that the\nproposed cluster-based FL framework can outperform FL baselines in terms of\nboth test accuracy and convergence rate. Moreover, jointly optimizing\nsub-channel and power allocation in NOMA-enhanced networks can lead to a\nsignificant improvement.\n", "link": "http://arxiv.org/abs/2403.03157v1", "date": "2024-03-05", "relevancy": 2.2662, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4596}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4544}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4457}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Clustered%20Federated%20Learning%20in%20NOMA%20Enhanced%20Wireless%0A%20%20Networks&entry.906535625=Yushen%20Lin%20and%20Kaidi%20Wang%20and%20Zhiguo%20Ding&entry.1292438233=%20%20This%20study%20explores%20the%20benefits%20of%20integrating%20the%20novel%20clustered%20federated%0Alearning%20%28CFL%29%20approach%20with%20non-orthogonal%20multiple%20access%20%28NOMA%29%20under%0Anon-independent%20and%20identically%20distributed%20%28non-IID%29%20datasets%2C%20where%20multiple%0Adevices%20participate%20in%20the%20aggregation%20with%20time%20limitations%20and%20a%20finite%0Anumber%20of%20sub-channels.%20A%20detailed%20theoretical%20analysis%20of%20the%20generalization%0Agap%20that%20measures%20the%20degree%20of%20non-IID%20in%20the%20data%20distribution%20is%20presented.%0AFollowing%20that%2C%20solutions%20to%20address%20the%20challenges%20posed%20by%20non-IID%20conditions%0Aare%20proposed%20with%20the%20analysis%20of%20the%20properties.%20Specifically%2C%20users%27%20data%0Adistributions%20are%20parameterized%20as%20concentration%20parameters%20and%20grouped%20using%0Aspectral%20clustering%2C%20with%20Dirichlet%20distribution%20serving%20as%20the%20prior.%20The%0Ainvestigation%20into%20the%20generalization%20gap%20and%20convergence%20rate%20guides%20the%0Adesign%20of%20sub-channel%20assignments%20through%20the%20matching-based%20algorithm%2C%20and%20the%0Apower%20allocation%20is%20achieved%20by%20Karush-Kuhn-Tucker%20%28KKT%29%20conditions%20with%20the%0Aderived%20closed-form%20solution.%20The%20extensive%20simulation%20results%20show%20that%20the%0Aproposed%20cluster-based%20FL%20framework%20can%20outperform%20FL%20baselines%20in%20terms%20of%0Aboth%20test%20accuracy%20and%20convergence%20rate.%20Moreover%2C%20jointly%20optimizing%0Asub-channel%20and%20power%20allocation%20in%20NOMA-enhanced%20networks%20can%20lead%20to%20a%0Asignificant%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03157v1&entry.124074799=Read"},
{"title": "Online Learning of Human Constraints from Feedback in Shared Autonomy", "author": "Shibei Zhu and Tran Nguyen Le and Samuel Kaski and Ville Kyrki", "abstract": "  Real-time collaboration with humans poses challenges due to the different\nbehavior patterns of humans resulting from diverse physical constraints.\nExisting works typically focus on learning safety constraints for\ncollaboration, or how to divide and distribute the subtasks between the\nparticipating agents to carry out the main task. In contrast, we propose to\nlearn a human constraints model that, in addition, considers the diverse\nbehaviors of different human operators. We consider a type of collaboration in\na shared-autonomy fashion, where both a human operator and an assistive robot\nact simultaneously in the same task space that affects each other's actions.\nThe task of the assistive agent is to augment the skill of humans to perform a\nshared task by supporting humans as much as possible, both in terms of reducing\nthe workload and minimizing the discomfort for the human operator. Therefore,\nwe propose an augmentative assistant agent capable of learning and adapting to\nhuman physical constraints, aligning its actions with the ergonomic preferences\nand limitations of the human operator.\n", "link": "http://arxiv.org/abs/2403.02974v1", "date": "2024-03-05", "relevancy": 2.2643, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.576}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5643}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Learning%20of%20Human%20Constraints%20from%20Feedback%20in%20Shared%20Autonomy&entry.906535625=Shibei%20Zhu%20and%20Tran%20Nguyen%20Le%20and%20Samuel%20Kaski%20and%20Ville%20Kyrki&entry.1292438233=%20%20Real-time%20collaboration%20with%20humans%20poses%20challenges%20due%20to%20the%20different%0Abehavior%20patterns%20of%20humans%20resulting%20from%20diverse%20physical%20constraints.%0AExisting%20works%20typically%20focus%20on%20learning%20safety%20constraints%20for%0Acollaboration%2C%20or%20how%20to%20divide%20and%20distribute%20the%20subtasks%20between%20the%0Aparticipating%20agents%20to%20carry%20out%20the%20main%20task.%20In%20contrast%2C%20we%20propose%20to%0Alearn%20a%20human%20constraints%20model%20that%2C%20in%20addition%2C%20considers%20the%20diverse%0Abehaviors%20of%20different%20human%20operators.%20We%20consider%20a%20type%20of%20collaboration%20in%0Aa%20shared-autonomy%20fashion%2C%20where%20both%20a%20human%20operator%20and%20an%20assistive%20robot%0Aact%20simultaneously%20in%20the%20same%20task%20space%20that%20affects%20each%20other%27s%20actions.%0AThe%20task%20of%20the%20assistive%20agent%20is%20to%20augment%20the%20skill%20of%20humans%20to%20perform%20a%0Ashared%20task%20by%20supporting%20humans%20as%20much%20as%20possible%2C%20both%20in%20terms%20of%20reducing%0Athe%20workload%20and%20minimizing%20the%20discomfort%20for%20the%20human%20operator.%20Therefore%2C%0Awe%20propose%20an%20augmentative%20assistant%20agent%20capable%20of%20learning%20and%20adapting%20to%0Ahuman%20physical%20constraints%2C%20aligning%20its%20actions%20with%20the%20ergonomic%20preferences%0Aand%20limitations%20of%20the%20human%20operator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02974v1&entry.124074799=Read"},
{"title": "AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN\n  Accelerators", "author": "Mahdi Taheri and Natalia Cherezova and Samira Nazari and Ahsan Rafiq and Ali Azarpeyvand and Tara Ghasempouri and Masoud Daneshtalab and Jaan Raik and Maksim Jenihhin", "abstract": "  In this paper, we propose an architecture of a novel adaptive fault-tolerant\napproximate multiplier tailored for ASIC-based DNN accelerators.\n", "link": "http://arxiv.org/abs/2403.02936v1", "date": "2024-03-05", "relevancy": 2.2621, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.472}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4625}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4228}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdAM%3A%20Adaptive%20Fault-Tolerant%20Approximate%20Multiplier%20for%20Edge%20DNN%0A%20%20Accelerators&entry.906535625=Mahdi%20Taheri%20and%20Natalia%20Cherezova%20and%20Samira%20Nazari%20and%20Ahsan%20Rafiq%20and%20Ali%20Azarpeyvand%20and%20Tara%20Ghasempouri%20and%20Masoud%20Daneshtalab%20and%20Jaan%20Raik%20and%20Maksim%20Jenihhin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20architecture%20of%20a%20novel%20adaptive%20fault-tolerant%0Aapproximate%20multiplier%20tailored%20for%20ASIC-based%20DNN%20accelerators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02936v1&entry.124074799=Read"},
{"title": "Towards Motion Forecasting with Real-World Perception Inputs: Are\n  End-to-End Approaches Competitive?", "author": "Yihong Xu and Lo\u00efck Chambon and \u00c9loi Zablocki and Micka\u00ebl Chen and Alexandre Alahi and Matthieu Cord and Patrick P\u00e9rez", "abstract": "  Motion forecasting is crucial in enabling autonomous vehicles to anticipate\nthe future trajectories of surrounding agents. To do so, it requires solving\nmapping, detection, tracking, and then forecasting problems, in a multi-step\npipeline. In this complex system, advances in conventional forecasting methods\nhave been made using curated data, i.e., with the assumption of perfect maps,\ndetection, and tracking. This paradigm, however, ignores any errors from\nupstream modules. Meanwhile, an emerging end-to-end paradigm, that tightly\nintegrates the perception and forecasting architectures into joint training,\npromises to solve this issue. However, the evaluation protocols between the two\nmethods were so far incompatible and their comparison was not possible. In\nfact, conventional forecasting methods are usually not trained nor tested in\nreal-world pipelines (e.g., with upstream detection, tracking, and mapping\nmodules). In this work, we aim to bring forecasting models closer to the\nreal-world deployment. First, we propose a unified evaluation pipeline for\nforecasting methods with real-world perception inputs, allowing us to compare\nconventional and end-to-end methods for the first time. Second, our in-depth\nstudy uncovers a substantial performance gap when transitioning from curated to\nperception-based data. In particular, we show that this gap (1) stems not only\nfrom differences in precision but also from the nature of imperfect inputs\nprovided by perception modules, and that (2) is not trivially reduced by simply\nfinetuning on perception outputs. Based on extensive experiments, we provide\nrecommendations for critical areas that require improvement and guidance\ntowards more robust motion forecasting in the real world. The evaluation\nlibrary for benchmarking models under standardized and practical conditions is\nprovided: \\url{https://github.com/valeoai/MFEval}.\n", "link": "http://arxiv.org/abs/2306.09281v4", "date": "2024-03-05", "relevancy": 2.2472, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5779}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5258}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Motion%20Forecasting%20with%20Real-World%20Perception%20Inputs%3A%20Are%0A%20%20End-to-End%20Approaches%20Competitive%3F&entry.906535625=Yihong%20Xu%20and%20Lo%C3%AFck%20Chambon%20and%20%C3%89loi%20Zablocki%20and%20Micka%C3%ABl%20Chen%20and%20Alexandre%20Alahi%20and%20Matthieu%20Cord%20and%20Patrick%20P%C3%A9rez&entry.1292438233=%20%20Motion%20forecasting%20is%20crucial%20in%20enabling%20autonomous%20vehicles%20to%20anticipate%0Athe%20future%20trajectories%20of%20surrounding%20agents.%20To%20do%20so%2C%20it%20requires%20solving%0Amapping%2C%20detection%2C%20tracking%2C%20and%20then%20forecasting%20problems%2C%20in%20a%20multi-step%0Apipeline.%20In%20this%20complex%20system%2C%20advances%20in%20conventional%20forecasting%20methods%0Ahave%20been%20made%20using%20curated%20data%2C%20i.e.%2C%20with%20the%20assumption%20of%20perfect%20maps%2C%0Adetection%2C%20and%20tracking.%20This%20paradigm%2C%20however%2C%20ignores%20any%20errors%20from%0Aupstream%20modules.%20Meanwhile%2C%20an%20emerging%20end-to-end%20paradigm%2C%20that%20tightly%0Aintegrates%20the%20perception%20and%20forecasting%20architectures%20into%20joint%20training%2C%0Apromises%20to%20solve%20this%20issue.%20However%2C%20the%20evaluation%20protocols%20between%20the%20two%0Amethods%20were%20so%20far%20incompatible%20and%20their%20comparison%20was%20not%20possible.%20In%0Afact%2C%20conventional%20forecasting%20methods%20are%20usually%20not%20trained%20nor%20tested%20in%0Areal-world%20pipelines%20%28e.g.%2C%20with%20upstream%20detection%2C%20tracking%2C%20and%20mapping%0Amodules%29.%20In%20this%20work%2C%20we%20aim%20to%20bring%20forecasting%20models%20closer%20to%20the%0Areal-world%20deployment.%20First%2C%20we%20propose%20a%20unified%20evaluation%20pipeline%20for%0Aforecasting%20methods%20with%20real-world%20perception%20inputs%2C%20allowing%20us%20to%20compare%0Aconventional%20and%20end-to-end%20methods%20for%20the%20first%20time.%20Second%2C%20our%20in-depth%0Astudy%20uncovers%20a%20substantial%20performance%20gap%20when%20transitioning%20from%20curated%20to%0Aperception-based%20data.%20In%20particular%2C%20we%20show%20that%20this%20gap%20%281%29%20stems%20not%20only%0Afrom%20differences%20in%20precision%20but%20also%20from%20the%20nature%20of%20imperfect%20inputs%0Aprovided%20by%20perception%20modules%2C%20and%20that%20%282%29%20is%20not%20trivially%20reduced%20by%20simply%0Afinetuning%20on%20perception%20outputs.%20Based%20on%20extensive%20experiments%2C%20we%20provide%0Arecommendations%20for%20critical%20areas%20that%20require%20improvement%20and%20guidance%0Atowards%20more%20robust%20motion%20forecasting%20in%20the%20real%20world.%20The%20evaluation%0Alibrary%20for%20benchmarking%20models%20under%20standardized%20and%20practical%20conditions%20is%0Aprovided%3A%20%5Curl%7Bhttps%3A//github.com/valeoai/MFEval%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09281v4&entry.124074799=Read"},
{"title": "Optimal Transport on the Lie Group of Roto-translations", "author": "Daan Bon and Gautam Pai and Gijs Bellaard and Olga Mula and Remco Duits", "abstract": "  The roto-translation group SE2 has been of active interest in image analysis\ndue to methods that lift the image data to multi-orientation representations\ndefined on this Lie group. This has led to impactful applications of\ncrossing-preserving flows for image de-noising, geodesic tracking, and\nroto-translation equivariant deep learning. In this paper, we develop a\ncomputational framework for optimal transportation over Lie groups, with a\nspecial focus on SE2. We make several theoretical contributions (generalizable\nto matrix Lie groups) such as the non-optimality of group actions as transport\nmaps, invariance and equivariance of optimal transport, and the quality of the\nentropic-regularized optimal transport plan using geodesic distance\napproximations. We develop a Sinkhorn like algorithm that can be efficiently\nimplemented using fast and accurate distance approximations of the Lie group\nand GPU-friendly group convolutions. We report valuable advancements in the\nexperiments on 1) image barycentric interpolation, 2) interpolation of planar\norientation fields, and 3) Wasserstein gradient flows on SE2. We observe that\nour framework of lifting images to SE2 and optimal transport with\nleft-invariant anisotropic metrics leads to equivariant transport along\ndominant contours and salient line structures in the image. This yields sharper\nand more meaningful interpolations compared to their counterparts on R^2\n", "link": "http://arxiv.org/abs/2402.15322v2", "date": "2024-03-05", "relevancy": 2.2277, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4349}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4243}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Transport%20on%20the%20Lie%20Group%20of%20Roto-translations&entry.906535625=Daan%20Bon%20and%20Gautam%20Pai%20and%20Gijs%20Bellaard%20and%20Olga%20Mula%20and%20Remco%20Duits&entry.1292438233=%20%20The%20roto-translation%20group%20SE2%20has%20been%20of%20active%20interest%20in%20image%20analysis%0Adue%20to%20methods%20that%20lift%20the%20image%20data%20to%20multi-orientation%20representations%0Adefined%20on%20this%20Lie%20group.%20This%20has%20led%20to%20impactful%20applications%20of%0Acrossing-preserving%20flows%20for%20image%20de-noising%2C%20geodesic%20tracking%2C%20and%0Aroto-translation%20equivariant%20deep%20learning.%20In%20this%20paper%2C%20we%20develop%20a%0Acomputational%20framework%20for%20optimal%20transportation%20over%20Lie%20groups%2C%20with%20a%0Aspecial%20focus%20on%20SE2.%20We%20make%20several%20theoretical%20contributions%20%28generalizable%0Ato%20matrix%20Lie%20groups%29%20such%20as%20the%20non-optimality%20of%20group%20actions%20as%20transport%0Amaps%2C%20invariance%20and%20equivariance%20of%20optimal%20transport%2C%20and%20the%20quality%20of%20the%0Aentropic-regularized%20optimal%20transport%20plan%20using%20geodesic%20distance%0Aapproximations.%20We%20develop%20a%20Sinkhorn%20like%20algorithm%20that%20can%20be%20efficiently%0Aimplemented%20using%20fast%20and%20accurate%20distance%20approximations%20of%20the%20Lie%20group%0Aand%20GPU-friendly%20group%20convolutions.%20We%20report%20valuable%20advancements%20in%20the%0Aexperiments%20on%201%29%20image%20barycentric%20interpolation%2C%202%29%20interpolation%20of%20planar%0Aorientation%20fields%2C%20and%203%29%20Wasserstein%20gradient%20flows%20on%20SE2.%20We%20observe%20that%0Aour%20framework%20of%20lifting%20images%20to%20SE2%20and%20optimal%20transport%20with%0Aleft-invariant%20anisotropic%20metrics%20leads%20to%20equivariant%20transport%20along%0Adominant%20contours%20and%20salient%20line%20structures%20in%20the%20image.%20This%20yields%20sharper%0Aand%20more%20meaningful%20interpolations%20compared%20to%20their%20counterparts%20on%20R%5E2%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15322v2&entry.124074799=Read"},
{"title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model\n  Pre-training", "author": "Hong Liu and Zhiyuan Li and David Hall and Percy Liang and Tengyu Ma", "abstract": "  Given the massive cost of language model pre-training, a non-trivial\nimprovement of the optimization algorithm would lead to a material reduction on\nthe time and cost of training. Adam and its variants have been state-of-the-art\nfor years, and more sophisticated second-order (Hessian-based) optimizers often\nincur too much per-step overhead. In this paper, we propose Sophia,\nSecond-order Clipped Stochastic Optimization, a simple scalable second-order\noptimizer that uses a light-weight estimate of the diagonal Hessian as the\npre-conditioner. The update is the moving average of the gradients divided by\nthe moving average of the estimated Hessian, followed by element-wise clipping.\nThe clipping controls the worst-case update size and tames the negative impact\nof non-convexity and rapid change of Hessian along the trajectory. Sophia only\nestimates the diagonal Hessian every handful of iterations, which has\nnegligible average per-step time and memory overhead. On language modeling with\nGPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up\ncompared to Adam in the number of steps, total compute, and wall-clock time,\nachieving the same perplexity with 50% fewer steps, less total compute, and\nreduced wall-clock time. Theoretically, we show that Sophia, in a much\nsimplified setting, adapts to the heterogeneous curvatures in different\nparameter dimensions, and thus has a run-time bound that does not depend on the\ncondition number of the loss.\n", "link": "http://arxiv.org/abs/2305.14342v4", "date": "2024-03-05", "relevancy": 2.2261, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4497}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4465}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4395}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sophia%3A%20A%20Scalable%20Stochastic%20Second-order%20Optimizer%20for%20Language%20Model%0A%20%20Pre-training&entry.906535625=Hong%20Liu%20and%20Zhiyuan%20Li%20and%20David%20Hall%20and%20Percy%20Liang%20and%20Tengyu%20Ma&entry.1292438233=%20%20Given%20the%20massive%20cost%20of%20language%20model%20pre-training%2C%20a%20non-trivial%0Aimprovement%20of%20the%20optimization%20algorithm%20would%20lead%20to%20a%20material%20reduction%20on%0Athe%20time%20and%20cost%20of%20training.%20Adam%20and%20its%20variants%20have%20been%20state-of-the-art%0Afor%20years%2C%20and%20more%20sophisticated%20second-order%20%28Hessian-based%29%20optimizers%20often%0Aincur%20too%20much%20per-step%20overhead.%20In%20this%20paper%2C%20we%20propose%20Sophia%2C%0ASecond-order%20Clipped%20Stochastic%20Optimization%2C%20a%20simple%20scalable%20second-order%0Aoptimizer%20that%20uses%20a%20light-weight%20estimate%20of%20the%20diagonal%20Hessian%20as%20the%0Apre-conditioner.%20The%20update%20is%20the%20moving%20average%20of%20the%20gradients%20divided%20by%0Athe%20moving%20average%20of%20the%20estimated%20Hessian%2C%20followed%20by%20element-wise%20clipping.%0AThe%20clipping%20controls%20the%20worst-case%20update%20size%20and%20tames%20the%20negative%20impact%0Aof%20non-convexity%20and%20rapid%20change%20of%20Hessian%20along%20the%20trajectory.%20Sophia%20only%0Aestimates%20the%20diagonal%20Hessian%20every%20handful%20of%20iterations%2C%20which%20has%0Anegligible%20average%20per-step%20time%20and%20memory%20overhead.%20On%20language%20modeling%20with%0AGPT%20models%20of%20sizes%20ranging%20from%20125M%20to%201.5B%2C%20Sophia%20achieves%20a%202x%20speed-up%0Acompared%20to%20Adam%20in%20the%20number%20of%20steps%2C%20total%20compute%2C%20and%20wall-clock%20time%2C%0Aachieving%20the%20same%20perplexity%20with%2050%25%20fewer%20steps%2C%20less%20total%20compute%2C%20and%0Areduced%20wall-clock%20time.%20Theoretically%2C%20we%20show%20that%20Sophia%2C%20in%20a%20much%0Asimplified%20setting%2C%20adapts%20to%20the%20heterogeneous%20curvatures%20in%20different%0Aparameter%20dimensions%2C%20and%20thus%20has%20a%20run-time%20bound%20that%20does%20not%20depend%20on%20the%0Acondition%20number%20of%20the%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.14342v4&entry.124074799=Read"},
{"title": "Shuffling Momentum Gradient Algorithm for Convex Optimization", "author": "Trang H. Tran and Quoc Tran-Dinh and Lam M. Nguyen", "abstract": "  The Stochastic Gradient Descent method (SGD) and its stochastic variants have\nbecome methods of choice for solving finite-sum optimization problems arising\nfrom machine learning and data science thanks to their ability to handle\nlarge-scale applications and big datasets. In the last decades, researchers\nhave made substantial effort to study the theoretical performance of SGD and\nits shuffling variants. However, only limited work has investigated its\nshuffling momentum variants, including shuffling heavy-ball momentum schemes\nfor non-convex problems and Nesterov's momentum for convex settings. In this\nwork, we extend the analysis of the shuffling momentum gradient method\ndeveloped in [Tran et al (2021)] to both finite-sum convex and strongly convex\noptimization problems. We provide the first analysis of shuffling\nmomentum-based methods for the strongly convex setting, attaining a convergence\nrate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number\nof training epochs. Our analysis is a state-of-the-art, matching the best rates\nof existing shuffling stochastic gradient algorithms in the literature.\n", "link": "http://arxiv.org/abs/2403.03180v1", "date": "2024-03-05", "relevancy": 2.2173, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4487}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.447}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4347}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shuffling%20Momentum%20Gradient%20Algorithm%20for%20Convex%20Optimization&entry.906535625=Trang%20H.%20Tran%20and%20Quoc%20Tran-Dinh%20and%20Lam%20M.%20Nguyen&entry.1292438233=%20%20The%20Stochastic%20Gradient%20Descent%20method%20%28SGD%29%20and%20its%20stochastic%20variants%20have%0Abecome%20methods%20of%20choice%20for%20solving%20finite-sum%20optimization%20problems%20arising%0Afrom%20machine%20learning%20and%20data%20science%20thanks%20to%20their%20ability%20to%20handle%0Alarge-scale%20applications%20and%20big%20datasets.%20In%20the%20last%20decades%2C%20researchers%0Ahave%20made%20substantial%20effort%20to%20study%20the%20theoretical%20performance%20of%20SGD%20and%0Aits%20shuffling%20variants.%20However%2C%20only%20limited%20work%20has%20investigated%20its%0Ashuffling%20momentum%20variants%2C%20including%20shuffling%20heavy-ball%20momentum%20schemes%0Afor%20non-convex%20problems%20and%20Nesterov%27s%20momentum%20for%20convex%20settings.%20In%20this%0Awork%2C%20we%20extend%20the%20analysis%20of%20the%20shuffling%20momentum%20gradient%20method%0Adeveloped%20in%20%5BTran%20et%20al%20%282021%29%5D%20to%20both%20finite-sum%20convex%20and%20strongly%20convex%0Aoptimization%20problems.%20We%20provide%20the%20first%20analysis%20of%20shuffling%0Amomentum-based%20methods%20for%20the%20strongly%20convex%20setting%2C%20attaining%20a%20convergence%0Arate%20of%20%24O%281/nT%5E2%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20samples%20and%20%24T%24%20is%20the%20number%0Aof%20training%20epochs.%20Our%20analysis%20is%20a%20state-of-the-art%2C%20matching%20the%20best%20rates%0Aof%20existing%20shuffling%20stochastic%20gradient%20algorithms%20in%20the%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03180v1&entry.124074799=Read"},
{"title": "Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation", "author": "Zhekai Du and Xinyao Li and Fengling Li and Ke Lu and Lei Zhu and Jingjing Li", "abstract": "  Conventional Unsupervised Domain Adaptation (UDA) strives to minimize\ndistribution discrepancy between domains, which neglects to harness rich\nsemantics from data and struggles to handle complex domain shifts. A promising\ntechnique is to leverage the knowledge of large-scale pre-trained\nvision-language models for more guided adaptation. Despite some endeavors,\ncurrent methods often learn textual prompts to embed domain semantics for\nsource and target domains separately and perform classification within each\ndomain, limiting cross-domain knowledge transfer. Moreover, prompting only the\nlanguage branch lacks flexibility to adapt both modalities dynamically. To\nbridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit\ndomain-invariant semantics by mutually aligning visual and textual embeddings.\nSpecifically, the image contextual information is utilized to prompt the\nlanguage branch in a domain-agnostic and instance-conditioned way. Meanwhile,\nvisual prompts are imposed based on the domain-agnostic textual prompt to\nelicit domain-invariant visual embeddings. These two branches of prompts are\nlearned mutually with a cross-attention module and regularized with a\nsemantic-consistency loss and an instance-discrimination contrastive loss.\nExperiments on three UDA benchmarks demonstrate the superiority of DAMP over\nstate-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2403.02899v1", "date": "2024-03-05", "relevancy": 2.2148, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5804}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5404}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5323}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Agnostic%20Mutual%20Prompting%20for%20Unsupervised%20Domain%20Adaptation&entry.906535625=Zhekai%20Du%20and%20Xinyao%20Li%20and%20Fengling%20Li%20and%20Ke%20Lu%20and%20Lei%20Zhu%20and%20Jingjing%20Li&entry.1292438233=%20%20Conventional%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20strives%20to%20minimize%0Adistribution%20discrepancy%20between%20domains%2C%20which%20neglects%20to%20harness%20rich%0Asemantics%20from%20data%20and%20struggles%20to%20handle%20complex%20domain%20shifts.%20A%20promising%0Atechnique%20is%20to%20leverage%20the%20knowledge%20of%20large-scale%20pre-trained%0Avision-language%20models%20for%20more%20guided%20adaptation.%20Despite%20some%20endeavors%2C%0Acurrent%20methods%20often%20learn%20textual%20prompts%20to%20embed%20domain%20semantics%20for%0Asource%20and%20target%20domains%20separately%20and%20perform%20classification%20within%20each%0Adomain%2C%20limiting%20cross-domain%20knowledge%20transfer.%20Moreover%2C%20prompting%20only%20the%0Alanguage%20branch%20lacks%20flexibility%20to%20adapt%20both%20modalities%20dynamically.%20To%0Abridge%20this%20gap%2C%20we%20propose%20Domain-Agnostic%20Mutual%20Prompting%20%28DAMP%29%20to%20exploit%0Adomain-invariant%20semantics%20by%20mutually%20aligning%20visual%20and%20textual%20embeddings.%0ASpecifically%2C%20the%20image%20contextual%20information%20is%20utilized%20to%20prompt%20the%0Alanguage%20branch%20in%20a%20domain-agnostic%20and%20instance-conditioned%20way.%20Meanwhile%2C%0Avisual%20prompts%20are%20imposed%20based%20on%20the%20domain-agnostic%20textual%20prompt%20to%0Aelicit%20domain-invariant%20visual%20embeddings.%20These%20two%20branches%20of%20prompts%20are%0Alearned%20mutually%20with%20a%20cross-attention%20module%20and%20regularized%20with%20a%0Asemantic-consistency%20loss%20and%20an%20instance-discrimination%20contrastive%20loss.%0AExperiments%20on%20three%20UDA%20benchmarks%20demonstrate%20the%20superiority%20of%20DAMP%20over%0Astate-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02899v1&entry.124074799=Read"},
{"title": "A Good Feature Extractor Is All You Need for Weakly Supervised Pathology\n  Slide Classification", "author": "Georg W\u00f6lflein and Dyke Ferber and Asier Rabasco Meneghetti and Omar S. M. El Nahhas and Daniel Truhn and Zunamys I. Carrero and David J. Harrison and Ognjen Arandjelovi\u0107 and Jakob N. Kather", "abstract": "  Stain normalisation is thought to be a crucial preprocessing step in\ncomputational pathology pipelines. We question this belief in the context of\nweakly supervised whole slide image classification, motivated by the emergence\nof powerful feature extractors trained using self-supervised learning on\ndiverse pathology datasets. To this end, we performed the most comprehensive\nevaluation of publicly available pathology feature extractors to date,\ninvolving more than 8,000 training runs across nine tasks, five datasets, three\ndownstream architectures, and various preprocessing setups. Notably, we find\nthat omitting stain normalisation and image augmentations does not compromise\ndownstream slide-level classification performance, while incurring substantial\nsavings in memory and compute. Using a new evaluation metric that facilitates\nrelative downstream performance comparison, we identify the best publicly\navailable extractors, and show that their latent spaces are remarkably robust\nto variations in stain and augmentations like rotation. Contrary to previous\npatch-level benchmarking studies, our approach emphasises clinical relevance by\nfocusing on slide-level biomarker prediction tasks in a weakly supervised\nsetting with external validation cohorts. Our findings stand to streamline\ndigital pathology workflows by minimising preprocessing needs and informing the\nselection of feature extractors. Code and data are available at\nhttps://georg.woelflein.eu/good-features.\n", "link": "http://arxiv.org/abs/2311.11772v4", "date": "2024-03-05", "relevancy": 2.212, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4609}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.438}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4284}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Good%20Feature%20Extractor%20Is%20All%20You%20Need%20for%20Weakly%20Supervised%20Pathology%0A%20%20Slide%20Classification&entry.906535625=Georg%20W%C3%B6lflein%20and%20Dyke%20Ferber%20and%20Asier%20Rabasco%20Meneghetti%20and%20Omar%20S.%20M.%20El%20Nahhas%20and%20Daniel%20Truhn%20and%20Zunamys%20I.%20Carrero%20and%20David%20J.%20Harrison%20and%20Ognjen%20Arandjelovi%C4%87%20and%20Jakob%20N.%20Kather&entry.1292438233=%20%20Stain%20normalisation%20is%20thought%20to%20be%20a%20crucial%20preprocessing%20step%20in%0Acomputational%20pathology%20pipelines.%20We%20question%20this%20belief%20in%20the%20context%20of%0Aweakly%20supervised%20whole%20slide%20image%20classification%2C%20motivated%20by%20the%20emergence%0Aof%20powerful%20feature%20extractors%20trained%20using%20self-supervised%20learning%20on%0Adiverse%20pathology%20datasets.%20To%20this%20end%2C%20we%20performed%20the%20most%20comprehensive%0Aevaluation%20of%20publicly%20available%20pathology%20feature%20extractors%20to%20date%2C%0Ainvolving%20more%20than%208%2C000%20training%20runs%20across%20nine%20tasks%2C%20five%20datasets%2C%20three%0Adownstream%20architectures%2C%20and%20various%20preprocessing%20setups.%20Notably%2C%20we%20find%0Athat%20omitting%20stain%20normalisation%20and%20image%20augmentations%20does%20not%20compromise%0Adownstream%20slide-level%20classification%20performance%2C%20while%20incurring%20substantial%0Asavings%20in%20memory%20and%20compute.%20Using%20a%20new%20evaluation%20metric%20that%20facilitates%0Arelative%20downstream%20performance%20comparison%2C%20we%20identify%20the%20best%20publicly%0Aavailable%20extractors%2C%20and%20show%20that%20their%20latent%20spaces%20are%20remarkably%20robust%0Ato%20variations%20in%20stain%20and%20augmentations%20like%20rotation.%20Contrary%20to%20previous%0Apatch-level%20benchmarking%20studies%2C%20our%20approach%20emphasises%20clinical%20relevance%20by%0Afocusing%20on%20slide-level%20biomarker%20prediction%20tasks%20in%20a%20weakly%20supervised%0Asetting%20with%20external%20validation%20cohorts.%20Our%20findings%20stand%20to%20streamline%0Adigital%20pathology%20workflows%20by%20minimising%20preprocessing%20needs%20and%20informing%20the%0Aselection%20of%20feature%20extractors.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//georg.woelflein.eu/good-features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11772v4&entry.124074799=Read"},
{"title": "Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large\n  Language Models", "author": "Gen Luo and Yiyi Zhou and Yuxin Zhang and Xiawu Zheng and Xiaoshuai Sun and Rongrong Ji", "abstract": "  Despite remarkable progress, existing multimodal large language models\n(MLLMs) are still inferior in granular visual recognition. Contrary to previous\nworks, we study this problem from the perspective of image resolution, and\nreveal that a combination of low- and high-resolution visual features can\neffectively mitigate this shortcoming. Based on this observation, we propose a\nnovel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation\n(MRA). In particular, MRA adopts two visual pathways for images with different\nresolutions, where high-resolution visual information is embedded into the\nlow-resolution pathway via the novel mixture-of-resolution adapters\n(MR-Adapters). This design also greatly reduces the input sequence length of\nMLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the\nnew model LLaVA-HR. We conduct extensive experiments on 11 vision-language (VL)\ntasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g.,\n+9.4% on TextVQA. More importantly, both training and inference of LLaVA-HR\nremain efficient with MRA, e.g., 20 training hours and 3$\\times$ inference\nspeed than LLaVA-1.5. Source codes are released at:\nhttps://github.com/luogen1996/LLaVA-HR.\n", "link": "http://arxiv.org/abs/2403.03003v1", "date": "2024-03-05", "relevancy": 2.2056, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5944}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5267}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5183}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feast%20Your%20Eyes%3A%20Mixture-of-Resolution%20Adaptation%20for%20Multimodal%20Large%0A%20%20Language%20Models&entry.906535625=Gen%20Luo%20and%20Yiyi%20Zhou%20and%20Yuxin%20Zhang%20and%20Xiawu%20Zheng%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20Despite%20remarkable%20progress%2C%20existing%20multimodal%20large%20language%20models%0A%28MLLMs%29%20are%20still%20inferior%20in%20granular%20visual%20recognition.%20Contrary%20to%20previous%0Aworks%2C%20we%20study%20this%20problem%20from%20the%20perspective%20of%20image%20resolution%2C%20and%0Areveal%20that%20a%20combination%20of%20low-%20and%20high-resolution%20visual%20features%20can%0Aeffectively%20mitigate%20this%20shortcoming.%20Based%20on%20this%20observation%2C%20we%20propose%20a%0Anovel%20and%20efficient%20method%20for%20MLLMs%2C%20termed%20Mixture-of-Resolution%20Adaptation%0A%28MRA%29.%20In%20particular%2C%20MRA%20adopts%20two%20visual%20pathways%20for%20images%20with%20different%0Aresolutions%2C%20where%20high-resolution%20visual%20information%20is%20embedded%20into%20the%0Alow-resolution%20pathway%20via%20the%20novel%20mixture-of-resolution%20adapters%0A%28MR-Adapters%29.%20This%20design%20also%20greatly%20reduces%20the%20input%20sequence%20length%20of%0AMLLMs.%20To%20validate%20MRA%2C%20we%20apply%20it%20to%20a%20recent%20MLLM%20called%20LLaVA%2C%20and%20term%20the%0Anew%20model%20LLaVA-HR.%20We%20conduct%20extensive%20experiments%20on%2011%20vision-language%20%28VL%29%0Atasks%2C%20which%20show%20that%20LLaVA-HR%20outperforms%20existing%20MLLMs%20on%208%20VL%20tasks%2C%20e.g.%2C%0A%2B9.4%25%20on%20TextVQA.%20More%20importantly%2C%20both%20training%20and%20inference%20of%20LLaVA-HR%0Aremain%20efficient%20with%20MRA%2C%20e.g.%2C%2020%20training%20hours%20and%203%24%5Ctimes%24%20inference%0Aspeed%20than%20LLaVA-1.5.%20Source%20codes%20are%20released%20at%3A%0Ahttps%3A//github.com/luogen1996/LLaVA-HR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03003v1&entry.124074799=Read"},
{"title": "PalmProbNet: A Probabilistic Approach to Understanding Palm\n  Distributions in Ecuadorian Tropical Forest via Transfer Learning", "author": "Kangning Cui and Zishan Shao and Gregory Larsen and Victor Pauca and Sarra Alqahtani and David Segurado and Jo\u00e3o Pinheiro and Manqi Wang and David Lutz and Robert Plemmons and Miles Silman", "abstract": "  Palms play an outsized role in tropical forests and are important resources\nfor humans and wildlife. A central question in tropical ecosystems is\nunderstanding palm distribution and abundance. However, accurately identifying\nand localizing palms in geospatial imagery presents significant challenges due\nto dense vegetation, overlapping canopies, and variable lighting conditions in\nmixed-forest landscapes. Addressing this, we introduce PalmProbNet, a\nprobabilistic approach utilizing transfer learning to analyze high-resolution\nUAV-derived orthomosaic imagery, enabling the detection of palm trees within\nthe dense canopy of the Ecuadorian Rainforest. This approach represents a\nsubstantial advancement in automated palm detection, effectively pinpointing\npalm presence and locality in mixed tropical rainforests. Our process begins by\ngenerating an orthomosaic image from UAV images, from which we extract and\nlabel palm and non-palm image patches in two distinct sizes. These patches are\nthen used to train models with an identical architecture, consisting of an\nunaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with\nspecifically trained parameters. Subsequently, PalmProbNet employs a sliding\nwindow technique on the landscape orthomosaic, using both small and large\nwindow sizes to generate a probability heatmap. This heatmap effectively\nvisualizes the distribution of palms, showcasing the scalability and\nadaptability of our approach in various forest densities. Despite the\nchallenging terrain, our method demonstrated remarkable performance, achieving\nan accuracy of 97.32% and a Cohen's kappa of 94.59% in testing.\n", "link": "http://arxiv.org/abs/2403.03161v1", "date": "2024-03-05", "relevancy": 2.2024, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6134}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5385}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4926}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PalmProbNet%3A%20A%20Probabilistic%20Approach%20to%20Understanding%20Palm%0A%20%20Distributions%20in%20Ecuadorian%20Tropical%20Forest%20via%20Transfer%20Learning&entry.906535625=Kangning%20Cui%20and%20Zishan%20Shao%20and%20Gregory%20Larsen%20and%20Victor%20Pauca%20and%20Sarra%20Alqahtani%20and%20David%20Segurado%20and%20Jo%C3%A3o%20Pinheiro%20and%20Manqi%20Wang%20and%20David%20Lutz%20and%20Robert%20Plemmons%20and%20Miles%20Silman&entry.1292438233=%20%20Palms%20play%20an%20outsized%20role%20in%20tropical%20forests%20and%20are%20important%20resources%0Afor%20humans%20and%20wildlife.%20A%20central%20question%20in%20tropical%20ecosystems%20is%0Aunderstanding%20palm%20distribution%20and%20abundance.%20However%2C%20accurately%20identifying%0Aand%20localizing%20palms%20in%20geospatial%20imagery%20presents%20significant%20challenges%20due%0Ato%20dense%20vegetation%2C%20overlapping%20canopies%2C%20and%20variable%20lighting%20conditions%20in%0Amixed-forest%20landscapes.%20Addressing%20this%2C%20we%20introduce%20PalmProbNet%2C%20a%0Aprobabilistic%20approach%20utilizing%20transfer%20learning%20to%20analyze%20high-resolution%0AUAV-derived%20orthomosaic%20imagery%2C%20enabling%20the%20detection%20of%20palm%20trees%20within%0Athe%20dense%20canopy%20of%20the%20Ecuadorian%20Rainforest.%20This%20approach%20represents%20a%0Asubstantial%20advancement%20in%20automated%20palm%20detection%2C%20effectively%20pinpointing%0Apalm%20presence%20and%20locality%20in%20mixed%20tropical%20rainforests.%20Our%20process%20begins%20by%0Agenerating%20an%20orthomosaic%20image%20from%20UAV%20images%2C%20from%20which%20we%20extract%20and%0Alabel%20palm%20and%20non-palm%20image%20patches%20in%20two%20distinct%20sizes.%20These%20patches%20are%0Athen%20used%20to%20train%20models%20with%20an%20identical%20architecture%2C%20consisting%20of%20an%0Aunaltered%20pre-trained%20ResNet-18%20and%20a%20Multilayer%20Perceptron%20%28MLP%29%20with%0Aspecifically%20trained%20parameters.%20Subsequently%2C%20PalmProbNet%20employs%20a%20sliding%0Awindow%20technique%20on%20the%20landscape%20orthomosaic%2C%20using%20both%20small%20and%20large%0Awindow%20sizes%20to%20generate%20a%20probability%20heatmap.%20This%20heatmap%20effectively%0Avisualizes%20the%20distribution%20of%20palms%2C%20showcasing%20the%20scalability%20and%0Aadaptability%20of%20our%20approach%20in%20various%20forest%20densities.%20Despite%20the%0Achallenging%20terrain%2C%20our%20method%20demonstrated%20remarkable%20performance%2C%20achieving%0Aan%20accuracy%20of%2097.32%25%20and%20a%20Cohen%27s%20kappa%20of%2094.59%25%20in%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03161v1&entry.124074799=Read"},
{"title": "Neural Control System for Continuous Glucose Monitoring and Maintenance", "author": "Azmine Toushik Wasi", "abstract": "  Precise glucose level monitoring is critical for people with diabetes to\navoid serious complications. While there are several methods for continuous\nglucose level monitoring, research on maintenance devices is limited. To\nmitigate the gap, we provide a novel neural control system for continuous\nglucose monitoring and management that uses differential predictive control.\nOur approach, led by a sophisticated neural policy and differentiable modeling,\nconstantly adjusts insulin supply in real-time, thereby improving glucose level\noptimization in the body. This end-to-end method maximizes efficiency,\nproviding personalized care and improved health outcomes, as confirmed by\nempirical evidence.\n", "link": "http://arxiv.org/abs/2402.13852v2", "date": "2024-03-05", "relevancy": 2.1999, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4442}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4409}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4348}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Control%20System%20for%20Continuous%20Glucose%20Monitoring%20and%20Maintenance&entry.906535625=Azmine%20Toushik%20Wasi&entry.1292438233=%20%20Precise%20glucose%20level%20monitoring%20is%20critical%20for%20people%20with%20diabetes%20to%0Aavoid%20serious%20complications.%20While%20there%20are%20several%20methods%20for%20continuous%0Aglucose%20level%20monitoring%2C%20research%20on%20maintenance%20devices%20is%20limited.%20To%0Amitigate%20the%20gap%2C%20we%20provide%20a%20novel%20neural%20control%20system%20for%20continuous%0Aglucose%20monitoring%20and%20management%20that%20uses%20differential%20predictive%20control.%0AOur%20approach%2C%20led%20by%20a%20sophisticated%20neural%20policy%20and%20differentiable%20modeling%2C%0Aconstantly%20adjusts%20insulin%20supply%20in%20real-time%2C%20thereby%20improving%20glucose%20level%0Aoptimization%20in%20the%20body.%20This%20end-to-end%20method%20maximizes%20efficiency%2C%0Aproviding%20personalized%20care%20and%20improved%20health%20outcomes%2C%20as%20confirmed%20by%0Aempirical%20evidence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13852v2&entry.124074799=Read"},
{"title": "ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous\n  Driving", "author": "Han Lu and Xiaosong Jia and Yichen Xie and Wenlong Liao and Xiaokang Yang and Junchi Yan", "abstract": "  End-to-end differentiable learning for autonomous driving (AD) has recently\nbecome a prominent paradigm. One main bottleneck lies in its voracious appetite\nfor high-quality labeled data e.g. 3D bounding boxes and semantic segmentation,\nwhich are notoriously expensive to manually annotate. The difficulty is further\npronounced due to the prominent fact that the behaviors within samples in AD\noften suffer from long tailed distribution. In other words, a large part of\ncollected data can be trivial (e.g. simply driving forward in a straight road)\nand only a few cases are safety-critical. In this paper, we explore a\npractically important yet under-explored problem about how to achieve sample\nand label efficiency for end-to-end AD. Specifically, we design a\nplanning-oriented active learning method which progressively annotates part of\ncollected raw data according to the proposed diversity and usefulness criteria\nfor planning routes. Empirically, we show that our planning-oriented approach\ncould outperform general active learning methods by a large margin. Notably,\nour method achieves comparable performance with state-of-the-art end-to-end AD\nmethods - by using only 30% nuScenes data. We hope our work could inspire\nfuture works to explore end-to-end AD from a data-centric perspective in\naddition to methodology efforts.\n", "link": "http://arxiv.org/abs/2403.02877v1", "date": "2024-03-05", "relevancy": 2.1782, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5483}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActiveAD%3A%20Planning-Oriented%20Active%20Learning%20for%20End-to-End%20Autonomous%0A%20%20Driving&entry.906535625=Han%20Lu%20and%20Xiaosong%20Jia%20and%20Yichen%20Xie%20and%20Wenlong%20Liao%20and%20Xiaokang%20Yang%20and%20Junchi%20Yan&entry.1292438233=%20%20End-to-end%20differentiable%20learning%20for%20autonomous%20driving%20%28AD%29%20has%20recently%0Abecome%20a%20prominent%20paradigm.%20One%20main%20bottleneck%20lies%20in%20its%20voracious%20appetite%0Afor%20high-quality%20labeled%20data%20e.g.%203D%20bounding%20boxes%20and%20semantic%20segmentation%2C%0Awhich%20are%20notoriously%20expensive%20to%20manually%20annotate.%20The%20difficulty%20is%20further%0Apronounced%20due%20to%20the%20prominent%20fact%20that%20the%20behaviors%20within%20samples%20in%20AD%0Aoften%20suffer%20from%20long%20tailed%20distribution.%20In%20other%20words%2C%20a%20large%20part%20of%0Acollected%20data%20can%20be%20trivial%20%28e.g.%20simply%20driving%20forward%20in%20a%20straight%20road%29%0Aand%20only%20a%20few%20cases%20are%20safety-critical.%20In%20this%20paper%2C%20we%20explore%20a%0Apractically%20important%20yet%20under-explored%20problem%20about%20how%20to%20achieve%20sample%0Aand%20label%20efficiency%20for%20end-to-end%20AD.%20Specifically%2C%20we%20design%20a%0Aplanning-oriented%20active%20learning%20method%20which%20progressively%20annotates%20part%20of%0Acollected%20raw%20data%20according%20to%20the%20proposed%20diversity%20and%20usefulness%20criteria%0Afor%20planning%20routes.%20Empirically%2C%20we%20show%20that%20our%20planning-oriented%20approach%0Acould%20outperform%20general%20active%20learning%20methods%20by%20a%20large%20margin.%20Notably%2C%0Aour%20method%20achieves%20comparable%20performance%20with%20state-of-the-art%20end-to-end%20AD%0Amethods%20-%20by%20using%20only%2030%25%20nuScenes%20data.%20We%20hope%20our%20work%20could%20inspire%0Afuture%20works%20to%20explore%20end-to-end%20AD%20from%20a%20data-centric%20perspective%20in%0Aaddition%20to%20methodology%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02877v1&entry.124074799=Read"},
{"title": "Towards General Computer Control: A Multimodal Agent for Red Dead\n  Redemption II as a Case Study", "author": "Weihao Tan and Ziluo Ding and Wentao Zhang and Boyu Li and Bohan Zhou and Junpeng Yue and Haochong Xia and Jiechuan Jiang and Longtao Zheng and Xinrun Xu and Yifei Bi and Pengjie Gu and Xinrun Wang and B\u00f6rje F. Karlsson and Bo An and Zongqing Lu", "abstract": "  Recent studies have demonstrated the success of foundation agents in specific\ntasks or scenarios. However, existing agents cannot generalize across different\nscenarios, mainly due to their diverse observation and action spaces and\nsemantic gaps, or reliance on task-specific resources. In this work, we propose\nthe General Computer Control (GCC) setting: building foundation agents that can\nmaster any computer task by taking only screen images (and possibly audio) of\nthe computer as input, and producing keyboard and mouse operations as output,\nsimilar to human-computer interaction. To target GCC, we propose Cradle, an\nagent framework with strong reasoning abilities, including self-reflection,\ntask inference, and skill curation, to ensure generalizability and\nself-improvement across various tasks. To demonstrate the capabilities of\nCradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as\na preliminary attempt towards GCC with a challenging target. Our agent can\nfollow the main storyline and finish real missions in this complex AAA game,\nwith minimal reliance on prior knowledge and application-specific resources.\nThe project website is at https://baai-agents.github.io/Cradle/.\n", "link": "http://arxiv.org/abs/2403.03186v1", "date": "2024-03-05", "relevancy": 2.1667, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5649}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5535}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5206}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20General%20Computer%20Control%3A%20A%20Multimodal%20Agent%20for%20Red%20Dead%0A%20%20Redemption%20II%20as%20a%20Case%20Study&entry.906535625=Weihao%20Tan%20and%20Ziluo%20Ding%20and%20Wentao%20Zhang%20and%20Boyu%20Li%20and%20Bohan%20Zhou%20and%20Junpeng%20Yue%20and%20Haochong%20Xia%20and%20Jiechuan%20Jiang%20and%20Longtao%20Zheng%20and%20Xinrun%20Xu%20and%20Yifei%20Bi%20and%20Pengjie%20Gu%20and%20Xinrun%20Wang%20and%20B%C3%B6rje%20F.%20Karlsson%20and%20Bo%20An%20and%20Zongqing%20Lu&entry.1292438233=%20%20Recent%20studies%20have%20demonstrated%20the%20success%20of%20foundation%20agents%20in%20specific%0Atasks%20or%20scenarios.%20However%2C%20existing%20agents%20cannot%20generalize%20across%20different%0Ascenarios%2C%20mainly%20due%20to%20their%20diverse%20observation%20and%20action%20spaces%20and%0Asemantic%20gaps%2C%20or%20reliance%20on%20task-specific%20resources.%20In%20this%20work%2C%20we%20propose%0Athe%20General%20Computer%20Control%20%28GCC%29%20setting%3A%20building%20foundation%20agents%20that%20can%0Amaster%20any%20computer%20task%20by%20taking%20only%20screen%20images%20%28and%20possibly%20audio%29%20of%0Athe%20computer%20as%20input%2C%20and%20producing%20keyboard%20and%20mouse%20operations%20as%20output%2C%0Asimilar%20to%20human-computer%20interaction.%20To%20target%20GCC%2C%20we%20propose%20Cradle%2C%20an%0Aagent%20framework%20with%20strong%20reasoning%20abilities%2C%20including%20self-reflection%2C%0Atask%20inference%2C%20and%20skill%20curation%2C%20to%20ensure%20generalizability%20and%0Aself-improvement%20across%20various%20tasks.%20To%20demonstrate%20the%20capabilities%20of%0ACradle%2C%20we%20deploy%20it%20in%20the%20complex%20AAA%20game%20Red%20Dead%20Redemption%20II%2C%20serving%20as%0Aa%20preliminary%20attempt%20towards%20GCC%20with%20a%20challenging%20target.%20Our%20agent%20can%0Afollow%20the%20main%20storyline%20and%20finish%20real%20missions%20in%20this%20complex%20AAA%20game%2C%0Awith%20minimal%20reliance%20on%20prior%20knowledge%20and%20application-specific%20resources.%0AThe%20project%20website%20is%20at%20https%3A//baai-agents.github.io/Cradle/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03186v1&entry.124074799=Read"},
{"title": "Localized Zeroth-Order Prompt Optimization", "author": "Wenyang Hu and Yao Shu and Zongmin Yu and Zhaoxuan Wu and Xiangqiang Lin and Zhongxiang Dai and See-Kiong Ng and Bryan Kian Hsiang Low", "abstract": "  The efficacy of large language models (LLMs) in understanding and generating\nnatural language has aroused a wide interest in developing prompt-based methods\nto harness the power of black-box LLMs. Existing methodologies usually\nprioritize a global optimization for finding the global optimum, which however\nwill perform poorly in certain tasks. This thus motivates us to re-think the\nnecessity of finding a global optimum in prompt optimization. To answer this,\nwe conduct a thorough empirical study on prompt optimization and draw two major\ninsights. Contrasting with the rarity of global optimum, local optima are\nusually prevalent and well-performed, which can be more worthwhile for\nefficient prompt optimization (Insight I). The choice of the input domain,\ncovering both the generation and the representation of prompts, affects the\nidentification of well-performing local optima (Insight II). Inspired by these\ninsights, we propose a novel algorithm, namely localized zeroth-order prompt\noptimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived\nGaussian process into standard zeroth-order optimization for an efficient\nsearch of well-performing local optima in prompt optimization. Remarkably, ZOPO\noutperforms existing baselines in terms of both the optimization performance\nand the query efficiency, which we demonstrate through extensive experiments.\n", "link": "http://arxiv.org/abs/2403.02993v1", "date": "2024-03-05", "relevancy": 2.1617, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4403}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4288}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4279}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localized%20Zeroth-Order%20Prompt%20Optimization&entry.906535625=Wenyang%20Hu%20and%20Yao%20Shu%20and%20Zongmin%20Yu%20and%20Zhaoxuan%20Wu%20and%20Xiangqiang%20Lin%20and%20Zhongxiang%20Dai%20and%20See-Kiong%20Ng%20and%20Bryan%20Kian%20Hsiang%20Low&entry.1292438233=%20%20The%20efficacy%20of%20large%20language%20models%20%28LLMs%29%20in%20understanding%20and%20generating%0Anatural%20language%20has%20aroused%20a%20wide%20interest%20in%20developing%20prompt-based%20methods%0Ato%20harness%20the%20power%20of%20black-box%20LLMs.%20Existing%20methodologies%20usually%0Aprioritize%20a%20global%20optimization%20for%20finding%20the%20global%20optimum%2C%20which%20however%0Awill%20perform%20poorly%20in%20certain%20tasks.%20This%20thus%20motivates%20us%20to%20re-think%20the%0Anecessity%20of%20finding%20a%20global%20optimum%20in%20prompt%20optimization.%20To%20answer%20this%2C%0Awe%20conduct%20a%20thorough%20empirical%20study%20on%20prompt%20optimization%20and%20draw%20two%20major%0Ainsights.%20Contrasting%20with%20the%20rarity%20of%20global%20optimum%2C%20local%20optima%20are%0Ausually%20prevalent%20and%20well-performed%2C%20which%20can%20be%20more%20worthwhile%20for%0Aefficient%20prompt%20optimization%20%28Insight%20I%29.%20The%20choice%20of%20the%20input%20domain%2C%0Acovering%20both%20the%20generation%20and%20the%20representation%20of%20prompts%2C%20affects%20the%0Aidentification%20of%20well-performing%20local%20optima%20%28Insight%20II%29.%20Inspired%20by%20these%0Ainsights%2C%20we%20propose%20a%20novel%20algorithm%2C%20namely%20localized%20zeroth-order%20prompt%0Aoptimization%20%28ZOPO%29%2C%20which%20incorporates%20a%20Neural%20Tangent%20Kernel-based%20derived%0AGaussian%20process%20into%20standard%20zeroth-order%20optimization%20for%20an%20efficient%0Asearch%20of%20well-performing%20local%20optima%20in%20prompt%20optimization.%20Remarkably%2C%20ZOPO%0Aoutperforms%20existing%20baselines%20in%20terms%20of%20both%20the%20optimization%20performance%0Aand%20the%20query%20efficiency%2C%20which%20we%20demonstrate%20through%20extensive%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02993v1&entry.124074799=Read"},
{"title": "VILA: On Pre-training for Visual Language Models", "author": "Ji Lin and Hongxu Yin and Wei Ping and Yao Lu and Pavlo Molchanov and Andrew Tao and Huizi Mao and Jan Kautz and Mohammad Shoeybi and Song Han", "abstract": "  Visual language models (VLMs) rapidly progressed with the recent success of\nlarge language models. There have been growing efforts on visual instruction\ntuning to extend the LLM with visual inputs, but lacks an in-depth study of the\nvisual language pre-training process, where the model learns to perform joint\nmodeling on both modalities. In this work, we examine the design options for\nVLM pre-training by augmenting LLM towards VLM through step-by-step\ncontrollable comparisons. We introduce three main findings: (1) freezing LLMs\nduring pre-training can achieve decent zero-shot performance, but lack\nin-context learning capability, which requires unfreezing the LLM; (2)\ninterleaved pre-training data is beneficial whereas image-text pairs alone are\nnot optimal; (3) re-blending text-only instruction data to image-text data\nduring instruction fine-tuning not only remedies the degradation of text-only\ntasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe\nwe build VILA, a Visual Language model family that consistently outperforms the\nstate-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells\nand whistles. Multi-modal pre-training also helps unveil appealing properties\nof VILA, including multi-image reasoning, enhanced in-context learning, and\nbetter world knowledge.\n", "link": "http://arxiv.org/abs/2312.07533v3", "date": "2024-03-05", "relevancy": 2.1556, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.584}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5083}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5027}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VILA%3A%20On%20Pre-training%20for%20Visual%20Language%20Models&entry.906535625=Ji%20Lin%20and%20Hongxu%20Yin%20and%20Wei%20Ping%20and%20Yao%20Lu%20and%20Pavlo%20Molchanov%20and%20Andrew%20Tao%20and%20Huizi%20Mao%20and%20Jan%20Kautz%20and%20Mohammad%20Shoeybi%20and%20Song%20Han&entry.1292438233=%20%20Visual%20language%20models%20%28VLMs%29%20rapidly%20progressed%20with%20the%20recent%20success%20of%0Alarge%20language%20models.%20There%20have%20been%20growing%20efforts%20on%20visual%20instruction%0Atuning%20to%20extend%20the%20LLM%20with%20visual%20inputs%2C%20but%20lacks%20an%20in-depth%20study%20of%20the%0Avisual%20language%20pre-training%20process%2C%20where%20the%20model%20learns%20to%20perform%20joint%0Amodeling%20on%20both%20modalities.%20In%20this%20work%2C%20we%20examine%20the%20design%20options%20for%0AVLM%20pre-training%20by%20augmenting%20LLM%20towards%20VLM%20through%20step-by-step%0Acontrollable%20comparisons.%20We%20introduce%20three%20main%20findings%3A%20%281%29%20freezing%20LLMs%0Aduring%20pre-training%20can%20achieve%20decent%20zero-shot%20performance%2C%20but%20lack%0Ain-context%20learning%20capability%2C%20which%20requires%20unfreezing%20the%20LLM%3B%20%282%29%0Ainterleaved%20pre-training%20data%20is%20beneficial%20whereas%20image-text%20pairs%20alone%20are%0Anot%20optimal%3B%20%283%29%20re-blending%20text-only%20instruction%20data%20to%20image-text%20data%0Aduring%20instruction%20fine-tuning%20not%20only%20remedies%20the%20degradation%20of%20text-only%0Atasks%2C%20but%20also%20boosts%20VLM%20task%20accuracy.%20With%20an%20enhanced%20pre-training%20recipe%0Awe%20build%20VILA%2C%20a%20Visual%20Language%20model%20family%20that%20consistently%20outperforms%20the%0Astate-of-the-art%20models%2C%20e.g.%2C%20LLaVA-1.5%2C%20across%20main%20benchmarks%20without%20bells%0Aand%20whistles.%20Multi-modal%20pre-training%20also%20helps%20unveil%20appealing%20properties%0Aof%20VILA%2C%20including%20multi-image%20reasoning%2C%20enhanced%20in-context%20learning%2C%20and%0Abetter%20world%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07533v3&entry.124074799=Read"},
{"title": "Gaze-Vector Estimation in the Dark with Temporally Encoded Event-driven\n  Neural Networks", "author": "Abeer Banerjee and Naval K. Mehta and Shyam S. Prasad and  Himanshu and Sumeet Saurav and Sanjay Singh", "abstract": "  In this paper, we address the intricate challenge of gaze vector prediction,\na pivotal task with applications ranging from human-computer interaction to\ndriver monitoring systems. Our innovative approach is designed for the\ndemanding setting of extremely low-light conditions, leveraging a novel\ntemporal event encoding scheme, and a dedicated neural network architecture.\nThe temporal encoding method seamlessly integrates Dynamic Vision Sensor (DVS)\nevents with grayscale guide frames, generating consecutively encoded images for\ninput into our neural network. This unique solution not only captures diverse\ngaze responses from participants within the active age group but also\nintroduces a curated dataset tailored for low-light conditions. The encoded\ntemporal frames paired with our network showcase impressive spatial\nlocalization and reliable gaze direction in their predictions. Achieving a\nremarkable 100-pixel accuracy of 100%, our research underscores the potency of\nour neural network to work with temporally consecutive encoded images for\nprecise gaze vector predictions in challenging low-light videos, contributing\nto the advancement of gaze prediction technologies.\n", "link": "http://arxiv.org/abs/2403.02909v1", "date": "2024-03-05", "relevancy": 2.1523, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5484}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5369}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5352}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaze-Vector%20Estimation%20in%20the%20Dark%20with%20Temporally%20Encoded%20Event-driven%0A%20%20Neural%20Networks&entry.906535625=Abeer%20Banerjee%20and%20Naval%20K.%20Mehta%20and%20Shyam%20S.%20Prasad%20and%20%20Himanshu%20and%20Sumeet%20Saurav%20and%20Sanjay%20Singh&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20intricate%20challenge%20of%20gaze%20vector%20prediction%2C%0Aa%20pivotal%20task%20with%20applications%20ranging%20from%20human-computer%20interaction%20to%0Adriver%20monitoring%20systems.%20Our%20innovative%20approach%20is%20designed%20for%20the%0Ademanding%20setting%20of%20extremely%20low-light%20conditions%2C%20leveraging%20a%20novel%0Atemporal%20event%20encoding%20scheme%2C%20and%20a%20dedicated%20neural%20network%20architecture.%0AThe%20temporal%20encoding%20method%20seamlessly%20integrates%20Dynamic%20Vision%20Sensor%20%28DVS%29%0Aevents%20with%20grayscale%20guide%20frames%2C%20generating%20consecutively%20encoded%20images%20for%0Ainput%20into%20our%20neural%20network.%20This%20unique%20solution%20not%20only%20captures%20diverse%0Agaze%20responses%20from%20participants%20within%20the%20active%20age%20group%20but%20also%0Aintroduces%20a%20curated%20dataset%20tailored%20for%20low-light%20conditions.%20The%20encoded%0Atemporal%20frames%20paired%20with%20our%20network%20showcase%20impressive%20spatial%0Alocalization%20and%20reliable%20gaze%20direction%20in%20their%20predictions.%20Achieving%20a%0Aremarkable%20100-pixel%20accuracy%20of%20100%25%2C%20our%20research%20underscores%20the%20potency%20of%0Aour%20neural%20network%20to%20work%20with%20temporally%20consecutive%20encoded%20images%20for%0Aprecise%20gaze%20vector%20predictions%20in%20challenging%20low-light%20videos%2C%20contributing%0Ato%20the%20advancement%20of%20gaze%20prediction%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02909v1&entry.124074799=Read"},
{"title": "Revisiting Confidence Estimation: Towards Reliable Failure Prediction", "author": "Fei Zhu and Xu-Yao Zhang and Zhen Cheng and Cheng-Lin Liu", "abstract": "  Reliable confidence estimation is a challenging yet fundamental requirement\nin many risk-sensitive applications. However, modern deep neural networks are\noften overconfident for their incorrect predictions, i.e., misclassified\nsamples from known classes, and out-of-distribution (OOD) samples from unknown\nclasses. In recent years, many confidence calibration and OOD detection methods\nhave been developed. In this paper, we find a general, widely existing but\nactually-neglected phenomenon that most confidence estimation methods are\nharmful for detecting misclassification errors. We investigate this problem and\nreveal that popular calibration and OOD detection methods often lead to worse\nconfidence separation between correctly classified and misclassified examples,\nmaking it difficult to decide whether to trust a prediction or not. Finally, we\npropose to enlarge the confidence gap by finding flat minima, which yields\nstate-of-the-art failure prediction performance under various settings\nincluding balanced, long-tailed, and covariate-shift classification scenarios.\nOur study not only provides a strong baseline for reliable confidence\nestimation but also acts as a bridge between understanding calibration, OOD\ndetection, and failure prediction. The code is available at\n\\url{https://github.com/Impression2805/FMFP}.\n", "link": "http://arxiv.org/abs/2403.02886v1", "date": "2024-03-05", "relevancy": 2.1448, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5446}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5384}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5269}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Confidence%20Estimation%3A%20Towards%20Reliable%20Failure%20Prediction&entry.906535625=Fei%20Zhu%20and%20Xu-Yao%20Zhang%20and%20Zhen%20Cheng%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Reliable%20confidence%20estimation%20is%20a%20challenging%20yet%20fundamental%20requirement%0Ain%20many%20risk-sensitive%20applications.%20However%2C%20modern%20deep%20neural%20networks%20are%0Aoften%20overconfident%20for%20their%20incorrect%20predictions%2C%20i.e.%2C%20misclassified%0Asamples%20from%20known%20classes%2C%20and%20out-of-distribution%20%28OOD%29%20samples%20from%20unknown%0Aclasses.%20In%20recent%20years%2C%20many%20confidence%20calibration%20and%20OOD%20detection%20methods%0Ahave%20been%20developed.%20In%20this%20paper%2C%20we%20find%20a%20general%2C%20widely%20existing%20but%0Aactually-neglected%20phenomenon%20that%20most%20confidence%20estimation%20methods%20are%0Aharmful%20for%20detecting%20misclassification%20errors.%20We%20investigate%20this%20problem%20and%0Areveal%20that%20popular%20calibration%20and%20OOD%20detection%20methods%20often%20lead%20to%20worse%0Aconfidence%20separation%20between%20correctly%20classified%20and%20misclassified%20examples%2C%0Amaking%20it%20difficult%20to%20decide%20whether%20to%20trust%20a%20prediction%20or%20not.%20Finally%2C%20we%0Apropose%20to%20enlarge%20the%20confidence%20gap%20by%20finding%20flat%20minima%2C%20which%20yields%0Astate-of-the-art%20failure%20prediction%20performance%20under%20various%20settings%0Aincluding%20balanced%2C%20long-tailed%2C%20and%20covariate-shift%20classification%20scenarios.%0AOur%20study%20not%20only%20provides%20a%20strong%20baseline%20for%20reliable%20confidence%0Aestimation%20but%20also%20acts%20as%20a%20bridge%20between%20understanding%20calibration%2C%20OOD%0Adetection%2C%20and%20failure%20prediction.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Impression2805/FMFP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02886v1&entry.124074799=Read"},
{"title": "3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming\n  of Photo-Realistic Free-Viewpoint Videos", "author": "Jiakai Sun and Han Jiao and Guangyuan Li and Zhanjie Zhang and Lei Zhao and Wei Xing", "abstract": "  Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes\nfrom multi-view videos remains a challenging endeavor. Despite the remarkable\nadvancements achieved by current neural rendering techniques, these methods\ngenerally require complete video sequences for offline training and are not\ncapable of real-time rendering. To address these constraints, we introduce\n3DGStream, a method designed for efficient FVV streaming of real-world dynamic\nscenes. Our method achieves fast on-the-fly per-frame reconstruction within 12\nseconds and real-time rendering at 200 FPS. Specifically, we utilize 3D\nGaussians (3DGs) to represent the scene. Instead of the na\\\"ive approach of\ndirectly optimizing 3DGs per-frame, we employ a compact Neural Transformation\nCache (NTC) to model the translations and rotations of 3DGs, markedly reducing\nthe training time and storage required for each FVV frame. Furthermore, we\npropose an adaptive 3DG addition strategy to handle emerging objects in dynamic\nscenes. Experiments demonstrate that 3DGStream achieves competitive performance\nin terms of rendering speed, image quality, training time, and model storage\nwhen compared with state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.01444v2", "date": "2024-03-05", "relevancy": 2.1297, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5509}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5331}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5136}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGStream%3A%20On-the-Fly%20Training%20of%203D%20Gaussians%20for%20Efficient%20Streaming%0A%20%20of%20Photo-Realistic%20Free-Viewpoint%20Videos&entry.906535625=Jiakai%20Sun%20and%20Han%20Jiao%20and%20Guangyuan%20Li%20and%20Zhanjie%20Zhang%20and%20Lei%20Zhao%20and%20Wei%20Xing&entry.1292438233=%20%20Constructing%20photo-realistic%20Free-Viewpoint%20Videos%20%28FVVs%29%20of%20dynamic%20scenes%0Afrom%20multi-view%20videos%20remains%20a%20challenging%20endeavor.%20Despite%20the%20remarkable%0Aadvancements%20achieved%20by%20current%20neural%20rendering%20techniques%2C%20these%20methods%0Agenerally%20require%20complete%20video%20sequences%20for%20offline%20training%20and%20are%20not%0Acapable%20of%20real-time%20rendering.%20To%20address%20these%20constraints%2C%20we%20introduce%0A3DGStream%2C%20a%20method%20designed%20for%20efficient%20FVV%20streaming%20of%20real-world%20dynamic%0Ascenes.%20Our%20method%20achieves%20fast%20on-the-fly%20per-frame%20reconstruction%20within%2012%0Aseconds%20and%20real-time%20rendering%20at%20200%20FPS.%20Specifically%2C%20we%20utilize%203D%0AGaussians%20%283DGs%29%20to%20represent%20the%20scene.%20Instead%20of%20the%20na%5C%22ive%20approach%20of%0Adirectly%20optimizing%203DGs%20per-frame%2C%20we%20employ%20a%20compact%20Neural%20Transformation%0ACache%20%28NTC%29%20to%20model%20the%20translations%20and%20rotations%20of%203DGs%2C%20markedly%20reducing%0Athe%20training%20time%20and%20storage%20required%20for%20each%20FVV%20frame.%20Furthermore%2C%20we%0Apropose%20an%20adaptive%203DG%20addition%20strategy%20to%20handle%20emerging%20objects%20in%20dynamic%0Ascenes.%20Experiments%20demonstrate%20that%203DGStream%20achieves%20competitive%20performance%0Ain%20terms%20of%20rendering%20speed%2C%20image%20quality%2C%20training%20time%2C%20and%20model%20storage%0Awhen%20compared%20with%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01444v2&entry.124074799=Read"},
{"title": "Neural Codebook Design for Network Beam Management", "author": "Ryan M. Dreifuerst and Robert W. Heath Jr", "abstract": "  Obtaining accurate and timely channel state information (CSI) is a\nfundamental challenge for large antenna systems. Mobile systems like 5G use a\nbeam management framework that joins the initial access, beamforming, CSI\nacquisition, and data transmission. The design of codebooks for these stages,\nhowever, is challenging due to their interrelationships, varying array sizes,\nand site-specific channel and user distributions. Furthermore, beam management\nis often focused on single-sector operations while ignoring the overarching\nnetwork- and system-level optimization. In this paper, we proposed an\nend-to-end learned codebook design algorithm, network beamspace learning (NBL),\nthat captures and optimizes codebooks to mitigate interference while maximizing\nthe achievable performance with extremely large hybrid arrays. The proposed\nalgorithm requires limited shared information yet designs codebooks that\noutperform traditional codebooks by over 10dB in beam alignment and achieve\nmore than 25% improvements in network spectral efficiency.\n", "link": "http://arxiv.org/abs/2403.03053v1", "date": "2024-03-05", "relevancy": 2.1272, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4493}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4195}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4076}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Codebook%20Design%20for%20Network%20Beam%20Management&entry.906535625=Ryan%20M.%20Dreifuerst%20and%20Robert%20W.%20Heath%20Jr&entry.1292438233=%20%20Obtaining%20accurate%20and%20timely%20channel%20state%20information%20%28CSI%29%20is%20a%0Afundamental%20challenge%20for%20large%20antenna%20systems.%20Mobile%20systems%20like%205G%20use%20a%0Abeam%20management%20framework%20that%20joins%20the%20initial%20access%2C%20beamforming%2C%20CSI%0Aacquisition%2C%20and%20data%20transmission.%20The%20design%20of%20codebooks%20for%20these%20stages%2C%0Ahowever%2C%20is%20challenging%20due%20to%20their%20interrelationships%2C%20varying%20array%20sizes%2C%0Aand%20site-specific%20channel%20and%20user%20distributions.%20Furthermore%2C%20beam%20management%0Ais%20often%20focused%20on%20single-sector%20operations%20while%20ignoring%20the%20overarching%0Anetwork-%20and%20system-level%20optimization.%20In%20this%20paper%2C%20we%20proposed%20an%0Aend-to-end%20learned%20codebook%20design%20algorithm%2C%20network%20beamspace%20learning%20%28NBL%29%2C%0Athat%20captures%20and%20optimizes%20codebooks%20to%20mitigate%20interference%20while%20maximizing%0Athe%20achievable%20performance%20with%20extremely%20large%20hybrid%20arrays.%20The%20proposed%0Aalgorithm%20requires%20limited%20shared%20information%20yet%20designs%20codebooks%20that%0Aoutperform%20traditional%20codebooks%20by%20over%2010dB%20in%20beam%20alignment%20and%20achieve%0Amore%20than%2025%25%20improvements%20in%20network%20spectral%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03053v1&entry.124074799=Read"},
{"title": "Mirror Descent Algorithms with Nearly Dimension-Independent Rates for\n  Differentially-Private Stochastic Saddle-Point Problems", "author": "Tom\u00e1s Gonz\u00e1lez and Crist\u00f3bal Guzm\u00e1n and Courtney Paquette", "abstract": "  We study the problem of differentially-private (DP) stochastic\n(convex-concave) saddle-points in the polyhedral setting. We propose\n$(\\varepsilon, \\delta)$-DP algorithms based on stochastic mirror descent that\nattain nearly dimension-independent convergence rates for the expected duality\ngap, a type of guarantee that was known before only for bilinear objectives.\nFor convex-concave and first-order-smooth stochastic objectives, our algorithms\nattain a rate of $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{1/3}$,\nwhere $d$ is the dimension of the problem and $n$ the dataset size. Under an\nadditional second-order-smoothness assumption, we improve the rate on the\nexpected gap to $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{2/5}$.\nUnder this additional assumption, we also show, by using bias-reduced gradient\nestimators, that the duality gap is bounded by $\\log(d)/\\sqrt{n} +\n\\log(d)/[n\\varepsilon]^{1/2}$ with constant success probability. This result\nprovides evidence of the near-optimality of the approach. Finally, we show that\ncombining our methods with acceleration techniques from online learning leads\nto the first algorithm for DP Stochastic Convex Optimization in the polyhedral\nsetting that is not based on Frank-Wolfe methods. For convex and\nfirst-order-smooth stochastic objectives, our algorithms attain an excess risk\nof $\\sqrt{\\log(d)/n} + \\log(d)^{7/10}/[n\\varepsilon]^{2/5}$, and when\nadditionally assuming second-order-smoothness, we improve the rate to\n$\\sqrt{\\log(d)/n} + \\log(d)/\\sqrt{n\\varepsilon}$. Instrumental to all of these\nresults are various extensions of the classical Maurey Sparsification Lemma,\nwhich may be of independent interest.\n", "link": "http://arxiv.org/abs/2403.02912v1", "date": "2024-03-05", "relevancy": 2.1172, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4377}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.421}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4117}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mirror%20Descent%20Algorithms%20with%20Nearly%20Dimension-Independent%20Rates%20for%0A%20%20Differentially-Private%20Stochastic%20Saddle-Point%20Problems&entry.906535625=Tom%C3%A1s%20Gonz%C3%A1lez%20and%20Crist%C3%B3bal%20Guzm%C3%A1n%20and%20Courtney%20Paquette&entry.1292438233=%20%20We%20study%20the%20problem%20of%20differentially-private%20%28DP%29%20stochastic%0A%28convex-concave%29%20saddle-points%20in%20the%20polyhedral%20setting.%20We%20propose%0A%24%28%5Cvarepsilon%2C%20%5Cdelta%29%24-DP%20algorithms%20based%20on%20stochastic%20mirror%20descent%20that%0Aattain%20nearly%20dimension-independent%20convergence%20rates%20for%20the%20expected%20duality%0Agap%2C%20a%20type%20of%20guarantee%20that%20was%20known%20before%20only%20for%20bilinear%20objectives.%0AFor%20convex-concave%20and%20first-order-smooth%20stochastic%20objectives%2C%20our%20algorithms%0Aattain%20a%20rate%20of%20%24%5Csqrt%7B%5Clog%28d%29/n%7D%20%2B%20%28%5Clog%28d%29%5E%7B3/2%7D/%5Bn%5Cvarepsilon%5D%29%5E%7B1/3%7D%24%2C%0Awhere%20%24d%24%20is%20the%20dimension%20of%20the%20problem%20and%20%24n%24%20the%20dataset%20size.%20Under%20an%0Aadditional%20second-order-smoothness%20assumption%2C%20we%20improve%20the%20rate%20on%20the%0Aexpected%20gap%20to%20%24%5Csqrt%7B%5Clog%28d%29/n%7D%20%2B%20%28%5Clog%28d%29%5E%7B3/2%7D/%5Bn%5Cvarepsilon%5D%29%5E%7B2/5%7D%24.%0AUnder%20this%20additional%20assumption%2C%20we%20also%20show%2C%20by%20using%20bias-reduced%20gradient%0Aestimators%2C%20that%20the%20duality%20gap%20is%20bounded%20by%20%24%5Clog%28d%29/%5Csqrt%7Bn%7D%20%2B%0A%5Clog%28d%29/%5Bn%5Cvarepsilon%5D%5E%7B1/2%7D%24%20with%20constant%20success%20probability.%20This%20result%0Aprovides%20evidence%20of%20the%20near-optimality%20of%20the%20approach.%20Finally%2C%20we%20show%20that%0Acombining%20our%20methods%20with%20acceleration%20techniques%20from%20online%20learning%20leads%0Ato%20the%20first%20algorithm%20for%20DP%20Stochastic%20Convex%20Optimization%20in%20the%20polyhedral%0Asetting%20that%20is%20not%20based%20on%20Frank-Wolfe%20methods.%20For%20convex%20and%0Afirst-order-smooth%20stochastic%20objectives%2C%20our%20algorithms%20attain%20an%20excess%20risk%0Aof%20%24%5Csqrt%7B%5Clog%28d%29/n%7D%20%2B%20%5Clog%28d%29%5E%7B7/10%7D/%5Bn%5Cvarepsilon%5D%5E%7B2/5%7D%24%2C%20and%20when%0Aadditionally%20assuming%20second-order-smoothness%2C%20we%20improve%20the%20rate%20to%0A%24%5Csqrt%7B%5Clog%28d%29/n%7D%20%2B%20%5Clog%28d%29/%5Csqrt%7Bn%5Cvarepsilon%7D%24.%20Instrumental%20to%20all%20of%20these%0Aresults%20are%20various%20extensions%20of%20the%20classical%20Maurey%20Sparsification%20Lemma%2C%0Awhich%20may%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02912v1&entry.124074799=Read"},
{"title": "Learning a Stable Dynamic System with a Lyapunov Energy Function for\n  Demonstratives Using Neural Networks", "author": "Yu Zhang and Yongxiang Zou and Haoyu Zhang and Xiuze Xia and Long Cheng", "abstract": "  Autonomous Dynamic System (DS)-based algorithms hold a pivotal and\nfoundational role in the field of Learning from Demonstration (LfD).\nNevertheless, they confront the formidable challenge of striking a delicate\nbalance between achieving precision in learning and ensuring the overall\nstability of the system. In response to this substantial challenge, this paper\nintroduces a novel DS algorithm rooted in neural network technology. This\nalgorithm not only possesses the capability to extract critical insights from\ndemonstration data but also demonstrates the capacity to learn a candidate\nLyapunov energy function that is consistent with the provided data. The model\npresented in this paper employs a straightforward neural network architecture\nthat excels in fulfilling a dual objective: optimizing accuracy while\nsimultaneously preserving global stability. To comprehensively evaluate the\neffectiveness of the proposed algorithm, rigorous assessments are conducted\nusing the LASA dataset, further reinforced by empirical validation through a\nrobotic experiment.\n", "link": "http://arxiv.org/abs/2309.08849v4", "date": "2024-03-05", "relevancy": 2.1033, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5485}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5084}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20a%20Stable%20Dynamic%20System%20with%20a%20Lyapunov%20Energy%20Function%20for%0A%20%20Demonstratives%20Using%20Neural%20Networks&entry.906535625=Yu%20Zhang%20and%20Yongxiang%20Zou%20and%20Haoyu%20Zhang%20and%20Xiuze%20Xia%20and%20Long%20Cheng&entry.1292438233=%20%20Autonomous%20Dynamic%20System%20%28DS%29-based%20algorithms%20hold%20a%20pivotal%20and%0Afoundational%20role%20in%20the%20field%20of%20Learning%20from%20Demonstration%20%28LfD%29.%0ANevertheless%2C%20they%20confront%20the%20formidable%20challenge%20of%20striking%20a%20delicate%0Abalance%20between%20achieving%20precision%20in%20learning%20and%20ensuring%20the%20overall%0Astability%20of%20the%20system.%20In%20response%20to%20this%20substantial%20challenge%2C%20this%20paper%0Aintroduces%20a%20novel%20DS%20algorithm%20rooted%20in%20neural%20network%20technology.%20This%0Aalgorithm%20not%20only%20possesses%20the%20capability%20to%20extract%20critical%20insights%20from%0Ademonstration%20data%20but%20also%20demonstrates%20the%20capacity%20to%20learn%20a%20candidate%0ALyapunov%20energy%20function%20that%20is%20consistent%20with%20the%20provided%20data.%20The%20model%0Apresented%20in%20this%20paper%20employs%20a%20straightforward%20neural%20network%20architecture%0Athat%20excels%20in%20fulfilling%20a%20dual%20objective%3A%20optimizing%20accuracy%20while%0Asimultaneously%20preserving%20global%20stability.%20To%20comprehensively%20evaluate%20the%0Aeffectiveness%20of%20the%20proposed%20algorithm%2C%20rigorous%20assessments%20are%20conducted%0Ausing%20the%20LASA%20dataset%2C%20further%20reinforced%20by%20empirical%20validation%20through%20a%0Arobotic%20experiment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08849v4&entry.124074799=Read"},
{"title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models", "author": "Zeqian Ju and Yuancheng Wang and Kai Shen and Xu Tan and Detai Xin and Dongchao Yang and Yanqing Liu and Yichong Leng and Kaitao Song and Siliang Tang and Zhizheng Wu and Tao Qin and Xiang-Yang Li and Wei Ye and Shikun Zhang and Jiang Bian and Lei He and Jinyu Li and Sheng Zhao", "abstract": "  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model the intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility. Furthermore, we achieve better performance by scaling to 1B\nparameters and 200K hours of training data.\n", "link": "http://arxiv.org/abs/2403.03100v1", "date": "2024-03-05", "relevancy": 2.0978, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5712}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5575}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4727}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NaturalSpeech%203%3A%20Zero-Shot%20Speech%20Synthesis%20with%20Factorized%20Codec%20and%0A%20%20Diffusion%20Models&entry.906535625=Zeqian%20Ju%20and%20Yuancheng%20Wang%20and%20Kai%20Shen%20and%20Xu%20Tan%20and%20Detai%20Xin%20and%20Dongchao%20Yang%20and%20Yanqing%20Liu%20and%20Yichong%20Leng%20and%20Kaitao%20Song%20and%20Siliang%20Tang%20and%20Zhizheng%20Wu%20and%20Tao%20Qin%20and%20Xiang-Yang%20Li%20and%20Wei%20Ye%20and%20Shikun%20Zhang%20and%20Jiang%20Bian%20and%20Lei%20He%20and%20Jinyu%20Li%20and%20Sheng%20Zhao&entry.1292438233=%20%20While%20recent%20large-scale%20text-to-speech%20%28TTS%29%20models%20have%20achieved%0Asignificant%20progress%2C%20they%20still%20fall%20short%20in%20speech%20quality%2C%20similarity%2C%20and%0Aprosody.%20Considering%20speech%20intricately%20encompasses%20various%20attributes%20%28e.g.%2C%0Acontent%2C%20prosody%2C%20timbre%2C%20and%20acoustic%20details%29%20that%20pose%20significant%0Achallenges%20for%20generation%2C%20a%20natural%20idea%20is%20to%20factorize%20speech%20into%0Aindividual%20subspaces%20representing%20different%20attributes%20and%20generate%20them%0Aindividually.%20Motivated%20by%20it%2C%20we%20propose%20NaturalSpeech%203%2C%20a%20TTS%20system%20with%0Anovel%20factorized%20diffusion%20models%20to%20generate%20natural%20speech%20in%20a%20zero-shot%0Away.%20Specifically%2C%201%29%20we%20design%20a%20neural%20codec%20with%20factorized%20vector%0Aquantization%20%28FVQ%29%20to%20disentangle%20speech%20waveform%20into%20subspaces%20of%20content%2C%0Aprosody%2C%20timbre%2C%20and%20acoustic%20details%3B%202%29%20we%20propose%20a%20factorized%20diffusion%0Amodel%20to%20generate%20attributes%20in%20each%20subspace%20following%20its%20corresponding%0Aprompt.%20With%20this%20factorization%20design%2C%20NaturalSpeech%203%20can%20effectively%20and%0Aefficiently%20model%20the%20intricate%20speech%20with%20disentangled%20subspaces%20in%20a%0Adivide-and-conquer%20way.%20Experiments%20show%20that%20NaturalSpeech%203%20outperforms%20the%0Astate-of-the-art%20TTS%20systems%20on%20quality%2C%20similarity%2C%20prosody%2C%20and%0Aintelligibility.%20Furthermore%2C%20we%20achieve%20better%20performance%20by%20scaling%20to%201B%0Aparameters%20and%20200K%20hours%20of%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03100v1&entry.124074799=Read"},
{"title": "Mitigating Biases with Diverse Ensembles and Diffusion Models", "author": "Luca Scimeca and Alexander Rubinstein and Damien Teney and Seong Joon Oh and Armand Mihai Nicolicioiu and Yoshua Bengio", "abstract": "  Spurious correlations in the data, where multiple cues are predictive of the\ntarget labels, often lead to a phenomenon known as shortcut bias, where a model\nrelies on erroneous, easy-to-learn cues while ignoring reliable ones. In this\nwork, we propose an ensemble diversification framework exploiting Diffusion\nProbabilistic Models (DPMs) for shortcut bias mitigation. We show that at\nparticular training intervals, DPMs can generate images with novel feature\ncombinations, even when trained on samples displaying correlated input\nfeatures. We leverage this crucial property to generate synthetic\ncounterfactuals to increase model diversity via ensemble disagreement. We show\nthat DPM-guided diversification is sufficient to remove dependence on primary\nshortcut cues, without a need for additional supervised signals. We further\nempirically quantify its efficacy on several diversification objectives, and\nfinally show improved generalization and diversification performance on par\nwith prior work that relies on auxiliary data collection.\n", "link": "http://arxiv.org/abs/2311.16176v2", "date": "2024-03-05", "relevancy": 2.0941, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5261}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5233}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5177}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Biases%20with%20Diverse%20Ensembles%20and%20Diffusion%20Models&entry.906535625=Luca%20Scimeca%20and%20Alexander%20Rubinstein%20and%20Damien%20Teney%20and%20Seong%20Joon%20Oh%20and%20Armand%20Mihai%20Nicolicioiu%20and%20Yoshua%20Bengio&entry.1292438233=%20%20Spurious%20correlations%20in%20the%20data%2C%20where%20multiple%20cues%20are%20predictive%20of%20the%0Atarget%20labels%2C%20often%20lead%20to%20a%20phenomenon%20known%20as%20shortcut%20bias%2C%20where%20a%20model%0Arelies%20on%20erroneous%2C%20easy-to-learn%20cues%20while%20ignoring%20reliable%20ones.%20In%20this%0Awork%2C%20we%20propose%20an%20ensemble%20diversification%20framework%20exploiting%20Diffusion%0AProbabilistic%20Models%20%28DPMs%29%20for%20shortcut%20bias%20mitigation.%20We%20show%20that%20at%0Aparticular%20training%20intervals%2C%20DPMs%20can%20generate%20images%20with%20novel%20feature%0Acombinations%2C%20even%20when%20trained%20on%20samples%20displaying%20correlated%20input%0Afeatures.%20We%20leverage%20this%20crucial%20property%20to%20generate%20synthetic%0Acounterfactuals%20to%20increase%20model%20diversity%20via%20ensemble%20disagreement.%20We%20show%0Athat%20DPM-guided%20diversification%20is%20sufficient%20to%20remove%20dependence%20on%20primary%0Ashortcut%20cues%2C%20without%20a%20need%20for%20additional%20supervised%20signals.%20We%20further%0Aempirically%20quantify%20its%20efficacy%20on%20several%20diversification%20objectives%2C%20and%0Afinally%20show%20improved%20generalization%20and%20diversification%20performance%20on%20par%0Awith%20prior%20work%20that%20relies%20on%20auxiliary%20data%20collection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16176v2&entry.124074799=Read"},
{"title": "Attacks on Node Attributes in Graph Neural Networks", "author": "Ying Xu and Michael Lanier and Anindya Sarkar and Yevgeniy Vorobeychik", "abstract": "  Graphs are commonly used to model complex networks prevalent in modern social\nmedia and literacy applications. Our research investigates the vulnerability of\nthese graphs through the application of feature based adversarial attacks,\nfocusing on both decision time attacks and poisoning attacks. In contrast to\nstate of the art models like Net Attack and Meta Attack, which target node\nattributes and graph structure, our study specifically targets node attributes.\nFor our analysis, we utilized the text dataset Hellaswag and graph datasets\nCora and CiteSeer, providing a diverse basis for evaluation. Our findings\nindicate that decision time attacks using Projected Gradient Descent (PGD) are\nmore potent compared to poisoning attacks that employ Mean Node Embeddings and\nGraph Contrastive Learning strategies. This provides insights for graph data\nsecurity, pinpointing where graph-based models are most vulnerable and thereby\ninforming the development of stronger defense mechanisms against such attacks.\n", "link": "http://arxiv.org/abs/2402.12426v2", "date": "2024-03-05", "relevancy": 2.0918, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4451}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4052}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4047}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attacks%20on%20Node%20Attributes%20in%20Graph%20Neural%20Networks&entry.906535625=Ying%20Xu%20and%20Michael%20Lanier%20and%20Anindya%20Sarkar%20and%20Yevgeniy%20Vorobeychik&entry.1292438233=%20%20Graphs%20are%20commonly%20used%20to%20model%20complex%20networks%20prevalent%20in%20modern%20social%0Amedia%20and%20literacy%20applications.%20Our%20research%20investigates%20the%20vulnerability%20of%0Athese%20graphs%20through%20the%20application%20of%20feature%20based%20adversarial%20attacks%2C%0Afocusing%20on%20both%20decision%20time%20attacks%20and%20poisoning%20attacks.%20In%20contrast%20to%0Astate%20of%20the%20art%20models%20like%20Net%20Attack%20and%20Meta%20Attack%2C%20which%20target%20node%0Aattributes%20and%20graph%20structure%2C%20our%20study%20specifically%20targets%20node%20attributes.%0AFor%20our%20analysis%2C%20we%20utilized%20the%20text%20dataset%20Hellaswag%20and%20graph%20datasets%0ACora%20and%20CiteSeer%2C%20providing%20a%20diverse%20basis%20for%20evaluation.%20Our%20findings%0Aindicate%20that%20decision%20time%20attacks%20using%20Projected%20Gradient%20Descent%20%28PGD%29%20are%0Amore%20potent%20compared%20to%20poisoning%20attacks%20that%20employ%20Mean%20Node%20Embeddings%20and%0AGraph%20Contrastive%20Learning%20strategies.%20This%20provides%20insights%20for%20graph%20data%0Asecurity%2C%20pinpointing%20where%20graph-based%20models%20are%20most%20vulnerable%20and%20thereby%0Ainforming%20the%20development%20of%20stronger%20defense%20mechanisms%20against%20such%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12426v2&entry.124074799=Read"},
{"title": "Solving the bongard-logo problem by modeling a probabilistic model", "author": "Ruizhuo Song and Beiming Yuan", "abstract": "  Abstract reasoning problems challenge the perceptual and cognitive abilities\nof AI algorithms, demanding deeper pattern discernment and inductive reasoning\nbeyond explicit image features. This study introduces PMoC, a tailored\nprobability model for the Bongard-Logo problem, achieving high reasoning\naccuracy by constructing independent probability models. Additionally, we\npresent Pose-Transformer, an enhanced Transformer-Encoder designed for complex\nabstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.\nPose-Transformer incorporates positional information learning, inspired by\ncapsule networks' pose matrices, enhancing its focus on local positional\nrelationships in image data processing. When integrated with PMoC, it further\nimproves reasoning accuracy. Our approach effectively addresses reasoning\ndifficulties associated with abstract entities' positional changes,\noutperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM\ndatabases. This research contributes to advancing AI's capabilities in abstract\nreasoning and cognitive pattern recognition.\n", "link": "http://arxiv.org/abs/2403.03173v1", "date": "2024-03-05", "relevancy": 2.0819, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5318}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5027}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20the%20bongard-logo%20problem%20by%20modeling%20a%20probabilistic%20model&entry.906535625=Ruizhuo%20Song%20and%20Beiming%20Yuan&entry.1292438233=%20%20Abstract%20reasoning%20problems%20challenge%20the%20perceptual%20and%20cognitive%20abilities%0Aof%20AI%20algorithms%2C%20demanding%20deeper%20pattern%20discernment%20and%20inductive%20reasoning%0Abeyond%20explicit%20image%20features.%20This%20study%20introduces%20PMoC%2C%20a%20tailored%0Aprobability%20model%20for%20the%20Bongard-Logo%20problem%2C%20achieving%20high%20reasoning%0Aaccuracy%20by%20constructing%20independent%20probability%20models.%20Additionally%2C%20we%0Apresent%20Pose-Transformer%2C%20an%20enhanced%20Transformer-Encoder%20designed%20for%20complex%0Aabstract%20reasoning%20tasks%2C%20including%20Bongard-Logo%2C%20RAVEN%2C%20I-RAVEN%2C%20and%20PGM.%0APose-Transformer%20incorporates%20positional%20information%20learning%2C%20inspired%20by%0Acapsule%20networks%27%20pose%20matrices%2C%20enhancing%20its%20focus%20on%20local%20positional%0Arelationships%20in%20image%20data%20processing.%20When%20integrated%20with%20PMoC%2C%20it%20further%0Aimproves%20reasoning%20accuracy.%20Our%20approach%20effectively%20addresses%20reasoning%0Adifficulties%20associated%20with%20abstract%20entities%27%20positional%20changes%2C%0Aoutperforming%20previous%20models%20on%20the%20OIG%2C%20D3%24%5Ctimes%243%20subsets%20of%20RAVEN%2C%20and%20PGM%0Adatabases.%20This%20research%20contributes%20to%20advancing%20AI%27s%20capabilities%20in%20abstract%0Areasoning%20and%20cognitive%20pattern%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03173v1&entry.124074799=Read"},
{"title": "Mixed-Strategy Nash Equilibrium for Crowd Navigation", "author": "Muchen Sun and Francesca Baldini and Peter Trautman and Todd Murphey", "abstract": "  We address the problem of finding mixed-strategy Nash equilibrium for crowd\nnavigation. Mixed-strategy Nash equilibrium provides a rigorous model for the\nrobot to anticipate uncertain yet cooperative human behavior in crowds, but the\ncomputation cost is often too high for scalable and real-time decision-making.\nHere we prove that a simple iterative Bayesian updating scheme converges to the\nNash equilibrium of a mixed-strategy social navigation game. Furthermore, we\npropose a data-driven framework to construct the game by initializing agent\nstrategies as Gaussian processes learned from human datasets. Based on the\nproposed mixed-strategy Nash equilibrium model, we develop a sampling-based\ncrowd navigation framework that can be integrated into existing navigation\nmethods and runs in real-time on a laptop CPU. We evaluate our framework in\nboth simulated environments and real-world human datasets in unstructured\nenvironments. Our framework consistently outperforms both non-learning and\nlearning-based methods on both safety and navigation efficiency and reaches\nhuman-level crowd navigation performance on top of a meta-planner.\n", "link": "http://arxiv.org/abs/2403.01537v2", "date": "2024-03-05", "relevancy": 2.077, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5176}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5151}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixed-Strategy%20Nash%20Equilibrium%20for%20Crowd%20Navigation&entry.906535625=Muchen%20Sun%20and%20Francesca%20Baldini%20and%20Peter%20Trautman%20and%20Todd%20Murphey&entry.1292438233=%20%20We%20address%20the%20problem%20of%20finding%20mixed-strategy%20Nash%20equilibrium%20for%20crowd%0Anavigation.%20Mixed-strategy%20Nash%20equilibrium%20provides%20a%20rigorous%20model%20for%20the%0Arobot%20to%20anticipate%20uncertain%20yet%20cooperative%20human%20behavior%20in%20crowds%2C%20but%20the%0Acomputation%20cost%20is%20often%20too%20high%20for%20scalable%20and%20real-time%20decision-making.%0AHere%20we%20prove%20that%20a%20simple%20iterative%20Bayesian%20updating%20scheme%20converges%20to%20the%0ANash%20equilibrium%20of%20a%20mixed-strategy%20social%20navigation%20game.%20Furthermore%2C%20we%0Apropose%20a%20data-driven%20framework%20to%20construct%20the%20game%20by%20initializing%20agent%0Astrategies%20as%20Gaussian%20processes%20learned%20from%20human%20datasets.%20Based%20on%20the%0Aproposed%20mixed-strategy%20Nash%20equilibrium%20model%2C%20we%20develop%20a%20sampling-based%0Acrowd%20navigation%20framework%20that%20can%20be%20integrated%20into%20existing%20navigation%0Amethods%20and%20runs%20in%20real-time%20on%20a%20laptop%20CPU.%20We%20evaluate%20our%20framework%20in%0Aboth%20simulated%20environments%20and%20real-world%20human%20datasets%20in%20unstructured%0Aenvironments.%20Our%20framework%20consistently%20outperforms%20both%20non-learning%20and%0Alearning-based%20methods%20on%20both%20safety%20and%20navigation%20efficiency%20and%20reaches%0Ahuman-level%20crowd%20navigation%20performance%20on%20top%20of%20a%20meta-planner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01537v2&entry.124074799=Read"},
{"title": "PALO: A Polyglot Large Multimodal Model for 5B People", "author": "Muhammad Maaz and Hanoona Rasheed and Abdelrahman Shaker and Salman Khan and Hisham Cholakal and Rao M. Anwer and Tim Baldwin and Michael Felsberg and Fahad S. Khan", "abstract": "  In pursuit of more inclusive Vision-Language Models (VLMs), this study\nintroduces a Large Multilingual Multimodal Model called PALO. PALO offers\nvisual reasoning capabilities in 10 major languages, including English,\nChinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese,\nthat span a total of ~5B people (65% of the world population). Our approach\ninvolves a semi-automated translation approach to adapt the multimodal\ninstruction dataset from English to the target languages using a fine-tuned\nLarge Language Model, thereby ensuring high linguistic fidelity while allowing\nscalability due to minimal manual effort. The incorporation of diverse\ninstruction sets helps us boost overall performance across multiple languages\nespecially those that are underrepresented like Hindi, Arabic, Bengali, and\nUrdu. The resulting models are trained across three scales (1.7B, 7B and 13B\nparameters) to show the generalization and scalability where we observe\nsubstantial improvements compared to strong baselines. We also propose the\nfirst multilingual multimodal benchmark for the forthcoming approaches to\nevaluate their vision-language reasoning capabilities across languages. Code:\nhttps://github.com/mbzuai-oryx/PALO.\n", "link": "http://arxiv.org/abs/2402.14818v2", "date": "2024-03-05", "relevancy": 2.0746, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5241}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.52}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5018}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PALO%3A%20A%20Polyglot%20Large%20Multimodal%20Model%20for%205B%20People&entry.906535625=Muhammad%20Maaz%20and%20Hanoona%20Rasheed%20and%20Abdelrahman%20Shaker%20and%20Salman%20Khan%20and%20Hisham%20Cholakal%20and%20Rao%20M.%20Anwer%20and%20Tim%20Baldwin%20and%20Michael%20Felsberg%20and%20Fahad%20S.%20Khan&entry.1292438233=%20%20In%20pursuit%20of%20more%20inclusive%20Vision-Language%20Models%20%28VLMs%29%2C%20this%20study%0Aintroduces%20a%20Large%20Multilingual%20Multimodal%20Model%20called%20PALO.%20PALO%20offers%0Avisual%20reasoning%20capabilities%20in%2010%20major%20languages%2C%20including%20English%2C%0AChinese%2C%20Hindi%2C%20Spanish%2C%20French%2C%20Arabic%2C%20Bengali%2C%20Russian%2C%20Urdu%2C%20and%20Japanese%2C%0Athat%20span%20a%20total%20of%20~5B%20people%20%2865%25%20of%20the%20world%20population%29.%20Our%20approach%0Ainvolves%20a%20semi-automated%20translation%20approach%20to%20adapt%20the%20multimodal%0Ainstruction%20dataset%20from%20English%20to%20the%20target%20languages%20using%20a%20fine-tuned%0ALarge%20Language%20Model%2C%20thereby%20ensuring%20high%20linguistic%20fidelity%20while%20allowing%0Ascalability%20due%20to%20minimal%20manual%20effort.%20The%20incorporation%20of%20diverse%0Ainstruction%20sets%20helps%20us%20boost%20overall%20performance%20across%20multiple%20languages%0Aespecially%20those%20that%20are%20underrepresented%20like%20Hindi%2C%20Arabic%2C%20Bengali%2C%20and%0AUrdu.%20The%20resulting%20models%20are%20trained%20across%20three%20scales%20%281.7B%2C%207B%20and%2013B%0Aparameters%29%20to%20show%20the%20generalization%20and%20scalability%20where%20we%20observe%0Asubstantial%20improvements%20compared%20to%20strong%20baselines.%20We%20also%20propose%20the%0Afirst%20multilingual%20multimodal%20benchmark%20for%20the%20forthcoming%20approaches%20to%0Aevaluate%20their%20vision-language%20reasoning%20capabilities%20across%20languages.%20Code%3A%0Ahttps%3A//github.com/mbzuai-oryx/PALO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14818v2&entry.124074799=Read"},
{"title": "Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and\n  Challenges", "author": "Bosheng Ding and Chengwei Qin and Ruochen Zhao and Tianze Luo and Xinze Li and Guizhen Chen and Wenhan Xia and Junjie Hu and Anh Tuan Luu and Shafiq Joty", "abstract": "  In the rapidly evolving field of machine learning (ML), data augmentation\n(DA) has emerged as a pivotal technique for enhancing model performance by\ndiversifying training examples without the need for additional data collection.\nThis survey explores the transformative impact of Large Language Models (LLMs)\non DA, particularly addressing the unique challenges and opportunities they\npresent in the context of natural language processing (NLP) and beyond. From a\ndata perspective and a learning perspective, we examine various strategies that\nutilize Large Language Models for data augmentation, including a novel\nexploration of learning paradigms where LLM-generated data is used for further\ntraining. Additionally, this paper delineates the primary challenges faced in\nthis domain, ranging from controllable data augmentation to multi modal data\naugmentation. This survey highlights the paradigm shift introduced by LLMs in\nDA, aims to serve as a foundational guide for researchers and practitioners in\nthis field.\n", "link": "http://arxiv.org/abs/2403.02990v1", "date": "2024-03-05", "relevancy": 2.0641, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5242}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Augmentation%20using%20LLMs%3A%20Data%20Perspectives%2C%20Learning%20Paradigms%20and%0A%20%20Challenges&entry.906535625=Bosheng%20Ding%20and%20Chengwei%20Qin%20and%20Ruochen%20Zhao%20and%20Tianze%20Luo%20and%20Xinze%20Li%20and%20Guizhen%20Chen%20and%20Wenhan%20Xia%20and%20Junjie%20Hu%20and%20Anh%20Tuan%20Luu%20and%20Shafiq%20Joty&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20machine%20learning%20%28ML%29%2C%20data%20augmentation%0A%28DA%29%20has%20emerged%20as%20a%20pivotal%20technique%20for%20enhancing%20model%20performance%20by%0Adiversifying%20training%20examples%20without%20the%20need%20for%20additional%20data%20collection.%0AThis%20survey%20explores%20the%20transformative%20impact%20of%20Large%20Language%20Models%20%28LLMs%29%0Aon%20DA%2C%20particularly%20addressing%20the%20unique%20challenges%20and%20opportunities%20they%0Apresent%20in%20the%20context%20of%20natural%20language%20processing%20%28NLP%29%20and%20beyond.%20From%20a%0Adata%20perspective%20and%20a%20learning%20perspective%2C%20we%20examine%20various%20strategies%20that%0Autilize%20Large%20Language%20Models%20for%20data%20augmentation%2C%20including%20a%20novel%0Aexploration%20of%20learning%20paradigms%20where%20LLM-generated%20data%20is%20used%20for%20further%0Atraining.%20Additionally%2C%20this%20paper%20delineates%20the%20primary%20challenges%20faced%20in%0Athis%20domain%2C%20ranging%20from%20controllable%20data%20augmentation%20to%20multi%20modal%20data%0Aaugmentation.%20This%20survey%20highlights%20the%20paradigm%20shift%20introduced%20by%20LLMs%20in%0ADA%2C%20aims%20to%20serve%20as%20a%20foundational%20guide%20for%20researchers%20and%20practitioners%20in%0Athis%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02990v1&entry.124074799=Read"},
{"title": "Making deep neural networks right for the right scientific reasons by\n  interacting with their explanations", "author": "Patrick Schramowski and Wolfgang Stammer and Stefano Teso and Anna Brugger and Xiaoting Shao and Hans-Georg Luigs and Anne-Katrin Mahlein and Kristian Kersting", "abstract": "  Deep neural networks have shown excellent performances in many real-world\napplications. Unfortunately, they may show \"Clever Hans\"-like behavior --\nmaking use of confounding factors within datasets -- to achieve high\nperformance. In this work, we introduce the novel learning setting of\n\"explanatory interactive learning\" (XIL) and illustrate its benefits on a plant\nphenotyping research task. XIL adds the scientist into the training loop such\nthat she interactively revises the original model via providing feedback on its\nexplanations. Our experimental results demonstrate that XIL can help avoiding\nClever Hans moments in machine learning and encourages (or discourages, if\nappropriate) trust into the underlying model.\n", "link": "http://arxiv.org/abs/2001.05371v4", "date": "2024-03-05", "relevancy": 2.0523, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5269}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5189}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4969}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Making%20deep%20neural%20networks%20right%20for%20the%20right%20scientific%20reasons%20by%0A%20%20interacting%20with%20their%20explanations&entry.906535625=Patrick%20Schramowski%20and%20Wolfgang%20Stammer%20and%20Stefano%20Teso%20and%20Anna%20Brugger%20and%20Xiaoting%20Shao%20and%20Hans-Georg%20Luigs%20and%20Anne-Katrin%20Mahlein%20and%20Kristian%20Kersting&entry.1292438233=%20%20Deep%20neural%20networks%20have%20shown%20excellent%20performances%20in%20many%20real-world%0Aapplications.%20Unfortunately%2C%20they%20may%20show%20%22Clever%20Hans%22-like%20behavior%20--%0Amaking%20use%20of%20confounding%20factors%20within%20datasets%20--%20to%20achieve%20high%0Aperformance.%20In%20this%20work%2C%20we%20introduce%20the%20novel%20learning%20setting%20of%0A%22explanatory%20interactive%20learning%22%20%28XIL%29%20and%20illustrate%20its%20benefits%20on%20a%20plant%0Aphenotyping%20research%20task.%20XIL%20adds%20the%20scientist%20into%20the%20training%20loop%20such%0Athat%20she%20interactively%20revises%20the%20original%20model%20via%20providing%20feedback%20on%20its%0Aexplanations.%20Our%20experimental%20results%20demonstrate%20that%20XIL%20can%20help%20avoiding%0AClever%20Hans%20moments%20in%20machine%20learning%20and%20encourages%20%28or%20discourages%2C%20if%0Aappropriate%29%20trust%20into%20the%20underlying%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2001.05371v4&entry.124074799=Read"},
{"title": "Autonomous vehicle decision and control through reinforcement learning\n  with traffic flow randomization", "author": "Yuan Lin and Antai Xie and Xiao Liu", "abstract": "  Most of the current studies on autonomous vehicle decision-making and control\ntasks based on reinforcement learning are conducted in simulated environments.\nThe training and testing of these studies are carried out under rule-based\nmicroscopic traffic flow, with little consideration of migrating them to real\nor near-real environments to test their performance. It may lead to a\ndegradation in performance when the trained model is tested in more realistic\ntraffic scenes. In this study, we propose a method to randomize the driving\nstyle and behavior of surrounding vehicles by randomizing certain parameters of\nthe car-following model and the lane-changing model of rule-based microscopic\ntraffic flow in SUMO. We trained policies with deep reinforcement learning\nalgorithms under the domain randomized rule-based microscopic traffic flow in\nfreeway and merging scenes, and then tested them separately in rule-based\nmicroscopic traffic flow and high-fidelity microscopic traffic flow. Results\nindicate that the policy trained under domain randomization traffic flow has\nsignificantly better success rate and calculative reward compared to the models\ntrained under other microscopic traffic flows.\n", "link": "http://arxiv.org/abs/2403.02882v1", "date": "2024-03-05", "relevancy": 2.0499, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5319}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5156}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4918}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20vehicle%20decision%20and%20control%20through%20reinforcement%20learning%0A%20%20with%20traffic%20flow%20randomization&entry.906535625=Yuan%20Lin%20and%20Antai%20Xie%20and%20Xiao%20Liu&entry.1292438233=%20%20Most%20of%20the%20current%20studies%20on%20autonomous%20vehicle%20decision-making%20and%20control%0Atasks%20based%20on%20reinforcement%20learning%20are%20conducted%20in%20simulated%20environments.%0AThe%20training%20and%20testing%20of%20these%20studies%20are%20carried%20out%20under%20rule-based%0Amicroscopic%20traffic%20flow%2C%20with%20little%20consideration%20of%20migrating%20them%20to%20real%0Aor%20near-real%20environments%20to%20test%20their%20performance.%20It%20may%20lead%20to%20a%0Adegradation%20in%20performance%20when%20the%20trained%20model%20is%20tested%20in%20more%20realistic%0Atraffic%20scenes.%20In%20this%20study%2C%20we%20propose%20a%20method%20to%20randomize%20the%20driving%0Astyle%20and%20behavior%20of%20surrounding%20vehicles%20by%20randomizing%20certain%20parameters%20of%0Athe%20car-following%20model%20and%20the%20lane-changing%20model%20of%20rule-based%20microscopic%0Atraffic%20flow%20in%20SUMO.%20We%20trained%20policies%20with%20deep%20reinforcement%20learning%0Aalgorithms%20under%20the%20domain%20randomized%20rule-based%20microscopic%20traffic%20flow%20in%0Afreeway%20and%20merging%20scenes%2C%20and%20then%20tested%20them%20separately%20in%20rule-based%0Amicroscopic%20traffic%20flow%20and%20high-fidelity%20microscopic%20traffic%20flow.%20Results%0Aindicate%20that%20the%20policy%20trained%20under%20domain%20randomization%20traffic%20flow%20has%0Asignificantly%20better%20success%20rate%20and%20calculative%20reward%20compared%20to%20the%20models%0Atrained%20under%20other%20microscopic%20traffic%20flows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02882v1&entry.124074799=Read"},
{"title": "Neural Redshift: Random Networks are not Random Functions", "author": "Damien Teney and Armand Nicolicioiu and Valentin Hartmann and Ehsan Abbasnejad", "abstract": "  Our understanding of the generalization capabilities of neural networks (NNs)\nis still incomplete. Prevailing explanations are based on implicit biases of\ngradient descent (GD) but they cannot account for the capabilities of models\nfrom gradient-free methods nor the simplicity bias recently observed in\nuntrained networks. This paper seeks other sources of generalization in NNs.\n  Findings. To understand the inductive biases provided by architectures\nindependently from GD, we examine untrained, random-weight networks. Even\nsimple MLPs show strong inductive biases: uniform sampling in weight space\nyields a very biased distribution of functions in terms of complexity. But\nunlike common wisdom, NNs do not have an inherent \"simplicity bias\". This\nproperty depends on components such as ReLUs, residual connections, and layer\nnormalizations. Alternative architectures can be built with a bias for any\nlevel of complexity. Transformers also inherit all these properties from their\nbuilding blocks.\n  Implications. We provide a fresh explanation for the success of deep learning\nindependent from gradient-based training. It points at promising avenues for\ncontrolling the solutions implemented by trained models.\n", "link": "http://arxiv.org/abs/2403.02241v2", "date": "2024-03-05", "relevancy": 2.045, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5919}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4643}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4494}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Redshift%3A%20Random%20Networks%20are%20not%20Random%20Functions&entry.906535625=Damien%20Teney%20and%20Armand%20Nicolicioiu%20and%20Valentin%20Hartmann%20and%20Ehsan%20Abbasnejad&entry.1292438233=%20%20Our%20understanding%20of%20the%20generalization%20capabilities%20of%20neural%20networks%20%28NNs%29%0Ais%20still%20incomplete.%20Prevailing%20explanations%20are%20based%20on%20implicit%20biases%20of%0Agradient%20descent%20%28GD%29%20but%20they%20cannot%20account%20for%20the%20capabilities%20of%20models%0Afrom%20gradient-free%20methods%20nor%20the%20simplicity%20bias%20recently%20observed%20in%0Auntrained%20networks.%20This%20paper%20seeks%20other%20sources%20of%20generalization%20in%20NNs.%0A%20%20Findings.%20To%20understand%20the%20inductive%20biases%20provided%20by%20architectures%0Aindependently%20from%20GD%2C%20we%20examine%20untrained%2C%20random-weight%20networks.%20Even%0Asimple%20MLPs%20show%20strong%20inductive%20biases%3A%20uniform%20sampling%20in%20weight%20space%0Ayields%20a%20very%20biased%20distribution%20of%20functions%20in%20terms%20of%20complexity.%20But%0Aunlike%20common%20wisdom%2C%20NNs%20do%20not%20have%20an%20inherent%20%22simplicity%20bias%22.%20This%0Aproperty%20depends%20on%20components%20such%20as%20ReLUs%2C%20residual%20connections%2C%20and%20layer%0Anormalizations.%20Alternative%20architectures%20can%20be%20built%20with%20a%20bias%20for%20any%0Alevel%20of%20complexity.%20Transformers%20also%20inherit%20all%20these%20properties%20from%20their%0Abuilding%20blocks.%0A%20%20Implications.%20We%20provide%20a%20fresh%20explanation%20for%20the%20success%20of%20deep%20learning%0Aindependent%20from%20gradient-based%20training.%20It%20points%20at%20promising%20avenues%20for%0Acontrolling%20the%20solutions%20implemented%20by%20trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02241v2&entry.124074799=Read"},
{"title": "Momentum Benefits Non-IID Federated Learning Simply and Provably", "author": "Ziheng Cheng and Xinmeng Huang and Pengfei Wu and Kun Yuan", "abstract": "  Federated learning is a powerful paradigm for large-scale machine learning,\nbut it faces significant challenges due to unreliable network connections, slow\ncommunication, and substantial data heterogeneity across clients. FedAvg and\nSCAFFOLD are two prominent algorithms to address these challenges. In\nparticular, FedAvg employs multiple local updates before communicating with a\ncentral server, while SCAFFOLD maintains a control variable on each client to\ncompensate for ``client drift'' in its local updates. Various methods have been\nproposed to enhance the convergence of these two algorithms, but they either\nmake impractical adjustments to the algorithmic structure or rely on the\nassumption of bounded data heterogeneity.\n  This paper explores the utilization of momentum to enhance the performance of\nFedAvg and SCAFFOLD. When all clients participate in the training process, we\ndemonstrate that incorporating momentum allows FedAvg to converge without\nrelying on the assumption of bounded data heterogeneity even using a constant\nlocal learning rate. This is novel and fairly surprising as existing analyses\nfor FedAvg require bounded data heterogeneity even with diminishing local\nlearning rates. In partial client participation, we show that momentum enables\nSCAFFOLD to converge provably faster without imposing any additional\nassumptions. Furthermore, we use momentum to develop new variance-reduced\nextensions of FedAvg and SCAFFOLD, which exhibit state-of-the-art convergence\nrates. Our experimental results support all theoretical findings.\n", "link": "http://arxiv.org/abs/2306.16504v3", "date": "2024-03-05", "relevancy": 2.0356, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5337}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4912}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4911}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Momentum%20Benefits%20Non-IID%20Federated%20Learning%20Simply%20and%20Provably&entry.906535625=Ziheng%20Cheng%20and%20Xinmeng%20Huang%20and%20Pengfei%20Wu%20and%20Kun%20Yuan&entry.1292438233=%20%20Federated%20learning%20is%20a%20powerful%20paradigm%20for%20large-scale%20machine%20learning%2C%0Abut%20it%20faces%20significant%20challenges%20due%20to%20unreliable%20network%20connections%2C%20slow%0Acommunication%2C%20and%20substantial%20data%20heterogeneity%20across%20clients.%20FedAvg%20and%0ASCAFFOLD%20are%20two%20prominent%20algorithms%20to%20address%20these%20challenges.%20In%0Aparticular%2C%20FedAvg%20employs%20multiple%20local%20updates%20before%20communicating%20with%20a%0Acentral%20server%2C%20while%20SCAFFOLD%20maintains%20a%20control%20variable%20on%20each%20client%20to%0Acompensate%20for%20%60%60client%20drift%27%27%20in%20its%20local%20updates.%20Various%20methods%20have%20been%0Aproposed%20to%20enhance%20the%20convergence%20of%20these%20two%20algorithms%2C%20but%20they%20either%0Amake%20impractical%20adjustments%20to%20the%20algorithmic%20structure%20or%20rely%20on%20the%0Aassumption%20of%20bounded%20data%20heterogeneity.%0A%20%20This%20paper%20explores%20the%20utilization%20of%20momentum%20to%20enhance%20the%20performance%20of%0AFedAvg%20and%20SCAFFOLD.%20When%20all%20clients%20participate%20in%20the%20training%20process%2C%20we%0Ademonstrate%20that%20incorporating%20momentum%20allows%20FedAvg%20to%20converge%20without%0Arelying%20on%20the%20assumption%20of%20bounded%20data%20heterogeneity%20even%20using%20a%20constant%0Alocal%20learning%20rate.%20This%20is%20novel%20and%20fairly%20surprising%20as%20existing%20analyses%0Afor%20FedAvg%20require%20bounded%20data%20heterogeneity%20even%20with%20diminishing%20local%0Alearning%20rates.%20In%20partial%20client%20participation%2C%20we%20show%20that%20momentum%20enables%0ASCAFFOLD%20to%20converge%20provably%20faster%20without%20imposing%20any%20additional%0Aassumptions.%20Furthermore%2C%20we%20use%20momentum%20to%20develop%20new%20variance-reduced%0Aextensions%20of%20FedAvg%20and%20SCAFFOLD%2C%20which%20exhibit%20state-of-the-art%20convergence%0Arates.%20Our%20experimental%20results%20support%20all%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.16504v3&entry.124074799=Read"},
{"title": "Enhancing the Rate-Distortion-Perception Flexibility of Learned Image\n  Codecs with Conditional Diffusion Decoders", "author": "Daniele Mari and Simone Milani", "abstract": "  Learned image compression codecs have recently achieved impressive\ncompression performances surpassing the most efficient image coding\narchitectures. However, most approaches are trained to minimize rate and\ndistortion which often leads to unsatisfactory visual results at low bitrates\nsince perceptual metrics are not taken into account. In this paper, we show\nthat conditional diffusion models can lead to promising results in the\ngenerative compression task when used as a decoder, and that, given a\ncompressed representation, they allow creating new tradeoff points between\ndistortion and perception at the decoder side based on the sampling method.\n", "link": "http://arxiv.org/abs/2403.02887v1", "date": "2024-03-05", "relevancy": 2.033, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5196}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5039}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4909}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20the%20Rate-Distortion-Perception%20Flexibility%20of%20Learned%20Image%0A%20%20Codecs%20with%20Conditional%20Diffusion%20Decoders&entry.906535625=Daniele%20Mari%20and%20Simone%20Milani&entry.1292438233=%20%20Learned%20image%20compression%20codecs%20have%20recently%20achieved%20impressive%0Acompression%20performances%20surpassing%20the%20most%20efficient%20image%20coding%0Aarchitectures.%20However%2C%20most%20approaches%20are%20trained%20to%20minimize%20rate%20and%0Adistortion%20which%20often%20leads%20to%20unsatisfactory%20visual%20results%20at%20low%20bitrates%0Asince%20perceptual%20metrics%20are%20not%20taken%20into%20account.%20In%20this%20paper%2C%20we%20show%0Athat%20conditional%20diffusion%20models%20can%20lead%20to%20promising%20results%20in%20the%0Agenerative%20compression%20task%20when%20used%20as%20a%20decoder%2C%20and%20that%2C%20given%20a%0Acompressed%20representation%2C%20they%20allow%20creating%20new%20tradeoff%20points%20between%0Adistortion%20and%20perception%20at%20the%20decoder%20side%20based%20on%20the%20sampling%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02887v1&entry.124074799=Read"},
{"title": "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context\n  Misinformation Detection", "author": "Peng Qi and Zehong Yan and Wynne Hsu and Mong Li Lee", "abstract": "  Misinformation is a prevalent societal issue due to its potential high risks.\nOut-of-context (OOC) misinformation, where authentic images are repurposed with\nfalse text, is one of the easiest and most effective ways to mislead audiences.\nCurrent methods focus on assessing image-text consistency but lack convincing\nexplanations for their judgments, which is essential for debunking\nmisinformation. While Multimodal Large Language Models (MLLMs) have rich\nknowledge and innate capability for visual reasoning and explanation\ngeneration, they still lack sophistication in understanding and discovering the\nsubtle crossmodal differences. In this paper, we introduce SNIFFER, a novel\nmultimodal large language model specifically engineered for OOC misinformation\ndetection and explanation. SNIFFER employs two-stage instruction tuning on\nInstructBLIP. The first stage refines the model's concept alignment of generic\nobjects with news-domain entities and the second stage leverages language-only\nGPT-4 generated OOC-specific instruction data to fine-tune the model's\ndiscriminatory powers. Enhanced by external tools and retrieval, SNIFFER not\nonly detects inconsistencies between text and image but also utilizes external\nknowledge for contextual verification. Our experiments show that SNIFFER\nsurpasses the original MLLM by over 40% and outperforms state-of-the-art\nmethods in detection accuracy. SNIFFER also provides accurate and persuasive\nexplanations as validated by quantitative and human evaluations.\n", "link": "http://arxiv.org/abs/2403.03170v1", "date": "2024-03-05", "relevancy": 2.0279, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5276}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4831}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SNIFFER%3A%20Multimodal%20Large%20Language%20Model%20for%20Explainable%20Out-of-Context%0A%20%20Misinformation%20Detection&entry.906535625=Peng%20Qi%20and%20Zehong%20Yan%20and%20Wynne%20Hsu%20and%20Mong%20Li%20Lee&entry.1292438233=%20%20Misinformation%20is%20a%20prevalent%20societal%20issue%20due%20to%20its%20potential%20high%20risks.%0AOut-of-context%20%28OOC%29%20misinformation%2C%20where%20authentic%20images%20are%20repurposed%20with%0Afalse%20text%2C%20is%20one%20of%20the%20easiest%20and%20most%20effective%20ways%20to%20mislead%20audiences.%0ACurrent%20methods%20focus%20on%20assessing%20image-text%20consistency%20but%20lack%20convincing%0Aexplanations%20for%20their%20judgments%2C%20which%20is%20essential%20for%20debunking%0Amisinformation.%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20rich%0Aknowledge%20and%20innate%20capability%20for%20visual%20reasoning%20and%20explanation%0Ageneration%2C%20they%20still%20lack%20sophistication%20in%20understanding%20and%20discovering%20the%0Asubtle%20crossmodal%20differences.%20In%20this%20paper%2C%20we%20introduce%20SNIFFER%2C%20a%20novel%0Amultimodal%20large%20language%20model%20specifically%20engineered%20for%20OOC%20misinformation%0Adetection%20and%20explanation.%20SNIFFER%20employs%20two-stage%20instruction%20tuning%20on%0AInstructBLIP.%20The%20first%20stage%20refines%20the%20model%27s%20concept%20alignment%20of%20generic%0Aobjects%20with%20news-domain%20entities%20and%20the%20second%20stage%20leverages%20language-only%0AGPT-4%20generated%20OOC-specific%20instruction%20data%20to%20fine-tune%20the%20model%27s%0Adiscriminatory%20powers.%20Enhanced%20by%20external%20tools%20and%20retrieval%2C%20SNIFFER%20not%0Aonly%20detects%20inconsistencies%20between%20text%20and%20image%20but%20also%20utilizes%20external%0Aknowledge%20for%20contextual%20verification.%20Our%20experiments%20show%20that%20SNIFFER%0Asurpasses%20the%20original%20MLLM%20by%20over%2040%25%20and%20outperforms%20state-of-the-art%0Amethods%20in%20detection%20accuracy.%20SNIFFER%20also%20provides%20accurate%20and%20persuasive%0Aexplanations%20as%20validated%20by%20quantitative%20and%20human%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03170v1&entry.124074799=Read"},
{"title": "CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven\n  Parallel Robots", "author": "Zeqing Zhang and Linhan Yang and Cong Sun and Weiwei Shang and Jia Pan", "abstract": "  The Cable-Driven Parallel Robots (CDPRs) have gained significant attention\ndue to their high payload capacity and large workspace. When deploying CDPRs in\npractice, one of the challenges is kinematic modeling. Unlike serial\nmechanisms, CDPRs have a simple inverse kinematics problem but a complex\nforward kinematics (FK) issue. Therefore, the development of accurate and\nefficient FK solvers has been a prominent research focus in CDPR applications.\nBy observing the topology within CDPRs, in this paper, we propose a graph-based\nrepresentation to model CDPRs and introduce CafkNet, a fast and general FK\nsolver, leveraging Graph Neural Network (GNN). CafkNet is extensively tested on\n3D and 2D CDPRs in different configurations, both in simulators and real\nscenarios. The results demonstrate its ability to learn CDPRs' internal\ntopology and accurately solve the FK problem. Then, the zero-shot\ngeneralization from one configuration to another is validated. Also, the\nsim2real gap can be bridged by CafkNet using both simulation and real-world\ndata. To the best of our knowledge, it is the first study that employs the GNN\nto solve FK problem for CDPRs.\n", "link": "http://arxiv.org/abs/2402.18420v2", "date": "2024-03-05", "relevancy": 2.0161, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5143}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5086}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4953}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CafkNet%3A%20GNN-Empowered%20Forward%20Kinematic%20Modeling%20for%20Cable-Driven%0A%20%20Parallel%20Robots&entry.906535625=Zeqing%20Zhang%20and%20Linhan%20Yang%20and%20Cong%20Sun%20and%20Weiwei%20Shang%20and%20Jia%20Pan&entry.1292438233=%20%20The%20Cable-Driven%20Parallel%20Robots%20%28CDPRs%29%20have%20gained%20significant%20attention%0Adue%20to%20their%20high%20payload%20capacity%20and%20large%20workspace.%20When%20deploying%20CDPRs%20in%0Apractice%2C%20one%20of%20the%20challenges%20is%20kinematic%20modeling.%20Unlike%20serial%0Amechanisms%2C%20CDPRs%20have%20a%20simple%20inverse%20kinematics%20problem%20but%20a%20complex%0Aforward%20kinematics%20%28FK%29%20issue.%20Therefore%2C%20the%20development%20of%20accurate%20and%0Aefficient%20FK%20solvers%20has%20been%20a%20prominent%20research%20focus%20in%20CDPR%20applications.%0ABy%20observing%20the%20topology%20within%20CDPRs%2C%20in%20this%20paper%2C%20we%20propose%20a%20graph-based%0Arepresentation%20to%20model%20CDPRs%20and%20introduce%20CafkNet%2C%20a%20fast%20and%20general%20FK%0Asolver%2C%20leveraging%20Graph%20Neural%20Network%20%28GNN%29.%20CafkNet%20is%20extensively%20tested%20on%0A3D%20and%202D%20CDPRs%20in%20different%20configurations%2C%20both%20in%20simulators%20and%20real%0Ascenarios.%20The%20results%20demonstrate%20its%20ability%20to%20learn%20CDPRs%27%20internal%0Atopology%20and%20accurately%20solve%20the%20FK%20problem.%20Then%2C%20the%20zero-shot%0Ageneralization%20from%20one%20configuration%20to%20another%20is%20validated.%20Also%2C%20the%0Asim2real%20gap%20can%20be%20bridged%20by%20CafkNet%20using%20both%20simulation%20and%20real-world%0Adata.%20To%20the%20best%20of%20our%20knowledge%2C%20it%20is%20the%20first%20study%20that%20employs%20the%20GNN%0Ato%20solve%20FK%20problem%20for%20CDPRs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18420v2&entry.124074799=Read"},
{"title": "DenseMamba: State Space Models with Dense Hidden Connection for\n  Efficient Large Language Models", "author": "Wei He and Kai Han and Yehui Tang and Chengcheng Wang and Yujie Yang and Tianyu Guo and Yunhe Wang", "abstract": "  Large language models (LLMs) face a daunting challenge due to the excessive\ncomputational and memory requirements of the commonly used Transformer\narchitecture. While state space model (SSM) is a new type of foundational\nnetwork architecture offering lower computational complexity, their performance\nhas yet to fully rival that of Transformers. This paper introduces DenseSSM, a\nnovel approach to enhance the flow of hidden information between layers in\nSSMs. By selectively integrating shallowlayer hidden states into deeper layers,\nDenseSSM retains fine-grained information crucial for the final output. Dense\nconnections enhanced DenseSSM still maintains the training parallelizability\nand inference efficiency. The proposed method can be widely applicable to\nvarious SSM types like RetNet and Mamba. With similar model size, DenseSSM\nachieves significant improvements, exemplified by DenseRetNet outperforming the\noriginal RetNet with up to 5% accuracy improvement on public benchmarks. code\nis avalaible at https://github.com/WailordHe/DenseSSM\n", "link": "http://arxiv.org/abs/2403.00818v2", "date": "2024-03-05", "relevancy": 2.0102, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5159}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4961}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4851}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenseMamba%3A%20State%20Space%20Models%20with%20Dense%20Hidden%20Connection%20for%0A%20%20Efficient%20Large%20Language%20Models&entry.906535625=Wei%20He%20and%20Kai%20Han%20and%20Yehui%20Tang%20and%20Chengcheng%20Wang%20and%20Yujie%20Yang%20and%20Tianyu%20Guo%20and%20Yunhe%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20face%20a%20daunting%20challenge%20due%20to%20the%20excessive%0Acomputational%20and%20memory%20requirements%20of%20the%20commonly%20used%20Transformer%0Aarchitecture.%20While%20state%20space%20model%20%28SSM%29%20is%20a%20new%20type%20of%20foundational%0Anetwork%20architecture%20offering%20lower%20computational%20complexity%2C%20their%20performance%0Ahas%20yet%20to%20fully%20rival%20that%20of%20Transformers.%20This%20paper%20introduces%20DenseSSM%2C%20a%0Anovel%20approach%20to%20enhance%20the%20flow%20of%20hidden%20information%20between%20layers%20in%0ASSMs.%20By%20selectively%20integrating%20shallowlayer%20hidden%20states%20into%20deeper%20layers%2C%0ADenseSSM%20retains%20fine-grained%20information%20crucial%20for%20the%20final%20output.%20Dense%0Aconnections%20enhanced%20DenseSSM%20still%20maintains%20the%20training%20parallelizability%0Aand%20inference%20efficiency.%20The%20proposed%20method%20can%20be%20widely%20applicable%20to%0Avarious%20SSM%20types%20like%20RetNet%20and%20Mamba.%20With%20similar%20model%20size%2C%20DenseSSM%0Aachieves%20significant%20improvements%2C%20exemplified%20by%20DenseRetNet%20outperforming%20the%0Aoriginal%20RetNet%20with%20up%20to%205%25%20accuracy%20improvement%20on%20public%20benchmarks.%20code%0Ais%20avalaible%20at%20https%3A//github.com/WailordHe/DenseSSM%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00818v2&entry.124074799=Read"},
{"title": "TaylorShift: Shifting the Complexity of Self-Attention from Squared to\n  Linear (and Back) using Taylor-Softmax", "author": "Tobias Christian Nauen and Sebastian Palacio and Andreas Dengel", "abstract": "  The quadratic complexity of the attention mechanism represents one of the\nbiggest hurdles for processing long sequences using Transformers. Current\nmethods, relying on sparse representations or stateful recurrence, sacrifice\ntoken-to-token interactions, which ultimately leads to compromises in\nperformance. This paper introduces TaylorShift, a novel reformulation of the\nTaylor softmax that enables computing full token-to-token interactions in\nlinear time and space. We analytically determine the crossover points where\nemploying TaylorShift becomes more efficient than traditional attention,\naligning closely with empirical measurements. Specifically, our findings\ndemonstrate that TaylorShift enhances memory efficiency for sequences as short\nas 800 tokens and accelerates inference for inputs of approximately 1700 tokens\nand beyond. For shorter sequences, TaylorShift scales comparably with the\nvanilla attention. Furthermore, a classification benchmark across five tasks\ninvolving long sequences reveals no degradation in accuracy when employing\nTransformers equipped with TaylorShift. For reproducibility, we provide access\nto our code under https://github.com/tobna/TaylorShift.\n", "link": "http://arxiv.org/abs/2403.02920v1", "date": "2024-03-05", "relevancy": 2.0054, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5206}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4862}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaylorShift%3A%20Shifting%20the%20Complexity%20of%20Self-Attention%20from%20Squared%20to%0A%20%20Linear%20%28and%20Back%29%20using%20Taylor-Softmax&entry.906535625=Tobias%20Christian%20Nauen%20and%20Sebastian%20Palacio%20and%20Andreas%20Dengel&entry.1292438233=%20%20The%20quadratic%20complexity%20of%20the%20attention%20mechanism%20represents%20one%20of%20the%0Abiggest%20hurdles%20for%20processing%20long%20sequences%20using%20Transformers.%20Current%0Amethods%2C%20relying%20on%20sparse%20representations%20or%20stateful%20recurrence%2C%20sacrifice%0Atoken-to-token%20interactions%2C%20which%20ultimately%20leads%20to%20compromises%20in%0Aperformance.%20This%20paper%20introduces%20TaylorShift%2C%20a%20novel%20reformulation%20of%20the%0ATaylor%20softmax%20that%20enables%20computing%20full%20token-to-token%20interactions%20in%0Alinear%20time%20and%20space.%20We%20analytically%20determine%20the%20crossover%20points%20where%0Aemploying%20TaylorShift%20becomes%20more%20efficient%20than%20traditional%20attention%2C%0Aaligning%20closely%20with%20empirical%20measurements.%20Specifically%2C%20our%20findings%0Ademonstrate%20that%20TaylorShift%20enhances%20memory%20efficiency%20for%20sequences%20as%20short%0Aas%20800%20tokens%20and%20accelerates%20inference%20for%20inputs%20of%20approximately%201700%20tokens%0Aand%20beyond.%20For%20shorter%20sequences%2C%20TaylorShift%20scales%20comparably%20with%20the%0Avanilla%20attention.%20Furthermore%2C%20a%20classification%20benchmark%20across%20five%20tasks%0Ainvolving%20long%20sequences%20reveals%20no%20degradation%20in%20accuracy%20when%20employing%0ATransformers%20equipped%20with%20TaylorShift.%20For%20reproducibility%2C%20we%20provide%20access%0Ato%20our%20code%20under%20https%3A//github.com/tobna/TaylorShift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02920v1&entry.124074799=Read"},
{"title": "From Spectra to Biophysical Insights: End-to-End Learning with a Biased\n  Radiative Transfer Model", "author": "Yihang She and Clement Atzberger and Andrew Blake and Srinivasan Keshav", "abstract": "  Advances in machine learning have boosted the use of Earth observation data\nfor climate change research. Yet, the interpretability of machine-learned\nrepresentations remains a challenge, particularly in understanding forests'\nbiophysical reactions to climate change. Traditional methods in remote sensing\nthat invert radiative transfer models (RTMs) to retrieve biophysical variables\nfrom spectral data often fail to account for biases inherent in the RTM,\nespecially for complex forests. We propose to integrate RTMs into an\nauto-encoder architecture, creating an end-to-end learning approach. Our method\nnot only corrects biases in RTMs but also outperforms traditional techniques\nfor variable retrieval like neural network regression. Furthermore, our\nframework has potential generally for inverting biased physical models. The\ncode is available on https://github.com/yihshe/ai-refined-rtm.git.\n", "link": "http://arxiv.org/abs/2403.02922v1", "date": "2024-03-05", "relevancy": 1.9939, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5347}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5017}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4808}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Spectra%20to%20Biophysical%20Insights%3A%20End-to-End%20Learning%20with%20a%20Biased%0A%20%20Radiative%20Transfer%20Model&entry.906535625=Yihang%20She%20and%20Clement%20Atzberger%20and%20Andrew%20Blake%20and%20Srinivasan%20Keshav&entry.1292438233=%20%20Advances%20in%20machine%20learning%20have%20boosted%20the%20use%20of%20Earth%20observation%20data%0Afor%20climate%20change%20research.%20Yet%2C%20the%20interpretability%20of%20machine-learned%0Arepresentations%20remains%20a%20challenge%2C%20particularly%20in%20understanding%20forests%27%0Abiophysical%20reactions%20to%20climate%20change.%20Traditional%20methods%20in%20remote%20sensing%0Athat%20invert%20radiative%20transfer%20models%20%28RTMs%29%20to%20retrieve%20biophysical%20variables%0Afrom%20spectral%20data%20often%20fail%20to%20account%20for%20biases%20inherent%20in%20the%20RTM%2C%0Aespecially%20for%20complex%20forests.%20We%20propose%20to%20integrate%20RTMs%20into%20an%0Aauto-encoder%20architecture%2C%20creating%20an%20end-to-end%20learning%20approach.%20Our%20method%0Anot%20only%20corrects%20biases%20in%20RTMs%20but%20also%20outperforms%20traditional%20techniques%0Afor%20variable%20retrieval%20like%20neural%20network%20regression.%20Furthermore%2C%20our%0Aframework%20has%20potential%20generally%20for%20inverting%20biased%20physical%20models.%20The%0Acode%20is%20available%20on%20https%3A//github.com/yihshe/ai-refined-rtm.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02922v1&entry.124074799=Read"},
{"title": "In Search of Truth: An Interrogation Approach to Hallucination Detection", "author": "Yakir Yehuda and Itzik Malkiel and Oren Barkan and Jonathan Weill and Royi Ronen and Noam Koenigstein", "abstract": "  Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 62% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\n(B-ACC) of 87%, all without relying on external knowledge.\n", "link": "http://arxiv.org/abs/2403.02889v1", "date": "2024-03-05", "relevancy": 1.9694, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4999}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4789}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20Search%20of%20Truth%3A%20An%20Interrogation%20Approach%20to%20Hallucination%20Detection&entry.906535625=Yakir%20Yehuda%20and%20Itzik%20Malkiel%20and%20Oren%20Barkan%20and%20Jonathan%20Weill%20and%20Royi%20Ronen%20and%20Noam%20Koenigstein&entry.1292438233=%20%20Despite%20the%20many%20advances%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20their%0Aunprecedented%20rapid%20evolution%2C%20their%20impact%20and%20integration%20into%20every%20facet%20of%0Aour%20daily%20lives%20is%20limited%20due%20to%20various%20reasons.%20One%20critical%20factor%0Ahindering%20their%20widespread%20adoption%20is%20the%20occurrence%20of%20hallucinations%2C%20where%0ALLMs%20invent%20answers%20that%20sound%20realistic%2C%20yet%20drift%20away%20from%20factual%20truth.%20In%0Athis%20paper%2C%20we%20present%20a%20novel%20method%20for%20detecting%20hallucinations%20in%20large%0Alanguage%20models%2C%20which%20tackles%20a%20critical%20issue%20in%20the%20adoption%20of%20these%20models%0Ain%20various%20real-world%20scenarios.%20Through%20extensive%20evaluations%20across%20multiple%0Adatasets%20and%20LLMs%2C%20including%20Llama-2%2C%20we%20study%20the%20hallucination%20levels%20of%0Avarious%20recent%20LLMs%20and%20demonstrate%20the%20effectiveness%20of%20our%20method%20to%0Aautomatically%20detect%20them.%20Notably%2C%20we%20observe%20up%20to%2062%25%20hallucinations%20for%0ALlama-2%20in%20a%20specific%20experiment%2C%20where%20our%20method%20achieves%20a%20Balanced%20Accuracy%0A%28B-ACC%29%20of%2087%25%2C%20all%20without%20relying%20on%20external%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02889v1&entry.124074799=Read"},
{"title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by\n  Exploring Refusal Loss Landscapes", "author": "Xiaomeng Hu and Pin-Yu Chen and Tsung-Yi Ho", "abstract": "  Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold.\n", "link": "http://arxiv.org/abs/2403.00867v2", "date": "2024-03-05", "relevancy": 1.9667, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5102}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4863}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4753}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Cuff%3A%20Detecting%20Jailbreak%20Attacks%20on%20Large%20Language%20Models%20by%0A%20%20Exploring%20Refusal%20Loss%20Landscapes&entry.906535625=Xiaomeng%20Hu%20and%20Pin-Yu%20Chen%20and%20Tsung-Yi%20Ho&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20becoming%20a%20prominent%20generative%20AI%20tool%2C%0Awhere%20the%20user%20enters%20a%20query%20and%20the%20LLM%20generates%20an%20answer.%20To%20reduce%20harm%0Aand%20misuse%2C%20efforts%20have%20been%20made%20to%20align%20these%20LLMs%20to%20human%20values%20using%0Aadvanced%20training%20techniques%20such%20as%20Reinforcement%20Learning%20from%20Human%20Feedback%0A%28RLHF%29.%20However%2C%20recent%20studies%20have%20highlighted%20the%20vulnerability%20of%20LLMs%20to%0Aadversarial%20jailbreak%20attempts%20aiming%20at%20subverting%20the%20embedded%20safety%0Aguardrails.%20To%20address%20this%20challenge%2C%20this%20paper%20defines%20and%20investigates%20the%0ARefusal%20Loss%20of%20LLMs%20and%20then%20proposes%20a%20method%20called%20Gradient%20Cuff%20to%20detect%0Ajailbreak%20attempts.%20Gradient%20Cuff%20exploits%20the%20unique%20properties%20observed%20in%0Athe%20refusal%20loss%20landscape%2C%20including%20functional%20values%20and%20its%20smoothness%2C%20to%0Adesign%20an%20effective%20two-step%20detection%20strategy.%20Experimental%20results%20on%20two%0Aaligned%20LLMs%20%28LLaMA-2-7B-Chat%20and%20Vicuna-7B-V1.5%29%20and%20six%20types%20of%20jailbreak%0Aattacks%20%28GCG%2C%20AutoDAN%2C%20PAIR%2C%20TAP%2C%20Base64%2C%20and%20LRL%29%20show%20that%20Gradient%20Cuff%20can%0Asignificantly%20improve%20the%20LLM%27s%20rejection%20capability%20for%20malicious%20jailbreak%0Aqueries%2C%20while%20maintaining%20the%20model%27s%20performance%20for%20benign%20user%20queries%20by%0Aadjusting%20the%20detection%20threshold.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00867v2&entry.124074799=Read"},
{"title": "Directed Acyclic Graph Structure Learning from Dynamic Graphs", "author": "Shaohua Fan and Shuyang Zhang and Xiao Wang and Chuan Shi", "abstract": "  Estimating the structure of directed acyclic graphs (DAGs) of features\n(variables) plays a vital role in revealing the latent data generation process\nand providing causal insights in various applications. Although there have been\nmany studies on structure learning with various types of data, the structure\nlearning on the dynamic graph has not been explored yet, and thus we study the\nlearning problem of node feature generation mechanism on such ubiquitous\ndynamic graph data. In a dynamic graph, we propose to simultaneously estimate\ncontemporaneous relationships and time-lagged interaction relationships between\nthe node features. These two kinds of relationships form a DAG, which could\neffectively characterize the feature generation process in a concise way. To\nlearn such a DAG, we cast the learning problem as a continuous score-based\noptimization problem, which consists of a differentiable score function to\nmeasure the validity of the learned DAGs and a smooth acyclicity constraint to\nensure the acyclicity of the learned DAGs. These two components are translated\ninto an unconstraint augmented Lagrangian objective which could be minimized by\nmature continuous optimization techniques. The resulting algorithm, named\nGraphNOTEARS, outperforms baselines on simulated data across a wide range of\nsettings that may encounter in real-world applications. We also apply the\nproposed approach on two dynamic graphs constructed from the real-world Yelp\ndataset, demonstrating our method could learn the connections between node\nfeatures, which conforms with the domain knowledge.\n", "link": "http://arxiv.org/abs/2211.17029v2", "date": "2024-03-05", "relevancy": 1.9659, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5055}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5045}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4723}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Directed%20Acyclic%20Graph%20Structure%20Learning%20from%20Dynamic%20Graphs&entry.906535625=Shaohua%20Fan%20and%20Shuyang%20Zhang%20and%20Xiao%20Wang%20and%20Chuan%20Shi&entry.1292438233=%20%20Estimating%20the%20structure%20of%20directed%20acyclic%20graphs%20%28DAGs%29%20of%20features%0A%28variables%29%20plays%20a%20vital%20role%20in%20revealing%20the%20latent%20data%20generation%20process%0Aand%20providing%20causal%20insights%20in%20various%20applications.%20Although%20there%20have%20been%0Amany%20studies%20on%20structure%20learning%20with%20various%20types%20of%20data%2C%20the%20structure%0Alearning%20on%20the%20dynamic%20graph%20has%20not%20been%20explored%20yet%2C%20and%20thus%20we%20study%20the%0Alearning%20problem%20of%20node%20feature%20generation%20mechanism%20on%20such%20ubiquitous%0Adynamic%20graph%20data.%20In%20a%20dynamic%20graph%2C%20we%20propose%20to%20simultaneously%20estimate%0Acontemporaneous%20relationships%20and%20time-lagged%20interaction%20relationships%20between%0Athe%20node%20features.%20These%20two%20kinds%20of%20relationships%20form%20a%20DAG%2C%20which%20could%0Aeffectively%20characterize%20the%20feature%20generation%20process%20in%20a%20concise%20way.%20To%0Alearn%20such%20a%20DAG%2C%20we%20cast%20the%20learning%20problem%20as%20a%20continuous%20score-based%0Aoptimization%20problem%2C%20which%20consists%20of%20a%20differentiable%20score%20function%20to%0Ameasure%20the%20validity%20of%20the%20learned%20DAGs%20and%20a%20smooth%20acyclicity%20constraint%20to%0Aensure%20the%20acyclicity%20of%20the%20learned%20DAGs.%20These%20two%20components%20are%20translated%0Ainto%20an%20unconstraint%20augmented%20Lagrangian%20objective%20which%20could%20be%20minimized%20by%0Amature%20continuous%20optimization%20techniques.%20The%20resulting%20algorithm%2C%20named%0AGraphNOTEARS%2C%20outperforms%20baselines%20on%20simulated%20data%20across%20a%20wide%20range%20of%0Asettings%20that%20may%20encounter%20in%20real-world%20applications.%20We%20also%20apply%20the%0Aproposed%20approach%20on%20two%20dynamic%20graphs%20constructed%20from%20the%20real-world%20Yelp%0Adataset%2C%20demonstrating%20our%20method%20could%20learn%20the%20connections%20between%20node%0Afeatures%2C%20which%20conforms%20with%20the%20domain%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.17029v2&entry.124074799=Read"},
{"title": "A General and Flexible Multi-concept Parsing Framework for Multilingual\n  Semantic Matching", "author": "Dong Yao and Asaad Alghamdi and Qingrong Xia and Xiaoye Qu and Xinyu Duan and Zhefeng Wang and Yi Zheng and Baoxing Huai and Peilun Cheng and Zhou Zhao", "abstract": "  Sentence semantic matching is a research hotspot in natural language\nprocessing, which is considerably significant in various key scenarios, such as\ncommunity question answering, searching, chatbot, and recommendation. Since\nmost of the advanced models directly model the semantic relevance among words\nbetween two sentences while neglecting the \\textit{keywords} and\n\\textit{intents} concepts of them, DC-Match is proposed to disentangle keywords\nfrom intents and utilizes them to optimize the matching performance. Although\nDC-Match is a simple yet effective method for semantic matching, it highly\ndepends on the external NER techniques to identify the keywords of sentences,\nwhich limits the performance of semantic matching for minor languages since\nsatisfactory NER tools are usually hard to obtain. In this paper, we propose to\ngenerally and flexibly resolve the text into multi concepts for multilingual\nsemantic matching to liberate the model from the reliance on NER models. To\nthis end, we devise a \\underline{M}ulti-\\underline{C}oncept \\underline{P}arsed\n\\underline{S}emantic \\underline{M}atching framework based on the pre-trained\nlanguage models, abbreviated as \\textbf{MCP-SM}, to extract various concepts\nand infuse them into the classification tokens. We conduct comprehensive\nexperiments on English datasets QQP and MRPC, and Chinese dataset Medical-SM.\nBesides, we experiment on Arabic datasets MQ2Q and XNLI, the outstanding\nperformance further prove MCP-SM's applicability in low-resource languages.\n", "link": "http://arxiv.org/abs/2403.02975v1", "date": "2024-03-05", "relevancy": 1.9628, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4873}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20and%20Flexible%20Multi-concept%20Parsing%20Framework%20for%20Multilingual%0A%20%20Semantic%20Matching&entry.906535625=Dong%20Yao%20and%20Asaad%20Alghamdi%20and%20Qingrong%20Xia%20and%20Xiaoye%20Qu%20and%20Xinyu%20Duan%20and%20Zhefeng%20Wang%20and%20Yi%20Zheng%20and%20Baoxing%20Huai%20and%20Peilun%20Cheng%20and%20Zhou%20Zhao&entry.1292438233=%20%20Sentence%20semantic%20matching%20is%20a%20research%20hotspot%20in%20natural%20language%0Aprocessing%2C%20which%20is%20considerably%20significant%20in%20various%20key%20scenarios%2C%20such%20as%0Acommunity%20question%20answering%2C%20searching%2C%20chatbot%2C%20and%20recommendation.%20Since%0Amost%20of%20the%20advanced%20models%20directly%20model%20the%20semantic%20relevance%20among%20words%0Abetween%20two%20sentences%20while%20neglecting%20the%20%5Ctextit%7Bkeywords%7D%20and%0A%5Ctextit%7Bintents%7D%20concepts%20of%20them%2C%20DC-Match%20is%20proposed%20to%20disentangle%20keywords%0Afrom%20intents%20and%20utilizes%20them%20to%20optimize%20the%20matching%20performance.%20Although%0ADC-Match%20is%20a%20simple%20yet%20effective%20method%20for%20semantic%20matching%2C%20it%20highly%0Adepends%20on%20the%20external%20NER%20techniques%20to%20identify%20the%20keywords%20of%20sentences%2C%0Awhich%20limits%20the%20performance%20of%20semantic%20matching%20for%20minor%20languages%20since%0Asatisfactory%20NER%20tools%20are%20usually%20hard%20to%20obtain.%20In%20this%20paper%2C%20we%20propose%20to%0Agenerally%20and%20flexibly%20resolve%20the%20text%20into%20multi%20concepts%20for%20multilingual%0Asemantic%20matching%20to%20liberate%20the%20model%20from%20the%20reliance%20on%20NER%20models.%20To%0Athis%20end%2C%20we%20devise%20a%20%5Cunderline%7BM%7Dulti-%5Cunderline%7BC%7Doncept%20%5Cunderline%7BP%7Darsed%0A%5Cunderline%7BS%7Demantic%20%5Cunderline%7BM%7Datching%20framework%20based%20on%20the%20pre-trained%0Alanguage%20models%2C%20abbreviated%20as%20%5Ctextbf%7BMCP-SM%7D%2C%20to%20extract%20various%20concepts%0Aand%20infuse%20them%20into%20the%20classification%20tokens.%20We%20conduct%20comprehensive%0Aexperiments%20on%20English%20datasets%20QQP%20and%20MRPC%2C%20and%20Chinese%20dataset%20Medical-SM.%0ABesides%2C%20we%20experiment%20on%20Arabic%20datasets%20MQ2Q%20and%20XNLI%2C%20the%20outstanding%0Aperformance%20further%20prove%20MCP-SM%27s%20applicability%20in%20low-resource%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02975v1&entry.124074799=Read"},
{"title": "How Well Can Transformers Emulate In-context Newton's Method?", "author": "Angeliki Giannou and Liu Yang and Tianhao Wang and Dimitris Papailiopoulos and Jason D. Lee", "abstract": "  Transformer-based models have demonstrated remarkable in-context learning\ncapabilities, prompting extensive research into its underlying mechanisms.\nRecent studies have suggested that Transformers can implement first-order\noptimization algorithms for in-context learning and even second order ones for\nthe case of linear regression. In this work, we study whether Transformers can\nperform higher order optimization methods, beyond the case of linear\nregression. We establish that linear attention Transformers with ReLU layers\ncan approximate second order optimization algorithms for the task of logistic\nregression and achieve $\\epsilon$ error with only a logarithmic to the error\nmore layers. As a by-product we demonstrate the ability of even linear\nattention-only Transformers in implementing a single step of Newton's iteration\nfor matrix inversion with merely two layers. These results suggest the ability\nof the Transformer architecture to implement complex algorithms, beyond\ngradient descent.\n", "link": "http://arxiv.org/abs/2403.03183v1", "date": "2024-03-05", "relevancy": 1.9576, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.52}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4848}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4607}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Well%20Can%20Transformers%20Emulate%20In-context%20Newton%27s%20Method%3F&entry.906535625=Angeliki%20Giannou%20and%20Liu%20Yang%20and%20Tianhao%20Wang%20and%20Dimitris%20Papailiopoulos%20and%20Jason%20D.%20Lee&entry.1292438233=%20%20Transformer-based%20models%20have%20demonstrated%20remarkable%20in-context%20learning%0Acapabilities%2C%20prompting%20extensive%20research%20into%20its%20underlying%20mechanisms.%0ARecent%20studies%20have%20suggested%20that%20Transformers%20can%20implement%20first-order%0Aoptimization%20algorithms%20for%20in-context%20learning%20and%20even%20second%20order%20ones%20for%0Athe%20case%20of%20linear%20regression.%20In%20this%20work%2C%20we%20study%20whether%20Transformers%20can%0Aperform%20higher%20order%20optimization%20methods%2C%20beyond%20the%20case%20of%20linear%0Aregression.%20We%20establish%20that%20linear%20attention%20Transformers%20with%20ReLU%20layers%0Acan%20approximate%20second%20order%20optimization%20algorithms%20for%20the%20task%20of%20logistic%0Aregression%20and%20achieve%20%24%5Cepsilon%24%20error%20with%20only%20a%20logarithmic%20to%20the%20error%0Amore%20layers.%20As%20a%20by-product%20we%20demonstrate%20the%20ability%20of%20even%20linear%0Aattention-only%20Transformers%20in%20implementing%20a%20single%20step%20of%20Newton%27s%20iteration%0Afor%20matrix%20inversion%20with%20merely%20two%20layers.%20These%20results%20suggest%20the%20ability%0Aof%20the%20Transformer%20architecture%20to%20implement%20complex%20algorithms%2C%20beyond%0Agradient%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03183v1&entry.124074799=Read"},
{"title": "\"In Dialogues We Learn\": Towards Personalized Dialogue Without\n  Pre-defined Profiles through In-Dialogue Learning", "author": "Chuanqi Cheng and Quan Tu and Wei Wu and Shuo Shang and Cunli Mao and Zhengtao Yu and Rui Yan", "abstract": "  Personalized dialogue systems have gained significant attention in recent\nyears for their ability to generate responses in alignment with different\npersonas. However, most existing approaches rely on pre-defined personal\nprofiles, which are not only time-consuming and labor-intensive to create but\nalso lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning\nframework that enhances the ability of pre-trained large language models to\nleverage dialogue history to characterize persona for completing personalized\ndialogue generation tasks without pre-defined profiles. Our experiments on\nthree datasets demonstrate that IDL brings substantial improvements, with BLEU\nand ROUGE scores increasing by up to 200% and 247%, respectively. Additionally,\nthe results of human evaluations further validate the efficacy of our proposed\nmethod.\n", "link": "http://arxiv.org/abs/2403.03102v1", "date": "2024-03-05", "relevancy": 1.9355, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5037}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4704}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22In%20Dialogues%20We%20Learn%22%3A%20Towards%20Personalized%20Dialogue%20Without%0A%20%20Pre-defined%20Profiles%20through%20In-Dialogue%20Learning&entry.906535625=Chuanqi%20Cheng%20and%20Quan%20Tu%20and%20Wei%20Wu%20and%20Shuo%20Shang%20and%20Cunli%20Mao%20and%20Zhengtao%20Yu%20and%20Rui%20Yan&entry.1292438233=%20%20Personalized%20dialogue%20systems%20have%20gained%20significant%20attention%20in%20recent%0Ayears%20for%20their%20ability%20to%20generate%20responses%20in%20alignment%20with%20different%0Apersonas.%20However%2C%20most%20existing%20approaches%20rely%20on%20pre-defined%20personal%0Aprofiles%2C%20which%20are%20not%20only%20time-consuming%20and%20labor-intensive%20to%20create%20but%0Aalso%20lack%20flexibility.%20We%20propose%20In-Dialogue%20Learning%20%28IDL%29%2C%20a%20fine-tuning%0Aframework%20that%20enhances%20the%20ability%20of%20pre-trained%20large%20language%20models%20to%0Aleverage%20dialogue%20history%20to%20characterize%20persona%20for%20completing%20personalized%0Adialogue%20generation%20tasks%20without%20pre-defined%20profiles.%20Our%20experiments%20on%0Athree%20datasets%20demonstrate%20that%20IDL%20brings%20substantial%20improvements%2C%20with%20BLEU%0Aand%20ROUGE%20scores%20increasing%20by%20up%20to%20200%25%20and%20247%25%2C%20respectively.%20Additionally%2C%0Athe%20results%20of%20human%20evaluations%20further%20validate%20the%20efficacy%20of%20our%20proposed%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03102v1&entry.124074799=Read"},
{"title": "Nonparametric Linear Feature Learning in Regression Through\n  Regularisation", "author": "Bertille Follain and Francis Bach", "abstract": "  Representation learning plays a crucial role in automated feature selection,\nparticularly in the context of high-dimensional data, where non-parametric\nmethods often struggle. In this study, we focus on supervised learning\nscenarios where the pertinent information resides within a lower-dimensional\nlinear subspace of the data, namely the multi-index model. If this subspace\nwere known, it would greatly enhance prediction, computation, and\ninterpretation. To address this challenge, we propose a novel method for linear\nfeature learning with non-parametric prediction, which simultaneously estimates\nthe prediction function and the linear subspace. Our approach employs empirical\nrisk minimisation, augmented with a penalty on function derivatives, ensuring\nversatility. Leveraging the orthogonality and rotation invariance properties of\nHermite polynomials, we introduce our estimator, named RegFeaL. By utilising\nalternative minimisation, we iteratively rotate the data to improve alignment\nwith leading directions and accurately estimate the relevant dimension in\npractical settings. We establish that our method yields a consistent estimator\nof the prediction function with explicit rates. Additionally, we provide\nempirical results demonstrating the performance of RegFeaL in various\nexperiments.\n", "link": "http://arxiv.org/abs/2307.12754v3", "date": "2024-03-05", "relevancy": 1.9322, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4948}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4772}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4682}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonparametric%20Linear%20Feature%20Learning%20in%20Regression%20Through%0A%20%20Regularisation&entry.906535625=Bertille%20Follain%20and%20Francis%20Bach&entry.1292438233=%20%20Representation%20learning%20plays%20a%20crucial%20role%20in%20automated%20feature%20selection%2C%0Aparticularly%20in%20the%20context%20of%20high-dimensional%20data%2C%20where%20non-parametric%0Amethods%20often%20struggle.%20In%20this%20study%2C%20we%20focus%20on%20supervised%20learning%0Ascenarios%20where%20the%20pertinent%20information%20resides%20within%20a%20lower-dimensional%0Alinear%20subspace%20of%20the%20data%2C%20namely%20the%20multi-index%20model.%20If%20this%20subspace%0Awere%20known%2C%20it%20would%20greatly%20enhance%20prediction%2C%20computation%2C%20and%0Ainterpretation.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20method%20for%20linear%0Afeature%20learning%20with%20non-parametric%20prediction%2C%20which%20simultaneously%20estimates%0Athe%20prediction%20function%20and%20the%20linear%20subspace.%20Our%20approach%20employs%20empirical%0Arisk%20minimisation%2C%20augmented%20with%20a%20penalty%20on%20function%20derivatives%2C%20ensuring%0Aversatility.%20Leveraging%20the%20orthogonality%20and%20rotation%20invariance%20properties%20of%0AHermite%20polynomials%2C%20we%20introduce%20our%20estimator%2C%20named%20RegFeaL.%20By%20utilising%0Aalternative%20minimisation%2C%20we%20iteratively%20rotate%20the%20data%20to%20improve%20alignment%0Awith%20leading%20directions%20and%20accurately%20estimate%20the%20relevant%20dimension%20in%0Apractical%20settings.%20We%20establish%20that%20our%20method%20yields%20a%20consistent%20estimator%0Aof%20the%20prediction%20function%20with%20explicit%20rates.%20Additionally%2C%20we%20provide%0Aempirical%20results%20demonstrating%20the%20performance%20of%20RegFeaL%20in%20various%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12754v3&entry.124074799=Read"},
{"title": "Evolution Transformer: In-Context Evolutionary Optimization", "author": "Robert Tjarko Lange and Yingtao Tian and Yujin Tang", "abstract": "  Evolutionary optimization algorithms are often derived from loose biological\nanalogies and struggle to leverage information obtained during the sequential\ncourse of optimization. An alternative promising approach is to leverage data\nand directly discover powerful optimization principles via meta-optimization.\nIn this work, we follow such a paradigm and introduce Evolution Transformer, a\ncausal Transformer architecture, which can flexibly characterize a family of\nEvolution Strategies. Given a trajectory of evaluations and search distribution\nstatistics, Evolution Transformer outputs a performance-improving update to the\nsearch distribution. The architecture imposes a set of suitable inductive\nbiases, i.e. the invariance of the distribution update to the order of\npopulation members within a generation and equivariance to the order of the\nsearch dimensions. We train the model weights using Evolutionary Algorithm\nDistillation, a technique for supervised optimization of sequence models using\nteacher algorithm trajectories. The resulting model exhibits strong in-context\noptimization performance and shows strong generalization capabilities to\notherwise challenging neuroevolution tasks. We analyze the resulting properties\nof the Evolution Transformer and propose a technique to fully\nself-referentially train the Evolution Transformer, starting from a random\ninitialization and bootstrapping its own learning progress. We provide an open\nsource implementation under https://github.com/RobertTLange/evosax.\n", "link": "http://arxiv.org/abs/2403.02985v1", "date": "2024-03-05", "relevancy": 1.9313, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4937}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4836}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4777}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolution%20Transformer%3A%20In-Context%20Evolutionary%20Optimization&entry.906535625=Robert%20Tjarko%20Lange%20and%20Yingtao%20Tian%20and%20Yujin%20Tang&entry.1292438233=%20%20Evolutionary%20optimization%20algorithms%20are%20often%20derived%20from%20loose%20biological%0Aanalogies%20and%20struggle%20to%20leverage%20information%20obtained%20during%20the%20sequential%0Acourse%20of%20optimization.%20An%20alternative%20promising%20approach%20is%20to%20leverage%20data%0Aand%20directly%20discover%20powerful%20optimization%20principles%20via%20meta-optimization.%0AIn%20this%20work%2C%20we%20follow%20such%20a%20paradigm%20and%20introduce%20Evolution%20Transformer%2C%20a%0Acausal%20Transformer%20architecture%2C%20which%20can%20flexibly%20characterize%20a%20family%20of%0AEvolution%20Strategies.%20Given%20a%20trajectory%20of%20evaluations%20and%20search%20distribution%0Astatistics%2C%20Evolution%20Transformer%20outputs%20a%20performance-improving%20update%20to%20the%0Asearch%20distribution.%20The%20architecture%20imposes%20a%20set%20of%20suitable%20inductive%0Abiases%2C%20i.e.%20the%20invariance%20of%20the%20distribution%20update%20to%20the%20order%20of%0Apopulation%20members%20within%20a%20generation%20and%20equivariance%20to%20the%20order%20of%20the%0Asearch%20dimensions.%20We%20train%20the%20model%20weights%20using%20Evolutionary%20Algorithm%0ADistillation%2C%20a%20technique%20for%20supervised%20optimization%20of%20sequence%20models%20using%0Ateacher%20algorithm%20trajectories.%20The%20resulting%20model%20exhibits%20strong%20in-context%0Aoptimization%20performance%20and%20shows%20strong%20generalization%20capabilities%20to%0Aotherwise%20challenging%20neuroevolution%20tasks.%20We%20analyze%20the%20resulting%20properties%0Aof%20the%20Evolution%20Transformer%20and%20propose%20a%20technique%20to%20fully%0Aself-referentially%20train%20the%20Evolution%20Transformer%2C%20starting%20from%20a%20random%0Ainitialization%20and%20bootstrapping%20its%20own%20learning%20progress.%20We%20provide%20an%20open%0Asource%20implementation%20under%20https%3A//github.com/RobertTLange/evosax.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02985v1&entry.124074799=Read"},
{"title": "Transfer Learning across Different Chemical Domains: Virtual Screening\n  of Organic Materials with Deep Learning Models Pretrained on Small Molecule\n  and Chemical Reaction Data", "author": "Chengwei Zhang and Yushuang Zhai and Ziyang Gong and Hongliang Duan and Yuan-Bin She and Yun-Fang Yang and An Su", "abstract": "  Machine learning is becoming a preferred method for the virtual screening of\norganic materials due to its cost-effectiveness over traditional\ncomputationally demanding techniques. However, the scarcity of labeled data for\norganic materials poses a significant challenge for training advanced machine\nlearning models. This study showcases the potential of utilizing databases of\ndrug-like small molecules and chemical reactions to pretrain the BERT model,\nenhancing its performance in the virtual screening of organic materials. By\nfine-tuning the BERT models with data from five virtual screening tasks, the\nversion pretrained with the USPTO-SMILES dataset achieved R2 scores exceeding\n0.94 for three tasks and over 0.81 for two others. This performance surpasses\nthat of models pretrained on the small molecule or organic materials databases\nand outperforms three traditional machine learning models trained directly on\nvirtual screening data. The success of the USPTO-SMILES pretrained BERT model\ncan be attributed to the diverse array of organic building blocks in the USPTO\ndatabase, offering a broader exploration of the chemical space. The study\nfurther suggests that accessing a reaction database with a wider range of\nreactions than the USPTO could further enhance model performance. Overall, this\nresearch validates the feasibility of applying transfer learning across\ndifferent chemical domains for the efficient virtual screening of organic\nmaterials.\n", "link": "http://arxiv.org/abs/2311.18377v2", "date": "2024-03-05", "relevancy": 1.9276, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4765}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4755}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20across%20Different%20Chemical%20Domains%3A%20Virtual%20Screening%0A%20%20of%20Organic%20Materials%20with%20Deep%20Learning%20Models%20Pretrained%20on%20Small%20Molecule%0A%20%20and%20Chemical%20Reaction%20Data&entry.906535625=Chengwei%20Zhang%20and%20Yushuang%20Zhai%20and%20Ziyang%20Gong%20and%20Hongliang%20Duan%20and%20Yuan-Bin%20She%20and%20Yun-Fang%20Yang%20and%20An%20Su&entry.1292438233=%20%20Machine%20learning%20is%20becoming%20a%20preferred%20method%20for%20the%20virtual%20screening%20of%0Aorganic%20materials%20due%20to%20its%20cost-effectiveness%20over%20traditional%0Acomputationally%20demanding%20techniques.%20However%2C%20the%20scarcity%20of%20labeled%20data%20for%0Aorganic%20materials%20poses%20a%20significant%20challenge%20for%20training%20advanced%20machine%0Alearning%20models.%20This%20study%20showcases%20the%20potential%20of%20utilizing%20databases%20of%0Adrug-like%20small%20molecules%20and%20chemical%20reactions%20to%20pretrain%20the%20BERT%20model%2C%0Aenhancing%20its%20performance%20in%20the%20virtual%20screening%20of%20organic%20materials.%20By%0Afine-tuning%20the%20BERT%20models%20with%20data%20from%20five%20virtual%20screening%20tasks%2C%20the%0Aversion%20pretrained%20with%20the%20USPTO-SMILES%20dataset%20achieved%20R2%20scores%20exceeding%0A0.94%20for%20three%20tasks%20and%20over%200.81%20for%20two%20others.%20This%20performance%20surpasses%0Athat%20of%20models%20pretrained%20on%20the%20small%20molecule%20or%20organic%20materials%20databases%0Aand%20outperforms%20three%20traditional%20machine%20learning%20models%20trained%20directly%20on%0Avirtual%20screening%20data.%20The%20success%20of%20the%20USPTO-SMILES%20pretrained%20BERT%20model%0Acan%20be%20attributed%20to%20the%20diverse%20array%20of%20organic%20building%20blocks%20in%20the%20USPTO%0Adatabase%2C%20offering%20a%20broader%20exploration%20of%20the%20chemical%20space.%20The%20study%0Afurther%20suggests%20that%20accessing%20a%20reaction%20database%20with%20a%20wider%20range%20of%0Areactions%20than%20the%20USPTO%20could%20further%20enhance%20model%20performance.%20Overall%2C%20this%0Aresearch%20validates%20the%20feasibility%20of%20applying%20transfer%20learning%20across%0Adifferent%20chemical%20domains%20for%20the%20efficient%20virtual%20screening%20of%20organic%0Amaterials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18377v2&entry.124074799=Read"},
{"title": "LLMLight: Large Language Models as Traffic Signal Control Agents", "author": "Siqi Lai and Zhao Xu and Weijia Zhang and Hao Liu and Hui Xiong", "abstract": "  Traffic Signal Control (TSC) is a crucial component in urban traffic\nmanagement, aiming to optimize road network efficiency and reduce congestion.\nTraditional methods in TSC, primarily based on transportation engineering and\nreinforcement learning (RL), often exhibit limitations in generalization across\nvaried traffic scenarios and lack interpretability. This paper presents\nLLMLight, a novel framework employing Large Language Models (LLMs) as\ndecision-making agents for TSC. Specifically, the framework begins by\ninstructing the LLM with a knowledgeable prompt detailing real-time traffic\nconditions. Leveraging the advanced generalization capabilities of LLMs,\nLLMLight engages a reasoning and decision-making process akin to human\nintuition for effective traffic control. Moreover, we build LightGPT, a\nspecialized backbone LLM tailored for TSC tasks. By learning nuanced traffic\npatterns and control strategies, LightGPT enhances the LLMLight framework\ncost-effectively. Extensive experiments on nine real-world and synthetic\ndatasets showcase the remarkable effectiveness, generalization ability, and\ninterpretability of LLMLight against nine transportation-based and RL-based\nbaselines.\n", "link": "http://arxiv.org/abs/2312.16044v4", "date": "2024-03-05", "relevancy": 1.9243, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4927}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4746}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.472}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMLight%3A%20Large%20Language%20Models%20as%20Traffic%20Signal%20Control%20Agents&entry.906535625=Siqi%20Lai%20and%20Zhao%20Xu%20and%20Weijia%20Zhang%20and%20Hao%20Liu%20and%20Hui%20Xiong&entry.1292438233=%20%20Traffic%20Signal%20Control%20%28TSC%29%20is%20a%20crucial%20component%20in%20urban%20traffic%0Amanagement%2C%20aiming%20to%20optimize%20road%20network%20efficiency%20and%20reduce%20congestion.%0ATraditional%20methods%20in%20TSC%2C%20primarily%20based%20on%20transportation%20engineering%20and%0Areinforcement%20learning%20%28RL%29%2C%20often%20exhibit%20limitations%20in%20generalization%20across%0Avaried%20traffic%20scenarios%20and%20lack%20interpretability.%20This%20paper%20presents%0ALLMLight%2C%20a%20novel%20framework%20employing%20Large%20Language%20Models%20%28LLMs%29%20as%0Adecision-making%20agents%20for%20TSC.%20Specifically%2C%20the%20framework%20begins%20by%0Ainstructing%20the%20LLM%20with%20a%20knowledgeable%20prompt%20detailing%20real-time%20traffic%0Aconditions.%20Leveraging%20the%20advanced%20generalization%20capabilities%20of%20LLMs%2C%0ALLMLight%20engages%20a%20reasoning%20and%20decision-making%20process%20akin%20to%20human%0Aintuition%20for%20effective%20traffic%20control.%20Moreover%2C%20we%20build%20LightGPT%2C%20a%0Aspecialized%20backbone%20LLM%20tailored%20for%20TSC%20tasks.%20By%20learning%20nuanced%20traffic%0Apatterns%20and%20control%20strategies%2C%20LightGPT%20enhances%20the%20LLMLight%20framework%0Acost-effectively.%20Extensive%20experiments%20on%20nine%20real-world%20and%20synthetic%0Adatasets%20showcase%20the%20remarkable%20effectiveness%2C%20generalization%20ability%2C%20and%0Ainterpretability%20of%20LLMLight%20against%20nine%20transportation-based%20and%20RL-based%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16044v4&entry.124074799=Read"},
{"title": "Distributed Policy Gradient for Linear Quadratic Networked Control with\n  Limited Communication Range", "author": "Yuzi Yan and Yuan Shen", "abstract": "  This paper proposes a scalable distributed policy gradient method and proves\nits convergence to near-optimal solution in multi-agent linear quadratic\nnetworked systems. The agents engage within a specified network under local\ncommunication constraints, implying that each agent can only exchange\ninformation with a limited number of neighboring agents. On the underlying\ngraph of the network, each agent implements its control input depending on its\nnearby neighbors' states in the linear quadratic control setting. We show that\nit is possible to approximate the exact gradient only using local information.\nCompared with the centralized optimal controller, the performance gap decreases\nto zero exponentially as the communication and control ranges increase. We also\ndemonstrate how increasing the communication range enhances system stability in\nthe gradient descent process, thereby elucidating a critical trade-off. The\nsimulation results verify our theoretical findings.\n", "link": "http://arxiv.org/abs/2403.03055v1", "date": "2024-03-05", "relevancy": 1.9187, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5388}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4458}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4341}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Policy%20Gradient%20for%20Linear%20Quadratic%20Networked%20Control%20with%0A%20%20Limited%20Communication%20Range&entry.906535625=Yuzi%20Yan%20and%20Yuan%20Shen&entry.1292438233=%20%20This%20paper%20proposes%20a%20scalable%20distributed%20policy%20gradient%20method%20and%20proves%0Aits%20convergence%20to%20near-optimal%20solution%20in%20multi-agent%20linear%20quadratic%0Anetworked%20systems.%20The%20agents%20engage%20within%20a%20specified%20network%20under%20local%0Acommunication%20constraints%2C%20implying%20that%20each%20agent%20can%20only%20exchange%0Ainformation%20with%20a%20limited%20number%20of%20neighboring%20agents.%20On%20the%20underlying%0Agraph%20of%20the%20network%2C%20each%20agent%20implements%20its%20control%20input%20depending%20on%20its%0Anearby%20neighbors%27%20states%20in%20the%20linear%20quadratic%20control%20setting.%20We%20show%20that%0Ait%20is%20possible%20to%20approximate%20the%20exact%20gradient%20only%20using%20local%20information.%0ACompared%20with%20the%20centralized%20optimal%20controller%2C%20the%20performance%20gap%20decreases%0Ato%20zero%20exponentially%20as%20the%20communication%20and%20control%20ranges%20increase.%20We%20also%0Ademonstrate%20how%20increasing%20the%20communication%20range%20enhances%20system%20stability%20in%0Athe%20gradient%20descent%20process%2C%20thereby%20elucidating%20a%20critical%20trade-off.%20The%0Asimulation%20results%20verify%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03055v1&entry.124074799=Read"},
{"title": "Identification for Tree-shaped Structural Causal Models in Polynomial\n  Time", "author": "Aaryan Gupta and Markus Bl\u00e4ser", "abstract": "  Linear structural causal models (SCMs) are used to express and analyse the\nrelationships between random variables. Direct causal effects are represented\nas directed edges and confounding factors as bidirected edges. Identifying the\ncausal parameters from correlations between the nodes is an open problem in\nartificial intelligence. In this paper, we study SCMs whose directed component\nforms a tree. Van der Zander et al. (AISTATS'22, PLMR 151, pp. 6770--6792,\n2022) give a PSPACE-algorithm for the identification problem in this case,\nwhich is a significant improvement over the general Gr\\\"obner basis approach,\nwhich has doubly-exponential time complexity in the number of structural\nparameters. In this work, we present a randomized polynomial-time algorithm,\nwhich solves the identification problem for tree-shaped SCMs. For every\nstructural parameter, our algorithms decides whether it is generically\nidentifiable, generically 2-identifiable, or generically unidentifiable. (No\nother cases can occur.) In the first two cases, it provides one or two\nfractional affine square root terms of polynomials (FASTPs) for the\ncorresponding parameter, respectively.\n", "link": "http://arxiv.org/abs/2311.14058v2", "date": "2024-03-05", "relevancy": 1.5992, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4306}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4024}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3849}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identification%20for%20Tree-shaped%20Structural%20Causal%20Models%20in%20Polynomial%0A%20%20Time&entry.906535625=Aaryan%20Gupta%20and%20Markus%20Bl%C3%A4ser&entry.1292438233=%20%20Linear%20structural%20causal%20models%20%28SCMs%29%20are%20used%20to%20express%20and%20analyse%20the%0Arelationships%20between%20random%20variables.%20Direct%20causal%20effects%20are%20represented%0Aas%20directed%20edges%20and%20confounding%20factors%20as%20bidirected%20edges.%20Identifying%20the%0Acausal%20parameters%20from%20correlations%20between%20the%20nodes%20is%20an%20open%20problem%20in%0Aartificial%20intelligence.%20In%20this%20paper%2C%20we%20study%20SCMs%20whose%20directed%20component%0Aforms%20a%20tree.%20Van%20der%20Zander%20et%20al.%20%28AISTATS%2722%2C%20PLMR%20151%2C%20pp.%206770--6792%2C%0A2022%29%20give%20a%20PSPACE-algorithm%20for%20the%20identification%20problem%20in%20this%20case%2C%0Awhich%20is%20a%20significant%20improvement%20over%20the%20general%20Gr%5C%22obner%20basis%20approach%2C%0Awhich%20has%20doubly-exponential%20time%20complexity%20in%20the%20number%20of%20structural%0Aparameters.%20In%20this%20work%2C%20we%20present%20a%20randomized%20polynomial-time%20algorithm%2C%0Awhich%20solves%20the%20identification%20problem%20for%20tree-shaped%20SCMs.%20For%20every%0Astructural%20parameter%2C%20our%20algorithms%20decides%20whether%20it%20is%20generically%0Aidentifiable%2C%20generically%202-identifiable%2C%20or%20generically%20unidentifiable.%20%28No%0Aother%20cases%20can%20occur.%29%20In%20the%20first%20two%20cases%2C%20it%20provides%20one%20or%20two%0Afractional%20affine%20square%20root%20terms%20of%20polynomials%20%28FASTPs%29%20for%20the%0Acorresponding%20parameter%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14058v2&entry.124074799=Read"},
{"title": "SAFFIRA: a Framework for Assessing the Reliability of\n  Systolic-Array-Based DNN Accelerators", "author": "Mahdi Taheri and Masoud Daneshtalab and Jaan Raik and Maksim Jenihhin and Salvatore Pappalardo and Paul Jimenez and Bastien Deveautour and Alberto Bosio", "abstract": "  Systolic array has emerged as a prominent architecture for Deep Neural\nNetwork (DNN) hardware accelerators, providing high-throughput and low-latency\nperformance essential for deploying DNNs across diverse applications. However,\nwhen used in safety-critical applications, reliability assessment is mandatory\nto guarantee the correct behavior of DNN accelerators. While fault injection\nstands out as a well-established practical and robust method for reliability\nassessment, it is still a very time-consuming process. This paper addresses the\ntime efficiency issue by introducing a novel hierarchical software-based\nhardware-aware fault injection strategy tailored for systolic array-based DNN\naccelerators.\n", "link": "http://arxiv.org/abs/2403.02946v1", "date": "2024-03-05", "relevancy": 1.7361, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4385}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.431}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4304}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAFFIRA%3A%20a%20Framework%20for%20Assessing%20the%20Reliability%20of%0A%20%20Systolic-Array-Based%20DNN%20Accelerators&entry.906535625=Mahdi%20Taheri%20and%20Masoud%20Daneshtalab%20and%20Jaan%20Raik%20and%20Maksim%20Jenihhin%20and%20Salvatore%20Pappalardo%20and%20Paul%20Jimenez%20and%20Bastien%20Deveautour%20and%20Alberto%20Bosio&entry.1292438233=%20%20Systolic%20array%20has%20emerged%20as%20a%20prominent%20architecture%20for%20Deep%20Neural%0ANetwork%20%28DNN%29%20hardware%20accelerators%2C%20providing%20high-throughput%20and%20low-latency%0Aperformance%20essential%20for%20deploying%20DNNs%20across%20diverse%20applications.%20However%2C%0Awhen%20used%20in%20safety-critical%20applications%2C%20reliability%20assessment%20is%20mandatory%0Ato%20guarantee%20the%20correct%20behavior%20of%20DNN%20accelerators.%20While%20fault%20injection%0Astands%20out%20as%20a%20well-established%20practical%20and%20robust%20method%20for%20reliability%0Aassessment%2C%20it%20is%20still%20a%20very%20time-consuming%20process.%20This%20paper%20addresses%20the%0Atime%20efficiency%20issue%20by%20introducing%20a%20novel%20hierarchical%20software-based%0Ahardware-aware%20fault%20injection%20strategy%20tailored%20for%20systolic%20array-based%20DNN%0Aaccelerators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02946v1&entry.124074799=Read"},
{"title": "Co-Design Optimisation of Morphing Topology and Control of Winged Drones", "author": "Fabio Bergonti and Gabriele Nava and Valentin W\u00fcest and Antonello Paolino and Giuseppe L'Erario and Daniele Pucci and Dario Floreano", "abstract": "  The design and control of winged aircraft and drones is an iterative process\naimed at identifying a compromise of mission-specific costs and constraints.\nWhen agility is required, shape-shifting (morphing) drones represent an\nefficient solution. However, morphing drones require the addition of actuated\njoints that increase the topology and control coupling, making the design\nprocess more complex. We propose a co-design optimisation method that assists\nthe engineers by proposing a morphing drone's conceptual design that includes\ntopology, actuation, morphing strategy, and controller parameters. The method\nconsists of applying multi-objective constraint-based optimisation to a\nmulti-body winged drone with trajectory optimisation to solve the motion\nintelligence problem under diverse flight mission requirements, such as energy\nconsumption and mission completion time. We show that co-designed morphing\ndrones outperform fixed-winged drones in terms of energy efficiency and mission\ntime, suggesting that the proposed co-design method could be a useful addition\nto the aircraft engineering toolbox.\n", "link": "http://arxiv.org/abs/2309.13948v2", "date": "2024-03-05", "relevancy": 1.701, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4309}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4275}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4208}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Design%20Optimisation%20of%20Morphing%20Topology%20and%20Control%20of%20Winged%20Drones&entry.906535625=Fabio%20Bergonti%20and%20Gabriele%20Nava%20and%20Valentin%20W%C3%BCest%20and%20Antonello%20Paolino%20and%20Giuseppe%20L%27Erario%20and%20Daniele%20Pucci%20and%20Dario%20Floreano&entry.1292438233=%20%20The%20design%20and%20control%20of%20winged%20aircraft%20and%20drones%20is%20an%20iterative%20process%0Aaimed%20at%20identifying%20a%20compromise%20of%20mission-specific%20costs%20and%20constraints.%0AWhen%20agility%20is%20required%2C%20shape-shifting%20%28morphing%29%20drones%20represent%20an%0Aefficient%20solution.%20However%2C%20morphing%20drones%20require%20the%20addition%20of%20actuated%0Ajoints%20that%20increase%20the%20topology%20and%20control%20coupling%2C%20making%20the%20design%0Aprocess%20more%20complex.%20We%20propose%20a%20co-design%20optimisation%20method%20that%20assists%0Athe%20engineers%20by%20proposing%20a%20morphing%20drone%27s%20conceptual%20design%20that%20includes%0Atopology%2C%20actuation%2C%20morphing%20strategy%2C%20and%20controller%20parameters.%20The%20method%0Aconsists%20of%20applying%20multi-objective%20constraint-based%20optimisation%20to%20a%0Amulti-body%20winged%20drone%20with%20trajectory%20optimisation%20to%20solve%20the%20motion%0Aintelligence%20problem%20under%20diverse%20flight%20mission%20requirements%2C%20such%20as%20energy%0Aconsumption%20and%20mission%20completion%20time.%20We%20show%20that%20co-designed%20morphing%0Adrones%20outperform%20fixed-winged%20drones%20in%20terms%20of%20energy%20efficiency%20and%20mission%0Atime%2C%20suggesting%20that%20the%20proposed%20co-design%20method%20could%20be%20a%20useful%20addition%0Ato%20the%20aircraft%20engineering%20toolbox.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.13948v2&entry.124074799=Read"},
{"title": "Improving Variational Autoencoder Estimation from Incomplete Data with\n  Mixture Variational Families", "author": "Vaidotas Simkus and Michael U. Gutmann", "abstract": "  We consider the task of estimating variational autoencoders (VAEs) when the\ntraining data is incomplete. We show that missing data increases the complexity\nof the model's posterior distribution over the latent variables compared to the\nfully-observed case. The increased complexity may adversely affect the fit of\nthe model due to a mismatch between the variational and model posterior\ndistributions. We introduce two strategies based on (i) finite\nvariational-mixture and (ii) imputation-based variational-mixture distributions\nto address the increased posterior complexity. Through a comprehensive\nevaluation of the proposed approaches, we show that variational mixtures are\neffective at improving the accuracy of VAE estimation from incomplete data.\n", "link": "http://arxiv.org/abs/2403.03069v1", "date": "2024-03-05", "relevancy": 1.3872, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4725}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4525}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.447}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Variational%20Autoencoder%20Estimation%20from%20Incomplete%20Data%20with%0A%20%20Mixture%20Variational%20Families&entry.906535625=Vaidotas%20Simkus%20and%20Michael%20U.%20Gutmann&entry.1292438233=%20%20We%20consider%20the%20task%20of%20estimating%20variational%20autoencoders%20%28VAEs%29%20when%20the%0Atraining%20data%20is%20incomplete.%20We%20show%20that%20missing%20data%20increases%20the%20complexity%0Aof%20the%20model%27s%20posterior%20distribution%20over%20the%20latent%20variables%20compared%20to%20the%0Afully-observed%20case.%20The%20increased%20complexity%20may%20adversely%20affect%20the%20fit%20of%0Athe%20model%20due%20to%20a%20mismatch%20between%20the%20variational%20and%20model%20posterior%0Adistributions.%20We%20introduce%20two%20strategies%20based%20on%20%28i%29%20finite%0Avariational-mixture%20and%20%28ii%29%20imputation-based%20variational-mixture%20distributions%0Ato%20address%20the%20increased%20posterior%20complexity.%20Through%20a%20comprehensive%0Aevaluation%20of%20the%20proposed%20approaches%2C%20we%20show%20that%20variational%20mixtures%20are%0Aeffective%20at%20improving%20the%20accuracy%20of%20VAE%20estimation%20from%20incomplete%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03069v1&entry.124074799=Read"},
{"title": "Mem-elements based Neuromorphic Hardware for Neural Network Application", "author": "Ankur Singh", "abstract": "  The thesis investigates the utilization of memristive and memcapacitive\ncrossbar arrays in low-power machine learning accelerators, offering a\ncomprehensive co-design framework for deep neural networks (DNN). The model,\nimplemented through a hybrid Python and PyTorch approach, accounts for various\nnon-idealities, achieving exceptional training accuracies of 90.02% and 91.03%\nfor the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on\nan 8-layer VGG network. Additionally, the thesis introduces a novel approach to\nemulate meminductor devices using Operational Transconductance Amplifiers (OTA)\nand capacitors, showcasing adjustable behavior. Transistor-level simulations in\n180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed\nmeminductor emulator's viability with a power consumption of 0.337 mW. The\ndesign is further validated in neuromorphic circuits and CNN accelerators,\nachieving training and testing accuracies of 91.04% and 88.82%, respectively.\nNotably, the exclusive use of MOS transistors ensures the feasibility of\nmonolithic IC fabrication. This research significantly contributes to the\nexploration of advanced hardware solutions for efficient and high-performance\nmachine-learning applications.\n", "link": "http://arxiv.org/abs/2403.03002v1", "date": "2024-03-05", "relevancy": 1.8139, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4816}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4422}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4299}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mem-elements%20based%20Neuromorphic%20Hardware%20for%20Neural%20Network%20Application&entry.906535625=Ankur%20Singh&entry.1292438233=%20%20The%20thesis%20investigates%20the%20utilization%20of%20memristive%20and%20memcapacitive%0Acrossbar%20arrays%20in%20low-power%20machine%20learning%20accelerators%2C%20offering%20a%0Acomprehensive%20co-design%20framework%20for%20deep%20neural%20networks%20%28DNN%29.%20The%20model%2C%0Aimplemented%20through%20a%20hybrid%20Python%20and%20PyTorch%20approach%2C%20accounts%20for%20various%0Anon-idealities%2C%20achieving%20exceptional%20training%20accuracies%20of%2090.02%25%20and%2091.03%25%0Afor%20the%20CIFAR-10%20dataset%20with%20memristive%20and%20memcapacitive%20crossbar%20arrays%20on%0Aan%208-layer%20VGG%20network.%20Additionally%2C%20the%20thesis%20introduces%20a%20novel%20approach%20to%0Aemulate%20meminductor%20devices%20using%20Operational%20Transconductance%20Amplifiers%20%28OTA%29%0Aand%20capacitors%2C%20showcasing%20adjustable%20behavior.%20Transistor-level%20simulations%20in%0A180%20nm%20CMOS%20technology%2C%20operating%20at%2060%20MHz%2C%20demonstrate%20the%20proposed%0Ameminductor%20emulator%27s%20viability%20with%20a%20power%20consumption%20of%200.337%20mW.%20The%0Adesign%20is%20further%20validated%20in%20neuromorphic%20circuits%20and%20CNN%20accelerators%2C%0Aachieving%20training%20and%20testing%20accuracies%20of%2091.04%25%20and%2088.82%25%2C%20respectively.%0ANotably%2C%20the%20exclusive%20use%20of%20MOS%20transistors%20ensures%20the%20feasibility%20of%0Amonolithic%20IC%20fabrication.%20This%20research%20significantly%20contributes%20to%20the%0Aexploration%20of%20advanced%20hardware%20solutions%20for%20efficient%20and%20high-performance%0Amachine-learning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03002v1&entry.124074799=Read"},
{"title": "Towards Democratized Flood Risk Management: An Advanced AI Assistant\n  Enabled by GPT-4 for Enhanced Interpretability and Public Engagement", "author": "Rafaela Martelo and Ruo-Qian Wang", "abstract": "  Real-time flood forecasting plays a crucial role in enabling timely and\neffective emergency responses. However, a significant challenge lies in\nbridging the gap between complex numerical flood models and practical\ndecision-making. Decision-makers often rely on experts to interpret these\nmodels for optimizing flood mitigation strategies. And the public requires\ncomplex techniques to inquiry and understand socio-cultural and institutional\nfactors, often hinders the public's understanding of flood risks. To overcome\nthese challenges, our study introduces an innovative solution: a customized AI\nAssistant powered by the GPT-4 Large Language Model. This AI Assistant is\ndesigned to facilitate effective communication between decision-makers, the\ngeneral public, and flood forecasters, without the requirement of specialized\nknowledge. The new framework utilizes GPT-4's advanced natural language\nunderstanding and function calling capabilities to provide immediate flood\nalerts and respond to various flood-related inquiries. Our developed prototype\nintegrates real-time flood warnings with flood maps and social vulnerability\ndata. It also effectively translates complex flood zone information into\nactionable risk management advice. To assess its performance, we evaluated the\nprototype using six criteria within three main categories: relevance, error\nresilience, and understanding of context. Our research marks a significant step\ntowards a more accessible and user-friendly approach in flood risk management.\nThis study highlights the potential of advanced AI tools like GPT-4 in\ndemocratizing information and enhancing public engagement in critical social\nand environmental issues.\n", "link": "http://arxiv.org/abs/2403.03188v1", "date": "2024-03-05", "relevancy": 1.4604, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5003}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4854}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4819}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Democratized%20Flood%20Risk%20Management%3A%20An%20Advanced%20AI%20Assistant%0A%20%20Enabled%20by%20GPT-4%20for%20Enhanced%20Interpretability%20and%20Public%20Engagement&entry.906535625=Rafaela%20Martelo%20and%20Ruo-Qian%20Wang&entry.1292438233=%20%20Real-time%20flood%20forecasting%20plays%20a%20crucial%20role%20in%20enabling%20timely%20and%0Aeffective%20emergency%20responses.%20However%2C%20a%20significant%20challenge%20lies%20in%0Abridging%20the%20gap%20between%20complex%20numerical%20flood%20models%20and%20practical%0Adecision-making.%20Decision-makers%20often%20rely%20on%20experts%20to%20interpret%20these%0Amodels%20for%20optimizing%20flood%20mitigation%20strategies.%20And%20the%20public%20requires%0Acomplex%20techniques%20to%20inquiry%20and%20understand%20socio-cultural%20and%20institutional%0Afactors%2C%20often%20hinders%20the%20public%27s%20understanding%20of%20flood%20risks.%20To%20overcome%0Athese%20challenges%2C%20our%20study%20introduces%20an%20innovative%20solution%3A%20a%20customized%20AI%0AAssistant%20powered%20by%20the%20GPT-4%20Large%20Language%20Model.%20This%20AI%20Assistant%20is%0Adesigned%20to%20facilitate%20effective%20communication%20between%20decision-makers%2C%20the%0Ageneral%20public%2C%20and%20flood%20forecasters%2C%20without%20the%20requirement%20of%20specialized%0Aknowledge.%20The%20new%20framework%20utilizes%20GPT-4%27s%20advanced%20natural%20language%0Aunderstanding%20and%20function%20calling%20capabilities%20to%20provide%20immediate%20flood%0Aalerts%20and%20respond%20to%20various%20flood-related%20inquiries.%20Our%20developed%20prototype%0Aintegrates%20real-time%20flood%20warnings%20with%20flood%20maps%20and%20social%20vulnerability%0Adata.%20It%20also%20effectively%20translates%20complex%20flood%20zone%20information%20into%0Aactionable%20risk%20management%20advice.%20To%20assess%20its%20performance%2C%20we%20evaluated%20the%0Aprototype%20using%20six%20criteria%20within%20three%20main%20categories%3A%20relevance%2C%20error%0Aresilience%2C%20and%20understanding%20of%20context.%20Our%20research%20marks%20a%20significant%20step%0Atowards%20a%20more%20accessible%20and%20user-friendly%20approach%20in%20flood%20risk%20management.%0AThis%20study%20highlights%20the%20potential%20of%20advanced%20AI%20tools%20like%20GPT-4%20in%0Ademocratizing%20information%20and%20enhancing%20public%20engagement%20in%20critical%20social%0Aand%20environmental%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03188v1&entry.124074799=Read"},
{"title": "Scalable Continuous-time Diffusion Framework for Network Inference and\n  Influence Estimation", "author": "Keke Huang and Ruize Gao and Bogdan Cautis and Xiaokui Xiao", "abstract": "  The study of continuous-time information diffusion has been an important area\nof research for many applications in recent years. When only the diffusion\ntraces (cascades) are accessible, cascade-based network inference and influence\nestimation are two essential problems to explore. Alas, existing methods\nexhibit limited capability to infer and process networks with more than a few\nthousand nodes, suffering from scalability issues. In this paper, we view the\ndiffusion process as a continuous-time dynamical system, based on which we\nestablish a continuous-time diffusion model. Subsequently, we instantiate the\nmodel to a scalable and effective framework (FIM) to approximate the diffusion\npropagation from available cascades, thereby inferring the underlying network\nstructure. Furthermore, we undertake an analysis of the approximation error of\nFIM for network inference. To achieve the desired scalability for influence\nestimation, we devise an advanced sampling technique and significantly boost\nthe efficiency. We also quantify the effect of the approximation error on\ninfluence estimation theoretically. Experimental results showcase the\neffectiveness and superior scalability of FIM on network inference and\ninfluence estimation.\n", "link": "http://arxiv.org/abs/2403.02867v1", "date": "2024-03-05", "relevancy": 1.8567, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4726}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4619}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4489}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Continuous-time%20Diffusion%20Framework%20for%20Network%20Inference%20and%0A%20%20Influence%20Estimation&entry.906535625=Keke%20Huang%20and%20Ruize%20Gao%20and%20Bogdan%20Cautis%20and%20Xiaokui%20Xiao&entry.1292438233=%20%20The%20study%20of%20continuous-time%20information%20diffusion%20has%20been%20an%20important%20area%0Aof%20research%20for%20many%20applications%20in%20recent%20years.%20When%20only%20the%20diffusion%0Atraces%20%28cascades%29%20are%20accessible%2C%20cascade-based%20network%20inference%20and%20influence%0Aestimation%20are%20two%20essential%20problems%20to%20explore.%20Alas%2C%20existing%20methods%0Aexhibit%20limited%20capability%20to%20infer%20and%20process%20networks%20with%20more%20than%20a%20few%0Athousand%20nodes%2C%20suffering%20from%20scalability%20issues.%20In%20this%20paper%2C%20we%20view%20the%0Adiffusion%20process%20as%20a%20continuous-time%20dynamical%20system%2C%20based%20on%20which%20we%0Aestablish%20a%20continuous-time%20diffusion%20model.%20Subsequently%2C%20we%20instantiate%20the%0Amodel%20to%20a%20scalable%20and%20effective%20framework%20%28FIM%29%20to%20approximate%20the%20diffusion%0Apropagation%20from%20available%20cascades%2C%20thereby%20inferring%20the%20underlying%20network%0Astructure.%20Furthermore%2C%20we%20undertake%20an%20analysis%20of%20the%20approximation%20error%20of%0AFIM%20for%20network%20inference.%20To%20achieve%20the%20desired%20scalability%20for%20influence%0Aestimation%2C%20we%20devise%20an%20advanced%20sampling%20technique%20and%20significantly%20boost%0Athe%20efficiency.%20We%20also%20quantify%20the%20effect%20of%20the%20approximation%20error%20on%0Ainfluence%20estimation%20theoretically.%20Experimental%20results%20showcase%20the%0Aeffectiveness%20and%20superior%20scalability%20of%20FIM%20on%20network%20inference%20and%0Ainfluence%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02867v1&entry.124074799=Read"},
{"title": "Robust Federated Learning Mitigates Client-side Training Data\n  Distribution Inference Attacks", "author": "Yichang Xu and Ming Yin and Minghong Fang and Neil Zhenqiang Gong", "abstract": "  Recent studies have revealed that federated learning (FL), once considered\nsecure due to clients not sharing their private data with the server, is\nvulnerable to attacks such as client-side training data distribution inference,\nwhere a malicious client can recreate the victim's data. While various\ncountermeasures exist, they are not practical, often assuming server access to\nsome training data or knowledge of label distribution before the attack.\n  In this work, we bridge the gap by proposing InferGuard, a novel\nByzantine-robust aggregation rule aimed at defending against client-side\ntraining data distribution inference attacks. In our proposed InferGuard, the\nserver first calculates the coordinate-wise median of all the model updates it\nreceives. A client's model update is considered malicious if it significantly\ndeviates from the computed median update. We conduct a thorough evaluation of\nour proposed InferGuard on five benchmark datasets and perform a comparison\nwith ten baseline methods. The results of our experiments indicate that our\ndefense mechanism is highly effective in protecting against client-side\ntraining data distribution inference attacks, even against strong adaptive\nattacks. Furthermore, our method substantially outperforms the baseline methods\nin various practical FL scenarios.\n", "link": "http://arxiv.org/abs/2403.03149v1", "date": "2024-03-05", "relevancy": 1.8489, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4806}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4607}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4565}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Federated%20Learning%20Mitigates%20Client-side%20Training%20Data%0A%20%20Distribution%20Inference%20Attacks&entry.906535625=Yichang%20Xu%20and%20Ming%20Yin%20and%20Minghong%20Fang%20and%20Neil%20Zhenqiang%20Gong&entry.1292438233=%20%20Recent%20studies%20have%20revealed%20that%20federated%20learning%20%28FL%29%2C%20once%20considered%0Asecure%20due%20to%20clients%20not%20sharing%20their%20private%20data%20with%20the%20server%2C%20is%0Avulnerable%20to%20attacks%20such%20as%20client-side%20training%20data%20distribution%20inference%2C%0Awhere%20a%20malicious%20client%20can%20recreate%20the%20victim%27s%20data.%20While%20various%0Acountermeasures%20exist%2C%20they%20are%20not%20practical%2C%20often%20assuming%20server%20access%20to%0Asome%20training%20data%20or%20knowledge%20of%20label%20distribution%20before%20the%20attack.%0A%20%20In%20this%20work%2C%20we%20bridge%20the%20gap%20by%20proposing%20InferGuard%2C%20a%20novel%0AByzantine-robust%20aggregation%20rule%20aimed%20at%20defending%20against%20client-side%0Atraining%20data%20distribution%20inference%20attacks.%20In%20our%20proposed%20InferGuard%2C%20the%0Aserver%20first%20calculates%20the%20coordinate-wise%20median%20of%20all%20the%20model%20updates%20it%0Areceives.%20A%20client%27s%20model%20update%20is%20considered%20malicious%20if%20it%20significantly%0Adeviates%20from%20the%20computed%20median%20update.%20We%20conduct%20a%20thorough%20evaluation%20of%0Aour%20proposed%20InferGuard%20on%20five%20benchmark%20datasets%20and%20perform%20a%20comparison%0Awith%20ten%20baseline%20methods.%20The%20results%20of%20our%20experiments%20indicate%20that%20our%0Adefense%20mechanism%20is%20highly%20effective%20in%20protecting%20against%20client-side%0Atraining%20data%20distribution%20inference%20attacks%2C%20even%20against%20strong%20adaptive%0Aattacks.%20Furthermore%2C%20our%20method%20substantially%20outperforms%20the%20baseline%20methods%0Ain%20various%20practical%20FL%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03149v1&entry.124074799=Read"},
{"title": "A Modular Pneumatic Soft Gripper Design for Aerial Grasping and Landing", "author": "Hiu Ching Cheung and Ching-Wei Chang and Bailun Jiang and Chih-Yung Wen and Henry K. Chu", "abstract": "  Aerial robots have garnered significant attention due to their potential\napplications in various industries, such as inspection, search and rescue, and\ndrone delivery. Successful missions often depend on the ability of these robots\nto grasp and land effectively. This paper presents a novel modular soft gripper\ndesign tailored explicitly for aerial grasping and landing operations. The\nproposed modular pneumatic soft gripper incorporates a feed-forward\nproportional controller to regulate pressure, enabling compliant gripping\ncapabilities. The modular connectors of the soft fingers offer two\nconfigurations for the 4-tip soft gripper, H-base (cylindrical) and X-base\n(spherical), allowing adaptability to different target objects. Additionally,\nthe gripper can serve as a soft landing gear when deflated, eliminating the\nneed for an extra landing gear. This design reduces weight, simplifies aerial\nmanipulation control, and enhances flight efficiency. We demonstrate the\nefficacy of indoor aerial grasping and achieve a maximum payload of 217 g using\nthe proposed soft aerial vehicle and its H-base pneumatic soft gripper (808 g).\n", "link": "http://arxiv.org/abs/2311.00390v2", "date": "2024-03-05", "relevancy": 1.5476, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5402}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4924}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4786}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Modular%20Pneumatic%20Soft%20Gripper%20Design%20for%20Aerial%20Grasping%20and%20Landing&entry.906535625=Hiu%20Ching%20Cheung%20and%20Ching-Wei%20Chang%20and%20Bailun%20Jiang%20and%20Chih-Yung%20Wen%20and%20Henry%20K.%20Chu&entry.1292438233=%20%20Aerial%20robots%20have%20garnered%20significant%20attention%20due%20to%20their%20potential%0Aapplications%20in%20various%20industries%2C%20such%20as%20inspection%2C%20search%20and%20rescue%2C%20and%0Adrone%20delivery.%20Successful%20missions%20often%20depend%20on%20the%20ability%20of%20these%20robots%0Ato%20grasp%20and%20land%20effectively.%20This%20paper%20presents%20a%20novel%20modular%20soft%20gripper%0Adesign%20tailored%20explicitly%20for%20aerial%20grasping%20and%20landing%20operations.%20The%0Aproposed%20modular%20pneumatic%20soft%20gripper%20incorporates%20a%20feed-forward%0Aproportional%20controller%20to%20regulate%20pressure%2C%20enabling%20compliant%20gripping%0Acapabilities.%20The%20modular%20connectors%20of%20the%20soft%20fingers%20offer%20two%0Aconfigurations%20for%20the%204-tip%20soft%20gripper%2C%20H-base%20%28cylindrical%29%20and%20X-base%0A%28spherical%29%2C%20allowing%20adaptability%20to%20different%20target%20objects.%20Additionally%2C%0Athe%20gripper%20can%20serve%20as%20a%20soft%20landing%20gear%20when%20deflated%2C%20eliminating%20the%0Aneed%20for%20an%20extra%20landing%20gear.%20This%20design%20reduces%20weight%2C%20simplifies%20aerial%0Amanipulation%20control%2C%20and%20enhances%20flight%20efficiency.%20We%20demonstrate%20the%0Aefficacy%20of%20indoor%20aerial%20grasping%20and%20achieve%20a%20maximum%20payload%20of%20217%20g%20using%0Athe%20proposed%20soft%20aerial%20vehicle%20and%20its%20H-base%20pneumatic%20soft%20gripper%20%28808%20g%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00390v2&entry.124074799=Read"},
{"title": "Word Importance Explains How Prompts Affect Language Model Outputs", "author": "Stefan Hackmann and Haniyeh Mahmoudian and Mark Steadman and Michael Schmidt", "abstract": "  The emergence of large language models (LLMs) has revolutionized numerous\napplications across industries. However, their \"black box\" nature often hinders\nthe understanding of how they make specific decisions, raising concerns about\ntheir transparency, reliability, and ethical use. This study presents a method\nto improve the explainability of LLMs by varying individual words in prompts to\nuncover their statistical impact on the model outputs. This approach, inspired\nby permutation importance for tabular data, masks each word in the system\nprompt and evaluates its effect on the outputs based on the available text\nscores aggregated over multiple user inputs. Unlike classical attention, word\nimportance measures the impact of prompt words on arbitrarily-defined text\nscores, which enables decomposing the importance of words into the specific\nmeasures of interest--including bias, reading level, verbosity, etc. This\nprocedure also enables measuring impact when attention weights are not\navailable. To test the fidelity of this approach, we explore the effect of\nadding different suffixes to multiple different system prompts and comparing\nsubsequent generations with different large language models. Results show that\nword importance scores are closely related to the expected suffix importances\nfor multiple scoring functions.\n", "link": "http://arxiv.org/abs/2403.03028v1", "date": "2024-03-05", "relevancy": 1.5236, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.402}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3793}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3741}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Word%20Importance%20Explains%20How%20Prompts%20Affect%20Language%20Model%20Outputs&entry.906535625=Stefan%20Hackmann%20and%20Haniyeh%20Mahmoudian%20and%20Mark%20Steadman%20and%20Michael%20Schmidt&entry.1292438233=%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20revolutionized%20numerous%0Aapplications%20across%20industries.%20However%2C%20their%20%22black%20box%22%20nature%20often%20hinders%0Athe%20understanding%20of%20how%20they%20make%20specific%20decisions%2C%20raising%20concerns%20about%0Atheir%20transparency%2C%20reliability%2C%20and%20ethical%20use.%20This%20study%20presents%20a%20method%0Ato%20improve%20the%20explainability%20of%20LLMs%20by%20varying%20individual%20words%20in%20prompts%20to%0Auncover%20their%20statistical%20impact%20on%20the%20model%20outputs.%20This%20approach%2C%20inspired%0Aby%20permutation%20importance%20for%20tabular%20data%2C%20masks%20each%20word%20in%20the%20system%0Aprompt%20and%20evaluates%20its%20effect%20on%20the%20outputs%20based%20on%20the%20available%20text%0Ascores%20aggregated%20over%20multiple%20user%20inputs.%20Unlike%20classical%20attention%2C%20word%0Aimportance%20measures%20the%20impact%20of%20prompt%20words%20on%20arbitrarily-defined%20text%0Ascores%2C%20which%20enables%20decomposing%20the%20importance%20of%20words%20into%20the%20specific%0Ameasures%20of%20interest--including%20bias%2C%20reading%20level%2C%20verbosity%2C%20etc.%20This%0Aprocedure%20also%20enables%20measuring%20impact%20when%20attention%20weights%20are%20not%0Aavailable.%20To%20test%20the%20fidelity%20of%20this%20approach%2C%20we%20explore%20the%20effect%20of%0Aadding%20different%20suffixes%20to%20multiple%20different%20system%20prompts%20and%20comparing%0Asubsequent%20generations%20with%20different%20large%20language%20models.%20Results%20show%20that%0Aword%20importance%20scores%20are%20closely%20related%20to%20the%20expected%20suffix%20importances%0Afor%20multiple%20scoring%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03028v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


