<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Prompt-Driven Dynamic Object-Centric Learning for Single Domain\n  Generalization", "author": "Deng Li and Aming Wu and Yaowei Wang and Yahong Han", "abstract": "  Single-domain generalization aims to learn a model from single source domain\ndata to achieve generalized performance on other unseen target domains.\nExisting works primarily focus on improving the generalization ability of\nstatic networks. However, static networks are unable to dynamically adapt to\nthe diverse variations in different image scenes, leading to limited\ngeneralization capability. Different scenes exhibit varying levels of\ncomplexity, and the complexity of images further varies significantly in\ncross-domain scenarios. In this paper, we propose a dynamic object-centric\nperception network based on prompt learning, aiming to adapt to the variations\nin image complexity. Specifically, we propose an object-centric gating module\nbased on prompt learning to focus attention on the object-centric features\nguided by the various scene prompts. Then, with the object-centric gating\nmasks, the dynamic selective module dynamically selects highly correlated\nfeature regions in both spatial and channel dimensions enabling the model to\nadaptively perceive object-centric relevant features, thereby enhancing the\ngeneralization capability. Extensive experiments were conducted on\nsingle-domain generalization tasks in image classification and object\ndetection. The experimental results demonstrate that our approach outperforms\nstate-of-the-art methods, which validates the effectiveness and generally of\nour proposed method.\n", "link": "http://arxiv.org/abs/2402.18447v1", "date": "2024-02-28", "relevancy": 2.7526, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5721}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5403}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5392}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Driven%20Dynamic%20Object-Centric%20Learning%20for%20Single%20Domain%0A%20%20Generalization&entry.906535625=Deng%20Li%20and%20Aming%20Wu%20and%20Yaowei%20Wang%20and%20Yahong%20Han&entry.1292438233=%20%20Single-domain%20generalization%20aims%20to%20learn%20a%20model%20from%20single%20source%20domain%0Adata%20to%20achieve%20generalized%20performance%20on%20other%20unseen%20target%20domains.%0AExisting%20works%20primarily%20focus%20on%20improving%20the%20generalization%20ability%20of%0Astatic%20networks.%20However%2C%20static%20networks%20are%20unable%20to%20dynamically%20adapt%20to%0Athe%20diverse%20variations%20in%20different%20image%20scenes%2C%20leading%20to%20limited%0Ageneralization%20capability.%20Different%20scenes%20exhibit%20varying%20levels%20of%0Acomplexity%2C%20and%20the%20complexity%20of%20images%20further%20varies%20significantly%20in%0Across-domain%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20dynamic%20object-centric%0Aperception%20network%20based%20on%20prompt%20learning%2C%20aiming%20to%20adapt%20to%20the%20variations%0Ain%20image%20complexity.%20Specifically%2C%20we%20propose%20an%20object-centric%20gating%20module%0Abased%20on%20prompt%20learning%20to%20focus%20attention%20on%20the%20object-centric%20features%0Aguided%20by%20the%20various%20scene%20prompts.%20Then%2C%20with%20the%20object-centric%20gating%0Amasks%2C%20the%20dynamic%20selective%20module%20dynamically%20selects%20highly%20correlated%0Afeature%20regions%20in%20both%20spatial%20and%20channel%20dimensions%20enabling%20the%20model%20to%0Aadaptively%20perceive%20object-centric%20relevant%20features%2C%20thereby%20enhancing%20the%0Ageneralization%20capability.%20Extensive%20experiments%20were%20conducted%20on%0Asingle-domain%20generalization%20tasks%20in%20image%20classification%20and%20object%0Adetection.%20The%20experimental%20results%20demonstrate%20that%20our%20approach%20outperforms%0Astate-of-the-art%20methods%2C%20which%20validates%20the%20effectiveness%20and%20generally%20of%0Aour%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18447v1&entry.124074799=Read"},
{"title": "Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in\n  Dynamic Scenes", "author": "Zhilu Zhang and Haoyu Wang and Shuai Liu and Xiaotao Wang and Lei Lei and Wangmeng Zuo", "abstract": "  Merging multi-exposure images is a common approach for obtaining high dynamic\nrange (HDR) images, with the primary challenge being the avoidance of ghosting\nartifacts in dynamic scenes. Recent methods have proposed using deep neural\nnetworks for deghosting. However, the methods typically rely on sufficient data\nwith HDR ground-truths, which are difficult and costly to collect. In this\nwork, to eliminate the need for labeled data, we propose SelfHDR, a\nself-supervised HDR reconstruction method that only requires dynamic\nmulti-exposure images during training. Specifically, SelfHDR learns a\nreconstruction network under the supervision of two complementary components,\nwhich can be constructed from multi-exposure images and focus on HDR color as\nwell as structure, respectively. The color component is estimated from aligned\nmulti-exposure images, while the structure one is generated through a\nstructure-focused network that is supervised by the color component and an\ninput reference (\\eg, medium-exposure) image. During testing, the learned\nreconstruction network is directly deployed to predict an HDR image.\nExperiments on real-world images demonstrate our SelfHDR achieves superior\nresults against the state-of-the-art self-supervised methods, and comparable\nperformance to supervised ones. Codes are available at\nhttps://github.com/cszhilu1998/SelfHDR\n", "link": "http://arxiv.org/abs/2310.01840v2", "date": "2024-02-28", "relevancy": 2.6575, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5357}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5187}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20High%20Dynamic%20Range%20Imaging%20with%20Multi-Exposure%20Images%20in%0A%20%20Dynamic%20Scenes&entry.906535625=Zhilu%20Zhang%20and%20Haoyu%20Wang%20and%20Shuai%20Liu%20and%20Xiaotao%20Wang%20and%20Lei%20Lei%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Merging%20multi-exposure%20images%20is%20a%20common%20approach%20for%20obtaining%20high%20dynamic%0Arange%20%28HDR%29%20images%2C%20with%20the%20primary%20challenge%20being%20the%20avoidance%20of%20ghosting%0Aartifacts%20in%20dynamic%20scenes.%20Recent%20methods%20have%20proposed%20using%20deep%20neural%0Anetworks%20for%20deghosting.%20However%2C%20the%20methods%20typically%20rely%20on%20sufficient%20data%0Awith%20HDR%20ground-truths%2C%20which%20are%20difficult%20and%20costly%20to%20collect.%20In%20this%0Awork%2C%20to%20eliminate%20the%20need%20for%20labeled%20data%2C%20we%20propose%20SelfHDR%2C%20a%0Aself-supervised%20HDR%20reconstruction%20method%20that%20only%20requires%20dynamic%0Amulti-exposure%20images%20during%20training.%20Specifically%2C%20SelfHDR%20learns%20a%0Areconstruction%20network%20under%20the%20supervision%20of%20two%20complementary%20components%2C%0Awhich%20can%20be%20constructed%20from%20multi-exposure%20images%20and%20focus%20on%20HDR%20color%20as%0Awell%20as%20structure%2C%20respectively.%20The%20color%20component%20is%20estimated%20from%20aligned%0Amulti-exposure%20images%2C%20while%20the%20structure%20one%20is%20generated%20through%20a%0Astructure-focused%20network%20that%20is%20supervised%20by%20the%20color%20component%20and%20an%0Ainput%20reference%20%28%5Ceg%2C%20medium-exposure%29%20image.%20During%20testing%2C%20the%20learned%0Areconstruction%20network%20is%20directly%20deployed%20to%20predict%20an%20HDR%20image.%0AExperiments%20on%20real-world%20images%20demonstrate%20our%20SelfHDR%20achieves%20superior%0Aresults%20against%20the%20state-of-the-art%20self-supervised%20methods%2C%20and%20comparable%0Aperformance%20to%20supervised%20ones.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/cszhilu1998/SelfHDR%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01840v2&entry.124074799=Read"},
{"title": "Diffusion Language Models Are Versatile Protein Learners", "author": "Xinyou Wang and Zaixiang Zheng and Fei Ye and Dongyu Xue and Shujian Huang and Quanquan Gu", "abstract": "  This paper introduces diffusion protein language model (DPLM), a versatile\nprotein language model that demonstrates strong generative and predictive\ncapabilities for protein sequences. We first pre-train scalable DPLMs from\nevolutionary-scale protein sequences within a generative self-supervised\ndiscrete diffusion probabilistic framework, which generalizes language modeling\nfor proteins in a principled way. After pre-training, DPLM exhibits the ability\nto generate structurally plausible, novel, and diverse protein sequences for\nunconditional generation. We further demonstrate the proposed diffusion\ngenerative pre-training makes DPLM possess a better understanding of proteins,\nmaking it a superior representation learner, which can be fine-tuned for\nvarious predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).\nMoreover, DPLM can be tailored for various needs, which showcases its prowess\nof conditional generation in several ways: (1) conditioning on partial peptide\nsequences, e.g., generating scaffolds for functional motifs with high success\nrate; (2) incorporating other modalities as conditioner, e.g.,\nstructure-conditioned generation for inverse folding; and (3) steering sequence\ngeneration towards desired properties, e.g., satisfying specified secondary\nstructures, through a plug-and-play classifier guidance.\n", "link": "http://arxiv.org/abs/2402.18567v1", "date": "2024-02-28", "relevancy": 2.5048, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5122}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4954}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4952}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Language%20Models%20Are%20Versatile%20Protein%20Learners&entry.906535625=Xinyou%20Wang%20and%20Zaixiang%20Zheng%20and%20Fei%20Ye%20and%20Dongyu%20Xue%20and%20Shujian%20Huang%20and%20Quanquan%20Gu&entry.1292438233=%20%20This%20paper%20introduces%20diffusion%20protein%20language%20model%20%28DPLM%29%2C%20a%20versatile%0Aprotein%20language%20model%20that%20demonstrates%20strong%20generative%20and%20predictive%0Acapabilities%20for%20protein%20sequences.%20We%20first%20pre-train%20scalable%20DPLMs%20from%0Aevolutionary-scale%20protein%20sequences%20within%20a%20generative%20self-supervised%0Adiscrete%20diffusion%20probabilistic%20framework%2C%20which%20generalizes%20language%20modeling%0Afor%20proteins%20in%20a%20principled%20way.%20After%20pre-training%2C%20DPLM%20exhibits%20the%20ability%0Ato%20generate%20structurally%20plausible%2C%20novel%2C%20and%20diverse%20protein%20sequences%20for%0Aunconditional%20generation.%20We%20further%20demonstrate%20the%20proposed%20diffusion%0Agenerative%20pre-training%20makes%20DPLM%20possess%20a%20better%20understanding%20of%20proteins%2C%0Amaking%20it%20a%20superior%20representation%20learner%2C%20which%20can%20be%20fine-tuned%20for%0Avarious%20predictive%20tasks%2C%20comparing%20favorably%20to%20ESM2%20%28Lin%20et%20al.%2C%202022%29.%0AMoreover%2C%20DPLM%20can%20be%20tailored%20for%20various%20needs%2C%20which%20showcases%20its%20prowess%0Aof%20conditional%20generation%20in%20several%20ways%3A%20%281%29%20conditioning%20on%20partial%20peptide%0Asequences%2C%20e.g.%2C%20generating%20scaffolds%20for%20functional%20motifs%20with%20high%20success%0Arate%3B%20%282%29%20incorporating%20other%20modalities%20as%20conditioner%2C%20e.g.%2C%0Astructure-conditioned%20generation%20for%20inverse%20folding%3B%20and%20%283%29%20steering%20sequence%0Ageneration%20towards%20desired%20properties%2C%20e.g.%2C%20satisfying%20specified%20secondary%0Astructures%2C%20through%20a%20plug-and-play%20classifier%20guidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18567v1&entry.124074799=Read"},
{"title": "Incorporating Prior Knowledge into Neural Networks through an Implicit\n  Composite Kernel", "author": "Ziyang Jiang and Tongshu Zheng and Yiling Liu and David Carlson", "abstract": "  It is challenging to guide neural network (NN) learning with prior knowledge.\nIn contrast, many known properties, such as spatial smoothness or seasonality,\nare straightforward to model by choosing an appropriate kernel in a Gaussian\nprocess (GP). Many deep learning applications could be enhanced by modeling\nsuch known properties. For example, convolutional neural networks (CNNs) are\nfrequently used in remote sensing, which is subject to strong seasonal effects.\nWe propose to blend the strengths of deep learning and the clear modeling\ncapabilities of GPs by using a composite kernel that combines a kernel\nimplicitly defined by a neural network with a second kernel function chosen to\nmodel known properties (e.g., seasonality). We implement this idea by combining\na deep network and an efficient mapping based on the Nystrom approximation,\nwhich we call Implicit Composite Kernel (ICK). We then adopt a\nsample-then-optimize approach to approximate the full GP posterior\ndistribution. We demonstrate that ICK has superior performance and flexibility\non both synthetic and real-world data sets. We believe that ICK framework can\nbe used to include prior information into neural networks in many applications.\n", "link": "http://arxiv.org/abs/2205.07384v8", "date": "2024-02-28", "relevancy": 2.5004, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5327}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4871}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4804}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Prior%20Knowledge%20into%20Neural%20Networks%20through%20an%20Implicit%0A%20%20Composite%20Kernel&entry.906535625=Ziyang%20Jiang%20and%20Tongshu%20Zheng%20and%20Yiling%20Liu%20and%20David%20Carlson&entry.1292438233=%20%20It%20is%20challenging%20to%20guide%20neural%20network%20%28NN%29%20learning%20with%20prior%20knowledge.%0AIn%20contrast%2C%20many%20known%20properties%2C%20such%20as%20spatial%20smoothness%20or%20seasonality%2C%0Aare%20straightforward%20to%20model%20by%20choosing%20an%20appropriate%20kernel%20in%20a%20Gaussian%0Aprocess%20%28GP%29.%20Many%20deep%20learning%20applications%20could%20be%20enhanced%20by%20modeling%0Asuch%20known%20properties.%20For%20example%2C%20convolutional%20neural%20networks%20%28CNNs%29%20are%0Afrequently%20used%20in%20remote%20sensing%2C%20which%20is%20subject%20to%20strong%20seasonal%20effects.%0AWe%20propose%20to%20blend%20the%20strengths%20of%20deep%20learning%20and%20the%20clear%20modeling%0Acapabilities%20of%20GPs%20by%20using%20a%20composite%20kernel%20that%20combines%20a%20kernel%0Aimplicitly%20defined%20by%20a%20neural%20network%20with%20a%20second%20kernel%20function%20chosen%20to%0Amodel%20known%20properties%20%28e.g.%2C%20seasonality%29.%20We%20implement%20this%20idea%20by%20combining%0Aa%20deep%20network%20and%20an%20efficient%20mapping%20based%20on%20the%20Nystrom%20approximation%2C%0Awhich%20we%20call%20Implicit%20Composite%20Kernel%20%28ICK%29.%20We%20then%20adopt%20a%0Asample-then-optimize%20approach%20to%20approximate%20the%20full%20GP%20posterior%0Adistribution.%20We%20demonstrate%20that%20ICK%20has%20superior%20performance%20and%20flexibility%0Aon%20both%20synthetic%20and%20real-world%20data%20sets.%20We%20believe%20that%20ICK%20framework%20can%0Abe%20used%20to%20include%20prior%20information%20into%20neural%20networks%20in%20many%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.07384v8&entry.124074799=Read"},
{"title": "Gradient Reweighting: Towards Imbalanced Class-Incremental Learning", "author": "Jiangpeng He and Fengqing Zhu", "abstract": "  Class-Incremental Learning (CIL) trains a model to continually recognize new\nclasses from non-stationary data while retaining learned knowledge. A major\nchallenge of CIL arises when applying to real-world data characterized by\nnon-uniform distribution, which introduces a dual imbalance problem involving\n(i) disparities between stored exemplars of old tasks and new class data\n(inter-phase imbalance), and (ii) severe class imbalances within each\nindividual task (intra-phase imbalance). We show that this dual imbalance issue\ncauses skewed gradient updates with biased weights in FC layers, thus inducing\nover/under-fitting and catastrophic forgetting in CIL. Our method addresses it\nby reweighting the gradients towards balanced optimization and unbiased\nclassifier learning. Additionally, we observe imbalanced forgetting where\nparadoxically the instance-rich classes suffer higher performance degradation\nduring CIL due to a larger amount of training data becoming unavailable in\nsubsequent learning phases. To tackle this, we further introduce a\ndistribution-aware knowledge distillation loss to mitigate forgetting by\naligning output logits proportionally with the distribution of lost training\ndata. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across\nvarious evaluation protocols and demonstrate consistent improvements compared\nto existing works, showing great potential to apply CIL in real-world scenarios\nwith enhanced robustness and effectiveness.\n", "link": "http://arxiv.org/abs/2402.18528v1", "date": "2024-02-28", "relevancy": 2.464, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5327}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4736}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4721}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Reweighting%3A%20Towards%20Imbalanced%20Class-Incremental%20Learning&entry.906535625=Jiangpeng%20He%20and%20Fengqing%20Zhu&entry.1292438233=%20%20Class-Incremental%20Learning%20%28CIL%29%20trains%20a%20model%20to%20continually%20recognize%20new%0Aclasses%20from%20non-stationary%20data%20while%20retaining%20learned%20knowledge.%20A%20major%0Achallenge%20of%20CIL%20arises%20when%20applying%20to%20real-world%20data%20characterized%20by%0Anon-uniform%20distribution%2C%20which%20introduces%20a%20dual%20imbalance%20problem%20involving%0A%28i%29%20disparities%20between%20stored%20exemplars%20of%20old%20tasks%20and%20new%20class%20data%0A%28inter-phase%20imbalance%29%2C%20and%20%28ii%29%20severe%20class%20imbalances%20within%20each%0Aindividual%20task%20%28intra-phase%20imbalance%29.%20We%20show%20that%20this%20dual%20imbalance%20issue%0Acauses%20skewed%20gradient%20updates%20with%20biased%20weights%20in%20FC%20layers%2C%20thus%20inducing%0Aover/under-fitting%20and%20catastrophic%20forgetting%20in%20CIL.%20Our%20method%20addresses%20it%0Aby%20reweighting%20the%20gradients%20towards%20balanced%20optimization%20and%20unbiased%0Aclassifier%20learning.%20Additionally%2C%20we%20observe%20imbalanced%20forgetting%20where%0Aparadoxically%20the%20instance-rich%20classes%20suffer%20higher%20performance%20degradation%0Aduring%20CIL%20due%20to%20a%20larger%20amount%20of%20training%20data%20becoming%20unavailable%20in%0Asubsequent%20learning%20phases.%20To%20tackle%20this%2C%20we%20further%20introduce%20a%0Adistribution-aware%20knowledge%20distillation%20loss%20to%20mitigate%20forgetting%20by%0Aaligning%20output%20logits%20proportionally%20with%20the%20distribution%20of%20lost%20training%0Adata.%20We%20validate%20our%20method%20on%20CIFAR-100%2C%20ImageNetSubset%2C%20and%20Food101%20across%0Avarious%20evaluation%20protocols%20and%20demonstrate%20consistent%20improvements%20compared%0Ato%20existing%20works%2C%20showing%20great%20potential%20to%20apply%20CIL%20in%20real-world%20scenarios%0Awith%20enhanced%20robustness%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18528v1&entry.124074799=Read"},
{"title": "Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling", "author": "Mahdi Karami and Ali Ghodsi", "abstract": "  In the rapidly evolving landscape of deep learning, the quest for models that\nbalance expressivity with computational efficiency has never been more\ncritical. This paper introduces Orchid, a novel architecture that reimagines\nsequence modeling by incorporating a new data-dependent convolution mechanism.\nOrchid is designed to address the inherent limitations of traditional attention\nmechanisms, particularly their quadratic complexity, without compromising the\nability to capture long-range dependencies and in-context learning. At the core\nof Orchid lies the data-dependent convolution layer, which dynamically adjusts\nits kernel conditioned on input data using a dedicated conditioning neural\nnetwork. We design two simple conditioning networks that maintain shift\nequivariance in the adaptive convolution operation. The dynamic nature of\ndata-dependent convolution kernel, coupled with gating operations, grants\nOrchid high expressivity while maintaining efficiency and quasilinear\nscalability for long sequences. We rigorously evaluate Orchid across multiple\ndomains, including language modeling and image classification, to showcase its\nperformance and generality. Our experiments demonstrate that Orchid\narchitecture not only outperforms traditional attention-based architectures\nsuch as BERT and Vision Transformers with smaller model sizes, but also extends\nthe feasible sequence length beyond the limitations of the dense attention\nlayers. This achievement represents a significant step towards more efficient\nand scalable deep learning models for sequence modeling.\n", "link": "http://arxiv.org/abs/2402.18508v1", "date": "2024-02-28", "relevancy": 2.4568, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.511}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4837}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4794}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orchid%3A%20Flexible%20and%20Data-Dependent%20Convolution%20for%20Sequence%20Modeling&entry.906535625=Mahdi%20Karami%20and%20Ali%20Ghodsi&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20landscape%20of%20deep%20learning%2C%20the%20quest%20for%20models%20that%0Abalance%20expressivity%20with%20computational%20efficiency%20has%20never%20been%20more%0Acritical.%20This%20paper%20introduces%20Orchid%2C%20a%20novel%20architecture%20that%20reimagines%0Asequence%20modeling%20by%20incorporating%20a%20new%20data-dependent%20convolution%20mechanism.%0AOrchid%20is%20designed%20to%20address%20the%20inherent%20limitations%20of%20traditional%20attention%0Amechanisms%2C%20particularly%20their%20quadratic%20complexity%2C%20without%20compromising%20the%0Aability%20to%20capture%20long-range%20dependencies%20and%20in-context%20learning.%20At%20the%20core%0Aof%20Orchid%20lies%20the%20data-dependent%20convolution%20layer%2C%20which%20dynamically%20adjusts%0Aits%20kernel%20conditioned%20on%20input%20data%20using%20a%20dedicated%20conditioning%20neural%0Anetwork.%20We%20design%20two%20simple%20conditioning%20networks%20that%20maintain%20shift%0Aequivariance%20in%20the%20adaptive%20convolution%20operation.%20The%20dynamic%20nature%20of%0Adata-dependent%20convolution%20kernel%2C%20coupled%20with%20gating%20operations%2C%20grants%0AOrchid%20high%20expressivity%20while%20maintaining%20efficiency%20and%20quasilinear%0Ascalability%20for%20long%20sequences.%20We%20rigorously%20evaluate%20Orchid%20across%20multiple%0Adomains%2C%20including%20language%20modeling%20and%20image%20classification%2C%20to%20showcase%20its%0Aperformance%20and%20generality.%20Our%20experiments%20demonstrate%20that%20Orchid%0Aarchitecture%20not%20only%20outperforms%20traditional%20attention-based%20architectures%0Asuch%20as%20BERT%20and%20Vision%20Transformers%20with%20smaller%20model%20sizes%2C%20but%20also%20extends%0Athe%20feasible%20sequence%20length%20beyond%20the%20limitations%20of%20the%20dense%20attention%0Alayers.%20This%20achievement%20represents%20a%20significant%20step%20towards%20more%20efficient%0Aand%20scalable%20deep%20learning%20models%20for%20sequence%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18508v1&entry.124074799=Read"},
{"title": "Numerical Stability of DeepGOPlus Inference", "author": "In\u00e9s Gonzalez Pepe and Yohan Chatelain and Gregory Kiar and Tristan Glatard", "abstract": "  Convolutional neural networks (CNNs) are currently among the most widely-used\ndeep neural network (DNN) architectures available and achieve state-of-the-art\nperformance for many problems. Originally applied to computer vision tasks,\nCNNs work well with any data with a spatial relationship, besides images, and\nhave been applied to different fields. However, recent works have highlighted\nnumerical stability challenges in DNNs, which also relates to their known\nsensitivity to noise injection. These challenges can jeopardise their\nperformance and reliability. This paper investigates DeepGOPlus, a CNN that\npredicts protein function. DeepGOPlus has achieved state-of-the-art performance\nand can successfully take advantage and annotate the abounding protein\nsequences emerging in proteomics. We determine the numerical stability of the\nmodel's inference stage by quantifying the numerical uncertainty resulting from\nperturbations of the underlying floating-point data. In addition, we explore\nthe opportunity to use reduced-precision floating point formats for DeepGOPlus\ninference, to reduce memory consumption and latency. This is achieved by\ninstrumenting DeepGOPlus' execution using Monte Carlo Arithmetic, a technique\nthat experimentally quantifies floating point operation errors and VPREC, a\ntool that emulates results with customizable floating point precision formats.\nFocus is placed on the inference stage as it is the primary deliverable of the\nDeepGOPlus model, widely applicable across different environments. All in all,\nour results show that although the DeepGOPlus CNN is very stable numerically,\nit can only be selectively implemented with lower-precision floating-point\nformats. We conclude that predictions obtained from the pre-trained DeepGOPlus\nmodel are very reliable numerically, and use existing floating-point formats\nefficiently.\n", "link": "http://arxiv.org/abs/2212.06361v4", "date": "2024-02-28", "relevancy": 2.4356, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5335}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4775}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4504}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Numerical%20Stability%20of%20DeepGOPlus%20Inference&entry.906535625=In%C3%A9s%20Gonzalez%20Pepe%20and%20Yohan%20Chatelain%20and%20Gregory%20Kiar%20and%20Tristan%20Glatard&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20currently%20among%20the%20most%20widely-used%0Adeep%20neural%20network%20%28DNN%29%20architectures%20available%20and%20achieve%20state-of-the-art%0Aperformance%20for%20many%20problems.%20Originally%20applied%20to%20computer%20vision%20tasks%2C%0ACNNs%20work%20well%20with%20any%20data%20with%20a%20spatial%20relationship%2C%20besides%20images%2C%20and%0Ahave%20been%20applied%20to%20different%20fields.%20However%2C%20recent%20works%20have%20highlighted%0Anumerical%20stability%20challenges%20in%20DNNs%2C%20which%20also%20relates%20to%20their%20known%0Asensitivity%20to%20noise%20injection.%20These%20challenges%20can%20jeopardise%20their%0Aperformance%20and%20reliability.%20This%20paper%20investigates%20DeepGOPlus%2C%20a%20CNN%20that%0Apredicts%20protein%20function.%20DeepGOPlus%20has%20achieved%20state-of-the-art%20performance%0Aand%20can%20successfully%20take%20advantage%20and%20annotate%20the%20abounding%20protein%0Asequences%20emerging%20in%20proteomics.%20We%20determine%20the%20numerical%20stability%20of%20the%0Amodel%27s%20inference%20stage%20by%20quantifying%20the%20numerical%20uncertainty%20resulting%20from%0Aperturbations%20of%20the%20underlying%20floating-point%20data.%20In%20addition%2C%20we%20explore%0Athe%20opportunity%20to%20use%20reduced-precision%20floating%20point%20formats%20for%20DeepGOPlus%0Ainference%2C%20to%20reduce%20memory%20consumption%20and%20latency.%20This%20is%20achieved%20by%0Ainstrumenting%20DeepGOPlus%27%20execution%20using%20Monte%20Carlo%20Arithmetic%2C%20a%20technique%0Athat%20experimentally%20quantifies%20floating%20point%20operation%20errors%20and%20VPREC%2C%20a%0Atool%20that%20emulates%20results%20with%20customizable%20floating%20point%20precision%20formats.%0AFocus%20is%20placed%20on%20the%20inference%20stage%20as%20it%20is%20the%20primary%20deliverable%20of%20the%0ADeepGOPlus%20model%2C%20widely%20applicable%20across%20different%20environments.%20All%20in%20all%2C%0Aour%20results%20show%20that%20although%20the%20DeepGOPlus%20CNN%20is%20very%20stable%20numerically%2C%0Ait%20can%20only%20be%20selectively%20implemented%20with%20lower-precision%20floating-point%0Aformats.%20We%20conclude%20that%20predictions%20obtained%20from%20the%20pre-trained%20DeepGOPlus%0Amodel%20are%20very%20reliable%20numerically%2C%20and%20use%20existing%20floating-point%20formats%0Aefficiently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.06361v4&entry.124074799=Read"},
{"title": "Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep\n  Structures", "author": "Andrei Cozma and Landon Harris and Hairong Qi and Ping Ji and Wenpeng Guo and Song Yuan", "abstract": "  This paper introduces a robust approach for automated defect detection in\ntire X-ray images by harnessing traditional feature extraction methods such as\nLocal Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features,\nas well as Fourier and Wavelet-based features, complemented by advanced machine\nlearning techniques. Recognizing the challenges inherent in the complex\npatterns and textures of tire X-ray images, the study emphasizes the\nsignificance of feature engineering to enhance the performance of defect\ndetection systems. By meticulously integrating combinations of these features\nwith a Random Forest (RF) classifier and comparing them against advanced models\nlike YOLOv8, the research not only benchmarks the performance of traditional\nfeatures in defect detection but also explores the synergy between classical\nand modern approaches. The experimental results demonstrate that these\ntraditional features, when fine-tuned and combined with machine learning\nmodels, can significantly improve the accuracy and reliability of tire defect\ndetection, aiming to set a new standard in automated quality assurance in tire\nmanufacturing.\n", "link": "http://arxiv.org/abs/2402.18527v1", "date": "2024-02-28", "relevancy": 2.4337, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5134}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.475}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4718}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defect%20Detection%20in%20Tire%20X-Ray%20Images%3A%20Conventional%20Methods%20Meet%20Deep%0A%20%20Structures&entry.906535625=Andrei%20Cozma%20and%20Landon%20Harris%20and%20Hairong%20Qi%20and%20Ping%20Ji%20and%20Wenpeng%20Guo%20and%20Song%20Yuan&entry.1292438233=%20%20This%20paper%20introduces%20a%20robust%20approach%20for%20automated%20defect%20detection%20in%0Atire%20X-ray%20images%20by%20harnessing%20traditional%20feature%20extraction%20methods%20such%20as%0ALocal%20Binary%20Pattern%20%28LBP%29%20and%20Gray%20Level%20Co-Occurrence%20Matrix%20%28GLCM%29%20features%2C%0Aas%20well%20as%20Fourier%20and%20Wavelet-based%20features%2C%20complemented%20by%20advanced%20machine%0Alearning%20techniques.%20Recognizing%20the%20challenges%20inherent%20in%20the%20complex%0Apatterns%20and%20textures%20of%20tire%20X-ray%20images%2C%20the%20study%20emphasizes%20the%0Asignificance%20of%20feature%20engineering%20to%20enhance%20the%20performance%20of%20defect%0Adetection%20systems.%20By%20meticulously%20integrating%20combinations%20of%20these%20features%0Awith%20a%20Random%20Forest%20%28RF%29%20classifier%20and%20comparing%20them%20against%20advanced%20models%0Alike%20YOLOv8%2C%20the%20research%20not%20only%20benchmarks%20the%20performance%20of%20traditional%0Afeatures%20in%20defect%20detection%20but%20also%20explores%20the%20synergy%20between%20classical%0Aand%20modern%20approaches.%20The%20experimental%20results%20demonstrate%20that%20these%0Atraditional%20features%2C%20when%20fine-tuned%20and%20combined%20with%20machine%20learning%0Amodels%2C%20can%20significantly%20improve%20the%20accuracy%20and%20reliability%20of%20tire%20defect%0Adetection%2C%20aiming%20to%20set%20a%20new%20standard%20in%20automated%20quality%20assurance%20in%20tire%0Amanufacturing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18527v1&entry.124074799=Read"},
{"title": "Out-of-Domain Generalization in Dynamical Systems Reconstruction", "author": "Niclas G\u00f6ring and Florian Hess and Manuel Brenner and Zahra Monfared and Daniel Durstewitz", "abstract": "  In science we are interested in finding the governing equations, the\ndynamical rules, underlying empirical phenomena. While traditionally scientific\nmodels are derived through cycles of human insight and experimentation,\nrecently deep learning (DL) techniques have been advanced to reconstruct\ndynamical systems (DS) directly from time series data. State-of-the-art\ndynamical systems reconstruction (DSR) methods show promise in capturing\ninvariant and long-term properties of observed DS, but their ability to\ngeneralize to unobserved domains remains an open challenge. Yet, this is a\ncrucial property we would expect from any viable scientific theory. In this\nwork, we provide a formal framework that addresses generalization in DSR. We\nexplain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly\ndiffers from OODG considered elsewhere in machine learning. We introduce\nmathematical notions based on topological concepts and ergodic theory to\nformalize the idea of learnability of a DSR model. We formally prove that\nblack-box DL techniques, without adequate structural priors, generally will not\nbe able to learn a generalizing DSR model. We also show this empirically,\nconsidering major classes of DSR algorithms proposed so far, and illustrate\nwhere and why they fail to generalize across the whole phase space. Our study\nprovides the first comprehensive mathematical treatment of OODG in DSR, and\ngives a deeper conceptual understanding of where the fundamental problems in\nOODG lie and how they could possibly be addressed in practice.\n", "link": "http://arxiv.org/abs/2402.18377v1", "date": "2024-02-28", "relevancy": 2.4136, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5047}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4734}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4701}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-Domain%20Generalization%20in%20Dynamical%20Systems%20Reconstruction&entry.906535625=Niclas%20G%C3%B6ring%20and%20Florian%20Hess%20and%20Manuel%20Brenner%20and%20Zahra%20Monfared%20and%20Daniel%20Durstewitz&entry.1292438233=%20%20In%20science%20we%20are%20interested%20in%20finding%20the%20governing%20equations%2C%20the%0Adynamical%20rules%2C%20underlying%20empirical%20phenomena.%20While%20traditionally%20scientific%0Amodels%20are%20derived%20through%20cycles%20of%20human%20insight%20and%20experimentation%2C%0Arecently%20deep%20learning%20%28DL%29%20techniques%20have%20been%20advanced%20to%20reconstruct%0Adynamical%20systems%20%28DS%29%20directly%20from%20time%20series%20data.%20State-of-the-art%0Adynamical%20systems%20reconstruction%20%28DSR%29%20methods%20show%20promise%20in%20capturing%0Ainvariant%20and%20long-term%20properties%20of%20observed%20DS%2C%20but%20their%20ability%20to%0Ageneralize%20to%20unobserved%20domains%20remains%20an%20open%20challenge.%20Yet%2C%20this%20is%20a%0Acrucial%20property%20we%20would%20expect%20from%20any%20viable%20scientific%20theory.%20In%20this%0Awork%2C%20we%20provide%20a%20formal%20framework%20that%20addresses%20generalization%20in%20DSR.%20We%0Aexplain%20why%20and%20how%20out-of-domain%20%28OOD%29%20generalization%20%28OODG%29%20in%20DSR%20profoundly%0Adiffers%20from%20OODG%20considered%20elsewhere%20in%20machine%20learning.%20We%20introduce%0Amathematical%20notions%20based%20on%20topological%20concepts%20and%20ergodic%20theory%20to%0Aformalize%20the%20idea%20of%20learnability%20of%20a%20DSR%20model.%20We%20formally%20prove%20that%0Ablack-box%20DL%20techniques%2C%20without%20adequate%20structural%20priors%2C%20generally%20will%20not%0Abe%20able%20to%20learn%20a%20generalizing%20DSR%20model.%20We%20also%20show%20this%20empirically%2C%0Aconsidering%20major%20classes%20of%20DSR%20algorithms%20proposed%20so%20far%2C%20and%20illustrate%0Awhere%20and%20why%20they%20fail%20to%20generalize%20across%20the%20whole%20phase%20space.%20Our%20study%0Aprovides%20the%20first%20comprehensive%20mathematical%20treatment%20of%20OODG%20in%20DSR%2C%20and%0Agives%20a%20deeper%20conceptual%20understanding%20of%20where%20the%20fundamental%20problems%20in%0AOODG%20lie%20and%20how%20they%20could%20possibly%20be%20addressed%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18377v1&entry.124074799=Read"},
{"title": "Efficient local linearity regularization to overcome catastrophic\n  overfitting", "author": "Elias Abad Rocamora and Fanghui Liu and Grigorios G. Chrysos and Pablo M. Olmos and Volkan Cevher", "abstract": "  Catastrophic overfitting (CO) in single-step adversarial training (AT)\nresults in abrupt drops in the adversarial test accuracy (even down to 0%). For\nmodels trained with multi-step AT, it has been observed that the loss function\nbehaves locally linearly with respect to the input, this is however lost in\nsingle-step AT. To address CO in single-step AT, several methods have been\nproposed to enforce local linearity of the loss via regularization. However,\nthese regularization terms considerably slow down training due to Double\nBackpropagation. Instead, in this work, we introduce a regularization term,\ncalled ELLE, to mitigate CO effectively and efficiently in classical AT\nevaluations, as well as some more difficult regimes, e.g., large adversarial\nperturbations and long training schedules. Our regularization term can be\ntheoretically linked to curvature of the loss function and is computationally\ncheaper than previous methods by avoiding Double Backpropagation. Our thorough\nexperimental validation demonstrates that our work does not suffer from CO,\neven in challenging settings where previous works suffer from it. We also\nnotice that adapting our regularization parameter during training (ELLE-A)\ngreatly improves the performance, specially in large $\\epsilon$ setups. Our\nimplementation is available in https://github.com/LIONS-EPFL/ELLE .\n", "link": "http://arxiv.org/abs/2401.11618v2", "date": "2024-02-28", "relevancy": 2.3977, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4989}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4789}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4608}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20local%20linearity%20regularization%20to%20overcome%20catastrophic%0A%20%20overfitting&entry.906535625=Elias%20Abad%20Rocamora%20and%20Fanghui%20Liu%20and%20Grigorios%20G.%20Chrysos%20and%20Pablo%20M.%20Olmos%20and%20Volkan%20Cevher&entry.1292438233=%20%20Catastrophic%20overfitting%20%28CO%29%20in%20single-step%20adversarial%20training%20%28AT%29%0Aresults%20in%20abrupt%20drops%20in%20the%20adversarial%20test%20accuracy%20%28even%20down%20to%200%25%29.%20For%0Amodels%20trained%20with%20multi-step%20AT%2C%20it%20has%20been%20observed%20that%20the%20loss%20function%0Abehaves%20locally%20linearly%20with%20respect%20to%20the%20input%2C%20this%20is%20however%20lost%20in%0Asingle-step%20AT.%20To%20address%20CO%20in%20single-step%20AT%2C%20several%20methods%20have%20been%0Aproposed%20to%20enforce%20local%20linearity%20of%20the%20loss%20via%20regularization.%20However%2C%0Athese%20regularization%20terms%20considerably%20slow%20down%20training%20due%20to%20Double%0ABackpropagation.%20Instead%2C%20in%20this%20work%2C%20we%20introduce%20a%20regularization%20term%2C%0Acalled%20ELLE%2C%20to%20mitigate%20CO%20effectively%20and%20efficiently%20in%20classical%20AT%0Aevaluations%2C%20as%20well%20as%20some%20more%20difficult%20regimes%2C%20e.g.%2C%20large%20adversarial%0Aperturbations%20and%20long%20training%20schedules.%20Our%20regularization%20term%20can%20be%0Atheoretically%20linked%20to%20curvature%20of%20the%20loss%20function%20and%20is%20computationally%0Acheaper%20than%20previous%20methods%20by%20avoiding%20Double%20Backpropagation.%20Our%20thorough%0Aexperimental%20validation%20demonstrates%20that%20our%20work%20does%20not%20suffer%20from%20CO%2C%0Aeven%20in%20challenging%20settings%20where%20previous%20works%20suffer%20from%20it.%20We%20also%0Anotice%20that%20adapting%20our%20regularization%20parameter%20during%20training%20%28ELLE-A%29%0Agreatly%20improves%20the%20performance%2C%20specially%20in%20large%20%24%5Cepsilon%24%20setups.%20Our%0Aimplementation%20is%20available%20in%20https%3A//github.com/LIONS-EPFL/ELLE%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11618v2&entry.124074799=Read"},
{"title": "UniMODE: Unified Monocular 3D Object Detection", "author": "Zhuoling Li and Xiaogang Xu and SerNam Lim and Hengshuang Zhao", "abstract": "  Realizing unified monocular 3D object detection, including both indoor and\noutdoor scenes, holds great importance in applications like robot navigation.\nHowever, involving various scenarios of data to train models poses challenges\ndue to their significantly different characteristics, e.g., diverse geometry\nproperties and heterogeneous domain distributions. To address these challenges,\nwe build a detector based on the bird's-eye-view (BEV) detection paradigm,\nwhere the explicit feature projection is beneficial to addressing the geometry\nlearning ambiguity when employing multiple scenarios of data to train\ndetectors. Then, we split the classical BEV detection architecture into two\nstages and propose an uneven BEV grid design to handle the convergence\ninstability caused by the aforementioned challenges. Moreover, we develop a\nsparse BEV feature projection strategy to reduce computational cost and a\nunified domain alignment method to handle heterogeneous domains. Combining\nthese techniques, a unified detector UniMODE is derived, which surpasses the\nprevious state-of-the-art on the challenging Omni3D dataset (a large-scale\ndataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the\nfirst successful generalization of a BEV detector to unified 3D object\ndetection.\n", "link": "http://arxiv.org/abs/2402.18573v1", "date": "2024-02-28", "relevancy": 2.3853, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6142}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6112}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5725}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMODE%3A%20Unified%20Monocular%203D%20Object%20Detection&entry.906535625=Zhuoling%20Li%20and%20Xiaogang%20Xu%20and%20SerNam%20Lim%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Realizing%20unified%20monocular%203D%20object%20detection%2C%20including%20both%20indoor%20and%0Aoutdoor%20scenes%2C%20holds%20great%20importance%20in%20applications%20like%20robot%20navigation.%0AHowever%2C%20involving%20various%20scenarios%20of%20data%20to%20train%20models%20poses%20challenges%0Adue%20to%20their%20significantly%20different%20characteristics%2C%20e.g.%2C%20diverse%20geometry%0Aproperties%20and%20heterogeneous%20domain%20distributions.%20To%20address%20these%20challenges%2C%0Awe%20build%20a%20detector%20based%20on%20the%20bird%27s-eye-view%20%28BEV%29%20detection%20paradigm%2C%0Awhere%20the%20explicit%20feature%20projection%20is%20beneficial%20to%20addressing%20the%20geometry%0Alearning%20ambiguity%20when%20employing%20multiple%20scenarios%20of%20data%20to%20train%0Adetectors.%20Then%2C%20we%20split%20the%20classical%20BEV%20detection%20architecture%20into%20two%0Astages%20and%20propose%20an%20uneven%20BEV%20grid%20design%20to%20handle%20the%20convergence%0Ainstability%20caused%20by%20the%20aforementioned%20challenges.%20Moreover%2C%20we%20develop%20a%0Asparse%20BEV%20feature%20projection%20strategy%20to%20reduce%20computational%20cost%20and%20a%0Aunified%20domain%20alignment%20method%20to%20handle%20heterogeneous%20domains.%20Combining%0Athese%20techniques%2C%20a%20unified%20detector%20UniMODE%20is%20derived%2C%20which%20surpasses%20the%0Aprevious%20state-of-the-art%20on%20the%20challenging%20Omni3D%20dataset%20%28a%20large-scale%0Adataset%20including%20both%20indoor%20and%20outdoor%20scenes%29%20by%204.9%25%20AP_3D%2C%20revealing%20the%0Afirst%20successful%20generalization%20of%20a%20BEV%20detector%20to%20unified%203D%20object%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18573v1&entry.124074799=Read"},
{"title": "Learning the References of Online Model Predictive Control for Urban\n  Self-Driving", "author": "Yubin Wang and Zengqi Peng and Yusen Xie and Yulin Li and Hakim Ghazzai and Jun Ma", "abstract": "  In this work, we propose a novel learning-based model predictive control\n(MPC) framework for motion planning and control of urban self-driving. In this\nframework, instantaneous references and cost functions of online MPC are\nlearned from raw sensor data without relying on any oracle or predicted states\nof traffic. Moreover, driving safety conditions are latently encoded via the\nintroduction of a learnable instantaneous reference vector. In particular, we\nimplement a deep reinforcement learning (DRL) framework for policy search,\nwhere practical and lightweight raw observations are processed to reason about\nthe traffic and provide the online MPC with instantaneous references. The\nproposed approach is validated in a high-fidelity simulator, where our\ndevelopment manifests remarkable adaptiveness to complex and dynamic traffic.\nFurthermore, sim-to-real deployments are also conducted to evaluate the\ngeneralizability of the proposed framework in various real-world applications.\nAlso, we provide the open-source code and video demonstrations at the project\nwebsite: https://latent-mpc.github.io/.\n", "link": "http://arxiv.org/abs/2308.15808v2", "date": "2024-02-28", "relevancy": 2.3051, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6247}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.567}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5662}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20References%20of%20Online%20Model%20Predictive%20Control%20for%20Urban%0A%20%20Self-Driving&entry.906535625=Yubin%20Wang%20and%20Zengqi%20Peng%20and%20Yusen%20Xie%20and%20Yulin%20Li%20and%20Hakim%20Ghazzai%20and%20Jun%20Ma&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20learning-based%20model%20predictive%20control%0A%28MPC%29%20framework%20for%20motion%20planning%20and%20control%20of%20urban%20self-driving.%20In%20this%0Aframework%2C%20instantaneous%20references%20and%20cost%20functions%20of%20online%20MPC%20are%0Alearned%20from%20raw%20sensor%20data%20without%20relying%20on%20any%20oracle%20or%20predicted%20states%0Aof%20traffic.%20Moreover%2C%20driving%20safety%20conditions%20are%20latently%20encoded%20via%20the%0Aintroduction%20of%20a%20learnable%20instantaneous%20reference%20vector.%20In%20particular%2C%20we%0Aimplement%20a%20deep%20reinforcement%20learning%20%28DRL%29%20framework%20for%20policy%20search%2C%0Awhere%20practical%20and%20lightweight%20raw%20observations%20are%20processed%20to%20reason%20about%0Athe%20traffic%20and%20provide%20the%20online%20MPC%20with%20instantaneous%20references.%20The%0Aproposed%20approach%20is%20validated%20in%20a%20high-fidelity%20simulator%2C%20where%20our%0Adevelopment%20manifests%20remarkable%20adaptiveness%20to%20complex%20and%20dynamic%20traffic.%0AFurthermore%2C%20sim-to-real%20deployments%20are%20also%20conducted%20to%20evaluate%20the%0Ageneralizability%20of%20the%20proposed%20framework%20in%20various%20real-world%20applications.%0AAlso%2C%20we%20provide%20the%20open-source%20code%20and%20video%20demonstrations%20at%20the%20project%0Awebsite%3A%20https%3A//latent-mpc.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.15808v2&entry.124074799=Read"},
{"title": "HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement\n  Learning Framework for Complex Environments", "author": "Yingru Li and Jiawei Xu and Lei Han and Zhi-Quan Luo", "abstract": "  To solve complex tasks under resource constraints, reinforcement learning\n(RL) agents need to be simple, efficient, and scalable, addressing (1) large\nstate spaces and (2) the continuous accumulation of interaction data. We\npropose HyperAgent, an RL framework featuring the hypermodel and index sampling\nschemes that enable computation-efficient incremental approximation for the\nposteriors associated with general value functions without the need for\nconjugacy, and data-efficient action selection. Implementing HyperAgent is\nstraightforward, requiring only one additional module beyond what is necessary\nfor Double-DQN. HyperAgent stands out as the first method to offer robust\nperformance in large-scale deep RL benchmarks while achieving provably scalable\nper-step computational complexity and attaining sublinear regret under tabular\nassumptions. HyperAgent can solve Deep Sea hard exploration problems with\nepisodes that optimally scale with problem size and exhibits significant\nefficiency gains in both data and computation under the Atari benchmark. The\ncore of our theoretical analysis is the sequential posterior approximation\nargument, enabled by the first analytical tool for sequential random projection\n-- a non-trivial martingale extension of the Johnson-Lindenstrauss. This work\nbridges the theoretical and practical realms of RL, establishing a new\nbenchmark for RL algorithm design.\n", "link": "http://arxiv.org/abs/2402.10228v2", "date": "2024-02-28", "relevancy": 2.2908, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5956}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.557}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5547}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperAgent%3A%20A%20Simple%2C%20Scalable%2C%20Efficient%20and%20Provable%20Reinforcement%0A%20%20Learning%20Framework%20for%20Complex%20Environments&entry.906535625=Yingru%20Li%20and%20Jiawei%20Xu%20and%20Lei%20Han%20and%20Zhi-Quan%20Luo&entry.1292438233=%20%20To%20solve%20complex%20tasks%20under%20resource%20constraints%2C%20reinforcement%20learning%0A%28RL%29%20agents%20need%20to%20be%20simple%2C%20efficient%2C%20and%20scalable%2C%20addressing%20%281%29%20large%0Astate%20spaces%20and%20%282%29%20the%20continuous%20accumulation%20of%20interaction%20data.%20We%0Apropose%20HyperAgent%2C%20an%20RL%20framework%20featuring%20the%20hypermodel%20and%20index%20sampling%0Aschemes%20that%20enable%20computation-efficient%20incremental%20approximation%20for%20the%0Aposteriors%20associated%20with%20general%20value%20functions%20without%20the%20need%20for%0Aconjugacy%2C%20and%20data-efficient%20action%20selection.%20Implementing%20HyperAgent%20is%0Astraightforward%2C%20requiring%20only%20one%20additional%20module%20beyond%20what%20is%20necessary%0Afor%20Double-DQN.%20HyperAgent%20stands%20out%20as%20the%20first%20method%20to%20offer%20robust%0Aperformance%20in%20large-scale%20deep%20RL%20benchmarks%20while%20achieving%20provably%20scalable%0Aper-step%20computational%20complexity%20and%20attaining%20sublinear%20regret%20under%20tabular%0Aassumptions.%20HyperAgent%20can%20solve%20Deep%20Sea%20hard%20exploration%20problems%20with%0Aepisodes%20that%20optimally%20scale%20with%20problem%20size%20and%20exhibits%20significant%0Aefficiency%20gains%20in%20both%20data%20and%20computation%20under%20the%20Atari%20benchmark.%20The%0Acore%20of%20our%20theoretical%20analysis%20is%20the%20sequential%20posterior%20approximation%0Aargument%2C%20enabled%20by%20the%20first%20analytical%20tool%20for%20sequential%20random%20projection%0A--%20a%20non-trivial%20martingale%20extension%20of%20the%20Johnson-Lindenstrauss.%20This%20work%0Abridges%20the%20theoretical%20and%20practical%20realms%20of%20RL%2C%20establishing%20a%20new%0Abenchmark%20for%20RL%20algorithm%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10228v2&entry.124074799=Read"},
{"title": "Dual-IMU State Estimation for Relative Localization of Two Mobile Agents", "author": "Wenqian Lai and Ruonan Guo and Kejian J. Wu", "abstract": "  In this paper, we address the problem of relative localization of two mobile\nagents. Specifically, we consider the Dual-IMU system, where each agent is\nequipped with one IMU, and employs relative pose observations between them.\nPrevious works, however, typically assumed known ego motion and ignored biases\nof the IMUs. Instead, we study the most general case of unknown biases for both\nIMUs. Besides the derivation of dynamic model equations of the proposed system,\nwe focus on the observability analysis, for the observability under general\nmotion and the unobservable directions arising from various special motions.\nThrough numerical simulations, we validate our key observability findings and\nexamine their impact on the estimation accuracy and consistency. Finally, the\nsystem is implemented to achieve effective relative localization of an HMD with\nrespect to a vehicle moving in the real world.\n", "link": "http://arxiv.org/abs/2402.18394v1", "date": "2024-02-28", "relevancy": 2.272, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5703}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5681}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.567}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-IMU%20State%20Estimation%20for%20Relative%20Localization%20of%20Two%20Mobile%20Agents&entry.906535625=Wenqian%20Lai%20and%20Ruonan%20Guo%20and%20Kejian%20J.%20Wu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20relative%20localization%20of%20two%20mobile%0Aagents.%20Specifically%2C%20we%20consider%20the%20Dual-IMU%20system%2C%20where%20each%20agent%20is%0Aequipped%20with%20one%20IMU%2C%20and%20employs%20relative%20pose%20observations%20between%20them.%0APrevious%20works%2C%20however%2C%20typically%20assumed%20known%20ego%20motion%20and%20ignored%20biases%0Aof%20the%20IMUs.%20Instead%2C%20we%20study%20the%20most%20general%20case%20of%20unknown%20biases%20for%20both%0AIMUs.%20Besides%20the%20derivation%20of%20dynamic%20model%20equations%20of%20the%20proposed%20system%2C%0Awe%20focus%20on%20the%20observability%20analysis%2C%20for%20the%20observability%20under%20general%0Amotion%20and%20the%20unobservable%20directions%20arising%20from%20various%20special%20motions.%0AThrough%20numerical%20simulations%2C%20we%20validate%20our%20key%20observability%20findings%20and%0Aexamine%20their%20impact%20on%20the%20estimation%20accuracy%20and%20consistency.%20Finally%2C%20the%0Asystem%20is%20implemented%20to%20achieve%20effective%20relative%20localization%20of%20an%20HMD%20with%0Arespect%20to%20a%20vehicle%20moving%20in%20the%20real%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18394v1&entry.124074799=Read"},
{"title": "KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose\n  Estimation", "author": "Ivano Donadi and Alberto Pretto", "abstract": "  Object pose estimation is a fundamental computer vision task exploited in\nseveral robotics and augmented reality applications. Many established\napproaches rely on predicting 2D-3D keypoint correspondences using RANSAC\n(Random sample consensus) and estimating the object pose using the PnP\n(Perspective-n-Point) algorithm. Being RANSAC non-differentiable,\ncorrespondences cannot be directly learned in an end-to-end fashion. In this\npaper, we address the stereo image-based object pose estimation problem by i)\nintroducing a differentiable RANSAC layer into a well-known monocular pose\nestimation network; ii) exploiting an uncertainty-driven multi-view PnP solver\nwhich can fuse information from multiple views. We evaluate our approach on a\nchallenging public stereo object pose estimation dataset and a custom-built\ndataset we call Transparent Tableware Dataset (TTD), yielding state-of-the-art\nresults against other recent approaches. Furthermore, in our ablation study, we\nshow that the differentiable RANSAC layer plays a significant role in the\naccuracy of the proposed method. We release with this paper the code of our\nmethod and the TTD dataset.\n", "link": "http://arxiv.org/abs/2307.11543v2", "date": "2024-02-28", "relevancy": 2.2598, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5832}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.558}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.537}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KVN%3A%20Keypoints%20Voting%20Network%20with%20Differentiable%20RANSAC%20for%20Stereo%20Pose%0A%20%20Estimation&entry.906535625=Ivano%20Donadi%20and%20Alberto%20Pretto&entry.1292438233=%20%20Object%20pose%20estimation%20is%20a%20fundamental%20computer%20vision%20task%20exploited%20in%0Aseveral%20robotics%20and%20augmented%20reality%20applications.%20Many%20established%0Aapproaches%20rely%20on%20predicting%202D-3D%20keypoint%20correspondences%20using%20RANSAC%0A%28Random%20sample%20consensus%29%20and%20estimating%20the%20object%20pose%20using%20the%20PnP%0A%28Perspective-n-Point%29%20algorithm.%20Being%20RANSAC%20non-differentiable%2C%0Acorrespondences%20cannot%20be%20directly%20learned%20in%20an%20end-to-end%20fashion.%20In%20this%0Apaper%2C%20we%20address%20the%20stereo%20image-based%20object%20pose%20estimation%20problem%20by%20i%29%0Aintroducing%20a%20differentiable%20RANSAC%20layer%20into%20a%20well-known%20monocular%20pose%0Aestimation%20network%3B%20ii%29%20exploiting%20an%20uncertainty-driven%20multi-view%20PnP%20solver%0Awhich%20can%20fuse%20information%20from%20multiple%20views.%20We%20evaluate%20our%20approach%20on%20a%0Achallenging%20public%20stereo%20object%20pose%20estimation%20dataset%20and%20a%20custom-built%0Adataset%20we%20call%20Transparent%20Tableware%20Dataset%20%28TTD%29%2C%20yielding%20state-of-the-art%0Aresults%20against%20other%20recent%20approaches.%20Furthermore%2C%20in%20our%20ablation%20study%2C%20we%0Ashow%20that%20the%20differentiable%20RANSAC%20layer%20plays%20a%20significant%20role%20in%20the%0Aaccuracy%20of%20the%20proposed%20method.%20We%20release%20with%20this%20paper%20the%20code%20of%20our%0Amethod%20and%20the%20TTD%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.11543v2&entry.124074799=Read"},
{"title": "Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust\n  3D Object Detection", "author": "Xun Huang and Hai Wu and Xin Li and Xiaoliang Fan and Chenglu Wen and Cheng Wang", "abstract": "  LiDAR-based 3D object detection models have traditionally struggled under\nrainy conditions due to the degraded and noisy scanning signals. Previous\nresearch has attempted to address this by simulating the noise from rain to\nimprove the robustness of detection models. However, significant disparities\nexist between simulated and actual rain-impacted data points. In this work, we\npropose a novel rain simulation method, termed DRET, that unifies Dynamics and\nRainy Environment Theory to provide a cost-effective means of expanding the\navailable realistic rain data for 3D detection training. Furthermore, we\npresent a Sunny-to-Rainy Knowledge Distillation (SRKD) approach to enhance 3D\ndetection under rainy conditions. Extensive experiments on the WaymoOpenDataset\nlarge-scale dataset show that, when combined with the state-of-the-art DSVT\nmodel and other classical 3D detectors, our proposed framework demonstrates\nsignificant detection accuracy improvements, without losing efficiency.\nRemarkably, our framework also improves detection capabilities under sunny\nconditions, therefore offering a robust solution for 3D detection regardless of\nwhether the weather is rainy or sunny\n", "link": "http://arxiv.org/abs/2402.18493v1", "date": "2024-02-28", "relevancy": 2.1933, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5611}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5574}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5319}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sunshine%20to%20Rainstorm%3A%20Cross-Weather%20Knowledge%20Distillation%20for%20Robust%0A%20%203D%20Object%20Detection&entry.906535625=Xun%20Huang%20and%20Hai%20Wu%20and%20Xin%20Li%20and%20Xiaoliang%20Fan%20and%20Chenglu%20Wen%20and%20Cheng%20Wang&entry.1292438233=%20%20LiDAR-based%203D%20object%20detection%20models%20have%20traditionally%20struggled%20under%0Arainy%20conditions%20due%20to%20the%20degraded%20and%20noisy%20scanning%20signals.%20Previous%0Aresearch%20has%20attempted%20to%20address%20this%20by%20simulating%20the%20noise%20from%20rain%20to%0Aimprove%20the%20robustness%20of%20detection%20models.%20However%2C%20significant%20disparities%0Aexist%20between%20simulated%20and%20actual%20rain-impacted%20data%20points.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20rain%20simulation%20method%2C%20termed%20DRET%2C%20that%20unifies%20Dynamics%20and%0ARainy%20Environment%20Theory%20to%20provide%20a%20cost-effective%20means%20of%20expanding%20the%0Aavailable%20realistic%20rain%20data%20for%203D%20detection%20training.%20Furthermore%2C%20we%0Apresent%20a%20Sunny-to-Rainy%20Knowledge%20Distillation%20%28SRKD%29%20approach%20to%20enhance%203D%0Adetection%20under%20rainy%20conditions.%20Extensive%20experiments%20on%20the%20WaymoOpenDataset%0Alarge-scale%20dataset%20show%20that%2C%20when%20combined%20with%20the%20state-of-the-art%20DSVT%0Amodel%20and%20other%20classical%203D%20detectors%2C%20our%20proposed%20framework%20demonstrates%0Asignificant%20detection%20accuracy%20improvements%2C%20without%20losing%20efficiency.%0ARemarkably%2C%20our%20framework%20also%20improves%20detection%20capabilities%20under%20sunny%0Aconditions%2C%20therefore%20offering%20a%20robust%20solution%20for%203D%20detection%20regardless%20of%0Awhether%20the%20weather%20is%20rainy%20or%20sunny%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18493v1&entry.124074799=Read"},
{"title": "Solving Multi-Entity Robotic Problems Using Permutation Invariant Neural\n  Networks", "author": "Tianxu An and Joonho Lee and Marko Bjelonic and Flavio De Vincenti and Marco Hutter", "abstract": "  Challenges in real-world robotic applications often stem from managing\nmultiple, dynamically varying entities such as neighboring robots, manipulable\nobjects, and navigation goals. Existing multi-agent control strategies face\nscalability limitations, struggling to handle arbitrary numbers of entities.\nAdditionally, they often rely on engineered heuristics for assigning entities\namong agents. We propose a data driven approach to address these limitations by\nintroducing a decentralized control system using neural network policies\ntrained in simulation. Leveraging permutation invariant neural network\narchitectures and model-free reinforcement learning, our approach allows\ncontrol agents to autonomously determine the relative importance of different\nentities without being biased by ordering or limited by a fixed capacity. We\nvalidate our approach through both simulations and real-world experiments\ninvolving multiple wheeled-legged quadrupedal robots, demonstrating their\ncollaborative control capabilities. We prove the effectiveness of our\narchitectural choice through experiments with three exemplary multi-entity\nproblems. Our analysis underscores the pivotal role of the end-to-end trained\npermutation invariant encoders in achieving scalability and improving the task\nperformance in multi-object manipulation or multi-goal navigation problems. The\nadaptability of our policy is further evidenced by its ability to manage\nvarying numbers of entities in a zero-shot manner, showcasing near-optimal\nautonomous task distribution and collision avoidance behaviors.\n", "link": "http://arxiv.org/abs/2402.18345v1", "date": "2024-02-28", "relevancy": 2.1932, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6019}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5362}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Multi-Entity%20Robotic%20Problems%20Using%20Permutation%20Invariant%20Neural%0A%20%20Networks&entry.906535625=Tianxu%20An%20and%20Joonho%20Lee%20and%20Marko%20Bjelonic%20and%20Flavio%20De%20Vincenti%20and%20Marco%20Hutter&entry.1292438233=%20%20Challenges%20in%20real-world%20robotic%20applications%20often%20stem%20from%20managing%0Amultiple%2C%20dynamically%20varying%20entities%20such%20as%20neighboring%20robots%2C%20manipulable%0Aobjects%2C%20and%20navigation%20goals.%20Existing%20multi-agent%20control%20strategies%20face%0Ascalability%20limitations%2C%20struggling%20to%20handle%20arbitrary%20numbers%20of%20entities.%0AAdditionally%2C%20they%20often%20rely%20on%20engineered%20heuristics%20for%20assigning%20entities%0Aamong%20agents.%20We%20propose%20a%20data%20driven%20approach%20to%20address%20these%20limitations%20by%0Aintroducing%20a%20decentralized%20control%20system%20using%20neural%20network%20policies%0Atrained%20in%20simulation.%20Leveraging%20permutation%20invariant%20neural%20network%0Aarchitectures%20and%20model-free%20reinforcement%20learning%2C%20our%20approach%20allows%0Acontrol%20agents%20to%20autonomously%20determine%20the%20relative%20importance%20of%20different%0Aentities%20without%20being%20biased%20by%20ordering%20or%20limited%20by%20a%20fixed%20capacity.%20We%0Avalidate%20our%20approach%20through%20both%20simulations%20and%20real-world%20experiments%0Ainvolving%20multiple%20wheeled-legged%20quadrupedal%20robots%2C%20demonstrating%20their%0Acollaborative%20control%20capabilities.%20We%20prove%20the%20effectiveness%20of%20our%0Aarchitectural%20choice%20through%20experiments%20with%20three%20exemplary%20multi-entity%0Aproblems.%20Our%20analysis%20underscores%20the%20pivotal%20role%20of%20the%20end-to-end%20trained%0Apermutation%20invariant%20encoders%20in%20achieving%20scalability%20and%20improving%20the%20task%0Aperformance%20in%20multi-object%20manipulation%20or%20multi-goal%20navigation%20problems.%20The%0Aadaptability%20of%20our%20policy%20is%20further%20evidenced%20by%20its%20ability%20to%20manage%0Avarying%20numbers%20of%20entities%20in%20a%20zero-shot%20manner%2C%20showcasing%20near-optimal%0Aautonomous%20task%20distribution%20and%20collision%20avoidance%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18345v1&entry.124074799=Read"},
{"title": "Leveraging Compliant Tactile Perception for Haptic Blind Surface\n  Reconstruction", "author": "Laurent Yves Emile Ramos Cheret and Vinicius Prado da Fonseca and Thiago Eustaquio Alves de Oliveira", "abstract": "  Non-flat surfaces pose difficulties for robots operating in unstructured\nenvironments. Reconstructions of uneven surfaces may only be partially possible\ndue to non-compliant end-effectors and limitations on vision systems such as\ntransparency, reflections, and occlusions. This study achieves blind surface\nreconstruction by harnessing the robotic manipulator's kinematic data and a\ncompliant tactile sensing module, which incorporates inertial, magnetic, and\npressure sensors. The module's flexibility enables us to estimate contact\npositions and surface normals by analyzing its deformation during interactions\nwith unknown objects. While previous works collect only positional information,\nwe include the local normals in a geometrical approach to estimate curvatures\nbetween adjacent contact points. These parameters then guide a spline-based\npatch generation, which allows us to recreate larger surfaces without an\nincrease in complexity while reducing the time-consuming step of probing the\nsurface. Experimental validation demonstrates that this approach outperforms an\noff-the-shelf vision system in estimation accuracy. Moreover, this compliant\nhaptic method works effectively even when the manipulator's approach angle is\nnot aligned with the surface normals, which is ideal for unknown non-flat\nsurfaces.\n", "link": "http://arxiv.org/abs/2402.18511v1", "date": "2024-02-28", "relevancy": 2.151, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5556}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5523}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5161}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Compliant%20Tactile%20Perception%20for%20Haptic%20Blind%20Surface%0A%20%20Reconstruction&entry.906535625=Laurent%20Yves%20Emile%20Ramos%20Cheret%20and%20Vinicius%20Prado%20da%20Fonseca%20and%20Thiago%20Eustaquio%20Alves%20de%20Oliveira&entry.1292438233=%20%20Non-flat%20surfaces%20pose%20difficulties%20for%20robots%20operating%20in%20unstructured%0Aenvironments.%20Reconstructions%20of%20uneven%20surfaces%20may%20only%20be%20partially%20possible%0Adue%20to%20non-compliant%20end-effectors%20and%20limitations%20on%20vision%20systems%20such%20as%0Atransparency%2C%20reflections%2C%20and%20occlusions.%20This%20study%20achieves%20blind%20surface%0Areconstruction%20by%20harnessing%20the%20robotic%20manipulator%27s%20kinematic%20data%20and%20a%0Acompliant%20tactile%20sensing%20module%2C%20which%20incorporates%20inertial%2C%20magnetic%2C%20and%0Apressure%20sensors.%20The%20module%27s%20flexibility%20enables%20us%20to%20estimate%20contact%0Apositions%20and%20surface%20normals%20by%20analyzing%20its%20deformation%20during%20interactions%0Awith%20unknown%20objects.%20While%20previous%20works%20collect%20only%20positional%20information%2C%0Awe%20include%20the%20local%20normals%20in%20a%20geometrical%20approach%20to%20estimate%20curvatures%0Abetween%20adjacent%20contact%20points.%20These%20parameters%20then%20guide%20a%20spline-based%0Apatch%20generation%2C%20which%20allows%20us%20to%20recreate%20larger%20surfaces%20without%20an%0Aincrease%20in%20complexity%20while%20reducing%20the%20time-consuming%20step%20of%20probing%20the%0Asurface.%20Experimental%20validation%20demonstrates%20that%20this%20approach%20outperforms%20an%0Aoff-the-shelf%20vision%20system%20in%20estimation%20accuracy.%20Moreover%2C%20this%20compliant%0Ahaptic%20method%20works%20effectively%20even%20when%20the%20manipulator%27s%20approach%20angle%20is%0Anot%20aligned%20with%20the%20surface%20normals%2C%20which%20is%20ideal%20for%20unknown%20non-flat%0Asurfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18511v1&entry.124074799=Read"},
{"title": "Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image\n  Labeling", "author": "Dongping Zhang and Angelos Chatzimparmpas and Negar Kamali and Jessica Hullman", "abstract": "  As deep neural networks are more commonly deployed in high-stakes domains,\ntheir black-box nature makes uncertainty quantification challenging. We\ninvestigate the effects of presenting conformal prediction sets -- a\ndistribution-free class of methods for generating prediction sets with\nspecified coverage -- to express uncertainty in AI-advised decision-making.\nThrough a large online experiment, we compare the utility of conformal\nprediction sets to displays of Top-$1$ and Top-$k$ predictions for AI-advised\nimage labeling. In a pre-registered analysis, we find that the utility of\nprediction sets for accuracy varies with the difficulty of the task: while they\nresult in accuracy on par with or less than Top-$1$ and Top-$k$ displays for\neasy images, prediction sets excel at assisting humans in labeling\nout-of-distribution (OOD) images, especially when the set size is small. Our\nresults empirically pinpoint practical challenges of conformal prediction sets\nand provide implications on how to incorporate them for real-world\ndecision-making.\n", "link": "http://arxiv.org/abs/2401.08876v4", "date": "2024-02-28", "relevancy": 2.1508, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5659}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5331}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5311}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Utility%20of%20Conformal%20Prediction%20Sets%20for%20AI-Advised%20Image%0A%20%20Labeling&entry.906535625=Dongping%20Zhang%20and%20Angelos%20Chatzimparmpas%20and%20Negar%20Kamali%20and%20Jessica%20Hullman&entry.1292438233=%20%20As%20deep%20neural%20networks%20are%20more%20commonly%20deployed%20in%20high-stakes%20domains%2C%0Atheir%20black-box%20nature%20makes%20uncertainty%20quantification%20challenging.%20We%0Ainvestigate%20the%20effects%20of%20presenting%20conformal%20prediction%20sets%20--%20a%0Adistribution-free%20class%20of%20methods%20for%20generating%20prediction%20sets%20with%0Aspecified%20coverage%20--%20to%20express%20uncertainty%20in%20AI-advised%20decision-making.%0AThrough%20a%20large%20online%20experiment%2C%20we%20compare%20the%20utility%20of%20conformal%0Aprediction%20sets%20to%20displays%20of%20Top-%241%24%20and%20Top-%24k%24%20predictions%20for%20AI-advised%0Aimage%20labeling.%20In%20a%20pre-registered%20analysis%2C%20we%20find%20that%20the%20utility%20of%0Aprediction%20sets%20for%20accuracy%20varies%20with%20the%20difficulty%20of%20the%20task%3A%20while%20they%0Aresult%20in%20accuracy%20on%20par%20with%20or%20less%20than%20Top-%241%24%20and%20Top-%24k%24%20displays%20for%0Aeasy%20images%2C%20prediction%20sets%20excel%20at%20assisting%20humans%20in%20labeling%0Aout-of-distribution%20%28OOD%29%20images%2C%20especially%20when%20the%20set%20size%20is%20small.%20Our%0Aresults%20empirically%20pinpoint%20practical%20challenges%20of%20conformal%20prediction%20sets%0Aand%20provide%20implications%20on%20how%20to%20incorporate%20them%20for%20real-world%0Adecision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08876v4&entry.124074799=Read"},
{"title": "ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype\n  Learning", "author": "Qin Zhang and Xiaowei Li and Jiexin Lu and Liping Qiu and Shirui Pan and Xiaojun Chen and Junyang Chen", "abstract": "  Open-set graph learning is a practical task that aims to classify the known\nclass nodes and to identify unknown class samples as unknowns. Conventional\nnode classification methods usually perform unsatisfactorily in open-set\nscenarios due to the complex data they encounter, such as out-of-distribution\n(OOD) data and in-distribution (IND) noise. OOD data are samples that do not\nbelong to any known classes. They are outliers if they occur in training (OOD\nnoise), and open-set samples if they occur in testing. IND noise are training\nsamples which are assigned incorrect labels. The existence of IND noise and OOD\nnoise is prevalent, which usually cause the ambiguity problem, including the\nintra-class variety problem and the inter-class confusion problem. Thus, to\nexplore robust open-set learning methods is necessary and difficult, and it\nbecomes even more difficult for non-IID graph data.To this end, we propose a\nunified framework named ROG$_{PL}$ to achieve robust open-set learning on\ncomplex noisy graph data, by introducing prototype learning. In specific,\nROG$_{PL}$ consists of two modules, i.e., denoising via label propagation and\nopen-set prototype learning via regions. The first module corrects noisy labels\nthrough similarity-based label propagation and removes low-confidence samples,\nto solve the intra-class variety problem caused by noise. The second module\nlearns open-set prototypes for each known class via non-overlapped regions and\nremains both interior and border prototypes to remedy the inter-class confusion\nproblem.The two modules are iteratively updated under the constraints of\nclassification loss and prototype diversity loss. To the best of our knowledge,\nthe proposed ROG$_{PL}$ is the first robust open-set node classification method\nfor graph data with complex noise.\n", "link": "http://arxiv.org/abs/2402.18495v1", "date": "2024-02-28", "relevancy": 2.1364, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5433}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5428}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5217}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROG%24_%7BPL%7D%24%3A%20Robust%20Open-Set%20Graph%20Learning%20via%20Region-Based%20Prototype%0A%20%20Learning&entry.906535625=Qin%20Zhang%20and%20Xiaowei%20Li%20and%20Jiexin%20Lu%20and%20Liping%20Qiu%20and%20Shirui%20Pan%20and%20Xiaojun%20Chen%20and%20Junyang%20Chen&entry.1292438233=%20%20Open-set%20graph%20learning%20is%20a%20practical%20task%20that%20aims%20to%20classify%20the%20known%0Aclass%20nodes%20and%20to%20identify%20unknown%20class%20samples%20as%20unknowns.%20Conventional%0Anode%20classification%20methods%20usually%20perform%20unsatisfactorily%20in%20open-set%0Ascenarios%20due%20to%20the%20complex%20data%20they%20encounter%2C%20such%20as%20out-of-distribution%0A%28OOD%29%20data%20and%20in-distribution%20%28IND%29%20noise.%20OOD%20data%20are%20samples%20that%20do%20not%0Abelong%20to%20any%20known%20classes.%20They%20are%20outliers%20if%20they%20occur%20in%20training%20%28OOD%0Anoise%29%2C%20and%20open-set%20samples%20if%20they%20occur%20in%20testing.%20IND%20noise%20are%20training%0Asamples%20which%20are%20assigned%20incorrect%20labels.%20The%20existence%20of%20IND%20noise%20and%20OOD%0Anoise%20is%20prevalent%2C%20which%20usually%20cause%20the%20ambiguity%20problem%2C%20including%20the%0Aintra-class%20variety%20problem%20and%20the%20inter-class%20confusion%20problem.%20Thus%2C%20to%0Aexplore%20robust%20open-set%20learning%20methods%20is%20necessary%20and%20difficult%2C%20and%20it%0Abecomes%20even%20more%20difficult%20for%20non-IID%20graph%20data.To%20this%20end%2C%20we%20propose%20a%0Aunified%20framework%20named%20ROG%24_%7BPL%7D%24%20to%20achieve%20robust%20open-set%20learning%20on%0Acomplex%20noisy%20graph%20data%2C%20by%20introducing%20prototype%20learning.%20In%20specific%2C%0AROG%24_%7BPL%7D%24%20consists%20of%20two%20modules%2C%20i.e.%2C%20denoising%20via%20label%20propagation%20and%0Aopen-set%20prototype%20learning%20via%20regions.%20The%20first%20module%20corrects%20noisy%20labels%0Athrough%20similarity-based%20label%20propagation%20and%20removes%20low-confidence%20samples%2C%0Ato%20solve%20the%20intra-class%20variety%20problem%20caused%20by%20noise.%20The%20second%20module%0Alearns%20open-set%20prototypes%20for%20each%20known%20class%20via%20non-overlapped%20regions%20and%0Aremains%20both%20interior%20and%20border%20prototypes%20to%20remedy%20the%20inter-class%20confusion%0Aproblem.The%20two%20modules%20are%20iteratively%20updated%20under%20the%20constraints%20of%0Aclassification%20loss%20and%20prototype%20diversity%20loss.%20To%20the%20best%20of%20our%20knowledge%2C%0Athe%20proposed%20ROG%24_%7BPL%7D%24%20is%20the%20first%20robust%20open-set%20node%20classification%20method%0Afor%20graph%20data%20with%20complex%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18495v1&entry.124074799=Read"},
{"title": "Partial Label Supervision for Agnostic Generative Noisy Label Learning", "author": "Fengbei Liu and Chong Wang and Yuanhong Chen and Yuyuan Liu and Gustavo Carneiro", "abstract": "  Noisy label learning has been tackled with both discriminative and generative\napproaches. Despite the simplicity and efficiency of discriminative methods,\ngenerative models offer a more principled way of disentangling clean and noisy\nlabels and estimating the label transition matrix. However, existing generative\nmethods often require inferring additional latent variables through costly\ngenerative modules or heuristic assumptions, which hinder adaptive optimisation\nfor different causal directions. They also assume a uniform clean label prior,\nwhich does not reflect the sample-wise clean label distribution and\nuncertainty. In this paper, we propose a novel framework for generative noisy\nlabel learning that addresses these challenges. First, we propose a new\nsingle-stage optimisation that directly approximates image generation by a\ndiscriminative classifier output. This approximation significantly reduces the\ncomputation cost of image generation, preserves the generative modelling\nbenefits, and enables our framework to be agnostic in regards to different\ncausality scenarios (i.e., image generate label or vice-versa). Second, we\nintroduce a new Partial Label Supervision (PLS) for noisy label learning that\naccounts for both clean label coverage and uncertainty. The supervision of PLS\ndoes not merely aim at minimising loss, but seeks to capture the underlying\nsample-wise clean label distribution and uncertainty. Extensive experiments on\ncomputer vision and natural language processing (NLP) benchmarks demonstrate\nthat our generative modelling achieves state-of-the-art results while\nsignificantly reducing the computation cost. Our code is available at\nhttps://github.com/lfb-1/GNL.\n", "link": "http://arxiv.org/abs/2308.01184v2", "date": "2024-02-28", "relevancy": 2.1032, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5366}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5365}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5107}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partial%20Label%20Supervision%20for%20Agnostic%20Generative%20Noisy%20Label%20Learning&entry.906535625=Fengbei%20Liu%20and%20Chong%20Wang%20and%20Yuanhong%20Chen%20and%20Yuyuan%20Liu%20and%20Gustavo%20Carneiro&entry.1292438233=%20%20Noisy%20label%20learning%20has%20been%20tackled%20with%20both%20discriminative%20and%20generative%0Aapproaches.%20Despite%20the%20simplicity%20and%20efficiency%20of%20discriminative%20methods%2C%0Agenerative%20models%20offer%20a%20more%20principled%20way%20of%20disentangling%20clean%20and%20noisy%0Alabels%20and%20estimating%20the%20label%20transition%20matrix.%20However%2C%20existing%20generative%0Amethods%20often%20require%20inferring%20additional%20latent%20variables%20through%20costly%0Agenerative%20modules%20or%20heuristic%20assumptions%2C%20which%20hinder%20adaptive%20optimisation%0Afor%20different%20causal%20directions.%20They%20also%20assume%20a%20uniform%20clean%20label%20prior%2C%0Awhich%20does%20not%20reflect%20the%20sample-wise%20clean%20label%20distribution%20and%0Auncertainty.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20generative%20noisy%0Alabel%20learning%20that%20addresses%20these%20challenges.%20First%2C%20we%20propose%20a%20new%0Asingle-stage%20optimisation%20that%20directly%20approximates%20image%20generation%20by%20a%0Adiscriminative%20classifier%20output.%20This%20approximation%20significantly%20reduces%20the%0Acomputation%20cost%20of%20image%20generation%2C%20preserves%20the%20generative%20modelling%0Abenefits%2C%20and%20enables%20our%20framework%20to%20be%20agnostic%20in%20regards%20to%20different%0Acausality%20scenarios%20%28i.e.%2C%20image%20generate%20label%20or%20vice-versa%29.%20Second%2C%20we%0Aintroduce%20a%20new%20Partial%20Label%20Supervision%20%28PLS%29%20for%20noisy%20label%20learning%20that%0Aaccounts%20for%20both%20clean%20label%20coverage%20and%20uncertainty.%20The%20supervision%20of%20PLS%0Adoes%20not%20merely%20aim%20at%20minimising%20loss%2C%20but%20seeks%20to%20capture%20the%20underlying%0Asample-wise%20clean%20label%20distribution%20and%20uncertainty.%20Extensive%20experiments%20on%0Acomputer%20vision%20and%20natural%20language%20processing%20%28NLP%29%20benchmarks%20demonstrate%0Athat%20our%20generative%20modelling%20achieves%20state-of-the-art%20results%20while%0Asignificantly%20reducing%20the%20computation%20cost.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lfb-1/GNL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.01184v2&entry.124074799=Read"},
{"title": "Human-Centric Aware UAV Trajectory Planning in Search and Rescue\n  Missions Employing Multi-Objective Reinforcement Learning with AHP and\n  Similarity-Based Experience Replay", "author": "Mahya Ramezani and Jose Luis Sanchez-Lopez", "abstract": "  The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue\n(SAR) missions presents a promising avenue for enhancing operational efficiency\nand effectiveness. However, the success of these missions is not solely\ndependent on the technical capabilities of the drones but also on their\nacceptance and interaction with humans on the ground. This paper explores the\neffect of human-centric factor in UAV trajectory planning for SAR missions. We\nintroduce a novel approach based on the reinforcement learning augmented with\nAnalytic Hierarchy Process and novel similarity-based experience replay to\noptimize UAV trajectories, balancing operational objectives with human comfort\nand safety considerations. Additionally, through a comprehensive survey, we\ninvestigate the impact of gender cues and anthropomorphism in UAV design on\npublic acceptance and trust, revealing significant implications for drone\ninteraction strategies in SAR. Our contributions include (1) a reinforcement\nlearning framework for UAV trajectory planning that dynamically integrates\nmulti-objective considerations, (2) an analysis of human perceptions towards\ngendered and anthropomorphized drones in SAR contexts, and (3) the application\nof similarity-based experience replay for enhanced learning efficiency in\ncomplex SAR scenarios. The findings offer valuable insights into designing UAV\nsystems that are not only technically proficient but also aligned with\nhuman-centric values.\n", "link": "http://arxiv.org/abs/2402.18487v1", "date": "2024-02-28", "relevancy": 2.0978, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5371}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5296}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5098}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Centric%20Aware%20UAV%20Trajectory%20Planning%20in%20Search%20and%20Rescue%0A%20%20Missions%20Employing%20Multi-Objective%20Reinforcement%20Learning%20with%20AHP%20and%0A%20%20Similarity-Based%20Experience%20Replay&entry.906535625=Mahya%20Ramezani%20and%20Jose%20Luis%20Sanchez-Lopez&entry.1292438233=%20%20The%20integration%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20into%20Search%20and%20Rescue%0A%28SAR%29%20missions%20presents%20a%20promising%20avenue%20for%20enhancing%20operational%20efficiency%0Aand%20effectiveness.%20However%2C%20the%20success%20of%20these%20missions%20is%20not%20solely%0Adependent%20on%20the%20technical%20capabilities%20of%20the%20drones%20but%20also%20on%20their%0Aacceptance%20and%20interaction%20with%20humans%20on%20the%20ground.%20This%20paper%20explores%20the%0Aeffect%20of%20human-centric%20factor%20in%20UAV%20trajectory%20planning%20for%20SAR%20missions.%20We%0Aintroduce%20a%20novel%20approach%20based%20on%20the%20reinforcement%20learning%20augmented%20with%0AAnalytic%20Hierarchy%20Process%20and%20novel%20similarity-based%20experience%20replay%20to%0Aoptimize%20UAV%20trajectories%2C%20balancing%20operational%20objectives%20with%20human%20comfort%0Aand%20safety%20considerations.%20Additionally%2C%20through%20a%20comprehensive%20survey%2C%20we%0Ainvestigate%20the%20impact%20of%20gender%20cues%20and%20anthropomorphism%20in%20UAV%20design%20on%0Apublic%20acceptance%20and%20trust%2C%20revealing%20significant%20implications%20for%20drone%0Ainteraction%20strategies%20in%20SAR.%20Our%20contributions%20include%20%281%29%20a%20reinforcement%0Alearning%20framework%20for%20UAV%20trajectory%20planning%20that%20dynamically%20integrates%0Amulti-objective%20considerations%2C%20%282%29%20an%20analysis%20of%20human%20perceptions%20towards%0Agendered%20and%20anthropomorphized%20drones%20in%20SAR%20contexts%2C%20and%20%283%29%20the%20application%0Aof%20similarity-based%20experience%20replay%20for%20enhanced%20learning%20efficiency%20in%0Acomplex%20SAR%20scenarios.%20The%20findings%20offer%20valuable%20insights%20into%20designing%20UAV%0Asystems%20that%20are%20not%20only%20technically%20proficient%20but%20also%20aligned%20with%0Ahuman-centric%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18487v1&entry.124074799=Read"},
{"title": "UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots", "author": "Ines Sorrentino and Giulio Romualdi and Daniele Pucci", "abstract": "  This paper proposes a novel sensor fusion based on Unscented Kalman Filtering\nfor the online estimation of joint-torques of humanoid robots without\njoint-torque sensors. At the feature level, the proposed approach considers\nmultimodal measurements (e.g. currents, accelerations, etc.) and non-directly\nmeasurable effects, such as external contacts, thus leading to joint torques\nreadily usable in control architectures for human-robot interaction. The\nproposed sensor fusion can also integrate distributed, non-collocated\nforce/torque sensors, thus being a flexible framework with respect to the\nunderlying robot sensor suit. To validate the approach, we show how the\nproposed sensor fusion can be integrated into a twolevel torque control\narchitecture aiming at task-space torquecontrol. The performances of the\nproposed approach are shown through extensive tests on the new humanoid robot\nergoCub, currently being developed at Istituto Italiano di Tecnologia. We also\ncompare our strategy with the existing state-of-theart approach based on the\nrecursive Newton-Euler algorithm. Results demonstrate that our method achieves\nlow root mean square errors in torque tracking, ranging from 0.05 Nm to 2.5 Nm,\neven in the presence of external contacts.\n", "link": "http://arxiv.org/abs/2402.18380v1", "date": "2024-02-28", "relevancy": 2.0894, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5616}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5294}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4995}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UKF-Based%20Sensor%20Fusion%20for%20Joint-Torque%20Sensorless%20Humanoid%20Robots&entry.906535625=Ines%20Sorrentino%20and%20Giulio%20Romualdi%20and%20Daniele%20Pucci&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20sensor%20fusion%20based%20on%20Unscented%20Kalman%20Filtering%0Afor%20the%20online%20estimation%20of%20joint-torques%20of%20humanoid%20robots%20without%0Ajoint-torque%20sensors.%20At%20the%20feature%20level%2C%20the%20proposed%20approach%20considers%0Amultimodal%20measurements%20%28e.g.%20currents%2C%20accelerations%2C%20etc.%29%20and%20non-directly%0Ameasurable%20effects%2C%20such%20as%20external%20contacts%2C%20thus%20leading%20to%20joint%20torques%0Areadily%20usable%20in%20control%20architectures%20for%20human-robot%20interaction.%20The%0Aproposed%20sensor%20fusion%20can%20also%20integrate%20distributed%2C%20non-collocated%0Aforce/torque%20sensors%2C%20thus%20being%20a%20flexible%20framework%20with%20respect%20to%20the%0Aunderlying%20robot%20sensor%20suit.%20To%20validate%20the%20approach%2C%20we%20show%20how%20the%0Aproposed%20sensor%20fusion%20can%20be%20integrated%20into%20a%20twolevel%20torque%20control%0Aarchitecture%20aiming%20at%20task-space%20torquecontrol.%20The%20performances%20of%20the%0Aproposed%20approach%20are%20shown%20through%20extensive%20tests%20on%20the%20new%20humanoid%20robot%0AergoCub%2C%20currently%20being%20developed%20at%20Istituto%20Italiano%20di%20Tecnologia.%20We%20also%0Acompare%20our%20strategy%20with%20the%20existing%20state-of-theart%20approach%20based%20on%20the%0Arecursive%20Newton-Euler%20algorithm.%20Results%20demonstrate%20that%20our%20method%20achieves%0Alow%20root%20mean%20square%20errors%20in%20torque%20tracking%2C%20ranging%20from%200.05%20Nm%20to%202.5%20Nm%2C%0Aeven%20in%20the%20presence%20of%20external%20contacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18380v1&entry.124074799=Read"},
{"title": "FinAgent: A Multimodal Foundation Agent for Financial Trading:\n  Tool-Augmented, Diversified, and Generalist", "author": "Wentao Zhang and Lingxuan Zhao and Haochong Xia and Shuo Sun and Jiaze Sun and Molei Qin and Xinyi Li and Yuqing Zhao and Yilei Zhao and Xinyu Cai and Longtao Zheng and Xinrun Wang and Bo An", "abstract": "  Financial trading is a crucial component of the markets, informed by a\nmultimodal information landscape encompassing news, prices, and Kline charts,\nand encompasses diverse tasks such as quantitative trading and high-frequency\ntrading with various assets. While advanced AI techniques like deep learning\nand reinforcement learning are extensively utilized in finance, their\napplication in financial trading tasks often faces challenges due to inadequate\nhandling of multimodal data and limited generalizability across various tasks.\nTo address these challenges, we present FinAgent, a multimodal foundational\nagent with tool augmentation for financial trading. FinAgent's market\nintelligence module processes a diverse range of data-numerical, textual, and\nvisual-to accurately analyze the financial market. Its unique dual-level\nreflection module not only enables rapid adaptation to market dynamics but also\nincorporates a diversified memory retrieval system, enhancing the agent's\nability to learn from historical data and improve decision-making processes.\nThe agent's emphasis on reasoning for actions fosters trust in its financial\ndecisions. Moreover, FinAgent integrates established trading strategies and\nexpert insights, ensuring that its trading approaches are both data-driven and\nrooted in sound financial principles. With comprehensive experiments on 6\nfinancial datasets, including stocks and Crypto, FinAgent significantly\noutperforms 9 state-of-the-art baselines in terms of 6 financial metrics with\nover 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%\nrelative improvement) is achieved on one dataset. Notably, FinAgent is the\nfirst advanced multimodal foundation agent designed for financial trading\ntasks.\n", "link": "http://arxiv.org/abs/2402.18485v1", "date": "2024-02-28", "relevancy": 2.0802, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4955}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinAgent%3A%20A%20Multimodal%20Foundation%20Agent%20for%20Financial%20Trading%3A%0A%20%20Tool-Augmented%2C%20Diversified%2C%20and%20Generalist&entry.906535625=Wentao%20Zhang%20and%20Lingxuan%20Zhao%20and%20Haochong%20Xia%20and%20Shuo%20Sun%20and%20Jiaze%20Sun%20and%20Molei%20Qin%20and%20Xinyi%20Li%20and%20Yuqing%20Zhao%20and%20Yilei%20Zhao%20and%20Xinyu%20Cai%20and%20Longtao%20Zheng%20and%20Xinrun%20Wang%20and%20Bo%20An&entry.1292438233=%20%20Financial%20trading%20is%20a%20crucial%20component%20of%20the%20markets%2C%20informed%20by%20a%0Amultimodal%20information%20landscape%20encompassing%20news%2C%20prices%2C%20and%20Kline%20charts%2C%0Aand%20encompasses%20diverse%20tasks%20such%20as%20quantitative%20trading%20and%20high-frequency%0Atrading%20with%20various%20assets.%20While%20advanced%20AI%20techniques%20like%20deep%20learning%0Aand%20reinforcement%20learning%20are%20extensively%20utilized%20in%20finance%2C%20their%0Aapplication%20in%20financial%20trading%20tasks%20often%20faces%20challenges%20due%20to%20inadequate%0Ahandling%20of%20multimodal%20data%20and%20limited%20generalizability%20across%20various%20tasks.%0ATo%20address%20these%20challenges%2C%20we%20present%20FinAgent%2C%20a%20multimodal%20foundational%0Aagent%20with%20tool%20augmentation%20for%20financial%20trading.%20FinAgent%27s%20market%0Aintelligence%20module%20processes%20a%20diverse%20range%20of%20data-numerical%2C%20textual%2C%20and%0Avisual-to%20accurately%20analyze%20the%20financial%20market.%20Its%20unique%20dual-level%0Areflection%20module%20not%20only%20enables%20rapid%20adaptation%20to%20market%20dynamics%20but%20also%0Aincorporates%20a%20diversified%20memory%20retrieval%20system%2C%20enhancing%20the%20agent%27s%0Aability%20to%20learn%20from%20historical%20data%20and%20improve%20decision-making%20processes.%0AThe%20agent%27s%20emphasis%20on%20reasoning%20for%20actions%20fosters%20trust%20in%20its%20financial%0Adecisions.%20Moreover%2C%20FinAgent%20integrates%20established%20trading%20strategies%20and%0Aexpert%20insights%2C%20ensuring%20that%20its%20trading%20approaches%20are%20both%20data-driven%20and%0Arooted%20in%20sound%20financial%20principles.%20With%20comprehensive%20experiments%20on%206%0Afinancial%20datasets%2C%20including%20stocks%20and%20Crypto%2C%20FinAgent%20significantly%0Aoutperforms%209%20state-of-the-art%20baselines%20in%20terms%20of%206%20financial%20metrics%20with%0Aover%2036%25%20average%20improvement%20on%20profit.%20Specifically%2C%20a%2092.27%25%20return%20%28a%2084.39%25%0Arelative%20improvement%29%20is%20achieved%20on%20one%20dataset.%20Notably%2C%20FinAgent%20is%20the%0Afirst%20advanced%20multimodal%20foundation%20agent%20designed%20for%20financial%20trading%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18485v1&entry.124074799=Read"},
{"title": "LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping", "author": "Changho Choi and Minho Kim and Junhyeok Lee and Hyoung-Kyu Song and Younggeun Kim and Seungryong Kim", "abstract": "  We propose LatentSwap, a simple face swapping framework generating a face\nswap latent code of a given generator. Utilizing randomly sampled latent codes,\nour framework is light and does not require datasets besides employing the\npre-trained models, with the training procedure also being fast and\nstraightforward. The loss objective consists of only three terms, and can\neffectively control the face swap results between source and target images. By\nattaching a pre-trained GAN inversion model independent to the model and using\nthe StyleGAN2 generator, our model produces photorealistic and high-resolution\nimages comparable to other competitive face swap models. We show that our\nframework is applicable to other generators such as StyleNeRF, paving a way to\n3D-aware face swapping and is also compatible with other downstream StyleGAN2\ngenerator tasks. The source code and models can be found at\n\\url{https://github.com/usingcolor/LatentSwap}.\n", "link": "http://arxiv.org/abs/2402.18351v1", "date": "2024-02-28", "relevancy": 2.0733, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5417}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5045}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5005}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LatentSwap%3A%20An%20Efficient%20Latent%20Code%20Mapping%20Framework%20for%20Face%20Swapping&entry.906535625=Changho%20Choi%20and%20Minho%20Kim%20and%20Junhyeok%20Lee%20and%20Hyoung-Kyu%20Song%20and%20Younggeun%20Kim%20and%20Seungryong%20Kim&entry.1292438233=%20%20We%20propose%20LatentSwap%2C%20a%20simple%20face%20swapping%20framework%20generating%20a%20face%0Aswap%20latent%20code%20of%20a%20given%20generator.%20Utilizing%20randomly%20sampled%20latent%20codes%2C%0Aour%20framework%20is%20light%20and%20does%20not%20require%20datasets%20besides%20employing%20the%0Apre-trained%20models%2C%20with%20the%20training%20procedure%20also%20being%20fast%20and%0Astraightforward.%20The%20loss%20objective%20consists%20of%20only%20three%20terms%2C%20and%20can%0Aeffectively%20control%20the%20face%20swap%20results%20between%20source%20and%20target%20images.%20By%0Aattaching%20a%20pre-trained%20GAN%20inversion%20model%20independent%20to%20the%20model%20and%20using%0Athe%20StyleGAN2%20generator%2C%20our%20model%20produces%20photorealistic%20and%20high-resolution%0Aimages%20comparable%20to%20other%20competitive%20face%20swap%20models.%20We%20show%20that%20our%0Aframework%20is%20applicable%20to%20other%20generators%20such%20as%20StyleNeRF%2C%20paving%20a%20way%20to%0A3D-aware%20face%20swapping%20and%20is%20also%20compatible%20with%20other%20downstream%20StyleGAN2%0Agenerator%20tasks.%20The%20source%20code%20and%20models%20can%20be%20found%20at%0A%5Curl%7Bhttps%3A//github.com/usingcolor/LatentSwap%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18351v1&entry.124074799=Read"},
{"title": "IBD: Alleviating Hallucinations in Large Vision-Language Models via\n  Image-Biased Decoding", "author": "Lanyun Zhu and Deyi Ji and Tianrun Chen and Peng Xu and Jieping Ye and Jun Liu", "abstract": "  Despite achieving rapid developments and with widespread applications, Large\nVision-Language Models (LVLMs) confront a serious challenge of being prone to\ngenerating hallucinations. An over-reliance on linguistic priors has been\nidentified as a key factor leading to these hallucinations. In this paper, we\npropose to alleviate this problem by introducing a novel image-biased decoding\n(IBD) technique. Our method derives the next-token probability distribution by\ncontrasting predictions from a conventional LVLM with those of an image-biased\nLVLM, thereby amplifying the correct information highly correlated with image\ncontent while mitigating the hallucinatory errors caused by excessive\ndependence on text. We further conduct a comprehensive statistical analysis to\nvalidate the reliability of our method, and design an adaptive adjustment\nstrategy to achieve robust and flexible handling under varying conditions.\nExperimental results across multiple evaluation metrics verify that our method,\ndespite not requiring additional training data and only with a minimal increase\nin model parameters, can significantly reduce hallucinations in LVLMs and\nenhance the truthfulness of the generated response.\n", "link": "http://arxiv.org/abs/2402.18476v1", "date": "2024-02-28", "relevancy": 2.0696, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5322}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5264}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5025}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IBD%3A%20Alleviating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%0A%20%20Image-Biased%20Decoding&entry.906535625=Lanyun%20Zhu%20and%20Deyi%20Ji%20and%20Tianrun%20Chen%20and%20Peng%20Xu%20and%20Jieping%20Ye%20and%20Jun%20Liu&entry.1292438233=%20%20Despite%20achieving%20rapid%20developments%20and%20with%20widespread%20applications%2C%20Large%0AVision-Language%20Models%20%28LVLMs%29%20confront%20a%20serious%20challenge%20of%20being%20prone%20to%0Agenerating%20hallucinations.%20An%20over-reliance%20on%20linguistic%20priors%20has%20been%0Aidentified%20as%20a%20key%20factor%20leading%20to%20these%20hallucinations.%20In%20this%20paper%2C%20we%0Apropose%20to%20alleviate%20this%20problem%20by%20introducing%20a%20novel%20image-biased%20decoding%0A%28IBD%29%20technique.%20Our%20method%20derives%20the%20next-token%20probability%20distribution%20by%0Acontrasting%20predictions%20from%20a%20conventional%20LVLM%20with%20those%20of%20an%20image-biased%0ALVLM%2C%20thereby%20amplifying%20the%20correct%20information%20highly%20correlated%20with%20image%0Acontent%20while%20mitigating%20the%20hallucinatory%20errors%20caused%20by%20excessive%0Adependence%20on%20text.%20We%20further%20conduct%20a%20comprehensive%20statistical%20analysis%20to%0Avalidate%20the%20reliability%20of%20our%20method%2C%20and%20design%20an%20adaptive%20adjustment%0Astrategy%20to%20achieve%20robust%20and%20flexible%20handling%20under%20varying%20conditions.%0AExperimental%20results%20across%20multiple%20evaluation%20metrics%20verify%20that%20our%20method%2C%0Adespite%20not%20requiring%20additional%20training%20data%20and%20only%20with%20a%20minimal%20increase%0Ain%20model%20parameters%2C%20can%20significantly%20reduce%20hallucinations%20in%20LVLMs%20and%0Aenhance%20the%20truthfulness%20of%20the%20generated%20response.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18476v1&entry.124074799=Read"},
{"title": "Separate and Conquer: Decoupling Co-occurrence via Decomposition and\n  Representation for Weakly Supervised Semantic Segmentation", "author": "Zhiwei Yang and Kexue Fu and Minghong Duan and Linhao Qu and Shuo Wang and Zhijian Song", "abstract": "  Attributed to the frequent coupling of co-occurring objects and the limited\nsupervision from image-level labels, the challenging co-occurrence problem is\nwidely present and leads to false activation of objects in weakly supervised\nsemantic segmentation (WSSS). In this work, we devise a 'Separate and Conquer'\nscheme SeCo to tackle this issue from dimensions of image space and feature\nspace. In the image space, we propose to 'separate' the co-occurring objects\nwith image decomposition by subdividing images into patches. Importantly, we\nassign each patch a category tag from Class Activation Maps (CAMs), which\nspatially helps remove the co-context bias and guide the subsequent\nrepresentation. In the feature space, we propose to 'conquer' the false\nactivation by enhancing semantic representation with multi-granularity\nknowledge contrast. To this end, a dual-teacher-single-student architecture is\ndesigned and tag-guided contrast is conducted to guarantee the correctness of\nknowledge and further facilitate the discrepancy among co-occurring objects. We\nstreamline the multi-staged WSSS pipeline end-to-end and tackle co-occurrence\nwithout external supervision. Extensive experiments are conducted, validating\nthe efficiency of our method tackling co-occurrence and the superiority over\nprevious single-staged and even multi-staged competitors on PASCAL VOC and MS\nCOCO. Code will be available.\n", "link": "http://arxiv.org/abs/2402.18467v1", "date": "2024-02-28", "relevancy": 2.0676, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5371}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5057}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separate%20and%20Conquer%3A%20Decoupling%20Co-occurrence%20via%20Decomposition%20and%0A%20%20Representation%20for%20Weakly%20Supervised%20Semantic%20Segmentation&entry.906535625=Zhiwei%20Yang%20and%20Kexue%20Fu%20and%20Minghong%20Duan%20and%20Linhao%20Qu%20and%20Shuo%20Wang%20and%20Zhijian%20Song&entry.1292438233=%20%20Attributed%20to%20the%20frequent%20coupling%20of%20co-occurring%20objects%20and%20the%20limited%0Asupervision%20from%20image-level%20labels%2C%20the%20challenging%20co-occurrence%20problem%20is%0Awidely%20present%20and%20leads%20to%20false%20activation%20of%20objects%20in%20weakly%20supervised%0Asemantic%20segmentation%20%28WSSS%29.%20In%20this%20work%2C%20we%20devise%20a%20%27Separate%20and%20Conquer%27%0Ascheme%20SeCo%20to%20tackle%20this%20issue%20from%20dimensions%20of%20image%20space%20and%20feature%0Aspace.%20In%20the%20image%20space%2C%20we%20propose%20to%20%27separate%27%20the%20co-occurring%20objects%0Awith%20image%20decomposition%20by%20subdividing%20images%20into%20patches.%20Importantly%2C%20we%0Aassign%20each%20patch%20a%20category%20tag%20from%20Class%20Activation%20Maps%20%28CAMs%29%2C%20which%0Aspatially%20helps%20remove%20the%20co-context%20bias%20and%20guide%20the%20subsequent%0Arepresentation.%20In%20the%20feature%20space%2C%20we%20propose%20to%20%27conquer%27%20the%20false%0Aactivation%20by%20enhancing%20semantic%20representation%20with%20multi-granularity%0Aknowledge%20contrast.%20To%20this%20end%2C%20a%20dual-teacher-single-student%20architecture%20is%0Adesigned%20and%20tag-guided%20contrast%20is%20conducted%20to%20guarantee%20the%20correctness%20of%0Aknowledge%20and%20further%20facilitate%20the%20discrepancy%20among%20co-occurring%20objects.%20We%0Astreamline%20the%20multi-staged%20WSSS%20pipeline%20end-to-end%20and%20tackle%20co-occurrence%0Awithout%20external%20supervision.%20Extensive%20experiments%20are%20conducted%2C%20validating%0Athe%20efficiency%20of%20our%20method%20tackling%20co-occurrence%20and%20the%20superiority%20over%0Aprevious%20single-staged%20and%20even%20multi-staged%20competitors%20on%20PASCAL%20VOC%20and%20MS%0ACOCO.%20Code%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18467v1&entry.124074799=Read"},
{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "author": "Ethan Smith and Nayan Saxena and Aninda Saha", "abstract": "  Attention mechanism has been crucial for image diffusion models, however,\ntheir quadratic computational complexity limits the sizes of images we can\nprocess within reasonable time and memory constraints. This paper investigates\nthe importance of dense attention in generative image models, which often\ncontain redundant features, making them suitable for sparser attention\nmechanisms. We propose a novel training-free method ToDo that relies on token\ndownsampling of key and value tokens to accelerate Stable Diffusion inference\nby up to 2x for common sizes and up to 4.5x or more for high resolutions like\n2048x2048. We demonstrate that our approach outperforms previous methods in\nbalancing efficient throughput and fidelity.\n", "link": "http://arxiv.org/abs/2402.13573v2", "date": "2024-02-28", "relevancy": 2.0509, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5391}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5186}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4841}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToDo%3A%20Token%20Downsampling%20for%20Efficient%20Generation%20of%20High-Resolution%0A%20%20Images&entry.906535625=Ethan%20Smith%20and%20Nayan%20Saxena%20and%20Aninda%20Saha&entry.1292438233=%20%20Attention%20mechanism%20has%20been%20crucial%20for%20image%20diffusion%20models%2C%20however%2C%0Atheir%20quadratic%20computational%20complexity%20limits%20the%20sizes%20of%20images%20we%20can%0Aprocess%20within%20reasonable%20time%20and%20memory%20constraints.%20This%20paper%20investigates%0Athe%20importance%20of%20dense%20attention%20in%20generative%20image%20models%2C%20which%20often%0Acontain%20redundant%20features%2C%20making%20them%20suitable%20for%20sparser%20attention%0Amechanisms.%20We%20propose%20a%20novel%20training-free%20method%20ToDo%20that%20relies%20on%20token%0Adownsampling%20of%20key%20and%20value%20tokens%20to%20accelerate%20Stable%20Diffusion%20inference%0Aby%20up%20to%202x%20for%20common%20sizes%20and%20up%20to%204.5x%20or%20more%20for%20high%20resolutions%20like%0A2048x2048.%20We%20demonstrate%20that%20our%20approach%20outperforms%20previous%20methods%20in%0Abalancing%20efficient%20throughput%20and%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13573v2&entry.124074799=Read"},
{"title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision Language Models", "author": "Xiujie Song and Mengyue Wu and Kenny Q. Zhu and Chunhao Zhang and Yanyi Chen", "abstract": "  Large Vision Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the \"Cookie Theft\" task in human cognition test, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive ability of LVLMs\nusing images with rich semantics. It defines eight reasoning capabilities and\nconsists of an image description task and a visual question answering task. Our\nevaluation on well-known LVLMs shows that there is still a large gap in\ncognitive ability between LVLMs and humans.\n", "link": "http://arxiv.org/abs/2402.18409v1", "date": "2024-02-28", "relevancy": 2.0354, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5288}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.493}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cognitive%20Evaluation%20Benchmark%20of%20Image%20Reasoning%20and%20Description%20for%0A%20%20Large%20Vision%20Language%20Models&entry.906535625=Xiujie%20Song%20and%20Mengyue%20Wu%20and%20Kenny%20Q.%20Zhu%20and%20Chunhao%20Zhang%20and%20Yanyi%20Chen&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%2C%20despite%20their%20recent%20success%2C%20are%0Ahardly%20comprehensively%20tested%20for%20their%20cognitive%20abilities.%20Inspired%20by%20the%0Aprevalent%20use%20of%20the%20%22Cookie%20Theft%22%20task%20in%20human%20cognition%20test%2C%20we%20propose%20a%0Anovel%20evaluation%20benchmark%20to%20evaluate%20high-level%20cognitive%20ability%20of%20LVLMs%0Ausing%20images%20with%20rich%20semantics.%20It%20defines%20eight%20reasoning%20capabilities%20and%0Aconsists%20of%20an%20image%20description%20task%20and%20a%20visual%20question%20answering%20task.%20Our%0Aevaluation%20on%20well-known%20LVLMs%20shows%20that%20there%20is%20still%20a%20large%20gap%20in%0Acognitive%20ability%20between%20LVLMs%20and%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18409v1&entry.124074799=Read"},
{"title": "Graph Regularized Encoder Training for Extreme Classification", "author": "Anshul Mittal and Shikhar Mohan and Deepak Saini and Suchith C. Prabhu and Jain jiao and Sumeet Agarwal and Soumen Chakrabarti and Purushottam Kar and Manik Varma", "abstract": "  Deep extreme classification (XC) aims to train an encoder architecture and an\naccompanying classifier architecture to tag a data point with the most relevant\nsubset of labels from a very large universe of labels. XC applications in\nranking, recommendation and tagging routinely encounter tail labels for which\nthe amount of training data is exceedingly small. Graph convolutional networks\n(GCN) present a convenient but computationally expensive way to leverage task\nmetadata and enhance model accuracies in these settings. This paper formally\nestablishes that in several use cases, the steep computational cost of GCNs is\nentirely avoidable by replacing GCNs with non-GCN architectures. The paper\nnotices that in these settings, it is much more effective to use graph data to\nregularize encoder training than to implement a GCN. Based on these insights,\nan alternative paradigm RAMEN is presented to utilize graph metadata in XC\nsettings that offers significant performance boosts with zero increase in\ninference computational costs. RAMEN scales to datasets with up to 1M labels\nand offers prediction accuracy up to 15% higher on benchmark datasets than\nstate of the art methods, including those that use graph metadata to train\nGCNs. RAMEN also offers 10% higher accuracy over the best baseline on a\nproprietary recommendation dataset sourced from click logs of a popular search\nengine. Code for RAMEN will be released publicly.\n", "link": "http://arxiv.org/abs/2402.18434v1", "date": "2024-02-28", "relevancy": 2.0291, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5182}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5056}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4841}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Regularized%20Encoder%20Training%20for%20Extreme%20Classification&entry.906535625=Anshul%20Mittal%20and%20Shikhar%20Mohan%20and%20Deepak%20Saini%20and%20Suchith%20C.%20Prabhu%20and%20Jain%20jiao%20and%20Sumeet%20Agarwal%20and%20Soumen%20Chakrabarti%20and%20Purushottam%20Kar%20and%20Manik%20Varma&entry.1292438233=%20%20Deep%20extreme%20classification%20%28XC%29%20aims%20to%20train%20an%20encoder%20architecture%20and%20an%0Aaccompanying%20classifier%20architecture%20to%20tag%20a%20data%20point%20with%20the%20most%20relevant%0Asubset%20of%20labels%20from%20a%20very%20large%20universe%20of%20labels.%20XC%20applications%20in%0Aranking%2C%20recommendation%20and%20tagging%20routinely%20encounter%20tail%20labels%20for%20which%0Athe%20amount%20of%20training%20data%20is%20exceedingly%20small.%20Graph%20convolutional%20networks%0A%28GCN%29%20present%20a%20convenient%20but%20computationally%20expensive%20way%20to%20leverage%20task%0Ametadata%20and%20enhance%20model%20accuracies%20in%20these%20settings.%20This%20paper%20formally%0Aestablishes%20that%20in%20several%20use%20cases%2C%20the%20steep%20computational%20cost%20of%20GCNs%20is%0Aentirely%20avoidable%20by%20replacing%20GCNs%20with%20non-GCN%20architectures.%20The%20paper%0Anotices%20that%20in%20these%20settings%2C%20it%20is%20much%20more%20effective%20to%20use%20graph%20data%20to%0Aregularize%20encoder%20training%20than%20to%20implement%20a%20GCN.%20Based%20on%20these%20insights%2C%0Aan%20alternative%20paradigm%20RAMEN%20is%20presented%20to%20utilize%20graph%20metadata%20in%20XC%0Asettings%20that%20offers%20significant%20performance%20boosts%20with%20zero%20increase%20in%0Ainference%20computational%20costs.%20RAMEN%20scales%20to%20datasets%20with%20up%20to%201M%20labels%0Aand%20offers%20prediction%20accuracy%20up%20to%2015%25%20higher%20on%20benchmark%20datasets%20than%0Astate%20of%20the%20art%20methods%2C%20including%20those%20that%20use%20graph%20metadata%20to%20train%0AGCNs.%20RAMEN%20also%20offers%2010%25%20higher%20accuracy%20over%20the%20best%20baseline%20on%20a%0Aproprietary%20recommendation%20dataset%20sourced%20from%20click%20logs%20of%20a%20popular%20search%0Aengine.%20Code%20for%20RAMEN%20will%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18434v1&entry.124074799=Read"},
{"title": "Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks", "author": "Benjamin David Evans and Raphael Trumpp and Marco Caccamo and Hendrik Willem Jordaan and Herman Arnold Engelbrecht", "abstract": "  The F1TENTH autonomous racing platform, consisting of 1:10 scale RC cars, has\nevolved into a leading research platform. The many publications and real-world\ncompetitions span many domains, from classical path planning to novel\nlearning-based algorithms. Consequently, the field is wide and disjointed,\nhindering direct comparison of methods and making it difficult to assess the\nstate-of-the-art. Therefore, we aim to unify the field by surveying current\napproaches, describing common methods and providing benchmark results to\nfacilitate clear comparison and establish a baseline for future work. We survey\ncurrent work in F1TENTH racing in the classical and learning categories,\nexplaining the different solution approaches. We describe particle filter\nlocalisation, trajectory optimisation and tracking, model predictive contouring\ncontrol (MPCC), follow-the-gap and end-to-end reinforcement learning. We\nprovide an open-source evaluation of benchmark methods and investigate\noverlooked factors of control frequency and localisation accuracy for classical\nmethods and reward signal and training map for learning methods. The evaluation\nshows that the optimisation and tracking method achieves the fastest lap times,\nfollowed by the MPCC planner. Finally, our work identifies and outlines the\nrelevant research aspects to help motivate future work in the F1TENTH domain.\n", "link": "http://arxiv.org/abs/2402.18558v1", "date": "2024-02-28", "relevancy": 2.013, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5187}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5103}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4849}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20F1TENTH%20Autonomous%20Racing%3A%20Survey%2C%20Methods%20and%20Benchmarks&entry.906535625=Benjamin%20David%20Evans%20and%20Raphael%20Trumpp%20and%20Marco%20Caccamo%20and%20Hendrik%20Willem%20Jordaan%20and%20Herman%20Arnold%20Engelbrecht&entry.1292438233=%20%20The%20F1TENTH%20autonomous%20racing%20platform%2C%20consisting%20of%201%3A10%20scale%20RC%20cars%2C%20has%0Aevolved%20into%20a%20leading%20research%20platform.%20The%20many%20publications%20and%20real-world%0Acompetitions%20span%20many%20domains%2C%20from%20classical%20path%20planning%20to%20novel%0Alearning-based%20algorithms.%20Consequently%2C%20the%20field%20is%20wide%20and%20disjointed%2C%0Ahindering%20direct%20comparison%20of%20methods%20and%20making%20it%20difficult%20to%20assess%20the%0Astate-of-the-art.%20Therefore%2C%20we%20aim%20to%20unify%20the%20field%20by%20surveying%20current%0Aapproaches%2C%20describing%20common%20methods%20and%20providing%20benchmark%20results%20to%0Afacilitate%20clear%20comparison%20and%20establish%20a%20baseline%20for%20future%20work.%20We%20survey%0Acurrent%20work%20in%20F1TENTH%20racing%20in%20the%20classical%20and%20learning%20categories%2C%0Aexplaining%20the%20different%20solution%20approaches.%20We%20describe%20particle%20filter%0Alocalisation%2C%20trajectory%20optimisation%20and%20tracking%2C%20model%20predictive%20contouring%0Acontrol%20%28MPCC%29%2C%20follow-the-gap%20and%20end-to-end%20reinforcement%20learning.%20We%0Aprovide%20an%20open-source%20evaluation%20of%20benchmark%20methods%20and%20investigate%0Aoverlooked%20factors%20of%20control%20frequency%20and%20localisation%20accuracy%20for%20classical%0Amethods%20and%20reward%20signal%20and%20training%20map%20for%20learning%20methods.%20The%20evaluation%0Ashows%20that%20the%20optimisation%20and%20tracking%20method%20achieves%20the%20fastest%20lap%20times%2C%0Afollowed%20by%20the%20MPCC%20planner.%20Finally%2C%20our%20work%20identifies%20and%20outlines%20the%0Arelevant%20research%20aspects%20to%20help%20motivate%20future%20work%20in%20the%20F1TENTH%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18558v1&entry.124074799=Read"},
{"title": "Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal\n  Transport", "author": "Bin Li and Ye Shi and Qian Yu and Jingya Wang", "abstract": "  Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images\nsharing the same category across diverse domains without relying on labeled\ndata. Prior approaches have typically decomposed the UCIR problem into two\ndistinct tasks: intra-domain representation learning and cross-domain feature\nalignment. However, these segregated strategies overlook the potential\nsynergies between these tasks. This paper introduces ProtoOT, a novel Optimal\nTransport formulation explicitly tailored for UCIR, which integrates\nintra-domain feature representation learning and cross-domain alignment into a\nunified framework. ProtoOT leverages the strengths of the K-means clustering\nmethod to effectively manage distribution imbalances inherent in UCIR. By\nutilizing K-means for generating initial prototypes and approximating class\nmarginal distributions, we modify the constraints in Optimal Transport\naccordingly, significantly enhancing its performance in UCIR scenarios.\nFurthermore, we incorporate contrastive learning into the ProtoOT framework to\nfurther improve representation learning. This encourages local semantic\nconsistency among features with similar semantics, while also explicitly\nenforcing separation between features and unmatched prototypes, thereby\nenhancing global discriminativeness. ProtoOT surpasses existing\nstate-of-the-art methods by a notable margin across benchmark datasets.\nNotably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%,\nand on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is\navailable at https://github.com/HCVLAB/ProtoOT.\n", "link": "http://arxiv.org/abs/2402.18411v1", "date": "2024-02-28", "relevancy": 2.0083, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5153}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4947}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4874}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Cross-Domain%20Image%20Retrieval%20via%20Prototypical%20Optimal%0A%20%20Transport&entry.906535625=Bin%20Li%20and%20Ye%20Shi%20and%20Qian%20Yu%20and%20Jingya%20Wang&entry.1292438233=%20%20Unsupervised%20cross-domain%20image%20retrieval%20%28UCIR%29%20aims%20to%20retrieve%20images%0Asharing%20the%20same%20category%20across%20diverse%20domains%20without%20relying%20on%20labeled%0Adata.%20Prior%20approaches%20have%20typically%20decomposed%20the%20UCIR%20problem%20into%20two%0Adistinct%20tasks%3A%20intra-domain%20representation%20learning%20and%20cross-domain%20feature%0Aalignment.%20However%2C%20these%20segregated%20strategies%20overlook%20the%20potential%0Asynergies%20between%20these%20tasks.%20This%20paper%20introduces%20ProtoOT%2C%20a%20novel%20Optimal%0ATransport%20formulation%20explicitly%20tailored%20for%20UCIR%2C%20which%20integrates%0Aintra-domain%20feature%20representation%20learning%20and%20cross-domain%20alignment%20into%20a%0Aunified%20framework.%20ProtoOT%20leverages%20the%20strengths%20of%20the%20K-means%20clustering%0Amethod%20to%20effectively%20manage%20distribution%20imbalances%20inherent%20in%20UCIR.%20By%0Autilizing%20K-means%20for%20generating%20initial%20prototypes%20and%20approximating%20class%0Amarginal%20distributions%2C%20we%20modify%20the%20constraints%20in%20Optimal%20Transport%0Aaccordingly%2C%20significantly%20enhancing%20its%20performance%20in%20UCIR%20scenarios.%0AFurthermore%2C%20we%20incorporate%20contrastive%20learning%20into%20the%20ProtoOT%20framework%20to%0Afurther%20improve%20representation%20learning.%20This%20encourages%20local%20semantic%0Aconsistency%20among%20features%20with%20similar%20semantics%2C%20while%20also%20explicitly%0Aenforcing%20separation%20between%20features%20and%20unmatched%20prototypes%2C%20thereby%0Aenhancing%20global%20discriminativeness.%20ProtoOT%20surpasses%20existing%0Astate-of-the-art%20methods%20by%20a%20notable%20margin%20across%20benchmark%20datasets.%0ANotably%2C%20on%20DomainNet%2C%20ProtoOT%20achieves%20an%20average%20P%40200%20enhancement%20of%2024.44%25%2C%0Aand%20on%20Office-Home%2C%20it%20demonstrates%20a%20P%4015%20improvement%20of%2012.12%25.%20Code%20is%0Aavailable%20at%20https%3A//github.com/HCVLAB/ProtoOT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18411v1&entry.124074799=Read"},
{"title": "CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven\n  Parallel Robots", "author": "Zeqing Zhang and Linhan Yang and Cong Sun and Weiwei Shang and Jia Pan", "abstract": "  When deploying Cable-Driven Parallel Robots (CDPRs) in practice, one of the\nchallenges is kinematic modeling. Unlike serial mechanisms, CDPRs have a simple\ninverse kinematics problem but a complex forward kinematics (FK) issue.\nTherefore, the development of accurate and efficient FK solvers has been a\nprominent research focus in CDPR applications. By observing the topology within\nCDPRs, in this letter, we propose a graph-based representation to model CDPRs\nand introduce CafkNet, a fast and general FK solver, leveraging Graph Neural\nNetwork (GNN). Extensive experiments are conducted on 3D and 2D CDPRs across\nvarious configurations, including under-constrained, fully-constrained, and\nover-constrained cases, in both simulation environments and real-world\nscenarios. The experimental results showcase that CafkNet can learn the\ninternal topological information of CDPRs and accurately solve the FK problem\nas an FK solver. Furthermore, training the CafkNet model on partial\nconfigurations enables zero-shot generalization to other configurations.\nLastly, CafkNet effectively bridges the sim2real gap by using both simulation\ndata and part of real-world data. To the best of our knowledge, it is the first\nstudy that employs the GNN to solve the FK problem for CDPRs.\n", "link": "http://arxiv.org/abs/2402.18420v1", "date": "2024-02-28", "relevancy": 2.007, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5045}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5021}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5003}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CafkNet%3A%20GNN-Empowered%20Forward%20Kinematic%20Modeling%20for%20Cable-Driven%0A%20%20Parallel%20Robots&entry.906535625=Zeqing%20Zhang%20and%20Linhan%20Yang%20and%20Cong%20Sun%20and%20Weiwei%20Shang%20and%20Jia%20Pan&entry.1292438233=%20%20When%20deploying%20Cable-Driven%20Parallel%20Robots%20%28CDPRs%29%20in%20practice%2C%20one%20of%20the%0Achallenges%20is%20kinematic%20modeling.%20Unlike%20serial%20mechanisms%2C%20CDPRs%20have%20a%20simple%0Ainverse%20kinematics%20problem%20but%20a%20complex%20forward%20kinematics%20%28FK%29%20issue.%0ATherefore%2C%20the%20development%20of%20accurate%20and%20efficient%20FK%20solvers%20has%20been%20a%0Aprominent%20research%20focus%20in%20CDPR%20applications.%20By%20observing%20the%20topology%20within%0ACDPRs%2C%20in%20this%20letter%2C%20we%20propose%20a%20graph-based%20representation%20to%20model%20CDPRs%0Aand%20introduce%20CafkNet%2C%20a%20fast%20and%20general%20FK%20solver%2C%20leveraging%20Graph%20Neural%0ANetwork%20%28GNN%29.%20Extensive%20experiments%20are%20conducted%20on%203D%20and%202D%20CDPRs%20across%0Avarious%20configurations%2C%20including%20under-constrained%2C%20fully-constrained%2C%20and%0Aover-constrained%20cases%2C%20in%20both%20simulation%20environments%20and%20real-world%0Ascenarios.%20The%20experimental%20results%20showcase%20that%20CafkNet%20can%20learn%20the%0Ainternal%20topological%20information%20of%20CDPRs%20and%20accurately%20solve%20the%20FK%20problem%0Aas%20an%20FK%20solver.%20Furthermore%2C%20training%20the%20CafkNet%20model%20on%20partial%0Aconfigurations%20enables%20zero-shot%20generalization%20to%20other%20configurations.%0ALastly%2C%20CafkNet%20effectively%20bridges%20the%20sim2real%20gap%20by%20using%20both%20simulation%0Adata%20and%20part%20of%20real-world%20data.%20To%20the%20best%20of%20our%20knowledge%2C%20it%20is%20the%20first%0Astudy%20that%20employs%20the%20GNN%20to%20solve%20the%20FK%20problem%20for%20CDPRs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18420v1&entry.124074799=Read"},
{"title": "Efficient ConvBN Blocks for Transfer Learning and Beyond", "author": "Kaichao You and Guo Qin and Anchang Bao and Meng Cao and Ping Huang and Jiulong Shan and Mingsheng Long", "abstract": "  Convolution-BatchNorm (ConvBN) blocks are integral components in various\ncomputer vision tasks and other domains. A ConvBN block can operate in three\nmodes: Train, Eval, and Deploy. While the Train mode is indispensable for\ntraining models from scratch, the Eval mode is suitable for transfer learning\nand beyond, and the Deploy mode is designed for the deployment of models. This\npaper focuses on the trade-off between stability and efficiency in ConvBN\nblocks: Deploy mode is efficient but suffers from training instability; Eval\nmode is widely used in transfer learning but lacks efficiency. To solve the\ndilemma, we theoretically reveal the reason behind the diminished training\nstability observed in the Deploy mode. Subsequently, we propose a novel Tune\nmode to bridge the gap between Eval mode and Deploy mode. The proposed Tune\nmode is as stable as Eval mode for transfer learning, and its computational\nefficiency closely matches that of the Deploy mode. Through extensive\nexperiments in object detection, classification, and adversarial example\ngeneration across $5$ datasets and $12$ model architectures, we demonstrate\nthat the proposed Tune mode retains the performance while significantly\nreducing GPU memory footprint and training time, thereby contributing efficient\nConvBN blocks for transfer learning and beyond. Our method has been integrated\ninto both PyTorch (general machine learning framework) and MMCV/MMEngine\n(computer vision framework). Practitioners just need one line of code to enjoy\nour efficient ConvBN blocks thanks to PyTorch's builtin machine learning\ncompilers.\n", "link": "http://arxiv.org/abs/2305.11624v2", "date": "2024-02-28", "relevancy": 2.0045, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5601}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.47}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4546}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20ConvBN%20Blocks%20for%20Transfer%20Learning%20and%20Beyond&entry.906535625=Kaichao%20You%20and%20Guo%20Qin%20and%20Anchang%20Bao%20and%20Meng%20Cao%20and%20Ping%20Huang%20and%20Jiulong%20Shan%20and%20Mingsheng%20Long&entry.1292438233=%20%20Convolution-BatchNorm%20%28ConvBN%29%20blocks%20are%20integral%20components%20in%20various%0Acomputer%20vision%20tasks%20and%20other%20domains.%20A%20ConvBN%20block%20can%20operate%20in%20three%0Amodes%3A%20Train%2C%20Eval%2C%20and%20Deploy.%20While%20the%20Train%20mode%20is%20indispensable%20for%0Atraining%20models%20from%20scratch%2C%20the%20Eval%20mode%20is%20suitable%20for%20transfer%20learning%0Aand%20beyond%2C%20and%20the%20Deploy%20mode%20is%20designed%20for%20the%20deployment%20of%20models.%20This%0Apaper%20focuses%20on%20the%20trade-off%20between%20stability%20and%20efficiency%20in%20ConvBN%0Ablocks%3A%20Deploy%20mode%20is%20efficient%20but%20suffers%20from%20training%20instability%3B%20Eval%0Amode%20is%20widely%20used%20in%20transfer%20learning%20but%20lacks%20efficiency.%20To%20solve%20the%0Adilemma%2C%20we%20theoretically%20reveal%20the%20reason%20behind%20the%20diminished%20training%0Astability%20observed%20in%20the%20Deploy%20mode.%20Subsequently%2C%20we%20propose%20a%20novel%20Tune%0Amode%20to%20bridge%20the%20gap%20between%20Eval%20mode%20and%20Deploy%20mode.%20The%20proposed%20Tune%0Amode%20is%20as%20stable%20as%20Eval%20mode%20for%20transfer%20learning%2C%20and%20its%20computational%0Aefficiency%20closely%20matches%20that%20of%20the%20Deploy%20mode.%20Through%20extensive%0Aexperiments%20in%20object%20detection%2C%20classification%2C%20and%20adversarial%20example%0Ageneration%20across%20%245%24%20datasets%20and%20%2412%24%20model%20architectures%2C%20we%20demonstrate%0Athat%20the%20proposed%20Tune%20mode%20retains%20the%20performance%20while%20significantly%0Areducing%20GPU%20memory%20footprint%20and%20training%20time%2C%20thereby%20contributing%20efficient%0AConvBN%20blocks%20for%20transfer%20learning%20and%20beyond.%20Our%20method%20has%20been%20integrated%0Ainto%20both%20PyTorch%20%28general%20machine%20learning%20framework%29%20and%20MMCV/MMEngine%0A%28computer%20vision%20framework%29.%20Practitioners%20just%20need%20one%20line%20of%20code%20to%20enjoy%0Aour%20efficient%20ConvBN%20blocks%20thanks%20to%20PyTorch%27s%20builtin%20machine%20learning%0Acompilers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.11624v2&entry.124074799=Read"},
{"title": "Autonomous Vehicles: Evolution of Artificial Intelligence and Learning\n  Algorithms", "author": "Divya Garikapati and Sneha Sudhir Shetiya", "abstract": "  The advent of autonomous vehicles has heralded a transformative era in\ntransportation, reshaping the landscape of mobility through cutting-edge\ntechnologies. Central to this evolution is the integration of Artificial\nIntelligence (AI) and learning algorithms, propelling vehicles into realms of\nunprecedented autonomy. This paper provides a comprehensive exploration of the\nevolutionary trajectory of AI within autonomous vehicles, tracing the journey\nfrom foundational principles to the most recent advancements. Commencing with a\ncurrent landscape overview, the paper delves into the fundamental role of AI in\nshaping the autonomous decision-making capabilities of vehicles. It elucidates\nthe steps involved in the AI-powered development life cycle in vehicles,\naddressing ethical considerations and bias in AI-driven software development\nfor autonomous vehicles. The study presents statistical insights into the usage\nand types of AI/learning algorithms over the years, showcasing the evolving\nresearch landscape within the automotive industry. Furthermore, the paper\nhighlights the pivotal role of parameters in refining algorithms for both\ntrucks and cars, facilitating vehicles to adapt, learn, and improve performance\nover time. It concludes by outlining different levels of autonomy, elucidating\nthe nuanced usage of AI and learning algorithms, and automating key tasks at\neach level. Additionally, the document discusses the variation in software\npackage sizes across different autonomy levels\n", "link": "http://arxiv.org/abs/2402.17690v2", "date": "2024-02-28", "relevancy": 1.9662, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5209}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4814}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4663}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Vehicles%3A%20Evolution%20of%20Artificial%20Intelligence%20and%20Learning%0A%20%20Algorithms&entry.906535625=Divya%20Garikapati%20and%20Sneha%20Sudhir%20Shetiya&entry.1292438233=%20%20The%20advent%20of%20autonomous%20vehicles%20has%20heralded%20a%20transformative%20era%20in%0Atransportation%2C%20reshaping%20the%20landscape%20of%20mobility%20through%20cutting-edge%0Atechnologies.%20Central%20to%20this%20evolution%20is%20the%20integration%20of%20Artificial%0AIntelligence%20%28AI%29%20and%20learning%20algorithms%2C%20propelling%20vehicles%20into%20realms%20of%0Aunprecedented%20autonomy.%20This%20paper%20provides%20a%20comprehensive%20exploration%20of%20the%0Aevolutionary%20trajectory%20of%20AI%20within%20autonomous%20vehicles%2C%20tracing%20the%20journey%0Afrom%20foundational%20principles%20to%20the%20most%20recent%20advancements.%20Commencing%20with%20a%0Acurrent%20landscape%20overview%2C%20the%20paper%20delves%20into%20the%20fundamental%20role%20of%20AI%20in%0Ashaping%20the%20autonomous%20decision-making%20capabilities%20of%20vehicles.%20It%20elucidates%0Athe%20steps%20involved%20in%20the%20AI-powered%20development%20life%20cycle%20in%20vehicles%2C%0Aaddressing%20ethical%20considerations%20and%20bias%20in%20AI-driven%20software%20development%0Afor%20autonomous%20vehicles.%20The%20study%20presents%20statistical%20insights%20into%20the%20usage%0Aand%20types%20of%20AI/learning%20algorithms%20over%20the%20years%2C%20showcasing%20the%20evolving%0Aresearch%20landscape%20within%20the%20automotive%20industry.%20Furthermore%2C%20the%20paper%0Ahighlights%20the%20pivotal%20role%20of%20parameters%20in%20refining%20algorithms%20for%20both%0Atrucks%20and%20cars%2C%20facilitating%20vehicles%20to%20adapt%2C%20learn%2C%20and%20improve%20performance%0Aover%20time.%20It%20concludes%20by%20outlining%20different%20levels%20of%20autonomy%2C%20elucidating%0Athe%20nuanced%20usage%20of%20AI%20and%20learning%20algorithms%2C%20and%20automating%20key%20tasks%20at%0Aeach%20level.%20Additionally%2C%20the%20document%20discusses%20the%20variation%20in%20software%0Apackage%20sizes%20across%20different%20autonomy%20levels%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17690v2&entry.124074799=Read"},
{"title": "SegForestNet: Spatial-Partitioning-Based Aerial Image Segmentation", "author": "Daniel Gritzner and J\u00f6rn Ostermann", "abstract": "  Aerial image segmentation is the basis for applications such as automatically\ncreating maps or tracking deforestation. In true orthophotos, which are often\nused in these applications, many objects and regions can be approximated well\nby polygons. However, this fact is rarely exploited by state-of-the-art\nsemantic segmentation models. Instead, most models allow unnecessary degrees of\nfreedom in their predictions by allowing arbitrary region shapes. We therefore\npresent a refinement of our deep learning model which predicts binary space\npartitioning trees, an efficient polygon representation. The refinements\ninclude a new feature decoder architecture and a new differentiable BSP tree\nrenderer which both avoid vanishing gradients. Additionally, we designed a\nnovel loss function specifically designed to improve the spatial partitioning\ndefined by the predicted trees. Furthermore, our expanded model can predict\nmultiple trees at once and thus can predict class-specific segmentations. As an\nadditional contribution, we investigate the impact of a non-optimal training\nprocess in comparison to an optimized training process. While model\narchitectures optimized for aerial images, such as PFNet or our own model, show\nan advantage under non-optimal conditions, this advantage disappears under\noptimal training conditions. Despite this observation, our model still makes\nbetter predictions for small rectangular objects, e.g., cars.\n", "link": "http://arxiv.org/abs/2302.01585v2", "date": "2024-02-28", "relevancy": 1.963, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5152}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4802}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4706}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegForestNet%3A%20Spatial-Partitioning-Based%20Aerial%20Image%20Segmentation&entry.906535625=Daniel%20Gritzner%20and%20J%C3%B6rn%20Ostermann&entry.1292438233=%20%20Aerial%20image%20segmentation%20is%20the%20basis%20for%20applications%20such%20as%20automatically%0Acreating%20maps%20or%20tracking%20deforestation.%20In%20true%20orthophotos%2C%20which%20are%20often%0Aused%20in%20these%20applications%2C%20many%20objects%20and%20regions%20can%20be%20approximated%20well%0Aby%20polygons.%20However%2C%20this%20fact%20is%20rarely%20exploited%20by%20state-of-the-art%0Asemantic%20segmentation%20models.%20Instead%2C%20most%20models%20allow%20unnecessary%20degrees%20of%0Afreedom%20in%20their%20predictions%20by%20allowing%20arbitrary%20region%20shapes.%20We%20therefore%0Apresent%20a%20refinement%20of%20our%20deep%20learning%20model%20which%20predicts%20binary%20space%0Apartitioning%20trees%2C%20an%20efficient%20polygon%20representation.%20The%20refinements%0Ainclude%20a%20new%20feature%20decoder%20architecture%20and%20a%20new%20differentiable%20BSP%20tree%0Arenderer%20which%20both%20avoid%20vanishing%20gradients.%20Additionally%2C%20we%20designed%20a%0Anovel%20loss%20function%20specifically%20designed%20to%20improve%20the%20spatial%20partitioning%0Adefined%20by%20the%20predicted%20trees.%20Furthermore%2C%20our%20expanded%20model%20can%20predict%0Amultiple%20trees%20at%20once%20and%20thus%20can%20predict%20class-specific%20segmentations.%20As%20an%0Aadditional%20contribution%2C%20we%20investigate%20the%20impact%20of%20a%20non-optimal%20training%0Aprocess%20in%20comparison%20to%20an%20optimized%20training%20process.%20While%20model%0Aarchitectures%20optimized%20for%20aerial%20images%2C%20such%20as%20PFNet%20or%20our%20own%20model%2C%20show%0Aan%20advantage%20under%20non-optimal%20conditions%2C%20this%20advantage%20disappears%20under%0Aoptimal%20training%20conditions.%20Despite%20this%20observation%2C%20our%20model%20still%20makes%0Abetter%20predictions%20for%20small%20rectangular%20objects%2C%20e.g.%2C%20cars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.01585v2&entry.124074799=Read"},
{"title": "Generalizability Under Sensor Failure: Tokenization + Transformers\n  Enable More Robust Latent Spaces", "author": "Geeling Chau and Yujin An and Ahamed Raffey Iqbal and Soon-Jo Chung and Yisong Yue and Sabera Talukder", "abstract": "  A major goal in neuroscience is to discover neural data representations that\ngeneralize. This goal is challenged by variability along recording sessions\n(e.g. environment), subjects (e.g. varying neural structures), and sensors\n(e.g. sensor noise), among others. Recent work has begun to address\ngeneralization across sessions and subjects, but few study robustness to sensor\nfailure which is highly prevalent in neuroscience experiments. In order to\naddress these generalizability dimensions we first collect our own\nelectroencephalography dataset with numerous sessions, subjects, and sensors,\nthen study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM\n(Talukder et al., 2024). EEGNet is a widely used convolutional neural network,\nwhile TOTEM is a discrete time series tokenizer and transformer model. We find\nthat TOTEM outperforms or matches EEGNet across all generalizability cases.\nFinally through analysis of TOTEM's latent codebook we observe that\ntokenization enables generalization.\n", "link": "http://arxiv.org/abs/2402.18546v1", "date": "2024-02-28", "relevancy": 1.9548, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4859}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4778}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizability%20Under%20Sensor%20Failure%3A%20Tokenization%20%2B%20Transformers%0A%20%20Enable%20More%20Robust%20Latent%20Spaces&entry.906535625=Geeling%20Chau%20and%20Yujin%20An%20and%20Ahamed%20Raffey%20Iqbal%20and%20Soon-Jo%20Chung%20and%20Yisong%20Yue%20and%20Sabera%20Talukder&entry.1292438233=%20%20A%20major%20goal%20in%20neuroscience%20is%20to%20discover%20neural%20data%20representations%20that%0Ageneralize.%20This%20goal%20is%20challenged%20by%20variability%20along%20recording%20sessions%0A%28e.g.%20environment%29%2C%20subjects%20%28e.g.%20varying%20neural%20structures%29%2C%20and%20sensors%0A%28e.g.%20sensor%20noise%29%2C%20among%20others.%20Recent%20work%20has%20begun%20to%20address%0Ageneralization%20across%20sessions%20and%20subjects%2C%20but%20few%20study%20robustness%20to%20sensor%0Afailure%20which%20is%20highly%20prevalent%20in%20neuroscience%20experiments.%20In%20order%20to%0Aaddress%20these%20generalizability%20dimensions%20we%20first%20collect%20our%20own%0Aelectroencephalography%20dataset%20with%20numerous%20sessions%2C%20subjects%2C%20and%20sensors%2C%0Athen%20study%20two%20time%20series%20models%3A%20EEGNet%20%28Lawhern%20et%20al.%2C%202018%29%20and%20TOTEM%0A%28Talukder%20et%20al.%2C%202024%29.%20EEGNet%20is%20a%20widely%20used%20convolutional%20neural%20network%2C%0Awhile%20TOTEM%20is%20a%20discrete%20time%20series%20tokenizer%20and%20transformer%20model.%20We%20find%0Athat%20TOTEM%20outperforms%20or%20matches%20EEGNet%20across%20all%20generalizability%20cases.%0AFinally%20through%20analysis%20of%20TOTEM%27s%20latent%20codebook%20we%20observe%20that%0Atokenization%20enables%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18546v1&entry.124074799=Read"},
{"title": "Probabilistic Bayesian optimal experimental design using conditional\n  normalizing flows", "author": "Rafael Orozco and Felix J. Herrmann and Peng Chen", "abstract": "  Bayesian optimal experimental design (OED) seeks to conduct the most\ninformative experiment under budget constraints to update the prior knowledge\nof a system to its posterior from the experimental data in a Bayesian\nframework. Such problems are computationally challenging because of (1)\nexpensive and repeated evaluation of some optimality criterion that typically\ninvolves a double integration with respect to both the system parameters and\nthe experimental data, (2) suffering from the curse-of-dimensionality when the\nsystem parameters and design variables are high-dimensional, (3) the\noptimization is combinatorial and highly non-convex if the design variables are\nbinary, often leading to non-robust designs. To make the solution of the\nBayesian OED problem efficient, scalable, and robust for practical\napplications, we propose a novel joint optimization approach. This approach\nperforms simultaneous (1) training of a scalable conditional normalizing flow\n(CNF) to efficiently maximize the expected information gain (EIG) of a jointly\nlearned experimental design (2) optimization of a probabilistic formulation of\nthe binary experimental design with a Bernoulli distribution. We demonstrate\nthe performance of our proposed method for a practical MRI data acquisition\nproblem, one of the most challenging Bayesian OED problems that has\nhigh-dimensional (320 $\\times$ 320) parameters at high image resolution,\nhigh-dimensional (640 $\\times$ 386) observations, and binary mask designs to\nselect the most informative observations.\n", "link": "http://arxiv.org/abs/2402.18337v1", "date": "2024-02-28", "relevancy": 1.9432, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5085}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4983}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4642}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Bayesian%20optimal%20experimental%20design%20using%20conditional%0A%20%20normalizing%20flows&entry.906535625=Rafael%20Orozco%20and%20Felix%20J.%20Herrmann%20and%20Peng%20Chen&entry.1292438233=%20%20Bayesian%20optimal%20experimental%20design%20%28OED%29%20seeks%20to%20conduct%20the%20most%0Ainformative%20experiment%20under%20budget%20constraints%20to%20update%20the%20prior%20knowledge%0Aof%20a%20system%20to%20its%20posterior%20from%20the%20experimental%20data%20in%20a%20Bayesian%0Aframework.%20Such%20problems%20are%20computationally%20challenging%20because%20of%20%281%29%0Aexpensive%20and%20repeated%20evaluation%20of%20some%20optimality%20criterion%20that%20typically%0Ainvolves%20a%20double%20integration%20with%20respect%20to%20both%20the%20system%20parameters%20and%0Athe%20experimental%20data%2C%20%282%29%20suffering%20from%20the%20curse-of-dimensionality%20when%20the%0Asystem%20parameters%20and%20design%20variables%20are%20high-dimensional%2C%20%283%29%20the%0Aoptimization%20is%20combinatorial%20and%20highly%20non-convex%20if%20the%20design%20variables%20are%0Abinary%2C%20often%20leading%20to%20non-robust%20designs.%20To%20make%20the%20solution%20of%20the%0ABayesian%20OED%20problem%20efficient%2C%20scalable%2C%20and%20robust%20for%20practical%0Aapplications%2C%20we%20propose%20a%20novel%20joint%20optimization%20approach.%20This%20approach%0Aperforms%20simultaneous%20%281%29%20training%20of%20a%20scalable%20conditional%20normalizing%20flow%0A%28CNF%29%20to%20efficiently%20maximize%20the%20expected%20information%20gain%20%28EIG%29%20of%20a%20jointly%0Alearned%20experimental%20design%20%282%29%20optimization%20of%20a%20probabilistic%20formulation%20of%0Athe%20binary%20experimental%20design%20with%20a%20Bernoulli%20distribution.%20We%20demonstrate%0Athe%20performance%20of%20our%20proposed%20method%20for%20a%20practical%20MRI%20data%20acquisition%0Aproblem%2C%20one%20of%20the%20most%20challenging%20Bayesian%20OED%20problems%20that%20has%0Ahigh-dimensional%20%28320%20%24%5Ctimes%24%20320%29%20parameters%20at%20high%20image%20resolution%2C%0Ahigh-dimensional%20%28640%20%24%5Ctimes%24%20386%29%20observations%2C%20and%20binary%20mask%20designs%20to%0Aselect%20the%20most%20informative%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18337v1&entry.124074799=Read"},
{"title": "Learning in Deep Factor Graphs with Gaussian Belief Propagation", "author": "Seth Nabarro and Mark van der Wilk and Andrew J Davison", "abstract": "  We propose an approach to do learning in Gaussian factor graphs. We treat all\nrelevant quantities (inputs, outputs, parameters, latents) as random variables\nin a graphical model, and view both training and prediction as inference\nproblems with different observed nodes. Our experiments show that these\nproblems can be efficiently solved with belief propagation (BP), whose updates\nare inherently local, presenting exciting opportunities for distributed and\nasynchronous training. Our approach can be scaled to deep networks and provides\na natural means to do continual learning: use the BP-estimated parameter\nmarginals of the current task as parameter priors for the next. On a video\ndenoising task we demonstrate the benefit of learnable parameters over a\nclassical factor graph approach and we show encouraging performance of deep\nfactor graphs for continual image classification.\n", "link": "http://arxiv.org/abs/2311.14649v2", "date": "2024-02-28", "relevancy": 1.9156, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.512}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4798}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4648}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20in%20Deep%20Factor%20Graphs%20with%20Gaussian%20Belief%20Propagation&entry.906535625=Seth%20Nabarro%20and%20Mark%20van%20der%20Wilk%20and%20Andrew%20J%20Davison&entry.1292438233=%20%20We%20propose%20an%20approach%20to%20do%20learning%20in%20Gaussian%20factor%20graphs.%20We%20treat%20all%0Arelevant%20quantities%20%28inputs%2C%20outputs%2C%20parameters%2C%20latents%29%20as%20random%20variables%0Ain%20a%20graphical%20model%2C%20and%20view%20both%20training%20and%20prediction%20as%20inference%0Aproblems%20with%20different%20observed%20nodes.%20Our%20experiments%20show%20that%20these%0Aproblems%20can%20be%20efficiently%20solved%20with%20belief%20propagation%20%28BP%29%2C%20whose%20updates%0Aare%20inherently%20local%2C%20presenting%20exciting%20opportunities%20for%20distributed%20and%0Aasynchronous%20training.%20Our%20approach%20can%20be%20scaled%20to%20deep%20networks%20and%20provides%0Aa%20natural%20means%20to%20do%20continual%20learning%3A%20use%20the%20BP-estimated%20parameter%0Amarginals%20of%20the%20current%20task%20as%20parameter%20priors%20for%20the%20next.%20On%20a%20video%0Adenoising%20task%20we%20demonstrate%20the%20benefit%20of%20learnable%20parameters%20over%20a%0Aclassical%20factor%20graph%20approach%20and%20we%20show%20encouraging%20performance%20of%20deep%0Afactor%20graphs%20for%20continual%20image%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14649v2&entry.124074799=Read"},
{"title": "Evolving machine learning workflows through interactive AutoML", "author": "Rafael Barbudo and Aurora Ram\u00edrez and Jos\u00e9 Ra\u00fal Romero", "abstract": "  Automatic workflow composition (AWC) is a relevant problem in automated\nmachine learning (AutoML) that allows finding suitable sequences of\npreprocessing and prediction models together with their optimal\nhyperparameters. This problem can be solved using evolutionary algorithms and,\nin particular, grammar-guided genetic programming (G3P). Current G3P approaches\nto AWC define a fixed grammar that formally specifies how workflow elements can\nbe combined and which algorithms can be included. In this paper we present\n\\ourmethod, an interactive G3P algorithm that allows users to dynamically\nmodify the grammar to prune the search space and focus on their regions of\ninterest. Our proposal is the first to combine the advantages of a G3P method\nwith ideas from interactive optimisation and human-guided machine learning, an\narea little explored in the context of AutoML. To evaluate our approach, we\npresent an experimental study in which 20 participants interact with \\ourmethod\nto evolve workflows according to their preferences. Our results confirm that\nthe collaboration between \\ourmethod and humans allows us to find\nhigh-performance workflows in terms of accuracy that require less tuning time\nthan those found without human intervention.\n", "link": "http://arxiv.org/abs/2402.18505v1", "date": "2024-02-28", "relevancy": 1.9114, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4966}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4914}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4568}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolving%20machine%20learning%20workflows%20through%20interactive%20AutoML&entry.906535625=Rafael%20Barbudo%20and%20Aurora%20Ram%C3%ADrez%20and%20Jos%C3%A9%20Ra%C3%BAl%20Romero&entry.1292438233=%20%20Automatic%20workflow%20composition%20%28AWC%29%20is%20a%20relevant%20problem%20in%20automated%0Amachine%20learning%20%28AutoML%29%20that%20allows%20finding%20suitable%20sequences%20of%0Apreprocessing%20and%20prediction%20models%20together%20with%20their%20optimal%0Ahyperparameters.%20This%20problem%20can%20be%20solved%20using%20evolutionary%20algorithms%20and%2C%0Ain%20particular%2C%20grammar-guided%20genetic%20programming%20%28G3P%29.%20Current%20G3P%20approaches%0Ato%20AWC%20define%20a%20fixed%20grammar%20that%20formally%20specifies%20how%20workflow%20elements%20can%0Abe%20combined%20and%20which%20algorithms%20can%20be%20included.%20In%20this%20paper%20we%20present%0A%5Courmethod%2C%20an%20interactive%20G3P%20algorithm%20that%20allows%20users%20to%20dynamically%0Amodify%20the%20grammar%20to%20prune%20the%20search%20space%20and%20focus%20on%20their%20regions%20of%0Ainterest.%20Our%20proposal%20is%20the%20first%20to%20combine%20the%20advantages%20of%20a%20G3P%20method%0Awith%20ideas%20from%20interactive%20optimisation%20and%20human-guided%20machine%20learning%2C%20an%0Aarea%20little%20explored%20in%20the%20context%20of%20AutoML.%20To%20evaluate%20our%20approach%2C%20we%0Apresent%20an%20experimental%20study%20in%20which%2020%20participants%20interact%20with%20%5Courmethod%0Ato%20evolve%20workflows%20according%20to%20their%20preferences.%20Our%20results%20confirm%20that%0Athe%20collaboration%20between%20%5Courmethod%20and%20humans%20allows%20us%20to%20find%0Ahigh-performance%20workflows%20in%20terms%20of%20accuracy%20that%20require%20less%20tuning%20time%0Athan%20those%20found%20without%20human%20intervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18505v1&entry.124074799=Read"},
{"title": "A Relational Inductive Bias for Dimensional Abstraction in Neural\n  Networks", "author": "Declan Campbell and Jonathan D. Cohen", "abstract": "  The human cognitive system exhibits remarkable flexibility and generalization\ncapabilities, partly due to its ability to form low-dimensional, compositional\nrepresentations of the environment. In contrast, standard neural network\narchitectures often struggle with abstract reasoning tasks, overfitting, and\nrequiring extensive data for training. This paper investigates the impact of\nthe relational bottleneck -- a mechanism that focuses processing on relations\namong inputs -- on the learning of factorized representations conducive to\ncompositional coding and the attendant flexibility of processing. We\ndemonstrate that such a bottleneck not only improves generalization and\nlearning efficiency, but also aligns network performance with human-like\nbehavioral biases. Networks trained with the relational bottleneck developed\northogonal representations of feature dimensions latent in the dataset,\nreflecting the factorized structure thought to underlie human cognitive\nflexibility. Moreover, the relational network mimics human biases towards\nregularity without pre-specified symbolic primitives, suggesting that the\nbottleneck fosters the emergence of abstract representations that confer\nflexibility akin to symbols.\n", "link": "http://arxiv.org/abs/2402.18426v1", "date": "2024-02-28", "relevancy": 1.8988, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4976}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4785}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4503}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Relational%20Inductive%20Bias%20for%20Dimensional%20Abstraction%20in%20Neural%0A%20%20Networks&entry.906535625=Declan%20Campbell%20and%20Jonathan%20D.%20Cohen&entry.1292438233=%20%20The%20human%20cognitive%20system%20exhibits%20remarkable%20flexibility%20and%20generalization%0Acapabilities%2C%20partly%20due%20to%20its%20ability%20to%20form%20low-dimensional%2C%20compositional%0Arepresentations%20of%20the%20environment.%20In%20contrast%2C%20standard%20neural%20network%0Aarchitectures%20often%20struggle%20with%20abstract%20reasoning%20tasks%2C%20overfitting%2C%20and%0Arequiring%20extensive%20data%20for%20training.%20This%20paper%20investigates%20the%20impact%20of%0Athe%20relational%20bottleneck%20--%20a%20mechanism%20that%20focuses%20processing%20on%20relations%0Aamong%20inputs%20--%20on%20the%20learning%20of%20factorized%20representations%20conducive%20to%0Acompositional%20coding%20and%20the%20attendant%20flexibility%20of%20processing.%20We%0Ademonstrate%20that%20such%20a%20bottleneck%20not%20only%20improves%20generalization%20and%0Alearning%20efficiency%2C%20but%20also%20aligns%20network%20performance%20with%20human-like%0Abehavioral%20biases.%20Networks%20trained%20with%20the%20relational%20bottleneck%20developed%0Aorthogonal%20representations%20of%20feature%20dimensions%20latent%20in%20the%20dataset%2C%0Areflecting%20the%20factorized%20structure%20thought%20to%20underlie%20human%20cognitive%0Aflexibility.%20Moreover%2C%20the%20relational%20network%20mimics%20human%20biases%20towards%0Aregularity%20without%20pre-specified%20symbolic%20primitives%2C%20suggesting%20that%20the%0Abottleneck%20fosters%20the%20emergence%20of%20abstract%20representations%20that%20confer%0Aflexibility%20akin%20to%20symbols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18426v1&entry.124074799=Read"},
{"title": "Large Language Models As Evolution Strategies", "author": "Robert Tjarko Lange and Yingtao Tian and Yujin Tang", "abstract": "  Large Transformer models are capable of implementing a plethora of so-called\nin-context learning algorithms. These include gradient descent, classification,\nsequence completion, transformation, and improvement. In this work, we\ninvestigate whether large language models (LLMs), which never explicitly\nencountered the task of black-box optimization, are in principle capable of\nimplementing evolutionary optimization algorithms. While previous works have\nsolely focused on language-based task specification, we move forward and focus\non the zero-shot application of LLMs to black-box optimization. We introduce a\nnovel prompting strategy, consisting of least-to-most sorting of discretized\npopulation members and querying the LLM to propose an improvement to the mean\nstatistic, i.e. perform a type of black-box recombination operation.\nEmpirically, we find that our setup allows the user to obtain an LLM-based\nevolution strategy, which we call `EvoLLM', that robustly outperforms baseline\nalgorithms such as random search and Gaussian Hill Climbing on synthetic BBOB\nfunctions as well as small neuroevolution tasks. Hence, LLMs can act as\n`plug-in' in-context recombination operators. We provide several comparative\nstudies of the LLM's model size, prompt strategy, and context construction.\nFinally, we show that one can flexibly improve EvoLLM's performance by\nproviding teacher algorithm information via instruction fine-tuning on\npreviously collected teacher optimization trajectories.\n", "link": "http://arxiv.org/abs/2402.18381v1", "date": "2024-02-28", "relevancy": 1.8863, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4844}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4703}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4678}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20As%20Evolution%20Strategies&entry.906535625=Robert%20Tjarko%20Lange%20and%20Yingtao%20Tian%20and%20Yujin%20Tang&entry.1292438233=%20%20Large%20Transformer%20models%20are%20capable%20of%20implementing%20a%20plethora%20of%20so-called%0Ain-context%20learning%20algorithms.%20These%20include%20gradient%20descent%2C%20classification%2C%0Asequence%20completion%2C%20transformation%2C%20and%20improvement.%20In%20this%20work%2C%20we%0Ainvestigate%20whether%20large%20language%20models%20%28LLMs%29%2C%20which%20never%20explicitly%0Aencountered%20the%20task%20of%20black-box%20optimization%2C%20are%20in%20principle%20capable%20of%0Aimplementing%20evolutionary%20optimization%20algorithms.%20While%20previous%20works%20have%0Asolely%20focused%20on%20language-based%20task%20specification%2C%20we%20move%20forward%20and%20focus%0Aon%20the%20zero-shot%20application%20of%20LLMs%20to%20black-box%20optimization.%20We%20introduce%20a%0Anovel%20prompting%20strategy%2C%20consisting%20of%20least-to-most%20sorting%20of%20discretized%0Apopulation%20members%20and%20querying%20the%20LLM%20to%20propose%20an%20improvement%20to%20the%20mean%0Astatistic%2C%20i.e.%20perform%20a%20type%20of%20black-box%20recombination%20operation.%0AEmpirically%2C%20we%20find%20that%20our%20setup%20allows%20the%20user%20to%20obtain%20an%20LLM-based%0Aevolution%20strategy%2C%20which%20we%20call%20%60EvoLLM%27%2C%20that%20robustly%20outperforms%20baseline%0Aalgorithms%20such%20as%20random%20search%20and%20Gaussian%20Hill%20Climbing%20on%20synthetic%20BBOB%0Afunctions%20as%20well%20as%20small%20neuroevolution%20tasks.%20Hence%2C%20LLMs%20can%20act%20as%0A%60plug-in%27%20in-context%20recombination%20operators.%20We%20provide%20several%20comparative%0Astudies%20of%20the%20LLM%27s%20model%20size%2C%20prompt%20strategy%2C%20and%20context%20construction.%0AFinally%2C%20we%20show%20that%20one%20can%20flexibly%20improve%20EvoLLM%27s%20performance%20by%0Aproviding%20teacher%20algorithm%20information%20via%20instruction%20fine-tuning%20on%0Apreviously%20collected%20teacher%20optimization%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18381v1&entry.124074799=Read"},
{"title": "MiniLLM: Knowledge Distillation of Large Language Models", "author": "Yuxian Gu and Li Dong and Furu Wei and Minlie Huang", "abstract": "  Knowledge Distillation (KD) is a promising technique for reducing the high\ncomputational demand of large language models (LLMs). However, previous KD\nmethods are primarily applied to white-box classification models or training\nsmall models to imitate black-box model APIs like ChatGPT. How to effectively\ndistill the knowledge of white-box LLMs into small models is still\nunder-explored, which becomes more important with the prosperity of open-source\nLLMs. In this work, we propose a KD approach that distills LLMs into smaller\nlanguage models. We first replace the forward Kullback-Leibler divergence (KLD)\nobjective in the standard KD approaches with reverse KLD, which is more\nsuitable for KD on generative language models, to prevent the student model\nfrom overestimating the low-probability regions of the teacher distribution.\nThen, we derive an effective optimization approach to learn this objective. The\nstudent models are named MiniLLM. Extensive experiments in the\ninstruction-following setting show that MiniLLM generates more precise\nresponses with higher overall quality, lower exposure bias, better calibration,\nand higher long-text generation performance than the baselines. Our method is\nscalable for different model families with 120M to 13B parameters. Our code,\ndata, and model checkpoints can be found in\n\\url{https://github.com/microsoft/LMOps/tree/main/minillm}.\n", "link": "http://arxiv.org/abs/2306.08543v2", "date": "2024-02-28", "relevancy": 1.8855, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4972}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4666}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4658}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniLLM%3A%20Knowledge%20Distillation%20of%20Large%20Language%20Models&entry.906535625=Yuxian%20Gu%20and%20Li%20Dong%20and%20Furu%20Wei%20and%20Minlie%20Huang&entry.1292438233=%20%20Knowledge%20Distillation%20%28KD%29%20is%20a%20promising%20technique%20for%20reducing%20the%20high%0Acomputational%20demand%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20previous%20KD%0Amethods%20are%20primarily%20applied%20to%20white-box%20classification%20models%20or%20training%0Asmall%20models%20to%20imitate%20black-box%20model%20APIs%20like%20ChatGPT.%20How%20to%20effectively%0Adistill%20the%20knowledge%20of%20white-box%20LLMs%20into%20small%20models%20is%20still%0Aunder-explored%2C%20which%20becomes%20more%20important%20with%20the%20prosperity%20of%20open-source%0ALLMs.%20In%20this%20work%2C%20we%20propose%20a%20KD%20approach%20that%20distills%20LLMs%20into%20smaller%0Alanguage%20models.%20We%20first%20replace%20the%20forward%20Kullback-Leibler%20divergence%20%28KLD%29%0Aobjective%20in%20the%20standard%20KD%20approaches%20with%20reverse%20KLD%2C%20which%20is%20more%0Asuitable%20for%20KD%20on%20generative%20language%20models%2C%20to%20prevent%20the%20student%20model%0Afrom%20overestimating%20the%20low-probability%20regions%20of%20the%20teacher%20distribution.%0AThen%2C%20we%20derive%20an%20effective%20optimization%20approach%20to%20learn%20this%20objective.%20The%0Astudent%20models%20are%20named%20MiniLLM.%20Extensive%20experiments%20in%20the%0Ainstruction-following%20setting%20show%20that%20MiniLLM%20generates%20more%20precise%0Aresponses%20with%20higher%20overall%20quality%2C%20lower%20exposure%20bias%2C%20better%20calibration%2C%0Aand%20higher%20long-text%20generation%20performance%20than%20the%20baselines.%20Our%20method%20is%0Ascalable%20for%20different%20model%20families%20with%20120M%20to%2013B%20parameters.%20Our%20code%2C%0Adata%2C%20and%20model%20checkpoints%20can%20be%20found%20in%0A%5Curl%7Bhttps%3A//github.com/microsoft/LMOps/tree/main/minillm%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.08543v2&entry.124074799=Read"},
{"title": "Rethinking Centered Kernel Alignment in Knowledge Distillation", "author": "Zikai Zhou and Yunhang Shen and Shitong Shao and Linrui Gong and Shaohui Lin", "abstract": "  Knowledge distillation has emerged as a highly effective method for bridging\nthe representation discrepancy between large-scale models and lightweight\nmodels. Prevalent approaches involve leveraging appropriate metrics to minimize\nthe divergence or distance between the knowledge extracted from the teacher\nmodel and the knowledge learned by the student model. Centered Kernel Alignment\n(CKA) is widely used to measure representation similarity and has been applied\nin several knowledge distillation methods. However, these methods are complex\nand fail to uncover the essence of CKA, thus not answering the question of how\nto use CKA to achieve simple and effective distillation properly. This paper\nfirst provides a theoretical perspective to illustrate the effectiveness of\nCKA, which decouples CKA to the upper bound of Maximum Mean Discrepancy~(MMD)\nand a constant term. Drawing from this, we propose a novel Relation-Centered\nKernel Alignment~(RCKA) framework, which practically establishes a connection\nbetween CKA and MMD. Furthermore, we dynamically customize the application of\nCKA based on the characteristics of each task, with less computational source\nyet comparable performance than the previous methods. The extensive experiments\non the CIFAR-100, ImageNet-1k, and MS-COCO demonstrate that our method achieves\nstate-of-the-art performance on almost all teacher-student pairs for image\nclassification and object detection, validating the effectiveness of our\napproaches.\n", "link": "http://arxiv.org/abs/2401.11824v2", "date": "2024-02-28", "relevancy": 1.8836, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5016}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4527}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4475}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Centered%20Kernel%20Alignment%20in%20Knowledge%20Distillation&entry.906535625=Zikai%20Zhou%20and%20Yunhang%20Shen%20and%20Shitong%20Shao%20and%20Linrui%20Gong%20and%20Shaohui%20Lin&entry.1292438233=%20%20Knowledge%20distillation%20has%20emerged%20as%20a%20highly%20effective%20method%20for%20bridging%0Athe%20representation%20discrepancy%20between%20large-scale%20models%20and%20lightweight%0Amodels.%20Prevalent%20approaches%20involve%20leveraging%20appropriate%20metrics%20to%20minimize%0Athe%20divergence%20or%20distance%20between%20the%20knowledge%20extracted%20from%20the%20teacher%0Amodel%20and%20the%20knowledge%20learned%20by%20the%20student%20model.%20Centered%20Kernel%20Alignment%0A%28CKA%29%20is%20widely%20used%20to%20measure%20representation%20similarity%20and%20has%20been%20applied%0Ain%20several%20knowledge%20distillation%20methods.%20However%2C%20these%20methods%20are%20complex%0Aand%20fail%20to%20uncover%20the%20essence%20of%20CKA%2C%20thus%20not%20answering%20the%20question%20of%20how%0Ato%20use%20CKA%20to%20achieve%20simple%20and%20effective%20distillation%20properly.%20This%20paper%0Afirst%20provides%20a%20theoretical%20perspective%20to%20illustrate%20the%20effectiveness%20of%0ACKA%2C%20which%20decouples%20CKA%20to%20the%20upper%20bound%20of%20Maximum%20Mean%20Discrepancy~%28MMD%29%0Aand%20a%20constant%20term.%20Drawing%20from%20this%2C%20we%20propose%20a%20novel%20Relation-Centered%0AKernel%20Alignment~%28RCKA%29%20framework%2C%20which%20practically%20establishes%20a%20connection%0Abetween%20CKA%20and%20MMD.%20Furthermore%2C%20we%20dynamically%20customize%20the%20application%20of%0ACKA%20based%20on%20the%20characteristics%20of%20each%20task%2C%20with%20less%20computational%20source%0Ayet%20comparable%20performance%20than%20the%20previous%20methods.%20The%20extensive%20experiments%0Aon%20the%20CIFAR-100%2C%20ImageNet-1k%2C%20and%20MS-COCO%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20on%20almost%20all%20teacher-student%20pairs%20for%20image%0Aclassification%20and%20object%20detection%2C%20validating%20the%20effectiveness%20of%20our%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11824v2&entry.124074799=Read"},
{"title": "Implicit Bias of Next-Token Prediction", "author": "Christos Thrampoulidis", "abstract": "  Next-token prediction (NTP), the go-to training paradigm in training large\nlanguage models, involves predicting the next token in a sequence. Departing\nfrom traditional one-hot classification, in NTP, multiple tokens with varying\nfrequencies follow each given context. This work frames NTP training as\ncross-entropy minimization over distinct contexts, each associated with a\nsparse empirical probability vector across a finite vocabulary. It then\naddresses the following question: do gradient-based optimizers exhibit a bias\ntowards solutions with specific structure as the NTP training loss reaches its\nlower bound (entropy)? Specifically, for linear NTP models trained using\ngradient descent (GD), we make the following contributions: Firstly, we\ndetermine NTP-separability conditions on the data, under which GD can attain\nits lower bound. We also demonstrate that these conditions hold under\noverparameterization. Secondly, we establish that the parameters of GD\nprojected onto an appropriate data subspace converge to the unique solution of\na system of linear equations, which requires the logits' difference of\nin-support tokens to be equal to the log-ratio of their respective\nprobabilities. Meanwhile, on the orthogonal subspace, the parameters diverge\nand converge in the direction of the solution of a max-margin quadratic\nprogram, minimizing the Euclidean norm of parameters satisfying the\n\\NTP-separability conditions. Akin to prior research on implicit bias of\none-hot classification, our work opens exciting avenues for future research\nthat can lead to better understanding optimization, generalization and\nrobustness principles of models trained with NTP.\n", "link": "http://arxiv.org/abs/2402.18551v1", "date": "2024-02-28", "relevancy": 1.8609, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4513}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4476}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Bias%20of%20Next-Token%20Prediction&entry.906535625=Christos%20Thrampoulidis&entry.1292438233=%20%20Next-token%20prediction%20%28NTP%29%2C%20the%20go-to%20training%20paradigm%20in%20training%20large%0Alanguage%20models%2C%20involves%20predicting%20the%20next%20token%20in%20a%20sequence.%20Departing%0Afrom%20traditional%20one-hot%20classification%2C%20in%20NTP%2C%20multiple%20tokens%20with%20varying%0Afrequencies%20follow%20each%20given%20context.%20This%20work%20frames%20NTP%20training%20as%0Across-entropy%20minimization%20over%20distinct%20contexts%2C%20each%20associated%20with%20a%0Asparse%20empirical%20probability%20vector%20across%20a%20finite%20vocabulary.%20It%20then%0Aaddresses%20the%20following%20question%3A%20do%20gradient-based%20optimizers%20exhibit%20a%20bias%0Atowards%20solutions%20with%20specific%20structure%20as%20the%20NTP%20training%20loss%20reaches%20its%0Alower%20bound%20%28entropy%29%3F%20Specifically%2C%20for%20linear%20NTP%20models%20trained%20using%0Agradient%20descent%20%28GD%29%2C%20we%20make%20the%20following%20contributions%3A%20Firstly%2C%20we%0Adetermine%20NTP-separability%20conditions%20on%20the%20data%2C%20under%20which%20GD%20can%20attain%0Aits%20lower%20bound.%20We%20also%20demonstrate%20that%20these%20conditions%20hold%20under%0Aoverparameterization.%20Secondly%2C%20we%20establish%20that%20the%20parameters%20of%20GD%0Aprojected%20onto%20an%20appropriate%20data%20subspace%20converge%20to%20the%20unique%20solution%20of%0Aa%20system%20of%20linear%20equations%2C%20which%20requires%20the%20logits%27%20difference%20of%0Ain-support%20tokens%20to%20be%20equal%20to%20the%20log-ratio%20of%20their%20respective%0Aprobabilities.%20Meanwhile%2C%20on%20the%20orthogonal%20subspace%2C%20the%20parameters%20diverge%0Aand%20converge%20in%20the%20direction%20of%20the%20solution%20of%20a%20max-margin%20quadratic%0Aprogram%2C%20minimizing%20the%20Euclidean%20norm%20of%20parameters%20satisfying%20the%0A%5CNTP-separability%20conditions.%20Akin%20to%20prior%20research%20on%20implicit%20bias%20of%0Aone-hot%20classification%2C%20our%20work%20opens%20exciting%20avenues%20for%20future%20research%0Athat%20can%20lead%20to%20better%20understanding%20optimization%2C%20generalization%20and%0Arobustness%20principles%20of%20models%20trained%20with%20NTP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18551v1&entry.124074799=Read"},
{"title": "NCART: Neural Classification and Regression Tree for Tabular Data", "author": "Jiaqi Luo and Shixin Xu", "abstract": "  Deep learning models have become popular in the analysis of tabular data, as\nthey address the limitations of decision trees and enable valuable applications\nlike semi-supervised learning, online learning, and transfer learning. However,\nthese deep-learning approaches often encounter a trade-off. On one hand, they\ncan be computationally expensive when dealing with large-scale or\nhigh-dimensional datasets. On the other hand, they may lack interpretability\nand may not be suitable for small-scale datasets. In this study, we propose a\nnovel interpretable neural network called Neural Classification and Regression\nTree (NCART) to overcome these challenges. NCART is a modified version of\nResidual Networks that replaces fully-connected layers with multiple\ndifferentiable oblivious decision trees. By integrating decision trees into the\narchitecture, NCART maintains its interpretability while benefiting from the\nend-to-end capabilities of neural networks. The simplicity of the NCART\narchitecture makes it well-suited for datasets of varying sizes and reduces\ncomputational costs compared to state-of-the-art deep learning models.\nExtensive numerical experiments demonstrate the superior performance of NCART\ncompared to existing deep learning models, establishing it as a strong\ncompetitor to tree-based models.\n", "link": "http://arxiv.org/abs/2307.12198v2", "date": "2024-02-28", "relevancy": 1.8216, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4768}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4532}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4073}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NCART%3A%20Neural%20Classification%20and%20Regression%20Tree%20for%20Tabular%20Data&entry.906535625=Jiaqi%20Luo%20and%20Shixin%20Xu&entry.1292438233=%20%20Deep%20learning%20models%20have%20become%20popular%20in%20the%20analysis%20of%20tabular%20data%2C%20as%0Athey%20address%20the%20limitations%20of%20decision%20trees%20and%20enable%20valuable%20applications%0Alike%20semi-supervised%20learning%2C%20online%20learning%2C%20and%20transfer%20learning.%20However%2C%0Athese%20deep-learning%20approaches%20often%20encounter%20a%20trade-off.%20On%20one%20hand%2C%20they%0Acan%20be%20computationally%20expensive%20when%20dealing%20with%20large-scale%20or%0Ahigh-dimensional%20datasets.%20On%20the%20other%20hand%2C%20they%20may%20lack%20interpretability%0Aand%20may%20not%20be%20suitable%20for%20small-scale%20datasets.%20In%20this%20study%2C%20we%20propose%20a%0Anovel%20interpretable%20neural%20network%20called%20Neural%20Classification%20and%20Regression%0ATree%20%28NCART%29%20to%20overcome%20these%20challenges.%20NCART%20is%20a%20modified%20version%20of%0AResidual%20Networks%20that%20replaces%20fully-connected%20layers%20with%20multiple%0Adifferentiable%20oblivious%20decision%20trees.%20By%20integrating%20decision%20trees%20into%20the%0Aarchitecture%2C%20NCART%20maintains%20its%20interpretability%20while%20benefiting%20from%20the%0Aend-to-end%20capabilities%20of%20neural%20networks.%20The%20simplicity%20of%20the%20NCART%0Aarchitecture%20makes%20it%20well-suited%20for%20datasets%20of%20varying%20sizes%20and%20reduces%0Acomputational%20costs%20compared%20to%20state-of-the-art%20deep%20learning%20models.%0AExtensive%20numerical%20experiments%20demonstrate%20the%20superior%20performance%20of%20NCART%0Acompared%20to%20existing%20deep%20learning%20models%2C%20establishing%20it%20as%20a%20strong%0Acompetitor%20to%20tree-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12198v2&entry.124074799=Read"},
{"title": "Log Neural Controlled Differential Equations: The Lie Brackets Make a\n  Difference", "author": "Benjamin Walker and Andrew D. McLeod and Tiexin Qin and Yichuan Cheng and Haoliang Li and Terry Lyons", "abstract": "  The vector field of a controlled differential equation (CDE) describes the\nrelationship between a control path and the evolution of a solution path.\nNeural CDEs (NCDEs) treat time series data as observations from a control path,\nparameterise a CDE's vector field using a neural network, and use the solution\npath as a continuously evolving hidden state. As their formulation makes them\nrobust to irregular sampling rates, NCDEs are a powerful approach for modelling\nreal-world data. Building on neural rough differential equations (NRDEs), we\nintroduce Log-NCDEs, a novel and effective method for training NCDEs. The core\ncomponent of Log-NCDEs is the Log-ODE method, a tool from the study of rough\npaths for approximating a CDE's solution. On a range of multivariate time\nseries classification benchmarks, Log-NCDEs are shown to achieve a higher\naverage test set accuracy than NCDEs, NRDEs, and two state-of-the-art models,\nS5 and the linear recurrent unit.\n", "link": "http://arxiv.org/abs/2402.18512v1", "date": "2024-02-28", "relevancy": 1.8063, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4904}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4346}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4195}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Log%20Neural%20Controlled%20Differential%20Equations%3A%20The%20Lie%20Brackets%20Make%20a%0A%20%20Difference&entry.906535625=Benjamin%20Walker%20and%20Andrew%20D.%20McLeod%20and%20Tiexin%20Qin%20and%20Yichuan%20Cheng%20and%20Haoliang%20Li%20and%20Terry%20Lyons&entry.1292438233=%20%20The%20vector%20field%20of%20a%20controlled%20differential%20equation%20%28CDE%29%20describes%20the%0Arelationship%20between%20a%20control%20path%20and%20the%20evolution%20of%20a%20solution%20path.%0ANeural%20CDEs%20%28NCDEs%29%20treat%20time%20series%20data%20as%20observations%20from%20a%20control%20path%2C%0Aparameterise%20a%20CDE%27s%20vector%20field%20using%20a%20neural%20network%2C%20and%20use%20the%20solution%0Apath%20as%20a%20continuously%20evolving%20hidden%20state.%20As%20their%20formulation%20makes%20them%0Arobust%20to%20irregular%20sampling%20rates%2C%20NCDEs%20are%20a%20powerful%20approach%20for%20modelling%0Areal-world%20data.%20Building%20on%20neural%20rough%20differential%20equations%20%28NRDEs%29%2C%20we%0Aintroduce%20Log-NCDEs%2C%20a%20novel%20and%20effective%20method%20for%20training%20NCDEs.%20The%20core%0Acomponent%20of%20Log-NCDEs%20is%20the%20Log-ODE%20method%2C%20a%20tool%20from%20the%20study%20of%20rough%0Apaths%20for%20approximating%20a%20CDE%27s%20solution.%20On%20a%20range%20of%20multivariate%20time%0Aseries%20classification%20benchmarks%2C%20Log-NCDEs%20are%20shown%20to%20achieve%20a%20higher%0Aaverage%20test%20set%20accuracy%20than%20NCDEs%2C%20NRDEs%2C%20and%20two%20state-of-the-art%20models%2C%0AS5%20and%20the%20linear%20recurrent%20unit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18512v1&entry.124074799=Read"},
{"title": "HOP to the Next Tasks and Domains for Continual Learning in NLP", "author": "Umberto Michieli and Mete Ozay", "abstract": "  Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and\ndomains) by transferring knowledge acquired on previous problems, whilst\navoiding forgetting of past ones. Different from previous approaches which\nfocused on CL for one NLP task or domain in a specific use-case, in this paper,\nwe address a more general CL setting to learn from a sequence of problems in a\nunique framework. Our method, HOP, permits to hop across tasks and domains by\naddressing the CL problem along three directions: (i) we employ a set of\nadapters to generalize a large pre-trained model to unseen problems, (ii) we\ncompute high-order moments over the distribution of embedded representations to\ndistinguish independent and correlated statistics across different tasks and\ndomains, (iii) we process this enriched information with auxiliary heads\nspecialized for each end problem. Extensive experimental campaign on 4 NLP\napplications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of\nour HOP.\n", "link": "http://arxiv.org/abs/2402.18449v1", "date": "2024-02-28", "relevancy": 1.8047, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4719}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4466}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4322}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOP%20to%20the%20Next%20Tasks%20and%20Domains%20for%20Continual%20Learning%20in%20NLP&entry.906535625=Umberto%20Michieli%20and%20Mete%20Ozay&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20aims%20to%20learn%20a%20sequence%20of%20problems%20%28i.e.%2C%20tasks%20and%0Adomains%29%20by%20transferring%20knowledge%20acquired%20on%20previous%20problems%2C%20whilst%0Aavoiding%20forgetting%20of%20past%20ones.%20Different%20from%20previous%20approaches%20which%0Afocused%20on%20CL%20for%20one%20NLP%20task%20or%20domain%20in%20a%20specific%20use-case%2C%20in%20this%20paper%2C%0Awe%20address%20a%20more%20general%20CL%20setting%20to%20learn%20from%20a%20sequence%20of%20problems%20in%20a%0Aunique%20framework.%20Our%20method%2C%20HOP%2C%20permits%20to%20hop%20across%20tasks%20and%20domains%20by%0Aaddressing%20the%20CL%20problem%20along%20three%20directions%3A%20%28i%29%20we%20employ%20a%20set%20of%0Aadapters%20to%20generalize%20a%20large%20pre-trained%20model%20to%20unseen%20problems%2C%20%28ii%29%20we%0Acompute%20high-order%20moments%20over%20the%20distribution%20of%20embedded%20representations%20to%0Adistinguish%20independent%20and%20correlated%20statistics%20across%20different%20tasks%20and%0Adomains%2C%20%28iii%29%20we%20process%20this%20enriched%20information%20with%20auxiliary%20heads%0Aspecialized%20for%20each%20end%20problem.%20Extensive%20experimental%20campaign%20on%204%20NLP%0Aapplications%2C%205%20benchmarks%20and%202%20CL%20setups%20demonstrates%20the%20effectiveness%20of%0Aour%20HOP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18449v1&entry.124074799=Read"},
{"title": "Training normalizing flows with computationally intensive target\n  probability distributions", "author": "Piotr Bialas and Piotr Korcyl and Tomasz Stebel", "abstract": "  Machine learning techniques, in particular the so-called normalizing flows,\nare becoming increasingly popular in the context of Monte Carlo simulations as\nthey can effectively approximate target probability distributions. In the case\nof lattice field theories (LFT) the target distribution is given by the\nexponential of the action. The common loss function's gradient estimator based\non the \"reparametrization trick\" requires the calculation of the derivative of\nthe action with respect to the fields. This can present a significant\ncomputational cost for complicated, non-local actions like e.g. fermionic\naction in QCD. In this contribution, we propose an estimator for normalizing\nflows based on the REINFORCE algorithm that avoids this issue. We apply it to\ntwo dimensional Schwinger model with Wilson fermions at criticality and show\nthat it is up to ten times faster in terms of the wall-clock time as well as\nrequiring up to $30\\%$ less memory than the reparameterization trick estimator.\nIt is also more numerically stable allowing for single precision calculations\nand the use of half-float tensor cores. We present an in-depth analysis of the\norigins of those improvements. We believe that these benefits will appear also\noutside the realm of the LFT, in each case where the target probability\ndistribution is computationally intensive.\n", "link": "http://arxiv.org/abs/2308.13294v2", "date": "2024-02-28", "relevancy": 1.7995, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4603}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4489}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4399}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20normalizing%20flows%20with%20computationally%20intensive%20target%0A%20%20probability%20distributions&entry.906535625=Piotr%20Bialas%20and%20Piotr%20Korcyl%20and%20Tomasz%20Stebel&entry.1292438233=%20%20Machine%20learning%20techniques%2C%20in%20particular%20the%20so-called%20normalizing%20flows%2C%0Aare%20becoming%20increasingly%20popular%20in%20the%20context%20of%20Monte%20Carlo%20simulations%20as%0Athey%20can%20effectively%20approximate%20target%20probability%20distributions.%20In%20the%20case%0Aof%20lattice%20field%20theories%20%28LFT%29%20the%20target%20distribution%20is%20given%20by%20the%0Aexponential%20of%20the%20action.%20The%20common%20loss%20function%27s%20gradient%20estimator%20based%0Aon%20the%20%22reparametrization%20trick%22%20requires%20the%20calculation%20of%20the%20derivative%20of%0Athe%20action%20with%20respect%20to%20the%20fields.%20This%20can%20present%20a%20significant%0Acomputational%20cost%20for%20complicated%2C%20non-local%20actions%20like%20e.g.%20fermionic%0Aaction%20in%20QCD.%20In%20this%20contribution%2C%20we%20propose%20an%20estimator%20for%20normalizing%0Aflows%20based%20on%20the%20REINFORCE%20algorithm%20that%20avoids%20this%20issue.%20We%20apply%20it%20to%0Atwo%20dimensional%20Schwinger%20model%20with%20Wilson%20fermions%20at%20criticality%20and%20show%0Athat%20it%20is%20up%20to%20ten%20times%20faster%20in%20terms%20of%20the%20wall-clock%20time%20as%20well%20as%0Arequiring%20up%20to%20%2430%5C%25%24%20less%20memory%20than%20the%20reparameterization%20trick%20estimator.%0AIt%20is%20also%20more%20numerically%20stable%20allowing%20for%20single%20precision%20calculations%0Aand%20the%20use%20of%20half-float%20tensor%20cores.%20We%20present%20an%20in-depth%20analysis%20of%20the%0Aorigins%20of%20those%20improvements.%20We%20believe%20that%20these%20benefits%20will%20appear%20also%0Aoutside%20the%20realm%20of%20the%20LFT%2C%20in%20each%20case%20where%20the%20target%20probability%0Adistribution%20is%20computationally%20intensive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13294v2&entry.124074799=Read"},
{"title": "Identification and Estimation for Nonignorable Missing Data: A Data\n  Fusion Approach", "author": "Zixiao Wang and AmirEmad Ghassami and Ilya Shpitser", "abstract": "  We consider the task of identifying and estimating a parameter of interest in\nsettings where data is missing not at random (MNAR). In general, such\nparameters are not identified without strong assumptions on the missing data\nmodel. In this paper, we take an alternative approach and introduce a method\ninspired by data fusion, where information in an MNAR dataset is augmented by\ninformation in an auxiliary dataset subject to missingness at random (MAR). We\nshow that even if the parameter of interest cannot be identified given either\ndataset alone, it can be identified given pooled data, under two complementary\nsets of assumptions. We derive an inverse probability weighted (IPW) estimator\nfor identified parameters, and evaluate the performance of our estimation\nstrategies via simulation studies, and a data application.\n", "link": "http://arxiv.org/abs/2311.09015v2", "date": "2024-02-28", "relevancy": 1.7879, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4627}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4535}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4342}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identification%20and%20Estimation%20for%20Nonignorable%20Missing%20Data%3A%20A%20Data%0A%20%20Fusion%20Approach&entry.906535625=Zixiao%20Wang%20and%20AmirEmad%20Ghassami%20and%20Ilya%20Shpitser&entry.1292438233=%20%20We%20consider%20the%20task%20of%20identifying%20and%20estimating%20a%20parameter%20of%20interest%20in%0Asettings%20where%20data%20is%20missing%20not%20at%20random%20%28MNAR%29.%20In%20general%2C%20such%0Aparameters%20are%20not%20identified%20without%20strong%20assumptions%20on%20the%20missing%20data%0Amodel.%20In%20this%20paper%2C%20we%20take%20an%20alternative%20approach%20and%20introduce%20a%20method%0Ainspired%20by%20data%20fusion%2C%20where%20information%20in%20an%20MNAR%20dataset%20is%20augmented%20by%0Ainformation%20in%20an%20auxiliary%20dataset%20subject%20to%20missingness%20at%20random%20%28MAR%29.%20We%0Ashow%20that%20even%20if%20the%20parameter%20of%20interest%20cannot%20be%20identified%20given%20either%0Adataset%20alone%2C%20it%20can%20be%20identified%20given%20pooled%20data%2C%20under%20two%20complementary%0Asets%20of%20assumptions.%20We%20derive%20an%20inverse%20probability%20weighted%20%28IPW%29%20estimator%0Afor%20identified%20parameters%2C%20and%20evaluate%20the%20performance%20of%20our%20estimation%0Astrategies%20via%20simulation%20studies%2C%20and%20a%20data%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09015v2&entry.124074799=Read"},
{"title": "RNNs are not Transformers (Yet): The Key Bottleneck on In-context\n  Retrieval", "author": "Kaiyue Wen and Xingyu Dang and Kaifeng Lyu", "abstract": "  This paper investigates the gap in representation powers of Recurrent Neural\nNetworks (RNNs) and Transformers in the context of solving algorithmic\nproblems. We focus on understanding whether RNNs, known for their memory\nefficiency in handling long sequences, can match the performance of\nTransformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.\nOur theoretical analysis reveals that CoT improves RNNs but is insufficient to\nclose the gap with Transformers. A key bottleneck lies in the inability of RNNs\nto perfectly retrieve information from the context, even with CoT: for several\ntasks that explicitly or implicitly require this capability, such as\nassociative recall and determining if a graph is a tree, we prove that RNNs are\nnot expressive enough to solve the tasks while Transformers can solve them with\nease. Conversely, we prove that adopting techniques to enhance the in-context\nretrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)\nand adding a single Transformer layer, can elevate RNNs to be capable of\nsolving all polynomial-time solvable problems with CoT, hence closing the\nrepresentation gap with Transformers.\n", "link": "http://arxiv.org/abs/2402.18510v1", "date": "2024-02-28", "relevancy": 1.7842, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4672}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.439}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4278}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RNNs%20are%20not%20Transformers%20%28Yet%29%3A%20The%20Key%20Bottleneck%20on%20In-context%0A%20%20Retrieval&entry.906535625=Kaiyue%20Wen%20and%20Xingyu%20Dang%20and%20Kaifeng%20Lyu&entry.1292438233=%20%20This%20paper%20investigates%20the%20gap%20in%20representation%20powers%20of%20Recurrent%20Neural%0ANetworks%20%28RNNs%29%20and%20Transformers%20in%20the%20context%20of%20solving%20algorithmic%0Aproblems.%20We%20focus%20on%20understanding%20whether%20RNNs%2C%20known%20for%20their%20memory%0Aefficiency%20in%20handling%20long%20sequences%2C%20can%20match%20the%20performance%20of%0ATransformers%2C%20particularly%20when%20enhanced%20with%20Chain-of-Thought%20%28CoT%29%20prompting.%0AOur%20theoretical%20analysis%20reveals%20that%20CoT%20improves%20RNNs%20but%20is%20insufficient%20to%0Aclose%20the%20gap%20with%20Transformers.%20A%20key%20bottleneck%20lies%20in%20the%20inability%20of%20RNNs%0Ato%20perfectly%20retrieve%20information%20from%20the%20context%2C%20even%20with%20CoT%3A%20for%20several%0Atasks%20that%20explicitly%20or%20implicitly%20require%20this%20capability%2C%20such%20as%0Aassociative%20recall%20and%20determining%20if%20a%20graph%20is%20a%20tree%2C%20we%20prove%20that%20RNNs%20are%0Anot%20expressive%20enough%20to%20solve%20the%20tasks%20while%20Transformers%20can%20solve%20them%20with%0Aease.%20Conversely%2C%20we%20prove%20that%20adopting%20techniques%20to%20enhance%20the%20in-context%0Aretrieval%20capability%20of%20RNNs%2C%20including%20Retrieval-Augmented%20Generation%20%28RAG%29%0Aand%20adding%20a%20single%20Transformer%20layer%2C%20can%20elevate%20RNNs%20to%20be%20capable%20of%0Asolving%20all%20polynomial-time%20solvable%20problems%20with%20CoT%2C%20hence%20closing%20the%0Arepresentation%20gap%20with%20Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18510v1&entry.124074799=Read"},
{"title": "Learned Contextual LiDAR Informed Visual Search in Unseen Environments", "author": "Ryan Gupta and Kyle Morgenstein and Steven Ortega and Luis Sentis", "abstract": "  This paper presents LIVES: LiDAR Informed Visual Search, an autonomous\nplanner for target search in unknown environments. We consider the pixel-wise\nenvironment perception problem where one is given wide Field of View 2D scan\ndata and must perform LiDAR segmentation to contextually label points in the\nsurroundings. These pixel classifications provide an informed prior on which to\nplan next best viewpoints during visual search tasks. The map-generalizable\nclassifier is trained from expert data collected using a simple cart platform\nequipped with a map-based classifier. An autonomous exploration planner takes\nthe contextual data from scans and uses that prior to plan viewpoints more\nlikely to yield detection of the search target. In order to achieve this, we\npropose a utility function that accounts for traditional metrics like\ninformation gain and path cost and also for the additional contextual\ninformation from the scan classifier. LIVES is baselined against several\nexisting exploration methods in simulation to verify its performance. Finally,\nit is validated in real-world experiments searching for single and multiple\ntargets with a Spot robot in two unseen environments. Videos of experimental\nvalidation, implementation details and open source code can be found on our\nproject website at https://sites.google.com/view/lives-2024/home.\n", "link": "http://arxiv.org/abs/2309.14150v3", "date": "2024-02-28", "relevancy": 1.7616, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5935}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5891}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5695}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Contextual%20LiDAR%20Informed%20Visual%20Search%20in%20Unseen%20Environments&entry.906535625=Ryan%20Gupta%20and%20Kyle%20Morgenstein%20and%20Steven%20Ortega%20and%20Luis%20Sentis&entry.1292438233=%20%20This%20paper%20presents%20LIVES%3A%20LiDAR%20Informed%20Visual%20Search%2C%20an%20autonomous%0Aplanner%20for%20target%20search%20in%20unknown%20environments.%20We%20consider%20the%20pixel-wise%0Aenvironment%20perception%20problem%20where%20one%20is%20given%20wide%20Field%20of%20View%202D%20scan%0Adata%20and%20must%20perform%20LiDAR%20segmentation%20to%20contextually%20label%20points%20in%20the%0Asurroundings.%20These%20pixel%20classifications%20provide%20an%20informed%20prior%20on%20which%20to%0Aplan%20next%20best%20viewpoints%20during%20visual%20search%20tasks.%20The%20map-generalizable%0Aclassifier%20is%20trained%20from%20expert%20data%20collected%20using%20a%20simple%20cart%20platform%0Aequipped%20with%20a%20map-based%20classifier.%20An%20autonomous%20exploration%20planner%20takes%0Athe%20contextual%20data%20from%20scans%20and%20uses%20that%20prior%20to%20plan%20viewpoints%20more%0Alikely%20to%20yield%20detection%20of%20the%20search%20target.%20In%20order%20to%20achieve%20this%2C%20we%0Apropose%20a%20utility%20function%20that%20accounts%20for%20traditional%20metrics%20like%0Ainformation%20gain%20and%20path%20cost%20and%20also%20for%20the%20additional%20contextual%0Ainformation%20from%20the%20scan%20classifier.%20LIVES%20is%20baselined%20against%20several%0Aexisting%20exploration%20methods%20in%20simulation%20to%20verify%20its%20performance.%20Finally%2C%0Ait%20is%20validated%20in%20real-world%20experiments%20searching%20for%20single%20and%20multiple%0Atargets%20with%20a%20Spot%20robot%20in%20two%20unseen%20environments.%20Videos%20of%20experimental%0Avalidation%2C%20implementation%20details%20and%20open%20source%20code%20can%20be%20found%20on%20our%0Aproject%20website%20at%20https%3A//sites.google.com/view/lives-2024/home.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14150v3&entry.124074799=Read"},
{"title": "A Game-theoretic Framework for Privacy-preserving Federated Learning", "author": "Xiaojin Zhang and Lixin Fan and Siwei Wang and Wenjie Li and Kai Chen and Qiang Yang", "abstract": "  In federated learning, benign participants aim to optimize a global model\ncollaboratively. However, the risk of \\textit{privacy leakage} cannot be\nignored in the presence of \\textit{semi-honest} adversaries. Existing research\nhas focused either on designing protection mechanisms or on inventing attacking\nmechanisms. While the battle between defenders and attackers seems\nnever-ending, we are concerned with one critical question: is it possible to\nprevent potential attacks in advance? To address this, we propose the first\ngame-theoretic framework that considers both FL defenders and attackers in\nterms of their respective payoffs, which include computational costs, FL model\nutilities, and privacy leakage risks. We name this game the federated learning\nprivacy game (FLPG), in which neither defenders nor attackers are aware of all\nparticipants' payoffs.\n  To handle the \\textit{incomplete information} inherent in this situation, we\npropose associating the FLPG with an \\textit{oracle} that has two primary\nresponsibilities. First, the oracle provides lower and upper bounds of the\npayoffs for the players. Second, the oracle acts as a correlation device,\nprivately providing suggested actions to each player. With this novel\nframework, we analyze the optimal strategies of defenders and attackers.\nFurthermore, we derive and demonstrate conditions under which the attacker, as\na rational decision-maker, should always follow the oracle's suggestion\n\\textit{not to attack}.\n", "link": "http://arxiv.org/abs/2304.05836v3", "date": "2024-02-28", "relevancy": 1.7545, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4474}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4381}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.418}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Game-theoretic%20Framework%20for%20Privacy-preserving%20Federated%20Learning&entry.906535625=Xiaojin%20Zhang%20and%20Lixin%20Fan%20and%20Siwei%20Wang%20and%20Wenjie%20Li%20and%20Kai%20Chen%20and%20Qiang%20Yang&entry.1292438233=%20%20In%20federated%20learning%2C%20benign%20participants%20aim%20to%20optimize%20a%20global%20model%0Acollaboratively.%20However%2C%20the%20risk%20of%20%5Ctextit%7Bprivacy%20leakage%7D%20cannot%20be%0Aignored%20in%20the%20presence%20of%20%5Ctextit%7Bsemi-honest%7D%20adversaries.%20Existing%20research%0Ahas%20focused%20either%20on%20designing%20protection%20mechanisms%20or%20on%20inventing%20attacking%0Amechanisms.%20While%20the%20battle%20between%20defenders%20and%20attackers%20seems%0Anever-ending%2C%20we%20are%20concerned%20with%20one%20critical%20question%3A%20is%20it%20possible%20to%0Aprevent%20potential%20attacks%20in%20advance%3F%20To%20address%20this%2C%20we%20propose%20the%20first%0Agame-theoretic%20framework%20that%20considers%20both%20FL%20defenders%20and%20attackers%20in%0Aterms%20of%20their%20respective%20payoffs%2C%20which%20include%20computational%20costs%2C%20FL%20model%0Autilities%2C%20and%20privacy%20leakage%20risks.%20We%20name%20this%20game%20the%20federated%20learning%0Aprivacy%20game%20%28FLPG%29%2C%20in%20which%20neither%20defenders%20nor%20attackers%20are%20aware%20of%20all%0Aparticipants%27%20payoffs.%0A%20%20To%20handle%20the%20%5Ctextit%7Bincomplete%20information%7D%20inherent%20in%20this%20situation%2C%20we%0Apropose%20associating%20the%20FLPG%20with%20an%20%5Ctextit%7Boracle%7D%20that%20has%20two%20primary%0Aresponsibilities.%20First%2C%20the%20oracle%20provides%20lower%20and%20upper%20bounds%20of%20the%0Apayoffs%20for%20the%20players.%20Second%2C%20the%20oracle%20acts%20as%20a%20correlation%20device%2C%0Aprivately%20providing%20suggested%20actions%20to%20each%20player.%20With%20this%20novel%0Aframework%2C%20we%20analyze%20the%20optimal%20strategies%20of%20defenders%20and%20attackers.%0AFurthermore%2C%20we%20derive%20and%20demonstrate%20conditions%20under%20which%20the%20attacker%2C%20as%0Aa%20rational%20decision-maker%2C%20should%20always%20follow%20the%20oracle%27s%20suggestion%0A%5Ctextit%7Bnot%20to%20attack%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.05836v3&entry.124074799=Read"},
{"title": "Dynamical Regimes of Diffusion Models", "author": "Giulio Biroli and Tony Bonnaire and Valentin de Bortoli and Marc M\u00e9zard", "abstract": "  Using statistical physics methods, we study generative diffusion models in\nthe regime where the dimension of space and the number of data are large, and\nthe score function has been trained optimally. Our analysis reveals three\ndistinct dynamical regimes during the backward generative diffusion process.\nThe generative dynamics, starting from pure noise, encounters first a\n'speciation' transition where the gross structure of data is unraveled, through\na mechanism similar to symmetry breaking in phase transitions. It is followed\nat later time by a 'collapse' transition where the trajectories of the dynamics\nbecome attracted to one of the memorized data points, through a mechanism which\nis similar to the condensation in a glass phase. For any dataset, the\nspeciation time can be found from a spectral analysis of the correlation\nmatrix, and the collapse time can be found from the estimation of an 'excess\nentropy' in the data. The dependence of the collapse time on the dimension and\nnumber of data provides a thorough characterization of the curse of\ndimensionality for diffusion models. Analytical solutions for simple models\nlike high-dimensional Gaussian mixtures substantiate these findings and provide\na theoretical framework, while extensions to more complex scenarios and\nnumerical validations with real datasets confirm the theoretical predictions.\n", "link": "http://arxiv.org/abs/2402.18491v1", "date": "2024-02-28", "relevancy": 1.7427, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4518}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.44}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4249}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamical%20Regimes%20of%20Diffusion%20Models&entry.906535625=Giulio%20Biroli%20and%20Tony%20Bonnaire%20and%20Valentin%20de%20Bortoli%20and%20Marc%20M%C3%A9zard&entry.1292438233=%20%20Using%20statistical%20physics%20methods%2C%20we%20study%20generative%20diffusion%20models%20in%0Athe%20regime%20where%20the%20dimension%20of%20space%20and%20the%20number%20of%20data%20are%20large%2C%20and%0Athe%20score%20function%20has%20been%20trained%20optimally.%20Our%20analysis%20reveals%20three%0Adistinct%20dynamical%20regimes%20during%20the%20backward%20generative%20diffusion%20process.%0AThe%20generative%20dynamics%2C%20starting%20from%20pure%20noise%2C%20encounters%20first%20a%0A%27speciation%27%20transition%20where%20the%20gross%20structure%20of%20data%20is%20unraveled%2C%20through%0Aa%20mechanism%20similar%20to%20symmetry%20breaking%20in%20phase%20transitions.%20It%20is%20followed%0Aat%20later%20time%20by%20a%20%27collapse%27%20transition%20where%20the%20trajectories%20of%20the%20dynamics%0Abecome%20attracted%20to%20one%20of%20the%20memorized%20data%20points%2C%20through%20a%20mechanism%20which%0Ais%20similar%20to%20the%20condensation%20in%20a%20glass%20phase.%20For%20any%20dataset%2C%20the%0Aspeciation%20time%20can%20be%20found%20from%20a%20spectral%20analysis%20of%20the%20correlation%0Amatrix%2C%20and%20the%20collapse%20time%20can%20be%20found%20from%20the%20estimation%20of%20an%20%27excess%0Aentropy%27%20in%20the%20data.%20The%20dependence%20of%20the%20collapse%20time%20on%20the%20dimension%20and%0Anumber%20of%20data%20provides%20a%20thorough%20characterization%20of%20the%20curse%20of%0Adimensionality%20for%20diffusion%20models.%20Analytical%20solutions%20for%20simple%20models%0Alike%20high-dimensional%20Gaussian%20mixtures%20substantiate%20these%20findings%20and%20provide%0Aa%20theoretical%20framework%2C%20while%20extensions%20to%20more%20complex%20scenarios%20and%0Anumerical%20validations%20with%20real%20datasets%20confirm%20the%20theoretical%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18491v1&entry.124074799=Read"},
{"title": "Errors are Robustly Tamed in Cumulative Knowledge Processes", "author": "Anna Brandenberger and Cassandra Marcussen and Elchanan Mossel and Madhu Sudan", "abstract": "  We study processes of societal knowledge accumulation, where the validity of\na new unit of knowledge depends both on the correctness of its derivation and\non the validity of the units it depends on. A fundamental question in this\nsetting is: If a constant fraction of the new derivations is wrong, can\ninvesting a constant fraction, bounded away from one, of effort ensure that a\nconstant fraction of knowledge in society is valid? Ben-Eliezer, Mikulincer,\nMossel, and Sudan (ITCS 2023) introduced a concrete probabilistic model to\nanalyze such questions and showed an affirmative answer to this question. Their\nstudy, however, focuses on the simple case where each new unit depends on just\none existing unit, and units attach according to a $\\textit{preferential\nattachment rule}$.\n  In this work, we consider much more general families of cumulative knowledge\nprocesses, where new units may attach according to varied attachment mechanisms\nand depend on multiple existing units. We also allow a (random) fraction of\ninsertions of adversarial nodes.\n  We give a robust affirmative answer to the above question by showing that for\n$\\textit{all}$ of these models, as long as many of the units follow simple\nheuristics for checking a bounded number of units they depend on, all errors\nwill be eventually eliminated. Our results indicate that preserving the quality\nof large interdependent collections of units of knowledge is feasible, as long\nas careful but not too costly checks are performed when new units are\nderived/deposited.\n", "link": "http://arxiv.org/abs/2309.05638v2", "date": "2024-02-28", "relevancy": 1.7305, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4487}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.441}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4132}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Errors%20are%20Robustly%20Tamed%20in%20Cumulative%20Knowledge%20Processes&entry.906535625=Anna%20Brandenberger%20and%20Cassandra%20Marcussen%20and%20Elchanan%20Mossel%20and%20Madhu%20Sudan&entry.1292438233=%20%20We%20study%20processes%20of%20societal%20knowledge%20accumulation%2C%20where%20the%20validity%20of%0Aa%20new%20unit%20of%20knowledge%20depends%20both%20on%20the%20correctness%20of%20its%20derivation%20and%0Aon%20the%20validity%20of%20the%20units%20it%20depends%20on.%20A%20fundamental%20question%20in%20this%0Asetting%20is%3A%20If%20a%20constant%20fraction%20of%20the%20new%20derivations%20is%20wrong%2C%20can%0Ainvesting%20a%20constant%20fraction%2C%20bounded%20away%20from%20one%2C%20of%20effort%20ensure%20that%20a%0Aconstant%20fraction%20of%20knowledge%20in%20society%20is%20valid%3F%20Ben-Eliezer%2C%20Mikulincer%2C%0AMossel%2C%20and%20Sudan%20%28ITCS%202023%29%20introduced%20a%20concrete%20probabilistic%20model%20to%0Aanalyze%20such%20questions%20and%20showed%20an%20affirmative%20answer%20to%20this%20question.%20Their%0Astudy%2C%20however%2C%20focuses%20on%20the%20simple%20case%20where%20each%20new%20unit%20depends%20on%20just%0Aone%20existing%20unit%2C%20and%20units%20attach%20according%20to%20a%20%24%5Ctextit%7Bpreferential%0Aattachment%20rule%7D%24.%0A%20%20In%20this%20work%2C%20we%20consider%20much%20more%20general%20families%20of%20cumulative%20knowledge%0Aprocesses%2C%20where%20new%20units%20may%20attach%20according%20to%20varied%20attachment%20mechanisms%0Aand%20depend%20on%20multiple%20existing%20units.%20We%20also%20allow%20a%20%28random%29%20fraction%20of%0Ainsertions%20of%20adversarial%20nodes.%0A%20%20We%20give%20a%20robust%20affirmative%20answer%20to%20the%20above%20question%20by%20showing%20that%20for%0A%24%5Ctextit%7Ball%7D%24%20of%20these%20models%2C%20as%20long%20as%20many%20of%20the%20units%20follow%20simple%0Aheuristics%20for%20checking%20a%20bounded%20number%20of%20units%20they%20depend%20on%2C%20all%20errors%0Awill%20be%20eventually%20eliminated.%20Our%20results%20indicate%20that%20preserving%20the%20quality%0Aof%20large%20interdependent%20collections%20of%20units%20of%20knowledge%20is%20feasible%2C%20as%20long%0Aas%20careful%20but%20not%20too%20costly%20checks%20are%20performed%20when%20new%20units%20are%0Aderived/deposited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05638v2&entry.124074799=Read"},
{"title": "Multimodal Learning To Improve Cardiac Late Mechanical Activation\n  Detection From Cine MR Images", "author": "Jiarui Xing and Nian Wu and Kenneth Bilchick and Frederick Epstein and Miaomiao Zhang", "abstract": "  This paper presents a multimodal deep learning framework that utilizes\nadvanced image techniques to improve the performance of clinical analysis\nheavily dependent on routinely acquired standard images. More specifically, we\ndevelop a joint learning network that for the first time leverages the accuracy\nand reproducibility of myocardial strains obtained from Displacement Encoding\nwith Stimulated Echo (DENSE) to guide the analysis of cine cardiac magnetic\nresonance (CMR) imaging in late mechanical activation (LMA) detection. An image\nregistration network is utilized to acquire the knowledge of cardiac motions,\nan important feature estimator of strain values, from standard cine CMRs. Our\nframework consists of two major components: (i) a DENSE-supervised strain\nnetwork leveraging latent motion features learned from a registration network\nto predict myocardial strains; and (ii) a LMA network taking advantage of the\npredicted strain for effective LMA detection. Experimental results show that\nour proposed work substantially improves the performance of strain analysis and\nLMA detection from cine CMR images, aligning more closely with the achievements\nof DENSE.\n", "link": "http://arxiv.org/abs/2402.18507v1", "date": "2024-02-28", "relevancy": 1.7203, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6007}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5239}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Learning%20To%20Improve%20Cardiac%20Late%20Mechanical%20Activation%0A%20%20Detection%20From%20Cine%20MR%20Images&entry.906535625=Jiarui%20Xing%20and%20Nian%20Wu%20and%20Kenneth%20Bilchick%20and%20Frederick%20Epstein%20and%20Miaomiao%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20a%20multimodal%20deep%20learning%20framework%20that%20utilizes%0Aadvanced%20image%20techniques%20to%20improve%20the%20performance%20of%20clinical%20analysis%0Aheavily%20dependent%20on%20routinely%20acquired%20standard%20images.%20More%20specifically%2C%20we%0Adevelop%20a%20joint%20learning%20network%20that%20for%20the%20first%20time%20leverages%20the%20accuracy%0Aand%20reproducibility%20of%20myocardial%20strains%20obtained%20from%20Displacement%20Encoding%0Awith%20Stimulated%20Echo%20%28DENSE%29%20to%20guide%20the%20analysis%20of%20cine%20cardiac%20magnetic%0Aresonance%20%28CMR%29%20imaging%20in%20late%20mechanical%20activation%20%28LMA%29%20detection.%20An%20image%0Aregistration%20network%20is%20utilized%20to%20acquire%20the%20knowledge%20of%20cardiac%20motions%2C%0Aan%20important%20feature%20estimator%20of%20strain%20values%2C%20from%20standard%20cine%20CMRs.%20Our%0Aframework%20consists%20of%20two%20major%20components%3A%20%28i%29%20a%20DENSE-supervised%20strain%0Anetwork%20leveraging%20latent%20motion%20features%20learned%20from%20a%20registration%20network%0Ato%20predict%20myocardial%20strains%3B%20and%20%28ii%29%20a%20LMA%20network%20taking%20advantage%20of%20the%0Apredicted%20strain%20for%20effective%20LMA%20detection.%20Experimental%20results%20show%20that%0Aour%20proposed%20work%20substantially%20improves%20the%20performance%20of%20strain%20analysis%20and%0ALMA%20detection%20from%20cine%20CMR%20images%2C%20aligning%20more%20closely%20with%20the%20achievements%0Aof%20DENSE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18507v1&entry.124074799=Read"},
{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "author": "Zhihao Zhang and Shengcao Cao and Yu-Xiong Wang", "abstract": "  The limited scale of current 3D shape datasets hinders the advancements in 3D\nshape understanding, and motivates multi-modal learning approaches which\ntransfer learned knowledge from data-abundant 2D image and language modalities\nto 3D shapes. However, even though the image and language representations have\nbeen aligned by cross-modal models like CLIP, we find that the image modality\nfails to contribute as much as the language in existing multi-modal 3D\nrepresentation learning methods. This is attributed to the domain shift in the\n2D images and the distinct focus of each modality. To more effectively leverage\nboth modalities in the pre-training, we introduce TriAdapter Multi-Modal\nLearning (TAMM) -- a novel two-stage learning approach based on three\nsynergetic adapters. First, our CLIP Image Adapter mitigates the domain gap\nbetween 3D-rendered images and natural images, by adapting the visual\nrepresentations of CLIP for synthetic image-text pairs. Subsequently, our Dual\nAdapters decouple the 3D shape representation space into two complementary\nsub-spaces: one focusing on visual attributes and the other for semantic\nunderstanding, which ensure a more comprehensive and effective multi-modal\npre-training. Extensive experiments demonstrate that TAMM consistently enhances\n3D representations for a wide range of 3D encoder architectures, pre-training\ndatasets, and downstream tasks. Notably, we boost the zero-shot classification\naccuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot\nlinear probing classification accuracy on ModelNet40 from 96.1 to 99.0. Project\npage: \\url{https://alanzhangcs.github.io/tamm-page}.\n", "link": "http://arxiv.org/abs/2402.18490v1", "date": "2024-02-28", "relevancy": 1.7024, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5779}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.564}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5449}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAMM%3A%20TriAdapter%20Multi-Modal%20Learning%20for%203D%20Shape%20Understanding&entry.906535625=Zhihao%20Zhang%20and%20Shengcao%20Cao%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20The%20limited%20scale%20of%20current%203D%20shape%20datasets%20hinders%20the%20advancements%20in%203D%0Ashape%20understanding%2C%20and%20motivates%20multi-modal%20learning%20approaches%20which%0Atransfer%20learned%20knowledge%20from%20data-abundant%202D%20image%20and%20language%20modalities%0Ato%203D%20shapes.%20However%2C%20even%20though%20the%20image%20and%20language%20representations%20have%0Abeen%20aligned%20by%20cross-modal%20models%20like%20CLIP%2C%20we%20find%20that%20the%20image%20modality%0Afails%20to%20contribute%20as%20much%20as%20the%20language%20in%20existing%20multi-modal%203D%0Arepresentation%20learning%20methods.%20This%20is%20attributed%20to%20the%20domain%20shift%20in%20the%0A2D%20images%20and%20the%20distinct%20focus%20of%20each%20modality.%20To%20more%20effectively%20leverage%0Aboth%20modalities%20in%20the%20pre-training%2C%20we%20introduce%20TriAdapter%20Multi-Modal%0ALearning%20%28TAMM%29%20--%20a%20novel%20two-stage%20learning%20approach%20based%20on%20three%0Asynergetic%20adapters.%20First%2C%20our%20CLIP%20Image%20Adapter%20mitigates%20the%20domain%20gap%0Abetween%203D-rendered%20images%20and%20natural%20images%2C%20by%20adapting%20the%20visual%0Arepresentations%20of%20CLIP%20for%20synthetic%20image-text%20pairs.%20Subsequently%2C%20our%20Dual%0AAdapters%20decouple%20the%203D%20shape%20representation%20space%20into%20two%20complementary%0Asub-spaces%3A%20one%20focusing%20on%20visual%20attributes%20and%20the%20other%20for%20semantic%0Aunderstanding%2C%20which%20ensure%20a%20more%20comprehensive%20and%20effective%20multi-modal%0Apre-training.%20Extensive%20experiments%20demonstrate%20that%20TAMM%20consistently%20enhances%0A3D%20representations%20for%20a%20wide%20range%20of%203D%20encoder%20architectures%2C%20pre-training%0Adatasets%2C%20and%20downstream%20tasks.%20Notably%2C%20we%20boost%20the%20zero-shot%20classification%0Aaccuracy%20on%20Objaverse-LVIS%20from%2046.8%20to%2050.7%2C%20and%20improve%20the%205-way%2010-shot%0Alinear%20probing%20classification%20accuracy%20on%20ModelNet40%20from%2096.1%20to%2099.0.%20Project%0Apage%3A%20%5Curl%7Bhttps%3A//alanzhangcs.github.io/tamm-page%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18490v1&entry.124074799=Read"},
{"title": "A Modular System for Enhanced Robustness of Multimedia Understanding\n  Networks via Deep Parametric Estimation", "author": "Francesco Barbato and Umberto Michieli and Mehmet Karim Yucel and Pietro Zanuttigh and Mete Ozay", "abstract": "  In multimedia understanding tasks, corrupted samples pose a critical\nchallenge, because when fed to machine learning models they lead to performance\ndegradation. In the past, three groups of approaches have been proposed to\nhandle noisy data: i) enhancer and denoiser modules to improve the quality of\nthe noisy data, ii) data augmentation approaches, and iii) domain adaptation\nstrategies. All the aforementioned approaches come with drawbacks that limit\ntheir applicability; the first has high computational costs and requires pairs\nof clean-corrupted data for training, while the others only allow deployment of\nthe same task/network they were trained on (\\ie, when upstream and downstream\ntask/network are the same). In this paper, we propose SyMPIE to solve these\nshortcomings. To this end, we design a small, modular, and efficient (just\n2GFLOPs to process a Full HD image) system to enhance input data for robust\ndownstream multimedia understanding with minimal computational cost. Our SyMPIE\nis pre-trained on an upstream task/network that should not match the downstream\nones and does not need paired clean-corrupted samples. Our key insight is that\nmost input corruptions found in real-world tasks can be modeled through global\noperations on color channels of images or spatial filters with small kernels.\nWe validate our approach on multiple datasets and tasks, such as image\nclassification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed\ncorruption benchmark named ImageNetC-mixed) and semantic segmentation (on\nCityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\\%\nrelative accuracy gain across the board. The code of our approach and the new\nImageNetC-mixed benchmark will be made available upon publication.\n", "link": "http://arxiv.org/abs/2402.18402v1", "date": "2024-02-28", "relevancy": 1.6804, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5677}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5592}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5421}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Modular%20System%20for%20Enhanced%20Robustness%20of%20Multimedia%20Understanding%0A%20%20Networks%20via%20Deep%20Parametric%20Estimation&entry.906535625=Francesco%20Barbato%20and%20Umberto%20Michieli%20and%20Mehmet%20Karim%20Yucel%20and%20Pietro%20Zanuttigh%20and%20Mete%20Ozay&entry.1292438233=%20%20In%20multimedia%20understanding%20tasks%2C%20corrupted%20samples%20pose%20a%20critical%0Achallenge%2C%20because%20when%20fed%20to%20machine%20learning%20models%20they%20lead%20to%20performance%0Adegradation.%20In%20the%20past%2C%20three%20groups%20of%20approaches%20have%20been%20proposed%20to%0Ahandle%20noisy%20data%3A%20i%29%20enhancer%20and%20denoiser%20modules%20to%20improve%20the%20quality%20of%0Athe%20noisy%20data%2C%20ii%29%20data%20augmentation%20approaches%2C%20and%20iii%29%20domain%20adaptation%0Astrategies.%20All%20the%20aforementioned%20approaches%20come%20with%20drawbacks%20that%20limit%0Atheir%20applicability%3B%20the%20first%20has%20high%20computational%20costs%20and%20requires%20pairs%0Aof%20clean-corrupted%20data%20for%20training%2C%20while%20the%20others%20only%20allow%20deployment%20of%0Athe%20same%20task/network%20they%20were%20trained%20on%20%28%5Cie%2C%20when%20upstream%20and%20downstream%0Atask/network%20are%20the%20same%29.%20In%20this%20paper%2C%20we%20propose%20SyMPIE%20to%20solve%20these%0Ashortcomings.%20To%20this%20end%2C%20we%20design%20a%20small%2C%20modular%2C%20and%20efficient%20%28just%0A2GFLOPs%20to%20process%20a%20Full%20HD%20image%29%20system%20to%20enhance%20input%20data%20for%20robust%0Adownstream%20multimedia%20understanding%20with%20minimal%20computational%20cost.%20Our%20SyMPIE%0Ais%20pre-trained%20on%20an%20upstream%20task/network%20that%20should%20not%20match%20the%20downstream%0Aones%20and%20does%20not%20need%20paired%20clean-corrupted%20samples.%20Our%20key%20insight%20is%20that%0Amost%20input%20corruptions%20found%20in%20real-world%20tasks%20can%20be%20modeled%20through%20global%0Aoperations%20on%20color%20channels%20of%20images%20or%20spatial%20filters%20with%20small%20kernels.%0AWe%20validate%20our%20approach%20on%20multiple%20datasets%20and%20tasks%2C%20such%20as%20image%0Aclassification%20%28on%20ImageNetC%2C%20ImageNetC-Bar%2C%20VizWiz%2C%20and%20a%20newly%20proposed%20mixed%0Acorruption%20benchmark%20named%20ImageNetC-mixed%29%20and%20semantic%20segmentation%20%28on%0ACityscapes%2C%20ACDC%2C%20and%20DarkZurich%29%20with%20consistent%20improvements%20of%20about%205%5C%25%0Arelative%20accuracy%20gain%20across%20the%20board.%20The%20code%20of%20our%20approach%20and%20the%20new%0AImageNetC-mixed%20benchmark%20will%20be%20made%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18402v1&entry.124074799=Read"},
{"title": "Sora: A Review on Background, Technology, Limitations, and Opportunities\n  of Large Vision Models", "author": "Yixin Liu and Kai Zhang and Yuan Li and Zhiling Yan and Chujie Gao and Ruoxi Chen and Zhengqing Yuan and Yue Huang and Hanchi Sun and Jianfeng Gao and Lifang He and Lichao Sun", "abstract": "  Sora is a text-to-video generative AI model, released by OpenAI in February\n2024. The model is trained to generate videos of realistic or imaginative\nscenes from text instructions and show potential in simulating the physical\nworld. Based on public technical reports and reverse engineering, this paper\npresents a comprehensive review of the model's background, related\ntechnologies, applications, remaining challenges, and future directions of\ntext-to-video AI models. We first trace Sora's development and investigate the\nunderlying technologies used to build this \"world simulator\". Then, we describe\nin detail the applications and potential impact of Sora in multiple industries\nranging from film-making and education to marketing. We discuss the main\nchallenges and limitations that need to be addressed to widely deploy Sora,\nsuch as ensuring safe and unbiased video generation. Lastly, we discuss the\nfuture development of Sora and video generation models in general, and how\nadvancements in the field could enable new ways of human-AI interaction,\nboosting productivity and creativity of video generation.\n", "link": "http://arxiv.org/abs/2402.17177v2", "date": "2024-02-28", "relevancy": 1.6728, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5766}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5382}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5296}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sora%3A%20A%20Review%20on%20Background%2C%20Technology%2C%20Limitations%2C%20and%20Opportunities%0A%20%20of%20Large%20Vision%20Models&entry.906535625=Yixin%20Liu%20and%20Kai%20Zhang%20and%20Yuan%20Li%20and%20Zhiling%20Yan%20and%20Chujie%20Gao%20and%20Ruoxi%20Chen%20and%20Zhengqing%20Yuan%20and%20Yue%20Huang%20and%20Hanchi%20Sun%20and%20Jianfeng%20Gao%20and%20Lifang%20He%20and%20Lichao%20Sun&entry.1292438233=%20%20Sora%20is%20a%20text-to-video%20generative%20AI%20model%2C%20released%20by%20OpenAI%20in%20February%0A2024.%20The%20model%20is%20trained%20to%20generate%20videos%20of%20realistic%20or%20imaginative%0Ascenes%20from%20text%20instructions%20and%20show%20potential%20in%20simulating%20the%20physical%0Aworld.%20Based%20on%20public%20technical%20reports%20and%20reverse%20engineering%2C%20this%20paper%0Apresents%20a%20comprehensive%20review%20of%20the%20model%27s%20background%2C%20related%0Atechnologies%2C%20applications%2C%20remaining%20challenges%2C%20and%20future%20directions%20of%0Atext-to-video%20AI%20models.%20We%20first%20trace%20Sora%27s%20development%20and%20investigate%20the%0Aunderlying%20technologies%20used%20to%20build%20this%20%22world%20simulator%22.%20Then%2C%20we%20describe%0Ain%20detail%20the%20applications%20and%20potential%20impact%20of%20Sora%20in%20multiple%20industries%0Aranging%20from%20film-making%20and%20education%20to%20marketing.%20We%20discuss%20the%20main%0Achallenges%20and%20limitations%20that%20need%20to%20be%20addressed%20to%20widely%20deploy%20Sora%2C%0Asuch%20as%20ensuring%20safe%20and%20unbiased%20video%20generation.%20Lastly%2C%20we%20discuss%20the%0Afuture%20development%20of%20Sora%20and%20video%20generation%20models%20in%20general%2C%20and%20how%0Aadvancements%20in%20the%20field%20could%20enable%20new%20ways%20of%20human-AI%20interaction%2C%0Aboosting%20productivity%20and%20creativity%20of%20video%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17177v2&entry.124074799=Read"},
{"title": "MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image\n  Reconstruction and Uncertainty Estimation", "author": "Jiahao Huang and Liutao Yang and Fanwen Wang and Yinzhe Wu and Yang Nan and Angelica I. Aviles-Rivero and Carola-Bibiane Sch\u00f6nlieb and Daoqiang Zhang and Guang Yang", "abstract": "  The recent Mamba model has shown remarkable adaptability for visual\nrepresentation learning, including in medical imaging tasks. This study\nintroduces MambaMIR, a Mamba-based model for medical image reconstruction, as\nwell as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our\nproposed MambaMIR inherits several advantages, such as linear complexity,\nglobal receptive fields, and dynamic weights, from the original Mamba model.\nThe innovated arbitrary-mask mechanism effectively adapt Mamba to our image\nreconstruction task, providing randomness for subsequent Monte Carlo-based\nuncertainty estimation. Experiments conducted on various medical image\nreconstruction tasks, including fast MRI and SVCT, which cover anatomical\nregions such as the knee, chest, and abdomen, have demonstrated that MambaMIR\nand MambaMIR-GAN achieve comparable or superior reconstruction results relative\nto state-of-the-art methods. Additionally, the estimated uncertainty maps offer\nfurther insights into the reliability of the reconstruction quality. The code\nis publicly available at https://github.com/ayanglab/MambaMIR.\n", "link": "http://arxiv.org/abs/2402.18451v1", "date": "2024-02-28", "relevancy": 1.6584, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5797}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5627}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaMIR%3A%20An%20Arbitrary-Masked%20Mamba%20for%20Joint%20Medical%20Image%0A%20%20Reconstruction%20and%20Uncertainty%20Estimation&entry.906535625=Jiahao%20Huang%20and%20Liutao%20Yang%20and%20Fanwen%20Wang%20and%20Yinzhe%20Wu%20and%20Yang%20Nan%20and%20Angelica%20I.%20Aviles-Rivero%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Daoqiang%20Zhang%20and%20Guang%20Yang&entry.1292438233=%20%20The%20recent%20Mamba%20model%20has%20shown%20remarkable%20adaptability%20for%20visual%0Arepresentation%20learning%2C%20including%20in%20medical%20imaging%20tasks.%20This%20study%0Aintroduces%20MambaMIR%2C%20a%20Mamba-based%20model%20for%20medical%20image%20reconstruction%2C%20as%0Awell%20as%20its%20Generative%20Adversarial%20Network-based%20variant%2C%20MambaMIR-GAN.%20Our%0Aproposed%20MambaMIR%20inherits%20several%20advantages%2C%20such%20as%20linear%20complexity%2C%0Aglobal%20receptive%20fields%2C%20and%20dynamic%20weights%2C%20from%20the%20original%20Mamba%20model.%0AThe%20innovated%20arbitrary-mask%20mechanism%20effectively%20adapt%20Mamba%20to%20our%20image%0Areconstruction%20task%2C%20providing%20randomness%20for%20subsequent%20Monte%20Carlo-based%0Auncertainty%20estimation.%20Experiments%20conducted%20on%20various%20medical%20image%0Areconstruction%20tasks%2C%20including%20fast%20MRI%20and%20SVCT%2C%20which%20cover%20anatomical%0Aregions%20such%20as%20the%20knee%2C%20chest%2C%20and%20abdomen%2C%20have%20demonstrated%20that%20MambaMIR%0Aand%20MambaMIR-GAN%20achieve%20comparable%20or%20superior%20reconstruction%20results%20relative%0Ato%20state-of-the-art%20methods.%20Additionally%2C%20the%20estimated%20uncertainty%20maps%20offer%0Afurther%20insights%20into%20the%20reliability%20of%20the%20reconstruction%20quality.%20The%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/ayanglab/MambaMIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18451v1&entry.124074799=Read"},
{"title": "Compass: A Decentralized Scheduler for Latency-Sensitive ML Workflows", "author": "Yuting Yang and Andrea Merlina and Weijia Song and Tiancheng Yuan and Ken Birman and Roman Vitenberg", "abstract": "  We consider ML query processing in distributed systems where GPU-enabled\nworkers coordinate to execute complex queries: a computing style often seen in\napplications that interact with users in support of image processing and\nnatural language processing. In such systems, coscheduling of GPU memory\nmanagement and task placement represents a promising opportunity. We propose\nCompass, a novel framework that unifies these functions to reduce job latency\nwhile using resources efficiently, placing tasks where data dependencies will\nbe satisfied, collocating tasks from the same job (when this will not overload\nthe host or its GPU), and efficiently managing GPU memory. Comparison with\nother state of the art schedulers shows a significant reduction in completion\ntimes while requiring the same amount or even fewer resources. In one case,\njust half the servers were needed for processing the same workload.\n", "link": "http://arxiv.org/abs/2402.17652v2", "date": "2024-02-28", "relevancy": 1.647, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4337}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3866}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compass%3A%20A%20Decentralized%20Scheduler%20for%20Latency-Sensitive%20ML%20Workflows&entry.906535625=Yuting%20Yang%20and%20Andrea%20Merlina%20and%20Weijia%20Song%20and%20Tiancheng%20Yuan%20and%20Ken%20Birman%20and%20Roman%20Vitenberg&entry.1292438233=%20%20We%20consider%20ML%20query%20processing%20in%20distributed%20systems%20where%20GPU-enabled%0Aworkers%20coordinate%20to%20execute%20complex%20queries%3A%20a%20computing%20style%20often%20seen%20in%0Aapplications%20that%20interact%20with%20users%20in%20support%20of%20image%20processing%20and%0Anatural%20language%20processing.%20In%20such%20systems%2C%20coscheduling%20of%20GPU%20memory%0Amanagement%20and%20task%20placement%20represents%20a%20promising%20opportunity.%20We%20propose%0ACompass%2C%20a%20novel%20framework%20that%20unifies%20these%20functions%20to%20reduce%20job%20latency%0Awhile%20using%20resources%20efficiently%2C%20placing%20tasks%20where%20data%20dependencies%20will%0Abe%20satisfied%2C%20collocating%20tasks%20from%20the%20same%20job%20%28when%20this%20will%20not%20overload%0Athe%20host%20or%20its%20GPU%29%2C%20and%20efficiently%20managing%20GPU%20memory.%20Comparison%20with%0Aother%20state%20of%20the%20art%20schedulers%20shows%20a%20significant%20reduction%20in%20completion%0Atimes%20while%20requiring%20the%20same%20amount%20or%20even%20fewer%20resources.%20In%20one%20case%2C%0Ajust%20half%20the%20servers%20were%20needed%20for%20processing%20the%20same%20workload.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17652v2&entry.124074799=Read"},
{"title": "Actor-Critic Model Predictive Control", "author": "Angel Romero and Yunlong Song and Davide Scaramuzza", "abstract": "  An open research question in robotics is how to combine the benefits of\nmodel-free reinforcement learning (RL) - known for its strong task performance\nand flexibility in optimizing general reward formulations - with the robustness\nand online replanning capabilities of model predictive control (MPC). This\npaper provides an answer by introducing a new framework called Actor-Critic\nModel Predictive Control. The key idea is to embed a differentiable MPC within\nan actor-critic RL framework. The proposed approach leverages the short-term\npredictive optimization capabilities of MPC with the exploratory and end-to-end\ntraining properties of RL. The resulting policy effectively manages both\nshort-term decisions through the MPC-based actor and long-term prediction via\nthe critic network, unifying the benefits of both model-based control and\nend-to-end learning. We validate our method in both simulation and the real\nworld with a quadcopter platform across various high-level tasks. We show that\nthe proposed architecture can achieve real-time control performance, learn\ncomplex behaviors via trial and error, and retain the predictive properties of\nthe MPC to better handle out of distribution behaviour.\n", "link": "http://arxiv.org/abs/2306.09852v4", "date": "2024-02-28", "relevancy": 1.6286, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5594}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5383}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5377}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Actor-Critic%20Model%20Predictive%20Control&entry.906535625=Angel%20Romero%20and%20Yunlong%20Song%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20An%20open%20research%20question%20in%20robotics%20is%20how%20to%20combine%20the%20benefits%20of%0Amodel-free%20reinforcement%20learning%20%28RL%29%20-%20known%20for%20its%20strong%20task%20performance%0Aand%20flexibility%20in%20optimizing%20general%20reward%20formulations%20-%20with%20the%20robustness%0Aand%20online%20replanning%20capabilities%20of%20model%20predictive%20control%20%28MPC%29.%20This%0Apaper%20provides%20an%20answer%20by%20introducing%20a%20new%20framework%20called%20Actor-Critic%0AModel%20Predictive%20Control.%20The%20key%20idea%20is%20to%20embed%20a%20differentiable%20MPC%20within%0Aan%20actor-critic%20RL%20framework.%20The%20proposed%20approach%20leverages%20the%20short-term%0Apredictive%20optimization%20capabilities%20of%20MPC%20with%20the%20exploratory%20and%20end-to-end%0Atraining%20properties%20of%20RL.%20The%20resulting%20policy%20effectively%20manages%20both%0Ashort-term%20decisions%20through%20the%20MPC-based%20actor%20and%20long-term%20prediction%20via%0Athe%20critic%20network%2C%20unifying%20the%20benefits%20of%20both%20model-based%20control%20and%0Aend-to-end%20learning.%20We%20validate%20our%20method%20in%20both%20simulation%20and%20the%20real%0Aworld%20with%20a%20quadcopter%20platform%20across%20various%20high-level%20tasks.%20We%20show%20that%0Athe%20proposed%20architecture%20can%20achieve%20real-time%20control%20performance%2C%20learn%0Acomplex%20behaviors%20via%20trial%20and%20error%2C%20and%20retain%20the%20predictive%20properties%20of%0Athe%20MPC%20to%20better%20handle%20out%20of%20distribution%20behaviour.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09852v4&entry.124074799=Read"},
{"title": "TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models", "author": "Yushi Huang and Ruihao Gong and Jing Liu and Tianlong Chen and Xianglong Liu", "abstract": "  The Diffusion model, a prevalent framework for image generation, encounters\nsignificant challenges in terms of broad applicability due to its extended\ninference times and substantial memory requirements. Efficient Post-training\nQuantization (PTQ) is pivotal for addressing these issues in traditional\nmodels. Different from traditional models, diffusion models heavily depend on\nthe time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$\nfrom the finite set $\\{1, \\ldots, T\\}$ is encoded to a temporal feature by a\nfew modules totally irrespective of the sampling data. However, existing PTQ\nmethods do not optimize these modules separately. They adopt inappropriate\nreconstruction targets and complex calibration methods, resulting in a severe\ndisturbance of the temporal feature and denoising trajectory, as well as a low\ncompression efficiency. To solve these, we propose a Temporal Feature\nMaintenance Quantization (TFMQ) framework building upon a Temporal Information\nBlock which is just related to the time-step $t$ and unrelated to the sampling\ndata. Powered by the pioneering block design, we devise temporal information\naware reconstruction (TIAR) and finite set calibration (FSC) to align the\nfull-precision temporal features in a limited time. Equipped with the\nframework, we can maintain the most temporal information and ensure the\nend-to-end generation quality. Extensive experiments on various datasets and\ndiffusion models prove our state-of-the-art results. Remarkably, our\nquantization approach, for the first time, achieves model performance nearly on\npar with the full-precision model under 4-bit weight quantization.\nAdditionally, our method incurs almost no extra computational cost and\naccelerates quantization time by $2.0 \\times$ on LSUN-Bedrooms $256 \\times 256$\ncompared to previous works.\n", "link": "http://arxiv.org/abs/2311.16503v2", "date": "2024-02-28", "relevancy": 1.6264, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5803}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5488}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4873}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TFMQ-DM%3A%20Temporal%20Feature%20Maintenance%20Quantization%20for%20Diffusion%20Models&entry.906535625=Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Tianlong%20Chen%20and%20Xianglong%20Liu&entry.1292438233=%20%20The%20Diffusion%20model%2C%20a%20prevalent%20framework%20for%20image%20generation%2C%20encounters%0Asignificant%20challenges%20in%20terms%20of%20broad%20applicability%20due%20to%20its%20extended%0Ainference%20times%20and%20substantial%20memory%20requirements.%20Efficient%20Post-training%0AQuantization%20%28PTQ%29%20is%20pivotal%20for%20addressing%20these%20issues%20in%20traditional%0Amodels.%20Different%20from%20traditional%20models%2C%20diffusion%20models%20heavily%20depend%20on%0Athe%20time-step%20%24t%24%20to%20achieve%20satisfactory%20multi-round%20denoising.%20Usually%2C%20%24t%24%0Afrom%20the%20finite%20set%20%24%5C%7B1%2C%20%5Cldots%2C%20T%5C%7D%24%20is%20encoded%20to%20a%20temporal%20feature%20by%20a%0Afew%20modules%20totally%20irrespective%20of%20the%20sampling%20data.%20However%2C%20existing%20PTQ%0Amethods%20do%20not%20optimize%20these%20modules%20separately.%20They%20adopt%20inappropriate%0Areconstruction%20targets%20and%20complex%20calibration%20methods%2C%20resulting%20in%20a%20severe%0Adisturbance%20of%20the%20temporal%20feature%20and%20denoising%20trajectory%2C%20as%20well%20as%20a%20low%0Acompression%20efficiency.%20To%20solve%20these%2C%20we%20propose%20a%20Temporal%20Feature%0AMaintenance%20Quantization%20%28TFMQ%29%20framework%20building%20upon%20a%20Temporal%20Information%0ABlock%20which%20is%20just%20related%20to%20the%20time-step%20%24t%24%20and%20unrelated%20to%20the%20sampling%0Adata.%20Powered%20by%20the%20pioneering%20block%20design%2C%20we%20devise%20temporal%20information%0Aaware%20reconstruction%20%28TIAR%29%20and%20finite%20set%20calibration%20%28FSC%29%20to%20align%20the%0Afull-precision%20temporal%20features%20in%20a%20limited%20time.%20Equipped%20with%20the%0Aframework%2C%20we%20can%20maintain%20the%20most%20temporal%20information%20and%20ensure%20the%0Aend-to-end%20generation%20quality.%20Extensive%20experiments%20on%20various%20datasets%20and%0Adiffusion%20models%20prove%20our%20state-of-the-art%20results.%20Remarkably%2C%20our%0Aquantization%20approach%2C%20for%20the%20first%20time%2C%20achieves%20model%20performance%20nearly%20on%0Apar%20with%20the%20full-precision%20model%20under%204-bit%20weight%20quantization.%0AAdditionally%2C%20our%20method%20incurs%20almost%20no%20extra%20computational%20cost%20and%0Aaccelerates%20quantization%20time%20by%20%242.0%20%5Ctimes%24%20on%20LSUN-Bedrooms%20%24256%20%5Ctimes%20256%24%0Acompared%20to%20previous%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16503v2&entry.124074799=Read"},
{"title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist\n  Autonomous Agents for Desktop and Web", "author": "Raghav Kapoor and Yash Parag Butala and Melisa Russak and Jing Yu Koh and Kiran Kamble and Waseem Alshikh and Ruslan Salakhutdinov", "abstract": "  For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens.\n", "link": "http://arxiv.org/abs/2402.17553v2", "date": "2024-02-28", "relevancy": 1.6251, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5742}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5366}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.522}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniACT%3A%20A%20Dataset%20and%20Benchmark%20for%20Enabling%20Multimodal%20Generalist%0A%20%20Autonomous%20Agents%20for%20Desktop%20and%20Web&entry.906535625=Raghav%20Kapoor%20and%20Yash%20Parag%20Butala%20and%20Melisa%20Russak%20and%20Jing%20Yu%20Koh%20and%20Kiran%20Kamble%20and%20Waseem%20Alshikh%20and%20Ruslan%20Salakhutdinov&entry.1292438233=%20%20For%20decades%2C%20human-computer%20interaction%20has%20fundamentally%20been%20manual.%20Even%0Atoday%2C%20almost%20all%20productive%20work%20done%20on%20the%20computer%20necessitates%20human%20input%0Aat%20every%20step.%20Autonomous%20virtual%20agents%20represent%20an%20exciting%20step%20in%0Aautomating%20many%20of%20these%20menial%20tasks.%20Virtual%20agents%20would%20empower%20users%20with%0Alimited%20technical%20proficiency%20to%20harness%20the%20full%20possibilities%20of%20computer%0Asystems.%20They%20could%20also%20enable%20the%20efficient%20streamlining%20of%20numerous%20computer%0Atasks%2C%20ranging%20from%20calendar%20management%20to%20complex%20travel%20bookings%2C%20with%0Aminimal%20human%20intervention.%20In%20this%20paper%2C%20we%20introduce%20OmniACT%2C%20the%0Afirst-of-a-kind%20dataset%20and%20benchmark%20for%20assessing%20an%20agent%27s%20capability%20to%0Agenerate%20executable%20programs%20to%20accomplish%20computer%20tasks.%20Our%20scope%20extends%0Abeyond%20traditional%20web%20automation%2C%20covering%20a%20diverse%20range%20of%20desktop%0Aapplications.%20The%20dataset%20consists%20of%20fundamental%20tasks%20such%20as%20%22Play%20the%20next%0Asong%22%2C%20as%20well%20as%20longer%20horizon%20tasks%20such%20as%20%22Send%20an%20email%20to%20John%20Doe%0Amentioning%20the%20time%20and%20place%20to%20meet%22.%20Specifically%2C%20given%20a%20pair%20of%20screen%0Aimage%20and%20a%20visually-grounded%20natural%20language%20task%2C%20the%20goal%20is%20to%20generate%20a%0Ascript%20capable%20of%20fully%20executing%20the%20task.%20We%20run%20several%20strong%20baseline%0Alanguage%20model%20agents%20on%20our%20benchmark.%20The%20strongest%20baseline%2C%20GPT-4%2C%20performs%0Athe%20best%20on%20our%20benchmark%20However%2C%20its%20performance%20level%20still%20reaches%20only%2015%25%0Aof%20the%20human%20proficiency%20in%20generating%20executable%20scripts%20capable%20of%20completing%0Athe%20task%2C%20demonstrating%20the%20challenge%20of%20our%20task%20for%20conventional%20web%20agents.%0AOur%20benchmark%20provides%20a%20platform%20to%20measure%20and%20evaluate%20the%20progress%20of%0Alanguage%20model%20agents%20in%20automating%20computer%20tasks%20and%20motivates%20future%20work%0Atowards%20building%20multimodal%20models%20that%20bridge%20large%20language%20models%20and%20the%0Avisual%20grounding%20of%20computer%20screens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17553v2&entry.124074799=Read"},
{"title": "Selection of appropriate multispectral camera exposure settings and\n  radiometric calibration methods for applications in phenotyping and precision\n  agriculture", "author": "Vaishali Swaminathan and J. Alex Thomasson and Robert G. Hardin and Nithya Rajan", "abstract": "  Radiometric accuracy of data is crucial in quantitative precision\nagriculture, to produce reliable and repeatable data for modeling and decision\nmaking. The effect of exposure time and gain settings on the radiometric\naccuracy of multispectral images was not explored enough. The goal of this\nstudy was to determine if having a fixed exposure (FE) time during image\nacquisition improved radiometric accuracy of images, compared to the default\nauto-exposure (AE) settings. This involved quantifying the errors from\nauto-exposure and determining ideal exposure values within which radiometric\nmean absolute percentage error (MAPE) were minimal (< 5%). The results showed\nthat FE orthomosaic was closer to ground-truth (higher R2 and lower MAPE) than\nAE orthomosaic. An ideal exposure range was determined for capturing canopy and\nsoil objects, without loss of information from under-exposure or saturation\nfrom over-exposure. A simulation of errors from AE showed that MAPE < 5% for\nthe blue, green, red, and NIR bands and < 7% for the red edge band for exposure\nsettings within the determined ideal ranges and increased exponentially beyond\nthe ideal exposure upper limit. Further, prediction of total plant nitrogen\nuptake (g/plant) using vegetation indices (VIs) from two different growing\nseasons were closer to the ground truth (mostly, R2 > 0.40, and MAPE = 12 to\n14%, p < 0.05) when FE was used, compared to the prediction from AE images\n(mostly, R2 < 0.13, MAPE = 15 to 18%, p >= 0.05).\n", "link": "http://arxiv.org/abs/2402.18553v1", "date": "2024-02-28", "relevancy": 1.5602, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4128}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3925}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3663}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selection%20of%20appropriate%20multispectral%20camera%20exposure%20settings%20and%0A%20%20radiometric%20calibration%20methods%20for%20applications%20in%20phenotyping%20and%20precision%0A%20%20agriculture&entry.906535625=Vaishali%20Swaminathan%20and%20J.%20Alex%20Thomasson%20and%20Robert%20G.%20Hardin%20and%20Nithya%20Rajan&entry.1292438233=%20%20Radiometric%20accuracy%20of%20data%20is%20crucial%20in%20quantitative%20precision%0Aagriculture%2C%20to%20produce%20reliable%20and%20repeatable%20data%20for%20modeling%20and%20decision%0Amaking.%20The%20effect%20of%20exposure%20time%20and%20gain%20settings%20on%20the%20radiometric%0Aaccuracy%20of%20multispectral%20images%20was%20not%20explored%20enough.%20The%20goal%20of%20this%0Astudy%20was%20to%20determine%20if%20having%20a%20fixed%20exposure%20%28FE%29%20time%20during%20image%0Aacquisition%20improved%20radiometric%20accuracy%20of%20images%2C%20compared%20to%20the%20default%0Aauto-exposure%20%28AE%29%20settings.%20This%20involved%20quantifying%20the%20errors%20from%0Aauto-exposure%20and%20determining%20ideal%20exposure%20values%20within%20which%20radiometric%0Amean%20absolute%20percentage%20error%20%28MAPE%29%20were%20minimal%20%28%3C%205%25%29.%20The%20results%20showed%0Athat%20FE%20orthomosaic%20was%20closer%20to%20ground-truth%20%28higher%20R2%20and%20lower%20MAPE%29%20than%0AAE%20orthomosaic.%20An%20ideal%20exposure%20range%20was%20determined%20for%20capturing%20canopy%20and%0Asoil%20objects%2C%20without%20loss%20of%20information%20from%20under-exposure%20or%20saturation%0Afrom%20over-exposure.%20A%20simulation%20of%20errors%20from%20AE%20showed%20that%20MAPE%20%3C%205%25%20for%0Athe%20blue%2C%20green%2C%20red%2C%20and%20NIR%20bands%20and%20%3C%207%25%20for%20the%20red%20edge%20band%20for%20exposure%0Asettings%20within%20the%20determined%20ideal%20ranges%20and%20increased%20exponentially%20beyond%0Athe%20ideal%20exposure%20upper%20limit.%20Further%2C%20prediction%20of%20total%20plant%20nitrogen%0Auptake%20%28g/plant%29%20using%20vegetation%20indices%20%28VIs%29%20from%20two%20different%20growing%0Aseasons%20were%20closer%20to%20the%20ground%20truth%20%28mostly%2C%20R2%20%3E%200.40%2C%20and%20MAPE%20%3D%2012%20to%0A14%25%2C%20p%20%3C%200.05%29%20when%20FE%20was%20used%2C%20compared%20to%20the%20prediction%20from%20AE%20images%0A%28mostly%2C%20R2%20%3C%200.13%2C%20MAPE%20%3D%2015%20to%2018%25%2C%20p%20%3E%3D%200.05%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18553v1&entry.124074799=Read"},
{"title": "Detection of Micromobility Vehicles in Urban Traffic Videos", "author": "Khalil Sabri and C\u00e9lia Djilali and Guillaume-Alexandre Bilodeau and Nicolas Saunier and Wassim Bouachir", "abstract": "  Urban traffic environments present unique challenges for object detection,\nparticularly with the increasing presence of micromobility vehicles like\ne-scooters and bikes. To address this object detection problem, this work\nintroduces an adapted detection model that combines the accuracy and speed of\nsingle-frame object detection with the richer features offered by video object\ndetection frameworks. This is done by applying aggregated feature maps from\nconsecutive frames processed through motion flow to the YOLOX architecture.\nThis fusion brings a temporal perspective to YOLOX detection abilities,\nallowing for a better understanding of urban mobility patterns and\nsubstantially improving detection reliability. Tested on a custom dataset\ncurated for urban micromobility scenarios, our model showcases substantial\nimprovement over existing state-of-the-art methods, demonstrating the need to\nconsider spatio-temporal information for detecting such small and thin objects.\nOur approach enhances detection in challenging conditions, including\nocclusions, ensuring temporal consistency, and effectively mitigating motion\nblur.\n", "link": "http://arxiv.org/abs/2402.18503v1", "date": "2024-02-28", "relevancy": 1.5495, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5213}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5152}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.515}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20of%20Micromobility%20Vehicles%20in%20Urban%20Traffic%20Videos&entry.906535625=Khalil%20Sabri%20and%20C%C3%A9lia%20Djilali%20and%20Guillaume-Alexandre%20Bilodeau%20and%20Nicolas%20Saunier%20and%20Wassim%20Bouachir&entry.1292438233=%20%20Urban%20traffic%20environments%20present%20unique%20challenges%20for%20object%20detection%2C%0Aparticularly%20with%20the%20increasing%20presence%20of%20micromobility%20vehicles%20like%0Ae-scooters%20and%20bikes.%20To%20address%20this%20object%20detection%20problem%2C%20this%20work%0Aintroduces%20an%20adapted%20detection%20model%20that%20combines%20the%20accuracy%20and%20speed%20of%0Asingle-frame%20object%20detection%20with%20the%20richer%20features%20offered%20by%20video%20object%0Adetection%20frameworks.%20This%20is%20done%20by%20applying%20aggregated%20feature%20maps%20from%0Aconsecutive%20frames%20processed%20through%20motion%20flow%20to%20the%20YOLOX%20architecture.%0AThis%20fusion%20brings%20a%20temporal%20perspective%20to%20YOLOX%20detection%20abilities%2C%0Aallowing%20for%20a%20better%20understanding%20of%20urban%20mobility%20patterns%20and%0Asubstantially%20improving%20detection%20reliability.%20Tested%20on%20a%20custom%20dataset%0Acurated%20for%20urban%20micromobility%20scenarios%2C%20our%20model%20showcases%20substantial%0Aimprovement%20over%20existing%20state-of-the-art%20methods%2C%20demonstrating%20the%20need%20to%0Aconsider%20spatio-temporal%20information%20for%20detecting%20such%20small%20and%20thin%20objects.%0AOur%20approach%20enhances%20detection%20in%20challenging%20conditions%2C%20including%0Aocclusions%2C%20ensuring%20temporal%20consistency%2C%20and%20effectively%20mitigating%20motion%0Ablur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18503v1&entry.124074799=Read"},
{"title": "Controlling Vision-Language Models for Multi-Task Image Restoration", "author": "Ziwei Luo and Fredrik K. Gustafsson and Zheng Zhao and Jens Sj\u00f6lund and Thomas B. Sch\u00f6n", "abstract": "  Vision-language models such as CLIP have shown great impact on diverse\ndownstream tasks for zero-shot or label-free predictions. However, when it\ncomes to low-level vision such as image restoration their performance\ndeteriorates dramatically due to corrupted inputs. In this paper, we present a\ndegradation-aware vision-language model (DA-CLIP) to better transfer pretrained\nvision-language models to low-level vision tasks as a multi-task framework for\nimage restoration. More specifically, DA-CLIP trains an additional controller\nthat adapts the fixed CLIP image encoder to predict high-quality feature\nembeddings. By integrating the embedding into an image restoration network via\ncross-attention, we are able to pilot the model to learn a high-fidelity image\nreconstruction. The controller itself will also output a degradation feature\nthat matches the real corruptions of the input, yielding a natural classifier\nfor different degradation types. In addition, we construct a mixed degradation\ndataset with synthetic captions for DA-CLIP training. Our approach advances\nstate-of-the-art performance on both \\emph{degradation-specific} and\n\\emph{unified} image restoration tasks, showing a promising direction of\nprompting image restoration with large-scale pretrained vision-language models.\nOur code is available at https://github.com/Algolzw/daclip-uir.\n", "link": "http://arxiv.org/abs/2310.01018v2", "date": "2024-02-28", "relevancy": 1.5313, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5286}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5054}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5052}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20Vision-Language%20Models%20for%20Multi-Task%20Image%20Restoration&entry.906535625=Ziwei%20Luo%20and%20Fredrik%20K.%20Gustafsson%20and%20Zheng%20Zhao%20and%20Jens%20Sj%C3%B6lund%20and%20Thomas%20B.%20Sch%C3%B6n&entry.1292438233=%20%20Vision-language%20models%20such%20as%20CLIP%20have%20shown%20great%20impact%20on%20diverse%0Adownstream%20tasks%20for%20zero-shot%20or%20label-free%20predictions.%20However%2C%20when%20it%0Acomes%20to%20low-level%20vision%20such%20as%20image%20restoration%20their%20performance%0Adeteriorates%20dramatically%20due%20to%20corrupted%20inputs.%20In%20this%20paper%2C%20we%20present%20a%0Adegradation-aware%20vision-language%20model%20%28DA-CLIP%29%20to%20better%20transfer%20pretrained%0Avision-language%20models%20to%20low-level%20vision%20tasks%20as%20a%20multi-task%20framework%20for%0Aimage%20restoration.%20More%20specifically%2C%20DA-CLIP%20trains%20an%20additional%20controller%0Athat%20adapts%20the%20fixed%20CLIP%20image%20encoder%20to%20predict%20high-quality%20feature%0Aembeddings.%20By%20integrating%20the%20embedding%20into%20an%20image%20restoration%20network%20via%0Across-attention%2C%20we%20are%20able%20to%20pilot%20the%20model%20to%20learn%20a%20high-fidelity%20image%0Areconstruction.%20The%20controller%20itself%20will%20also%20output%20a%20degradation%20feature%0Athat%20matches%20the%20real%20corruptions%20of%20the%20input%2C%20yielding%20a%20natural%20classifier%0Afor%20different%20degradation%20types.%20In%20addition%2C%20we%20construct%20a%20mixed%20degradation%0Adataset%20with%20synthetic%20captions%20for%20DA-CLIP%20training.%20Our%20approach%20advances%0Astate-of-the-art%20performance%20on%20both%20%5Cemph%7Bdegradation-specific%7D%20and%0A%5Cemph%7Bunified%7D%20image%20restoration%20tasks%2C%20showing%20a%20promising%20direction%20of%0Aprompting%20image%20restoration%20with%20large-scale%20pretrained%20vision-language%20models.%0AOur%20code%20is%20available%20at%20https%3A//github.com/Algolzw/daclip-uir.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01018v2&entry.124074799=Read"},
{"title": "Objective and Interpretable Breast Cosmesis Evaluation with Attention\n  Guided Denoising Diffusion Anomaly Detection Model", "author": "Sangjoon Park and Yong Bae Kim and Jee Suk Chang and Seo Hee Choi and Hyungjin Chung and Ik Jae Lee and Hwa Kyung Byun", "abstract": "  As advancements in the field of breast cancer treatment continue to progress,\nthe assessment of post-surgical cosmetic outcomes has gained increasing\nsignificance due to its substantial impact on patients' quality of life.\nHowever, evaluating breast cosmesis presents challenges due to the inherently\nsubjective nature of expert labeling. In this study, we present a novel\nautomated approach, Attention-Guided Denoising Diffusion Anomaly Detection\n(AG-DDAD), designed to assess breast cosmesis following surgery, addressing the\nlimitations of conventional supervised learning and existing anomaly detection\nmodels. Our approach leverages the attention mechanism of the distillation with\nno label (DINO) self-supervised Vision Transformer (ViT) in combination with a\ndiffusion model to achieve high-quality image reconstruction and precise\ntransformation of discriminative regions. By training the diffusion model on\nunlabeled data predominantly with normal cosmesis, we adopt an unsupervised\nanomaly detection perspective to automatically score the cosmesis. Real-world\ndata experiments demonstrate the effectiveness of our method, providing\nvisually appealing representations and quantifiable scores for cosmesis\nevaluation. Compared to commonly used rule-based programs, our fully automated\napproach eliminates the need for manual annotations and offers objective\nevaluation. Moreover, our anomaly detection model exhibits state-of-the-art\nperformance, surpassing existing models in accuracy. Going beyond the scope of\nbreast cosmesis, our research represents a significant advancement in\nunsupervised anomaly detection within the medical domain, thereby paving the\nway for future investigations.\n", "link": "http://arxiv.org/abs/2402.18362v1", "date": "2024-02-28", "relevancy": 1.5139, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.543}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4976}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4921}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Objective%20and%20Interpretable%20Breast%20Cosmesis%20Evaluation%20with%20Attention%0A%20%20Guided%20Denoising%20Diffusion%20Anomaly%20Detection%20Model&entry.906535625=Sangjoon%20Park%20and%20Yong%20Bae%20Kim%20and%20Jee%20Suk%20Chang%20and%20Seo%20Hee%20Choi%20and%20Hyungjin%20Chung%20and%20Ik%20Jae%20Lee%20and%20Hwa%20Kyung%20Byun&entry.1292438233=%20%20As%20advancements%20in%20the%20field%20of%20breast%20cancer%20treatment%20continue%20to%20progress%2C%0Athe%20assessment%20of%20post-surgical%20cosmetic%20outcomes%20has%20gained%20increasing%0Asignificance%20due%20to%20its%20substantial%20impact%20on%20patients%27%20quality%20of%20life.%0AHowever%2C%20evaluating%20breast%20cosmesis%20presents%20challenges%20due%20to%20the%20inherently%0Asubjective%20nature%20of%20expert%20labeling.%20In%20this%20study%2C%20we%20present%20a%20novel%0Aautomated%20approach%2C%20Attention-Guided%20Denoising%20Diffusion%20Anomaly%20Detection%0A%28AG-DDAD%29%2C%20designed%20to%20assess%20breast%20cosmesis%20following%20surgery%2C%20addressing%20the%0Alimitations%20of%20conventional%20supervised%20learning%20and%20existing%20anomaly%20detection%0Amodels.%20Our%20approach%20leverages%20the%20attention%20mechanism%20of%20the%20distillation%20with%0Ano%20label%20%28DINO%29%20self-supervised%20Vision%20Transformer%20%28ViT%29%20in%20combination%20with%20a%0Adiffusion%20model%20to%20achieve%20high-quality%20image%20reconstruction%20and%20precise%0Atransformation%20of%20discriminative%20regions.%20By%20training%20the%20diffusion%20model%20on%0Aunlabeled%20data%20predominantly%20with%20normal%20cosmesis%2C%20we%20adopt%20an%20unsupervised%0Aanomaly%20detection%20perspective%20to%20automatically%20score%20the%20cosmesis.%20Real-world%0Adata%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20providing%0Avisually%20appealing%20representations%20and%20quantifiable%20scores%20for%20cosmesis%0Aevaluation.%20Compared%20to%20commonly%20used%20rule-based%20programs%2C%20our%20fully%20automated%0Aapproach%20eliminates%20the%20need%20for%20manual%20annotations%20and%20offers%20objective%0Aevaluation.%20Moreover%2C%20our%20anomaly%20detection%20model%20exhibits%20state-of-the-art%0Aperformance%2C%20surpassing%20existing%20models%20in%20accuracy.%20Going%20beyond%20the%20scope%20of%0Abreast%20cosmesis%2C%20our%20research%20represents%20a%20significant%20advancement%20in%0Aunsupervised%20anomaly%20detection%20within%20the%20medical%20domain%2C%20thereby%20paving%20the%0Away%20for%20future%20investigations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18362v1&entry.124074799=Read"},
{"title": "LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs", "author": "Md Hafizur Rahman and Prabuddha Chakraborty", "abstract": "  Building efficient neural network architectures can be a time-consuming task\nrequiring extensive expert knowledge. This task becomes particularly\nchallenging for edge devices because one has to consider parameters such as\npower consumption during inferencing, model size, inferencing speed, and CO2\nemissions. In this article, we introduce a novel framework designed to\nautomatically discover new neural network architectures based on user-defined\nparameters, an expert system, and an LLM trained on a large amount of\nopen-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be\nused by non-AI experts, does not require a predetermined neural architecture\nsearch space, and considers a large set of edge device-specific parameters. We\nimplement and validate this proposed neural architecture discovery framework\nusing CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo\nand Gemini as the LLM component. We observe that the proposed framework can\nrapidly (within hours) discover intricate neural network models that perform\nextremely well across a diverse set of application settings defined by the\nuser.\n", "link": "http://arxiv.org/abs/2402.18443v1", "date": "2024-02-28", "relevancy": 1.5037, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5183}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4842}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4757}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeMo-NADe%3A%20Multi-Parameter%20Neural%20Architecture%20Discovery%20with%20LLMs&entry.906535625=Md%20Hafizur%20Rahman%20and%20Prabuddha%20Chakraborty&entry.1292438233=%20%20Building%20efficient%20neural%20network%20architectures%20can%20be%20a%20time-consuming%20task%0Arequiring%20extensive%20expert%20knowledge.%20This%20task%20becomes%20particularly%0Achallenging%20for%20edge%20devices%20because%20one%20has%20to%20consider%20parameters%20such%20as%0Apower%20consumption%20during%20inferencing%2C%20model%20size%2C%20inferencing%20speed%2C%20and%20CO2%0Aemissions.%20In%20this%20article%2C%20we%20introduce%20a%20novel%20framework%20designed%20to%0Aautomatically%20discover%20new%20neural%20network%20architectures%20based%20on%20user-defined%0Aparameters%2C%20an%20expert%20system%2C%20and%20an%20LLM%20trained%20on%20a%20large%20amount%20of%0Aopen-domain%20knowledge.%20The%20introduced%20framework%20%28LeMo-NADe%29%20is%20tailored%20to%20be%0Aused%20by%20non-AI%20experts%2C%20does%20not%20require%20a%20predetermined%20neural%20architecture%0Asearch%20space%2C%20and%20considers%20a%20large%20set%20of%20edge%20device-specific%20parameters.%20We%0Aimplement%20and%20validate%20this%20proposed%20neural%20architecture%20discovery%20framework%0Ausing%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet16-120%20datasets%20while%20using%20GPT-4%20Turbo%0Aand%20Gemini%20as%20the%20LLM%20component.%20We%20observe%20that%20the%20proposed%20framework%20can%0Arapidly%20%28within%20hours%29%20discover%20intricate%20neural%20network%20models%20that%20perform%0Aextremely%20well%20across%20a%20diverse%20set%20of%20application%20settings%20defined%20by%20the%0Auser.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18443v1&entry.124074799=Read"},
{"title": "Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale\n  Libraries", "author": "Zijun Long and Xuri Ge and Richard Mccreadie and Joemon Jose", "abstract": "  Text-to-image retrieval plays a crucial role across various applications,\nincluding digital libraries, e-commerce platforms, and multimedia databases, by\nenabling the search for images using text queries. Despite the advancements in\nMultimodal Large Language Models (MLLMs), which offer leading-edge performance,\ntheir applicability in large-scale, varied, and ambiguous retrieval scenarios\nis constrained by significant computational demands and the generation of\ninjective embeddings. This paper introduces the Text2Pic Swift framework,\ntailored for efficient and robust retrieval of images corresponding to\nextensive textual descriptions in sizable datasets. The framework employs a\ntwo-tier approach: the initial Entity-based Ranking (ER) stage addresses the\nambiguity inherent in lengthy text queries through a\nmultiple-queries-to-multiple-targets strategy, effectively narrowing down\npotential candidates for subsequent analysis. Following this, the Summary-based\nRe-ranking (SR) stage further refines these selections based on concise query\nsummaries. Additionally, we present a novel Decoupling-BEiT-3 encoder,\nspecifically designed to tackle the challenges of ambiguous queries and to\nfacilitate both stages of the retrieval process, thereby significantly\nimproving computational efficiency via vector-based similarity assessments. Our\nevaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift\noutperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000,\nalongside reductions in training and retrieval durations by 68.75% and 99.79%,\nrespectively.\n", "link": "http://arxiv.org/abs/2402.15276v2", "date": "2024-02-28", "relevancy": 1.502, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5112}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5044}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.495}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Pic%20Swift%3A%20Enhancing%20Long-Text%20to%20Image%20Retrieval%20for%20Large-Scale%0A%20%20Libraries&entry.906535625=Zijun%20Long%20and%20Xuri%20Ge%20and%20Richard%20Mccreadie%20and%20Joemon%20Jose&entry.1292438233=%20%20Text-to-image%20retrieval%20plays%20a%20crucial%20role%20across%20various%20applications%2C%0Aincluding%20digital%20libraries%2C%20e-commerce%20platforms%2C%20and%20multimedia%20databases%2C%20by%0Aenabling%20the%20search%20for%20images%20using%20text%20queries.%20Despite%20the%20advancements%20in%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20which%20offer%20leading-edge%20performance%2C%0Atheir%20applicability%20in%20large-scale%2C%20varied%2C%20and%20ambiguous%20retrieval%20scenarios%0Ais%20constrained%20by%20significant%20computational%20demands%20and%20the%20generation%20of%0Ainjective%20embeddings.%20This%20paper%20introduces%20the%20Text2Pic%20Swift%20framework%2C%0Atailored%20for%20efficient%20and%20robust%20retrieval%20of%20images%20corresponding%20to%0Aextensive%20textual%20descriptions%20in%20sizable%20datasets.%20The%20framework%20employs%20a%0Atwo-tier%20approach%3A%20the%20initial%20Entity-based%20Ranking%20%28ER%29%20stage%20addresses%20the%0Aambiguity%20inherent%20in%20lengthy%20text%20queries%20through%20a%0Amultiple-queries-to-multiple-targets%20strategy%2C%20effectively%20narrowing%20down%0Apotential%20candidates%20for%20subsequent%20analysis.%20Following%20this%2C%20the%20Summary-based%0ARe-ranking%20%28SR%29%20stage%20further%20refines%20these%20selections%20based%20on%20concise%20query%0Asummaries.%20Additionally%2C%20we%20present%20a%20novel%20Decoupling-BEiT-3%20encoder%2C%0Aspecifically%20designed%20to%20tackle%20the%20challenges%20of%20ambiguous%20queries%20and%20to%0Afacilitate%20both%20stages%20of%20the%20retrieval%20process%2C%20thereby%20significantly%0Aimproving%20computational%20efficiency%20via%20vector-based%20similarity%20assessments.%20Our%0Aevaluation%2C%20conducted%20on%20the%20AToMiC%20dataset%2C%20demonstrates%20that%20Text2Pic%20Swift%0Aoutperforms%20current%20MLLMs%20by%20achieving%20up%20to%20an%2011.06%25%20increase%20in%20Recall%401000%2C%0Aalongside%20reductions%20in%20training%20and%20retrieval%20durations%20by%2068.75%25%20and%2099.79%25%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15276v2&entry.124074799=Read"},
{"title": "Evaluating Decision Optimality of Autonomous Driving via Metamorphic\n  Testing", "author": "Mingfei Cheng and Yuan Zhou and Xiaofei Xie and Junjie Wang and Guozhu Meng and Kairui Yang", "abstract": "  Autonomous Driving System (ADS) testing is crucial in ADS development, with\nthe current primary focus being on safety. However, the evaluation of\nnon-safety-critical performance, particularly the ADS's ability to make optimal\ndecisions and produce optimal paths for autonomous vehicles (AVs), is equally\nvital to ensure the intelligence and reduce risks of AVs. Currently, there is\nlittle work dedicated to assessing ADSs' optimal decision-making performance\ndue to the lack of corresponding oracles and the difficulty in generating\nscenarios with non-optimal decisions. In this paper, we focus on evaluating the\ndecision-making quality of an ADS and propose the first method for detecting\nnon-optimal decision scenarios (NoDSs), where the ADS does not compute optimal\npaths for AVs. Firstly, to deal with the oracle problem, we propose a novel\nmetamorphic relation (MR) aimed at exposing violations of optimal decisions.\nThe MR identifies the property that the ADS should retain optimal decisions\nwhen the optimal path remains unaffected by non-invasive changes. Subsequently,\nwe develop a new framework, Decictor, designed to generate NoDSs efficiently.\nDecictor comprises three main components: Non-invasive Mutation, MR Check, and\nFeedback. The Non-invasive Mutation ensures that the original optimal path in\nthe mutated scenarios is not affected, while the MR Check is responsible for\ndetermining whether non-optimal decisions are made. To enhance the\neffectiveness of identifying NoDSs, we design a feedback metric that combines\nboth spatial and temporal aspects of the AV's movement. We evaluate Decictor on\nBaidu Apollo, an open-source and production-grade ADS. The experimental results\nvalidate the effectiveness of Decictor in detecting non-optimal decisions of\nADSs. Our work provides valuable and original insights into evaluating the\nnon-safety-critical performance of ADSs.\n", "link": "http://arxiv.org/abs/2402.18393v1", "date": "2024-02-28", "relevancy": 1.4975, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5019}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4985}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4931}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Decision%20Optimality%20of%20Autonomous%20Driving%20via%20Metamorphic%0A%20%20Testing&entry.906535625=Mingfei%20Cheng%20and%20Yuan%20Zhou%20and%20Xiaofei%20Xie%20and%20Junjie%20Wang%20and%20Guozhu%20Meng%20and%20Kairui%20Yang&entry.1292438233=%20%20Autonomous%20Driving%20System%20%28ADS%29%20testing%20is%20crucial%20in%20ADS%20development%2C%20with%0Athe%20current%20primary%20focus%20being%20on%20safety.%20However%2C%20the%20evaluation%20of%0Anon-safety-critical%20performance%2C%20particularly%20the%20ADS%27s%20ability%20to%20make%20optimal%0Adecisions%20and%20produce%20optimal%20paths%20for%20autonomous%20vehicles%20%28AVs%29%2C%20is%20equally%0Avital%20to%20ensure%20the%20intelligence%20and%20reduce%20risks%20of%20AVs.%20Currently%2C%20there%20is%0Alittle%20work%20dedicated%20to%20assessing%20ADSs%27%20optimal%20decision-making%20performance%0Adue%20to%20the%20lack%20of%20corresponding%20oracles%20and%20the%20difficulty%20in%20generating%0Ascenarios%20with%20non-optimal%20decisions.%20In%20this%20paper%2C%20we%20focus%20on%20evaluating%20the%0Adecision-making%20quality%20of%20an%20ADS%20and%20propose%20the%20first%20method%20for%20detecting%0Anon-optimal%20decision%20scenarios%20%28NoDSs%29%2C%20where%20the%20ADS%20does%20not%20compute%20optimal%0Apaths%20for%20AVs.%20Firstly%2C%20to%20deal%20with%20the%20oracle%20problem%2C%20we%20propose%20a%20novel%0Ametamorphic%20relation%20%28MR%29%20aimed%20at%20exposing%20violations%20of%20optimal%20decisions.%0AThe%20MR%20identifies%20the%20property%20that%20the%20ADS%20should%20retain%20optimal%20decisions%0Awhen%20the%20optimal%20path%20remains%20unaffected%20by%20non-invasive%20changes.%20Subsequently%2C%0Awe%20develop%20a%20new%20framework%2C%20Decictor%2C%20designed%20to%20generate%20NoDSs%20efficiently.%0ADecictor%20comprises%20three%20main%20components%3A%20Non-invasive%20Mutation%2C%20MR%20Check%2C%20and%0AFeedback.%20The%20Non-invasive%20Mutation%20ensures%20that%20the%20original%20optimal%20path%20in%0Athe%20mutated%20scenarios%20is%20not%20affected%2C%20while%20the%20MR%20Check%20is%20responsible%20for%0Adetermining%20whether%20non-optimal%20decisions%20are%20made.%20To%20enhance%20the%0Aeffectiveness%20of%20identifying%20NoDSs%2C%20we%20design%20a%20feedback%20metric%20that%20combines%0Aboth%20spatial%20and%20temporal%20aspects%20of%20the%20AV%27s%20movement.%20We%20evaluate%20Decictor%20on%0ABaidu%20Apollo%2C%20an%20open-source%20and%20production-grade%20ADS.%20The%20experimental%20results%0Avalidate%20the%20effectiveness%20of%20Decictor%20in%20detecting%20non-optimal%20decisions%20of%0AADSs.%20Our%20work%20provides%20valuable%20and%20original%20insights%20into%20evaluating%20the%0Anon-safety-critical%20performance%20of%20ADSs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18393v1&entry.124074799=Read"},
{"title": "A non-intrusive machine learning framework for debiasing long-time\n  coarse resolution climate simulations and quantifying rare events statistics", "author": "Benedikt Barthel Sorensen and Alexis Charalampopoulos and Shixuan Zhang and Bryce Harrop and Ruby Leung and Themistoklis Sapsis", "abstract": "  Due to the rapidly changing climate, the frequency and severity of extreme\nweather is expected to increase over the coming decades. As fully-resolved\nclimate simulations remain computationally intractable, policy makers must rely\non coarse-models to quantify risk for extremes. However, coarse models suffer\nfrom inherent bias due to the ignored \"sub-grid\" scales. We propose a framework\nto non-intrusively debias coarse-resolution climate predictions using\nneural-network (NN) correction operators. Previous efforts have attempted to\ntrain such operators using loss functions that match statistics. However, this\napproach falls short with events that have longer return period than that of\nthe training data, since the reference statistics have not converged. Here, the\nscope is to formulate a learning method that allows for correction of dynamics\nand quantification of extreme events with longer return period than the\ntraining data. The key obstacle is the chaotic nature of the underlying\ndynamics. To overcome this challenge, we introduce a dynamical systems approach\nwhere the correction operator is trained using reference data and a coarse\nmodel simulation nudged towards that reference. The method is demonstrated on\ndebiasing an under-resolved quasi-geostrophic model and the Energy Exascale\nEarth System Model (E3SM). For the former, our method enables the\nquantification of events that have return period two orders longer than the\ntraining data. For the latter, when trained on 8 years of ERA5 data, our\napproach is able to correct the coarse E3SM output to closely reflect the\n36-year ERA5 statistics for all prognostic variables and significantly reduce\ntheir spatial biases.\n", "link": "http://arxiv.org/abs/2402.18484v1", "date": "2024-02-28", "relevancy": 1.4954, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4862}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20non-intrusive%20machine%20learning%20framework%20for%20debiasing%20long-time%0A%20%20coarse%20resolution%20climate%20simulations%20and%20quantifying%20rare%20events%20statistics&entry.906535625=Benedikt%20Barthel%20Sorensen%20and%20Alexis%20Charalampopoulos%20and%20Shixuan%20Zhang%20and%20Bryce%20Harrop%20and%20Ruby%20Leung%20and%20Themistoklis%20Sapsis&entry.1292438233=%20%20Due%20to%20the%20rapidly%20changing%20climate%2C%20the%20frequency%20and%20severity%20of%20extreme%0Aweather%20is%20expected%20to%20increase%20over%20the%20coming%20decades.%20As%20fully-resolved%0Aclimate%20simulations%20remain%20computationally%20intractable%2C%20policy%20makers%20must%20rely%0Aon%20coarse-models%20to%20quantify%20risk%20for%20extremes.%20However%2C%20coarse%20models%20suffer%0Afrom%20inherent%20bias%20due%20to%20the%20ignored%20%22sub-grid%22%20scales.%20We%20propose%20a%20framework%0Ato%20non-intrusively%20debias%20coarse-resolution%20climate%20predictions%20using%0Aneural-network%20%28NN%29%20correction%20operators.%20Previous%20efforts%20have%20attempted%20to%0Atrain%20such%20operators%20using%20loss%20functions%20that%20match%20statistics.%20However%2C%20this%0Aapproach%20falls%20short%20with%20events%20that%20have%20longer%20return%20period%20than%20that%20of%0Athe%20training%20data%2C%20since%20the%20reference%20statistics%20have%20not%20converged.%20Here%2C%20the%0Ascope%20is%20to%20formulate%20a%20learning%20method%20that%20allows%20for%20correction%20of%20dynamics%0Aand%20quantification%20of%20extreme%20events%20with%20longer%20return%20period%20than%20the%0Atraining%20data.%20The%20key%20obstacle%20is%20the%20chaotic%20nature%20of%20the%20underlying%0Adynamics.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20a%20dynamical%20systems%20approach%0Awhere%20the%20correction%20operator%20is%20trained%20using%20reference%20data%20and%20a%20coarse%0Amodel%20simulation%20nudged%20towards%20that%20reference.%20The%20method%20is%20demonstrated%20on%0Adebiasing%20an%20under-resolved%20quasi-geostrophic%20model%20and%20the%20Energy%20Exascale%0AEarth%20System%20Model%20%28E3SM%29.%20For%20the%20former%2C%20our%20method%20enables%20the%0Aquantification%20of%20events%20that%20have%20return%20period%20two%20orders%20longer%20than%20the%0Atraining%20data.%20For%20the%20latter%2C%20when%20trained%20on%208%20years%20of%20ERA5%20data%2C%20our%0Aapproach%20is%20able%20to%20correct%20the%20coarse%20E3SM%20output%20to%20closely%20reflect%20the%0A36-year%20ERA5%20statistics%20for%20all%20prognostic%20variables%20and%20significantly%20reduce%0Atheir%20spatial%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18484v1&entry.124074799=Read"},
{"title": "Robust Quantification of Percent Emphysema on CT via Domain Attention:\n  the Multi-Ethnic Study of Atherosclerosis (MESA) Lung Study", "author": "Xuzhe Zhang and Elsa D. Angelini and Eric A. Hoffman and Karol E. Watson and Benjamin M. Smith and R. Graham Barr and Andrew F. Laine", "abstract": "  Robust quantification of pulmonary emphysema on computed tomography (CT)\nremains challenging for large-scale research studies that involve scans from\ndifferent scanner types and for translation to clinical scans. Existing studies\nhave explored several directions to tackle this challenge, including density\ncorrection, noise filtering, regression, hidden Markov measure field (HMMF)\nmodel-based segmentation, and volume-adjusted lung density. Despite some\npromising results, previous studies either required a tedious workflow or\nlimited opportunities for downstream emphysema subtyping, limiting efficient\nadaptation on a large-scale study. To alleviate this dilemma, we developed an\nend-to-end deep learning framework based on an existing HMMF segmentation\nframework. We first demonstrate that a regular UNet cannot replicate the\nexisting HMMF results because of the lack of scanner priors. We then design a\nnovel domain attention block to fuse image feature with quantitative scanner\npriors which significantly improves the results.\n", "link": "http://arxiv.org/abs/2402.18383v1", "date": "2024-02-28", "relevancy": 1.4863, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5368}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4931}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4599}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Quantification%20of%20Percent%20Emphysema%20on%20CT%20via%20Domain%20Attention%3A%0A%20%20the%20Multi-Ethnic%20Study%20of%20Atherosclerosis%20%28MESA%29%20Lung%20Study&entry.906535625=Xuzhe%20Zhang%20and%20Elsa%20D.%20Angelini%20and%20Eric%20A.%20Hoffman%20and%20Karol%20E.%20Watson%20and%20Benjamin%20M.%20Smith%20and%20R.%20Graham%20Barr%20and%20Andrew%20F.%20Laine&entry.1292438233=%20%20Robust%20quantification%20of%20pulmonary%20emphysema%20on%20computed%20tomography%20%28CT%29%0Aremains%20challenging%20for%20large-scale%20research%20studies%20that%20involve%20scans%20from%0Adifferent%20scanner%20types%20and%20for%20translation%20to%20clinical%20scans.%20Existing%20studies%0Ahave%20explored%20several%20directions%20to%20tackle%20this%20challenge%2C%20including%20density%0Acorrection%2C%20noise%20filtering%2C%20regression%2C%20hidden%20Markov%20measure%20field%20%28HMMF%29%0Amodel-based%20segmentation%2C%20and%20volume-adjusted%20lung%20density.%20Despite%20some%0Apromising%20results%2C%20previous%20studies%20either%20required%20a%20tedious%20workflow%20or%0Alimited%20opportunities%20for%20downstream%20emphysema%20subtyping%2C%20limiting%20efficient%0Aadaptation%20on%20a%20large-scale%20study.%20To%20alleviate%20this%20dilemma%2C%20we%20developed%20an%0Aend-to-end%20deep%20learning%20framework%20based%20on%20an%20existing%20HMMF%20segmentation%0Aframework.%20We%20first%20demonstrate%20that%20a%20regular%20UNet%20cannot%20replicate%20the%0Aexisting%20HMMF%20results%20because%20of%20the%20lack%20of%20scanner%20priors.%20We%20then%20design%20a%0Anovel%20domain%20attention%20block%20to%20fuse%20image%20feature%20with%20quantitative%20scanner%0Apriors%20which%20significantly%20improves%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18383v1&entry.124074799=Read"},
{"title": "Looping in the Human Collaborative and Explainable Bayesian Optimization", "author": "Masaki Adachi and Brady Planden and David A. Howey and Michael A. Osborne and Sebastian Orbell and Natalia Ares and Krikamol Muandet and Siu Lun Chau", "abstract": "  Like many optimizers, Bayesian optimization often falls short of gaining user\ntrust due to opacity. While attempts have been made to develop human-centric\noptimizers, they typically assume user knowledge is well-specified and\nerror-free, employing users mainly as supervisors of the optimization process.\nWe relax these assumptions and propose a more balanced human-AI partnership\nwith our Collaborative and Explainable Bayesian Optimization (CoExBO)\nframework. Instead of explicitly requiring a user to provide a knowledge model,\nCoExBO employs preference learning to seamlessly integrate human insights into\nthe optimization, resulting in algorithmic suggestions that resonate with user\npreference. CoExBO explains its candidate selection every iteration to foster\ntrust, empowering users with a clearer grasp of the optimization. Furthermore,\nCoExBO offers a no-harm guarantee, allowing users to make mistakes; even with\nextreme adversarial interventions, the algorithm converges asymptotically to a\nvanilla Bayesian optimization. We validate CoExBO's efficacy through human-AI\nteaming experiments in lithium-ion battery design, highlighting substantial\nimprovements over conventional methods. Code is available\nhttps://github.com/ma921/CoExBO.\n", "link": "http://arxiv.org/abs/2310.17273v4", "date": "2024-02-28", "relevancy": 1.468, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5089}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4647}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looping%20in%20the%20Human%20Collaborative%20and%20Explainable%20Bayesian%20Optimization&entry.906535625=Masaki%20Adachi%20and%20Brady%20Planden%20and%20David%20A.%20Howey%20and%20Michael%20A.%20Osborne%20and%20Sebastian%20Orbell%20and%20Natalia%20Ares%20and%20Krikamol%20Muandet%20and%20Siu%20Lun%20Chau&entry.1292438233=%20%20Like%20many%20optimizers%2C%20Bayesian%20optimization%20often%20falls%20short%20of%20gaining%20user%0Atrust%20due%20to%20opacity.%20While%20attempts%20have%20been%20made%20to%20develop%20human-centric%0Aoptimizers%2C%20they%20typically%20assume%20user%20knowledge%20is%20well-specified%20and%0Aerror-free%2C%20employing%20users%20mainly%20as%20supervisors%20of%20the%20optimization%20process.%0AWe%20relax%20these%20assumptions%20and%20propose%20a%20more%20balanced%20human-AI%20partnership%0Awith%20our%20Collaborative%20and%20Explainable%20Bayesian%20Optimization%20%28CoExBO%29%0Aframework.%20Instead%20of%20explicitly%20requiring%20a%20user%20to%20provide%20a%20knowledge%20model%2C%0ACoExBO%20employs%20preference%20learning%20to%20seamlessly%20integrate%20human%20insights%20into%0Athe%20optimization%2C%20resulting%20in%20algorithmic%20suggestions%20that%20resonate%20with%20user%0Apreference.%20CoExBO%20explains%20its%20candidate%20selection%20every%20iteration%20to%20foster%0Atrust%2C%20empowering%20users%20with%20a%20clearer%20grasp%20of%20the%20optimization.%20Furthermore%2C%0ACoExBO%20offers%20a%20no-harm%20guarantee%2C%20allowing%20users%20to%20make%20mistakes%3B%20even%20with%0Aextreme%20adversarial%20interventions%2C%20the%20algorithm%20converges%20asymptotically%20to%20a%0Avanilla%20Bayesian%20optimization.%20We%20validate%20CoExBO%27s%20efficacy%20through%20human-AI%0Ateaming%20experiments%20in%20lithium-ion%20battery%20design%2C%20highlighting%20substantial%0Aimprovements%20over%20conventional%20methods.%20Code%20is%20available%0Ahttps%3A//github.com/ma921/CoExBO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17273v4&entry.124074799=Read"},
{"title": "Unveiling the Potential of Robustness in Evaluating Causal Inference\n  Models", "author": "Yiyan Huang and Cheuk Hang Leung and Siyi Wang and Yijun Li and Qi Wu", "abstract": "  The growing demand for personalized decision-making has led to a surge of\ninterest in estimating the Conditional Average Treatment Effect (CATE). The\nintersection of machine learning and causal inference has yielded various\neffective CATE estimators. However, deploying these estimators in practice is\noften hindered by the absence of counterfactual labels, making it challenging\nto select the desirable CATE estimator using conventional model selection\nprocedures like cross-validation. Existing approaches for CATE estimator\nselection, such as plug-in and pseudo-outcome metrics, face two inherent\nchallenges. Firstly, they are required to determine the metric form and the\nunderlying machine learning models for fitting nuisance parameters or plug-in\nlearners. Secondly, they lack a specific focus on selecting a robust estimator.\nTo address these challenges, this paper introduces a novel approach, the\nDistributionally Robust Metric (DRM), for CATE estimator selection. The\nproposed DRM not only eliminates the need to fit additional models but also\nexcels at selecting a robust CATE estimator. Experimental studies demonstrate\nthe efficacy of the DRM method, showcasing its consistent effectiveness in\nidentifying superior estimators while mitigating the risk of selecting inferior\nones.\n", "link": "http://arxiv.org/abs/2402.18392v1", "date": "2024-02-28", "relevancy": 1.458, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5111}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5043}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4686}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Potential%20of%20Robustness%20in%20Evaluating%20Causal%20Inference%0A%20%20Models&entry.906535625=Yiyan%20Huang%20and%20Cheuk%20Hang%20Leung%20and%20Siyi%20Wang%20and%20Yijun%20Li%20and%20Qi%20Wu&entry.1292438233=%20%20The%20growing%20demand%20for%20personalized%20decision-making%20has%20led%20to%20a%20surge%20of%0Ainterest%20in%20estimating%20the%20Conditional%20Average%20Treatment%20Effect%20%28CATE%29.%20The%0Aintersection%20of%20machine%20learning%20and%20causal%20inference%20has%20yielded%20various%0Aeffective%20CATE%20estimators.%20However%2C%20deploying%20these%20estimators%20in%20practice%20is%0Aoften%20hindered%20by%20the%20absence%20of%20counterfactual%20labels%2C%20making%20it%20challenging%0Ato%20select%20the%20desirable%20CATE%20estimator%20using%20conventional%20model%20selection%0Aprocedures%20like%20cross-validation.%20Existing%20approaches%20for%20CATE%20estimator%0Aselection%2C%20such%20as%20plug-in%20and%20pseudo-outcome%20metrics%2C%20face%20two%20inherent%0Achallenges.%20Firstly%2C%20they%20are%20required%20to%20determine%20the%20metric%20form%20and%20the%0Aunderlying%20machine%20learning%20models%20for%20fitting%20nuisance%20parameters%20or%20plug-in%0Alearners.%20Secondly%2C%20they%20lack%20a%20specific%20focus%20on%20selecting%20a%20robust%20estimator.%0ATo%20address%20these%20challenges%2C%20this%20paper%20introduces%20a%20novel%20approach%2C%20the%0ADistributionally%20Robust%20Metric%20%28DRM%29%2C%20for%20CATE%20estimator%20selection.%20The%0Aproposed%20DRM%20not%20only%20eliminates%20the%20need%20to%20fit%20additional%20models%20but%20also%0Aexcels%20at%20selecting%20a%20robust%20CATE%20estimator.%20Experimental%20studies%20demonstrate%0Athe%20efficacy%20of%20the%20DRM%20method%2C%20showcasing%20its%20consistent%20effectiveness%20in%0Aidentifying%20superior%20estimators%20while%20mitigating%20the%20risk%20of%20selecting%20inferior%0Aones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18392v1&entry.124074799=Read"},
{"title": "Exploring the Promise and Limits of Real-Time Recurrent Learning", "author": "Kazuki Irie and Anand Gopalakrishnan and J\u00fcrgen Schmidhuber", "abstract": "  Real-time recurrent learning (RTRL) for sequence-processing recurrent neural\nnetworks (RNNs) offers certain conceptual advantages over backpropagation\nthrough time (BPTT). RTRL requires neither caching past activations nor\ntruncating context, and enables online learning. However, RTRL's time and space\ncomplexity make it impractical. To overcome this problem, most recent work on\nRTRL focuses on approximation theories, while experiments are often limited to\ndiagnostic settings. Here we explore the practical promise of RTRL in more\nrealistic settings. We study actor-critic methods that combine RTRL and policy\ngradients, and test them in several subsets of DMLab-30, ProcGen, and\nAtari-2600 environments. On DMLab memory tasks, our system trained on fewer\nthan 1.2 B environmental frames is competitive with or outperforms well-known\nIMPALA and R2D2 baselines trained on 10 B frames. To scale to such challenging\ntasks, we focus on certain well-known neural architectures with element-wise\nrecurrence, allowing for tractable RTRL without approximation. Importantly, we\nalso discuss rarely addressed limitations of RTRL in real-world applications,\nsuch as its complexity in the multi-layer case.\n", "link": "http://arxiv.org/abs/2305.19044v3", "date": "2024-02-28", "relevancy": 1.4515, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4876}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4804}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4778}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Promise%20and%20Limits%20of%20Real-Time%20Recurrent%20Learning&entry.906535625=Kazuki%20Irie%20and%20Anand%20Gopalakrishnan%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Real-time%20recurrent%20learning%20%28RTRL%29%20for%20sequence-processing%20recurrent%20neural%0Anetworks%20%28RNNs%29%20offers%20certain%20conceptual%20advantages%20over%20backpropagation%0Athrough%20time%20%28BPTT%29.%20RTRL%20requires%20neither%20caching%20past%20activations%20nor%0Atruncating%20context%2C%20and%20enables%20online%20learning.%20However%2C%20RTRL%27s%20time%20and%20space%0Acomplexity%20make%20it%20impractical.%20To%20overcome%20this%20problem%2C%20most%20recent%20work%20on%0ARTRL%20focuses%20on%20approximation%20theories%2C%20while%20experiments%20are%20often%20limited%20to%0Adiagnostic%20settings.%20Here%20we%20explore%20the%20practical%20promise%20of%20RTRL%20in%20more%0Arealistic%20settings.%20We%20study%20actor-critic%20methods%20that%20combine%20RTRL%20and%20policy%0Agradients%2C%20and%20test%20them%20in%20several%20subsets%20of%20DMLab-30%2C%20ProcGen%2C%20and%0AAtari-2600%20environments.%20On%20DMLab%20memory%20tasks%2C%20our%20system%20trained%20on%20fewer%0Athan%201.2%20B%20environmental%20frames%20is%20competitive%20with%20or%20outperforms%20well-known%0AIMPALA%20and%20R2D2%20baselines%20trained%20on%2010%20B%20frames.%20To%20scale%20to%20such%20challenging%0Atasks%2C%20we%20focus%20on%20certain%20well-known%20neural%20architectures%20with%20element-wise%0Arecurrence%2C%20allowing%20for%20tractable%20RTRL%20without%20approximation.%20Importantly%2C%20we%0Aalso%20discuss%20rarely%20addressed%20limitations%20of%20RTRL%20in%20real-world%20applications%2C%0Asuch%20as%20its%20complexity%20in%20the%20multi-layer%20case.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.19044v3&entry.124074799=Read"},
{"title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt\n  Templates", "author": "Kaifeng Lyu and Haoyu Zhao and Xinran Gu and Dingli Yu and Anirudh Goyal and Sanjeev Arora", "abstract": "  Public LLMs such as the Llama 2-Chat have driven huge activity in LLM\nresearch. These models underwent alignment training and were considered safe.\nRecently Qi et al. (2023) reported that even benign fine-tuning (e.g., on\nseemingly safe datasets) can give rise to unsafe behaviors in the models. The\ncurrent paper is about methods and best practices to mitigate such loss of\nalignment. Through extensive experiments on several chat models (Meta's Llama\n2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo),\nthis paper uncovers that the prompt templates used during fine-tuning and\ninference play a crucial role in preserving safety alignment, and proposes the\n\"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a\nsafety prompt, but include it at test time. Fine-tuning experiments on GSM8K,\nChatDoctor, and OpenOrca show that PTST significantly reduces the rise of\nunsafe behaviors, and even almost eliminates them in some cases.\n", "link": "http://arxiv.org/abs/2402.18540v1", "date": "2024-02-28", "relevancy": 1.4394, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4912}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4838}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4473}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keeping%20LLMs%20Aligned%20After%20Fine-tuning%3A%20The%20Crucial%20Role%20of%20Prompt%0A%20%20Templates&entry.906535625=Kaifeng%20Lyu%20and%20Haoyu%20Zhao%20and%20Xinran%20Gu%20and%20Dingli%20Yu%20and%20Anirudh%20Goyal%20and%20Sanjeev%20Arora&entry.1292438233=%20%20Public%20LLMs%20such%20as%20the%20Llama%202-Chat%20have%20driven%20huge%20activity%20in%20LLM%0Aresearch.%20These%20models%20underwent%20alignment%20training%20and%20were%20considered%20safe.%0ARecently%20Qi%20et%20al.%20%282023%29%20reported%20that%20even%20benign%20fine-tuning%20%28e.g.%2C%20on%0Aseemingly%20safe%20datasets%29%20can%20give%20rise%20to%20unsafe%20behaviors%20in%20the%20models.%20The%0Acurrent%20paper%20is%20about%20methods%20and%20best%20practices%20to%20mitigate%20such%20loss%20of%0Aalignment.%20Through%20extensive%20experiments%20on%20several%20chat%20models%20%28Meta%27s%20Llama%0A2-Chat%2C%20Mistral%20AI%27s%20Mistral%207B%20Instruct%20v0.2%2C%20and%20OpenAI%27s%20GPT-3.5%20Turbo%29%2C%0Athis%20paper%20uncovers%20that%20the%20prompt%20templates%20used%20during%20fine-tuning%20and%0Ainference%20play%20a%20crucial%20role%20in%20preserving%20safety%20alignment%2C%20and%20proposes%20the%0A%22Pure%20Tuning%2C%20Safe%20Testing%22%20%28PTST%29%20principle%20--%20fine-tune%20models%20without%20a%0Asafety%20prompt%2C%20but%20include%20it%20at%20test%20time.%20Fine-tuning%20experiments%20on%20GSM8K%2C%0AChatDoctor%2C%20and%20OpenOrca%20show%20that%20PTST%20significantly%20reduces%20the%20rise%20of%0Aunsafe%20behaviors%2C%20and%20even%20almost%20eliminates%20them%20in%20some%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18540v1&entry.124074799=Read"},
{"title": "Approaching Human-Level Forecasting with Language Models", "author": "Danny Halawi and Fred Zhang and Chen Yueh-Han and Jacob Steinhardt", "abstract": "  Forecasting future events is important for policy and decision making. In\nthis work, we study whether language models (LMs) can forecast at the level of\ncompetitive human forecasters. Towards this goal, we develop a\nretrieval-augmented LM system designed to automatically search for relevant\ninformation, generate forecasts, and aggregate predictions. To facilitate our\nstudy, we collect a large dataset of questions from competitive forecasting\nplatforms. Under a test set published after the knowledge cut-offs of our LMs,\nwe evaluate the end-to-end performance of our system against the aggregates of\nhuman forecasts. On average, the system nears the crowd aggregate of\ncompetitive forecasters, and in some settings surpasses it. Our work suggests\nthat using LMs to forecast the future could provide accurate predictions at\nscale and help to inform institutional decision making.\n", "link": "http://arxiv.org/abs/2402.18563v1", "date": "2024-02-28", "relevancy": 1.439, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4997}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4779}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4724}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approaching%20Human-Level%20Forecasting%20with%20Language%20Models&entry.906535625=Danny%20Halawi%20and%20Fred%20Zhang%20and%20Chen%20Yueh-Han%20and%20Jacob%20Steinhardt&entry.1292438233=%20%20Forecasting%20future%20events%20is%20important%20for%20policy%20and%20decision%20making.%20In%0Athis%20work%2C%20we%20study%20whether%20language%20models%20%28LMs%29%20can%20forecast%20at%20the%20level%20of%0Acompetitive%20human%20forecasters.%20Towards%20this%20goal%2C%20we%20develop%20a%0Aretrieval-augmented%20LM%20system%20designed%20to%20automatically%20search%20for%20relevant%0Ainformation%2C%20generate%20forecasts%2C%20and%20aggregate%20predictions.%20To%20facilitate%20our%0Astudy%2C%20we%20collect%20a%20large%20dataset%20of%20questions%20from%20competitive%20forecasting%0Aplatforms.%20Under%20a%20test%20set%20published%20after%20the%20knowledge%20cut-offs%20of%20our%20LMs%2C%0Awe%20evaluate%20the%20end-to-end%20performance%20of%20our%20system%20against%20the%20aggregates%20of%0Ahuman%20forecasts.%20On%20average%2C%20the%20system%20nears%20the%20crowd%20aggregate%20of%0Acompetitive%20forecasters%2C%20and%20in%20some%20settings%20surpasses%20it.%20Our%20work%20suggests%0Athat%20using%20LMs%20to%20forecast%20the%20future%20could%20provide%20accurate%20predictions%20at%0Ascale%20and%20help%20to%20inform%20institutional%20decision%20making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18563v1&entry.124074799=Read"},
{"title": "Are you Struggling? Dataset and Baselines for Struggle Determination in\n  Assembly Videos", "author": "Shijia Feng and Michael Wray and Brian Sullivan and Youngkyoon Jang and Casimir Ludwig and Iain Gilchrist and Walterio Mayol-Cuevas", "abstract": "  Determining when people are struggling from video enables a finer-grained\nunderstanding of actions and opens opportunities for building intelligent\nsupport visual interfaces. In this paper, we present a new dataset with three\nassembly activities and corresponding performance baselines for the\ndetermination of struggle from video. Three real-world problem-solving\nactivities including assembling plumbing pipes (Pipes-Struggle), pitching\ncamping tents (Tent-Struggle) and solving the Tower of Hanoi puzzle\n(Tower-Struggle) are introduced. Video segments were scored w.r.t. the level of\nstruggle as perceived by annotators using a forced choice 4-point scale. Each\nvideo segment was annotated by a single expert annotator in addition to\ncrowd-sourced annotations. The dataset is the first struggle annotation dataset\nand contains 5.1 hours of video and 725,100 frames from 73 participants in\ntotal. We evaluate three decision-making tasks: struggle classification,\nstruggle level regression, and struggle label distribution learning. We provide\nbaseline results for each of the tasks utilising several mainstream deep neural\nnetworks, along with an ablation study and visualisation of results. Our work\nis motivated toward assistive systems that analyze struggle, support users\nduring manual activities and encourage learning, as well as other video\nunderstanding competencies.\n", "link": "http://arxiv.org/abs/2402.11057v2", "date": "2024-02-28", "relevancy": 1.4386, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4881}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4718}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4659}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20you%20Struggling%3F%20Dataset%20and%20Baselines%20for%20Struggle%20Determination%20in%0A%20%20Assembly%20Videos&entry.906535625=Shijia%20Feng%20and%20Michael%20Wray%20and%20Brian%20Sullivan%20and%20Youngkyoon%20Jang%20and%20Casimir%20Ludwig%20and%20Iain%20Gilchrist%20and%20Walterio%20Mayol-Cuevas&entry.1292438233=%20%20Determining%20when%20people%20are%20struggling%20from%20video%20enables%20a%20finer-grained%0Aunderstanding%20of%20actions%20and%20opens%20opportunities%20for%20building%20intelligent%0Asupport%20visual%20interfaces.%20In%20this%20paper%2C%20we%20present%20a%20new%20dataset%20with%20three%0Aassembly%20activities%20and%20corresponding%20performance%20baselines%20for%20the%0Adetermination%20of%20struggle%20from%20video.%20Three%20real-world%20problem-solving%0Aactivities%20including%20assembling%20plumbing%20pipes%20%28Pipes-Struggle%29%2C%20pitching%0Acamping%20tents%20%28Tent-Struggle%29%20and%20solving%20the%20Tower%20of%20Hanoi%20puzzle%0A%28Tower-Struggle%29%20are%20introduced.%20Video%20segments%20were%20scored%20w.r.t.%20the%20level%20of%0Astruggle%20as%20perceived%20by%20annotators%20using%20a%20forced%20choice%204-point%20scale.%20Each%0Avideo%20segment%20was%20annotated%20by%20a%20single%20expert%20annotator%20in%20addition%20to%0Acrowd-sourced%20annotations.%20The%20dataset%20is%20the%20first%20struggle%20annotation%20dataset%0Aand%20contains%205.1%20hours%20of%20video%20and%20725%2C100%20frames%20from%2073%20participants%20in%0Atotal.%20We%20evaluate%20three%20decision-making%20tasks%3A%20struggle%20classification%2C%0Astruggle%20level%20regression%2C%20and%20struggle%20label%20distribution%20learning.%20We%20provide%0Abaseline%20results%20for%20each%20of%20the%20tasks%20utilising%20several%20mainstream%20deep%20neural%0Anetworks%2C%20along%20with%20an%20ablation%20study%20and%20visualisation%20of%20results.%20Our%20work%0Ais%20motivated%20toward%20assistive%20systems%20that%20analyze%20struggle%2C%20support%20users%0Aduring%20manual%20activities%20and%20encourage%20learning%2C%20as%20well%20as%20other%20video%0Aunderstanding%20competencies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11057v2&entry.124074799=Read"},
{"title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional\n  Preference Alignment with Multi-Objective Rewards", "author": "Haoxiang Wang and Yong Lin and Wei Xiong and Rui Yang and Shizhe Diao and Shuang Qiu and Han Zhao and Tong Zhang", "abstract": "  Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).\n", "link": "http://arxiv.org/abs/2402.18571v1", "date": "2024-02-28", "relevancy": 1.437, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4845}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4803}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.464}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arithmetic%20Control%20of%20LLMs%20for%20Diverse%20User%20Preferences%3A%20Directional%0A%20%20Preference%20Alignment%20with%20Multi-Objective%20Rewards&entry.906535625=Haoxiang%20Wang%20and%20Yong%20Lin%20and%20Wei%20Xiong%20and%20Rui%20Yang%20and%20Shizhe%20Diao%20and%20Shuang%20Qiu%20and%20Han%20Zhao%20and%20Tong%20Zhang&entry.1292438233=%20%20Fine-grained%20control%20over%20large%20language%20models%20%28LLMs%29%20remains%20a%20significant%0Achallenge%2C%20hindering%20their%20adaptability%20to%20diverse%20user%20needs.%20While%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20shows%20promise%20in%20aligning%0ALLMs%2C%20its%20reliance%20on%20scalar%20rewards%20often%20limits%20its%20ability%20to%20capture%0Adiverse%20user%20preferences%20in%20real-world%20applications.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20the%20Directional%20Preference%20Alignment%20%28DPA%29%20framework.%0AUnlike%20the%20scalar-reward%20RLHF%2C%20DPA%20incorporates%20multi-objective%20reward%20modeling%0Ato%20represent%20diverse%20preference%20profiles.%20Additionally%2C%20DPA%20models%20user%0Apreferences%20as%20directions%20%28i.e.%2C%20unit%20vectors%29%20in%20the%20reward%20space%20to%20achieve%0Auser-dependent%20preference%20control.%20Our%20method%20involves%20training%20a%0Amulti-objective%20reward%20model%20and%20then%20fine-tuning%20the%20LLM%20with%20a%0Apreference-conditioned%20variant%20of%20Rejection%20Sampling%20Finetuning%20%28RSF%29%2C%20an%20RLHF%0Amethod%20adopted%20by%20Llama%202.%20This%20method%20enjoys%20a%20better%20performance%20trade-off%0Aacross%20various%20reward%20objectives.%20In%20comparison%20with%20the%20scalar-reward%20RLHF%2C%0ADPA%20offers%20users%20intuitive%20control%20over%20LLM%20generation%3A%20they%20can%20arithmetically%0Aspecify%20their%20desired%20trade-offs%20%28e.g.%2C%20more%20helpfulness%20with%20less%20verbosity%29.%0AWe%20also%20validate%20the%20effectiveness%20of%20DPA%20with%20real-world%20alignment%20experiments%0Aon%20Mistral-7B.%20Our%20method%20provides%20straightforward%20arithmetic%20control%20over%20the%0Atrade-off%20between%20helpfulness%20and%20verbosity%20while%20maintaining%20competitive%0Aperformance%20with%20strong%20baselines%20such%20as%20Direct%20Preference%20Optimization%20%28DPO%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18571v1&entry.124074799=Read"},
{"title": "The First Place Solution of WSDM Cup 2024: Leveraging Large Language\n  Models for Conversational Multi-Doc QA", "author": "Yiming Li and Zhao Zhang", "abstract": "  Conversational multi-doc question answering aims to answer specific questions\nbased on the retrieved documents as well as the contextual conversations. In\nthis paper, we introduce our winning approach for the \"Conversational Multi-Doc\nQA\" challenge in WSDM Cup 2024, which exploits the superior natural language\nunderstanding and generation capability of Large Language Models (LLMs). We\nfirst adapt LLMs to the task, then devise a hybrid training strategy to make\nthe most of in-domain unlabeled data. Moreover, an advanced text embedding\nmodel is adopted to filter out potentially irrelevant documents and several\napproaches are designed and compared for the model ensemble. Equipped with all\nthese techniques, our solution finally ranked 1st place in WSDM Cup 2024,\nsurpassing its rivals to a large extent. The source codes have been released at\nhttps://github.com/zhangzhao219/WSDM-Cup-2024.\n", "link": "http://arxiv.org/abs/2402.18385v1", "date": "2024-02-28", "relevancy": 1.4329, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5248}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4682}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4541}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20First%20Place%20Solution%20of%20WSDM%20Cup%202024%3A%20Leveraging%20Large%20Language%0A%20%20Models%20for%20Conversational%20Multi-Doc%20QA&entry.906535625=Yiming%20Li%20and%20Zhao%20Zhang&entry.1292438233=%20%20Conversational%20multi-doc%20question%20answering%20aims%20to%20answer%20specific%20questions%0Abased%20on%20the%20retrieved%20documents%20as%20well%20as%20the%20contextual%20conversations.%20In%0Athis%20paper%2C%20we%20introduce%20our%20winning%20approach%20for%20the%20%22Conversational%20Multi-Doc%0AQA%22%20challenge%20in%20WSDM%20Cup%202024%2C%20which%20exploits%20the%20superior%20natural%20language%0Aunderstanding%20and%20generation%20capability%20of%20Large%20Language%20Models%20%28LLMs%29.%20We%0Afirst%20adapt%20LLMs%20to%20the%20task%2C%20then%20devise%20a%20hybrid%20training%20strategy%20to%20make%0Athe%20most%20of%20in-domain%20unlabeled%20data.%20Moreover%2C%20an%20advanced%20text%20embedding%0Amodel%20is%20adopted%20to%20filter%20out%20potentially%20irrelevant%20documents%20and%20several%0Aapproaches%20are%20designed%20and%20compared%20for%20the%20model%20ensemble.%20Equipped%20with%20all%0Athese%20techniques%2C%20our%20solution%20finally%20ranked%201st%20place%20in%20WSDM%20Cup%202024%2C%0Asurpassing%20its%20rivals%20to%20a%20large%20extent.%20The%20source%20codes%20have%20been%20released%20at%0Ahttps%3A//github.com/zhangzhao219/WSDM-Cup-2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18385v1&entry.124074799=Read"},
{"title": "Deep Confident Steps to New Pockets: Strategies for Docking\n  Generalization", "author": "Gabriele Corso and Arthur Deng and Benjamin Fry and Nicholas Polizzi and Regina Barzilay and Tommi Jaakkola", "abstract": "  Accurate blind docking has the potential to lead to new biological\nbreakthroughs, but for this promise to be realized, docking methods must\ngeneralize well across the proteome. Existing benchmarks, however, fail to\nrigorously assess generalizability. Therefore, we develop DockGen, a new\nbenchmark based on the ligand-binding domains of proteins, and we show that\nexisting machine learning-based docking models have very weak generalization\nabilities. We carefully analyze the scaling laws of ML-based docking and show\nthat, by scaling data and model size, as well as integrating synthetic data\nstrategies, we are able to significantly increase the generalization capacity\nand set new state-of-the-art performance across benchmarks. Further, we propose\nConfidence Bootstrapping, a new training paradigm that solely relies on the\ninteraction between diffusion and confidence models and exploits the\nmulti-resolution generation process of diffusion models. We demonstrate that\nConfidence Bootstrapping significantly improves the ability of ML-based docking\nmethods to dock to unseen protein classes, edging closer to accurate and\ngeneralizable blind docking methods.\n", "link": "http://arxiv.org/abs/2402.18396v1", "date": "2024-02-28", "relevancy": 1.4219, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4876}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4606}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4533}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Confident%20Steps%20to%20New%20Pockets%3A%20Strategies%20for%20Docking%0A%20%20Generalization&entry.906535625=Gabriele%20Corso%20and%20Arthur%20Deng%20and%20Benjamin%20Fry%20and%20Nicholas%20Polizzi%20and%20Regina%20Barzilay%20and%20Tommi%20Jaakkola&entry.1292438233=%20%20Accurate%20blind%20docking%20has%20the%20potential%20to%20lead%20to%20new%20biological%0Abreakthroughs%2C%20but%20for%20this%20promise%20to%20be%20realized%2C%20docking%20methods%20must%0Ageneralize%20well%20across%20the%20proteome.%20Existing%20benchmarks%2C%20however%2C%20fail%20to%0Arigorously%20assess%20generalizability.%20Therefore%2C%20we%20develop%20DockGen%2C%20a%20new%0Abenchmark%20based%20on%20the%20ligand-binding%20domains%20of%20proteins%2C%20and%20we%20show%20that%0Aexisting%20machine%20learning-based%20docking%20models%20have%20very%20weak%20generalization%0Aabilities.%20We%20carefully%20analyze%20the%20scaling%20laws%20of%20ML-based%20docking%20and%20show%0Athat%2C%20by%20scaling%20data%20and%20model%20size%2C%20as%20well%20as%20integrating%20synthetic%20data%0Astrategies%2C%20we%20are%20able%20to%20significantly%20increase%20the%20generalization%20capacity%0Aand%20set%20new%20state-of-the-art%20performance%20across%20benchmarks.%20Further%2C%20we%20propose%0AConfidence%20Bootstrapping%2C%20a%20new%20training%20paradigm%20that%20solely%20relies%20on%20the%0Ainteraction%20between%20diffusion%20and%20confidence%20models%20and%20exploits%20the%0Amulti-resolution%20generation%20process%20of%20diffusion%20models.%20We%20demonstrate%20that%0AConfidence%20Bootstrapping%20significantly%20improves%20the%20ability%20of%20ML-based%20docking%0Amethods%20to%20dock%20to%20unseen%20protein%20classes%2C%20edging%20closer%20to%20accurate%20and%0Ageneralizable%20blind%20docking%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18396v1&entry.124074799=Read"},
{"title": "Physics-aware Machine Learning Revolutionizes Scientific Paradigm for\n  Machine Learning and Process-based Hydrology", "author": "Qingsong Xu and Yilei Shi and Jonathan Bamber and Ye Tuo and Ralf Ludwig and Xiao Xiang Zhu", "abstract": "  Accurate hydrological understanding and water cycle prediction are crucial\nfor addressing scientific and societal challenges associated with the\nmanagement of water resources, particularly under the dynamic influence of\nanthropogenic climate change. Existing reviews predominantly concentrate on the\ndevelopment of machine learning (ML) in this field, yet there is a clear\ndistinction between hydrology and ML as separate paradigms. Here, we introduce\nphysics-aware ML as a transformative approach to overcome the perceived barrier\nand revolutionize both fields. Specifically, we present a comprehensive review\nof the physics-aware ML methods, building a structured community (PaML) of\nexisting methodologies that integrate prior physical knowledge or physics-based\nmodeling into ML. We systematically analyze these PaML methodologies with\nrespect to four aspects: physical data-guided ML, physics-informed ML,\nphysics-embedded ML, and physics-aware hybrid learning. PaML facilitates\nML-aided hypotheses, accelerating insights from big data and fostering\nscientific discoveries. We first conduct a systematic review of hydrology in\nPaML, including rainfall-runoff hydrological processes and hydrodynamic\nprocesses, and highlight the most promising and challenging directions for\ndifferent objectives and PaML methods. Finally, a new PaML-based hydrology\nplatform, termed HydroPML, is released as a foundation for hydrological\napplications. HydroPML enhances the explainability and causality of ML and lays\nthe groundwork for the digital water cycle's realization. The HydroPML platform\nis publicly available at https://hydropml.github.io/.\n", "link": "http://arxiv.org/abs/2310.05227v3", "date": "2024-02-28", "relevancy": 1.4121, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5088}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4704}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4334}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-aware%20Machine%20Learning%20Revolutionizes%20Scientific%20Paradigm%20for%0A%20%20Machine%20Learning%20and%20Process-based%20Hydrology&entry.906535625=Qingsong%20Xu%20and%20Yilei%20Shi%20and%20Jonathan%20Bamber%20and%20Ye%20Tuo%20and%20Ralf%20Ludwig%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20Accurate%20hydrological%20understanding%20and%20water%20cycle%20prediction%20are%20crucial%0Afor%20addressing%20scientific%20and%20societal%20challenges%20associated%20with%20the%0Amanagement%20of%20water%20resources%2C%20particularly%20under%20the%20dynamic%20influence%20of%0Aanthropogenic%20climate%20change.%20Existing%20reviews%20predominantly%20concentrate%20on%20the%0Adevelopment%20of%20machine%20learning%20%28ML%29%20in%20this%20field%2C%20yet%20there%20is%20a%20clear%0Adistinction%20between%20hydrology%20and%20ML%20as%20separate%20paradigms.%20Here%2C%20we%20introduce%0Aphysics-aware%20ML%20as%20a%20transformative%20approach%20to%20overcome%20the%20perceived%20barrier%0Aand%20revolutionize%20both%20fields.%20Specifically%2C%20we%20present%20a%20comprehensive%20review%0Aof%20the%20physics-aware%20ML%20methods%2C%20building%20a%20structured%20community%20%28PaML%29%20of%0Aexisting%20methodologies%20that%20integrate%20prior%20physical%20knowledge%20or%20physics-based%0Amodeling%20into%20ML.%20We%20systematically%20analyze%20these%20PaML%20methodologies%20with%0Arespect%20to%20four%20aspects%3A%20physical%20data-guided%20ML%2C%20physics-informed%20ML%2C%0Aphysics-embedded%20ML%2C%20and%20physics-aware%20hybrid%20learning.%20PaML%20facilitates%0AML-aided%20hypotheses%2C%20accelerating%20insights%20from%20big%20data%20and%20fostering%0Ascientific%20discoveries.%20We%20first%20conduct%20a%20systematic%20review%20of%20hydrology%20in%0APaML%2C%20including%20rainfall-runoff%20hydrological%20processes%20and%20hydrodynamic%0Aprocesses%2C%20and%20highlight%20the%20most%20promising%20and%20challenging%20directions%20for%0Adifferent%20objectives%20and%20PaML%20methods.%20Finally%2C%20a%20new%20PaML-based%20hydrology%0Aplatform%2C%20termed%20HydroPML%2C%20is%20released%20as%20a%20foundation%20for%20hydrological%0Aapplications.%20HydroPML%20enhances%20the%20explainability%20and%20causality%20of%20ML%20and%20lays%0Athe%20groundwork%20for%20the%20digital%20water%20cycle%27s%20realization.%20The%20HydroPML%20platform%0Ais%20publicly%20available%20at%20https%3A//hydropml.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05227v3&entry.124074799=Read"},
{"title": "Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI", "author": "Sean I. Young and Ya\u00ebl Balbastre and Bruce Fischl and Polina Golland and Juan Eugenio Iglesias", "abstract": "  In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR)\nrefers to computational reconstruction of an unknown 3D magnetic resonance\nvolume from stacks of 2D slices corrupted by motion. While promising, current\nSVR methods require multiple slice stacks for accurate 3D reconstruction,\nleading to long scans and limiting their use in time-sensitive applications\nsuch as fetal fMRI. Here, we propose a SVR method that overcomes the\nshortcomings of previous work and produces state-of-the-art reconstructions in\nthe presence of extreme inter-slice motion. Inspired by the recent success of\nsingle-view depth estimation methods, we formulate SVR as a single-stack motion\nestimation task and train a fully convolutional network to predict a motion\nstack for a given slice stack, producing a 3D reconstruction as a byproduct of\nthe predicted motion. Extensive experiments on the SVR of adult and fetal\nbrains demonstrate that our fully convolutional method is twice as accurate as\nprevious SVR methods. Our code is available at github.com/seannz/svr.\n", "link": "http://arxiv.org/abs/2312.03102v2", "date": "2024-02-28", "relevancy": 1.3785, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4736}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4335}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Convolutional%20Slice-to-Volume%20Reconstruction%20for%20Single-Stack%20MRI&entry.906535625=Sean%20I.%20Young%20and%20Ya%C3%ABl%20Balbastre%20and%20Bruce%20Fischl%20and%20Polina%20Golland%20and%20Juan%20Eugenio%20Iglesias&entry.1292438233=%20%20In%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20slice-to-volume%20reconstruction%20%28SVR%29%0Arefers%20to%20computational%20reconstruction%20of%20an%20unknown%203D%20magnetic%20resonance%0Avolume%20from%20stacks%20of%202D%20slices%20corrupted%20by%20motion.%20While%20promising%2C%20current%0ASVR%20methods%20require%20multiple%20slice%20stacks%20for%20accurate%203D%20reconstruction%2C%0Aleading%20to%20long%20scans%20and%20limiting%20their%20use%20in%20time-sensitive%20applications%0Asuch%20as%20fetal%20fMRI.%20Here%2C%20we%20propose%20a%20SVR%20method%20that%20overcomes%20the%0Ashortcomings%20of%20previous%20work%20and%20produces%20state-of-the-art%20reconstructions%20in%0Athe%20presence%20of%20extreme%20inter-slice%20motion.%20Inspired%20by%20the%20recent%20success%20of%0Asingle-view%20depth%20estimation%20methods%2C%20we%20formulate%20SVR%20as%20a%20single-stack%20motion%0Aestimation%20task%20and%20train%20a%20fully%20convolutional%20network%20to%20predict%20a%20motion%0Astack%20for%20a%20given%20slice%20stack%2C%20producing%20a%203D%20reconstruction%20as%20a%20byproduct%20of%0Athe%20predicted%20motion.%20Extensive%20experiments%20on%20the%20SVR%20of%20adult%20and%20fetal%0Abrains%20demonstrate%20that%20our%20fully%20convolutional%20method%20is%20twice%20as%20accurate%20as%0Aprevious%20SVR%20methods.%20Our%20code%20is%20available%20at%20github.com/seannz/svr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03102v2&entry.124074799=Read"},
{"title": "Neuromorphic Event-Driven Semantic Communication in Microgrids", "author": "Xiaoguang Diao and Yubo Song and Subham Sahoo and Yuan Li", "abstract": "  Synergies between advanced communications, computing and artificial\nintelligence are unraveling new directions of coordinated operation and\nresiliency in microgrids. On one hand, coordination among sources is\nfacilitated by distributed, privacy-minded processing at multiple locations,\nwhereas on the other hand, it also creates exogenous data arrival paths for\nadversaries that can lead to cyber-physical attacks amongst other reliability\nissues in the communication layer. This long-standing problem necessitates new\nintrinsic ways of exchanging information between converters through power lines\nto optimize the system's control performance. Going beyond the existing power\nand data co-transfer technologies that are limited by efficiency and\nscalability concerns, this paper proposes neuromorphic learning to implant\ncommunicative features using spiking neural networks (SNNs) at each node, which\nis trained collaboratively in an online manner simply using the power exchanges\nbetween the nodes. As opposed to the conventional neuromorphic sensors that\noperate with spiking signals, we employ an event-driven selective process to\ncollect sparse data for training of SNNs. Finally, its multi-fold effectiveness\nand reliable performance is validated under simulation conditions with\ndifferent microgrid topologies and components to establish a new direction in\nthe sense-actuate-compute cycle for power electronic dominated grids and\nmicrogrids.\n", "link": "http://arxiv.org/abs/2402.18390v1", "date": "2024-02-28", "relevancy": 1.3614, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4668}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.457}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4473}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuromorphic%20Event-Driven%20Semantic%20Communication%20in%20Microgrids&entry.906535625=Xiaoguang%20Diao%20and%20Yubo%20Song%20and%20Subham%20Sahoo%20and%20Yuan%20Li&entry.1292438233=%20%20Synergies%20between%20advanced%20communications%2C%20computing%20and%20artificial%0Aintelligence%20are%20unraveling%20new%20directions%20of%20coordinated%20operation%20and%0Aresiliency%20in%20microgrids.%20On%20one%20hand%2C%20coordination%20among%20sources%20is%0Afacilitated%20by%20distributed%2C%20privacy-minded%20processing%20at%20multiple%20locations%2C%0Awhereas%20on%20the%20other%20hand%2C%20it%20also%20creates%20exogenous%20data%20arrival%20paths%20for%0Aadversaries%20that%20can%20lead%20to%20cyber-physical%20attacks%20amongst%20other%20reliability%0Aissues%20in%20the%20communication%20layer.%20This%20long-standing%20problem%20necessitates%20new%0Aintrinsic%20ways%20of%20exchanging%20information%20between%20converters%20through%20power%20lines%0Ato%20optimize%20the%20system%27s%20control%20performance.%20Going%20beyond%20the%20existing%20power%0Aand%20data%20co-transfer%20technologies%20that%20are%20limited%20by%20efficiency%20and%0Ascalability%20concerns%2C%20this%20paper%20proposes%20neuromorphic%20learning%20to%20implant%0Acommunicative%20features%20using%20spiking%20neural%20networks%20%28SNNs%29%20at%20each%20node%2C%20which%0Ais%20trained%20collaboratively%20in%20an%20online%20manner%20simply%20using%20the%20power%20exchanges%0Abetween%20the%20nodes.%20As%20opposed%20to%20the%20conventional%20neuromorphic%20sensors%20that%0Aoperate%20with%20spiking%20signals%2C%20we%20employ%20an%20event-driven%20selective%20process%20to%0Acollect%20sparse%20data%20for%20training%20of%20SNNs.%20Finally%2C%20its%20multi-fold%20effectiveness%0Aand%20reliable%20performance%20is%20validated%20under%20simulation%20conditions%20with%0Adifferent%20microgrid%20topologies%20and%20components%20to%20establish%20a%20new%20direction%20in%0Athe%20sense-actuate-compute%20cycle%20for%20power%20electronic%20dominated%20grids%20and%0Amicrogrids.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18390v1&entry.124074799=Read"},
{"title": "Similarity-based analogical proportions", "author": "Christian Anti\u0107", "abstract": "  The author has recently introduced abstract algebraic frameworks of\nanalogical proportions and similarity within the general setting of universal\nalgebra. The purpose of this paper is to build a bridge from similarity to\nanalogical proportions by formulating the latter in terms of the former. The\nbenefit of this similarity-based approach is that the connection between\nproportions and similarity is built into the framework and therefore evident\nwhich is appealing since proportions and similarity are both at the center of\nanalogy; moreover, future results on similarity can directly be applied to\nanalogical proportions.\n", "link": "http://arxiv.org/abs/2402.18360v1", "date": "2024-02-28", "relevancy": 1.359, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4041}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3397}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3141}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Similarity-based%20analogical%20proportions&entry.906535625=Christian%20Anti%C4%87&entry.1292438233=%20%20The%20author%20has%20recently%20introduced%20abstract%20algebraic%20frameworks%20of%0Aanalogical%20proportions%20and%20similarity%20within%20the%20general%20setting%20of%20universal%0Aalgebra.%20The%20purpose%20of%20this%20paper%20is%20to%20build%20a%20bridge%20from%20similarity%20to%0Aanalogical%20proportions%20by%20formulating%20the%20latter%20in%20terms%20of%20the%20former.%20The%0Abenefit%20of%20this%20similarity-based%20approach%20is%20that%20the%20connection%20between%0Aproportions%20and%20similarity%20is%20built%20into%20the%20framework%20and%20therefore%20evident%0Awhich%20is%20appealing%20since%20proportions%20and%20similarity%20are%20both%20at%20the%20center%20of%0Aanalogy%3B%20moreover%2C%20future%20results%20on%20similarity%20can%20directly%20be%20applied%20to%0Aanalogical%20proportions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18360v1&entry.124074799=Read"},
{"title": "Language Models Represent Beliefs of Self and Others", "author": "Wentao Zhu and Zhining Zhang and Yizhou Wang", "abstract": "  Understanding and attributing mental states, known as Theory of Mind (ToM),\nemerges as a fundamental capability for human social reasoning. While Large\nLanguage Models (LLMs) appear to possess certain ToM abilities, the mechanisms\nunderlying these capabilities remain elusive. In this study, we discover that\nit is possible to linearly decode the belief status from the perspectives of\nvarious agents through neural activations of language models, indicating the\nexistence of internal representations of self and others' beliefs. By\nmanipulating these representations, we observe dramatic changes in the models'\nToM performance, underscoring their pivotal role in the social reasoning\nprocess. Additionally, our findings extend to diverse social reasoning tasks\nthat involve different causal inference patterns, suggesting the potential\ngeneralizability of these representations.\n", "link": "http://arxiv.org/abs/2402.18496v1", "date": "2024-02-28", "relevancy": 1.3254, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4667}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4358}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4318}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20Represent%20Beliefs%20of%20Self%20and%20Others&entry.906535625=Wentao%20Zhu%20and%20Zhining%20Zhang%20and%20Yizhou%20Wang&entry.1292438233=%20%20Understanding%20and%20attributing%20mental%20states%2C%20known%20as%20Theory%20of%20Mind%20%28ToM%29%2C%0Aemerges%20as%20a%20fundamental%20capability%20for%20human%20social%20reasoning.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20appear%20to%20possess%20certain%20ToM%20abilities%2C%20the%20mechanisms%0Aunderlying%20these%20capabilities%20remain%20elusive.%20In%20this%20study%2C%20we%20discover%20that%0Ait%20is%20possible%20to%20linearly%20decode%20the%20belief%20status%20from%20the%20perspectives%20of%0Avarious%20agents%20through%20neural%20activations%20of%20language%20models%2C%20indicating%20the%0Aexistence%20of%20internal%20representations%20of%20self%20and%20others%27%20beliefs.%20By%0Amanipulating%20these%20representations%2C%20we%20observe%20dramatic%20changes%20in%20the%20models%27%0AToM%20performance%2C%20underscoring%20their%20pivotal%20role%20in%20the%20social%20reasoning%0Aprocess.%20Additionally%2C%20our%20findings%20extend%20to%20diverse%20social%20reasoning%20tasks%0Athat%20involve%20different%20causal%20inference%20patterns%2C%20suggesting%20the%20potential%0Ageneralizability%20of%20these%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18496v1&entry.124074799=Read"},
{"title": "Detecting algorithmic bias in medical AI-models", "author": "Jeffrey Smith and Andre Holder and Rishikesan Kamaleswaran and Yao Xie", "abstract": "  With the growing prevalence of machine learning and artificial\nintelligence-based medical decision support systems, it is equally important to\nensure that these systems provide patient outcomes in a fair and equitable\nfashion. This paper presents an innovative framework for detecting areas of\nalgorithmic bias in medical-AI decision support systems. Our approach\nefficiently identifies potential biases in medical-AI models, specifically in\nthe context of sepsis prediction, by employing the Classification and\nRegression Trees (CART) algorithm. We verify our methodology by conducting a\nseries of synthetic data experiments, showcasing its ability to estimate areas\nof bias in controlled settings precisely. The effectiveness of the concept is\nfurther validated by experiments using electronic medical records from Grady\nMemorial Hospital in Atlanta, Georgia. These tests demonstrate the practical\nimplementation of our strategy in a clinical environment, where it can function\nas a vital instrument for guaranteeing fairness and equity in AI-based medical\ndecisions.\n", "link": "http://arxiv.org/abs/2312.02959v3", "date": "2024-02-28", "relevancy": 1.3161, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4759}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4441}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4217}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20algorithmic%20bias%20in%20medical%20AI-models&entry.906535625=Jeffrey%20Smith%20and%20Andre%20Holder%20and%20Rishikesan%20Kamaleswaran%20and%20Yao%20Xie&entry.1292438233=%20%20With%20the%20growing%20prevalence%20of%20machine%20learning%20and%20artificial%0Aintelligence-based%20medical%20decision%20support%20systems%2C%20it%20is%20equally%20important%20to%0Aensure%20that%20these%20systems%20provide%20patient%20outcomes%20in%20a%20fair%20and%20equitable%0Afashion.%20This%20paper%20presents%20an%20innovative%20framework%20for%20detecting%20areas%20of%0Aalgorithmic%20bias%20in%20medical-AI%20decision%20support%20systems.%20Our%20approach%0Aefficiently%20identifies%20potential%20biases%20in%20medical-AI%20models%2C%20specifically%20in%0Athe%20context%20of%20sepsis%20prediction%2C%20by%20employing%20the%20Classification%20and%0ARegression%20Trees%20%28CART%29%20algorithm.%20We%20verify%20our%20methodology%20by%20conducting%20a%0Aseries%20of%20synthetic%20data%20experiments%2C%20showcasing%20its%20ability%20to%20estimate%20areas%0Aof%20bias%20in%20controlled%20settings%20precisely.%20The%20effectiveness%20of%20the%20concept%20is%0Afurther%20validated%20by%20experiments%20using%20electronic%20medical%20records%20from%20Grady%0AMemorial%20Hospital%20in%20Atlanta%2C%20Georgia.%20These%20tests%20demonstrate%20the%20practical%0Aimplementation%20of%20our%20strategy%20in%20a%20clinical%20environment%2C%20where%20it%20can%20function%0Aas%20a%20vital%20instrument%20for%20guaranteeing%20fairness%20and%20equity%20in%20AI-based%20medical%0Adecisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02959v3&entry.124074799=Read"},
{"title": "BLT: Can Large Language Models Handle Basic Legal Text?", "author": "Andrew Blair-Stanek and Nils Holzenberger and Benjamin Van Durme", "abstract": "  We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM\n2} currently perform poorly at basic legal text handling. We introduce a\nbenchmark consisting of tasks that lawyers and paralegals would expect LLMs to\nhandle zero-shot, such as looking up the text at a line of a witness deposition\nor at a subsection of a contract. LLMs' poor performance on this benchmark\ncasts into doubt their reliability as-is for legal practice. However,\nfine-tuning for these tasks brings even a smaller model to near-perfect\nperformance on our test set and also raises performance on a related legal\ntask. These results suggest that many simple behaviors needed for a domain may\nnot be present in foundational LLMs, without additional engagement from subject\nmatter experts.\n", "link": "http://arxiv.org/abs/2311.09693v2", "date": "2024-02-28", "relevancy": 1.2976, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4543}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4364}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.401}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLT%3A%20Can%20Large%20Language%20Models%20Handle%20Basic%20Legal%20Text%3F&entry.906535625=Andrew%20Blair-Stanek%20and%20Nils%20Holzenberger%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20We%20find%20that%20the%20best%20publicly%20available%20LLMs%20like%20GPT-4%2C%20Claude%2C%20and%20%7BPaLM%0A2%7D%20currently%20perform%20poorly%20at%20basic%20legal%20text%20handling.%20We%20introduce%20a%0Abenchmark%20consisting%20of%20tasks%20that%20lawyers%20and%20paralegals%20would%20expect%20LLMs%20to%0Ahandle%20zero-shot%2C%20such%20as%20looking%20up%20the%20text%20at%20a%20line%20of%20a%20witness%20deposition%0Aor%20at%20a%20subsection%20of%20a%20contract.%20LLMs%27%20poor%20performance%20on%20this%20benchmark%0Acasts%20into%20doubt%20their%20reliability%20as-is%20for%20legal%20practice.%20However%2C%0Afine-tuning%20for%20these%20tasks%20brings%20even%20a%20smaller%20model%20to%20near-perfect%0Aperformance%20on%20our%20test%20set%20and%20also%20raises%20performance%20on%20a%20related%20legal%0Atask.%20These%20results%20suggest%20that%20many%20simple%20behaviors%20needed%20for%20a%20domain%20may%0Anot%20be%20present%20in%20foundational%20LLMs%2C%20without%20additional%20engagement%20from%20subject%0Amatter%20experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09693v2&entry.124074799=Read"},
{"title": "A Closer Look at the Limitations of Instruction Tuning", "author": "Sreyan Ghosh and Chandra Kiran Reddy Evuru and Sonal Kumar and Ramaneswaran S and Deepali Aneja and Zeyu Jin and Ramani Duraiswami and Dinesh Manocha", "abstract": "  Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed inspire future work.\n", "link": "http://arxiv.org/abs/2402.05119v3", "date": "2024-02-28", "relevancy": 1.2961, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4522}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4201}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.3934}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Closer%20Look%20at%20the%20Limitations%20of%20Instruction%20Tuning&entry.906535625=Sreyan%20Ghosh%20and%20Chandra%20Kiran%20Reddy%20Evuru%20and%20Sonal%20Kumar%20and%20Ramaneswaran%20S%20and%20Deepali%20Aneja%20and%20Zeyu%20Jin%20and%20Ramani%20Duraiswami%20and%20Dinesh%20Manocha&entry.1292438233=%20%20Instruction%20Tuning%20%28IT%29%2C%20the%20process%20of%20training%20large%20language%20models%20%28LLMs%29%0Ausing%20instruction-response%20pairs%2C%20has%20emerged%20as%20the%20predominant%20method%20for%0Atransforming%20base%20pre-trained%20LLMs%20into%20open-domain%20conversational%20agents.%0AWhile%20IT%20has%20achieved%20notable%20success%20and%20widespread%20adoption%2C%20its%20limitations%0Aand%20shortcomings%20remain%20underexplored.%20In%20this%20paper%2C%20through%20rigorous%0Aexperiments%20and%20an%20in-depth%20analysis%20of%20the%20changes%20LLMs%20undergo%20through%20IT%2C%20we%0Areveal%20various%20limitations%20of%20IT.%20In%20particular%2C%20we%20show%20that%20%281%29%20IT%20fails%20to%0Aenhance%20knowledge%20or%20skills%20in%20LLMs.%20LoRA%20fine-tuning%20is%20limited%20to%20learning%0Aresponse%20initiation%20and%20style%20tokens%2C%20and%20full-parameter%20fine-tuning%20leads%20to%0Aknowledge%20degradation.%20%282%29%20Copying%20response%20patterns%20from%20IT%20datasets%20derived%0Afrom%20knowledgeable%20sources%20leads%20to%20a%20decline%20in%20response%20quality.%20%283%29%0AFull-parameter%20fine-tuning%20increases%20hallucination%20by%20inaccurately%20borrowing%0Atokens%20from%20conceptually%20similar%20instances%20in%20the%20IT%20dataset%20for%20generating%0Aresponses.%20%284%29%20Popular%20methods%20to%20improve%20IT%20do%20not%20lead%20to%20performance%0Aimprovements%20over%20a%20simple%20LoRA%20fine-tuned%20model.%20Our%20findings%20reveal%20that%0Aresponses%20generated%20solely%20from%20pre-trained%20knowledge%20consistently%20outperform%0Aresponses%20by%20models%20that%20learn%20any%20form%20of%20new%20knowledge%20from%20IT%20on%20open-source%0Adatasets.%20We%20hope%20the%20insights%20and%20challenges%20revealed%20inspire%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05119v3&entry.124074799=Read"},
{"title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for\n  Enhanced Reasoning and Communication", "author": "Weize Chen and Chenfei Yuan and Jiarui Yuan and Yusheng Su and Chen Qian and Cheng Yang and Ruobing Xie and Zhiyuan Liu and Maosong Sun", "abstract": "  Natural language (NL) has long been the predominant format for human\ncognition and communication, and by extension, has been similarly pivotal in\nthe development and application of Large Language Models (LLMs). Yet, besides\nNL, LLMs have seen various non-NL formats during pre-training, such as code and\nlogical expression. NL's status as the optimal format for LLMs, particularly in\nsingle-LLM reasoning and multi-agent communication, has not been thoroughly\nexamined. In this work, we challenge the default use of NL by exploring the\nutility of non-NL formats in these contexts. We show that allowing LLMs to\nautonomously select the most suitable format before reasoning or communicating\nleads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs,\nand up to a 72.7\\% reduction in token usage in multi-agent communication, all\nwhile maintaining communicative effectiveness. Our comprehensive analysis\nfurther reveals that LLMs can devise a format from limited task instructions\nand that the devised format is effectively transferable across different LLMs.\nIntriguingly, the structured communication format decided by LLMs exhibits\nnotable parallels with established agent communication languages, suggesting a\nnatural evolution towards efficient, structured communication in agent\ncommunication. Our code is released at\n\\url{https://github.com/thunlp/AutoForm}.\n", "link": "http://arxiv.org/abs/2402.18439v1", "date": "2024-02-28", "relevancy": 0.971, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4869}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4852}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4843}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Natural%20Language%3A%20LLMs%20Leveraging%20Alternative%20Formats%20for%0A%20%20Enhanced%20Reasoning%20and%20Communication&entry.906535625=Weize%20Chen%20and%20Chenfei%20Yuan%20and%20Jiarui%20Yuan%20and%20Yusheng%20Su%20and%20Chen%20Qian%20and%20Cheng%20Yang%20and%20Ruobing%20Xie%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Natural%20language%20%28NL%29%20has%20long%20been%20the%20predominant%20format%20for%20human%0Acognition%20and%20communication%2C%20and%20by%20extension%2C%20has%20been%20similarly%20pivotal%20in%0Athe%20development%20and%20application%20of%20Large%20Language%20Models%20%28LLMs%29.%20Yet%2C%20besides%0ANL%2C%20LLMs%20have%20seen%20various%20non-NL%20formats%20during%20pre-training%2C%20such%20as%20code%20and%0Alogical%20expression.%20NL%27s%20status%20as%20the%20optimal%20format%20for%20LLMs%2C%20particularly%20in%0Asingle-LLM%20reasoning%20and%20multi-agent%20communication%2C%20has%20not%20been%20thoroughly%0Aexamined.%20In%20this%20work%2C%20we%20challenge%20the%20default%20use%20of%20NL%20by%20exploring%20the%0Autility%20of%20non-NL%20formats%20in%20these%20contexts.%20We%20show%20that%20allowing%20LLMs%20to%0Aautonomously%20select%20the%20most%20suitable%20format%20before%20reasoning%20or%20communicating%0Aleads%20to%20a%203.3%20to%205.7%5C%25%20improvement%20in%20reasoning%20efficiency%20for%20different%20LLMs%2C%0Aand%20up%20to%20a%2072.7%5C%25%20reduction%20in%20token%20usage%20in%20multi-agent%20communication%2C%20all%0Awhile%20maintaining%20communicative%20effectiveness.%20Our%20comprehensive%20analysis%0Afurther%20reveals%20that%20LLMs%20can%20devise%20a%20format%20from%20limited%20task%20instructions%0Aand%20that%20the%20devised%20format%20is%20effectively%20transferable%20across%20different%20LLMs.%0AIntriguingly%2C%20the%20structured%20communication%20format%20decided%20by%20LLMs%20exhibits%0Anotable%20parallels%20with%20established%20agent%20communication%20languages%2C%20suggesting%20a%0Anatural%20evolution%20towards%20efficient%2C%20structured%20communication%20in%20agent%0Acommunication.%20Our%20code%20is%20released%20at%0A%5Curl%7Bhttps%3A//github.com/thunlp/AutoForm%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18439v1&entry.124074799=Read"},
{"title": "Multi-stakeholder Perspective on Responsible Artificial Intelligence and\n  Acceptability in Education", "author": "A. J. Karran and P. Charland and J-T. Martineau and A. Ortiz de Guinea Lopez de Arana and AM. Lesage and S. Senecal and P-M. Leger", "abstract": "  This study investigates the acceptability of different artificial\nintelligence (AI) applications in education from a multi-stakeholder\nperspective, including students, teachers, and parents. Acknowledging the\ntransformative potential of AI in education, it addresses concerns related to\ndata privacy, AI agency, transparency, explainability and the ethical\ndeployment of AI. Through a vignette methodology, participants were presented\nwith four scenarios where AI's agency, transparency, explainability, and\nprivacy were manipulated. After each scenario, participants completed a survey\nthat captured their perceptions of AI's global utility, individual usefulness,\njustice, confidence, risk, and intention to use each scenario's AI if\navailable. The data collection comprising a final sample of 1198\nmulti-stakeholder participants was distributed through a partner institution\nand social media campaigns and focused on individual responses to four AI use\ncases. A mediation analysis of the data indicated that acceptance and trust in\nAI varies significantly across stakeholder groups. We found that the key\nmediators between high and low levels of AI's agency, transparency, and\nexplainability, as well as the intention to use the different educational AI,\nincluded perceived global utility, justice, and confidence. The study\nhighlights that the acceptance of AI in education is a nuanced and multifaceted\nissue that requires careful consideration of specific AI applications and their\ncharacteristics, in addition to the diverse stakeholders' perceptions.\n", "link": "http://arxiv.org/abs/2402.15027v2", "date": "2024-02-28", "relevancy": 0.8789, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4653}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4328}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4202}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-stakeholder%20Perspective%20on%20Responsible%20Artificial%20Intelligence%20and%0A%20%20Acceptability%20in%20Education&entry.906535625=A.%20J.%20Karran%20and%20P.%20Charland%20and%20J-T.%20Martineau%20and%20A.%20Ortiz%20de%20Guinea%20Lopez%20de%20Arana%20and%20AM.%20Lesage%20and%20S.%20Senecal%20and%20P-M.%20Leger&entry.1292438233=%20%20This%20study%20investigates%20the%20acceptability%20of%20different%20artificial%0Aintelligence%20%28AI%29%20applications%20in%20education%20from%20a%20multi-stakeholder%0Aperspective%2C%20including%20students%2C%20teachers%2C%20and%20parents.%20Acknowledging%20the%0Atransformative%20potential%20of%20AI%20in%20education%2C%20it%20addresses%20concerns%20related%20to%0Adata%20privacy%2C%20AI%20agency%2C%20transparency%2C%20explainability%20and%20the%20ethical%0Adeployment%20of%20AI.%20Through%20a%20vignette%20methodology%2C%20participants%20were%20presented%0Awith%20four%20scenarios%20where%20AI%27s%20agency%2C%20transparency%2C%20explainability%2C%20and%0Aprivacy%20were%20manipulated.%20After%20each%20scenario%2C%20participants%20completed%20a%20survey%0Athat%20captured%20their%20perceptions%20of%20AI%27s%20global%20utility%2C%20individual%20usefulness%2C%0Ajustice%2C%20confidence%2C%20risk%2C%20and%20intention%20to%20use%20each%20scenario%27s%20AI%20if%0Aavailable.%20The%20data%20collection%20comprising%20a%20final%20sample%20of%201198%0Amulti-stakeholder%20participants%20was%20distributed%20through%20a%20partner%20institution%0Aand%20social%20media%20campaigns%20and%20focused%20on%20individual%20responses%20to%20four%20AI%20use%0Acases.%20A%20mediation%20analysis%20of%20the%20data%20indicated%20that%20acceptance%20and%20trust%20in%0AAI%20varies%20significantly%20across%20stakeholder%20groups.%20We%20found%20that%20the%20key%0Amediators%20between%20high%20and%20low%20levels%20of%20AI%27s%20agency%2C%20transparency%2C%20and%0Aexplainability%2C%20as%20well%20as%20the%20intention%20to%20use%20the%20different%20educational%20AI%2C%0Aincluded%20perceived%20global%20utility%2C%20justice%2C%20and%20confidence.%20The%20study%0Ahighlights%20that%20the%20acceptance%20of%20AI%20in%20education%20is%20a%20nuanced%20and%20multifaceted%0Aissue%20that%20requires%20careful%20consideration%20of%20specific%20AI%20applications%20and%20their%0Acharacteristics%2C%20in%20addition%20to%20the%20diverse%20stakeholders%27%20perceptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15027v2&entry.124074799=Read"},
{"title": "Can GPT Improve the State of Prior Authorization via Guideline Based\n  Automated Question Answering?", "author": "Shubham Vatsal and Ayush Singh and Shabnam Tafreshi", "abstract": "  Health insurance companies have a defined process called prior authorization\n(PA) which is a health plan cost-control process that requires doctors and\nother healthcare professionals to get clearance in advance from a health plan\nbefore performing a particular procedure on a patient in order to be eligible\nfor payment coverage. For health insurance companies, approving PA requests for\npatients in the medical domain is a time-consuming and challenging task. One of\nthose key challenges is validating if a request matches up to certain criteria\nsuch as age, gender, etc. In this work, we evaluate whether GPT can validate\nnumerous key factors, in turn helping health plans reach a decision drastically\nfaster. We frame it as a question answering task, prompting GPT to answer a\nquestion from patient electronic health record. We experiment with different\nconventional prompting techniques as well as introduce our own novel prompting\ntechnique. Moreover, we report qualitative assessment by humans on the natural\nlanguage generation outputs from our approach. Results show that our method\nachieves superior performance with the mean weighted F1 score of 0.61 as\ncompared to its standard counterparts.\n", "link": "http://arxiv.org/abs/2402.18419v1", "date": "2024-02-28", "relevancy": 1.1724, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.413}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3887}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3741}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20GPT%20Improve%20the%20State%20of%20Prior%20Authorization%20via%20Guideline%20Based%0A%20%20Automated%20Question%20Answering%3F&entry.906535625=Shubham%20Vatsal%20and%20Ayush%20Singh%20and%20Shabnam%20Tafreshi&entry.1292438233=%20%20Health%20insurance%20companies%20have%20a%20defined%20process%20called%20prior%20authorization%0A%28PA%29%20which%20is%20a%20health%20plan%20cost-control%20process%20that%20requires%20doctors%20and%0Aother%20healthcare%20professionals%20to%20get%20clearance%20in%20advance%20from%20a%20health%20plan%0Abefore%20performing%20a%20particular%20procedure%20on%20a%20patient%20in%20order%20to%20be%20eligible%0Afor%20payment%20coverage.%20For%20health%20insurance%20companies%2C%20approving%20PA%20requests%20for%0Apatients%20in%20the%20medical%20domain%20is%20a%20time-consuming%20and%20challenging%20task.%20One%20of%0Athose%20key%20challenges%20is%20validating%20if%20a%20request%20matches%20up%20to%20certain%20criteria%0Asuch%20as%20age%2C%20gender%2C%20etc.%20In%20this%20work%2C%20we%20evaluate%20whether%20GPT%20can%20validate%0Anumerous%20key%20factors%2C%20in%20turn%20helping%20health%20plans%20reach%20a%20decision%20drastically%0Afaster.%20We%20frame%20it%20as%20a%20question%20answering%20task%2C%20prompting%20GPT%20to%20answer%20a%0Aquestion%20from%20patient%20electronic%20health%20record.%20We%20experiment%20with%20different%0Aconventional%20prompting%20techniques%20as%20well%20as%20introduce%20our%20own%20novel%20prompting%0Atechnique.%20Moreover%2C%20we%20report%20qualitative%20assessment%20by%20humans%20on%20the%20natural%0Alanguage%20generation%20outputs%20from%20our%20approach.%20Results%20show%20that%20our%20method%0Aachieves%20superior%20performance%20with%20the%20mean%20weighted%20F1%20score%20of%200.61%20as%0Acompared%20to%20its%20standard%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18419v1&entry.124074799=Read"},
{"title": "Signature Kernel Conditional Independence Tests in Causal Discovery for\n  Stochastic Processes", "author": "Georg Manten and Cecilia Casolo and Emilio Ferrucci and S\u00f8ren Wengel Mogensen and Cristopher Salvi and Niki Kilbertus", "abstract": "  Inferring the causal structure underlying stochastic dynamical systems from\nobservational data holds great promise in domains ranging from science and\nhealth to finance. Such processes can often be accurately modeled via\nstochastic differential equations (SDEs), which naturally imply causal\nrelationships via \"which variables enter the differential of which other\nvariables\". In this paper, we develop a kernel-based test of conditional\nindependence (CI) on \"path-space\" -- solutions to SDEs -- by leveraging recent\nadvances in signature kernels. We demonstrate strictly superior performance of\nour proposed CI test compared to existing approaches on path-space. Then, we\ndevelop constraint-based causal discovery algorithms for acyclic stochastic\ndynamical systems (allowing for loops) that leverage temporal information to\nrecover the entire directed graph. Assuming faithfulness and a CI oracle, our\nalgorithm is sound and complete. We empirically verify that our developed CI\ntest in conjunction with the causal discovery algorithm reliably outperforms\nbaselines across a range of settings.\n", "link": "http://arxiv.org/abs/2402.18477v1", "date": "2024-02-28", "relevancy": 1.1956, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4248}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3996}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3697}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signature%20Kernel%20Conditional%20Independence%20Tests%20in%20Causal%20Discovery%20for%0A%20%20Stochastic%20Processes&entry.906535625=Georg%20Manten%20and%20Cecilia%20Casolo%20and%20Emilio%20Ferrucci%20and%20S%C3%B8ren%20Wengel%20Mogensen%20and%20Cristopher%20Salvi%20and%20Niki%20Kilbertus&entry.1292438233=%20%20Inferring%20the%20causal%20structure%20underlying%20stochastic%20dynamical%20systems%20from%0Aobservational%20data%20holds%20great%20promise%20in%20domains%20ranging%20from%20science%20and%0Ahealth%20to%20finance.%20Such%20processes%20can%20often%20be%20accurately%20modeled%20via%0Astochastic%20differential%20equations%20%28SDEs%29%2C%20which%20naturally%20imply%20causal%0Arelationships%20via%20%22which%20variables%20enter%20the%20differential%20of%20which%20other%0Avariables%22.%20In%20this%20paper%2C%20we%20develop%20a%20kernel-based%20test%20of%20conditional%0Aindependence%20%28CI%29%20on%20%22path-space%22%20--%20solutions%20to%20SDEs%20--%20by%20leveraging%20recent%0Aadvances%20in%20signature%20kernels.%20We%20demonstrate%20strictly%20superior%20performance%20of%0Aour%20proposed%20CI%20test%20compared%20to%20existing%20approaches%20on%20path-space.%20Then%2C%20we%0Adevelop%20constraint-based%20causal%20discovery%20algorithms%20for%20acyclic%20stochastic%0Adynamical%20systems%20%28allowing%20for%20loops%29%20that%20leverage%20temporal%20information%20to%0Arecover%20the%20entire%20directed%20graph.%20Assuming%20faithfulness%20and%20a%20CI%20oracle%2C%20our%0Aalgorithm%20is%20sound%20and%20complete.%20We%20empirically%20verify%20that%20our%20developed%20CI%0Atest%20in%20conjunction%20with%20the%20causal%20discovery%20algorithm%20reliably%20outperforms%0Abaselines%20across%20a%20range%20of%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18477v1&entry.124074799=Read"},
{"title": "Online Signal Estimation on the Graph Edges via Line Graph\n  Transformation", "author": "Yi Yan and Ercan Engin Kuruoglu", "abstract": "  The processing of signals on graph edges is challenging considering that\nGraph Signal Processing techniques are defined only on the graph nodes.\nLeveraging the Line Graph to transform a graph edge signal onto the node of its\nedge-to-vertex dual, we propose the Line Graph Least Mean Square (LGLMS)\nalgorithm for online time-varying graph edge signal prediction. By setting up\nan $l_2$-norm optimization problem, LGLMS forms an adaptive algorithm as the\ngraph edge analogy of the classical adaptive LMS algorithm. Additionally, the\nLGLMS inherits all the GSP concepts and techniques that can previously be\ndeployed on the graph nodes, but without the need to redefine them on the graph\nedges. Experimenting with transportation graphs and meteorological graphs, with\nthe signal observations having noisy and missing values, we confirmed that\nLGLMS is suitable for the online prediction of time-varying edge signals.\n", "link": "http://arxiv.org/abs/2311.00656v2", "date": "2024-02-28", "relevancy": 1.2525, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4266}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4115}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4008}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Signal%20Estimation%20on%20the%20Graph%20Edges%20via%20Line%20Graph%0A%20%20Transformation&entry.906535625=Yi%20Yan%20and%20Ercan%20Engin%20Kuruoglu&entry.1292438233=%20%20The%20processing%20of%20signals%20on%20graph%20edges%20is%20challenging%20considering%20that%0AGraph%20Signal%20Processing%20techniques%20are%20defined%20only%20on%20the%20graph%20nodes.%0ALeveraging%20the%20Line%20Graph%20to%20transform%20a%20graph%20edge%20signal%20onto%20the%20node%20of%20its%0Aedge-to-vertex%20dual%2C%20we%20propose%20the%20Line%20Graph%20Least%20Mean%20Square%20%28LGLMS%29%0Aalgorithm%20for%20online%20time-varying%20graph%20edge%20signal%20prediction.%20By%20setting%20up%0Aan%20%24l_2%24-norm%20optimization%20problem%2C%20LGLMS%20forms%20an%20adaptive%20algorithm%20as%20the%0Agraph%20edge%20analogy%20of%20the%20classical%20adaptive%20LMS%20algorithm.%20Additionally%2C%20the%0ALGLMS%20inherits%20all%20the%20GSP%20concepts%20and%20techniques%20that%20can%20previously%20be%0Adeployed%20on%20the%20graph%20nodes%2C%20but%20without%20the%20need%20to%20redefine%20them%20on%20the%20graph%0Aedges.%20Experimenting%20with%20transportation%20graphs%20and%20meteorological%20graphs%2C%20with%0Athe%20signal%20observations%20having%20noisy%20and%20missing%20values%2C%20we%20confirmed%20that%0ALGLMS%20is%20suitable%20for%20the%20online%20prediction%20of%20time-varying%20edge%20signals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00656v2&entry.124074799=Read"},
{"title": "Emotion Classification in Low and Moderate Resource Languages", "author": "Shabnam Tafreshi and Shubham Vatsal and Mona Diab", "abstract": "  It is important to be able to analyze the emotional state of people around\nthe globe. There are 7100+ active languages spoken around the world and\nbuilding emotion classification for each language is labor intensive.\nParticularly for low-resource and endangered languages, building emotion\nclassification can be quite challenging. We present a cross-lingual emotion\nclassifier, where we train an emotion classifier with resource-rich languages\n(i.e. \\textit{English} in our work) and transfer the learning to low and\nmoderate resource languages. We compare and contrast two approaches of transfer\nlearning from a high-resource language to a low or moderate-resource language.\nOne approach projects the annotation from a high-resource language to low and\nmoderate-resource language in parallel corpora and the other one uses direct\ntransfer from high-resource language to the other languages. We show the\nefficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano,\nOdia, and Azerbaijani. Our results indicate that our approaches outperform\nrandom baselines and transfer emotions across languages successfully. For all\nlanguages, the direct cross-lingual transfer of emotion yields better results.\nWe also create annotated emotion-labeled resources for four languages: Farsi,\nAzerbaijani, Ilocano and Odia.\n", "link": "http://arxiv.org/abs/2402.18424v1", "date": "2024-02-28", "relevancy": 1.2441, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4263}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4254}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4058}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emotion%20Classification%20in%20Low%20and%20Moderate%20Resource%20Languages&entry.906535625=Shabnam%20Tafreshi%20and%20Shubham%20Vatsal%20and%20Mona%20Diab&entry.1292438233=%20%20It%20is%20important%20to%20be%20able%20to%20analyze%20the%20emotional%20state%20of%20people%20around%0Athe%20globe.%20There%20are%207100%2B%20active%20languages%20spoken%20around%20the%20world%20and%0Abuilding%20emotion%20classification%20for%20each%20language%20is%20labor%20intensive.%0AParticularly%20for%20low-resource%20and%20endangered%20languages%2C%20building%20emotion%0Aclassification%20can%20be%20quite%20challenging.%20We%20present%20a%20cross-lingual%20emotion%0Aclassifier%2C%20where%20we%20train%20an%20emotion%20classifier%20with%20resource-rich%20languages%0A%28i.e.%20%5Ctextit%7BEnglish%7D%20in%20our%20work%29%20and%20transfer%20the%20learning%20to%20low%20and%0Amoderate%20resource%20languages.%20We%20compare%20and%20contrast%20two%20approaches%20of%20transfer%0Alearning%20from%20a%20high-resource%20language%20to%20a%20low%20or%20moderate-resource%20language.%0AOne%20approach%20projects%20the%20annotation%20from%20a%20high-resource%20language%20to%20low%20and%0Amoderate-resource%20language%20in%20parallel%20corpora%20and%20the%20other%20one%20uses%20direct%0Atransfer%20from%20high-resource%20language%20to%20the%20other%20languages.%20We%20show%20the%0Aefficacy%20of%20our%20approaches%20on%206%20languages%3A%20Farsi%2C%20Arabic%2C%20Spanish%2C%20Ilocano%2C%0AOdia%2C%20and%20Azerbaijani.%20Our%20results%20indicate%20that%20our%20approaches%20outperform%0Arandom%20baselines%20and%20transfer%20emotions%20across%20languages%20successfully.%20For%20all%0Alanguages%2C%20the%20direct%20cross-lingual%20transfer%20of%20emotion%20yields%20better%20results.%0AWe%20also%20create%20annotated%20emotion-labeled%20resources%20for%20four%20languages%3A%20Farsi%2C%0AAzerbaijani%2C%20Ilocano%20and%20Odia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18424v1&entry.124074799=Read"},
{"title": "Tokenization Is More Than Compression", "author": "Craig W. Schmidt and Varshini Reddy and Haoran Zhang and Alec Alameddine and Omri Uzan and Yuval Pinter and Chris Tanner", "abstract": "  Tokenization is a foundational step in Natural Language Processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.\n", "link": "http://arxiv.org/abs/2402.18376v1", "date": "2024-02-28", "relevancy": 1.2537, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4255}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4118}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.405}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenization%20Is%20More%20Than%20Compression&entry.906535625=Craig%20W.%20Schmidt%20and%20Varshini%20Reddy%20and%20Haoran%20Zhang%20and%20Alec%20Alameddine%20and%20Omri%20Uzan%20and%20Yuval%20Pinter%20and%20Chris%20Tanner&entry.1292438233=%20%20Tokenization%20is%20a%20foundational%20step%20in%20Natural%20Language%20Processing%20%28NLP%29%0Atasks%2C%20bridging%20raw%20text%20and%20language%20models.%20Existing%20tokenization%20approaches%0Alike%20Byte-Pair%20Encoding%20%28BPE%29%20originate%20from%20the%20field%20of%20data%20compression%2C%20and%0Ait%20has%20been%20suggested%20that%20the%20effectiveness%20of%20BPE%20stems%20from%20its%20ability%20to%0Acondense%20text%20into%20a%20relatively%20small%20number%20of%20tokens.%20We%20test%20the%20hypothesis%0Athat%20fewer%20tokens%20lead%20to%20better%20downstream%20performance%20by%20introducing%0APathPiece%2C%20a%20new%20tokenizer%20that%20segments%20a%20document%27s%20text%20into%20the%20minimum%0Anumber%20of%20tokens%20for%20a%20given%20vocabulary.%20Through%20extensive%20experimentation%20we%0Afind%20this%20hypothesis%20not%20to%20be%20the%20case%2C%20casting%20doubt%20on%20the%20understanding%20of%0Athe%20reasons%20for%20effective%20tokenization.%20To%20examine%20which%20other%20factors%20play%20a%0Arole%2C%20we%20evaluate%20design%20decisions%20across%20all%20three%20phases%20of%20tokenization%3A%0Apre-tokenization%2C%20vocabulary%20construction%2C%20and%20segmentation%2C%20offering%20new%0Ainsights%20into%20the%20design%20of%20effective%20tokenizers.%20Specifically%2C%20we%20illustrate%0Athe%20importance%20of%20pre-tokenization%20and%20the%20benefits%20of%20using%20BPE%20to%20initialize%0Avocabulary%20construction.%20We%20train%2064%20language%20models%20with%20varying%20tokenization%2C%0Aranging%20in%20size%20from%20350M%20to%202.4B%20parameters%2C%20all%20of%20which%20are%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18376v1&entry.124074799=Read"},
{"title": "Memory GAPS: Would LLMs pass the Tulving Test?", "author": "Jean-Marie Chauvet", "abstract": "  The Tulving Test was designed to investigate memory performance in\nrecognition and recall tasks. Its results help assess the relevance of the\n\"Synergistic Ecphory Model\" of memory and similar RK paradigms in human\nperformance. This paper starts investigating whether the more than\nforty-year-old framework sheds some light on LLMs' acts of remembering.\n", "link": "http://arxiv.org/abs/2402.16505v2", "date": "2024-02-28", "relevancy": 1.0832, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3699}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3641}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.336}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20GAPS%3A%20Would%20LLMs%20pass%20the%20Tulving%20Test%3F&entry.906535625=Jean-Marie%20Chauvet&entry.1292438233=%20%20The%20Tulving%20Test%20was%20designed%20to%20investigate%20memory%20performance%20in%0Arecognition%20and%20recall%20tasks.%20Its%20results%20help%20assess%20the%20relevance%20of%20the%0A%22Synergistic%20Ecphory%20Model%22%20of%20memory%20and%20similar%20RK%20paradigms%20in%20human%0Aperformance.%20This%20paper%20starts%20investigating%20whether%20the%20more%20than%0Aforty-year-old%20framework%20sheds%20some%20light%20on%20LLMs%27%20acts%20of%20remembering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16505v2&entry.124074799=Read"},
{"title": "Right on Time: Revising Time Series Models by Constraining their\n  Explanations", "author": "Maurice Kraus and David Steinmann and Antonia W\u00fcst and Andre Kokozinski and Kristian Kersting", "abstract": "  The reliability of deep time series models is often compromised by their\ntendency to rely on confounding factors, which may lead to misleading results.\nOur newly recorded, naturally confounded dataset named P2S from a real\nmechanical production line emphasizes this. To tackle the challenging problem\nof mitigating confounders in time series data, we introduce Right on Time\n(RioT). Our method enables interactions with model explanations across both the\ntime and frequency domain. Feedback on explanations in both domains is then\nused to constrain the model, steering it away from the annotated confounding\nfactors. The dual-domain interaction strategy is crucial for effectively\naddressing confounders in time series datasets. We empirically demonstrate that\nRioT can effectively guide models away from the wrong reasons in P2S as well as\npopular time series classification and forecasting datasets.\n", "link": "http://arxiv.org/abs/2402.12921v2", "date": "2024-02-28", "relevancy": 0.8319, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4271}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4123}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4085}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Right%20on%20Time%3A%20Revising%20Time%20Series%20Models%20by%20Constraining%20their%0A%20%20Explanations&entry.906535625=Maurice%20Kraus%20and%20David%20Steinmann%20and%20Antonia%20W%C3%BCst%20and%20Andre%20Kokozinski%20and%20Kristian%20Kersting&entry.1292438233=%20%20The%20reliability%20of%20deep%20time%20series%20models%20is%20often%20compromised%20by%20their%0Atendency%20to%20rely%20on%20confounding%20factors%2C%20which%20may%20lead%20to%20misleading%20results.%0AOur%20newly%20recorded%2C%20naturally%20confounded%20dataset%20named%20P2S%20from%20a%20real%0Amechanical%20production%20line%20emphasizes%20this.%20To%20tackle%20the%20challenging%20problem%0Aof%20mitigating%20confounders%20in%20time%20series%20data%2C%20we%20introduce%20Right%20on%20Time%0A%28RioT%29.%20Our%20method%20enables%20interactions%20with%20model%20explanations%20across%20both%20the%0Atime%20and%20frequency%20domain.%20Feedback%20on%20explanations%20in%20both%20domains%20is%20then%0Aused%20to%20constrain%20the%20model%2C%20steering%20it%20away%20from%20the%20annotated%20confounding%0Afactors.%20The%20dual-domain%20interaction%20strategy%20is%20crucial%20for%20effectively%0Aaddressing%20confounders%20in%20time%20series%20datasets.%20We%20empirically%20demonstrate%20that%0ARioT%20can%20effectively%20guide%20models%20away%20from%20the%20wrong%20reasons%20in%20P2S%20as%20well%20as%0Apopular%20time%20series%20classification%20and%20forecasting%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12921v2&entry.124074799=Read"},
{"title": "Prediction of recurrence free survival of head and neck cancer using\n  PET/CT radiomics and clinical information", "author": "Mona Furukawa and Daniel R. McGowan and Bart\u0142omiej W. Papie\u017c", "abstract": "  The 5-year survival rate of Head and Neck Cancer (HNC) has not improved over\nthe past decade and one common cause of treatment failure is recurrence. In\nthis paper, we built Cox proportional hazard (CoxPH) models that predict the\nrecurrence free survival (RFS) of oropharyngeal HNC patients. Our models\nutilise both clinical information and multimodal radiomics features extracted\nfrom tumour regions in Computed Tomography (CT) and Positron Emission\nTomography (PET). Furthermore, we were one of the first studies to explore the\nimpact of segmentation accuracy on the predictive power of the extracted\nradiomics features, through under- and over-segmentation study. Our models were\ntrained using the HEad and neCK TumOR (HECKTOR) challenge data, and the best\nperforming model achieved a concordance index (C-index) of 0.74 for the model\nutilising clinical information and multimodal CT and PET radiomics features,\nwhich compares favourably with the model that only used clinical information\n(C-index of 0.67). Our under- and over-segmentation study confirms that\nsegmentation accuracy affects radiomics extraction, however, it affects PET and\nCT differently.\n", "link": "http://arxiv.org/abs/2402.18417v1", "date": "2024-02-28", "relevancy": 1.2218, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4189}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4078}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3775}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20of%20recurrence%20free%20survival%20of%20head%20and%20neck%20cancer%20using%0A%20%20PET/CT%20radiomics%20and%20clinical%20information&entry.906535625=Mona%20Furukawa%20and%20Daniel%20R.%20McGowan%20and%20Bart%C5%82omiej%20W.%20Papie%C5%BC&entry.1292438233=%20%20The%205-year%20survival%20rate%20of%20Head%20and%20Neck%20Cancer%20%28HNC%29%20has%20not%20improved%20over%0Athe%20past%20decade%20and%20one%20common%20cause%20of%20treatment%20failure%20is%20recurrence.%20In%0Athis%20paper%2C%20we%20built%20Cox%20proportional%20hazard%20%28CoxPH%29%20models%20that%20predict%20the%0Arecurrence%20free%20survival%20%28RFS%29%20of%20oropharyngeal%20HNC%20patients.%20Our%20models%0Autilise%20both%20clinical%20information%20and%20multimodal%20radiomics%20features%20extracted%0Afrom%20tumour%20regions%20in%20Computed%20Tomography%20%28CT%29%20and%20Positron%20Emission%0ATomography%20%28PET%29.%20Furthermore%2C%20we%20were%20one%20of%20the%20first%20studies%20to%20explore%20the%0Aimpact%20of%20segmentation%20accuracy%20on%20the%20predictive%20power%20of%20the%20extracted%0Aradiomics%20features%2C%20through%20under-%20and%20over-segmentation%20study.%20Our%20models%20were%0Atrained%20using%20the%20HEad%20and%20neCK%20TumOR%20%28HECKTOR%29%20challenge%20data%2C%20and%20the%20best%0Aperforming%20model%20achieved%20a%20concordance%20index%20%28C-index%29%20of%200.74%20for%20the%20model%0Autilising%20clinical%20information%20and%20multimodal%20CT%20and%20PET%20radiomics%20features%2C%0Awhich%20compares%20favourably%20with%20the%20model%20that%20only%20used%20clinical%20information%0A%28C-index%20of%200.67%29.%20Our%20under-%20and%20over-segmentation%20study%20confirms%20that%0Asegmentation%20accuracy%20affects%20radiomics%20extraction%2C%20however%2C%20it%20affects%20PET%20and%0ACT%20differently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18417v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


