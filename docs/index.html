<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Genie: Generative Interactive Environments", "author": "Jake Bruce and Michael Dennis and Ashley Edwards and Jack Parker-Holder and Yuge Shi and Edward Hughes and Matthew Lai and Aditi Mavalankar and Richie Steigerwald and Chris Apps and Yusuf Aytar and Sarah Bechtle and Feryal Behbahani and Stephanie Chan and Nicolas Heess and Lucy Gonzalez and Simon Osindero and Sherjil Ozair and Scott Reed and Jingwei Zhang and Konrad Zolna and Jeff Clune and Nando de Freitas and Satinder Singh and Tim Rockt\u00e4schel", "abstract": "  We introduce Genie, the first generative interactive environment trained in\nan unsupervised manner from unlabelled Internet videos. The model can be\nprompted to generate an endless variety of action-controllable virtual worlds\ndescribed through text, synthetic images, photographs, and even sketches. At\n11B parameters, Genie can be considered a foundation world model. It is\ncomprised of a spatiotemporal video tokenizer, an autoregressive dynamics\nmodel, and a simple and scalable latent action model. Genie enables users to\nact in the generated environments on a frame-by-frame basis despite training\nwithout any ground-truth action labels or other domain-specific requirements\ntypically found in the world model literature. Further the resulting learned\nlatent action space facilitates training agents to imitate behaviors from\nunseen videos, opening the path for training generalist agents of the future.\n", "link": "http://arxiv.org/abs/2402.15391v1", "date": "2024-02-23", "relevancy": 2.7818, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5912}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5692}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5086}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Genie%3A%20Generative%20Interactive%20Environments&entry.906535625=Jake%20Bruce%20and%20Michael%20Dennis%20and%20Ashley%20Edwards%20and%20Jack%20Parker-Holder%20and%20Yuge%20Shi%20and%20Edward%20Hughes%20and%20Matthew%20Lai%20and%20Aditi%20Mavalankar%20and%20Richie%20Steigerwald%20and%20Chris%20Apps%20and%20Yusuf%20Aytar%20and%20Sarah%20Bechtle%20and%20Feryal%20Behbahani%20and%20Stephanie%20Chan%20and%20Nicolas%20Heess%20and%20Lucy%20Gonzalez%20and%20Simon%20Osindero%20and%20Sherjil%20Ozair%20and%20Scott%20Reed%20and%20Jingwei%20Zhang%20and%20Konrad%20Zolna%20and%20Jeff%20Clune%20and%20Nando%20de%20Freitas%20and%20Satinder%20Singh%20and%20Tim%20Rockt%C3%A4schel&entry.1292438233=%20%20We%20introduce%20Genie%2C%20the%20first%20generative%20interactive%20environment%20trained%20in%0Aan%20unsupervised%20manner%20from%20unlabelled%20Internet%20videos.%20The%20model%20can%20be%0Aprompted%20to%20generate%20an%20endless%20variety%20of%20action-controllable%20virtual%20worlds%0Adescribed%20through%20text%2C%20synthetic%20images%2C%20photographs%2C%20and%20even%20sketches.%20At%0A11B%20parameters%2C%20Genie%20can%20be%20considered%20a%20foundation%20world%20model.%20It%20is%0Acomprised%20of%20a%20spatiotemporal%20video%20tokenizer%2C%20an%20autoregressive%20dynamics%0Amodel%2C%20and%20a%20simple%20and%20scalable%20latent%20action%20model.%20Genie%20enables%20users%20to%0Aact%20in%20the%20generated%20environments%20on%20a%20frame-by-frame%20basis%20despite%20training%0Awithout%20any%20ground-truth%20action%20labels%20or%20other%20domain-specific%20requirements%0Atypically%20found%20in%20the%20world%20model%20literature.%20Further%20the%20resulting%20learned%0Alatent%20action%20space%20facilitates%20training%20agents%20to%20imitate%20behaviors%20from%0Aunseen%20videos%2C%20opening%20the%20path%20for%20training%20generalist%20agents%20of%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15391v1&entry.124074799=Read"},
{"title": "Linear Dynamics-embedded Neural Network for Long-Sequence Modeling", "author": "Tongyi Liang and Han-Xiong Li", "abstract": "  The trade-off between performance and computational efficiency in\nlong-sequence modeling becomes a bottleneck for existing models. Inspired by\nthe continuous state space models (SSMs) with multi-input and multi-output in\ncontrol theory, we propose a new neural network called Linear Dynamics-embedded\nNeural Network (LDNN). SSMs' continuous, discrete, and convolutional properties\nenable LDNN to have few parameters, flexible inference, and efficient training\nin long-sequence tasks. Two efficient strategies, diagonalization and\n$'\\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to\nreduce the time complexity of convolution from $O(LNH\\max\\{L, N\\})$ to\n$O(LN\\max \\{H, \\log L\\})$. We further improve LDNN through bidirectional\nnoncausal and multi-head settings to accommodate a broader range of\napplications. Extensive experiments on the Long Range Arena (LRA) demonstrate\nthe effectiveness and state-of-the-art performance of LDNN.\n", "link": "http://arxiv.org/abs/2402.15290v1", "date": "2024-02-23", "relevancy": 2.6536, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.588}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5045}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4997}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Dynamics-embedded%20Neural%20Network%20for%20Long-Sequence%20Modeling&entry.906535625=Tongyi%20Liang%20and%20Han-Xiong%20Li&entry.1292438233=%20%20The%20trade-off%20between%20performance%20and%20computational%20efficiency%20in%0Along-sequence%20modeling%20becomes%20a%20bottleneck%20for%20existing%20models.%20Inspired%20by%0Athe%20continuous%20state%20space%20models%20%28SSMs%29%20with%20multi-input%20and%20multi-output%20in%0Acontrol%20theory%2C%20we%20propose%20a%20new%20neural%20network%20called%20Linear%20Dynamics-embedded%0ANeural%20Network%20%28LDNN%29.%20SSMs%27%20continuous%2C%20discrete%2C%20and%20convolutional%20properties%0Aenable%20LDNN%20to%20have%20few%20parameters%2C%20flexible%20inference%2C%20and%20efficient%20training%0Ain%20long-sequence%20tasks.%20Two%20efficient%20strategies%2C%20diagonalization%20and%0A%24%27%5Ctext%7BDisentanglement%20then%20Fast%20Fourier%20Transform%20%28FFT%29%7D%27%24%2C%20are%20developed%20to%0Areduce%20the%20time%20complexity%20of%20convolution%20from%20%24O%28LNH%5Cmax%5C%7BL%2C%20N%5C%7D%29%24%20to%0A%24O%28LN%5Cmax%20%5C%7BH%2C%20%5Clog%20L%5C%7D%29%24.%20We%20further%20improve%20LDNN%20through%20bidirectional%0Anoncausal%20and%20multi-head%20settings%20to%20accommodate%20a%20broader%20range%20of%0Aapplications.%20Extensive%20experiments%20on%20the%20Long%20Range%20Arena%20%28LRA%29%20demonstrate%0Athe%20effectiveness%20and%20state-of-the-art%20performance%20of%20LDNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15290v1&entry.124074799=Read"},
{"title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with\n  Hierarchical Mixture of Experts", "author": "Yuejiang Liu and Alexandre Alahi", "abstract": "  Steering the behavior of a strong model pre-trained on internet-scale data\ncan be difficult due to the scarcity of competent supervisors. Recent studies\nreveal that, despite supervisory noises, a strong student model may surpass its\nweak teacher when fine-tuned on specific objectives. Yet, the effectiveness of\nsuch weak-to-strong generalization remains limited, especially in the presence\nof large capability gaps. In this paper, we propose to address this challenge\nby harnessing a diverse set of specialized teachers, instead of a single\ngeneralist one, that collectively supervises the strong student. Our approach\nresembles the classical hierarchical mixture of experts, with two components\ntailored for co-supervision: (i) we progressively alternate student training\nand teacher assignment, leveraging the growth of the strong student to identify\nplausible supervisions; (ii) we conservatively enforce teacher-student and\nlocal-global consistency, leveraging their dependencies to reject potential\nannotation noises. We validate the proposed method through visual recognition\ntasks on the OpenAI weak-to-strong benchmark and additional multi-domain\ndatasets. Our code is available at \\url{https://github.com/yuejiangliu/csl}.\n", "link": "http://arxiv.org/abs/2402.15505v1", "date": "2024-02-23", "relevancy": 2.6169, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5528}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5175}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4999}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Supervised%20Learning%3A%20Improving%20Weak-to-Strong%20Generalization%20with%0A%20%20Hierarchical%20Mixture%20of%20Experts&entry.906535625=Yuejiang%20Liu%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Steering%20the%20behavior%20of%20a%20strong%20model%20pre-trained%20on%20internet-scale%20data%0Acan%20be%20difficult%20due%20to%20the%20scarcity%20of%20competent%20supervisors.%20Recent%20studies%0Areveal%20that%2C%20despite%20supervisory%20noises%2C%20a%20strong%20student%20model%20may%20surpass%20its%0Aweak%20teacher%20when%20fine-tuned%20on%20specific%20objectives.%20Yet%2C%20the%20effectiveness%20of%0Asuch%20weak-to-strong%20generalization%20remains%20limited%2C%20especially%20in%20the%20presence%0Aof%20large%20capability%20gaps.%20In%20this%20paper%2C%20we%20propose%20to%20address%20this%20challenge%0Aby%20harnessing%20a%20diverse%20set%20of%20specialized%20teachers%2C%20instead%20of%20a%20single%0Ageneralist%20one%2C%20that%20collectively%20supervises%20the%20strong%20student.%20Our%20approach%0Aresembles%20the%20classical%20hierarchical%20mixture%20of%20experts%2C%20with%20two%20components%0Atailored%20for%20co-supervision%3A%20%28i%29%20we%20progressively%20alternate%20student%20training%0Aand%20teacher%20assignment%2C%20leveraging%20the%20growth%20of%20the%20strong%20student%20to%20identify%0Aplausible%20supervisions%3B%20%28ii%29%20we%20conservatively%20enforce%20teacher-student%20and%0Alocal-global%20consistency%2C%20leveraging%20their%20dependencies%20to%20reject%20potential%0Aannotation%20noises.%20We%20validate%20the%20proposed%20method%20through%20visual%20recognition%0Atasks%20on%20the%20OpenAI%20weak-to-strong%20benchmark%20and%20additional%20multi-domain%0Adatasets.%20Our%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/yuejiangliu/csl%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15505v1&entry.124074799=Read"},
{"title": "Scalable Human-Machine Point Cloud Compression", "author": "Mateen Ulhaq and Ivan V. Baji\u0107", "abstract": "  Due to the limited computational capabilities of edge devices, deep learning\ninference can be quite expensive. One remedy is to compress and transmit point\ncloud data over the network for server-side processing. Unfortunately, this\napproach can be sensitive to network factors, including available bitrate.\nLuckily, the bitrate requirements can be reduced without sacrificing inference\naccuracy by using a machine task-specialized codec. In this paper, we present a\nscalable codec for point-cloud data that is specialized for the machine task of\nclassification, while also providing a mechanism for human viewing. In the\nproposed scalable codec, the \"base\" bitstream supports the machine task, and an\n\"enhancement\" bitstream may be used for better input reconstruction performance\nfor human viewing. We base our architecture on PointNet++, and test its\nefficacy on the ModelNet40 dataset. We show significant improvements over prior\nnon-specialized codecs.\n", "link": "http://arxiv.org/abs/2402.12532v3", "date": "2024-02-23", "relevancy": 2.5362, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5422}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4898}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4897}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Human-Machine%20Point%20Cloud%20Compression&entry.906535625=Mateen%20Ulhaq%20and%20Ivan%20V.%20Baji%C4%87&entry.1292438233=%20%20Due%20to%20the%20limited%20computational%20capabilities%20of%20edge%20devices%2C%20deep%20learning%0Ainference%20can%20be%20quite%20expensive.%20One%20remedy%20is%20to%20compress%20and%20transmit%20point%0Acloud%20data%20over%20the%20network%20for%20server-side%20processing.%20Unfortunately%2C%20this%0Aapproach%20can%20be%20sensitive%20to%20network%20factors%2C%20including%20available%20bitrate.%0ALuckily%2C%20the%20bitrate%20requirements%20can%20be%20reduced%20without%20sacrificing%20inference%0Aaccuracy%20by%20using%20a%20machine%20task-specialized%20codec.%20In%20this%20paper%2C%20we%20present%20a%0Ascalable%20codec%20for%20point-cloud%20data%20that%20is%20specialized%20for%20the%20machine%20task%20of%0Aclassification%2C%20while%20also%20providing%20a%20mechanism%20for%20human%20viewing.%20In%20the%0Aproposed%20scalable%20codec%2C%20the%20%22base%22%20bitstream%20supports%20the%20machine%20task%2C%20and%20an%0A%22enhancement%22%20bitstream%20may%20be%20used%20for%20better%20input%20reconstruction%20performance%0Afor%20human%20viewing.%20We%20base%20our%20architecture%20on%20PointNet%2B%2B%2C%20and%20test%20its%0Aefficacy%20on%20the%20ModelNet40%20dataset.%20We%20show%20significant%20improvements%20over%20prior%0Anon-specialized%20codecs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12532v3&entry.124074799=Read"},
{"title": "Smoothed Graph Contrastive Learning via Seamless Proximity Integration", "author": "Maysam Behmanesh and Maks Ovsjanikov", "abstract": "  Graph contrastive learning (GCL) aligns node representations by classifying\nnode pairs into positives and negatives using a selection process that\ntypically relies on establishing correspondences within two augmented graphs.\nThe conventional GCL approaches incorporate negative samples uniformly in the\ncontrastive loss, resulting in the equal treatment negative nodes, regardless\nof their proximity to the true positive. In this paper, we present a Smoothed\nGraph Contrastive Learning model (SGCL), which leverages the geometric\nstructure of augmented graphs to inject proximity information associated with\npositive/negative pairs in the contrastive loss, thus significantly\nregularizing the learning process. The proposed SGCL adjusts the penalties\nassociated with node pairs in the contrastive loss by incorporating three\ndistinct smoothing techniques that result in proximity aware positives and\nnegatives. To enhance scalability for large-scale graphs, the proposed\nframework incorporates a graph batch-generating strategy that partitions the\ngiven graphs into multiple subgraphs, facilitating efficient training in\nseparate batches. Through extensive experimentation in the unsupervised setting\non various benchmarks, particularly those of large scale, we demonstrate the\nsuperiority of our proposed framework against recent baselines.\n", "link": "http://arxiv.org/abs/2402.15270v1", "date": "2024-02-23", "relevancy": 2.4803, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5417}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4809}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4655}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smoothed%20Graph%20Contrastive%20Learning%20via%20Seamless%20Proximity%20Integration&entry.906535625=Maysam%20Behmanesh%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20Graph%20contrastive%20learning%20%28GCL%29%20aligns%20node%20representations%20by%20classifying%0Anode%20pairs%20into%20positives%20and%20negatives%20using%20a%20selection%20process%20that%0Atypically%20relies%20on%20establishing%20correspondences%20within%20two%20augmented%20graphs.%0AThe%20conventional%20GCL%20approaches%20incorporate%20negative%20samples%20uniformly%20in%20the%0Acontrastive%20loss%2C%20resulting%20in%20the%20equal%20treatment%20negative%20nodes%2C%20regardless%0Aof%20their%20proximity%20to%20the%20true%20positive.%20In%20this%20paper%2C%20we%20present%20a%20Smoothed%0AGraph%20Contrastive%20Learning%20model%20%28SGCL%29%2C%20which%20leverages%20the%20geometric%0Astructure%20of%20augmented%20graphs%20to%20inject%20proximity%20information%20associated%20with%0Apositive/negative%20pairs%20in%20the%20contrastive%20loss%2C%20thus%20significantly%0Aregularizing%20the%20learning%20process.%20The%20proposed%20SGCL%20adjusts%20the%20penalties%0Aassociated%20with%20node%20pairs%20in%20the%20contrastive%20loss%20by%20incorporating%20three%0Adistinct%20smoothing%20techniques%20that%20result%20in%20proximity%20aware%20positives%20and%0Anegatives.%20To%20enhance%20scalability%20for%20large-scale%20graphs%2C%20the%20proposed%0Aframework%20incorporates%20a%20graph%20batch-generating%20strategy%20that%20partitions%20the%0Agiven%20graphs%20into%20multiple%20subgraphs%2C%20facilitating%20efficient%20training%20in%0Aseparate%20batches.%20Through%20extensive%20experimentation%20in%20the%20unsupervised%20setting%0Aon%20various%20benchmarks%2C%20particularly%20those%20of%20large%20scale%2C%20we%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20framework%20against%20recent%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15270v1&entry.124074799=Read"},
{"title": "Contrastive Learning Is Spectral Clustering On Similarity Graph", "author": "Zhiquan Tan and Yifan Zhang and Jingqin Yang and Yang Yuan", "abstract": "  Contrastive learning is a powerful self-supervised learning method, but we\nhave a limited theoretical understanding of how it works and why it works. In\nthis paper, we prove that contrastive learning with the standard InfoNCE loss\nis equivalent to spectral clustering on the similarity graph. Using this\nequivalence as the building block, we extend our analysis to the CLIP model and\nrigorously characterize how similar multi-modal objects are embedded together.\nMotivated by our theoretical insights, we introduce the Kernel-InfoNCE loss,\nincorporating mixtures of kernel functions that outperform the standard\nGaussian kernel on several vision datasets. The code is available at\nhttps://github.com/yifanzhang-pro/Kernel-InfoNCE.\n", "link": "http://arxiv.org/abs/2303.15103v4", "date": "2024-02-23", "relevancy": 2.4707, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5014}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4918}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4893}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20Is%20Spectral%20Clustering%20On%20Similarity%20Graph&entry.906535625=Zhiquan%20Tan%20and%20Yifan%20Zhang%20and%20Jingqin%20Yang%20and%20Yang%20Yuan&entry.1292438233=%20%20Contrastive%20learning%20is%20a%20powerful%20self-supervised%20learning%20method%2C%20but%20we%0Ahave%20a%20limited%20theoretical%20understanding%20of%20how%20it%20works%20and%20why%20it%20works.%20In%0Athis%20paper%2C%20we%20prove%20that%20contrastive%20learning%20with%20the%20standard%20InfoNCE%20loss%0Ais%20equivalent%20to%20spectral%20clustering%20on%20the%20similarity%20graph.%20Using%20this%0Aequivalence%20as%20the%20building%20block%2C%20we%20extend%20our%20analysis%20to%20the%20CLIP%20model%20and%0Arigorously%20characterize%20how%20similar%20multi-modal%20objects%20are%20embedded%20together.%0AMotivated%20by%20our%20theoretical%20insights%2C%20we%20introduce%20the%20Kernel-InfoNCE%20loss%2C%0Aincorporating%20mixtures%20of%20kernel%20functions%20that%20outperform%20the%20standard%0AGaussian%20kernel%20on%20several%20vision%20datasets.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/yifanzhang-pro/Kernel-InfoNCE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15103v4&entry.124074799=Read"},
{"title": "Gen4Gen: Generative Data Pipeline for Generative Multi-Concept\n  Composition", "author": "Chun-Hsiao Yeh and Ta-Ying Cheng and He-Yen Hsieh and Chuan-En Lin and Yi Ma and Andrew Markham and Niki Trigoni and H. T. Kung and Yubei Chen", "abstract": "  Recent text-to-image diffusion models are able to learn and synthesize images\ncontaining novel, personalized concepts (e.g., their own pets or specific\nitems) with just a few examples for training. This paper tackles two\ninterconnected issues within this realm of personalizing text-to-image\ndiffusion models. First, current personalization techniques fail to reliably\nextend to multiple concepts -- we hypothesize this to be due to the mismatch\nbetween complex scenes and simple text descriptions in the pre-training dataset\n(e.g., LAION). Second, given an image containing multiple personalized\nconcepts, there lacks a holistic metric that evaluates performance on not just\nthe degree of resemblance of personalized concepts, but also whether all\nconcepts are present in the image and whether the image accurately reflects the\noverall text description. To address these issues, we introduce Gen4Gen, a\nsemi-automated dataset creation pipeline utilizing generative models to combine\npersonalized concepts into complex compositions along with text-descriptions.\nUsing this, we create a dataset called MyCanvas, that can be used to benchmark\nthe task of multi-concept personalization. In addition, we design a\ncomprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better\nquantifying the performance of multi-concept, personalized text-to-image\ndiffusion methods. We provide a simple baseline built on top of Custom\nDiffusion with empirical prompting strategies for future researchers to\nevaluate on MyCanvas. We show that by improving data quality and prompting\nstrategies, we can significantly increase multi-concept personalized image\ngeneration quality, without requiring any modifications to model architecture\nor training algorithms.\n", "link": "http://arxiv.org/abs/2402.15504v1", "date": "2024-02-23", "relevancy": 2.4296, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6406}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6124}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5722}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gen4Gen%3A%20Generative%20Data%20Pipeline%20for%20Generative%20Multi-Concept%0A%20%20Composition&entry.906535625=Chun-Hsiao%20Yeh%20and%20Ta-Ying%20Cheng%20and%20He-Yen%20Hsieh%20and%20Chuan-En%20Lin%20and%20Yi%20Ma%20and%20Andrew%20Markham%20and%20Niki%20Trigoni%20and%20H.%20T.%20Kung%20and%20Yubei%20Chen&entry.1292438233=%20%20Recent%20text-to-image%20diffusion%20models%20are%20able%20to%20learn%20and%20synthesize%20images%0Acontaining%20novel%2C%20personalized%20concepts%20%28e.g.%2C%20their%20own%20pets%20or%20specific%0Aitems%29%20with%20just%20a%20few%20examples%20for%20training.%20This%20paper%20tackles%20two%0Ainterconnected%20issues%20within%20this%20realm%20of%20personalizing%20text-to-image%0Adiffusion%20models.%20First%2C%20current%20personalization%20techniques%20fail%20to%20reliably%0Aextend%20to%20multiple%20concepts%20--%20we%20hypothesize%20this%20to%20be%20due%20to%20the%20mismatch%0Abetween%20complex%20scenes%20and%20simple%20text%20descriptions%20in%20the%20pre-training%20dataset%0A%28e.g.%2C%20LAION%29.%20Second%2C%20given%20an%20image%20containing%20multiple%20personalized%0Aconcepts%2C%20there%20lacks%20a%20holistic%20metric%20that%20evaluates%20performance%20on%20not%20just%0Athe%20degree%20of%20resemblance%20of%20personalized%20concepts%2C%20but%20also%20whether%20all%0Aconcepts%20are%20present%20in%20the%20image%20and%20whether%20the%20image%20accurately%20reflects%20the%0Aoverall%20text%20description.%20To%20address%20these%20issues%2C%20we%20introduce%20Gen4Gen%2C%20a%0Asemi-automated%20dataset%20creation%20pipeline%20utilizing%20generative%20models%20to%20combine%0Apersonalized%20concepts%20into%20complex%20compositions%20along%20with%20text-descriptions.%0AUsing%20this%2C%20we%20create%20a%20dataset%20called%20MyCanvas%2C%20that%20can%20be%20used%20to%20benchmark%0Athe%20task%20of%20multi-concept%20personalization.%20In%20addition%2C%20we%20design%20a%0Acomprehensive%20metric%20comprising%20two%20scores%20%28CP-CLIP%20and%20TI-CLIP%29%20for%20better%0Aquantifying%20the%20performance%20of%20multi-concept%2C%20personalized%20text-to-image%0Adiffusion%20methods.%20We%20provide%20a%20simple%20baseline%20built%20on%20top%20of%20Custom%0ADiffusion%20with%20empirical%20prompting%20strategies%20for%20future%20researchers%20to%0Aevaluate%20on%20MyCanvas.%20We%20show%20that%20by%20improving%20data%20quality%20and%20prompting%0Astrategies%2C%20we%20can%20significantly%20increase%20multi-concept%20personalized%20image%0Ageneration%20quality%2C%20without%20requiring%20any%20modifications%20to%20model%20architecture%0Aor%20training%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15504v1&entry.124074799=Read"},
{"title": "Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective\n  of Operator Semigroup Theory", "author": "Weichen Zhao and Chenguang Wang and Xinyan Wang and Congying Han and Tiande Guo and Tianshu Yu", "abstract": "  This paper presents a novel study of the oversmoothing issue in\ndiffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches\ngrounded in random walk analysis or particle systems, we approach this problem\nthrough operator semigroup theory. This theoretical framework allows us to\nrigorously prove that oversmoothing is intrinsically linked to the ergodicity\nof the diffusion operator. This finding further poses a general and mild\nergodicity-breaking condition, encompassing the various specific solutions\npreviously offered, thereby presenting a more universal and theoretically\ngrounded approach to mitigating oversmoothing in diffusion-based GNNs.\nAdditionally, we offer a probabilistic interpretation of our theory, forging a\nlink with prior works and broadening the theoretical horizon. Our experimental\nresults reveal that this ergodicity-breaking term effectively mitigates\noversmoothing measured by Dirichlet energy, and simultaneously enhances\nperformance in node classification tasks.\n", "link": "http://arxiv.org/abs/2402.15326v1", "date": "2024-02-23", "relevancy": 2.383, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4991}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4779}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4528}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Oversmoothing%20in%20Diffusion-Based%20GNNs%20From%20the%20Perspective%0A%20%20of%20Operator%20Semigroup%20Theory&entry.906535625=Weichen%20Zhao%20and%20Chenguang%20Wang%20and%20Xinyan%20Wang%20and%20Congying%20Han%20and%20Tiande%20Guo%20and%20Tianshu%20Yu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20study%20of%20the%20oversmoothing%20issue%20in%0Adiffusion-based%20Graph%20Neural%20Networks%20%28GNNs%29.%20Diverging%20from%20extant%20approaches%0Agrounded%20in%20random%20walk%20analysis%20or%20particle%20systems%2C%20we%20approach%20this%20problem%0Athrough%20operator%20semigroup%20theory.%20This%20theoretical%20framework%20allows%20us%20to%0Arigorously%20prove%20that%20oversmoothing%20is%20intrinsically%20linked%20to%20the%20ergodicity%0Aof%20the%20diffusion%20operator.%20This%20finding%20further%20poses%20a%20general%20and%20mild%0Aergodicity-breaking%20condition%2C%20encompassing%20the%20various%20specific%20solutions%0Apreviously%20offered%2C%20thereby%20presenting%20a%20more%20universal%20and%20theoretically%0Agrounded%20approach%20to%20mitigating%20oversmoothing%20in%20diffusion-based%20GNNs.%0AAdditionally%2C%20we%20offer%20a%20probabilistic%20interpretation%20of%20our%20theory%2C%20forging%20a%0Alink%20with%20prior%20works%20and%20broadening%20the%20theoretical%20horizon.%20Our%20experimental%0Aresults%20reveal%20that%20this%20ergodicity-breaking%20term%20effectively%20mitigates%0Aoversmoothing%20measured%20by%20Dirichlet%20energy%2C%20and%20simultaneously%20enhances%0Aperformance%20in%20node%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15326v1&entry.124074799=Read"},
{"title": "Follow the Footprints: Self-supervised Traversability Estimation for\n  Off-road Vehicle Navigation based on Geometric and Visual Cues", "author": "Yurim Jeon and E In Son and Seung-Woo Seo", "abstract": "  In this study, we address the off-road traversability estimation problem,\nthat predicts areas where a robot can navigate in off-road environments. An\noff-road environment is an unstructured environment comprising a combination of\ntraversable and non-traversable spaces, which presents a challenge for\nestimating traversability. This study highlights three primary factors that\naffect a robot's traversability in an off-road environment: surface slope,\nsemantic information, and robot platform. We present two strategies for\nestimating traversability, using a guide filter network (GFN) and footprint\nsupervision module (FSM). The first strategy involves building a novel GFN\nusing a newly designed guide filter layer. The GFN interprets the surface and\nsemantic information from the input data and integrates them to extract\nfeatures optimized for traversability estimation. The second strategy involves\ndeveloping an FSM, which is a self-supervision module that utilizes the path\ntraversed by the robot in pre-driving, also known as a footprint. This enables\nthe prediction of traversability that reflects the characteristics of the robot\nplatform. Based on these two strategies, the proposed method overcomes the\nlimitations of existing methods, which require laborious human supervision and\nlack scalability. Extensive experiments in diverse conditions, including\nautomobiles and unmanned ground vehicles, herbfields, woodlands, and farmlands,\ndemonstrate that the proposed method is compatible for various robot platforms\nand adaptable to a range of terrains. Code is available at\nhttps://github.com/yurimjeon1892/FtFoot.\n", "link": "http://arxiv.org/abs/2402.15363v1", "date": "2024-02-23", "relevancy": 2.3773, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6143}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5994}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5723}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Follow%20the%20Footprints%3A%20Self-supervised%20Traversability%20Estimation%20for%0A%20%20Off-road%20Vehicle%20Navigation%20based%20on%20Geometric%20and%20Visual%20Cues&entry.906535625=Yurim%20Jeon%20and%20E%20In%20Son%20and%20Seung-Woo%20Seo&entry.1292438233=%20%20In%20this%20study%2C%20we%20address%20the%20off-road%20traversability%20estimation%20problem%2C%0Athat%20predicts%20areas%20where%20a%20robot%20can%20navigate%20in%20off-road%20environments.%20An%0Aoff-road%20environment%20is%20an%20unstructured%20environment%20comprising%20a%20combination%20of%0Atraversable%20and%20non-traversable%20spaces%2C%20which%20presents%20a%20challenge%20for%0Aestimating%20traversability.%20This%20study%20highlights%20three%20primary%20factors%20that%0Aaffect%20a%20robot%27s%20traversability%20in%20an%20off-road%20environment%3A%20surface%20slope%2C%0Asemantic%20information%2C%20and%20robot%20platform.%20We%20present%20two%20strategies%20for%0Aestimating%20traversability%2C%20using%20a%20guide%20filter%20network%20%28GFN%29%20and%20footprint%0Asupervision%20module%20%28FSM%29.%20The%20first%20strategy%20involves%20building%20a%20novel%20GFN%0Ausing%20a%20newly%20designed%20guide%20filter%20layer.%20The%20GFN%20interprets%20the%20surface%20and%0Asemantic%20information%20from%20the%20input%20data%20and%20integrates%20them%20to%20extract%0Afeatures%20optimized%20for%20traversability%20estimation.%20The%20second%20strategy%20involves%0Adeveloping%20an%20FSM%2C%20which%20is%20a%20self-supervision%20module%20that%20utilizes%20the%20path%0Atraversed%20by%20the%20robot%20in%20pre-driving%2C%20also%20known%20as%20a%20footprint.%20This%20enables%0Athe%20prediction%20of%20traversability%20that%20reflects%20the%20characteristics%20of%20the%20robot%0Aplatform.%20Based%20on%20these%20two%20strategies%2C%20the%20proposed%20method%20overcomes%20the%0Alimitations%20of%20existing%20methods%2C%20which%20require%20laborious%20human%20supervision%20and%0Alack%20scalability.%20Extensive%20experiments%20in%20diverse%20conditions%2C%20including%0Aautomobiles%20and%20unmanned%20ground%20vehicles%2C%20herbfields%2C%20woodlands%2C%20and%20farmlands%2C%0Ademonstrate%20that%20the%20proposed%20method%20is%20compatible%20for%20various%20robot%20platforms%0Aand%20adaptable%20to%20a%20range%20of%20terrains.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yurimjeon1892/FtFoot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15363v1&entry.124074799=Read"},
{"title": "Adversarial Feature Map Pruning for Backdoor", "author": "Dong Huang and Qingwen Bu", "abstract": "  Deep neural networks have been widely used in many critical applications,\nsuch as autonomous vehicles and medical diagnosis. However, their security is\nthreatened by backdoor attacks, which are achieved by adding artificial\npatterns to specific training data. Existing defense strategies primarily focus\non using reverse engineering to reproduce the backdoor trigger generated by\nattackers and subsequently repair the DNN model by adding the trigger into\ninputs and fine-tuning the model with ground-truth labels. However, once the\ntrigger generated by the attackers is complex and invisible, the defender\ncannot reproduce the trigger successfully then the DNN model will not be\nrepaired, as the trigger is not effectively removed.\n  In this work, we propose Adversarial Feature Map Pruning for Backdoor (FMP)\nto mitigate backdoor from the DNN. Unlike existing defense strategies, which\nfocus on reproducing backdoor triggers, FMP attempts to prune backdoor feature\nmaps, which are trained to extract backdoor information from inputs. After\npruning these backdoor feature maps, FMP will fine-tune the model with a secure\nsubset of training data. Our experiments demonstrate that, compared to existing\ndefense strategies, FMP can effectively reduce the Attack Success Rate (ASR)\neven against the most complex and invisible attack triggers (e.g., FMP\ndecreases the ASR to 2.86\\% in CIFAR10, which is 19.2\\% to 65.41\\% lower than\nbaselines). Second, unlike conventional defense methods that tend to exhibit\nlow robust accuracy (that is, the accuracy of the model on poisoned data), FMP\nachieves a higher RA, indicating its superiority in maintaining model\nperformance while mitigating the effects of backdoor attacks (e.g., FMP obtains\n87.40\\% RA in CIFAR10). Our code is publicly available at:\nhttps://github.com/retsuh-bqw/FMP.\n", "link": "http://arxiv.org/abs/2307.11565v2", "date": "2024-02-23", "relevancy": 2.3728, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5031}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4652}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4554}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Feature%20Map%20Pruning%20for%20Backdoor&entry.906535625=Dong%20Huang%20and%20Qingwen%20Bu&entry.1292438233=%20%20Deep%20neural%20networks%20have%20been%20widely%20used%20in%20many%20critical%20applications%2C%0Asuch%20as%20autonomous%20vehicles%20and%20medical%20diagnosis.%20However%2C%20their%20security%20is%0Athreatened%20by%20backdoor%20attacks%2C%20which%20are%20achieved%20by%20adding%20artificial%0Apatterns%20to%20specific%20training%20data.%20Existing%20defense%20strategies%20primarily%20focus%0Aon%20using%20reverse%20engineering%20to%20reproduce%20the%20backdoor%20trigger%20generated%20by%0Aattackers%20and%20subsequently%20repair%20the%20DNN%20model%20by%20adding%20the%20trigger%20into%0Ainputs%20and%20fine-tuning%20the%20model%20with%20ground-truth%20labels.%20However%2C%20once%20the%0Atrigger%20generated%20by%20the%20attackers%20is%20complex%20and%20invisible%2C%20the%20defender%0Acannot%20reproduce%20the%20trigger%20successfully%20then%20the%20DNN%20model%20will%20not%20be%0Arepaired%2C%20as%20the%20trigger%20is%20not%20effectively%20removed.%0A%20%20In%20this%20work%2C%20we%20propose%20Adversarial%20Feature%20Map%20Pruning%20for%20Backdoor%20%28FMP%29%0Ato%20mitigate%20backdoor%20from%20the%20DNN.%20Unlike%20existing%20defense%20strategies%2C%20which%0Afocus%20on%20reproducing%20backdoor%20triggers%2C%20FMP%20attempts%20to%20prune%20backdoor%20feature%0Amaps%2C%20which%20are%20trained%20to%20extract%20backdoor%20information%20from%20inputs.%20After%0Apruning%20these%20backdoor%20feature%20maps%2C%20FMP%20will%20fine-tune%20the%20model%20with%20a%20secure%0Asubset%20of%20training%20data.%20Our%20experiments%20demonstrate%20that%2C%20compared%20to%20existing%0Adefense%20strategies%2C%20FMP%20can%20effectively%20reduce%20the%20Attack%20Success%20Rate%20%28ASR%29%0Aeven%20against%20the%20most%20complex%20and%20invisible%20attack%20triggers%20%28e.g.%2C%20FMP%0Adecreases%20the%20ASR%20to%202.86%5C%25%20in%20CIFAR10%2C%20which%20is%2019.2%5C%25%20to%2065.41%5C%25%20lower%20than%0Abaselines%29.%20Second%2C%20unlike%20conventional%20defense%20methods%20that%20tend%20to%20exhibit%0Alow%20robust%20accuracy%20%28that%20is%2C%20the%20accuracy%20of%20the%20model%20on%20poisoned%20data%29%2C%20FMP%0Aachieves%20a%20higher%20RA%2C%20indicating%20its%20superiority%20in%20maintaining%20model%0Aperformance%20while%20mitigating%20the%20effects%20of%20backdoor%20attacks%20%28e.g.%2C%20FMP%20obtains%0A87.40%5C%25%20RA%20in%20CIFAR10%29.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/retsuh-bqw/FMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.11565v2&entry.124074799=Read"},
{"title": "Inversion dynamics of class manifolds in deep learning reveals tradeoffs\n  underlying generalisation", "author": "Simone Ciceri and Lorenzo Cassani and Matteo Osella and Pietro Rotondo and Filippo Valle and Marco Gherardi", "abstract": "  To achieve near-zero training error in a classification problem, the layers\nof a feed-forward network have to disentangle the manifolds of data points with\ndifferent labels, to facilitate the discrimination. However, excessive class\nseparation can bring to overfitting since good generalisation requires learning\ninvariant features, which involve some level of entanglement. We report on\nnumerical experiments showing how the optimisation dynamics finds\nrepresentations that balance these opposing tendencies with a non-monotonic\ntrend. After a fast segregation phase, a slower rearrangement (conserved across\ndata sets and architectures) increases the class entanglement.The training\nerror at the inversion is stable under subsampling, and across network\ninitialisations and optimisers, which characterises it as a property solely of\nthe data structure and (very weakly) of the architecture. The inversion is the\nmanifestation of tradeoffs elicited by well-defined and maximally stable\nelements of the training set, coined ``stragglers'', particularly influential\nfor generalisation.\n", "link": "http://arxiv.org/abs/2303.05161v2", "date": "2024-02-23", "relevancy": 2.3681, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5166}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4505}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inversion%20dynamics%20of%20class%20manifolds%20in%20deep%20learning%20reveals%20tradeoffs%0A%20%20underlying%20generalisation&entry.906535625=Simone%20Ciceri%20and%20Lorenzo%20Cassani%20and%20Matteo%20Osella%20and%20Pietro%20Rotondo%20and%20Filippo%20Valle%20and%20Marco%20Gherardi&entry.1292438233=%20%20To%20achieve%20near-zero%20training%20error%20in%20a%20classification%20problem%2C%20the%20layers%0Aof%20a%20feed-forward%20network%20have%20to%20disentangle%20the%20manifolds%20of%20data%20points%20with%0Adifferent%20labels%2C%20to%20facilitate%20the%20discrimination.%20However%2C%20excessive%20class%0Aseparation%20can%20bring%20to%20overfitting%20since%20good%20generalisation%20requires%20learning%0Ainvariant%20features%2C%20which%20involve%20some%20level%20of%20entanglement.%20We%20report%20on%0Anumerical%20experiments%20showing%20how%20the%20optimisation%20dynamics%20finds%0Arepresentations%20that%20balance%20these%20opposing%20tendencies%20with%20a%20non-monotonic%0Atrend.%20After%20a%20fast%20segregation%20phase%2C%20a%20slower%20rearrangement%20%28conserved%20across%0Adata%20sets%20and%20architectures%29%20increases%20the%20class%20entanglement.The%20training%0Aerror%20at%20the%20inversion%20is%20stable%20under%20subsampling%2C%20and%20across%20network%0Ainitialisations%20and%20optimisers%2C%20which%20characterises%20it%20as%20a%20property%20solely%20of%0Athe%20data%20structure%20and%20%28very%20weakly%29%20of%20the%20architecture.%20The%20inversion%20is%20the%0Amanifestation%20of%20tradeoffs%20elicited%20by%20well-defined%20and%20maximally%20stable%0Aelements%20of%20the%20training%20set%2C%20coined%20%60%60stragglers%27%27%2C%20particularly%20influential%0Afor%20generalisation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.05161v2&entry.124074799=Read"},
{"title": "United We Pretrain, Divided We Fail! Representation Learning for Time\n  Series by Pretraining on 75 Datasets at Once", "author": "Maurice Kraus and Felix Divo and David Steinmann and Devendra Singh Dhami and Kristian Kersting", "abstract": "  In natural language processing and vision, pretraining is utilized to learn\neffective representations. Unfortunately, the success of pretraining does not\neasily carry over to time series due to potential mismatch between sources and\ntarget. Actually, common belief is that multi-dataset pretraining does not work\nfor time series! Au contraire, we introduce a new self-supervised contrastive\npretraining approach to learn one encoding from many unlabeled and diverse time\nseries datasets, so that the single learned representation can then be reused\nin several target domains for, say, classification. Specifically, we propose\nthe XD-MixUp interpolation method and the Soft Interpolation Contextual\nContrasting (SICC) loss. Empirically, this outperforms both supervised training\nand other self-supervised pretraining methods when finetuning on low-data\nregimes. This disproves the common belief: We can actually learn from multiple\ntime series datasets, even from 75 at once.\n", "link": "http://arxiv.org/abs/2402.15404v1", "date": "2024-02-23", "relevancy": 2.366, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5393}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4456}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4347}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=United%20We%20Pretrain%2C%20Divided%20We%20Fail%21%20Representation%20Learning%20for%20Time%0A%20%20Series%20by%20Pretraining%20on%2075%20Datasets%20at%20Once&entry.906535625=Maurice%20Kraus%20and%20Felix%20Divo%20and%20David%20Steinmann%20and%20Devendra%20Singh%20Dhami%20and%20Kristian%20Kersting&entry.1292438233=%20%20In%20natural%20language%20processing%20and%20vision%2C%20pretraining%20is%20utilized%20to%20learn%0Aeffective%20representations.%20Unfortunately%2C%20the%20success%20of%20pretraining%20does%20not%0Aeasily%20carry%20over%20to%20time%20series%20due%20to%20potential%20mismatch%20between%20sources%20and%0Atarget.%20Actually%2C%20common%20belief%20is%20that%20multi-dataset%20pretraining%20does%20not%20work%0Afor%20time%20series%21%20Au%20contraire%2C%20we%20introduce%20a%20new%20self-supervised%20contrastive%0Apretraining%20approach%20to%20learn%20one%20encoding%20from%20many%20unlabeled%20and%20diverse%20time%0Aseries%20datasets%2C%20so%20that%20the%20single%20learned%20representation%20can%20then%20be%20reused%0Ain%20several%20target%20domains%20for%2C%20say%2C%20classification.%20Specifically%2C%20we%20propose%0Athe%20XD-MixUp%20interpolation%20method%20and%20the%20Soft%20Interpolation%20Contextual%0AContrasting%20%28SICC%29%20loss.%20Empirically%2C%20this%20outperforms%20both%20supervised%20training%0Aand%20other%20self-supervised%20pretraining%20methods%20when%20finetuning%20on%20low-data%0Aregimes.%20This%20disproves%20the%20common%20belief%3A%20We%20can%20actually%20learn%20from%20multiple%0Atime%20series%20datasets%2C%20even%20from%2075%20at%20once.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15404v1&entry.124074799=Read"},
{"title": "Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy\n  Structure Prior", "author": "Kechun Xu and Zhongxiang Zhou and Jun Wu and Haojian Lu and Rong Xiong and Yue Wang", "abstract": "  We focus on the task of unknown object rearrangement, where a robot is\nsupposed to re-configure the objects into a desired goal configuration\nspecified by an RGB-D image. Recent works explore unknown object rearrangement\nsystems by incorporating learning-based perception modules. However, they are\nsensitive to perception error, and pay less attention to task-level\nperformance. In this paper, we aim to develop an effective system for unknown\nobject rearrangement amidst perception noise. We theoretically reveal the noisy\nperception impacts grasp and place in a decoupled way, and show such a\ndecoupled structure is non-trivial to improve task optimality. We propose GSP,\na dual-loop system with the decoupled structure as prior. For the inner loop,\nwe learn an active seeing policy for self-confident object matching to improve\nthe perception of place. For the outer loop, we learn a grasp policy aware of\nobject matching and grasp capability guided by task-level rewards. We leverage\nthe foundation model CLIP for object matching, policy learning and\nself-termination. A series of experiments indicate that GSP can conduct unknown\nobject rearrangement with higher completion rate and less steps.\n", "link": "http://arxiv.org/abs/2402.15402v1", "date": "2024-02-23", "relevancy": 2.3601, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5571}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5509}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grasp%2C%20See%20and%20Place%3A%20Efficient%20Unknown%20Object%20Rearrangement%20with%20Policy%0A%20%20Structure%20Prior&entry.906535625=Kechun%20Xu%20and%20Zhongxiang%20Zhou%20and%20Jun%20Wu%20and%20Haojian%20Lu%20and%20Rong%20Xiong%20and%20Yue%20Wang&entry.1292438233=%20%20We%20focus%20on%20the%20task%20of%20unknown%20object%20rearrangement%2C%20where%20a%20robot%20is%0Asupposed%20to%20re-configure%20the%20objects%20into%20a%20desired%20goal%20configuration%0Aspecified%20by%20an%20RGB-D%20image.%20Recent%20works%20explore%20unknown%20object%20rearrangement%0Asystems%20by%20incorporating%20learning-based%20perception%20modules.%20However%2C%20they%20are%0Asensitive%20to%20perception%20error%2C%20and%20pay%20less%20attention%20to%20task-level%0Aperformance.%20In%20this%20paper%2C%20we%20aim%20to%20develop%20an%20effective%20system%20for%20unknown%0Aobject%20rearrangement%20amidst%20perception%20noise.%20We%20theoretically%20reveal%20the%20noisy%0Aperception%20impacts%20grasp%20and%20place%20in%20a%20decoupled%20way%2C%20and%20show%20such%20a%0Adecoupled%20structure%20is%20non-trivial%20to%20improve%20task%20optimality.%20We%20propose%20GSP%2C%0Aa%20dual-loop%20system%20with%20the%20decoupled%20structure%20as%20prior.%20For%20the%20inner%20loop%2C%0Awe%20learn%20an%20active%20seeing%20policy%20for%20self-confident%20object%20matching%20to%20improve%0Athe%20perception%20of%20place.%20For%20the%20outer%20loop%2C%20we%20learn%20a%20grasp%20policy%20aware%20of%0Aobject%20matching%20and%20grasp%20capability%20guided%20by%20task-level%20rewards.%20We%20leverage%0Athe%20foundation%20model%20CLIP%20for%20object%20matching%2C%20policy%20learning%20and%0Aself-termination.%20A%20series%20of%20experiments%20indicate%20that%20GSP%20can%20conduct%20unknown%0Aobject%20rearrangement%20with%20higher%20completion%20rate%20and%20less%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15402v1&entry.124074799=Read"},
{"title": "SparDL: Distributed Deep Learning Training with Efficient Sparse\n  Communication", "author": "Minjun Zhao and Yichen Yin and Yuren Mao and Qing Liu and Lu Chen and Yunjun Gao", "abstract": "  Top-k sparsification has recently been widely used to reduce the\ncommunication volume in distributed deep learning. However, due to the Sparse\nGradient Accumulation (SGA) dilemma, the performance of top-k sparsification\nstill has limitations. Recently, a few methods have been put forward to handle\nthe SGA dilemma. Regrettably, even the state-of-the-art method suffers from\nseveral drawbacks, e.g., it relies on an inefficient communication algorithm\nand requires extra transmission steps. Motivated by the limitations of existing\nmethods, we propose a novel efficient sparse communication framework, called\nSparDL. Specifically, SparDL uses the Spar-Reduce-Scatter algorithm, which is\nbased on an efficient Reduce-Scatter model, to handle the SGA dilemma without\nadditional communication operations. Besides, to further reduce the latency\ncost and improve the efficiency of SparDL, we propose the Spar-All-Gather\nalgorithm. Moreover, we propose the global residual collection algorithm to\nensure fast convergence of model training. Finally, extensive experiments are\nconducted to validate the superiority of SparDL.\n", "link": "http://arxiv.org/abs/2304.00737v2", "date": "2024-02-23", "relevancy": 2.3536, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4879}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4663}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.458}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparDL%3A%20Distributed%20Deep%20Learning%20Training%20with%20Efficient%20Sparse%0A%20%20Communication&entry.906535625=Minjun%20Zhao%20and%20Yichen%20Yin%20and%20Yuren%20Mao%20and%20Qing%20Liu%20and%20Lu%20Chen%20and%20Yunjun%20Gao&entry.1292438233=%20%20Top-k%20sparsification%20has%20recently%20been%20widely%20used%20to%20reduce%20the%0Acommunication%20volume%20in%20distributed%20deep%20learning.%20However%2C%20due%20to%20the%20Sparse%0AGradient%20Accumulation%20%28SGA%29%20dilemma%2C%20the%20performance%20of%20top-k%20sparsification%0Astill%20has%20limitations.%20Recently%2C%20a%20few%20methods%20have%20been%20put%20forward%20to%20handle%0Athe%20SGA%20dilemma.%20Regrettably%2C%20even%20the%20state-of-the-art%20method%20suffers%20from%0Aseveral%20drawbacks%2C%20e.g.%2C%20it%20relies%20on%20an%20inefficient%20communication%20algorithm%0Aand%20requires%20extra%20transmission%20steps.%20Motivated%20by%20the%20limitations%20of%20existing%0Amethods%2C%20we%20propose%20a%20novel%20efficient%20sparse%20communication%20framework%2C%20called%0ASparDL.%20Specifically%2C%20SparDL%20uses%20the%20Spar-Reduce-Scatter%20algorithm%2C%20which%20is%0Abased%20on%20an%20efficient%20Reduce-Scatter%20model%2C%20to%20handle%20the%20SGA%20dilemma%20without%0Aadditional%20communication%20operations.%20Besides%2C%20to%20further%20reduce%20the%20latency%0Acost%20and%20improve%20the%20efficiency%20of%20SparDL%2C%20we%20propose%20the%20Spar-All-Gather%0Aalgorithm.%20Moreover%2C%20we%20propose%20the%20global%20residual%20collection%20algorithm%20to%0Aensure%20fast%20convergence%20of%20model%20training.%20Finally%2C%20extensive%20experiments%20are%0Aconducted%20to%20validate%20the%20superiority%20of%20SparDL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.00737v2&entry.124074799=Read"},
{"title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications,\n  Challenges, and Future Trends", "author": "Abolfazl Younesi and Mohsen Ansari and MohammadAmin Fazli and Alireza Ejlali and Muhammad Shafique and J\u00f6rg Henkel", "abstract": "  In today's digital age, Convolutional Neural Networks (CNNs), a subset of\nDeep Learning (DL), are widely used for various computer vision tasks such as\nimage classification, object detection, and image segmentation. There are\nnumerous types of CNNs designed to meet specific needs and requirements,\nincluding 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention,\ndepthwise convolutions, and NAS, among others. Each type of CNN has its unique\nstructure and characteristics, making it suitable for specific tasks. It's\ncrucial to gain a thorough understanding and perform a comparative analysis of\nthese different CNN types to understand their strengths and weaknesses.\nFurthermore, studying the performance, limitations, and practical applications\nof each type of CNN can aid in the development of new and improved\narchitectures in the future. We also dive into the platforms and frameworks\nthat researchers utilize for their research or development from various\nperspectives. Additionally, we explore the main research fields of CNN like 6D\nvision, generative models, and meta-learning. This survey paper provides a\ncomprehensive examination and comparison of various CNN architectures,\nhighlighting their architectural differences and emphasizing their respective\nadvantages, disadvantages, applications, challenges, and future trends.\n", "link": "http://arxiv.org/abs/2402.15490v1", "date": "2024-02-23", "relevancy": 2.35, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4939}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4617}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4544}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20of%20Convolutions%20in%20Deep%20Learning%3A%20Applications%2C%0A%20%20Challenges%2C%20and%20Future%20Trends&entry.906535625=Abolfazl%20Younesi%20and%20Mohsen%20Ansari%20and%20MohammadAmin%20Fazli%20and%20Alireza%20Ejlali%20and%20Muhammad%20Shafique%20and%20J%C3%B6rg%20Henkel&entry.1292438233=%20%20In%20today%27s%20digital%20age%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20a%20subset%20of%0ADeep%20Learning%20%28DL%29%2C%20are%20widely%20used%20for%20various%20computer%20vision%20tasks%20such%20as%0Aimage%20classification%2C%20object%20detection%2C%20and%20image%20segmentation.%20There%20are%0Anumerous%20types%20of%20CNNs%20designed%20to%20meet%20specific%20needs%20and%20requirements%2C%0Aincluding%201D%2C%202D%2C%20and%203D%20CNNs%2C%20as%20well%20as%20dilated%2C%20grouped%2C%20attention%2C%0Adepthwise%20convolutions%2C%20and%20NAS%2C%20among%20others.%20Each%20type%20of%20CNN%20has%20its%20unique%0Astructure%20and%20characteristics%2C%20making%20it%20suitable%20for%20specific%20tasks.%20It%27s%0Acrucial%20to%20gain%20a%20thorough%20understanding%20and%20perform%20a%20comparative%20analysis%20of%0Athese%20different%20CNN%20types%20to%20understand%20their%20strengths%20and%20weaknesses.%0AFurthermore%2C%20studying%20the%20performance%2C%20limitations%2C%20and%20practical%20applications%0Aof%20each%20type%20of%20CNN%20can%20aid%20in%20the%20development%20of%20new%20and%20improved%0Aarchitectures%20in%20the%20future.%20We%20also%20dive%20into%20the%20platforms%20and%20frameworks%0Athat%20researchers%20utilize%20for%20their%20research%20or%20development%20from%20various%0Aperspectives.%20Additionally%2C%20we%20explore%20the%20main%20research%20fields%20of%20CNN%20like%206D%0Avision%2C%20generative%20models%2C%20and%20meta-learning.%20This%20survey%20paper%20provides%20a%0Acomprehensive%20examination%20and%20comparison%20of%20various%20CNN%20architectures%2C%0Ahighlighting%20their%20architectural%20differences%20and%20emphasizing%20their%20respective%0Aadvantages%2C%20disadvantages%2C%20applications%2C%20challenges%2C%20and%20future%20trends.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15490v1&entry.124074799=Read"},
{"title": "Convolutional Deep Kernel Machines", "author": "Edward Milsom and Ben Anson and Laurence Aitchison", "abstract": "  Standard infinite-width limits of neural networks sacrifice the ability for\nintermediate layers to learn representations from data. Recent work (A theory\nof representation learning gives a deep generalisation of kernel methods, Yang\net al. 2023) modified the Neural Network Gaussian Process (NNGP) limit of\nBayesian neural networks so that representation learning is retained.\nFurthermore, they found that applying this modified limit to a deep Gaussian\nprocess gives a practical learning algorithm which they dubbed the deep kernel\nmachine (DKM). However, they only considered the simplest possible setting:\nregression in small, fully connected networks with e.g. 10 input features.\nHere, we introduce convolutional deep kernel machines. This required us to\ndevelop a novel inter-domain inducing point approximation, as well as\nintroducing and experimentally assessing a number of techniques not previously\nseen in DKMs, including analogues to batch normalisation, different\nlikelihoods, and different types of top-layer. The resulting model trains in\nroughly 77 GPU hours, achieving around 99% test accuracy on MNIST, 72% on\nCIFAR-100, and 92.7% on CIFAR-10, which is SOTA for kernel methods.\n", "link": "http://arxiv.org/abs/2309.09814v2", "date": "2024-02-23", "relevancy": 2.3169, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4913}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4649}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.434}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20Deep%20Kernel%20Machines&entry.906535625=Edward%20Milsom%20and%20Ben%20Anson%20and%20Laurence%20Aitchison&entry.1292438233=%20%20Standard%20infinite-width%20limits%20of%20neural%20networks%20sacrifice%20the%20ability%20for%0Aintermediate%20layers%20to%20learn%20representations%20from%20data.%20Recent%20work%20%28A%20theory%0Aof%20representation%20learning%20gives%20a%20deep%20generalisation%20of%20kernel%20methods%2C%20Yang%0Aet%20al.%202023%29%20modified%20the%20Neural%20Network%20Gaussian%20Process%20%28NNGP%29%20limit%20of%0ABayesian%20neural%20networks%20so%20that%20representation%20learning%20is%20retained.%0AFurthermore%2C%20they%20found%20that%20applying%20this%20modified%20limit%20to%20a%20deep%20Gaussian%0Aprocess%20gives%20a%20practical%20learning%20algorithm%20which%20they%20dubbed%20the%20deep%20kernel%0Amachine%20%28DKM%29.%20However%2C%20they%20only%20considered%20the%20simplest%20possible%20setting%3A%0Aregression%20in%20small%2C%20fully%20connected%20networks%20with%20e.g.%2010%20input%20features.%0AHere%2C%20we%20introduce%20convolutional%20deep%20kernel%20machines.%20This%20required%20us%20to%0Adevelop%20a%20novel%20inter-domain%20inducing%20point%20approximation%2C%20as%20well%20as%0Aintroducing%20and%20experimentally%20assessing%20a%20number%20of%20techniques%20not%20previously%0Aseen%20in%20DKMs%2C%20including%20analogues%20to%20batch%20normalisation%2C%20different%0Alikelihoods%2C%20and%20different%20types%20of%20top-layer.%20The%20resulting%20model%20trains%20in%0Aroughly%2077%20GPU%20hours%2C%20achieving%20around%2099%25%20test%20accuracy%20on%20MNIST%2C%2072%25%20on%0ACIFAR-100%2C%20and%2092.7%25%20on%20CIFAR-10%2C%20which%20is%20SOTA%20for%20kernel%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09814v2&entry.124074799=Read"},
{"title": "G-RepsNet: A Fast and General Construction of Equivariant Networks for\n  Arbitrary Matrix Groups", "author": "Sourya Basu and Suhas Lohit and Matthew Brand", "abstract": "  Group equivariance is a strong inductive bias useful in a wide range of deep\nlearning tasks. However, constructing efficient equivariant networks for\ngeneral groups and domains is difficult. Recent work by Finzi et al. (2021)\ndirectly solves the equivariance constraint for arbitrary matrix groups to\nobtain equivariant MLPs (EMLPs). But this method does not scale well and\nscaling is crucial in deep learning. Here, we introduce Group Representation\nNetworks (G-RepsNets), a lightweight equivariant network for arbitrary matrix\ngroups with features represented using tensor polynomials. The key intuition\nfor our design is that using tensor representations in the hidden layers of a\nneural network along with simple inexpensive tensor operations can lead to\nexpressive universal equivariant networks. We find G-RepsNet to be competitive\nto EMLP on several tasks with group symmetries such as O(5), O(1, 3), and O(3)\nwith scalars, vectors, and second-order tensors as data types. On image\nclassification tasks, we find that G-RepsNet using second-order representations\nis competitive and often even outperforms sophisticated state-of-the-art\nequivariant models such as GCNNs (Cohen & Welling, 2016a) and E(2)-CNNs (Weiler\n& Cesa, 2019). To further illustrate the generality of our approach, we show\nthat G-RepsNet is competitive to G-FNO (Helwig et al., 2023) and EGNN (Satorras\net al., 2021) on N-body predictions and solving PDEs, respectively, while being\nefficient.\n", "link": "http://arxiv.org/abs/2402.15413v1", "date": "2024-02-23", "relevancy": 2.2992, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4875}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4678}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4242}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-RepsNet%3A%20A%20Fast%20and%20General%20Construction%20of%20Equivariant%20Networks%20for%0A%20%20Arbitrary%20Matrix%20Groups&entry.906535625=Sourya%20Basu%20and%20Suhas%20Lohit%20and%20Matthew%20Brand&entry.1292438233=%20%20Group%20equivariance%20is%20a%20strong%20inductive%20bias%20useful%20in%20a%20wide%20range%20of%20deep%0Alearning%20tasks.%20However%2C%20constructing%20efficient%20equivariant%20networks%20for%0Ageneral%20groups%20and%20domains%20is%20difficult.%20Recent%20work%20by%20Finzi%20et%20al.%20%282021%29%0Adirectly%20solves%20the%20equivariance%20constraint%20for%20arbitrary%20matrix%20groups%20to%0Aobtain%20equivariant%20MLPs%20%28EMLPs%29.%20But%20this%20method%20does%20not%20scale%20well%20and%0Ascaling%20is%20crucial%20in%20deep%20learning.%20Here%2C%20we%20introduce%20Group%20Representation%0ANetworks%20%28G-RepsNets%29%2C%20a%20lightweight%20equivariant%20network%20for%20arbitrary%20matrix%0Agroups%20with%20features%20represented%20using%20tensor%20polynomials.%20The%20key%20intuition%0Afor%20our%20design%20is%20that%20using%20tensor%20representations%20in%20the%20hidden%20layers%20of%20a%0Aneural%20network%20along%20with%20simple%20inexpensive%20tensor%20operations%20can%20lead%20to%0Aexpressive%20universal%20equivariant%20networks.%20We%20find%20G-RepsNet%20to%20be%20competitive%0Ato%20EMLP%20on%20several%20tasks%20with%20group%20symmetries%20such%20as%20O%285%29%2C%20O%281%2C%203%29%2C%20and%20O%283%29%0Awith%20scalars%2C%20vectors%2C%20and%20second-order%20tensors%20as%20data%20types.%20On%20image%0Aclassification%20tasks%2C%20we%20find%20that%20G-RepsNet%20using%20second-order%20representations%0Ais%20competitive%20and%20often%20even%20outperforms%20sophisticated%20state-of-the-art%0Aequivariant%20models%20such%20as%20GCNNs%20%28Cohen%20%26%20Welling%2C%202016a%29%20and%20E%282%29-CNNs%20%28Weiler%0A%26%20Cesa%2C%202019%29.%20To%20further%20illustrate%20the%20generality%20of%20our%20approach%2C%20we%20show%0Athat%20G-RepsNet%20is%20competitive%20to%20G-FNO%20%28Helwig%20et%20al.%2C%202023%29%20and%20EGNN%20%28Satorras%0Aet%20al.%2C%202021%29%20on%20N-body%20predictions%20and%20solving%20PDEs%2C%20respectively%2C%20while%20being%0Aefficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15413v1&entry.124074799=Read"},
{"title": "Safe Task Planning for Language-Instructed Multi-Robot Systems using\n  Conformal Prediction", "author": "Jun Wang and Guocheng He and Yiannis Kantaros", "abstract": "  This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities (e.g., mobility, manipulation, and sensing) at various\nlocations and semantic objects. Several recent works have addressed similar\nplanning problems by leveraging pre-trained Large Language Models (LLMs) to\ndesign effective multi-robot plans. However, these approaches lack mission\nperformance and safety guarantees. To address this challenge, we introduce a\nnew decentralized LLM-based planner that is capable of achieving high mission\nsuccess rates. This is accomplished by leveraging conformal prediction (CP), a\ndistribution-free uncertainty quantification tool in black-box models. CP\nallows the proposed multi-robot planner to reason about its inherent\nuncertainty in a decentralized fashion, enabling robots to make individual\ndecisions when they are sufficiently certain and seek help otherwise. We show,\nboth theoretically and empirically, that the proposed planner can achieve\nuser-specified task success rates while minimizing the overall number of help\nrequests. We demonstrate the performance of our approach on multi-robot home\nservice applications. We also show through comparative experiments, that our\nmethod outperforms recent centralized and decentralized multi-robot LLM-based\nplanners in terms of in terms of its ability to design correct plans. The\nadvantage of our algorithm over baselines becomes more pronounced with\nincreasing mission complexity and robot team size.\n", "link": "http://arxiv.org/abs/2402.15368v1", "date": "2024-02-23", "relevancy": 2.2886, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6115}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5796}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.549}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Task%20Planning%20for%20Language-Instructed%20Multi-Robot%20Systems%20using%0A%20%20Conformal%20Prediction&entry.906535625=Jun%20Wang%20and%20Guocheng%20He%20and%20Yiannis%20Kantaros&entry.1292438233=%20%20This%20paper%20addresses%20task%20planning%20problems%20for%20language-instructed%20robot%0Ateams.%20Tasks%20are%20expressed%20in%20natural%20language%20%28NL%29%2C%20requiring%20the%20robots%20to%0Aapply%20their%20capabilities%20%28e.g.%2C%20mobility%2C%20manipulation%2C%20and%20sensing%29%20at%20various%0Alocations%20and%20semantic%20objects.%20Several%20recent%20works%20have%20addressed%20similar%0Aplanning%20problems%20by%20leveraging%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20to%0Adesign%20effective%20multi-robot%20plans.%20However%2C%20these%20approaches%20lack%20mission%0Aperformance%20and%20safety%20guarantees.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%0Anew%20decentralized%20LLM-based%20planner%20that%20is%20capable%20of%20achieving%20high%20mission%0Asuccess%20rates.%20This%20is%20accomplished%20by%20leveraging%20conformal%20prediction%20%28CP%29%2C%20a%0Adistribution-free%20uncertainty%20quantification%20tool%20in%20black-box%20models.%20CP%0Aallows%20the%20proposed%20multi-robot%20planner%20to%20reason%20about%20its%20inherent%0Auncertainty%20in%20a%20decentralized%20fashion%2C%20enabling%20robots%20to%20make%20individual%0Adecisions%20when%20they%20are%20sufficiently%20certain%20and%20seek%20help%20otherwise.%20We%20show%2C%0Aboth%20theoretically%20and%20empirically%2C%20that%20the%20proposed%20planner%20can%20achieve%0Auser-specified%20task%20success%20rates%20while%20minimizing%20the%20overall%20number%20of%20help%0Arequests.%20We%20demonstrate%20the%20performance%20of%20our%20approach%20on%20multi-robot%20home%0Aservice%20applications.%20We%20also%20show%20through%20comparative%20experiments%2C%20that%20our%0Amethod%20outperforms%20recent%20centralized%20and%20decentralized%20multi-robot%20LLM-based%0Aplanners%20in%20terms%20of%20in%20terms%20of%20its%20ability%20to%20design%20correct%20plans.%20The%0Aadvantage%20of%20our%20algorithm%20over%20baselines%20becomes%20more%20pronounced%20with%0Aincreasing%20mission%20complexity%20and%20robot%20team%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15368v1&entry.124074799=Read"},
{"title": "CLIPPER+: A Fast Maximal Clique Algorithm for Robust Global Registration", "author": "Kaveh Fathian and Tyler Summers", "abstract": "  We present CLIPPER+, an algorithm for finding maximal cliques in unweighted\ngraphs for outlier-robust global registration. The registration problem can be\nformulated as a graph and solved by finding its maximum clique. This\nformulation leads to extreme robustness to outliers; however, finding the\nmaximum clique is an NP-hard problem, and therefore approximation is required\nin practice for large-size problems. The performance of an approximation\nalgorithm is evaluated by its computational complexity (the lower the runtime,\nthe better) and solution accuracy (how close the solution is to the maximum\nclique). Accordingly, the main contribution of CLIPPER+ is outperforming the\nstate-of-the-art in accuracy while maintaining a relatively low runtime.\nCLIPPER+ builds on prior work (CLIPPER [1] and PMC [2]) and prunes the graph by\nremoving vertices that have a small core number and cannot be a part of the\nmaximum clique. This will result in a smaller graph, on which the maximum\nclique can be estimated considerably faster. We evaluate the performance of\nCLIPPER+ on standard graph benchmarks, as well as synthetic and real-world\npoint cloud registration problems. These evaluations demonstrate that CLIPPER+\nhas the highest accuracy and can register point clouds in scenarios where over\n$99\\%$ of associations are outliers. Our code and evaluation benchmarks are\nreleased at https://github.com/ariarobotics/clipperp.\n", "link": "http://arxiv.org/abs/2402.15464v1", "date": "2024-02-23", "relevancy": 2.286, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4516}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4288}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPPER%2B%3A%20A%20Fast%20Maximal%20Clique%20Algorithm%20for%20Robust%20Global%20Registration&entry.906535625=Kaveh%20Fathian%20and%20Tyler%20Summers&entry.1292438233=%20%20We%20present%20CLIPPER%2B%2C%20an%20algorithm%20for%20finding%20maximal%20cliques%20in%20unweighted%0Agraphs%20for%20outlier-robust%20global%20registration.%20The%20registration%20problem%20can%20be%0Aformulated%20as%20a%20graph%20and%20solved%20by%20finding%20its%20maximum%20clique.%20This%0Aformulation%20leads%20to%20extreme%20robustness%20to%20outliers%3B%20however%2C%20finding%20the%0Amaximum%20clique%20is%20an%20NP-hard%20problem%2C%20and%20therefore%20approximation%20is%20required%0Ain%20practice%20for%20large-size%20problems.%20The%20performance%20of%20an%20approximation%0Aalgorithm%20is%20evaluated%20by%20its%20computational%20complexity%20%28the%20lower%20the%20runtime%2C%0Athe%20better%29%20and%20solution%20accuracy%20%28how%20close%20the%20solution%20is%20to%20the%20maximum%0Aclique%29.%20Accordingly%2C%20the%20main%20contribution%20of%20CLIPPER%2B%20is%20outperforming%20the%0Astate-of-the-art%20in%20accuracy%20while%20maintaining%20a%20relatively%20low%20runtime.%0ACLIPPER%2B%20builds%20on%20prior%20work%20%28CLIPPER%20%5B1%5D%20and%20PMC%20%5B2%5D%29%20and%20prunes%20the%20graph%20by%0Aremoving%20vertices%20that%20have%20a%20small%20core%20number%20and%20cannot%20be%20a%20part%20of%20the%0Amaximum%20clique.%20This%20will%20result%20in%20a%20smaller%20graph%2C%20on%20which%20the%20maximum%0Aclique%20can%20be%20estimated%20considerably%20faster.%20We%20evaluate%20the%20performance%20of%0ACLIPPER%2B%20on%20standard%20graph%20benchmarks%2C%20as%20well%20as%20synthetic%20and%20real-world%0Apoint%20cloud%20registration%20problems.%20These%20evaluations%20demonstrate%20that%20CLIPPER%2B%0Ahas%20the%20highest%20accuracy%20and%20can%20register%20point%20clouds%20in%20scenarios%20where%20over%0A%2499%5C%25%24%20of%20associations%20are%20outliers.%20Our%20code%20and%20evaluation%20benchmarks%20are%0Areleased%20at%20https%3A//github.com/ariarobotics/clipperp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15464v1&entry.124074799=Read"},
{"title": "UFO: A UI-Focused Agent for Windows OS Interaction", "author": "Chaoyun Zhang and Liqun Li and Shilin He and Xu Zhang and Bo Qiao and Si Qin and Minghua Ma and Yu Kang and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang and Qi Zhang", "abstract": "  We introduce UFO, an innovative UI-Focused agent to fulfill user requests\ntailored to applications on Windows OS, harnessing the capabilities of\nGPT-Vision. UFO employs a dual-agent framework to meticulously observe and\nanalyze the graphical user interface (GUI) and control information of Windows\napplications. This enables the agent to seamlessly navigate and operate within\nindividual applications and across them to fulfill user requests, even when\nspanning multiple applications. The framework incorporates a control\ninteraction module, facilitating action grounding without human intervention\nand enabling fully automated execution. Consequently, UFO transforms arduous\nand time-consuming processes into simple tasks achievable solely through\nnatural language commands. We conducted testing of UFO across 9 popular Windows\napplications, encompassing a variety of scenarios reflective of users' daily\nusage. The results, derived from both quantitative metrics and real-case\nstudies, underscore the superior effectiveness of UFO in fulfilling user\nrequests. To the best of our knowledge, UFO stands as the first UI agent\nspecifically tailored for task completion within the Windows OS environment.\nThe open-source code for UFO is available on https://github.com/microsoft/UFO.\n", "link": "http://arxiv.org/abs/2402.07939v3", "date": "2024-02-23", "relevancy": 2.2777, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4578}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4546}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4541}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UFO%3A%20A%20UI-Focused%20Agent%20for%20Windows%20OS%20Interaction&entry.906535625=Chaoyun%20Zhang%20and%20Liqun%20Li%20and%20Shilin%20He%20and%20Xu%20Zhang%20and%20Bo%20Qiao%20and%20Si%20Qin%20and%20Minghua%20Ma%20and%20Yu%20Kang%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang&entry.1292438233=%20%20We%20introduce%20UFO%2C%20an%20innovative%20UI-Focused%20agent%20to%20fulfill%20user%20requests%0Atailored%20to%20applications%20on%20Windows%20OS%2C%20harnessing%20the%20capabilities%20of%0AGPT-Vision.%20UFO%20employs%20a%20dual-agent%20framework%20to%20meticulously%20observe%20and%0Aanalyze%20the%20graphical%20user%20interface%20%28GUI%29%20and%20control%20information%20of%20Windows%0Aapplications.%20This%20enables%20the%20agent%20to%20seamlessly%20navigate%20and%20operate%20within%0Aindividual%20applications%20and%20across%20them%20to%20fulfill%20user%20requests%2C%20even%20when%0Aspanning%20multiple%20applications.%20The%20framework%20incorporates%20a%20control%0Ainteraction%20module%2C%20facilitating%20action%20grounding%20without%20human%20intervention%0Aand%20enabling%20fully%20automated%20execution.%20Consequently%2C%20UFO%20transforms%20arduous%0Aand%20time-consuming%20processes%20into%20simple%20tasks%20achievable%20solely%20through%0Anatural%20language%20commands.%20We%20conducted%20testing%20of%20UFO%20across%209%20popular%20Windows%0Aapplications%2C%20encompassing%20a%20variety%20of%20scenarios%20reflective%20of%20users%27%20daily%0Ausage.%20The%20results%2C%20derived%20from%20both%20quantitative%20metrics%20and%20real-case%0Astudies%2C%20underscore%20the%20superior%20effectiveness%20of%20UFO%20in%20fulfilling%20user%0Arequests.%20To%20the%20best%20of%20our%20knowledge%2C%20UFO%20stands%20as%20the%20first%20UI%20agent%0Aspecifically%20tailored%20for%20task%20completion%20within%20the%20Windows%20OS%20environment.%0AThe%20open-source%20code%20for%20UFO%20is%20available%20on%20https%3A//github.com/microsoft/UFO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07939v3&entry.124074799=Read"},
{"title": "Offline Inverse RL: New Solution Concepts and Provably Efficient\n  Algorithms", "author": "Filippo Lazzati and Mirco Mutti and Alberto Maria Metelli", "abstract": "  Inverse reinforcement learning (IRL) aims to recover the reward function of\nan expert agent from demonstrations of behavior. It is well known that the IRL\nproblem is fundamentally ill-posed, i.e., many reward functions can explain the\ndemonstrations. For this reason, IRL has been recently reframed in terms of\nestimating the feasible reward set, thus, postponing the selection of a single\nreward. However, so far, the available formulations and algorithmic solutions\nhave been proposed and analyzed mainly for the online setting, where the\nlearner can interact with the environment and query the expert at will. This is\nclearly unrealistic in most practical applications, where the availability of\nan offline dataset is a much more common scenario. In this paper, we introduce\na novel notion of feasible reward set capturing the opportunities and\nlimitations of the offline setting and we analyze the complexity of its\nestimation. This requires the introduction an original learning framework that\ncopes with the intrinsic difficulty of the setting, for which the data coverage\nis not under control. Then, we propose two computationally and statistically\nefficient algorithms, IRLO and PIRLO, for addressing the problem. In\nparticular, the latter adopts a specific form of pessimism to enforce the novel\ndesirable property of inclusion monotonicity of the delivered feasible set.\nWith this work, we aim to provide a panorama of the challenges of the offline\nIRL problem and how they can be fruitfully addressed.\n", "link": "http://arxiv.org/abs/2402.15392v1", "date": "2024-02-23", "relevancy": 2.2647, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4566}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4519}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4503}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Offline%20Inverse%20RL%3A%20New%20Solution%20Concepts%20and%20Provably%20Efficient%0A%20%20Algorithms&entry.906535625=Filippo%20Lazzati%20and%20Mirco%20Mutti%20and%20Alberto%20Maria%20Metelli&entry.1292438233=%20%20Inverse%20reinforcement%20learning%20%28IRL%29%20aims%20to%20recover%20the%20reward%20function%20of%0Aan%20expert%20agent%20from%20demonstrations%20of%20behavior.%20It%20is%20well%20known%20that%20the%20IRL%0Aproblem%20is%20fundamentally%20ill-posed%2C%20i.e.%2C%20many%20reward%20functions%20can%20explain%20the%0Ademonstrations.%20For%20this%20reason%2C%20IRL%20has%20been%20recently%20reframed%20in%20terms%20of%0Aestimating%20the%20feasible%20reward%20set%2C%20thus%2C%20postponing%20the%20selection%20of%20a%20single%0Areward.%20However%2C%20so%20far%2C%20the%20available%20formulations%20and%20algorithmic%20solutions%0Ahave%20been%20proposed%20and%20analyzed%20mainly%20for%20the%20online%20setting%2C%20where%20the%0Alearner%20can%20interact%20with%20the%20environment%20and%20query%20the%20expert%20at%20will.%20This%20is%0Aclearly%20unrealistic%20in%20most%20practical%20applications%2C%20where%20the%20availability%20of%0Aan%20offline%20dataset%20is%20a%20much%20more%20common%20scenario.%20In%20this%20paper%2C%20we%20introduce%0Aa%20novel%20notion%20of%20feasible%20reward%20set%20capturing%20the%20opportunities%20and%0Alimitations%20of%20the%20offline%20setting%20and%20we%20analyze%20the%20complexity%20of%20its%0Aestimation.%20This%20requires%20the%20introduction%20an%20original%20learning%20framework%20that%0Acopes%20with%20the%20intrinsic%20difficulty%20of%20the%20setting%2C%20for%20which%20the%20data%20coverage%0Ais%20not%20under%20control.%20Then%2C%20we%20propose%20two%20computationally%20and%20statistically%0Aefficient%20algorithms%2C%20IRLO%20and%20PIRLO%2C%20for%20addressing%20the%20problem.%20In%0Aparticular%2C%20the%20latter%20adopts%20a%20specific%20form%20of%20pessimism%20to%20enforce%20the%20novel%0Adesirable%20property%20of%20inclusion%20monotonicity%20of%20the%20delivered%20feasible%20set.%0AWith%20this%20work%2C%20we%20aim%20to%20provide%20a%20panorama%20of%20the%20challenges%20of%20the%20offline%0AIRL%20problem%20and%20how%20they%20can%20be%20fruitfully%20addressed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15392v1&entry.124074799=Read"},
{"title": "Computer Vision for Multimedia Geolocation in Human Trafficking\n  Investigation: A Systematic Literature Review", "author": "Opeyemi Bamigbade and John Sheppard and Mark Scanlon", "abstract": "  The task of multimedia geolocation is becoming an increasingly essential\ncomponent of the digital forensics toolkit to effectively combat human\ntrafficking, child sexual exploitation, and other illegal acts. Typically,\nmetadata-based geolocation information is stripped when multimedia content is\nshared via instant messaging and social media. The intricacy of geolocating,\ngeotagging, or finding geographical clues in this content is often overly\nburdensome for investigators. Recent research has shown that contemporary\nadvancements in artificial intelligence, specifically computer vision and deep\nlearning, show significant promise towards expediting the multimedia\ngeolocation task. This systematic literature review thoroughly examines the\nstate-of-the-art leveraging computer vision techniques for multimedia\ngeolocation and assesses their potential to expedite human trafficking\ninvestigation. This includes a comprehensive overview of the application of\ncomputer vision-based approaches to multimedia geolocation, identifies their\napplicability in combating human trafficking, and highlights the potential\nimplications of enhanced multimedia geolocation for prosecuting human\ntrafficking. 123 articles inform this systematic literature review. The\nfindings suggest numerous potential paths for future impactful research on the\nsubject.\n", "link": "http://arxiv.org/abs/2402.15448v1", "date": "2024-02-23", "relevancy": 2.2597, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4635}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4476}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4448}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer%20Vision%20for%20Multimedia%20Geolocation%20in%20Human%20Trafficking%0A%20%20Investigation%3A%20A%20Systematic%20Literature%20Review&entry.906535625=Opeyemi%20Bamigbade%20and%20John%20Sheppard%20and%20Mark%20Scanlon&entry.1292438233=%20%20The%20task%20of%20multimedia%20geolocation%20is%20becoming%20an%20increasingly%20essential%0Acomponent%20of%20the%20digital%20forensics%20toolkit%20to%20effectively%20combat%20human%0Atrafficking%2C%20child%20sexual%20exploitation%2C%20and%20other%20illegal%20acts.%20Typically%2C%0Ametadata-based%20geolocation%20information%20is%20stripped%20when%20multimedia%20content%20is%0Ashared%20via%20instant%20messaging%20and%20social%20media.%20The%20intricacy%20of%20geolocating%2C%0Ageotagging%2C%20or%20finding%20geographical%20clues%20in%20this%20content%20is%20often%20overly%0Aburdensome%20for%20investigators.%20Recent%20research%20has%20shown%20that%20contemporary%0Aadvancements%20in%20artificial%20intelligence%2C%20specifically%20computer%20vision%20and%20deep%0Alearning%2C%20show%20significant%20promise%20towards%20expediting%20the%20multimedia%0Ageolocation%20task.%20This%20systematic%20literature%20review%20thoroughly%20examines%20the%0Astate-of-the-art%20leveraging%20computer%20vision%20techniques%20for%20multimedia%0Ageolocation%20and%20assesses%20their%20potential%20to%20expedite%20human%20trafficking%0Ainvestigation.%20This%20includes%20a%20comprehensive%20overview%20of%20the%20application%20of%0Acomputer%20vision-based%20approaches%20to%20multimedia%20geolocation%2C%20identifies%20their%0Aapplicability%20in%20combating%20human%20trafficking%2C%20and%20highlights%20the%20potential%0Aimplications%20of%20enhanced%20multimedia%20geolocation%20for%20prosecuting%20human%0Atrafficking.%20123%20articles%20inform%20this%20systematic%20literature%20review.%20The%0Afindings%20suggest%20numerous%20potential%20paths%20for%20future%20impactful%20research%20on%20the%0Asubject.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15448v1&entry.124074799=Read"},
{"title": "Semi-Supervised Semantic Segmentation using Redesigned Self-Training for\n  White Blood Cells", "author": "Vinh Quoc Luu and Duy Khanh Le and Huy Thanh Nguyen and Minh Thanh Nguyen and Thinh Tien Nguyen and Vinh Quang Dinh", "abstract": "  Artificial Intelligence (AI) in healthcare, especially in white blood cell\ncancer diagnosis, is hindered by two primary challenges: the lack of\nlarge-scale labeled datasets for white blood cell (WBC) segmentation and\noutdated segmentation methods. These challenges inhibit the development of more\naccurate and modern techniques to diagnose cancer relating to white blood\ncells. To address the first challenge, a semi-supervised learning framework\nshould be devised to efficiently capitalize on the scarcity of the dataset\navailable. In this work, we address this issue by proposing a novel\nself-training pipeline with the incorporation of FixMatch. Self-training is a\ntechnique that utilizes the model trained on labeled data to generate\npseudo-labels for the unlabeled data and then re-train on both of them.\nFixMatch is a consistency-regularization algorithm to enforce the model's\nrobustness against variations in the input image. We discover that by\nincorporating FixMatch in the self-training pipeline, the performance improves\nin the majority of cases. Our performance achieved the best performance with\nthe self-training scheme with consistency on DeepLab-V3 architecture and\nResNet-50, reaching 90.69%, 87.37%, and 76.49% on Zheng 1, Zheng 2, and LISC\ndatasets, respectively.\n", "link": "http://arxiv.org/abs/2401.07278v3", "date": "2024-02-23", "relevancy": 2.2539, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4645}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4568}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.431}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Semantic%20Segmentation%20using%20Redesigned%20Self-Training%20for%0A%20%20White%20Blood%20Cells&entry.906535625=Vinh%20Quoc%20Luu%20and%20Duy%20Khanh%20Le%20and%20Huy%20Thanh%20Nguyen%20and%20Minh%20Thanh%20Nguyen%20and%20Thinh%20Tien%20Nguyen%20and%20Vinh%20Quang%20Dinh&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20in%20healthcare%2C%20especially%20in%20white%20blood%20cell%0Acancer%20diagnosis%2C%20is%20hindered%20by%20two%20primary%20challenges%3A%20the%20lack%20of%0Alarge-scale%20labeled%20datasets%20for%20white%20blood%20cell%20%28WBC%29%20segmentation%20and%0Aoutdated%20segmentation%20methods.%20These%20challenges%20inhibit%20the%20development%20of%20more%0Aaccurate%20and%20modern%20techniques%20to%20diagnose%20cancer%20relating%20to%20white%20blood%0Acells.%20To%20address%20the%20first%20challenge%2C%20a%20semi-supervised%20learning%20framework%0Ashould%20be%20devised%20to%20efficiently%20capitalize%20on%20the%20scarcity%20of%20the%20dataset%0Aavailable.%20In%20this%20work%2C%20we%20address%20this%20issue%20by%20proposing%20a%20novel%0Aself-training%20pipeline%20with%20the%20incorporation%20of%20FixMatch.%20Self-training%20is%20a%0Atechnique%20that%20utilizes%20the%20model%20trained%20on%20labeled%20data%20to%20generate%0Apseudo-labels%20for%20the%20unlabeled%20data%20and%20then%20re-train%20on%20both%20of%20them.%0AFixMatch%20is%20a%20consistency-regularization%20algorithm%20to%20enforce%20the%20model%27s%0Arobustness%20against%20variations%20in%20the%20input%20image.%20We%20discover%20that%20by%0Aincorporating%20FixMatch%20in%20the%20self-training%20pipeline%2C%20the%20performance%20improves%0Ain%20the%20majority%20of%20cases.%20Our%20performance%20achieved%20the%20best%20performance%20with%0Athe%20self-training%20scheme%20with%20consistency%20on%20DeepLab-V3%20architecture%20and%0AResNet-50%2C%20reaching%2090.69%25%2C%2087.37%25%2C%20and%2076.49%25%20on%20Zheng%201%2C%20Zheng%202%2C%20and%20LISC%0Adatasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07278v3&entry.124074799=Read"},
{"title": "EMIFF: Enhanced Multi-scale Image Feature Fusion for\n  Vehicle-Infrastructure Cooperative 3D Object Detection", "author": "Zhe Wang and Siqi Fan and Xiaoliang Huo and Tongda Xu and Yan Wang and Jingjing Liu and Yilun Chen and Ya-Qin Zhang", "abstract": "  In autonomous driving, cooperative perception makes use of multi-view cameras\nfrom both vehicles and infrastructure, providing a global vantage point with\nrich semantic context of road conditions beyond a single vehicle viewpoint.\nCurrently, two major challenges persist in vehicle-infrastructure cooperative\n3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view\nimages, caused by time asynchrony across cameras; $2)$ information loss in\ntransmission process resulted from limited communication bandwidth. To address\nthese issues, we propose a novel camera-based 3D detection framework for VIC3D\ntask, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit\nholistic perspectives from both vehicles and infrastructure, we propose\nMulti-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM)\nmodules to enhance infrastructure and vehicle features at scale, spatial, and\nchannel levels to correct the pose error introduced by camera asynchrony. We\nalso introduce a Feature Compression (FC) module with channel and spatial\ncompression blocks for transmission efficiency. Experiments show that EMIFF\nachieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous\nearly-fusion and late-fusion methods with comparable transmission costs.\n", "link": "http://arxiv.org/abs/2402.15272v1", "date": "2024-02-23", "relevancy": 2.2436, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5956}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5427}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5335}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMIFF%3A%20Enhanced%20Multi-scale%20Image%20Feature%20Fusion%20for%0A%20%20Vehicle-Infrastructure%20Cooperative%203D%20Object%20Detection&entry.906535625=Zhe%20Wang%20and%20Siqi%20Fan%20and%20Xiaoliang%20Huo%20and%20Tongda%20Xu%20and%20Yan%20Wang%20and%20Jingjing%20Liu%20and%20Yilun%20Chen%20and%20Ya-Qin%20Zhang&entry.1292438233=%20%20In%20autonomous%20driving%2C%20cooperative%20perception%20makes%20use%20of%20multi-view%20cameras%0Afrom%20both%20vehicles%20and%20infrastructure%2C%20providing%20a%20global%20vantage%20point%20with%0Arich%20semantic%20context%20of%20road%20conditions%20beyond%20a%20single%20vehicle%20viewpoint.%0ACurrently%2C%20two%20major%20challenges%20persist%20in%20vehicle-infrastructure%20cooperative%0A3D%20%28VIC3D%29%20object%20detection%3A%20%241%29%24%20inherent%20pose%20errors%20when%20fusing%20multi-view%0Aimages%2C%20caused%20by%20time%20asynchrony%20across%20cameras%3B%20%242%29%24%20information%20loss%20in%0Atransmission%20process%20resulted%20from%20limited%20communication%20bandwidth.%20To%20address%0Athese%20issues%2C%20we%20propose%20a%20novel%20camera-based%203D%20detection%20framework%20for%20VIC3D%0Atask%2C%20Enhanced%20Multi-scale%20Image%20Feature%20Fusion%20%28EMIFF%29.%20To%20fully%20exploit%0Aholistic%20perspectives%20from%20both%20vehicles%20and%20infrastructure%2C%20we%20propose%0AMulti-scale%20Cross%20Attention%20%28MCA%29%20and%20Camera-aware%20Channel%20Masking%20%28CCM%29%0Amodules%20to%20enhance%20infrastructure%20and%20vehicle%20features%20at%20scale%2C%20spatial%2C%20and%0Achannel%20levels%20to%20correct%20the%20pose%20error%20introduced%20by%20camera%20asynchrony.%20We%0Aalso%20introduce%20a%20Feature%20Compression%20%28FC%29%20module%20with%20channel%20and%20spatial%0Acompression%20blocks%20for%20transmission%20efficiency.%20Experiments%20show%20that%20EMIFF%0Aachieves%20SOTA%20on%20DAIR-V2X-C%20datasets%2C%20significantly%20outperforming%20previous%0Aearly-fusion%20and%20late-fusion%20methods%20with%20comparable%20transmission%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15272v1&entry.124074799=Read"},
{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "author": "Hanxiao Jiang and Binghao Huang and Ruihai Wu and Zhuoran Li and Shubham Garg and Hooshang Nayyeri and Shenlong Wang and Yunzhu Li", "abstract": "  Robots need to explore their surroundings to adapt to and tackle tasks in\nunknown environments. Prior work has proposed building scene graphs of the\nenvironment but typically assumes that the environment is static, omitting\nregions that require active interactions. This severely limits their ability to\nhandle more complex tasks in household and office environments: before setting\nup a table, robots must explore drawers and cabinets to locate all utensils and\ncondiments. In this work, we introduce the novel task of interactive scene\nexploration, wherein robots autonomously explore environments and produce an\naction-conditioned scene graph (ACSG) that captures the structure of the\nunderlying environment. The ACSG accounts for both low-level information, such\nas geometry and semantics, and high-level information, such as the\naction-conditioned relationships between different entities in the scene. To\nthis end, we present the Robotic Exploration (RoboEXP) system, which\nincorporates the Large Multimodal Model (LMM) and an explicit memory design to\nenhance our system's capabilities. The robot reasons about what and how to\nexplore an object, accumulating new information through the interaction process\nand incrementally constructing the ACSG. We apply our system across various\nreal-world settings in a zero-shot manner, demonstrating its effectiveness in\nexploring and modeling environments it has never seen before. Leveraging the\nconstructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP\nsystem in facilitating a wide range of real-world manipulation tasks involving\nrigid, articulated objects, nested objects like Matryoshka dolls, and\ndeformable objects like cloth.\n", "link": "http://arxiv.org/abs/2402.15487v1", "date": "2024-02-23", "relevancy": 2.2152, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.566}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5515}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5513}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboEXP%3A%20Action-Conditioned%20Scene%20Graph%20via%20Interactive%20Exploration%20for%0A%20%20Robotic%20Manipulation&entry.906535625=Hanxiao%20Jiang%20and%20Binghao%20Huang%20and%20Ruihai%20Wu%20and%20Zhuoran%20Li%20and%20Shubham%20Garg%20and%20Hooshang%20Nayyeri%20and%20Shenlong%20Wang%20and%20Yunzhu%20Li&entry.1292438233=%20%20Robots%20need%20to%20explore%20their%20surroundings%20to%20adapt%20to%20and%20tackle%20tasks%20in%0Aunknown%20environments.%20Prior%20work%20has%20proposed%20building%20scene%20graphs%20of%20the%0Aenvironment%20but%20typically%20assumes%20that%20the%20environment%20is%20static%2C%20omitting%0Aregions%20that%20require%20active%20interactions.%20This%20severely%20limits%20their%20ability%20to%0Ahandle%20more%20complex%20tasks%20in%20household%20and%20office%20environments%3A%20before%20setting%0Aup%20a%20table%2C%20robots%20must%20explore%20drawers%20and%20cabinets%20to%20locate%20all%20utensils%20and%0Acondiments.%20In%20this%20work%2C%20we%20introduce%20the%20novel%20task%20of%20interactive%20scene%0Aexploration%2C%20wherein%20robots%20autonomously%20explore%20environments%20and%20produce%20an%0Aaction-conditioned%20scene%20graph%20%28ACSG%29%20that%20captures%20the%20structure%20of%20the%0Aunderlying%20environment.%20The%20ACSG%20accounts%20for%20both%20low-level%20information%2C%20such%0Aas%20geometry%20and%20semantics%2C%20and%20high-level%20information%2C%20such%20as%20the%0Aaction-conditioned%20relationships%20between%20different%20entities%20in%20the%20scene.%20To%0Athis%20end%2C%20we%20present%20the%20Robotic%20Exploration%20%28RoboEXP%29%20system%2C%20which%0Aincorporates%20the%20Large%20Multimodal%20Model%20%28LMM%29%20and%20an%20explicit%20memory%20design%20to%0Aenhance%20our%20system%27s%20capabilities.%20The%20robot%20reasons%20about%20what%20and%20how%20to%0Aexplore%20an%20object%2C%20accumulating%20new%20information%20through%20the%20interaction%20process%0Aand%20incrementally%20constructing%20the%20ACSG.%20We%20apply%20our%20system%20across%20various%0Areal-world%20settings%20in%20a%20zero-shot%20manner%2C%20demonstrating%20its%20effectiveness%20in%0Aexploring%20and%20modeling%20environments%20it%20has%20never%20seen%20before.%20Leveraging%20the%0Aconstructed%20ACSG%2C%20we%20illustrate%20the%20effectiveness%20and%20efficiency%20of%20our%20RoboEXP%0Asystem%20in%20facilitating%20a%20wide%20range%20of%20real-world%20manipulation%20tasks%20involving%0Arigid%2C%20articulated%20objects%2C%20nested%20objects%20like%20Matryoshka%20dolls%2C%20and%0Adeformable%20objects%20like%20cloth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15487v1&entry.124074799=Read"},
{"title": "Graph Transformers without Positional Encodings", "author": "Ayush Garg", "abstract": "  Recently, Transformers for graph representation learning have become\nincreasingly popular, achieving state-of-the-art performance on a wide-variety\nof datasets, either alone or in combination with message-passing graph neural\nnetworks (MP-GNNs). Infusing graph inductive-biases in the innately\nstructure-agnostic transformer architecture in the form of structural or\npositional encodings (PEs) is key to achieving these impressive results.\nHowever, designing such encodings is tricky and disparate attempts have been\nmade to engineer such encodings including Laplacian eigenvectors, relative\nrandom-walk probabilities (RRWP), spatial encodings, centrality encodings, edge\nencodings etc. In this work, we argue that such encodings may not be required\nat all, provided the attention mechanism itself incorporates information about\nthe graph structure. We introduce Eigenformer, a Graph Transformer employing a\nnovel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of\nthe graph, and empirically show that it achieves performance comparable to SOTA\nGraph Transformers on a number of standard GNN benchmark datasets, even\nsurpassing the SOTA on some datasets. The simpler attention mechanism also\nallows us to train wider and deeper models for a given parameter budget.\n", "link": "http://arxiv.org/abs/2401.17791v2", "date": "2024-02-23", "relevancy": 2.2122, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.444}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4429}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4404}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Transformers%20without%20Positional%20Encodings&entry.906535625=Ayush%20Garg&entry.1292438233=%20%20Recently%2C%20Transformers%20for%20graph%20representation%20learning%20have%20become%0Aincreasingly%20popular%2C%20achieving%20state-of-the-art%20performance%20on%20a%20wide-variety%0Aof%20datasets%2C%20either%20alone%20or%20in%20combination%20with%20message-passing%20graph%20neural%0Anetworks%20%28MP-GNNs%29.%20Infusing%20graph%20inductive-biases%20in%20the%20innately%0Astructure-agnostic%20transformer%20architecture%20in%20the%20form%20of%20structural%20or%0Apositional%20encodings%20%28PEs%29%20is%20key%20to%20achieving%20these%20impressive%20results.%0AHowever%2C%20designing%20such%20encodings%20is%20tricky%20and%20disparate%20attempts%20have%20been%0Amade%20to%20engineer%20such%20encodings%20including%20Laplacian%20eigenvectors%2C%20relative%0Arandom-walk%20probabilities%20%28RRWP%29%2C%20spatial%20encodings%2C%20centrality%20encodings%2C%20edge%0Aencodings%20etc.%20In%20this%20work%2C%20we%20argue%20that%20such%20encodings%20may%20not%20be%20required%0Aat%20all%2C%20provided%20the%20attention%20mechanism%20itself%20incorporates%20information%20about%0Athe%20graph%20structure.%20We%20introduce%20Eigenformer%2C%20a%20Graph%20Transformer%20employing%20a%0Anovel%20spectrum-aware%20attention%20mechanism%20cognizant%20of%20the%20Laplacian%20spectrum%20of%0Athe%20graph%2C%20and%20empirically%20show%20that%20it%20achieves%20performance%20comparable%20to%20SOTA%0AGraph%20Transformers%20on%20a%20number%20of%20standard%20GNN%20benchmark%20datasets%2C%20even%0Asurpassing%20the%20SOTA%20on%20some%20datasets.%20The%20simpler%20attention%20mechanism%20also%0Aallows%20us%20to%20train%20wider%20and%20deeper%20models%20for%20a%20given%20parameter%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17791v2&entry.124074799=Read"},
{"title": "Counterfactual Generation with Identifiability Guarantees", "author": "Hanqi Yan and Lingjing Kong and Lin Gui and Yuejie Chi and Eric Xing and Yulan He and Kun Zhang", "abstract": "  Counterfactual generation lies at the core of various machine learning tasks,\nincluding image translation and controllable text generation. This generation\nprocess usually requires the identification of the disentangled latent\nrepresentations, such as content and style, that underlie the observed data.\nHowever, it becomes more challenging when faced with a scarcity of paired data\nand labeling information. Existing disentangled methods crucially rely on\noversimplified assumptions, such as assuming independent content and style\nvariables, to identify the latent variables, even though such assumptions may\nnot hold for complex data distributions. For instance, food reviews tend to\ninvolve words like tasty, whereas movie reviews commonly contain words such as\nthrilling for the same positive sentiment. This problem is exacerbated when\ndata are sampled from multiple domains since the dependence between content and\nstyle may vary significantly over domains. In this work, we tackle the\ndomain-varying dependence between the content and the style variables inherent\nin the counterfactual generation task. We provide identification guarantees for\nsuch latent-variable models by leveraging the relative sparsity of the\ninfluences from different latent variables. Our theoretical insights enable the\ndevelopment of a doMain AdapTive counTerfactual gEneration model, called\n(MATTE). Our theoretically grounded framework achieves state-of-the-art\nperformance in unsupervised style transfer tasks, where neither paired data nor\nstyle labels are utilized, across four large-scale datasets. Code is available\nat https://github.com/hanqi-qi/Matte.git\n", "link": "http://arxiv.org/abs/2402.15309v1", "date": "2024-02-23", "relevancy": 2.193, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6109}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.52}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4969}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Generation%20with%20Identifiability%20Guarantees&entry.906535625=Hanqi%20Yan%20and%20Lingjing%20Kong%20and%20Lin%20Gui%20and%20Yuejie%20Chi%20and%20Eric%20Xing%20and%20Yulan%20He%20and%20Kun%20Zhang&entry.1292438233=%20%20Counterfactual%20generation%20lies%20at%20the%20core%20of%20various%20machine%20learning%20tasks%2C%0Aincluding%20image%20translation%20and%20controllable%20text%20generation.%20This%20generation%0Aprocess%20usually%20requires%20the%20identification%20of%20the%20disentangled%20latent%0Arepresentations%2C%20such%20as%20content%20and%20style%2C%20that%20underlie%20the%20observed%20data.%0AHowever%2C%20it%20becomes%20more%20challenging%20when%20faced%20with%20a%20scarcity%20of%20paired%20data%0Aand%20labeling%20information.%20Existing%20disentangled%20methods%20crucially%20rely%20on%0Aoversimplified%20assumptions%2C%20such%20as%20assuming%20independent%20content%20and%20style%0Avariables%2C%20to%20identify%20the%20latent%20variables%2C%20even%20though%20such%20assumptions%20may%0Anot%20hold%20for%20complex%20data%20distributions.%20For%20instance%2C%20food%20reviews%20tend%20to%0Ainvolve%20words%20like%20tasty%2C%20whereas%20movie%20reviews%20commonly%20contain%20words%20such%20as%0Athrilling%20for%20the%20same%20positive%20sentiment.%20This%20problem%20is%20exacerbated%20when%0Adata%20are%20sampled%20from%20multiple%20domains%20since%20the%20dependence%20between%20content%20and%0Astyle%20may%20vary%20significantly%20over%20domains.%20In%20this%20work%2C%20we%20tackle%20the%0Adomain-varying%20dependence%20between%20the%20content%20and%20the%20style%20variables%20inherent%0Ain%20the%20counterfactual%20generation%20task.%20We%20provide%20identification%20guarantees%20for%0Asuch%20latent-variable%20models%20by%20leveraging%20the%20relative%20sparsity%20of%20the%0Ainfluences%20from%20different%20latent%20variables.%20Our%20theoretical%20insights%20enable%20the%0Adevelopment%20of%20a%20doMain%20AdapTive%20counTerfactual%20gEneration%20model%2C%20called%0A%28MATTE%29.%20Our%20theoretically%20grounded%20framework%20achieves%20state-of-the-art%0Aperformance%20in%20unsupervised%20style%20transfer%20tasks%2C%20where%20neither%20paired%20data%20nor%0Astyle%20labels%20are%20utilized%2C%20across%20four%20large-scale%20datasets.%20Code%20is%20available%0Aat%20https%3A//github.com/hanqi-qi/Matte.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15309v1&entry.124074799=Read"},
{"title": "Low-Rank Representations Meets Deep Unfolding: A Generalized and\n  Interpretable Network for Hyperspectral Anomaly Detection", "author": "Chenyu Li and Bing Zhang and Danfeng Hong and Jing Yao and Jocelyn Chanussot", "abstract": "  Current hyperspectral anomaly detection (HAD) benchmark datasets suffer from\nlow resolution, simple background, and small size of the detection data. These\nfactors also limit the performance of the well-known low-rank representation\n(LRR) models in terms of robustness on the separation of background and target\nfeatures and the reliance on manual parameter selection. To this end, we build\na new set of HAD benchmark datasets for improving the robustness of the HAD\nalgorithm in complex scenarios, AIR-HAD for short. Accordingly, we propose a\ngeneralized and interpretable HAD network by deeply unfolding a\ndictionary-learnable LLR model, named LRR-Net$^+$, which is capable of\nspectrally decoupling the background structure and object properties in a more\ngeneralized fashion and eliminating the bias introduced by vital interference\ntargets concurrently. In addition, LRR-Net$^+$ integrates the solution process\nof the Alternating Direction Method of Multipliers (ADMM) optimizer with the\ndeep network, guiding its search process and imparting a level of\ninterpretability to parameter optimization. Additionally, the integration of\nphysical models with DL techniques eliminates the need for manual parameter\ntuning. The manually tuned parameters are seamlessly transformed into trainable\nparameters for deep neural networks, facilitating a more efficient and\nautomated optimization process. Extensive experiments conducted on the AIR-HAD\ndataset show the superiority of our LRR-Net$^+$ in terms of detection\nperformance and generalization ability, compared to top-performing rivals.\nFurthermore, the compilable codes and our AIR-HAD benchmark datasets in this\npaper will be made available freely and openly at\n\\url{https://sites.google.com/view/danfeng-hong}.\n", "link": "http://arxiv.org/abs/2402.15335v1", "date": "2024-02-23", "relevancy": 2.1879, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5694}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5439}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5412}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Representations%20Meets%20Deep%20Unfolding%3A%20A%20Generalized%20and%0A%20%20Interpretable%20Network%20for%20Hyperspectral%20Anomaly%20Detection&entry.906535625=Chenyu%20Li%20and%20Bing%20Zhang%20and%20Danfeng%20Hong%20and%20Jing%20Yao%20and%20Jocelyn%20Chanussot&entry.1292438233=%20%20Current%20hyperspectral%20anomaly%20detection%20%28HAD%29%20benchmark%20datasets%20suffer%20from%0Alow%20resolution%2C%20simple%20background%2C%20and%20small%20size%20of%20the%20detection%20data.%20These%0Afactors%20also%20limit%20the%20performance%20of%20the%20well-known%20low-rank%20representation%0A%28LRR%29%20models%20in%20terms%20of%20robustness%20on%20the%20separation%20of%20background%20and%20target%0Afeatures%20and%20the%20reliance%20on%20manual%20parameter%20selection.%20To%20this%20end%2C%20we%20build%0Aa%20new%20set%20of%20HAD%20benchmark%20datasets%20for%20improving%20the%20robustness%20of%20the%20HAD%0Aalgorithm%20in%20complex%20scenarios%2C%20AIR-HAD%20for%20short.%20Accordingly%2C%20we%20propose%20a%0Ageneralized%20and%20interpretable%20HAD%20network%20by%20deeply%20unfolding%20a%0Adictionary-learnable%20LLR%20model%2C%20named%20LRR-Net%24%5E%2B%24%2C%20which%20is%20capable%20of%0Aspectrally%20decoupling%20the%20background%20structure%20and%20object%20properties%20in%20a%20more%0Ageneralized%20fashion%20and%20eliminating%20the%20bias%20introduced%20by%20vital%20interference%0Atargets%20concurrently.%20In%20addition%2C%20LRR-Net%24%5E%2B%24%20integrates%20the%20solution%20process%0Aof%20the%20Alternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29%20optimizer%20with%20the%0Adeep%20network%2C%20guiding%20its%20search%20process%20and%20imparting%20a%20level%20of%0Ainterpretability%20to%20parameter%20optimization.%20Additionally%2C%20the%20integration%20of%0Aphysical%20models%20with%20DL%20techniques%20eliminates%20the%20need%20for%20manual%20parameter%0Atuning.%20The%20manually%20tuned%20parameters%20are%20seamlessly%20transformed%20into%20trainable%0Aparameters%20for%20deep%20neural%20networks%2C%20facilitating%20a%20more%20efficient%20and%0Aautomated%20optimization%20process.%20Extensive%20experiments%20conducted%20on%20the%20AIR-HAD%0Adataset%20show%20the%20superiority%20of%20our%20LRR-Net%24%5E%2B%24%20in%20terms%20of%20detection%0Aperformance%20and%20generalization%20ability%2C%20compared%20to%20top-performing%20rivals.%0AFurthermore%2C%20the%20compilable%20codes%20and%20our%20AIR-HAD%20benchmark%20datasets%20in%20this%0Apaper%20will%20be%20made%20available%20freely%20and%20openly%20at%0A%5Curl%7Bhttps%3A//sites.google.com/view/danfeng-hong%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15335v1&entry.124074799=Read"},
{"title": "AltNeRF: Learning Robust Neural Radiance Field via Alternating\n  Depth-Pose Optimization", "author": "Kun Wang and Zhiqiang Yan and Huang Tian and Zhenyu Zhang and Xiang Li and Jun Li and Jian Yang", "abstract": "  Neural Radiance Fields (NeRF) have shown promise in generating realistic\nnovel views from sparse scene images. However, existing NeRF approaches often\nencounter challenges due to the lack of explicit 3D supervision and imprecise\ncamera poses, resulting in suboptimal outcomes. To tackle these issues, we\npropose AltNeRF -- a novel framework designed to create resilient NeRF\nrepresentations using self-supervised monocular depth estimation (SMDE) from\nmonocular videos, without relying on known camera poses. SMDE in AltNeRF\nmasterfully learns depth and pose priors to regulate NeRF training. The depth\nprior enriches NeRF's capacity for precise scene geometry depiction, while the\npose prior provides a robust starting point for subsequent pose refinement.\nMoreover, we introduce an alternating algorithm that harmoniously melds NeRF\noutputs into SMDE through a consistence-driven mechanism, thus enhancing the\nintegrity of depth priors. This alternation empowers AltNeRF to progressively\nrefine NeRF representations, yielding the synthesis of realistic novel views.\nExtensive experiments showcase the compelling capabilities of AltNeRF in\ngenerating high-fidelity and robust novel views that closely resemble reality.\n", "link": "http://arxiv.org/abs/2308.10001v2", "date": "2024-02-23", "relevancy": 2.1879, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5516}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5449}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5432}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AltNeRF%3A%20Learning%20Robust%20Neural%20Radiance%20Field%20via%20Alternating%0A%20%20Depth-Pose%20Optimization&entry.906535625=Kun%20Wang%20and%20Zhiqiang%20Yan%20and%20Huang%20Tian%20and%20Zhenyu%20Zhang%20and%20Xiang%20Li%20and%20Jun%20Li%20and%20Jian%20Yang&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20shown%20promise%20in%20generating%20realistic%0Anovel%20views%20from%20sparse%20scene%20images.%20However%2C%20existing%20NeRF%20approaches%20often%0Aencounter%20challenges%20due%20to%20the%20lack%20of%20explicit%203D%20supervision%20and%20imprecise%0Acamera%20poses%2C%20resulting%20in%20suboptimal%20outcomes.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20AltNeRF%20--%20a%20novel%20framework%20designed%20to%20create%20resilient%20NeRF%0Arepresentations%20using%20self-supervised%20monocular%20depth%20estimation%20%28SMDE%29%20from%0Amonocular%20videos%2C%20without%20relying%20on%20known%20camera%20poses.%20SMDE%20in%20AltNeRF%0Amasterfully%20learns%20depth%20and%20pose%20priors%20to%20regulate%20NeRF%20training.%20The%20depth%0Aprior%20enriches%20NeRF%27s%20capacity%20for%20precise%20scene%20geometry%20depiction%2C%20while%20the%0Apose%20prior%20provides%20a%20robust%20starting%20point%20for%20subsequent%20pose%20refinement.%0AMoreover%2C%20we%20introduce%20an%20alternating%20algorithm%20that%20harmoniously%20melds%20NeRF%0Aoutputs%20into%20SMDE%20through%20a%20consistence-driven%20mechanism%2C%20thus%20enhancing%20the%0Aintegrity%20of%20depth%20priors.%20This%20alternation%20empowers%20AltNeRF%20to%20progressively%0Arefine%20NeRF%20representations%2C%20yielding%20the%20synthesis%20of%20realistic%20novel%20views.%0AExtensive%20experiments%20showcase%20the%20compelling%20capabilities%20of%20AltNeRF%20in%0Agenerating%20high-fidelity%20and%20robust%20novel%20views%20that%20closely%20resemble%20reality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10001v2&entry.124074799=Read"},
{"title": "When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination", "author": "Martin Benfeghoul and Umais Zahid and Qinghai Guo and Zafeirios Fountas", "abstract": "  In an unfamiliar setting, a model-based reinforcement learning agent can be\nlimited by the accuracy of its world model. In this work, we present a novel,\ntraining-free approach to improving the performance of such agents separately\nfrom planning and learning. We do so by applying iterative inference at\ndecision-time, to fine-tune the inferred agent states based on the coherence of\nfuture state representations. Our approach achieves a consistent improvement in\nboth reconstruction accuracy and task performance when applied to visual 3D\nnavigation tasks. We go on to show that considering more future states further\nimproves the performance of the agent in partially-observable environments, but\nnot in a fully-observable one. Finally, we demonstrate that agents with less\ntraining pre-evaluation benefit most from our approach.\n", "link": "http://arxiv.org/abs/2402.15283v1", "date": "2024-02-23", "relevancy": 2.1833, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5438}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5379}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20in%20Doubt%2C%20Think%20Slow%3A%20Iterative%20Reasoning%20with%20Latent%20Imagination&entry.906535625=Martin%20Benfeghoul%20and%20Umais%20Zahid%20and%20Qinghai%20Guo%20and%20Zafeirios%20Fountas&entry.1292438233=%20%20In%20an%20unfamiliar%20setting%2C%20a%20model-based%20reinforcement%20learning%20agent%20can%20be%0Alimited%20by%20the%20accuracy%20of%20its%20world%20model.%20In%20this%20work%2C%20we%20present%20a%20novel%2C%0Atraining-free%20approach%20to%20improving%20the%20performance%20of%20such%20agents%20separately%0Afrom%20planning%20and%20learning.%20We%20do%20so%20by%20applying%20iterative%20inference%20at%0Adecision-time%2C%20to%20fine-tune%20the%20inferred%20agent%20states%20based%20on%20the%20coherence%20of%0Afuture%20state%20representations.%20Our%20approach%20achieves%20a%20consistent%20improvement%20in%0Aboth%20reconstruction%20accuracy%20and%20task%20performance%20when%20applied%20to%20visual%203D%0Anavigation%20tasks.%20We%20go%20on%20to%20show%20that%20considering%20more%20future%20states%20further%0Aimproves%20the%20performance%20of%20the%20agent%20in%20partially-observable%20environments%2C%20but%0Anot%20in%20a%20fully-observable%20one.%20Finally%2C%20we%20demonstrate%20that%20agents%20with%20less%0Atraining%20pre-evaluation%20benefit%20most%20from%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15283v1&entry.124074799=Read"},
{"title": "Immersive Video Compression using Implicit Neural Representations", "author": "Ho Man Kwan and Fan Zhang and Andrew Gower and David Bull", "abstract": "  Recent work on implicit neural representations (INRs) has evidenced their\npotential for efficiently representing and encoding conventional video content.\nIn this paper we, for the first time, extend their application to immersive\n(multi-view) videos, by proposing MV-HiNeRV, a new INR-based immersive video\ncodec. MV-HiNeRV is an enhanced version of a state-of-the-art INR-based video\ncodec, HiNeRV, which was developed for single-view video compression. We have\nmodified the model to learn a different group of feature grids for each view,\nand share the learnt network parameters among all views. This enables the model\nto effectively exploit the spatio-temporal and the inter-view redundancy that\nexists within multi-view videos. The proposed codec was used to compress\nmulti-view texture and depth video sequences in the MPEG Immersive Video (MIV)\nCommon Test Conditions, and tested against the MIV Test model (TMIV) that uses\nthe VVenC video codec. The results demonstrate the superior performance of\nMV-HiNeRV, with significant coding gains (up to 72.33\\%) over TMIV. The\nimplementation of MV-HiNeRV is published for further development and\nevaluation.\n", "link": "http://arxiv.org/abs/2402.01596v2", "date": "2024-02-23", "relevancy": 2.1771, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5564}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5452}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5118}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Immersive%20Video%20Compression%20using%20Implicit%20Neural%20Representations&entry.906535625=Ho%20Man%20Kwan%20and%20Fan%20Zhang%20and%20Andrew%20Gower%20and%20David%20Bull&entry.1292438233=%20%20Recent%20work%20on%20implicit%20neural%20representations%20%28INRs%29%20has%20evidenced%20their%0Apotential%20for%20efficiently%20representing%20and%20encoding%20conventional%20video%20content.%0AIn%20this%20paper%20we%2C%20for%20the%20first%20time%2C%20extend%20their%20application%20to%20immersive%0A%28multi-view%29%20videos%2C%20by%20proposing%20MV-HiNeRV%2C%20a%20new%20INR-based%20immersive%20video%0Acodec.%20MV-HiNeRV%20is%20an%20enhanced%20version%20of%20a%20state-of-the-art%20INR-based%20video%0Acodec%2C%20HiNeRV%2C%20which%20was%20developed%20for%20single-view%20video%20compression.%20We%20have%0Amodified%20the%20model%20to%20learn%20a%20different%20group%20of%20feature%20grids%20for%20each%20view%2C%0Aand%20share%20the%20learnt%20network%20parameters%20among%20all%20views.%20This%20enables%20the%20model%0Ato%20effectively%20exploit%20the%20spatio-temporal%20and%20the%20inter-view%20redundancy%20that%0Aexists%20within%20multi-view%20videos.%20The%20proposed%20codec%20was%20used%20to%20compress%0Amulti-view%20texture%20and%20depth%20video%20sequences%20in%20the%20MPEG%20Immersive%20Video%20%28MIV%29%0ACommon%20Test%20Conditions%2C%20and%20tested%20against%20the%20MIV%20Test%20model%20%28TMIV%29%20that%20uses%0Athe%20VVenC%20video%20codec.%20The%20results%20demonstrate%20the%20superior%20performance%20of%0AMV-HiNeRV%2C%20with%20significant%20coding%20gains%20%28up%20to%2072.33%5C%25%29%20over%20TMIV.%20The%0Aimplementation%20of%20MV-HiNeRV%20is%20published%20for%20further%20development%20and%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01596v2&entry.124074799=Read"},
{"title": "Conformalized-DeepONet: A Distribution-Free Framework for Uncertainty\n  Quantification in Deep Operator Networks", "author": "Christian Moya and Amirhossein Mollaali and Zecheng Zhang and Lu Lu and Guang Lin", "abstract": "  In this paper, we adopt conformal prediction, a distribution-free uncertainty\nquantification (UQ) framework, to obtain confidence prediction intervals with\ncoverage guarantees for Deep Operator Network (DeepONet) regression. Initially,\nwe enhance the uncertainty quantification frameworks (B-DeepONet and\nProb-DeepONet) previously proposed by the authors by using split conformal\nprediction. By combining conformal prediction with our Prob- and B-DeepONets,\nwe effectively quantify uncertainty by generating rigorous confidence intervals\nfor DeepONet prediction. Additionally, we design a novel Quantile-DeepONet that\nallows for a more natural use of split conformal prediction. We refer to this\ndistribution-free effective uncertainty quantification framework as split\nconformal Quantile-DeepONet regression. Finally, we demonstrate the\neffectiveness of the proposed methods using various ordinary, partial\ndifferential equation numerical examples, and multi-fidelity learning.\n", "link": "http://arxiv.org/abs/2402.15406v1", "date": "2024-02-23", "relevancy": 2.1751, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5755}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5555}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5193}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformalized-DeepONet%3A%20A%20Distribution-Free%20Framework%20for%20Uncertainty%0A%20%20Quantification%20in%20Deep%20Operator%20Networks&entry.906535625=Christian%20Moya%20and%20Amirhossein%20Mollaali%20and%20Zecheng%20Zhang%20and%20Lu%20Lu%20and%20Guang%20Lin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20adopt%20conformal%20prediction%2C%20a%20distribution-free%20uncertainty%0Aquantification%20%28UQ%29%20framework%2C%20to%20obtain%20confidence%20prediction%20intervals%20with%0Acoverage%20guarantees%20for%20Deep%20Operator%20Network%20%28DeepONet%29%20regression.%20Initially%2C%0Awe%20enhance%20the%20uncertainty%20quantification%20frameworks%20%28B-DeepONet%20and%0AProb-DeepONet%29%20previously%20proposed%20by%20the%20authors%20by%20using%20split%20conformal%0Aprediction.%20By%20combining%20conformal%20prediction%20with%20our%20Prob-%20and%20B-DeepONets%2C%0Awe%20effectively%20quantify%20uncertainty%20by%20generating%20rigorous%20confidence%20intervals%0Afor%20DeepONet%20prediction.%20Additionally%2C%20we%20design%20a%20novel%20Quantile-DeepONet%20that%0Aallows%20for%20a%20more%20natural%20use%20of%20split%20conformal%20prediction.%20We%20refer%20to%20this%0Adistribution-free%20effective%20uncertainty%20quantification%20framework%20as%20split%0Aconformal%20Quantile-DeepONet%20regression.%20Finally%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20methods%20using%20various%20ordinary%2C%20partial%0Adifferential%20equation%20numerical%20examples%2C%20and%20multi-fidelity%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15406v1&entry.124074799=Read"},
{"title": "Retinotopic Mapping Enhances the Robustness of Convolutional Neural\n  Networks", "author": "Jean-Nicolas J\u00e9r\u00e9mie and Emmanuel Dauc\u00e9 and Laurent U Perrinet", "abstract": "  Foveated vision, a trait shared by many animals, including humans, has not\nbeen fully utilized in machine learning applications, despite its significant\ncontributions to biological visual function. This study investigates whether\nretinotopic mapping, a critical component of foveated vision, can enhance image\ncategorization and localization performance when integrated into deep\nconvolutional neural networks (CNNs). Retinotopic mapping was integrated into\nthe inputs of standard off-the-shelf convolutional neural networks (CNNs),\nwhich were then retrained on the ImageNet task. As expected, the\nlogarithmic-polar mapping improved the network's ability to handle arbitrary\nimage zooms and rotations, particularly for isolated objects. Surprisingly, the\nretinotopically mapped network achieved comparable performance in\nclassification. Furthermore, the network demonstrated improved classification\nlocalization when the foveated center of the transform was shifted. This\nreplicates a crucial ability of the human visual system that is absent in\ntypical convolutional neural networks (CNNs). These findings suggest that\nretinotopic mapping may be fundamental to significant preattentive visual\nprocesses.\n", "link": "http://arxiv.org/abs/2402.15480v1", "date": "2024-02-23", "relevancy": 2.1496, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6004}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4967}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retinotopic%20Mapping%20Enhances%20the%20Robustness%20of%20Convolutional%20Neural%0A%20%20Networks&entry.906535625=Jean-Nicolas%20J%C3%A9r%C3%A9mie%20and%20Emmanuel%20Dauc%C3%A9%20and%20Laurent%20U%20Perrinet&entry.1292438233=%20%20Foveated%20vision%2C%20a%20trait%20shared%20by%20many%20animals%2C%20including%20humans%2C%20has%20not%0Abeen%20fully%20utilized%20in%20machine%20learning%20applications%2C%20despite%20its%20significant%0Acontributions%20to%20biological%20visual%20function.%20This%20study%20investigates%20whether%0Aretinotopic%20mapping%2C%20a%20critical%20component%20of%20foveated%20vision%2C%20can%20enhance%20image%0Acategorization%20and%20localization%20performance%20when%20integrated%20into%20deep%0Aconvolutional%20neural%20networks%20%28CNNs%29.%20Retinotopic%20mapping%20was%20integrated%20into%0Athe%20inputs%20of%20standard%20off-the-shelf%20convolutional%20neural%20networks%20%28CNNs%29%2C%0Awhich%20were%20then%20retrained%20on%20the%20ImageNet%20task.%20As%20expected%2C%20the%0Alogarithmic-polar%20mapping%20improved%20the%20network%27s%20ability%20to%20handle%20arbitrary%0Aimage%20zooms%20and%20rotations%2C%20particularly%20for%20isolated%20objects.%20Surprisingly%2C%20the%0Aretinotopically%20mapped%20network%20achieved%20comparable%20performance%20in%0Aclassification.%20Furthermore%2C%20the%20network%20demonstrated%20improved%20classification%0Alocalization%20when%20the%20foveated%20center%20of%20the%20transform%20was%20shifted.%20This%0Areplicates%20a%20crucial%20ability%20of%20the%20human%20visual%20system%20that%20is%20absent%20in%0Atypical%20convolutional%20neural%20networks%20%28CNNs%29.%20These%20findings%20suggest%20that%0Aretinotopic%20mapping%20may%20be%20fundamental%20to%20significant%20preattentive%20visual%0Aprocesses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15480v1&entry.124074799=Read"},
{"title": "Adaptive Deep Learning for Efficient Visual Pose Estimation aboard\n  Ultra-low-power Nano-drones", "author": "Beatrice Alessandra Motetti and Luca Crupi and Mustafa Omer Mohammed Elamin Elshaigi and Matteo Risso and Daniele Jahier Pagliari and Daniele Palossi and Alessio Burrello", "abstract": "  Sub-10cm diameter nano-drones are gaining momentum thanks to their\napplicability in scenarios prevented to bigger flying drones, such as in narrow\nenvironments and close to humans. However, their tiny form factor also brings\ntheir major drawback: ultra-constrained memory and processors for the onboard\nexecution of their perception pipelines. Therefore, lightweight deep\nlearning-based approaches are becoming increasingly popular, stressing how\ncomputational efficiency and energy-saving are paramount as they can make the\ndifference between a fully working closed-loop system and a failing one. In\nthis work, to maximize the exploitation of the ultra-limited resources aboard\nnano-drones, we present a novel adaptive deep learning-based mechanism for the\nefficient execution of a vision-based human pose estimation task. We leverage\ntwo State-of-the-Art (SoA) convolutional neural networks (CNNs) with different\nregression performance vs. computational costs trade-offs. By combining these\nCNNs with three novel adaptation strategies based on the output's temporal\nconsistency and on auxiliary tasks to swap the CNN being executed proactively,\nwe present six different systems. On a real-world dataset and the actual\nnano-drone hardware, our best-performing system, compared to executing only the\nbigger and most accurate SoA model, shows 28% latency reduction while keeping\nthe same mean absolute error (MAE), 3% MAE reduction while being iso-latency,\nand the absolute peak performance, i.e., 6% better than SoA model.\n", "link": "http://arxiv.org/abs/2401.15236v2", "date": "2024-02-23", "relevancy": 2.1377, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5487}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5119}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Deep%20Learning%20for%20Efficient%20Visual%20Pose%20Estimation%20aboard%0A%20%20Ultra-low-power%20Nano-drones&entry.906535625=Beatrice%20Alessandra%20Motetti%20and%20Luca%20Crupi%20and%20Mustafa%20Omer%20Mohammed%20Elamin%20Elshaigi%20and%20Matteo%20Risso%20and%20Daniele%20Jahier%20Pagliari%20and%20Daniele%20Palossi%20and%20Alessio%20Burrello&entry.1292438233=%20%20Sub-10cm%20diameter%20nano-drones%20are%20gaining%20momentum%20thanks%20to%20their%0Aapplicability%20in%20scenarios%20prevented%20to%20bigger%20flying%20drones%2C%20such%20as%20in%20narrow%0Aenvironments%20and%20close%20to%20humans.%20However%2C%20their%20tiny%20form%20factor%20also%20brings%0Atheir%20major%20drawback%3A%20ultra-constrained%20memory%20and%20processors%20for%20the%20onboard%0Aexecution%20of%20their%20perception%20pipelines.%20Therefore%2C%20lightweight%20deep%0Alearning-based%20approaches%20are%20becoming%20increasingly%20popular%2C%20stressing%20how%0Acomputational%20efficiency%20and%20energy-saving%20are%20paramount%20as%20they%20can%20make%20the%0Adifference%20between%20a%20fully%20working%20closed-loop%20system%20and%20a%20failing%20one.%20In%0Athis%20work%2C%20to%20maximize%20the%20exploitation%20of%20the%20ultra-limited%20resources%20aboard%0Anano-drones%2C%20we%20present%20a%20novel%20adaptive%20deep%20learning-based%20mechanism%20for%20the%0Aefficient%20execution%20of%20a%20vision-based%20human%20pose%20estimation%20task.%20We%20leverage%0Atwo%20State-of-the-Art%20%28SoA%29%20convolutional%20neural%20networks%20%28CNNs%29%20with%20different%0Aregression%20performance%20vs.%20computational%20costs%20trade-offs.%20By%20combining%20these%0ACNNs%20with%20three%20novel%20adaptation%20strategies%20based%20on%20the%20output%27s%20temporal%0Aconsistency%20and%20on%20auxiliary%20tasks%20to%20swap%20the%20CNN%20being%20executed%20proactively%2C%0Awe%20present%20six%20different%20systems.%20On%20a%20real-world%20dataset%20and%20the%20actual%0Anano-drone%20hardware%2C%20our%20best-performing%20system%2C%20compared%20to%20executing%20only%20the%0Abigger%20and%20most%20accurate%20SoA%20model%2C%20shows%2028%25%20latency%20reduction%20while%20keeping%0Athe%20same%20mean%20absolute%20error%20%28MAE%29%2C%203%25%20MAE%20reduction%20while%20being%20iso-latency%2C%0Aand%20the%20absolute%20peak%20performance%2C%20i.e.%2C%206%25%20better%20than%20SoA%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15236v2&entry.124074799=Read"},
{"title": "Hierarchical Invariance for Robust and Interpretable Vision Tasks at\n  Larger Scales", "author": "Shuren Qi and Yushu Zhang and Chao Wang and Zhihua Xia and Jian Weng and Xiaochun Cao", "abstract": "  Developing robust and interpretable vision systems is a crucial step towards\ntrustworthy artificial intelligence. In this regard, a promising paradigm\nconsiders embedding task-required invariant structures, e.g., geometric\ninvariance, in the fundamental image representation. However, such invariant\nrepresentations typically exhibit limited discriminability, limiting their\napplications in larger-scale trustworthy vision tasks. For this open problem,\nwe conduct a systematic investigation of hierarchical invariance, exploring\nthis topic from theoretical, practical, and application perspectives. At the\ntheoretical level, we show how to construct over-complete invariants with a\nConvolutional Neural Networks (CNN)-like hierarchical architecture yet in a\nfully interpretable manner. The general blueprint, specific definitions,\ninvariant properties, and numerical implementations are provided. At the\npractical level, we discuss how to customize this theoretical framework into a\ngiven task. With the over-completeness, discriminative features w.r.t. the task\ncan be adaptively formed in a Neural Architecture Search (NAS)-like manner. We\ndemonstrate the above arguments with accuracy, invariance, and efficiency\nresults on texture, digit, and parasite classification experiments.\nFurthermore, at the application level, our representations are explored in\nreal-world forensics tasks on adversarial perturbations and Artificial\nIntelligence Generated Content (AIGC). Such applications reveal that the\nproposed strategy not only realizes the theoretically promised invariance, but\nalso exhibits competitive discriminability even in the era of deep learning.\nFor robust and interpretable vision tasks at larger scales, hierarchical\ninvariant representation can be considered as an effective alternative to\ntraditional CNN and invariants.\n", "link": "http://arxiv.org/abs/2402.15430v1", "date": "2024-02-23", "relevancy": 2.1374, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5343}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5228}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Invariance%20for%20Robust%20and%20Interpretable%20Vision%20Tasks%20at%0A%20%20Larger%20Scales&entry.906535625=Shuren%20Qi%20and%20Yushu%20Zhang%20and%20Chao%20Wang%20and%20Zhihua%20Xia%20and%20Jian%20Weng%20and%20Xiaochun%20Cao&entry.1292438233=%20%20Developing%20robust%20and%20interpretable%20vision%20systems%20is%20a%20crucial%20step%20towards%0Atrustworthy%20artificial%20intelligence.%20In%20this%20regard%2C%20a%20promising%20paradigm%0Aconsiders%20embedding%20task-required%20invariant%20structures%2C%20e.g.%2C%20geometric%0Ainvariance%2C%20in%20the%20fundamental%20image%20representation.%20However%2C%20such%20invariant%0Arepresentations%20typically%20exhibit%20limited%20discriminability%2C%20limiting%20their%0Aapplications%20in%20larger-scale%20trustworthy%20vision%20tasks.%20For%20this%20open%20problem%2C%0Awe%20conduct%20a%20systematic%20investigation%20of%20hierarchical%20invariance%2C%20exploring%0Athis%20topic%20from%20theoretical%2C%20practical%2C%20and%20application%20perspectives.%20At%20the%0Atheoretical%20level%2C%20we%20show%20how%20to%20construct%20over-complete%20invariants%20with%20a%0AConvolutional%20Neural%20Networks%20%28CNN%29-like%20hierarchical%20architecture%20yet%20in%20a%0Afully%20interpretable%20manner.%20The%20general%20blueprint%2C%20specific%20definitions%2C%0Ainvariant%20properties%2C%20and%20numerical%20implementations%20are%20provided.%20At%20the%0Apractical%20level%2C%20we%20discuss%20how%20to%20customize%20this%20theoretical%20framework%20into%20a%0Agiven%20task.%20With%20the%20over-completeness%2C%20discriminative%20features%20w.r.t.%20the%20task%0Acan%20be%20adaptively%20formed%20in%20a%20Neural%20Architecture%20Search%20%28NAS%29-like%20manner.%20We%0Ademonstrate%20the%20above%20arguments%20with%20accuracy%2C%20invariance%2C%20and%20efficiency%0Aresults%20on%20texture%2C%20digit%2C%20and%20parasite%20classification%20experiments.%0AFurthermore%2C%20at%20the%20application%20level%2C%20our%20representations%20are%20explored%20in%0Areal-world%20forensics%20tasks%20on%20adversarial%20perturbations%20and%20Artificial%0AIntelligence%20Generated%20Content%20%28AIGC%29.%20Such%20applications%20reveal%20that%20the%0Aproposed%20strategy%20not%20only%20realizes%20the%20theoretically%20promised%20invariance%2C%20but%0Aalso%20exhibits%20competitive%20discriminability%20even%20in%20the%20era%20of%20deep%20learning.%0AFor%20robust%20and%20interpretable%20vision%20tasks%20at%20larger%20scales%2C%20hierarchical%0Ainvariant%20representation%20can%20be%20considered%20as%20an%20effective%20alternative%20to%0Atraditional%20CNN%20and%20invariants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15430v1&entry.124074799=Read"},
{"title": "Optimized Deployment of Deep Neural Networks for Visual Pose Estimation\n  on Nano-drones", "author": "Matteo Risso and Francesco Daghero and Beatrice Alessandra Motetti and Daniele Jahier Pagliari and Enrico Macii and Massimo Poncino and Alessio Burrello", "abstract": "  Miniaturized autonomous unmanned aerial vehicles (UAVs) are gaining\npopularity due to their small size, enabling new tasks such as indoor\nnavigation or people monitoring. Nonetheless, their size and simple electronics\npose severe challenges in implementing advanced onboard intelligence. This work\nproposes a new automatic optimization pipeline for visual pose estimation tasks\nusing Deep Neural Networks (DNNs). The pipeline leverages two different Neural\nArchitecture Search (NAS) algorithms to pursue a vast complexity-driven\nexploration in the DNNs' architectural space. The obtained networks are then\ndeployed on an off-the-shelf nano-drone equipped with a parallel ultra-low\npower System-on-Chip leveraging a set of novel software kernels for the\nefficient fused execution of critical DNN layer sequences. Our results improve\nthe state-of-the-art reducing inference latency by up to 3.22x at iso-error.\n", "link": "http://arxiv.org/abs/2402.15273v1", "date": "2024-02-23", "relevancy": 2.1311, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5417}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5348}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5052}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimized%20Deployment%20of%20Deep%20Neural%20Networks%20for%20Visual%20Pose%20Estimation%0A%20%20on%20Nano-drones&entry.906535625=Matteo%20Risso%20and%20Francesco%20Daghero%20and%20Beatrice%20Alessandra%20Motetti%20and%20Daniele%20Jahier%20Pagliari%20and%20Enrico%20Macii%20and%20Massimo%20Poncino%20and%20Alessio%20Burrello&entry.1292438233=%20%20Miniaturized%20autonomous%20unmanned%20aerial%20vehicles%20%28UAVs%29%20are%20gaining%0Apopularity%20due%20to%20their%20small%20size%2C%20enabling%20new%20tasks%20such%20as%20indoor%0Anavigation%20or%20people%20monitoring.%20Nonetheless%2C%20their%20size%20and%20simple%20electronics%0Apose%20severe%20challenges%20in%20implementing%20advanced%20onboard%20intelligence.%20This%20work%0Aproposes%20a%20new%20automatic%20optimization%20pipeline%20for%20visual%20pose%20estimation%20tasks%0Ausing%20Deep%20Neural%20Networks%20%28DNNs%29.%20The%20pipeline%20leverages%20two%20different%20Neural%0AArchitecture%20Search%20%28NAS%29%20algorithms%20to%20pursue%20a%20vast%20complexity-driven%0Aexploration%20in%20the%20DNNs%27%20architectural%20space.%20The%20obtained%20networks%20are%20then%0Adeployed%20on%20an%20off-the-shelf%20nano-drone%20equipped%20with%20a%20parallel%20ultra-low%0Apower%20System-on-Chip%20leveraging%20a%20set%20of%20novel%20software%20kernels%20for%20the%0Aefficient%20fused%20execution%20of%20critical%20DNN%20layer%20sequences.%20Our%20results%20improve%0Athe%20state-of-the-art%20reducing%20inference%20latency%20by%20up%20to%203.22x%20at%20iso-error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15273v1&entry.124074799=Read"},
{"title": "Iteration and Stochastic First-order Oracle Complexities of Stochastic\n  Gradient Descent using Constant and Decaying Learning Rates", "author": "Kento Imaizumi and Hideaki Iiduka", "abstract": "  The performance of stochastic gradient descent (SGD), which is the simplest\nfirst-order optimizer for training deep neural networks, depends on not only\nthe learning rate but also the batch size. They both affect the number of\niterations and the stochastic first-order oracle (SFO) complexity needed for\ntraining. In particular, the previous numerical results indicated that, for SGD\nusing a constant learning rate, the number of iterations needed for training\ndecreases when the batch size increases, and the SFO complexity needed for\ntraining is minimized at a critical batch size and that it increases once the\nbatch size exceeds that size. Here, we study the relationship between batch\nsize and the iteration and SFO complexities needed for nonconvex optimization\nin deep learning with SGD using constant or decaying learning rates and show\nthat SGD using the critical batch size minimizes the SFO complexity. We also\nprovide numerical comparisons of SGD with the existing first-order optimizers\nand show the usefulness of SGD using a critical batch size. Moreover, we show\nthat measured critical batch sizes are close to the sizes estimated from our\ntheoretical results.\n", "link": "http://arxiv.org/abs/2402.15344v1", "date": "2024-02-23", "relevancy": 2.13, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4644}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4146}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.399}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iteration%20and%20Stochastic%20First-order%20Oracle%20Complexities%20of%20Stochastic%0A%20%20Gradient%20Descent%20using%20Constant%20and%20Decaying%20Learning%20Rates&entry.906535625=Kento%20Imaizumi%20and%20Hideaki%20Iiduka&entry.1292438233=%20%20The%20performance%20of%20stochastic%20gradient%20descent%20%28SGD%29%2C%20which%20is%20the%20simplest%0Afirst-order%20optimizer%20for%20training%20deep%20neural%20networks%2C%20depends%20on%20not%20only%0Athe%20learning%20rate%20but%20also%20the%20batch%20size.%20They%20both%20affect%20the%20number%20of%0Aiterations%20and%20the%20stochastic%20first-order%20oracle%20%28SFO%29%20complexity%20needed%20for%0Atraining.%20In%20particular%2C%20the%20previous%20numerical%20results%20indicated%20that%2C%20for%20SGD%0Ausing%20a%20constant%20learning%20rate%2C%20the%20number%20of%20iterations%20needed%20for%20training%0Adecreases%20when%20the%20batch%20size%20increases%2C%20and%20the%20SFO%20complexity%20needed%20for%0Atraining%20is%20minimized%20at%20a%20critical%20batch%20size%20and%20that%20it%20increases%20once%20the%0Abatch%20size%20exceeds%20that%20size.%20Here%2C%20we%20study%20the%20relationship%20between%20batch%0Asize%20and%20the%20iteration%20and%20SFO%20complexities%20needed%20for%20nonconvex%20optimization%0Ain%20deep%20learning%20with%20SGD%20using%20constant%20or%20decaying%20learning%20rates%20and%20show%0Athat%20SGD%20using%20the%20critical%20batch%20size%20minimizes%20the%20SFO%20complexity.%20We%20also%0Aprovide%20numerical%20comparisons%20of%20SGD%20with%20the%20existing%20first-order%20optimizers%0Aand%20show%20the%20usefulness%20of%20SGD%20using%20a%20critical%20batch%20size.%20Moreover%2C%20we%20show%0Athat%20measured%20critical%20batch%20sizes%20are%20close%20to%20the%20sizes%20estimated%20from%20our%0Atheoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15344v1&entry.124074799=Read"},
{"title": "Turning Federated Learning Systems Into Covert Channels", "author": "Gabriele Costa and Fabio Pinelli and Simone Soderi and Gabriele Tolomei", "abstract": "  Federated learning (FL) goes beyond traditional, centralized machine learning\nby distributing model training among a large collection of edge clients. These\nclients cooperatively train a global, e.g., cloud-hosted, model without\ndisclosing their local, private training data. The global model is then shared\namong all the participants which use it for local predictions. In this paper,\nwe put forward a novel attacker model aiming at turning FL systems into covert\nchannels to implement a stealth communication infrastructure. The main\nintuition is that, during federated training, a malicious sender can poison the\nglobal model by submitting purposely crafted examples. Although the effect of\nthe model poisoning is negligible to other participants, and does not alter the\noverall model performance, it can be observed by a malicious receiver and used\nto transmit a single bit.\n", "link": "http://arxiv.org/abs/2104.10561v3", "date": "2024-02-23", "relevancy": 2.1227, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4304}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4298}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4134}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Turning%20Federated%20Learning%20Systems%20Into%20Covert%20Channels&entry.906535625=Gabriele%20Costa%20and%20Fabio%20Pinelli%20and%20Simone%20Soderi%20and%20Gabriele%20Tolomei&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20goes%20beyond%20traditional%2C%20centralized%20machine%20learning%0Aby%20distributing%20model%20training%20among%20a%20large%20collection%20of%20edge%20clients.%20These%0Aclients%20cooperatively%20train%20a%20global%2C%20e.g.%2C%20cloud-hosted%2C%20model%20without%0Adisclosing%20their%20local%2C%20private%20training%20data.%20The%20global%20model%20is%20then%20shared%0Aamong%20all%20the%20participants%20which%20use%20it%20for%20local%20predictions.%20In%20this%20paper%2C%0Awe%20put%20forward%20a%20novel%20attacker%20model%20aiming%20at%20turning%20FL%20systems%20into%20covert%0Achannels%20to%20implement%20a%20stealth%20communication%20infrastructure.%20The%20main%0Aintuition%20is%20that%2C%20during%20federated%20training%2C%20a%20malicious%20sender%20can%20poison%20the%0Aglobal%20model%20by%20submitting%20purposely%20crafted%20examples.%20Although%20the%20effect%20of%0Athe%20model%20poisoning%20is%20negligible%20to%20other%20participants%2C%20and%20does%20not%20alter%20the%0Aoverall%20model%20performance%2C%20it%20can%20be%20observed%20by%20a%20malicious%20receiver%20and%20used%0Ato%20transmit%20a%20single%20bit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2104.10561v3&entry.124074799=Read"},
{"title": "Almost Equivariance via Lie Algebra Convolutions", "author": "Daniel McNeela", "abstract": "  Recently, the equivariance of models with respect to a group action has\nbecome an important topic of research in machine learning. Analysis of the\nbuilt-in equivariance of existing neural network architectures, as well as the\nstudy of building models that explicitly \"bake in\" equivariance, have become\nsignificant research areas in their own right. However, imbuing an architecture\nwith a specific group equivariance imposes a strong prior on the types of data\ntransformations that the model expects to see. While strictly-equivariant\nmodels enforce symmetries, real-world data does not always conform to such\nstrict equivariances. In such cases, the prior of strict equivariance can\nactually prove too strong and cause models to underperform. Therefore, in this\nwork we study a closely related topic, that of almost equivariance. We provide\na definition of almost equivariance and give a practical method for encoding\nalmost equivariance in models by appealing to the Lie algebra of a Lie group.\nSpecifically, we define Lie algebra convolutions and demonstrate that they\noffer several benefits over Lie group convolutions, including being\nwell-defined for non-compact Lie groups having non-surjective exponential map.\nFrom there, we demonstrate connections between the notions of equivariance and\nisometry and those of almost equivariance and almost isometry. We prove two\nexistence theorems, one showing the existence of almost isometries within\nbounded distance of isometries of a manifold, and another showing the converse\nfor Hilbert spaces. We extend these theorems to prove the existence of almost\nequivariant manifold embeddings within bounded distance of fully equivariant\nembedding functions, subject to certain constraints on the group action and the\nfunction class. Finally, we demonstrate the validity of our approach by\nbenchmarking against datasets in fully equivariant and almost equivariant\nsettings.\n", "link": "http://arxiv.org/abs/2310.13164v5", "date": "2024-02-23", "relevancy": 2.1195, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4368}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.418}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4169}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Almost%20Equivariance%20via%20Lie%20Algebra%20Convolutions&entry.906535625=Daniel%20McNeela&entry.1292438233=%20%20Recently%2C%20the%20equivariance%20of%20models%20with%20respect%20to%20a%20group%20action%20has%0Abecome%20an%20important%20topic%20of%20research%20in%20machine%20learning.%20Analysis%20of%20the%0Abuilt-in%20equivariance%20of%20existing%20neural%20network%20architectures%2C%20as%20well%20as%20the%0Astudy%20of%20building%20models%20that%20explicitly%20%22bake%20in%22%20equivariance%2C%20have%20become%0Asignificant%20research%20areas%20in%20their%20own%20right.%20However%2C%20imbuing%20an%20architecture%0Awith%20a%20specific%20group%20equivariance%20imposes%20a%20strong%20prior%20on%20the%20types%20of%20data%0Atransformations%20that%20the%20model%20expects%20to%20see.%20While%20strictly-equivariant%0Amodels%20enforce%20symmetries%2C%20real-world%20data%20does%20not%20always%20conform%20to%20such%0Astrict%20equivariances.%20In%20such%20cases%2C%20the%20prior%20of%20strict%20equivariance%20can%0Aactually%20prove%20too%20strong%20and%20cause%20models%20to%20underperform.%20Therefore%2C%20in%20this%0Awork%20we%20study%20a%20closely%20related%20topic%2C%20that%20of%20almost%20equivariance.%20We%20provide%0Aa%20definition%20of%20almost%20equivariance%20and%20give%20a%20practical%20method%20for%20encoding%0Aalmost%20equivariance%20in%20models%20by%20appealing%20to%20the%20Lie%20algebra%20of%20a%20Lie%20group.%0ASpecifically%2C%20we%20define%20Lie%20algebra%20convolutions%20and%20demonstrate%20that%20they%0Aoffer%20several%20benefits%20over%20Lie%20group%20convolutions%2C%20including%20being%0Awell-defined%20for%20non-compact%20Lie%20groups%20having%20non-surjective%20exponential%20map.%0AFrom%20there%2C%20we%20demonstrate%20connections%20between%20the%20notions%20of%20equivariance%20and%0Aisometry%20and%20those%20of%20almost%20equivariance%20and%20almost%20isometry.%20We%20prove%20two%0Aexistence%20theorems%2C%20one%20showing%20the%20existence%20of%20almost%20isometries%20within%0Abounded%20distance%20of%20isometries%20of%20a%20manifold%2C%20and%20another%20showing%20the%20converse%0Afor%20Hilbert%20spaces.%20We%20extend%20these%20theorems%20to%20prove%20the%20existence%20of%20almost%0Aequivariant%20manifold%20embeddings%20within%20bounded%20distance%20of%20fully%20equivariant%0Aembedding%20functions%2C%20subject%20to%20certain%20constraints%20on%20the%20group%20action%20and%20the%0Afunction%20class.%20Finally%2C%20we%20demonstrate%20the%20validity%20of%20our%20approach%20by%0Abenchmarking%20against%20datasets%20in%20fully%20equivariant%20and%20almost%20equivariant%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13164v5&entry.124074799=Read"},
{"title": "Homeostatic motion planning with innate physics knowledge", "author": "Giulia Lafratta and Bernd Porr and Christopher Chandler and Alice Miller", "abstract": "  Living organisms interact with their surroundings in a closed-loop fashion,\nwhere sensory inputs dictate the initiation and termination of behaviours. Even\nsimple animals are able to develop and execute complex plans, which has not yet\nbeen replicated in robotics using pure closed-loop input control. We propose a\nsolution to this problem by defining a set of discrete and temporary\nclosed-loop controllers, called \"tasks\", each representing a closed-loop\nbehaviour. We further introduce a supervisory module which has an innate\nunderstanding of physics and causality, through which it can simulate the\nexecution of task sequences over time and store the results in a model of the\nenvironment. On the basis of this model, plans can be made by chaining\ntemporary closed-loop controllers. The proposed framework was implemented for a\nreal robot and tested in two scenarios as proof of concept.\n", "link": "http://arxiv.org/abs/2402.15384v1", "date": "2024-02-23", "relevancy": 2.118, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5823}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5496}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4883}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Homeostatic%20motion%20planning%20with%20innate%20physics%20knowledge&entry.906535625=Giulia%20Lafratta%20and%20Bernd%20Porr%20and%20Christopher%20Chandler%20and%20Alice%20Miller&entry.1292438233=%20%20Living%20organisms%20interact%20with%20their%20surroundings%20in%20a%20closed-loop%20fashion%2C%0Awhere%20sensory%20inputs%20dictate%20the%20initiation%20and%20termination%20of%20behaviours.%20Even%0Asimple%20animals%20are%20able%20to%20develop%20and%20execute%20complex%20plans%2C%20which%20has%20not%20yet%0Abeen%20replicated%20in%20robotics%20using%20pure%20closed-loop%20input%20control.%20We%20propose%20a%0Asolution%20to%20this%20problem%20by%20defining%20a%20set%20of%20discrete%20and%20temporary%0Aclosed-loop%20controllers%2C%20called%20%22tasks%22%2C%20each%20representing%20a%20closed-loop%0Abehaviour.%20We%20further%20introduce%20a%20supervisory%20module%20which%20has%20an%20innate%0Aunderstanding%20of%20physics%20and%20causality%2C%20through%20which%20it%20can%20simulate%20the%0Aexecution%20of%20task%20sequences%20over%20time%20and%20store%20the%20results%20in%20a%20model%20of%20the%0Aenvironment.%20On%20the%20basis%20of%20this%20model%2C%20plans%20can%20be%20made%20by%20chaining%0Atemporary%20closed-loop%20controllers.%20The%20proposed%20framework%20was%20implemented%20for%20a%0Areal%20robot%20and%20tested%20in%20two%20scenarios%20as%20proof%20of%20concept.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15384v1&entry.124074799=Read"},
{"title": "SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal\n  Instance Segmentation of Cluttered Tabletop Scenes", "author": "Zhili Ng and Haozhe Wang and Zhengshen Zhang and Francis Tay Eng Hock and Marcelo H. Ang Jr", "abstract": "  In this work, we present SynTable, a unified and flexible Python-based\ndataset generator built using NVIDIA's Isaac Sim Replicator Composer for\ngenerating high-quality synthetic datasets for unseen object amodal instance\nsegmentation of cluttered tabletop scenes. Our dataset generation tool can\nrender a complex 3D scene containing object meshes, materials, textures,\nlighting, and backgrounds. Metadata, such as modal and amodal instance\nsegmentation masks, occlusion masks, depth maps, bounding boxes, and material\nproperties, can be generated to automatically annotate the scene according to\nthe users' requirements. Our tool eliminates the need for manual labeling in\nthe dataset generation process while ensuring the quality and accuracy of the\ndataset. In this work, we discuss our design goals, framework architecture, and\nthe performance of our tool. We demonstrate the use of a sample dataset\ngenerated using SynTable by ray tracing for training a state-of-the-art model,\nUOAIS-Net. The results show significantly improved performance in Sim-to-Real\ntransfer when evaluated on the OSD-Amodal dataset. We offer this tool as an\nopen-source, easy-to-use, photorealistic dataset generator for advancing\nresearch in deep learning and synthetic data generation.\n", "link": "http://arxiv.org/abs/2307.07333v2", "date": "2024-02-23", "relevancy": 2.1085, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5211}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5207}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynTable%3A%20A%20Synthetic%20Data%20Generation%20Pipeline%20for%20Unseen%20Object%20Amodal%0A%20%20Instance%20Segmentation%20of%20Cluttered%20Tabletop%20Scenes&entry.906535625=Zhili%20Ng%20and%20Haozhe%20Wang%20and%20Zhengshen%20Zhang%20and%20Francis%20Tay%20Eng%20Hock%20and%20Marcelo%20H.%20Ang%20Jr&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20SynTable%2C%20a%20unified%20and%20flexible%20Python-based%0Adataset%20generator%20built%20using%20NVIDIA%27s%20Isaac%20Sim%20Replicator%20Composer%20for%0Agenerating%20high-quality%20synthetic%20datasets%20for%20unseen%20object%20amodal%20instance%0Asegmentation%20of%20cluttered%20tabletop%20scenes.%20Our%20dataset%20generation%20tool%20can%0Arender%20a%20complex%203D%20scene%20containing%20object%20meshes%2C%20materials%2C%20textures%2C%0Alighting%2C%20and%20backgrounds.%20Metadata%2C%20such%20as%20modal%20and%20amodal%20instance%0Asegmentation%20masks%2C%20occlusion%20masks%2C%20depth%20maps%2C%20bounding%20boxes%2C%20and%20material%0Aproperties%2C%20can%20be%20generated%20to%20automatically%20annotate%20the%20scene%20according%20to%0Athe%20users%27%20requirements.%20Our%20tool%20eliminates%20the%20need%20for%20manual%20labeling%20in%0Athe%20dataset%20generation%20process%20while%20ensuring%20the%20quality%20and%20accuracy%20of%20the%0Adataset.%20In%20this%20work%2C%20we%20discuss%20our%20design%20goals%2C%20framework%20architecture%2C%20and%0Athe%20performance%20of%20our%20tool.%20We%20demonstrate%20the%20use%20of%20a%20sample%20dataset%0Agenerated%20using%20SynTable%20by%20ray%20tracing%20for%20training%20a%20state-of-the-art%20model%2C%0AUOAIS-Net.%20The%20results%20show%20significantly%20improved%20performance%20in%20Sim-to-Real%0Atransfer%20when%20evaluated%20on%20the%20OSD-Amodal%20dataset.%20We%20offer%20this%20tool%20as%20an%0Aopen-source%2C%20easy-to-use%2C%20photorealistic%20dataset%20generator%20for%20advancing%0Aresearch%20in%20deep%20learning%20and%20synthetic%20data%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.07333v2&entry.124074799=Read"},
{"title": "Learning a Stable Dynamic System with a Lyapunov Energy Function for\n  Demonstratives Using Neural Networks", "author": "Yu Zhang and Yongxiang Zou and Haoyu Zhang and Xiuze Xia and Long Cheng", "abstract": "  Autonomous Dynamic System (DS)-based algorithms hold a pivotal and\nfoundational role in the field of Learning from Demonstration (LfD).\nNevertheless, they confront the formidable challenge of striking a delicate\nbalance between achieving precision in learning and ensuring the overall\nstability of the system. In response to this substantial challenge, this paper\nintroduces a novel DS algorithm rooted in neural network technology. This\nalgorithm not only possesses the capability to extract critical insights from\ndemonstration data but also demonstrates the capacity to learn a candidate\nLyapunov energy function that is consistent with the provided data. The model\npresented in this paper employs a straightforward neural network architecture\nthat excels in fulfilling a dual objective: optimizing accuracy while\nsimultaneously preserving global stability. To comprehensively evaluate the\neffectiveness of the proposed algorithm, rigorous assessments are conducted\nusing the LASA dataset, further reinforced by empirical validation through a\nrobotic experiment.\n", "link": "http://arxiv.org/abs/2309.08849v2", "date": "2024-02-23", "relevancy": 2.1033, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5485}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5084}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20a%20Stable%20Dynamic%20System%20with%20a%20Lyapunov%20Energy%20Function%20for%0A%20%20Demonstratives%20Using%20Neural%20Networks&entry.906535625=Yu%20Zhang%20and%20Yongxiang%20Zou%20and%20Haoyu%20Zhang%20and%20Xiuze%20Xia%20and%20Long%20Cheng&entry.1292438233=%20%20Autonomous%20Dynamic%20System%20%28DS%29-based%20algorithms%20hold%20a%20pivotal%20and%0Afoundational%20role%20in%20the%20field%20of%20Learning%20from%20Demonstration%20%28LfD%29.%0ANevertheless%2C%20they%20confront%20the%20formidable%20challenge%20of%20striking%20a%20delicate%0Abalance%20between%20achieving%20precision%20in%20learning%20and%20ensuring%20the%20overall%0Astability%20of%20the%20system.%20In%20response%20to%20this%20substantial%20challenge%2C%20this%20paper%0Aintroduces%20a%20novel%20DS%20algorithm%20rooted%20in%20neural%20network%20technology.%20This%0Aalgorithm%20not%20only%20possesses%20the%20capability%20to%20extract%20critical%20insights%20from%0Ademonstration%20data%20but%20also%20demonstrates%20the%20capacity%20to%20learn%20a%20candidate%0ALyapunov%20energy%20function%20that%20is%20consistent%20with%20the%20provided%20data.%20The%20model%0Apresented%20in%20this%20paper%20employs%20a%20straightforward%20neural%20network%20architecture%0Athat%20excels%20in%20fulfilling%20a%20dual%20objective%3A%20optimizing%20accuracy%20while%0Asimultaneously%20preserving%20global%20stability.%20To%20comprehensively%20evaluate%20the%0Aeffectiveness%20of%20the%20proposed%20algorithm%2C%20rigorous%20assessments%20are%20conducted%0Ausing%20the%20LASA%20dataset%2C%20further%20reinforced%20by%20empirical%20validation%20through%20a%0Arobotic%20experiment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08849v2&entry.124074799=Read"},
{"title": "Safety-Conscious Pushing on Diverse Oriented Surfaces with Underactuated\n  Aerial Vehicles", "author": "Tong Hui and Manuel J. Fernandez Gonzalez and Matteo Fumagalli", "abstract": "  Pushing tasks performed by aerial manipulators can be used for contact-based\nindustrial inspections. Underactuated aerial vehicles are widely employed in\naerial manipulation due to their widespread availability and relatively low\ncost. Industrial infrastructures often consist of diverse oriented work\nsurfaces. When interacting with such surfaces, the coupled gravity compensation\nand interaction force generation of underactuated aerial vehicles can present\nthe potential challenge of near-saturation operations. The blind utilization of\nthese platforms for such tasks can lead to instability and accidents, creating\nunsafe operating conditions and potentially damaging the platform. In order to\nensure safe pushing on these surfaces while managing platform saturation, this\nwork establishes a safety assessment process. This process involves the\nprediction of the saturation level of each actuator during pushing across\nvariable surface orientations. Furthermore, the assessment results are used to\nplan and execute physical experiments, ensuring safe operations and preventing\nplatform damage.\n", "link": "http://arxiv.org/abs/2402.15243v1", "date": "2024-02-23", "relevancy": 2.1033, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5514}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5042}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety-Conscious%20Pushing%20on%20Diverse%20Oriented%20Surfaces%20with%20Underactuated%0A%20%20Aerial%20Vehicles&entry.906535625=Tong%20Hui%20and%20Manuel%20J.%20Fernandez%20Gonzalez%20and%20Matteo%20Fumagalli&entry.1292438233=%20%20Pushing%20tasks%20performed%20by%20aerial%20manipulators%20can%20be%20used%20for%20contact-based%0Aindustrial%20inspections.%20Underactuated%20aerial%20vehicles%20are%20widely%20employed%20in%0Aaerial%20manipulation%20due%20to%20their%20widespread%20availability%20and%20relatively%20low%0Acost.%20Industrial%20infrastructures%20often%20consist%20of%20diverse%20oriented%20work%0Asurfaces.%20When%20interacting%20with%20such%20surfaces%2C%20the%20coupled%20gravity%20compensation%0Aand%20interaction%20force%20generation%20of%20underactuated%20aerial%20vehicles%20can%20present%0Athe%20potential%20challenge%20of%20near-saturation%20operations.%20The%20blind%20utilization%20of%0Athese%20platforms%20for%20such%20tasks%20can%20lead%20to%20instability%20and%20accidents%2C%20creating%0Aunsafe%20operating%20conditions%20and%20potentially%20damaging%20the%20platform.%20In%20order%20to%0Aensure%20safe%20pushing%20on%20these%20surfaces%20while%20managing%20platform%20saturation%2C%20this%0Awork%20establishes%20a%20safety%20assessment%20process.%20This%20process%20involves%20the%0Aprediction%20of%20the%20saturation%20level%20of%20each%20actuator%20during%20pushing%20across%0Avariable%20surface%20orientations.%20Furthermore%2C%20the%20assessment%20results%20are%20used%20to%0Aplan%20and%20execute%20physical%20experiments%2C%20ensuring%20safe%20operations%20and%20preventing%0Aplatform%20damage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15243v1&entry.124074799=Read"},
{"title": "Unsupervised Domain Adaptation for Brain Vessel Segmentation through\n  Transwarp Contrastive Learning", "author": "Fengming Lin and Yan Xia and Michael MacRaild and Yash Deo and Haoran Dou and Qiongyao Liu and Kun Wu and Nishant Ravikumar and Alejandro F. Frangi", "abstract": "  Unsupervised domain adaptation (UDA) aims to align the labelled source\ndistribution with the unlabelled target distribution to obtain domain-invariant\npredictive models. Since cross-modality medical data exhibit significant intra\nand inter-domain shifts and most are unlabelled, UDA is more important while\nchallenging in medical image analysis. This paper proposes a simple yet potent\ncontrastive learning framework for UDA to narrow the inter-domain gap between\nlabelled source and unlabelled target distribution. Our method is validated on\ncerebral vessel datasets. Experimental results show that our approach can learn\nlatent features from labelled 3DRA modality data and improve vessel\nsegmentation performance in unlabelled MRA modality data.\n", "link": "http://arxiv.org/abs/2402.15237v1", "date": "2024-02-23", "relevancy": 2.1029, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5468}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5259}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5046}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Domain%20Adaptation%20for%20Brain%20Vessel%20Segmentation%20through%0A%20%20Transwarp%20Contrastive%20Learning&entry.906535625=Fengming%20Lin%20and%20Yan%20Xia%20and%20Michael%20MacRaild%20and%20Yash%20Deo%20and%20Haoran%20Dou%20and%20Qiongyao%20Liu%20and%20Kun%20Wu%20and%20Nishant%20Ravikumar%20and%20Alejandro%20F.%20Frangi&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20aims%20to%20align%20the%20labelled%20source%0Adistribution%20with%20the%20unlabelled%20target%20distribution%20to%20obtain%20domain-invariant%0Apredictive%20models.%20Since%20cross-modality%20medical%20data%20exhibit%20significant%20intra%0Aand%20inter-domain%20shifts%20and%20most%20are%20unlabelled%2C%20UDA%20is%20more%20important%20while%0Achallenging%20in%20medical%20image%20analysis.%20This%20paper%20proposes%20a%20simple%20yet%20potent%0Acontrastive%20learning%20framework%20for%20UDA%20to%20narrow%20the%20inter-domain%20gap%20between%0Alabelled%20source%20and%20unlabelled%20target%20distribution.%20Our%20method%20is%20validated%20on%0Acerebral%20vessel%20datasets.%20Experimental%20results%20show%20that%20our%20approach%20can%20learn%0Alatent%20features%20from%20labelled%203DRA%20modality%20data%20and%20improve%20vessel%0Asegmentation%20performance%20in%20unlabelled%20MRA%20modality%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15237v1&entry.124074799=Read"},
{"title": "End-to-end Supervised Prediction of Arbitrary-size Graphs with\n  Partially-Masked Fused Gromov-Wasserstein Matching", "author": "Paul Krzakala and Junjie Yang and R\u00e9mi Flamary and Florence d'Alch\u00e9-Buc and Charlotte Laclau and Matthieu Labeau", "abstract": "  We present a novel end-to-end deep learning-based approach for Supervised\nGraph Prediction (SGP). We introduce an original Optimal Transport (OT)-based\nloss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows\nto directly leverage graph representations such as adjacency and feature\nmatrices. PM-FGW exhibits all the desirable properties for SGP: it is node\npermutation invariant, sub-differentiable and handles graphs of different sizes\nby comparing their padded representations as well as their masking vectors.\nMoreover, we present a flexible transformer-based architecture that easily\nadapts to different types of input data. In the experimental section, three\ndifferent tasks, a novel and challenging synthetic dataset (image2graph) and\ntwo real-world tasks, image2map and fingerprint2molecule - showcase the\nefficiency and versatility of the approach compared to competitors.\n", "link": "http://arxiv.org/abs/2402.12269v2", "date": "2024-02-23", "relevancy": 2.0834, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5334}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5063}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-end%20Supervised%20Prediction%20of%20Arbitrary-size%20Graphs%20with%0A%20%20Partially-Masked%20Fused%20Gromov-Wasserstein%20Matching&entry.906535625=Paul%20Krzakala%20and%20Junjie%20Yang%20and%20R%C3%A9mi%20Flamary%20and%20Florence%20d%27Alch%C3%A9-Buc%20and%20Charlotte%20Laclau%20and%20Matthieu%20Labeau&entry.1292438233=%20%20We%20present%20a%20novel%20end-to-end%20deep%20learning-based%20approach%20for%20Supervised%0AGraph%20Prediction%20%28SGP%29.%20We%20introduce%20an%20original%20Optimal%20Transport%20%28OT%29-based%0Aloss%2C%20the%20Partially-Masked%20Fused%20Gromov-Wasserstein%20loss%20%28PM-FGW%29%2C%20that%20allows%0Ato%20directly%20leverage%20graph%20representations%20such%20as%20adjacency%20and%20feature%0Amatrices.%20PM-FGW%20exhibits%20all%20the%20desirable%20properties%20for%20SGP%3A%20it%20is%20node%0Apermutation%20invariant%2C%20sub-differentiable%20and%20handles%20graphs%20of%20different%20sizes%0Aby%20comparing%20their%20padded%20representations%20as%20well%20as%20their%20masking%20vectors.%0AMoreover%2C%20we%20present%20a%20flexible%20transformer-based%20architecture%20that%20easily%0Aadapts%20to%20different%20types%20of%20input%20data.%20In%20the%20experimental%20section%2C%20three%0Adifferent%20tasks%2C%20a%20novel%20and%20challenging%20synthetic%20dataset%20%28image2graph%29%20and%0Atwo%20real-world%20tasks%2C%20image2map%20and%20fingerprint2molecule%20-%20showcase%20the%0Aefficiency%20and%20versatility%20of%20the%20approach%20compared%20to%20competitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12269v2&entry.124074799=Read"},
{"title": "AccessLens: Auto-detecting Inaccessibility of Everyday Objects", "author": "Nahyun Kwon and Qian Lu and Muhammad Hasham Qazi and Joanne Liu and Changhoon Oh and Shu Kong and Jeeeun Kim", "abstract": "  In our increasingly diverse society, everyday physical interfaces often\npresent barriers, impacting individuals across various contexts. This\noversight, from small cabinet knobs to identical wall switches that can pose\ndifferent contextual challenges, highlights an imperative need for solutions.\nLeveraging low-cost 3D-printed augmentations such as knob magnifiers and\ntactile labels seems promising, yet the process of discovering unrecognized\nbarriers remains challenging because disability is context-dependent. We\nintroduce AccessLens, an end-to-end system designed to identify inaccessible\ninterfaces in daily objects, and recommend 3D-printable augmentations for\naccessibility enhancement. Our approach involves training a detector using the\nnovel AccessDB dataset designed to automatically recognize 21 distinct\nInaccessibility Classes (e.g., bar-small and round-rotate) within 6 common\nobject categories (e.g., handle and knob). AccessMeta serves as a robust way to\nbuild a comprehensive dictionary linking these accessibility classes to\nopen-source 3D augmentation designs. Experiments demonstrate our detector's\nperformance in detecting inaccessible objects.\n", "link": "http://arxiv.org/abs/2401.15996v2", "date": "2024-02-23", "relevancy": 2.0785, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5456}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5116}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4968}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AccessLens%3A%20Auto-detecting%20Inaccessibility%20of%20Everyday%20Objects&entry.906535625=Nahyun%20Kwon%20and%20Qian%20Lu%20and%20Muhammad%20Hasham%20Qazi%20and%20Joanne%20Liu%20and%20Changhoon%20Oh%20and%20Shu%20Kong%20and%20Jeeeun%20Kim&entry.1292438233=%20%20In%20our%20increasingly%20diverse%20society%2C%20everyday%20physical%20interfaces%20often%0Apresent%20barriers%2C%20impacting%20individuals%20across%20various%20contexts.%20This%0Aoversight%2C%20from%20small%20cabinet%20knobs%20to%20identical%20wall%20switches%20that%20can%20pose%0Adifferent%20contextual%20challenges%2C%20highlights%20an%20imperative%20need%20for%20solutions.%0ALeveraging%20low-cost%203D-printed%20augmentations%20such%20as%20knob%20magnifiers%20and%0Atactile%20labels%20seems%20promising%2C%20yet%20the%20process%20of%20discovering%20unrecognized%0Abarriers%20remains%20challenging%20because%20disability%20is%20context-dependent.%20We%0Aintroduce%20AccessLens%2C%20an%20end-to-end%20system%20designed%20to%20identify%20inaccessible%0Ainterfaces%20in%20daily%20objects%2C%20and%20recommend%203D-printable%20augmentations%20for%0Aaccessibility%20enhancement.%20Our%20approach%20involves%20training%20a%20detector%20using%20the%0Anovel%20AccessDB%20dataset%20designed%20to%20automatically%20recognize%2021%20distinct%0AInaccessibility%20Classes%20%28e.g.%2C%20bar-small%20and%20round-rotate%29%20within%206%20common%0Aobject%20categories%20%28e.g.%2C%20handle%20and%20knob%29.%20AccessMeta%20serves%20as%20a%20robust%20way%20to%0Abuild%20a%20comprehensive%20dictionary%20linking%20these%20accessibility%20classes%20to%0Aopen-source%203D%20augmentation%20designs.%20Experiments%20demonstrate%20our%20detector%27s%0Aperformance%20in%20detecting%20inaccessible%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15996v2&entry.124074799=Read"},
{"title": "Outlier detection by ensembling uncertainty with negative objectness", "author": "Anja Deli\u0107 and Matej Grci\u0107 and Sini\u0161a \u0160egvi\u0107", "abstract": "  Outlier detection is an essential capability in safety-critical applications\nof supervised visual recognition. Most of the existing methods deliver best\nresults by encouraging standard closed-set models to produce low-confidence\npredictions in negative training data. However, that approach conflates\nprediction uncertainty with recognition of the negative class. We therefore\nreconsider direct prediction of K+1 logits that correspond to K groundtruth\nclasses and one outlier class. This setup allows us to formulate a novel\nanomaly score as an ensemble of in-distribution uncertainty and the posterior\nof the outlier class which we term negative objectness. Now outliers can be\nindependently detected due to i) high prediction uncertainty or ii) similarity\nwith negative data. We embed our method into a dense prediction architecture\nwith mask-level recognition over K+2 classes. The training procedure encourages\nthe novel K+2-th class to learn negative objectness at pasted negative\ninstances. Our models outperform the current state-of-the art on standard\nbenchmarks for image-wide and pixel-level outlier detection with and without\ntraining on real negative data.\n", "link": "http://arxiv.org/abs/2402.15374v1", "date": "2024-02-23", "relevancy": 2.0716, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5466}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5351}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4892}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Outlier%20detection%20by%20ensembling%20uncertainty%20with%20negative%20objectness&entry.906535625=Anja%20Deli%C4%87%20and%20Matej%20Grci%C4%87%20and%20Sini%C5%A1a%20%C5%A0egvi%C4%87&entry.1292438233=%20%20Outlier%20detection%20is%20an%20essential%20capability%20in%20safety-critical%20applications%0Aof%20supervised%20visual%20recognition.%20Most%20of%20the%20existing%20methods%20deliver%20best%0Aresults%20by%20encouraging%20standard%20closed-set%20models%20to%20produce%20low-confidence%0Apredictions%20in%20negative%20training%20data.%20However%2C%20that%20approach%20conflates%0Aprediction%20uncertainty%20with%20recognition%20of%20the%20negative%20class.%20We%20therefore%0Areconsider%20direct%20prediction%20of%20K%2B1%20logits%20that%20correspond%20to%20K%20groundtruth%0Aclasses%20and%20one%20outlier%20class.%20This%20setup%20allows%20us%20to%20formulate%20a%20novel%0Aanomaly%20score%20as%20an%20ensemble%20of%20in-distribution%20uncertainty%20and%20the%20posterior%0Aof%20the%20outlier%20class%20which%20we%20term%20negative%20objectness.%20Now%20outliers%20can%20be%0Aindependently%20detected%20due%20to%20i%29%20high%20prediction%20uncertainty%20or%20ii%29%20similarity%0Awith%20negative%20data.%20We%20embed%20our%20method%20into%20a%20dense%20prediction%20architecture%0Awith%20mask-level%20recognition%20over%20K%2B2%20classes.%20The%20training%20procedure%20encourages%0Athe%20novel%20K%2B2-th%20class%20to%20learn%20negative%20objectness%20at%20pasted%20negative%0Ainstances.%20Our%20models%20outperform%20the%20current%20state-of-the%20art%20on%20standard%0Abenchmarks%20for%20image-wide%20and%20pixel-level%20outlier%20detection%20with%20and%20without%0Atraining%20on%20real%20negative%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15374v1&entry.124074799=Read"},
{"title": "Contact Energy Based Hindsight Experience Prioritization", "author": "Erdi Sayar and Zhenshan Bing and Carlo D'Eramo and Ozgur S. Oguz and Alois Knoll", "abstract": "  Multi-goal robot manipulation tasks with sparse rewards are difficult for\nreinforcement learning (RL) algorithms due to the inefficiency in collecting\nsuccessful experiences. Recent algorithms such as Hindsight Experience Replay\n(HER) expedite learning by taking advantage of failed trajectories and\nreplacing the desired goal with one of the achieved states so that any failed\ntrajectory can be utilized as a contribution to learning. However, HER\nuniformly chooses failed trajectories, without taking into account which ones\nmight be the most valuable for learning. In this paper, we address this problem\nand propose a novel approach Contact Energy Based Prioritization~(CEBP) to\nselect the samples from the replay buffer based on rich information due to\ncontact, leveraging the touch sensors in the gripper of the robot and object\ndisplacement. Our prioritization scheme favors sampling of contact-rich\nexperiences, which are arguably the ones providing the largest amount of\ninformation. We evaluate our proposed approach on various sparse reward robotic\ntasks and compare them with the state-of-the-art methods. We show that our\nmethod surpasses or performs on par with those methods on robot manipulation\ntasks. Finally, we deploy the trained policy from our method to a real Franka\nrobot for a pick-and-place task. We observe that the robot can solve the task\nsuccessfully. The videos and code are publicly available at:\nhttps://erdiphd.github.io/HER_force\n", "link": "http://arxiv.org/abs/2312.02677v2", "date": "2024-02-23", "relevancy": 2.0715, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5426}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5217}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5042}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contact%20Energy%20Based%20Hindsight%20Experience%20Prioritization&entry.906535625=Erdi%20Sayar%20and%20Zhenshan%20Bing%20and%20Carlo%20D%27Eramo%20and%20Ozgur%20S.%20Oguz%20and%20Alois%20Knoll&entry.1292438233=%20%20Multi-goal%20robot%20manipulation%20tasks%20with%20sparse%20rewards%20are%20difficult%20for%0Areinforcement%20learning%20%28RL%29%20algorithms%20due%20to%20the%20inefficiency%20in%20collecting%0Asuccessful%20experiences.%20Recent%20algorithms%20such%20as%20Hindsight%20Experience%20Replay%0A%28HER%29%20expedite%20learning%20by%20taking%20advantage%20of%20failed%20trajectories%20and%0Areplacing%20the%20desired%20goal%20with%20one%20of%20the%20achieved%20states%20so%20that%20any%20failed%0Atrajectory%20can%20be%20utilized%20as%20a%20contribution%20to%20learning.%20However%2C%20HER%0Auniformly%20chooses%20failed%20trajectories%2C%20without%20taking%20into%20account%20which%20ones%0Amight%20be%20the%20most%20valuable%20for%20learning.%20In%20this%20paper%2C%20we%20address%20this%20problem%0Aand%20propose%20a%20novel%20approach%20Contact%20Energy%20Based%20Prioritization~%28CEBP%29%20to%0Aselect%20the%20samples%20from%20the%20replay%20buffer%20based%20on%20rich%20information%20due%20to%0Acontact%2C%20leveraging%20the%20touch%20sensors%20in%20the%20gripper%20of%20the%20robot%20and%20object%0Adisplacement.%20Our%20prioritization%20scheme%20favors%20sampling%20of%20contact-rich%0Aexperiences%2C%20which%20are%20arguably%20the%20ones%20providing%20the%20largest%20amount%20of%0Ainformation.%20We%20evaluate%20our%20proposed%20approach%20on%20various%20sparse%20reward%20robotic%0Atasks%20and%20compare%20them%20with%20the%20state-of-the-art%20methods.%20We%20show%20that%20our%0Amethod%20surpasses%20or%20performs%20on%20par%20with%20those%20methods%20on%20robot%20manipulation%0Atasks.%20Finally%2C%20we%20deploy%20the%20trained%20policy%20from%20our%20method%20to%20a%20real%20Franka%0Arobot%20for%20a%20pick-and-place%20task.%20We%20observe%20that%20the%20robot%20can%20solve%20the%20task%0Asuccessfully.%20The%20videos%20and%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//erdiphd.github.io/HER_force%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02677v2&entry.124074799=Read"},
{"title": "InteRACT: Transformer Models for Human Intent Prediction Conditioned on\n  Robot Actions", "author": "Kushal Kedia and Atiksh Bhardwaj and Prithwish Dan and Sanjiban Choudhury", "abstract": "  In collaborative human-robot manipulation, a robot must predict human intents\nand adapt its actions accordingly to smoothly execute tasks. However, the\nhuman's intent in turn depends on actions the robot takes, creating a\nchicken-or-egg problem. Prior methods ignore such inter-dependency and instead\ntrain marginal intent prediction models independent of robot actions. This is\nbecause training conditional models is hard given a lack of paired human-robot\ninteraction datasets. Can we instead leverage large-scale human-human\ninteraction data that is more easily accessible? Our key insight is to exploit\na correspondence between human and robot actions that enables transfer learning\nfrom human-human to human-robot data. We propose a novel architecture,\nInteRACT, that pre-trains a conditional intent prediction model on large\nhuman-human datasets and fine-tunes on a small human-robot dataset. We evaluate\non a set of real-world collaborative human-robot manipulation tasks and show\nthat our conditional model improves over various marginal baselines. We also\nintroduce new techniques to tele-operate a 7-DoF robot arm and collect a\ndiverse range of human-robot collaborative manipulation data, which we\nopen-source.\n", "link": "http://arxiv.org/abs/2311.12943v2", "date": "2024-02-23", "relevancy": 2.0679, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5726}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5265}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4852}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InteRACT%3A%20Transformer%20Models%20for%20Human%20Intent%20Prediction%20Conditioned%20on%0A%20%20Robot%20Actions&entry.906535625=Kushal%20Kedia%20and%20Atiksh%20Bhardwaj%20and%20Prithwish%20Dan%20and%20Sanjiban%20Choudhury&entry.1292438233=%20%20In%20collaborative%20human-robot%20manipulation%2C%20a%20robot%20must%20predict%20human%20intents%0Aand%20adapt%20its%20actions%20accordingly%20to%20smoothly%20execute%20tasks.%20However%2C%20the%0Ahuman%27s%20intent%20in%20turn%20depends%20on%20actions%20the%20robot%20takes%2C%20creating%20a%0Achicken-or-egg%20problem.%20Prior%20methods%20ignore%20such%20inter-dependency%20and%20instead%0Atrain%20marginal%20intent%20prediction%20models%20independent%20of%20robot%20actions.%20This%20is%0Abecause%20training%20conditional%20models%20is%20hard%20given%20a%20lack%20of%20paired%20human-robot%0Ainteraction%20datasets.%20Can%20we%20instead%20leverage%20large-scale%20human-human%0Ainteraction%20data%20that%20is%20more%20easily%20accessible%3F%20Our%20key%20insight%20is%20to%20exploit%0Aa%20correspondence%20between%20human%20and%20robot%20actions%20that%20enables%20transfer%20learning%0Afrom%20human-human%20to%20human-robot%20data.%20We%20propose%20a%20novel%20architecture%2C%0AInteRACT%2C%20that%20pre-trains%20a%20conditional%20intent%20prediction%20model%20on%20large%0Ahuman-human%20datasets%20and%20fine-tunes%20on%20a%20small%20human-robot%20dataset.%20We%20evaluate%0Aon%20a%20set%20of%20real-world%20collaborative%20human-robot%20manipulation%20tasks%20and%20show%0Athat%20our%20conditional%20model%20improves%20over%20various%20marginal%20baselines.%20We%20also%0Aintroduce%20new%20techniques%20to%20tele-operate%20a%207-DoF%20robot%20arm%20and%20collect%20a%0Adiverse%20range%20of%20human-robot%20collaborative%20manipulation%20data%2C%20which%20we%0Aopen-source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12943v2&entry.124074799=Read"},
{"title": "Seeing is Believing: Mitigating Hallucination in Large Vision-Language\n  Models via CLIP-Guided Decoding", "author": "Ailin Deng and Zhirui Chen and Bryan Hooi", "abstract": "  Large Vision-Language Models (LVLMs) are susceptible to object\nhallucinations, an issue in which their generated text contains non-existent\nobjects, greatly limiting their reliability and practicality. Current\napproaches often rely on the model's token likelihoods or other internal\ninformation, instruction tuning on additional datasets, or incorporating\ncomplex external tools. We first perform empirical analysis on sentence-level\nLVLM hallucination, finding that CLIP similarity to the image acts as a\nstronger and more robust indicator of hallucination compared to token\nlikelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)\napproach, a straightforward but effective training-free approach to reduce\nobject hallucination at decoding time. CGD uses CLIP to guide the model's\ndecoding process by enhancing visual grounding of generated text with the\nimage. Experiments demonstrate that CGD effectively mitigates object\nhallucination across multiple LVLM families while preserving the utility of\ntext generation.\n", "link": "http://arxiv.org/abs/2402.15300v1", "date": "2024-02-23", "relevancy": 2.0612, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.521}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5163}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5093}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20is%20Believing%3A%20Mitigating%20Hallucination%20in%20Large%20Vision-Language%0A%20%20Models%20via%20CLIP-Guided%20Decoding&entry.906535625=Ailin%20Deng%20and%20Zhirui%20Chen%20and%20Bryan%20Hooi&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20are%20susceptible%20to%20object%0Ahallucinations%2C%20an%20issue%20in%20which%20their%20generated%20text%20contains%20non-existent%0Aobjects%2C%20greatly%20limiting%20their%20reliability%20and%20practicality.%20Current%0Aapproaches%20often%20rely%20on%20the%20model%27s%20token%20likelihoods%20or%20other%20internal%0Ainformation%2C%20instruction%20tuning%20on%20additional%20datasets%2C%20or%20incorporating%0Acomplex%20external%20tools.%20We%20first%20perform%20empirical%20analysis%20on%20sentence-level%0ALVLM%20hallucination%2C%20finding%20that%20CLIP%20similarity%20to%20the%20image%20acts%20as%20a%0Astronger%20and%20more%20robust%20indicator%20of%20hallucination%20compared%20to%20token%0Alikelihoods.%20Motivated%20by%20this%2C%20we%20introduce%20our%20CLIP-Guided%20Decoding%20%28CGD%29%0Aapproach%2C%20a%20straightforward%20but%20effective%20training-free%20approach%20to%20reduce%0Aobject%20hallucination%20at%20decoding%20time.%20CGD%20uses%20CLIP%20to%20guide%20the%20model%27s%0Adecoding%20process%20by%20enhancing%20visual%20grounding%20of%20generated%20text%20with%20the%0Aimage.%20Experiments%20demonstrate%20that%20CGD%20effectively%20mitigates%20object%0Ahallucination%20across%20multiple%20LVLM%20families%20while%20preserving%20the%20utility%20of%0Atext%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15300v1&entry.124074799=Read"},
{"title": "Semi-supervised Counting via Pixel-by-pixel Density Distribution\n  Modelling", "author": "Hui Lin and Zhiheng Ma and Rongrong Ji and Yaowei Wang and Zhou Su and Xiaopeng Hong and Deyu Meng", "abstract": "  This paper focuses on semi-supervised crowd counting, where only a small\nportion of the training data are labeled. We formulate the pixel-wise density\nvalue to regress as a probability distribution, instead of a single\ndeterministic value. On this basis, we propose a semi-supervised crowd-counting\nmodel. Firstly, we design a pixel-wise distribution matching loss to measure\nthe differences in the pixel-wise density distributions between the prediction\nand the ground truth; Secondly, we enhance the transformer decoder by using\ndensity tokens to specialize the forwards of decoders w.r.t. different density\nintervals; Thirdly, we design the interleaving consistency self-supervised\nlearning mechanism to learn from unlabeled data efficiently. Extensive\nexperiments on four datasets are performed to show that our method clearly\noutperforms the competitors by a large margin under various labeled ratio\nsettings. Code will be released at\nhttps://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling.\n", "link": "http://arxiv.org/abs/2402.15297v1", "date": "2024-02-23", "relevancy": 2.0507, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.528}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5256}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4937}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-supervised%20Counting%20via%20Pixel-by-pixel%20Density%20Distribution%0A%20%20Modelling&entry.906535625=Hui%20Lin%20and%20Zhiheng%20Ma%20and%20Rongrong%20Ji%20and%20Yaowei%20Wang%20and%20Zhou%20Su%20and%20Xiaopeng%20Hong%20and%20Deyu%20Meng&entry.1292438233=%20%20This%20paper%20focuses%20on%20semi-supervised%20crowd%20counting%2C%20where%20only%20a%20small%0Aportion%20of%20the%20training%20data%20are%20labeled.%20We%20formulate%20the%20pixel-wise%20density%0Avalue%20to%20regress%20as%20a%20probability%20distribution%2C%20instead%20of%20a%20single%0Adeterministic%20value.%20On%20this%20basis%2C%20we%20propose%20a%20semi-supervised%20crowd-counting%0Amodel.%20Firstly%2C%20we%20design%20a%20pixel-wise%20distribution%20matching%20loss%20to%20measure%0Athe%20differences%20in%20the%20pixel-wise%20density%20distributions%20between%20the%20prediction%0Aand%20the%20ground%20truth%3B%20Secondly%2C%20we%20enhance%20the%20transformer%20decoder%20by%20using%0Adensity%20tokens%20to%20specialize%20the%20forwards%20of%20decoders%20w.r.t.%20different%20density%0Aintervals%3B%20Thirdly%2C%20we%20design%20the%20interleaving%20consistency%20self-supervised%0Alearning%20mechanism%20to%20learn%20from%20unlabeled%20data%20efficiently.%20Extensive%0Aexperiments%20on%20four%20datasets%20are%20performed%20to%20show%20that%20our%20method%20clearly%0Aoutperforms%20the%20competitors%20by%20a%20large%20margin%20under%20various%20labeled%20ratio%0Asettings.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15297v1&entry.124074799=Read"},
{"title": "Artificial Bee Colony optimization of Deep Convolutional Neural Networks\n  in the context of Biomedical Imaging", "author": "Adri Gomez Martin and Carlos Fernandez del Cerro and Monica Abella Garcia and Manuel Desco Menendez", "abstract": "  Most efforts in Computer Vision focus on natural images or artwork, which\ndiffer significantly both in size and contents from the kind of data biomedical\nimage processing deals with. Thus, Transfer Learning models often prove\nthemselves suboptimal for these tasks, even after manual finetuning. The\ndevelopment of architectures from scratch is oftentimes unfeasible due to the\nvastness of the hyperparameter space and a shortage of time, computational\nresources and Deep Learning experts in most biomedical research laboratories.\nAn alternative to manually defining the models is the use of Neuroevolution,\nwhich employs metaheuristic techniques to optimize Deep Learning architectures.\nHowever, many algorithms proposed in the neuroevolutive literature are either\ntoo unreliable or limited to a small, predefined region of the hyperparameter\nspace. To overcome these shortcomings, we propose the Chimera Algorithm, a\nnovel, hybrid neuroevolutive algorithm that integrates the Artificial Bee\nColony Algorithm with Evolutionary Computation tools to generate models from\nscratch, as well as to refine a given previous architecture to better fit the\ntask at hand. The Chimera Algorithm has been validated with two datasets of\nnatural and medical images, producing models that surpassed the performance of\nthose coming from Transfer Learning.\n", "link": "http://arxiv.org/abs/2402.15246v1", "date": "2024-02-23", "relevancy": 2.0402, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5397}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4955}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4723}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Bee%20Colony%20optimization%20of%20Deep%20Convolutional%20Neural%20Networks%0A%20%20in%20the%20context%20of%20Biomedical%20Imaging&entry.906535625=Adri%20Gomez%20Martin%20and%20Carlos%20Fernandez%20del%20Cerro%20and%20Monica%20Abella%20Garcia%20and%20Manuel%20Desco%20Menendez&entry.1292438233=%20%20Most%20efforts%20in%20Computer%20Vision%20focus%20on%20natural%20images%20or%20artwork%2C%20which%0Adiffer%20significantly%20both%20in%20size%20and%20contents%20from%20the%20kind%20of%20data%20biomedical%0Aimage%20processing%20deals%20with.%20Thus%2C%20Transfer%20Learning%20models%20often%20prove%0Athemselves%20suboptimal%20for%20these%20tasks%2C%20even%20after%20manual%20finetuning.%20The%0Adevelopment%20of%20architectures%20from%20scratch%20is%20oftentimes%20unfeasible%20due%20to%20the%0Avastness%20of%20the%20hyperparameter%20space%20and%20a%20shortage%20of%20time%2C%20computational%0Aresources%20and%20Deep%20Learning%20experts%20in%20most%20biomedical%20research%20laboratories.%0AAn%20alternative%20to%20manually%20defining%20the%20models%20is%20the%20use%20of%20Neuroevolution%2C%0Awhich%20employs%20metaheuristic%20techniques%20to%20optimize%20Deep%20Learning%20architectures.%0AHowever%2C%20many%20algorithms%20proposed%20in%20the%20neuroevolutive%20literature%20are%20either%0Atoo%20unreliable%20or%20limited%20to%20a%20small%2C%20predefined%20region%20of%20the%20hyperparameter%0Aspace.%20To%20overcome%20these%20shortcomings%2C%20we%20propose%20the%20Chimera%20Algorithm%2C%20a%0Anovel%2C%20hybrid%20neuroevolutive%20algorithm%20that%20integrates%20the%20Artificial%20Bee%0AColony%20Algorithm%20with%20Evolutionary%20Computation%20tools%20to%20generate%20models%20from%0Ascratch%2C%20as%20well%20as%20to%20refine%20a%20given%20previous%20architecture%20to%20better%20fit%20the%0Atask%20at%20hand.%20The%20Chimera%20Algorithm%20has%20been%20validated%20with%20two%20datasets%20of%0Anatural%20and%20medical%20images%2C%20producing%20models%20that%20surpassed%20the%20performance%20of%0Athose%20coming%20from%20Transfer%20Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15246v1&entry.124074799=Read"},
{"title": "GS-EMA: Integrating Gradient Surgery Exponential Moving Average with\n  Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in\n  Aneurysm Segmentation", "author": "Fengming Lin and Yan Xia and Michael MacRaild and Yash Deo and Haoran Dou and Qiongyao Liu and Nina Cheng and Nishant Ravikumar and Alejandro F. Frangi", "abstract": "  The automated segmentation of cerebral aneurysms is pivotal for accurate\ndiagnosis and treatment planning. Confronted with significant domain shifts and\nclass imbalance in 3D Rotational Angiography (3DRA) data from various medical\ninstitutions, the task becomes challenging. These shifts include differences in\nimage appearance, intensity distribution, resolution, and aneurysm size, all of\nwhich complicate the segmentation process. To tackle these issues, we propose a\nnovel domain generalization strategy that employs gradient surgery exponential\nmoving average (GS-EMA) optimization technique coupled with boundary-aware\ncontrastive learning (BACL). Our approach is distinct in its ability to adapt\nto new, unseen domains by learning domain-invariant features, thereby improving\nthe robustness and accuracy of aneurysm segmentation across diverse clinical\ndatasets. The results demonstrate that our proposed approach can extract more\ndomain-invariant features, minimizing over-segmentation and capturing more\ncomplete aneurysm structures.\n", "link": "http://arxiv.org/abs/2402.15239v1", "date": "2024-02-23", "relevancy": 2.0314, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5106}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5094}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.497}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-EMA%3A%20Integrating%20Gradient%20Surgery%20Exponential%20Moving%20Average%20with%0A%20%20Boundary-Aware%20Contrastive%20Learning%20for%20Enhanced%20Domain%20Generalization%20in%0A%20%20Aneurysm%20Segmentation&entry.906535625=Fengming%20Lin%20and%20Yan%20Xia%20and%20Michael%20MacRaild%20and%20Yash%20Deo%20and%20Haoran%20Dou%20and%20Qiongyao%20Liu%20and%20Nina%20Cheng%20and%20Nishant%20Ravikumar%20and%20Alejandro%20F.%20Frangi&entry.1292438233=%20%20The%20automated%20segmentation%20of%20cerebral%20aneurysms%20is%20pivotal%20for%20accurate%0Adiagnosis%20and%20treatment%20planning.%20Confronted%20with%20significant%20domain%20shifts%20and%0Aclass%20imbalance%20in%203D%20Rotational%20Angiography%20%283DRA%29%20data%20from%20various%20medical%0Ainstitutions%2C%20the%20task%20becomes%20challenging.%20These%20shifts%20include%20differences%20in%0Aimage%20appearance%2C%20intensity%20distribution%2C%20resolution%2C%20and%20aneurysm%20size%2C%20all%20of%0Awhich%20complicate%20the%20segmentation%20process.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%0Anovel%20domain%20generalization%20strategy%20that%20employs%20gradient%20surgery%20exponential%0Amoving%20average%20%28GS-EMA%29%20optimization%20technique%20coupled%20with%20boundary-aware%0Acontrastive%20learning%20%28BACL%29.%20Our%20approach%20is%20distinct%20in%20its%20ability%20to%20adapt%0Ato%20new%2C%20unseen%20domains%20by%20learning%20domain-invariant%20features%2C%20thereby%20improving%0Athe%20robustness%20and%20accuracy%20of%20aneurysm%20segmentation%20across%20diverse%20clinical%0Adatasets.%20The%20results%20demonstrate%20that%20our%20proposed%20approach%20can%20extract%20more%0Adomain-invariant%20features%2C%20minimizing%20over-segmentation%20and%20capturing%20more%0Acomplete%20aneurysm%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15239v1&entry.124074799=Read"},
{"title": "Spatiotemporal Observer Design for Predictive Learning of\n  High-Dimensional Data", "author": "Tongyi Liang and Han-Xiong Li", "abstract": "  Although deep learning-based methods have shown great success in\nspatiotemporal predictive learning, the framework of those models is designed\nmainly by intuition. How to make spatiotemporal forecasting with theoretical\nguarantees is still a challenging issue. In this work, we tackle this problem\nby applying domain knowledge from the dynamical system to the framework design\nof deep learning models. An observer theory-guided deep learning architecture,\ncalled Spatiotemporal Observer, is designed for predictive learning of high\ndimensional data. The characteristics of the proposed framework are twofold:\nfirstly, it provides the generalization error bound and convergence guarantee\nfor spatiotemporal prediction; secondly, dynamical regularization is introduced\nto enable the model to learn system dynamics better during training. Further\nexperimental results show that this framework could capture the spatiotemporal\ndynamics and make accurate predictions in both one-step-ahead and\nmulti-step-ahead forecasting scenarios.\n", "link": "http://arxiv.org/abs/2402.15284v1", "date": "2024-02-23", "relevancy": 2.0286, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5144}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.509}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4992}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatiotemporal%20Observer%20Design%20for%20Predictive%20Learning%20of%0A%20%20High-Dimensional%20Data&entry.906535625=Tongyi%20Liang%20and%20Han-Xiong%20Li&entry.1292438233=%20%20Although%20deep%20learning-based%20methods%20have%20shown%20great%20success%20in%0Aspatiotemporal%20predictive%20learning%2C%20the%20framework%20of%20those%20models%20is%20designed%0Amainly%20by%20intuition.%20How%20to%20make%20spatiotemporal%20forecasting%20with%20theoretical%0Aguarantees%20is%20still%20a%20challenging%20issue.%20In%20this%20work%2C%20we%20tackle%20this%20problem%0Aby%20applying%20domain%20knowledge%20from%20the%20dynamical%20system%20to%20the%20framework%20design%0Aof%20deep%20learning%20models.%20An%20observer%20theory-guided%20deep%20learning%20architecture%2C%0Acalled%20Spatiotemporal%20Observer%2C%20is%20designed%20for%20predictive%20learning%20of%20high%0Adimensional%20data.%20The%20characteristics%20of%20the%20proposed%20framework%20are%20twofold%3A%0Afirstly%2C%20it%20provides%20the%20generalization%20error%20bound%20and%20convergence%20guarantee%0Afor%20spatiotemporal%20prediction%3B%20secondly%2C%20dynamical%20regularization%20is%20introduced%0Ato%20enable%20the%20model%20to%20learn%20system%20dynamics%20better%20during%20training.%20Further%0Aexperimental%20results%20show%20that%20this%20framework%20could%20capture%20the%20spatiotemporal%0Adynamics%20and%20make%20accurate%20predictions%20in%20both%20one-step-ahead%20and%0Amulti-step-ahead%20forecasting%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15284v1&entry.124074799=Read"},
{"title": "FAIR: Filtering of Automatically Induced Rules", "author": "Divya Jyoti Bajpai and Ayush Maheshwari and Manjesh Kumar Hanawal and Ganesh Ramakrishnan", "abstract": "  The availability of large annotated data can be a critical bottleneck in\ntraining machine learning algorithms successfully, especially when applied to\ndiverse domains. Weak supervision offers a promising alternative by\naccelerating the creation of labeled training data using domain-specific rules.\nHowever, it requires users to write a diverse set of high-quality rules to\nassign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches\ncircumvent this problem by automatically creating rules from features on a\nsmall labeled set and filtering a final set of rules from them. In the ARI\napproach, the crucial step is to filter out a set of a high-quality useful\nsubset of rules from the large set of automatically created rules. In this\npaper, we propose an algorithm (Filtering of Automatically Induced Rules) to\nfilter rules from a large number of automatically induced rules using\nsubmodular objective functions that account for the collective precision,\ncoverage, and conflicts of the rule set. We experiment with three ARI\napproaches and five text classification datasets to validate the superior\nperformance of our algorithm with respect to several semi-supervised label\naggregation approaches. Further, we show that achieves statistically\nsignificant results in comparison to existing rule-filtering approaches.\n", "link": "http://arxiv.org/abs/2402.15472v1", "date": "2024-02-23", "relevancy": 2.0278, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4094}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4072}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4001}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAIR%3A%20Filtering%20of%20Automatically%20Induced%20Rules&entry.906535625=Divya%20Jyoti%20Bajpai%20and%20Ayush%20Maheshwari%20and%20Manjesh%20Kumar%20Hanawal%20and%20Ganesh%20Ramakrishnan&entry.1292438233=%20%20The%20availability%20of%20large%20annotated%20data%20can%20be%20a%20critical%20bottleneck%20in%0Atraining%20machine%20learning%20algorithms%20successfully%2C%20especially%20when%20applied%20to%0Adiverse%20domains.%20Weak%20supervision%20offers%20a%20promising%20alternative%20by%0Aaccelerating%20the%20creation%20of%20labeled%20training%20data%20using%20domain-specific%20rules.%0AHowever%2C%20it%20requires%20users%20to%20write%20a%20diverse%20set%20of%20high-quality%20rules%20to%0Aassign%20labels%20to%20the%20unlabeled%20data.%20Automatic%20Rule%20Induction%20%28ARI%29%20approaches%0Acircumvent%20this%20problem%20by%20automatically%20creating%20rules%20from%20features%20on%20a%0Asmall%20labeled%20set%20and%20filtering%20a%20final%20set%20of%20rules%20from%20them.%20In%20the%20ARI%0Aapproach%2C%20the%20crucial%20step%20is%20to%20filter%20out%20a%20set%20of%20a%20high-quality%20useful%0Asubset%20of%20rules%20from%20the%20large%20set%20of%20automatically%20created%20rules.%20In%20this%0Apaper%2C%20we%20propose%20an%20algorithm%20%28Filtering%20of%20Automatically%20Induced%20Rules%29%20to%0Afilter%20rules%20from%20a%20large%20number%20of%20automatically%20induced%20rules%20using%0Asubmodular%20objective%20functions%20that%20account%20for%20the%20collective%20precision%2C%0Acoverage%2C%20and%20conflicts%20of%20the%20rule%20set.%20We%20experiment%20with%20three%20ARI%0Aapproaches%20and%20five%20text%20classification%20datasets%20to%20validate%20the%20superior%0Aperformance%20of%20our%20algorithm%20with%20respect%20to%20several%20semi-supervised%20label%0Aaggregation%20approaches.%20Further%2C%20we%20show%20that%20achieves%20statistically%0Asignificant%20results%20in%20comparison%20to%20existing%20rule-filtering%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15472v1&entry.124074799=Read"},
{"title": "Mechanics-Informed Autoencoder Enables Automated Detection and\n  Localization of Unforeseen Structural Damage", "author": "Xuyang Li and Hamed Bolandi and Mahdi Masmoudi and Talal Salem and Nizar Lajnef and Vishnu Naresh Boddeti", "abstract": "  Structural health monitoring (SHM) is vital for ensuring the safety and\nlongevity of structures like buildings and bridges. As the volume and scale of\nstructures and the impact of their failure continue to grow, there is a dire\nneed for SHM techniques that are scalable, inexpensive, operate passively\nwithout human intervention, and customized for each mechanical structure\nwithout the need for complex baseline models. We present a novel\n\"deploy-and-forget\" approach for automated detection and localization of\ndamages in structures. It is based on a synergistic combination of fully\npassive measurements from inexpensive sensors and a mechanics-informed\nautoencoder. Once deployed, our solution continuously learns and adapts a\nbespoke baseline model for each structure, learning from its undamaged state's\nresponse characteristics. After learning from just 3 hours of data, it can\nautonomously detect and localize different types of unforeseen damage. Results\nfrom numerical simulations and experiments indicate that incorporating the\nmechanical characteristics into the variational autoencoder allows for up to\n35\\% earlier detection and localization of damage over a standard autoencoder.\nOur approach holds substantial promise for a significant reduction in human\nintervention and inspection costs and enables proactive and preventive\nmaintenance strategies, thus extending the lifespan, reliability, and\nsustainability of civil infrastructures.\n", "link": "http://arxiv.org/abs/2402.15492v1", "date": "2024-02-23", "relevancy": 2.0244, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5014}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4976}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanics-Informed%20Autoencoder%20Enables%20Automated%20Detection%20and%0A%20%20Localization%20of%20Unforeseen%20Structural%20Damage&entry.906535625=Xuyang%20Li%20and%20Hamed%20Bolandi%20and%20Mahdi%20Masmoudi%20and%20Talal%20Salem%20and%20Nizar%20Lajnef%20and%20Vishnu%20Naresh%20Boddeti&entry.1292438233=%20%20Structural%20health%20monitoring%20%28SHM%29%20is%20vital%20for%20ensuring%20the%20safety%20and%0Alongevity%20of%20structures%20like%20buildings%20and%20bridges.%20As%20the%20volume%20and%20scale%20of%0Astructures%20and%20the%20impact%20of%20their%20failure%20continue%20to%20grow%2C%20there%20is%20a%20dire%0Aneed%20for%20SHM%20techniques%20that%20are%20scalable%2C%20inexpensive%2C%20operate%20passively%0Awithout%20human%20intervention%2C%20and%20customized%20for%20each%20mechanical%20structure%0Awithout%20the%20need%20for%20complex%20baseline%20models.%20We%20present%20a%20novel%0A%22deploy-and-forget%22%20approach%20for%20automated%20detection%20and%20localization%20of%0Adamages%20in%20structures.%20It%20is%20based%20on%20a%20synergistic%20combination%20of%20fully%0Apassive%20measurements%20from%20inexpensive%20sensors%20and%20a%20mechanics-informed%0Aautoencoder.%20Once%20deployed%2C%20our%20solution%20continuously%20learns%20and%20adapts%20a%0Abespoke%20baseline%20model%20for%20each%20structure%2C%20learning%20from%20its%20undamaged%20state%27s%0Aresponse%20characteristics.%20After%20learning%20from%20just%203%20hours%20of%20data%2C%20it%20can%0Aautonomously%20detect%20and%20localize%20different%20types%20of%20unforeseen%20damage.%20Results%0Afrom%20numerical%20simulations%20and%20experiments%20indicate%20that%20incorporating%20the%0Amechanical%20characteristics%20into%20the%20variational%20autoencoder%20allows%20for%20up%20to%0A35%5C%25%20earlier%20detection%20and%20localization%20of%20damage%20over%20a%20standard%20autoencoder.%0AOur%20approach%20holds%20substantial%20promise%20for%20a%20significant%20reduction%20in%20human%0Aintervention%20and%20inspection%20costs%20and%20enables%20proactive%20and%20preventive%0Amaintenance%20strategies%2C%20thus%20extending%20the%20lifespan%2C%20reliability%2C%20and%0Asustainability%20of%20civil%20infrastructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15492v1&entry.124074799=Read"},
{"title": "Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at\n  Irregularly Spaced Data", "author": "Jonathan W. Siegel", "abstract": "  We study the interpolation power of deep ReLU neural networks. Specifically,\nwe consider the question of how efficiently, in terms of the number of\nparameters, deep ReLU networks can interpolate values at $N$ datapoints in the\nunit ball which are separated by a distance $\\delta$. We show that $\\Omega(N)$\nparameters are required in the regime where $\\delta$ is exponentially small in\n$N$, which gives the sharp result in this regime since $O(N)$ parameters are\nalways sufficient. This also shows that the bit-extraction technique used to\nprove lower bounds on the VC dimension cannot be applied to irregularly spaced\ndatapoints. Finally, as an application we give a lower bound on the\napproximation rates that deep ReLU neural networks can achieve for Sobolev\nspaces at the embedding endpoint.\n", "link": "http://arxiv.org/abs/2302.00834v2", "date": "2024-02-23", "relevancy": 2.0187, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4134}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3993}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3984}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20Lower%20Bounds%20on%20Interpolation%20by%20Deep%20ReLU%20Neural%20Networks%20at%0A%20%20Irregularly%20Spaced%20Data&entry.906535625=Jonathan%20W.%20Siegel&entry.1292438233=%20%20We%20study%20the%20interpolation%20power%20of%20deep%20ReLU%20neural%20networks.%20Specifically%2C%0Awe%20consider%20the%20question%20of%20how%20efficiently%2C%20in%20terms%20of%20the%20number%20of%0Aparameters%2C%20deep%20ReLU%20networks%20can%20interpolate%20values%20at%20%24N%24%20datapoints%20in%20the%0Aunit%20ball%20which%20are%20separated%20by%20a%20distance%20%24%5Cdelta%24.%20We%20show%20that%20%24%5COmega%28N%29%24%0Aparameters%20are%20required%20in%20the%20regime%20where%20%24%5Cdelta%24%20is%20exponentially%20small%20in%0A%24N%24%2C%20which%20gives%20the%20sharp%20result%20in%20this%20regime%20since%20%24O%28N%29%24%20parameters%20are%0Aalways%20sufficient.%20This%20also%20shows%20that%20the%20bit-extraction%20technique%20used%20to%0Aprove%20lower%20bounds%20on%20the%20VC%20dimension%20cannot%20be%20applied%20to%20irregularly%20spaced%0Adatapoints.%20Finally%2C%20as%20an%20application%20we%20give%20a%20lower%20bound%20on%20the%0Aapproximation%20rates%20that%20deep%20ReLU%20neural%20networks%20can%20achieve%20for%20Sobolev%0Aspaces%20at%20the%20embedding%20endpoint.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.00834v2&entry.124074799=Read"},
{"title": "Information-Theoretic Safe Bayesian Optimization", "author": "Alessandro G. Bottero and Carlos E. Luis and Julia Vinogradska and Felix Berkenkamp and Jan Peters", "abstract": "  We consider a sequential decision making task, where the goal is to optimize\nan unknown function without evaluating parameters that violate an a~priori\nunknown (safety) constraint. A common approach is to place a Gaussian process\nprior on the unknown functions and allow evaluations only in regions that are\nsafe with high probability. Most current methods rely on a discretization of\nthe domain and cannot be directly extended to the continuous case. Moreover,\nthe way in which they exploit regularity assumptions about the constraint\nintroduces an additional critical hyperparameter. In this paper, we propose an\ninformation-theoretic safe exploration criterion that directly exploits the GP\nposterior to identify the most informative safe parameters to evaluate. The\ncombination of this exploration criterion with a well known Bayesian\noptimization acquisition function yields a novel safe Bayesian optimization\nselection criterion. Our approach is naturally applicable to continuous domains\nand does not require additional explicit hyperparameters. We theoretically\nanalyze the method and show that we do not violate the safety constraint with\nhigh probability and that we learn about the value of the safe optimum up to\narbitrary precision. Empirical evaluations demonstrate improved data-efficiency\nand scalability.\n", "link": "http://arxiv.org/abs/2402.15347v1", "date": "2024-02-23", "relevancy": 2.0123, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5521}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5046}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.482}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-Theoretic%20Safe%20Bayesian%20Optimization&entry.906535625=Alessandro%20G.%20Bottero%20and%20Carlos%20E.%20Luis%20and%20Julia%20Vinogradska%20and%20Felix%20Berkenkamp%20and%20Jan%20Peters&entry.1292438233=%20%20We%20consider%20a%20sequential%20decision%20making%20task%2C%20where%20the%20goal%20is%20to%20optimize%0Aan%20unknown%20function%20without%20evaluating%20parameters%20that%20violate%20an%20a~priori%0Aunknown%20%28safety%29%20constraint.%20A%20common%20approach%20is%20to%20place%20a%20Gaussian%20process%0Aprior%20on%20the%20unknown%20functions%20and%20allow%20evaluations%20only%20in%20regions%20that%20are%0Asafe%20with%20high%20probability.%20Most%20current%20methods%20rely%20on%20a%20discretization%20of%0Athe%20domain%20and%20cannot%20be%20directly%20extended%20to%20the%20continuous%20case.%20Moreover%2C%0Athe%20way%20in%20which%20they%20exploit%20regularity%20assumptions%20about%20the%20constraint%0Aintroduces%20an%20additional%20critical%20hyperparameter.%20In%20this%20paper%2C%20we%20propose%20an%0Ainformation-theoretic%20safe%20exploration%20criterion%20that%20directly%20exploits%20the%20GP%0Aposterior%20to%20identify%20the%20most%20informative%20safe%20parameters%20to%20evaluate.%20The%0Acombination%20of%20this%20exploration%20criterion%20with%20a%20well%20known%20Bayesian%0Aoptimization%20acquisition%20function%20yields%20a%20novel%20safe%20Bayesian%20optimization%0Aselection%20criterion.%20Our%20approach%20is%20naturally%20applicable%20to%20continuous%20domains%0Aand%20does%20not%20require%20additional%20explicit%20hyperparameters.%20We%20theoretically%0Aanalyze%20the%20method%20and%20show%20that%20we%20do%20not%20violate%20the%20safety%20constraint%20with%0Ahigh%20probability%20and%20that%20we%20learn%20about%20the%20value%20of%20the%20safe%20optimum%20up%20to%0Aarbitrary%20precision.%20Empirical%20evaluations%20demonstrate%20improved%20data-efficiency%0Aand%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15347v1&entry.124074799=Read"},
{"title": "NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks", "author": "Bernardo Esteves and Miguel Vasco and Francisco S. Melo", "abstract": "  While machine learning methods excel at pattern recognition, they struggle\nwith complex reasoning tasks in a scalable, algorithmic manner. Recent Deep\nThinking methods show promise in learning algorithms that extrapolate: learning\nin smaller environments and executing the learned algorithm in larger\nenvironments. However, these works are limited to symmetrical tasks, where the\ninput and output dimensionalities are the same. To address this gap, we propose\nNeuralThink, a new recurrent architecture that can consistently extrapolate to\nboth symmetrical and asymmetrical tasks, where the dimensionality of the input\nand output are different. We contribute with a novel benchmark of asymmetrical\ntasks for extrapolation. We show that NeuralThink consistently outperforms the\nprior state-of-the-art Deep Thinking architectures, in regards to stable\nextrapolation to large observations from smaller training sizes.\n", "link": "http://arxiv.org/abs/2402.15393v1", "date": "2024-02-23", "relevancy": 1.9992, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4731}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralThink%3A%20Algorithm%20Synthesis%20that%20Extrapolates%20in%20General%20Tasks&entry.906535625=Bernardo%20Esteves%20and%20Miguel%20Vasco%20and%20Francisco%20S.%20Melo&entry.1292438233=%20%20While%20machine%20learning%20methods%20excel%20at%20pattern%20recognition%2C%20they%20struggle%0Awith%20complex%20reasoning%20tasks%20in%20a%20scalable%2C%20algorithmic%20manner.%20Recent%20Deep%0AThinking%20methods%20show%20promise%20in%20learning%20algorithms%20that%20extrapolate%3A%20learning%0Ain%20smaller%20environments%20and%20executing%20the%20learned%20algorithm%20in%20larger%0Aenvironments.%20However%2C%20these%20works%20are%20limited%20to%20symmetrical%20tasks%2C%20where%20the%0Ainput%20and%20output%20dimensionalities%20are%20the%20same.%20To%20address%20this%20gap%2C%20we%20propose%0ANeuralThink%2C%20a%20new%20recurrent%20architecture%20that%20can%20consistently%20extrapolate%20to%0Aboth%20symmetrical%20and%20asymmetrical%20tasks%2C%20where%20the%20dimensionality%20of%20the%20input%0Aand%20output%20are%20different.%20We%20contribute%20with%20a%20novel%20benchmark%20of%20asymmetrical%0Atasks%20for%20extrapolation.%20We%20show%20that%20NeuralThink%20consistently%20outperforms%20the%0Aprior%20state-of-the-art%20Deep%20Thinking%20architectures%2C%20in%20regards%20to%20stable%0Aextrapolation%20to%20large%20observations%20from%20smaller%20training%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15393v1&entry.124074799=Read"},
{"title": "Open Ad Hoc Teamwork with Cooperative Game Theory", "author": "Jianhong Wang and Yang Li and Yuan Zhang and Wei Pan and Samuel Kaski", "abstract": "  Ad hoc teamwork poses a challenging problem, requiring the design of an agent\nto collaborate with teammates without prior coordination or joint training.\nOpen ad hoc teamwork further complicates this challenge by considering\nenvironments with a changing number of teammates, referred to as open teams.\nThe state-of-the-art solution to this problem is graph-based policy learning\n(GPL), leveraging the generalizability of graph neural networks to handle an\nunrestricted number of agents and effectively address open teams. GPL's\nperformance is superior to other methods, but its joint Q-value representation\npresents challenges for interpretation, hindering further development of this\nresearch line and applicability. In this paper, we establish a new theory to\ngive an interpretation for the joint Q-value representation employed in GPL,\nfrom the perspective of cooperative game theory. Building on our theory, we\npropose a novel algorithm based on GPL framework, to complement the critical\nfeatures that facilitate learning, but overlooked in GPL. Through experiments,\nwe demonstrate the correctness of our theory by comparing the performance of\nthe resulting algorithm with GPL in dynamic team compositions.\n", "link": "http://arxiv.org/abs/2402.15259v1", "date": "2024-02-23", "relevancy": 1.9903, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5251}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4795}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.474}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Ad%20Hoc%20Teamwork%20with%20Cooperative%20Game%20Theory&entry.906535625=Jianhong%20Wang%20and%20Yang%20Li%20and%20Yuan%20Zhang%20and%20Wei%20Pan%20and%20Samuel%20Kaski&entry.1292438233=%20%20Ad%20hoc%20teamwork%20poses%20a%20challenging%20problem%2C%20requiring%20the%20design%20of%20an%20agent%0Ato%20collaborate%20with%20teammates%20without%20prior%20coordination%20or%20joint%20training.%0AOpen%20ad%20hoc%20teamwork%20further%20complicates%20this%20challenge%20by%20considering%0Aenvironments%20with%20a%20changing%20number%20of%20teammates%2C%20referred%20to%20as%20open%20teams.%0AThe%20state-of-the-art%20solution%20to%20this%20problem%20is%20graph-based%20policy%20learning%0A%28GPL%29%2C%20leveraging%20the%20generalizability%20of%20graph%20neural%20networks%20to%20handle%20an%0Aunrestricted%20number%20of%20agents%20and%20effectively%20address%20open%20teams.%20GPL%27s%0Aperformance%20is%20superior%20to%20other%20methods%2C%20but%20its%20joint%20Q-value%20representation%0Apresents%20challenges%20for%20interpretation%2C%20hindering%20further%20development%20of%20this%0Aresearch%20line%20and%20applicability.%20In%20this%20paper%2C%20we%20establish%20a%20new%20theory%20to%0Agive%20an%20interpretation%20for%20the%20joint%20Q-value%20representation%20employed%20in%20GPL%2C%0Afrom%20the%20perspective%20of%20cooperative%20game%20theory.%20Building%20on%20our%20theory%2C%20we%0Apropose%20a%20novel%20algorithm%20based%20on%20GPL%20framework%2C%20to%20complement%20the%20critical%0Afeatures%20that%20facilitate%20learning%2C%20but%20overlooked%20in%20GPL.%20Through%20experiments%2C%0Awe%20demonstrate%20the%20correctness%20of%20our%20theory%20by%20comparing%20the%20performance%20of%0Athe%20resulting%20algorithm%20with%20GPL%20in%20dynamic%20team%20compositions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15259v1&entry.124074799=Read"},
{"title": "Neural Implicit Swept Volume Models for Fast Collision Detection", "author": "Dominik Joho and Jonas Schwinn and Kirill Safronov", "abstract": "  Collision detection is one of the most time-consuming operations during\nmotion planning. Thus, there is an increasing interest in exploring machine\nlearning techniques to speed up collision detection and sampling-based motion\nplanning. A recent line of research focuses on utilizing neural signed distance\nfunctions of either the robot geometry or the swept volume of the robot motion.\nBuilding on this, we present a novel neural implicit swept volume model that is\nthe first to continuously represent arbitrary motions parameterized by their\nstart and goal configurations. This allows to quickly compute signed distances\nfor any point in the task space to the robot motion. Further, we present an\nalgorithm combining the speed of the deep learning-based signed distance\ncomputations with the strong accuracy guarantees of geometric collision\ncheckers. We validate our approach in simulated and real-world robotic\nexperiments, and demonstrate that it is able to speed up a commercial bin\npicking application.\n", "link": "http://arxiv.org/abs/2402.15281v1", "date": "2024-02-23", "relevancy": 1.9736, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5132}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4975}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4814}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Implicit%20Swept%20Volume%20Models%20for%20Fast%20Collision%20Detection&entry.906535625=Dominik%20Joho%20and%20Jonas%20Schwinn%20and%20Kirill%20Safronov&entry.1292438233=%20%20Collision%20detection%20is%20one%20of%20the%20most%20time-consuming%20operations%20during%0Amotion%20planning.%20Thus%2C%20there%20is%20an%20increasing%20interest%20in%20exploring%20machine%0Alearning%20techniques%20to%20speed%20up%20collision%20detection%20and%20sampling-based%20motion%0Aplanning.%20A%20recent%20line%20of%20research%20focuses%20on%20utilizing%20neural%20signed%20distance%0Afunctions%20of%20either%20the%20robot%20geometry%20or%20the%20swept%20volume%20of%20the%20robot%20motion.%0ABuilding%20on%20this%2C%20we%20present%20a%20novel%20neural%20implicit%20swept%20volume%20model%20that%20is%0Athe%20first%20to%20continuously%20represent%20arbitrary%20motions%20parameterized%20by%20their%0Astart%20and%20goal%20configurations.%20This%20allows%20to%20quickly%20compute%20signed%20distances%0Afor%20any%20point%20in%20the%20task%20space%20to%20the%20robot%20motion.%20Further%2C%20we%20present%20an%0Aalgorithm%20combining%20the%20speed%20of%20the%20deep%20learning-based%20signed%20distance%0Acomputations%20with%20the%20strong%20accuracy%20guarantees%20of%20geometric%20collision%0Acheckers.%20We%20validate%20our%20approach%20in%20simulated%20and%20real-world%20robotic%0Aexperiments%2C%20and%20demonstrate%20that%20it%20is%20able%20to%20speed%20up%20a%20commercial%20bin%0Apicking%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15281v1&entry.124074799=Read"},
{"title": "OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene\n  Understanding", "author": "Francis Engelmann and Ayca Takmaz and Jonas Schult and Elisabetta Fedele and Johanna Wald and Songyou Peng and Xi Wang and Or Litany and Siyu Tang and Federico Tombari and Marc Pollefeys and Leonidas Guibas and Hongbo Tian and Chunjie Wang and Xiaosheng Yan and Bingwen Wang and Xuanyang Zhang and Xiao Liu and Phuc Nguyen and Khoi Nguyen and Anh Tran and Cuong Pham and Zhening Huang and Xiaoyang Wu and Xi Chen and Hengshuang Zhao and Lei Zhu and Joan Lasenby", "abstract": "  This report provides an overview of the challenge hosted at the OpenSUN3D\nWorkshop on Open-Vocabulary 3D Scene Understanding held in conjunction with\nICCV 2023. The goal of this workshop series is to provide a platform for\nexploration and discussion of open-vocabulary 3D scene understanding tasks,\nincluding but not limited to segmentation, detection and mapping. We provide an\noverview of the challenge hosted at the workshop, present the challenge\ndataset, the evaluation methodology, and brief descriptions of the winning\nmethods. For additional details, please see\nhttps://opensun3d.github.io/index_iccv23.html.\n", "link": "http://arxiv.org/abs/2402.15321v1", "date": "2024-02-23", "relevancy": 1.9672, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4982}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4925}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4851}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenSUN3D%3A%201st%20Workshop%20Challenge%20on%20Open-Vocabulary%203D%20Scene%0A%20%20Understanding&entry.906535625=Francis%20Engelmann%20and%20Ayca%20Takmaz%20and%20Jonas%20Schult%20and%20Elisabetta%20Fedele%20and%20Johanna%20Wald%20and%20Songyou%20Peng%20and%20Xi%20Wang%20and%20Or%20Litany%20and%20Siyu%20Tang%20and%20Federico%20Tombari%20and%20Marc%20Pollefeys%20and%20Leonidas%20Guibas%20and%20Hongbo%20Tian%20and%20Chunjie%20Wang%20and%20Xiaosheng%20Yan%20and%20Bingwen%20Wang%20and%20Xuanyang%20Zhang%20and%20Xiao%20Liu%20and%20Phuc%20Nguyen%20and%20Khoi%20Nguyen%20and%20Anh%20Tran%20and%20Cuong%20Pham%20and%20Zhening%20Huang%20and%20Xiaoyang%20Wu%20and%20Xi%20Chen%20and%20Hengshuang%20Zhao%20and%20Lei%20Zhu%20and%20Joan%20Lasenby&entry.1292438233=%20%20This%20report%20provides%20an%20overview%20of%20the%20challenge%20hosted%20at%20the%20OpenSUN3D%0AWorkshop%20on%20Open-Vocabulary%203D%20Scene%20Understanding%20held%20in%20conjunction%20with%0AICCV%202023.%20The%20goal%20of%20this%20workshop%20series%20is%20to%20provide%20a%20platform%20for%0Aexploration%20and%20discussion%20of%20open-vocabulary%203D%20scene%20understanding%20tasks%2C%0Aincluding%20but%20not%20limited%20to%20segmentation%2C%20detection%20and%20mapping.%20We%20provide%20an%0Aoverview%20of%20the%20challenge%20hosted%20at%20the%20workshop%2C%20present%20the%20challenge%0Adataset%2C%20the%20evaluation%20methodology%2C%20and%20brief%20descriptions%20of%20the%20winning%0Amethods.%20For%20additional%20details%2C%20please%20see%0Ahttps%3A//opensun3d.github.io/index_iccv23.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15321v1&entry.124074799=Read"},
{"title": "Adversarial Robustness of Deep Learning-based Malware Detectors via\n  (De)Randomized Smoothing", "author": "Daniel Gibert and Giulio Zizzo and Quan Le and Jordi Planes", "abstract": "  Deep learning-based malware detectors have been shown to be susceptible to\nadversarial malware examples, i.e. malware examples that have been deliberately\nmanipulated in order to avoid detection. In light of the vulnerability of deep\nlearning detectors to subtle input file modifications, we propose a practical\ndefense against adversarial malware examples inspired by (de)randomized\nsmoothing. In this work, we reduce the chances of sampling adversarial content\ninjected by malware authors by selecting correlated subsets of bytes, rather\nthan using Gaussian noise to randomize inputs like in the Computer Vision (CV)\ndomain. During training, our ablation-based smoothing scheme trains a base\nclassifier to make classifications on a subset of contiguous bytes or chunk of\nbytes. At test time, a large number of chunks are then classified by a base\nclassifier and the consensus among these classifications is then reported as\nthe final prediction. We propose two strategies to determine the location of\nthe chunks used for classification: (1) randomly selecting the locations of the\nchunks and (2) selecting contiguous adjacent chunks. To showcase the\neffectiveness of our approach, we have trained two classifiers with our\nchunk-based ablation schemes on the BODMAS dataset. Our findings reveal that\nthe chunk-based smoothing classifiers exhibit greater resilience against\nadversarial malware examples generated with state-of-the-are evasion attacks,\noutperforming a non-smoothed classifier and a randomized smoothing-based\nclassifier by a great margin.\n", "link": "http://arxiv.org/abs/2402.15267v1", "date": "2024-02-23", "relevancy": 1.9664, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5111}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4784}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4761}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Robustness%20of%20Deep%20Learning-based%20Malware%20Detectors%20via%0A%20%20%28De%29Randomized%20Smoothing&entry.906535625=Daniel%20Gibert%20and%20Giulio%20Zizzo%20and%20Quan%20Le%20and%20Jordi%20Planes&entry.1292438233=%20%20Deep%20learning-based%20malware%20detectors%20have%20been%20shown%20to%20be%20susceptible%20to%0Aadversarial%20malware%20examples%2C%20i.e.%20malware%20examples%20that%20have%20been%20deliberately%0Amanipulated%20in%20order%20to%20avoid%20detection.%20In%20light%20of%20the%20vulnerability%20of%20deep%0Alearning%20detectors%20to%20subtle%20input%20file%20modifications%2C%20we%20propose%20a%20practical%0Adefense%20against%20adversarial%20malware%20examples%20inspired%20by%20%28de%29randomized%0Asmoothing.%20In%20this%20work%2C%20we%20reduce%20the%20chances%20of%20sampling%20adversarial%20content%0Ainjected%20by%20malware%20authors%20by%20selecting%20correlated%20subsets%20of%20bytes%2C%20rather%0Athan%20using%20Gaussian%20noise%20to%20randomize%20inputs%20like%20in%20the%20Computer%20Vision%20%28CV%29%0Adomain.%20During%20training%2C%20our%20ablation-based%20smoothing%20scheme%20trains%20a%20base%0Aclassifier%20to%20make%20classifications%20on%20a%20subset%20of%20contiguous%20bytes%20or%20chunk%20of%0Abytes.%20At%20test%20time%2C%20a%20large%20number%20of%20chunks%20are%20then%20classified%20by%20a%20base%0Aclassifier%20and%20the%20consensus%20among%20these%20classifications%20is%20then%20reported%20as%0Athe%20final%20prediction.%20We%20propose%20two%20strategies%20to%20determine%20the%20location%20of%0Athe%20chunks%20used%20for%20classification%3A%20%281%29%20randomly%20selecting%20the%20locations%20of%20the%0Achunks%20and%20%282%29%20selecting%20contiguous%20adjacent%20chunks.%20To%20showcase%20the%0Aeffectiveness%20of%20our%20approach%2C%20we%20have%20trained%20two%20classifiers%20with%20our%0Achunk-based%20ablation%20schemes%20on%20the%20BODMAS%20dataset.%20Our%20findings%20reveal%20that%0Athe%20chunk-based%20smoothing%20classifiers%20exhibit%20greater%20resilience%20against%0Aadversarial%20malware%20examples%20generated%20with%20state-of-the-are%20evasion%20attacks%2C%0Aoutperforming%20a%20non-smoothed%20classifier%20and%20a%20randomized%20smoothing-based%0Aclassifier%20by%20a%20great%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15267v1&entry.124074799=Read"},
{"title": "Representing Online Handwriting for Recognition in Large Vision-Language\n  Models", "author": "Anastasiia Fadeeva and Philippe Schlattner and Andrii Maksai and Mark Collier and Efi Kokiopoulou and Jesse Berent and Claudiu Musat", "abstract": "  The adoption of tablets with touchscreens and styluses is increasing, and a\nkey feature is converting handwriting to text, enabling search, indexing, and\nAI assistance. Meanwhile, vision-language models (VLMs) are now the go-to\nsolution for image understanding, thanks to both their state-of-the-art\nperformance across a variety of tasks and the simplicity of a unified approach\nto training, fine-tuning, and inference. While VLMs obtain high performance on\nimage-based tasks, they perform poorly on handwriting recognition when applied\nnaively, i.e., by rendering handwriting as an image and performing optical\ncharacter recognition (OCR). In this paper, we study online handwriting\nrecognition with VLMs, going beyond naive OCR. We propose a novel tokenized\nrepresentation of digital ink (online handwriting) that includes both a\ntime-ordered sequence of strokes as text, and as image. We show that this\nrepresentation yields results comparable to or better than state-of-the-art\nonline handwriting recognizers. Wide applicability is shown through results\nwith two different VLM families, on multiple public datasets. Our approach can\nbe applied to off-the-shelf VLMs, does not require any changes in their\narchitecture, and can be used in both fine-tuning and parameter-efficient\ntuning. We perform a detailed ablation study to identify the key elements of\nthe proposed representation.\n", "link": "http://arxiv.org/abs/2402.15307v1", "date": "2024-02-23", "relevancy": 1.9615, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4987}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.492}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4854}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representing%20Online%20Handwriting%20for%20Recognition%20in%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Anastasiia%20Fadeeva%20and%20Philippe%20Schlattner%20and%20Andrii%20Maksai%20and%20Mark%20Collier%20and%20Efi%20Kokiopoulou%20and%20Jesse%20Berent%20and%20Claudiu%20Musat&entry.1292438233=%20%20The%20adoption%20of%20tablets%20with%20touchscreens%20and%20styluses%20is%20increasing%2C%20and%20a%0Akey%20feature%20is%20converting%20handwriting%20to%20text%2C%20enabling%20search%2C%20indexing%2C%20and%0AAI%20assistance.%20Meanwhile%2C%20vision-language%20models%20%28VLMs%29%20are%20now%20the%20go-to%0Asolution%20for%20image%20understanding%2C%20thanks%20to%20both%20their%20state-of-the-art%0Aperformance%20across%20a%20variety%20of%20tasks%20and%20the%20simplicity%20of%20a%20unified%20approach%0Ato%20training%2C%20fine-tuning%2C%20and%20inference.%20While%20VLMs%20obtain%20high%20performance%20on%0Aimage-based%20tasks%2C%20they%20perform%20poorly%20on%20handwriting%20recognition%20when%20applied%0Anaively%2C%20i.e.%2C%20by%20rendering%20handwriting%20as%20an%20image%20and%20performing%20optical%0Acharacter%20recognition%20%28OCR%29.%20In%20this%20paper%2C%20we%20study%20online%20handwriting%0Arecognition%20with%20VLMs%2C%20going%20beyond%20naive%20OCR.%20We%20propose%20a%20novel%20tokenized%0Arepresentation%20of%20digital%20ink%20%28online%20handwriting%29%20that%20includes%20both%20a%0Atime-ordered%20sequence%20of%20strokes%20as%20text%2C%20and%20as%20image.%20We%20show%20that%20this%0Arepresentation%20yields%20results%20comparable%20to%20or%20better%20than%20state-of-the-art%0Aonline%20handwriting%20recognizers.%20Wide%20applicability%20is%20shown%20through%20results%0Awith%20two%20different%20VLM%20families%2C%20on%20multiple%20public%20datasets.%20Our%20approach%20can%0Abe%20applied%20to%20off-the-shelf%20VLMs%2C%20does%20not%20require%20any%20changes%20in%20their%0Aarchitecture%2C%20and%20can%20be%20used%20in%20both%20fine-tuning%20and%20parameter-efficient%0Atuning.%20We%20perform%20a%20detailed%20ablation%20study%20to%20identify%20the%20key%20elements%20of%0Athe%20proposed%20representation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15307v1&entry.124074799=Read"},
{"title": "Centaur: Federated Learning for Constrained Edge Devices", "author": "Fan Mo and Mohammad Malekzadeh and Soumyajit Chatterjee and Fahim Kawsar and Akhil Mathur", "abstract": "  Federated learning (FL) facilitates new applications at the edge, especially\nfor wearable and Internet-of-Thing devices. Such devices capture a large and\ndiverse amount of data, but they have memory, compute, power, and connectivity\nconstraints which hinder their participation in FL. We propose Centaur, a\nmultitier FL framework, enabling ultra-constrained devices to efficiently\nparticipate in FL on large neural nets. Centaur combines two major ideas: (i) a\ndata selection scheme to choose a portion of samples that accelerates the\nlearning, and (ii) a partition-based training algorithm that integrates both\nconstrained and powerful devices owned by the same user. Evaluations, on four\nbenchmark neural nets and three datasets, show that Centaur gains ~10\\% higher\naccuracy than local training on constrained devices with ~58\\% energy saving on\naverage. Our experimental results also demonstrate the superior efficiency of\nCentaur when dealing with imbalanced data, client participation heterogeneity,\nand various network connection probabilities.\n", "link": "http://arxiv.org/abs/2211.04175v3", "date": "2024-02-23", "relevancy": 1.9603, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5024}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4826}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4779}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Centaur%3A%20Federated%20Learning%20for%20Constrained%20Edge%20Devices&entry.906535625=Fan%20Mo%20and%20Mohammad%20Malekzadeh%20and%20Soumyajit%20Chatterjee%20and%20Fahim%20Kawsar%20and%20Akhil%20Mathur&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20facilitates%20new%20applications%20at%20the%20edge%2C%20especially%0Afor%20wearable%20and%20Internet-of-Thing%20devices.%20Such%20devices%20capture%20a%20large%20and%0Adiverse%20amount%20of%20data%2C%20but%20they%20have%20memory%2C%20compute%2C%20power%2C%20and%20connectivity%0Aconstraints%20which%20hinder%20their%20participation%20in%20FL.%20We%20propose%20Centaur%2C%20a%0Amultitier%20FL%20framework%2C%20enabling%20ultra-constrained%20devices%20to%20efficiently%0Aparticipate%20in%20FL%20on%20large%20neural%20nets.%20Centaur%20combines%20two%20major%20ideas%3A%20%28i%29%20a%0Adata%20selection%20scheme%20to%20choose%20a%20portion%20of%20samples%20that%20accelerates%20the%0Alearning%2C%20and%20%28ii%29%20a%20partition-based%20training%20algorithm%20that%20integrates%20both%0Aconstrained%20and%20powerful%20devices%20owned%20by%20the%20same%20user.%20Evaluations%2C%20on%20four%0Abenchmark%20neural%20nets%20and%20three%20datasets%2C%20show%20that%20Centaur%20gains%20~10%5C%25%20higher%0Aaccuracy%20than%20local%20training%20on%20constrained%20devices%20with%20~58%5C%25%20energy%20saving%20on%0Aaverage.%20Our%20experimental%20results%20also%20demonstrate%20the%20superior%20efficiency%20of%0ACentaur%20when%20dealing%20with%20imbalanced%20data%2C%20client%20participation%20heterogeneity%2C%0Aand%20various%20network%20connection%20probabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.04175v3&entry.124074799=Read"},
{"title": "Score-based generative models break the curse of dimensionality in\n  learning a family of sub-Gaussian probability distributions", "author": "Frank Cole and Yulong Lu", "abstract": "  While score-based generative models (SGMs) have achieved remarkable success\nin enormous image generation tasks, their mathematical foundations are still\nlimited. In this paper, we analyze the approximation and generalization of SGMs\nin learning a family of sub-Gaussian probability distributions. We introduce a\nnotion of complexity for probability distributions in terms of their relative\ndensity with respect to the standard Gaussian measure. We prove that if the\nlog-relative density can be locally approximated by a neural network whose\nparameters can be suitably bounded, then the distribution generated by\nempirical score matching approximates the target distribution in total\nvariation with a dimension-independent rate. We illustrate our theory through\nexamples, which include certain mixtures of Gaussians. An essential ingredient\nof our proof is to derive a dimension-free deep neural network approximation\nrate for the true score function associated with the forward process, which is\ninteresting in its own right.\n", "link": "http://arxiv.org/abs/2402.08082v3", "date": "2024-02-23", "relevancy": 1.9544, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5158}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4809}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4397}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Score-based%20generative%20models%20break%20the%20curse%20of%20dimensionality%20in%0A%20%20learning%20a%20family%20of%20sub-Gaussian%20probability%20distributions&entry.906535625=Frank%20Cole%20and%20Yulong%20Lu&entry.1292438233=%20%20While%20score-based%20generative%20models%20%28SGMs%29%20have%20achieved%20remarkable%20success%0Ain%20enormous%20image%20generation%20tasks%2C%20their%20mathematical%20foundations%20are%20still%0Alimited.%20In%20this%20paper%2C%20we%20analyze%20the%20approximation%20and%20generalization%20of%20SGMs%0Ain%20learning%20a%20family%20of%20sub-Gaussian%20probability%20distributions.%20We%20introduce%20a%0Anotion%20of%20complexity%20for%20probability%20distributions%20in%20terms%20of%20their%20relative%0Adensity%20with%20respect%20to%20the%20standard%20Gaussian%20measure.%20We%20prove%20that%20if%20the%0Alog-relative%20density%20can%20be%20locally%20approximated%20by%20a%20neural%20network%20whose%0Aparameters%20can%20be%20suitably%20bounded%2C%20then%20the%20distribution%20generated%20by%0Aempirical%20score%20matching%20approximates%20the%20target%20distribution%20in%20total%0Avariation%20with%20a%20dimension-independent%20rate.%20We%20illustrate%20our%20theory%20through%0Aexamples%2C%20which%20include%20certain%20mixtures%20of%20Gaussians.%20An%20essential%20ingredient%0Aof%20our%20proof%20is%20to%20derive%20a%20dimension-free%20deep%20neural%20network%20approximation%0Arate%20for%20the%20true%20score%20function%20associated%20with%20the%20forward%20process%2C%20which%20is%0Ainteresting%20in%20its%20own%20right.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08082v3&entry.124074799=Read"},
{"title": "Factored Online Planning in Many-Agent POMDPs", "author": "Maris F. L. Galesloot and Thiago D. Sim\u00e3o and Sebastian Junges and Nils Jansen", "abstract": "  In centralized multi-agent systems, often modeled as multi-agent partially\nobservable Markov decision processes (MPOMDPs), the action and observation\nspaces grow exponentially with the number of agents, making the value and\nbelief estimation of single-agent online planning ineffective. Prior work\npartially tackles value estimation by exploiting the inherent structure of\nmulti-agent settings via so-called coordination graphs. Additionally, belief\nestimation methods have been improved by incorporating the likelihood of\nobservations into the approximation. However, the challenges of value\nestimation and belief estimation have only been tackled individually, which\nprevents existing methods from scaling to settings with many agents. Therefore,\nwe address these challenges simultaneously. First, we introduce weighted\nparticle filtering to a sample-based online planner for MPOMDPs. Second, we\npresent a scalable approximation of the belief. Third, we bring an approach\nthat exploits the typical locality of agent interactions to novel online\nplanning algorithms for MPOMDPs operating on a so-called sparse particle filter\ntree. Our experimental evaluation against several state-of-the-art baselines\nshows that our methods (1) are competitive in settings with only a few agents\nand (2) improve over the baselines in the presence of many agents.\n", "link": "http://arxiv.org/abs/2312.11434v3", "date": "2024-02-23", "relevancy": 1.9524, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5597}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4876}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.46}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factored%20Online%20Planning%20in%20Many-Agent%20POMDPs&entry.906535625=Maris%20F.%20L.%20Galesloot%20and%20Thiago%20D.%20Sim%C3%A3o%20and%20Sebastian%20Junges%20and%20Nils%20Jansen&entry.1292438233=%20%20In%20centralized%20multi-agent%20systems%2C%20often%20modeled%20as%20multi-agent%20partially%0Aobservable%20Markov%20decision%20processes%20%28MPOMDPs%29%2C%20the%20action%20and%20observation%0Aspaces%20grow%20exponentially%20with%20the%20number%20of%20agents%2C%20making%20the%20value%20and%0Abelief%20estimation%20of%20single-agent%20online%20planning%20ineffective.%20Prior%20work%0Apartially%20tackles%20value%20estimation%20by%20exploiting%20the%20inherent%20structure%20of%0Amulti-agent%20settings%20via%20so-called%20coordination%20graphs.%20Additionally%2C%20belief%0Aestimation%20methods%20have%20been%20improved%20by%20incorporating%20the%20likelihood%20of%0Aobservations%20into%20the%20approximation.%20However%2C%20the%20challenges%20of%20value%0Aestimation%20and%20belief%20estimation%20have%20only%20been%20tackled%20individually%2C%20which%0Aprevents%20existing%20methods%20from%20scaling%20to%20settings%20with%20many%20agents.%20Therefore%2C%0Awe%20address%20these%20challenges%20simultaneously.%20First%2C%20we%20introduce%20weighted%0Aparticle%20filtering%20to%20a%20sample-based%20online%20planner%20for%20MPOMDPs.%20Second%2C%20we%0Apresent%20a%20scalable%20approximation%20of%20the%20belief.%20Third%2C%20we%20bring%20an%20approach%0Athat%20exploits%20the%20typical%20locality%20of%20agent%20interactions%20to%20novel%20online%0Aplanning%20algorithms%20for%20MPOMDPs%20operating%20on%20a%20so-called%20sparse%20particle%20filter%0Atree.%20Our%20experimental%20evaluation%20against%20several%20state-of-the-art%20baselines%0Ashows%20that%20our%20methods%20%281%29%20are%20competitive%20in%20settings%20with%20only%20a%20few%20agents%0Aand%20%282%29%20improve%20over%20the%20baselines%20in%20the%20presence%20of%20many%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11434v3&entry.124074799=Read"},
{"title": "CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid\n  Dynamics", "author": "Yining Luo and Yingfa Chen and Zhen Zhang", "abstract": "  In recent years, applying deep learning to solve physics problems has\nattracted much attention. Data-driven deep learning methods produce fast\nnumerical operators that can learn approximate solutions to the whole system of\npartial differential equations (i.e., surrogate modeling). Although these\nneural networks may have lower accuracy than traditional numerical methods,\nthey, once trained, are orders of magnitude faster at inference. Hence, one\ncrucial feature is that these operators can generalize to unseen PDE parameters\nwithout expensive re-training.In this paper, we construct CFDBench, a benchmark\ntailored for evaluating the generalization ability of neural operators after\ntraining in computational fluid dynamics (CFD) problems. It features four\nclassic CFD problems: lid-driven cavity flow, laminar boundary layer flow in\ncircular tubes, dam flows through the steps, and periodic Karman vortex street.\nThe data contains a total of 302K frames of velocity and pressure fields,\ninvolving 739 cases with different operating condition parameters, generated\nwith numerical methods. We evaluate the effectiveness of popular neural\noperators including feed-forward networks, DeepONet, FNO, U-Net, etc. on\nCFDBnech by predicting flows with non-periodic boundary conditions, fluid\nproperties, and flow domain shapes that are not seen during training.\nAppropriate modifications were made to apply popular deep neural networks to\nCFDBench and enable the accommodation of more changing inputs. Empirical\nresults on CFDBench show many baseline models have errors as high as 300% in\nsome problems, and severe error accumulation when performing autoregressive\ninference. CFDBench facilitates a more comprehensive comparison between\ndifferent neural operators for CFD compared to existing benchmarks.\n", "link": "http://arxiv.org/abs/2310.05963v2", "date": "2024-02-23", "relevancy": 1.9427, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5334}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4646}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4464}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CFDBench%3A%20A%20Large-Scale%20Benchmark%20for%20Machine%20Learning%20Methods%20in%20Fluid%0A%20%20Dynamics&entry.906535625=Yining%20Luo%20and%20Yingfa%20Chen%20and%20Zhen%20Zhang&entry.1292438233=%20%20In%20recent%20years%2C%20applying%20deep%20learning%20to%20solve%20physics%20problems%20has%0Aattracted%20much%20attention.%20Data-driven%20deep%20learning%20methods%20produce%20fast%0Anumerical%20operators%20that%20can%20learn%20approximate%20solutions%20to%20the%20whole%20system%20of%0Apartial%20differential%20equations%20%28i.e.%2C%20surrogate%20modeling%29.%20Although%20these%0Aneural%20networks%20may%20have%20lower%20accuracy%20than%20traditional%20numerical%20methods%2C%0Athey%2C%20once%20trained%2C%20are%20orders%20of%20magnitude%20faster%20at%20inference.%20Hence%2C%20one%0Acrucial%20feature%20is%20that%20these%20operators%20can%20generalize%20to%20unseen%20PDE%20parameters%0Awithout%20expensive%20re-training.In%20this%20paper%2C%20we%20construct%20CFDBench%2C%20a%20benchmark%0Atailored%20for%20evaluating%20the%20generalization%20ability%20of%20neural%20operators%20after%0Atraining%20in%20computational%20fluid%20dynamics%20%28CFD%29%20problems.%20It%20features%20four%0Aclassic%20CFD%20problems%3A%20lid-driven%20cavity%20flow%2C%20laminar%20boundary%20layer%20flow%20in%0Acircular%20tubes%2C%20dam%20flows%20through%20the%20steps%2C%20and%20periodic%20Karman%20vortex%20street.%0AThe%20data%20contains%20a%20total%20of%20302K%20frames%20of%20velocity%20and%20pressure%20fields%2C%0Ainvolving%20739%20cases%20with%20different%20operating%20condition%20parameters%2C%20generated%0Awith%20numerical%20methods.%20We%20evaluate%20the%20effectiveness%20of%20popular%20neural%0Aoperators%20including%20feed-forward%20networks%2C%20DeepONet%2C%20FNO%2C%20U-Net%2C%20etc.%20on%0ACFDBnech%20by%20predicting%20flows%20with%20non-periodic%20boundary%20conditions%2C%20fluid%0Aproperties%2C%20and%20flow%20domain%20shapes%20that%20are%20not%20seen%20during%20training.%0AAppropriate%20modifications%20were%20made%20to%20apply%20popular%20deep%20neural%20networks%20to%0ACFDBench%20and%20enable%20the%20accommodation%20of%20more%20changing%20inputs.%20Empirical%0Aresults%20on%20CFDBench%20show%20many%20baseline%20models%20have%20errors%20as%20high%20as%20300%25%20in%0Asome%20problems%2C%20and%20severe%20error%20accumulation%20when%20performing%20autoregressive%0Ainference.%20CFDBench%20facilitates%20a%20more%20comprehensive%20comparison%20between%0Adifferent%20neural%20operators%20for%20CFD%20compared%20to%20existing%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05963v2&entry.124074799=Read"},
{"title": "Optimisic Information Directed Sampling", "author": "Gergely Neu and Matteo Papini and Ludovic Schwartz", "abstract": "  We study the problem of online learning in contextual bandit problems where\nthe loss function is assumed to belong to a known parametric function class. We\npropose a new analytic framework for this setting that bridges the Bayesian\ntheory of information-directed sampling due to Russo and Van Roy (2018) and the\nworst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the\ndecision-estimation coefficient. Drawing from both lines of work, we propose a\nalgorithmic template called Optimistic Information-Directed Sampling and show\nthat it can achieve instance-dependent regret guarantees similar to the ones\nachievable by the classic Bayesian IDS method, but with the major advantage of\nnot requiring any Bayesian assumptions. The key technical innovation of our\nanalysis is introducing an optimistic surrogate model for the regret and using\nit to define a frequentist version of the Information Ratio of Russo and Van\nRoy (2018), and a less conservative version of the Decision Estimation\nCoefficient of Foster et al. (2021). Keywords: Contextual bandits,\ninformation-directed sampling, decision estimation coefficient, first-order\nregret bounds.\n", "link": "http://arxiv.org/abs/2402.15411v1", "date": "2024-02-23", "relevancy": 1.9392, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5115}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4816}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4773}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimisic%20Information%20Directed%20Sampling&entry.906535625=Gergely%20Neu%20and%20Matteo%20Papini%20and%20Ludovic%20Schwartz&entry.1292438233=%20%20We%20study%20the%20problem%20of%20online%20learning%20in%20contextual%20bandit%20problems%20where%0Athe%20loss%20function%20is%20assumed%20to%20belong%20to%20a%20known%20parametric%20function%20class.%20We%0Apropose%20a%20new%20analytic%20framework%20for%20this%20setting%20that%20bridges%20the%20Bayesian%0Atheory%20of%20information-directed%20sampling%20due%20to%20Russo%20and%20Van%20Roy%20%282018%29%20and%20the%0Aworst-case%20theory%20of%20Foster%2C%20Kakade%2C%20Qian%2C%20and%20Rakhlin%20%282021%29%20based%20on%20the%0Adecision-estimation%20coefficient.%20Drawing%20from%20both%20lines%20of%20work%2C%20we%20propose%20a%0Aalgorithmic%20template%20called%20Optimistic%20Information-Directed%20Sampling%20and%20show%0Athat%20it%20can%20achieve%20instance-dependent%20regret%20guarantees%20similar%20to%20the%20ones%0Aachievable%20by%20the%20classic%20Bayesian%20IDS%20method%2C%20but%20with%20the%20major%20advantage%20of%0Anot%20requiring%20any%20Bayesian%20assumptions.%20The%20key%20technical%20innovation%20of%20our%0Aanalysis%20is%20introducing%20an%20optimistic%20surrogate%20model%20for%20the%20regret%20and%20using%0Ait%20to%20define%20a%20frequentist%20version%20of%20the%20Information%20Ratio%20of%20Russo%20and%20Van%0ARoy%20%282018%29%2C%20and%20a%20less%20conservative%20version%20of%20the%20Decision%20Estimation%0ACoefficient%20of%20Foster%20et%20al.%20%282021%29.%20Keywords%3A%20Contextual%20bandits%2C%0Ainformation-directed%20sampling%2C%20decision%20estimation%20coefficient%2C%20first-order%0Aregret%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15411v1&entry.124074799=Read"},
{"title": "Machine unlearning through fine-grained model parameters perturbation", "author": "Zhiwei Zuo and Zhuo Tang and Kenli Li and Anwitaman Datta", "abstract": "  Machine unlearning techniques, which involve retracting data records and\nreducing influence of said data on trained models, help with the user privacy\nprotection objective but incur significant computational costs. Weight\nperturbation-based unlearning is a general approach, but it typically involves\nglobally modifying the parameters. We propose fine-grained Top-K and Random-k\nparameters perturbed inexact machine unlearning strategies that address the\nprivacy needs while keeping the computational costs tractable.\n  In order to demonstrate the efficacy of our strategies we also tackle the\nchallenge of evaluating the effectiveness of machine unlearning by considering\nthe model's generalization performance across both unlearning and remaining\ndata. To better assess the unlearning effect and model generalization, we\npropose novel metrics, namely, the forgetting rate and memory retention rate.\nHowever, for inexact machine unlearning, current metrics are inadequate in\nquantifying the degree of forgetting that occurs after unlearning strategies\nare applied. To address this, we introduce SPD-GAN, which subtly perturbs the\ndistribution of data targeted for unlearning. Then, we evaluate the degree of\nunlearning by measuring the performance difference of the models on the\nperturbed unlearning data before and after the unlearning process. By\nimplementing these innovative techniques and metrics, we achieve\ncomputationally efficacious privacy protection in machine learning applications\nwithout significant sacrifice of model performance. Furthermore, this approach\nprovides a novel method for evaluating the degree of unlearning.\n", "link": "http://arxiv.org/abs/2401.04385v2", "date": "2024-02-23", "relevancy": 1.937, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5109}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4815}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4763}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20unlearning%20through%20fine-grained%20model%20parameters%20perturbation&entry.906535625=Zhiwei%20Zuo%20and%20Zhuo%20Tang%20and%20Kenli%20Li%20and%20Anwitaman%20Datta&entry.1292438233=%20%20Machine%20unlearning%20techniques%2C%20which%20involve%20retracting%20data%20records%20and%0Areducing%20influence%20of%20said%20data%20on%20trained%20models%2C%20help%20with%20the%20user%20privacy%0Aprotection%20objective%20but%20incur%20significant%20computational%20costs.%20Weight%0Aperturbation-based%20unlearning%20is%20a%20general%20approach%2C%20but%20it%20typically%20involves%0Aglobally%20modifying%20the%20parameters.%20We%20propose%20fine-grained%20Top-K%20and%20Random-k%0Aparameters%20perturbed%20inexact%20machine%20unlearning%20strategies%20that%20address%20the%0Aprivacy%20needs%20while%20keeping%20the%20computational%20costs%20tractable.%0A%20%20In%20order%20to%20demonstrate%20the%20efficacy%20of%20our%20strategies%20we%20also%20tackle%20the%0Achallenge%20of%20evaluating%20the%20effectiveness%20of%20machine%20unlearning%20by%20considering%0Athe%20model%27s%20generalization%20performance%20across%20both%20unlearning%20and%20remaining%0Adata.%20To%20better%20assess%20the%20unlearning%20effect%20and%20model%20generalization%2C%20we%0Apropose%20novel%20metrics%2C%20namely%2C%20the%20forgetting%20rate%20and%20memory%20retention%20rate.%0AHowever%2C%20for%20inexact%20machine%20unlearning%2C%20current%20metrics%20are%20inadequate%20in%0Aquantifying%20the%20degree%20of%20forgetting%20that%20occurs%20after%20unlearning%20strategies%0Aare%20applied.%20To%20address%20this%2C%20we%20introduce%20SPD-GAN%2C%20which%20subtly%20perturbs%20the%0Adistribution%20of%20data%20targeted%20for%20unlearning.%20Then%2C%20we%20evaluate%20the%20degree%20of%0Aunlearning%20by%20measuring%20the%20performance%20difference%20of%20the%20models%20on%20the%0Aperturbed%20unlearning%20data%20before%20and%20after%20the%20unlearning%20process.%20By%0Aimplementing%20these%20innovative%20techniques%20and%20metrics%2C%20we%20achieve%0Acomputationally%20efficacious%20privacy%20protection%20in%20machine%20learning%20applications%0Awithout%20significant%20sacrifice%20of%20model%20performance.%20Furthermore%2C%20this%20approach%0Aprovides%20a%20novel%20method%20for%20evaluating%20the%20degree%20of%20unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04385v2&entry.124074799=Read"},
{"title": "Real-Time FPGA Demonstrator of ANN-Based Equalization for Optical\n  Communications", "author": "Jonas Ney and Patrick Matalla and Vincent Lauinger and Laurent Schmalen and Sebastian Randel and Norbert Wehn", "abstract": "  In this work, we present a high-throughput field programmable gate array\n(FPGA) demonstrator of an artificial neural network (ANN)-based equalizer. The\nequalization is performed and illustrated in real-time for a 30 GBd, two-level\npulse amplitude modulation (PAM2) optical communication system.\n", "link": "http://arxiv.org/abs/2402.15288v1", "date": "2024-02-23", "relevancy": 1.9347, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3915}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3858}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3835}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20FPGA%20Demonstrator%20of%20ANN-Based%20Equalization%20for%20Optical%0A%20%20Communications&entry.906535625=Jonas%20Ney%20and%20Patrick%20Matalla%20and%20Vincent%20Lauinger%20and%20Laurent%20Schmalen%20and%20Sebastian%20Randel%20and%20Norbert%20Wehn&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20high-throughput%20field%20programmable%20gate%20array%0A%28FPGA%29%20demonstrator%20of%20an%20artificial%20neural%20network%20%28ANN%29-based%20equalizer.%20The%0Aequalization%20is%20performed%20and%20illustrated%20in%20real-time%20for%20a%2030%20GBd%2C%20two-level%0Apulse%20amplitude%20modulation%20%28PAM2%29%20optical%20communication%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15288v1&entry.124074799=Read"},
{"title": "Towards Principled Task Grouping for Multi-Task Learning", "author": "Chenguang Wang and Xuanhao Pan and Tianshu Yu", "abstract": "  This paper presents a novel approach to task grouping in Multitask Learning\n(MTL), advancing beyond existing methods by addressing key theoretical and\npractical limitations. Unlike prior studies, our approach offers a more\ntheoretically grounded method that does not rely on restrictive assumptions for\nconstructing transfer gains. We also propose a flexible mathematical\nprogramming formulation which can accommodate a wide spectrum of resource\nconstraints, thus enhancing its versatility. Experimental results across\ndiverse domains, including computer vision datasets, combinatorial optimization\nbenchmarks and time series tasks, demonstrate the superiority of our method\nover extensive baselines, validating its effectiveness and general\napplicability in MTL.\n", "link": "http://arxiv.org/abs/2402.15328v1", "date": "2024-02-23", "relevancy": 1.9319, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4865}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4835}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4792}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Principled%20Task%20Grouping%20for%20Multi-Task%20Learning&entry.906535625=Chenguang%20Wang%20and%20Xuanhao%20Pan%20and%20Tianshu%20Yu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20task%20grouping%20in%20Multitask%20Learning%0A%28MTL%29%2C%20advancing%20beyond%20existing%20methods%20by%20addressing%20key%20theoretical%20and%0Apractical%20limitations.%20Unlike%20prior%20studies%2C%20our%20approach%20offers%20a%20more%0Atheoretically%20grounded%20method%20that%20does%20not%20rely%20on%20restrictive%20assumptions%20for%0Aconstructing%20transfer%20gains.%20We%20also%20propose%20a%20flexible%20mathematical%0Aprogramming%20formulation%20which%20can%20accommodate%20a%20wide%20spectrum%20of%20resource%0Aconstraints%2C%20thus%20enhancing%20its%20versatility.%20Experimental%20results%20across%0Adiverse%20domains%2C%20including%20computer%20vision%20datasets%2C%20combinatorial%20optimization%0Abenchmarks%20and%20time%20series%20tasks%2C%20demonstrate%20the%20superiority%20of%20our%20method%0Aover%20extensive%20baselines%2C%20validating%20its%20effectiveness%20and%20general%0Aapplicability%20in%20MTL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15328v1&entry.124074799=Read"},
{"title": "Towards Principled Task Grouping for Multi-Task Learning", "author": "Chenguang Wang and Xuanhao Pan and Tianshu Yu", "abstract": "  This paper presents a novel approach to task grouping in Multitask Learning\n(MTL), advancing beyond existing methods by addressing key theoretical and\npractical limitations. Unlike prior studies, our approach offers a more\ntheoretically grounded method that does not rely on restrictive assumptions for\nconstructing transfer gains. We also propose a flexible mathematical\nprogramming formulation which can accommodate a wide spectrum of resource\nconstraints, thus enhancing its versatility. Experimental results across\ndiverse domains, including computer vision datasets, combinatorial optimization\nbenchmarks and time series tasks, demonstrate the superiority of our method\nover extensive baselines, validating its effectiveness and general\napplicability in MTL.\n", "link": "http://arxiv.org/abs/2402.15328v1", "date": "2024-02-23", "relevancy": 1.9319, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4865}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4835}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4792}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Principled%20Task%20Grouping%20for%20Multi-Task%20Learning&entry.906535625=Chenguang%20Wang%20and%20Xuanhao%20Pan%20and%20Tianshu%20Yu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20task%20grouping%20in%20Multitask%20Learning%0A%28MTL%29%2C%20advancing%20beyond%20existing%20methods%20by%20addressing%20key%20theoretical%20and%0Apractical%20limitations.%20Unlike%20prior%20studies%2C%20our%20approach%20offers%20a%20more%0Atheoretically%20grounded%20method%20that%20does%20not%20rely%20on%20restrictive%20assumptions%20for%0Aconstructing%20transfer%20gains.%20We%20also%20propose%20a%20flexible%20mathematical%0Aprogramming%20formulation%20which%20can%20accommodate%20a%20wide%20spectrum%20of%20resource%0Aconstraints%2C%20thus%20enhancing%20its%20versatility.%20Experimental%20results%20across%0Adiverse%20domains%2C%20including%20computer%20vision%20datasets%2C%20combinatorial%20optimization%0Abenchmarks%20and%20time%20series%20tasks%2C%20demonstrate%20the%20superiority%20of%20our%20method%0Aover%20extensive%20baselines%2C%20validating%20its%20effectiveness%20and%20general%0Aapplicability%20in%20MTL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15328v1&entry.124074799=Read"},
{"title": "Dual Encoder: Exploiting the Potential of Syntactic and Semantic for\n  Aspect Sentiment Triplet Extraction", "author": "Xiaowei Zhao and Yong Zhou and Xiujuan Xu", "abstract": "  Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained\nsentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to\nmodel the syntax-semantic relationships inherent in triplet elements. However,\nthey have yet to fully tap into the vast potential of syntactic and semantic\ninformation within the ASTE task. In this work, we propose a \\emph{Dual\nEncoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S),\nwhich maximizes the syntactic and semantic relationships among words.\nSpecifically, our model utilizes a dual-channel encoder with a BERT channel to\ncapture semantic information, and an enhanced LSTM channel for comprehensive\nsyntactic information capture. Subsequently, we introduce the heterogeneous\nfeature interaction module to capture intricate interactions between dependency\nsyntax and attention semantics, and to dynamically select vital nodes. We\nleverage the synergy of these modules to harness the significant potential of\nsyntactic and semantic information in ASTE tasks. Testing on public benchmarks,\nour D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its\neffectiveness.\n", "link": "http://arxiv.org/abs/2402.15370v1", "date": "2024-02-23", "relevancy": 1.9315, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4889}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4797}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4758}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Encoder%3A%20Exploiting%20the%20Potential%20of%20Syntactic%20and%20Semantic%20for%0A%20%20Aspect%20Sentiment%20Triplet%20Extraction&entry.906535625=Xiaowei%20Zhao%20and%20Yong%20Zhou%20and%20Xiujuan%20Xu&entry.1292438233=%20%20Aspect%20Sentiment%20Triple%20Extraction%20%28ASTE%29%20is%20an%20emerging%20task%20in%20fine-grained%0Asentiment%20analysis.%20Recent%20studies%20have%20employed%20Graph%20Neural%20Networks%20%28GNN%29%20to%0Amodel%20the%20syntax-semantic%20relationships%20inherent%20in%20triplet%20elements.%20However%2C%0Athey%20have%20yet%20to%20fully%20tap%20into%20the%20vast%20potential%20of%20syntactic%20and%20semantic%0Ainformation%20within%20the%20ASTE%20task.%20In%20this%20work%2C%20we%20propose%20a%20%5Cemph%7BDual%0AEncoder%3A%20Exploiting%20the%20potential%20of%20Syntactic%20and%20Semantic%7D%20model%20%28D2E2S%29%2C%0Awhich%20maximizes%20the%20syntactic%20and%20semantic%20relationships%20among%20words.%0ASpecifically%2C%20our%20model%20utilizes%20a%20dual-channel%20encoder%20with%20a%20BERT%20channel%20to%0Acapture%20semantic%20information%2C%20and%20an%20enhanced%20LSTM%20channel%20for%20comprehensive%0Asyntactic%20information%20capture.%20Subsequently%2C%20we%20introduce%20the%20heterogeneous%0Afeature%20interaction%20module%20to%20capture%20intricate%20interactions%20between%20dependency%0Asyntax%20and%20attention%20semantics%2C%20and%20to%20dynamically%20select%20vital%20nodes.%20We%0Aleverage%20the%20synergy%20of%20these%20modules%20to%20harness%20the%20significant%20potential%20of%0Asyntactic%20and%20semantic%20information%20in%20ASTE%20tasks.%20Testing%20on%20public%20benchmarks%2C%0Aour%20D2E2S%20model%20surpasses%20the%20current%20state-of-the-art%28SOTA%29%2C%20demonstrating%20its%0Aeffectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15370v1&entry.124074799=Read"},
{"title": "Learning Action Embeddings for Off-Policy Evaluation", "author": "Matej Cief and Jacek Golebiowski and Philipp Schmidt and Ziawasch Abedjan and Artur Bekasov", "abstract": "  Off-policy evaluation (OPE) methods allow us to compute the expected reward\nof a policy by using the logged data collected by a different policy. OPE is a\nviable alternative to running expensive online A/B tests: it can speed up the\ndevelopment of new policies, and reduces the risk of exposing customers to\nsuboptimal treatments. However, when the number of actions is large, or certain\nactions are under-explored by the logging policy, existing estimators based on\ninverse-propensity scoring (IPS) can have a high or even infinite variance.\nSaito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS)\nthat uses action embeddings instead, which reduces the variance of IPS in large\naction spaces. MIPS assumes that good action embeddings can be defined by the\npractitioner, which is difficult to do in many real-world applications. In this\nwork, we explore learning action embeddings from logged data. In particular, we\nuse intermediate outputs of a trained reward model to define action embeddings\nfor MIPS. This approach extends MIPS to more applications, and in our\nexperiments improves upon MIPS with pre-defined embeddings, as well as standard\nbaselines, both on synthetic and real-world data. Our method does not make\nassumptions about the reward model class, and supports using additional action\ninformation to further improve the estimates. The proposed approach presents an\nappealing alternative to DR for combining the low variance of DM with the low\nbias of IPS.\n", "link": "http://arxiv.org/abs/2305.03954v2", "date": "2024-02-23", "relevancy": 1.9276, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.481}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4629}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Action%20Embeddings%20for%20Off-Policy%20Evaluation&entry.906535625=Matej%20Cief%20and%20Jacek%20Golebiowski%20and%20Philipp%20Schmidt%20and%20Ziawasch%20Abedjan%20and%20Artur%20Bekasov&entry.1292438233=%20%20Off-policy%20evaluation%20%28OPE%29%20methods%20allow%20us%20to%20compute%20the%20expected%20reward%0Aof%20a%20policy%20by%20using%20the%20logged%20data%20collected%20by%20a%20different%20policy.%20OPE%20is%20a%0Aviable%20alternative%20to%20running%20expensive%20online%20A/B%20tests%3A%20it%20can%20speed%20up%20the%0Adevelopment%20of%20new%20policies%2C%20and%20reduces%20the%20risk%20of%20exposing%20customers%20to%0Asuboptimal%20treatments.%20However%2C%20when%20the%20number%20of%20actions%20is%20large%2C%20or%20certain%0Aactions%20are%20under-explored%20by%20the%20logging%20policy%2C%20existing%20estimators%20based%20on%0Ainverse-propensity%20scoring%20%28IPS%29%20can%20have%20a%20high%20or%20even%20infinite%20variance.%0ASaito%20and%20Joachims%20%28arXiv%3A2202.06317v2%20%5Bcs.LG%5D%29%20propose%20marginalized%20IPS%20%28MIPS%29%0Athat%20uses%20action%20embeddings%20instead%2C%20which%20reduces%20the%20variance%20of%20IPS%20in%20large%0Aaction%20spaces.%20MIPS%20assumes%20that%20good%20action%20embeddings%20can%20be%20defined%20by%20the%0Apractitioner%2C%20which%20is%20difficult%20to%20do%20in%20many%20real-world%20applications.%20In%20this%0Awork%2C%20we%20explore%20learning%20action%20embeddings%20from%20logged%20data.%20In%20particular%2C%20we%0Ause%20intermediate%20outputs%20of%20a%20trained%20reward%20model%20to%20define%20action%20embeddings%0Afor%20MIPS.%20This%20approach%20extends%20MIPS%20to%20more%20applications%2C%20and%20in%20our%0Aexperiments%20improves%20upon%20MIPS%20with%20pre-defined%20embeddings%2C%20as%20well%20as%20standard%0Abaselines%2C%20both%20on%20synthetic%20and%20real-world%20data.%20Our%20method%20does%20not%20make%0Aassumptions%20about%20the%20reward%20model%20class%2C%20and%20supports%20using%20additional%20action%0Ainformation%20to%20further%20improve%20the%20estimates.%20The%20proposed%20approach%20presents%20an%0Aappealing%20alternative%20to%20DR%20for%20combining%20the%20low%20variance%20of%20DM%20with%20the%20low%0Abias%20of%20IPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.03954v2&entry.124074799=Read"},
{"title": "Bernstein Flows for Flexible Posteriors in Variational Bayes", "author": "Oliver D\u00fcrr and Stephan H\u00f6rling and Daniel Dold and Ivonne Kovylov and Beate Sick", "abstract": "  Variational inference (VI) is a technique to approximate difficult to compute\nposteriors by optimization. In contrast to MCMC, VI scales to many\nobservations. In the case of complex posteriors, however, state-of-the-art VI\napproaches often yield unsatisfactory posterior approximations. This paper\npresents Bernstein flow variational inference (BF-VI), a robust and easy-to-use\nmethod, flexible enough to approximate complex multivariate posteriors. BF-VI\ncombines ideas from normalizing flows and Bernstein polynomial-based\ntransformation models. In benchmark experiments, we compare BF-VI solutions\nwith exact posteriors, MCMC solutions, and state-of-the-art VI methods\nincluding normalizing flow based VI. We show for low-dimensional models that\nBF-VI accurately approximates the true posterior; in higher-dimensional models,\nBF-VI outperforms other VI methods. Further, we develop with BF-VI a Bayesian\nmodel for the semi-structured Melanoma challenge data, combining a CNN model\npart for image data with an interpretable model part for tabular data, and\ndemonstrate for the first time how the use of VI in semi-structured models.\n", "link": "http://arxiv.org/abs/2202.05650v2", "date": "2024-02-23", "relevancy": 1.9251, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5239}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4868}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4587}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bernstein%20Flows%20for%20Flexible%20Posteriors%20in%20Variational%20Bayes&entry.906535625=Oliver%20D%C3%BCrr%20and%20Stephan%20H%C3%B6rling%20and%20Daniel%20Dold%20and%20Ivonne%20Kovylov%20and%20Beate%20Sick&entry.1292438233=%20%20Variational%20inference%20%28VI%29%20is%20a%20technique%20to%20approximate%20difficult%20to%20compute%0Aposteriors%20by%20optimization.%20In%20contrast%20to%20MCMC%2C%20VI%20scales%20to%20many%0Aobservations.%20In%20the%20case%20of%20complex%20posteriors%2C%20however%2C%20state-of-the-art%20VI%0Aapproaches%20often%20yield%20unsatisfactory%20posterior%20approximations.%20This%20paper%0Apresents%20Bernstein%20flow%20variational%20inference%20%28BF-VI%29%2C%20a%20robust%20and%20easy-to-use%0Amethod%2C%20flexible%20enough%20to%20approximate%20complex%20multivariate%20posteriors.%20BF-VI%0Acombines%20ideas%20from%20normalizing%20flows%20and%20Bernstein%20polynomial-based%0Atransformation%20models.%20In%20benchmark%20experiments%2C%20we%20compare%20BF-VI%20solutions%0Awith%20exact%20posteriors%2C%20MCMC%20solutions%2C%20and%20state-of-the-art%20VI%20methods%0Aincluding%20normalizing%20flow%20based%20VI.%20We%20show%20for%20low-dimensional%20models%20that%0ABF-VI%20accurately%20approximates%20the%20true%20posterior%3B%20in%20higher-dimensional%20models%2C%0ABF-VI%20outperforms%20other%20VI%20methods.%20Further%2C%20we%20develop%20with%20BF-VI%20a%20Bayesian%0Amodel%20for%20the%20semi-structured%20Melanoma%20challenge%20data%2C%20combining%20a%20CNN%20model%0Apart%20for%20image%20data%20with%20an%20interpretable%20model%20part%20for%20tabular%20data%2C%20and%0Ademonstrate%20for%20the%20first%20time%20how%20the%20use%20of%20VI%20in%20semi-structured%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.05650v2&entry.124074799=Read"},
{"title": "Hyperbolic Hierarchical Knowledge Graph Embeddings for Link Prediction\n  in Low Dimensions", "author": "Wenjie Zheng and Wenxue Wang and Shu Zhao and Fulan Qian", "abstract": "  Knowledge graph embeddings (KGE) have been validated as powerful methods for\ninferring missing links in knowledge graphs (KGs) that they typically map\nentities into Euclidean space and treat relations as transformations of\nentities. Recently, some Euclidean KGE methods have been enhanced to model\nsemantic hierarchies commonly found in KGs, improving the performance of link\nprediction. To embed hierarchical data, hyperbolic space has emerged as a\npromising alternative to traditional Euclidean space, offering high fidelity\nand lower memory consumption. Unlike Euclidean, hyperbolic space provides\ncountless curvatures to choose from. However, it is difficult for existing\nhyperbolic KGE methods to obtain the optimal curvature settings manually,\nthereby limiting their ability to effectively model semantic hierarchies. To\naddress this limitation, we propose a novel KGE model called\n$\\textbf{Hyp}$erbolic $\\textbf{H}$ierarchical $\\textbf{KGE}$ (HypHKGE). This\nmodel introduces attention-based learnable curvatures for hyperbolic space,\nwhich helps preserve rich semantic hierarchies. Furthermore, to utilize the\npreserved hierarchies for inferring missing links, we define hyperbolic\nhierarchical transformations based on the theory of hyperbolic geometry,\nincluding both inter-level and intra-level modeling. Experiments demonstrate\nthe effectiveness of the proposed HypHKGE model on the three benchmark datasets\n(WN18RR, FB15K-237, and YAGO3-10). The source code will be publicly released at\nhttps://github.com/wjzheng96/HypHKGE.\n", "link": "http://arxiv.org/abs/2204.13704v2", "date": "2024-02-23", "relevancy": 1.9193, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5158}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4799}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4654}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperbolic%20Hierarchical%20Knowledge%20Graph%20Embeddings%20for%20Link%20Prediction%0A%20%20in%20Low%20Dimensions&entry.906535625=Wenjie%20Zheng%20and%20Wenxue%20Wang%20and%20Shu%20Zhao%20and%20Fulan%20Qian&entry.1292438233=%20%20Knowledge%20graph%20embeddings%20%28KGE%29%20have%20been%20validated%20as%20powerful%20methods%20for%0Ainferring%20missing%20links%20in%20knowledge%20graphs%20%28KGs%29%20that%20they%20typically%20map%0Aentities%20into%20Euclidean%20space%20and%20treat%20relations%20as%20transformations%20of%0Aentities.%20Recently%2C%20some%20Euclidean%20KGE%20methods%20have%20been%20enhanced%20to%20model%0Asemantic%20hierarchies%20commonly%20found%20in%20KGs%2C%20improving%20the%20performance%20of%20link%0Aprediction.%20To%20embed%20hierarchical%20data%2C%20hyperbolic%20space%20has%20emerged%20as%20a%0Apromising%20alternative%20to%20traditional%20Euclidean%20space%2C%20offering%20high%20fidelity%0Aand%20lower%20memory%20consumption.%20Unlike%20Euclidean%2C%20hyperbolic%20space%20provides%0Acountless%20curvatures%20to%20choose%20from.%20However%2C%20it%20is%20difficult%20for%20existing%0Ahyperbolic%20KGE%20methods%20to%20obtain%20the%20optimal%20curvature%20settings%20manually%2C%0Athereby%20limiting%20their%20ability%20to%20effectively%20model%20semantic%20hierarchies.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20novel%20KGE%20model%20called%0A%24%5Ctextbf%7BHyp%7D%24erbolic%20%24%5Ctextbf%7BH%7D%24ierarchical%20%24%5Ctextbf%7BKGE%7D%24%20%28HypHKGE%29.%20This%0Amodel%20introduces%20attention-based%20learnable%20curvatures%20for%20hyperbolic%20space%2C%0Awhich%20helps%20preserve%20rich%20semantic%20hierarchies.%20Furthermore%2C%20to%20utilize%20the%0Apreserved%20hierarchies%20for%20inferring%20missing%20links%2C%20we%20define%20hyperbolic%0Ahierarchical%20transformations%20based%20on%20the%20theory%20of%20hyperbolic%20geometry%2C%0Aincluding%20both%20inter-level%20and%20intra-level%20modeling.%20Experiments%20demonstrate%0Athe%20effectiveness%20of%20the%20proposed%20HypHKGE%20model%20on%20the%20three%20benchmark%20datasets%0A%28WN18RR%2C%20FB15K-237%2C%20and%20YAGO3-10%29.%20The%20source%20code%20will%20be%20publicly%20released%20at%0Ahttps%3A//github.com/wjzheng96/HypHKGE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.13704v2&entry.124074799=Read"},
{"title": "Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method\n  and Its Application to Energy Network", "author": "Jianhong Wang", "abstract": "  Multi-agent reinforcement learning is an area of rapid advancement in\nartificial intelligence and machine learning. One of the important questions to\nbe answered is how to conduct credit assignment in a multi-agent system. There\nhave been many schemes designed to conduct credit assignment by multi-agent\nreinforcement learning algorithms. Although these credit assignment schemes\nhave been proved useful in improving the performance of multi-agent\nreinforcement learning, most of them are designed heuristically without a\nrigorous theoretic basis and therefore infeasible to understand how agents\ncooperate. In this thesis, we aim at investigating the foundation of credit\nassignment in multi-agent reinforcement learning via cooperative game theory.\nWe first extend a game model called convex game and a payoff distribution\nscheme called Shapley value in cooperative game theory to Markov decision\nprocess, named as Markov convex game and Markov Shapley value respectively. We\nrepresent a global reward game as a Markov convex game under the grand\ncoalition. As a result, Markov Shapley value can be reasonably used as a credit\nassignment scheme in the global reward game. Markov Shapley value possesses the\nfollowing virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii)\nreflecting the contribution and (iv) symmetry, which form the fair credit\nassignment. Based on Markov Shapley value, we propose three multi-agent\nreinforcement learning algorithms called SHAQ, SQDDPG and SMFPPO. Furthermore,\nwe extend Markov convex game to partial observability to deal with the\npartially observable problems, named as partially observable Markov convex\ngame. In application, we evaluate SQDDPG and SMFPPO on the real-world problem\nin energy networks.\n", "link": "http://arxiv.org/abs/2402.15324v1", "date": "2024-02-23", "relevancy": 1.9109, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5225}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5163}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4212}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shapley%20Value%20Based%20Multi-Agent%20Reinforcement%20Learning%3A%20Theory%2C%20Method%0A%20%20and%20Its%20Application%20to%20Energy%20Network&entry.906535625=Jianhong%20Wang&entry.1292438233=%20%20Multi-agent%20reinforcement%20learning%20is%20an%20area%20of%20rapid%20advancement%20in%0Aartificial%20intelligence%20and%20machine%20learning.%20One%20of%20the%20important%20questions%20to%0Abe%20answered%20is%20how%20to%20conduct%20credit%20assignment%20in%20a%20multi-agent%20system.%20There%0Ahave%20been%20many%20schemes%20designed%20to%20conduct%20credit%20assignment%20by%20multi-agent%0Areinforcement%20learning%20algorithms.%20Although%20these%20credit%20assignment%20schemes%0Ahave%20been%20proved%20useful%20in%20improving%20the%20performance%20of%20multi-agent%0Areinforcement%20learning%2C%20most%20of%20them%20are%20designed%20heuristically%20without%20a%0Arigorous%20theoretic%20basis%20and%20therefore%20infeasible%20to%20understand%20how%20agents%0Acooperate.%20In%20this%20thesis%2C%20we%20aim%20at%20investigating%20the%20foundation%20of%20credit%0Aassignment%20in%20multi-agent%20reinforcement%20learning%20via%20cooperative%20game%20theory.%0AWe%20first%20extend%20a%20game%20model%20called%20convex%20game%20and%20a%20payoff%20distribution%0Ascheme%20called%20Shapley%20value%20in%20cooperative%20game%20theory%20to%20Markov%20decision%0Aprocess%2C%20named%20as%20Markov%20convex%20game%20and%20Markov%20Shapley%20value%20respectively.%20We%0Arepresent%20a%20global%20reward%20game%20as%20a%20Markov%20convex%20game%20under%20the%20grand%0Acoalition.%20As%20a%20result%2C%20Markov%20Shapley%20value%20can%20be%20reasonably%20used%20as%20a%20credit%0Aassignment%20scheme%20in%20the%20global%20reward%20game.%20Markov%20Shapley%20value%20possesses%20the%0Afollowing%20virtues%3A%20%28i%29%20efficiency%3B%20%28ii%29%20identifiability%20of%20dummy%20agents%3B%20%28iii%29%0Areflecting%20the%20contribution%20and%20%28iv%29%20symmetry%2C%20which%20form%20the%20fair%20credit%0Aassignment.%20Based%20on%20Markov%20Shapley%20value%2C%20we%20propose%20three%20multi-agent%0Areinforcement%20learning%20algorithms%20called%20SHAQ%2C%20SQDDPG%20and%20SMFPPO.%20Furthermore%2C%0Awe%20extend%20Markov%20convex%20game%20to%20partial%20observability%20to%20deal%20with%20the%0Apartially%20observable%20problems%2C%20named%20as%20partially%20observable%20Markov%20convex%0Agame.%20In%20application%2C%20we%20evaluate%20SQDDPG%20and%20SMFPPO%20on%20the%20real-world%20problem%0Ain%20energy%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15324v1&entry.124074799=Read"},
{"title": "Structured Probabilistic Coding", "author": "Dou Hu and Lingwei Wei and Yaxin Liu and Wei Zhou and Songlin Hu", "abstract": "  This paper presents a new supervised representation learning framework,\nnamely structured probabilistic coding (SPC), to learn compact and informative\nrepresentations from input related to the target task. SPC is an encoder-only\nprobabilistic coding technology with a structured regularization from the\ntarget space. It can enhance the generalization ability of pre-trained language\nmodels for better language understanding. Specifically, our probabilistic\ncoding simultaneously performs information encoding and task prediction in one\nmodule to more fully utilize the effective information from input data. It uses\nvariational inference in the output space to reduce randomness and uncertainty.\nBesides, to better control the learning process of probabilistic\nrepresentations, a structured regularization is proposed to promote uniformity\nacross classes in the latent space. With the regularization term, SPC can\npreserve the Gaussian structure of the latent code and achieve better coverage\nof the hidden space with class uniformly. Experimental results on 12 natural\nlanguage understanding tasks demonstrate that our SPC effectively improves the\nperformance of pre-trained language models for classification and regression.\nExtensive experiments show that SPC can enhance the generalization capability,\nrobustness to label noise, and clustering quality of output representations.\n", "link": "http://arxiv.org/abs/2312.13933v4", "date": "2024-02-23", "relevancy": 1.9101, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5083}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4851}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4577}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Probabilistic%20Coding&entry.906535625=Dou%20Hu%20and%20Lingwei%20Wei%20and%20Yaxin%20Liu%20and%20Wei%20Zhou%20and%20Songlin%20Hu&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20supervised%20representation%20learning%20framework%2C%0Anamely%20structured%20probabilistic%20coding%20%28SPC%29%2C%20to%20learn%20compact%20and%20informative%0Arepresentations%20from%20input%20related%20to%20the%20target%20task.%20SPC%20is%20an%20encoder-only%0Aprobabilistic%20coding%20technology%20with%20a%20structured%20regularization%20from%20the%0Atarget%20space.%20It%20can%20enhance%20the%20generalization%20ability%20of%20pre-trained%20language%0Amodels%20for%20better%20language%20understanding.%20Specifically%2C%20our%20probabilistic%0Acoding%20simultaneously%20performs%20information%20encoding%20and%20task%20prediction%20in%20one%0Amodule%20to%20more%20fully%20utilize%20the%20effective%20information%20from%20input%20data.%20It%20uses%0Avariational%20inference%20in%20the%20output%20space%20to%20reduce%20randomness%20and%20uncertainty.%0ABesides%2C%20to%20better%20control%20the%20learning%20process%20of%20probabilistic%0Arepresentations%2C%20a%20structured%20regularization%20is%20proposed%20to%20promote%20uniformity%0Aacross%20classes%20in%20the%20latent%20space.%20With%20the%20regularization%20term%2C%20SPC%20can%0Apreserve%20the%20Gaussian%20structure%20of%20the%20latent%20code%20and%20achieve%20better%20coverage%0Aof%20the%20hidden%20space%20with%20class%20uniformly.%20Experimental%20results%20on%2012%20natural%0Alanguage%20understanding%20tasks%20demonstrate%20that%20our%20SPC%20effectively%20improves%20the%0Aperformance%20of%20pre-trained%20language%20models%20for%20classification%20and%20regression.%0AExtensive%20experiments%20show%20that%20SPC%20can%20enhance%20the%20generalization%20capability%2C%0Arobustness%20to%20label%20noise%2C%20and%20clustering%20quality%20of%20output%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13933v4&entry.124074799=Read"},
{"title": "Dynamic Memory Based Adaptive Optimization", "author": "Bal\u00e1zs Szegedy and Domonkos Czifra and P\u00e9ter K\u0151r\u00f6si-Szab\u00f3", "abstract": "  Define an optimizer as having memory $k$ if it stores $k$ dynamically\nchanging vectors in the parameter space. Classical SGD has memory $0$, momentum\nSGD optimizer has $1$ and Adam optimizer has $2$. We address the following\nquestions: How can optimizers make use of more memory units? What information\nshould be stored in them? How to use them for the learning steps? As an\napproach to the last question, we introduce a general method called\n\"Retrospective Learning Law Correction\" or shortly RLLC. This method is\ndesigned to calculate a dynamically varying linear combination (called learning\nlaw) of memory units, which themselves may evolve arbitrarily. We demonstrate\nRLLC on optimizers whose memory units have linear update rules and small memory\n($\\leq 4$ memory units). Our experiments show that in a variety of standard\nproblems, these optimizers outperform the above mentioned three classical\noptimizers. We conclude that RLLC is a promising framework for boosting the\nperformance of known optimizers by adding more memory units and by making them\nmore adaptive.\n", "link": "http://arxiv.org/abs/2402.15262v1", "date": "2024-02-23", "relevancy": 1.9054, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5394}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4449}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4259}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Memory%20Based%20Adaptive%20Optimization&entry.906535625=Bal%C3%A1zs%20Szegedy%20and%20Domonkos%20Czifra%20and%20P%C3%A9ter%20K%C5%91r%C3%B6si-Szab%C3%B3&entry.1292438233=%20%20Define%20an%20optimizer%20as%20having%20memory%20%24k%24%20if%20it%20stores%20%24k%24%20dynamically%0Achanging%20vectors%20in%20the%20parameter%20space.%20Classical%20SGD%20has%20memory%20%240%24%2C%20momentum%0ASGD%20optimizer%20has%20%241%24%20and%20Adam%20optimizer%20has%20%242%24.%20We%20address%20the%20following%0Aquestions%3A%20How%20can%20optimizers%20make%20use%20of%20more%20memory%20units%3F%20What%20information%0Ashould%20be%20stored%20in%20them%3F%20How%20to%20use%20them%20for%20the%20learning%20steps%3F%20As%20an%0Aapproach%20to%20the%20last%20question%2C%20we%20introduce%20a%20general%20method%20called%0A%22Retrospective%20Learning%20Law%20Correction%22%20or%20shortly%20RLLC.%20This%20method%20is%0Adesigned%20to%20calculate%20a%20dynamically%20varying%20linear%20combination%20%28called%20learning%0Alaw%29%20of%20memory%20units%2C%20which%20themselves%20may%20evolve%20arbitrarily.%20We%20demonstrate%0ARLLC%20on%20optimizers%20whose%20memory%20units%20have%20linear%20update%20rules%20and%20small%20memory%0A%28%24%5Cleq%204%24%20memory%20units%29.%20Our%20experiments%20show%20that%20in%20a%20variety%20of%20standard%0Aproblems%2C%20these%20optimizers%20outperform%20the%20above%20mentioned%20three%20classical%0Aoptimizers.%20We%20conclude%20that%20RLLC%20is%20a%20promising%20framework%20for%20boosting%20the%0Aperformance%20of%20known%20optimizers%20by%20adding%20more%20memory%20units%20and%20by%20making%20them%0Amore%20adaptive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15262v1&entry.124074799=Read"},
{"title": "GPTVQ: The Blessing of Dimensionality for LLM Quantization", "author": "Mart van Baalen and Andrey Kuzmin and Markus Nagel and Peter Couperus and Cedric Bastoul and Eric Mahurin and Tijmen Blankevoort and Paul Whatmough", "abstract": "  In this work we show that the size versus accuracy trade-off of neural\nnetwork quantization can be significantly improved by increasing the\nquantization dimensionality. We propose the GPTVQ method, a new fast method for\npost-training vector quantization (VQ) that scales well to Large Language\nModels (LLMs). Our method interleaves quantization of one or more columns with\nupdates to the remaining unquantized weights, using information from the\nHessian of the per-layer output reconstruction MSE. Quantization codebooks are\ninitialized using an efficient data-aware version of the EM algorithm. The\ncodebooks are then updated, and further compressed by using integer\nquantization and SVD-based compression. GPTVQ establishes a new state-of-the\nart in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2\nand Mistral. Furthermore, our method is efficient: on a single H100 it takes\nbetween 3 and 11 hours to process a Llamav2-70B model, depending on\nquantization setting. Lastly, with on-device timings for VQ decompression on a\nmobile CPU we show that VQ leads to improved latency compared to using a 4-bit\ninteger format.\n", "link": "http://arxiv.org/abs/2402.15319v1", "date": "2024-02-23", "relevancy": 1.9043, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4864}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4634}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPTVQ%3A%20The%20Blessing%20of%20Dimensionality%20for%20LLM%20Quantization&entry.906535625=Mart%20van%20Baalen%20and%20Andrey%20Kuzmin%20and%20Markus%20Nagel%20and%20Peter%20Couperus%20and%20Cedric%20Bastoul%20and%20Eric%20Mahurin%20and%20Tijmen%20Blankevoort%20and%20Paul%20Whatmough&entry.1292438233=%20%20In%20this%20work%20we%20show%20that%20the%20size%20versus%20accuracy%20trade-off%20of%20neural%0Anetwork%20quantization%20can%20be%20significantly%20improved%20by%20increasing%20the%0Aquantization%20dimensionality.%20We%20propose%20the%20GPTVQ%20method%2C%20a%20new%20fast%20method%20for%0Apost-training%20vector%20quantization%20%28VQ%29%20that%20scales%20well%20to%20Large%20Language%0AModels%20%28LLMs%29.%20Our%20method%20interleaves%20quantization%20of%20one%20or%20more%20columns%20with%0Aupdates%20to%20the%20remaining%20unquantized%20weights%2C%20using%20information%20from%20the%0AHessian%20of%20the%20per-layer%20output%20reconstruction%20MSE.%20Quantization%20codebooks%20are%0Ainitialized%20using%20an%20efficient%20data-aware%20version%20of%20the%20EM%20algorithm.%20The%0Acodebooks%20are%20then%20updated%2C%20and%20further%20compressed%20by%20using%20integer%0Aquantization%20and%20SVD-based%20compression.%20GPTVQ%20establishes%20a%20new%20state-of-the%0Aart%20in%20the%20size%20vs%20accuracy%20trade-offs%20on%20a%20wide%20range%20of%20LLMs%20such%20as%20Llama-v2%0Aand%20Mistral.%20Furthermore%2C%20our%20method%20is%20efficient%3A%20on%20a%20single%20H100%20it%20takes%0Abetween%203%20and%2011%20hours%20to%20process%20a%20Llamav2-70B%20model%2C%20depending%20on%0Aquantization%20setting.%20Lastly%2C%20with%20on-device%20timings%20for%20VQ%20decompression%20on%20a%0Amobile%20CPU%20we%20show%20that%20VQ%20leads%20to%20improved%20latency%20compared%20to%20using%20a%204-bit%0Ainteger%20format.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15319v1&entry.124074799=Read"},
{"title": "Neural Networks and Friction: Slide, Hold, Learn", "author": "Joaquin Garcia-Suarez", "abstract": "  In this study, it is demonstrated that Recurrent Neural Networks (RNNs),\nspecifically those utilizing Gated Recurrent Unit (GRU) architecture, possess\nthe capability to learn the complex dynamics of rate-and-state friction laws\nfrom synthetic data. The data employed for training the network is generated\nthrough the application of traditional rate-and-state friction equations\ncoupled with the aging law for state evolution. A novel aspect of our approach\nis the formulation of a loss function that explicitly accounts for initial\nconditions, the direct effect, and the evolution of state variables during\ntraining. It is found that the RNN, with its GRU architecture, effectively\nlearns to predict changes in the friction coefficient resulting from velocity\njumps, thereby showcasing the potential of machine learning models in\nunderstanding and simulating the physics of frictional processes.\n", "link": "http://arxiv.org/abs/2402.14148v2", "date": "2024-02-23", "relevancy": 1.9003, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4682}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.457}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Networks%20and%20Friction%3A%20Slide%2C%20Hold%2C%20Learn&entry.906535625=Joaquin%20Garcia-Suarez&entry.1292438233=%20%20In%20this%20study%2C%20it%20is%20demonstrated%20that%20Recurrent%20Neural%20Networks%20%28RNNs%29%2C%0Aspecifically%20those%20utilizing%20Gated%20Recurrent%20Unit%20%28GRU%29%20architecture%2C%20possess%0Athe%20capability%20to%20learn%20the%20complex%20dynamics%20of%20rate-and-state%20friction%20laws%0Afrom%20synthetic%20data.%20The%20data%20employed%20for%20training%20the%20network%20is%20generated%0Athrough%20the%20application%20of%20traditional%20rate-and-state%20friction%20equations%0Acoupled%20with%20the%20aging%20law%20for%20state%20evolution.%20A%20novel%20aspect%20of%20our%20approach%0Ais%20the%20formulation%20of%20a%20loss%20function%20that%20explicitly%20accounts%20for%20initial%0Aconditions%2C%20the%20direct%20effect%2C%20and%20the%20evolution%20of%20state%20variables%20during%0Atraining.%20It%20is%20found%20that%20the%20RNN%2C%20with%20its%20GRU%20architecture%2C%20effectively%0Alearns%20to%20predict%20changes%20in%20the%20friction%20coefficient%20resulting%20from%20velocity%0Ajumps%2C%20thereby%20showcasing%20the%20potential%20of%20machine%20learning%20models%20in%0Aunderstanding%20and%20simulating%20the%20physics%20of%20frictional%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14148v2&entry.124074799=Read"},
{"title": "Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian\n  Perspective", "author": "Victor Gallego", "abstract": "  This paper proposes an interpretation of RLAIF as Bayesian inference by\nintroducing distilled Self-Critique (dSC), which refines the outputs of a LLM\nthrough a Gibbs sampler that is later distilled into a fine-tuned model. Only\nrequiring synthetic data, dSC is exercised in experiments regarding safety,\nsentiment, and privacy control, showing it can be a viable and cheap\nalternative to align LLMs. Code released at\n\\url{https://github.com/vicgalle/distilled-self-critique}.\n", "link": "http://arxiv.org/abs/2312.01957v2", "date": "2024-02-23", "relevancy": 1.8894, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4832}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4718}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4686}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilled%20Self-Critique%20of%20LLMs%20with%20Synthetic%20Data%3A%20a%20Bayesian%0A%20%20Perspective&entry.906535625=Victor%20Gallego&entry.1292438233=%20%20This%20paper%20proposes%20an%20interpretation%20of%20RLAIF%20as%20Bayesian%20inference%20by%0Aintroducing%20distilled%20Self-Critique%20%28dSC%29%2C%20which%20refines%20the%20outputs%20of%20a%20LLM%0Athrough%20a%20Gibbs%20sampler%20that%20is%20later%20distilled%20into%20a%20fine-tuned%20model.%20Only%0Arequiring%20synthetic%20data%2C%20dSC%20is%20exercised%20in%20experiments%20regarding%20safety%2C%0Asentiment%2C%20and%20privacy%20control%2C%20showing%20it%20can%20be%20a%20viable%20and%20cheap%0Aalternative%20to%20align%20LLMs.%20Code%20released%20at%0A%5Curl%7Bhttps%3A//github.com/vicgalle/distilled-self-critique%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01957v2&entry.124074799=Read"},
{"title": "Diffusion Models for Reinforcement Learning: A Survey", "author": "Zhengbang Zhu and Hanye Zhao and Haoran He and Yichao Zhong and Shenyu Zhang and Haoquan Guo and Tingting Chen and Weinan Zhang", "abstract": "  Diffusion models surpass previous generative models in sample quality and\ntraining stability. Recent works have shown the advantages of diffusion models\nin improving reinforcement learning (RL) solutions. This survey aims to provide\nan overview of this emerging field and hopes to inspire new avenues of\nresearch. First, we examine several challenges encountered by RL algorithms.\nThen, we present a taxonomy of existing methods based on the roles of diffusion\nmodels in RL and explore how the preceding challenges are addressed. We further\noutline successful applications of diffusion models in various RL-related\ntasks. Finally, we conclude the survey and offer insights into future research\ndirections. We are actively maintaining a GitHub repository for papers and\nother related resources in utilizing diffusion models in RL:\nhttps://github.com/apexrl/Diff4RLSurvey.\n", "link": "http://arxiv.org/abs/2311.01223v4", "date": "2024-02-23", "relevancy": 1.8876, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4954}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4866}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4478}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Models%20for%20Reinforcement%20Learning%3A%20A%20Survey&entry.906535625=Zhengbang%20Zhu%20and%20Hanye%20Zhao%20and%20Haoran%20He%20and%20Yichao%20Zhong%20and%20Shenyu%20Zhang%20and%20Haoquan%20Guo%20and%20Tingting%20Chen%20and%20Weinan%20Zhang&entry.1292438233=%20%20Diffusion%20models%20surpass%20previous%20generative%20models%20in%20sample%20quality%20and%0Atraining%20stability.%20Recent%20works%20have%20shown%20the%20advantages%20of%20diffusion%20models%0Ain%20improving%20reinforcement%20learning%20%28RL%29%20solutions.%20This%20survey%20aims%20to%20provide%0Aan%20overview%20of%20this%20emerging%20field%20and%20hopes%20to%20inspire%20new%20avenues%20of%0Aresearch.%20First%2C%20we%20examine%20several%20challenges%20encountered%20by%20RL%20algorithms.%0AThen%2C%20we%20present%20a%20taxonomy%20of%20existing%20methods%20based%20on%20the%20roles%20of%20diffusion%0Amodels%20in%20RL%20and%20explore%20how%20the%20preceding%20challenges%20are%20addressed.%20We%20further%0Aoutline%20successful%20applications%20of%20diffusion%20models%20in%20various%20RL-related%0Atasks.%20Finally%2C%20we%20conclude%20the%20survey%20and%20offer%20insights%20into%20future%20research%0Adirections.%20We%20are%20actively%20maintaining%20a%20GitHub%20repository%20for%20papers%20and%0Aother%20related%20resources%20in%20utilizing%20diffusion%20models%20in%20RL%3A%0Ahttps%3A//github.com/apexrl/Diff4RLSurvey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01223v4&entry.124074799=Read"},
{"title": "Benchmarking Retrieval-Augmented Generation for Medicine", "author": "Guangzhi Xiong and Qiao Jin and Zhiyong Lu and Aidong Zhang", "abstract": "  While large language models (LLMs) have achieved state-of-the-art performance\non a wide range of medical question answering (QA) tasks, they still face\nchallenges with hallucinations and outdated knowledge. Retrieval-augmented\ngeneration (RAG) is a promising solution and has been widely adopted. However,\na RAG system can involve multiple flexible components, and there is a lack of\nbest practices regarding the optimal RAG setting for various medical purposes.\nTo systematically evaluate such systems, we propose the Medical Information\nRetrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind\nbenchmark including 7,663 questions from five medical QA datasets. Using\nMIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt\ntokens on 41 combinations of different corpora, retrievers, and backbone LLMs\nthrough the MedRAG toolkit introduced in this work. Overall, MedRAG improves\nthe accuracy of six different LLMs by up to 18% over chain-of-thought\nprompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our\nresults show that the combination of various medical corpora and retrievers\nachieves the best performance. In addition, we discovered a log-linear scaling\nproperty and the \"lost-in-the-middle\" effects in medical RAG. We believe our\ncomprehensive evaluations can serve as practical guidelines for implementing\nRAG systems for medicine.\n", "link": "http://arxiv.org/abs/2402.13178v2", "date": "2024-02-23", "relevancy": 1.8842, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5065}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4833}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4446}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Retrieval-Augmented%20Generation%20for%20Medicine&entry.906535625=Guangzhi%20Xiong%20and%20Qiao%20Jin%20and%20Zhiyong%20Lu%20and%20Aidong%20Zhang&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20have%20achieved%20state-of-the-art%20performance%0Aon%20a%20wide%20range%20of%20medical%20question%20answering%20%28QA%29%20tasks%2C%20they%20still%20face%0Achallenges%20with%20hallucinations%20and%20outdated%20knowledge.%20Retrieval-augmented%0Ageneration%20%28RAG%29%20is%20a%20promising%20solution%20and%20has%20been%20widely%20adopted.%20However%2C%0Aa%20RAG%20system%20can%20involve%20multiple%20flexible%20components%2C%20and%20there%20is%20a%20lack%20of%0Abest%20practices%20regarding%20the%20optimal%20RAG%20setting%20for%20various%20medical%20purposes.%0ATo%20systematically%20evaluate%20such%20systems%2C%20we%20propose%20the%20Medical%20Information%0ARetrieval-Augmented%20Generation%20Evaluation%20%28MIRAGE%29%2C%20a%20first-of-its-kind%0Abenchmark%20including%207%2C663%20questions%20from%20five%20medical%20QA%20datasets.%20Using%0AMIRAGE%2C%20we%20conducted%20large-scale%20experiments%20with%20over%201.8%20trillion%20prompt%0Atokens%20on%2041%20combinations%20of%20different%20corpora%2C%20retrievers%2C%20and%20backbone%20LLMs%0Athrough%20the%20MedRAG%20toolkit%20introduced%20in%20this%20work.%20Overall%2C%20MedRAG%20improves%0Athe%20accuracy%20of%20six%20different%20LLMs%20by%20up%20to%2018%25%20over%20chain-of-thought%0Aprompting%2C%20elevating%20the%20performance%20of%20GPT-3.5%20and%20Mixtral%20to%20GPT-4-level.%20Our%0Aresults%20show%20that%20the%20combination%20of%20various%20medical%20corpora%20and%20retrievers%0Aachieves%20the%20best%20performance.%20In%20addition%2C%20we%20discovered%20a%20log-linear%20scaling%0Aproperty%20and%20the%20%22lost-in-the-middle%22%20effects%20in%20medical%20RAG.%20We%20believe%20our%0Acomprehensive%20evaluations%20can%20serve%20as%20practical%20guidelines%20for%20implementing%0ARAG%20systems%20for%20medicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13178v2&entry.124074799=Read"},
{"title": "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "author": "Sergei Bogdanov and Alexandre Constantin and Timoth\u00e9e Bernard and Benoit Crabb\u00e9 and Etienne Bernard", "abstract": "  Large Language Models (LLMs) have shown impressive abilities in data\nannotation, opening the way for new approaches to solve classic NLP problems.\nIn this paper, we show how to use LLMs to create NuNER, a compact language\nrepresentation model specialized in the Named Entity Recognition (NER) task.\nNuNER can be fine-tuned to solve downstream NER problems in a data-efficient\nway, outperforming similar-sized foundation models in the few-shot regime and\ncompeting with much larger LLMs. We find that the size and entity-type\ndiversity of the pre-training dataset are key to achieving good performance. We\nview NuNER as a member of the broader family of task-specific foundation\nmodels, recently unlocked by LLMs.\n", "link": "http://arxiv.org/abs/2402.15343v1", "date": "2024-02-23", "relevancy": 1.883, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5322}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4684}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4485}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NuNER%3A%20Entity%20Recognition%20Encoder%20Pre-training%20via%20LLM-Annotated%20Data&entry.906535625=Sergei%20Bogdanov%20and%20Alexandre%20Constantin%20and%20Timoth%C3%A9e%20Bernard%20and%20Benoit%20Crabb%C3%A9%20and%20Etienne%20Bernard&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20abilities%20in%20data%0Aannotation%2C%20opening%20the%20way%20for%20new%20approaches%20to%20solve%20classic%20NLP%20problems.%0AIn%20this%20paper%2C%20we%20show%20how%20to%20use%20LLMs%20to%20create%20NuNER%2C%20a%20compact%20language%0Arepresentation%20model%20specialized%20in%20the%20Named%20Entity%20Recognition%20%28NER%29%20task.%0ANuNER%20can%20be%20fine-tuned%20to%20solve%20downstream%20NER%20problems%20in%20a%20data-efficient%0Away%2C%20outperforming%20similar-sized%20foundation%20models%20in%20the%20few-shot%20regime%20and%0Acompeting%20with%20much%20larger%20LLMs.%20We%20find%20that%20the%20size%20and%20entity-type%0Adiversity%20of%20the%20pre-training%20dataset%20are%20key%20to%20achieving%20good%20performance.%20We%0Aview%20NuNER%20as%20a%20member%20of%20the%20broader%20family%20of%20task-specific%20foundation%0Amodels%2C%20recently%20unlocked%20by%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15343v1&entry.124074799=Read"},
{"title": "A Bargaining-based Approach for Feature Trading in Vertical Federated\n  Learning", "author": "Yue Cui and Liuyi Yao and Zitao Li and Yaliang Li and Bolin Ding and Xiaofang Zhou", "abstract": "  Vertical Federated Learning (VFL) has emerged as a popular machine learning\nparadigm, enabling model training across the data and the task parties with\ndifferent features about the same user set while preserving data privacy. In\nproduction environment, VFL usually involves one task party and one data party.\nFair and economically efficient feature trading is crucial to the\ncommercialization of VFL, where the task party is considered as the data\nconsumer who buys the data party's features. However, current VFL feature\ntrading practices often price the data party's data as a whole and assume\ntransactions occur prior to the performing VFL. Neglecting the performance\ngains resulting from traded features may lead to underpayment and overpayment\nissues. In this study, we propose a bargaining-based feature trading approach\nin VFL to encourage economically efficient transactions. Our model incorporates\nperformance gain-based pricing, taking into account the revenue-based\noptimization objectives of both parties. We analyze the proposed bargaining\nmodel under perfect and imperfect performance information settings, proving the\nexistence of an equilibrium that optimizes the parties' objectives. Moreover,\nwe develop performance gain estimation-based bargaining strategies for\nimperfect performance information scenarios and discuss potential security\nissues and solutions. Experiments on three real-world datasets demonstrate the\neffectiveness of the proposed bargaining model.\n", "link": "http://arxiv.org/abs/2402.15247v1", "date": "2024-02-23", "relevancy": 1.874, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4908}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4448}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bargaining-based%20Approach%20for%20Feature%20Trading%20in%20Vertical%20Federated%0A%20%20Learning&entry.906535625=Yue%20Cui%20and%20Liuyi%20Yao%20and%20Zitao%20Li%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Xiaofang%20Zhou&entry.1292438233=%20%20Vertical%20Federated%20Learning%20%28VFL%29%20has%20emerged%20as%20a%20popular%20machine%20learning%0Aparadigm%2C%20enabling%20model%20training%20across%20the%20data%20and%20the%20task%20parties%20with%0Adifferent%20features%20about%20the%20same%20user%20set%20while%20preserving%20data%20privacy.%20In%0Aproduction%20environment%2C%20VFL%20usually%20involves%20one%20task%20party%20and%20one%20data%20party.%0AFair%20and%20economically%20efficient%20feature%20trading%20is%20crucial%20to%20the%0Acommercialization%20of%20VFL%2C%20where%20the%20task%20party%20is%20considered%20as%20the%20data%0Aconsumer%20who%20buys%20the%20data%20party%27s%20features.%20However%2C%20current%20VFL%20feature%0Atrading%20practices%20often%20price%20the%20data%20party%27s%20data%20as%20a%20whole%20and%20assume%0Atransactions%20occur%20prior%20to%20the%20performing%20VFL.%20Neglecting%20the%20performance%0Agains%20resulting%20from%20traded%20features%20may%20lead%20to%20underpayment%20and%20overpayment%0Aissues.%20In%20this%20study%2C%20we%20propose%20a%20bargaining-based%20feature%20trading%20approach%0Ain%20VFL%20to%20encourage%20economically%20efficient%20transactions.%20Our%20model%20incorporates%0Aperformance%20gain-based%20pricing%2C%20taking%20into%20account%20the%20revenue-based%0Aoptimization%20objectives%20of%20both%20parties.%20We%20analyze%20the%20proposed%20bargaining%0Amodel%20under%20perfect%20and%20imperfect%20performance%20information%20settings%2C%20proving%20the%0Aexistence%20of%20an%20equilibrium%20that%20optimizes%20the%20parties%27%20objectives.%20Moreover%2C%0Awe%20develop%20performance%20gain%20estimation-based%20bargaining%20strategies%20for%0Aimperfect%20performance%20information%20scenarios%20and%20discuss%20potential%20security%0Aissues%20and%20solutions.%20Experiments%20on%20three%20real-world%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20bargaining%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15247v1&entry.124074799=Read"},
{"title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation", "author": "Tianyu Zheng and Shuyue Guo and Xingwei Qu and Jiawei Guo and Weixu Zhang and Xinrun Du and Qi Jia and Chenghua Lin and Wenhao Huang and Wenhu Chen and Jie Fu and Ge Zhang", "abstract": "  In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun\n", "link": "http://arxiv.org/abs/2401.06477v2", "date": "2024-02-23", "relevancy": 1.8735, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4977}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4685}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4565}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kun%3A%20Answer%20Polishment%20for%20Chinese%20Self-Alignment%20with%20Instruction%0A%20%20Back-Translation&entry.906535625=Tianyu%20Zheng%20and%20Shuyue%20Guo%20and%20Xingwei%20Qu%20and%20Jiawei%20Guo%20and%20Weixu%20Zhang%20and%20Xinrun%20Du%20and%20Qi%20Jia%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Wenhu%20Chen%20and%20Jie%20Fu%20and%20Ge%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Kun%2C%20a%20novel%20approach%20for%20creating%20high-quality%0Ainstruction-tuning%20datasets%20for%20large%20language%20models%20%28LLMs%29%20without%20relying%20on%0Amanual%20annotations.%20Adapting%20a%20self-training%20algorithm%20based%20on%20instruction%0Aback-translation%20and%20answer%20polishment%2C%20Kun%20leverages%20unlabelled%20data%20from%0Adiverse%20sources%20such%20as%20Wudao%2C%20Wanjuan%2C%20and%20SkyPile%20to%20generate%20a%20substantial%0Adataset%20of%20over%20a%20million%20Chinese%20instructional%20data%20points.%20This%20approach%0Asignificantly%20deviates%20from%20traditional%20methods%20by%20using%20a%20self-curation%0Aprocess%20to%20refine%20and%20select%20the%20most%20effective%20instruction-output%20pairs.%20Our%0Aexperiments%20with%20the%206B-parameter%20Yi%20model%20across%20various%20benchmarks%0Ademonstrate%20Kun%27s%20robustness%20and%20scalability.%20Our%20method%27s%20core%20contributions%0Alie%20in%20its%20algorithmic%20advancement%2C%20which%20enhances%20data%20retention%20and%20clarity%2C%0Aand%20its%20innovative%20data%20generation%20approach%20that%20substantially%20reduces%20the%0Areliance%20on%20costly%20and%20time-consuming%20manual%20annotations.%20This%20methodology%0Apresents%20a%20scalable%20and%20efficient%20solution%20for%20improving%20the%0Ainstruction-following%20capabilities%20of%20LLMs%2C%20with%20significant%20implications%20for%0Atheir%20application%20across%20diverse%20fields.%20The%20code%20and%20dataset%20can%20be%20found%20at%0Ahttps%3A//github.com/Zheng0428/COIG-Kun%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06477v2&entry.124074799=Read"},
{"title": "Tailoring Instructions to Student's Learning Levels Boosts Knowledge\n  Distillation", "author": "Yuxin Ren and Zihan Zhong and Xingjian Shi and Yi Zhu and Chun Yuan and Mu Li", "abstract": "  It has been commonly observed that a teacher model with superior performance\ndoes not necessarily result in a stronger student, highlighting a discrepancy\nbetween current teacher training practices and effective knowledge transfer. In\norder to enhance the guidance of the teacher training process, we introduce the\nconcept of distillation influence to determine the impact of distillation from\neach training sample on the student's generalization ability. In this paper, we\npropose Learning Good Teacher Matters (LGTM), an efficient training technique\nfor incorporating distillation influence into the teacher's learning process.\nBy prioritizing samples that are likely to enhance the student's generalization\nability, our LGTM outperforms 10 common knowledge distillation baselines on 6\ntext classification tasks in the GLUE benchmark.\n", "link": "http://arxiv.org/abs/2305.09651v2", "date": "2024-02-23", "relevancy": 1.8711, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4807}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4805}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4499}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailoring%20Instructions%20to%20Student%27s%20Learning%20Levels%20Boosts%20Knowledge%0A%20%20Distillation&entry.906535625=Yuxin%20Ren%20and%20Zihan%20Zhong%20and%20Xingjian%20Shi%20and%20Yi%20Zhu%20and%20Chun%20Yuan%20and%20Mu%20Li&entry.1292438233=%20%20It%20has%20been%20commonly%20observed%20that%20a%20teacher%20model%20with%20superior%20performance%0Adoes%20not%20necessarily%20result%20in%20a%20stronger%20student%2C%20highlighting%20a%20discrepancy%0Abetween%20current%20teacher%20training%20practices%20and%20effective%20knowledge%20transfer.%20In%0Aorder%20to%20enhance%20the%20guidance%20of%20the%20teacher%20training%20process%2C%20we%20introduce%20the%0Aconcept%20of%20distillation%20influence%20to%20determine%20the%20impact%20of%20distillation%20from%0Aeach%20training%20sample%20on%20the%20student%27s%20generalization%20ability.%20In%20this%20paper%2C%20we%0Apropose%20Learning%20Good%20Teacher%20Matters%20%28LGTM%29%2C%20an%20efficient%20training%20technique%0Afor%20incorporating%20distillation%20influence%20into%20the%20teacher%27s%20learning%20process.%0ABy%20prioritizing%20samples%20that%20are%20likely%20to%20enhance%20the%20student%27s%20generalization%0Aability%2C%20our%20LGTM%20outperforms%2010%20common%20knowledge%20distillation%20baselines%20on%206%0Atext%20classification%20tasks%20in%20the%20GLUE%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.09651v2&entry.124074799=Read"},
{"title": "On normalization-equivariance properties of supervised and unsupervised\n  denoising methods: a survey", "author": "S\u00e9bastien Herbreteau and Charles Kervrann", "abstract": "  Image denoising is probably the oldest and still one of the most active\nresearch topic in image processing. Many methodological concepts have been\nintroduced in the past decades and have improved performances significantly in\nrecent years, especially with the emergence of convolutional neural networks\nand supervised deep learning. In this paper, we propose a survey of guided tour\nof supervised and unsupervised learning methods for image denoising,\nclassifying the main principles elaborated during this evolution, with a\nparticular concern given to recent developments in supervised learning. It is\nconceived as a tutorial organizing in a comprehensive framework current\napproaches. We give insights on the rationales and limitations of the most\nperformant methods in the literature, and we highlight the common features\nbetween many of them. Finally, we focus on on the normalization equivariance\nproperties that is surprisingly not guaranteed with most of supervised methods.\nIt is of paramount importance that intensity shifting or scaling applied to the\ninput image results in a corresponding change in the denoiser output.\n", "link": "http://arxiv.org/abs/2402.15352v1", "date": "2024-02-23", "relevancy": 1.8566, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4673}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4656}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4528}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20normalization-equivariance%20properties%20of%20supervised%20and%20unsupervised%0A%20%20denoising%20methods%3A%20a%20survey&entry.906535625=S%C3%A9bastien%20Herbreteau%20and%20Charles%20Kervrann&entry.1292438233=%20%20Image%20denoising%20is%20probably%20the%20oldest%20and%20still%20one%20of%20the%20most%20active%0Aresearch%20topic%20in%20image%20processing.%20Many%20methodological%20concepts%20have%20been%0Aintroduced%20in%20the%20past%20decades%20and%20have%20improved%20performances%20significantly%20in%0Arecent%20years%2C%20especially%20with%20the%20emergence%20of%20convolutional%20neural%20networks%0Aand%20supervised%20deep%20learning.%20In%20this%20paper%2C%20we%20propose%20a%20survey%20of%20guided%20tour%0Aof%20supervised%20and%20unsupervised%20learning%20methods%20for%20image%20denoising%2C%0Aclassifying%20the%20main%20principles%20elaborated%20during%20this%20evolution%2C%20with%20a%0Aparticular%20concern%20given%20to%20recent%20developments%20in%20supervised%20learning.%20It%20is%0Aconceived%20as%20a%20tutorial%20organizing%20in%20a%20comprehensive%20framework%20current%0Aapproaches.%20We%20give%20insights%20on%20the%20rationales%20and%20limitations%20of%20the%20most%0Aperformant%20methods%20in%20the%20literature%2C%20and%20we%20highlight%20the%20common%20features%0Abetween%20many%20of%20them.%20Finally%2C%20we%20focus%20on%20on%20the%20normalization%20equivariance%0Aproperties%20that%20is%20surprisingly%20not%20guaranteed%20with%20most%20of%20supervised%20methods.%0AIt%20is%20of%20paramount%20importance%20that%20intensity%20shifting%20or%20scaling%20applied%20to%20the%0Ainput%20image%20results%20in%20a%20corresponding%20change%20in%20the%20denoiser%20output.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15352v1&entry.124074799=Read"},
{"title": "Debiasing Machine Learning Models by Using Weakly Supervised Learning", "author": "Renan D. B. Brotto and Jean-Michel Loubes and Laurent Risser and Jean-Pierre Florens and Kenji Nose-Filho and Jo\u00e3o M. T. Romano", "abstract": "  We tackle the problem of bias mitigation of algorithmic decisions in a\nsetting where both the output of the algorithm and the sensitive variable are\ncontinuous. Most of prior work deals with discrete sensitive variables, meaning\nthat the biases are measured for subgroups of persons defined by a label,\nleaving out important algorithmic bias cases, where the sensitive variable is\ncontinuous. Typical examples are unfair decisions made with respect to the age\nor the financial status. In our work, we then propose a bias mitigation\nstrategy for continuous sensitive variables, based on the notion of endogeneity\nwhich comes from the field of econometrics. In addition to solve this new\nproblem, our bias mitigation strategy is a weakly supervised learning method\nwhich requires that a small portion of the data can be measured in a fair\nmanner. It is model agnostic, in the sense that it does not make any hypothesis\non the prediction model. It also makes use of a reasonably large amount of\ninput observations and their corresponding predictions. Only a small fraction\nof the true output predictions should be known. This therefore limits the need\nfor expert interventions. Results obtained on synthetic data show the\neffectiveness of our approach for examples as close as possible to real-life\napplications in econometrics.\n", "link": "http://arxiv.org/abs/2402.15477v1", "date": "2024-02-23", "relevancy": 1.4253, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4787}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4754}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4735}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debiasing%20Machine%20Learning%20Models%20by%20Using%20Weakly%20Supervised%20Learning&entry.906535625=Renan%20D.%20B.%20Brotto%20and%20Jean-Michel%20Loubes%20and%20Laurent%20Risser%20and%20Jean-Pierre%20Florens%20and%20Kenji%20Nose-Filho%20and%20Jo%C3%A3o%20M.%20T.%20Romano&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20bias%20mitigation%20of%20algorithmic%20decisions%20in%20a%0Asetting%20where%20both%20the%20output%20of%20the%20algorithm%20and%20the%20sensitive%20variable%20are%0Acontinuous.%20Most%20of%20prior%20work%20deals%20with%20discrete%20sensitive%20variables%2C%20meaning%0Athat%20the%20biases%20are%20measured%20for%20subgroups%20of%20persons%20defined%20by%20a%20label%2C%0Aleaving%20out%20important%20algorithmic%20bias%20cases%2C%20where%20the%20sensitive%20variable%20is%0Acontinuous.%20Typical%20examples%20are%20unfair%20decisions%20made%20with%20respect%20to%20the%20age%0Aor%20the%20financial%20status.%20In%20our%20work%2C%20we%20then%20propose%20a%20bias%20mitigation%0Astrategy%20for%20continuous%20sensitive%20variables%2C%20based%20on%20the%20notion%20of%20endogeneity%0Awhich%20comes%20from%20the%20field%20of%20econometrics.%20In%20addition%20to%20solve%20this%20new%0Aproblem%2C%20our%20bias%20mitigation%20strategy%20is%20a%20weakly%20supervised%20learning%20method%0Awhich%20requires%20that%20a%20small%20portion%20of%20the%20data%20can%20be%20measured%20in%20a%20fair%0Amanner.%20It%20is%20model%20agnostic%2C%20in%20the%20sense%20that%20it%20does%20not%20make%20any%20hypothesis%0Aon%20the%20prediction%20model.%20It%20also%20makes%20use%20of%20a%20reasonably%20large%20amount%20of%0Ainput%20observations%20and%20their%20corresponding%20predictions.%20Only%20a%20small%20fraction%0Aof%20the%20true%20output%20predictions%20should%20be%20known.%20This%20therefore%20limits%20the%20need%0Afor%20expert%20interventions.%20Results%20obtained%20on%20synthetic%20data%20show%20the%0Aeffectiveness%20of%20our%20approach%20for%20examples%20as%20close%20as%20possible%20to%20real-life%0Aapplications%20in%20econometrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15477v1&entry.124074799=Read"},
{"title": "Classification Under Strategic Self-Selection", "author": "Guy Horowitz and Yonatan Sommer and Moran Koren and Nir Rosenfeld", "abstract": "  When users stand to gain from certain predictions, they are prone to act\nstrategically to obtain favorable predictive outcomes. Whereas most works on\nstrategic classification consider user actions that manifest as feature\nmodifications, we study a novel setting in which users decide -- in response to\nthe learned classifier -- whether to at all participate (or not). For learning\napproaches of increasing strategic awareness, we study the effects of\nself-selection on learning, and the implications of learning on the composition\nof the self-selected population. We then propose a differentiable framework for\nlearning under self-selective behavior, which can be optimized effectively. We\nconclude with experiments on real data and simulated behavior that both\ncomplement our analysis and demonstrate the utility of our approach.\n", "link": "http://arxiv.org/abs/2402.15274v1", "date": "2024-02-23", "relevancy": 1.8418, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4705}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4692}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4477}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20Under%20Strategic%20Self-Selection&entry.906535625=Guy%20Horowitz%20and%20Yonatan%20Sommer%20and%20Moran%20Koren%20and%20Nir%20Rosenfeld&entry.1292438233=%20%20When%20users%20stand%20to%20gain%20from%20certain%20predictions%2C%20they%20are%20prone%20to%20act%0Astrategically%20to%20obtain%20favorable%20predictive%20outcomes.%20Whereas%20most%20works%20on%0Astrategic%20classification%20consider%20user%20actions%20that%20manifest%20as%20feature%0Amodifications%2C%20we%20study%20a%20novel%20setting%20in%20which%20users%20decide%20--%20in%20response%20to%0Athe%20learned%20classifier%20--%20whether%20to%20at%20all%20participate%20%28or%20not%29.%20For%20learning%0Aapproaches%20of%20increasing%20strategic%20awareness%2C%20we%20study%20the%20effects%20of%0Aself-selection%20on%20learning%2C%20and%20the%20implications%20of%20learning%20on%20the%20composition%0Aof%20the%20self-selected%20population.%20We%20then%20propose%20a%20differentiable%20framework%20for%0Alearning%20under%20self-selective%20behavior%2C%20which%20can%20be%20optimized%20effectively.%20We%0Aconclude%20with%20experiments%20on%20real%20data%20and%20simulated%20behavior%20that%20both%0Acomplement%20our%20analysis%20and%20demonstrate%20the%20utility%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15274v1&entry.124074799=Read"},
{"title": "Can we forget how we learned? Doxastic redundancy in iterated belief\n  revision", "author": "Paolo Liberatore", "abstract": "  How information was acquired may become irrelevant. An obvious case is when\nsomething is confirmed many times. In terms of iterated belief revision, a\nspecific revision may become irrelevant in presence of others. Simple\nrepetitions are an example, but not the only case when this happens. Sometimes,\na revision becomes redundant even in presence of none equal, or even no else\nimplying it. A necessary and sufficient condition for the redundancy of the\nfirst of a sequence of lexicographic revisions is given. The problem is\ncoNP-complete even with two propositional revisions only. Complexity is the\nsame in the Horn case but only with an unbounded number of revisions: it\nbecomes polynomial with two revisions. Lexicographic revisions are not only\nrelevant by themselves, but also because sequences of them are the most compact\nof the common mechanisms used to represent the state of an iterated revision\nprocess. Shortening sequences of lexicographic revisions is shortening the most\ncompact representations of iterated belief revision states.\n", "link": "http://arxiv.org/abs/2402.15445v1", "date": "2024-02-23", "relevancy": 1.2103, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4307}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3822}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.3564}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20we%20forget%20how%20we%20learned%3F%20Doxastic%20redundancy%20in%20iterated%20belief%0A%20%20revision&entry.906535625=Paolo%20Liberatore&entry.1292438233=%20%20How%20information%20was%20acquired%20may%20become%20irrelevant.%20An%20obvious%20case%20is%20when%0Asomething%20is%20confirmed%20many%20times.%20In%20terms%20of%20iterated%20belief%20revision%2C%20a%0Aspecific%20revision%20may%20become%20irrelevant%20in%20presence%20of%20others.%20Simple%0Arepetitions%20are%20an%20example%2C%20but%20not%20the%20only%20case%20when%20this%20happens.%20Sometimes%2C%0Aa%20revision%20becomes%20redundant%20even%20in%20presence%20of%20none%20equal%2C%20or%20even%20no%20else%0Aimplying%20it.%20A%20necessary%20and%20sufficient%20condition%20for%20the%20redundancy%20of%20the%0Afirst%20of%20a%20sequence%20of%20lexicographic%20revisions%20is%20given.%20The%20problem%20is%0AcoNP-complete%20even%20with%20two%20propositional%20revisions%20only.%20Complexity%20is%20the%0Asame%20in%20the%20Horn%20case%20but%20only%20with%20an%20unbounded%20number%20of%20revisions%3A%20it%0Abecomes%20polynomial%20with%20two%20revisions.%20Lexicographic%20revisions%20are%20not%20only%0Arelevant%20by%20themselves%2C%20but%20also%20because%20sequences%20of%20them%20are%20the%20most%20compact%0Aof%20the%20common%20mechanisms%20used%20to%20represent%20the%20state%20of%20an%20iterated%20revision%0Aprocess.%20Shortening%20sequences%20of%20lexicographic%20revisions%20is%20shortening%20the%20most%0Acompact%20representations%20of%20iterated%20belief%20revision%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15445v1&entry.124074799=Read"},
{"title": "Calibration of Deep Learning Classification Models in fNIRS", "author": "Zhihao Cao and Zizhou Luo", "abstract": "  Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool\nfor monitoring brain activity. The classification of fNIRS data in relation to\nconscious activity holds significance for advancing our understanding of the\nbrain and facilitating the development of brain-computer interfaces (BCI). Many\nresearchers have turned to deep learning to tackle the classification\nchallenges inherent in fNIRS data due to its strong generalization and\nrobustness. In the application of fNIRS, reliability is really important, and\none mathematical formulation of the reliability of confidence is calibration.\nHowever, many researchers overlook the important issue of calibration. To\naddress this gap, we propose integrating calibration into fNIRS field and\nassess the reliability of existing models. Surprisingly, our results indicate\npoor calibration performance in many proposed models. To advance calibration\ndevelopment in the fNIRS field, we summarize three practical tips. Through this\nletter, we hope to emphasize the critical role of calibration in fNIRS research\nand argue for enhancing the reliability of deep learning-based predictions in\nfNIRS classification tasks. All data from our experimental process are openly\navailable on GitHub.\n", "link": "http://arxiv.org/abs/2402.15266v1", "date": "2024-02-23", "relevancy": 1.7997, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4542}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.452}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4462}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibration%20of%20Deep%20Learning%20Classification%20Models%20in%20fNIRS&entry.906535625=Zhihao%20Cao%20and%20Zizhou%20Luo&entry.1292438233=%20%20Functional%20near-infrared%20spectroscopy%20%28fNIRS%29%20is%20a%20valuable%20non-invasive%20tool%0Afor%20monitoring%20brain%20activity.%20The%20classification%20of%20fNIRS%20data%20in%20relation%20to%0Aconscious%20activity%20holds%20significance%20for%20advancing%20our%20understanding%20of%20the%0Abrain%20and%20facilitating%20the%20development%20of%20brain-computer%20interfaces%20%28BCI%29.%20Many%0Aresearchers%20have%20turned%20to%20deep%20learning%20to%20tackle%20the%20classification%0Achallenges%20inherent%20in%20fNIRS%20data%20due%20to%20its%20strong%20generalization%20and%0Arobustness.%20In%20the%20application%20of%20fNIRS%2C%20reliability%20is%20really%20important%2C%20and%0Aone%20mathematical%20formulation%20of%20the%20reliability%20of%20confidence%20is%20calibration.%0AHowever%2C%20many%20researchers%20overlook%20the%20important%20issue%20of%20calibration.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20integrating%20calibration%20into%20fNIRS%20field%20and%0Aassess%20the%20reliability%20of%20existing%20models.%20Surprisingly%2C%20our%20results%20indicate%0Apoor%20calibration%20performance%20in%20many%20proposed%20models.%20To%20advance%20calibration%0Adevelopment%20in%20the%20fNIRS%20field%2C%20we%20summarize%20three%20practical%20tips.%20Through%20this%0Aletter%2C%20we%20hope%20to%20emphasize%20the%20critical%20role%20of%20calibration%20in%20fNIRS%20research%0Aand%20argue%20for%20enhancing%20the%20reliability%20of%20deep%20learning-based%20predictions%20in%0AfNIRS%20classification%20tasks.%20All%20data%20from%20our%20experimental%20process%20are%20openly%0Aavailable%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15266v1&entry.124074799=Read"},
{"title": "Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A\n  Case-Study in E-Commerce Opinion Summarization", "author": "Swaroop Nath and Tejpalsingh Siledar and Sankara Sri Raghava Ravindra Muddu and Rupasai Rangaraju and Harshad Khadilkar and Pushpak Bhattacharyya and Suman Banerjee and Amey Patil and Sudhanshu Shekhar Singh and Muthusamy Chelliah and Nikesh Garera", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has become a dominating\nstrategy in steering Language Models (LMs) towards human values/goals. The key\nto the strategy is employing a reward model ({$\\varphi$}) which can reflect a\nlatent reward model with humans. While this strategy has proven to be\neffective, the training methodology requires a lot of human preference\nannotation (usually of the order of tens of thousands) to train {$\\varphi$}.\nSuch large-scale preference annotations can be achievable if the reward model\ncan be ubiquitously used. However, human values/goals are subjective and depend\non the nature of the task. This poses a challenge in collecting diverse\npreferences for downstream applications. To address this, we propose a novel\nmethodology to infuse domain knowledge into {$\\varphi$}, which reduces the size\nof preference annotation required. We validate our approach in E-Commerce\nOpinion Summarization, with a significant reduction in dataset size (just $940$\nsamples) while advancing the state-of-the-art. Our contributions include a\nnovel Reward Modelling technique, a new dataset (PromptOpinSumm) for Opinion\nSummarization, and a human preference dataset (OpinPref). The proposed\nmethodology opens avenues for efficient RLHF, making it more adaptable to\ndiverse applications with varying human values. We release the artifacts for\nusage under MIT License.\n", "link": "http://arxiv.org/abs/2402.15473v1", "date": "2024-02-23", "relevancy": 1.3954, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4664}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4495}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Domain%20Knowledge%20for%20Efficient%20Reward%20Modelling%20in%20RLHF%3A%20A%0A%20%20Case-Study%20in%20E-Commerce%20Opinion%20Summarization&entry.906535625=Swaroop%20Nath%20and%20Tejpalsingh%20Siledar%20and%20Sankara%20Sri%20Raghava%20Ravindra%20Muddu%20and%20Rupasai%20Rangaraju%20and%20Harshad%20Khadilkar%20and%20Pushpak%20Bhattacharyya%20and%20Suman%20Banerjee%20and%20Amey%20Patil%20and%20Sudhanshu%20Shekhar%20Singh%20and%20Muthusamy%20Chelliah%20and%20Nikesh%20Garera&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20become%20a%20dominating%0Astrategy%20in%20steering%20Language%20Models%20%28LMs%29%20towards%20human%20values/goals.%20The%20key%0Ato%20the%20strategy%20is%20employing%20a%20reward%20model%20%28%7B%24%5Cvarphi%24%7D%29%20which%20can%20reflect%20a%0Alatent%20reward%20model%20with%20humans.%20While%20this%20strategy%20has%20proven%20to%20be%0Aeffective%2C%20the%20training%20methodology%20requires%20a%20lot%20of%20human%20preference%0Aannotation%20%28usually%20of%20the%20order%20of%20tens%20of%20thousands%29%20to%20train%20%7B%24%5Cvarphi%24%7D.%0ASuch%20large-scale%20preference%20annotations%20can%20be%20achievable%20if%20the%20reward%20model%0Acan%20be%20ubiquitously%20used.%20However%2C%20human%20values/goals%20are%20subjective%20and%20depend%0Aon%20the%20nature%20of%20the%20task.%20This%20poses%20a%20challenge%20in%20collecting%20diverse%0Apreferences%20for%20downstream%20applications.%20To%20address%20this%2C%20we%20propose%20a%20novel%0Amethodology%20to%20infuse%20domain%20knowledge%20into%20%7B%24%5Cvarphi%24%7D%2C%20which%20reduces%20the%20size%0Aof%20preference%20annotation%20required.%20We%20validate%20our%20approach%20in%20E-Commerce%0AOpinion%20Summarization%2C%20with%20a%20significant%20reduction%20in%20dataset%20size%20%28just%20%24940%24%0Asamples%29%20while%20advancing%20the%20state-of-the-art.%20Our%20contributions%20include%20a%0Anovel%20Reward%20Modelling%20technique%2C%20a%20new%20dataset%20%28PromptOpinSumm%29%20for%20Opinion%0ASummarization%2C%20and%20a%20human%20preference%20dataset%20%28OpinPref%29.%20The%20proposed%0Amethodology%20opens%20avenues%20for%20efficient%20RLHF%2C%20making%20it%20more%20adaptable%20to%0Adiverse%20applications%20with%20varying%20human%20values.%20We%20release%20the%20artifacts%20for%0Ausage%20under%20MIT%20License.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15473v1&entry.124074799=Read"},
{"title": "Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for\n  Enhancing SocialBot Conversations", "author": "Ond\u0159ej Kobza and Jan \u010cuhel and Tommaso Gargiani and David Herel and Petr Marek", "abstract": "  We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize\nSocialBot Grand Challenge~5. Building upon previous versions of our system, we\nintroduce the NRG Barista and outline several innovative approaches for\nintegrating Barista into our SocialBot, improving the overall conversational\nexperience. Additionally, we extend our SocialBot to support multimodal\ndevices. This paper offers insights into the development of Alquist~5.0, which\nmeets evolving user expectations while maintaining empathetic and knowledgeable\nconversational abilities across diverse topics.\n", "link": "http://arxiv.org/abs/2310.16119v2", "date": "2024-02-23", "relevancy": 1.7649, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4685}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4539}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4089}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alquist%205.0%3A%20Dialogue%20Trees%20Meet%20Generative%20Models.%20A%20Novel%20Approach%20for%0A%20%20Enhancing%20SocialBot%20Conversations&entry.906535625=Ond%C5%99ej%20Kobza%20and%20Jan%20%C4%8Cuhel%20and%20Tommaso%20Gargiani%20and%20David%20Herel%20and%20Petr%20Marek&entry.1292438233=%20%20We%20present%20our%20SocialBot%20--%20Alquist~5.0%20--%20developed%20for%20the%20Alexa%20Prize%0ASocialBot%20Grand%20Challenge~5.%20Building%20upon%20previous%20versions%20of%20our%20system%2C%20we%0Aintroduce%20the%20NRG%20Barista%20and%20outline%20several%20innovative%20approaches%20for%0Aintegrating%20Barista%20into%20our%20SocialBot%2C%20improving%20the%20overall%20conversational%0Aexperience.%20Additionally%2C%20we%20extend%20our%20SocialBot%20to%20support%20multimodal%0Adevices.%20This%20paper%20offers%20insights%20into%20the%20development%20of%20Alquist~5.0%2C%20which%0Ameets%20evolving%20user%20expectations%20while%20maintaining%20empathetic%20and%20knowledgeable%0Aconversational%20abilities%20across%20diverse%20topics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16119v2&entry.124074799=Read"},
{"title": "Optimal Transport for Structure Learning Under Missing Data", "author": "Vy Vo and He Zhao and Trung Le and Edwin V. Bonilla and Dinh Phung", "abstract": "  Causal discovery in the presence of missing data introduces a chicken-and-egg\ndilemma. While the goal is to recover the true causal structure, robust\nimputation requires considering the dependencies or preferably causal relations\namong variables. Merely filling in missing values with existing imputation\nmethods and subsequently applying structure learning on the complete data is\nempirical shown to be sub-optimal. To this end, we propose in this paper a\nscore-based algorithm, based on optimal transport, for learning causal\nstructure from missing data. This optimal transport viewpoint diverges from\nexisting score-based approaches that are dominantly based on EM. We project\nstructure learning as a density fitting problem, where the goal is to find the\ncausal model that induces a distribution of minimum Wasserstein distance with\nthe distribution over the observed data. Through extensive simulations and\nreal-data experiments, our framework is shown to recover the true causal graphs\nmore effectively than the baselines in various simulations and real-data\nexperiments. Empirical evidences also demonstrate the superior scalability of\nour approach, along with the flexibility to incorporate any off-the-shelf\ncausal discovery methods for complete data.\n", "link": "http://arxiv.org/abs/2402.15255v1", "date": "2024-02-23", "relevancy": 1.7391, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4319}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4204}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Transport%20for%20Structure%20Learning%20Under%20Missing%20Data&entry.906535625=Vy%20Vo%20and%20He%20Zhao%20and%20Trung%20Le%20and%20Edwin%20V.%20Bonilla%20and%20Dinh%20Phung&entry.1292438233=%20%20Causal%20discovery%20in%20the%20presence%20of%20missing%20data%20introduces%20a%20chicken-and-egg%0Adilemma.%20While%20the%20goal%20is%20to%20recover%20the%20true%20causal%20structure%2C%20robust%0Aimputation%20requires%20considering%20the%20dependencies%20or%20preferably%20causal%20relations%0Aamong%20variables.%20Merely%20filling%20in%20missing%20values%20with%20existing%20imputation%0Amethods%20and%20subsequently%20applying%20structure%20learning%20on%20the%20complete%20data%20is%0Aempirical%20shown%20to%20be%20sub-optimal.%20To%20this%20end%2C%20we%20propose%20in%20this%20paper%20a%0Ascore-based%20algorithm%2C%20based%20on%20optimal%20transport%2C%20for%20learning%20causal%0Astructure%20from%20missing%20data.%20This%20optimal%20transport%20viewpoint%20diverges%20from%0Aexisting%20score-based%20approaches%20that%20are%20dominantly%20based%20on%20EM.%20We%20project%0Astructure%20learning%20as%20a%20density%20fitting%20problem%2C%20where%20the%20goal%20is%20to%20find%20the%0Acausal%20model%20that%20induces%20a%20distribution%20of%20minimum%20Wasserstein%20distance%20with%0Athe%20distribution%20over%20the%20observed%20data.%20Through%20extensive%20simulations%20and%0Areal-data%20experiments%2C%20our%20framework%20is%20shown%20to%20recover%20the%20true%20causal%20graphs%0Amore%20effectively%20than%20the%20baselines%20in%20various%20simulations%20and%20real-data%0Aexperiments.%20Empirical%20evidences%20also%20demonstrate%20the%20superior%20scalability%20of%0Aour%20approach%2C%20along%20with%20the%20flexibility%20to%20incorporate%20any%20off-the-shelf%0Acausal%20discovery%20methods%20for%20complete%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15255v1&entry.124074799=Read"},
{"title": "RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language\n  Models", "author": "Zijun Long and George Killick and Richard McCreadie and Gerardo Aragon Camarasa", "abstract": "  Robotic vision applications often necessitate a wide range of visual\nperception tasks, such as object detection, segmentation, and identification.\nWhile there have been substantial advances in these individual tasks,\nintegrating specialized models into a unified vision pipeline presents\nsignificant engineering challenges and costs. Recently, Multimodal Large\nLanguage Models (MLLMs) have emerged as novel backbones for various downstream\ntasks. We argue that leveraging the pre-training capabilities of MLLMs enables\nthe creation of a simplified framework, thus mitigating the need for\ntask-specific encoders. Specifically, the large-scale pretrained knowledge in\nMLLMs allows for easier fine-tuning to downstream robotic vision tasks and\nyields superior performance. We introduce the RoboLLM framework, equipped with\na BEiT-3 backbone, to address all visual perception tasks in the ARMBench\nchallenge-a large-scale robotic manipulation dataset about real-world warehouse\nscenarios. RoboLLM not only outperforms existing baselines but also\nsubstantially reduces the engineering burden associated with model selection\nand tuning. The source code is publicly available at\nhttps://github.com/longkukuhi/armbench.\n", "link": "http://arxiv.org/abs/2310.10221v2", "date": "2024-02-23", "relevancy": 1.7041, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5953}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5753}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5543}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboLLM%3A%20Robotic%20Vision%20Tasks%20Grounded%20on%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Zijun%20Long%20and%20George%20Killick%20and%20Richard%20McCreadie%20and%20Gerardo%20Aragon%20Camarasa&entry.1292438233=%20%20Robotic%20vision%20applications%20often%20necessitate%20a%20wide%20range%20of%20visual%0Aperception%20tasks%2C%20such%20as%20object%20detection%2C%20segmentation%2C%20and%20identification.%0AWhile%20there%20have%20been%20substantial%20advances%20in%20these%20individual%20tasks%2C%0Aintegrating%20specialized%20models%20into%20a%20unified%20vision%20pipeline%20presents%0Asignificant%20engineering%20challenges%20and%20costs.%20Recently%2C%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20have%20emerged%20as%20novel%20backbones%20for%20various%20downstream%0Atasks.%20We%20argue%20that%20leveraging%20the%20pre-training%20capabilities%20of%20MLLMs%20enables%0Athe%20creation%20of%20a%20simplified%20framework%2C%20thus%20mitigating%20the%20need%20for%0Atask-specific%20encoders.%20Specifically%2C%20the%20large-scale%20pretrained%20knowledge%20in%0AMLLMs%20allows%20for%20easier%20fine-tuning%20to%20downstream%20robotic%20vision%20tasks%20and%0Ayields%20superior%20performance.%20We%20introduce%20the%20RoboLLM%20framework%2C%20equipped%20with%0Aa%20BEiT-3%20backbone%2C%20to%20address%20all%20visual%20perception%20tasks%20in%20the%20ARMBench%0Achallenge-a%20large-scale%20robotic%20manipulation%20dataset%20about%20real-world%20warehouse%0Ascenarios.%20RoboLLM%20not%20only%20outperforms%20existing%20baselines%20but%20also%0Asubstantially%20reduces%20the%20engineering%20burden%20associated%20with%20model%20selection%0Aand%20tuning.%20The%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/longkukuhi/armbench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10221v2&entry.124074799=Read"},
{"title": "Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering\n  Error in Sub-Exponential Mixture Models", "author": "Maximilien Dreveton and Alperen G\u00f6zeten and Matthias Grossglauser and Patrick Thiran", "abstract": "  Clustering is a pivotal challenge in unsupervised machine learning and is\noften investigated through the lens of mixture models. The optimal error rate\nfor recovering cluster labels in Gaussian and sub-Gaussian mixture models\ninvolves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as\nLloyd's algorithm, attain this optimal error rate. In this paper, we first\nestablish a universal lower bound for the error rate in clustering any mixture\nmodel, expressed through a Chernoff divergence, a more versatile measure of\nmodel information than signal-to-noise ratios. We then demonstrate that\niterative algorithms attain this lower bound in mixture models with\nsub-exponential tails, notably emphasizing location-scale mixtures featuring\nLaplace-distributed errors. Additionally, for datasets better modelled by\nPoisson or Negative Binomial mixtures, we study mixture models whose\ndistributions belong to an exponential family. In such mixtures, we establish\nthat Bregman hard clustering, a variant of Lloyd's algorithm employing a\nBregman divergence, is rate optimal.\n", "link": "http://arxiv.org/abs/2402.15432v1", "date": "2024-02-23", "relevancy": 1.6718, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4295}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.412}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4039}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Lower%20Bounds%20and%20Optimal%20Rates%3A%20Achieving%20Minimax%20Clustering%0A%20%20Error%20in%20Sub-Exponential%20Mixture%20Models&entry.906535625=Maximilien%20Dreveton%20and%20Alperen%20G%C3%B6zeten%20and%20Matthias%20Grossglauser%20and%20Patrick%20Thiran&entry.1292438233=%20%20Clustering%20is%20a%20pivotal%20challenge%20in%20unsupervised%20machine%20learning%20and%20is%0Aoften%20investigated%20through%20the%20lens%20of%20mixture%20models.%20The%20optimal%20error%20rate%0Afor%20recovering%20cluster%20labels%20in%20Gaussian%20and%20sub-Gaussian%20mixture%20models%0Ainvolves%20ad%20hoc%20signal-to-noise%20ratios.%20Simple%20iterative%20algorithms%2C%20such%20as%0ALloyd%27s%20algorithm%2C%20attain%20this%20optimal%20error%20rate.%20In%20this%20paper%2C%20we%20first%0Aestablish%20a%20universal%20lower%20bound%20for%20the%20error%20rate%20in%20clustering%20any%20mixture%0Amodel%2C%20expressed%20through%20a%20Chernoff%20divergence%2C%20a%20more%20versatile%20measure%20of%0Amodel%20information%20than%20signal-to-noise%20ratios.%20We%20then%20demonstrate%20that%0Aiterative%20algorithms%20attain%20this%20lower%20bound%20in%20mixture%20models%20with%0Asub-exponential%20tails%2C%20notably%20emphasizing%20location-scale%20mixtures%20featuring%0ALaplace-distributed%20errors.%20Additionally%2C%20for%20datasets%20better%20modelled%20by%0APoisson%20or%20Negative%20Binomial%20mixtures%2C%20we%20study%20mixture%20models%20whose%0Adistributions%20belong%20to%20an%20exponential%20family.%20In%20such%20mixtures%2C%20we%20establish%0Athat%20Bregman%20hard%20clustering%2C%20a%20variant%20of%20Lloyd%27s%20algorithm%20employing%20a%0ABregman%20divergence%2C%20is%20rate%20optimal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15432v1&entry.124074799=Read"},
{"title": "TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow\n  Attention for Commuting Flow Prediction", "author": "Yan Luo and Zhuoyue Wan and Yuzhong Chen and Gengchen Mai and Fu-lai Chung and Kent Larson", "abstract": "  Understanding the link between urban planning and commuting flows is crucial\nfor guiding urban development and policymaking. This research, bridging\ncomputer science and urban studies, addresses the challenge of integrating\nthese fields with their distinct focuses. Traditional urban studies methods,\nlike the gravity and radiation models, often underperform in complex scenarios\ndue to their limited handling of multiple variables and reliance on overly\nsimplistic and unrealistic assumptions, such as spatial isotropy. While deep\nlearning models offer improved accuracy, their black-box nature poses a\ntrade-off between performance and explainability -- both vital for analyzing\ncomplex societal phenomena like commuting flows. To address this, we introduce\nTransFlower, an explainable, transformer-based model employing flow-to-flow\nattention to predict urban commuting patterns. It features a geospatial encoder\nwith an anisotropy-aware relative location encoder for nuanced flow\nrepresentation. Following this, the transformer-based flow predictor enhances\nthis by leveraging attention mechanisms to efficiently capture flow\ninteractions. Our model outperforms existing methods by up to 30.8% Common Part\nof Commuters, offering insights into mobility dynamics crucial for urban\nplanning and policy decisions.\n", "link": "http://arxiv.org/abs/2402.15398v1", "date": "2024-02-23", "relevancy": 1.4029, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4981}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4558}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransFlower%3A%20An%20Explainable%20Transformer-Based%20Model%20with%20Flow-to-Flow%0A%20%20Attention%20for%20Commuting%20Flow%20Prediction&entry.906535625=Yan%20Luo%20and%20Zhuoyue%20Wan%20and%20Yuzhong%20Chen%20and%20Gengchen%20Mai%20and%20Fu-lai%20Chung%20and%20Kent%20Larson&entry.1292438233=%20%20Understanding%20the%20link%20between%20urban%20planning%20and%20commuting%20flows%20is%20crucial%0Afor%20guiding%20urban%20development%20and%20policymaking.%20This%20research%2C%20bridging%0Acomputer%20science%20and%20urban%20studies%2C%20addresses%20the%20challenge%20of%20integrating%0Athese%20fields%20with%20their%20distinct%20focuses.%20Traditional%20urban%20studies%20methods%2C%0Alike%20the%20gravity%20and%20radiation%20models%2C%20often%20underperform%20in%20complex%20scenarios%0Adue%20to%20their%20limited%20handling%20of%20multiple%20variables%20and%20reliance%20on%20overly%0Asimplistic%20and%20unrealistic%20assumptions%2C%20such%20as%20spatial%20isotropy.%20While%20deep%0Alearning%20models%20offer%20improved%20accuracy%2C%20their%20black-box%20nature%20poses%20a%0Atrade-off%20between%20performance%20and%20explainability%20--%20both%20vital%20for%20analyzing%0Acomplex%20societal%20phenomena%20like%20commuting%20flows.%20To%20address%20this%2C%20we%20introduce%0ATransFlower%2C%20an%20explainable%2C%20transformer-based%20model%20employing%20flow-to-flow%0Aattention%20to%20predict%20urban%20commuting%20patterns.%20It%20features%20a%20geospatial%20encoder%0Awith%20an%20anisotropy-aware%20relative%20location%20encoder%20for%20nuanced%20flow%0Arepresentation.%20Following%20this%2C%20the%20transformer-based%20flow%20predictor%20enhances%0Athis%20by%20leveraging%20attention%20mechanisms%20to%20efficiently%20capture%20flow%0Ainteractions.%20Our%20model%20outperforms%20existing%20methods%20by%20up%20to%2030.8%25%20Common%20Part%0Aof%20Commuters%2C%20offering%20insights%20into%20mobility%20dynamics%20crucial%20for%20urban%0Aplanning%20and%20policy%20decisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15398v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


