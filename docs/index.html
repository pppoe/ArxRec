<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 1200px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "author": "Helbert Paat and Qing Lian and Weilong Yao and Tong Zhang", "abstract": "  Advancements in deep learning-based 3D object detection necessitate the\navailability of large-scale datasets. However, this requirement introduces the\nchallenge of manual annotation, which is often both burdensome and\ntime-consuming. To tackle this issue, the literature has seen the emergence of\nseveral weakly supervised frameworks for 3D object detection which can\nautomatically generate pseudo labels for unlabeled data. Nevertheless, these\ngenerated pseudo labels contain noise and are not as accurate as those labeled\nby humans. In this paper, we present the first approach that addresses the\ninherent ambiguities present in pseudo labels by introducing an Evidential Deep\nLearning (EDL) based uncertainty estimation framework. Specifically, we propose\nMEDL-U, an EDL framework based on MTrans, which not only generates pseudo\nlabels but also quantifies the associated uncertainties. However, applying EDL\nto 3D object detection presents three primary challenges: (1) relatively lower\npseudolabel quality in comparison to other autolabelers; (2) excessively high\nevidential uncertainty estimates; and (3) lack of clear interpretability and\neffective utilization of uncertainties for downstream tasks. We tackle these\nissues through the introduction of an uncertainty-aware IoU-based loss, an\nevidence-aware multi-task loss function, and the implementation of a\npost-processing stage for uncertainty refinement. Our experimental results\ndemonstrate that probabilistic detectors trained using the outputs of MEDL-U\nsurpass deterministic detectors trained using outputs from previous 3D\nannotators on the KITTI val set for all difficulty levels. Moreover, MEDL-U\nachieves state-of-the-art results on the KITTI official test set compared to\nexisting 3D automatic annotators.\n", "link": "http://arxiv.org/abs/2309.09599v3", "date": "2024-02-15", "relevancy": 4.1385, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 1.0}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6178}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5899}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEDL-U%3A%20Uncertainty-aware%203D%20Automatic%20Annotation%20based%20on%20Evidential%0A%20%20Deep%20Learning&entry.906535625=Helbert%20Paat%20and%20Qing%20Lian%20and%20Weilong%20Yao%20and%20Tong%20Zhang&entry.1292438233=%20%20Advancements%20in%20deep%20learning-based%203D%20object%20detection%20necessitate%20the%0Aavailability%20of%20large-scale%20datasets.%20However%2C%20this%20requirement%20introduces%20the%0Achallenge%20of%20manual%20annotation%2C%20which%20is%20often%20both%20burdensome%20and%0Atime-consuming.%20To%20tackle%20this%20issue%2C%20the%20literature%20has%20seen%20the%20emergence%20of%0Aseveral%20weakly%20supervised%20frameworks%20for%203D%20object%20detection%20which%20can%0Aautomatically%20generate%20pseudo%20labels%20for%20unlabeled%20data.%20Nevertheless%2C%20these%0Agenerated%20pseudo%20labels%20contain%20noise%20and%20are%20not%20as%20accurate%20as%20those%20labeled%0Aby%20humans.%20In%20this%20paper%2C%20we%20present%20the%20first%20approach%20that%20addresses%20the%0Ainherent%20ambiguities%20present%20in%20pseudo%20labels%20by%20introducing%20an%20Evidential%20Deep%0ALearning%20%28EDL%29%20based%20uncertainty%20estimation%20framework.%20Specifically%2C%20we%20propose%0AMEDL-U%2C%20an%20EDL%20framework%20based%20on%20MTrans%2C%20which%20not%20only%20generates%20pseudo%0Alabels%20but%20also%20quantifies%20the%20associated%20uncertainties.%20However%2C%20applying%20EDL%0Ato%203D%20object%20detection%20presents%20three%20primary%20challenges%3A%20%281%29%20relatively%20lower%0Apseudolabel%20quality%20in%20comparison%20to%20other%20autolabelers%3B%20%282%29%20excessively%20high%0Aevidential%20uncertainty%20estimates%3B%20and%20%283%29%20lack%20of%20clear%20interpretability%20and%0Aeffective%20utilization%20of%20uncertainties%20for%20downstream%20tasks.%20We%20tackle%20these%0Aissues%20through%20the%20introduction%20of%20an%20uncertainty-aware%20IoU-based%20loss%2C%20an%0Aevidence-aware%20multi-task%20loss%20function%2C%20and%20the%20implementation%20of%20a%0Apost-processing%20stage%20for%20uncertainty%20refinement.%20Our%20experimental%20results%0Ademonstrate%20that%20probabilistic%20detectors%20trained%20using%20the%20outputs%20of%20MEDL-U%0Asurpass%20deterministic%20detectors%20trained%20using%20outputs%20from%20previous%203D%0Aannotators%20on%20the%20KITTI%20val%20set%20for%20all%20difficulty%20levels.%20Moreover%2C%20MEDL-U%0Aachieves%20state-of-the-art%20results%20on%20the%20KITTI%20official%20test%20set%20compared%20to%0Aexisting%203D%20automatic%20annotators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09599v3&entry.124074799=Read"},
{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "author": "Parker Ewen and Hao Chen and Yuzhen Chen and Anran Li and Anup Bagali and Gitesh Gunjal and Ram Vasudevan", "abstract": "  Robots must be able to understand their surroundings to perform complex tasks\nin challenging environments and many of these complex tasks require estimates\nof physical properties such as friction or weight. Estimating such properties\nusing learning is challenging due to the large amounts of labelled data\nrequired for training and the difficulty of updating these learned models\nonline at run time. To overcome these challenges, this paper introduces a\nnovel, multi-modal approach for representing semantic predictions and physical\nproperty estimates jointly in a probabilistic manner. By using conjugate pairs,\nthe proposed method enables closed-form Bayesian updates given visual and\ntactile measurements without requiring additional training data. The efficacy\nof the proposed algorithm is demonstrated through several hardware experiments.\nIn particular, this paper illustrates that by conditioning semantic\nclassifications on physical properties, the proposed method quantitatively\noutperforms state-of-the-art semantic classification methods that rely on\nvision alone. To further illustrate its utility, the proposed method is used in\nseveral applications including to represent affordance-based properties\nprobabilistically and a challenging terrain traversal task using a legged\nrobot. In the latter task, the proposed method represents the coefficient of\nfriction of the terrain probabilistically, which enables the use of an on-line\nrisk-aware planner that switches the legged robot from a dynamic gait to a\nstatic, stable gait when the expected value of the coefficient of friction\nfalls below a given threshold. Videos of these case studies as well as the\nopen-source C++ and ROS interface can be found at\nhttps://roahmlab.github.io/multimodal_mapping/.\n", "link": "http://arxiv.org/abs/2402.05872v3", "date": "2024-02-15", "relevancy": 4.1172, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 1.0}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.61}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5658}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%27ve%20Got%20to%20Feel%20It%20To%20Believe%20It%3A%20Multi-Modal%20Bayesian%20Inference%20for%0A%20%20Semantic%20and%20Property%20Prediction&entry.906535625=Parker%20Ewen%20and%20Hao%20Chen%20and%20Yuzhen%20Chen%20and%20Anran%20Li%20and%20Anup%20Bagali%20and%20Gitesh%20Gunjal%20and%20Ram%20Vasudevan&entry.1292438233=%20%20Robots%20must%20be%20able%20to%20understand%20their%20surroundings%20to%20perform%20complex%20tasks%0Ain%20challenging%20environments%20and%20many%20of%20these%20complex%20tasks%20require%20estimates%0Aof%20physical%20properties%20such%20as%20friction%20or%20weight.%20Estimating%20such%20properties%0Ausing%20learning%20is%20challenging%20due%20to%20the%20large%20amounts%20of%20labelled%20data%0Arequired%20for%20training%20and%20the%20difficulty%20of%20updating%20these%20learned%20models%0Aonline%20at%20run%20time.%20To%20overcome%20these%20challenges%2C%20this%20paper%20introduces%20a%0Anovel%2C%20multi-modal%20approach%20for%20representing%20semantic%20predictions%20and%20physical%0Aproperty%20estimates%20jointly%20in%20a%20probabilistic%20manner.%20By%20using%20conjugate%20pairs%2C%0Athe%20proposed%20method%20enables%20closed-form%20Bayesian%20updates%20given%20visual%20and%0Atactile%20measurements%20without%20requiring%20additional%20training%20data.%20The%20efficacy%0Aof%20the%20proposed%20algorithm%20is%20demonstrated%20through%20several%20hardware%20experiments.%0AIn%20particular%2C%20this%20paper%20illustrates%20that%20by%20conditioning%20semantic%0Aclassifications%20on%20physical%20properties%2C%20the%20proposed%20method%20quantitatively%0Aoutperforms%20state-of-the-art%20semantic%20classification%20methods%20that%20rely%20on%0Avision%20alone.%20To%20further%20illustrate%20its%20utility%2C%20the%20proposed%20method%20is%20used%20in%0Aseveral%20applications%20including%20to%20represent%20affordance-based%20properties%0Aprobabilistically%20and%20a%20challenging%20terrain%20traversal%20task%20using%20a%20legged%0Arobot.%20In%20the%20latter%20task%2C%20the%20proposed%20method%20represents%20the%20coefficient%20of%0Afriction%20of%20the%20terrain%20probabilistically%2C%20which%20enables%20the%20use%20of%20an%20on-line%0Arisk-aware%20planner%20that%20switches%20the%20legged%20robot%20from%20a%20dynamic%20gait%20to%20a%0Astatic%2C%20stable%20gait%20when%20the%20expected%20value%20of%20the%20coefficient%20of%20friction%0Afalls%20below%20a%20given%20threshold.%20Videos%20of%20these%20case%20studies%20as%20well%20as%20the%0Aopen-source%20C%2B%2B%20and%20ROS%20interface%20can%20be%20found%20at%0Ahttps%3A//roahmlab.github.io/multimodal_mapping/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05872v3&entry.124074799=Read"},
{"title": "Online Algorithms for Hierarchical Inference in Deep Learning\n  applications at the Edge", "author": "Vishnu Narayanan Moothedath and Jaya Prakash Champati and James Gross", "abstract": "  We consider a resource-constrained Edge Device (ED), such as an IoT sensor or\na microcontroller unit, embedded with a small-size ML model (S-ML) for a\ngeneric classification application and an Edge Server (ES) that hosts a\nlarge-size ML model (L-ML). Since the inference accuracy of S-ML is lower than\nthat of the L-ML, offloading all the data samples to the ES results in high\ninference accuracy, but it defeats the purpose of embedding S-ML on the ED and\ndeprives the benefits of reduced latency, bandwidth savings, and energy\nefficiency of doing local inference. In order to get the best out of both\nworlds, i.e., the benefits of doing inference on the ED and the benefits of\ndoing inference on ES, we explore the idea of Hierarchical Inference (HI),\nwherein S-ML inference is only accepted when it is correct, otherwise the data\nsample is offloaded for L-ML inference. However, the ideal implementation of HI\nis infeasible as the correctness of the S-ML inference is not known to the ED.\nWe propose an online meta-learning framework that the ED can use to predict the\ncorrectness of the S-ML inference. In particular, we propose to use the maximum\nsoftmax value output by S-ML for a data sample and decide whether to offload it\nor not. The resulting online learning problem turns out to be a Prediction with\nExpert Advice (PEA) problem with continuous expert space. We propose two\ndifferent algorithms and prove sublinear regret bounds for them without any\nassumption on the smoothness of the loss function. We evaluate and benchmark\nthe performance of the proposed algorithms for image classification application\nusing four datasets, namely, Imagenette and Imagewoof, MNIST, and CIFAR-10.\n", "link": "http://arxiv.org/abs/2304.00891v2", "date": "2024-02-15", "relevancy": 4.0421, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5744}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.554}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5438}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Algorithms%20for%20Hierarchical%20Inference%20in%20Deep%20Learning%0A%20%20applications%20at%20the%20Edge&entry.906535625=Vishnu%20Narayanan%20Moothedath%20and%20Jaya%20Prakash%20Champati%20and%20James%20Gross&entry.1292438233=%20%20We%20consider%20a%20resource-constrained%20Edge%20Device%20%28ED%29%2C%20such%20as%20an%20IoT%20sensor%20or%0Aa%20microcontroller%20unit%2C%20embedded%20with%20a%20small-size%20ML%20model%20%28S-ML%29%20for%20a%0Ageneric%20classification%20application%20and%20an%20Edge%20Server%20%28ES%29%20that%20hosts%20a%0Alarge-size%20ML%20model%20%28L-ML%29.%20Since%20the%20inference%20accuracy%20of%20S-ML%20is%20lower%20than%0Athat%20of%20the%20L-ML%2C%20offloading%20all%20the%20data%20samples%20to%20the%20ES%20results%20in%20high%0Ainference%20accuracy%2C%20but%20it%20defeats%20the%20purpose%20of%20embedding%20S-ML%20on%20the%20ED%20and%0Adeprives%20the%20benefits%20of%20reduced%20latency%2C%20bandwidth%20savings%2C%20and%20energy%0Aefficiency%20of%20doing%20local%20inference.%20In%20order%20to%20get%20the%20best%20out%20of%20both%0Aworlds%2C%20i.e.%2C%20the%20benefits%20of%20doing%20inference%20on%20the%20ED%20and%20the%20benefits%20of%0Adoing%20inference%20on%20ES%2C%20we%20explore%20the%20idea%20of%20Hierarchical%20Inference%20%28HI%29%2C%0Awherein%20S-ML%20inference%20is%20only%20accepted%20when%20it%20is%20correct%2C%20otherwise%20the%20data%0Asample%20is%20offloaded%20for%20L-ML%20inference.%20However%2C%20the%20ideal%20implementation%20of%20HI%0Ais%20infeasible%20as%20the%20correctness%20of%20the%20S-ML%20inference%20is%20not%20known%20to%20the%20ED.%0AWe%20propose%20an%20online%20meta-learning%20framework%20that%20the%20ED%20can%20use%20to%20predict%20the%0Acorrectness%20of%20the%20S-ML%20inference.%20In%20particular%2C%20we%20propose%20to%20use%20the%20maximum%0Asoftmax%20value%20output%20by%20S-ML%20for%20a%20data%20sample%20and%20decide%20whether%20to%20offload%20it%0Aor%20not.%20The%20resulting%20online%20learning%20problem%20turns%20out%20to%20be%20a%20Prediction%20with%0AExpert%20Advice%20%28PEA%29%20problem%20with%20continuous%20expert%20space.%20We%20propose%20two%0Adifferent%20algorithms%20and%20prove%20sublinear%20regret%20bounds%20for%20them%20without%20any%0Aassumption%20on%20the%20smoothness%20of%20the%20loss%20function.%20We%20evaluate%20and%20benchmark%0Athe%20performance%20of%20the%20proposed%20algorithms%20for%20image%20classification%20application%0Ausing%20four%20datasets%2C%20namely%2C%20Imagenette%20and%20Imagewoof%2C%20MNIST%2C%20and%20CIFAR-10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.00891v2&entry.124074799=Read"},
{"title": "MC-DBN: A Deep Belief Network-Based Model for Modality Completion", "author": "Zihong Luo and Haochen Xue and Mingyu Jin and Chengzhi Liu and Zile Huang and Chong Zhang and Shuliang Zhao", "abstract": "  Recent advancements in multi-modal artificial intelligence (AI) have\nrevolutionized the fields of stock market forecasting and heart rate\nmonitoring. Utilizing diverse data sources can substantially improve prediction\naccuracy. Nonetheless, additional data may not always align with the original\ndataset. Interpolation methods are commonly utilized for handling missing\nvalues in modal data, though they may exhibit limitations in the context of\nsparse information. Addressing this challenge, we propose a Modality Completion\nDeep Belief Network-Based Model (MC-DBN). This approach utilizes implicit\nfeatures of complete data to compensate for gaps between itself and additional\nincomplete data. It ensures that the enhanced multi-modal data closely aligns\nwith the dynamic nature of the real world to enhance the effectiveness of the\nmodel. We conduct evaluations of the MC-DBN model in two datasets from the\nstock market forecasting and heart rate monitoring domains. Comprehensive\nexperiments showcase the model's capacity to bridge the semantic divide present\nin multi-modal data, subsequently enhancing its performance. The source code is\navailable at: https://github.com/logan-0623/DBN-generate\n", "link": "http://arxiv.org/abs/2402.09782v1", "date": "2024-02-15", "relevancy": 3.9788, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5722}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5269}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.516}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MC-DBN%3A%20A%20Deep%20Belief%20Network-Based%20Model%20for%20Modality%20Completion&entry.906535625=Zihong%20Luo%20and%20Haochen%20Xue%20and%20Mingyu%20Jin%20and%20Chengzhi%20Liu%20and%20Zile%20Huang%20and%20Chong%20Zhang%20and%20Shuliang%20Zhao&entry.1292438233=%20%20Recent%20advancements%20in%20multi-modal%20artificial%20intelligence%20%28AI%29%20have%0Arevolutionized%20the%20fields%20of%20stock%20market%20forecasting%20and%20heart%20rate%0Amonitoring.%20Utilizing%20diverse%20data%20sources%20can%20substantially%20improve%20prediction%0Aaccuracy.%20Nonetheless%2C%20additional%20data%20may%20not%20always%20align%20with%20the%20original%0Adataset.%20Interpolation%20methods%20are%20commonly%20utilized%20for%20handling%20missing%0Avalues%20in%20modal%20data%2C%20though%20they%20may%20exhibit%20limitations%20in%20the%20context%20of%0Asparse%20information.%20Addressing%20this%20challenge%2C%20we%20propose%20a%20Modality%20Completion%0ADeep%20Belief%20Network-Based%20Model%20%28MC-DBN%29.%20This%20approach%20utilizes%20implicit%0Afeatures%20of%20complete%20data%20to%20compensate%20for%20gaps%20between%20itself%20and%20additional%0Aincomplete%20data.%20It%20ensures%20that%20the%20enhanced%20multi-modal%20data%20closely%20aligns%0Awith%20the%20dynamic%20nature%20of%20the%20real%20world%20to%20enhance%20the%20effectiveness%20of%20the%0Amodel.%20We%20conduct%20evaluations%20of%20the%20MC-DBN%20model%20in%20two%20datasets%20from%20the%0Astock%20market%20forecasting%20and%20heart%20rate%20monitoring%20domains.%20Comprehensive%0Aexperiments%20showcase%20the%20model%27s%20capacity%20to%20bridge%20the%20semantic%20divide%20present%0Ain%20multi-modal%20data%2C%20subsequently%20enhancing%20its%20performance.%20The%20source%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/logan-0623/DBN-generate%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09782v1&entry.124074799=Read"},
{"title": "Is Continual Learning Ready for Real-world Challenges?", "author": "Theodora Kontogianni and Yuanwen Yue and Siyu Tang and Konrad Schindler", "abstract": "  Despite continual learning's long and well-established academic history, its\napplication in real-world scenarios remains rather limited. This paper contends\nthat this gap is attributable to a misalignment between the actual challenges\nof continual learning and the evaluation protocols in use, rendering proposed\nsolutions ineffective for addressing the complexities of real-world setups. We\nvalidate our hypothesis and assess progress to date, using a new 3D semantic\nsegmentation benchmark, OCL-3DSS. We investigate various continual learning\nschemes from the literature by utilizing more realistic protocols that\nnecessitate online and continual learning for dynamic, real-world scenarios\n(eg., in robotics and 3D vision applications). The outcomes are sobering: all\nconsidered methods perform poorly, significantly deviating from the upper bound\nof joint offline training. This raises questions about the applicability of\nexisting methods in realistic settings. Our paper aims to initiate a paradigm\nshift, advocating for the adoption of continual learning methods through new\nexperimental protocols that better emulate real-world conditions to facilitate\nbreakthroughs in the field.\n", "link": "http://arxiv.org/abs/2402.10130v1", "date": "2024-02-15", "relevancy": 3.9046, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5373}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5288}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5261}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Continual%20Learning%20Ready%20for%20Real-world%20Challenges%3F&entry.906535625=Theodora%20Kontogianni%20and%20Yuanwen%20Yue%20and%20Siyu%20Tang%20and%20Konrad%20Schindler&entry.1292438233=%20%20Despite%20continual%20learning%27s%20long%20and%20well-established%20academic%20history%2C%20its%0Aapplication%20in%20real-world%20scenarios%20remains%20rather%20limited.%20This%20paper%20contends%0Athat%20this%20gap%20is%20attributable%20to%20a%20misalignment%20between%20the%20actual%20challenges%0Aof%20continual%20learning%20and%20the%20evaluation%20protocols%20in%20use%2C%20rendering%20proposed%0Asolutions%20ineffective%20for%20addressing%20the%20complexities%20of%20real-world%20setups.%20We%0Avalidate%20our%20hypothesis%20and%20assess%20progress%20to%20date%2C%20using%20a%20new%203D%20semantic%0Asegmentation%20benchmark%2C%20OCL-3DSS.%20We%20investigate%20various%20continual%20learning%0Aschemes%20from%20the%20literature%20by%20utilizing%20more%20realistic%20protocols%20that%0Anecessitate%20online%20and%20continual%20learning%20for%20dynamic%2C%20real-world%20scenarios%0A%28eg.%2C%20in%20robotics%20and%203D%20vision%20applications%29.%20The%20outcomes%20are%20sobering%3A%20all%0Aconsidered%20methods%20perform%20poorly%2C%20significantly%20deviating%20from%20the%20upper%20bound%0Aof%20joint%20offline%20training.%20This%20raises%20questions%20about%20the%20applicability%20of%0Aexisting%20methods%20in%20realistic%20settings.%20Our%20paper%20aims%20to%20initiate%20a%20paradigm%0Ashift%2C%20advocating%20for%20the%20adoption%20of%20continual%20learning%20methods%20through%20new%0Aexperimental%20protocols%20that%20better%20emulate%20real-world%20conditions%20to%20facilitate%0Abreakthroughs%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10130v1&entry.124074799=Read"},
{"title": "Tell Me More! Towards Implicit User Intention Understanding of Language\n  Model Driven Agents", "author": "Cheng Qian and Bingxiang He and Zhong Zhuang and Jia Deng and Yujia Qin and Xin Cong and Zhong Zhang and Jie Zhou and Yankai Lin and Zhiyuan Liu and Maosong Sun", "abstract": "  Current language model-driven agents often lack mechanisms for effective user\nparticipation, which is crucial given the vagueness commonly found in user\ninstructions. Although adept at devising strategies and performing tasks, these\nagents struggle with seeking clarification and grasping precise user\nintentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a\nnovel benchmark designed to inspect users' implicit intentions through explicit\nqueries. Next, we propose the incorporation of model experts as the upstream in\nagent designs to enhance user-agent interaction. Employing IN3, we empirically\ntrain Mistral-Interact, a powerful model that proactively assesses task\nvagueness, inquires user intentions, and refines them into actionable goals\nbefore starting downstream agent task execution. Integrating it into the XAgent\nframework, we comprehensively evaluate the enhanced agent system regarding user\ninstruction understanding and execution, revealing that our approach notably\nexcels at identifying vague user tasks, recovering and summarizing critical\nmissing information, setting precise and necessary agent execution goals, and\nminimizing redundant tool usage, thus boosting overall efficiency. All the data\nand codes are released.\n", "link": "http://arxiv.org/abs/2402.09205v2", "date": "2024-02-15", "relevancy": 3.765, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5221}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5065}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5046}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tell%20Me%20More%21%20Towards%20Implicit%20User%20Intention%20Understanding%20of%20Language%0A%20%20Model%20Driven%20Agents&entry.906535625=Cheng%20Qian%20and%20Bingxiang%20He%20and%20Zhong%20Zhuang%20and%20Jia%20Deng%20and%20Yujia%20Qin%20and%20Xin%20Cong%20and%20Zhong%20Zhang%20and%20Jie%20Zhou%20and%20Yankai%20Lin%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Current%20language%20model-driven%20agents%20often%20lack%20mechanisms%20for%20effective%20user%0Aparticipation%2C%20which%20is%20crucial%20given%20the%20vagueness%20commonly%20found%20in%20user%0Ainstructions.%20Although%20adept%20at%20devising%20strategies%20and%20performing%20tasks%2C%20these%0Aagents%20struggle%20with%20seeking%20clarification%20and%20grasping%20precise%20user%0Aintentions.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Intention-in-Interaction%20%28IN3%29%2C%20a%0Anovel%20benchmark%20designed%20to%20inspect%20users%27%20implicit%20intentions%20through%20explicit%0Aqueries.%20Next%2C%20we%20propose%20the%20incorporation%20of%20model%20experts%20as%20the%20upstream%20in%0Aagent%20designs%20to%20enhance%20user-agent%20interaction.%20Employing%20IN3%2C%20we%20empirically%0Atrain%20Mistral-Interact%2C%20a%20powerful%20model%20that%20proactively%20assesses%20task%0Avagueness%2C%20inquires%20user%20intentions%2C%20and%20refines%20them%20into%20actionable%20goals%0Abefore%20starting%20downstream%20agent%20task%20execution.%20Integrating%20it%20into%20the%20XAgent%0Aframework%2C%20we%20comprehensively%20evaluate%20the%20enhanced%20agent%20system%20regarding%20user%0Ainstruction%20understanding%20and%20execution%2C%20revealing%20that%20our%20approach%20notably%0Aexcels%20at%20identifying%20vague%20user%20tasks%2C%20recovering%20and%20summarizing%20critical%0Amissing%20information%2C%20setting%20precise%20and%20necessary%20agent%20execution%20goals%2C%20and%0Aminimizing%20redundant%20tool%20usage%2C%20thus%20boosting%20overall%20efficiency.%20All%20the%20data%0Aand%20codes%20are%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09205v2&entry.124074799=Read"},
{"title": "ED2: Environment Dynamics Decomposition World Models for Continuous\n  Control", "author": "Jianye Hao and Yifu Yuan and Cong Wang and Zhen Wang", "abstract": "  Model-based reinforcement learning (MBRL) achieves significant sample\nefficiency in practice in comparison to model-free RL, but its performance is\noften limited by the existence of model prediction error. To reduce the model\nerror, standard MBRL approaches train a single well-designed network to fit the\nentire environment dynamics, but this wastes rich information on multiple\nsub-dynamics which can be modeled separately, allowing us to construct the\nworld model more accurately. In this paper, we propose the Environment Dynamics\nDecomposition (ED2), a novel world model construction framework that models the\nenvironment in a decomposing manner. ED2 contains two key components:\nsub-dynamics discovery (SD2) and dynamics decomposition prediction (D2P). SD2\ndiscovers the sub-dynamics in an environment automatically and then D2P\nconstructs the decomposed world model following the sub-dynamics. ED2 can be\neasily combined with existing MBRL algorithms and empirical results show that\nED2 significantly reduces the model error, increases the sample efficiency, and\nachieves higher asymptotic performance when combined with the state-of-the-art\nMBRL algorithms on various continuous control tasks. Our code is open source\nand available at https://github.com/ED2-source-code/ED2.\n", "link": "http://arxiv.org/abs/2112.02817v2", "date": "2024-02-15", "relevancy": 3.7514, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5306}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5228}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4965}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ED2%3A%20Environment%20Dynamics%20Decomposition%20World%20Models%20for%20Continuous%0A%20%20Control&entry.906535625=Jianye%20Hao%20and%20Yifu%20Yuan%20and%20Cong%20Wang%20and%20Zhen%20Wang&entry.1292438233=%20%20Model-based%20reinforcement%20learning%20%28MBRL%29%20achieves%20significant%20sample%0Aefficiency%20in%20practice%20in%20comparison%20to%20model-free%20RL%2C%20but%20its%20performance%20is%0Aoften%20limited%20by%20the%20existence%20of%20model%20prediction%20error.%20To%20reduce%20the%20model%0Aerror%2C%20standard%20MBRL%20approaches%20train%20a%20single%20well-designed%20network%20to%20fit%20the%0Aentire%20environment%20dynamics%2C%20but%20this%20wastes%20rich%20information%20on%20multiple%0Asub-dynamics%20which%20can%20be%20modeled%20separately%2C%20allowing%20us%20to%20construct%20the%0Aworld%20model%20more%20accurately.%20In%20this%20paper%2C%20we%20propose%20the%20Environment%20Dynamics%0ADecomposition%20%28ED2%29%2C%20a%20novel%20world%20model%20construction%20framework%20that%20models%20the%0Aenvironment%20in%20a%20decomposing%20manner.%20ED2%20contains%20two%20key%20components%3A%0Asub-dynamics%20discovery%20%28SD2%29%20and%20dynamics%20decomposition%20prediction%20%28D2P%29.%20SD2%0Adiscovers%20the%20sub-dynamics%20in%20an%20environment%20automatically%20and%20then%20D2P%0Aconstructs%20the%20decomposed%20world%20model%20following%20the%20sub-dynamics.%20ED2%20can%20be%0Aeasily%20combined%20with%20existing%20MBRL%20algorithms%20and%20empirical%20results%20show%20that%0AED2%20significantly%20reduces%20the%20model%20error%2C%20increases%20the%20sample%20efficiency%2C%20and%0Aachieves%20higher%20asymptotic%20performance%20when%20combined%20with%20the%20state-of-the-art%0AMBRL%20algorithms%20on%20various%20continuous%20control%20tasks.%20Our%20code%20is%20open%20source%0Aand%20available%20at%20https%3A//github.com/ED2-source-code/ED2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2112.02817v2&entry.124074799=Read"},
{"title": "Towards better Human-Agent Alignment: Assessing Task Utility in\n  LLM-Powered Applications", "author": "Negar Arabzadeh and Julia Kiseleva and Qingyun Wu and Chi Wang and Ahmed Awadallah and Victor Dibia and Adam Fourney and Charles Clarke", "abstract": "  The rapid development in the field of Large Language Models (LLMs) has led to\na surge in applications that facilitate collaboration among multiple agents to\nassist humans in their daily tasks. However, a significant gap remains in\nassessing whether LLM-powered applications genuinely enhance user experience\nand task execution efficiency. This highlights the pressing need for methods to\nverify utility of LLM-powered applications, particularly by ensuring alignment\nbetween the application's functionality and end-user needs. We introduce\nAgentEval provides an implementation for the math problems}, a novel framework\ndesigned to simplify the utility verification process by automatically\nproposing a set of criteria tailored to the unique purpose of any given\napplication. This allows for a comprehensive assessment, quantifying the\nutility of an application against the suggested criteria. We present a\ncomprehensive analysis of the robustness of quantifier's work.\n", "link": "http://arxiv.org/abs/2402.09015v2", "date": "2024-02-15", "relevancy": 3.6969, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5165}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.502}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4529}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20better%20Human-Agent%20Alignment%3A%20Assessing%20Task%20Utility%20in%0A%20%20LLM-Powered%20Applications&entry.906535625=Negar%20Arabzadeh%20and%20Julia%20Kiseleva%20and%20Qingyun%20Wu%20and%20Chi%20Wang%20and%20Ahmed%20Awadallah%20and%20Victor%20Dibia%20and%20Adam%20Fourney%20and%20Charles%20Clarke&entry.1292438233=%20%20The%20rapid%20development%20in%20the%20field%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%0Aa%20surge%20in%20applications%20that%20facilitate%20collaboration%20among%20multiple%20agents%20to%0Aassist%20humans%20in%20their%20daily%20tasks.%20However%2C%20a%20significant%20gap%20remains%20in%0Aassessing%20whether%20LLM-powered%20applications%20genuinely%20enhance%20user%20experience%0Aand%20task%20execution%20efficiency.%20This%20highlights%20the%20pressing%20need%20for%20methods%20to%0Averify%20utility%20of%20LLM-powered%20applications%2C%20particularly%20by%20ensuring%20alignment%0Abetween%20the%20application%27s%20functionality%20and%20end-user%20needs.%20We%20introduce%0AAgentEval%20provides%20an%20implementation%20for%20the%20math%20problems%7D%2C%20a%20novel%20framework%0Adesigned%20to%20simplify%20the%20utility%20verification%20process%20by%20automatically%0Aproposing%20a%20set%20of%20criteria%20tailored%20to%20the%20unique%20purpose%20of%20any%20given%0Aapplication.%20This%20allows%20for%20a%20comprehensive%20assessment%2C%20quantifying%20the%0Autility%20of%20an%20application%20against%20the%20suggested%20criteria.%20We%20present%20a%0Acomprehensive%20analysis%20of%20the%20robustness%20of%20quantifier%27s%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09015v2&entry.124074799=Read"},
{"title": "How to Train Data-Efficient LLMs", "author": "Noveen Sachdeva and Benjamin Coleman and Wang-Cheng Kang and Jianmo Ni and Lichan Hong and Ed H. Chi and James Caverlee and Julian McAuley and Derek Zhiyuan Cheng", "abstract": "  The training of large language models (LLMs) is expensive. In this paper, we\nstudy data-efficient approaches for pre-training LLMs, i.e., techniques that\naim to optimize the Pareto frontier of model quality and training resource/data\nconsumption. We seek to understand the tradeoffs associated with data selection\nroutines based on (i) expensive-to-compute data-quality estimates, and (ii)\nmaximization of coverage and diversity-based measures in the feature space. Our\nfirst technique, Ask-LLM, leverages the zero-shot reasoning capabilities of\ninstruction-tuned LLMs to directly assess the quality of a training example. To\ntarget coverage, we propose Density sampling, which models the data\ndistribution to select a diverse sample. In our comparison of 19 samplers,\ninvolving hundreds of evaluation tasks and pre-training runs, we find that\nAsk-LLM and Density are the best methods in their respective categories.\nCoverage sampling can recover the performance of the full data, while models\ntrained on Ask-LLM data consistently outperform full-data training -- even when\nwe reject 90% of the original dataset, while converging up to 70% faster.\n", "link": "http://arxiv.org/abs/2402.09668v1", "date": "2024-02-15", "relevancy": 2.3559, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5238}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5062}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4791}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Train%20Data-Efficient%20LLMs&entry.906535625=Noveen%20Sachdeva%20and%20Benjamin%20Coleman%20and%20Wang-Cheng%20Kang%20and%20Jianmo%20Ni%20and%20Lichan%20Hong%20and%20Ed%20H.%20Chi%20and%20James%20Caverlee%20and%20Julian%20McAuley%20and%20Derek%20Zhiyuan%20Cheng&entry.1292438233=%20%20The%20training%20of%20large%20language%20models%20%28LLMs%29%20is%20expensive.%20In%20this%20paper%2C%20we%0Astudy%20data-efficient%20approaches%20for%20pre-training%20LLMs%2C%20i.e.%2C%20techniques%20that%0Aaim%20to%20optimize%20the%20Pareto%20frontier%20of%20model%20quality%20and%20training%20resource/data%0Aconsumption.%20We%20seek%20to%20understand%20the%20tradeoffs%20associated%20with%20data%20selection%0Aroutines%20based%20on%20%28i%29%20expensive-to-compute%20data-quality%20estimates%2C%20and%20%28ii%29%0Amaximization%20of%20coverage%20and%20diversity-based%20measures%20in%20the%20feature%20space.%20Our%0Afirst%20technique%2C%20Ask-LLM%2C%20leverages%20the%20zero-shot%20reasoning%20capabilities%20of%0Ainstruction-tuned%20LLMs%20to%20directly%20assess%20the%20quality%20of%20a%20training%20example.%20To%0Atarget%20coverage%2C%20we%20propose%20Density%20sampling%2C%20which%20models%20the%20data%0Adistribution%20to%20select%20a%20diverse%20sample.%20In%20our%20comparison%20of%2019%20samplers%2C%0Ainvolving%20hundreds%20of%20evaluation%20tasks%20and%20pre-training%20runs%2C%20we%20find%20that%0AAsk-LLM%20and%20Density%20are%20the%20best%20methods%20in%20their%20respective%20categories.%0ACoverage%20sampling%20can%20recover%20the%20performance%20of%20the%20full%20data%2C%20while%20models%0Atrained%20on%20Ask-LLM%20data%20consistently%20outperform%20full-data%20training%20--%20even%20when%0Awe%20reject%2090%25%20of%20the%20original%20dataset%2C%20while%20converging%20up%20to%2070%25%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09668v1&entry.124074799=Read"},
{"title": "On-Demand Myoelectric Control Using Wake Gestures to Eliminate False\n  Activations During Activities of Daily Living", "author": "Ethan Eddy and Evan Campbell and Scott Bateman and Erik Scheme", "abstract": "  While myoelectric control has recently become a focus of increased research\nas a possible flexible hands-free input modality, current control approaches\nare prone to inadvertent false activations in real-world conditions. In this\nwork, a novel myoelectric control paradigm -- on-demand myoelectric control --\nis proposed, designed, and evaluated, to reduce the number of unrelated muscle\nmovements that are incorrectly interpreted as input gestures . By leveraging\nthe concept of wake gestures, users were able to switch between a dedicated\ncontrol mode and a sleep mode, effectively eliminating inadvertent activations\nduring activities of daily living (ADLs). The feasibility of wake gestures was\ndemonstrated in this work through two online ubiquitous EMG control tasks with\nvarying difficulty levels; dismissing an alarm and controlling a robot. The\nproposed control scheme was able to appropriately ignore almost all\nnon-targeted muscular inputs during ADLs (>99.9%) while maintaining sufficient\nsensitivity for reliable mode switching during intentional wake gesture\nelicitation. These results highlight the potential of wake gestures as a\ncritical step towards enabling ubiquitous myoelectric control-based on-demand\ninput for a wide range of applications.\n", "link": "http://arxiv.org/abs/2402.10050v1", "date": "2024-02-15", "relevancy": 2.3148, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5604}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.489}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.467}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-Demand%20Myoelectric%20Control%20Using%20Wake%20Gestures%20to%20Eliminate%20False%0A%20%20Activations%20During%20Activities%20of%20Daily%20Living&entry.906535625=Ethan%20Eddy%20and%20Evan%20Campbell%20and%20Scott%20Bateman%20and%20Erik%20Scheme&entry.1292438233=%20%20While%20myoelectric%20control%20has%20recently%20become%20a%20focus%20of%20increased%20research%0Aas%20a%20possible%20flexible%20hands-free%20input%20modality%2C%20current%20control%20approaches%0Aare%20prone%20to%20inadvertent%20false%20activations%20in%20real-world%20conditions.%20In%20this%0Awork%2C%20a%20novel%20myoelectric%20control%20paradigm%20--%20on-demand%20myoelectric%20control%20--%0Ais%20proposed%2C%20designed%2C%20and%20evaluated%2C%20to%20reduce%20the%20number%20of%20unrelated%20muscle%0Amovements%20that%20are%20incorrectly%20interpreted%20as%20input%20gestures%20.%20By%20leveraging%0Athe%20concept%20of%20wake%20gestures%2C%20users%20were%20able%20to%20switch%20between%20a%20dedicated%0Acontrol%20mode%20and%20a%20sleep%20mode%2C%20effectively%20eliminating%20inadvertent%20activations%0Aduring%20activities%20of%20daily%20living%20%28ADLs%29.%20The%20feasibility%20of%20wake%20gestures%20was%0Ademonstrated%20in%20this%20work%20through%20two%20online%20ubiquitous%20EMG%20control%20tasks%20with%0Avarying%20difficulty%20levels%3B%20dismissing%20an%20alarm%20and%20controlling%20a%20robot.%20The%0Aproposed%20control%20scheme%20was%20able%20to%20appropriately%20ignore%20almost%20all%0Anon-targeted%20muscular%20inputs%20during%20ADLs%20%28%3E99.9%25%29%20while%20maintaining%20sufficient%0Asensitivity%20for%20reliable%20mode%20switching%20during%20intentional%20wake%20gesture%0Aelicitation.%20These%20results%20highlight%20the%20potential%20of%20wake%20gestures%20as%20a%0Acritical%20step%20towards%20enabling%20ubiquitous%20myoelectric%20control-based%20on-demand%0Ainput%20for%20a%20wide%20range%20of%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10050v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


