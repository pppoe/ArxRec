<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251021.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Moving Light Adaptive Colonoscopy Reconstruction via\n  Illumination-Attenuation-Aware 3D Gaussian Splatting", "author": "Hao Wang and Ying Zhou and Haoyu Zhao and Rui Wang and Qiang Hu and Xing Zhang and Qiang Li and Zhiwei Wang", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time\nview synthesis in colonoscopy, enabling critical applications such as virtual\ncolonoscopy and lesion tracking. However, the vanilla 3DGS assumes static\nillumination and that observed appearance depends solely on viewing angle,\nwhich causes incompatibility with the photometric variations in colonoscopic\nscenes induced by dynamic light source/camera. This mismatch forces most 3DGS\nmethods to introduce structure-violating vaporous Gaussian blobs between the\ncamera and tissues to compensate for illumination attenuation, ultimately\ndegrading the quality of 3D reconstructions. Previous works only consider the\nillumination attenuation caused by light distance, ignoring the physical\ncharacters of light source and camera. In this paper, we propose ColIAGS, an\nimproved 3DGS framework tailored for colonoscopy. To mimic realistic appearance\nunder varying illumination, we introduce an Improved Appearance Modeling with\ntwo types of illumination attenuation factors, which enables Gaussians to adapt\nto photometric variations while preserving geometry accuracy. To ensure the\ngeometry approximation condition of appearance modeling, we propose an Improved\nGeometry Modeling using high-dimensional view embedding to enhance Gaussian\ngeometry attribute prediction. Furthermore, another cosine embedding input is\nleveraged to generate illumination attenuation solutions in an implicit manner.\nComprehensive experimental results on standard benchmarks demonstrate that our\nproposed ColIAGS achieves the dual capabilities of novel view synthesis and\naccurate geometric reconstruction. It notably outperforms other\nstate-of-the-art methods by achieving superior rendering fidelity while\nsignificantly reducing Depth MSE. Code will be available.\n", "link": "http://arxiv.org/abs/2510.18739v1", "date": "2025-10-21", "relevancy": 3.2395, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6757}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6453}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moving%20Light%20Adaptive%20Colonoscopy%20Reconstruction%20via%0A%20%20Illumination-Attenuation-Aware%203D%20Gaussian%20Splatting&body=Title%3A%20Moving%20Light%20Adaptive%20Colonoscopy%20Reconstruction%20via%0A%20%20Illumination-Attenuation-Aware%203D%20Gaussian%20Splatting%0AAuthor%3A%20Hao%20Wang%20and%20Ying%20Zhou%20and%20Haoyu%20Zhao%20and%20Rui%20Wang%20and%20Qiang%20Hu%20and%20Xing%20Zhang%20and%20Qiang%20Li%20and%20Zhiwei%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20pivotal%20technique%20for%20real-time%0Aview%20synthesis%20in%20colonoscopy%2C%20enabling%20critical%20applications%20such%20as%20virtual%0Acolonoscopy%20and%20lesion%20tracking.%20However%2C%20the%20vanilla%203DGS%20assumes%20static%0Aillumination%20and%20that%20observed%20appearance%20depends%20solely%20on%20viewing%20angle%2C%0Awhich%20causes%20incompatibility%20with%20the%20photometric%20variations%20in%20colonoscopic%0Ascenes%20induced%20by%20dynamic%20light%20source/camera.%20This%20mismatch%20forces%20most%203DGS%0Amethods%20to%20introduce%20structure-violating%20vaporous%20Gaussian%20blobs%20between%20the%0Acamera%20and%20tissues%20to%20compensate%20for%20illumination%20attenuation%2C%20ultimately%0Adegrading%20the%20quality%20of%203D%20reconstructions.%20Previous%20works%20only%20consider%20the%0Aillumination%20attenuation%20caused%20by%20light%20distance%2C%20ignoring%20the%20physical%0Acharacters%20of%20light%20source%20and%20camera.%20In%20this%20paper%2C%20we%20propose%20ColIAGS%2C%20an%0Aimproved%203DGS%20framework%20tailored%20for%20colonoscopy.%20To%20mimic%20realistic%20appearance%0Aunder%20varying%20illumination%2C%20we%20introduce%20an%20Improved%20Appearance%20Modeling%20with%0Atwo%20types%20of%20illumination%20attenuation%20factors%2C%20which%20enables%20Gaussians%20to%20adapt%0Ato%20photometric%20variations%20while%20preserving%20geometry%20accuracy.%20To%20ensure%20the%0Ageometry%20approximation%20condition%20of%20appearance%20modeling%2C%20we%20propose%20an%20Improved%0AGeometry%20Modeling%20using%20high-dimensional%20view%20embedding%20to%20enhance%20Gaussian%0Ageometry%20attribute%20prediction.%20Furthermore%2C%20another%20cosine%20embedding%20input%20is%0Aleveraged%20to%20generate%20illumination%20attenuation%20solutions%20in%20an%20implicit%20manner.%0AComprehensive%20experimental%20results%20on%20standard%20benchmarks%20demonstrate%20that%20our%0Aproposed%20ColIAGS%20achieves%20the%20dual%20capabilities%20of%20novel%20view%20synthesis%20and%0Aaccurate%20geometric%20reconstruction.%20It%20notably%20outperforms%20other%0Astate-of-the-art%20methods%20by%20achieving%20superior%20rendering%20fidelity%20while%0Asignificantly%20reducing%20Depth%20MSE.%20Code%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoving%2520Light%2520Adaptive%2520Colonoscopy%2520Reconstruction%2520via%250A%2520%2520Illumination-Attenuation-Aware%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DHao%2520Wang%2520and%2520Ying%2520Zhou%2520and%2520Haoyu%2520Zhao%2520and%2520Rui%2520Wang%2520and%2520Qiang%2520Hu%2520and%2520Xing%2520Zhang%2520and%2520Qiang%2520Li%2520and%2520Zhiwei%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520pivotal%2520technique%2520for%2520real-time%250Aview%2520synthesis%2520in%2520colonoscopy%252C%2520enabling%2520critical%2520applications%2520such%2520as%2520virtual%250Acolonoscopy%2520and%2520lesion%2520tracking.%2520However%252C%2520the%2520vanilla%25203DGS%2520assumes%2520static%250Aillumination%2520and%2520that%2520observed%2520appearance%2520depends%2520solely%2520on%2520viewing%2520angle%252C%250Awhich%2520causes%2520incompatibility%2520with%2520the%2520photometric%2520variations%2520in%2520colonoscopic%250Ascenes%2520induced%2520by%2520dynamic%2520light%2520source/camera.%2520This%2520mismatch%2520forces%2520most%25203DGS%250Amethods%2520to%2520introduce%2520structure-violating%2520vaporous%2520Gaussian%2520blobs%2520between%2520the%250Acamera%2520and%2520tissues%2520to%2520compensate%2520for%2520illumination%2520attenuation%252C%2520ultimately%250Adegrading%2520the%2520quality%2520of%25203D%2520reconstructions.%2520Previous%2520works%2520only%2520consider%2520the%250Aillumination%2520attenuation%2520caused%2520by%2520light%2520distance%252C%2520ignoring%2520the%2520physical%250Acharacters%2520of%2520light%2520source%2520and%2520camera.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ColIAGS%252C%2520an%250Aimproved%25203DGS%2520framework%2520tailored%2520for%2520colonoscopy.%2520To%2520mimic%2520realistic%2520appearance%250Aunder%2520varying%2520illumination%252C%2520we%2520introduce%2520an%2520Improved%2520Appearance%2520Modeling%2520with%250Atwo%2520types%2520of%2520illumination%2520attenuation%2520factors%252C%2520which%2520enables%2520Gaussians%2520to%2520adapt%250Ato%2520photometric%2520variations%2520while%2520preserving%2520geometry%2520accuracy.%2520To%2520ensure%2520the%250Ageometry%2520approximation%2520condition%2520of%2520appearance%2520modeling%252C%2520we%2520propose%2520an%2520Improved%250AGeometry%2520Modeling%2520using%2520high-dimensional%2520view%2520embedding%2520to%2520enhance%2520Gaussian%250Ageometry%2520attribute%2520prediction.%2520Furthermore%252C%2520another%2520cosine%2520embedding%2520input%2520is%250Aleveraged%2520to%2520generate%2520illumination%2520attenuation%2520solutions%2520in%2520an%2520implicit%2520manner.%250AComprehensive%2520experimental%2520results%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520our%250Aproposed%2520ColIAGS%2520achieves%2520the%2520dual%2520capabilities%2520of%2520novel%2520view%2520synthesis%2520and%250Aaccurate%2520geometric%2520reconstruction.%2520It%2520notably%2520outperforms%2520other%250Astate-of-the-art%2520methods%2520by%2520achieving%2520superior%2520rendering%2520fidelity%2520while%250Asignificantly%2520reducing%2520Depth%2520MSE.%2520Code%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moving%20Light%20Adaptive%20Colonoscopy%20Reconstruction%20via%0A%20%20Illumination-Attenuation-Aware%203D%20Gaussian%20Splatting&entry.906535625=Hao%20Wang%20and%20Ying%20Zhou%20and%20Haoyu%20Zhao%20and%20Rui%20Wang%20and%20Qiang%20Hu%20and%20Xing%20Zhang%20and%20Qiang%20Li%20and%20Zhiwei%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20pivotal%20technique%20for%20real-time%0Aview%20synthesis%20in%20colonoscopy%2C%20enabling%20critical%20applications%20such%20as%20virtual%0Acolonoscopy%20and%20lesion%20tracking.%20However%2C%20the%20vanilla%203DGS%20assumes%20static%0Aillumination%20and%20that%20observed%20appearance%20depends%20solely%20on%20viewing%20angle%2C%0Awhich%20causes%20incompatibility%20with%20the%20photometric%20variations%20in%20colonoscopic%0Ascenes%20induced%20by%20dynamic%20light%20source/camera.%20This%20mismatch%20forces%20most%203DGS%0Amethods%20to%20introduce%20structure-violating%20vaporous%20Gaussian%20blobs%20between%20the%0Acamera%20and%20tissues%20to%20compensate%20for%20illumination%20attenuation%2C%20ultimately%0Adegrading%20the%20quality%20of%203D%20reconstructions.%20Previous%20works%20only%20consider%20the%0Aillumination%20attenuation%20caused%20by%20light%20distance%2C%20ignoring%20the%20physical%0Acharacters%20of%20light%20source%20and%20camera.%20In%20this%20paper%2C%20we%20propose%20ColIAGS%2C%20an%0Aimproved%203DGS%20framework%20tailored%20for%20colonoscopy.%20To%20mimic%20realistic%20appearance%0Aunder%20varying%20illumination%2C%20we%20introduce%20an%20Improved%20Appearance%20Modeling%20with%0Atwo%20types%20of%20illumination%20attenuation%20factors%2C%20which%20enables%20Gaussians%20to%20adapt%0Ato%20photometric%20variations%20while%20preserving%20geometry%20accuracy.%20To%20ensure%20the%0Ageometry%20approximation%20condition%20of%20appearance%20modeling%2C%20we%20propose%20an%20Improved%0AGeometry%20Modeling%20using%20high-dimensional%20view%20embedding%20to%20enhance%20Gaussian%0Ageometry%20attribute%20prediction.%20Furthermore%2C%20another%20cosine%20embedding%20input%20is%0Aleveraged%20to%20generate%20illumination%20attenuation%20solutions%20in%20an%20implicit%20manner.%0AComprehensive%20experimental%20results%20on%20standard%20benchmarks%20demonstrate%20that%20our%0Aproposed%20ColIAGS%20achieves%20the%20dual%20capabilities%20of%20novel%20view%20synthesis%20and%0Aaccurate%20geometric%20reconstruction.%20It%20notably%20outperforms%20other%0Astate-of-the-art%20methods%20by%20achieving%20superior%20rendering%20fidelity%20while%0Asignificantly%20reducing%20Depth%20MSE.%20Code%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18739v1&entry.124074799=Read"},
{"title": "PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward\n  Planar Splatting", "author": "Changkun Liu and Bin Tan and Zeran Ke and Shangzhan Zhang and Jiachen Liu and Ming Qian and Nan Xue and Yujun Shen and Tristan Braud", "abstract": "  This paper addresses metric 3D reconstruction of indoor scenes by exploiting\ntheir inherent geometric regularities with compact representations. Using\nplanar 3D primitives - a well-suited representation for man-made environments -\nwe introduce PLANA3R, a pose-free framework for metric Planar 3D Reconstruction\nfrom unposed two-view images. Our approach employs Vision Transformers to\nextract a set of sparse planar primitives, estimate relative camera poses, and\nsupervise geometry learning via planar splatting, where gradients are\npropagated through high-resolution rendered depth and normal maps of\nprimitives. Unlike prior feedforward methods that require 3D plane annotations\nduring training, PLANA3R learns planar 3D structures without explicit plane\nsupervision, enabling scalable training on large-scale stereo datasets using\nonly depth and normal annotations. We validate PLANA3R on multiple indoor-scene\ndatasets with metric supervision and demonstrate strong generalization to\nout-of-domain indoor environments across diverse tasks under metric evaluation\nprotocols, including 3D surface reconstruction, depth estimation, and relative\npose estimation. Furthermore, by formulating with planar 3D representation, our\nmethod emerges with the ability for accurate plane segmentation. The project\npage is available at https://lck666666.github.io/plana3r\n", "link": "http://arxiv.org/abs/2510.18714v1", "date": "2025-10-21", "relevancy": 3.1761, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6527}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6436}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLANA3R%3A%20Zero-shot%20Metric%20Planar%203D%20Reconstruction%20via%20Feed-Forward%0A%20%20Planar%20Splatting&body=Title%3A%20PLANA3R%3A%20Zero-shot%20Metric%20Planar%203D%20Reconstruction%20via%20Feed-Forward%0A%20%20Planar%20Splatting%0AAuthor%3A%20Changkun%20Liu%20and%20Bin%20Tan%20and%20Zeran%20Ke%20and%20Shangzhan%20Zhang%20and%20Jiachen%20Liu%20and%20Ming%20Qian%20and%20Nan%20Xue%20and%20Yujun%20Shen%20and%20Tristan%20Braud%0AAbstract%3A%20%20%20This%20paper%20addresses%20metric%203D%20reconstruction%20of%20indoor%20scenes%20by%20exploiting%0Atheir%20inherent%20geometric%20regularities%20with%20compact%20representations.%20Using%0Aplanar%203D%20primitives%20-%20a%20well-suited%20representation%20for%20man-made%20environments%20-%0Awe%20introduce%20PLANA3R%2C%20a%20pose-free%20framework%20for%20metric%20Planar%203D%20Reconstruction%0Afrom%20unposed%20two-view%20images.%20Our%20approach%20employs%20Vision%20Transformers%20to%0Aextract%20a%20set%20of%20sparse%20planar%20primitives%2C%20estimate%20relative%20camera%20poses%2C%20and%0Asupervise%20geometry%20learning%20via%20planar%20splatting%2C%20where%20gradients%20are%0Apropagated%20through%20high-resolution%20rendered%20depth%20and%20normal%20maps%20of%0Aprimitives.%20Unlike%20prior%20feedforward%20methods%20that%20require%203D%20plane%20annotations%0Aduring%20training%2C%20PLANA3R%20learns%20planar%203D%20structures%20without%20explicit%20plane%0Asupervision%2C%20enabling%20scalable%20training%20on%20large-scale%20stereo%20datasets%20using%0Aonly%20depth%20and%20normal%20annotations.%20We%20validate%20PLANA3R%20on%20multiple%20indoor-scene%0Adatasets%20with%20metric%20supervision%20and%20demonstrate%20strong%20generalization%20to%0Aout-of-domain%20indoor%20environments%20across%20diverse%20tasks%20under%20metric%20evaluation%0Aprotocols%2C%20including%203D%20surface%20reconstruction%2C%20depth%20estimation%2C%20and%20relative%0Apose%20estimation.%20Furthermore%2C%20by%20formulating%20with%20planar%203D%20representation%2C%20our%0Amethod%20emerges%20with%20the%20ability%20for%20accurate%20plane%20segmentation.%20The%20project%0Apage%20is%20available%20at%20https%3A//lck666666.github.io/plana3r%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLANA3R%253A%2520Zero-shot%2520Metric%2520Planar%25203D%2520Reconstruction%2520via%2520Feed-Forward%250A%2520%2520Planar%2520Splatting%26entry.906535625%3DChangkun%2520Liu%2520and%2520Bin%2520Tan%2520and%2520Zeran%2520Ke%2520and%2520Shangzhan%2520Zhang%2520and%2520Jiachen%2520Liu%2520and%2520Ming%2520Qian%2520and%2520Nan%2520Xue%2520and%2520Yujun%2520Shen%2520and%2520Tristan%2520Braud%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520metric%25203D%2520reconstruction%2520of%2520indoor%2520scenes%2520by%2520exploiting%250Atheir%2520inherent%2520geometric%2520regularities%2520with%2520compact%2520representations.%2520Using%250Aplanar%25203D%2520primitives%2520-%2520a%2520well-suited%2520representation%2520for%2520man-made%2520environments%2520-%250Awe%2520introduce%2520PLANA3R%252C%2520a%2520pose-free%2520framework%2520for%2520metric%2520Planar%25203D%2520Reconstruction%250Afrom%2520unposed%2520two-view%2520images.%2520Our%2520approach%2520employs%2520Vision%2520Transformers%2520to%250Aextract%2520a%2520set%2520of%2520sparse%2520planar%2520primitives%252C%2520estimate%2520relative%2520camera%2520poses%252C%2520and%250Asupervise%2520geometry%2520learning%2520via%2520planar%2520splatting%252C%2520where%2520gradients%2520are%250Apropagated%2520through%2520high-resolution%2520rendered%2520depth%2520and%2520normal%2520maps%2520of%250Aprimitives.%2520Unlike%2520prior%2520feedforward%2520methods%2520that%2520require%25203D%2520plane%2520annotations%250Aduring%2520training%252C%2520PLANA3R%2520learns%2520planar%25203D%2520structures%2520without%2520explicit%2520plane%250Asupervision%252C%2520enabling%2520scalable%2520training%2520on%2520large-scale%2520stereo%2520datasets%2520using%250Aonly%2520depth%2520and%2520normal%2520annotations.%2520We%2520validate%2520PLANA3R%2520on%2520multiple%2520indoor-scene%250Adatasets%2520with%2520metric%2520supervision%2520and%2520demonstrate%2520strong%2520generalization%2520to%250Aout-of-domain%2520indoor%2520environments%2520across%2520diverse%2520tasks%2520under%2520metric%2520evaluation%250Aprotocols%252C%2520including%25203D%2520surface%2520reconstruction%252C%2520depth%2520estimation%252C%2520and%2520relative%250Apose%2520estimation.%2520Furthermore%252C%2520by%2520formulating%2520with%2520planar%25203D%2520representation%252C%2520our%250Amethod%2520emerges%2520with%2520the%2520ability%2520for%2520accurate%2520plane%2520segmentation.%2520The%2520project%250Apage%2520is%2520available%2520at%2520https%253A//lck666666.github.io/plana3r%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLANA3R%3A%20Zero-shot%20Metric%20Planar%203D%20Reconstruction%20via%20Feed-Forward%0A%20%20Planar%20Splatting&entry.906535625=Changkun%20Liu%20and%20Bin%20Tan%20and%20Zeran%20Ke%20and%20Shangzhan%20Zhang%20and%20Jiachen%20Liu%20and%20Ming%20Qian%20and%20Nan%20Xue%20and%20Yujun%20Shen%20and%20Tristan%20Braud&entry.1292438233=%20%20This%20paper%20addresses%20metric%203D%20reconstruction%20of%20indoor%20scenes%20by%20exploiting%0Atheir%20inherent%20geometric%20regularities%20with%20compact%20representations.%20Using%0Aplanar%203D%20primitives%20-%20a%20well-suited%20representation%20for%20man-made%20environments%20-%0Awe%20introduce%20PLANA3R%2C%20a%20pose-free%20framework%20for%20metric%20Planar%203D%20Reconstruction%0Afrom%20unposed%20two-view%20images.%20Our%20approach%20employs%20Vision%20Transformers%20to%0Aextract%20a%20set%20of%20sparse%20planar%20primitives%2C%20estimate%20relative%20camera%20poses%2C%20and%0Asupervise%20geometry%20learning%20via%20planar%20splatting%2C%20where%20gradients%20are%0Apropagated%20through%20high-resolution%20rendered%20depth%20and%20normal%20maps%20of%0Aprimitives.%20Unlike%20prior%20feedforward%20methods%20that%20require%203D%20plane%20annotations%0Aduring%20training%2C%20PLANA3R%20learns%20planar%203D%20structures%20without%20explicit%20plane%0Asupervision%2C%20enabling%20scalable%20training%20on%20large-scale%20stereo%20datasets%20using%0Aonly%20depth%20and%20normal%20annotations.%20We%20validate%20PLANA3R%20on%20multiple%20indoor-scene%0Adatasets%20with%20metric%20supervision%20and%20demonstrate%20strong%20generalization%20to%0Aout-of-domain%20indoor%20environments%20across%20diverse%20tasks%20under%20metric%20evaluation%0Aprotocols%2C%20including%203D%20surface%20reconstruction%2C%20depth%20estimation%2C%20and%20relative%0Apose%20estimation.%20Furthermore%2C%20by%20formulating%20with%20planar%203D%20representation%2C%20our%0Amethod%20emerges%20with%20the%20ability%20for%20accurate%20plane%20segmentation.%20The%20project%0Apage%20is%20available%20at%20https%3A//lck666666.github.io/plana3r%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18714v1&entry.124074799=Read"},
{"title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention", "author": "Teng Hu and Jiangning Zhang and Zihan Su and Ran Yi", "abstract": "  Recent advances in video generation have made it possible to produce visually\ncompelling videos, with wide-ranging applications in content creation,\nentertainment, and virtual reality. However, most existing diffusion\ntransformer based video generation models are limited to low-resolution outputs\n(<=720P) due to the quadratic computational complexity of the attention\nmechanism with respect to the output width and height. This computational\nbottleneck makes native high-resolution video generation (1080P/2K/4K)\nimpractical for both training and inference. To address this challenge, we\npresent UltraGen, a novel video generation framework that enables i) efficient\nand ii) end-to-end native high-resolution video synthesis. Specifically,\nUltraGen features a hierarchical dual-branch attention architecture based on\nglobal-local attention decomposition, which decouples full attention into a\nlocal attention branch for high-fidelity regional content and a global\nattention branch for overall semantic consistency. We further propose a\nspatially compressed global modeling strategy to efficiently learn global\ndependencies, and a hierarchical cross-window local attention mechanism to\nreduce computational costs while enhancing information flow across different\nlocal windows. Extensive experiments demonstrate that UltraGen can effectively\nscale pre-trained low-resolution video models to 1080P and even 4K resolution\nfor the first time, outperforming existing state-of-the-art methods and\nsuper-resolution based two-stage pipelines in both qualitative and quantitative\nevaluations.\n", "link": "http://arxiv.org/abs/2510.18775v1", "date": "2025-10-21", "relevancy": 3.1604, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.653}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6345}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltraGen%3A%20High-Resolution%20Video%20Generation%20with%20Hierarchical%20Attention&body=Title%3A%20UltraGen%3A%20High-Resolution%20Video%20Generation%20with%20Hierarchical%20Attention%0AAuthor%3A%20Teng%20Hu%20and%20Jiangning%20Zhang%20and%20Zihan%20Su%20and%20Ran%20Yi%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20generation%20have%20made%20it%20possible%20to%20produce%20visually%0Acompelling%20videos%2C%20with%20wide-ranging%20applications%20in%20content%20creation%2C%0Aentertainment%2C%20and%20virtual%20reality.%20However%2C%20most%20existing%20diffusion%0Atransformer%20based%20video%20generation%20models%20are%20limited%20to%20low-resolution%20outputs%0A%28%3C%3D720P%29%20due%20to%20the%20quadratic%20computational%20complexity%20of%20the%20attention%0Amechanism%20with%20respect%20to%20the%20output%20width%20and%20height.%20This%20computational%0Abottleneck%20makes%20native%20high-resolution%20video%20generation%20%281080P/2K/4K%29%0Aimpractical%20for%20both%20training%20and%20inference.%20To%20address%20this%20challenge%2C%20we%0Apresent%20UltraGen%2C%20a%20novel%20video%20generation%20framework%20that%20enables%20i%29%20efficient%0Aand%20ii%29%20end-to-end%20native%20high-resolution%20video%20synthesis.%20Specifically%2C%0AUltraGen%20features%20a%20hierarchical%20dual-branch%20attention%20architecture%20based%20on%0Aglobal-local%20attention%20decomposition%2C%20which%20decouples%20full%20attention%20into%20a%0Alocal%20attention%20branch%20for%20high-fidelity%20regional%20content%20and%20a%20global%0Aattention%20branch%20for%20overall%20semantic%20consistency.%20We%20further%20propose%20a%0Aspatially%20compressed%20global%20modeling%20strategy%20to%20efficiently%20learn%20global%0Adependencies%2C%20and%20a%20hierarchical%20cross-window%20local%20attention%20mechanism%20to%0Areduce%20computational%20costs%20while%20enhancing%20information%20flow%20across%20different%0Alocal%20windows.%20Extensive%20experiments%20demonstrate%20that%20UltraGen%20can%20effectively%0Ascale%20pre-trained%20low-resolution%20video%20models%20to%201080P%20and%20even%204K%20resolution%0Afor%20the%20first%20time%2C%20outperforming%20existing%20state-of-the-art%20methods%20and%0Asuper-resolution%20based%20two-stage%20pipelines%20in%20both%20qualitative%20and%20quantitative%0Aevaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltraGen%253A%2520High-Resolution%2520Video%2520Generation%2520with%2520Hierarchical%2520Attention%26entry.906535625%3DTeng%2520Hu%2520and%2520Jiangning%2520Zhang%2520and%2520Zihan%2520Su%2520and%2520Ran%2520Yi%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520generation%2520have%2520made%2520it%2520possible%2520to%2520produce%2520visually%250Acompelling%2520videos%252C%2520with%2520wide-ranging%2520applications%2520in%2520content%2520creation%252C%250Aentertainment%252C%2520and%2520virtual%2520reality.%2520However%252C%2520most%2520existing%2520diffusion%250Atransformer%2520based%2520video%2520generation%2520models%2520are%2520limited%2520to%2520low-resolution%2520outputs%250A%2528%253C%253D720P%2529%2520due%2520to%2520the%2520quadratic%2520computational%2520complexity%2520of%2520the%2520attention%250Amechanism%2520with%2520respect%2520to%2520the%2520output%2520width%2520and%2520height.%2520This%2520computational%250Abottleneck%2520makes%2520native%2520high-resolution%2520video%2520generation%2520%25281080P/2K/4K%2529%250Aimpractical%2520for%2520both%2520training%2520and%2520inference.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apresent%2520UltraGen%252C%2520a%2520novel%2520video%2520generation%2520framework%2520that%2520enables%2520i%2529%2520efficient%250Aand%2520ii%2529%2520end-to-end%2520native%2520high-resolution%2520video%2520synthesis.%2520Specifically%252C%250AUltraGen%2520features%2520a%2520hierarchical%2520dual-branch%2520attention%2520architecture%2520based%2520on%250Aglobal-local%2520attention%2520decomposition%252C%2520which%2520decouples%2520full%2520attention%2520into%2520a%250Alocal%2520attention%2520branch%2520for%2520high-fidelity%2520regional%2520content%2520and%2520a%2520global%250Aattention%2520branch%2520for%2520overall%2520semantic%2520consistency.%2520We%2520further%2520propose%2520a%250Aspatially%2520compressed%2520global%2520modeling%2520strategy%2520to%2520efficiently%2520learn%2520global%250Adependencies%252C%2520and%2520a%2520hierarchical%2520cross-window%2520local%2520attention%2520mechanism%2520to%250Areduce%2520computational%2520costs%2520while%2520enhancing%2520information%2520flow%2520across%2520different%250Alocal%2520windows.%2520Extensive%2520experiments%2520demonstrate%2520that%2520UltraGen%2520can%2520effectively%250Ascale%2520pre-trained%2520low-resolution%2520video%2520models%2520to%25201080P%2520and%2520even%25204K%2520resolution%250Afor%2520the%2520first%2520time%252C%2520outperforming%2520existing%2520state-of-the-art%2520methods%2520and%250Asuper-resolution%2520based%2520two-stage%2520pipelines%2520in%2520both%2520qualitative%2520and%2520quantitative%250Aevaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltraGen%3A%20High-Resolution%20Video%20Generation%20with%20Hierarchical%20Attention&entry.906535625=Teng%20Hu%20and%20Jiangning%20Zhang%20and%20Zihan%20Su%20and%20Ran%20Yi&entry.1292438233=%20%20Recent%20advances%20in%20video%20generation%20have%20made%20it%20possible%20to%20produce%20visually%0Acompelling%20videos%2C%20with%20wide-ranging%20applications%20in%20content%20creation%2C%0Aentertainment%2C%20and%20virtual%20reality.%20However%2C%20most%20existing%20diffusion%0Atransformer%20based%20video%20generation%20models%20are%20limited%20to%20low-resolution%20outputs%0A%28%3C%3D720P%29%20due%20to%20the%20quadratic%20computational%20complexity%20of%20the%20attention%0Amechanism%20with%20respect%20to%20the%20output%20width%20and%20height.%20This%20computational%0Abottleneck%20makes%20native%20high-resolution%20video%20generation%20%281080P/2K/4K%29%0Aimpractical%20for%20both%20training%20and%20inference.%20To%20address%20this%20challenge%2C%20we%0Apresent%20UltraGen%2C%20a%20novel%20video%20generation%20framework%20that%20enables%20i%29%20efficient%0Aand%20ii%29%20end-to-end%20native%20high-resolution%20video%20synthesis.%20Specifically%2C%0AUltraGen%20features%20a%20hierarchical%20dual-branch%20attention%20architecture%20based%20on%0Aglobal-local%20attention%20decomposition%2C%20which%20decouples%20full%20attention%20into%20a%0Alocal%20attention%20branch%20for%20high-fidelity%20regional%20content%20and%20a%20global%0Aattention%20branch%20for%20overall%20semantic%20consistency.%20We%20further%20propose%20a%0Aspatially%20compressed%20global%20modeling%20strategy%20to%20efficiently%20learn%20global%0Adependencies%2C%20and%20a%20hierarchical%20cross-window%20local%20attention%20mechanism%20to%0Areduce%20computational%20costs%20while%20enhancing%20information%20flow%20across%20different%0Alocal%20windows.%20Extensive%20experiments%20demonstrate%20that%20UltraGen%20can%20effectively%0Ascale%20pre-trained%20low-resolution%20video%20models%20to%201080P%20and%20even%204K%20resolution%0Afor%20the%20first%20time%2C%20outperforming%20existing%20state-of-the-art%20methods%20and%0Asuper-resolution%20based%20two-stage%20pipelines%20in%20both%20qualitative%20and%20quantitative%0Aevaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18775v1&entry.124074799=Read"},
{"title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs", "author": "Haochen Wang and Yuhao Wang and Tao Zhang and Yikang Zhou and Yanwei Li and Jiacong Wang and Ye Tian and Jiahao Meng and Zilong Huang and Guangcan Mai and Anran Wang and Yunhai Tong and Zhuochen Wang and Xiangtai Li and Zhaoxiang Zhang", "abstract": "  While Multimodal Large Language Models (MLLMs) excel at holistic\nunderstanding, they struggle in capturing the dense world with complex scenes,\nrequiring fine-grained analysis of intricate details and object\ninter-relationships. Region-level MLLMs have been a promising step. However,\nprevious attempts are generally optimized to understand given regions in\nisolation, neglecting crucial global contexts. To address this, we introduce\nGrasp Any Region (GAR) for comprehen- sive region-level visual understanding.\nEmpowered by an effective RoI-aligned feature replay technique, GAR supports\n(1) precise perception by leveraging necessary global contexts, and (2)\nmodeling interactions between multiple prompts. Together, it then naturally\nachieves (3) advanced compositional reasoning to answer specific free-form\nquestions about any region, shifting the paradigm from passive description to\nactive dialogue. Moreover, we construct GAR-Bench, which not only provides a\nmore accurate evaluation of single-region comprehension, but also, more\nimportantly, measures interactions and complex reasoning across multiple\nregions. Extensive experiments have demonstrated that GAR-1B not only maintains\nthe state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5\non DLC-Bench, but also excels at modeling relationships between multiple\nprompts with advanced comprehension capabilities, even surpassing InternVL3-78B\non GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms\nin-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong\ncapabilities can be easily transferred to videos.\n", "link": "http://arxiv.org/abs/2510.18876v1", "date": "2025-10-21", "relevancy": 3.0592, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6281}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grasp%20Any%20Region%3A%20Towards%20Precise%2C%20Contextual%20Pixel%20Understanding%20for%0A%20%20Multimodal%20LLMs&body=Title%3A%20Grasp%20Any%20Region%3A%20Towards%20Precise%2C%20Contextual%20Pixel%20Understanding%20for%0A%20%20Multimodal%20LLMs%0AAuthor%3A%20Haochen%20Wang%20and%20Yuhao%20Wang%20and%20Tao%20Zhang%20and%20Yikang%20Zhou%20and%20Yanwei%20Li%20and%20Jiacong%20Wang%20and%20Ye%20Tian%20and%20Jiahao%20Meng%20and%20Zilong%20Huang%20and%20Guangcan%20Mai%20and%20Anran%20Wang%20and%20Yunhai%20Tong%20and%20Zhuochen%20Wang%20and%20Xiangtai%20Li%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20holistic%0Aunderstanding%2C%20they%20struggle%20in%20capturing%20the%20dense%20world%20with%20complex%20scenes%2C%0Arequiring%20fine-grained%20analysis%20of%20intricate%20details%20and%20object%0Ainter-relationships.%20Region-level%20MLLMs%20have%20been%20a%20promising%20step.%20However%2C%0Aprevious%20attempts%20are%20generally%20optimized%20to%20understand%20given%20regions%20in%0Aisolation%2C%20neglecting%20crucial%20global%20contexts.%20To%20address%20this%2C%20we%20introduce%0AGrasp%20Any%20Region%20%28GAR%29%20for%20comprehen-%20sive%20region-level%20visual%20understanding.%0AEmpowered%20by%20an%20effective%20RoI-aligned%20feature%20replay%20technique%2C%20GAR%20supports%0A%281%29%20precise%20perception%20by%20leveraging%20necessary%20global%20contexts%2C%20and%20%282%29%0Amodeling%20interactions%20between%20multiple%20prompts.%20Together%2C%20it%20then%20naturally%0Aachieves%20%283%29%20advanced%20compositional%20reasoning%20to%20answer%20specific%20free-form%0Aquestions%20about%20any%20region%2C%20shifting%20the%20paradigm%20from%20passive%20description%20to%0Aactive%20dialogue.%20Moreover%2C%20we%20construct%20GAR-Bench%2C%20which%20not%20only%20provides%20a%0Amore%20accurate%20evaluation%20of%20single-region%20comprehension%2C%20but%20also%2C%20more%0Aimportantly%2C%20measures%20interactions%20and%20complex%20reasoning%20across%20multiple%0Aregions.%20Extensive%20experiments%20have%20demonstrated%20that%20GAR-1B%20not%20only%20maintains%0Athe%20state-of-the-art%20captioning%20capabilities%2C%20e.g.%2C%20outperforming%20DAM-3B%20%2B4.5%0Aon%20DLC-Bench%2C%20but%20also%20excels%20at%20modeling%20relationships%20between%20multiple%0Aprompts%20with%20advanced%20comprehension%20capabilities%2C%20even%20surpassing%20InternVL3-78B%0Aon%20GAR-Bench-VQA.%20More%20importantly%2C%20our%20zero-shot%20GAR-8B%20even%20outperforms%0Ain-domain%20VideoRefer-7B%20on%20VideoRefer-BenchQ%2C%20indicating%20its%20strong%0Acapabilities%20can%20be%20easily%20transferred%20to%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrasp%2520Any%2520Region%253A%2520Towards%2520Precise%252C%2520Contextual%2520Pixel%2520Understanding%2520for%250A%2520%2520Multimodal%2520LLMs%26entry.906535625%3DHaochen%2520Wang%2520and%2520Yuhao%2520Wang%2520and%2520Tao%2520Zhang%2520and%2520Yikang%2520Zhou%2520and%2520Yanwei%2520Li%2520and%2520Jiacong%2520Wang%2520and%2520Ye%2520Tian%2520and%2520Jiahao%2520Meng%2520and%2520Zilong%2520Huang%2520and%2520Guangcan%2520Mai%2520and%2520Anran%2520Wang%2520and%2520Yunhai%2520Tong%2520and%2520Zhuochen%2520Wang%2520and%2520Xiangtai%2520Li%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520holistic%250Aunderstanding%252C%2520they%2520struggle%2520in%2520capturing%2520the%2520dense%2520world%2520with%2520complex%2520scenes%252C%250Arequiring%2520fine-grained%2520analysis%2520of%2520intricate%2520details%2520and%2520object%250Ainter-relationships.%2520Region-level%2520MLLMs%2520have%2520been%2520a%2520promising%2520step.%2520However%252C%250Aprevious%2520attempts%2520are%2520generally%2520optimized%2520to%2520understand%2520given%2520regions%2520in%250Aisolation%252C%2520neglecting%2520crucial%2520global%2520contexts.%2520To%2520address%2520this%252C%2520we%2520introduce%250AGrasp%2520Any%2520Region%2520%2528GAR%2529%2520for%2520comprehen-%2520sive%2520region-level%2520visual%2520understanding.%250AEmpowered%2520by%2520an%2520effective%2520RoI-aligned%2520feature%2520replay%2520technique%252C%2520GAR%2520supports%250A%25281%2529%2520precise%2520perception%2520by%2520leveraging%2520necessary%2520global%2520contexts%252C%2520and%2520%25282%2529%250Amodeling%2520interactions%2520between%2520multiple%2520prompts.%2520Together%252C%2520it%2520then%2520naturally%250Aachieves%2520%25283%2529%2520advanced%2520compositional%2520reasoning%2520to%2520answer%2520specific%2520free-form%250Aquestions%2520about%2520any%2520region%252C%2520shifting%2520the%2520paradigm%2520from%2520passive%2520description%2520to%250Aactive%2520dialogue.%2520Moreover%252C%2520we%2520construct%2520GAR-Bench%252C%2520which%2520not%2520only%2520provides%2520a%250Amore%2520accurate%2520evaluation%2520of%2520single-region%2520comprehension%252C%2520but%2520also%252C%2520more%250Aimportantly%252C%2520measures%2520interactions%2520and%2520complex%2520reasoning%2520across%2520multiple%250Aregions.%2520Extensive%2520experiments%2520have%2520demonstrated%2520that%2520GAR-1B%2520not%2520only%2520maintains%250Athe%2520state-of-the-art%2520captioning%2520capabilities%252C%2520e.g.%252C%2520outperforming%2520DAM-3B%2520%252B4.5%250Aon%2520DLC-Bench%252C%2520but%2520also%2520excels%2520at%2520modeling%2520relationships%2520between%2520multiple%250Aprompts%2520with%2520advanced%2520comprehension%2520capabilities%252C%2520even%2520surpassing%2520InternVL3-78B%250Aon%2520GAR-Bench-VQA.%2520More%2520importantly%252C%2520our%2520zero-shot%2520GAR-8B%2520even%2520outperforms%250Ain-domain%2520VideoRefer-7B%2520on%2520VideoRefer-BenchQ%252C%2520indicating%2520its%2520strong%250Acapabilities%2520can%2520be%2520easily%2520transferred%2520to%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grasp%20Any%20Region%3A%20Towards%20Precise%2C%20Contextual%20Pixel%20Understanding%20for%0A%20%20Multimodal%20LLMs&entry.906535625=Haochen%20Wang%20and%20Yuhao%20Wang%20and%20Tao%20Zhang%20and%20Yikang%20Zhou%20and%20Yanwei%20Li%20and%20Jiacong%20Wang%20and%20Ye%20Tian%20and%20Jiahao%20Meng%20and%20Zilong%20Huang%20and%20Guangcan%20Mai%20and%20Anran%20Wang%20and%20Yunhai%20Tong%20and%20Zhuochen%20Wang%20and%20Xiangtai%20Li%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20holistic%0Aunderstanding%2C%20they%20struggle%20in%20capturing%20the%20dense%20world%20with%20complex%20scenes%2C%0Arequiring%20fine-grained%20analysis%20of%20intricate%20details%20and%20object%0Ainter-relationships.%20Region-level%20MLLMs%20have%20been%20a%20promising%20step.%20However%2C%0Aprevious%20attempts%20are%20generally%20optimized%20to%20understand%20given%20regions%20in%0Aisolation%2C%20neglecting%20crucial%20global%20contexts.%20To%20address%20this%2C%20we%20introduce%0AGrasp%20Any%20Region%20%28GAR%29%20for%20comprehen-%20sive%20region-level%20visual%20understanding.%0AEmpowered%20by%20an%20effective%20RoI-aligned%20feature%20replay%20technique%2C%20GAR%20supports%0A%281%29%20precise%20perception%20by%20leveraging%20necessary%20global%20contexts%2C%20and%20%282%29%0Amodeling%20interactions%20between%20multiple%20prompts.%20Together%2C%20it%20then%20naturally%0Aachieves%20%283%29%20advanced%20compositional%20reasoning%20to%20answer%20specific%20free-form%0Aquestions%20about%20any%20region%2C%20shifting%20the%20paradigm%20from%20passive%20description%20to%0Aactive%20dialogue.%20Moreover%2C%20we%20construct%20GAR-Bench%2C%20which%20not%20only%20provides%20a%0Amore%20accurate%20evaluation%20of%20single-region%20comprehension%2C%20but%20also%2C%20more%0Aimportantly%2C%20measures%20interactions%20and%20complex%20reasoning%20across%20multiple%0Aregions.%20Extensive%20experiments%20have%20demonstrated%20that%20GAR-1B%20not%20only%20maintains%0Athe%20state-of-the-art%20captioning%20capabilities%2C%20e.g.%2C%20outperforming%20DAM-3B%20%2B4.5%0Aon%20DLC-Bench%2C%20but%20also%20excels%20at%20modeling%20relationships%20between%20multiple%0Aprompts%20with%20advanced%20comprehension%20capabilities%2C%20even%20surpassing%20InternVL3-78B%0Aon%20GAR-Bench-VQA.%20More%20importantly%2C%20our%20zero-shot%20GAR-8B%20even%20outperforms%0Ain-domain%20VideoRefer-7B%20on%20VideoRefer-BenchQ%2C%20indicating%20its%20strong%0Acapabilities%20can%20be%20easily%20transferred%20to%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18876v1&entry.124074799=Read"},
{"title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence", "author": "Ziang Zhang and Zehan Wang and Guanghao Zhang and Weilong Dai and Yan Xia and Ziang Yan and Minjie Hong and Zhou Zhao", "abstract": "  Reasoning about dynamic spatial relationships is essential, as both observers\nand objects often move simultaneously. Although vision-language models (VLMs)\nand visual expertise models excel in 2D tasks and static scenarios, their\nability to fully understand dynamic 3D scenarios remains limited. We introduce\nDynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly\n1,000 dynamic videos and over 1,700 manually annotated questions covering nine\ndecoupled motion patterns of observers and objects. Spatially and temporally\nsymmetric designs reduce biases and enable systematic evaluation of models'\nreasoning about self-motion and object motion. Our evaluation of 14 VLMs and\nexpert models reveals key limitations: models often conflate observer and\nobject motion, exhibit semantic biases, and fail to accurately infer relative\nrelationships in dynamic scenarios. Our DSI-Bench provides valuable findings\nand insights about the future development of general and expertise models with\ndynamic spatial intelligence.\n", "link": "http://arxiv.org/abs/2510.18873v1", "date": "2025-10-21", "relevancy": 2.9938, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6168}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSI-Bench%3A%20A%20Benchmark%20for%20Dynamic%20Spatial%20Intelligence&body=Title%3A%20DSI-Bench%3A%20A%20Benchmark%20for%20Dynamic%20Spatial%20Intelligence%0AAuthor%3A%20Ziang%20Zhang%20and%20Zehan%20Wang%20and%20Guanghao%20Zhang%20and%20Weilong%20Dai%20and%20Yan%20Xia%20and%20Ziang%20Yan%20and%20Minjie%20Hong%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Reasoning%20about%20dynamic%20spatial%20relationships%20is%20essential%2C%20as%20both%20observers%0Aand%20objects%20often%20move%20simultaneously.%20Although%20vision-language%20models%20%28VLMs%29%0Aand%20visual%20expertise%20models%20excel%20in%202D%20tasks%20and%20static%20scenarios%2C%20their%0Aability%20to%20fully%20understand%20dynamic%203D%20scenarios%20remains%20limited.%20We%20introduce%0ADynamic%20Spatial%20Intelligence%20and%20propose%20DSI-Bench%2C%20a%20benchmark%20with%20nearly%0A1%2C000%20dynamic%20videos%20and%20over%201%2C700%20manually%20annotated%20questions%20covering%20nine%0Adecoupled%20motion%20patterns%20of%20observers%20and%20objects.%20Spatially%20and%20temporally%0Asymmetric%20designs%20reduce%20biases%20and%20enable%20systematic%20evaluation%20of%20models%27%0Areasoning%20about%20self-motion%20and%20object%20motion.%20Our%20evaluation%20of%2014%20VLMs%20and%0Aexpert%20models%20reveals%20key%20limitations%3A%20models%20often%20conflate%20observer%20and%0Aobject%20motion%2C%20exhibit%20semantic%20biases%2C%20and%20fail%20to%20accurately%20infer%20relative%0Arelationships%20in%20dynamic%20scenarios.%20Our%20DSI-Bench%20provides%20valuable%20findings%0Aand%20insights%20about%20the%20future%20development%20of%20general%20and%20expertise%20models%20with%0Adynamic%20spatial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSI-Bench%253A%2520A%2520Benchmark%2520for%2520Dynamic%2520Spatial%2520Intelligence%26entry.906535625%3DZiang%2520Zhang%2520and%2520Zehan%2520Wang%2520and%2520Guanghao%2520Zhang%2520and%2520Weilong%2520Dai%2520and%2520Yan%2520Xia%2520and%2520Ziang%2520Yan%2520and%2520Minjie%2520Hong%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Reasoning%2520about%2520dynamic%2520spatial%2520relationships%2520is%2520essential%252C%2520as%2520both%2520observers%250Aand%2520objects%2520often%2520move%2520simultaneously.%2520Although%2520vision-language%2520models%2520%2528VLMs%2529%250Aand%2520visual%2520expertise%2520models%2520excel%2520in%25202D%2520tasks%2520and%2520static%2520scenarios%252C%2520their%250Aability%2520to%2520fully%2520understand%2520dynamic%25203D%2520scenarios%2520remains%2520limited.%2520We%2520introduce%250ADynamic%2520Spatial%2520Intelligence%2520and%2520propose%2520DSI-Bench%252C%2520a%2520benchmark%2520with%2520nearly%250A1%252C000%2520dynamic%2520videos%2520and%2520over%25201%252C700%2520manually%2520annotated%2520questions%2520covering%2520nine%250Adecoupled%2520motion%2520patterns%2520of%2520observers%2520and%2520objects.%2520Spatially%2520and%2520temporally%250Asymmetric%2520designs%2520reduce%2520biases%2520and%2520enable%2520systematic%2520evaluation%2520of%2520models%2527%250Areasoning%2520about%2520self-motion%2520and%2520object%2520motion.%2520Our%2520evaluation%2520of%252014%2520VLMs%2520and%250Aexpert%2520models%2520reveals%2520key%2520limitations%253A%2520models%2520often%2520conflate%2520observer%2520and%250Aobject%2520motion%252C%2520exhibit%2520semantic%2520biases%252C%2520and%2520fail%2520to%2520accurately%2520infer%2520relative%250Arelationships%2520in%2520dynamic%2520scenarios.%2520Our%2520DSI-Bench%2520provides%2520valuable%2520findings%250Aand%2520insights%2520about%2520the%2520future%2520development%2520of%2520general%2520and%2520expertise%2520models%2520with%250Adynamic%2520spatial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSI-Bench%3A%20A%20Benchmark%20for%20Dynamic%20Spatial%20Intelligence&entry.906535625=Ziang%20Zhang%20and%20Zehan%20Wang%20and%20Guanghao%20Zhang%20and%20Weilong%20Dai%20and%20Yan%20Xia%20and%20Ziang%20Yan%20and%20Minjie%20Hong%20and%20Zhou%20Zhao&entry.1292438233=%20%20Reasoning%20about%20dynamic%20spatial%20relationships%20is%20essential%2C%20as%20both%20observers%0Aand%20objects%20often%20move%20simultaneously.%20Although%20vision-language%20models%20%28VLMs%29%0Aand%20visual%20expertise%20models%20excel%20in%202D%20tasks%20and%20static%20scenarios%2C%20their%0Aability%20to%20fully%20understand%20dynamic%203D%20scenarios%20remains%20limited.%20We%20introduce%0ADynamic%20Spatial%20Intelligence%20and%20propose%20DSI-Bench%2C%20a%20benchmark%20with%20nearly%0A1%2C000%20dynamic%20videos%20and%20over%201%2C700%20manually%20annotated%20questions%20covering%20nine%0Adecoupled%20motion%20patterns%20of%20observers%20and%20objects.%20Spatially%20and%20temporally%0Asymmetric%20designs%20reduce%20biases%20and%20enable%20systematic%20evaluation%20of%20models%27%0Areasoning%20about%20self-motion%20and%20object%20motion.%20Our%20evaluation%20of%2014%20VLMs%20and%0Aexpert%20models%20reveals%20key%20limitations%3A%20models%20often%20conflate%20observer%20and%0Aobject%20motion%2C%20exhibit%20semantic%20biases%2C%20and%20fail%20to%20accurately%20infer%20relative%0Arelationships%20in%20dynamic%20scenarios.%20Our%20DSI-Bench%20provides%20valuable%20findings%0Aand%20insights%20about%20the%20future%20development%20of%20general%20and%20expertise%20models%20with%0Adynamic%20spatial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18873v1&entry.124074799=Read"},
{"title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder", "author": "Xiaoxing Hu and Kaicheng Yang and Ziyong Feng and Qi Ming and Zonghao Guo and Xiang An and Ziyong Feng and Junchi Yan and Xue Yang", "abstract": "  The original CLIP text encoder is limited by a maximum input length of 77\ntokens, which hampers its ability to effectively process long texts and perform\nfine-grained semantic understanding. In addition, the CLIP text encoder lacks\nsupport for multilingual inputs. All these limitations significantly restrict\nits applicability across a broader range of tasks. Recent studies have\nattempted to replace the CLIP text encoder with an LLM-based embedder to\nenhance its ability in processing long texts, multilingual understanding, and\nfine-grained semantic comprehension. However, because the representation spaces\nof LLMs and the vision-language space of CLIP are pretrained independently\nwithout alignment priors, direct alignment using contrastive learning can\ndisrupt the intrinsic vision-language alignment in the CLIP image encoder,\nleading to an underutilization of the knowledge acquired during pre-training.\nTo address this challenge, we propose ProCLIP, a curriculum learning-based\nprogressive vision-language alignment framework to effectively align the CLIP\nimage encoder with an LLM-based embedder. Specifically, ProCLIP first distills\nknowledge from CLIP's text encoder into the LLM-based embedder to leverage\nCLIP's rich pretrained knowledge while establishing initial alignment between\nthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns\nthe CLIP image encoder with the LLM-based embedder through image-text\ncontrastive tuning, employing self-distillation regularization to avoid\noverfitting. To achieve a more effective alignment, instance semantic alignment\nloss and embedding structure alignment loss are employed during representation\ninheritance and contrastive tuning. The Code is available at\nhttps://github.com/VisionXLab/ProCLIP\n", "link": "http://arxiv.org/abs/2510.18795v1", "date": "2025-10-21", "relevancy": 2.989, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProCLIP%3A%20Progressive%20Vision-Language%20Alignment%20via%20LLM-based%20Embedder&body=Title%3A%20ProCLIP%3A%20Progressive%20Vision-Language%20Alignment%20via%20LLM-based%20Embedder%0AAuthor%3A%20Xiaoxing%20Hu%20and%20Kaicheng%20Yang%20and%20Ziyong%20Feng%20and%20Qi%20Ming%20and%20Zonghao%20Guo%20and%20Xiang%20An%20and%20Ziyong%20Feng%20and%20Junchi%20Yan%20and%20Xue%20Yang%0AAbstract%3A%20%20%20The%20original%20CLIP%20text%20encoder%20is%20limited%20by%20a%20maximum%20input%20length%20of%2077%0Atokens%2C%20which%20hampers%20its%20ability%20to%20effectively%20process%20long%20texts%20and%20perform%0Afine-grained%20semantic%20understanding.%20In%20addition%2C%20the%20CLIP%20text%20encoder%20lacks%0Asupport%20for%20multilingual%20inputs.%20All%20these%20limitations%20significantly%20restrict%0Aits%20applicability%20across%20a%20broader%20range%20of%20tasks.%20Recent%20studies%20have%0Aattempted%20to%20replace%20the%20CLIP%20text%20encoder%20with%20an%20LLM-based%20embedder%20to%0Aenhance%20its%20ability%20in%20processing%20long%20texts%2C%20multilingual%20understanding%2C%20and%0Afine-grained%20semantic%20comprehension.%20However%2C%20because%20the%20representation%20spaces%0Aof%20LLMs%20and%20the%20vision-language%20space%20of%20CLIP%20are%20pretrained%20independently%0Awithout%20alignment%20priors%2C%20direct%20alignment%20using%20contrastive%20learning%20can%0Adisrupt%20the%20intrinsic%20vision-language%20alignment%20in%20the%20CLIP%20image%20encoder%2C%0Aleading%20to%20an%20underutilization%20of%20the%20knowledge%20acquired%20during%20pre-training.%0ATo%20address%20this%20challenge%2C%20we%20propose%20ProCLIP%2C%20a%20curriculum%20learning-based%0Aprogressive%20vision-language%20alignment%20framework%20to%20effectively%20align%20the%20CLIP%0Aimage%20encoder%20with%20an%20LLM-based%20embedder.%20Specifically%2C%20ProCLIP%20first%20distills%0Aknowledge%20from%20CLIP%27s%20text%20encoder%20into%20the%20LLM-based%20embedder%20to%20leverage%0ACLIP%27s%20rich%20pretrained%20knowledge%20while%20establishing%20initial%20alignment%20between%0Athe%20LLM%20embedder%20and%20CLIP%20image%20encoder.%20Subsequently%2C%20ProCLIP%20further%20aligns%0Athe%20CLIP%20image%20encoder%20with%20the%20LLM-based%20embedder%20through%20image-text%0Acontrastive%20tuning%2C%20employing%20self-distillation%20regularization%20to%20avoid%0Aoverfitting.%20To%20achieve%20a%20more%20effective%20alignment%2C%20instance%20semantic%20alignment%0Aloss%20and%20embedding%20structure%20alignment%20loss%20are%20employed%20during%20representation%0Ainheritance%20and%20contrastive%20tuning.%20The%20Code%20is%20available%20at%0Ahttps%3A//github.com/VisionXLab/ProCLIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProCLIP%253A%2520Progressive%2520Vision-Language%2520Alignment%2520via%2520LLM-based%2520Embedder%26entry.906535625%3DXiaoxing%2520Hu%2520and%2520Kaicheng%2520Yang%2520and%2520Ziyong%2520Feng%2520and%2520Qi%2520Ming%2520and%2520Zonghao%2520Guo%2520and%2520Xiang%2520An%2520and%2520Ziyong%2520Feng%2520and%2520Junchi%2520Yan%2520and%2520Xue%2520Yang%26entry.1292438233%3D%2520%2520The%2520original%2520CLIP%2520text%2520encoder%2520is%2520limited%2520by%2520a%2520maximum%2520input%2520length%2520of%252077%250Atokens%252C%2520which%2520hampers%2520its%2520ability%2520to%2520effectively%2520process%2520long%2520texts%2520and%2520perform%250Afine-grained%2520semantic%2520understanding.%2520In%2520addition%252C%2520the%2520CLIP%2520text%2520encoder%2520lacks%250Asupport%2520for%2520multilingual%2520inputs.%2520All%2520these%2520limitations%2520significantly%2520restrict%250Aits%2520applicability%2520across%2520a%2520broader%2520range%2520of%2520tasks.%2520Recent%2520studies%2520have%250Aattempted%2520to%2520replace%2520the%2520CLIP%2520text%2520encoder%2520with%2520an%2520LLM-based%2520embedder%2520to%250Aenhance%2520its%2520ability%2520in%2520processing%2520long%2520texts%252C%2520multilingual%2520understanding%252C%2520and%250Afine-grained%2520semantic%2520comprehension.%2520However%252C%2520because%2520the%2520representation%2520spaces%250Aof%2520LLMs%2520and%2520the%2520vision-language%2520space%2520of%2520CLIP%2520are%2520pretrained%2520independently%250Awithout%2520alignment%2520priors%252C%2520direct%2520alignment%2520using%2520contrastive%2520learning%2520can%250Adisrupt%2520the%2520intrinsic%2520vision-language%2520alignment%2520in%2520the%2520CLIP%2520image%2520encoder%252C%250Aleading%2520to%2520an%2520underutilization%2520of%2520the%2520knowledge%2520acquired%2520during%2520pre-training.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520propose%2520ProCLIP%252C%2520a%2520curriculum%2520learning-based%250Aprogressive%2520vision-language%2520alignment%2520framework%2520to%2520effectively%2520align%2520the%2520CLIP%250Aimage%2520encoder%2520with%2520an%2520LLM-based%2520embedder.%2520Specifically%252C%2520ProCLIP%2520first%2520distills%250Aknowledge%2520from%2520CLIP%2527s%2520text%2520encoder%2520into%2520the%2520LLM-based%2520embedder%2520to%2520leverage%250ACLIP%2527s%2520rich%2520pretrained%2520knowledge%2520while%2520establishing%2520initial%2520alignment%2520between%250Athe%2520LLM%2520embedder%2520and%2520CLIP%2520image%2520encoder.%2520Subsequently%252C%2520ProCLIP%2520further%2520aligns%250Athe%2520CLIP%2520image%2520encoder%2520with%2520the%2520LLM-based%2520embedder%2520through%2520image-text%250Acontrastive%2520tuning%252C%2520employing%2520self-distillation%2520regularization%2520to%2520avoid%250Aoverfitting.%2520To%2520achieve%2520a%2520more%2520effective%2520alignment%252C%2520instance%2520semantic%2520alignment%250Aloss%2520and%2520embedding%2520structure%2520alignment%2520loss%2520are%2520employed%2520during%2520representation%250Ainheritance%2520and%2520contrastive%2520tuning.%2520The%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/VisionXLab/ProCLIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProCLIP%3A%20Progressive%20Vision-Language%20Alignment%20via%20LLM-based%20Embedder&entry.906535625=Xiaoxing%20Hu%20and%20Kaicheng%20Yang%20and%20Ziyong%20Feng%20and%20Qi%20Ming%20and%20Zonghao%20Guo%20and%20Xiang%20An%20and%20Ziyong%20Feng%20and%20Junchi%20Yan%20and%20Xue%20Yang&entry.1292438233=%20%20The%20original%20CLIP%20text%20encoder%20is%20limited%20by%20a%20maximum%20input%20length%20of%2077%0Atokens%2C%20which%20hampers%20its%20ability%20to%20effectively%20process%20long%20texts%20and%20perform%0Afine-grained%20semantic%20understanding.%20In%20addition%2C%20the%20CLIP%20text%20encoder%20lacks%0Asupport%20for%20multilingual%20inputs.%20All%20these%20limitations%20significantly%20restrict%0Aits%20applicability%20across%20a%20broader%20range%20of%20tasks.%20Recent%20studies%20have%0Aattempted%20to%20replace%20the%20CLIP%20text%20encoder%20with%20an%20LLM-based%20embedder%20to%0Aenhance%20its%20ability%20in%20processing%20long%20texts%2C%20multilingual%20understanding%2C%20and%0Afine-grained%20semantic%20comprehension.%20However%2C%20because%20the%20representation%20spaces%0Aof%20LLMs%20and%20the%20vision-language%20space%20of%20CLIP%20are%20pretrained%20independently%0Awithout%20alignment%20priors%2C%20direct%20alignment%20using%20contrastive%20learning%20can%0Adisrupt%20the%20intrinsic%20vision-language%20alignment%20in%20the%20CLIP%20image%20encoder%2C%0Aleading%20to%20an%20underutilization%20of%20the%20knowledge%20acquired%20during%20pre-training.%0ATo%20address%20this%20challenge%2C%20we%20propose%20ProCLIP%2C%20a%20curriculum%20learning-based%0Aprogressive%20vision-language%20alignment%20framework%20to%20effectively%20align%20the%20CLIP%0Aimage%20encoder%20with%20an%20LLM-based%20embedder.%20Specifically%2C%20ProCLIP%20first%20distills%0Aknowledge%20from%20CLIP%27s%20text%20encoder%20into%20the%20LLM-based%20embedder%20to%20leverage%0ACLIP%27s%20rich%20pretrained%20knowledge%20while%20establishing%20initial%20alignment%20between%0Athe%20LLM%20embedder%20and%20CLIP%20image%20encoder.%20Subsequently%2C%20ProCLIP%20further%20aligns%0Athe%20CLIP%20image%20encoder%20with%20the%20LLM-based%20embedder%20through%20image-text%0Acontrastive%20tuning%2C%20employing%20self-distillation%20regularization%20to%20avoid%0Aoverfitting.%20To%20achieve%20a%20more%20effective%20alignment%2C%20instance%20semantic%20alignment%0Aloss%20and%20embedding%20structure%20alignment%20loss%20are%20employed%20during%20representation%0Ainheritance%20and%20contrastive%20tuning.%20The%20Code%20is%20available%20at%0Ahttps%3A//github.com/VisionXLab/ProCLIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18795v1&entry.124074799=Read"},
{"title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and\n  Generation via Reinforcement Learning", "author": "Kaihang Pan and Yang Wu and Wendong Bu and Kai Shen and Juncheng Li and Yingting Wang and Yunfei Li and Siliang Tang and Jun Xiao and Fei Wu and Hang Zhao and Yueting Zhuang", "abstract": "  Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation. However, these two capabilities remain\nlargely independent, as if they are two separate functions encapsulated within\nthe same model. Consequently, visual comprehension does not enhance visual\ngeneration, and the reasoning mechanisms of LLMs have not been fully integrated\nto revolutionize image generation. In this paper, we propose to enable the\ncollaborative co-evolution of visual comprehension and generation, advancing\nimage generation into an iterative introspective process. We introduce a\ntwo-stage training approach: supervised fine-tuning teaches the MLLM with the\nfoundational ability to generate genuine CoT for visual generation, while\nreinforcement learning activates its full potential via an\nexploration-exploitation trade-off. Ultimately, we unlock the Aha moment in\nvisual generation, advancing MLLMs from text-to-image tasks to unified image\ngeneration. Extensive experiments demonstrate that our model not only excels in\ntext-to-image generation and image editing, but also functions as a superior\nimage semantic evaluator with enhanced visual comprehension capabilities.\nProject Page: https://janus-pro-r1.github.io.\n", "link": "http://arxiv.org/abs/2506.01480v2", "date": "2025-10-21", "relevancy": 2.9063, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Janus-Pro-R1%3A%20Advancing%20Collaborative%20Visual%20Comprehension%20and%0A%20%20Generation%20via%20Reinforcement%20Learning&body=Title%3A%20Janus-Pro-R1%3A%20Advancing%20Collaborative%20Visual%20Comprehension%20and%0A%20%20Generation%20via%20Reinforcement%20Learning%0AAuthor%3A%20Kaihang%20Pan%20and%20Yang%20Wu%20and%20Wendong%20Bu%20and%20Kai%20Shen%20and%20Juncheng%20Li%20and%20Yingting%20Wang%20and%20Yunfei%20Li%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Fei%20Wu%20and%20Hang%20Zhao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Recent%20endeavors%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20aim%20to%20unify%0Avisual%20comprehension%20and%20generation.%20However%2C%20these%20two%20capabilities%20remain%0Alargely%20independent%2C%20as%20if%20they%20are%20two%20separate%20functions%20encapsulated%20within%0Athe%20same%20model.%20Consequently%2C%20visual%20comprehension%20does%20not%20enhance%20visual%0Ageneration%2C%20and%20the%20reasoning%20mechanisms%20of%20LLMs%20have%20not%20been%20fully%20integrated%0Ato%20revolutionize%20image%20generation.%20In%20this%20paper%2C%20we%20propose%20to%20enable%20the%0Acollaborative%20co-evolution%20of%20visual%20comprehension%20and%20generation%2C%20advancing%0Aimage%20generation%20into%20an%20iterative%20introspective%20process.%20We%20introduce%20a%0Atwo-stage%20training%20approach%3A%20supervised%20fine-tuning%20teaches%20the%20MLLM%20with%20the%0Afoundational%20ability%20to%20generate%20genuine%20CoT%20for%20visual%20generation%2C%20while%0Areinforcement%20learning%20activates%20its%20full%20potential%20via%20an%0Aexploration-exploitation%20trade-off.%20Ultimately%2C%20we%20unlock%20the%20Aha%20moment%20in%0Avisual%20generation%2C%20advancing%20MLLMs%20from%20text-to-image%20tasks%20to%20unified%20image%0Ageneration.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20not%20only%20excels%20in%0Atext-to-image%20generation%20and%20image%20editing%2C%20but%20also%20functions%20as%20a%20superior%0Aimage%20semantic%20evaluator%20with%20enhanced%20visual%20comprehension%20capabilities.%0AProject%20Page%3A%20https%3A//janus-pro-r1.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJanus-Pro-R1%253A%2520Advancing%2520Collaborative%2520Visual%2520Comprehension%2520and%250A%2520%2520Generation%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DKaihang%2520Pan%2520and%2520Yang%2520Wu%2520and%2520Wendong%2520Bu%2520and%2520Kai%2520Shen%2520and%2520Juncheng%2520Li%2520and%2520Yingting%2520Wang%2520and%2520Yunfei%2520Li%2520and%2520Siliang%2520Tang%2520and%2520Jun%2520Xiao%2520and%2520Fei%2520Wu%2520and%2520Hang%2520Zhao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Recent%2520endeavors%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520aim%2520to%2520unify%250Avisual%2520comprehension%2520and%2520generation.%2520However%252C%2520these%2520two%2520capabilities%2520remain%250Alargely%2520independent%252C%2520as%2520if%2520they%2520are%2520two%2520separate%2520functions%2520encapsulated%2520within%250Athe%2520same%2520model.%2520Consequently%252C%2520visual%2520comprehension%2520does%2520not%2520enhance%2520visual%250Ageneration%252C%2520and%2520the%2520reasoning%2520mechanisms%2520of%2520LLMs%2520have%2520not%2520been%2520fully%2520integrated%250Ato%2520revolutionize%2520image%2520generation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520enable%2520the%250Acollaborative%2520co-evolution%2520of%2520visual%2520comprehension%2520and%2520generation%252C%2520advancing%250Aimage%2520generation%2520into%2520an%2520iterative%2520introspective%2520process.%2520We%2520introduce%2520a%250Atwo-stage%2520training%2520approach%253A%2520supervised%2520fine-tuning%2520teaches%2520the%2520MLLM%2520with%2520the%250Afoundational%2520ability%2520to%2520generate%2520genuine%2520CoT%2520for%2520visual%2520generation%252C%2520while%250Areinforcement%2520learning%2520activates%2520its%2520full%2520potential%2520via%2520an%250Aexploration-exploitation%2520trade-off.%2520Ultimately%252C%2520we%2520unlock%2520the%2520Aha%2520moment%2520in%250Avisual%2520generation%252C%2520advancing%2520MLLMs%2520from%2520text-to-image%2520tasks%2520to%2520unified%2520image%250Ageneration.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520model%2520not%2520only%2520excels%2520in%250Atext-to-image%2520generation%2520and%2520image%2520editing%252C%2520but%2520also%2520functions%2520as%2520a%2520superior%250Aimage%2520semantic%2520evaluator%2520with%2520enhanced%2520visual%2520comprehension%2520capabilities.%250AProject%2520Page%253A%2520https%253A//janus-pro-r1.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Janus-Pro-R1%3A%20Advancing%20Collaborative%20Visual%20Comprehension%20and%0A%20%20Generation%20via%20Reinforcement%20Learning&entry.906535625=Kaihang%20Pan%20and%20Yang%20Wu%20and%20Wendong%20Bu%20and%20Kai%20Shen%20and%20Juncheng%20Li%20and%20Yingting%20Wang%20and%20Yunfei%20Li%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Fei%20Wu%20and%20Hang%20Zhao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Recent%20endeavors%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20aim%20to%20unify%0Avisual%20comprehension%20and%20generation.%20However%2C%20these%20two%20capabilities%20remain%0Alargely%20independent%2C%20as%20if%20they%20are%20two%20separate%20functions%20encapsulated%20within%0Athe%20same%20model.%20Consequently%2C%20visual%20comprehension%20does%20not%20enhance%20visual%0Ageneration%2C%20and%20the%20reasoning%20mechanisms%20of%20LLMs%20have%20not%20been%20fully%20integrated%0Ato%20revolutionize%20image%20generation.%20In%20this%20paper%2C%20we%20propose%20to%20enable%20the%0Acollaborative%20co-evolution%20of%20visual%20comprehension%20and%20generation%2C%20advancing%0Aimage%20generation%20into%20an%20iterative%20introspective%20process.%20We%20introduce%20a%0Atwo-stage%20training%20approach%3A%20supervised%20fine-tuning%20teaches%20the%20MLLM%20with%20the%0Afoundational%20ability%20to%20generate%20genuine%20CoT%20for%20visual%20generation%2C%20while%0Areinforcement%20learning%20activates%20its%20full%20potential%20via%20an%0Aexploration-exploitation%20trade-off.%20Ultimately%2C%20we%20unlock%20the%20Aha%20moment%20in%0Avisual%20generation%2C%20advancing%20MLLMs%20from%20text-to-image%20tasks%20to%20unified%20image%0Ageneration.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20not%20only%20excels%20in%0Atext-to-image%20generation%20and%20image%20editing%2C%20but%20also%20functions%20as%20a%20superior%0Aimage%20semantic%20evaluator%20with%20enhanced%20visual%20comprehension%20capabilities.%0AProject%20Page%3A%20https%3A//janus-pro-r1.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01480v2&entry.124074799=Read"},
{"title": "IF-VidCap: Can Video Caption Models Follow Instructions?", "author": "Shihao Li and Yuanxing Zhang and Jiangtao Wu and Zhide Lei and Yiwen He and Runzhe Wen and Chenxi Liao and Chengkang Jiang and An Ping and Shuo Gao and Suhan Wang and Zhaozhou Bian and Zijun Zhou and Jingyi Xie and Jiayi Zhou and Jing Wang and Yifan Yao and Weihao Xie and Yingshui Tan and Yanghai Wang and Qianqian Xie and Zhaoxiang Zhang and Jiaheng Liu", "abstract": "  Although Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in video captioning, practical applications require captions that\nfollow specific user instructions rather than generating exhaustive,\nunconstrained descriptions. Current benchmarks, however, primarily assess\ndescriptive comprehensiveness while largely overlooking instruction-following\ncapabilities. To address this gap, we introduce IF-VidCap, a new benchmark for\nevaluating controllable video captioning, which contains 1,400 high-quality\nsamples. Distinct from existing video captioning or general\ninstruction-following benchmarks, IF-VidCap incorporates a systematic framework\nthat assesses captions on two dimensions: format correctness and content\ncorrectness. Our comprehensive evaluation of over 20 prominent models reveals a\nnuanced landscape: despite the continued dominance of proprietary models, the\nperformance gap is closing, with top-tier open-source solutions now achieving\nnear-parity. Furthermore, we find that models specialized for dense captioning\nunderperform general-purpose MLLMs on complex instructions, indicating that\nfuture work should simultaneously advance both descriptive richness and\ninstruction-following fidelity.\n", "link": "http://arxiv.org/abs/2510.18726v1", "date": "2025-10-21", "relevancy": 2.7594, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5632}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5632}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IF-VidCap%3A%20Can%20Video%20Caption%20Models%20Follow%20Instructions%3F&body=Title%3A%20IF-VidCap%3A%20Can%20Video%20Caption%20Models%20Follow%20Instructions%3F%0AAuthor%3A%20Shihao%20Li%20and%20Yuanxing%20Zhang%20and%20Jiangtao%20Wu%20and%20Zhide%20Lei%20and%20Yiwen%20He%20and%20Runzhe%20Wen%20and%20Chenxi%20Liao%20and%20Chengkang%20Jiang%20and%20An%20Ping%20and%20Shuo%20Gao%20and%20Suhan%20Wang%20and%20Zhaozhou%20Bian%20and%20Zijun%20Zhou%20and%20Jingyi%20Xie%20and%20Jiayi%20Zhou%20and%20Jing%20Wang%20and%20Yifan%20Yao%20and%20Weihao%20Xie%20and%20Yingshui%20Tan%20and%20Yanghai%20Wang%20and%20Qianqian%20Xie%20and%20Zhaoxiang%20Zhang%20and%20Jiaheng%20Liu%0AAbstract%3A%20%20%20Although%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%0Aproficiency%20in%20video%20captioning%2C%20practical%20applications%20require%20captions%20that%0Afollow%20specific%20user%20instructions%20rather%20than%20generating%20exhaustive%2C%0Aunconstrained%20descriptions.%20Current%20benchmarks%2C%20however%2C%20primarily%20assess%0Adescriptive%20comprehensiveness%20while%20largely%20overlooking%20instruction-following%0Acapabilities.%20To%20address%20this%20gap%2C%20we%20introduce%20IF-VidCap%2C%20a%20new%20benchmark%20for%0Aevaluating%20controllable%20video%20captioning%2C%20which%20contains%201%2C400%20high-quality%0Asamples.%20Distinct%20from%20existing%20video%20captioning%20or%20general%0Ainstruction-following%20benchmarks%2C%20IF-VidCap%20incorporates%20a%20systematic%20framework%0Athat%20assesses%20captions%20on%20two%20dimensions%3A%20format%20correctness%20and%20content%0Acorrectness.%20Our%20comprehensive%20evaluation%20of%20over%2020%20prominent%20models%20reveals%20a%0Anuanced%20landscape%3A%20despite%20the%20continued%20dominance%20of%20proprietary%20models%2C%20the%0Aperformance%20gap%20is%20closing%2C%20with%20top-tier%20open-source%20solutions%20now%20achieving%0Anear-parity.%20Furthermore%2C%20we%20find%20that%20models%20specialized%20for%20dense%20captioning%0Aunderperform%20general-purpose%20MLLMs%20on%20complex%20instructions%2C%20indicating%20that%0Afuture%20work%20should%20simultaneously%20advance%20both%20descriptive%20richness%20and%0Ainstruction-following%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIF-VidCap%253A%2520Can%2520Video%2520Caption%2520Models%2520Follow%2520Instructions%253F%26entry.906535625%3DShihao%2520Li%2520and%2520Yuanxing%2520Zhang%2520and%2520Jiangtao%2520Wu%2520and%2520Zhide%2520Lei%2520and%2520Yiwen%2520He%2520and%2520Runzhe%2520Wen%2520and%2520Chenxi%2520Liao%2520and%2520Chengkang%2520Jiang%2520and%2520An%2520Ping%2520and%2520Shuo%2520Gao%2520and%2520Suhan%2520Wang%2520and%2520Zhaozhou%2520Bian%2520and%2520Zijun%2520Zhou%2520and%2520Jingyi%2520Xie%2520and%2520Jiayi%2520Zhou%2520and%2520Jing%2520Wang%2520and%2520Yifan%2520Yao%2520and%2520Weihao%2520Xie%2520and%2520Yingshui%2520Tan%2520and%2520Yanghai%2520Wang%2520and%2520Qianqian%2520Xie%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Jiaheng%2520Liu%26entry.1292438233%3D%2520%2520Although%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%250Aproficiency%2520in%2520video%2520captioning%252C%2520practical%2520applications%2520require%2520captions%2520that%250Afollow%2520specific%2520user%2520instructions%2520rather%2520than%2520generating%2520exhaustive%252C%250Aunconstrained%2520descriptions.%2520Current%2520benchmarks%252C%2520however%252C%2520primarily%2520assess%250Adescriptive%2520comprehensiveness%2520while%2520largely%2520overlooking%2520instruction-following%250Acapabilities.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520IF-VidCap%252C%2520a%2520new%2520benchmark%2520for%250Aevaluating%2520controllable%2520video%2520captioning%252C%2520which%2520contains%25201%252C400%2520high-quality%250Asamples.%2520Distinct%2520from%2520existing%2520video%2520captioning%2520or%2520general%250Ainstruction-following%2520benchmarks%252C%2520IF-VidCap%2520incorporates%2520a%2520systematic%2520framework%250Athat%2520assesses%2520captions%2520on%2520two%2520dimensions%253A%2520format%2520correctness%2520and%2520content%250Acorrectness.%2520Our%2520comprehensive%2520evaluation%2520of%2520over%252020%2520prominent%2520models%2520reveals%2520a%250Anuanced%2520landscape%253A%2520despite%2520the%2520continued%2520dominance%2520of%2520proprietary%2520models%252C%2520the%250Aperformance%2520gap%2520is%2520closing%252C%2520with%2520top-tier%2520open-source%2520solutions%2520now%2520achieving%250Anear-parity.%2520Furthermore%252C%2520we%2520find%2520that%2520models%2520specialized%2520for%2520dense%2520captioning%250Aunderperform%2520general-purpose%2520MLLMs%2520on%2520complex%2520instructions%252C%2520indicating%2520that%250Afuture%2520work%2520should%2520simultaneously%2520advance%2520both%2520descriptive%2520richness%2520and%250Ainstruction-following%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IF-VidCap%3A%20Can%20Video%20Caption%20Models%20Follow%20Instructions%3F&entry.906535625=Shihao%20Li%20and%20Yuanxing%20Zhang%20and%20Jiangtao%20Wu%20and%20Zhide%20Lei%20and%20Yiwen%20He%20and%20Runzhe%20Wen%20and%20Chenxi%20Liao%20and%20Chengkang%20Jiang%20and%20An%20Ping%20and%20Shuo%20Gao%20and%20Suhan%20Wang%20and%20Zhaozhou%20Bian%20and%20Zijun%20Zhou%20and%20Jingyi%20Xie%20and%20Jiayi%20Zhou%20and%20Jing%20Wang%20and%20Yifan%20Yao%20and%20Weihao%20Xie%20and%20Yingshui%20Tan%20and%20Yanghai%20Wang%20and%20Qianqian%20Xie%20and%20Zhaoxiang%20Zhang%20and%20Jiaheng%20Liu&entry.1292438233=%20%20Although%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%0Aproficiency%20in%20video%20captioning%2C%20practical%20applications%20require%20captions%20that%0Afollow%20specific%20user%20instructions%20rather%20than%20generating%20exhaustive%2C%0Aunconstrained%20descriptions.%20Current%20benchmarks%2C%20however%2C%20primarily%20assess%0Adescriptive%20comprehensiveness%20while%20largely%20overlooking%20instruction-following%0Acapabilities.%20To%20address%20this%20gap%2C%20we%20introduce%20IF-VidCap%2C%20a%20new%20benchmark%20for%0Aevaluating%20controllable%20video%20captioning%2C%20which%20contains%201%2C400%20high-quality%0Asamples.%20Distinct%20from%20existing%20video%20captioning%20or%20general%0Ainstruction-following%20benchmarks%2C%20IF-VidCap%20incorporates%20a%20systematic%20framework%0Athat%20assesses%20captions%20on%20two%20dimensions%3A%20format%20correctness%20and%20content%0Acorrectness.%20Our%20comprehensive%20evaluation%20of%20over%2020%20prominent%20models%20reveals%20a%0Anuanced%20landscape%3A%20despite%20the%20continued%20dominance%20of%20proprietary%20models%2C%20the%0Aperformance%20gap%20is%20closing%2C%20with%20top-tier%20open-source%20solutions%20now%20achieving%0Anear-parity.%20Furthermore%2C%20we%20find%20that%20models%20specialized%20for%20dense%20captioning%0Aunderperform%20general-purpose%20MLLMs%20on%20complex%20instructions%2C%20indicating%20that%0Afuture%20work%20should%20simultaneously%20advance%20both%20descriptive%20richness%20and%0Ainstruction-following%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18726v1&entry.124074799=Read"},
{"title": "See the Text: From Tokenization to Visual Reading", "author": "Ling Xing and Alex Jinpeng Wang and Rui Yan and Hongyu Qu and Zechao Li and Jinhui Tang", "abstract": "  People see text. Humans read by recognizing words as visual objects,\nincluding their shapes, layouts, and patterns, before connecting them to\nmeaning, which enables us to handle typos, distorted fonts, and various scripts\neffectively. Modern large language models (LLMs), however, rely on subword\ntokenization, fragmenting text into pieces from a fixed vocabulary. While\neffective for high-resource languages, this approach over-segments low-resource\nlanguages, yielding long, linguistically meaningless sequences and inflating\ncomputation. In this work, we challenge this entrenched paradigm and move\ntoward a vision-centric alternative. Our method, SeeTok, renders text as images\n(visual-text) and leverages pretrained multimodal LLMs to interpret them,\nreusing strong OCR and text-vision alignment abilities learned from large-scale\nmultimodal training. Across three different language tasks, SeeTok matches or\nsurpasses subword tokenizers while requiring 4.43 times fewer tokens and\nreducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,\nrobustness to typographic noise, and linguistic hierarchy. SeeTok signals a\nshift from symbolic tokenization to human-like visual reading, and takes a step\ntoward more natural and cognitively inspired language models.\n", "link": "http://arxiv.org/abs/2510.18840v1", "date": "2025-10-21", "relevancy": 2.7463, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20the%20Text%3A%20From%20Tokenization%20to%20Visual%20Reading&body=Title%3A%20See%20the%20Text%3A%20From%20Tokenization%20to%20Visual%20Reading%0AAuthor%3A%20Ling%20Xing%20and%20Alex%20Jinpeng%20Wang%20and%20Rui%20Yan%20and%20Hongyu%20Qu%20and%20Zechao%20Li%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20People%20see%20text.%20Humans%20read%20by%20recognizing%20words%20as%20visual%20objects%2C%0Aincluding%20their%20shapes%2C%20layouts%2C%20and%20patterns%2C%20before%20connecting%20them%20to%0Ameaning%2C%20which%20enables%20us%20to%20handle%20typos%2C%20distorted%20fonts%2C%20and%20various%20scripts%0Aeffectively.%20Modern%20large%20language%20models%20%28LLMs%29%2C%20however%2C%20rely%20on%20subword%0Atokenization%2C%20fragmenting%20text%20into%20pieces%20from%20a%20fixed%20vocabulary.%20While%0Aeffective%20for%20high-resource%20languages%2C%20this%20approach%20over-segments%20low-resource%0Alanguages%2C%20yielding%20long%2C%20linguistically%20meaningless%20sequences%20and%20inflating%0Acomputation.%20In%20this%20work%2C%20we%20challenge%20this%20entrenched%20paradigm%20and%20move%0Atoward%20a%20vision-centric%20alternative.%20Our%20method%2C%20SeeTok%2C%20renders%20text%20as%20images%0A%28visual-text%29%20and%20leverages%20pretrained%20multimodal%20LLMs%20to%20interpret%20them%2C%0Areusing%20strong%20OCR%20and%20text-vision%20alignment%20abilities%20learned%20from%20large-scale%0Amultimodal%20training.%20Across%20three%20different%20language%20tasks%2C%20SeeTok%20matches%20or%0Asurpasses%20subword%20tokenizers%20while%20requiring%204.43%20times%20fewer%20tokens%20and%0Areducing%20FLOPs%20by%2070.5%25%2C%20with%20additional%20gains%20in%20cross-lingual%20generalization%2C%0Arobustness%20to%20typographic%20noise%2C%20and%20linguistic%20hierarchy.%20SeeTok%20signals%20a%0Ashift%20from%20symbolic%20tokenization%20to%20human-like%20visual%20reading%2C%20and%20takes%20a%20step%0Atoward%20more%20natural%20and%20cognitively%20inspired%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520the%2520Text%253A%2520From%2520Tokenization%2520to%2520Visual%2520Reading%26entry.906535625%3DLing%2520Xing%2520and%2520Alex%2520Jinpeng%2520Wang%2520and%2520Rui%2520Yan%2520and%2520Hongyu%2520Qu%2520and%2520Zechao%2520Li%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520People%2520see%2520text.%2520Humans%2520read%2520by%2520recognizing%2520words%2520as%2520visual%2520objects%252C%250Aincluding%2520their%2520shapes%252C%2520layouts%252C%2520and%2520patterns%252C%2520before%2520connecting%2520them%2520to%250Ameaning%252C%2520which%2520enables%2520us%2520to%2520handle%2520typos%252C%2520distorted%2520fonts%252C%2520and%2520various%2520scripts%250Aeffectively.%2520Modern%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520however%252C%2520rely%2520on%2520subword%250Atokenization%252C%2520fragmenting%2520text%2520into%2520pieces%2520from%2520a%2520fixed%2520vocabulary.%2520While%250Aeffective%2520for%2520high-resource%2520languages%252C%2520this%2520approach%2520over-segments%2520low-resource%250Alanguages%252C%2520yielding%2520long%252C%2520linguistically%2520meaningless%2520sequences%2520and%2520inflating%250Acomputation.%2520In%2520this%2520work%252C%2520we%2520challenge%2520this%2520entrenched%2520paradigm%2520and%2520move%250Atoward%2520a%2520vision-centric%2520alternative.%2520Our%2520method%252C%2520SeeTok%252C%2520renders%2520text%2520as%2520images%250A%2528visual-text%2529%2520and%2520leverages%2520pretrained%2520multimodal%2520LLMs%2520to%2520interpret%2520them%252C%250Areusing%2520strong%2520OCR%2520and%2520text-vision%2520alignment%2520abilities%2520learned%2520from%2520large-scale%250Amultimodal%2520training.%2520Across%2520three%2520different%2520language%2520tasks%252C%2520SeeTok%2520matches%2520or%250Asurpasses%2520subword%2520tokenizers%2520while%2520requiring%25204.43%2520times%2520fewer%2520tokens%2520and%250Areducing%2520FLOPs%2520by%252070.5%2525%252C%2520with%2520additional%2520gains%2520in%2520cross-lingual%2520generalization%252C%250Arobustness%2520to%2520typographic%2520noise%252C%2520and%2520linguistic%2520hierarchy.%2520SeeTok%2520signals%2520a%250Ashift%2520from%2520symbolic%2520tokenization%2520to%2520human-like%2520visual%2520reading%252C%2520and%2520takes%2520a%2520step%250Atoward%2520more%2520natural%2520and%2520cognitively%2520inspired%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20the%20Text%3A%20From%20Tokenization%20to%20Visual%20Reading&entry.906535625=Ling%20Xing%20and%20Alex%20Jinpeng%20Wang%20and%20Rui%20Yan%20and%20Hongyu%20Qu%20and%20Zechao%20Li%20and%20Jinhui%20Tang&entry.1292438233=%20%20People%20see%20text.%20Humans%20read%20by%20recognizing%20words%20as%20visual%20objects%2C%0Aincluding%20their%20shapes%2C%20layouts%2C%20and%20patterns%2C%20before%20connecting%20them%20to%0Ameaning%2C%20which%20enables%20us%20to%20handle%20typos%2C%20distorted%20fonts%2C%20and%20various%20scripts%0Aeffectively.%20Modern%20large%20language%20models%20%28LLMs%29%2C%20however%2C%20rely%20on%20subword%0Atokenization%2C%20fragmenting%20text%20into%20pieces%20from%20a%20fixed%20vocabulary.%20While%0Aeffective%20for%20high-resource%20languages%2C%20this%20approach%20over-segments%20low-resource%0Alanguages%2C%20yielding%20long%2C%20linguistically%20meaningless%20sequences%20and%20inflating%0Acomputation.%20In%20this%20work%2C%20we%20challenge%20this%20entrenched%20paradigm%20and%20move%0Atoward%20a%20vision-centric%20alternative.%20Our%20method%2C%20SeeTok%2C%20renders%20text%20as%20images%0A%28visual-text%29%20and%20leverages%20pretrained%20multimodal%20LLMs%20to%20interpret%20them%2C%0Areusing%20strong%20OCR%20and%20text-vision%20alignment%20abilities%20learned%20from%20large-scale%0Amultimodal%20training.%20Across%20three%20different%20language%20tasks%2C%20SeeTok%20matches%20or%0Asurpasses%20subword%20tokenizers%20while%20requiring%204.43%20times%20fewer%20tokens%20and%0Areducing%20FLOPs%20by%2070.5%25%2C%20with%20additional%20gains%20in%20cross-lingual%20generalization%2C%0Arobustness%20to%20typographic%20noise%2C%20and%20linguistic%20hierarchy.%20SeeTok%20signals%20a%0Ashift%20from%20symbolic%20tokenization%20to%20human-like%20visual%20reading%2C%20and%20takes%20a%20step%0Atoward%20more%20natural%20and%20cognitively%20inspired%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18840v1&entry.124074799=Read"},
{"title": "SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction", "author": "Kaveh Moradkhani and R Jarrett Rushmore and Sylvain Bouix", "abstract": "  Accurate cortical surface reconstruction from magnetic resonance imaging\n(MRI) data is crucial for reliable neuroanatomical analyses. Current methods\nhave to contend with complex cortical geometries, strict topological\nrequirements, and often produce surfaces with overlaps, self-intersections, and\ntopological defects. To overcome these shortcomings, we introduce SimCortex, a\ndeep learning framework that simultaneously reconstructs all brain surfaces\n(left/right white-matter and pial) from T1-weighted(T1w) MRI volumes while\npreserving topological properties. Our method first segments the T1w image into\na nine-class tissue label map. From these segmentations, we generate\nsubject-specific, collision-free initial surface meshes. These surfaces serve\nas precise initializations for subsequent multiscale diffeomorphic\ndeformations. Employing stationary velocity fields (SVFs) integrated via\nscaling-and-squaring, our approach ensures smooth, topology-preserving\ntransformations with significantly reduced surface collisions and\nself-intersections. Evaluations on standard datasets demonstrate that SimCortex\ndramatically reduces surface overlaps and self-intersections, surpassing\ncurrent methods while maintaining state-of-the-art geometric accuracy.\n", "link": "http://arxiv.org/abs/2507.06955v2", "date": "2025-10-21", "relevancy": 2.6594, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimCortex%3A%20Collision-free%20Simultaneous%20Cortical%20Surfaces%20Reconstruction&body=Title%3A%20SimCortex%3A%20Collision-free%20Simultaneous%20Cortical%20Surfaces%20Reconstruction%0AAuthor%3A%20Kaveh%20Moradkhani%20and%20R%20Jarrett%20Rushmore%20and%20Sylvain%20Bouix%0AAbstract%3A%20%20%20Accurate%20cortical%20surface%20reconstruction%20from%20magnetic%20resonance%20imaging%0A%28MRI%29%20data%20is%20crucial%20for%20reliable%20neuroanatomical%20analyses.%20Current%20methods%0Ahave%20to%20contend%20with%20complex%20cortical%20geometries%2C%20strict%20topological%0Arequirements%2C%20and%20often%20produce%20surfaces%20with%20overlaps%2C%20self-intersections%2C%20and%0Atopological%20defects.%20To%20overcome%20these%20shortcomings%2C%20we%20introduce%20SimCortex%2C%20a%0Adeep%20learning%20framework%20that%20simultaneously%20reconstructs%20all%20brain%20surfaces%0A%28left/right%20white-matter%20and%20pial%29%20from%20T1-weighted%28T1w%29%20MRI%20volumes%20while%0Apreserving%20topological%20properties.%20Our%20method%20first%20segments%20the%20T1w%20image%20into%0Aa%20nine-class%20tissue%20label%20map.%20From%20these%20segmentations%2C%20we%20generate%0Asubject-specific%2C%20collision-free%20initial%20surface%20meshes.%20These%20surfaces%20serve%0Aas%20precise%20initializations%20for%20subsequent%20multiscale%20diffeomorphic%0Adeformations.%20Employing%20stationary%20velocity%20fields%20%28SVFs%29%20integrated%20via%0Ascaling-and-squaring%2C%20our%20approach%20ensures%20smooth%2C%20topology-preserving%0Atransformations%20with%20significantly%20reduced%20surface%20collisions%20and%0Aself-intersections.%20Evaluations%20on%20standard%20datasets%20demonstrate%20that%20SimCortex%0Adramatically%20reduces%20surface%20overlaps%20and%20self-intersections%2C%20surpassing%0Acurrent%20methods%20while%20maintaining%20state-of-the-art%20geometric%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimCortex%253A%2520Collision-free%2520Simultaneous%2520Cortical%2520Surfaces%2520Reconstruction%26entry.906535625%3DKaveh%2520Moradkhani%2520and%2520R%2520Jarrett%2520Rushmore%2520and%2520Sylvain%2520Bouix%26entry.1292438233%3D%2520%2520Accurate%2520cortical%2520surface%2520reconstruction%2520from%2520magnetic%2520resonance%2520imaging%250A%2528MRI%2529%2520data%2520is%2520crucial%2520for%2520reliable%2520neuroanatomical%2520analyses.%2520Current%2520methods%250Ahave%2520to%2520contend%2520with%2520complex%2520cortical%2520geometries%252C%2520strict%2520topological%250Arequirements%252C%2520and%2520often%2520produce%2520surfaces%2520with%2520overlaps%252C%2520self-intersections%252C%2520and%250Atopological%2520defects.%2520To%2520overcome%2520these%2520shortcomings%252C%2520we%2520introduce%2520SimCortex%252C%2520a%250Adeep%2520learning%2520framework%2520that%2520simultaneously%2520reconstructs%2520all%2520brain%2520surfaces%250A%2528left/right%2520white-matter%2520and%2520pial%2529%2520from%2520T1-weighted%2528T1w%2529%2520MRI%2520volumes%2520while%250Apreserving%2520topological%2520properties.%2520Our%2520method%2520first%2520segments%2520the%2520T1w%2520image%2520into%250Aa%2520nine-class%2520tissue%2520label%2520map.%2520From%2520these%2520segmentations%252C%2520we%2520generate%250Asubject-specific%252C%2520collision-free%2520initial%2520surface%2520meshes.%2520These%2520surfaces%2520serve%250Aas%2520precise%2520initializations%2520for%2520subsequent%2520multiscale%2520diffeomorphic%250Adeformations.%2520Employing%2520stationary%2520velocity%2520fields%2520%2528SVFs%2529%2520integrated%2520via%250Ascaling-and-squaring%252C%2520our%2520approach%2520ensures%2520smooth%252C%2520topology-preserving%250Atransformations%2520with%2520significantly%2520reduced%2520surface%2520collisions%2520and%250Aself-intersections.%2520Evaluations%2520on%2520standard%2520datasets%2520demonstrate%2520that%2520SimCortex%250Adramatically%2520reduces%2520surface%2520overlaps%2520and%2520self-intersections%252C%2520surpassing%250Acurrent%2520methods%2520while%2520maintaining%2520state-of-the-art%2520geometric%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimCortex%3A%20Collision-free%20Simultaneous%20Cortical%20Surfaces%20Reconstruction&entry.906535625=Kaveh%20Moradkhani%20and%20R%20Jarrett%20Rushmore%20and%20Sylvain%20Bouix&entry.1292438233=%20%20Accurate%20cortical%20surface%20reconstruction%20from%20magnetic%20resonance%20imaging%0A%28MRI%29%20data%20is%20crucial%20for%20reliable%20neuroanatomical%20analyses.%20Current%20methods%0Ahave%20to%20contend%20with%20complex%20cortical%20geometries%2C%20strict%20topological%0Arequirements%2C%20and%20often%20produce%20surfaces%20with%20overlaps%2C%20self-intersections%2C%20and%0Atopological%20defects.%20To%20overcome%20these%20shortcomings%2C%20we%20introduce%20SimCortex%2C%20a%0Adeep%20learning%20framework%20that%20simultaneously%20reconstructs%20all%20brain%20surfaces%0A%28left/right%20white-matter%20and%20pial%29%20from%20T1-weighted%28T1w%29%20MRI%20volumes%20while%0Apreserving%20topological%20properties.%20Our%20method%20first%20segments%20the%20T1w%20image%20into%0Aa%20nine-class%20tissue%20label%20map.%20From%20these%20segmentations%2C%20we%20generate%0Asubject-specific%2C%20collision-free%20initial%20surface%20meshes.%20These%20surfaces%20serve%0Aas%20precise%20initializations%20for%20subsequent%20multiscale%20diffeomorphic%0Adeformations.%20Employing%20stationary%20velocity%20fields%20%28SVFs%29%20integrated%20via%0Ascaling-and-squaring%2C%20our%20approach%20ensures%20smooth%2C%20topology-preserving%0Atransformations%20with%20significantly%20reduced%20surface%20collisions%20and%0Aself-intersections.%20Evaluations%20on%20standard%20datasets%20demonstrate%20that%20SimCortex%0Adramatically%20reduces%20surface%20overlaps%20and%20self-intersections%2C%20surpassing%0Acurrent%20methods%20while%20maintaining%20state-of-the-art%20geometric%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06955v2&entry.124074799=Read"},
{"title": "AstroMMBench: A Benchmark for Evaluating Multimodal Large Language\n  Models Capabilities in Astronomy", "author": "Jinghang Shi and Xiaoyu Tang and Yang Huang and Yuyang Li and Xiao Kong and Yanxia Zhang and Caizhan Yue", "abstract": "  Astronomical image interpretation presents a significant challenge for\napplying multimodal large language models (MLLMs) to specialized scientific\ntasks. Existing benchmarks focus on general multimodal capabilities but fail to\ncapture the complexity of astronomical data. To bridge this gap, we introduce\nAstroMMBench, the first comprehensive benchmark designed to evaluate MLLMs in\nastronomical image understanding. AstroMMBench comprises 621 multiple-choice\nquestions across six astrophysical subfields, curated and reviewed by 15 domain\nexperts for quality and relevance. We conducted an extensive evaluation of 25\ndiverse MLLMs, including 22 open-source and 3 closed-source models, using\nAstroMMBench. The results show that Ovis2-34B achieved the highest overall\naccuracy (70.5%), demonstrating leading capabilities even compared to strong\nclosed-source models. Performance showed variations across the six\nastrophysical subfields, proving particularly challenging in domains like\ncosmology and high-energy astrophysics, while models performed relatively\nbetter in others, such as instrumentation and solar astrophysics. These\nfindings underscore the vital role of domain-specific benchmarks like\nAstroMMBench in critically evaluating MLLM performance and guiding their\ntargeted development for scientific applications. AstroMMBench provides a\nfoundational resource and a dynamic tool to catalyze advancements at the\nintersection of AI and astronomy.\n", "link": "http://arxiv.org/abs/2510.00063v2", "date": "2025-10-21", "relevancy": 2.6014, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AstroMMBench%3A%20A%20Benchmark%20for%20Evaluating%20Multimodal%20Large%20Language%0A%20%20Models%20Capabilities%20in%20Astronomy&body=Title%3A%20AstroMMBench%3A%20A%20Benchmark%20for%20Evaluating%20Multimodal%20Large%20Language%0A%20%20Models%20Capabilities%20in%20Astronomy%0AAuthor%3A%20Jinghang%20Shi%20and%20Xiaoyu%20Tang%20and%20Yang%20Huang%20and%20Yuyang%20Li%20and%20Xiao%20Kong%20and%20Yanxia%20Zhang%20and%20Caizhan%20Yue%0AAbstract%3A%20%20%20Astronomical%20image%20interpretation%20presents%20a%20significant%20challenge%20for%0Aapplying%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20specialized%20scientific%0Atasks.%20Existing%20benchmarks%20focus%20on%20general%20multimodal%20capabilities%20but%20fail%20to%0Acapture%20the%20complexity%20of%20astronomical%20data.%20To%20bridge%20this%20gap%2C%20we%20introduce%0AAstroMMBench%2C%20the%20first%20comprehensive%20benchmark%20designed%20to%20evaluate%20MLLMs%20in%0Aastronomical%20image%20understanding.%20AstroMMBench%20comprises%20621%20multiple-choice%0Aquestions%20across%20six%20astrophysical%20subfields%2C%20curated%20and%20reviewed%20by%2015%20domain%0Aexperts%20for%20quality%20and%20relevance.%20We%20conducted%20an%20extensive%20evaluation%20of%2025%0Adiverse%20MLLMs%2C%20including%2022%20open-source%20and%203%20closed-source%20models%2C%20using%0AAstroMMBench.%20The%20results%20show%20that%20Ovis2-34B%20achieved%20the%20highest%20overall%0Aaccuracy%20%2870.5%25%29%2C%20demonstrating%20leading%20capabilities%20even%20compared%20to%20strong%0Aclosed-source%20models.%20Performance%20showed%20variations%20across%20the%20six%0Aastrophysical%20subfields%2C%20proving%20particularly%20challenging%20in%20domains%20like%0Acosmology%20and%20high-energy%20astrophysics%2C%20while%20models%20performed%20relatively%0Abetter%20in%20others%2C%20such%20as%20instrumentation%20and%20solar%20astrophysics.%20These%0Afindings%20underscore%20the%20vital%20role%20of%20domain-specific%20benchmarks%20like%0AAstroMMBench%20in%20critically%20evaluating%20MLLM%20performance%20and%20guiding%20their%0Atargeted%20development%20for%20scientific%20applications.%20AstroMMBench%20provides%20a%0Afoundational%20resource%20and%20a%20dynamic%20tool%20to%20catalyze%20advancements%20at%20the%0Aintersection%20of%20AI%20and%20astronomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00063v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAstroMMBench%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%2520Capabilities%2520in%2520Astronomy%26entry.906535625%3DJinghang%2520Shi%2520and%2520Xiaoyu%2520Tang%2520and%2520Yang%2520Huang%2520and%2520Yuyang%2520Li%2520and%2520Xiao%2520Kong%2520and%2520Yanxia%2520Zhang%2520and%2520Caizhan%2520Yue%26entry.1292438233%3D%2520%2520Astronomical%2520image%2520interpretation%2520presents%2520a%2520significant%2520challenge%2520for%250Aapplying%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520specialized%2520scientific%250Atasks.%2520Existing%2520benchmarks%2520focus%2520on%2520general%2520multimodal%2520capabilities%2520but%2520fail%2520to%250Acapture%2520the%2520complexity%2520of%2520astronomical%2520data.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%250AAstroMMBench%252C%2520the%2520first%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520MLLMs%2520in%250Aastronomical%2520image%2520understanding.%2520AstroMMBench%2520comprises%2520621%2520multiple-choice%250Aquestions%2520across%2520six%2520astrophysical%2520subfields%252C%2520curated%2520and%2520reviewed%2520by%252015%2520domain%250Aexperts%2520for%2520quality%2520and%2520relevance.%2520We%2520conducted%2520an%2520extensive%2520evaluation%2520of%252025%250Adiverse%2520MLLMs%252C%2520including%252022%2520open-source%2520and%25203%2520closed-source%2520models%252C%2520using%250AAstroMMBench.%2520The%2520results%2520show%2520that%2520Ovis2-34B%2520achieved%2520the%2520highest%2520overall%250Aaccuracy%2520%252870.5%2525%2529%252C%2520demonstrating%2520leading%2520capabilities%2520even%2520compared%2520to%2520strong%250Aclosed-source%2520models.%2520Performance%2520showed%2520variations%2520across%2520the%2520six%250Aastrophysical%2520subfields%252C%2520proving%2520particularly%2520challenging%2520in%2520domains%2520like%250Acosmology%2520and%2520high-energy%2520astrophysics%252C%2520while%2520models%2520performed%2520relatively%250Abetter%2520in%2520others%252C%2520such%2520as%2520instrumentation%2520and%2520solar%2520astrophysics.%2520These%250Afindings%2520underscore%2520the%2520vital%2520role%2520of%2520domain-specific%2520benchmarks%2520like%250AAstroMMBench%2520in%2520critically%2520evaluating%2520MLLM%2520performance%2520and%2520guiding%2520their%250Atargeted%2520development%2520for%2520scientific%2520applications.%2520AstroMMBench%2520provides%2520a%250Afoundational%2520resource%2520and%2520a%2520dynamic%2520tool%2520to%2520catalyze%2520advancements%2520at%2520the%250Aintersection%2520of%2520AI%2520and%2520astronomy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00063v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AstroMMBench%3A%20A%20Benchmark%20for%20Evaluating%20Multimodal%20Large%20Language%0A%20%20Models%20Capabilities%20in%20Astronomy&entry.906535625=Jinghang%20Shi%20and%20Xiaoyu%20Tang%20and%20Yang%20Huang%20and%20Yuyang%20Li%20and%20Xiao%20Kong%20and%20Yanxia%20Zhang%20and%20Caizhan%20Yue&entry.1292438233=%20%20Astronomical%20image%20interpretation%20presents%20a%20significant%20challenge%20for%0Aapplying%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20specialized%20scientific%0Atasks.%20Existing%20benchmarks%20focus%20on%20general%20multimodal%20capabilities%20but%20fail%20to%0Acapture%20the%20complexity%20of%20astronomical%20data.%20To%20bridge%20this%20gap%2C%20we%20introduce%0AAstroMMBench%2C%20the%20first%20comprehensive%20benchmark%20designed%20to%20evaluate%20MLLMs%20in%0Aastronomical%20image%20understanding.%20AstroMMBench%20comprises%20621%20multiple-choice%0Aquestions%20across%20six%20astrophysical%20subfields%2C%20curated%20and%20reviewed%20by%2015%20domain%0Aexperts%20for%20quality%20and%20relevance.%20We%20conducted%20an%20extensive%20evaluation%20of%2025%0Adiverse%20MLLMs%2C%20including%2022%20open-source%20and%203%20closed-source%20models%2C%20using%0AAstroMMBench.%20The%20results%20show%20that%20Ovis2-34B%20achieved%20the%20highest%20overall%0Aaccuracy%20%2870.5%25%29%2C%20demonstrating%20leading%20capabilities%20even%20compared%20to%20strong%0Aclosed-source%20models.%20Performance%20showed%20variations%20across%20the%20six%0Aastrophysical%20subfields%2C%20proving%20particularly%20challenging%20in%20domains%20like%0Acosmology%20and%20high-energy%20astrophysics%2C%20while%20models%20performed%20relatively%0Abetter%20in%20others%2C%20such%20as%20instrumentation%20and%20solar%20astrophysics.%20These%0Afindings%20underscore%20the%20vital%20role%20of%20domain-specific%20benchmarks%20like%0AAstroMMBench%20in%20critically%20evaluating%20MLLM%20performance%20and%20guiding%20their%0Atargeted%20development%20for%20scientific%20applications.%20AstroMMBench%20provides%20a%0Afoundational%20resource%20and%20a%20dynamic%20tool%20to%20catalyze%20advancements%20at%20the%0Aintersection%20of%20AI%20and%20astronomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00063v2&entry.124074799=Read"},
{"title": "SEAL: Semantic-Aware Hierarchical Learning for Generalized Category\n  Discovery", "author": "Zhenqi He and Yuanpei Liu and Kai Han", "abstract": "  This paper investigates the problem of Generalized Category Discovery (GCD).\nGiven a partially labelled dataset, GCD aims to categorize all unlabelled\nimages, regardless of whether they belong to known or unknown classes. Existing\napproaches typically depend on either single-level semantics or manually\ndesigned abstract hierarchies, which limit their generalizability and\nscalability. To address these limitations, we introduce a SEmantic-aware\nhierArchical Learning framework (SEAL), guided by naturally occurring and\neasily accessible hierarchical structures. Within SEAL, we propose a\nHierarchical Semantic-Guided Soft Contrastive Learning approach that exploits\nhierarchical similarity to generate informative soft negatives, addressing the\nlimitations of conventional contrastive losses that treat all negatives\nequally. Furthermore, a Cross-Granularity Consistency (CGC) module is designed\nto align the predictions from different levels of granularity. SEAL\nconsistently achieves state-of-the-art performance on fine-grained benchmarks,\nincluding the SSB benchmark, Oxford-Pet, and the Herbarium19 dataset, and\nfurther demonstrates generalization on coarse-grained datasets. Project page:\nhttps://visual-ai.github.io/seal/\n", "link": "http://arxiv.org/abs/2510.18740v1", "date": "2025-10-21", "relevancy": 2.5761, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5182}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAL%3A%20Semantic-Aware%20Hierarchical%20Learning%20for%20Generalized%20Category%0A%20%20Discovery&body=Title%3A%20SEAL%3A%20Semantic-Aware%20Hierarchical%20Learning%20for%20Generalized%20Category%0A%20%20Discovery%0AAuthor%3A%20Zhenqi%20He%20and%20Yuanpei%20Liu%20and%20Kai%20Han%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20problem%20of%20Generalized%20Category%20Discovery%20%28GCD%29.%0AGiven%20a%20partially%20labelled%20dataset%2C%20GCD%20aims%20to%20categorize%20all%20unlabelled%0Aimages%2C%20regardless%20of%20whether%20they%20belong%20to%20known%20or%20unknown%20classes.%20Existing%0Aapproaches%20typically%20depend%20on%20either%20single-level%20semantics%20or%20manually%0Adesigned%20abstract%20hierarchies%2C%20which%20limit%20their%20generalizability%20and%0Ascalability.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20SEmantic-aware%0AhierArchical%20Learning%20framework%20%28SEAL%29%2C%20guided%20by%20naturally%20occurring%20and%0Aeasily%20accessible%20hierarchical%20structures.%20Within%20SEAL%2C%20we%20propose%20a%0AHierarchical%20Semantic-Guided%20Soft%20Contrastive%20Learning%20approach%20that%20exploits%0Ahierarchical%20similarity%20to%20generate%20informative%20soft%20negatives%2C%20addressing%20the%0Alimitations%20of%20conventional%20contrastive%20losses%20that%20treat%20all%20negatives%0Aequally.%20Furthermore%2C%20a%20Cross-Granularity%20Consistency%20%28CGC%29%20module%20is%20designed%0Ato%20align%20the%20predictions%20from%20different%20levels%20of%20granularity.%20SEAL%0Aconsistently%20achieves%20state-of-the-art%20performance%20on%20fine-grained%20benchmarks%2C%0Aincluding%20the%20SSB%20benchmark%2C%20Oxford-Pet%2C%20and%20the%20Herbarium19%20dataset%2C%20and%0Afurther%20demonstrates%20generalization%20on%20coarse-grained%20datasets.%20Project%20page%3A%0Ahttps%3A//visual-ai.github.io/seal/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAL%253A%2520Semantic-Aware%2520Hierarchical%2520Learning%2520for%2520Generalized%2520Category%250A%2520%2520Discovery%26entry.906535625%3DZhenqi%2520He%2520and%2520Yuanpei%2520Liu%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520problem%2520of%2520Generalized%2520Category%2520Discovery%2520%2528GCD%2529.%250AGiven%2520a%2520partially%2520labelled%2520dataset%252C%2520GCD%2520aims%2520to%2520categorize%2520all%2520unlabelled%250Aimages%252C%2520regardless%2520of%2520whether%2520they%2520belong%2520to%2520known%2520or%2520unknown%2520classes.%2520Existing%250Aapproaches%2520typically%2520depend%2520on%2520either%2520single-level%2520semantics%2520or%2520manually%250Adesigned%2520abstract%2520hierarchies%252C%2520which%2520limit%2520their%2520generalizability%2520and%250Ascalability.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520a%2520SEmantic-aware%250AhierArchical%2520Learning%2520framework%2520%2528SEAL%2529%252C%2520guided%2520by%2520naturally%2520occurring%2520and%250Aeasily%2520accessible%2520hierarchical%2520structures.%2520Within%2520SEAL%252C%2520we%2520propose%2520a%250AHierarchical%2520Semantic-Guided%2520Soft%2520Contrastive%2520Learning%2520approach%2520that%2520exploits%250Ahierarchical%2520similarity%2520to%2520generate%2520informative%2520soft%2520negatives%252C%2520addressing%2520the%250Alimitations%2520of%2520conventional%2520contrastive%2520losses%2520that%2520treat%2520all%2520negatives%250Aequally.%2520Furthermore%252C%2520a%2520Cross-Granularity%2520Consistency%2520%2528CGC%2529%2520module%2520is%2520designed%250Ato%2520align%2520the%2520predictions%2520from%2520different%2520levels%2520of%2520granularity.%2520SEAL%250Aconsistently%2520achieves%2520state-of-the-art%2520performance%2520on%2520fine-grained%2520benchmarks%252C%250Aincluding%2520the%2520SSB%2520benchmark%252C%2520Oxford-Pet%252C%2520and%2520the%2520Herbarium19%2520dataset%252C%2520and%250Afurther%2520demonstrates%2520generalization%2520on%2520coarse-grained%2520datasets.%2520Project%2520page%253A%250Ahttps%253A//visual-ai.github.io/seal/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAL%3A%20Semantic-Aware%20Hierarchical%20Learning%20for%20Generalized%20Category%0A%20%20Discovery&entry.906535625=Zhenqi%20He%20and%20Yuanpei%20Liu%20and%20Kai%20Han&entry.1292438233=%20%20This%20paper%20investigates%20the%20problem%20of%20Generalized%20Category%20Discovery%20%28GCD%29.%0AGiven%20a%20partially%20labelled%20dataset%2C%20GCD%20aims%20to%20categorize%20all%20unlabelled%0Aimages%2C%20regardless%20of%20whether%20they%20belong%20to%20known%20or%20unknown%20classes.%20Existing%0Aapproaches%20typically%20depend%20on%20either%20single-level%20semantics%20or%20manually%0Adesigned%20abstract%20hierarchies%2C%20which%20limit%20their%20generalizability%20and%0Ascalability.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20SEmantic-aware%0AhierArchical%20Learning%20framework%20%28SEAL%29%2C%20guided%20by%20naturally%20occurring%20and%0Aeasily%20accessible%20hierarchical%20structures.%20Within%20SEAL%2C%20we%20propose%20a%0AHierarchical%20Semantic-Guided%20Soft%20Contrastive%20Learning%20approach%20that%20exploits%0Ahierarchical%20similarity%20to%20generate%20informative%20soft%20negatives%2C%20addressing%20the%0Alimitations%20of%20conventional%20contrastive%20losses%20that%20treat%20all%20negatives%0Aequally.%20Furthermore%2C%20a%20Cross-Granularity%20Consistency%20%28CGC%29%20module%20is%20designed%0Ato%20align%20the%20predictions%20from%20different%20levels%20of%20granularity.%20SEAL%0Aconsistently%20achieves%20state-of-the-art%20performance%20on%20fine-grained%20benchmarks%2C%0Aincluding%20the%20SSB%20benchmark%2C%20Oxford-Pet%2C%20and%20the%20Herbarium19%20dataset%2C%20and%0Afurther%20demonstrates%20generalization%20on%20coarse-grained%20datasets.%20Project%20page%3A%0Ahttps%3A//visual-ai.github.io/seal/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18740v1&entry.124074799=Read"},
{"title": "Understanding In-Context Learning on Structured Manifolds: Bridging\n  Attention to Kernel Methods", "author": "Zhaiming Shen and Alexander Hsu and Rongjie Lai and Wenjing Liao", "abstract": "  While in-context learning (ICL) has achieved remarkable success in natural\nlanguage and vision domains, its theoretical understanding-particularly in the\ncontext of structured geometric data-remains unexplored. This paper initiates a\ntheoretical study of ICL for regression of H\\\"older functions on manifolds. We\nestablish a novel connection between the attention mechanism and classical\nkernel methods, demonstrating that transformers effectively perform\nkernel-based prediction at a new query through its interaction with the prompt.\nThis connection is validated by numerical experiments, revealing that the\nlearned query-prompt scores for H\\\"older functions are highly correlated with\nthe Gaussian kernel. Building on this insight, we derive generalization error\nbounds in terms of the prompt length and the number of training tasks. When a\nsufficient number of training tasks are observed, transformers give rise to the\nminimax regression rate of H\\\"older functions on manifolds, which scales\nexponentially with the intrinsic dimension of the manifold, rather than the\nambient space dimension. Our result also characterizes how the generalization\nerror scales with the number of training tasks, shedding light on the\ncomplexity of transformers as in-context kernel algorithm learners. Our\nfindings provide foundational insights into the role of geometry in ICL and\nnovels tools to study ICL of nonlinear models.\n", "link": "http://arxiv.org/abs/2506.10959v2", "date": "2025-10-21", "relevancy": 2.5713, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20In-Context%20Learning%20on%20Structured%20Manifolds%3A%20Bridging%0A%20%20Attention%20to%20Kernel%20Methods&body=Title%3A%20Understanding%20In-Context%20Learning%20on%20Structured%20Manifolds%3A%20Bridging%0A%20%20Attention%20to%20Kernel%20Methods%0AAuthor%3A%20Zhaiming%20Shen%20and%20Alexander%20Hsu%20and%20Rongjie%20Lai%20and%20Wenjing%20Liao%0AAbstract%3A%20%20%20While%20in-context%20learning%20%28ICL%29%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20and%20vision%20domains%2C%20its%20theoretical%20understanding-particularly%20in%20the%0Acontext%20of%20structured%20geometric%20data-remains%20unexplored.%20This%20paper%20initiates%20a%0Atheoretical%20study%20of%20ICL%20for%20regression%20of%20H%5C%22older%20functions%20on%20manifolds.%20We%0Aestablish%20a%20novel%20connection%20between%20the%20attention%20mechanism%20and%20classical%0Akernel%20methods%2C%20demonstrating%20that%20transformers%20effectively%20perform%0Akernel-based%20prediction%20at%20a%20new%20query%20through%20its%20interaction%20with%20the%20prompt.%0AThis%20connection%20is%20validated%20by%20numerical%20experiments%2C%20revealing%20that%20the%0Alearned%20query-prompt%20scores%20for%20H%5C%22older%20functions%20are%20highly%20correlated%20with%0Athe%20Gaussian%20kernel.%20Building%20on%20this%20insight%2C%20we%20derive%20generalization%20error%0Abounds%20in%20terms%20of%20the%20prompt%20length%20and%20the%20number%20of%20training%20tasks.%20When%20a%0Asufficient%20number%20of%20training%20tasks%20are%20observed%2C%20transformers%20give%20rise%20to%20the%0Aminimax%20regression%20rate%20of%20H%5C%22older%20functions%20on%20manifolds%2C%20which%20scales%0Aexponentially%20with%20the%20intrinsic%20dimension%20of%20the%20manifold%2C%20rather%20than%20the%0Aambient%20space%20dimension.%20Our%20result%20also%20characterizes%20how%20the%20generalization%0Aerror%20scales%20with%20the%20number%20of%20training%20tasks%2C%20shedding%20light%20on%20the%0Acomplexity%20of%20transformers%20as%20in-context%20kernel%20algorithm%20learners.%20Our%0Afindings%20provide%20foundational%20insights%20into%20the%20role%20of%20geometry%20in%20ICL%20and%0Anovels%20tools%20to%20study%20ICL%20of%20nonlinear%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520In-Context%2520Learning%2520on%2520Structured%2520Manifolds%253A%2520Bridging%250A%2520%2520Attention%2520to%2520Kernel%2520Methods%26entry.906535625%3DZhaiming%2520Shen%2520and%2520Alexander%2520Hsu%2520and%2520Rongjie%2520Lai%2520and%2520Wenjing%2520Liao%26entry.1292438233%3D%2520%2520While%2520in-context%2520learning%2520%2528ICL%2529%2520has%2520achieved%2520remarkable%2520success%2520in%2520natural%250Alanguage%2520and%2520vision%2520domains%252C%2520its%2520theoretical%2520understanding-particularly%2520in%2520the%250Acontext%2520of%2520structured%2520geometric%2520data-remains%2520unexplored.%2520This%2520paper%2520initiates%2520a%250Atheoretical%2520study%2520of%2520ICL%2520for%2520regression%2520of%2520H%255C%2522older%2520functions%2520on%2520manifolds.%2520We%250Aestablish%2520a%2520novel%2520connection%2520between%2520the%2520attention%2520mechanism%2520and%2520classical%250Akernel%2520methods%252C%2520demonstrating%2520that%2520transformers%2520effectively%2520perform%250Akernel-based%2520prediction%2520at%2520a%2520new%2520query%2520through%2520its%2520interaction%2520with%2520the%2520prompt.%250AThis%2520connection%2520is%2520validated%2520by%2520numerical%2520experiments%252C%2520revealing%2520that%2520the%250Alearned%2520query-prompt%2520scores%2520for%2520H%255C%2522older%2520functions%2520are%2520highly%2520correlated%2520with%250Athe%2520Gaussian%2520kernel.%2520Building%2520on%2520this%2520insight%252C%2520we%2520derive%2520generalization%2520error%250Abounds%2520in%2520terms%2520of%2520the%2520prompt%2520length%2520and%2520the%2520number%2520of%2520training%2520tasks.%2520When%2520a%250Asufficient%2520number%2520of%2520training%2520tasks%2520are%2520observed%252C%2520transformers%2520give%2520rise%2520to%2520the%250Aminimax%2520regression%2520rate%2520of%2520H%255C%2522older%2520functions%2520on%2520manifolds%252C%2520which%2520scales%250Aexponentially%2520with%2520the%2520intrinsic%2520dimension%2520of%2520the%2520manifold%252C%2520rather%2520than%2520the%250Aambient%2520space%2520dimension.%2520Our%2520result%2520also%2520characterizes%2520how%2520the%2520generalization%250Aerror%2520scales%2520with%2520the%2520number%2520of%2520training%2520tasks%252C%2520shedding%2520light%2520on%2520the%250Acomplexity%2520of%2520transformers%2520as%2520in-context%2520kernel%2520algorithm%2520learners.%2520Our%250Afindings%2520provide%2520foundational%2520insights%2520into%2520the%2520role%2520of%2520geometry%2520in%2520ICL%2520and%250Anovels%2520tools%2520to%2520study%2520ICL%2520of%2520nonlinear%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20In-Context%20Learning%20on%20Structured%20Manifolds%3A%20Bridging%0A%20%20Attention%20to%20Kernel%20Methods&entry.906535625=Zhaiming%20Shen%20and%20Alexander%20Hsu%20and%20Rongjie%20Lai%20and%20Wenjing%20Liao&entry.1292438233=%20%20While%20in-context%20learning%20%28ICL%29%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20and%20vision%20domains%2C%20its%20theoretical%20understanding-particularly%20in%20the%0Acontext%20of%20structured%20geometric%20data-remains%20unexplored.%20This%20paper%20initiates%20a%0Atheoretical%20study%20of%20ICL%20for%20regression%20of%20H%5C%22older%20functions%20on%20manifolds.%20We%0Aestablish%20a%20novel%20connection%20between%20the%20attention%20mechanism%20and%20classical%0Akernel%20methods%2C%20demonstrating%20that%20transformers%20effectively%20perform%0Akernel-based%20prediction%20at%20a%20new%20query%20through%20its%20interaction%20with%20the%20prompt.%0AThis%20connection%20is%20validated%20by%20numerical%20experiments%2C%20revealing%20that%20the%0Alearned%20query-prompt%20scores%20for%20H%5C%22older%20functions%20are%20highly%20correlated%20with%0Athe%20Gaussian%20kernel.%20Building%20on%20this%20insight%2C%20we%20derive%20generalization%20error%0Abounds%20in%20terms%20of%20the%20prompt%20length%20and%20the%20number%20of%20training%20tasks.%20When%20a%0Asufficient%20number%20of%20training%20tasks%20are%20observed%2C%20transformers%20give%20rise%20to%20the%0Aminimax%20regression%20rate%20of%20H%5C%22older%20functions%20on%20manifolds%2C%20which%20scales%0Aexponentially%20with%20the%20intrinsic%20dimension%20of%20the%20manifold%2C%20rather%20than%20the%0Aambient%20space%20dimension.%20Our%20result%20also%20characterizes%20how%20the%20generalization%0Aerror%20scales%20with%20the%20number%20of%20training%20tasks%2C%20shedding%20light%20on%20the%0Acomplexity%20of%20transformers%20as%20in-context%20kernel%20algorithm%20learners.%20Our%0Afindings%20provide%20foundational%20insights%20into%20the%20role%20of%20geometry%20in%20ICL%20and%0Anovels%20tools%20to%20study%20ICL%20of%20nonlinear%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10959v2&entry.124074799=Read"},
{"title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs", "author": "Yi Zhang and Bolin Ni and Xin-Sheng Chen and Heng-Rui Zhang and Yongming Rao and Houwen Peng and Qinglin Lu and Han Hu and Meng-Hao Guo and Shi-Min Hu", "abstract": "  Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.\n", "link": "http://arxiv.org/abs/2510.13795v2", "date": "2025-10-21", "relevancy": 2.5543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bee%3A%20A%20High-Quality%20Corpus%20and%20Full-Stack%20Suite%20to%20Unlock%20Advanced%20Fully%0A%20%20Open%20MLLMs&body=Title%3A%20Bee%3A%20A%20High-Quality%20Corpus%20and%20Full-Stack%20Suite%20to%20Unlock%20Advanced%20Fully%0A%20%20Open%20MLLMs%0AAuthor%3A%20Yi%20Zhang%20and%20Bolin%20Ni%20and%20Xin-Sheng%20Chen%20and%20Heng-Rui%20Zhang%20and%20Yongming%20Rao%20and%20Houwen%20Peng%20and%20Qinglin%20Lu%20and%20Han%20Hu%20and%20Meng-Hao%20Guo%20and%20Shi-Min%20Hu%0AAbstract%3A%20%20%20Fully%20open%20multimodal%20large%20language%20models%20%28MLLMs%29%20currently%20lag%20behind%0Aproprietary%20counterparts%2C%20primarily%20due%20to%20a%20significant%20gap%20in%20data%20quality%0Afor%20supervised%20fine-tuning%20%28SFT%29.%20Existing%20open-source%20datasets%20are%20often%0Aplagued%20by%20widespread%20noise%20and%20a%20critical%20deficit%20in%20complex%20reasoning%20data%2C%0Asuch%20as%20Chain-of-Thought%20%28CoT%29%2C%20which%20hinders%20the%20development%20of%20advanced%20model%0Acapabilities.%20Addressing%20these%20challenges%2C%20our%20work%20makes%20three%20primary%0Acontributions.%20First%2C%20we%20introduce%20Honey-Data-15M%2C%20a%20new%20SFT%20dataset%20comprising%0Aapproximately%2015%20million%20QA%20pairs%2C%20processed%20through%20multiple%20cleaning%0Atechniques%20and%20enhanced%20with%20a%20novel%20dual-level%20%28short%20and%20long%29%20CoT%20enrichment%0Astrategy.%20Second%2C%20we%20introduce%20HoneyPipe%2C%20the%20data%20curation%20pipeline%2C%20and%20its%0Aunderlying%20framework%20DataStudio%2C%20providing%20the%20community%20with%20a%20transparent%20and%0Aadaptable%20methodology%20for%20data%20curation%20that%20moves%20beyond%20static%20dataset%0Areleases.%20Finally%2C%20to%20validate%20our%20dataset%20and%20pipeline%2C%20we%20train%20Bee-8B%2C%20an%208B%0Amodel%20on%20Honey-Data-15M.%20Experiments%20show%20that%20Bee-8B%20establishes%20a%20new%0Astate-of-the-art%20%28SOTA%29%20for%20fully%20open%20MLLMs%2C%20achieving%20performance%20that%20is%0Acompetitive%20with%2C%20and%20in%20some%20cases%20surpasses%2C%20recent%20semi-open%20models%20such%20as%0AInternVL3.5-8B.%20Our%20work%20delivers%20to%20the%20community%20a%20suite%20of%20foundational%0Aresources%2C%20including%3A%20the%20Honey-Data-15M%20corpus%3B%20the%20full-stack%20suite%0Acomprising%20HoneyPipe%20and%20DataStudio%3B%20training%20recipes%3B%20an%20evaluation%20harness%3B%0Aand%20the%20model%20weights.%20This%20effort%20demonstrates%20that%20a%20principled%20focus%20on%20data%0Aquality%20is%20a%20key%20pathway%20to%20developing%20fully%20open%20MLLMs%20that%20are%20highly%0Acompetitive%20with%20their%20semi-open%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBee%253A%2520A%2520High-Quality%2520Corpus%2520and%2520Full-Stack%2520Suite%2520to%2520Unlock%2520Advanced%2520Fully%250A%2520%2520Open%2520MLLMs%26entry.906535625%3DYi%2520Zhang%2520and%2520Bolin%2520Ni%2520and%2520Xin-Sheng%2520Chen%2520and%2520Heng-Rui%2520Zhang%2520and%2520Yongming%2520Rao%2520and%2520Houwen%2520Peng%2520and%2520Qinglin%2520Lu%2520and%2520Han%2520Hu%2520and%2520Meng-Hao%2520Guo%2520and%2520Shi-Min%2520Hu%26entry.1292438233%3D%2520%2520Fully%2520open%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520currently%2520lag%2520behind%250Aproprietary%2520counterparts%252C%2520primarily%2520due%2520to%2520a%2520significant%2520gap%2520in%2520data%2520quality%250Afor%2520supervised%2520fine-tuning%2520%2528SFT%2529.%2520Existing%2520open-source%2520datasets%2520are%2520often%250Aplagued%2520by%2520widespread%2520noise%2520and%2520a%2520critical%2520deficit%2520in%2520complex%2520reasoning%2520data%252C%250Asuch%2520as%2520Chain-of-Thought%2520%2528CoT%2529%252C%2520which%2520hinders%2520the%2520development%2520of%2520advanced%2520model%250Acapabilities.%2520Addressing%2520these%2520challenges%252C%2520our%2520work%2520makes%2520three%2520primary%250Acontributions.%2520First%252C%2520we%2520introduce%2520Honey-Data-15M%252C%2520a%2520new%2520SFT%2520dataset%2520comprising%250Aapproximately%252015%2520million%2520QA%2520pairs%252C%2520processed%2520through%2520multiple%2520cleaning%250Atechniques%2520and%2520enhanced%2520with%2520a%2520novel%2520dual-level%2520%2528short%2520and%2520long%2529%2520CoT%2520enrichment%250Astrategy.%2520Second%252C%2520we%2520introduce%2520HoneyPipe%252C%2520the%2520data%2520curation%2520pipeline%252C%2520and%2520its%250Aunderlying%2520framework%2520DataStudio%252C%2520providing%2520the%2520community%2520with%2520a%2520transparent%2520and%250Aadaptable%2520methodology%2520for%2520data%2520curation%2520that%2520moves%2520beyond%2520static%2520dataset%250Areleases.%2520Finally%252C%2520to%2520validate%2520our%2520dataset%2520and%2520pipeline%252C%2520we%2520train%2520Bee-8B%252C%2520an%25208B%250Amodel%2520on%2520Honey-Data-15M.%2520Experiments%2520show%2520that%2520Bee-8B%2520establishes%2520a%2520new%250Astate-of-the-art%2520%2528SOTA%2529%2520for%2520fully%2520open%2520MLLMs%252C%2520achieving%2520performance%2520that%2520is%250Acompetitive%2520with%252C%2520and%2520in%2520some%2520cases%2520surpasses%252C%2520recent%2520semi-open%2520models%2520such%2520as%250AInternVL3.5-8B.%2520Our%2520work%2520delivers%2520to%2520the%2520community%2520a%2520suite%2520of%2520foundational%250Aresources%252C%2520including%253A%2520the%2520Honey-Data-15M%2520corpus%253B%2520the%2520full-stack%2520suite%250Acomprising%2520HoneyPipe%2520and%2520DataStudio%253B%2520training%2520recipes%253B%2520an%2520evaluation%2520harness%253B%250Aand%2520the%2520model%2520weights.%2520This%2520effort%2520demonstrates%2520that%2520a%2520principled%2520focus%2520on%2520data%250Aquality%2520is%2520a%2520key%2520pathway%2520to%2520developing%2520fully%2520open%2520MLLMs%2520that%2520are%2520highly%250Acompetitive%2520with%2520their%2520semi-open%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bee%3A%20A%20High-Quality%20Corpus%20and%20Full-Stack%20Suite%20to%20Unlock%20Advanced%20Fully%0A%20%20Open%20MLLMs&entry.906535625=Yi%20Zhang%20and%20Bolin%20Ni%20and%20Xin-Sheng%20Chen%20and%20Heng-Rui%20Zhang%20and%20Yongming%20Rao%20and%20Houwen%20Peng%20and%20Qinglin%20Lu%20and%20Han%20Hu%20and%20Meng-Hao%20Guo%20and%20Shi-Min%20Hu&entry.1292438233=%20%20Fully%20open%20multimodal%20large%20language%20models%20%28MLLMs%29%20currently%20lag%20behind%0Aproprietary%20counterparts%2C%20primarily%20due%20to%20a%20significant%20gap%20in%20data%20quality%0Afor%20supervised%20fine-tuning%20%28SFT%29.%20Existing%20open-source%20datasets%20are%20often%0Aplagued%20by%20widespread%20noise%20and%20a%20critical%20deficit%20in%20complex%20reasoning%20data%2C%0Asuch%20as%20Chain-of-Thought%20%28CoT%29%2C%20which%20hinders%20the%20development%20of%20advanced%20model%0Acapabilities.%20Addressing%20these%20challenges%2C%20our%20work%20makes%20three%20primary%0Acontributions.%20First%2C%20we%20introduce%20Honey-Data-15M%2C%20a%20new%20SFT%20dataset%20comprising%0Aapproximately%2015%20million%20QA%20pairs%2C%20processed%20through%20multiple%20cleaning%0Atechniques%20and%20enhanced%20with%20a%20novel%20dual-level%20%28short%20and%20long%29%20CoT%20enrichment%0Astrategy.%20Second%2C%20we%20introduce%20HoneyPipe%2C%20the%20data%20curation%20pipeline%2C%20and%20its%0Aunderlying%20framework%20DataStudio%2C%20providing%20the%20community%20with%20a%20transparent%20and%0Aadaptable%20methodology%20for%20data%20curation%20that%20moves%20beyond%20static%20dataset%0Areleases.%20Finally%2C%20to%20validate%20our%20dataset%20and%20pipeline%2C%20we%20train%20Bee-8B%2C%20an%208B%0Amodel%20on%20Honey-Data-15M.%20Experiments%20show%20that%20Bee-8B%20establishes%20a%20new%0Astate-of-the-art%20%28SOTA%29%20for%20fully%20open%20MLLMs%2C%20achieving%20performance%20that%20is%0Acompetitive%20with%2C%20and%20in%20some%20cases%20surpasses%2C%20recent%20semi-open%20models%20such%20as%0AInternVL3.5-8B.%20Our%20work%20delivers%20to%20the%20community%20a%20suite%20of%20foundational%0Aresources%2C%20including%3A%20the%20Honey-Data-15M%20corpus%3B%20the%20full-stack%20suite%0Acomprising%20HoneyPipe%20and%20DataStudio%3B%20training%20recipes%3B%20an%20evaluation%20harness%3B%0Aand%20the%20model%20weights.%20This%20effort%20demonstrates%20that%20a%20principled%20focus%20on%20data%0Aquality%20is%20a%20key%20pathway%20to%20developing%20fully%20open%20MLLMs%20that%20are%20highly%0Acompetitive%20with%20their%20semi-open%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13795v2&entry.124074799=Read"},
{"title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual\n  Navigation", "author": "Yiyuan Pan and Yunzhe Xu and Zhe Liu and Hesheng Wang", "abstract": "  Visual navigation is a fundamental problem in embodied AI, yet practical\ndeployments demand long-horizon planning capabilities to address\nmulti-objective tasks. A major bottleneck is data scarcity: policies learned\nfrom limited data often overfit and fail to generalize OOD. Existing neural\nnetwork-based agents typically increase architectural complexity that\nparadoxically become counterproductive in the small-sample regime. This paper\nintroduce NeuRO, a integrated learning-to-optimize framework that tightly\ncouples perception networks with downstream task-level robust optimization.\nSpecifically, NeuRO addresses core difficulties in this integration: (i) it\ntransforms noisy visual predictions under data scarcity into convex uncertainty\nsets using Partially Input Convex Neural Networks (PICNNs) with conformal\ncalibration, which directly parameterize the optimization constraints; and (ii)\nit reformulates planning under partial observability as a robust optimization\nproblem, enabling uncertainty-aware policies that transfer across environments.\nExtensive experiments on both unordered and sequential multi-object navigation\ntasks demonstrate that NeuRO establishes SoTA performance, particularly in\ngeneralization to unseen environments. Our work thus presents a significant\nadvancement for developing robust, generalizable autonomous agents.\n", "link": "http://arxiv.org/abs/2510.00441v3", "date": "2025-10-21", "relevancy": 2.4886, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.639}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6145}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20through%20Uncertainty%3A%20Robust%20Task-Oriented%20Optimization%20in%20Visual%0A%20%20Navigation&body=Title%3A%20Seeing%20through%20Uncertainty%3A%20Robust%20Task-Oriented%20Optimization%20in%20Visual%0A%20%20Navigation%0AAuthor%3A%20Yiyuan%20Pan%20and%20Yunzhe%20Xu%20and%20Zhe%20Liu%20and%20Hesheng%20Wang%0AAbstract%3A%20%20%20Visual%20navigation%20is%20a%20fundamental%20problem%20in%20embodied%20AI%2C%20yet%20practical%0Adeployments%20demand%20long-horizon%20planning%20capabilities%20to%20address%0Amulti-objective%20tasks.%20A%20major%20bottleneck%20is%20data%20scarcity%3A%20policies%20learned%0Afrom%20limited%20data%20often%20overfit%20and%20fail%20to%20generalize%20OOD.%20Existing%20neural%0Anetwork-based%20agents%20typically%20increase%20architectural%20complexity%20that%0Aparadoxically%20become%20counterproductive%20in%20the%20small-sample%20regime.%20This%20paper%0Aintroduce%20NeuRO%2C%20a%20integrated%20learning-to-optimize%20framework%20that%20tightly%0Acouples%20perception%20networks%20with%20downstream%20task-level%20robust%20optimization.%0ASpecifically%2C%20NeuRO%20addresses%20core%20difficulties%20in%20this%20integration%3A%20%28i%29%20it%0Atransforms%20noisy%20visual%20predictions%20under%20data%20scarcity%20into%20convex%20uncertainty%0Asets%20using%20Partially%20Input%20Convex%20Neural%20Networks%20%28PICNNs%29%20with%20conformal%0Acalibration%2C%20which%20directly%20parameterize%20the%20optimization%20constraints%3B%20and%20%28ii%29%0Ait%20reformulates%20planning%20under%20partial%20observability%20as%20a%20robust%20optimization%0Aproblem%2C%20enabling%20uncertainty-aware%20policies%20that%20transfer%20across%20environments.%0AExtensive%20experiments%20on%20both%20unordered%20and%20sequential%20multi-object%20navigation%0Atasks%20demonstrate%20that%20NeuRO%20establishes%20SoTA%20performance%2C%20particularly%20in%0Ageneralization%20to%20unseen%20environments.%20Our%20work%20thus%20presents%20a%20significant%0Aadvancement%20for%20developing%20robust%2C%20generalizable%20autonomous%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520through%2520Uncertainty%253A%2520Robust%2520Task-Oriented%2520Optimization%2520in%2520Visual%250A%2520%2520Navigation%26entry.906535625%3DYiyuan%2520Pan%2520and%2520Yunzhe%2520Xu%2520and%2520Zhe%2520Liu%2520and%2520Hesheng%2520Wang%26entry.1292438233%3D%2520%2520Visual%2520navigation%2520is%2520a%2520fundamental%2520problem%2520in%2520embodied%2520AI%252C%2520yet%2520practical%250Adeployments%2520demand%2520long-horizon%2520planning%2520capabilities%2520to%2520address%250Amulti-objective%2520tasks.%2520A%2520major%2520bottleneck%2520is%2520data%2520scarcity%253A%2520policies%2520learned%250Afrom%2520limited%2520data%2520often%2520overfit%2520and%2520fail%2520to%2520generalize%2520OOD.%2520Existing%2520neural%250Anetwork-based%2520agents%2520typically%2520increase%2520architectural%2520complexity%2520that%250Aparadoxically%2520become%2520counterproductive%2520in%2520the%2520small-sample%2520regime.%2520This%2520paper%250Aintroduce%2520NeuRO%252C%2520a%2520integrated%2520learning-to-optimize%2520framework%2520that%2520tightly%250Acouples%2520perception%2520networks%2520with%2520downstream%2520task-level%2520robust%2520optimization.%250ASpecifically%252C%2520NeuRO%2520addresses%2520core%2520difficulties%2520in%2520this%2520integration%253A%2520%2528i%2529%2520it%250Atransforms%2520noisy%2520visual%2520predictions%2520under%2520data%2520scarcity%2520into%2520convex%2520uncertainty%250Asets%2520using%2520Partially%2520Input%2520Convex%2520Neural%2520Networks%2520%2528PICNNs%2529%2520with%2520conformal%250Acalibration%252C%2520which%2520directly%2520parameterize%2520the%2520optimization%2520constraints%253B%2520and%2520%2528ii%2529%250Ait%2520reformulates%2520planning%2520under%2520partial%2520observability%2520as%2520a%2520robust%2520optimization%250Aproblem%252C%2520enabling%2520uncertainty-aware%2520policies%2520that%2520transfer%2520across%2520environments.%250AExtensive%2520experiments%2520on%2520both%2520unordered%2520and%2520sequential%2520multi-object%2520navigation%250Atasks%2520demonstrate%2520that%2520NeuRO%2520establishes%2520SoTA%2520performance%252C%2520particularly%2520in%250Ageneralization%2520to%2520unseen%2520environments.%2520Our%2520work%2520thus%2520presents%2520a%2520significant%250Aadvancement%2520for%2520developing%2520robust%252C%2520generalizable%2520autonomous%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20through%20Uncertainty%3A%20Robust%20Task-Oriented%20Optimization%20in%20Visual%0A%20%20Navigation&entry.906535625=Yiyuan%20Pan%20and%20Yunzhe%20Xu%20and%20Zhe%20Liu%20and%20Hesheng%20Wang&entry.1292438233=%20%20Visual%20navigation%20is%20a%20fundamental%20problem%20in%20embodied%20AI%2C%20yet%20practical%0Adeployments%20demand%20long-horizon%20planning%20capabilities%20to%20address%0Amulti-objective%20tasks.%20A%20major%20bottleneck%20is%20data%20scarcity%3A%20policies%20learned%0Afrom%20limited%20data%20often%20overfit%20and%20fail%20to%20generalize%20OOD.%20Existing%20neural%0Anetwork-based%20agents%20typically%20increase%20architectural%20complexity%20that%0Aparadoxically%20become%20counterproductive%20in%20the%20small-sample%20regime.%20This%20paper%0Aintroduce%20NeuRO%2C%20a%20integrated%20learning-to-optimize%20framework%20that%20tightly%0Acouples%20perception%20networks%20with%20downstream%20task-level%20robust%20optimization.%0ASpecifically%2C%20NeuRO%20addresses%20core%20difficulties%20in%20this%20integration%3A%20%28i%29%20it%0Atransforms%20noisy%20visual%20predictions%20under%20data%20scarcity%20into%20convex%20uncertainty%0Asets%20using%20Partially%20Input%20Convex%20Neural%20Networks%20%28PICNNs%29%20with%20conformal%0Acalibration%2C%20which%20directly%20parameterize%20the%20optimization%20constraints%3B%20and%20%28ii%29%0Ait%20reformulates%20planning%20under%20partial%20observability%20as%20a%20robust%20optimization%0Aproblem%2C%20enabling%20uncertainty-aware%20policies%20that%20transfer%20across%20environments.%0AExtensive%20experiments%20on%20both%20unordered%20and%20sequential%20multi-object%20navigation%0Atasks%20demonstrate%20that%20NeuRO%20establishes%20SoTA%20performance%2C%20particularly%20in%0Ageneralization%20to%20unseen%20environments.%20Our%20work%20thus%20presents%20a%20significant%0Aadvancement%20for%20developing%20robust%2C%20generalizable%20autonomous%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00441v3&entry.124074799=Read"},
{"title": "SO(3)-invariant PCA with application to molecular data", "author": "Michael Fraiman and Paulina Hoyos and Tamir Bendory and Joe Kileel and Oscar Mickelin and Nir Sharon and Amit Singer", "abstract": "  Principal component analysis (PCA) is a fundamental technique for\ndimensionality reduction and denoising; however, its application to\nthree-dimensional data with arbitrary orientations -- common in structural\nbiology -- presents significant challenges. A naive approach requires\naugmenting the dataset with many rotated copies of each sample, incurring\nprohibitive computational costs. In this paper, we extend PCA to 3D volumetric\ndatasets with unknown orientations by developing an efficient and principled\nframework for SO(3)-invariant PCA that implicitly accounts for all rotations\nwithout explicit data augmentation. By exploiting underlying algebraic\nstructure, we demonstrate that the computation involves only the square root of\nthe total number of covariance entries, resulting in a substantial reduction in\ncomplexity. We validate the method on real-world molecular datasets,\ndemonstrating its effectiveness and opening up new possibilities for\nlarge-scale, high-dimensional reconstruction problems.\n", "link": "http://arxiv.org/abs/2510.18827v1", "date": "2025-10-21", "relevancy": 2.4826, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5115}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5115}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SO%283%29-invariant%20PCA%20with%20application%20to%20molecular%20data&body=Title%3A%20SO%283%29-invariant%20PCA%20with%20application%20to%20molecular%20data%0AAuthor%3A%20Michael%20Fraiman%20and%20Paulina%20Hoyos%20and%20Tamir%20Bendory%20and%20Joe%20Kileel%20and%20Oscar%20Mickelin%20and%20Nir%20Sharon%20and%20Amit%20Singer%0AAbstract%3A%20%20%20Principal%20component%20analysis%20%28PCA%29%20is%20a%20fundamental%20technique%20for%0Adimensionality%20reduction%20and%20denoising%3B%20however%2C%20its%20application%20to%0Athree-dimensional%20data%20with%20arbitrary%20orientations%20--%20common%20in%20structural%0Abiology%20--%20presents%20significant%20challenges.%20A%20naive%20approach%20requires%0Aaugmenting%20the%20dataset%20with%20many%20rotated%20copies%20of%20each%20sample%2C%20incurring%0Aprohibitive%20computational%20costs.%20In%20this%20paper%2C%20we%20extend%20PCA%20to%203D%20volumetric%0Adatasets%20with%20unknown%20orientations%20by%20developing%20an%20efficient%20and%20principled%0Aframework%20for%20SO%283%29-invariant%20PCA%20that%20implicitly%20accounts%20for%20all%20rotations%0Awithout%20explicit%20data%20augmentation.%20By%20exploiting%20underlying%20algebraic%0Astructure%2C%20we%20demonstrate%20that%20the%20computation%20involves%20only%20the%20square%20root%20of%0Athe%20total%20number%20of%20covariance%20entries%2C%20resulting%20in%20a%20substantial%20reduction%20in%0Acomplexity.%20We%20validate%20the%20method%20on%20real-world%20molecular%20datasets%2C%0Ademonstrating%20its%20effectiveness%20and%20opening%20up%20new%20possibilities%20for%0Alarge-scale%2C%20high-dimensional%20reconstruction%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSO%25283%2529-invariant%2520PCA%2520with%2520application%2520to%2520molecular%2520data%26entry.906535625%3DMichael%2520Fraiman%2520and%2520Paulina%2520Hoyos%2520and%2520Tamir%2520Bendory%2520and%2520Joe%2520Kileel%2520and%2520Oscar%2520Mickelin%2520and%2520Nir%2520Sharon%2520and%2520Amit%2520Singer%26entry.1292438233%3D%2520%2520Principal%2520component%2520analysis%2520%2528PCA%2529%2520is%2520a%2520fundamental%2520technique%2520for%250Adimensionality%2520reduction%2520and%2520denoising%253B%2520however%252C%2520its%2520application%2520to%250Athree-dimensional%2520data%2520with%2520arbitrary%2520orientations%2520--%2520common%2520in%2520structural%250Abiology%2520--%2520presents%2520significant%2520challenges.%2520A%2520naive%2520approach%2520requires%250Aaugmenting%2520the%2520dataset%2520with%2520many%2520rotated%2520copies%2520of%2520each%2520sample%252C%2520incurring%250Aprohibitive%2520computational%2520costs.%2520In%2520this%2520paper%252C%2520we%2520extend%2520PCA%2520to%25203D%2520volumetric%250Adatasets%2520with%2520unknown%2520orientations%2520by%2520developing%2520an%2520efficient%2520and%2520principled%250Aframework%2520for%2520SO%25283%2529-invariant%2520PCA%2520that%2520implicitly%2520accounts%2520for%2520all%2520rotations%250Awithout%2520explicit%2520data%2520augmentation.%2520By%2520exploiting%2520underlying%2520algebraic%250Astructure%252C%2520we%2520demonstrate%2520that%2520the%2520computation%2520involves%2520only%2520the%2520square%2520root%2520of%250Athe%2520total%2520number%2520of%2520covariance%2520entries%252C%2520resulting%2520in%2520a%2520substantial%2520reduction%2520in%250Acomplexity.%2520We%2520validate%2520the%2520method%2520on%2520real-world%2520molecular%2520datasets%252C%250Ademonstrating%2520its%2520effectiveness%2520and%2520opening%2520up%2520new%2520possibilities%2520for%250Alarge-scale%252C%2520high-dimensional%2520reconstruction%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SO%283%29-invariant%20PCA%20with%20application%20to%20molecular%20data&entry.906535625=Michael%20Fraiman%20and%20Paulina%20Hoyos%20and%20Tamir%20Bendory%20and%20Joe%20Kileel%20and%20Oscar%20Mickelin%20and%20Nir%20Sharon%20and%20Amit%20Singer&entry.1292438233=%20%20Principal%20component%20analysis%20%28PCA%29%20is%20a%20fundamental%20technique%20for%0Adimensionality%20reduction%20and%20denoising%3B%20however%2C%20its%20application%20to%0Athree-dimensional%20data%20with%20arbitrary%20orientations%20--%20common%20in%20structural%0Abiology%20--%20presents%20significant%20challenges.%20A%20naive%20approach%20requires%0Aaugmenting%20the%20dataset%20with%20many%20rotated%20copies%20of%20each%20sample%2C%20incurring%0Aprohibitive%20computational%20costs.%20In%20this%20paper%2C%20we%20extend%20PCA%20to%203D%20volumetric%0Adatasets%20with%20unknown%20orientations%20by%20developing%20an%20efficient%20and%20principled%0Aframework%20for%20SO%283%29-invariant%20PCA%20that%20implicitly%20accounts%20for%20all%20rotations%0Awithout%20explicit%20data%20augmentation.%20By%20exploiting%20underlying%20algebraic%0Astructure%2C%20we%20demonstrate%20that%20the%20computation%20involves%20only%20the%20square%20root%20of%0Athe%20total%20number%20of%20covariance%20entries%2C%20resulting%20in%20a%20substantial%20reduction%20in%0Acomplexity.%20We%20validate%20the%20method%20on%20real-world%20molecular%20datasets%2C%0Ademonstrating%20its%20effectiveness%20and%20opening%20up%20new%20possibilities%20for%0Alarge-scale%2C%20high-dimensional%20reconstruction%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18827v1&entry.124074799=Read"},
{"title": "VideoVerse: How Far is Your T2V Generator from a World Model?", "author": "Zeqing Wang and Xinyu Wei and Bairui Li and Zhen Guo and Jinrui Zhang and Hongyang Wei and Keze Wang and Lei Zhang", "abstract": "  The recent rapid advancement of Text-to-Video (T2V) generation technologies,\nwhich are critical to build ``world models'', makes the existing benchmarks\nincreasingly insufficient to evaluate state-of-the-art T2V models. First,\ncurrent evaluation dimensions, such as per-frame aesthetic quality and temporal\nconsistency, are no longer able to differentiate state-of-the-art T2V models.\nSecond, event-level temporal causality, which not only distinguishes video from\nother modalities but also constitutes a crucial component of world models, is\nseverely underexplored in existing benchmarks. Third, existing benchmarks lack\na systematic assessment of world knowledge, which are essential capabilities\nfor building world models. To address these issues, we introduce VideoVerse, a\ncomprehensive benchmark that focuses on evaluating whether a T2V model could\nunderstand complex temporal causality and world knowledge in the real world. We\ncollect representative videos across diverse domains (e.g., natural landscapes,\nsports, indoor scenes, science fiction, chemical and physical experiments) and\nextract their event-level descriptions with inherent temporal causality, which\nare then rewritten into text-to-video prompts by independent annotators. For\neach prompt, we design a suite of binary evaluation questions from the\nperspective of dynamic and static properties, with a total of ten carefully\ndefined evaluation dimensions. In total, our VideoVerse comprises 300 carefully\ncurated prompts, involving 815 events and 793 binary evaluation questions.\nConsequently, a human preference aligned QA-based evaluation pipeline is\ndeveloped by using modern vision-language models. Finally, we perform a\nsystematic evaluation of state-of-the-art open-source and closed-source T2V\nmodels on VideoVerse, providing in-depth analysis on how far the current T2V\ngenerators are from world models.\n", "link": "http://arxiv.org/abs/2510.08398v2", "date": "2025-10-21", "relevancy": 2.4814, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6362}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoVerse%3A%20How%20Far%20is%20Your%20T2V%20Generator%20from%20a%20World%20Model%3F&body=Title%3A%20VideoVerse%3A%20How%20Far%20is%20Your%20T2V%20Generator%20from%20a%20World%20Model%3F%0AAuthor%3A%20Zeqing%20Wang%20and%20Xinyu%20Wei%20and%20Bairui%20Li%20and%20Zhen%20Guo%20and%20Jinrui%20Zhang%20and%20Hongyang%20Wei%20and%20Keze%20Wang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20The%20recent%20rapid%20advancement%20of%20Text-to-Video%20%28T2V%29%20generation%20technologies%2C%0Awhich%20are%20critical%20to%20build%20%60%60world%20models%27%27%2C%20makes%20the%20existing%20benchmarks%0Aincreasingly%20insufficient%20to%20evaluate%20state-of-the-art%20T2V%20models.%20First%2C%0Acurrent%20evaluation%20dimensions%2C%20such%20as%20per-frame%20aesthetic%20quality%20and%20temporal%0Aconsistency%2C%20are%20no%20longer%20able%20to%20differentiate%20state-of-the-art%20T2V%20models.%0ASecond%2C%20event-level%20temporal%20causality%2C%20which%20not%20only%20distinguishes%20video%20from%0Aother%20modalities%20but%20also%20constitutes%20a%20crucial%20component%20of%20world%20models%2C%20is%0Aseverely%20underexplored%20in%20existing%20benchmarks.%20Third%2C%20existing%20benchmarks%20lack%0Aa%20systematic%20assessment%20of%20world%20knowledge%2C%20which%20are%20essential%20capabilities%0Afor%20building%20world%20models.%20To%20address%20these%20issues%2C%20we%20introduce%20VideoVerse%2C%20a%0Acomprehensive%20benchmark%20that%20focuses%20on%20evaluating%20whether%20a%20T2V%20model%20could%0Aunderstand%20complex%20temporal%20causality%20and%20world%20knowledge%20in%20the%20real%20world.%20We%0Acollect%20representative%20videos%20across%20diverse%20domains%20%28e.g.%2C%20natural%20landscapes%2C%0Asports%2C%20indoor%20scenes%2C%20science%20fiction%2C%20chemical%20and%20physical%20experiments%29%20and%0Aextract%20their%20event-level%20descriptions%20with%20inherent%20temporal%20causality%2C%20which%0Aare%20then%20rewritten%20into%20text-to-video%20prompts%20by%20independent%20annotators.%20For%0Aeach%20prompt%2C%20we%20design%20a%20suite%20of%20binary%20evaluation%20questions%20from%20the%0Aperspective%20of%20dynamic%20and%20static%20properties%2C%20with%20a%20total%20of%20ten%20carefully%0Adefined%20evaluation%20dimensions.%20In%20total%2C%20our%20VideoVerse%20comprises%20300%20carefully%0Acurated%20prompts%2C%20involving%20815%20events%20and%20793%20binary%20evaluation%20questions.%0AConsequently%2C%20a%20human%20preference%20aligned%20QA-based%20evaluation%20pipeline%20is%0Adeveloped%20by%20using%20modern%20vision-language%20models.%20Finally%2C%20we%20perform%20a%0Asystematic%20evaluation%20of%20state-of-the-art%20open-source%20and%20closed-source%20T2V%0Amodels%20on%20VideoVerse%2C%20providing%20in-depth%20analysis%20on%20how%20far%20the%20current%20T2V%0Agenerators%20are%20from%20world%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoVerse%253A%2520How%2520Far%2520is%2520Your%2520T2V%2520Generator%2520from%2520a%2520World%2520Model%253F%26entry.906535625%3DZeqing%2520Wang%2520and%2520Xinyu%2520Wei%2520and%2520Bairui%2520Li%2520and%2520Zhen%2520Guo%2520and%2520Jinrui%2520Zhang%2520and%2520Hongyang%2520Wei%2520and%2520Keze%2520Wang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520recent%2520rapid%2520advancement%2520of%2520Text-to-Video%2520%2528T2V%2529%2520generation%2520technologies%252C%250Awhich%2520are%2520critical%2520to%2520build%2520%2560%2560world%2520models%2527%2527%252C%2520makes%2520the%2520existing%2520benchmarks%250Aincreasingly%2520insufficient%2520to%2520evaluate%2520state-of-the-art%2520T2V%2520models.%2520First%252C%250Acurrent%2520evaluation%2520dimensions%252C%2520such%2520as%2520per-frame%2520aesthetic%2520quality%2520and%2520temporal%250Aconsistency%252C%2520are%2520no%2520longer%2520able%2520to%2520differentiate%2520state-of-the-art%2520T2V%2520models.%250ASecond%252C%2520event-level%2520temporal%2520causality%252C%2520which%2520not%2520only%2520distinguishes%2520video%2520from%250Aother%2520modalities%2520but%2520also%2520constitutes%2520a%2520crucial%2520component%2520of%2520world%2520models%252C%2520is%250Aseverely%2520underexplored%2520in%2520existing%2520benchmarks.%2520Third%252C%2520existing%2520benchmarks%2520lack%250Aa%2520systematic%2520assessment%2520of%2520world%2520knowledge%252C%2520which%2520are%2520essential%2520capabilities%250Afor%2520building%2520world%2520models.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520VideoVerse%252C%2520a%250Acomprehensive%2520benchmark%2520that%2520focuses%2520on%2520evaluating%2520whether%2520a%2520T2V%2520model%2520could%250Aunderstand%2520complex%2520temporal%2520causality%2520and%2520world%2520knowledge%2520in%2520the%2520real%2520world.%2520We%250Acollect%2520representative%2520videos%2520across%2520diverse%2520domains%2520%2528e.g.%252C%2520natural%2520landscapes%252C%250Asports%252C%2520indoor%2520scenes%252C%2520science%2520fiction%252C%2520chemical%2520and%2520physical%2520experiments%2529%2520and%250Aextract%2520their%2520event-level%2520descriptions%2520with%2520inherent%2520temporal%2520causality%252C%2520which%250Aare%2520then%2520rewritten%2520into%2520text-to-video%2520prompts%2520by%2520independent%2520annotators.%2520For%250Aeach%2520prompt%252C%2520we%2520design%2520a%2520suite%2520of%2520binary%2520evaluation%2520questions%2520from%2520the%250Aperspective%2520of%2520dynamic%2520and%2520static%2520properties%252C%2520with%2520a%2520total%2520of%2520ten%2520carefully%250Adefined%2520evaluation%2520dimensions.%2520In%2520total%252C%2520our%2520VideoVerse%2520comprises%2520300%2520carefully%250Acurated%2520prompts%252C%2520involving%2520815%2520events%2520and%2520793%2520binary%2520evaluation%2520questions.%250AConsequently%252C%2520a%2520human%2520preference%2520aligned%2520QA-based%2520evaluation%2520pipeline%2520is%250Adeveloped%2520by%2520using%2520modern%2520vision-language%2520models.%2520Finally%252C%2520we%2520perform%2520a%250Asystematic%2520evaluation%2520of%2520state-of-the-art%2520open-source%2520and%2520closed-source%2520T2V%250Amodels%2520on%2520VideoVerse%252C%2520providing%2520in-depth%2520analysis%2520on%2520how%2520far%2520the%2520current%2520T2V%250Agenerators%2520are%2520from%2520world%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoVerse%3A%20How%20Far%20is%20Your%20T2V%20Generator%20from%20a%20World%20Model%3F&entry.906535625=Zeqing%20Wang%20and%20Xinyu%20Wei%20and%20Bairui%20Li%20and%20Zhen%20Guo%20and%20Jinrui%20Zhang%20and%20Hongyang%20Wei%20and%20Keze%20Wang%20and%20Lei%20Zhang&entry.1292438233=%20%20The%20recent%20rapid%20advancement%20of%20Text-to-Video%20%28T2V%29%20generation%20technologies%2C%0Awhich%20are%20critical%20to%20build%20%60%60world%20models%27%27%2C%20makes%20the%20existing%20benchmarks%0Aincreasingly%20insufficient%20to%20evaluate%20state-of-the-art%20T2V%20models.%20First%2C%0Acurrent%20evaluation%20dimensions%2C%20such%20as%20per-frame%20aesthetic%20quality%20and%20temporal%0Aconsistency%2C%20are%20no%20longer%20able%20to%20differentiate%20state-of-the-art%20T2V%20models.%0ASecond%2C%20event-level%20temporal%20causality%2C%20which%20not%20only%20distinguishes%20video%20from%0Aother%20modalities%20but%20also%20constitutes%20a%20crucial%20component%20of%20world%20models%2C%20is%0Aseverely%20underexplored%20in%20existing%20benchmarks.%20Third%2C%20existing%20benchmarks%20lack%0Aa%20systematic%20assessment%20of%20world%20knowledge%2C%20which%20are%20essential%20capabilities%0Afor%20building%20world%20models.%20To%20address%20these%20issues%2C%20we%20introduce%20VideoVerse%2C%20a%0Acomprehensive%20benchmark%20that%20focuses%20on%20evaluating%20whether%20a%20T2V%20model%20could%0Aunderstand%20complex%20temporal%20causality%20and%20world%20knowledge%20in%20the%20real%20world.%20We%0Acollect%20representative%20videos%20across%20diverse%20domains%20%28e.g.%2C%20natural%20landscapes%2C%0Asports%2C%20indoor%20scenes%2C%20science%20fiction%2C%20chemical%20and%20physical%20experiments%29%20and%0Aextract%20their%20event-level%20descriptions%20with%20inherent%20temporal%20causality%2C%20which%0Aare%20then%20rewritten%20into%20text-to-video%20prompts%20by%20independent%20annotators.%20For%0Aeach%20prompt%2C%20we%20design%20a%20suite%20of%20binary%20evaluation%20questions%20from%20the%0Aperspective%20of%20dynamic%20and%20static%20properties%2C%20with%20a%20total%20of%20ten%20carefully%0Adefined%20evaluation%20dimensions.%20In%20total%2C%20our%20VideoVerse%20comprises%20300%20carefully%0Acurated%20prompts%2C%20involving%20815%20events%20and%20793%20binary%20evaluation%20questions.%0AConsequently%2C%20a%20human%20preference%20aligned%20QA-based%20evaluation%20pipeline%20is%0Adeveloped%20by%20using%20modern%20vision-language%20models.%20Finally%2C%20we%20perform%20a%0Asystematic%20evaluation%20of%20state-of-the-art%20open-source%20and%20closed-source%20T2V%0Amodels%20on%20VideoVerse%2C%20providing%20in-depth%20analysis%20on%20how%20far%20the%20current%20T2V%0Agenerators%20are%20from%20world%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08398v2&entry.124074799=Read"},
{"title": "Understanding Reinforcement Learning for Model Training, and future\n  directions with GRAPE", "author": "Rohit Patel", "abstract": "  This paper provides a self-contained, from-scratch, exposition of key\nalgorithms for instruction tuning of models: SFT, Rejection Sampling,\nREINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy\nOptimization (PPO), Group Relative Policy Optimization (GRPO), and Direct\nPreference Optimization (DPO). Explanations of these algorithms often assume\nprior knowledge, lack critical details, and/or are overly generalized and\ncomplex. Here, each method is discussed and developed step by step using\nsimplified and explicit notation focused on LLMs, aiming to eliminate ambiguity\nand provide a clear and intuitive understanding of the concepts. By minimizing\ndetours into the broader RL literature and connecting concepts to LLMs, we\neliminate superfluous abstractions and reduce cognitive overhead. Following\nthis exposition, we provide a literature review of new techniques and\napproaches beyond those detailed. Finally, new ideas for research and\nexploration in the form of GRAPE (Generalized Relative Advantage Policy\nEvolution) are presented.\n", "link": "http://arxiv.org/abs/2509.04501v2", "date": "2025-10-21", "relevancy": 2.4549, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Reinforcement%20Learning%20for%20Model%20Training%2C%20and%20future%0A%20%20directions%20with%20GRAPE&body=Title%3A%20Understanding%20Reinforcement%20Learning%20for%20Model%20Training%2C%20and%20future%0A%20%20directions%20with%20GRAPE%0AAuthor%3A%20Rohit%20Patel%0AAbstract%3A%20%20%20This%20paper%20provides%20a%20self-contained%2C%20from-scratch%2C%20exposition%20of%20key%0Aalgorithms%20for%20instruction%20tuning%20of%20models%3A%20SFT%2C%20Rejection%20Sampling%2C%0AREINFORCE%2C%20Trust%20Region%20Policy%20Optimization%20%28TRPO%29%2C%20Proximal%20Policy%0AOptimization%20%28PPO%29%2C%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20and%20Direct%0APreference%20Optimization%20%28DPO%29.%20Explanations%20of%20these%20algorithms%20often%20assume%0Aprior%20knowledge%2C%20lack%20critical%20details%2C%20and/or%20are%20overly%20generalized%20and%0Acomplex.%20Here%2C%20each%20method%20is%20discussed%20and%20developed%20step%20by%20step%20using%0Asimplified%20and%20explicit%20notation%20focused%20on%20LLMs%2C%20aiming%20to%20eliminate%20ambiguity%0Aand%20provide%20a%20clear%20and%20intuitive%20understanding%20of%20the%20concepts.%20By%20minimizing%0Adetours%20into%20the%20broader%20RL%20literature%20and%20connecting%20concepts%20to%20LLMs%2C%20we%0Aeliminate%20superfluous%20abstractions%20and%20reduce%20cognitive%20overhead.%20Following%0Athis%20exposition%2C%20we%20provide%20a%20literature%20review%20of%20new%20techniques%20and%0Aapproaches%20beyond%20those%20detailed.%20Finally%2C%20new%20ideas%20for%20research%20and%0Aexploration%20in%20the%20form%20of%20GRAPE%20%28Generalized%20Relative%20Advantage%20Policy%0AEvolution%29%20are%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Reinforcement%2520Learning%2520for%2520Model%2520Training%252C%2520and%2520future%250A%2520%2520directions%2520with%2520GRAPE%26entry.906535625%3DRohit%2520Patel%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520a%2520self-contained%252C%2520from-scratch%252C%2520exposition%2520of%2520key%250Aalgorithms%2520for%2520instruction%2520tuning%2520of%2520models%253A%2520SFT%252C%2520Rejection%2520Sampling%252C%250AREINFORCE%252C%2520Trust%2520Region%2520Policy%2520Optimization%2520%2528TRPO%2529%252C%2520Proximal%2520Policy%250AOptimization%2520%2528PPO%2529%252C%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520and%2520Direct%250APreference%2520Optimization%2520%2528DPO%2529.%2520Explanations%2520of%2520these%2520algorithms%2520often%2520assume%250Aprior%2520knowledge%252C%2520lack%2520critical%2520details%252C%2520and/or%2520are%2520overly%2520generalized%2520and%250Acomplex.%2520Here%252C%2520each%2520method%2520is%2520discussed%2520and%2520developed%2520step%2520by%2520step%2520using%250Asimplified%2520and%2520explicit%2520notation%2520focused%2520on%2520LLMs%252C%2520aiming%2520to%2520eliminate%2520ambiguity%250Aand%2520provide%2520a%2520clear%2520and%2520intuitive%2520understanding%2520of%2520the%2520concepts.%2520By%2520minimizing%250Adetours%2520into%2520the%2520broader%2520RL%2520literature%2520and%2520connecting%2520concepts%2520to%2520LLMs%252C%2520we%250Aeliminate%2520superfluous%2520abstractions%2520and%2520reduce%2520cognitive%2520overhead.%2520Following%250Athis%2520exposition%252C%2520we%2520provide%2520a%2520literature%2520review%2520of%2520new%2520techniques%2520and%250Aapproaches%2520beyond%2520those%2520detailed.%2520Finally%252C%2520new%2520ideas%2520for%2520research%2520and%250Aexploration%2520in%2520the%2520form%2520of%2520GRAPE%2520%2528Generalized%2520Relative%2520Advantage%2520Policy%250AEvolution%2529%2520are%2520presented.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Reinforcement%20Learning%20for%20Model%20Training%2C%20and%20future%0A%20%20directions%20with%20GRAPE&entry.906535625=Rohit%20Patel&entry.1292438233=%20%20This%20paper%20provides%20a%20self-contained%2C%20from-scratch%2C%20exposition%20of%20key%0Aalgorithms%20for%20instruction%20tuning%20of%20models%3A%20SFT%2C%20Rejection%20Sampling%2C%0AREINFORCE%2C%20Trust%20Region%20Policy%20Optimization%20%28TRPO%29%2C%20Proximal%20Policy%0AOptimization%20%28PPO%29%2C%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20and%20Direct%0APreference%20Optimization%20%28DPO%29.%20Explanations%20of%20these%20algorithms%20often%20assume%0Aprior%20knowledge%2C%20lack%20critical%20details%2C%20and/or%20are%20overly%20generalized%20and%0Acomplex.%20Here%2C%20each%20method%20is%20discussed%20and%20developed%20step%20by%20step%20using%0Asimplified%20and%20explicit%20notation%20focused%20on%20LLMs%2C%20aiming%20to%20eliminate%20ambiguity%0Aand%20provide%20a%20clear%20and%20intuitive%20understanding%20of%20the%20concepts.%20By%20minimizing%0Adetours%20into%20the%20broader%20RL%20literature%20and%20connecting%20concepts%20to%20LLMs%2C%20we%0Aeliminate%20superfluous%20abstractions%20and%20reduce%20cognitive%20overhead.%20Following%0Athis%20exposition%2C%20we%20provide%20a%20literature%20review%20of%20new%20techniques%20and%0Aapproaches%20beyond%20those%20detailed.%20Finally%2C%20new%20ideas%20for%20research%20and%0Aexploration%20in%20the%20form%20of%20GRAPE%20%28Generalized%20Relative%20Advantage%20Policy%0AEvolution%29%20are%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04501v2&entry.124074799=Read"},
{"title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal\n  Unalignment for Large Language Model", "author": "Jinwei Hu and Zhenglin Huang and Xiangyu Yin and Wenjie Ruan and Guangliang Cheng and Yi Dong and Xiaowei Huang", "abstract": "  Large language models have been widely applied, but can inadvertently encode\nsensitive or harmful information, raising significant safety concerns. Machine\nunlearning has emerged to alleviate this concern; however, existing\ntraining-time unlearning approaches, relying on coarse-grained loss\ncombinations, have limitations in precisely separating knowledge and balancing\nremoval effectiveness with model utility. In contrast, we propose Fine-grained\nActivation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel\nrepresentation-guided unlearning approach that leverages information-theoretic\nguidance for efficient parameter selection, employs contrastive mechanisms to\nenhance representation separation, and projects conflict gradients onto\northogonal subspaces to resolve conflicts between forgetting and retention\nobjectives. Extensive experiments demonstrate that FALCON achieves superior\nunlearning effectiveness while maintaining model utility, exhibiting robust\nresistance against knowledge recovery attempts.\n", "link": "http://arxiv.org/abs/2502.01472v3", "date": "2025-10-21", "relevancy": 2.4341, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4916}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FALCON%3A%20Fine-grained%20Activation%20Manipulation%20by%20Contrastive%20Orthogonal%0A%20%20Unalignment%20for%20Large%20Language%20Model&body=Title%3A%20FALCON%3A%20Fine-grained%20Activation%20Manipulation%20by%20Contrastive%20Orthogonal%0A%20%20Unalignment%20for%20Large%20Language%20Model%0AAuthor%3A%20Jinwei%20Hu%20and%20Zhenglin%20Huang%20and%20Xiangyu%20Yin%20and%20Wenjie%20Ruan%20and%20Guangliang%20Cheng%20and%20Yi%20Dong%20and%20Xiaowei%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20have%20been%20widely%20applied%2C%20but%20can%20inadvertently%20encode%0Asensitive%20or%20harmful%20information%2C%20raising%20significant%20safety%20concerns.%20Machine%0Aunlearning%20has%20emerged%20to%20alleviate%20this%20concern%3B%20however%2C%20existing%0Atraining-time%20unlearning%20approaches%2C%20relying%20on%20coarse-grained%20loss%0Acombinations%2C%20have%20limitations%20in%20precisely%20separating%20knowledge%20and%20balancing%0Aremoval%20effectiveness%20with%20model%20utility.%20In%20contrast%2C%20we%20propose%20Fine-grained%0AActivation%20manipuLation%20by%20Contrastive%20Orthogonal%20uNalignment%20%28FALCON%29%2C%20a%20novel%0Arepresentation-guided%20unlearning%20approach%20that%20leverages%20information-theoretic%0Aguidance%20for%20efficient%20parameter%20selection%2C%20employs%20contrastive%20mechanisms%20to%0Aenhance%20representation%20separation%2C%20and%20projects%20conflict%20gradients%20onto%0Aorthogonal%20subspaces%20to%20resolve%20conflicts%20between%20forgetting%20and%20retention%0Aobjectives.%20Extensive%20experiments%20demonstrate%20that%20FALCON%20achieves%20superior%0Aunlearning%20effectiveness%20while%20maintaining%20model%20utility%2C%20exhibiting%20robust%0Aresistance%20against%20knowledge%20recovery%20attempts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01472v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFALCON%253A%2520Fine-grained%2520Activation%2520Manipulation%2520by%2520Contrastive%2520Orthogonal%250A%2520%2520Unalignment%2520for%2520Large%2520Language%2520Model%26entry.906535625%3DJinwei%2520Hu%2520and%2520Zhenglin%2520Huang%2520and%2520Xiangyu%2520Yin%2520and%2520Wenjie%2520Ruan%2520and%2520Guangliang%2520Cheng%2520and%2520Yi%2520Dong%2520and%2520Xiaowei%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520been%2520widely%2520applied%252C%2520but%2520can%2520inadvertently%2520encode%250Asensitive%2520or%2520harmful%2520information%252C%2520raising%2520significant%2520safety%2520concerns.%2520Machine%250Aunlearning%2520has%2520emerged%2520to%2520alleviate%2520this%2520concern%253B%2520however%252C%2520existing%250Atraining-time%2520unlearning%2520approaches%252C%2520relying%2520on%2520coarse-grained%2520loss%250Acombinations%252C%2520have%2520limitations%2520in%2520precisely%2520separating%2520knowledge%2520and%2520balancing%250Aremoval%2520effectiveness%2520with%2520model%2520utility.%2520In%2520contrast%252C%2520we%2520propose%2520Fine-grained%250AActivation%2520manipuLation%2520by%2520Contrastive%2520Orthogonal%2520uNalignment%2520%2528FALCON%2529%252C%2520a%2520novel%250Arepresentation-guided%2520unlearning%2520approach%2520that%2520leverages%2520information-theoretic%250Aguidance%2520for%2520efficient%2520parameter%2520selection%252C%2520employs%2520contrastive%2520mechanisms%2520to%250Aenhance%2520representation%2520separation%252C%2520and%2520projects%2520conflict%2520gradients%2520onto%250Aorthogonal%2520subspaces%2520to%2520resolve%2520conflicts%2520between%2520forgetting%2520and%2520retention%250Aobjectives.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FALCON%2520achieves%2520superior%250Aunlearning%2520effectiveness%2520while%2520maintaining%2520model%2520utility%252C%2520exhibiting%2520robust%250Aresistance%2520against%2520knowledge%2520recovery%2520attempts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01472v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FALCON%3A%20Fine-grained%20Activation%20Manipulation%20by%20Contrastive%20Orthogonal%0A%20%20Unalignment%20for%20Large%20Language%20Model&entry.906535625=Jinwei%20Hu%20and%20Zhenglin%20Huang%20and%20Xiangyu%20Yin%20and%20Wenjie%20Ruan%20and%20Guangliang%20Cheng%20and%20Yi%20Dong%20and%20Xiaowei%20Huang&entry.1292438233=%20%20Large%20language%20models%20have%20been%20widely%20applied%2C%20but%20can%20inadvertently%20encode%0Asensitive%20or%20harmful%20information%2C%20raising%20significant%20safety%20concerns.%20Machine%0Aunlearning%20has%20emerged%20to%20alleviate%20this%20concern%3B%20however%2C%20existing%0Atraining-time%20unlearning%20approaches%2C%20relying%20on%20coarse-grained%20loss%0Acombinations%2C%20have%20limitations%20in%20precisely%20separating%20knowledge%20and%20balancing%0Aremoval%20effectiveness%20with%20model%20utility.%20In%20contrast%2C%20we%20propose%20Fine-grained%0AActivation%20manipuLation%20by%20Contrastive%20Orthogonal%20uNalignment%20%28FALCON%29%2C%20a%20novel%0Arepresentation-guided%20unlearning%20approach%20that%20leverages%20information-theoretic%0Aguidance%20for%20efficient%20parameter%20selection%2C%20employs%20contrastive%20mechanisms%20to%0Aenhance%20representation%20separation%2C%20and%20projects%20conflict%20gradients%20onto%0Aorthogonal%20subspaces%20to%20resolve%20conflicts%20between%20forgetting%20and%20retention%0Aobjectives.%20Extensive%20experiments%20demonstrate%20that%20FALCON%20achieves%20superior%0Aunlearning%20effectiveness%20while%20maintaining%20model%20utility%2C%20exhibiting%20robust%0Aresistance%20against%20knowledge%20recovery%20attempts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01472v3&entry.124074799=Read"},
{"title": "UniVideo: Unified Understanding, Generation, and Editing for Videos", "author": "Cong Wei and Quande Liu and Zixuan Ye and Qiulin Wang and Xintao Wang and Pengfei Wan and Kun Gai and Wenhu Chen", "abstract": "  Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.\n", "link": "http://arxiv.org/abs/2510.08377v2", "date": "2025-10-21", "relevancy": 2.4126, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6165}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniVideo%3A%20Unified%20Understanding%2C%20Generation%2C%20and%20Editing%20for%20Videos&body=Title%3A%20UniVideo%3A%20Unified%20Understanding%2C%20Generation%2C%20and%20Editing%20for%20Videos%0AAuthor%3A%20Cong%20Wei%20and%20Quande%20Liu%20and%20Zixuan%20Ye%20and%20Qiulin%20Wang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Unified%20multimodal%20models%20have%20shown%20promising%20results%20in%20multimodal%20content%0Ageneration%20and%20editing%20but%20remain%20largely%20limited%20to%20the%20image%20domain.%20In%20this%0Awork%2C%20we%20present%20UniVideo%2C%20a%20versatile%20framework%20that%20extends%20unified%20modeling%0Ato%20the%20video%20domain.%20UniVideo%20adopts%20a%20dual-stream%20design%2C%20combining%20a%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29%20for%20instruction%20understanding%20with%20a%0AMultimodal%20DiT%20%28MMDiT%29%20for%20video%20generation.%20This%20design%20enables%20accurate%0Ainterpretation%20of%20complex%20multimodal%20instructions%20while%20preserving%20visual%0Aconsistency.%20Built%20on%20this%20architecture%2C%20UniVideo%20unifies%20diverse%20video%0Ageneration%20and%20editing%20tasks%20under%20a%20single%20multimodal%20instruction%20paradigm%20and%0Ais%20jointly%20trained%20across%20them.%20Extensive%20experiments%20demonstrate%20that%20UniVideo%0Amatches%20or%20surpasses%20state-of-the-art%20task-specific%20baselines%20in%0Atext/image-to-video%20generation%2C%20in-context%20video%20generation%20and%20in-context%0Avideo%20editing.%20Notably%2C%20the%20unified%20design%20of%20UniVideo%20enables%20two%20forms%20of%0Ageneralization.%20First%2C%20UniVideo%20supports%20task%20composition%2C%20such%20as%20combining%0Aediting%20with%20style%20transfer%2C%20by%20integrating%20multiple%20capabilities%20within%20a%0Asingle%20instruction.%20Second%2C%20even%20without%20explicit%20training%20on%20free-form%20video%0Aediting%2C%20UniVideo%20transfers%20its%20editing%20capability%20from%20large-scale%20image%0Aediting%20data%20to%20this%20setting%2C%20handling%20unseen%20instructions%20such%20as%0Agreen-screening%20characters%20or%20changing%20materials%20within%20a%20video.%20Beyond%20these%0Acore%20capabilities%2C%20UniVideo%20also%20supports%20visual-prompt-based%20video%20generation%2C%0Awhere%20the%20MLLM%20interprets%20visual%20prompts%20and%20guides%20the%20MMDiT%20during%20synthesis.%0ATo%20foster%20future%20research%2C%20we%20will%20release%20our%20model%20and%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniVideo%253A%2520Unified%2520Understanding%252C%2520Generation%252C%2520and%2520Editing%2520for%2520Videos%26entry.906535625%3DCong%2520Wei%2520and%2520Quande%2520Liu%2520and%2520Zixuan%2520Ye%2520and%2520Qiulin%2520Wang%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Kun%2520Gai%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Unified%2520multimodal%2520models%2520have%2520shown%2520promising%2520results%2520in%2520multimodal%2520content%250Ageneration%2520and%2520editing%2520but%2520remain%2520largely%2520limited%2520to%2520the%2520image%2520domain.%2520In%2520this%250Awork%252C%2520we%2520present%2520UniVideo%252C%2520a%2520versatile%2520framework%2520that%2520extends%2520unified%2520modeling%250Ato%2520the%2520video%2520domain.%2520UniVideo%2520adopts%2520a%2520dual-stream%2520design%252C%2520combining%2520a%250AMultimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520for%2520instruction%2520understanding%2520with%2520a%250AMultimodal%2520DiT%2520%2528MMDiT%2529%2520for%2520video%2520generation.%2520This%2520design%2520enables%2520accurate%250Ainterpretation%2520of%2520complex%2520multimodal%2520instructions%2520while%2520preserving%2520visual%250Aconsistency.%2520Built%2520on%2520this%2520architecture%252C%2520UniVideo%2520unifies%2520diverse%2520video%250Ageneration%2520and%2520editing%2520tasks%2520under%2520a%2520single%2520multimodal%2520instruction%2520paradigm%2520and%250Ais%2520jointly%2520trained%2520across%2520them.%2520Extensive%2520experiments%2520demonstrate%2520that%2520UniVideo%250Amatches%2520or%2520surpasses%2520state-of-the-art%2520task-specific%2520baselines%2520in%250Atext/image-to-video%2520generation%252C%2520in-context%2520video%2520generation%2520and%2520in-context%250Avideo%2520editing.%2520Notably%252C%2520the%2520unified%2520design%2520of%2520UniVideo%2520enables%2520two%2520forms%2520of%250Ageneralization.%2520First%252C%2520UniVideo%2520supports%2520task%2520composition%252C%2520such%2520as%2520combining%250Aediting%2520with%2520style%2520transfer%252C%2520by%2520integrating%2520multiple%2520capabilities%2520within%2520a%250Asingle%2520instruction.%2520Second%252C%2520even%2520without%2520explicit%2520training%2520on%2520free-form%2520video%250Aediting%252C%2520UniVideo%2520transfers%2520its%2520editing%2520capability%2520from%2520large-scale%2520image%250Aediting%2520data%2520to%2520this%2520setting%252C%2520handling%2520unseen%2520instructions%2520such%2520as%250Agreen-screening%2520characters%2520or%2520changing%2520materials%2520within%2520a%2520video.%2520Beyond%2520these%250Acore%2520capabilities%252C%2520UniVideo%2520also%2520supports%2520visual-prompt-based%2520video%2520generation%252C%250Awhere%2520the%2520MLLM%2520interprets%2520visual%2520prompts%2520and%2520guides%2520the%2520MMDiT%2520during%2520synthesis.%250ATo%2520foster%2520future%2520research%252C%2520we%2520will%2520release%2520our%2520model%2520and%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniVideo%3A%20Unified%20Understanding%2C%20Generation%2C%20and%20Editing%20for%20Videos&entry.906535625=Cong%20Wei%20and%20Quande%20Liu%20and%20Zixuan%20Ye%20and%20Qiulin%20Wang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Wenhu%20Chen&entry.1292438233=%20%20Unified%20multimodal%20models%20have%20shown%20promising%20results%20in%20multimodal%20content%0Ageneration%20and%20editing%20but%20remain%20largely%20limited%20to%20the%20image%20domain.%20In%20this%0Awork%2C%20we%20present%20UniVideo%2C%20a%20versatile%20framework%20that%20extends%20unified%20modeling%0Ato%20the%20video%20domain.%20UniVideo%20adopts%20a%20dual-stream%20design%2C%20combining%20a%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29%20for%20instruction%20understanding%20with%20a%0AMultimodal%20DiT%20%28MMDiT%29%20for%20video%20generation.%20This%20design%20enables%20accurate%0Ainterpretation%20of%20complex%20multimodal%20instructions%20while%20preserving%20visual%0Aconsistency.%20Built%20on%20this%20architecture%2C%20UniVideo%20unifies%20diverse%20video%0Ageneration%20and%20editing%20tasks%20under%20a%20single%20multimodal%20instruction%20paradigm%20and%0Ais%20jointly%20trained%20across%20them.%20Extensive%20experiments%20demonstrate%20that%20UniVideo%0Amatches%20or%20surpasses%20state-of-the-art%20task-specific%20baselines%20in%0Atext/image-to-video%20generation%2C%20in-context%20video%20generation%20and%20in-context%0Avideo%20editing.%20Notably%2C%20the%20unified%20design%20of%20UniVideo%20enables%20two%20forms%20of%0Ageneralization.%20First%2C%20UniVideo%20supports%20task%20composition%2C%20such%20as%20combining%0Aediting%20with%20style%20transfer%2C%20by%20integrating%20multiple%20capabilities%20within%20a%0Asingle%20instruction.%20Second%2C%20even%20without%20explicit%20training%20on%20free-form%20video%0Aediting%2C%20UniVideo%20transfers%20its%20editing%20capability%20from%20large-scale%20image%0Aediting%20data%20to%20this%20setting%2C%20handling%20unseen%20instructions%20such%20as%0Agreen-screening%20characters%20or%20changing%20materials%20within%20a%20video.%20Beyond%20these%0Acore%20capabilities%2C%20UniVideo%20also%20supports%20visual-prompt-based%20video%20generation%2C%0Awhere%20the%20MLLM%20interprets%20visual%20prompts%20and%20guides%20the%20MMDiT%20during%20synthesis.%0ATo%20foster%20future%20research%2C%20we%20will%20release%20our%20model%20and%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08377v2&entry.124074799=Read"},
{"title": "Adapting Language Balance in Code-Switching Speech", "author": "Enes Yavuz Ugan and Ngoc-Quan Pham and Alexander Waibel", "abstract": "  Despite achieving impressive results on standard benchmarks, large\nfoundational models still struggle against code-switching test cases. When data\nscarcity cannot be used as the usual justification for poor performance, the\nreason may lie in the infrequent occurrence of code-switched moments, where the\nembedding of the second language appears subtly. Instead of expecting the\nmodels to learn this infrequency on their own, it might be beneficial to\nprovide the training process with labels. Evaluating model performance on\ncode-switching data requires careful localization of code-switching points\nwhere recognition errors are most consequential, so that the analysis\nemphasizes mistakes occurring at those moments. Building on this observation,\nwe leverage the difference between the embedded and the main language to\nhighlight those code-switching points and thereby emphasize learning at those\nlocations. This simple yet effective differentiable surrogate mitigates context\nbias during generation -- the central challenge in code-switching -- thereby\nimproving the model's robustness. Our experiments with Arabic and\nChinese-English showed that the models are able to predict the switching places\nmore correctly, reflected by the reduced substitution error.\n", "link": "http://arxiv.org/abs/2510.18724v1", "date": "2025-10-21", "relevancy": 2.4024, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4713}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Language%20Balance%20in%20Code-Switching%20Speech&body=Title%3A%20Adapting%20Language%20Balance%20in%20Code-Switching%20Speech%0AAuthor%3A%20Enes%20Yavuz%20Ugan%20and%20Ngoc-Quan%20Pham%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20Despite%20achieving%20impressive%20results%20on%20standard%20benchmarks%2C%20large%0Afoundational%20models%20still%20struggle%20against%20code-switching%20test%20cases.%20When%20data%0Ascarcity%20cannot%20be%20used%20as%20the%20usual%20justification%20for%20poor%20performance%2C%20the%0Areason%20may%20lie%20in%20the%20infrequent%20occurrence%20of%20code-switched%20moments%2C%20where%20the%0Aembedding%20of%20the%20second%20language%20appears%20subtly.%20Instead%20of%20expecting%20the%0Amodels%20to%20learn%20this%20infrequency%20on%20their%20own%2C%20it%20might%20be%20beneficial%20to%0Aprovide%20the%20training%20process%20with%20labels.%20Evaluating%20model%20performance%20on%0Acode-switching%20data%20requires%20careful%20localization%20of%20code-switching%20points%0Awhere%20recognition%20errors%20are%20most%20consequential%2C%20so%20that%20the%20analysis%0Aemphasizes%20mistakes%20occurring%20at%20those%20moments.%20Building%20on%20this%20observation%2C%0Awe%20leverage%20the%20difference%20between%20the%20embedded%20and%20the%20main%20language%20to%0Ahighlight%20those%20code-switching%20points%20and%20thereby%20emphasize%20learning%20at%20those%0Alocations.%20This%20simple%20yet%20effective%20differentiable%20surrogate%20mitigates%20context%0Abias%20during%20generation%20--%20the%20central%20challenge%20in%20code-switching%20--%20thereby%0Aimproving%20the%20model%27s%20robustness.%20Our%20experiments%20with%20Arabic%20and%0AChinese-English%20showed%20that%20the%20models%20are%20able%20to%20predict%20the%20switching%20places%0Amore%20correctly%2C%20reflected%20by%20the%20reduced%20substitution%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Language%2520Balance%2520in%2520Code-Switching%2520Speech%26entry.906535625%3DEnes%2520Yavuz%2520Ugan%2520and%2520Ngoc-Quan%2520Pham%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520Despite%2520achieving%2520impressive%2520results%2520on%2520standard%2520benchmarks%252C%2520large%250Afoundational%2520models%2520still%2520struggle%2520against%2520code-switching%2520test%2520cases.%2520When%2520data%250Ascarcity%2520cannot%2520be%2520used%2520as%2520the%2520usual%2520justification%2520for%2520poor%2520performance%252C%2520the%250Areason%2520may%2520lie%2520in%2520the%2520infrequent%2520occurrence%2520of%2520code-switched%2520moments%252C%2520where%2520the%250Aembedding%2520of%2520the%2520second%2520language%2520appears%2520subtly.%2520Instead%2520of%2520expecting%2520the%250Amodels%2520to%2520learn%2520this%2520infrequency%2520on%2520their%2520own%252C%2520it%2520might%2520be%2520beneficial%2520to%250Aprovide%2520the%2520training%2520process%2520with%2520labels.%2520Evaluating%2520model%2520performance%2520on%250Acode-switching%2520data%2520requires%2520careful%2520localization%2520of%2520code-switching%2520points%250Awhere%2520recognition%2520errors%2520are%2520most%2520consequential%252C%2520so%2520that%2520the%2520analysis%250Aemphasizes%2520mistakes%2520occurring%2520at%2520those%2520moments.%2520Building%2520on%2520this%2520observation%252C%250Awe%2520leverage%2520the%2520difference%2520between%2520the%2520embedded%2520and%2520the%2520main%2520language%2520to%250Ahighlight%2520those%2520code-switching%2520points%2520and%2520thereby%2520emphasize%2520learning%2520at%2520those%250Alocations.%2520This%2520simple%2520yet%2520effective%2520differentiable%2520surrogate%2520mitigates%2520context%250Abias%2520during%2520generation%2520--%2520the%2520central%2520challenge%2520in%2520code-switching%2520--%2520thereby%250Aimproving%2520the%2520model%2527s%2520robustness.%2520Our%2520experiments%2520with%2520Arabic%2520and%250AChinese-English%2520showed%2520that%2520the%2520models%2520are%2520able%2520to%2520predict%2520the%2520switching%2520places%250Amore%2520correctly%252C%2520reflected%2520by%2520the%2520reduced%2520substitution%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Language%20Balance%20in%20Code-Switching%20Speech&entry.906535625=Enes%20Yavuz%20Ugan%20and%20Ngoc-Quan%20Pham%20and%20Alexander%20Waibel&entry.1292438233=%20%20Despite%20achieving%20impressive%20results%20on%20standard%20benchmarks%2C%20large%0Afoundational%20models%20still%20struggle%20against%20code-switching%20test%20cases.%20When%20data%0Ascarcity%20cannot%20be%20used%20as%20the%20usual%20justification%20for%20poor%20performance%2C%20the%0Areason%20may%20lie%20in%20the%20infrequent%20occurrence%20of%20code-switched%20moments%2C%20where%20the%0Aembedding%20of%20the%20second%20language%20appears%20subtly.%20Instead%20of%20expecting%20the%0Amodels%20to%20learn%20this%20infrequency%20on%20their%20own%2C%20it%20might%20be%20beneficial%20to%0Aprovide%20the%20training%20process%20with%20labels.%20Evaluating%20model%20performance%20on%0Acode-switching%20data%20requires%20careful%20localization%20of%20code-switching%20points%0Awhere%20recognition%20errors%20are%20most%20consequential%2C%20so%20that%20the%20analysis%0Aemphasizes%20mistakes%20occurring%20at%20those%20moments.%20Building%20on%20this%20observation%2C%0Awe%20leverage%20the%20difference%20between%20the%20embedded%20and%20the%20main%20language%20to%0Ahighlight%20those%20code-switching%20points%20and%20thereby%20emphasize%20learning%20at%20those%0Alocations.%20This%20simple%20yet%20effective%20differentiable%20surrogate%20mitigates%20context%0Abias%20during%20generation%20--%20the%20central%20challenge%20in%20code-switching%20--%20thereby%0Aimproving%20the%20model%27s%20robustness.%20Our%20experiments%20with%20Arabic%20and%0AChinese-English%20showed%20that%20the%20models%20are%20able%20to%20predict%20the%20switching%20places%0Amore%20correctly%2C%20reflected%20by%20the%20reduced%20substitution%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18724v1&entry.124074799=Read"},
{"title": "LightMem: Lightweight and Efficient Memory-Augmented Generation", "author": "Jizhan Fang and Xinle Deng and Haoming Xu and Ziyan Jiang and Yuqi Tang and Ziwen Xu and Shumin Deng and Yunzhi Yao and Mengru Wang and Shuofei Qiao and Huajun Chen and Ningyu Zhang", "abstract": "  Despite their remarkable capabilities, Large Language Models (LLMs) struggle\nto effectively leverage historical interaction information in dynamic and\ncomplex environments. Memory systems enable LLMs to move beyond stateless\ninteractions by introducing persistent information storage, retrieval, and\nutilization mechanisms. However, existing memory systems often introduce\nsubstantial time and computational overhead. To this end, we introduce a new\nmemory system called LightMem, which strikes a balance between the performance\nand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of\nhuman memory, LightMem organizes memory into three complementary stages. First,\ncognition-inspired sensory memory rapidly filters irrelevant information\nthrough lightweight compression and groups information according to their\ntopics. Next, topic-aware short-term memory consolidates these topic-based\ngroups, organizing and summarizing content for more structured access. Finally,\nlong-term memory with sleep-time update employs an offline procedure that\ndecouples consolidation from online inference. Experiments on LongMemEval with\nGPT and Qwen backbones show that LightMem outperforms strong baselines in\naccuracy (up to 10.9% gains) while reducing token usage by up to 117x, API\ncalls by up to 159x, and runtime by over 12x. The code is available at\nhttps://github.com/zjunlp/LightMem.\n", "link": "http://arxiv.org/abs/2510.18866v1", "date": "2025-10-21", "relevancy": 2.3901, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightMem%3A%20Lightweight%20and%20Efficient%20Memory-Augmented%20Generation&body=Title%3A%20LightMem%3A%20Lightweight%20and%20Efficient%20Memory-Augmented%20Generation%0AAuthor%3A%20Jizhan%20Fang%20and%20Xinle%20Deng%20and%20Haoming%20Xu%20and%20Ziyan%20Jiang%20and%20Yuqi%20Tang%20and%20Ziwen%20Xu%20and%20Shumin%20Deng%20and%20Yunzhi%20Yao%20and%20Mengru%20Wang%20and%20Shuofei%20Qiao%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20Despite%20their%20remarkable%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20struggle%0Ato%20effectively%20leverage%20historical%20interaction%20information%20in%20dynamic%20and%0Acomplex%20environments.%20Memory%20systems%20enable%20LLMs%20to%20move%20beyond%20stateless%0Ainteractions%20by%20introducing%20persistent%20information%20storage%2C%20retrieval%2C%20and%0Autilization%20mechanisms.%20However%2C%20existing%20memory%20systems%20often%20introduce%0Asubstantial%20time%20and%20computational%20overhead.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Amemory%20system%20called%20LightMem%2C%20which%20strikes%20a%20balance%20between%20the%20performance%0Aand%20efficiency%20of%20memory%20systems.%20Inspired%20by%20the%20Atkinson-Shiffrin%20model%20of%0Ahuman%20memory%2C%20LightMem%20organizes%20memory%20into%20three%20complementary%20stages.%20First%2C%0Acognition-inspired%20sensory%20memory%20rapidly%20filters%20irrelevant%20information%0Athrough%20lightweight%20compression%20and%20groups%20information%20according%20to%20their%0Atopics.%20Next%2C%20topic-aware%20short-term%20memory%20consolidates%20these%20topic-based%0Agroups%2C%20organizing%20and%20summarizing%20content%20for%20more%20structured%20access.%20Finally%2C%0Along-term%20memory%20with%20sleep-time%20update%20employs%20an%20offline%20procedure%20that%0Adecouples%20consolidation%20from%20online%20inference.%20Experiments%20on%20LongMemEval%20with%0AGPT%20and%20Qwen%20backbones%20show%20that%20LightMem%20outperforms%20strong%20baselines%20in%0Aaccuracy%20%28up%20to%2010.9%25%20gains%29%20while%20reducing%20token%20usage%20by%20up%20to%20117x%2C%20API%0Acalls%20by%20up%20to%20159x%2C%20and%20runtime%20by%20over%2012x.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/LightMem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightMem%253A%2520Lightweight%2520and%2520Efficient%2520Memory-Augmented%2520Generation%26entry.906535625%3DJizhan%2520Fang%2520and%2520Xinle%2520Deng%2520and%2520Haoming%2520Xu%2520and%2520Ziyan%2520Jiang%2520and%2520Yuqi%2520Tang%2520and%2520Ziwen%2520Xu%2520and%2520Shumin%2520Deng%2520and%2520Yunzhi%2520Yao%2520and%2520Mengru%2520Wang%2520and%2520Shuofei%2520Qiao%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520their%2520remarkable%2520capabilities%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520struggle%250Ato%2520effectively%2520leverage%2520historical%2520interaction%2520information%2520in%2520dynamic%2520and%250Acomplex%2520environments.%2520Memory%2520systems%2520enable%2520LLMs%2520to%2520move%2520beyond%2520stateless%250Ainteractions%2520by%2520introducing%2520persistent%2520information%2520storage%252C%2520retrieval%252C%2520and%250Autilization%2520mechanisms.%2520However%252C%2520existing%2520memory%2520systems%2520often%2520introduce%250Asubstantial%2520time%2520and%2520computational%2520overhead.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520new%250Amemory%2520system%2520called%2520LightMem%252C%2520which%2520strikes%2520a%2520balance%2520between%2520the%2520performance%250Aand%2520efficiency%2520of%2520memory%2520systems.%2520Inspired%2520by%2520the%2520Atkinson-Shiffrin%2520model%2520of%250Ahuman%2520memory%252C%2520LightMem%2520organizes%2520memory%2520into%2520three%2520complementary%2520stages.%2520First%252C%250Acognition-inspired%2520sensory%2520memory%2520rapidly%2520filters%2520irrelevant%2520information%250Athrough%2520lightweight%2520compression%2520and%2520groups%2520information%2520according%2520to%2520their%250Atopics.%2520Next%252C%2520topic-aware%2520short-term%2520memory%2520consolidates%2520these%2520topic-based%250Agroups%252C%2520organizing%2520and%2520summarizing%2520content%2520for%2520more%2520structured%2520access.%2520Finally%252C%250Along-term%2520memory%2520with%2520sleep-time%2520update%2520employs%2520an%2520offline%2520procedure%2520that%250Adecouples%2520consolidation%2520from%2520online%2520inference.%2520Experiments%2520on%2520LongMemEval%2520with%250AGPT%2520and%2520Qwen%2520backbones%2520show%2520that%2520LightMem%2520outperforms%2520strong%2520baselines%2520in%250Aaccuracy%2520%2528up%2520to%252010.9%2525%2520gains%2529%2520while%2520reducing%2520token%2520usage%2520by%2520up%2520to%2520117x%252C%2520API%250Acalls%2520by%2520up%2520to%2520159x%252C%2520and%2520runtime%2520by%2520over%252012x.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zjunlp/LightMem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightMem%3A%20Lightweight%20and%20Efficient%20Memory-Augmented%20Generation&entry.906535625=Jizhan%20Fang%20and%20Xinle%20Deng%20and%20Haoming%20Xu%20and%20Ziyan%20Jiang%20and%20Yuqi%20Tang%20and%20Ziwen%20Xu%20and%20Shumin%20Deng%20and%20Yunzhi%20Yao%20and%20Mengru%20Wang%20and%20Shuofei%20Qiao%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20Despite%20their%20remarkable%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20struggle%0Ato%20effectively%20leverage%20historical%20interaction%20information%20in%20dynamic%20and%0Acomplex%20environments.%20Memory%20systems%20enable%20LLMs%20to%20move%20beyond%20stateless%0Ainteractions%20by%20introducing%20persistent%20information%20storage%2C%20retrieval%2C%20and%0Autilization%20mechanisms.%20However%2C%20existing%20memory%20systems%20often%20introduce%0Asubstantial%20time%20and%20computational%20overhead.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Amemory%20system%20called%20LightMem%2C%20which%20strikes%20a%20balance%20between%20the%20performance%0Aand%20efficiency%20of%20memory%20systems.%20Inspired%20by%20the%20Atkinson-Shiffrin%20model%20of%0Ahuman%20memory%2C%20LightMem%20organizes%20memory%20into%20three%20complementary%20stages.%20First%2C%0Acognition-inspired%20sensory%20memory%20rapidly%20filters%20irrelevant%20information%0Athrough%20lightweight%20compression%20and%20groups%20information%20according%20to%20their%0Atopics.%20Next%2C%20topic-aware%20short-term%20memory%20consolidates%20these%20topic-based%0Agroups%2C%20organizing%20and%20summarizing%20content%20for%20more%20structured%20access.%20Finally%2C%0Along-term%20memory%20with%20sleep-time%20update%20employs%20an%20offline%20procedure%20that%0Adecouples%20consolidation%20from%20online%20inference.%20Experiments%20on%20LongMemEval%20with%0AGPT%20and%20Qwen%20backbones%20show%20that%20LightMem%20outperforms%20strong%20baselines%20in%0Aaccuracy%20%28up%20to%2010.9%25%20gains%29%20while%20reducing%20token%20usage%20by%20up%20to%20117x%2C%20API%0Acalls%20by%20up%20to%20159x%2C%20and%20runtime%20by%20over%2012x.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/LightMem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18866v1&entry.124074799=Read"},
{"title": "CAGE: Curvature-Aware Gradient Estimation For Accurate\n  Quantization-Aware Training", "author": "Soroush Tabesh and Mher Safaryan and Dan Alistarh", "abstract": "  Despite significant work on low-bit quantization-aware training (QAT), there\nis still a large accuracy gap between such techniques and native training. To\naddress this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new\nQAT method that augments the straight-through estimator (STE) gradient with a\ncurvature-aware correction designed to counteract the loss increase induced by\nquantization. CAGE is derived from a multi-objective view of QAT that balances\nloss minimization with adherence to quantization constraints, yielding a\nprincipled correction term that depends on local curvature information. On the\ntheoretical side, we introduce the notion of Pareto-optimal solutions for\nquantized optimization, and establish that CAGE yields strong convergence\nguarantees in the smooth non-convex setting. In terms of implementation, our\napproach is optimizer-agnostic, but we provide a highly-efficient\nimplementation that leverages Adam statistics. When pre-training Llama-style\nmodels of up to 800M-parameters, CAGE recovers over 10% of the\nquantization-induced loss increase in the W4A4 regime over outlier-mitigation\nmethods. These results indicate that curvature-aware gradient corrections can\nbridge the remaining performance gap beyond current outlier-handling methods.\n", "link": "http://arxiv.org/abs/2510.18784v1", "date": "2025-10-21", "relevancy": 2.3819, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4864}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4771}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAGE%3A%20Curvature-Aware%20Gradient%20Estimation%20For%20Accurate%0A%20%20Quantization-Aware%20Training&body=Title%3A%20CAGE%3A%20Curvature-Aware%20Gradient%20Estimation%20For%20Accurate%0A%20%20Quantization-Aware%20Training%0AAuthor%3A%20Soroush%20Tabesh%20and%20Mher%20Safaryan%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20Despite%20significant%20work%20on%20low-bit%20quantization-aware%20training%20%28QAT%29%2C%20there%0Ais%20still%20a%20large%20accuracy%20gap%20between%20such%20techniques%20and%20native%20training.%20To%0Aaddress%20this%2C%20we%20introduce%20CAGE%20%28Curvature-Aware%20Gradient%20Estimation%29%2C%20a%20new%0AQAT%20method%20that%20augments%20the%20straight-through%20estimator%20%28STE%29%20gradient%20with%20a%0Acurvature-aware%20correction%20designed%20to%20counteract%20the%20loss%20increase%20induced%20by%0Aquantization.%20CAGE%20is%20derived%20from%20a%20multi-objective%20view%20of%20QAT%20that%20balances%0Aloss%20minimization%20with%20adherence%20to%20quantization%20constraints%2C%20yielding%20a%0Aprincipled%20correction%20term%20that%20depends%20on%20local%20curvature%20information.%20On%20the%0Atheoretical%20side%2C%20we%20introduce%20the%20notion%20of%20Pareto-optimal%20solutions%20for%0Aquantized%20optimization%2C%20and%20establish%20that%20CAGE%20yields%20strong%20convergence%0Aguarantees%20in%20the%20smooth%20non-convex%20setting.%20In%20terms%20of%20implementation%2C%20our%0Aapproach%20is%20optimizer-agnostic%2C%20but%20we%20provide%20a%20highly-efficient%0Aimplementation%20that%20leverages%20Adam%20statistics.%20When%20pre-training%20Llama-style%0Amodels%20of%20up%20to%20800M-parameters%2C%20CAGE%20recovers%20over%2010%25%20of%20the%0Aquantization-induced%20loss%20increase%20in%20the%20W4A4%20regime%20over%20outlier-mitigation%0Amethods.%20These%20results%20indicate%20that%20curvature-aware%20gradient%20corrections%20can%0Abridge%20the%20remaining%20performance%20gap%20beyond%20current%20outlier-handling%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAGE%253A%2520Curvature-Aware%2520Gradient%2520Estimation%2520For%2520Accurate%250A%2520%2520Quantization-Aware%2520Training%26entry.906535625%3DSoroush%2520Tabesh%2520and%2520Mher%2520Safaryan%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520Despite%2520significant%2520work%2520on%2520low-bit%2520quantization-aware%2520training%2520%2528QAT%2529%252C%2520there%250Ais%2520still%2520a%2520large%2520accuracy%2520gap%2520between%2520such%2520techniques%2520and%2520native%2520training.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520CAGE%2520%2528Curvature-Aware%2520Gradient%2520Estimation%2529%252C%2520a%2520new%250AQAT%2520method%2520that%2520augments%2520the%2520straight-through%2520estimator%2520%2528STE%2529%2520gradient%2520with%2520a%250Acurvature-aware%2520correction%2520designed%2520to%2520counteract%2520the%2520loss%2520increase%2520induced%2520by%250Aquantization.%2520CAGE%2520is%2520derived%2520from%2520a%2520multi-objective%2520view%2520of%2520QAT%2520that%2520balances%250Aloss%2520minimization%2520with%2520adherence%2520to%2520quantization%2520constraints%252C%2520yielding%2520a%250Aprincipled%2520correction%2520term%2520that%2520depends%2520on%2520local%2520curvature%2520information.%2520On%2520the%250Atheoretical%2520side%252C%2520we%2520introduce%2520the%2520notion%2520of%2520Pareto-optimal%2520solutions%2520for%250Aquantized%2520optimization%252C%2520and%2520establish%2520that%2520CAGE%2520yields%2520strong%2520convergence%250Aguarantees%2520in%2520the%2520smooth%2520non-convex%2520setting.%2520In%2520terms%2520of%2520implementation%252C%2520our%250Aapproach%2520is%2520optimizer-agnostic%252C%2520but%2520we%2520provide%2520a%2520highly-efficient%250Aimplementation%2520that%2520leverages%2520Adam%2520statistics.%2520When%2520pre-training%2520Llama-style%250Amodels%2520of%2520up%2520to%2520800M-parameters%252C%2520CAGE%2520recovers%2520over%252010%2525%2520of%2520the%250Aquantization-induced%2520loss%2520increase%2520in%2520the%2520W4A4%2520regime%2520over%2520outlier-mitigation%250Amethods.%2520These%2520results%2520indicate%2520that%2520curvature-aware%2520gradient%2520corrections%2520can%250Abridge%2520the%2520remaining%2520performance%2520gap%2520beyond%2520current%2520outlier-handling%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAGE%3A%20Curvature-Aware%20Gradient%20Estimation%20For%20Accurate%0A%20%20Quantization-Aware%20Training&entry.906535625=Soroush%20Tabesh%20and%20Mher%20Safaryan%20and%20Dan%20Alistarh&entry.1292438233=%20%20Despite%20significant%20work%20on%20low-bit%20quantization-aware%20training%20%28QAT%29%2C%20there%0Ais%20still%20a%20large%20accuracy%20gap%20between%20such%20techniques%20and%20native%20training.%20To%0Aaddress%20this%2C%20we%20introduce%20CAGE%20%28Curvature-Aware%20Gradient%20Estimation%29%2C%20a%20new%0AQAT%20method%20that%20augments%20the%20straight-through%20estimator%20%28STE%29%20gradient%20with%20a%0Acurvature-aware%20correction%20designed%20to%20counteract%20the%20loss%20increase%20induced%20by%0Aquantization.%20CAGE%20is%20derived%20from%20a%20multi-objective%20view%20of%20QAT%20that%20balances%0Aloss%20minimization%20with%20adherence%20to%20quantization%20constraints%2C%20yielding%20a%0Aprincipled%20correction%20term%20that%20depends%20on%20local%20curvature%20information.%20On%20the%0Atheoretical%20side%2C%20we%20introduce%20the%20notion%20of%20Pareto-optimal%20solutions%20for%0Aquantized%20optimization%2C%20and%20establish%20that%20CAGE%20yields%20strong%20convergence%0Aguarantees%20in%20the%20smooth%20non-convex%20setting.%20In%20terms%20of%20implementation%2C%20our%0Aapproach%20is%20optimizer-agnostic%2C%20but%20we%20provide%20a%20highly-efficient%0Aimplementation%20that%20leverages%20Adam%20statistics.%20When%20pre-training%20Llama-style%0Amodels%20of%20up%20to%20800M-parameters%2C%20CAGE%20recovers%20over%2010%25%20of%20the%0Aquantization-induced%20loss%20increase%20in%20the%20W4A4%20regime%20over%20outlier-mitigation%0Amethods.%20These%20results%20indicate%20that%20curvature-aware%20gradient%20corrections%20can%0Abridge%20the%20remaining%20performance%20gap%20beyond%20current%20outlier-handling%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18784v1&entry.124074799=Read"},
{"title": "HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large\n  Language Models", "author": "Sidhant Narula and Javad Rafiei Asl and Mohammad Ghasemigol and Eduardo Blanco and Daniel Takabi", "abstract": "  Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak\nattacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a\nhierarchical semantic network; a feedback-driven Simulator for iterative query\nrefinement; and a Network Traverser for real-time adaptive attack execution.\nHarmNet systematically explores and refines the adversarial space to uncover\nstealthy, high-success attack paths. Experiments across closed-source and\nopen-source LLMs show that HarmNet outperforms state-of-the-art methods,\nachieving higher attack success rates. For example, on Mistral-7B, HarmNet\nachieves a 99.4% attack success rate, 13.9% higher than the best baseline.\nIndex terms: jailbreak attacks; large language models; adversarial framework;\nquery refinement.\n", "link": "http://arxiv.org/abs/2510.18728v1", "date": "2025-10-21", "relevancy": 2.2861, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HarmNet%3A%20A%20Framework%20for%20Adaptive%20Multi-Turn%20Jailbreak%20Attacks%20on%20Large%0A%20%20Language%20Models&body=Title%3A%20HarmNet%3A%20A%20Framework%20for%20Adaptive%20Multi-Turn%20Jailbreak%20Attacks%20on%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Sidhant%20Narula%20and%20Javad%20Rafiei%20Asl%20and%20Mohammad%20Ghasemigol%20and%20Eduardo%20Blanco%20and%20Daniel%20Takabi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20remain%20vulnerable%20to%20multi-turn%20jailbreak%0Aattacks.%20We%20introduce%20HarmNet%2C%20a%20modular%20framework%20comprising%20ThoughtNet%2C%20a%0Ahierarchical%20semantic%20network%3B%20a%20feedback-driven%20Simulator%20for%20iterative%20query%0Arefinement%3B%20and%20a%20Network%20Traverser%20for%20real-time%20adaptive%20attack%20execution.%0AHarmNet%20systematically%20explores%20and%20refines%20the%20adversarial%20space%20to%20uncover%0Astealthy%2C%20high-success%20attack%20paths.%20Experiments%20across%20closed-source%20and%0Aopen-source%20LLMs%20show%20that%20HarmNet%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20higher%20attack%20success%20rates.%20For%20example%2C%20on%20Mistral-7B%2C%20HarmNet%0Aachieves%20a%2099.4%25%20attack%20success%20rate%2C%2013.9%25%20higher%20than%20the%20best%20baseline.%0AIndex%20terms%3A%20jailbreak%20attacks%3B%20large%20language%20models%3B%20adversarial%20framework%3B%0Aquery%20refinement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmNet%253A%2520A%2520Framework%2520for%2520Adaptive%2520Multi-Turn%2520Jailbreak%2520Attacks%2520on%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DSidhant%2520Narula%2520and%2520Javad%2520Rafiei%2520Asl%2520and%2520Mohammad%2520Ghasemigol%2520and%2520Eduardo%2520Blanco%2520and%2520Daniel%2520Takabi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520remain%2520vulnerable%2520to%2520multi-turn%2520jailbreak%250Aattacks.%2520We%2520introduce%2520HarmNet%252C%2520a%2520modular%2520framework%2520comprising%2520ThoughtNet%252C%2520a%250Ahierarchical%2520semantic%2520network%253B%2520a%2520feedback-driven%2520Simulator%2520for%2520iterative%2520query%250Arefinement%253B%2520and%2520a%2520Network%2520Traverser%2520for%2520real-time%2520adaptive%2520attack%2520execution.%250AHarmNet%2520systematically%2520explores%2520and%2520refines%2520the%2520adversarial%2520space%2520to%2520uncover%250Astealthy%252C%2520high-success%2520attack%2520paths.%2520Experiments%2520across%2520closed-source%2520and%250Aopen-source%2520LLMs%2520show%2520that%2520HarmNet%2520outperforms%2520state-of-the-art%2520methods%252C%250Aachieving%2520higher%2520attack%2520success%2520rates.%2520For%2520example%252C%2520on%2520Mistral-7B%252C%2520HarmNet%250Aachieves%2520a%252099.4%2525%2520attack%2520success%2520rate%252C%252013.9%2525%2520higher%2520than%2520the%2520best%2520baseline.%250AIndex%2520terms%253A%2520jailbreak%2520attacks%253B%2520large%2520language%2520models%253B%2520adversarial%2520framework%253B%250Aquery%2520refinement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HarmNet%3A%20A%20Framework%20for%20Adaptive%20Multi-Turn%20Jailbreak%20Attacks%20on%20Large%0A%20%20Language%20Models&entry.906535625=Sidhant%20Narula%20and%20Javad%20Rafiei%20Asl%20and%20Mohammad%20Ghasemigol%20and%20Eduardo%20Blanco%20and%20Daniel%20Takabi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20remain%20vulnerable%20to%20multi-turn%20jailbreak%0Aattacks.%20We%20introduce%20HarmNet%2C%20a%20modular%20framework%20comprising%20ThoughtNet%2C%20a%0Ahierarchical%20semantic%20network%3B%20a%20feedback-driven%20Simulator%20for%20iterative%20query%0Arefinement%3B%20and%20a%20Network%20Traverser%20for%20real-time%20adaptive%20attack%20execution.%0AHarmNet%20systematically%20explores%20and%20refines%20the%20adversarial%20space%20to%20uncover%0Astealthy%2C%20high-success%20attack%20paths.%20Experiments%20across%20closed-source%20and%0Aopen-source%20LLMs%20show%20that%20HarmNet%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20higher%20attack%20success%20rates.%20For%20example%2C%20on%20Mistral-7B%2C%20HarmNet%0Aachieves%20a%2099.4%25%20attack%20success%20rate%2C%2013.9%25%20higher%20than%20the%20best%20baseline.%0AIndex%20terms%3A%20jailbreak%20attacks%3B%20large%20language%20models%3B%20adversarial%20framework%3B%0Aquery%20refinement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18728v1&entry.124074799=Read"},
{"title": "FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning", "author": "Yubin Zheng and Pak-Hei Yeung and Jing Xia and Tianjie Ju and Peng Tang and Weidong Qiu and Jagath C. Rajapakse", "abstract": "  Federated learning (FL) enables multiple clients to collaboratively train\nmachine learning models without exposing local data, balancing performance and\nprivacy. However, domain shift and label heterogeneity across clients often\nhinder the generalization of the aggregated global model. Recently, large-scale\nvision-language models like CLIP have shown strong zero-shot classification\ncapabilities, raising the question of how to effectively fine-tune CLIP across\ndomains in a federated setting. In this work, we propose an adaptive federated\nprompt tuning framework, FedDEAP, to enhance CLIP's generalization in\nmulti-domain scenarios. Our method includes the following three key components:\n(1) To mitigate the loss of domain-specific information caused by\nlabel-supervised tuning, we disentangle semantic and domain-specific features\nin images by using semantic and domain transformation networks with unbiased\nmappings; (2) To preserve domain-specific knowledge during global prompt\naggregation, we introduce a dual-prompt design with a global semantic prompt\nand a local domain prompt to balance shared and personalized information; (3)\nTo maximize the inclusion of semantic and domain information from images in the\ngenerated text features, we align textual and visual representations under the\ntwo learned transformations to preserve semantic and domain consistency.\nTheoretical analysis and extensive experiments on four datasets demonstrate the\neffectiveness of our method in enhancing the generalization of CLIP for\nfederated image recognition across multiple domains.\n", "link": "http://arxiv.org/abs/2510.18837v1", "date": "2025-10-21", "relevancy": 2.2679, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6004}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.544}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedDEAP%3A%20Adaptive%20Dual-Prompt%20Tuning%20for%20Multi-Domain%20Federated%20Learning&body=Title%3A%20FedDEAP%3A%20Adaptive%20Dual-Prompt%20Tuning%20for%20Multi-Domain%20Federated%20Learning%0AAuthor%3A%20Yubin%20Zheng%20and%20Pak-Hei%20Yeung%20and%20Jing%20Xia%20and%20Tianjie%20Ju%20and%20Peng%20Tang%20and%20Weidong%20Qiu%20and%20Jagath%20C.%20Rajapakse%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20multiple%20clients%20to%20collaboratively%20train%0Amachine%20learning%20models%20without%20exposing%20local%20data%2C%20balancing%20performance%20and%0Aprivacy.%20However%2C%20domain%20shift%20and%20label%20heterogeneity%20across%20clients%20often%0Ahinder%20the%20generalization%20of%20the%20aggregated%20global%20model.%20Recently%2C%20large-scale%0Avision-language%20models%20like%20CLIP%20have%20shown%20strong%20zero-shot%20classification%0Acapabilities%2C%20raising%20the%20question%20of%20how%20to%20effectively%20fine-tune%20CLIP%20across%0Adomains%20in%20a%20federated%20setting.%20In%20this%20work%2C%20we%20propose%20an%20adaptive%20federated%0Aprompt%20tuning%20framework%2C%20FedDEAP%2C%20to%20enhance%20CLIP%27s%20generalization%20in%0Amulti-domain%20scenarios.%20Our%20method%20includes%20the%20following%20three%20key%20components%3A%0A%281%29%20To%20mitigate%20the%20loss%20of%20domain-specific%20information%20caused%20by%0Alabel-supervised%20tuning%2C%20we%20disentangle%20semantic%20and%20domain-specific%20features%0Ain%20images%20by%20using%20semantic%20and%20domain%20transformation%20networks%20with%20unbiased%0Amappings%3B%20%282%29%20To%20preserve%20domain-specific%20knowledge%20during%20global%20prompt%0Aaggregation%2C%20we%20introduce%20a%20dual-prompt%20design%20with%20a%20global%20semantic%20prompt%0Aand%20a%20local%20domain%20prompt%20to%20balance%20shared%20and%20personalized%20information%3B%20%283%29%0ATo%20maximize%20the%20inclusion%20of%20semantic%20and%20domain%20information%20from%20images%20in%20the%0Agenerated%20text%20features%2C%20we%20align%20textual%20and%20visual%20representations%20under%20the%0Atwo%20learned%20transformations%20to%20preserve%20semantic%20and%20domain%20consistency.%0ATheoretical%20analysis%20and%20extensive%20experiments%20on%20four%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20enhancing%20the%20generalization%20of%20CLIP%20for%0Afederated%20image%20recognition%20across%20multiple%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedDEAP%253A%2520Adaptive%2520Dual-Prompt%2520Tuning%2520for%2520Multi-Domain%2520Federated%2520Learning%26entry.906535625%3DYubin%2520Zheng%2520and%2520Pak-Hei%2520Yeung%2520and%2520Jing%2520Xia%2520and%2520Tianjie%2520Ju%2520and%2520Peng%2520Tang%2520and%2520Weidong%2520Qiu%2520and%2520Jagath%2520C.%2520Rajapakse%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520multiple%2520clients%2520to%2520collaboratively%2520train%250Amachine%2520learning%2520models%2520without%2520exposing%2520local%2520data%252C%2520balancing%2520performance%2520and%250Aprivacy.%2520However%252C%2520domain%2520shift%2520and%2520label%2520heterogeneity%2520across%2520clients%2520often%250Ahinder%2520the%2520generalization%2520of%2520the%2520aggregated%2520global%2520model.%2520Recently%252C%2520large-scale%250Avision-language%2520models%2520like%2520CLIP%2520have%2520shown%2520strong%2520zero-shot%2520classification%250Acapabilities%252C%2520raising%2520the%2520question%2520of%2520how%2520to%2520effectively%2520fine-tune%2520CLIP%2520across%250Adomains%2520in%2520a%2520federated%2520setting.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520adaptive%2520federated%250Aprompt%2520tuning%2520framework%252C%2520FedDEAP%252C%2520to%2520enhance%2520CLIP%2527s%2520generalization%2520in%250Amulti-domain%2520scenarios.%2520Our%2520method%2520includes%2520the%2520following%2520three%2520key%2520components%253A%250A%25281%2529%2520To%2520mitigate%2520the%2520loss%2520of%2520domain-specific%2520information%2520caused%2520by%250Alabel-supervised%2520tuning%252C%2520we%2520disentangle%2520semantic%2520and%2520domain-specific%2520features%250Ain%2520images%2520by%2520using%2520semantic%2520and%2520domain%2520transformation%2520networks%2520with%2520unbiased%250Amappings%253B%2520%25282%2529%2520To%2520preserve%2520domain-specific%2520knowledge%2520during%2520global%2520prompt%250Aaggregation%252C%2520we%2520introduce%2520a%2520dual-prompt%2520design%2520with%2520a%2520global%2520semantic%2520prompt%250Aand%2520a%2520local%2520domain%2520prompt%2520to%2520balance%2520shared%2520and%2520personalized%2520information%253B%2520%25283%2529%250ATo%2520maximize%2520the%2520inclusion%2520of%2520semantic%2520and%2520domain%2520information%2520from%2520images%2520in%2520the%250Agenerated%2520text%2520features%252C%2520we%2520align%2520textual%2520and%2520visual%2520representations%2520under%2520the%250Atwo%2520learned%2520transformations%2520to%2520preserve%2520semantic%2520and%2520domain%2520consistency.%250ATheoretical%2520analysis%2520and%2520extensive%2520experiments%2520on%2520four%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520enhancing%2520the%2520generalization%2520of%2520CLIP%2520for%250Afederated%2520image%2520recognition%2520across%2520multiple%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedDEAP%3A%20Adaptive%20Dual-Prompt%20Tuning%20for%20Multi-Domain%20Federated%20Learning&entry.906535625=Yubin%20Zheng%20and%20Pak-Hei%20Yeung%20and%20Jing%20Xia%20and%20Tianjie%20Ju%20and%20Peng%20Tang%20and%20Weidong%20Qiu%20and%20Jagath%20C.%20Rajapakse&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20multiple%20clients%20to%20collaboratively%20train%0Amachine%20learning%20models%20without%20exposing%20local%20data%2C%20balancing%20performance%20and%0Aprivacy.%20However%2C%20domain%20shift%20and%20label%20heterogeneity%20across%20clients%20often%0Ahinder%20the%20generalization%20of%20the%20aggregated%20global%20model.%20Recently%2C%20large-scale%0Avision-language%20models%20like%20CLIP%20have%20shown%20strong%20zero-shot%20classification%0Acapabilities%2C%20raising%20the%20question%20of%20how%20to%20effectively%20fine-tune%20CLIP%20across%0Adomains%20in%20a%20federated%20setting.%20In%20this%20work%2C%20we%20propose%20an%20adaptive%20federated%0Aprompt%20tuning%20framework%2C%20FedDEAP%2C%20to%20enhance%20CLIP%27s%20generalization%20in%0Amulti-domain%20scenarios.%20Our%20method%20includes%20the%20following%20three%20key%20components%3A%0A%281%29%20To%20mitigate%20the%20loss%20of%20domain-specific%20information%20caused%20by%0Alabel-supervised%20tuning%2C%20we%20disentangle%20semantic%20and%20domain-specific%20features%0Ain%20images%20by%20using%20semantic%20and%20domain%20transformation%20networks%20with%20unbiased%0Amappings%3B%20%282%29%20To%20preserve%20domain-specific%20knowledge%20during%20global%20prompt%0Aaggregation%2C%20we%20introduce%20a%20dual-prompt%20design%20with%20a%20global%20semantic%20prompt%0Aand%20a%20local%20domain%20prompt%20to%20balance%20shared%20and%20personalized%20information%3B%20%283%29%0ATo%20maximize%20the%20inclusion%20of%20semantic%20and%20domain%20information%20from%20images%20in%20the%0Agenerated%20text%20features%2C%20we%20align%20textual%20and%20visual%20representations%20under%20the%0Atwo%20learned%20transformations%20to%20preserve%20semantic%20and%20domain%20consistency.%0ATheoretical%20analysis%20and%20extensive%20experiments%20on%20four%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20enhancing%20the%20generalization%20of%20CLIP%20for%0Afederated%20image%20recognition%20across%20multiple%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18837v1&entry.124074799=Read"},
{"title": "High-Fidelity And Complex Test Data Generation For Google SQL Code\n  Generation Services", "author": "Shivasankari Kannan and Yeounoh Chung and Amita Gondi and Tristan Swadell and Fatma Ozcan", "abstract": "  The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically relevant high-fidelity mock data for\ncomplex data structures that includes columns with nested structures that we\nfrequently encounter in Google workloads. We highlight the limitations of\nexisting approaches used in production, particularly their inability to handle\nlarge and complex data structures, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate syntactically correct and semantically\nrelevant high-fidelity test data that adheres to complex structural constraints\nand maintains semantic integrity to the SQL test targets (queries/functions).\nThis approach supports comprehensive testing of complex SQL queries involving\njoins, aggregations, and even deeply nested subqueries, ensuring robust\nevaluation of SQL code generation services, like NL2SQL and SQL Code Assistant.\nOur results demonstrate the practical utility of an LLM (\\textit{gemini}) based\ntest data generation for industrial SQL code generation services where\ngenerating high-fidelity test data is essential due to the frequent\nunavailability and inaccessibility of production datasets for testing.\n", "link": "http://arxiv.org/abs/2504.17203v3", "date": "2025-10-21", "relevancy": 2.2355, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4515}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4493}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Fidelity%20And%20Complex%20Test%20Data%20Generation%20For%20Google%20SQL%20Code%0A%20%20Generation%20Services&body=Title%3A%20High-Fidelity%20And%20Complex%20Test%20Data%20Generation%20For%20Google%20SQL%20Code%0A%20%20Generation%20Services%0AAuthor%3A%20Shivasankari%20Kannan%20and%20Yeounoh%20Chung%20and%20Amita%20Gondi%20and%20Tristan%20Swadell%20and%20Fatma%20Ozcan%0AAbstract%3A%20%20%20The%20demand%20for%20high-fidelity%20test%20data%20is%20paramount%20in%20industrial%20settings%0Awhere%20access%20to%20production%20data%20is%20largely%20restricted.%20Traditional%20data%0Ageneration%20methods%20often%20fall%20short%2C%20struggling%20with%20low-fidelity%20and%20the%0Aability%20to%20model%20complex%20data%20structures%20and%20semantic%20relationships%20that%20are%0Acritical%20for%20testing%20complex%20SQL%20code%20generation%20services%20like%20Natural%20Language%0Ato%20SQL%20%28NL2SQL%29.%20In%20this%20paper%2C%20we%20address%20the%20critical%20need%20for%20generating%0Asyntactically%20correct%20and%20semantically%20relevant%20high-fidelity%20mock%20data%20for%0Acomplex%20data%20structures%20that%20includes%20columns%20with%20nested%20structures%20that%20we%0Afrequently%20encounter%20in%20Google%20workloads.%20We%20highlight%20the%20limitations%20of%0Aexisting%20approaches%20used%20in%20production%2C%20particularly%20their%20inability%20to%20handle%0Alarge%20and%20complex%20data%20structures%2C%20as%20well%20as%20the%20lack%20of%20semantically%20coherent%0Atest%20data%20that%20lead%20to%20limited%20test%20coverage.%20We%20demonstrate%20that%20by%20leveraging%0ALarge%20Language%20Models%20%28LLMs%29%20and%20incorporating%20strategic%20pre-%20and%0Apost-processing%20steps%2C%20we%20can%20generate%20syntactically%20correct%20and%20semantically%0Arelevant%20high-fidelity%20test%20data%20that%20adheres%20to%20complex%20structural%20constraints%0Aand%20maintains%20semantic%20integrity%20to%20the%20SQL%20test%20targets%20%28queries/functions%29.%0AThis%20approach%20supports%20comprehensive%20testing%20of%20complex%20SQL%20queries%20involving%0Ajoins%2C%20aggregations%2C%20and%20even%20deeply%20nested%20subqueries%2C%20ensuring%20robust%0Aevaluation%20of%20SQL%20code%20generation%20services%2C%20like%20NL2SQL%20and%20SQL%20Code%20Assistant.%0AOur%20results%20demonstrate%20the%20practical%20utility%20of%20an%20LLM%20%28%5Ctextit%7Bgemini%7D%29%20based%0Atest%20data%20generation%20for%20industrial%20SQL%20code%20generation%20services%20where%0Agenerating%20high-fidelity%20test%20data%20is%20essential%20due%20to%20the%20frequent%0Aunavailability%20and%20inaccessibility%20of%20production%20datasets%20for%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17203v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Fidelity%2520And%2520Complex%2520Test%2520Data%2520Generation%2520For%2520Google%2520SQL%2520Code%250A%2520%2520Generation%2520Services%26entry.906535625%3DShivasankari%2520Kannan%2520and%2520Yeounoh%2520Chung%2520and%2520Amita%2520Gondi%2520and%2520Tristan%2520Swadell%2520and%2520Fatma%2520Ozcan%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520high-fidelity%2520test%2520data%2520is%2520paramount%2520in%2520industrial%2520settings%250Awhere%2520access%2520to%2520production%2520data%2520is%2520largely%2520restricted.%2520Traditional%2520data%250Ageneration%2520methods%2520often%2520fall%2520short%252C%2520struggling%2520with%2520low-fidelity%2520and%2520the%250Aability%2520to%2520model%2520complex%2520data%2520structures%2520and%2520semantic%2520relationships%2520that%2520are%250Acritical%2520for%2520testing%2520complex%2520SQL%2520code%2520generation%2520services%2520like%2520Natural%2520Language%250Ato%2520SQL%2520%2528NL2SQL%2529.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520critical%2520need%2520for%2520generating%250Asyntactically%2520correct%2520and%2520semantically%2520relevant%2520high-fidelity%2520mock%2520data%2520for%250Acomplex%2520data%2520structures%2520that%2520includes%2520columns%2520with%2520nested%2520structures%2520that%2520we%250Afrequently%2520encounter%2520in%2520Google%2520workloads.%2520We%2520highlight%2520the%2520limitations%2520of%250Aexisting%2520approaches%2520used%2520in%2520production%252C%2520particularly%2520their%2520inability%2520to%2520handle%250Alarge%2520and%2520complex%2520data%2520structures%252C%2520as%2520well%2520as%2520the%2520lack%2520of%2520semantically%2520coherent%250Atest%2520data%2520that%2520lead%2520to%2520limited%2520test%2520coverage.%2520We%2520demonstrate%2520that%2520by%2520leveraging%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520incorporating%2520strategic%2520pre-%2520and%250Apost-processing%2520steps%252C%2520we%2520can%2520generate%2520syntactically%2520correct%2520and%2520semantically%250Arelevant%2520high-fidelity%2520test%2520data%2520that%2520adheres%2520to%2520complex%2520structural%2520constraints%250Aand%2520maintains%2520semantic%2520integrity%2520to%2520the%2520SQL%2520test%2520targets%2520%2528queries/functions%2529.%250AThis%2520approach%2520supports%2520comprehensive%2520testing%2520of%2520complex%2520SQL%2520queries%2520involving%250Ajoins%252C%2520aggregations%252C%2520and%2520even%2520deeply%2520nested%2520subqueries%252C%2520ensuring%2520robust%250Aevaluation%2520of%2520SQL%2520code%2520generation%2520services%252C%2520like%2520NL2SQL%2520and%2520SQL%2520Code%2520Assistant.%250AOur%2520results%2520demonstrate%2520the%2520practical%2520utility%2520of%2520an%2520LLM%2520%2528%255Ctextit%257Bgemini%257D%2529%2520based%250Atest%2520data%2520generation%2520for%2520industrial%2520SQL%2520code%2520generation%2520services%2520where%250Agenerating%2520high-fidelity%2520test%2520data%2520is%2520essential%2520due%2520to%2520the%2520frequent%250Aunavailability%2520and%2520inaccessibility%2520of%2520production%2520datasets%2520for%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17203v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Fidelity%20And%20Complex%20Test%20Data%20Generation%20For%20Google%20SQL%20Code%0A%20%20Generation%20Services&entry.906535625=Shivasankari%20Kannan%20and%20Yeounoh%20Chung%20and%20Amita%20Gondi%20and%20Tristan%20Swadell%20and%20Fatma%20Ozcan&entry.1292438233=%20%20The%20demand%20for%20high-fidelity%20test%20data%20is%20paramount%20in%20industrial%20settings%0Awhere%20access%20to%20production%20data%20is%20largely%20restricted.%20Traditional%20data%0Ageneration%20methods%20often%20fall%20short%2C%20struggling%20with%20low-fidelity%20and%20the%0Aability%20to%20model%20complex%20data%20structures%20and%20semantic%20relationships%20that%20are%0Acritical%20for%20testing%20complex%20SQL%20code%20generation%20services%20like%20Natural%20Language%0Ato%20SQL%20%28NL2SQL%29.%20In%20this%20paper%2C%20we%20address%20the%20critical%20need%20for%20generating%0Asyntactically%20correct%20and%20semantically%20relevant%20high-fidelity%20mock%20data%20for%0Acomplex%20data%20structures%20that%20includes%20columns%20with%20nested%20structures%20that%20we%0Afrequently%20encounter%20in%20Google%20workloads.%20We%20highlight%20the%20limitations%20of%0Aexisting%20approaches%20used%20in%20production%2C%20particularly%20their%20inability%20to%20handle%0Alarge%20and%20complex%20data%20structures%2C%20as%20well%20as%20the%20lack%20of%20semantically%20coherent%0Atest%20data%20that%20lead%20to%20limited%20test%20coverage.%20We%20demonstrate%20that%20by%20leveraging%0ALarge%20Language%20Models%20%28LLMs%29%20and%20incorporating%20strategic%20pre-%20and%0Apost-processing%20steps%2C%20we%20can%20generate%20syntactically%20correct%20and%20semantically%0Arelevant%20high-fidelity%20test%20data%20that%20adheres%20to%20complex%20structural%20constraints%0Aand%20maintains%20semantic%20integrity%20to%20the%20SQL%20test%20targets%20%28queries/functions%29.%0AThis%20approach%20supports%20comprehensive%20testing%20of%20complex%20SQL%20queries%20involving%0Ajoins%2C%20aggregations%2C%20and%20even%20deeply%20nested%20subqueries%2C%20ensuring%20robust%0Aevaluation%20of%20SQL%20code%20generation%20services%2C%20like%20NL2SQL%20and%20SQL%20Code%20Assistant.%0AOur%20results%20demonstrate%20the%20practical%20utility%20of%20an%20LLM%20%28%5Ctextit%7Bgemini%7D%29%20based%0Atest%20data%20generation%20for%20industrial%20SQL%20code%20generation%20services%20where%0Agenerating%20high-fidelity%20test%20data%20is%20essential%20due%20to%20the%20frequent%0Aunavailability%20and%20inaccessibility%20of%20production%20datasets%20for%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17203v3&entry.124074799=Read"},
{"title": "Glyph: Scaling Context Windows via Visual-Text Compression", "author": "Jiale Cheng and Yusen Liu and Xinyu Zhang and Yulin Fei and Wenyi Hong and Ruiliang Lyu and Weihan Wang and Zhe Su and Xiaotao Gu and Xiao Liu and Yushi Bai and Jie Tang and Hongning Wang and Minlie Huang", "abstract": "  Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.\n", "link": "http://arxiv.org/abs/2510.17800v2", "date": "2025-10-21", "relevancy": 2.2069, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glyph%3A%20Scaling%20Context%20Windows%20via%20Visual-Text%20Compression&body=Title%3A%20Glyph%3A%20Scaling%20Context%20Windows%20via%20Visual-Text%20Compression%0AAuthor%3A%20Jiale%20Cheng%20and%20Yusen%20Liu%20and%20Xinyu%20Zhang%20and%20Yulin%20Fei%20and%20Wenyi%20Hong%20and%20Ruiliang%20Lyu%20and%20Weihan%20Wang%20and%20Zhe%20Su%20and%20Xiaotao%20Gu%20and%20Xiao%20Liu%20and%20Yushi%20Bai%20and%20Jie%20Tang%20and%20Hongning%20Wang%20and%20Minlie%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20increasingly%20rely%20on%20long-context%20modeling%20for%0Atasks%20such%20as%20document%20understanding%2C%20code%20analysis%2C%20and%20multi-step%20reasoning.%0AHowever%2C%20scaling%20context%20windows%20to%20the%20million-token%20level%20brings%20prohibitive%0Acomputational%20and%20memory%20costs%2C%20limiting%20the%20practicality%20of%20long-context%20LLMs.%0AIn%20this%20work%2C%20we%20take%20a%20different%20perspective-visual%20context%20scaling-to%20tackle%0Athis%20challenge.%20Instead%20of%20extending%20token-based%20sequences%2C%20we%20propose%20Glyph%2C%20a%0Aframework%20that%20renders%20long%20texts%20into%20images%20and%20processes%20them%20with%0Avision-language%20models%20%28VLMs%29.%20This%20approach%20substantially%20compresses%20textual%0Ainput%20while%20preserving%20semantic%20information%2C%20and%20we%20further%20design%20an%0ALLM-driven%20genetic%20search%20to%20identify%20optimal%20visual%20rendering%20configurations%0Afor%20balancing%20accuracy%20and%20compression.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20achieves%203-4x%20token%20compression%20while%20maintaining%0Aaccuracy%20comparable%20to%20leading%20LLMs%20such%20as%20Qwen3-8B%20on%20various%20long-context%0Abenchmarks.%20This%20compression%20also%20leads%20to%20around%204x%20faster%20prefilling%20and%0Adecoding%2C%20and%20approximately%202x%20faster%20SFT%20training.%20Furthermore%2C%20under%20extreme%0Acompression%2C%20a%20128K-context%20VLM%20could%20scale%20to%20handle%201M-token-level%20text%0Atasks.%20In%20addition%2C%20the%20rendered%20text%20data%20benefits%20real-world%20multimodal%0Atasks%2C%20such%20as%20document%20understanding.%20Our%20code%20and%20model%20are%20released%20at%0Ahttps%3A//github.com/thu-coai/Glyph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlyph%253A%2520Scaling%2520Context%2520Windows%2520via%2520Visual-Text%2520Compression%26entry.906535625%3DJiale%2520Cheng%2520and%2520Yusen%2520Liu%2520and%2520Xinyu%2520Zhang%2520and%2520Yulin%2520Fei%2520and%2520Wenyi%2520Hong%2520and%2520Ruiliang%2520Lyu%2520and%2520Weihan%2520Wang%2520and%2520Zhe%2520Su%2520and%2520Xiaotao%2520Gu%2520and%2520Xiao%2520Liu%2520and%2520Yushi%2520Bai%2520and%2520Jie%2520Tang%2520and%2520Hongning%2520Wang%2520and%2520Minlie%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520rely%2520on%2520long-context%2520modeling%2520for%250Atasks%2520such%2520as%2520document%2520understanding%252C%2520code%2520analysis%252C%2520and%2520multi-step%2520reasoning.%250AHowever%252C%2520scaling%2520context%2520windows%2520to%2520the%2520million-token%2520level%2520brings%2520prohibitive%250Acomputational%2520and%2520memory%2520costs%252C%2520limiting%2520the%2520practicality%2520of%2520long-context%2520LLMs.%250AIn%2520this%2520work%252C%2520we%2520take%2520a%2520different%2520perspective-visual%2520context%2520scaling-to%2520tackle%250Athis%2520challenge.%2520Instead%2520of%2520extending%2520token-based%2520sequences%252C%2520we%2520propose%2520Glyph%252C%2520a%250Aframework%2520that%2520renders%2520long%2520texts%2520into%2520images%2520and%2520processes%2520them%2520with%250Avision-language%2520models%2520%2528VLMs%2529.%2520This%2520approach%2520substantially%2520compresses%2520textual%250Ainput%2520while%2520preserving%2520semantic%2520information%252C%2520and%2520we%2520further%2520design%2520an%250ALLM-driven%2520genetic%2520search%2520to%2520identify%2520optimal%2520visual%2520rendering%2520configurations%250Afor%2520balancing%2520accuracy%2520and%2520compression.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520our%2520method%2520achieves%25203-4x%2520token%2520compression%2520while%2520maintaining%250Aaccuracy%2520comparable%2520to%2520leading%2520LLMs%2520such%2520as%2520Qwen3-8B%2520on%2520various%2520long-context%250Abenchmarks.%2520This%2520compression%2520also%2520leads%2520to%2520around%25204x%2520faster%2520prefilling%2520and%250Adecoding%252C%2520and%2520approximately%25202x%2520faster%2520SFT%2520training.%2520Furthermore%252C%2520under%2520extreme%250Acompression%252C%2520a%2520128K-context%2520VLM%2520could%2520scale%2520to%2520handle%25201M-token-level%2520text%250Atasks.%2520In%2520addition%252C%2520the%2520rendered%2520text%2520data%2520benefits%2520real-world%2520multimodal%250Atasks%252C%2520such%2520as%2520document%2520understanding.%2520Our%2520code%2520and%2520model%2520are%2520released%2520at%250Ahttps%253A//github.com/thu-coai/Glyph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glyph%3A%20Scaling%20Context%20Windows%20via%20Visual-Text%20Compression&entry.906535625=Jiale%20Cheng%20and%20Yusen%20Liu%20and%20Xinyu%20Zhang%20and%20Yulin%20Fei%20and%20Wenyi%20Hong%20and%20Ruiliang%20Lyu%20and%20Weihan%20Wang%20and%20Zhe%20Su%20and%20Xiaotao%20Gu%20and%20Xiao%20Liu%20and%20Yushi%20Bai%20and%20Jie%20Tang%20and%20Hongning%20Wang%20and%20Minlie%20Huang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20increasingly%20rely%20on%20long-context%20modeling%20for%0Atasks%20such%20as%20document%20understanding%2C%20code%20analysis%2C%20and%20multi-step%20reasoning.%0AHowever%2C%20scaling%20context%20windows%20to%20the%20million-token%20level%20brings%20prohibitive%0Acomputational%20and%20memory%20costs%2C%20limiting%20the%20practicality%20of%20long-context%20LLMs.%0AIn%20this%20work%2C%20we%20take%20a%20different%20perspective-visual%20context%20scaling-to%20tackle%0Athis%20challenge.%20Instead%20of%20extending%20token-based%20sequences%2C%20we%20propose%20Glyph%2C%20a%0Aframework%20that%20renders%20long%20texts%20into%20images%20and%20processes%20them%20with%0Avision-language%20models%20%28VLMs%29.%20This%20approach%20substantially%20compresses%20textual%0Ainput%20while%20preserving%20semantic%20information%2C%20and%20we%20further%20design%20an%0ALLM-driven%20genetic%20search%20to%20identify%20optimal%20visual%20rendering%20configurations%0Afor%20balancing%20accuracy%20and%20compression.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20achieves%203-4x%20token%20compression%20while%20maintaining%0Aaccuracy%20comparable%20to%20leading%20LLMs%20such%20as%20Qwen3-8B%20on%20various%20long-context%0Abenchmarks.%20This%20compression%20also%20leads%20to%20around%204x%20faster%20prefilling%20and%0Adecoding%2C%20and%20approximately%202x%20faster%20SFT%20training.%20Furthermore%2C%20under%20extreme%0Acompression%2C%20a%20128K-context%20VLM%20could%20scale%20to%20handle%201M-token-level%20text%0Atasks.%20In%20addition%2C%20the%20rendered%20text%20data%20benefits%20real-world%20multimodal%0Atasks%2C%20such%20as%20document%20understanding.%20Our%20code%20and%20model%20are%20released%20at%0Ahttps%3A//github.com/thu-coai/Glyph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17800v2&entry.124074799=Read"},
{"title": "Causally Perturbed Fairness Testing", "author": "Chengwen Du and Tao Chen", "abstract": "  To mitigate unfair and unethical discrimination over sensitive features\n(e.g., gender, age, or race), fairness testing plays an integral role in\nengineering systems that leverage AI models to handle tabular data. A key\nchallenge therein is how to effectively reveal fairness bugs under an\nintractable sample size using perturbation. Much current work has been focusing\non designing the test sample generators, ignoring the valuable knowledge about\ndata characteristics that can help guide the perturbation and hence limiting\ntheir full potential. In this paper, we seek to bridge such a gap by proposing\na generic framework of causally perturbed fairness testing, dubbed CausalFT.\nThrough causal inference, the key idea of CausalFT is to extract the most\ndirectly and causally relevant non-sensitive feature to its sensitive\ncounterpart, which can jointly influence the prediction of the label. Such a\ncausal relationship is then seamlessly injected into the perturbation to guide\na test sample generator. Unlike existing generator-level work, CausalFT serves\nas a higher-level framework that can be paired with diverse base generators.\nExtensive experiments on 1296 cases confirm that CausalFT can considerably\nimprove arbitrary base generators in revealing fairness bugs over 93% of the\ncases with acceptable extra runtime overhead. Compared with a state-of-the-art\napproach that ranks the non-sensitive features solely based on correlation,\nCausalFT performs significantly better on 64% cases while being much more\nefficient. Further, CausalFT can better improve bias resilience in nearly all\ncases.\n", "link": "http://arxiv.org/abs/2510.18719v1", "date": "2025-10-21", "relevancy": 2.1973, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4485}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4447}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causally%20Perturbed%20Fairness%20Testing&body=Title%3A%20Causally%20Perturbed%20Fairness%20Testing%0AAuthor%3A%20Chengwen%20Du%20and%20Tao%20Chen%0AAbstract%3A%20%20%20To%20mitigate%20unfair%20and%20unethical%20discrimination%20over%20sensitive%20features%0A%28e.g.%2C%20gender%2C%20age%2C%20or%20race%29%2C%20fairness%20testing%20plays%20an%20integral%20role%20in%0Aengineering%20systems%20that%20leverage%20AI%20models%20to%20handle%20tabular%20data.%20A%20key%0Achallenge%20therein%20is%20how%20to%20effectively%20reveal%20fairness%20bugs%20under%20an%0Aintractable%20sample%20size%20using%20perturbation.%20Much%20current%20work%20has%20been%20focusing%0Aon%20designing%20the%20test%20sample%20generators%2C%20ignoring%20the%20valuable%20knowledge%20about%0Adata%20characteristics%20that%20can%20help%20guide%20the%20perturbation%20and%20hence%20limiting%0Atheir%20full%20potential.%20In%20this%20paper%2C%20we%20seek%20to%20bridge%20such%20a%20gap%20by%20proposing%0Aa%20generic%20framework%20of%20causally%20perturbed%20fairness%20testing%2C%20dubbed%20CausalFT.%0AThrough%20causal%20inference%2C%20the%20key%20idea%20of%20CausalFT%20is%20to%20extract%20the%20most%0Adirectly%20and%20causally%20relevant%20non-sensitive%20feature%20to%20its%20sensitive%0Acounterpart%2C%20which%20can%20jointly%20influence%20the%20prediction%20of%20the%20label.%20Such%20a%0Acausal%20relationship%20is%20then%20seamlessly%20injected%20into%20the%20perturbation%20to%20guide%0Aa%20test%20sample%20generator.%20Unlike%20existing%20generator-level%20work%2C%20CausalFT%20serves%0Aas%20a%20higher-level%20framework%20that%20can%20be%20paired%20with%20diverse%20base%20generators.%0AExtensive%20experiments%20on%201296%20cases%20confirm%20that%20CausalFT%20can%20considerably%0Aimprove%20arbitrary%20base%20generators%20in%20revealing%20fairness%20bugs%20over%2093%25%20of%20the%0Acases%20with%20acceptable%20extra%20runtime%20overhead.%20Compared%20with%20a%20state-of-the-art%0Aapproach%20that%20ranks%20the%20non-sensitive%20features%20solely%20based%20on%20correlation%2C%0ACausalFT%20performs%20significantly%20better%20on%2064%25%20cases%20while%20being%20much%20more%0Aefficient.%20Further%2C%20CausalFT%20can%20better%20improve%20bias%20resilience%20in%20nearly%20all%0Acases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausally%2520Perturbed%2520Fairness%2520Testing%26entry.906535625%3DChengwen%2520Du%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520To%2520mitigate%2520unfair%2520and%2520unethical%2520discrimination%2520over%2520sensitive%2520features%250A%2528e.g.%252C%2520gender%252C%2520age%252C%2520or%2520race%2529%252C%2520fairness%2520testing%2520plays%2520an%2520integral%2520role%2520in%250Aengineering%2520systems%2520that%2520leverage%2520AI%2520models%2520to%2520handle%2520tabular%2520data.%2520A%2520key%250Achallenge%2520therein%2520is%2520how%2520to%2520effectively%2520reveal%2520fairness%2520bugs%2520under%2520an%250Aintractable%2520sample%2520size%2520using%2520perturbation.%2520Much%2520current%2520work%2520has%2520been%2520focusing%250Aon%2520designing%2520the%2520test%2520sample%2520generators%252C%2520ignoring%2520the%2520valuable%2520knowledge%2520about%250Adata%2520characteristics%2520that%2520can%2520help%2520guide%2520the%2520perturbation%2520and%2520hence%2520limiting%250Atheir%2520full%2520potential.%2520In%2520this%2520paper%252C%2520we%2520seek%2520to%2520bridge%2520such%2520a%2520gap%2520by%2520proposing%250Aa%2520generic%2520framework%2520of%2520causally%2520perturbed%2520fairness%2520testing%252C%2520dubbed%2520CausalFT.%250AThrough%2520causal%2520inference%252C%2520the%2520key%2520idea%2520of%2520CausalFT%2520is%2520to%2520extract%2520the%2520most%250Adirectly%2520and%2520causally%2520relevant%2520non-sensitive%2520feature%2520to%2520its%2520sensitive%250Acounterpart%252C%2520which%2520can%2520jointly%2520influence%2520the%2520prediction%2520of%2520the%2520label.%2520Such%2520a%250Acausal%2520relationship%2520is%2520then%2520seamlessly%2520injected%2520into%2520the%2520perturbation%2520to%2520guide%250Aa%2520test%2520sample%2520generator.%2520Unlike%2520existing%2520generator-level%2520work%252C%2520CausalFT%2520serves%250Aas%2520a%2520higher-level%2520framework%2520that%2520can%2520be%2520paired%2520with%2520diverse%2520base%2520generators.%250AExtensive%2520experiments%2520on%25201296%2520cases%2520confirm%2520that%2520CausalFT%2520can%2520considerably%250Aimprove%2520arbitrary%2520base%2520generators%2520in%2520revealing%2520fairness%2520bugs%2520over%252093%2525%2520of%2520the%250Acases%2520with%2520acceptable%2520extra%2520runtime%2520overhead.%2520Compared%2520with%2520a%2520state-of-the-art%250Aapproach%2520that%2520ranks%2520the%2520non-sensitive%2520features%2520solely%2520based%2520on%2520correlation%252C%250ACausalFT%2520performs%2520significantly%2520better%2520on%252064%2525%2520cases%2520while%2520being%2520much%2520more%250Aefficient.%2520Further%252C%2520CausalFT%2520can%2520better%2520improve%2520bias%2520resilience%2520in%2520nearly%2520all%250Acases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causally%20Perturbed%20Fairness%20Testing&entry.906535625=Chengwen%20Du%20and%20Tao%20Chen&entry.1292438233=%20%20To%20mitigate%20unfair%20and%20unethical%20discrimination%20over%20sensitive%20features%0A%28e.g.%2C%20gender%2C%20age%2C%20or%20race%29%2C%20fairness%20testing%20plays%20an%20integral%20role%20in%0Aengineering%20systems%20that%20leverage%20AI%20models%20to%20handle%20tabular%20data.%20A%20key%0Achallenge%20therein%20is%20how%20to%20effectively%20reveal%20fairness%20bugs%20under%20an%0Aintractable%20sample%20size%20using%20perturbation.%20Much%20current%20work%20has%20been%20focusing%0Aon%20designing%20the%20test%20sample%20generators%2C%20ignoring%20the%20valuable%20knowledge%20about%0Adata%20characteristics%20that%20can%20help%20guide%20the%20perturbation%20and%20hence%20limiting%0Atheir%20full%20potential.%20In%20this%20paper%2C%20we%20seek%20to%20bridge%20such%20a%20gap%20by%20proposing%0Aa%20generic%20framework%20of%20causally%20perturbed%20fairness%20testing%2C%20dubbed%20CausalFT.%0AThrough%20causal%20inference%2C%20the%20key%20idea%20of%20CausalFT%20is%20to%20extract%20the%20most%0Adirectly%20and%20causally%20relevant%20non-sensitive%20feature%20to%20its%20sensitive%0Acounterpart%2C%20which%20can%20jointly%20influence%20the%20prediction%20of%20the%20label.%20Such%20a%0Acausal%20relationship%20is%20then%20seamlessly%20injected%20into%20the%20perturbation%20to%20guide%0Aa%20test%20sample%20generator.%20Unlike%20existing%20generator-level%20work%2C%20CausalFT%20serves%0Aas%20a%20higher-level%20framework%20that%20can%20be%20paired%20with%20diverse%20base%20generators.%0AExtensive%20experiments%20on%201296%20cases%20confirm%20that%20CausalFT%20can%20considerably%0Aimprove%20arbitrary%20base%20generators%20in%20revealing%20fairness%20bugs%20over%2093%25%20of%20the%0Acases%20with%20acceptable%20extra%20runtime%20overhead.%20Compared%20with%20a%20state-of-the-art%0Aapproach%20that%20ranks%20the%20non-sensitive%20features%20solely%20based%20on%20correlation%2C%0ACausalFT%20performs%20significantly%20better%20on%2064%25%20cases%20while%20being%20much%20more%0Aefficient.%20Further%2C%20CausalFT%20can%20better%20improve%20bias%20resilience%20in%20nearly%20all%0Acases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18719v1&entry.124074799=Read"},
{"title": "How Do LLMs Use Their Depth?", "author": "Akshat Gupta and Jay Yeung and Gopala Anumanchipalli and Anna Ivanova", "abstract": "  Growing evidence suggests that large language models do not use their depth\nuniformly, yet we still lack a fine-grained understanding of their layer-wise\nprediction dynamics. In this paper, we trace the intermediate representations\nof several open-weight models during inference and reveal a structured and\nnuanced use of depth. Specifically, we propose a \"Guess-then-Refine\" framework\nthat explains how LLMs internally structure their computations to make\npredictions. We first show that the top-ranked predictions in early LLM layers\nare composed primarily of high-frequency tokens, which act as statistical\nguesses proposed by the model early on due to the lack of appropriate\ncontextual information. As contextual information develops deeper into the\nmodel, these initial guesses get refined into contextually appropriate tokens.\nEven high-frequency token predictions from early layers get refined >70% of the\ntime, indicating that correct token prediction is not \"one-and-done\". We then\ngo beyond frequency-based prediction to examine the dynamic usage of layer\ndepth across three case studies. (i) Part-of-speech analysis shows that\nfunction words are, on average, the earliest to be predicted correctly. (ii)\nFact recall task analysis shows that, in a multi-token answer, the first token\nrequires more computational depth than the rest. (iii) Multiple-choice task\nanalysis shows that the model identifies the format of the response within the\nfirst half of the layers, but finalizes its response only toward the end.\nTogether, our results provide a detailed view of depth usage in LLMs, shedding\nlight on the layer-by-layer computations that underlie successful predictions\nand providing insights for future works to improve computational efficiency in\ntransformer-based models.\n", "link": "http://arxiv.org/abs/2510.18871v1", "date": "2025-10-21", "relevancy": 2.1957, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Do%20LLMs%20Use%20Their%20Depth%3F&body=Title%3A%20How%20Do%20LLMs%20Use%20Their%20Depth%3F%0AAuthor%3A%20Akshat%20Gupta%20and%20Jay%20Yeung%20and%20Gopala%20Anumanchipalli%20and%20Anna%20Ivanova%0AAbstract%3A%20%20%20Growing%20evidence%20suggests%20that%20large%20language%20models%20do%20not%20use%20their%20depth%0Auniformly%2C%20yet%20we%20still%20lack%20a%20fine-grained%20understanding%20of%20their%20layer-wise%0Aprediction%20dynamics.%20In%20this%20paper%2C%20we%20trace%20the%20intermediate%20representations%0Aof%20several%20open-weight%20models%20during%20inference%20and%20reveal%20a%20structured%20and%0Anuanced%20use%20of%20depth.%20Specifically%2C%20we%20propose%20a%20%22Guess-then-Refine%22%20framework%0Athat%20explains%20how%20LLMs%20internally%20structure%20their%20computations%20to%20make%0Apredictions.%20We%20first%20show%20that%20the%20top-ranked%20predictions%20in%20early%20LLM%20layers%0Aare%20composed%20primarily%20of%20high-frequency%20tokens%2C%20which%20act%20as%20statistical%0Aguesses%20proposed%20by%20the%20model%20early%20on%20due%20to%20the%20lack%20of%20appropriate%0Acontextual%20information.%20As%20contextual%20information%20develops%20deeper%20into%20the%0Amodel%2C%20these%20initial%20guesses%20get%20refined%20into%20contextually%20appropriate%20tokens.%0AEven%20high-frequency%20token%20predictions%20from%20early%20layers%20get%20refined%20%3E70%25%20of%20the%0Atime%2C%20indicating%20that%20correct%20token%20prediction%20is%20not%20%22one-and-done%22.%20We%20then%0Ago%20beyond%20frequency-based%20prediction%20to%20examine%20the%20dynamic%20usage%20of%20layer%0Adepth%20across%20three%20case%20studies.%20%28i%29%20Part-of-speech%20analysis%20shows%20that%0Afunction%20words%20are%2C%20on%20average%2C%20the%20earliest%20to%20be%20predicted%20correctly.%20%28ii%29%0AFact%20recall%20task%20analysis%20shows%20that%2C%20in%20a%20multi-token%20answer%2C%20the%20first%20token%0Arequires%20more%20computational%20depth%20than%20the%20rest.%20%28iii%29%20Multiple-choice%20task%0Aanalysis%20shows%20that%20the%20model%20identifies%20the%20format%20of%20the%20response%20within%20the%0Afirst%20half%20of%20the%20layers%2C%20but%20finalizes%20its%20response%20only%20toward%20the%20end.%0ATogether%2C%20our%20results%20provide%20a%20detailed%20view%20of%20depth%20usage%20in%20LLMs%2C%20shedding%0Alight%20on%20the%20layer-by-layer%20computations%20that%20underlie%20successful%20predictions%0Aand%20providing%20insights%20for%20future%20works%20to%20improve%20computational%20efficiency%20in%0Atransformer-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Do%2520LLMs%2520Use%2520Their%2520Depth%253F%26entry.906535625%3DAkshat%2520Gupta%2520and%2520Jay%2520Yeung%2520and%2520Gopala%2520Anumanchipalli%2520and%2520Anna%2520Ivanova%26entry.1292438233%3D%2520%2520Growing%2520evidence%2520suggests%2520that%2520large%2520language%2520models%2520do%2520not%2520use%2520their%2520depth%250Auniformly%252C%2520yet%2520we%2520still%2520lack%2520a%2520fine-grained%2520understanding%2520of%2520their%2520layer-wise%250Aprediction%2520dynamics.%2520In%2520this%2520paper%252C%2520we%2520trace%2520the%2520intermediate%2520representations%250Aof%2520several%2520open-weight%2520models%2520during%2520inference%2520and%2520reveal%2520a%2520structured%2520and%250Anuanced%2520use%2520of%2520depth.%2520Specifically%252C%2520we%2520propose%2520a%2520%2522Guess-then-Refine%2522%2520framework%250Athat%2520explains%2520how%2520LLMs%2520internally%2520structure%2520their%2520computations%2520to%2520make%250Apredictions.%2520We%2520first%2520show%2520that%2520the%2520top-ranked%2520predictions%2520in%2520early%2520LLM%2520layers%250Aare%2520composed%2520primarily%2520of%2520high-frequency%2520tokens%252C%2520which%2520act%2520as%2520statistical%250Aguesses%2520proposed%2520by%2520the%2520model%2520early%2520on%2520due%2520to%2520the%2520lack%2520of%2520appropriate%250Acontextual%2520information.%2520As%2520contextual%2520information%2520develops%2520deeper%2520into%2520the%250Amodel%252C%2520these%2520initial%2520guesses%2520get%2520refined%2520into%2520contextually%2520appropriate%2520tokens.%250AEven%2520high-frequency%2520token%2520predictions%2520from%2520early%2520layers%2520get%2520refined%2520%253E70%2525%2520of%2520the%250Atime%252C%2520indicating%2520that%2520correct%2520token%2520prediction%2520is%2520not%2520%2522one-and-done%2522.%2520We%2520then%250Ago%2520beyond%2520frequency-based%2520prediction%2520to%2520examine%2520the%2520dynamic%2520usage%2520of%2520layer%250Adepth%2520across%2520three%2520case%2520studies.%2520%2528i%2529%2520Part-of-speech%2520analysis%2520shows%2520that%250Afunction%2520words%2520are%252C%2520on%2520average%252C%2520the%2520earliest%2520to%2520be%2520predicted%2520correctly.%2520%2528ii%2529%250AFact%2520recall%2520task%2520analysis%2520shows%2520that%252C%2520in%2520a%2520multi-token%2520answer%252C%2520the%2520first%2520token%250Arequires%2520more%2520computational%2520depth%2520than%2520the%2520rest.%2520%2528iii%2529%2520Multiple-choice%2520task%250Aanalysis%2520shows%2520that%2520the%2520model%2520identifies%2520the%2520format%2520of%2520the%2520response%2520within%2520the%250Afirst%2520half%2520of%2520the%2520layers%252C%2520but%2520finalizes%2520its%2520response%2520only%2520toward%2520the%2520end.%250ATogether%252C%2520our%2520results%2520provide%2520a%2520detailed%2520view%2520of%2520depth%2520usage%2520in%2520LLMs%252C%2520shedding%250Alight%2520on%2520the%2520layer-by-layer%2520computations%2520that%2520underlie%2520successful%2520predictions%250Aand%2520providing%2520insights%2520for%2520future%2520works%2520to%2520improve%2520computational%2520efficiency%2520in%250Atransformer-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Do%20LLMs%20Use%20Their%20Depth%3F&entry.906535625=Akshat%20Gupta%20and%20Jay%20Yeung%20and%20Gopala%20Anumanchipalli%20and%20Anna%20Ivanova&entry.1292438233=%20%20Growing%20evidence%20suggests%20that%20large%20language%20models%20do%20not%20use%20their%20depth%0Auniformly%2C%20yet%20we%20still%20lack%20a%20fine-grained%20understanding%20of%20their%20layer-wise%0Aprediction%20dynamics.%20In%20this%20paper%2C%20we%20trace%20the%20intermediate%20representations%0Aof%20several%20open-weight%20models%20during%20inference%20and%20reveal%20a%20structured%20and%0Anuanced%20use%20of%20depth.%20Specifically%2C%20we%20propose%20a%20%22Guess-then-Refine%22%20framework%0Athat%20explains%20how%20LLMs%20internally%20structure%20their%20computations%20to%20make%0Apredictions.%20We%20first%20show%20that%20the%20top-ranked%20predictions%20in%20early%20LLM%20layers%0Aare%20composed%20primarily%20of%20high-frequency%20tokens%2C%20which%20act%20as%20statistical%0Aguesses%20proposed%20by%20the%20model%20early%20on%20due%20to%20the%20lack%20of%20appropriate%0Acontextual%20information.%20As%20contextual%20information%20develops%20deeper%20into%20the%0Amodel%2C%20these%20initial%20guesses%20get%20refined%20into%20contextually%20appropriate%20tokens.%0AEven%20high-frequency%20token%20predictions%20from%20early%20layers%20get%20refined%20%3E70%25%20of%20the%0Atime%2C%20indicating%20that%20correct%20token%20prediction%20is%20not%20%22one-and-done%22.%20We%20then%0Ago%20beyond%20frequency-based%20prediction%20to%20examine%20the%20dynamic%20usage%20of%20layer%0Adepth%20across%20three%20case%20studies.%20%28i%29%20Part-of-speech%20analysis%20shows%20that%0Afunction%20words%20are%2C%20on%20average%2C%20the%20earliest%20to%20be%20predicted%20correctly.%20%28ii%29%0AFact%20recall%20task%20analysis%20shows%20that%2C%20in%20a%20multi-token%20answer%2C%20the%20first%20token%0Arequires%20more%20computational%20depth%20than%20the%20rest.%20%28iii%29%20Multiple-choice%20task%0Aanalysis%20shows%20that%20the%20model%20identifies%20the%20format%20of%20the%20response%20within%20the%0Afirst%20half%20of%20the%20layers%2C%20but%20finalizes%20its%20response%20only%20toward%20the%20end.%0ATogether%2C%20our%20results%20provide%20a%20detailed%20view%20of%20depth%20usage%20in%20LLMs%2C%20shedding%0Alight%20on%20the%20layer-by-layer%20computations%20that%20underlie%20successful%20predictions%0Aand%20providing%20insights%20for%20future%20works%20to%20improve%20computational%20efficiency%20in%0Atransformer-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18871v1&entry.124074799=Read"},
{"title": "Rebellious Student: A Complementary Learning Framework for Background\n  Feature Enhancement in Hyperspectral Anomaly Detection", "author": "Wenping Jin and Yuyang Tang and Li Zhu and Fei Guo", "abstract": "  A recent class of hyperspectral anomaly detection methods that can be trained\nonce on background datasets and then universally deployed -- without per-scene\nretraining or parameter tuning -- has demonstrated remarkable efficiency and\nrobustness. Building upon this paradigm, we focus on the integration of\nspectral and spatial cues and introduce a novel \"Rebellious Student\" framework\nfor complementary feature learning. Unlike conventional teacher-student\nparadigms driven by imitation, our method intentionally trains the spatial\nbranch to diverge from the spectral teacher, thereby learning complementary\nspatial patterns that the teacher fails to capture. A two-stage learning\nstrategy is adopted: (1) a spectral enhancement network is first trained via\nreverse distillation to obtain robust background spectral representations; and\n(2) a spatial network -- the rebellious student -- is subsequently optimized\nusing decorrelation losses that enforce feature orthogonality while maintaining\nreconstruction fidelity to avoid irrelevant noise. Once trained, the framework\nenhances both spectral and spatial background features, enabling parameter-free\nand training-free anomaly detection when paired with conventional detectors.\nExtensive experiments on the HAD100 benchmark show substantial improvements\nover several established baselines with minimal computational overhead,\nconfirming the effectiveness and generality of the proposed complementary\nlearning paradigm. Our code is publicly available at\nhttps://github.com/xjpp2016/FERS.\n", "link": "http://arxiv.org/abs/2510.18781v1", "date": "2025-10-21", "relevancy": 2.184, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5755}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5305}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rebellious%20Student%3A%20A%20Complementary%20Learning%20Framework%20for%20Background%0A%20%20Feature%20Enhancement%20in%20Hyperspectral%20Anomaly%20Detection&body=Title%3A%20Rebellious%20Student%3A%20A%20Complementary%20Learning%20Framework%20for%20Background%0A%20%20Feature%20Enhancement%20in%20Hyperspectral%20Anomaly%20Detection%0AAuthor%3A%20Wenping%20Jin%20and%20Yuyang%20Tang%20and%20Li%20Zhu%20and%20Fei%20Guo%0AAbstract%3A%20%20%20A%20recent%20class%20of%20hyperspectral%20anomaly%20detection%20methods%20that%20can%20be%20trained%0Aonce%20on%20background%20datasets%20and%20then%20universally%20deployed%20--%20without%20per-scene%0Aretraining%20or%20parameter%20tuning%20--%20has%20demonstrated%20remarkable%20efficiency%20and%0Arobustness.%20Building%20upon%20this%20paradigm%2C%20we%20focus%20on%20the%20integration%20of%0Aspectral%20and%20spatial%20cues%20and%20introduce%20a%20novel%20%22Rebellious%20Student%22%20framework%0Afor%20complementary%20feature%20learning.%20Unlike%20conventional%20teacher-student%0Aparadigms%20driven%20by%20imitation%2C%20our%20method%20intentionally%20trains%20the%20spatial%0Abranch%20to%20diverge%20from%20the%20spectral%20teacher%2C%20thereby%20learning%20complementary%0Aspatial%20patterns%20that%20the%20teacher%20fails%20to%20capture.%20A%20two-stage%20learning%0Astrategy%20is%20adopted%3A%20%281%29%20a%20spectral%20enhancement%20network%20is%20first%20trained%20via%0Areverse%20distillation%20to%20obtain%20robust%20background%20spectral%20representations%3B%20and%0A%282%29%20a%20spatial%20network%20--%20the%20rebellious%20student%20--%20is%20subsequently%20optimized%0Ausing%20decorrelation%20losses%20that%20enforce%20feature%20orthogonality%20while%20maintaining%0Areconstruction%20fidelity%20to%20avoid%20irrelevant%20noise.%20Once%20trained%2C%20the%20framework%0Aenhances%20both%20spectral%20and%20spatial%20background%20features%2C%20enabling%20parameter-free%0Aand%20training-free%20anomaly%20detection%20when%20paired%20with%20conventional%20detectors.%0AExtensive%20experiments%20on%20the%20HAD100%20benchmark%20show%20substantial%20improvements%0Aover%20several%20established%20baselines%20with%20minimal%20computational%20overhead%2C%0Aconfirming%20the%20effectiveness%20and%20generality%20of%20the%20proposed%20complementary%0Alearning%20paradigm.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/xjpp2016/FERS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRebellious%2520Student%253A%2520A%2520Complementary%2520Learning%2520Framework%2520for%2520Background%250A%2520%2520Feature%2520Enhancement%2520in%2520Hyperspectral%2520Anomaly%2520Detection%26entry.906535625%3DWenping%2520Jin%2520and%2520Yuyang%2520Tang%2520and%2520Li%2520Zhu%2520and%2520Fei%2520Guo%26entry.1292438233%3D%2520%2520A%2520recent%2520class%2520of%2520hyperspectral%2520anomaly%2520detection%2520methods%2520that%2520can%2520be%2520trained%250Aonce%2520on%2520background%2520datasets%2520and%2520then%2520universally%2520deployed%2520--%2520without%2520per-scene%250Aretraining%2520or%2520parameter%2520tuning%2520--%2520has%2520demonstrated%2520remarkable%2520efficiency%2520and%250Arobustness.%2520Building%2520upon%2520this%2520paradigm%252C%2520we%2520focus%2520on%2520the%2520integration%2520of%250Aspectral%2520and%2520spatial%2520cues%2520and%2520introduce%2520a%2520novel%2520%2522Rebellious%2520Student%2522%2520framework%250Afor%2520complementary%2520feature%2520learning.%2520Unlike%2520conventional%2520teacher-student%250Aparadigms%2520driven%2520by%2520imitation%252C%2520our%2520method%2520intentionally%2520trains%2520the%2520spatial%250Abranch%2520to%2520diverge%2520from%2520the%2520spectral%2520teacher%252C%2520thereby%2520learning%2520complementary%250Aspatial%2520patterns%2520that%2520the%2520teacher%2520fails%2520to%2520capture.%2520A%2520two-stage%2520learning%250Astrategy%2520is%2520adopted%253A%2520%25281%2529%2520a%2520spectral%2520enhancement%2520network%2520is%2520first%2520trained%2520via%250Areverse%2520distillation%2520to%2520obtain%2520robust%2520background%2520spectral%2520representations%253B%2520and%250A%25282%2529%2520a%2520spatial%2520network%2520--%2520the%2520rebellious%2520student%2520--%2520is%2520subsequently%2520optimized%250Ausing%2520decorrelation%2520losses%2520that%2520enforce%2520feature%2520orthogonality%2520while%2520maintaining%250Areconstruction%2520fidelity%2520to%2520avoid%2520irrelevant%2520noise.%2520Once%2520trained%252C%2520the%2520framework%250Aenhances%2520both%2520spectral%2520and%2520spatial%2520background%2520features%252C%2520enabling%2520parameter-free%250Aand%2520training-free%2520anomaly%2520detection%2520when%2520paired%2520with%2520conventional%2520detectors.%250AExtensive%2520experiments%2520on%2520the%2520HAD100%2520benchmark%2520show%2520substantial%2520improvements%250Aover%2520several%2520established%2520baselines%2520with%2520minimal%2520computational%2520overhead%252C%250Aconfirming%2520the%2520effectiveness%2520and%2520generality%2520of%2520the%2520proposed%2520complementary%250Alearning%2520paradigm.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/xjpp2016/FERS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rebellious%20Student%3A%20A%20Complementary%20Learning%20Framework%20for%20Background%0A%20%20Feature%20Enhancement%20in%20Hyperspectral%20Anomaly%20Detection&entry.906535625=Wenping%20Jin%20and%20Yuyang%20Tang%20and%20Li%20Zhu%20and%20Fei%20Guo&entry.1292438233=%20%20A%20recent%20class%20of%20hyperspectral%20anomaly%20detection%20methods%20that%20can%20be%20trained%0Aonce%20on%20background%20datasets%20and%20then%20universally%20deployed%20--%20without%20per-scene%0Aretraining%20or%20parameter%20tuning%20--%20has%20demonstrated%20remarkable%20efficiency%20and%0Arobustness.%20Building%20upon%20this%20paradigm%2C%20we%20focus%20on%20the%20integration%20of%0Aspectral%20and%20spatial%20cues%20and%20introduce%20a%20novel%20%22Rebellious%20Student%22%20framework%0Afor%20complementary%20feature%20learning.%20Unlike%20conventional%20teacher-student%0Aparadigms%20driven%20by%20imitation%2C%20our%20method%20intentionally%20trains%20the%20spatial%0Abranch%20to%20diverge%20from%20the%20spectral%20teacher%2C%20thereby%20learning%20complementary%0Aspatial%20patterns%20that%20the%20teacher%20fails%20to%20capture.%20A%20two-stage%20learning%0Astrategy%20is%20adopted%3A%20%281%29%20a%20spectral%20enhancement%20network%20is%20first%20trained%20via%0Areverse%20distillation%20to%20obtain%20robust%20background%20spectral%20representations%3B%20and%0A%282%29%20a%20spatial%20network%20--%20the%20rebellious%20student%20--%20is%20subsequently%20optimized%0Ausing%20decorrelation%20losses%20that%20enforce%20feature%20orthogonality%20while%20maintaining%0Areconstruction%20fidelity%20to%20avoid%20irrelevant%20noise.%20Once%20trained%2C%20the%20framework%0Aenhances%20both%20spectral%20and%20spatial%20background%20features%2C%20enabling%20parameter-free%0Aand%20training-free%20anomaly%20detection%20when%20paired%20with%20conventional%20detectors.%0AExtensive%20experiments%20on%20the%20HAD100%20benchmark%20show%20substantial%20improvements%0Aover%20several%20established%20baselines%20with%20minimal%20computational%20overhead%2C%0Aconfirming%20the%20effectiveness%20and%20generality%20of%20the%20proposed%20complementary%0Alearning%20paradigm.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/xjpp2016/FERS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18781v1&entry.124074799=Read"},
{"title": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision", "author": "Hongliang Lu and Yuhang Wen and Pengyu Cheng and Ruijin Ding and Haotian Xu and Jiaqi Guo and Chutian Wang and Haonan Chen and Xiaoxi Jiang and Guanjun Jiang", "abstract": "  Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP.\n", "link": "http://arxiv.org/abs/2510.18821v1", "date": "2025-10-21", "relevancy": 2.1757, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5724}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.565}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Search%20Self-play%3A%20Pushing%20the%20Frontier%20of%20Agent%20Capability%20without%0A%20%20Supervision&body=Title%3A%20Search%20Self-play%3A%20Pushing%20the%20Frontier%20of%20Agent%20Capability%20without%0A%20%20Supervision%0AAuthor%3A%20Hongliang%20Lu%20and%20Yuhang%20Wen%20and%20Pengyu%20Cheng%20and%20Ruijin%20Ding%20and%20Haotian%20Xu%20and%20Jiaqi%20Guo%20and%20Chutian%20Wang%20and%20Haonan%20Chen%20and%20Xiaoxi%20Jiang%20and%20Guanjun%20Jiang%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20become%20the%0Amainstream%20technique%20for%20training%20LLM%20agents.%20However%2C%20RLVR%20highly%20depends%20on%0Awell-crafted%20task%20queries%20and%20corresponding%20ground-truth%20answers%20to%20provide%0Aaccurate%20rewards%2C%20which%20requires%20massive%20human%20efforts%20and%20hinders%20the%20RL%0Ascaling%20processes%2C%20especially%20under%20agentic%20scenarios.%20Although%20a%20few%20recent%0Aworks%20explore%20task%20synthesis%20methods%2C%20the%20difficulty%20of%20generated%20agentic%20tasks%0Acan%20hardly%20be%20controlled%20to%20provide%20effective%20RL%20training%20advantages.%20To%0Aachieve%20agentic%20RLVR%20with%20higher%20scalability%2C%20we%20explore%20self-play%20training%20for%0Adeep%20search%20agents%2C%20in%20which%20the%20learning%20LLM%20utilizes%20multi-turn%20search%20engine%0Acalling%20and%20acts%20simultaneously%20as%20both%20a%20task%20proposer%20and%20a%20problem%20solver.%0AThe%20task%20proposer%20aims%20to%20generate%20deep%20search%20queries%20with%20well-defined%0Aground-truth%20answers%20and%20increasing%20task%20difficulty.%20The%20problem%20solver%20tries%0Ato%20handle%20the%20generated%20search%20queries%20and%20output%20the%20correct%20answer%0Apredictions.%20To%20ensure%20that%20each%20generated%20search%20query%20has%20accurate%20ground%0Atruth%2C%20we%20collect%20all%20the%20searching%20results%20from%20the%20proposer%27s%20trajectory%20as%0Aexternal%20knowledge%2C%20then%20conduct%20retrieval-augmentation%20generation%20%28RAG%29%20to%0Atest%20whether%20the%20proposed%20query%20can%20be%20correctly%20answered%20with%20all%20necessary%0Asearch%20documents%20provided.%20In%20this%20search%20self-play%20%28SSP%29%20game%2C%20the%20proposer%0Aand%20the%20solver%20co-evolve%20their%20agent%20capabilities%20through%20both%20competition%20and%0Acooperation.%20With%20substantial%20experimental%20results%2C%20we%20find%20that%20SSP%20can%0Asignificantly%20improve%20search%20agents%27%20performance%20uniformly%20on%20various%0Abenchmarks%20without%20any%20supervision%20under%20both%20from-scratch%20and%20continuous%20RL%0Atraining%20setups.%20The%20code%20is%20at%20https%3A//github.com/Alibaba-Quark/SSP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearch%2520Self-play%253A%2520Pushing%2520the%2520Frontier%2520of%2520Agent%2520Capability%2520without%250A%2520%2520Supervision%26entry.906535625%3DHongliang%2520Lu%2520and%2520Yuhang%2520Wen%2520and%2520Pengyu%2520Cheng%2520and%2520Ruijin%2520Ding%2520and%2520Haotian%2520Xu%2520and%2520Jiaqi%2520Guo%2520and%2520Chutian%2520Wang%2520and%2520Haonan%2520Chen%2520and%2520Xiaoxi%2520Jiang%2520and%2520Guanjun%2520Jiang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520has%2520become%2520the%250Amainstream%2520technique%2520for%2520training%2520LLM%2520agents.%2520However%252C%2520RLVR%2520highly%2520depends%2520on%250Awell-crafted%2520task%2520queries%2520and%2520corresponding%2520ground-truth%2520answers%2520to%2520provide%250Aaccurate%2520rewards%252C%2520which%2520requires%2520massive%2520human%2520efforts%2520and%2520hinders%2520the%2520RL%250Ascaling%2520processes%252C%2520especially%2520under%2520agentic%2520scenarios.%2520Although%2520a%2520few%2520recent%250Aworks%2520explore%2520task%2520synthesis%2520methods%252C%2520the%2520difficulty%2520of%2520generated%2520agentic%2520tasks%250Acan%2520hardly%2520be%2520controlled%2520to%2520provide%2520effective%2520RL%2520training%2520advantages.%2520To%250Aachieve%2520agentic%2520RLVR%2520with%2520higher%2520scalability%252C%2520we%2520explore%2520self-play%2520training%2520for%250Adeep%2520search%2520agents%252C%2520in%2520which%2520the%2520learning%2520LLM%2520utilizes%2520multi-turn%2520search%2520engine%250Acalling%2520and%2520acts%2520simultaneously%2520as%2520both%2520a%2520task%2520proposer%2520and%2520a%2520problem%2520solver.%250AThe%2520task%2520proposer%2520aims%2520to%2520generate%2520deep%2520search%2520queries%2520with%2520well-defined%250Aground-truth%2520answers%2520and%2520increasing%2520task%2520difficulty.%2520The%2520problem%2520solver%2520tries%250Ato%2520handle%2520the%2520generated%2520search%2520queries%2520and%2520output%2520the%2520correct%2520answer%250Apredictions.%2520To%2520ensure%2520that%2520each%2520generated%2520search%2520query%2520has%2520accurate%2520ground%250Atruth%252C%2520we%2520collect%2520all%2520the%2520searching%2520results%2520from%2520the%2520proposer%2527s%2520trajectory%2520as%250Aexternal%2520knowledge%252C%2520then%2520conduct%2520retrieval-augmentation%2520generation%2520%2528RAG%2529%2520to%250Atest%2520whether%2520the%2520proposed%2520query%2520can%2520be%2520correctly%2520answered%2520with%2520all%2520necessary%250Asearch%2520documents%2520provided.%2520In%2520this%2520search%2520self-play%2520%2528SSP%2529%2520game%252C%2520the%2520proposer%250Aand%2520the%2520solver%2520co-evolve%2520their%2520agent%2520capabilities%2520through%2520both%2520competition%2520and%250Acooperation.%2520With%2520substantial%2520experimental%2520results%252C%2520we%2520find%2520that%2520SSP%2520can%250Asignificantly%2520improve%2520search%2520agents%2527%2520performance%2520uniformly%2520on%2520various%250Abenchmarks%2520without%2520any%2520supervision%2520under%2520both%2520from-scratch%2520and%2520continuous%2520RL%250Atraining%2520setups.%2520The%2520code%2520is%2520at%2520https%253A//github.com/Alibaba-Quark/SSP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search%20Self-play%3A%20Pushing%20the%20Frontier%20of%20Agent%20Capability%20without%0A%20%20Supervision&entry.906535625=Hongliang%20Lu%20and%20Yuhang%20Wen%20and%20Pengyu%20Cheng%20and%20Ruijin%20Ding%20and%20Haotian%20Xu%20and%20Jiaqi%20Guo%20and%20Chutian%20Wang%20and%20Haonan%20Chen%20and%20Xiaoxi%20Jiang%20and%20Guanjun%20Jiang&entry.1292438233=%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20become%20the%0Amainstream%20technique%20for%20training%20LLM%20agents.%20However%2C%20RLVR%20highly%20depends%20on%0Awell-crafted%20task%20queries%20and%20corresponding%20ground-truth%20answers%20to%20provide%0Aaccurate%20rewards%2C%20which%20requires%20massive%20human%20efforts%20and%20hinders%20the%20RL%0Ascaling%20processes%2C%20especially%20under%20agentic%20scenarios.%20Although%20a%20few%20recent%0Aworks%20explore%20task%20synthesis%20methods%2C%20the%20difficulty%20of%20generated%20agentic%20tasks%0Acan%20hardly%20be%20controlled%20to%20provide%20effective%20RL%20training%20advantages.%20To%0Aachieve%20agentic%20RLVR%20with%20higher%20scalability%2C%20we%20explore%20self-play%20training%20for%0Adeep%20search%20agents%2C%20in%20which%20the%20learning%20LLM%20utilizes%20multi-turn%20search%20engine%0Acalling%20and%20acts%20simultaneously%20as%20both%20a%20task%20proposer%20and%20a%20problem%20solver.%0AThe%20task%20proposer%20aims%20to%20generate%20deep%20search%20queries%20with%20well-defined%0Aground-truth%20answers%20and%20increasing%20task%20difficulty.%20The%20problem%20solver%20tries%0Ato%20handle%20the%20generated%20search%20queries%20and%20output%20the%20correct%20answer%0Apredictions.%20To%20ensure%20that%20each%20generated%20search%20query%20has%20accurate%20ground%0Atruth%2C%20we%20collect%20all%20the%20searching%20results%20from%20the%20proposer%27s%20trajectory%20as%0Aexternal%20knowledge%2C%20then%20conduct%20retrieval-augmentation%20generation%20%28RAG%29%20to%0Atest%20whether%20the%20proposed%20query%20can%20be%20correctly%20answered%20with%20all%20necessary%0Asearch%20documents%20provided.%20In%20this%20search%20self-play%20%28SSP%29%20game%2C%20the%20proposer%0Aand%20the%20solver%20co-evolve%20their%20agent%20capabilities%20through%20both%20competition%20and%0Acooperation.%20With%20substantial%20experimental%20results%2C%20we%20find%20that%20SSP%20can%0Asignificantly%20improve%20search%20agents%27%20performance%20uniformly%20on%20various%0Abenchmarks%20without%20any%20supervision%20under%20both%20from-scratch%20and%20continuous%20RL%0Atraining%20setups.%20The%20code%20is%20at%20https%3A//github.com/Alibaba-Quark/SSP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18821v1&entry.124074799=Read"},
{"title": "Increasing the Utility of Synthetic Images through Chamfer Guidance", "author": "Nicola Dall'Asen and Xiaofeng Zhang and Reyhane Askari Hemmat and Melissa Hall and Jakob Verbeek and Adriana Romero-Soriano and Michal Drozdzal", "abstract": "  Conditional image generative models hold considerable promise to produce\ninfinite amounts of synthetic training data. Yet, recent progress in generation\nquality has come at the expense of generation diversity, limiting the utility\nof these models as a source of synthetic training data. Although guidance-based\napproaches have been introduced to improve the utility of generated data by\nfocusing on quality or diversity, the (implicit or explicit) utility functions\noftentimes disregard the potential distribution shift between synthetic and\nreal data. In this work, we introduce Chamfer Guidance: a training-free\nguidance approach which leverages a handful of real exemplar images to\ncharacterize the quality and diversity of synthetic data. We show that by\nleveraging the proposed Chamfer Guidance, we can boost the diversity of the\ngenerations w.r.t. a dataset of real images while maintaining or improving the\ngeneration quality on ImageNet-1k and standard geo-diversity benchmarks. Our\napproach achieves state-of-the-art few-shot performance with as little as 2\nexemplar real images, obtaining 96.4% in terms of precision, and 86.4% in terms\nof distributional coverage, which increase to 97.5% and 92.7%, respectively,\nwhen using 32 real images. We showcase the benefits of the Chamfer Guidance\ngeneration by training downstream image classifiers on synthetic data,\nachieving accuracy boost of up to 15% for in-distribution over the baselines,\nand up to 16% in out-of-distribution. Furthermore, our approach does not\nrequire using the unconditional model, and thus obtains a 31% reduction in\nFLOPs w.r.t. classifier-free-guidance-based approaches at sampling time.\n", "link": "http://arxiv.org/abs/2508.10631v2", "date": "2025-10-21", "relevancy": 2.1664, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5797}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5389}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Increasing%20the%20Utility%20of%20Synthetic%20Images%20through%20Chamfer%20Guidance&body=Title%3A%20Increasing%20the%20Utility%20of%20Synthetic%20Images%20through%20Chamfer%20Guidance%0AAuthor%3A%20Nicola%20Dall%27Asen%20and%20Xiaofeng%20Zhang%20and%20Reyhane%20Askari%20Hemmat%20and%20Melissa%20Hall%20and%20Jakob%20Verbeek%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal%0AAbstract%3A%20%20%20Conditional%20image%20generative%20models%20hold%20considerable%20promise%20to%20produce%0Ainfinite%20amounts%20of%20synthetic%20training%20data.%20Yet%2C%20recent%20progress%20in%20generation%0Aquality%20has%20come%20at%20the%20expense%20of%20generation%20diversity%2C%20limiting%20the%20utility%0Aof%20these%20models%20as%20a%20source%20of%20synthetic%20training%20data.%20Although%20guidance-based%0Aapproaches%20have%20been%20introduced%20to%20improve%20the%20utility%20of%20generated%20data%20by%0Afocusing%20on%20quality%20or%20diversity%2C%20the%20%28implicit%20or%20explicit%29%20utility%20functions%0Aoftentimes%20disregard%20the%20potential%20distribution%20shift%20between%20synthetic%20and%0Areal%20data.%20In%20this%20work%2C%20we%20introduce%20Chamfer%20Guidance%3A%20a%20training-free%0Aguidance%20approach%20which%20leverages%20a%20handful%20of%20real%20exemplar%20images%20to%0Acharacterize%20the%20quality%20and%20diversity%20of%20synthetic%20data.%20We%20show%20that%20by%0Aleveraging%20the%20proposed%20Chamfer%20Guidance%2C%20we%20can%20boost%20the%20diversity%20of%20the%0Agenerations%20w.r.t.%20a%20dataset%20of%20real%20images%20while%20maintaining%20or%20improving%20the%0Ageneration%20quality%20on%20ImageNet-1k%20and%20standard%20geo-diversity%20benchmarks.%20Our%0Aapproach%20achieves%20state-of-the-art%20few-shot%20performance%20with%20as%20little%20as%202%0Aexemplar%20real%20images%2C%20obtaining%2096.4%25%20in%20terms%20of%20precision%2C%20and%2086.4%25%20in%20terms%0Aof%20distributional%20coverage%2C%20which%20increase%20to%2097.5%25%20and%2092.7%25%2C%20respectively%2C%0Awhen%20using%2032%20real%20images.%20We%20showcase%20the%20benefits%20of%20the%20Chamfer%20Guidance%0Ageneration%20by%20training%20downstream%20image%20classifiers%20on%20synthetic%20data%2C%0Aachieving%20accuracy%20boost%20of%20up%20to%2015%25%20for%20in-distribution%20over%20the%20baselines%2C%0Aand%20up%20to%2016%25%20in%20out-of-distribution.%20Furthermore%2C%20our%20approach%20does%20not%0Arequire%20using%20the%20unconditional%20model%2C%20and%20thus%20obtains%20a%2031%25%20reduction%20in%0AFLOPs%20w.r.t.%20classifier-free-guidance-based%20approaches%20at%20sampling%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncreasing%2520the%2520Utility%2520of%2520Synthetic%2520Images%2520through%2520Chamfer%2520Guidance%26entry.906535625%3DNicola%2520Dall%2527Asen%2520and%2520Xiaofeng%2520Zhang%2520and%2520Reyhane%2520Askari%2520Hemmat%2520and%2520Melissa%2520Hall%2520and%2520Jakob%2520Verbeek%2520and%2520Adriana%2520Romero-Soriano%2520and%2520Michal%2520Drozdzal%26entry.1292438233%3D%2520%2520Conditional%2520image%2520generative%2520models%2520hold%2520considerable%2520promise%2520to%2520produce%250Ainfinite%2520amounts%2520of%2520synthetic%2520training%2520data.%2520Yet%252C%2520recent%2520progress%2520in%2520generation%250Aquality%2520has%2520come%2520at%2520the%2520expense%2520of%2520generation%2520diversity%252C%2520limiting%2520the%2520utility%250Aof%2520these%2520models%2520as%2520a%2520source%2520of%2520synthetic%2520training%2520data.%2520Although%2520guidance-based%250Aapproaches%2520have%2520been%2520introduced%2520to%2520improve%2520the%2520utility%2520of%2520generated%2520data%2520by%250Afocusing%2520on%2520quality%2520or%2520diversity%252C%2520the%2520%2528implicit%2520or%2520explicit%2529%2520utility%2520functions%250Aoftentimes%2520disregard%2520the%2520potential%2520distribution%2520shift%2520between%2520synthetic%2520and%250Areal%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Chamfer%2520Guidance%253A%2520a%2520training-free%250Aguidance%2520approach%2520which%2520leverages%2520a%2520handful%2520of%2520real%2520exemplar%2520images%2520to%250Acharacterize%2520the%2520quality%2520and%2520diversity%2520of%2520synthetic%2520data.%2520We%2520show%2520that%2520by%250Aleveraging%2520the%2520proposed%2520Chamfer%2520Guidance%252C%2520we%2520can%2520boost%2520the%2520diversity%2520of%2520the%250Agenerations%2520w.r.t.%2520a%2520dataset%2520of%2520real%2520images%2520while%2520maintaining%2520or%2520improving%2520the%250Ageneration%2520quality%2520on%2520ImageNet-1k%2520and%2520standard%2520geo-diversity%2520benchmarks.%2520Our%250Aapproach%2520achieves%2520state-of-the-art%2520few-shot%2520performance%2520with%2520as%2520little%2520as%25202%250Aexemplar%2520real%2520images%252C%2520obtaining%252096.4%2525%2520in%2520terms%2520of%2520precision%252C%2520and%252086.4%2525%2520in%2520terms%250Aof%2520distributional%2520coverage%252C%2520which%2520increase%2520to%252097.5%2525%2520and%252092.7%2525%252C%2520respectively%252C%250Awhen%2520using%252032%2520real%2520images.%2520We%2520showcase%2520the%2520benefits%2520of%2520the%2520Chamfer%2520Guidance%250Ageneration%2520by%2520training%2520downstream%2520image%2520classifiers%2520on%2520synthetic%2520data%252C%250Aachieving%2520accuracy%2520boost%2520of%2520up%2520to%252015%2525%2520for%2520in-distribution%2520over%2520the%2520baselines%252C%250Aand%2520up%2520to%252016%2525%2520in%2520out-of-distribution.%2520Furthermore%252C%2520our%2520approach%2520does%2520not%250Arequire%2520using%2520the%2520unconditional%2520model%252C%2520and%2520thus%2520obtains%2520a%252031%2525%2520reduction%2520in%250AFLOPs%2520w.r.t.%2520classifier-free-guidance-based%2520approaches%2520at%2520sampling%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Increasing%20the%20Utility%20of%20Synthetic%20Images%20through%20Chamfer%20Guidance&entry.906535625=Nicola%20Dall%27Asen%20and%20Xiaofeng%20Zhang%20and%20Reyhane%20Askari%20Hemmat%20and%20Melissa%20Hall%20and%20Jakob%20Verbeek%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal&entry.1292438233=%20%20Conditional%20image%20generative%20models%20hold%20considerable%20promise%20to%20produce%0Ainfinite%20amounts%20of%20synthetic%20training%20data.%20Yet%2C%20recent%20progress%20in%20generation%0Aquality%20has%20come%20at%20the%20expense%20of%20generation%20diversity%2C%20limiting%20the%20utility%0Aof%20these%20models%20as%20a%20source%20of%20synthetic%20training%20data.%20Although%20guidance-based%0Aapproaches%20have%20been%20introduced%20to%20improve%20the%20utility%20of%20generated%20data%20by%0Afocusing%20on%20quality%20or%20diversity%2C%20the%20%28implicit%20or%20explicit%29%20utility%20functions%0Aoftentimes%20disregard%20the%20potential%20distribution%20shift%20between%20synthetic%20and%0Areal%20data.%20In%20this%20work%2C%20we%20introduce%20Chamfer%20Guidance%3A%20a%20training-free%0Aguidance%20approach%20which%20leverages%20a%20handful%20of%20real%20exemplar%20images%20to%0Acharacterize%20the%20quality%20and%20diversity%20of%20synthetic%20data.%20We%20show%20that%20by%0Aleveraging%20the%20proposed%20Chamfer%20Guidance%2C%20we%20can%20boost%20the%20diversity%20of%20the%0Agenerations%20w.r.t.%20a%20dataset%20of%20real%20images%20while%20maintaining%20or%20improving%20the%0Ageneration%20quality%20on%20ImageNet-1k%20and%20standard%20geo-diversity%20benchmarks.%20Our%0Aapproach%20achieves%20state-of-the-art%20few-shot%20performance%20with%20as%20little%20as%202%0Aexemplar%20real%20images%2C%20obtaining%2096.4%25%20in%20terms%20of%20precision%2C%20and%2086.4%25%20in%20terms%0Aof%20distributional%20coverage%2C%20which%20increase%20to%2097.5%25%20and%2092.7%25%2C%20respectively%2C%0Awhen%20using%2032%20real%20images.%20We%20showcase%20the%20benefits%20of%20the%20Chamfer%20Guidance%0Ageneration%20by%20training%20downstream%20image%20classifiers%20on%20synthetic%20data%2C%0Aachieving%20accuracy%20boost%20of%20up%20to%2015%25%20for%20in-distribution%20over%20the%20baselines%2C%0Aand%20up%20to%2016%25%20in%20out-of-distribution.%20Furthermore%2C%20our%20approach%20does%20not%0Arequire%20using%20the%20unconditional%20model%2C%20and%20thus%20obtains%20a%2031%25%20reduction%20in%0AFLOPs%20w.r.t.%20classifier-free-guidance-based%20approaches%20at%20sampling%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10631v2&entry.124074799=Read"},
{"title": "Counterfactual reasoning: an analysis of in-context emergence", "author": "Moritz Miller and Bernhard Sch\u00f6lkopf and Siyuan Guo", "abstract": "  Large-scale neural language models exhibit remarkable performance in\nin-context learning: the ability to learn and reason about the input context on\nthe fly. This work studies in-context counterfactual reasoning in language\nmodels, that is, the ability to predict consequences of a hypothetical\nscenario. We focus on a well-defined, synthetic linear regression task that\nrequires noise abduction. Accurate prediction is based on (1) inferring an\nunobserved latent concept and (2) copying contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning. Further, we enhance existing identifiability results and reduce\ncounterfactual reasoning for a broad class of functions to a transformation on\nin-context observations. In Transformers, we find that self-attention, model\ndepth and pre-training data diversity drive performance. Moreover, we provide\nmechanistic evidence that the latent concept is linearly represented in the\nresidual stream and we introduce designated \\textit{noise abduction heads}\ncentral to performing counterfactual reasoning. Lastly, our findings extend to\ncounterfactual reasoning under SDE dynamics and reflect that Transformers can\nperform noise abduction on sequential data, providing preliminary evidence on\nthe potential for counterfactual story generation. Our code is available under\nhttps://github.com/mrtzmllr/iccr.\n", "link": "http://arxiv.org/abs/2506.05188v2", "date": "2025-10-21", "relevancy": 2.1648, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20reasoning%3A%20an%20analysis%20of%20in-context%20emergence&body=Title%3A%20Counterfactual%20reasoning%3A%20an%20analysis%20of%20in-context%20emergence%0AAuthor%3A%20Moritz%20Miller%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Siyuan%20Guo%0AAbstract%3A%20%20%20Large-scale%20neural%20language%20models%20exhibit%20remarkable%20performance%20in%0Ain-context%20learning%3A%20the%20ability%20to%20learn%20and%20reason%20about%20the%20input%20context%20on%0Athe%20fly.%20This%20work%20studies%20in-context%20counterfactual%20reasoning%20in%20language%0Amodels%2C%20that%20is%2C%20the%20ability%20to%20predict%20consequences%20of%20a%20hypothetical%0Ascenario.%20We%20focus%20on%20a%20well-defined%2C%20synthetic%20linear%20regression%20task%20that%0Arequires%20noise%20abduction.%20Accurate%20prediction%20is%20based%20on%20%281%29%20inferring%20an%0Aunobserved%20latent%20concept%20and%20%282%29%20copying%20contextual%20noise%20from%20factual%0Aobservations.%20We%20show%20that%20language%20models%20are%20capable%20of%20counterfactual%0Areasoning.%20Further%2C%20we%20enhance%20existing%20identifiability%20results%20and%20reduce%0Acounterfactual%20reasoning%20for%20a%20broad%20class%20of%20functions%20to%20a%20transformation%20on%0Ain-context%20observations.%20In%20Transformers%2C%20we%20find%20that%20self-attention%2C%20model%0Adepth%20and%20pre-training%20data%20diversity%20drive%20performance.%20Moreover%2C%20we%20provide%0Amechanistic%20evidence%20that%20the%20latent%20concept%20is%20linearly%20represented%20in%20the%0Aresidual%20stream%20and%20we%20introduce%20designated%20%5Ctextit%7Bnoise%20abduction%20heads%7D%0Acentral%20to%20performing%20counterfactual%20reasoning.%20Lastly%2C%20our%20findings%20extend%20to%0Acounterfactual%20reasoning%20under%20SDE%20dynamics%20and%20reflect%20that%20Transformers%20can%0Aperform%20noise%20abduction%20on%20sequential%20data%2C%20providing%20preliminary%20evidence%20on%0Athe%20potential%20for%20counterfactual%20story%20generation.%20Our%20code%20is%20available%20under%0Ahttps%3A//github.com/mrtzmllr/iccr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520reasoning%253A%2520an%2520analysis%2520of%2520in-context%2520emergence%26entry.906535625%3DMoritz%2520Miller%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Siyuan%2520Guo%26entry.1292438233%3D%2520%2520Large-scale%2520neural%2520language%2520models%2520exhibit%2520remarkable%2520performance%2520in%250Ain-context%2520learning%253A%2520the%2520ability%2520to%2520learn%2520and%2520reason%2520about%2520the%2520input%2520context%2520on%250Athe%2520fly.%2520This%2520work%2520studies%2520in-context%2520counterfactual%2520reasoning%2520in%2520language%250Amodels%252C%2520that%2520is%252C%2520the%2520ability%2520to%2520predict%2520consequences%2520of%2520a%2520hypothetical%250Ascenario.%2520We%2520focus%2520on%2520a%2520well-defined%252C%2520synthetic%2520linear%2520regression%2520task%2520that%250Arequires%2520noise%2520abduction.%2520Accurate%2520prediction%2520is%2520based%2520on%2520%25281%2529%2520inferring%2520an%250Aunobserved%2520latent%2520concept%2520and%2520%25282%2529%2520copying%2520contextual%2520noise%2520from%2520factual%250Aobservations.%2520We%2520show%2520that%2520language%2520models%2520are%2520capable%2520of%2520counterfactual%250Areasoning.%2520Further%252C%2520we%2520enhance%2520existing%2520identifiability%2520results%2520and%2520reduce%250Acounterfactual%2520reasoning%2520for%2520a%2520broad%2520class%2520of%2520functions%2520to%2520a%2520transformation%2520on%250Ain-context%2520observations.%2520In%2520Transformers%252C%2520we%2520find%2520that%2520self-attention%252C%2520model%250Adepth%2520and%2520pre-training%2520data%2520diversity%2520drive%2520performance.%2520Moreover%252C%2520we%2520provide%250Amechanistic%2520evidence%2520that%2520the%2520latent%2520concept%2520is%2520linearly%2520represented%2520in%2520the%250Aresidual%2520stream%2520and%2520we%2520introduce%2520designated%2520%255Ctextit%257Bnoise%2520abduction%2520heads%257D%250Acentral%2520to%2520performing%2520counterfactual%2520reasoning.%2520Lastly%252C%2520our%2520findings%2520extend%2520to%250Acounterfactual%2520reasoning%2520under%2520SDE%2520dynamics%2520and%2520reflect%2520that%2520Transformers%2520can%250Aperform%2520noise%2520abduction%2520on%2520sequential%2520data%252C%2520providing%2520preliminary%2520evidence%2520on%250Athe%2520potential%2520for%2520counterfactual%2520story%2520generation.%2520Our%2520code%2520is%2520available%2520under%250Ahttps%253A//github.com/mrtzmllr/iccr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20reasoning%3A%20an%20analysis%20of%20in-context%20emergence&entry.906535625=Moritz%20Miller%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Siyuan%20Guo&entry.1292438233=%20%20Large-scale%20neural%20language%20models%20exhibit%20remarkable%20performance%20in%0Ain-context%20learning%3A%20the%20ability%20to%20learn%20and%20reason%20about%20the%20input%20context%20on%0Athe%20fly.%20This%20work%20studies%20in-context%20counterfactual%20reasoning%20in%20language%0Amodels%2C%20that%20is%2C%20the%20ability%20to%20predict%20consequences%20of%20a%20hypothetical%0Ascenario.%20We%20focus%20on%20a%20well-defined%2C%20synthetic%20linear%20regression%20task%20that%0Arequires%20noise%20abduction.%20Accurate%20prediction%20is%20based%20on%20%281%29%20inferring%20an%0Aunobserved%20latent%20concept%20and%20%282%29%20copying%20contextual%20noise%20from%20factual%0Aobservations.%20We%20show%20that%20language%20models%20are%20capable%20of%20counterfactual%0Areasoning.%20Further%2C%20we%20enhance%20existing%20identifiability%20results%20and%20reduce%0Acounterfactual%20reasoning%20for%20a%20broad%20class%20of%20functions%20to%20a%20transformation%20on%0Ain-context%20observations.%20In%20Transformers%2C%20we%20find%20that%20self-attention%2C%20model%0Adepth%20and%20pre-training%20data%20diversity%20drive%20performance.%20Moreover%2C%20we%20provide%0Amechanistic%20evidence%20that%20the%20latent%20concept%20is%20linearly%20represented%20in%20the%0Aresidual%20stream%20and%20we%20introduce%20designated%20%5Ctextit%7Bnoise%20abduction%20heads%7D%0Acentral%20to%20performing%20counterfactual%20reasoning.%20Lastly%2C%20our%20findings%20extend%20to%0Acounterfactual%20reasoning%20under%20SDE%20dynamics%20and%20reflect%20that%20Transformers%20can%0Aperform%20noise%20abduction%20on%20sequential%20data%2C%20providing%20preliminary%20evidence%20on%0Athe%20potential%20for%20counterfactual%20story%20generation.%20Our%20code%20is%20available%20under%0Ahttps%3A//github.com/mrtzmllr/iccr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05188v2&entry.124074799=Read"},
{"title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization", "author": "Yuanli Wu and Long Zhang and Yue Du and Bin Li", "abstract": "  With video exploding across social media, surveillance, and education,\ncompressing long footage into concise yet faithful surrogates is crucial.\nSupervised methods learn frame/shot importance from dense labels and excel\nin-domain, but are costly and brittle across datasets; unsupervised methods\navoid labels but often miss high-level semantics and narrative cues. Recent\nzero-shot pipelines use LLMs for training-free summarization, yet remain\nsensitive to handcrafted prompts and dataset-specific normalization.We propose\na rubric-guided, pseudo-labeled prompting framework. A small subset of human\nannotations is converted into high-confidence pseudo labels and aggregated into\nstructured, dataset-adaptive scoring rubrics for interpretable scene\nevaluation. At inference, boundary scenes (first/last) are scored from their\nown descriptions, while intermediate scenes include brief summaries of adjacent\nsegments to assess progression and redundancy, enabling the LLM to balance\nlocal salience with global coherence without parameter tuning.Across three\nbenchmarks, our method is consistently effective. On SumMe and TVSum it\nachieves F1 of 57.58 and 63.05, surpassing a zero-shot baseline (56.73, 62.21)\nby +0.85 and +0.84 and approaching supervised performance. On the query-focused\nQFVS benchmark it attains 53.79 F1, beating 53.42 by +0.37 and remaining stable\nacross validation videos. These results show that rubric-guided pseudo\nlabeling, coupled with contextual prompting, stabilizes LLM-based scoring and\nyields a general, interpretable zero-shot paradigm for both generic and\nquery-focused video summarization.\n", "link": "http://arxiv.org/abs/2510.17501v2", "date": "2025-10-21", "relevancy": 2.1631, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Pseudo-Label%20Scoring%20for%20Zero-Shot%20Video%20Summarization&body=Title%3A%20Context-Aware%20Pseudo-Label%20Scoring%20for%20Zero-Shot%20Video%20Summarization%0AAuthor%3A%20Yuanli%20Wu%20and%20Long%20Zhang%20and%20Yue%20Du%20and%20Bin%20Li%0AAbstract%3A%20%20%20With%20video%20exploding%20across%20social%20media%2C%20surveillance%2C%20and%20education%2C%0Acompressing%20long%20footage%20into%20concise%20yet%20faithful%20surrogates%20is%20crucial.%0ASupervised%20methods%20learn%20frame/shot%20importance%20from%20dense%20labels%20and%20excel%0Ain-domain%2C%20but%20are%20costly%20and%20brittle%20across%20datasets%3B%20unsupervised%20methods%0Aavoid%20labels%20but%20often%20miss%20high-level%20semantics%20and%20narrative%20cues.%20Recent%0Azero-shot%20pipelines%20use%20LLMs%20for%20training-free%20summarization%2C%20yet%20remain%0Asensitive%20to%20handcrafted%20prompts%20and%20dataset-specific%20normalization.We%20propose%0Aa%20rubric-guided%2C%20pseudo-labeled%20prompting%20framework.%20A%20small%20subset%20of%20human%0Aannotations%20is%20converted%20into%20high-confidence%20pseudo%20labels%20and%20aggregated%20into%0Astructured%2C%20dataset-adaptive%20scoring%20rubrics%20for%20interpretable%20scene%0Aevaluation.%20At%20inference%2C%20boundary%20scenes%20%28first/last%29%20are%20scored%20from%20their%0Aown%20descriptions%2C%20while%20intermediate%20scenes%20include%20brief%20summaries%20of%20adjacent%0Asegments%20to%20assess%20progression%20and%20redundancy%2C%20enabling%20the%20LLM%20to%20balance%0Alocal%20salience%20with%20global%20coherence%20without%20parameter%20tuning.Across%20three%0Abenchmarks%2C%20our%20method%20is%20consistently%20effective.%20On%20SumMe%20and%20TVSum%20it%0Aachieves%20F1%20of%2057.58%20and%2063.05%2C%20surpassing%20a%20zero-shot%20baseline%20%2856.73%2C%2062.21%29%0Aby%20%2B0.85%20and%20%2B0.84%20and%20approaching%20supervised%20performance.%20On%20the%20query-focused%0AQFVS%20benchmark%20it%20attains%2053.79%20F1%2C%20beating%2053.42%20by%20%2B0.37%20and%20remaining%20stable%0Aacross%20validation%20videos.%20These%20results%20show%20that%20rubric-guided%20pseudo%0Alabeling%2C%20coupled%20with%20contextual%20prompting%2C%20stabilizes%20LLM-based%20scoring%20and%0Ayields%20a%20general%2C%20interpretable%20zero-shot%20paradigm%20for%20both%20generic%20and%0Aquery-focused%20video%20summarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Pseudo-Label%2520Scoring%2520for%2520Zero-Shot%2520Video%2520Summarization%26entry.906535625%3DYuanli%2520Wu%2520and%2520Long%2520Zhang%2520and%2520Yue%2520Du%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520With%2520video%2520exploding%2520across%2520social%2520media%252C%2520surveillance%252C%2520and%2520education%252C%250Acompressing%2520long%2520footage%2520into%2520concise%2520yet%2520faithful%2520surrogates%2520is%2520crucial.%250ASupervised%2520methods%2520learn%2520frame/shot%2520importance%2520from%2520dense%2520labels%2520and%2520excel%250Ain-domain%252C%2520but%2520are%2520costly%2520and%2520brittle%2520across%2520datasets%253B%2520unsupervised%2520methods%250Aavoid%2520labels%2520but%2520often%2520miss%2520high-level%2520semantics%2520and%2520narrative%2520cues.%2520Recent%250Azero-shot%2520pipelines%2520use%2520LLMs%2520for%2520training-free%2520summarization%252C%2520yet%2520remain%250Asensitive%2520to%2520handcrafted%2520prompts%2520and%2520dataset-specific%2520normalization.We%2520propose%250Aa%2520rubric-guided%252C%2520pseudo-labeled%2520prompting%2520framework.%2520A%2520small%2520subset%2520of%2520human%250Aannotations%2520is%2520converted%2520into%2520high-confidence%2520pseudo%2520labels%2520and%2520aggregated%2520into%250Astructured%252C%2520dataset-adaptive%2520scoring%2520rubrics%2520for%2520interpretable%2520scene%250Aevaluation.%2520At%2520inference%252C%2520boundary%2520scenes%2520%2528first/last%2529%2520are%2520scored%2520from%2520their%250Aown%2520descriptions%252C%2520while%2520intermediate%2520scenes%2520include%2520brief%2520summaries%2520of%2520adjacent%250Asegments%2520to%2520assess%2520progression%2520and%2520redundancy%252C%2520enabling%2520the%2520LLM%2520to%2520balance%250Alocal%2520salience%2520with%2520global%2520coherence%2520without%2520parameter%2520tuning.Across%2520three%250Abenchmarks%252C%2520our%2520method%2520is%2520consistently%2520effective.%2520On%2520SumMe%2520and%2520TVSum%2520it%250Aachieves%2520F1%2520of%252057.58%2520and%252063.05%252C%2520surpassing%2520a%2520zero-shot%2520baseline%2520%252856.73%252C%252062.21%2529%250Aby%2520%252B0.85%2520and%2520%252B0.84%2520and%2520approaching%2520supervised%2520performance.%2520On%2520the%2520query-focused%250AQFVS%2520benchmark%2520it%2520attains%252053.79%2520F1%252C%2520beating%252053.42%2520by%2520%252B0.37%2520and%2520remaining%2520stable%250Aacross%2520validation%2520videos.%2520These%2520results%2520show%2520that%2520rubric-guided%2520pseudo%250Alabeling%252C%2520coupled%2520with%2520contextual%2520prompting%252C%2520stabilizes%2520LLM-based%2520scoring%2520and%250Ayields%2520a%2520general%252C%2520interpretable%2520zero-shot%2520paradigm%2520for%2520both%2520generic%2520and%250Aquery-focused%2520video%2520summarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Pseudo-Label%20Scoring%20for%20Zero-Shot%20Video%20Summarization&entry.906535625=Yuanli%20Wu%20and%20Long%20Zhang%20and%20Yue%20Du%20and%20Bin%20Li&entry.1292438233=%20%20With%20video%20exploding%20across%20social%20media%2C%20surveillance%2C%20and%20education%2C%0Acompressing%20long%20footage%20into%20concise%20yet%20faithful%20surrogates%20is%20crucial.%0ASupervised%20methods%20learn%20frame/shot%20importance%20from%20dense%20labels%20and%20excel%0Ain-domain%2C%20but%20are%20costly%20and%20brittle%20across%20datasets%3B%20unsupervised%20methods%0Aavoid%20labels%20but%20often%20miss%20high-level%20semantics%20and%20narrative%20cues.%20Recent%0Azero-shot%20pipelines%20use%20LLMs%20for%20training-free%20summarization%2C%20yet%20remain%0Asensitive%20to%20handcrafted%20prompts%20and%20dataset-specific%20normalization.We%20propose%0Aa%20rubric-guided%2C%20pseudo-labeled%20prompting%20framework.%20A%20small%20subset%20of%20human%0Aannotations%20is%20converted%20into%20high-confidence%20pseudo%20labels%20and%20aggregated%20into%0Astructured%2C%20dataset-adaptive%20scoring%20rubrics%20for%20interpretable%20scene%0Aevaluation.%20At%20inference%2C%20boundary%20scenes%20%28first/last%29%20are%20scored%20from%20their%0Aown%20descriptions%2C%20while%20intermediate%20scenes%20include%20brief%20summaries%20of%20adjacent%0Asegments%20to%20assess%20progression%20and%20redundancy%2C%20enabling%20the%20LLM%20to%20balance%0Alocal%20salience%20with%20global%20coherence%20without%20parameter%20tuning.Across%20three%0Abenchmarks%2C%20our%20method%20is%20consistently%20effective.%20On%20SumMe%20and%20TVSum%20it%0Aachieves%20F1%20of%2057.58%20and%2063.05%2C%20surpassing%20a%20zero-shot%20baseline%20%2856.73%2C%2062.21%29%0Aby%20%2B0.85%20and%20%2B0.84%20and%20approaching%20supervised%20performance.%20On%20the%20query-focused%0AQFVS%20benchmark%20it%20attains%2053.79%20F1%2C%20beating%2053.42%20by%20%2B0.37%20and%20remaining%20stable%0Aacross%20validation%20videos.%20These%20results%20show%20that%20rubric-guided%20pseudo%0Alabeling%2C%20coupled%20with%20contextual%20prompting%2C%20stabilizes%20LLM-based%20scoring%20and%0Ayields%20a%20general%2C%20interpretable%20zero-shot%20paradigm%20for%20both%20generic%20and%0Aquery-focused%20video%20summarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17501v2&entry.124074799=Read"},
{"title": "Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning\n  with Prediction Augmentation", "author": "Muqun Hu and Wenxi Chen and Wenjing Li and Falak Mandali and Zijian He and Renhong Zhang and Praveen Krisna and Katherine Christian and Leo Benaharon and Dizhi Ma and Karthik Ramani and Yan Gu", "abstract": "  Humanoid table tennis (TT) demands rapid perception, proactive whole-body\nmotion, and agile footwork under strict timing -- capabilities that remain\ndifficult for unified controllers. We propose a reinforcement learning\nframework that maps ball-position observations directly to whole-body joint\ncommands for both arm striking and leg locomotion, strengthened by predictive\nsignals and dense, physics-guided rewards. A lightweight learned predictor, fed\nwith recent ball positions, estimates future ball states and augments the\npolicy's observations for proactive decision-making. During training, a\nphysics-based predictor supplies precise future states to construct dense,\ninformative rewards that lead to effective exploration. The resulting policy\nattains strong performance across varied serve ranges (hit rate $\\geq$ 96% and\nsuccess rate $\\geq$ 92%) in simulations. Ablation studies confirm that both the\nlearned predictor and the predictive reward design are critical for end-to-end\nlearning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute\njoints, the policy produces coordinated lateral and forward-backward footwork\nwith accurate, fast returns, suggesting a practical path toward versatile,\ncompetitive humanoid TT.\n", "link": "http://arxiv.org/abs/2509.21690v2", "date": "2025-10-21", "relevancy": 2.1617, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5803}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5424}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Versatile%20Humanoid%20Table%20Tennis%3A%20Unified%20Reinforcement%20Learning%0A%20%20with%20Prediction%20Augmentation&body=Title%3A%20Towards%20Versatile%20Humanoid%20Table%20Tennis%3A%20Unified%20Reinforcement%20Learning%0A%20%20with%20Prediction%20Augmentation%0AAuthor%3A%20Muqun%20Hu%20and%20Wenxi%20Chen%20and%20Wenjing%20Li%20and%20Falak%20Mandali%20and%20Zijian%20He%20and%20Renhong%20Zhang%20and%20Praveen%20Krisna%20and%20Katherine%20Christian%20and%20Leo%20Benaharon%20and%20Dizhi%20Ma%20and%20Karthik%20Ramani%20and%20Yan%20Gu%0AAbstract%3A%20%20%20Humanoid%20table%20tennis%20%28TT%29%20demands%20rapid%20perception%2C%20proactive%20whole-body%0Amotion%2C%20and%20agile%20footwork%20under%20strict%20timing%20--%20capabilities%20that%20remain%0Adifficult%20for%20unified%20controllers.%20We%20propose%20a%20reinforcement%20learning%0Aframework%20that%20maps%20ball-position%20observations%20directly%20to%20whole-body%20joint%0Acommands%20for%20both%20arm%20striking%20and%20leg%20locomotion%2C%20strengthened%20by%20predictive%0Asignals%20and%20dense%2C%20physics-guided%20rewards.%20A%20lightweight%20learned%20predictor%2C%20fed%0Awith%20recent%20ball%20positions%2C%20estimates%20future%20ball%20states%20and%20augments%20the%0Apolicy%27s%20observations%20for%20proactive%20decision-making.%20During%20training%2C%20a%0Aphysics-based%20predictor%20supplies%20precise%20future%20states%20to%20construct%20dense%2C%0Ainformative%20rewards%20that%20lead%20to%20effective%20exploration.%20The%20resulting%20policy%0Aattains%20strong%20performance%20across%20varied%20serve%20ranges%20%28hit%20rate%20%24%5Cgeq%24%2096%25%20and%0Asuccess%20rate%20%24%5Cgeq%24%2092%25%29%20in%20simulations.%20Ablation%20studies%20confirm%20that%20both%20the%0Alearned%20predictor%20and%20the%20predictive%20reward%20design%20are%20critical%20for%20end-to-end%0Alearning.%20Deployed%20zero-shot%20on%20a%20physical%20Booster%20T1%20humanoid%20with%2023%20revolute%0Ajoints%2C%20the%20policy%20produces%20coordinated%20lateral%20and%20forward-backward%20footwork%0Awith%20accurate%2C%20fast%20returns%2C%20suggesting%20a%20practical%20path%20toward%20versatile%2C%0Acompetitive%20humanoid%20TT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Versatile%2520Humanoid%2520Table%2520Tennis%253A%2520Unified%2520Reinforcement%2520Learning%250A%2520%2520with%2520Prediction%2520Augmentation%26entry.906535625%3DMuqun%2520Hu%2520and%2520Wenxi%2520Chen%2520and%2520Wenjing%2520Li%2520and%2520Falak%2520Mandali%2520and%2520Zijian%2520He%2520and%2520Renhong%2520Zhang%2520and%2520Praveen%2520Krisna%2520and%2520Katherine%2520Christian%2520and%2520Leo%2520Benaharon%2520and%2520Dizhi%2520Ma%2520and%2520Karthik%2520Ramani%2520and%2520Yan%2520Gu%26entry.1292438233%3D%2520%2520Humanoid%2520table%2520tennis%2520%2528TT%2529%2520demands%2520rapid%2520perception%252C%2520proactive%2520whole-body%250Amotion%252C%2520and%2520agile%2520footwork%2520under%2520strict%2520timing%2520--%2520capabilities%2520that%2520remain%250Adifficult%2520for%2520unified%2520controllers.%2520We%2520propose%2520a%2520reinforcement%2520learning%250Aframework%2520that%2520maps%2520ball-position%2520observations%2520directly%2520to%2520whole-body%2520joint%250Acommands%2520for%2520both%2520arm%2520striking%2520and%2520leg%2520locomotion%252C%2520strengthened%2520by%2520predictive%250Asignals%2520and%2520dense%252C%2520physics-guided%2520rewards.%2520A%2520lightweight%2520learned%2520predictor%252C%2520fed%250Awith%2520recent%2520ball%2520positions%252C%2520estimates%2520future%2520ball%2520states%2520and%2520augments%2520the%250Apolicy%2527s%2520observations%2520for%2520proactive%2520decision-making.%2520During%2520training%252C%2520a%250Aphysics-based%2520predictor%2520supplies%2520precise%2520future%2520states%2520to%2520construct%2520dense%252C%250Ainformative%2520rewards%2520that%2520lead%2520to%2520effective%2520exploration.%2520The%2520resulting%2520policy%250Aattains%2520strong%2520performance%2520across%2520varied%2520serve%2520ranges%2520%2528hit%2520rate%2520%2524%255Cgeq%2524%252096%2525%2520and%250Asuccess%2520rate%2520%2524%255Cgeq%2524%252092%2525%2529%2520in%2520simulations.%2520Ablation%2520studies%2520confirm%2520that%2520both%2520the%250Alearned%2520predictor%2520and%2520the%2520predictive%2520reward%2520design%2520are%2520critical%2520for%2520end-to-end%250Alearning.%2520Deployed%2520zero-shot%2520on%2520a%2520physical%2520Booster%2520T1%2520humanoid%2520with%252023%2520revolute%250Ajoints%252C%2520the%2520policy%2520produces%2520coordinated%2520lateral%2520and%2520forward-backward%2520footwork%250Awith%2520accurate%252C%2520fast%2520returns%252C%2520suggesting%2520a%2520practical%2520path%2520toward%2520versatile%252C%250Acompetitive%2520humanoid%2520TT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Versatile%20Humanoid%20Table%20Tennis%3A%20Unified%20Reinforcement%20Learning%0A%20%20with%20Prediction%20Augmentation&entry.906535625=Muqun%20Hu%20and%20Wenxi%20Chen%20and%20Wenjing%20Li%20and%20Falak%20Mandali%20and%20Zijian%20He%20and%20Renhong%20Zhang%20and%20Praveen%20Krisna%20and%20Katherine%20Christian%20and%20Leo%20Benaharon%20and%20Dizhi%20Ma%20and%20Karthik%20Ramani%20and%20Yan%20Gu&entry.1292438233=%20%20Humanoid%20table%20tennis%20%28TT%29%20demands%20rapid%20perception%2C%20proactive%20whole-body%0Amotion%2C%20and%20agile%20footwork%20under%20strict%20timing%20--%20capabilities%20that%20remain%0Adifficult%20for%20unified%20controllers.%20We%20propose%20a%20reinforcement%20learning%0Aframework%20that%20maps%20ball-position%20observations%20directly%20to%20whole-body%20joint%0Acommands%20for%20both%20arm%20striking%20and%20leg%20locomotion%2C%20strengthened%20by%20predictive%0Asignals%20and%20dense%2C%20physics-guided%20rewards.%20A%20lightweight%20learned%20predictor%2C%20fed%0Awith%20recent%20ball%20positions%2C%20estimates%20future%20ball%20states%20and%20augments%20the%0Apolicy%27s%20observations%20for%20proactive%20decision-making.%20During%20training%2C%20a%0Aphysics-based%20predictor%20supplies%20precise%20future%20states%20to%20construct%20dense%2C%0Ainformative%20rewards%20that%20lead%20to%20effective%20exploration.%20The%20resulting%20policy%0Aattains%20strong%20performance%20across%20varied%20serve%20ranges%20%28hit%20rate%20%24%5Cgeq%24%2096%25%20and%0Asuccess%20rate%20%24%5Cgeq%24%2092%25%29%20in%20simulations.%20Ablation%20studies%20confirm%20that%20both%20the%0Alearned%20predictor%20and%20the%20predictive%20reward%20design%20are%20critical%20for%20end-to-end%0Alearning.%20Deployed%20zero-shot%20on%20a%20physical%20Booster%20T1%20humanoid%20with%2023%20revolute%0Ajoints%2C%20the%20policy%20produces%20coordinated%20lateral%20and%20forward-backward%20footwork%0Awith%20accurate%2C%20fast%20returns%2C%20suggesting%20a%20practical%20path%20toward%20versatile%2C%0Acompetitive%20humanoid%20TT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21690v2&entry.124074799=Read"},
{"title": "Correct-Detect: Balancing Performance and Ambiguity Through the Lens of\n  Coreference Resolution in LLMs", "author": "Amber Shore and Russell Scheinberg and Ameeta Agrawal and So Young Lee", "abstract": "  Large Language Models (LLMs) are intended to reflect human linguistic\ncompetencies. But humans have access to a broad and embodied context, which is\nkey in detecting and resolving linguistic ambiguities, even in isolated text\nspans. A foundational case of semantic ambiguity is found in the task of\ncoreference resolution: how is a pronoun related to an earlier person mention?\nThis capability is implicit in nearly every downstream task, and the presence\nof ambiguity at this level can alter performance significantly. We show that\nLLMs can achieve good performance with minimal prompting in both coreference\ndisambiguation and the detection of ambiguity in coreference, however, they\ncannot do both at the same time. We present the CORRECT-DETECT trade-off:\nthough models have both capabilities and deploy them implicitly, successful\nperformance balancing these two abilities remains elusive.\n", "link": "http://arxiv.org/abs/2509.14456v2", "date": "2025-10-21", "relevancy": 2.137, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correct-Detect%3A%20Balancing%20Performance%20and%20Ambiguity%20Through%20the%20Lens%20of%0A%20%20Coreference%20Resolution%20in%20LLMs&body=Title%3A%20Correct-Detect%3A%20Balancing%20Performance%20and%20Ambiguity%20Through%20the%20Lens%20of%0A%20%20Coreference%20Resolution%20in%20LLMs%0AAuthor%3A%20Amber%20Shore%20and%20Russell%20Scheinberg%20and%20Ameeta%20Agrawal%20and%20So%20Young%20Lee%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20intended%20to%20reflect%20human%20linguistic%0Acompetencies.%20But%20humans%20have%20access%20to%20a%20broad%20and%20embodied%20context%2C%20which%20is%0Akey%20in%20detecting%20and%20resolving%20linguistic%20ambiguities%2C%20even%20in%20isolated%20text%0Aspans.%20A%20foundational%20case%20of%20semantic%20ambiguity%20is%20found%20in%20the%20task%20of%0Acoreference%20resolution%3A%20how%20is%20a%20pronoun%20related%20to%20an%20earlier%20person%20mention%3F%0AThis%20capability%20is%20implicit%20in%20nearly%20every%20downstream%20task%2C%20and%20the%20presence%0Aof%20ambiguity%20at%20this%20level%20can%20alter%20performance%20significantly.%20We%20show%20that%0ALLMs%20can%20achieve%20good%20performance%20with%20minimal%20prompting%20in%20both%20coreference%0Adisambiguation%20and%20the%20detection%20of%20ambiguity%20in%20coreference%2C%20however%2C%20they%0Acannot%20do%20both%20at%20the%20same%20time.%20We%20present%20the%20CORRECT-DETECT%20trade-off%3A%0Athough%20models%20have%20both%20capabilities%20and%20deploy%20them%20implicitly%2C%20successful%0Aperformance%20balancing%20these%20two%20abilities%20remains%20elusive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrect-Detect%253A%2520Balancing%2520Performance%2520and%2520Ambiguity%2520Through%2520the%2520Lens%2520of%250A%2520%2520Coreference%2520Resolution%2520in%2520LLMs%26entry.906535625%3DAmber%2520Shore%2520and%2520Russell%2520Scheinberg%2520and%2520Ameeta%2520Agrawal%2520and%2520So%2520Young%2520Lee%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520intended%2520to%2520reflect%2520human%2520linguistic%250Acompetencies.%2520But%2520humans%2520have%2520access%2520to%2520a%2520broad%2520and%2520embodied%2520context%252C%2520which%2520is%250Akey%2520in%2520detecting%2520and%2520resolving%2520linguistic%2520ambiguities%252C%2520even%2520in%2520isolated%2520text%250Aspans.%2520A%2520foundational%2520case%2520of%2520semantic%2520ambiguity%2520is%2520found%2520in%2520the%2520task%2520of%250Acoreference%2520resolution%253A%2520how%2520is%2520a%2520pronoun%2520related%2520to%2520an%2520earlier%2520person%2520mention%253F%250AThis%2520capability%2520is%2520implicit%2520in%2520nearly%2520every%2520downstream%2520task%252C%2520and%2520the%2520presence%250Aof%2520ambiguity%2520at%2520this%2520level%2520can%2520alter%2520performance%2520significantly.%2520We%2520show%2520that%250ALLMs%2520can%2520achieve%2520good%2520performance%2520with%2520minimal%2520prompting%2520in%2520both%2520coreference%250Adisambiguation%2520and%2520the%2520detection%2520of%2520ambiguity%2520in%2520coreference%252C%2520however%252C%2520they%250Acannot%2520do%2520both%2520at%2520the%2520same%2520time.%2520We%2520present%2520the%2520CORRECT-DETECT%2520trade-off%253A%250Athough%2520models%2520have%2520both%2520capabilities%2520and%2520deploy%2520them%2520implicitly%252C%2520successful%250Aperformance%2520balancing%2520these%2520two%2520abilities%2520remains%2520elusive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correct-Detect%3A%20Balancing%20Performance%20and%20Ambiguity%20Through%20the%20Lens%20of%0A%20%20Coreference%20Resolution%20in%20LLMs&entry.906535625=Amber%20Shore%20and%20Russell%20Scheinberg%20and%20Ameeta%20Agrawal%20and%20So%20Young%20Lee&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20intended%20to%20reflect%20human%20linguistic%0Acompetencies.%20But%20humans%20have%20access%20to%20a%20broad%20and%20embodied%20context%2C%20which%20is%0Akey%20in%20detecting%20and%20resolving%20linguistic%20ambiguities%2C%20even%20in%20isolated%20text%0Aspans.%20A%20foundational%20case%20of%20semantic%20ambiguity%20is%20found%20in%20the%20task%20of%0Acoreference%20resolution%3A%20how%20is%20a%20pronoun%20related%20to%20an%20earlier%20person%20mention%3F%0AThis%20capability%20is%20implicit%20in%20nearly%20every%20downstream%20task%2C%20and%20the%20presence%0Aof%20ambiguity%20at%20this%20level%20can%20alter%20performance%20significantly.%20We%20show%20that%0ALLMs%20can%20achieve%20good%20performance%20with%20minimal%20prompting%20in%20both%20coreference%0Adisambiguation%20and%20the%20detection%20of%20ambiguity%20in%20coreference%2C%20however%2C%20they%0Acannot%20do%20both%20at%20the%20same%20time.%20We%20present%20the%20CORRECT-DETECT%20trade-off%3A%0Athough%20models%20have%20both%20capabilities%20and%20deploy%20them%20implicitly%2C%20successful%0Aperformance%20balancing%20these%20two%20abilities%20remains%20elusive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14456v2&entry.124074799=Read"},
{"title": "Sharing the Load: Distributed Model-Predictive Control for Precise\n  Multi-Rover Cargo Transport", "author": "Alexander Krawciw and Sven Lilge and Luka Antonyshyn and Timothy D. Barfoot", "abstract": "  For autonomous cargo transportation, teams of mobile robots can provide more\noperational flexibility than a single large robot. In these scenarios,\nprecision in both inter-vehicle distance and path tracking is key. With this\nmotivation, we develop a distributed model-predictive controller (MPC) for\nmulti-vehicle cargo operations that builds on the precise path-tracking of\nlidar teach and repeat. To carry cargo, a following vehicle must maintain a\nEuclidean distance offset from a lead vehicle regardless of the path curvature.\nOur approach uses a shared map to localize the robots relative to each other\nwithout GNSS or direct observations. We compare our approach to a centralized\nMPC and a baseline approach that directly measures the inter-vehicle distance.\nThe distributed MPC shows equivalent nominal performance to the more complex\ncentralized MPC. Using a direct measurement of the relative distance between\nthe leader and follower shows improved tracking performance in close-range\nscenarios but struggles with long-range offsets. The operational flexibility\nprovided by distributing the computation makes it well suited for real\ndeployments. We evaluate four types of convoyed path trackers with over 10 km\nof driving in a coupled convoy. With convoys of two and three rovers, the\nproposed distributed MPC method works in real-time to allow map-based convoying\nto maintain maximum spacing within 20 cm of the target in various conditions.\n", "link": "http://arxiv.org/abs/2510.18766v1", "date": "2025-10-21", "relevancy": 2.1293, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.568}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5436}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharing%20the%20Load%3A%20Distributed%20Model-Predictive%20Control%20for%20Precise%0A%20%20Multi-Rover%20Cargo%20Transport&body=Title%3A%20Sharing%20the%20Load%3A%20Distributed%20Model-Predictive%20Control%20for%20Precise%0A%20%20Multi-Rover%20Cargo%20Transport%0AAuthor%3A%20Alexander%20Krawciw%20and%20Sven%20Lilge%20and%20Luka%20Antonyshyn%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20For%20autonomous%20cargo%20transportation%2C%20teams%20of%20mobile%20robots%20can%20provide%20more%0Aoperational%20flexibility%20than%20a%20single%20large%20robot.%20In%20these%20scenarios%2C%0Aprecision%20in%20both%20inter-vehicle%20distance%20and%20path%20tracking%20is%20key.%20With%20this%0Amotivation%2C%20we%20develop%20a%20distributed%20model-predictive%20controller%20%28MPC%29%20for%0Amulti-vehicle%20cargo%20operations%20that%20builds%20on%20the%20precise%20path-tracking%20of%0Alidar%20teach%20and%20repeat.%20To%20carry%20cargo%2C%20a%20following%20vehicle%20must%20maintain%20a%0AEuclidean%20distance%20offset%20from%20a%20lead%20vehicle%20regardless%20of%20the%20path%20curvature.%0AOur%20approach%20uses%20a%20shared%20map%20to%20localize%20the%20robots%20relative%20to%20each%20other%0Awithout%20GNSS%20or%20direct%20observations.%20We%20compare%20our%20approach%20to%20a%20centralized%0AMPC%20and%20a%20baseline%20approach%20that%20directly%20measures%20the%20inter-vehicle%20distance.%0AThe%20distributed%20MPC%20shows%20equivalent%20nominal%20performance%20to%20the%20more%20complex%0Acentralized%20MPC.%20Using%20a%20direct%20measurement%20of%20the%20relative%20distance%20between%0Athe%20leader%20and%20follower%20shows%20improved%20tracking%20performance%20in%20close-range%0Ascenarios%20but%20struggles%20with%20long-range%20offsets.%20The%20operational%20flexibility%0Aprovided%20by%20distributing%20the%20computation%20makes%20it%20well%20suited%20for%20real%0Adeployments.%20We%20evaluate%20four%20types%20of%20convoyed%20path%20trackers%20with%20over%2010%20km%0Aof%20driving%20in%20a%20coupled%20convoy.%20With%20convoys%20of%20two%20and%20three%20rovers%2C%20the%0Aproposed%20distributed%20MPC%20method%20works%20in%20real-time%20to%20allow%20map-based%20convoying%0Ato%20maintain%20maximum%20spacing%20within%2020%20cm%20of%20the%20target%20in%20various%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharing%2520the%2520Load%253A%2520Distributed%2520Model-Predictive%2520Control%2520for%2520Precise%250A%2520%2520Multi-Rover%2520Cargo%2520Transport%26entry.906535625%3DAlexander%2520Krawciw%2520and%2520Sven%2520Lilge%2520and%2520Luka%2520Antonyshyn%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520For%2520autonomous%2520cargo%2520transportation%252C%2520teams%2520of%2520mobile%2520robots%2520can%2520provide%2520more%250Aoperational%2520flexibility%2520than%2520a%2520single%2520large%2520robot.%2520In%2520these%2520scenarios%252C%250Aprecision%2520in%2520both%2520inter-vehicle%2520distance%2520and%2520path%2520tracking%2520is%2520key.%2520With%2520this%250Amotivation%252C%2520we%2520develop%2520a%2520distributed%2520model-predictive%2520controller%2520%2528MPC%2529%2520for%250Amulti-vehicle%2520cargo%2520operations%2520that%2520builds%2520on%2520the%2520precise%2520path-tracking%2520of%250Alidar%2520teach%2520and%2520repeat.%2520To%2520carry%2520cargo%252C%2520a%2520following%2520vehicle%2520must%2520maintain%2520a%250AEuclidean%2520distance%2520offset%2520from%2520a%2520lead%2520vehicle%2520regardless%2520of%2520the%2520path%2520curvature.%250AOur%2520approach%2520uses%2520a%2520shared%2520map%2520to%2520localize%2520the%2520robots%2520relative%2520to%2520each%2520other%250Awithout%2520GNSS%2520or%2520direct%2520observations.%2520We%2520compare%2520our%2520approach%2520to%2520a%2520centralized%250AMPC%2520and%2520a%2520baseline%2520approach%2520that%2520directly%2520measures%2520the%2520inter-vehicle%2520distance.%250AThe%2520distributed%2520MPC%2520shows%2520equivalent%2520nominal%2520performance%2520to%2520the%2520more%2520complex%250Acentralized%2520MPC.%2520Using%2520a%2520direct%2520measurement%2520of%2520the%2520relative%2520distance%2520between%250Athe%2520leader%2520and%2520follower%2520shows%2520improved%2520tracking%2520performance%2520in%2520close-range%250Ascenarios%2520but%2520struggles%2520with%2520long-range%2520offsets.%2520The%2520operational%2520flexibility%250Aprovided%2520by%2520distributing%2520the%2520computation%2520makes%2520it%2520well%2520suited%2520for%2520real%250Adeployments.%2520We%2520evaluate%2520four%2520types%2520of%2520convoyed%2520path%2520trackers%2520with%2520over%252010%2520km%250Aof%2520driving%2520in%2520a%2520coupled%2520convoy.%2520With%2520convoys%2520of%2520two%2520and%2520three%2520rovers%252C%2520the%250Aproposed%2520distributed%2520MPC%2520method%2520works%2520in%2520real-time%2520to%2520allow%2520map-based%2520convoying%250Ato%2520maintain%2520maximum%2520spacing%2520within%252020%2520cm%2520of%2520the%2520target%2520in%2520various%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharing%20the%20Load%3A%20Distributed%20Model-Predictive%20Control%20for%20Precise%0A%20%20Multi-Rover%20Cargo%20Transport&entry.906535625=Alexander%20Krawciw%20and%20Sven%20Lilge%20and%20Luka%20Antonyshyn%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20For%20autonomous%20cargo%20transportation%2C%20teams%20of%20mobile%20robots%20can%20provide%20more%0Aoperational%20flexibility%20than%20a%20single%20large%20robot.%20In%20these%20scenarios%2C%0Aprecision%20in%20both%20inter-vehicle%20distance%20and%20path%20tracking%20is%20key.%20With%20this%0Amotivation%2C%20we%20develop%20a%20distributed%20model-predictive%20controller%20%28MPC%29%20for%0Amulti-vehicle%20cargo%20operations%20that%20builds%20on%20the%20precise%20path-tracking%20of%0Alidar%20teach%20and%20repeat.%20To%20carry%20cargo%2C%20a%20following%20vehicle%20must%20maintain%20a%0AEuclidean%20distance%20offset%20from%20a%20lead%20vehicle%20regardless%20of%20the%20path%20curvature.%0AOur%20approach%20uses%20a%20shared%20map%20to%20localize%20the%20robots%20relative%20to%20each%20other%0Awithout%20GNSS%20or%20direct%20observations.%20We%20compare%20our%20approach%20to%20a%20centralized%0AMPC%20and%20a%20baseline%20approach%20that%20directly%20measures%20the%20inter-vehicle%20distance.%0AThe%20distributed%20MPC%20shows%20equivalent%20nominal%20performance%20to%20the%20more%20complex%0Acentralized%20MPC.%20Using%20a%20direct%20measurement%20of%20the%20relative%20distance%20between%0Athe%20leader%20and%20follower%20shows%20improved%20tracking%20performance%20in%20close-range%0Ascenarios%20but%20struggles%20with%20long-range%20offsets.%20The%20operational%20flexibility%0Aprovided%20by%20distributing%20the%20computation%20makes%20it%20well%20suited%20for%20real%0Adeployments.%20We%20evaluate%20four%20types%20of%20convoyed%20path%20trackers%20with%20over%2010%20km%0Aof%20driving%20in%20a%20coupled%20convoy.%20With%20convoys%20of%20two%20and%20three%20rovers%2C%20the%0Aproposed%20distributed%20MPC%20method%20works%20in%20real-time%20to%20allow%20map-based%20convoying%0Ato%20maintain%20maximum%20spacing%20within%2020%20cm%20of%20the%20target%20in%20various%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18766v1&entry.124074799=Read"},
{"title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over\n  Time Series?", "author": "Zewen Liu and Juntong Ni and Xianfeng Tang and Max S. Y. Lau and Wenpeng Yin and Wei Jin", "abstract": "  Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.\n", "link": "http://arxiv.org/abs/2508.03963v3", "date": "2025-10-21", "relevancy": 2.1252, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20Adequately%20Perform%20Symbolic%20Reasoning%20Over%0A%20%20Time%20Series%3F&body=Title%3A%20Can%20Large%20Language%20Models%20Adequately%20Perform%20Symbolic%20Reasoning%20Over%0A%20%20Time%20Series%3F%0AAuthor%3A%20Zewen%20Liu%20and%20Juntong%20Ni%20and%20Xianfeng%20Tang%20and%20Max%20S.%20Y.%20Lau%20and%20Wenpeng%20Yin%20and%20Wei%20Jin%0AAbstract%3A%20%20%20Uncovering%20hidden%20symbolic%20laws%20from%20time%20series%20data%2C%20as%20an%20aspiration%0Adating%20back%20to%20Kepler%27s%20discovery%20of%20planetary%20motion%2C%20remains%20a%20core%20challenge%0Ain%20scientific%20discovery%20and%20artificial%20intelligence.%20While%20Large%20Language%0AModels%20show%20promise%20in%20structured%20reasoning%20tasks%2C%20their%20ability%20to%20infer%0Ainterpretable%2C%20context-aligned%20symbolic%20structures%20from%20time%20series%20data%20is%0Astill%20underexplored.%20To%20systematically%20evaluate%20this%20capability%2C%20we%20introduce%0ASymbolBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20assess%20symbolic%20reasoning%0Aover%20real-world%20time%20series%20across%20three%20tasks%3A%20multivariate%20symbolic%0Aregression%2C%20Boolean%20network%20inference%2C%20and%20causal%20discovery.%20Unlike%20prior%0Aefforts%20limited%20to%20simple%20algebraic%20equations%2C%20SymbolBench%20spans%20a%20diverse%20set%0Aof%20symbolic%20forms%20with%20varying%20complexity.%20We%20further%20propose%20a%20unified%0Aframework%20that%20integrates%20LLMs%20with%20genetic%20programming%20to%20form%20a%20closed-loop%0Asymbolic%20reasoning%20system%2C%20where%20LLMs%20act%20both%20as%20predictors%20and%20evaluators.%0AOur%20empirical%20results%20reveal%20key%20strengths%20and%20limitations%20of%20current%20models%2C%0Ahighlighting%20the%20importance%20of%20combining%20domain%20knowledge%2C%20context%20alignment%2C%0Aand%20reasoning%20structure%20to%20improve%20LLMs%20in%20automated%20scientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03963v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520Adequately%2520Perform%2520Symbolic%2520Reasoning%2520Over%250A%2520%2520Time%2520Series%253F%26entry.906535625%3DZewen%2520Liu%2520and%2520Juntong%2520Ni%2520and%2520Xianfeng%2520Tang%2520and%2520Max%2520S.%2520Y.%2520Lau%2520and%2520Wenpeng%2520Yin%2520and%2520Wei%2520Jin%26entry.1292438233%3D%2520%2520Uncovering%2520hidden%2520symbolic%2520laws%2520from%2520time%2520series%2520data%252C%2520as%2520an%2520aspiration%250Adating%2520back%2520to%2520Kepler%2527s%2520discovery%2520of%2520planetary%2520motion%252C%2520remains%2520a%2520core%2520challenge%250Ain%2520scientific%2520discovery%2520and%2520artificial%2520intelligence.%2520While%2520Large%2520Language%250AModels%2520show%2520promise%2520in%2520structured%2520reasoning%2520tasks%252C%2520their%2520ability%2520to%2520infer%250Ainterpretable%252C%2520context-aligned%2520symbolic%2520structures%2520from%2520time%2520series%2520data%2520is%250Astill%2520underexplored.%2520To%2520systematically%2520evaluate%2520this%2520capability%252C%2520we%2520introduce%250ASymbolBench%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520assess%2520symbolic%2520reasoning%250Aover%2520real-world%2520time%2520series%2520across%2520three%2520tasks%253A%2520multivariate%2520symbolic%250Aregression%252C%2520Boolean%2520network%2520inference%252C%2520and%2520causal%2520discovery.%2520Unlike%2520prior%250Aefforts%2520limited%2520to%2520simple%2520algebraic%2520equations%252C%2520SymbolBench%2520spans%2520a%2520diverse%2520set%250Aof%2520symbolic%2520forms%2520with%2520varying%2520complexity.%2520We%2520further%2520propose%2520a%2520unified%250Aframework%2520that%2520integrates%2520LLMs%2520with%2520genetic%2520programming%2520to%2520form%2520a%2520closed-loop%250Asymbolic%2520reasoning%2520system%252C%2520where%2520LLMs%2520act%2520both%2520as%2520predictors%2520and%2520evaluators.%250AOur%2520empirical%2520results%2520reveal%2520key%2520strengths%2520and%2520limitations%2520of%2520current%2520models%252C%250Ahighlighting%2520the%2520importance%2520of%2520combining%2520domain%2520knowledge%252C%2520context%2520alignment%252C%250Aand%2520reasoning%2520structure%2520to%2520improve%2520LLMs%2520in%2520automated%2520scientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03963v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20Adequately%20Perform%20Symbolic%20Reasoning%20Over%0A%20%20Time%20Series%3F&entry.906535625=Zewen%20Liu%20and%20Juntong%20Ni%20and%20Xianfeng%20Tang%20and%20Max%20S.%20Y.%20Lau%20and%20Wenpeng%20Yin%20and%20Wei%20Jin&entry.1292438233=%20%20Uncovering%20hidden%20symbolic%20laws%20from%20time%20series%20data%2C%20as%20an%20aspiration%0Adating%20back%20to%20Kepler%27s%20discovery%20of%20planetary%20motion%2C%20remains%20a%20core%20challenge%0Ain%20scientific%20discovery%20and%20artificial%20intelligence.%20While%20Large%20Language%0AModels%20show%20promise%20in%20structured%20reasoning%20tasks%2C%20their%20ability%20to%20infer%0Ainterpretable%2C%20context-aligned%20symbolic%20structures%20from%20time%20series%20data%20is%0Astill%20underexplored.%20To%20systematically%20evaluate%20this%20capability%2C%20we%20introduce%0ASymbolBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20assess%20symbolic%20reasoning%0Aover%20real-world%20time%20series%20across%20three%20tasks%3A%20multivariate%20symbolic%0Aregression%2C%20Boolean%20network%20inference%2C%20and%20causal%20discovery.%20Unlike%20prior%0Aefforts%20limited%20to%20simple%20algebraic%20equations%2C%20SymbolBench%20spans%20a%20diverse%20set%0Aof%20symbolic%20forms%20with%20varying%20complexity.%20We%20further%20propose%20a%20unified%0Aframework%20that%20integrates%20LLMs%20with%20genetic%20programming%20to%20form%20a%20closed-loop%0Asymbolic%20reasoning%20system%2C%20where%20LLMs%20act%20both%20as%20predictors%20and%20evaluators.%0AOur%20empirical%20results%20reveal%20key%20strengths%20and%20limitations%20of%20current%20models%2C%0Ahighlighting%20the%20importance%20of%20combining%20domain%20knowledge%2C%20context%20alignment%2C%0Aand%20reasoning%20structure%20to%20improve%20LLMs%20in%20automated%20scientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03963v3&entry.124074799=Read"},
{"title": "SAM 2++: Tracking Anything at Any Granularity", "author": "Jiaming Zhang and Cheng Liang and Yichun Yang and Chenkai Zeng and Yutao Cui and Xinwen Zhang and Xin Zhou and Kai Ma and Gangshan Wu and Limin Wang", "abstract": "  Video tracking aims at finding the specific target in subsequent frames given\nits initial state. Due to the varying granularity of target states across\ndifferent tasks, most existing trackers are tailored to a single task and\nheavily rely on custom-designed modules within the individual task, which\nlimits their generalization and leads to redundancy in both model design and\nparameters. To unify video tracking tasks, we present SAM 2++, a unified model\ntowards tracking at any granularity, including masks, boxes, and points. First,\nto extend target granularity, we design task-specific prompts to encode various\ntask inputs into general prompt embeddings, and a unified decoder to unify\ndiverse task results into a unified form pre-output. Next, to satisfy memory\nmatching, the core operation of tracking, we introduce a task-adaptive memory\nmechanism that unifies memory across different granularities. Finally, we\nintroduce a customized data engine to support tracking training at any\ngranularity, producing a large and diverse video tracking dataset with rich\nannotations at three granularities, termed Tracking-Any-Granularity, which\nrepresents a comprehensive resource for training and benchmarking on unified\ntracking. Comprehensive experiments on multiple benchmarks confirm that SAM 2++\nsets a new state of the art across diverse tracking tasks at different\ngranularities, establishing a unified and robust tracking framework.\n", "link": "http://arxiv.org/abs/2510.18822v1", "date": "2025-10-21", "relevancy": 2.1247, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5349}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5322}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM%202%2B%2B%3A%20Tracking%20Anything%20at%20Any%20Granularity&body=Title%3A%20SAM%202%2B%2B%3A%20Tracking%20Anything%20at%20Any%20Granularity%0AAuthor%3A%20Jiaming%20Zhang%20and%20Cheng%20Liang%20and%20Yichun%20Yang%20and%20Chenkai%20Zeng%20and%20Yutao%20Cui%20and%20Xinwen%20Zhang%20and%20Xin%20Zhou%20and%20Kai%20Ma%20and%20Gangshan%20Wu%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Video%20tracking%20aims%20at%20finding%20the%20specific%20target%20in%20subsequent%20frames%20given%0Aits%20initial%20state.%20Due%20to%20the%20varying%20granularity%20of%20target%20states%20across%0Adifferent%20tasks%2C%20most%20existing%20trackers%20are%20tailored%20to%20a%20single%20task%20and%0Aheavily%20rely%20on%20custom-designed%20modules%20within%20the%20individual%20task%2C%20which%0Alimits%20their%20generalization%20and%20leads%20to%20redundancy%20in%20both%20model%20design%20and%0Aparameters.%20To%20unify%20video%20tracking%20tasks%2C%20we%20present%20SAM%202%2B%2B%2C%20a%20unified%20model%0Atowards%20tracking%20at%20any%20granularity%2C%20including%20masks%2C%20boxes%2C%20and%20points.%20First%2C%0Ato%20extend%20target%20granularity%2C%20we%20design%20task-specific%20prompts%20to%20encode%20various%0Atask%20inputs%20into%20general%20prompt%20embeddings%2C%20and%20a%20unified%20decoder%20to%20unify%0Adiverse%20task%20results%20into%20a%20unified%20form%20pre-output.%20Next%2C%20to%20satisfy%20memory%0Amatching%2C%20the%20core%20operation%20of%20tracking%2C%20we%20introduce%20a%20task-adaptive%20memory%0Amechanism%20that%20unifies%20memory%20across%20different%20granularities.%20Finally%2C%20we%0Aintroduce%20a%20customized%20data%20engine%20to%20support%20tracking%20training%20at%20any%0Agranularity%2C%20producing%20a%20large%20and%20diverse%20video%20tracking%20dataset%20with%20rich%0Aannotations%20at%20three%20granularities%2C%20termed%20Tracking-Any-Granularity%2C%20which%0Arepresents%20a%20comprehensive%20resource%20for%20training%20and%20benchmarking%20on%20unified%0Atracking.%20Comprehensive%20experiments%20on%20multiple%20benchmarks%20confirm%20that%20SAM%202%2B%2B%0Asets%20a%20new%20state%20of%20the%20art%20across%20diverse%20tracking%20tasks%20at%20different%0Agranularities%2C%20establishing%20a%20unified%20and%20robust%20tracking%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM%25202%252B%252B%253A%2520Tracking%2520Anything%2520at%2520Any%2520Granularity%26entry.906535625%3DJiaming%2520Zhang%2520and%2520Cheng%2520Liang%2520and%2520Yichun%2520Yang%2520and%2520Chenkai%2520Zeng%2520and%2520Yutao%2520Cui%2520and%2520Xinwen%2520Zhang%2520and%2520Xin%2520Zhou%2520and%2520Kai%2520Ma%2520and%2520Gangshan%2520Wu%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Video%2520tracking%2520aims%2520at%2520finding%2520the%2520specific%2520target%2520in%2520subsequent%2520frames%2520given%250Aits%2520initial%2520state.%2520Due%2520to%2520the%2520varying%2520granularity%2520of%2520target%2520states%2520across%250Adifferent%2520tasks%252C%2520most%2520existing%2520trackers%2520are%2520tailored%2520to%2520a%2520single%2520task%2520and%250Aheavily%2520rely%2520on%2520custom-designed%2520modules%2520within%2520the%2520individual%2520task%252C%2520which%250Alimits%2520their%2520generalization%2520and%2520leads%2520to%2520redundancy%2520in%2520both%2520model%2520design%2520and%250Aparameters.%2520To%2520unify%2520video%2520tracking%2520tasks%252C%2520we%2520present%2520SAM%25202%252B%252B%252C%2520a%2520unified%2520model%250Atowards%2520tracking%2520at%2520any%2520granularity%252C%2520including%2520masks%252C%2520boxes%252C%2520and%2520points.%2520First%252C%250Ato%2520extend%2520target%2520granularity%252C%2520we%2520design%2520task-specific%2520prompts%2520to%2520encode%2520various%250Atask%2520inputs%2520into%2520general%2520prompt%2520embeddings%252C%2520and%2520a%2520unified%2520decoder%2520to%2520unify%250Adiverse%2520task%2520results%2520into%2520a%2520unified%2520form%2520pre-output.%2520Next%252C%2520to%2520satisfy%2520memory%250Amatching%252C%2520the%2520core%2520operation%2520of%2520tracking%252C%2520we%2520introduce%2520a%2520task-adaptive%2520memory%250Amechanism%2520that%2520unifies%2520memory%2520across%2520different%2520granularities.%2520Finally%252C%2520we%250Aintroduce%2520a%2520customized%2520data%2520engine%2520to%2520support%2520tracking%2520training%2520at%2520any%250Agranularity%252C%2520producing%2520a%2520large%2520and%2520diverse%2520video%2520tracking%2520dataset%2520with%2520rich%250Aannotations%2520at%2520three%2520granularities%252C%2520termed%2520Tracking-Any-Granularity%252C%2520which%250Arepresents%2520a%2520comprehensive%2520resource%2520for%2520training%2520and%2520benchmarking%2520on%2520unified%250Atracking.%2520Comprehensive%2520experiments%2520on%2520multiple%2520benchmarks%2520confirm%2520that%2520SAM%25202%252B%252B%250Asets%2520a%2520new%2520state%2520of%2520the%2520art%2520across%2520diverse%2520tracking%2520tasks%2520at%2520different%250Agranularities%252C%2520establishing%2520a%2520unified%2520and%2520robust%2520tracking%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM%202%2B%2B%3A%20Tracking%20Anything%20at%20Any%20Granularity&entry.906535625=Jiaming%20Zhang%20and%20Cheng%20Liang%20and%20Yichun%20Yang%20and%20Chenkai%20Zeng%20and%20Yutao%20Cui%20and%20Xinwen%20Zhang%20and%20Xin%20Zhou%20and%20Kai%20Ma%20and%20Gangshan%20Wu%20and%20Limin%20Wang&entry.1292438233=%20%20Video%20tracking%20aims%20at%20finding%20the%20specific%20target%20in%20subsequent%20frames%20given%0Aits%20initial%20state.%20Due%20to%20the%20varying%20granularity%20of%20target%20states%20across%0Adifferent%20tasks%2C%20most%20existing%20trackers%20are%20tailored%20to%20a%20single%20task%20and%0Aheavily%20rely%20on%20custom-designed%20modules%20within%20the%20individual%20task%2C%20which%0Alimits%20their%20generalization%20and%20leads%20to%20redundancy%20in%20both%20model%20design%20and%0Aparameters.%20To%20unify%20video%20tracking%20tasks%2C%20we%20present%20SAM%202%2B%2B%2C%20a%20unified%20model%0Atowards%20tracking%20at%20any%20granularity%2C%20including%20masks%2C%20boxes%2C%20and%20points.%20First%2C%0Ato%20extend%20target%20granularity%2C%20we%20design%20task-specific%20prompts%20to%20encode%20various%0Atask%20inputs%20into%20general%20prompt%20embeddings%2C%20and%20a%20unified%20decoder%20to%20unify%0Adiverse%20task%20results%20into%20a%20unified%20form%20pre-output.%20Next%2C%20to%20satisfy%20memory%0Amatching%2C%20the%20core%20operation%20of%20tracking%2C%20we%20introduce%20a%20task-adaptive%20memory%0Amechanism%20that%20unifies%20memory%20across%20different%20granularities.%20Finally%2C%20we%0Aintroduce%20a%20customized%20data%20engine%20to%20support%20tracking%20training%20at%20any%0Agranularity%2C%20producing%20a%20large%20and%20diverse%20video%20tracking%20dataset%20with%20rich%0Aannotations%20at%20three%20granularities%2C%20termed%20Tracking-Any-Granularity%2C%20which%0Arepresents%20a%20comprehensive%20resource%20for%20training%20and%20benchmarking%20on%20unified%0Atracking.%20Comprehensive%20experiments%20on%20multiple%20benchmarks%20confirm%20that%20SAM%202%2B%2B%0Asets%20a%20new%20state%20of%20the%20art%20across%20diverse%20tracking%20tasks%20at%20different%0Agranularities%2C%20establishing%20a%20unified%20and%20robust%20tracking%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18822v1&entry.124074799=Read"},
{"title": "MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long\n  Context Training", "author": "Wenxuan Li and Chengruidong Zhang and Huiqiang Jiang and Yucheng Li and Yuqing Yang and Lili Qiu", "abstract": "  The adoption of long context windows has become a standard feature in Large\nLanguage Models (LLMs), as extended contexts significantly enhance their\ncapacity for complex reasoning and broaden their applicability across diverse\nscenarios. Dynamic sparse attention is a promising approach for reducing the\ncomputational cost of long-context. However, efficiently training LLMs with\ndynamic sparse attention on ultra-long contexts-especially in distributed\nsettings-remains a significant challenge, due in large part to worker- and\nstep-level imbalance. This paper introduces MTraining, a novel distributed\nmethodology leveraging dynamic sparse attention to enable efficient training\nfor LLMs with ultra-long contexts. Specifically, MTraining integrates three key\ncomponents: a dynamic sparse training pattern, balanced sparse ring attention,\nand hierarchical sparse ring attention. These components are designed to\nsynergistically address the computational imbalance and communication overheads\ninherent in dynamic sparse attention mechanisms during the training of models\nwith extensive context lengths. We demonstrate the efficacy of MTraining by\ntraining Qwen2.5-3B, successfully expanding its context window from 32K to 512K\ntokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite\nof downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A\nHaystack, reveal that MTraining achieves up to a 6x higher training throughput\nwhile preserving model accuracy. Our code is available at\nhttps://github.com/microsoft/MInference/tree/main/MTraining.\n", "link": "http://arxiv.org/abs/2510.18830v1", "date": "2025-10-21", "relevancy": 2.0922, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5389}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5154}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTraining%3A%20Distributed%20Dynamic%20Sparse%20Attention%20for%20Efficient%20Ultra-Long%0A%20%20Context%20Training&body=Title%3A%20MTraining%3A%20Distributed%20Dynamic%20Sparse%20Attention%20for%20Efficient%20Ultra-Long%0A%20%20Context%20Training%0AAuthor%3A%20Wenxuan%20Li%20and%20Chengruidong%20Zhang%20and%20Huiqiang%20Jiang%20and%20Yucheng%20Li%20and%20Yuqing%20Yang%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20The%20adoption%20of%20long%20context%20windows%20has%20become%20a%20standard%20feature%20in%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20as%20extended%20contexts%20significantly%20enhance%20their%0Acapacity%20for%20complex%20reasoning%20and%20broaden%20their%20applicability%20across%20diverse%0Ascenarios.%20Dynamic%20sparse%20attention%20is%20a%20promising%20approach%20for%20reducing%20the%0Acomputational%20cost%20of%20long-context.%20However%2C%20efficiently%20training%20LLMs%20with%0Adynamic%20sparse%20attention%20on%20ultra-long%20contexts-especially%20in%20distributed%0Asettings-remains%20a%20significant%20challenge%2C%20due%20in%20large%20part%20to%20worker-%20and%0Astep-level%20imbalance.%20This%20paper%20introduces%20MTraining%2C%20a%20novel%20distributed%0Amethodology%20leveraging%20dynamic%20sparse%20attention%20to%20enable%20efficient%20training%0Afor%20LLMs%20with%20ultra-long%20contexts.%20Specifically%2C%20MTraining%20integrates%20three%20key%0Acomponents%3A%20a%20dynamic%20sparse%20training%20pattern%2C%20balanced%20sparse%20ring%20attention%2C%0Aand%20hierarchical%20sparse%20ring%20attention.%20These%20components%20are%20designed%20to%0Asynergistically%20address%20the%20computational%20imbalance%20and%20communication%20overheads%0Ainherent%20in%20dynamic%20sparse%20attention%20mechanisms%20during%20the%20training%20of%20models%0Awith%20extensive%20context%20lengths.%20We%20demonstrate%20the%20efficacy%20of%20MTraining%20by%0Atraining%20Qwen2.5-3B%2C%20successfully%20expanding%20its%20context%20window%20from%2032K%20to%20512K%0Atokens%20on%20a%20cluster%20of%2032%20A100%20GPUs.%20Our%20evaluations%20on%20a%20comprehensive%20suite%0Aof%20downstream%20tasks%2C%20including%20RULER%2C%20PG-19%2C%20InfiniteBench%2C%20and%20Needle%20In%20A%0AHaystack%2C%20reveal%20that%20MTraining%20achieves%20up%20to%20a%206x%20higher%20training%20throughput%0Awhile%20preserving%20model%20accuracy.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/microsoft/MInference/tree/main/MTraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTraining%253A%2520Distributed%2520Dynamic%2520Sparse%2520Attention%2520for%2520Efficient%2520Ultra-Long%250A%2520%2520Context%2520Training%26entry.906535625%3DWenxuan%2520Li%2520and%2520Chengruidong%2520Zhang%2520and%2520Huiqiang%2520Jiang%2520and%2520Yucheng%2520Li%2520and%2520Yuqing%2520Yang%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520The%2520adoption%2520of%2520long%2520context%2520windows%2520has%2520become%2520a%2520standard%2520feature%2520in%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520as%2520extended%2520contexts%2520significantly%2520enhance%2520their%250Acapacity%2520for%2520complex%2520reasoning%2520and%2520broaden%2520their%2520applicability%2520across%2520diverse%250Ascenarios.%2520Dynamic%2520sparse%2520attention%2520is%2520a%2520promising%2520approach%2520for%2520reducing%2520the%250Acomputational%2520cost%2520of%2520long-context.%2520However%252C%2520efficiently%2520training%2520LLMs%2520with%250Adynamic%2520sparse%2520attention%2520on%2520ultra-long%2520contexts-especially%2520in%2520distributed%250Asettings-remains%2520a%2520significant%2520challenge%252C%2520due%2520in%2520large%2520part%2520to%2520worker-%2520and%250Astep-level%2520imbalance.%2520This%2520paper%2520introduces%2520MTraining%252C%2520a%2520novel%2520distributed%250Amethodology%2520leveraging%2520dynamic%2520sparse%2520attention%2520to%2520enable%2520efficient%2520training%250Afor%2520LLMs%2520with%2520ultra-long%2520contexts.%2520Specifically%252C%2520MTraining%2520integrates%2520three%2520key%250Acomponents%253A%2520a%2520dynamic%2520sparse%2520training%2520pattern%252C%2520balanced%2520sparse%2520ring%2520attention%252C%250Aand%2520hierarchical%2520sparse%2520ring%2520attention.%2520These%2520components%2520are%2520designed%2520to%250Asynergistically%2520address%2520the%2520computational%2520imbalance%2520and%2520communication%2520overheads%250Ainherent%2520in%2520dynamic%2520sparse%2520attention%2520mechanisms%2520during%2520the%2520training%2520of%2520models%250Awith%2520extensive%2520context%2520lengths.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520MTraining%2520by%250Atraining%2520Qwen2.5-3B%252C%2520successfully%2520expanding%2520its%2520context%2520window%2520from%252032K%2520to%2520512K%250Atokens%2520on%2520a%2520cluster%2520of%252032%2520A100%2520GPUs.%2520Our%2520evaluations%2520on%2520a%2520comprehensive%2520suite%250Aof%2520downstream%2520tasks%252C%2520including%2520RULER%252C%2520PG-19%252C%2520InfiniteBench%252C%2520and%2520Needle%2520In%2520A%250AHaystack%252C%2520reveal%2520that%2520MTraining%2520achieves%2520up%2520to%2520a%25206x%2520higher%2520training%2520throughput%250Awhile%2520preserving%2520model%2520accuracy.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/microsoft/MInference/tree/main/MTraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTraining%3A%20Distributed%20Dynamic%20Sparse%20Attention%20for%20Efficient%20Ultra-Long%0A%20%20Context%20Training&entry.906535625=Wenxuan%20Li%20and%20Chengruidong%20Zhang%20and%20Huiqiang%20Jiang%20and%20Yucheng%20Li%20and%20Yuqing%20Yang%20and%20Lili%20Qiu&entry.1292438233=%20%20The%20adoption%20of%20long%20context%20windows%20has%20become%20a%20standard%20feature%20in%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20as%20extended%20contexts%20significantly%20enhance%20their%0Acapacity%20for%20complex%20reasoning%20and%20broaden%20their%20applicability%20across%20diverse%0Ascenarios.%20Dynamic%20sparse%20attention%20is%20a%20promising%20approach%20for%20reducing%20the%0Acomputational%20cost%20of%20long-context.%20However%2C%20efficiently%20training%20LLMs%20with%0Adynamic%20sparse%20attention%20on%20ultra-long%20contexts-especially%20in%20distributed%0Asettings-remains%20a%20significant%20challenge%2C%20due%20in%20large%20part%20to%20worker-%20and%0Astep-level%20imbalance.%20This%20paper%20introduces%20MTraining%2C%20a%20novel%20distributed%0Amethodology%20leveraging%20dynamic%20sparse%20attention%20to%20enable%20efficient%20training%0Afor%20LLMs%20with%20ultra-long%20contexts.%20Specifically%2C%20MTraining%20integrates%20three%20key%0Acomponents%3A%20a%20dynamic%20sparse%20training%20pattern%2C%20balanced%20sparse%20ring%20attention%2C%0Aand%20hierarchical%20sparse%20ring%20attention.%20These%20components%20are%20designed%20to%0Asynergistically%20address%20the%20computational%20imbalance%20and%20communication%20overheads%0Ainherent%20in%20dynamic%20sparse%20attention%20mechanisms%20during%20the%20training%20of%20models%0Awith%20extensive%20context%20lengths.%20We%20demonstrate%20the%20efficacy%20of%20MTraining%20by%0Atraining%20Qwen2.5-3B%2C%20successfully%20expanding%20its%20context%20window%20from%2032K%20to%20512K%0Atokens%20on%20a%20cluster%20of%2032%20A100%20GPUs.%20Our%20evaluations%20on%20a%20comprehensive%20suite%0Aof%20downstream%20tasks%2C%20including%20RULER%2C%20PG-19%2C%20InfiniteBench%2C%20and%20Needle%20In%20A%0AHaystack%2C%20reveal%20that%20MTraining%20achieves%20up%20to%20a%206x%20higher%20training%20throughput%0Awhile%20preserving%20model%20accuracy.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/microsoft/MInference/tree/main/MTraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18830v1&entry.124074799=Read"},
{"title": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via\n  Critic-Guided Search", "author": "Zijian Wu and Suozhi Huang and Zhejian Zhou and Huaiyuan Ying and Zheng Yuan and Wenwei Zhang and Dahua Lin and Kai Chen", "abstract": "  Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. A\nprevalent proof method involves the LLM prover iteratively constructing the\nproof tactic by tactic, typically following a best-first search scheme.\nHowever, this method often ignores the critical preference information inside\nthe existing tactic trajectories, hindering the search for deeper proofs. We\npropose an intuitive yet effective method, which utilizes a critic model to\ncapture the preference information and to guide the search of the prover model\nat runtime. Given the prover-critic framework, a large-scale expert iteration\nwith more than 20,000 CPU days is then applied to further fine-tune the prover\nand the critic. The trained InternLM2.5-StepProver critic significantly boosts\nthe performance of the prover model (59.4% to 65.9%). We also analyze the\nimpact of the critic on various aspects of the theorem proving process during\nexpert iteration, providing insights into its effectiveness. We open-source our\nmodels and searched proofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.\n", "link": "http://arxiv.org/abs/2410.15700v2", "date": "2025-10-21", "relevancy": 2.0604, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternLM2.5-StepProver%3A%20Advancing%20Automated%20Theorem%20Proving%20via%0A%20%20Critic-Guided%20Search&body=Title%3A%20InternLM2.5-StepProver%3A%20Advancing%20Automated%20Theorem%20Proving%20via%0A%20%20Critic-Guided%20Search%0AAuthor%3A%20Zijian%20Wu%20and%20Suozhi%20Huang%20and%20Zhejian%20Zhou%20and%20Huaiyuan%20Ying%20and%20Zheng%20Yuan%20and%20Wenwei%20Zhang%20and%20Dahua%20Lin%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20in%20mathematical%0Atheorem%20proving%2C%20particularly%20when%20utilizing%20formal%20languages%20such%20as%20LEAN.%20A%0Aprevalent%20proof%20method%20involves%20the%20LLM%20prover%20iteratively%20constructing%20the%0Aproof%20tactic%20by%20tactic%2C%20typically%20following%20a%20best-first%20search%20scheme.%0AHowever%2C%20this%20method%20often%20ignores%20the%20critical%20preference%20information%20inside%0Athe%20existing%20tactic%20trajectories%2C%20hindering%20the%20search%20for%20deeper%20proofs.%20We%0Apropose%20an%20intuitive%20yet%20effective%20method%2C%20which%20utilizes%20a%20critic%20model%20to%0Acapture%20the%20preference%20information%20and%20to%20guide%20the%20search%20of%20the%20prover%20model%0Aat%20runtime.%20Given%20the%20prover-critic%20framework%2C%20a%20large-scale%20expert%20iteration%0Awith%20more%20than%2020%2C000%20CPU%20days%20is%20then%20applied%20to%20further%20fine-tune%20the%20prover%0Aand%20the%20critic.%20The%20trained%20InternLM2.5-StepProver%20critic%20significantly%20boosts%0Athe%20performance%20of%20the%20prover%20model%20%2859.4%25%20to%2065.9%25%29.%20We%20also%20analyze%20the%0Aimpact%20of%20the%20critic%20on%20various%20aspects%20of%20the%20theorem%20proving%20process%20during%0Aexpert%20iteration%2C%20providing%20insights%20into%20its%20effectiveness.%20We%20open-source%20our%0Amodels%20and%20searched%20proofs%20at%20https%3A//github.com/InternLM/InternLM-Math%20and%0Ahttps%3A//huggingface.co/datasets/internlm/Lean-Workbook.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternLM2.5-StepProver%253A%2520Advancing%2520Automated%2520Theorem%2520Proving%2520via%250A%2520%2520Critic-Guided%2520Search%26entry.906535625%3DZijian%2520Wu%2520and%2520Suozhi%2520Huang%2520and%2520Zhejian%2520Zhou%2520and%2520Huaiyuan%2520Ying%2520and%2520Zheng%2520Yuan%2520and%2520Wenwei%2520Zhang%2520and%2520Dahua%2520Lin%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520in%2520mathematical%250Atheorem%2520proving%252C%2520particularly%2520when%2520utilizing%2520formal%2520languages%2520such%2520as%2520LEAN.%2520A%250Aprevalent%2520proof%2520method%2520involves%2520the%2520LLM%2520prover%2520iteratively%2520constructing%2520the%250Aproof%2520tactic%2520by%2520tactic%252C%2520typically%2520following%2520a%2520best-first%2520search%2520scheme.%250AHowever%252C%2520this%2520method%2520often%2520ignores%2520the%2520critical%2520preference%2520information%2520inside%250Athe%2520existing%2520tactic%2520trajectories%252C%2520hindering%2520the%2520search%2520for%2520deeper%2520proofs.%2520We%250Apropose%2520an%2520intuitive%2520yet%2520effective%2520method%252C%2520which%2520utilizes%2520a%2520critic%2520model%2520to%250Acapture%2520the%2520preference%2520information%2520and%2520to%2520guide%2520the%2520search%2520of%2520the%2520prover%2520model%250Aat%2520runtime.%2520Given%2520the%2520prover-critic%2520framework%252C%2520a%2520large-scale%2520expert%2520iteration%250Awith%2520more%2520than%252020%252C000%2520CPU%2520days%2520is%2520then%2520applied%2520to%2520further%2520fine-tune%2520the%2520prover%250Aand%2520the%2520critic.%2520The%2520trained%2520InternLM2.5-StepProver%2520critic%2520significantly%2520boosts%250Athe%2520performance%2520of%2520the%2520prover%2520model%2520%252859.4%2525%2520to%252065.9%2525%2529.%2520We%2520also%2520analyze%2520the%250Aimpact%2520of%2520the%2520critic%2520on%2520various%2520aspects%2520of%2520the%2520theorem%2520proving%2520process%2520during%250Aexpert%2520iteration%252C%2520providing%2520insights%2520into%2520its%2520effectiveness.%2520We%2520open-source%2520our%250Amodels%2520and%2520searched%2520proofs%2520at%2520https%253A//github.com/InternLM/InternLM-Math%2520and%250Ahttps%253A//huggingface.co/datasets/internlm/Lean-Workbook.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternLM2.5-StepProver%3A%20Advancing%20Automated%20Theorem%20Proving%20via%0A%20%20Critic-Guided%20Search&entry.906535625=Zijian%20Wu%20and%20Suozhi%20Huang%20and%20Zhejian%20Zhou%20and%20Huaiyuan%20Ying%20and%20Zheng%20Yuan%20and%20Wenwei%20Zhang%20and%20Dahua%20Lin%20and%20Kai%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20in%20mathematical%0Atheorem%20proving%2C%20particularly%20when%20utilizing%20formal%20languages%20such%20as%20LEAN.%20A%0Aprevalent%20proof%20method%20involves%20the%20LLM%20prover%20iteratively%20constructing%20the%0Aproof%20tactic%20by%20tactic%2C%20typically%20following%20a%20best-first%20search%20scheme.%0AHowever%2C%20this%20method%20often%20ignores%20the%20critical%20preference%20information%20inside%0Athe%20existing%20tactic%20trajectories%2C%20hindering%20the%20search%20for%20deeper%20proofs.%20We%0Apropose%20an%20intuitive%20yet%20effective%20method%2C%20which%20utilizes%20a%20critic%20model%20to%0Acapture%20the%20preference%20information%20and%20to%20guide%20the%20search%20of%20the%20prover%20model%0Aat%20runtime.%20Given%20the%20prover-critic%20framework%2C%20a%20large-scale%20expert%20iteration%0Awith%20more%20than%2020%2C000%20CPU%20days%20is%20then%20applied%20to%20further%20fine-tune%20the%20prover%0Aand%20the%20critic.%20The%20trained%20InternLM2.5-StepProver%20critic%20significantly%20boosts%0Athe%20performance%20of%20the%20prover%20model%20%2859.4%25%20to%2065.9%25%29.%20We%20also%20analyze%20the%0Aimpact%20of%20the%20critic%20on%20various%20aspects%20of%20the%20theorem%20proving%20process%20during%0Aexpert%20iteration%2C%20providing%20insights%20into%20its%20effectiveness.%20We%20open-source%20our%0Amodels%20and%20searched%20proofs%20at%20https%3A//github.com/InternLM/InternLM-Math%20and%0Ahttps%3A//huggingface.co/datasets/internlm/Lean-Workbook.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15700v2&entry.124074799=Read"},
{"title": "A unified framework for establishing the universal approximation of\n  transformer-type architectures", "author": "Jingpu Cheng and Ting Lin and Zuowei Shen and Qianxiao Li", "abstract": "  We investigate the universal approximation property (UAP) of transformer-type\narchitectures, providing a unified theoretical framework that extends prior\nresults on residual networks to models incorporating attention mechanisms. Our\nwork identifies token distinguishability as a fundamental requirement for UAP\nand introduces a general sufficient condition that applies to a broad class of\narchitectures. Leveraging an analyticity assumption on the attention layer, we\ncan significantly simplify the verification of this condition, providing a\nnon-constructive approach in establishing UAP for such architectures. We\ndemonstrate the applicability of our framework by proving UAP for transformers\nwith various attention mechanisms, including kernel-based and sparse attention\nmechanisms. The corollaries of our results either generalize prior works or\nestablish UAP for architectures not previously covered. Furthermore, our\nframework offers a principled foundation for designing novel transformer\narchitectures with inherent UAP guarantees, including those with specific\nfunctional symmetries. We propose examples to illustrate these insights.\n", "link": "http://arxiv.org/abs/2506.23551v2", "date": "2025-10-21", "relevancy": 2.0362, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5413}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5231}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20unified%20framework%20for%20establishing%20the%20universal%20approximation%20of%0A%20%20transformer-type%20architectures&body=Title%3A%20A%20unified%20framework%20for%20establishing%20the%20universal%20approximation%20of%0A%20%20transformer-type%20architectures%0AAuthor%3A%20Jingpu%20Cheng%20and%20Ting%20Lin%20and%20Zuowei%20Shen%20and%20Qianxiao%20Li%0AAbstract%3A%20%20%20We%20investigate%20the%20universal%20approximation%20property%20%28UAP%29%20of%20transformer-type%0Aarchitectures%2C%20providing%20a%20unified%20theoretical%20framework%20that%20extends%20prior%0Aresults%20on%20residual%20networks%20to%20models%20incorporating%20attention%20mechanisms.%20Our%0Awork%20identifies%20token%20distinguishability%20as%20a%20fundamental%20requirement%20for%20UAP%0Aand%20introduces%20a%20general%20sufficient%20condition%20that%20applies%20to%20a%20broad%20class%20of%0Aarchitectures.%20Leveraging%20an%20analyticity%20assumption%20on%20the%20attention%20layer%2C%20we%0Acan%20significantly%20simplify%20the%20verification%20of%20this%20condition%2C%20providing%20a%0Anon-constructive%20approach%20in%20establishing%20UAP%20for%20such%20architectures.%20We%0Ademonstrate%20the%20applicability%20of%20our%20framework%20by%20proving%20UAP%20for%20transformers%0Awith%20various%20attention%20mechanisms%2C%20including%20kernel-based%20and%20sparse%20attention%0Amechanisms.%20The%20corollaries%20of%20our%20results%20either%20generalize%20prior%20works%20or%0Aestablish%20UAP%20for%20architectures%20not%20previously%20covered.%20Furthermore%2C%20our%0Aframework%20offers%20a%20principled%20foundation%20for%20designing%20novel%20transformer%0Aarchitectures%20with%20inherent%20UAP%20guarantees%2C%20including%20those%20with%20specific%0Afunctional%20symmetries.%20We%20propose%20examples%20to%20illustrate%20these%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520unified%2520framework%2520for%2520establishing%2520the%2520universal%2520approximation%2520of%250A%2520%2520transformer-type%2520architectures%26entry.906535625%3DJingpu%2520Cheng%2520and%2520Ting%2520Lin%2520and%2520Zuowei%2520Shen%2520and%2520Qianxiao%2520Li%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520universal%2520approximation%2520property%2520%2528UAP%2529%2520of%2520transformer-type%250Aarchitectures%252C%2520providing%2520a%2520unified%2520theoretical%2520framework%2520that%2520extends%2520prior%250Aresults%2520on%2520residual%2520networks%2520to%2520models%2520incorporating%2520attention%2520mechanisms.%2520Our%250Awork%2520identifies%2520token%2520distinguishability%2520as%2520a%2520fundamental%2520requirement%2520for%2520UAP%250Aand%2520introduces%2520a%2520general%2520sufficient%2520condition%2520that%2520applies%2520to%2520a%2520broad%2520class%2520of%250Aarchitectures.%2520Leveraging%2520an%2520analyticity%2520assumption%2520on%2520the%2520attention%2520layer%252C%2520we%250Acan%2520significantly%2520simplify%2520the%2520verification%2520of%2520this%2520condition%252C%2520providing%2520a%250Anon-constructive%2520approach%2520in%2520establishing%2520UAP%2520for%2520such%2520architectures.%2520We%250Ademonstrate%2520the%2520applicability%2520of%2520our%2520framework%2520by%2520proving%2520UAP%2520for%2520transformers%250Awith%2520various%2520attention%2520mechanisms%252C%2520including%2520kernel-based%2520and%2520sparse%2520attention%250Amechanisms.%2520The%2520corollaries%2520of%2520our%2520results%2520either%2520generalize%2520prior%2520works%2520or%250Aestablish%2520UAP%2520for%2520architectures%2520not%2520previously%2520covered.%2520Furthermore%252C%2520our%250Aframework%2520offers%2520a%2520principled%2520foundation%2520for%2520designing%2520novel%2520transformer%250Aarchitectures%2520with%2520inherent%2520UAP%2520guarantees%252C%2520including%2520those%2520with%2520specific%250Afunctional%2520symmetries.%2520We%2520propose%2520examples%2520to%2520illustrate%2520these%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20unified%20framework%20for%20establishing%20the%20universal%20approximation%20of%0A%20%20transformer-type%20architectures&entry.906535625=Jingpu%20Cheng%20and%20Ting%20Lin%20and%20Zuowei%20Shen%20and%20Qianxiao%20Li&entry.1292438233=%20%20We%20investigate%20the%20universal%20approximation%20property%20%28UAP%29%20of%20transformer-type%0Aarchitectures%2C%20providing%20a%20unified%20theoretical%20framework%20that%20extends%20prior%0Aresults%20on%20residual%20networks%20to%20models%20incorporating%20attention%20mechanisms.%20Our%0Awork%20identifies%20token%20distinguishability%20as%20a%20fundamental%20requirement%20for%20UAP%0Aand%20introduces%20a%20general%20sufficient%20condition%20that%20applies%20to%20a%20broad%20class%20of%0Aarchitectures.%20Leveraging%20an%20analyticity%20assumption%20on%20the%20attention%20layer%2C%20we%0Acan%20significantly%20simplify%20the%20verification%20of%20this%20condition%2C%20providing%20a%0Anon-constructive%20approach%20in%20establishing%20UAP%20for%20such%20architectures.%20We%0Ademonstrate%20the%20applicability%20of%20our%20framework%20by%20proving%20UAP%20for%20transformers%0Awith%20various%20attention%20mechanisms%2C%20including%20kernel-based%20and%20sparse%20attention%0Amechanisms.%20The%20corollaries%20of%20our%20results%20either%20generalize%20prior%20works%20or%0Aestablish%20UAP%20for%20architectures%20not%20previously%20covered.%20Furthermore%2C%20our%0Aframework%20offers%20a%20principled%20foundation%20for%20designing%20novel%20transformer%0Aarchitectures%20with%20inherent%20UAP%20guarantees%2C%20including%20those%20with%20specific%0Afunctional%20symmetries.%20We%20propose%20examples%20to%20illustrate%20these%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23551v2&entry.124074799=Read"},
{"title": "A Geometric Approach to Steerable Convolutions", "author": "Soumyabrata Kundu and Risi Kondor", "abstract": "  In contrast to the somewhat abstract, group theoretical approach adopted by\nmany papers, our work provides a new and more intuitive derivation of steerable\nconvolutional neural networks in $d$ dimensions. This derivation is based on\ngeometric arguments and fundamental principles of pattern matching. We offer an\nintuitive explanation for the appearance of the Clebsch--Gordan decomposition\nand spherical harmonic basis functions. Furthermore, we suggest a novel way to\nconstruct steerable convolution layers using interpolation kernels that improve\nupon existing implementation, and offer greater robustness to noisy data.\n", "link": "http://arxiv.org/abs/2510.18813v1", "date": "2025-10-21", "relevancy": 2.033, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5339}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.512}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Geometric%20Approach%20to%20Steerable%20Convolutions&body=Title%3A%20A%20Geometric%20Approach%20to%20Steerable%20Convolutions%0AAuthor%3A%20Soumyabrata%20Kundu%20and%20Risi%20Kondor%0AAbstract%3A%20%20%20In%20contrast%20to%20the%20somewhat%20abstract%2C%20group%20theoretical%20approach%20adopted%20by%0Amany%20papers%2C%20our%20work%20provides%20a%20new%20and%20more%20intuitive%20derivation%20of%20steerable%0Aconvolutional%20neural%20networks%20in%20%24d%24%20dimensions.%20This%20derivation%20is%20based%20on%0Ageometric%20arguments%20and%20fundamental%20principles%20of%20pattern%20matching.%20We%20offer%20an%0Aintuitive%20explanation%20for%20the%20appearance%20of%20the%20Clebsch--Gordan%20decomposition%0Aand%20spherical%20harmonic%20basis%20functions.%20Furthermore%2C%20we%20suggest%20a%20novel%20way%20to%0Aconstruct%20steerable%20convolution%20layers%20using%20interpolation%20kernels%20that%20improve%0Aupon%20existing%20implementation%2C%20and%20offer%20greater%20robustness%20to%20noisy%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Geometric%2520Approach%2520to%2520Steerable%2520Convolutions%26entry.906535625%3DSoumyabrata%2520Kundu%2520and%2520Risi%2520Kondor%26entry.1292438233%3D%2520%2520In%2520contrast%2520to%2520the%2520somewhat%2520abstract%252C%2520group%2520theoretical%2520approach%2520adopted%2520by%250Amany%2520papers%252C%2520our%2520work%2520provides%2520a%2520new%2520and%2520more%2520intuitive%2520derivation%2520of%2520steerable%250Aconvolutional%2520neural%2520networks%2520in%2520%2524d%2524%2520dimensions.%2520This%2520derivation%2520is%2520based%2520on%250Ageometric%2520arguments%2520and%2520fundamental%2520principles%2520of%2520pattern%2520matching.%2520We%2520offer%2520an%250Aintuitive%2520explanation%2520for%2520the%2520appearance%2520of%2520the%2520Clebsch--Gordan%2520decomposition%250Aand%2520spherical%2520harmonic%2520basis%2520functions.%2520Furthermore%252C%2520we%2520suggest%2520a%2520novel%2520way%2520to%250Aconstruct%2520steerable%2520convolution%2520layers%2520using%2520interpolation%2520kernels%2520that%2520improve%250Aupon%2520existing%2520implementation%252C%2520and%2520offer%2520greater%2520robustness%2520to%2520noisy%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Geometric%20Approach%20to%20Steerable%20Convolutions&entry.906535625=Soumyabrata%20Kundu%20and%20Risi%20Kondor&entry.1292438233=%20%20In%20contrast%20to%20the%20somewhat%20abstract%2C%20group%20theoretical%20approach%20adopted%20by%0Amany%20papers%2C%20our%20work%20provides%20a%20new%20and%20more%20intuitive%20derivation%20of%20steerable%0Aconvolutional%20neural%20networks%20in%20%24d%24%20dimensions.%20This%20derivation%20is%20based%20on%0Ageometric%20arguments%20and%20fundamental%20principles%20of%20pattern%20matching.%20We%20offer%20an%0Aintuitive%20explanation%20for%20the%20appearance%20of%20the%20Clebsch--Gordan%20decomposition%0Aand%20spherical%20harmonic%20basis%20functions.%20Furthermore%2C%20we%20suggest%20a%20novel%20way%20to%0Aconstruct%20steerable%20convolution%20layers%20using%20interpolation%20kernels%20that%20improve%0Aupon%20existing%20implementation%2C%20and%20offer%20greater%20robustness%20to%20noisy%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18813v1&entry.124074799=Read"},
{"title": "How Transformers Learn In-Context Recall Tasks? Optimality, Training\n  Dynamics and Generalization", "author": "Quan Nguyen and Thanh Nguyen-Tang", "abstract": "  We study the approximation capabilities, convergence speeds and\non-convergence behaviors of transformers trained on in-context recall tasks --\nwhich requires to recognize the \\emph{positional} association between a pair of\ntokens from in-context examples. Existing theoretical results only focus on the\nin-context reasoning behavior of transformers after being trained for the\n\\emph{one} gradient descent step. It remains unclear what is the on-convergence\nbehavior of transformers being trained by gradient descent and how fast the\nconvergence rate is. In addition, the generalization of transformers in\none-step in-context reasoning has not been formally investigated. This work\naddresses these gaps. We first show that a class of transformers with either\nlinear, ReLU or softmax attentions, is provably Bayes-optimal for an in-context\nrecall task. When being trained with gradient descent, we show via a\nfinite-sample analysis that the expected loss converges at linear rate to the\nBayes risks. Moreover, we show that the trained transformers exhibit\nout-of-distribution (OOD) generalization, i.e., generalizing to samples outside\nof the population distribution. Our theoretical findings are further supported\nby extensive empirical validations, showing that \\emph{without} proper\nparameterization, models with larger expressive power surprisingly \\emph{fail}\nto generalize OOD after being trained by gradient descent.\n", "link": "http://arxiv.org/abs/2505.15009v3", "date": "2025-10-21", "relevancy": 2.0297, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5615}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Transformers%20Learn%20In-Context%20Recall%20Tasks%3F%20Optimality%2C%20Training%0A%20%20Dynamics%20and%20Generalization&body=Title%3A%20How%20Transformers%20Learn%20In-Context%20Recall%20Tasks%3F%20Optimality%2C%20Training%0A%20%20Dynamics%20and%20Generalization%0AAuthor%3A%20Quan%20Nguyen%20and%20Thanh%20Nguyen-Tang%0AAbstract%3A%20%20%20We%20study%20the%20approximation%20capabilities%2C%20convergence%20speeds%20and%0Aon-convergence%20behaviors%20of%20transformers%20trained%20on%20in-context%20recall%20tasks%20--%0Awhich%20requires%20to%20recognize%20the%20%5Cemph%7Bpositional%7D%20association%20between%20a%20pair%20of%0Atokens%20from%20in-context%20examples.%20Existing%20theoretical%20results%20only%20focus%20on%20the%0Ain-context%20reasoning%20behavior%20of%20transformers%20after%20being%20trained%20for%20the%0A%5Cemph%7Bone%7D%20gradient%20descent%20step.%20It%20remains%20unclear%20what%20is%20the%20on-convergence%0Abehavior%20of%20transformers%20being%20trained%20by%20gradient%20descent%20and%20how%20fast%20the%0Aconvergence%20rate%20is.%20In%20addition%2C%20the%20generalization%20of%20transformers%20in%0Aone-step%20in-context%20reasoning%20has%20not%20been%20formally%20investigated.%20This%20work%0Aaddresses%20these%20gaps.%20We%20first%20show%20that%20a%20class%20of%20transformers%20with%20either%0Alinear%2C%20ReLU%20or%20softmax%20attentions%2C%20is%20provably%20Bayes-optimal%20for%20an%20in-context%0Arecall%20task.%20When%20being%20trained%20with%20gradient%20descent%2C%20we%20show%20via%20a%0Afinite-sample%20analysis%20that%20the%20expected%20loss%20converges%20at%20linear%20rate%20to%20the%0ABayes%20risks.%20Moreover%2C%20we%20show%20that%20the%20trained%20transformers%20exhibit%0Aout-of-distribution%20%28OOD%29%20generalization%2C%20i.e.%2C%20generalizing%20to%20samples%20outside%0Aof%20the%20population%20distribution.%20Our%20theoretical%20findings%20are%20further%20supported%0Aby%20extensive%20empirical%20validations%2C%20showing%20that%20%5Cemph%7Bwithout%7D%20proper%0Aparameterization%2C%20models%20with%20larger%20expressive%20power%20surprisingly%20%5Cemph%7Bfail%7D%0Ato%20generalize%20OOD%20after%20being%20trained%20by%20gradient%20descent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15009v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Transformers%2520Learn%2520In-Context%2520Recall%2520Tasks%253F%2520Optimality%252C%2520Training%250A%2520%2520Dynamics%2520and%2520Generalization%26entry.906535625%3DQuan%2520Nguyen%2520and%2520Thanh%2520Nguyen-Tang%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520approximation%2520capabilities%252C%2520convergence%2520speeds%2520and%250Aon-convergence%2520behaviors%2520of%2520transformers%2520trained%2520on%2520in-context%2520recall%2520tasks%2520--%250Awhich%2520requires%2520to%2520recognize%2520the%2520%255Cemph%257Bpositional%257D%2520association%2520between%2520a%2520pair%2520of%250Atokens%2520from%2520in-context%2520examples.%2520Existing%2520theoretical%2520results%2520only%2520focus%2520on%2520the%250Ain-context%2520reasoning%2520behavior%2520of%2520transformers%2520after%2520being%2520trained%2520for%2520the%250A%255Cemph%257Bone%257D%2520gradient%2520descent%2520step.%2520It%2520remains%2520unclear%2520what%2520is%2520the%2520on-convergence%250Abehavior%2520of%2520transformers%2520being%2520trained%2520by%2520gradient%2520descent%2520and%2520how%2520fast%2520the%250Aconvergence%2520rate%2520is.%2520In%2520addition%252C%2520the%2520generalization%2520of%2520transformers%2520in%250Aone-step%2520in-context%2520reasoning%2520has%2520not%2520been%2520formally%2520investigated.%2520This%2520work%250Aaddresses%2520these%2520gaps.%2520We%2520first%2520show%2520that%2520a%2520class%2520of%2520transformers%2520with%2520either%250Alinear%252C%2520ReLU%2520or%2520softmax%2520attentions%252C%2520is%2520provably%2520Bayes-optimal%2520for%2520an%2520in-context%250Arecall%2520task.%2520When%2520being%2520trained%2520with%2520gradient%2520descent%252C%2520we%2520show%2520via%2520a%250Afinite-sample%2520analysis%2520that%2520the%2520expected%2520loss%2520converges%2520at%2520linear%2520rate%2520to%2520the%250ABayes%2520risks.%2520Moreover%252C%2520we%2520show%2520that%2520the%2520trained%2520transformers%2520exhibit%250Aout-of-distribution%2520%2528OOD%2529%2520generalization%252C%2520i.e.%252C%2520generalizing%2520to%2520samples%2520outside%250Aof%2520the%2520population%2520distribution.%2520Our%2520theoretical%2520findings%2520are%2520further%2520supported%250Aby%2520extensive%2520empirical%2520validations%252C%2520showing%2520that%2520%255Cemph%257Bwithout%257D%2520proper%250Aparameterization%252C%2520models%2520with%2520larger%2520expressive%2520power%2520surprisingly%2520%255Cemph%257Bfail%257D%250Ato%2520generalize%2520OOD%2520after%2520being%2520trained%2520by%2520gradient%2520descent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15009v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Transformers%20Learn%20In-Context%20Recall%20Tasks%3F%20Optimality%2C%20Training%0A%20%20Dynamics%20and%20Generalization&entry.906535625=Quan%20Nguyen%20and%20Thanh%20Nguyen-Tang&entry.1292438233=%20%20We%20study%20the%20approximation%20capabilities%2C%20convergence%20speeds%20and%0Aon-convergence%20behaviors%20of%20transformers%20trained%20on%20in-context%20recall%20tasks%20--%0Awhich%20requires%20to%20recognize%20the%20%5Cemph%7Bpositional%7D%20association%20between%20a%20pair%20of%0Atokens%20from%20in-context%20examples.%20Existing%20theoretical%20results%20only%20focus%20on%20the%0Ain-context%20reasoning%20behavior%20of%20transformers%20after%20being%20trained%20for%20the%0A%5Cemph%7Bone%7D%20gradient%20descent%20step.%20It%20remains%20unclear%20what%20is%20the%20on-convergence%0Abehavior%20of%20transformers%20being%20trained%20by%20gradient%20descent%20and%20how%20fast%20the%0Aconvergence%20rate%20is.%20In%20addition%2C%20the%20generalization%20of%20transformers%20in%0Aone-step%20in-context%20reasoning%20has%20not%20been%20formally%20investigated.%20This%20work%0Aaddresses%20these%20gaps.%20We%20first%20show%20that%20a%20class%20of%20transformers%20with%20either%0Alinear%2C%20ReLU%20or%20softmax%20attentions%2C%20is%20provably%20Bayes-optimal%20for%20an%20in-context%0Arecall%20task.%20When%20being%20trained%20with%20gradient%20descent%2C%20we%20show%20via%20a%0Afinite-sample%20analysis%20that%20the%20expected%20loss%20converges%20at%20linear%20rate%20to%20the%0ABayes%20risks.%20Moreover%2C%20we%20show%20that%20the%20trained%20transformers%20exhibit%0Aout-of-distribution%20%28OOD%29%20generalization%2C%20i.e.%2C%20generalizing%20to%20samples%20outside%0Aof%20the%20population%20distribution.%20Our%20theoretical%20findings%20are%20further%20supported%0Aby%20extensive%20empirical%20validations%2C%20showing%20that%20%5Cemph%7Bwithout%7D%20proper%0Aparameterization%2C%20models%20with%20larger%20expressive%20power%20surprisingly%20%5Cemph%7Bfail%7D%0Ato%20generalize%20OOD%20after%20being%20trained%20by%20gradient%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15009v3&entry.124074799=Read"},
{"title": "Decoding Funded Research: Comparative Analysis of Topic Models and\n  Uncovering the Effect of Gender and Geographic Location", "author": "Shirin Tavakoli Kafiabad and Andrea Schiffauerova and Ashkan Ebadi", "abstract": "  Optimizing national scientific investment requires a clear understanding of\nevolving research trends and the demographic and geographical forces shaping\nthem, particularly in light of commitments to equity, diversity, and inclusion.\nThis study addresses this need by analyzing 18 years (2005-2022) of research\nproposals funded by the Natural Sciences and Engineering Research Council of\nCanada (NSERC). We conducted a comprehensive comparative evaluation of three\ntopic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic\nModelling (STM), and BERTopic. We also introduced a novel algorithm, named\nCOFFEE, designed to enable robust covariate effect estimation for BERTopic.\nThis advancement addresses a significant gap, as BERTopic lacks a native\nfunction for covariate analysis, unlike the probabilistic STM. Our findings\nhighlight that while all models effectively delineate core scientific domains,\nBERTopic outperformed by consistently identifying more granular, coherent, and\nemergent themes, such as the rapid expansion of artificial intelligence.\nAdditionally, the covariate analysis, powered by COFFEE, confirmed distinct\nprovincial research specializations and revealed consistent gender-based\nthematic patterns across various scientific disciplines. These insights offer a\nrobust empirical foundation for funding organizations to formulate more\nequitable and impactful funding strategies, thereby enhancing the effectiveness\nof the scientific ecosystem.\n", "link": "http://arxiv.org/abs/2510.18803v1", "date": "2025-10-21", "relevancy": 2.0171, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Funded%20Research%3A%20Comparative%20Analysis%20of%20Topic%20Models%20and%0A%20%20Uncovering%20the%20Effect%20of%20Gender%20and%20Geographic%20Location&body=Title%3A%20Decoding%20Funded%20Research%3A%20Comparative%20Analysis%20of%20Topic%20Models%20and%0A%20%20Uncovering%20the%20Effect%20of%20Gender%20and%20Geographic%20Location%0AAuthor%3A%20Shirin%20Tavakoli%20Kafiabad%20and%20Andrea%20Schiffauerova%20and%20Ashkan%20Ebadi%0AAbstract%3A%20%20%20Optimizing%20national%20scientific%20investment%20requires%20a%20clear%20understanding%20of%0Aevolving%20research%20trends%20and%20the%20demographic%20and%20geographical%20forces%20shaping%0Athem%2C%20particularly%20in%20light%20of%20commitments%20to%20equity%2C%20diversity%2C%20and%20inclusion.%0AThis%20study%20addresses%20this%20need%20by%20analyzing%2018%20years%20%282005-2022%29%20of%20research%0Aproposals%20funded%20by%20the%20Natural%20Sciences%20and%20Engineering%20Research%20Council%20of%0ACanada%20%28NSERC%29.%20We%20conducted%20a%20comprehensive%20comparative%20evaluation%20of%20three%0Atopic%20modelling%20approaches%3A%20Latent%20Dirichlet%20Allocation%20%28LDA%29%2C%20Structural%20Topic%0AModelling%20%28STM%29%2C%20and%20BERTopic.%20We%20also%20introduced%20a%20novel%20algorithm%2C%20named%0ACOFFEE%2C%20designed%20to%20enable%20robust%20covariate%20effect%20estimation%20for%20BERTopic.%0AThis%20advancement%20addresses%20a%20significant%20gap%2C%20as%20BERTopic%20lacks%20a%20native%0Afunction%20for%20covariate%20analysis%2C%20unlike%20the%20probabilistic%20STM.%20Our%20findings%0Ahighlight%20that%20while%20all%20models%20effectively%20delineate%20core%20scientific%20domains%2C%0ABERTopic%20outperformed%20by%20consistently%20identifying%20more%20granular%2C%20coherent%2C%20and%0Aemergent%20themes%2C%20such%20as%20the%20rapid%20expansion%20of%20artificial%20intelligence.%0AAdditionally%2C%20the%20covariate%20analysis%2C%20powered%20by%20COFFEE%2C%20confirmed%20distinct%0Aprovincial%20research%20specializations%20and%20revealed%20consistent%20gender-based%0Athematic%20patterns%20across%20various%20scientific%20disciplines.%20These%20insights%20offer%20a%0Arobust%20empirical%20foundation%20for%20funding%20organizations%20to%20formulate%20more%0Aequitable%20and%20impactful%20funding%20strategies%2C%20thereby%20enhancing%20the%20effectiveness%0Aof%20the%20scientific%20ecosystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Funded%2520Research%253A%2520Comparative%2520Analysis%2520of%2520Topic%2520Models%2520and%250A%2520%2520Uncovering%2520the%2520Effect%2520of%2520Gender%2520and%2520Geographic%2520Location%26entry.906535625%3DShirin%2520Tavakoli%2520Kafiabad%2520and%2520Andrea%2520Schiffauerova%2520and%2520Ashkan%2520Ebadi%26entry.1292438233%3D%2520%2520Optimizing%2520national%2520scientific%2520investment%2520requires%2520a%2520clear%2520understanding%2520of%250Aevolving%2520research%2520trends%2520and%2520the%2520demographic%2520and%2520geographical%2520forces%2520shaping%250Athem%252C%2520particularly%2520in%2520light%2520of%2520commitments%2520to%2520equity%252C%2520diversity%252C%2520and%2520inclusion.%250AThis%2520study%2520addresses%2520this%2520need%2520by%2520analyzing%252018%2520years%2520%25282005-2022%2529%2520of%2520research%250Aproposals%2520funded%2520by%2520the%2520Natural%2520Sciences%2520and%2520Engineering%2520Research%2520Council%2520of%250ACanada%2520%2528NSERC%2529.%2520We%2520conducted%2520a%2520comprehensive%2520comparative%2520evaluation%2520of%2520three%250Atopic%2520modelling%2520approaches%253A%2520Latent%2520Dirichlet%2520Allocation%2520%2528LDA%2529%252C%2520Structural%2520Topic%250AModelling%2520%2528STM%2529%252C%2520and%2520BERTopic.%2520We%2520also%2520introduced%2520a%2520novel%2520algorithm%252C%2520named%250ACOFFEE%252C%2520designed%2520to%2520enable%2520robust%2520covariate%2520effect%2520estimation%2520for%2520BERTopic.%250AThis%2520advancement%2520addresses%2520a%2520significant%2520gap%252C%2520as%2520BERTopic%2520lacks%2520a%2520native%250Afunction%2520for%2520covariate%2520analysis%252C%2520unlike%2520the%2520probabilistic%2520STM.%2520Our%2520findings%250Ahighlight%2520that%2520while%2520all%2520models%2520effectively%2520delineate%2520core%2520scientific%2520domains%252C%250ABERTopic%2520outperformed%2520by%2520consistently%2520identifying%2520more%2520granular%252C%2520coherent%252C%2520and%250Aemergent%2520themes%252C%2520such%2520as%2520the%2520rapid%2520expansion%2520of%2520artificial%2520intelligence.%250AAdditionally%252C%2520the%2520covariate%2520analysis%252C%2520powered%2520by%2520COFFEE%252C%2520confirmed%2520distinct%250Aprovincial%2520research%2520specializations%2520and%2520revealed%2520consistent%2520gender-based%250Athematic%2520patterns%2520across%2520various%2520scientific%2520disciplines.%2520These%2520insights%2520offer%2520a%250Arobust%2520empirical%2520foundation%2520for%2520funding%2520organizations%2520to%2520formulate%2520more%250Aequitable%2520and%2520impactful%2520funding%2520strategies%252C%2520thereby%2520enhancing%2520the%2520effectiveness%250Aof%2520the%2520scientific%2520ecosystem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Funded%20Research%3A%20Comparative%20Analysis%20of%20Topic%20Models%20and%0A%20%20Uncovering%20the%20Effect%20of%20Gender%20and%20Geographic%20Location&entry.906535625=Shirin%20Tavakoli%20Kafiabad%20and%20Andrea%20Schiffauerova%20and%20Ashkan%20Ebadi&entry.1292438233=%20%20Optimizing%20national%20scientific%20investment%20requires%20a%20clear%20understanding%20of%0Aevolving%20research%20trends%20and%20the%20demographic%20and%20geographical%20forces%20shaping%0Athem%2C%20particularly%20in%20light%20of%20commitments%20to%20equity%2C%20diversity%2C%20and%20inclusion.%0AThis%20study%20addresses%20this%20need%20by%20analyzing%2018%20years%20%282005-2022%29%20of%20research%0Aproposals%20funded%20by%20the%20Natural%20Sciences%20and%20Engineering%20Research%20Council%20of%0ACanada%20%28NSERC%29.%20We%20conducted%20a%20comprehensive%20comparative%20evaluation%20of%20three%0Atopic%20modelling%20approaches%3A%20Latent%20Dirichlet%20Allocation%20%28LDA%29%2C%20Structural%20Topic%0AModelling%20%28STM%29%2C%20and%20BERTopic.%20We%20also%20introduced%20a%20novel%20algorithm%2C%20named%0ACOFFEE%2C%20designed%20to%20enable%20robust%20covariate%20effect%20estimation%20for%20BERTopic.%0AThis%20advancement%20addresses%20a%20significant%20gap%2C%20as%20BERTopic%20lacks%20a%20native%0Afunction%20for%20covariate%20analysis%2C%20unlike%20the%20probabilistic%20STM.%20Our%20findings%0Ahighlight%20that%20while%20all%20models%20effectively%20delineate%20core%20scientific%20domains%2C%0ABERTopic%20outperformed%20by%20consistently%20identifying%20more%20granular%2C%20coherent%2C%20and%0Aemergent%20themes%2C%20such%20as%20the%20rapid%20expansion%20of%20artificial%20intelligence.%0AAdditionally%2C%20the%20covariate%20analysis%2C%20powered%20by%20COFFEE%2C%20confirmed%20distinct%0Aprovincial%20research%20specializations%20and%20revealed%20consistent%20gender-based%0Athematic%20patterns%20across%20various%20scientific%20disciplines.%20These%20insights%20offer%20a%0Arobust%20empirical%20foundation%20for%20funding%20organizations%20to%20formulate%20more%0Aequitable%20and%20impactful%20funding%20strategies%2C%20thereby%20enhancing%20the%20effectiveness%0Aof%20the%20scientific%20ecosystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18803v1&entry.124074799=Read"},
{"title": "FedMeld: A Model-dispersal Federated Learning Framework for Space-ground\n  Integrated Networks", "author": "Qian Chen and Xianhao Chen and Kaibin Huang", "abstract": "  To bridge the digital divide, the space-ground integrated networks (SGINs),\nwhich will be a key component of the six-generation (6G) mobile networks, are\nexpected to deliver artificial intelligence (AI) services to every corner of\nthe world. One mission of SGINs is to support federated learning (FL) at a\nglobal scale. However, existing space-ground integrated FL frameworks involve\nground stations or costly inter-satellite links, entailing excessive training\nlatency and communication costs. To overcome these limitations, we propose an\ninfrastructure-free federated learning framework based on a model dispersal\n(FedMeld) strategy, which exploits periodic movement patterns and\nstore-carry-forward capabilities of satellites to enable parameter mixing\nacross large-scale geographical regions. We theoretically show that FedMeld\nleads to global model convergence and quantify the effects of round interval\nand mixing ratio between adjacent areas on its learning performance. Based on\nthe theoretical results, we formulate a joint optimization problem to design\nthe staleness control and mixing ratio (SC-MR) for minimizing the training\nloss. By decomposing the problem into sequential SC and MR subproblems without\ncompromising the optimality, we derive the round interval solution in a closed\nform and the mixing ratio in a semi-closed form to achieve the optimal\nlatency-accuracy tradeoff. Experiments using various datasets demonstrate that\nFedMeld achieves superior model accuracy while significantly reducing\ncommunication costs as compared with traditional FL schemes for SGINs.\n", "link": "http://arxiv.org/abs/2412.17231v2", "date": "2025-10-21", "relevancy": 1.9968, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5144}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.497}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedMeld%3A%20A%20Model-dispersal%20Federated%20Learning%20Framework%20for%20Space-ground%0A%20%20Integrated%20Networks&body=Title%3A%20FedMeld%3A%20A%20Model-dispersal%20Federated%20Learning%20Framework%20for%20Space-ground%0A%20%20Integrated%20Networks%0AAuthor%3A%20Qian%20Chen%20and%20Xianhao%20Chen%20and%20Kaibin%20Huang%0AAbstract%3A%20%20%20To%20bridge%20the%20digital%20divide%2C%20the%20space-ground%20integrated%20networks%20%28SGINs%29%2C%0Awhich%20will%20be%20a%20key%20component%20of%20the%20six-generation%20%286G%29%20mobile%20networks%2C%20are%0Aexpected%20to%20deliver%20artificial%20intelligence%20%28AI%29%20services%20to%20every%20corner%20of%0Athe%20world.%20One%20mission%20of%20SGINs%20is%20to%20support%20federated%20learning%20%28FL%29%20at%20a%0Aglobal%20scale.%20However%2C%20existing%20space-ground%20integrated%20FL%20frameworks%20involve%0Aground%20stations%20or%20costly%20inter-satellite%20links%2C%20entailing%20excessive%20training%0Alatency%20and%20communication%20costs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%0Ainfrastructure-free%20federated%20learning%20framework%20based%20on%20a%20model%20dispersal%0A%28FedMeld%29%20strategy%2C%20which%20exploits%20periodic%20movement%20patterns%20and%0Astore-carry-forward%20capabilities%20of%20satellites%20to%20enable%20parameter%20mixing%0Aacross%20large-scale%20geographical%20regions.%20We%20theoretically%20show%20that%20FedMeld%0Aleads%20to%20global%20model%20convergence%20and%20quantify%20the%20effects%20of%20round%20interval%0Aand%20mixing%20ratio%20between%20adjacent%20areas%20on%20its%20learning%20performance.%20Based%20on%0Athe%20theoretical%20results%2C%20we%20formulate%20a%20joint%20optimization%20problem%20to%20design%0Athe%20staleness%20control%20and%20mixing%20ratio%20%28SC-MR%29%20for%20minimizing%20the%20training%0Aloss.%20By%20decomposing%20the%20problem%20into%20sequential%20SC%20and%20MR%20subproblems%20without%0Acompromising%20the%20optimality%2C%20we%20derive%20the%20round%20interval%20solution%20in%20a%20closed%0Aform%20and%20the%20mixing%20ratio%20in%20a%20semi-closed%20form%20to%20achieve%20the%20optimal%0Alatency-accuracy%20tradeoff.%20Experiments%20using%20various%20datasets%20demonstrate%20that%0AFedMeld%20achieves%20superior%20model%20accuracy%20while%20significantly%20reducing%0Acommunication%20costs%20as%20compared%20with%20traditional%20FL%20schemes%20for%20SGINs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedMeld%253A%2520A%2520Model-dispersal%2520Federated%2520Learning%2520Framework%2520for%2520Space-ground%250A%2520%2520Integrated%2520Networks%26entry.906535625%3DQian%2520Chen%2520and%2520Xianhao%2520Chen%2520and%2520Kaibin%2520Huang%26entry.1292438233%3D%2520%2520To%2520bridge%2520the%2520digital%2520divide%252C%2520the%2520space-ground%2520integrated%2520networks%2520%2528SGINs%2529%252C%250Awhich%2520will%2520be%2520a%2520key%2520component%2520of%2520the%2520six-generation%2520%25286G%2529%2520mobile%2520networks%252C%2520are%250Aexpected%2520to%2520deliver%2520artificial%2520intelligence%2520%2528AI%2529%2520services%2520to%2520every%2520corner%2520of%250Athe%2520world.%2520One%2520mission%2520of%2520SGINs%2520is%2520to%2520support%2520federated%2520learning%2520%2528FL%2529%2520at%2520a%250Aglobal%2520scale.%2520However%252C%2520existing%2520space-ground%2520integrated%2520FL%2520frameworks%2520involve%250Aground%2520stations%2520or%2520costly%2520inter-satellite%2520links%252C%2520entailing%2520excessive%2520training%250Alatency%2520and%2520communication%2520costs.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520an%250Ainfrastructure-free%2520federated%2520learning%2520framework%2520based%2520on%2520a%2520model%2520dispersal%250A%2528FedMeld%2529%2520strategy%252C%2520which%2520exploits%2520periodic%2520movement%2520patterns%2520and%250Astore-carry-forward%2520capabilities%2520of%2520satellites%2520to%2520enable%2520parameter%2520mixing%250Aacross%2520large-scale%2520geographical%2520regions.%2520We%2520theoretically%2520show%2520that%2520FedMeld%250Aleads%2520to%2520global%2520model%2520convergence%2520and%2520quantify%2520the%2520effects%2520of%2520round%2520interval%250Aand%2520mixing%2520ratio%2520between%2520adjacent%2520areas%2520on%2520its%2520learning%2520performance.%2520Based%2520on%250Athe%2520theoretical%2520results%252C%2520we%2520formulate%2520a%2520joint%2520optimization%2520problem%2520to%2520design%250Athe%2520staleness%2520control%2520and%2520mixing%2520ratio%2520%2528SC-MR%2529%2520for%2520minimizing%2520the%2520training%250Aloss.%2520By%2520decomposing%2520the%2520problem%2520into%2520sequential%2520SC%2520and%2520MR%2520subproblems%2520without%250Acompromising%2520the%2520optimality%252C%2520we%2520derive%2520the%2520round%2520interval%2520solution%2520in%2520a%2520closed%250Aform%2520and%2520the%2520mixing%2520ratio%2520in%2520a%2520semi-closed%2520form%2520to%2520achieve%2520the%2520optimal%250Alatency-accuracy%2520tradeoff.%2520Experiments%2520using%2520various%2520datasets%2520demonstrate%2520that%250AFedMeld%2520achieves%2520superior%2520model%2520accuracy%2520while%2520significantly%2520reducing%250Acommunication%2520costs%2520as%2520compared%2520with%2520traditional%2520FL%2520schemes%2520for%2520SGINs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedMeld%3A%20A%20Model-dispersal%20Federated%20Learning%20Framework%20for%20Space-ground%0A%20%20Integrated%20Networks&entry.906535625=Qian%20Chen%20and%20Xianhao%20Chen%20and%20Kaibin%20Huang&entry.1292438233=%20%20To%20bridge%20the%20digital%20divide%2C%20the%20space-ground%20integrated%20networks%20%28SGINs%29%2C%0Awhich%20will%20be%20a%20key%20component%20of%20the%20six-generation%20%286G%29%20mobile%20networks%2C%20are%0Aexpected%20to%20deliver%20artificial%20intelligence%20%28AI%29%20services%20to%20every%20corner%20of%0Athe%20world.%20One%20mission%20of%20SGINs%20is%20to%20support%20federated%20learning%20%28FL%29%20at%20a%0Aglobal%20scale.%20However%2C%20existing%20space-ground%20integrated%20FL%20frameworks%20involve%0Aground%20stations%20or%20costly%20inter-satellite%20links%2C%20entailing%20excessive%20training%0Alatency%20and%20communication%20costs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%0Ainfrastructure-free%20federated%20learning%20framework%20based%20on%20a%20model%20dispersal%0A%28FedMeld%29%20strategy%2C%20which%20exploits%20periodic%20movement%20patterns%20and%0Astore-carry-forward%20capabilities%20of%20satellites%20to%20enable%20parameter%20mixing%0Aacross%20large-scale%20geographical%20regions.%20We%20theoretically%20show%20that%20FedMeld%0Aleads%20to%20global%20model%20convergence%20and%20quantify%20the%20effects%20of%20round%20interval%0Aand%20mixing%20ratio%20between%20adjacent%20areas%20on%20its%20learning%20performance.%20Based%20on%0Athe%20theoretical%20results%2C%20we%20formulate%20a%20joint%20optimization%20problem%20to%20design%0Athe%20staleness%20control%20and%20mixing%20ratio%20%28SC-MR%29%20for%20minimizing%20the%20training%0Aloss.%20By%20decomposing%20the%20problem%20into%20sequential%20SC%20and%20MR%20subproblems%20without%0Acompromising%20the%20optimality%2C%20we%20derive%20the%20round%20interval%20solution%20in%20a%20closed%0Aform%20and%20the%20mixing%20ratio%20in%20a%20semi-closed%20form%20to%20achieve%20the%20optimal%0Alatency-accuracy%20tradeoff.%20Experiments%20using%20various%20datasets%20demonstrate%20that%0AFedMeld%20achieves%20superior%20model%20accuracy%20while%20significantly%20reducing%0Acommunication%20costs%20as%20compared%20with%20traditional%20FL%20schemes%20for%20SGINs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17231v2&entry.124074799=Read"},
{"title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based\n  Structures", "author": "Mihir Gupte and Paolo Giusto and Ramesh S", "abstract": "  Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures.\n", "link": "http://arxiv.org/abs/2510.10806v2", "date": "2025-10-21", "relevancy": 1.9849, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Implicit%20Knowledge%20Enough%20for%20LLMs%3F%20A%20RAG%20Approach%20for%20Tree-based%0A%20%20Structures&body=Title%3A%20Is%20Implicit%20Knowledge%20Enough%20for%20LLMs%3F%20A%20RAG%20Approach%20for%20Tree-based%0A%20%20Structures%0AAuthor%3A%20Mihir%20Gupte%20and%20Paolo%20Giusto%20and%20Ramesh%20S%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20adept%20at%20generating%20responses%20based%20on%0Ainformation%20within%20their%20context.%20While%20this%20ability%20is%20useful%20for%20interacting%0Awith%20structured%20data%20like%20code%20files%2C%20another%20popular%20method%2C%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20retrieves%20relevant%20documents%20to%20augment%0Athe%20model%27s%20in-context%20learning.%20However%2C%20it%20is%20not%20well-explored%20how%20to%20best%0Arepresent%20this%20retrieved%20knowledge%20for%20generating%20responses%20on%20structured%20data%2C%0Aparticularly%20hierarchical%20structures%20like%20trees.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20bottom-up%20method%20to%20linearize%20knowledge%20from%20tree-like%20structures%20%28like%20a%0AGitHub%20repository%29%20by%20generating%20implicit%2C%20aggregated%20summaries%20at%20each%0Ahierarchical%20level.%20This%20approach%20enables%20the%20knowledge%20to%20be%20stored%20in%20a%0Aknowledge%20base%20and%20used%20directly%20with%20RAG.%20We%20then%20compare%20our%20method%20to%20using%0ARAG%20on%20raw%2C%20unstructured%20code%2C%20evaluating%20the%20accuracy%20and%20quality%20of%20the%0Agenerated%20responses.%20Our%20results%20show%20that%20while%20response%20quality%20is%20comparable%0Aacross%20both%20methods%2C%20our%20approach%20generates%20over%2068%25%20fewer%20documents%20in%20the%0Aretriever%2C%20a%20significant%20gain%20in%20efficiency.%20This%20finding%20suggests%20that%0Aleveraging%20implicit%2C%20linearized%20knowledge%20may%20be%20a%20highly%20effective%20and%0Ascalable%20strategy%20for%20handling%20complex%2C%20hierarchical%20data%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.10806v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Implicit%2520Knowledge%2520Enough%2520for%2520LLMs%253F%2520A%2520RAG%2520Approach%2520for%2520Tree-based%250A%2520%2520Structures%26entry.906535625%3DMihir%2520Gupte%2520and%2520Paolo%2520Giusto%2520and%2520Ramesh%2520S%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520adept%2520at%2520generating%2520responses%2520based%2520on%250Ainformation%2520within%2520their%2520context.%2520While%2520this%2520ability%2520is%2520useful%2520for%2520interacting%250Awith%2520structured%2520data%2520like%2520code%2520files%252C%2520another%2520popular%2520method%252C%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%252C%2520retrieves%2520relevant%2520documents%2520to%2520augment%250Athe%2520model%2527s%2520in-context%2520learning.%2520However%252C%2520it%2520is%2520not%2520well-explored%2520how%2520to%2520best%250Arepresent%2520this%2520retrieved%2520knowledge%2520for%2520generating%2520responses%2520on%2520structured%2520data%252C%250Aparticularly%2520hierarchical%2520structures%2520like%2520trees.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520bottom-up%2520method%2520to%2520linearize%2520knowledge%2520from%2520tree-like%2520structures%2520%2528like%2520a%250AGitHub%2520repository%2529%2520by%2520generating%2520implicit%252C%2520aggregated%2520summaries%2520at%2520each%250Ahierarchical%2520level.%2520This%2520approach%2520enables%2520the%2520knowledge%2520to%2520be%2520stored%2520in%2520a%250Aknowledge%2520base%2520and%2520used%2520directly%2520with%2520RAG.%2520We%2520then%2520compare%2520our%2520method%2520to%2520using%250ARAG%2520on%2520raw%252C%2520unstructured%2520code%252C%2520evaluating%2520the%2520accuracy%2520and%2520quality%2520of%2520the%250Agenerated%2520responses.%2520Our%2520results%2520show%2520that%2520while%2520response%2520quality%2520is%2520comparable%250Aacross%2520both%2520methods%252C%2520our%2520approach%2520generates%2520over%252068%2525%2520fewer%2520documents%2520in%2520the%250Aretriever%252C%2520a%2520significant%2520gain%2520in%2520efficiency.%2520This%2520finding%2520suggests%2520that%250Aleveraging%2520implicit%252C%2520linearized%2520knowledge%2520may%2520be%2520a%2520highly%2520effective%2520and%250Ascalable%2520strategy%2520for%2520handling%2520complex%252C%2520hierarchical%2520data%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10806v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Implicit%20Knowledge%20Enough%20for%20LLMs%3F%20A%20RAG%20Approach%20for%20Tree-based%0A%20%20Structures&entry.906535625=Mihir%20Gupte%20and%20Paolo%20Giusto%20and%20Ramesh%20S&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20adept%20at%20generating%20responses%20based%20on%0Ainformation%20within%20their%20context.%20While%20this%20ability%20is%20useful%20for%20interacting%0Awith%20structured%20data%20like%20code%20files%2C%20another%20popular%20method%2C%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20retrieves%20relevant%20documents%20to%20augment%0Athe%20model%27s%20in-context%20learning.%20However%2C%20it%20is%20not%20well-explored%20how%20to%20best%0Arepresent%20this%20retrieved%20knowledge%20for%20generating%20responses%20on%20structured%20data%2C%0Aparticularly%20hierarchical%20structures%20like%20trees.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20bottom-up%20method%20to%20linearize%20knowledge%20from%20tree-like%20structures%20%28like%20a%0AGitHub%20repository%29%20by%20generating%20implicit%2C%20aggregated%20summaries%20at%20each%0Ahierarchical%20level.%20This%20approach%20enables%20the%20knowledge%20to%20be%20stored%20in%20a%0Aknowledge%20base%20and%20used%20directly%20with%20RAG.%20We%20then%20compare%20our%20method%20to%20using%0ARAG%20on%20raw%2C%20unstructured%20code%2C%20evaluating%20the%20accuracy%20and%20quality%20of%20the%0Agenerated%20responses.%20Our%20results%20show%20that%20while%20response%20quality%20is%20comparable%0Aacross%20both%20methods%2C%20our%20approach%20generates%20over%2068%25%20fewer%20documents%20in%20the%0Aretriever%2C%20a%20significant%20gain%20in%20efficiency.%20This%20finding%20suggests%20that%0Aleveraging%20implicit%2C%20linearized%20knowledge%20may%20be%20a%20highly%20effective%20and%0Ascalable%20strategy%20for%20handling%20complex%2C%20hierarchical%20data%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.10806v2&entry.124074799=Read"},
{"title": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode\n  Accelerator with Table-Lookup Matmul on Edge FPGAs", "author": "Ye Qiao and Zhiheng Chen and Yifan Zhang and Yian Wang and Sitao Huang", "abstract": "  With the emergence of wearable devices and other embedded systems, deploying\nlarge language models (LLMs) on edge platforms has become an urgent need.\nHowever, this is challenging because of their high computational and memory\ndemands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)\ncompress weights to as low as 1.58~bits with minimal accuracy loss, edge\ndeployment is still constrained by limited on-chip resources, power budgets,\nand the often-neglected long latency of the prefill stage. We present\n\\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for\nlow-power edge FPGAs that fully supports both prefill and autoregressive\ndecoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates\nseveral novel techniques, including (1) a table-lookup-based ternary matrix\nmultiplication (TLMM) engine utilizing grouped activations and online\nprecomputation for low resource utilization and high throughput; (2) a\nfine-grained analytic URAM-based weight buffer management scheme for efficient\nloading and compute engine access; (3) a streaming dataflow architecture that\nfuses floating-point element-wise operations with linear computations to hide\nlatency; (4) a reversed-reordered prefill stage attention with fused attention\noperations for high memory efficiency; and (5) a resource-efficient specialized\ndecoding stage attention. Under a 5~W power budget, TeLLMe delivers up to\n25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for\n64--128 token prompts, marking a significant energy-efficiency advancement in\nLLM inference on edge FPGAs.\n", "link": "http://arxiv.org/abs/2510.15926v2", "date": "2025-10-21", "relevancy": 1.98, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5078}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeLLMe%20v2%3A%20An%20Efficient%20End-to-End%20Ternary%20LLM%20Prefill%20and%20Decode%0A%20%20Accelerator%20with%20Table-Lookup%20Matmul%20on%20Edge%20FPGAs&body=Title%3A%20TeLLMe%20v2%3A%20An%20Efficient%20End-to-End%20Ternary%20LLM%20Prefill%20and%20Decode%0A%20%20Accelerator%20with%20Table-Lookup%20Matmul%20on%20Edge%20FPGAs%0AAuthor%3A%20Ye%20Qiao%20and%20Zhiheng%20Chen%20and%20Yifan%20Zhang%20and%20Yian%20Wang%20and%20Sitao%20Huang%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20wearable%20devices%20and%20other%20embedded%20systems%2C%20deploying%0Alarge%20language%20models%20%28LLMs%29%20on%20edge%20platforms%20has%20become%20an%20urgent%20need.%0AHowever%2C%20this%20is%20challenging%20because%20of%20their%20high%20computational%20and%20memory%0Ademands.%20Although%20recent%20low-bit%20quantization%20methods%20%28e.g.%2C%20BitNet%2C%20DeepSeek%29%0Acompress%20weights%20to%20as%20low%20as%201.58~bits%20with%20minimal%20accuracy%20loss%2C%20edge%0Adeployment%20is%20still%20constrained%20by%20limited%20on-chip%20resources%2C%20power%20budgets%2C%0Aand%20the%20often-neglected%20long%20latency%20of%20the%20prefill%20stage.%20We%20present%0A%5Ctextbf%7BTeLLMe%7D%2C%20the%20first%20table-lookup-based%20ternary%20LLM%20accelerator%20for%0Alow-power%20edge%20FPGAs%20that%20fully%20supports%20both%20prefill%20and%20autoregressive%0Adecoding%20using%201.58-bit%20weights%20and%208-bit%20activations.%20TeLLMe%20incorporates%0Aseveral%20novel%20techniques%2C%20including%20%281%29%20a%20table-lookup-based%20ternary%20matrix%0Amultiplication%20%28TLMM%29%20engine%20utilizing%20grouped%20activations%20and%20online%0Aprecomputation%20for%20low%20resource%20utilization%20and%20high%20throughput%3B%20%282%29%20a%0Afine-grained%20analytic%20URAM-based%20weight%20buffer%20management%20scheme%20for%20efficient%0Aloading%20and%20compute%20engine%20access%3B%20%283%29%20a%20streaming%20dataflow%20architecture%20that%0Afuses%20floating-point%20element-wise%20operations%20with%20linear%20computations%20to%20hide%0Alatency%3B%20%284%29%20a%20reversed-reordered%20prefill%20stage%20attention%20with%20fused%20attention%0Aoperations%20for%20high%20memory%20efficiency%3B%20and%20%285%29%20a%20resource-efficient%20specialized%0Adecoding%20stage%20attention.%20Under%20a%205~W%20power%20budget%2C%20TeLLMe%20delivers%20up%20to%0A25~tokens/s%20decoding%20throughput%20and%200.45--0.96~s%20time-to-first-token%20%28TTFT%29%20for%0A64--128%20token%20prompts%2C%20marking%20a%20significant%20energy-efficiency%20advancement%20in%0ALLM%20inference%20on%20edge%20FPGAs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeLLMe%2520v2%253A%2520An%2520Efficient%2520End-to-End%2520Ternary%2520LLM%2520Prefill%2520and%2520Decode%250A%2520%2520Accelerator%2520with%2520Table-Lookup%2520Matmul%2520on%2520Edge%2520FPGAs%26entry.906535625%3DYe%2520Qiao%2520and%2520Zhiheng%2520Chen%2520and%2520Yifan%2520Zhang%2520and%2520Yian%2520Wang%2520and%2520Sitao%2520Huang%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520wearable%2520devices%2520and%2520other%2520embedded%2520systems%252C%2520deploying%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520on%2520edge%2520platforms%2520has%2520become%2520an%2520urgent%2520need.%250AHowever%252C%2520this%2520is%2520challenging%2520because%2520of%2520their%2520high%2520computational%2520and%2520memory%250Ademands.%2520Although%2520recent%2520low-bit%2520quantization%2520methods%2520%2528e.g.%252C%2520BitNet%252C%2520DeepSeek%2529%250Acompress%2520weights%2520to%2520as%2520low%2520as%25201.58~bits%2520with%2520minimal%2520accuracy%2520loss%252C%2520edge%250Adeployment%2520is%2520still%2520constrained%2520by%2520limited%2520on-chip%2520resources%252C%2520power%2520budgets%252C%250Aand%2520the%2520often-neglected%2520long%2520latency%2520of%2520the%2520prefill%2520stage.%2520We%2520present%250A%255Ctextbf%257BTeLLMe%257D%252C%2520the%2520first%2520table-lookup-based%2520ternary%2520LLM%2520accelerator%2520for%250Alow-power%2520edge%2520FPGAs%2520that%2520fully%2520supports%2520both%2520prefill%2520and%2520autoregressive%250Adecoding%2520using%25201.58-bit%2520weights%2520and%25208-bit%2520activations.%2520TeLLMe%2520incorporates%250Aseveral%2520novel%2520techniques%252C%2520including%2520%25281%2529%2520a%2520table-lookup-based%2520ternary%2520matrix%250Amultiplication%2520%2528TLMM%2529%2520engine%2520utilizing%2520grouped%2520activations%2520and%2520online%250Aprecomputation%2520for%2520low%2520resource%2520utilization%2520and%2520high%2520throughput%253B%2520%25282%2529%2520a%250Afine-grained%2520analytic%2520URAM-based%2520weight%2520buffer%2520management%2520scheme%2520for%2520efficient%250Aloading%2520and%2520compute%2520engine%2520access%253B%2520%25283%2529%2520a%2520streaming%2520dataflow%2520architecture%2520that%250Afuses%2520floating-point%2520element-wise%2520operations%2520with%2520linear%2520computations%2520to%2520hide%250Alatency%253B%2520%25284%2529%2520a%2520reversed-reordered%2520prefill%2520stage%2520attention%2520with%2520fused%2520attention%250Aoperations%2520for%2520high%2520memory%2520efficiency%253B%2520and%2520%25285%2529%2520a%2520resource-efficient%2520specialized%250Adecoding%2520stage%2520attention.%2520Under%2520a%25205~W%2520power%2520budget%252C%2520TeLLMe%2520delivers%2520up%2520to%250A25~tokens/s%2520decoding%2520throughput%2520and%25200.45--0.96~s%2520time-to-first-token%2520%2528TTFT%2529%2520for%250A64--128%2520token%2520prompts%252C%2520marking%2520a%2520significant%2520energy-efficiency%2520advancement%2520in%250ALLM%2520inference%2520on%2520edge%2520FPGAs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeLLMe%20v2%3A%20An%20Efficient%20End-to-End%20Ternary%20LLM%20Prefill%20and%20Decode%0A%20%20Accelerator%20with%20Table-Lookup%20Matmul%20on%20Edge%20FPGAs&entry.906535625=Ye%20Qiao%20and%20Zhiheng%20Chen%20and%20Yifan%20Zhang%20and%20Yian%20Wang%20and%20Sitao%20Huang&entry.1292438233=%20%20With%20the%20emergence%20of%20wearable%20devices%20and%20other%20embedded%20systems%2C%20deploying%0Alarge%20language%20models%20%28LLMs%29%20on%20edge%20platforms%20has%20become%20an%20urgent%20need.%0AHowever%2C%20this%20is%20challenging%20because%20of%20their%20high%20computational%20and%20memory%0Ademands.%20Although%20recent%20low-bit%20quantization%20methods%20%28e.g.%2C%20BitNet%2C%20DeepSeek%29%0Acompress%20weights%20to%20as%20low%20as%201.58~bits%20with%20minimal%20accuracy%20loss%2C%20edge%0Adeployment%20is%20still%20constrained%20by%20limited%20on-chip%20resources%2C%20power%20budgets%2C%0Aand%20the%20often-neglected%20long%20latency%20of%20the%20prefill%20stage.%20We%20present%0A%5Ctextbf%7BTeLLMe%7D%2C%20the%20first%20table-lookup-based%20ternary%20LLM%20accelerator%20for%0Alow-power%20edge%20FPGAs%20that%20fully%20supports%20both%20prefill%20and%20autoregressive%0Adecoding%20using%201.58-bit%20weights%20and%208-bit%20activations.%20TeLLMe%20incorporates%0Aseveral%20novel%20techniques%2C%20including%20%281%29%20a%20table-lookup-based%20ternary%20matrix%0Amultiplication%20%28TLMM%29%20engine%20utilizing%20grouped%20activations%20and%20online%0Aprecomputation%20for%20low%20resource%20utilization%20and%20high%20throughput%3B%20%282%29%20a%0Afine-grained%20analytic%20URAM-based%20weight%20buffer%20management%20scheme%20for%20efficient%0Aloading%20and%20compute%20engine%20access%3B%20%283%29%20a%20streaming%20dataflow%20architecture%20that%0Afuses%20floating-point%20element-wise%20operations%20with%20linear%20computations%20to%20hide%0Alatency%3B%20%284%29%20a%20reversed-reordered%20prefill%20stage%20attention%20with%20fused%20attention%0Aoperations%20for%20high%20memory%20efficiency%3B%20and%20%285%29%20a%20resource-efficient%20specialized%0Adecoding%20stage%20attention.%20Under%20a%205~W%20power%20budget%2C%20TeLLMe%20delivers%20up%20to%0A25~tokens/s%20decoding%20throughput%20and%200.45--0.96~s%20time-to-first-token%20%28TTFT%29%20for%0A64--128%20token%20prompts%2C%20marking%20a%20significant%20energy-efficiency%20advancement%20in%0ALLM%20inference%20on%20edge%20FPGAs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15926v2&entry.124074799=Read"},
{"title": "A Unified Perspective on Optimization in Machine Learning and\n  Neuroscience: From Gradient Descent to Neural Adaptation", "author": "Jes\u00fas Garc\u00eda Fern\u00e1ndez and Nasir Ahmad and Marcel van Gerven", "abstract": "  Iterative optimization is central to modern artificial intelligence (AI) and\nprovides a crucial framework for understanding adaptive systems. This review\nprovides a unified perspective on this subject, bridging classic theory with\nneural network training and biological learning. Although gradient-based\nmethods, powered by the efficient but biologically implausible backpropagation\n(BP), dominate machine learning, their computational demands can hinder\nscalability in high-dimensional settings. In contrast, derivative-free or\nzeroth-order (ZO) optimization feature computationally lighter approaches that\nrely only on function evaluations and randomness. While generally less sample\nefficient, recent breakthroughs demonstrate that modern ZO methods can\neffectively approximate gradients and achieve performance competitive with BP\nin neural network models. This ZO paradigm is also particularly relevant for\nbiology. Its core principles of random exploration (probing) and\nfeedback-guided adaptation (reinforcing) parallel key mechanisms of biological\nlearning, offering a mathematically principled perspective on how the brain\nlearns. In this review, we begin by categorizing optimization approaches based\non the order of derivative information they utilize, ranging from first-,\nsecond-, and higher-order gradient-based to ZO methods. We then explore how\nthese methods are adapted to the unique challenges of neural network training\nand the resulting learning dynamics. Finally, we build upon these insights to\nview biological learning through an optimization lens, arguing that a ZO\nparadigm leverages the brain's intrinsic noise as a computational resource.\nThis framework not only illuminates our understanding of natural intelligence\nbut also holds vast implications for neuromorphic hardware, helping us design\nfast and energy-efficient AI systems that exploit intrinsic hardware noise.\n", "link": "http://arxiv.org/abs/2510.18812v1", "date": "2025-10-21", "relevancy": 1.9685, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Perspective%20on%20Optimization%20in%20Machine%20Learning%20and%0A%20%20Neuroscience%3A%20From%20Gradient%20Descent%20to%20Neural%20Adaptation&body=Title%3A%20A%20Unified%20Perspective%20on%20Optimization%20in%20Machine%20Learning%20and%0A%20%20Neuroscience%3A%20From%20Gradient%20Descent%20to%20Neural%20Adaptation%0AAuthor%3A%20Jes%C3%BAs%20Garc%C3%ADa%20Fern%C3%A1ndez%20and%20Nasir%20Ahmad%20and%20Marcel%20van%20Gerven%0AAbstract%3A%20%20%20Iterative%20optimization%20is%20central%20to%20modern%20artificial%20intelligence%20%28AI%29%20and%0Aprovides%20a%20crucial%20framework%20for%20understanding%20adaptive%20systems.%20This%20review%0Aprovides%20a%20unified%20perspective%20on%20this%20subject%2C%20bridging%20classic%20theory%20with%0Aneural%20network%20training%20and%20biological%20learning.%20Although%20gradient-based%0Amethods%2C%20powered%20by%20the%20efficient%20but%20biologically%20implausible%20backpropagation%0A%28BP%29%2C%20dominate%20machine%20learning%2C%20their%20computational%20demands%20can%20hinder%0Ascalability%20in%20high-dimensional%20settings.%20In%20contrast%2C%20derivative-free%20or%0Azeroth-order%20%28ZO%29%20optimization%20feature%20computationally%20lighter%20approaches%20that%0Arely%20only%20on%20function%20evaluations%20and%20randomness.%20While%20generally%20less%20sample%0Aefficient%2C%20recent%20breakthroughs%20demonstrate%20that%20modern%20ZO%20methods%20can%0Aeffectively%20approximate%20gradients%20and%20achieve%20performance%20competitive%20with%20BP%0Ain%20neural%20network%20models.%20This%20ZO%20paradigm%20is%20also%20particularly%20relevant%20for%0Abiology.%20Its%20core%20principles%20of%20random%20exploration%20%28probing%29%20and%0Afeedback-guided%20adaptation%20%28reinforcing%29%20parallel%20key%20mechanisms%20of%20biological%0Alearning%2C%20offering%20a%20mathematically%20principled%20perspective%20on%20how%20the%20brain%0Alearns.%20In%20this%20review%2C%20we%20begin%20by%20categorizing%20optimization%20approaches%20based%0Aon%20the%20order%20of%20derivative%20information%20they%20utilize%2C%20ranging%20from%20first-%2C%0Asecond-%2C%20and%20higher-order%20gradient-based%20to%20ZO%20methods.%20We%20then%20explore%20how%0Athese%20methods%20are%20adapted%20to%20the%20unique%20challenges%20of%20neural%20network%20training%0Aand%20the%20resulting%20learning%20dynamics.%20Finally%2C%20we%20build%20upon%20these%20insights%20to%0Aview%20biological%20learning%20through%20an%20optimization%20lens%2C%20arguing%20that%20a%20ZO%0Aparadigm%20leverages%20the%20brain%27s%20intrinsic%20noise%20as%20a%20computational%20resource.%0AThis%20framework%20not%20only%20illuminates%20our%20understanding%20of%20natural%20intelligence%0Abut%20also%20holds%20vast%20implications%20for%20neuromorphic%20hardware%2C%20helping%20us%20design%0Afast%20and%20energy-efficient%20AI%20systems%20that%20exploit%20intrinsic%20hardware%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Perspective%2520on%2520Optimization%2520in%2520Machine%2520Learning%2520and%250A%2520%2520Neuroscience%253A%2520From%2520Gradient%2520Descent%2520to%2520Neural%2520Adaptation%26entry.906535625%3DJes%25C3%25BAs%2520Garc%25C3%25ADa%2520Fern%25C3%25A1ndez%2520and%2520Nasir%2520Ahmad%2520and%2520Marcel%2520van%2520Gerven%26entry.1292438233%3D%2520%2520Iterative%2520optimization%2520is%2520central%2520to%2520modern%2520artificial%2520intelligence%2520%2528AI%2529%2520and%250Aprovides%2520a%2520crucial%2520framework%2520for%2520understanding%2520adaptive%2520systems.%2520This%2520review%250Aprovides%2520a%2520unified%2520perspective%2520on%2520this%2520subject%252C%2520bridging%2520classic%2520theory%2520with%250Aneural%2520network%2520training%2520and%2520biological%2520learning.%2520Although%2520gradient-based%250Amethods%252C%2520powered%2520by%2520the%2520efficient%2520but%2520biologically%2520implausible%2520backpropagation%250A%2528BP%2529%252C%2520dominate%2520machine%2520learning%252C%2520their%2520computational%2520demands%2520can%2520hinder%250Ascalability%2520in%2520high-dimensional%2520settings.%2520In%2520contrast%252C%2520derivative-free%2520or%250Azeroth-order%2520%2528ZO%2529%2520optimization%2520feature%2520computationally%2520lighter%2520approaches%2520that%250Arely%2520only%2520on%2520function%2520evaluations%2520and%2520randomness.%2520While%2520generally%2520less%2520sample%250Aefficient%252C%2520recent%2520breakthroughs%2520demonstrate%2520that%2520modern%2520ZO%2520methods%2520can%250Aeffectively%2520approximate%2520gradients%2520and%2520achieve%2520performance%2520competitive%2520with%2520BP%250Ain%2520neural%2520network%2520models.%2520This%2520ZO%2520paradigm%2520is%2520also%2520particularly%2520relevant%2520for%250Abiology.%2520Its%2520core%2520principles%2520of%2520random%2520exploration%2520%2528probing%2529%2520and%250Afeedback-guided%2520adaptation%2520%2528reinforcing%2529%2520parallel%2520key%2520mechanisms%2520of%2520biological%250Alearning%252C%2520offering%2520a%2520mathematically%2520principled%2520perspective%2520on%2520how%2520the%2520brain%250Alearns.%2520In%2520this%2520review%252C%2520we%2520begin%2520by%2520categorizing%2520optimization%2520approaches%2520based%250Aon%2520the%2520order%2520of%2520derivative%2520information%2520they%2520utilize%252C%2520ranging%2520from%2520first-%252C%250Asecond-%252C%2520and%2520higher-order%2520gradient-based%2520to%2520ZO%2520methods.%2520We%2520then%2520explore%2520how%250Athese%2520methods%2520are%2520adapted%2520to%2520the%2520unique%2520challenges%2520of%2520neural%2520network%2520training%250Aand%2520the%2520resulting%2520learning%2520dynamics.%2520Finally%252C%2520we%2520build%2520upon%2520these%2520insights%2520to%250Aview%2520biological%2520learning%2520through%2520an%2520optimization%2520lens%252C%2520arguing%2520that%2520a%2520ZO%250Aparadigm%2520leverages%2520the%2520brain%2527s%2520intrinsic%2520noise%2520as%2520a%2520computational%2520resource.%250AThis%2520framework%2520not%2520only%2520illuminates%2520our%2520understanding%2520of%2520natural%2520intelligence%250Abut%2520also%2520holds%2520vast%2520implications%2520for%2520neuromorphic%2520hardware%252C%2520helping%2520us%2520design%250Afast%2520and%2520energy-efficient%2520AI%2520systems%2520that%2520exploit%2520intrinsic%2520hardware%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Perspective%20on%20Optimization%20in%20Machine%20Learning%20and%0A%20%20Neuroscience%3A%20From%20Gradient%20Descent%20to%20Neural%20Adaptation&entry.906535625=Jes%C3%BAs%20Garc%C3%ADa%20Fern%C3%A1ndez%20and%20Nasir%20Ahmad%20and%20Marcel%20van%20Gerven&entry.1292438233=%20%20Iterative%20optimization%20is%20central%20to%20modern%20artificial%20intelligence%20%28AI%29%20and%0Aprovides%20a%20crucial%20framework%20for%20understanding%20adaptive%20systems.%20This%20review%0Aprovides%20a%20unified%20perspective%20on%20this%20subject%2C%20bridging%20classic%20theory%20with%0Aneural%20network%20training%20and%20biological%20learning.%20Although%20gradient-based%0Amethods%2C%20powered%20by%20the%20efficient%20but%20biologically%20implausible%20backpropagation%0A%28BP%29%2C%20dominate%20machine%20learning%2C%20their%20computational%20demands%20can%20hinder%0Ascalability%20in%20high-dimensional%20settings.%20In%20contrast%2C%20derivative-free%20or%0Azeroth-order%20%28ZO%29%20optimization%20feature%20computationally%20lighter%20approaches%20that%0Arely%20only%20on%20function%20evaluations%20and%20randomness.%20While%20generally%20less%20sample%0Aefficient%2C%20recent%20breakthroughs%20demonstrate%20that%20modern%20ZO%20methods%20can%0Aeffectively%20approximate%20gradients%20and%20achieve%20performance%20competitive%20with%20BP%0Ain%20neural%20network%20models.%20This%20ZO%20paradigm%20is%20also%20particularly%20relevant%20for%0Abiology.%20Its%20core%20principles%20of%20random%20exploration%20%28probing%29%20and%0Afeedback-guided%20adaptation%20%28reinforcing%29%20parallel%20key%20mechanisms%20of%20biological%0Alearning%2C%20offering%20a%20mathematically%20principled%20perspective%20on%20how%20the%20brain%0Alearns.%20In%20this%20review%2C%20we%20begin%20by%20categorizing%20optimization%20approaches%20based%0Aon%20the%20order%20of%20derivative%20information%20they%20utilize%2C%20ranging%20from%20first-%2C%0Asecond-%2C%20and%20higher-order%20gradient-based%20to%20ZO%20methods.%20We%20then%20explore%20how%0Athese%20methods%20are%20adapted%20to%20the%20unique%20challenges%20of%20neural%20network%20training%0Aand%20the%20resulting%20learning%20dynamics.%20Finally%2C%20we%20build%20upon%20these%20insights%20to%0Aview%20biological%20learning%20through%20an%20optimization%20lens%2C%20arguing%20that%20a%20ZO%0Aparadigm%20leverages%20the%20brain%27s%20intrinsic%20noise%20as%20a%20computational%20resource.%0AThis%20framework%20not%20only%20illuminates%20our%20understanding%20of%20natural%20intelligence%0Abut%20also%20holds%20vast%20implications%20for%20neuromorphic%20hardware%2C%20helping%20us%20design%0Afast%20and%20energy-efficient%20AI%20systems%20that%20exploit%20intrinsic%20hardware%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18812v1&entry.124074799=Read"},
{"title": "Improving the Generation and Evaluation of Synthetic Data for Downstream\n  Medical Causal Inference", "author": "Harry Amad and Zhaozhi Qian and Dennis Frauen and Julianna Piskorz and Stefan Feuerriegel and Mihaela van der Schaar", "abstract": "  Causal inference is essential for developing and evaluating medical\ninterventions, yet real-world medical datasets are often difficult to access\ndue to regulatory barriers. This makes synthetic data a potentially valuable\nasset that enables these medical analyses, along with the development of new\ninference methods themselves. Generative models can produce synthetic data that\nclosely approximate real data distributions, yet existing methods do not\nconsider the unique challenges that downstream causal inference tasks, and\nspecifically those focused on treatments, pose. We establish a set of\ndesiderata that synthetic data containing treatments should satisfy to maximise\ndownstream utility: preservation of (i) the covariate distribution, (ii) the\ntreatment assignment mechanism, and (iii) the outcome generation mechanism.\nBased on these desiderata, we propose a set of evaluation metrics to assess\nsuch synthetic data. Finally, we present STEAM: a novel method for generating\nSynthetic data for Treatment Effect Analysis in Medicine that mimics the\ndata-generating process of data containing treatments and optimises for our\ndesiderata. We empirically demonstrate that STEAM achieves state-of-the-art\nperformance across our metrics as compared to existing generative models,\nparticularly as the complexity of the true data-generating process increases.\n", "link": "http://arxiv.org/abs/2510.18768v1", "date": "2025-10-21", "relevancy": 1.9657, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5217}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4715}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Generation%20and%20Evaluation%20of%20Synthetic%20Data%20for%20Downstream%0A%20%20Medical%20Causal%20Inference&body=Title%3A%20Improving%20the%20Generation%20and%20Evaluation%20of%20Synthetic%20Data%20for%20Downstream%0A%20%20Medical%20Causal%20Inference%0AAuthor%3A%20Harry%20Amad%20and%20Zhaozhi%20Qian%20and%20Dennis%20Frauen%20and%20Julianna%20Piskorz%20and%20Stefan%20Feuerriegel%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20%20%20Causal%20inference%20is%20essential%20for%20developing%20and%20evaluating%20medical%0Ainterventions%2C%20yet%20real-world%20medical%20datasets%20are%20often%20difficult%20to%20access%0Adue%20to%20regulatory%20barriers.%20This%20makes%20synthetic%20data%20a%20potentially%20valuable%0Aasset%20that%20enables%20these%20medical%20analyses%2C%20along%20with%20the%20development%20of%20new%0Ainference%20methods%20themselves.%20Generative%20models%20can%20produce%20synthetic%20data%20that%0Aclosely%20approximate%20real%20data%20distributions%2C%20yet%20existing%20methods%20do%20not%0Aconsider%20the%20unique%20challenges%20that%20downstream%20causal%20inference%20tasks%2C%20and%0Aspecifically%20those%20focused%20on%20treatments%2C%20pose.%20We%20establish%20a%20set%20of%0Adesiderata%20that%20synthetic%20data%20containing%20treatments%20should%20satisfy%20to%20maximise%0Adownstream%20utility%3A%20preservation%20of%20%28i%29%20the%20covariate%20distribution%2C%20%28ii%29%20the%0Atreatment%20assignment%20mechanism%2C%20and%20%28iii%29%20the%20outcome%20generation%20mechanism.%0ABased%20on%20these%20desiderata%2C%20we%20propose%20a%20set%20of%20evaluation%20metrics%20to%20assess%0Asuch%20synthetic%20data.%20Finally%2C%20we%20present%20STEAM%3A%20a%20novel%20method%20for%20generating%0ASynthetic%20data%20for%20Treatment%20Effect%20Analysis%20in%20Medicine%20that%20mimics%20the%0Adata-generating%20process%20of%20data%20containing%20treatments%20and%20optimises%20for%20our%0Adesiderata.%20We%20empirically%20demonstrate%20that%20STEAM%20achieves%20state-of-the-art%0Aperformance%20across%20our%20metrics%20as%20compared%20to%20existing%20generative%20models%2C%0Aparticularly%20as%20the%20complexity%20of%20the%20true%20data-generating%20process%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Generation%2520and%2520Evaluation%2520of%2520Synthetic%2520Data%2520for%2520Downstream%250A%2520%2520Medical%2520Causal%2520Inference%26entry.906535625%3DHarry%2520Amad%2520and%2520Zhaozhi%2520Qian%2520and%2520Dennis%2520Frauen%2520and%2520Julianna%2520Piskorz%2520and%2520Stefan%2520Feuerriegel%2520and%2520Mihaela%2520van%2520der%2520Schaar%26entry.1292438233%3D%2520%2520Causal%2520inference%2520is%2520essential%2520for%2520developing%2520and%2520evaluating%2520medical%250Ainterventions%252C%2520yet%2520real-world%2520medical%2520datasets%2520are%2520often%2520difficult%2520to%2520access%250Adue%2520to%2520regulatory%2520barriers.%2520This%2520makes%2520synthetic%2520data%2520a%2520potentially%2520valuable%250Aasset%2520that%2520enables%2520these%2520medical%2520analyses%252C%2520along%2520with%2520the%2520development%2520of%2520new%250Ainference%2520methods%2520themselves.%2520Generative%2520models%2520can%2520produce%2520synthetic%2520data%2520that%250Aclosely%2520approximate%2520real%2520data%2520distributions%252C%2520yet%2520existing%2520methods%2520do%2520not%250Aconsider%2520the%2520unique%2520challenges%2520that%2520downstream%2520causal%2520inference%2520tasks%252C%2520and%250Aspecifically%2520those%2520focused%2520on%2520treatments%252C%2520pose.%2520We%2520establish%2520a%2520set%2520of%250Adesiderata%2520that%2520synthetic%2520data%2520containing%2520treatments%2520should%2520satisfy%2520to%2520maximise%250Adownstream%2520utility%253A%2520preservation%2520of%2520%2528i%2529%2520the%2520covariate%2520distribution%252C%2520%2528ii%2529%2520the%250Atreatment%2520assignment%2520mechanism%252C%2520and%2520%2528iii%2529%2520the%2520outcome%2520generation%2520mechanism.%250ABased%2520on%2520these%2520desiderata%252C%2520we%2520propose%2520a%2520set%2520of%2520evaluation%2520metrics%2520to%2520assess%250Asuch%2520synthetic%2520data.%2520Finally%252C%2520we%2520present%2520STEAM%253A%2520a%2520novel%2520method%2520for%2520generating%250ASynthetic%2520data%2520for%2520Treatment%2520Effect%2520Analysis%2520in%2520Medicine%2520that%2520mimics%2520the%250Adata-generating%2520process%2520of%2520data%2520containing%2520treatments%2520and%2520optimises%2520for%2520our%250Adesiderata.%2520We%2520empirically%2520demonstrate%2520that%2520STEAM%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520our%2520metrics%2520as%2520compared%2520to%2520existing%2520generative%2520models%252C%250Aparticularly%2520as%2520the%2520complexity%2520of%2520the%2520true%2520data-generating%2520process%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Generation%20and%20Evaluation%20of%20Synthetic%20Data%20for%20Downstream%0A%20%20Medical%20Causal%20Inference&entry.906535625=Harry%20Amad%20and%20Zhaozhi%20Qian%20and%20Dennis%20Frauen%20and%20Julianna%20Piskorz%20and%20Stefan%20Feuerriegel%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=%20%20Causal%20inference%20is%20essential%20for%20developing%20and%20evaluating%20medical%0Ainterventions%2C%20yet%20real-world%20medical%20datasets%20are%20often%20difficult%20to%20access%0Adue%20to%20regulatory%20barriers.%20This%20makes%20synthetic%20data%20a%20potentially%20valuable%0Aasset%20that%20enables%20these%20medical%20analyses%2C%20along%20with%20the%20development%20of%20new%0Ainference%20methods%20themselves.%20Generative%20models%20can%20produce%20synthetic%20data%20that%0Aclosely%20approximate%20real%20data%20distributions%2C%20yet%20existing%20methods%20do%20not%0Aconsider%20the%20unique%20challenges%20that%20downstream%20causal%20inference%20tasks%2C%20and%0Aspecifically%20those%20focused%20on%20treatments%2C%20pose.%20We%20establish%20a%20set%20of%0Adesiderata%20that%20synthetic%20data%20containing%20treatments%20should%20satisfy%20to%20maximise%0Adownstream%20utility%3A%20preservation%20of%20%28i%29%20the%20covariate%20distribution%2C%20%28ii%29%20the%0Atreatment%20assignment%20mechanism%2C%20and%20%28iii%29%20the%20outcome%20generation%20mechanism.%0ABased%20on%20these%20desiderata%2C%20we%20propose%20a%20set%20of%20evaluation%20metrics%20to%20assess%0Asuch%20synthetic%20data.%20Finally%2C%20we%20present%20STEAM%3A%20a%20novel%20method%20for%20generating%0ASynthetic%20data%20for%20Treatment%20Effect%20Analysis%20in%20Medicine%20that%20mimics%20the%0Adata-generating%20process%20of%20data%20containing%20treatments%20and%20optimises%20for%20our%0Adesiderata.%20We%20empirically%20demonstrate%20that%20STEAM%20achieves%20state-of-the-art%0Aperformance%20across%20our%20metrics%20as%20compared%20to%20existing%20generative%20models%2C%0Aparticularly%20as%20the%20complexity%20of%20the%20true%20data-generating%20process%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18768v1&entry.124074799=Read"},
{"title": "Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting", "author": "Howard Chen and Noam Razin and Karthik Narasimhan and Danqi Chen", "abstract": "  Adapting language models (LMs) to new tasks via post-training carries the\nrisk of degrading existing capabilities -- a phenomenon classically known as\ncatastrophic forgetting. In this paper, toward identifying guidelines for\nmitigating this phenomenon, we systematically compare the forgetting patterns\nof two widely adopted post-training methods: supervised fine-tuning (SFT) and\nreinforcement learning (RL). Our experiments reveal a consistent trend across\nLM families (Llama, Qwen) and tasks (instruction following, general knowledge,\nand arithmetic reasoning): RL leads to less forgetting than SFT while achieving\ncomparable or higher target task performance. To investigate the cause for this\ndifference, we consider a simplified setting in which the LM is modeled as a\nmixture of two distributions, one corresponding to prior knowledge and the\nother to the target task. We identify that the mode-seeking nature of RL, which\nstems from its use of on-policy data, enables keeping prior knowledge intact\nwhen learning the target task. We then verify this insight by demonstrating\nthat the use on-policy data underlies the robustness of RL to forgetting in\npractical settings, as opposed to other algorithmic choices such as the KL\nregularization or advantage estimation. Lastly, as a practical implication, our\nresults highlight the potential of mitigating forgetting using approximately\non-policy data, which can be substantially more efficient to obtain than fully\non-policy data.\n", "link": "http://arxiv.org/abs/2510.18874v1", "date": "2025-10-21", "relevancy": 1.937, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4909}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4811}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retaining%20by%20Doing%3A%20The%20Role%20of%20On-Policy%20Data%20in%20Mitigating%20Forgetting&body=Title%3A%20Retaining%20by%20Doing%3A%20The%20Role%20of%20On-Policy%20Data%20in%20Mitigating%20Forgetting%0AAuthor%3A%20Howard%20Chen%20and%20Noam%20Razin%20and%20Karthik%20Narasimhan%20and%20Danqi%20Chen%0AAbstract%3A%20%20%20Adapting%20language%20models%20%28LMs%29%20to%20new%20tasks%20via%20post-training%20carries%20the%0Arisk%20of%20degrading%20existing%20capabilities%20--%20a%20phenomenon%20classically%20known%20as%0Acatastrophic%20forgetting.%20In%20this%20paper%2C%20toward%20identifying%20guidelines%20for%0Amitigating%20this%20phenomenon%2C%20we%20systematically%20compare%20the%20forgetting%20patterns%0Aof%20two%20widely%20adopted%20post-training%20methods%3A%20supervised%20fine-tuning%20%28SFT%29%20and%0Areinforcement%20learning%20%28RL%29.%20Our%20experiments%20reveal%20a%20consistent%20trend%20across%0ALM%20families%20%28Llama%2C%20Qwen%29%20and%20tasks%20%28instruction%20following%2C%20general%20knowledge%2C%0Aand%20arithmetic%20reasoning%29%3A%20RL%20leads%20to%20less%20forgetting%20than%20SFT%20while%20achieving%0Acomparable%20or%20higher%20target%20task%20performance.%20To%20investigate%20the%20cause%20for%20this%0Adifference%2C%20we%20consider%20a%20simplified%20setting%20in%20which%20the%20LM%20is%20modeled%20as%20a%0Amixture%20of%20two%20distributions%2C%20one%20corresponding%20to%20prior%20knowledge%20and%20the%0Aother%20to%20the%20target%20task.%20We%20identify%20that%20the%20mode-seeking%20nature%20of%20RL%2C%20which%0Astems%20from%20its%20use%20of%20on-policy%20data%2C%20enables%20keeping%20prior%20knowledge%20intact%0Awhen%20learning%20the%20target%20task.%20We%20then%20verify%20this%20insight%20by%20demonstrating%0Athat%20the%20use%20on-policy%20data%20underlies%20the%20robustness%20of%20RL%20to%20forgetting%20in%0Apractical%20settings%2C%20as%20opposed%20to%20other%20algorithmic%20choices%20such%20as%20the%20KL%0Aregularization%20or%20advantage%20estimation.%20Lastly%2C%20as%20a%20practical%20implication%2C%20our%0Aresults%20highlight%20the%20potential%20of%20mitigating%20forgetting%20using%20approximately%0Aon-policy%20data%2C%20which%20can%20be%20substantially%20more%20efficient%20to%20obtain%20than%20fully%0Aon-policy%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetaining%2520by%2520Doing%253A%2520The%2520Role%2520of%2520On-Policy%2520Data%2520in%2520Mitigating%2520Forgetting%26entry.906535625%3DHoward%2520Chen%2520and%2520Noam%2520Razin%2520and%2520Karthik%2520Narasimhan%2520and%2520Danqi%2520Chen%26entry.1292438233%3D%2520%2520Adapting%2520language%2520models%2520%2528LMs%2529%2520to%2520new%2520tasks%2520via%2520post-training%2520carries%2520the%250Arisk%2520of%2520degrading%2520existing%2520capabilities%2520--%2520a%2520phenomenon%2520classically%2520known%2520as%250Acatastrophic%2520forgetting.%2520In%2520this%2520paper%252C%2520toward%2520identifying%2520guidelines%2520for%250Amitigating%2520this%2520phenomenon%252C%2520we%2520systematically%2520compare%2520the%2520forgetting%2520patterns%250Aof%2520two%2520widely%2520adopted%2520post-training%2520methods%253A%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%250Areinforcement%2520learning%2520%2528RL%2529.%2520Our%2520experiments%2520reveal%2520a%2520consistent%2520trend%2520across%250ALM%2520families%2520%2528Llama%252C%2520Qwen%2529%2520and%2520tasks%2520%2528instruction%2520following%252C%2520general%2520knowledge%252C%250Aand%2520arithmetic%2520reasoning%2529%253A%2520RL%2520leads%2520to%2520less%2520forgetting%2520than%2520SFT%2520while%2520achieving%250Acomparable%2520or%2520higher%2520target%2520task%2520performance.%2520To%2520investigate%2520the%2520cause%2520for%2520this%250Adifference%252C%2520we%2520consider%2520a%2520simplified%2520setting%2520in%2520which%2520the%2520LM%2520is%2520modeled%2520as%2520a%250Amixture%2520of%2520two%2520distributions%252C%2520one%2520corresponding%2520to%2520prior%2520knowledge%2520and%2520the%250Aother%2520to%2520the%2520target%2520task.%2520We%2520identify%2520that%2520the%2520mode-seeking%2520nature%2520of%2520RL%252C%2520which%250Astems%2520from%2520its%2520use%2520of%2520on-policy%2520data%252C%2520enables%2520keeping%2520prior%2520knowledge%2520intact%250Awhen%2520learning%2520the%2520target%2520task.%2520We%2520then%2520verify%2520this%2520insight%2520by%2520demonstrating%250Athat%2520the%2520use%2520on-policy%2520data%2520underlies%2520the%2520robustness%2520of%2520RL%2520to%2520forgetting%2520in%250Apractical%2520settings%252C%2520as%2520opposed%2520to%2520other%2520algorithmic%2520choices%2520such%2520as%2520the%2520KL%250Aregularization%2520or%2520advantage%2520estimation.%2520Lastly%252C%2520as%2520a%2520practical%2520implication%252C%2520our%250Aresults%2520highlight%2520the%2520potential%2520of%2520mitigating%2520forgetting%2520using%2520approximately%250Aon-policy%2520data%252C%2520which%2520can%2520be%2520substantially%2520more%2520efficient%2520to%2520obtain%2520than%2520fully%250Aon-policy%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retaining%20by%20Doing%3A%20The%20Role%20of%20On-Policy%20Data%20in%20Mitigating%20Forgetting&entry.906535625=Howard%20Chen%20and%20Noam%20Razin%20and%20Karthik%20Narasimhan%20and%20Danqi%20Chen&entry.1292438233=%20%20Adapting%20language%20models%20%28LMs%29%20to%20new%20tasks%20via%20post-training%20carries%20the%0Arisk%20of%20degrading%20existing%20capabilities%20--%20a%20phenomenon%20classically%20known%20as%0Acatastrophic%20forgetting.%20In%20this%20paper%2C%20toward%20identifying%20guidelines%20for%0Amitigating%20this%20phenomenon%2C%20we%20systematically%20compare%20the%20forgetting%20patterns%0Aof%20two%20widely%20adopted%20post-training%20methods%3A%20supervised%20fine-tuning%20%28SFT%29%20and%0Areinforcement%20learning%20%28RL%29.%20Our%20experiments%20reveal%20a%20consistent%20trend%20across%0ALM%20families%20%28Llama%2C%20Qwen%29%20and%20tasks%20%28instruction%20following%2C%20general%20knowledge%2C%0Aand%20arithmetic%20reasoning%29%3A%20RL%20leads%20to%20less%20forgetting%20than%20SFT%20while%20achieving%0Acomparable%20or%20higher%20target%20task%20performance.%20To%20investigate%20the%20cause%20for%20this%0Adifference%2C%20we%20consider%20a%20simplified%20setting%20in%20which%20the%20LM%20is%20modeled%20as%20a%0Amixture%20of%20two%20distributions%2C%20one%20corresponding%20to%20prior%20knowledge%20and%20the%0Aother%20to%20the%20target%20task.%20We%20identify%20that%20the%20mode-seeking%20nature%20of%20RL%2C%20which%0Astems%20from%20its%20use%20of%20on-policy%20data%2C%20enables%20keeping%20prior%20knowledge%20intact%0Awhen%20learning%20the%20target%20task.%20We%20then%20verify%20this%20insight%20by%20demonstrating%0Athat%20the%20use%20on-policy%20data%20underlies%20the%20robustness%20of%20RL%20to%20forgetting%20in%0Apractical%20settings%2C%20as%20opposed%20to%20other%20algorithmic%20choices%20such%20as%20the%20KL%0Aregularization%20or%20advantage%20estimation.%20Lastly%2C%20as%20a%20practical%20implication%2C%20our%0Aresults%20highlight%20the%20potential%20of%20mitigating%20forgetting%20using%20approximately%0Aon-policy%20data%2C%20which%20can%20be%20substantially%20more%20efficient%20to%20obtain%20than%20fully%0Aon-policy%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18874v1&entry.124074799=Read"},
{"title": "Lyapunov-Aware Quantum-Inspired Reinforcement Learning for\n  Continuous-Time Vehicle Control: A Feasibility Study", "author": "Nutkritta Kraipatthanapong and Natthaphat Thathong and Pannita Suksawas and Thanunnut Klunklin and Kritin Vongthonglua and Krit Attahakul and Aueaphum Aueawatthanaphisut", "abstract": "  This paper presents a novel Lyapunov-Based Quantum Reinforcement Learning\n(LQRL) framework that integrates quantum policy optimization with Lyapunov\nstability analysis for continuous-time vehicle control. The proposed approach\ncombines the representational power of variational quantum circuits (VQCs) with\na stability-aware policy gradient mechanism to ensure asymptotic convergence\nand safe decision-making under dynamic environments. The vehicle longitudinal\ncontrol problem was formulated as a continuous-state reinforcement learning\ntask, where the quantum policy network generates control actions subject to\nLyapunov stability constraints. Simulation experiments were conducted in a\nclosed-loop adaptive cruise control scenario using a quantum-inspired policy\ntrained under stability feedback. The results demonstrate that the LQRL\nframework successfully embeds Lyapunov stability verification into quantum\npolicy learning, enabling interpretable and stability-aware control\nperformance. Although transient overshoot and Lyapunov divergence were observed\nunder aggressive acceleration, the system maintained bounded state evolution,\nvalidating the feasibility of integrating safety guarantees within quantum\nreinforcement learning architectures. The proposed framework provides a\nfoundational step toward provably safe quantum control in autonomous systems\nand hybrid quantum-classical optimization domains.\n", "link": "http://arxiv.org/abs/2510.18852v1", "date": "2025-10-21", "relevancy": 1.9358, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4926}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4825}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lyapunov-Aware%20Quantum-Inspired%20Reinforcement%20Learning%20for%0A%20%20Continuous-Time%20Vehicle%20Control%3A%20A%20Feasibility%20Study&body=Title%3A%20Lyapunov-Aware%20Quantum-Inspired%20Reinforcement%20Learning%20for%0A%20%20Continuous-Time%20Vehicle%20Control%3A%20A%20Feasibility%20Study%0AAuthor%3A%20Nutkritta%20Kraipatthanapong%20and%20Natthaphat%20Thathong%20and%20Pannita%20Suksawas%20and%20Thanunnut%20Klunklin%20and%20Kritin%20Vongthonglua%20and%20Krit%20Attahakul%20and%20Aueaphum%20Aueawatthanaphisut%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20Lyapunov-Based%20Quantum%20Reinforcement%20Learning%0A%28LQRL%29%20framework%20that%20integrates%20quantum%20policy%20optimization%20with%20Lyapunov%0Astability%20analysis%20for%20continuous-time%20vehicle%20control.%20The%20proposed%20approach%0Acombines%20the%20representational%20power%20of%20variational%20quantum%20circuits%20%28VQCs%29%20with%0Aa%20stability-aware%20policy%20gradient%20mechanism%20to%20ensure%20asymptotic%20convergence%0Aand%20safe%20decision-making%20under%20dynamic%20environments.%20The%20vehicle%20longitudinal%0Acontrol%20problem%20was%20formulated%20as%20a%20continuous-state%20reinforcement%20learning%0Atask%2C%20where%20the%20quantum%20policy%20network%20generates%20control%20actions%20subject%20to%0ALyapunov%20stability%20constraints.%20Simulation%20experiments%20were%20conducted%20in%20a%0Aclosed-loop%20adaptive%20cruise%20control%20scenario%20using%20a%20quantum-inspired%20policy%0Atrained%20under%20stability%20feedback.%20The%20results%20demonstrate%20that%20the%20LQRL%0Aframework%20successfully%20embeds%20Lyapunov%20stability%20verification%20into%20quantum%0Apolicy%20learning%2C%20enabling%20interpretable%20and%20stability-aware%20control%0Aperformance.%20Although%20transient%20overshoot%20and%20Lyapunov%20divergence%20were%20observed%0Aunder%20aggressive%20acceleration%2C%20the%20system%20maintained%20bounded%20state%20evolution%2C%0Avalidating%20the%20feasibility%20of%20integrating%20safety%20guarantees%20within%20quantum%0Areinforcement%20learning%20architectures.%20The%20proposed%20framework%20provides%20a%0Afoundational%20step%20toward%20provably%20safe%20quantum%20control%20in%20autonomous%20systems%0Aand%20hybrid%20quantum-classical%20optimization%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLyapunov-Aware%2520Quantum-Inspired%2520Reinforcement%2520Learning%2520for%250A%2520%2520Continuous-Time%2520Vehicle%2520Control%253A%2520A%2520Feasibility%2520Study%26entry.906535625%3DNutkritta%2520Kraipatthanapong%2520and%2520Natthaphat%2520Thathong%2520and%2520Pannita%2520Suksawas%2520and%2520Thanunnut%2520Klunklin%2520and%2520Kritin%2520Vongthonglua%2520and%2520Krit%2520Attahakul%2520and%2520Aueaphum%2520Aueawatthanaphisut%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520Lyapunov-Based%2520Quantum%2520Reinforcement%2520Learning%250A%2528LQRL%2529%2520framework%2520that%2520integrates%2520quantum%2520policy%2520optimization%2520with%2520Lyapunov%250Astability%2520analysis%2520for%2520continuous-time%2520vehicle%2520control.%2520The%2520proposed%2520approach%250Acombines%2520the%2520representational%2520power%2520of%2520variational%2520quantum%2520circuits%2520%2528VQCs%2529%2520with%250Aa%2520stability-aware%2520policy%2520gradient%2520mechanism%2520to%2520ensure%2520asymptotic%2520convergence%250Aand%2520safe%2520decision-making%2520under%2520dynamic%2520environments.%2520The%2520vehicle%2520longitudinal%250Acontrol%2520problem%2520was%2520formulated%2520as%2520a%2520continuous-state%2520reinforcement%2520learning%250Atask%252C%2520where%2520the%2520quantum%2520policy%2520network%2520generates%2520control%2520actions%2520subject%2520to%250ALyapunov%2520stability%2520constraints.%2520Simulation%2520experiments%2520were%2520conducted%2520in%2520a%250Aclosed-loop%2520adaptive%2520cruise%2520control%2520scenario%2520using%2520a%2520quantum-inspired%2520policy%250Atrained%2520under%2520stability%2520feedback.%2520The%2520results%2520demonstrate%2520that%2520the%2520LQRL%250Aframework%2520successfully%2520embeds%2520Lyapunov%2520stability%2520verification%2520into%2520quantum%250Apolicy%2520learning%252C%2520enabling%2520interpretable%2520and%2520stability-aware%2520control%250Aperformance.%2520Although%2520transient%2520overshoot%2520and%2520Lyapunov%2520divergence%2520were%2520observed%250Aunder%2520aggressive%2520acceleration%252C%2520the%2520system%2520maintained%2520bounded%2520state%2520evolution%252C%250Avalidating%2520the%2520feasibility%2520of%2520integrating%2520safety%2520guarantees%2520within%2520quantum%250Areinforcement%2520learning%2520architectures.%2520The%2520proposed%2520framework%2520provides%2520a%250Afoundational%2520step%2520toward%2520provably%2520safe%2520quantum%2520control%2520in%2520autonomous%2520systems%250Aand%2520hybrid%2520quantum-classical%2520optimization%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lyapunov-Aware%20Quantum-Inspired%20Reinforcement%20Learning%20for%0A%20%20Continuous-Time%20Vehicle%20Control%3A%20A%20Feasibility%20Study&entry.906535625=Nutkritta%20Kraipatthanapong%20and%20Natthaphat%20Thathong%20and%20Pannita%20Suksawas%20and%20Thanunnut%20Klunklin%20and%20Kritin%20Vongthonglua%20and%20Krit%20Attahakul%20and%20Aueaphum%20Aueawatthanaphisut&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20Lyapunov-Based%20Quantum%20Reinforcement%20Learning%0A%28LQRL%29%20framework%20that%20integrates%20quantum%20policy%20optimization%20with%20Lyapunov%0Astability%20analysis%20for%20continuous-time%20vehicle%20control.%20The%20proposed%20approach%0Acombines%20the%20representational%20power%20of%20variational%20quantum%20circuits%20%28VQCs%29%20with%0Aa%20stability-aware%20policy%20gradient%20mechanism%20to%20ensure%20asymptotic%20convergence%0Aand%20safe%20decision-making%20under%20dynamic%20environments.%20The%20vehicle%20longitudinal%0Acontrol%20problem%20was%20formulated%20as%20a%20continuous-state%20reinforcement%20learning%0Atask%2C%20where%20the%20quantum%20policy%20network%20generates%20control%20actions%20subject%20to%0ALyapunov%20stability%20constraints.%20Simulation%20experiments%20were%20conducted%20in%20a%0Aclosed-loop%20adaptive%20cruise%20control%20scenario%20using%20a%20quantum-inspired%20policy%0Atrained%20under%20stability%20feedback.%20The%20results%20demonstrate%20that%20the%20LQRL%0Aframework%20successfully%20embeds%20Lyapunov%20stability%20verification%20into%20quantum%0Apolicy%20learning%2C%20enabling%20interpretable%20and%20stability-aware%20control%0Aperformance.%20Although%20transient%20overshoot%20and%20Lyapunov%20divergence%20were%20observed%0Aunder%20aggressive%20acceleration%2C%20the%20system%20maintained%20bounded%20state%20evolution%2C%0Avalidating%20the%20feasibility%20of%20integrating%20safety%20guarantees%20within%20quantum%0Areinforcement%20learning%20architectures.%20The%20proposed%20framework%20provides%20a%0Afoundational%20step%20toward%20provably%20safe%20quantum%20control%20in%20autonomous%20systems%0Aand%20hybrid%20quantum-classical%20optimization%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18852v1&entry.124074799=Read"},
{"title": "Nondeterminism-Aware Optimistic Verification for Floating-Point Neural\n  Networks", "author": "Jianzhu Yao and Hongxu Su and Taobo Liao and Zerui Cheng and Huan Zhang and Xuechao Wang and Pramod Viswanath", "abstract": "  Neural networks increasingly run on hardware outside the user's control\n(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about\nwhat actually ran or whether returned outputs faithfully reflect the intended\ninputs. Users lack recourse against service downgrades (model swaps,\nquantization, graph rewrites, or discrepancies like altered ad embeddings).\nVerifying outputs is hard because floating-point(FP) execution on heterogeneous\naccelerators is inherently nondeterministic. Existing approaches are either\nimpractical for real FP neural networks or reintroduce vendor trust. We present\nNAO: a Nondeterministic tolerance Aware Optimistic verification protocol that\naccepts outputs within principled operator-level acceptance regions rather than\nrequiring bitwise equality. NAO combines two error models: (i) sound\nper-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile\nprofiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,\nthreshold-guided dispute game that recursively partitions the computation graph\nuntil one operator remains, where adjudication reduces to a lightweight\ntheoretical-bound check or a small honest-majority vote against empirical\nthresholds. Unchallenged results finalize after a challenge window, without\nrequiring trusted hardware or deterministic kernels. We implement NAO as a\nPyTorch-compatible runtime and a contract layer currently deployed on Ethereum\nHolesky testnet. The runtime instruments graphs, computes per-operator bounds,\nand runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on\nQwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,\nRTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than\ntheoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO\nreconciles scalability with verifiability for real-world heterogeneous ML\ncompute.\n", "link": "http://arxiv.org/abs/2510.16028v2", "date": "2025-10-21", "relevancy": 1.9326, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4865}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nondeterminism-Aware%20Optimistic%20Verification%20for%20Floating-Point%20Neural%0A%20%20Networks&body=Title%3A%20Nondeterminism-Aware%20Optimistic%20Verification%20for%20Floating-Point%20Neural%0A%20%20Networks%0AAuthor%3A%20Jianzhu%20Yao%20and%20Hongxu%20Su%20and%20Taobo%20Liao%20and%20Zerui%20Cheng%20and%20Huan%20Zhang%20and%20Xuechao%20Wang%20and%20Pramod%20Viswanath%0AAbstract%3A%20%20%20Neural%20networks%20increasingly%20run%20on%20hardware%20outside%20the%20user%27s%20control%0A%28cloud%20GPUs%2C%20inference%20marketplaces%29.%20Yet%20ML-as-a-Service%20reveals%20little%20about%0Awhat%20actually%20ran%20or%20whether%20returned%20outputs%20faithfully%20reflect%20the%20intended%0Ainputs.%20Users%20lack%20recourse%20against%20service%20downgrades%20%28model%20swaps%2C%0Aquantization%2C%20graph%20rewrites%2C%20or%20discrepancies%20like%20altered%20ad%20embeddings%29.%0AVerifying%20outputs%20is%20hard%20because%20floating-point%28FP%29%20execution%20on%20heterogeneous%0Aaccelerators%20is%20inherently%20nondeterministic.%20Existing%20approaches%20are%20either%0Aimpractical%20for%20real%20FP%20neural%20networks%20or%20reintroduce%20vendor%20trust.%20We%20present%0ANAO%3A%20a%20Nondeterministic%20tolerance%20Aware%20Optimistic%20verification%20protocol%20that%0Aaccepts%20outputs%20within%20principled%20operator-level%20acceptance%20regions%20rather%20than%0Arequiring%20bitwise%20equality.%20NAO%20combines%20two%20error%20models%3A%20%28i%29%20sound%0Aper-operator%20IEEE-754%20worst-case%20bounds%20and%20%28ii%29%20tight%20empirical%20percentile%0Aprofiles%20calibrated%20across%20hardware.%20Discrepancies%20trigger%20a%20Merkle-anchored%2C%0Athreshold-guided%20dispute%20game%20that%20recursively%20partitions%20the%20computation%20graph%0Auntil%20one%20operator%20remains%2C%20where%20adjudication%20reduces%20to%20a%20lightweight%0Atheoretical-bound%20check%20or%20a%20small%20honest-majority%20vote%20against%20empirical%0Athresholds.%20Unchallenged%20results%20finalize%20after%20a%20challenge%20window%2C%20without%0Arequiring%20trusted%20hardware%20or%20deterministic%20kernels.%20We%20implement%20NAO%20as%20a%0APyTorch-compatible%20runtime%20and%20a%20contract%20layer%20currently%20deployed%20on%20Ethereum%0AHolesky%20testnet.%20The%20runtime%20instruments%20graphs%2C%20computes%20per-operator%20bounds%2C%0Aand%20runs%20unmodified%20vendor%20kernels%20in%20FP32%20with%20negligible%20overhead%20%280.3%25%20on%0AQwen3-8B%29.%20Across%20CNNs%2C%20Transformers%20and%20diffusion%20models%20on%20A100%2C%20H100%2C%0ARTX6000%2C%20RTX4090%2C%20empirical%20thresholds%20are%20%2410%5E2-10%5E3%24%20times%20tighter%20than%0Atheoretical%20bounds%2C%20and%20bound-aware%20adversarial%20attacks%20achieve%200%25%20success.%20NAO%0Areconciles%20scalability%20with%20verifiability%20for%20real-world%20heterogeneous%20ML%0Acompute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.16028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNondeterminism-Aware%2520Optimistic%2520Verification%2520for%2520Floating-Point%2520Neural%250A%2520%2520Networks%26entry.906535625%3DJianzhu%2520Yao%2520and%2520Hongxu%2520Su%2520and%2520Taobo%2520Liao%2520and%2520Zerui%2520Cheng%2520and%2520Huan%2520Zhang%2520and%2520Xuechao%2520Wang%2520and%2520Pramod%2520Viswanath%26entry.1292438233%3D%2520%2520Neural%2520networks%2520increasingly%2520run%2520on%2520hardware%2520outside%2520the%2520user%2527s%2520control%250A%2528cloud%2520GPUs%252C%2520inference%2520marketplaces%2529.%2520Yet%2520ML-as-a-Service%2520reveals%2520little%2520about%250Awhat%2520actually%2520ran%2520or%2520whether%2520returned%2520outputs%2520faithfully%2520reflect%2520the%2520intended%250Ainputs.%2520Users%2520lack%2520recourse%2520against%2520service%2520downgrades%2520%2528model%2520swaps%252C%250Aquantization%252C%2520graph%2520rewrites%252C%2520or%2520discrepancies%2520like%2520altered%2520ad%2520embeddings%2529.%250AVerifying%2520outputs%2520is%2520hard%2520because%2520floating-point%2528FP%2529%2520execution%2520on%2520heterogeneous%250Aaccelerators%2520is%2520inherently%2520nondeterministic.%2520Existing%2520approaches%2520are%2520either%250Aimpractical%2520for%2520real%2520FP%2520neural%2520networks%2520or%2520reintroduce%2520vendor%2520trust.%2520We%2520present%250ANAO%253A%2520a%2520Nondeterministic%2520tolerance%2520Aware%2520Optimistic%2520verification%2520protocol%2520that%250Aaccepts%2520outputs%2520within%2520principled%2520operator-level%2520acceptance%2520regions%2520rather%2520than%250Arequiring%2520bitwise%2520equality.%2520NAO%2520combines%2520two%2520error%2520models%253A%2520%2528i%2529%2520sound%250Aper-operator%2520IEEE-754%2520worst-case%2520bounds%2520and%2520%2528ii%2529%2520tight%2520empirical%2520percentile%250Aprofiles%2520calibrated%2520across%2520hardware.%2520Discrepancies%2520trigger%2520a%2520Merkle-anchored%252C%250Athreshold-guided%2520dispute%2520game%2520that%2520recursively%2520partitions%2520the%2520computation%2520graph%250Auntil%2520one%2520operator%2520remains%252C%2520where%2520adjudication%2520reduces%2520to%2520a%2520lightweight%250Atheoretical-bound%2520check%2520or%2520a%2520small%2520honest-majority%2520vote%2520against%2520empirical%250Athresholds.%2520Unchallenged%2520results%2520finalize%2520after%2520a%2520challenge%2520window%252C%2520without%250Arequiring%2520trusted%2520hardware%2520or%2520deterministic%2520kernels.%2520We%2520implement%2520NAO%2520as%2520a%250APyTorch-compatible%2520runtime%2520and%2520a%2520contract%2520layer%2520currently%2520deployed%2520on%2520Ethereum%250AHolesky%2520testnet.%2520The%2520runtime%2520instruments%2520graphs%252C%2520computes%2520per-operator%2520bounds%252C%250Aand%2520runs%2520unmodified%2520vendor%2520kernels%2520in%2520FP32%2520with%2520negligible%2520overhead%2520%25280.3%2525%2520on%250AQwen3-8B%2529.%2520Across%2520CNNs%252C%2520Transformers%2520and%2520diffusion%2520models%2520on%2520A100%252C%2520H100%252C%250ARTX6000%252C%2520RTX4090%252C%2520empirical%2520thresholds%2520are%2520%252410%255E2-10%255E3%2524%2520times%2520tighter%2520than%250Atheoretical%2520bounds%252C%2520and%2520bound-aware%2520adversarial%2520attacks%2520achieve%25200%2525%2520success.%2520NAO%250Areconciles%2520scalability%2520with%2520verifiability%2520for%2520real-world%2520heterogeneous%2520ML%250Acompute.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nondeterminism-Aware%20Optimistic%20Verification%20for%20Floating-Point%20Neural%0A%20%20Networks&entry.906535625=Jianzhu%20Yao%20and%20Hongxu%20Su%20and%20Taobo%20Liao%20and%20Zerui%20Cheng%20and%20Huan%20Zhang%20and%20Xuechao%20Wang%20and%20Pramod%20Viswanath&entry.1292438233=%20%20Neural%20networks%20increasingly%20run%20on%20hardware%20outside%20the%20user%27s%20control%0A%28cloud%20GPUs%2C%20inference%20marketplaces%29.%20Yet%20ML-as-a-Service%20reveals%20little%20about%0Awhat%20actually%20ran%20or%20whether%20returned%20outputs%20faithfully%20reflect%20the%20intended%0Ainputs.%20Users%20lack%20recourse%20against%20service%20downgrades%20%28model%20swaps%2C%0Aquantization%2C%20graph%20rewrites%2C%20or%20discrepancies%20like%20altered%20ad%20embeddings%29.%0AVerifying%20outputs%20is%20hard%20because%20floating-point%28FP%29%20execution%20on%20heterogeneous%0Aaccelerators%20is%20inherently%20nondeterministic.%20Existing%20approaches%20are%20either%0Aimpractical%20for%20real%20FP%20neural%20networks%20or%20reintroduce%20vendor%20trust.%20We%20present%0ANAO%3A%20a%20Nondeterministic%20tolerance%20Aware%20Optimistic%20verification%20protocol%20that%0Aaccepts%20outputs%20within%20principled%20operator-level%20acceptance%20regions%20rather%20than%0Arequiring%20bitwise%20equality.%20NAO%20combines%20two%20error%20models%3A%20%28i%29%20sound%0Aper-operator%20IEEE-754%20worst-case%20bounds%20and%20%28ii%29%20tight%20empirical%20percentile%0Aprofiles%20calibrated%20across%20hardware.%20Discrepancies%20trigger%20a%20Merkle-anchored%2C%0Athreshold-guided%20dispute%20game%20that%20recursively%20partitions%20the%20computation%20graph%0Auntil%20one%20operator%20remains%2C%20where%20adjudication%20reduces%20to%20a%20lightweight%0Atheoretical-bound%20check%20or%20a%20small%20honest-majority%20vote%20against%20empirical%0Athresholds.%20Unchallenged%20results%20finalize%20after%20a%20challenge%20window%2C%20without%0Arequiring%20trusted%20hardware%20or%20deterministic%20kernels.%20We%20implement%20NAO%20as%20a%0APyTorch-compatible%20runtime%20and%20a%20contract%20layer%20currently%20deployed%20on%20Ethereum%0AHolesky%20testnet.%20The%20runtime%20instruments%20graphs%2C%20computes%20per-operator%20bounds%2C%0Aand%20runs%20unmodified%20vendor%20kernels%20in%20FP32%20with%20negligible%20overhead%20%280.3%25%20on%0AQwen3-8B%29.%20Across%20CNNs%2C%20Transformers%20and%20diffusion%20models%20on%20A100%2C%20H100%2C%0ARTX6000%2C%20RTX4090%2C%20empirical%20thresholds%20are%20%2410%5E2-10%5E3%24%20times%20tighter%20than%0Atheoretical%20bounds%2C%20and%20bound-aware%20adversarial%20attacks%20achieve%200%25%20success.%20NAO%0Areconciles%20scalability%20with%20verifiability%20for%20real-world%20heterogeneous%20ML%0Acompute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.16028v2&entry.124074799=Read"},
{"title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation", "author": "Siyong Jian and Huan Wang", "abstract": "  Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.\n", "link": "http://arxiv.org/abs/2510.18716v1", "date": "2025-10-21", "relevancy": 1.9326, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6714}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6397}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSD%3A%20Spatial-Semantic%20Head%20Decoupling%20for%20Efficient%20Autoregressive%20Image%0A%20%20Generation&body=Title%3A%20SSD%3A%20Spatial-Semantic%20Head%20Decoupling%20for%20Efficient%20Autoregressive%20Image%0A%20%20Generation%0AAuthor%3A%20Siyong%20Jian%20and%20Huan%20Wang%0AAbstract%3A%20%20%20Autoregressive%20image%20generation%20models%20like%20Janus-Pro%20produce%20high-quality%0Aimages%2C%20but%20at%20the%20significant%20cost%20of%20high%20memory%20and%20ever-growing%0Acomputational%20demands%20due%20to%20the%20large%20number%20of%20visual%20tokens.%20While%20KV%20cache%0Acompression%20has%20been%20extensively%20studied%20in%20language%20modeling%2C%20it%20still%20remains%0Alargely%20unexplored%20for%20the%20image%20generation%20domain.%20In%20this%20work%2C%20we%20begin%20by%0Aidentifying%20a%20distinct%20and%20prominent%20attention%20phenomenon%2C%20which%20we%20term%0Aspatial%20locality%20and%20emergent%20semantic%20sink.%20To%20leverage%20this%20key%20insight%2C%20we%0Aintroduce%20a%20novel%20KV%20cache%20compression%20framework.%20Specifically%2C%20we%20compress%20the%0AKV%20cache%20for%20all%20visual%20tokens%20by%20adaptively%20decoupling%20attention%20heads%20into%0Atwo%20separate%20types%3A%20for%20spatial-locality%20heads%2C%20our%20method%20maintains%20a%20short%0Arecent%20token%20window%3B%20for%20semantic-sink%20heads%2C%20it%20strategically%20preserves%20a%0Acompact%20set%20of%20highly-attended%20tokens.%20Our%20extensive%20experiments%20demonstrate%0Athat%20the%20proposed%20method%20achieves%20a%205%24%5Ctimes%24%20reduction%20in%20memory%20usage%20and%20a%0Anotable%206.6%24%5Ctimes%24%20speedup%20in%20overall%20throughput%20with%20only%20minimal%20visual%0Aquality%20loss%2C%20thereby%20enabling%20highly%20efficient%20native%20autoregressive%20image%0Ageneration%20on%20resource-constrained%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSD%253A%2520Spatial-Semantic%2520Head%2520Decoupling%2520for%2520Efficient%2520Autoregressive%2520Image%250A%2520%2520Generation%26entry.906535625%3DSiyong%2520Jian%2520and%2520Huan%2520Wang%26entry.1292438233%3D%2520%2520Autoregressive%2520image%2520generation%2520models%2520like%2520Janus-Pro%2520produce%2520high-quality%250Aimages%252C%2520but%2520at%2520the%2520significant%2520cost%2520of%2520high%2520memory%2520and%2520ever-growing%250Acomputational%2520demands%2520due%2520to%2520the%2520large%2520number%2520of%2520visual%2520tokens.%2520While%2520KV%2520cache%250Acompression%2520has%2520been%2520extensively%2520studied%2520in%2520language%2520modeling%252C%2520it%2520still%2520remains%250Alargely%2520unexplored%2520for%2520the%2520image%2520generation%2520domain.%2520In%2520this%2520work%252C%2520we%2520begin%2520by%250Aidentifying%2520a%2520distinct%2520and%2520prominent%2520attention%2520phenomenon%252C%2520which%2520we%2520term%250Aspatial%2520locality%2520and%2520emergent%2520semantic%2520sink.%2520To%2520leverage%2520this%2520key%2520insight%252C%2520we%250Aintroduce%2520a%2520novel%2520KV%2520cache%2520compression%2520framework.%2520Specifically%252C%2520we%2520compress%2520the%250AKV%2520cache%2520for%2520all%2520visual%2520tokens%2520by%2520adaptively%2520decoupling%2520attention%2520heads%2520into%250Atwo%2520separate%2520types%253A%2520for%2520spatial-locality%2520heads%252C%2520our%2520method%2520maintains%2520a%2520short%250Arecent%2520token%2520window%253B%2520for%2520semantic-sink%2520heads%252C%2520it%2520strategically%2520preserves%2520a%250Acompact%2520set%2520of%2520highly-attended%2520tokens.%2520Our%2520extensive%2520experiments%2520demonstrate%250Athat%2520the%2520proposed%2520method%2520achieves%2520a%25205%2524%255Ctimes%2524%2520reduction%2520in%2520memory%2520usage%2520and%2520a%250Anotable%25206.6%2524%255Ctimes%2524%2520speedup%2520in%2520overall%2520throughput%2520with%2520only%2520minimal%2520visual%250Aquality%2520loss%252C%2520thereby%2520enabling%2520highly%2520efficient%2520native%2520autoregressive%2520image%250Ageneration%2520on%2520resource-constrained%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSD%3A%20Spatial-Semantic%20Head%20Decoupling%20for%20Efficient%20Autoregressive%20Image%0A%20%20Generation&entry.906535625=Siyong%20Jian%20and%20Huan%20Wang&entry.1292438233=%20%20Autoregressive%20image%20generation%20models%20like%20Janus-Pro%20produce%20high-quality%0Aimages%2C%20but%20at%20the%20significant%20cost%20of%20high%20memory%20and%20ever-growing%0Acomputational%20demands%20due%20to%20the%20large%20number%20of%20visual%20tokens.%20While%20KV%20cache%0Acompression%20has%20been%20extensively%20studied%20in%20language%20modeling%2C%20it%20still%20remains%0Alargely%20unexplored%20for%20the%20image%20generation%20domain.%20In%20this%20work%2C%20we%20begin%20by%0Aidentifying%20a%20distinct%20and%20prominent%20attention%20phenomenon%2C%20which%20we%20term%0Aspatial%20locality%20and%20emergent%20semantic%20sink.%20To%20leverage%20this%20key%20insight%2C%20we%0Aintroduce%20a%20novel%20KV%20cache%20compression%20framework.%20Specifically%2C%20we%20compress%20the%0AKV%20cache%20for%20all%20visual%20tokens%20by%20adaptively%20decoupling%20attention%20heads%20into%0Atwo%20separate%20types%3A%20for%20spatial-locality%20heads%2C%20our%20method%20maintains%20a%20short%0Arecent%20token%20window%3B%20for%20semantic-sink%20heads%2C%20it%20strategically%20preserves%20a%0Acompact%20set%20of%20highly-attended%20tokens.%20Our%20extensive%20experiments%20demonstrate%0Athat%20the%20proposed%20method%20achieves%20a%205%24%5Ctimes%24%20reduction%20in%20memory%20usage%20and%20a%0Anotable%206.6%24%5Ctimes%24%20speedup%20in%20overall%20throughput%20with%20only%20minimal%20visual%0Aquality%20loss%2C%20thereby%20enabling%20highly%20efficient%20native%20autoregressive%20image%0Ageneration%20on%20resource-constrained%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18716v1&entry.124074799=Read"},
{"title": "Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate\n  Lost-in-Conversation", "author": "Ming Li", "abstract": "  Large Language Models demonstrate strong capabilities in single-turn\ninstruction following but suffer from Lost-in-Conversation (LiC), a degradation\nin performance as information is revealed progressively in multi-turn settings.\nMotivated by the current progress on Reinforcement Learning with Verifiable\nRewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable\nAccuracy and Abstention Rewards (RLAAR), a framework that encourages models not\nonly to generate correct answers, but also to judge the solvability of\nquestions in the multi-turn conversation setting. Our approach employs a\ncompetence-gated curriculum that incrementally increases dialogue difficulty\n(in terms of instruction shards), stabilizing training while promoting\nreliability. Using multi-turn, on-policy rollouts and a mixed-reward system,\nRLAAR teaches models to balance problem-solving with informed abstention,\nreducing premature answering behaviors that cause LiC. Evaluated on LiC\nbenchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to\n75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,\nthese results provide a practical recipe for building multi-turn reliable and\ntrustworthy LLMs.\n", "link": "http://arxiv.org/abs/2510.18731v1", "date": "2025-10-21", "relevancy": 1.9202, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4999}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4763}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verifiable%20Accuracy%20and%20Abstention%20Rewards%20in%20Curriculum%20RL%20to%20Alleviate%0A%20%20Lost-in-Conversation&body=Title%3A%20Verifiable%20Accuracy%20and%20Abstention%20Rewards%20in%20Curriculum%20RL%20to%20Alleviate%0A%20%20Lost-in-Conversation%0AAuthor%3A%20Ming%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20demonstrate%20strong%20capabilities%20in%20single-turn%0Ainstruction%20following%20but%20suffer%20from%20Lost-in-Conversation%20%28LiC%29%2C%20a%20degradation%0Ain%20performance%20as%20information%20is%20revealed%20progressively%20in%20multi-turn%20settings.%0AMotivated%20by%20the%20current%20progress%20on%20Reinforcement%20Learning%20with%20Verifiable%0ARewards%20%28RLVR%29%2C%20we%20propose%20Curriculum%20Reinforcement%20Learning%20with%20Verifiable%0AAccuracy%20and%20Abstention%20Rewards%20%28RLAAR%29%2C%20a%20framework%20that%20encourages%20models%20not%0Aonly%20to%20generate%20correct%20answers%2C%20but%20also%20to%20judge%20the%20solvability%20of%0Aquestions%20in%20the%20multi-turn%20conversation%20setting.%20Our%20approach%20employs%20a%0Acompetence-gated%20curriculum%20that%20incrementally%20increases%20dialogue%20difficulty%0A%28in%20terms%20of%20instruction%20shards%29%2C%20stabilizing%20training%20while%20promoting%0Areliability.%20Using%20multi-turn%2C%20on-policy%20rollouts%20and%20a%20mixed-reward%20system%2C%0ARLAAR%20teaches%20models%20to%20balance%20problem-solving%20with%20informed%20abstention%2C%0Areducing%20premature%20answering%20behaviors%20that%20cause%20LiC.%20Evaluated%20on%20LiC%0Abenchmarks%2C%20RLAAR%20significantly%20mitigates%20LiC%20performance%20decay%20%2862.6%25%20to%0A75.1%25%29%20and%20improves%20calibrated%20abstention%20rates%20%2833.5%25%20to%2073.4%25%29.%20Together%2C%0Athese%20results%20provide%20a%20practical%20recipe%20for%20building%20multi-turn%20reliable%20and%0Atrustworthy%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerifiable%2520Accuracy%2520and%2520Abstention%2520Rewards%2520in%2520Curriculum%2520RL%2520to%2520Alleviate%250A%2520%2520Lost-in-Conversation%26entry.906535625%3DMing%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520demonstrate%2520strong%2520capabilities%2520in%2520single-turn%250Ainstruction%2520following%2520but%2520suffer%2520from%2520Lost-in-Conversation%2520%2528LiC%2529%252C%2520a%2520degradation%250Ain%2520performance%2520as%2520information%2520is%2520revealed%2520progressively%2520in%2520multi-turn%2520settings.%250AMotivated%2520by%2520the%2520current%2520progress%2520on%2520Reinforcement%2520Learning%2520with%2520Verifiable%250ARewards%2520%2528RLVR%2529%252C%2520we%2520propose%2520Curriculum%2520Reinforcement%2520Learning%2520with%2520Verifiable%250AAccuracy%2520and%2520Abstention%2520Rewards%2520%2528RLAAR%2529%252C%2520a%2520framework%2520that%2520encourages%2520models%2520not%250Aonly%2520to%2520generate%2520correct%2520answers%252C%2520but%2520also%2520to%2520judge%2520the%2520solvability%2520of%250Aquestions%2520in%2520the%2520multi-turn%2520conversation%2520setting.%2520Our%2520approach%2520employs%2520a%250Acompetence-gated%2520curriculum%2520that%2520incrementally%2520increases%2520dialogue%2520difficulty%250A%2528in%2520terms%2520of%2520instruction%2520shards%2529%252C%2520stabilizing%2520training%2520while%2520promoting%250Areliability.%2520Using%2520multi-turn%252C%2520on-policy%2520rollouts%2520and%2520a%2520mixed-reward%2520system%252C%250ARLAAR%2520teaches%2520models%2520to%2520balance%2520problem-solving%2520with%2520informed%2520abstention%252C%250Areducing%2520premature%2520answering%2520behaviors%2520that%2520cause%2520LiC.%2520Evaluated%2520on%2520LiC%250Abenchmarks%252C%2520RLAAR%2520significantly%2520mitigates%2520LiC%2520performance%2520decay%2520%252862.6%2525%2520to%250A75.1%2525%2529%2520and%2520improves%2520calibrated%2520abstention%2520rates%2520%252833.5%2525%2520to%252073.4%2525%2529.%2520Together%252C%250Athese%2520results%2520provide%2520a%2520practical%2520recipe%2520for%2520building%2520multi-turn%2520reliable%2520and%250Atrustworthy%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verifiable%20Accuracy%20and%20Abstention%20Rewards%20in%20Curriculum%20RL%20to%20Alleviate%0A%20%20Lost-in-Conversation&entry.906535625=Ming%20Li&entry.1292438233=%20%20Large%20Language%20Models%20demonstrate%20strong%20capabilities%20in%20single-turn%0Ainstruction%20following%20but%20suffer%20from%20Lost-in-Conversation%20%28LiC%29%2C%20a%20degradation%0Ain%20performance%20as%20information%20is%20revealed%20progressively%20in%20multi-turn%20settings.%0AMotivated%20by%20the%20current%20progress%20on%20Reinforcement%20Learning%20with%20Verifiable%0ARewards%20%28RLVR%29%2C%20we%20propose%20Curriculum%20Reinforcement%20Learning%20with%20Verifiable%0AAccuracy%20and%20Abstention%20Rewards%20%28RLAAR%29%2C%20a%20framework%20that%20encourages%20models%20not%0Aonly%20to%20generate%20correct%20answers%2C%20but%20also%20to%20judge%20the%20solvability%20of%0Aquestions%20in%20the%20multi-turn%20conversation%20setting.%20Our%20approach%20employs%20a%0Acompetence-gated%20curriculum%20that%20incrementally%20increases%20dialogue%20difficulty%0A%28in%20terms%20of%20instruction%20shards%29%2C%20stabilizing%20training%20while%20promoting%0Areliability.%20Using%20multi-turn%2C%20on-policy%20rollouts%20and%20a%20mixed-reward%20system%2C%0ARLAAR%20teaches%20models%20to%20balance%20problem-solving%20with%20informed%20abstention%2C%0Areducing%20premature%20answering%20behaviors%20that%20cause%20LiC.%20Evaluated%20on%20LiC%0Abenchmarks%2C%20RLAAR%20significantly%20mitigates%20LiC%20performance%20decay%20%2862.6%25%20to%0A75.1%25%29%20and%20improves%20calibrated%20abstention%20rates%20%2833.5%25%20to%2073.4%25%29.%20Together%2C%0Athese%20results%20provide%20a%20practical%20recipe%20for%20building%20multi-turn%20reliable%20and%0Atrustworthy%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18731v1&entry.124074799=Read"},
{"title": "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for\n  Industrial Asset Health Monitoring", "author": "Shuxin Lin and Dhaval Patel and Christodoulos Constantinides", "abstract": "  Small Language Models (SLMs) are becoming increasingly popular in specialized\nfields, such as industrial applications, due to their efficiency, lower\ncomputational requirements, and ability to be fine-tuned for domain-specific\ntasks, enabling accurate and cost-effective solutions. However, performing\ncomplex reasoning using SLMs in specialized fields such as Industry 4.0 remains\nchallenging. In this paper, we propose a knowledge distillation framework for\nindustrial asset health, which transfers reasoning capabilities via\nChain-of-Thought (CoT) distillation from Large Language Models (LLMs) to\nsmaller, more efficient models (SLMs). We discuss the advantages and the\nprocess of distilling LLMs using multi-choice question answering (MCQA) prompts\nto enhance reasoning and refine decision-making. We also perform in-context\nlearning to verify the quality of the generated knowledge and benchmark the\nperformance of fine-tuned SLMs with generated knowledge against widely used\nLLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform\nthe base models by a significant margin, narrowing the gap to their LLM\ncounterparts. Our code is open-sourced at:\nhttps://github.com/IBM/FailureSensorIQ.\n", "link": "http://arxiv.org/abs/2510.18817v1", "date": "2025-10-21", "relevancy": 1.92, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuned%20Thoughts%3A%20Leveraging%20Chain-of-Thought%20Reasoning%20for%0A%20%20Industrial%20Asset%20Health%20Monitoring&body=Title%3A%20Fine-Tuned%20Thoughts%3A%20Leveraging%20Chain-of-Thought%20Reasoning%20for%0A%20%20Industrial%20Asset%20Health%20Monitoring%0AAuthor%3A%20Shuxin%20Lin%20and%20Dhaval%20Patel%20and%20Christodoulos%20Constantinides%0AAbstract%3A%20%20%20Small%20Language%20Models%20%28SLMs%29%20are%20becoming%20increasingly%20popular%20in%20specialized%0Afields%2C%20such%20as%20industrial%20applications%2C%20due%20to%20their%20efficiency%2C%20lower%0Acomputational%20requirements%2C%20and%20ability%20to%20be%20fine-tuned%20for%20domain-specific%0Atasks%2C%20enabling%20accurate%20and%20cost-effective%20solutions.%20However%2C%20performing%0Acomplex%20reasoning%20using%20SLMs%20in%20specialized%20fields%20such%20as%20Industry%204.0%20remains%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20a%20knowledge%20distillation%20framework%20for%0Aindustrial%20asset%20health%2C%20which%20transfers%20reasoning%20capabilities%20via%0AChain-of-Thought%20%28CoT%29%20distillation%20from%20Large%20Language%20Models%20%28LLMs%29%20to%0Asmaller%2C%20more%20efficient%20models%20%28SLMs%29.%20We%20discuss%20the%20advantages%20and%20the%0Aprocess%20of%20distilling%20LLMs%20using%20multi-choice%20question%20answering%20%28MCQA%29%20prompts%0Ato%20enhance%20reasoning%20and%20refine%20decision-making.%20We%20also%20perform%20in-context%0Alearning%20to%20verify%20the%20quality%20of%20the%20generated%20knowledge%20and%20benchmark%20the%0Aperformance%20of%20fine-tuned%20SLMs%20with%20generated%20knowledge%20against%20widely%20used%0ALLMs.%20The%20results%20show%20that%20the%20fine-tuned%20SLMs%20with%20CoT%20reasoning%20outperform%0Athe%20base%20models%20by%20a%20significant%20margin%2C%20narrowing%20the%20gap%20to%20their%20LLM%0Acounterparts.%20Our%20code%20is%20open-sourced%20at%3A%0Ahttps%3A//github.com/IBM/FailureSensorIQ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuned%2520Thoughts%253A%2520Leveraging%2520Chain-of-Thought%2520Reasoning%2520for%250A%2520%2520Industrial%2520Asset%2520Health%2520Monitoring%26entry.906535625%3DShuxin%2520Lin%2520and%2520Dhaval%2520Patel%2520and%2520Christodoulos%2520Constantinides%26entry.1292438233%3D%2520%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520are%2520becoming%2520increasingly%2520popular%2520in%2520specialized%250Afields%252C%2520such%2520as%2520industrial%2520applications%252C%2520due%2520to%2520their%2520efficiency%252C%2520lower%250Acomputational%2520requirements%252C%2520and%2520ability%2520to%2520be%2520fine-tuned%2520for%2520domain-specific%250Atasks%252C%2520enabling%2520accurate%2520and%2520cost-effective%2520solutions.%2520However%252C%2520performing%250Acomplex%2520reasoning%2520using%2520SLMs%2520in%2520specialized%2520fields%2520such%2520as%2520Industry%25204.0%2520remains%250Achallenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520knowledge%2520distillation%2520framework%2520for%250Aindustrial%2520asset%2520health%252C%2520which%2520transfers%2520reasoning%2520capabilities%2520via%250AChain-of-Thought%2520%2528CoT%2529%2520distillation%2520from%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Asmaller%252C%2520more%2520efficient%2520models%2520%2528SLMs%2529.%2520We%2520discuss%2520the%2520advantages%2520and%2520the%250Aprocess%2520of%2520distilling%2520LLMs%2520using%2520multi-choice%2520question%2520answering%2520%2528MCQA%2529%2520prompts%250Ato%2520enhance%2520reasoning%2520and%2520refine%2520decision-making.%2520We%2520also%2520perform%2520in-context%250Alearning%2520to%2520verify%2520the%2520quality%2520of%2520the%2520generated%2520knowledge%2520and%2520benchmark%2520the%250Aperformance%2520of%2520fine-tuned%2520SLMs%2520with%2520generated%2520knowledge%2520against%2520widely%2520used%250ALLMs.%2520The%2520results%2520show%2520that%2520the%2520fine-tuned%2520SLMs%2520with%2520CoT%2520reasoning%2520outperform%250Athe%2520base%2520models%2520by%2520a%2520significant%2520margin%252C%2520narrowing%2520the%2520gap%2520to%2520their%2520LLM%250Acounterparts.%2520Our%2520code%2520is%2520open-sourced%2520at%253A%250Ahttps%253A//github.com/IBM/FailureSensorIQ.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuned%20Thoughts%3A%20Leveraging%20Chain-of-Thought%20Reasoning%20for%0A%20%20Industrial%20Asset%20Health%20Monitoring&entry.906535625=Shuxin%20Lin%20and%20Dhaval%20Patel%20and%20Christodoulos%20Constantinides&entry.1292438233=%20%20Small%20Language%20Models%20%28SLMs%29%20are%20becoming%20increasingly%20popular%20in%20specialized%0Afields%2C%20such%20as%20industrial%20applications%2C%20due%20to%20their%20efficiency%2C%20lower%0Acomputational%20requirements%2C%20and%20ability%20to%20be%20fine-tuned%20for%20domain-specific%0Atasks%2C%20enabling%20accurate%20and%20cost-effective%20solutions.%20However%2C%20performing%0Acomplex%20reasoning%20using%20SLMs%20in%20specialized%20fields%20such%20as%20Industry%204.0%20remains%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20a%20knowledge%20distillation%20framework%20for%0Aindustrial%20asset%20health%2C%20which%20transfers%20reasoning%20capabilities%20via%0AChain-of-Thought%20%28CoT%29%20distillation%20from%20Large%20Language%20Models%20%28LLMs%29%20to%0Asmaller%2C%20more%20efficient%20models%20%28SLMs%29.%20We%20discuss%20the%20advantages%20and%20the%0Aprocess%20of%20distilling%20LLMs%20using%20multi-choice%20question%20answering%20%28MCQA%29%20prompts%0Ato%20enhance%20reasoning%20and%20refine%20decision-making.%20We%20also%20perform%20in-context%0Alearning%20to%20verify%20the%20quality%20of%20the%20generated%20knowledge%20and%20benchmark%20the%0Aperformance%20of%20fine-tuned%20SLMs%20with%20generated%20knowledge%20against%20widely%20used%0ALLMs.%20The%20results%20show%20that%20the%20fine-tuned%20SLMs%20with%20CoT%20reasoning%20outperform%0Athe%20base%20models%20by%20a%20significant%20margin%2C%20narrowing%20the%20gap%20to%20their%20LLM%0Acounterparts.%20Our%20code%20is%20open-sourced%20at%3A%0Ahttps%3A//github.com/IBM/FailureSensorIQ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18817v1&entry.124074799=Read"},
{"title": "Learning to See and Act: Task-Aware View Planning for Robotic\n  Manipulation", "author": "Yongjie Bai and Zhouxia Wang and Yang Liu and Weixing Chen and Ziliang Chen and Mingtong Dai and Yongsen Zheng and Lingbo Liu and Guanbin Li and Liang Lin", "abstract": "  Recent vision-language-action (VLA) models for multi-task robotic\nmanipulation commonly rely on static viewpoints and shared visual encoders,\nwhich limit 3D perception and cause task interference, hindering robustness and\ngeneralization. In this work, we propose Task-Aware View Planning (TAVP), a\nframework designed to overcome these challenges by integrating active view\nplanning with task-specific representation learning. TAVP employs an efficient\nexploration policy, accelerated by a novel pseudo-environment, to actively\nacquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)\nvisual encoder to disentangle features across different tasks, boosting both\nrepresentation fidelity and task generalization. By learning to see the world\nin a task-aware way, TAVP generates more complete and discriminative visual\nrepresentations, demonstrating significantly enhanced action prediction across\na wide array of manipulation challenges. Extensive experiments on RLBench tasks\nshow that our proposed TAVP model achieves superior performance over\nstate-of-the-art fixed-view approaches. Visual results and code are provided\nat: https://hcplab-sysu.github.io/TAVP.\n", "link": "http://arxiv.org/abs/2508.05186v2", "date": "2025-10-21", "relevancy": 1.8938, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6633}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6241}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20See%20and%20Act%3A%20Task-Aware%20View%20Planning%20for%20Robotic%0A%20%20Manipulation&body=Title%3A%20Learning%20to%20See%20and%20Act%3A%20Task-Aware%20View%20Planning%20for%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Yongjie%20Bai%20and%20Zhouxia%20Wang%20and%20Yang%20Liu%20and%20Weixing%20Chen%20and%20Ziliang%20Chen%20and%20Mingtong%20Dai%20and%20Yongsen%20Zheng%20and%20Lingbo%20Liu%20and%20Guanbin%20Li%20and%20Liang%20Lin%0AAbstract%3A%20%20%20Recent%20vision-language-action%20%28VLA%29%20models%20for%20multi-task%20robotic%0Amanipulation%20commonly%20rely%20on%20static%20viewpoints%20and%20shared%20visual%20encoders%2C%0Awhich%20limit%203D%20perception%20and%20cause%20task%20interference%2C%20hindering%20robustness%20and%0Ageneralization.%20In%20this%20work%2C%20we%20propose%20Task-Aware%20View%20Planning%20%28TAVP%29%2C%20a%0Aframework%20designed%20to%20overcome%20these%20challenges%20by%20integrating%20active%20view%0Aplanning%20with%20task-specific%20representation%20learning.%20TAVP%20employs%20an%20efficient%0Aexploration%20policy%2C%20accelerated%20by%20a%20novel%20pseudo-environment%2C%20to%20actively%0Aacquire%20informative%20views.%20Furthermore%2C%20we%20introduce%20a%20Mixture-of-Experts%20%28MoE%29%0Avisual%20encoder%20to%20disentangle%20features%20across%20different%20tasks%2C%20boosting%20both%0Arepresentation%20fidelity%20and%20task%20generalization.%20By%20learning%20to%20see%20the%20world%0Ain%20a%20task-aware%20way%2C%20TAVP%20generates%20more%20complete%20and%20discriminative%20visual%0Arepresentations%2C%20demonstrating%20significantly%20enhanced%20action%20prediction%20across%0Aa%20wide%20array%20of%20manipulation%20challenges.%20Extensive%20experiments%20on%20RLBench%20tasks%0Ashow%20that%20our%20proposed%20TAVP%20model%20achieves%20superior%20performance%20over%0Astate-of-the-art%20fixed-view%20approaches.%20Visual%20results%20and%20code%20are%20provided%0Aat%3A%20https%3A//hcplab-sysu.github.io/TAVP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05186v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520See%2520and%2520Act%253A%2520Task-Aware%2520View%2520Planning%2520for%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DYongjie%2520Bai%2520and%2520Zhouxia%2520Wang%2520and%2520Yang%2520Liu%2520and%2520Weixing%2520Chen%2520and%2520Ziliang%2520Chen%2520and%2520Mingtong%2520Dai%2520and%2520Yongsen%2520Zheng%2520and%2520Lingbo%2520Liu%2520and%2520Guanbin%2520Li%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520vision-language-action%2520%2528VLA%2529%2520models%2520for%2520multi-task%2520robotic%250Amanipulation%2520commonly%2520rely%2520on%2520static%2520viewpoints%2520and%2520shared%2520visual%2520encoders%252C%250Awhich%2520limit%25203D%2520perception%2520and%2520cause%2520task%2520interference%252C%2520hindering%2520robustness%2520and%250Ageneralization.%2520In%2520this%2520work%252C%2520we%2520propose%2520Task-Aware%2520View%2520Planning%2520%2528TAVP%2529%252C%2520a%250Aframework%2520designed%2520to%2520overcome%2520these%2520challenges%2520by%2520integrating%2520active%2520view%250Aplanning%2520with%2520task-specific%2520representation%2520learning.%2520TAVP%2520employs%2520an%2520efficient%250Aexploration%2520policy%252C%2520accelerated%2520by%2520a%2520novel%2520pseudo-environment%252C%2520to%2520actively%250Aacquire%2520informative%2520views.%2520Furthermore%252C%2520we%2520introduce%2520a%2520Mixture-of-Experts%2520%2528MoE%2529%250Avisual%2520encoder%2520to%2520disentangle%2520features%2520across%2520different%2520tasks%252C%2520boosting%2520both%250Arepresentation%2520fidelity%2520and%2520task%2520generalization.%2520By%2520learning%2520to%2520see%2520the%2520world%250Ain%2520a%2520task-aware%2520way%252C%2520TAVP%2520generates%2520more%2520complete%2520and%2520discriminative%2520visual%250Arepresentations%252C%2520demonstrating%2520significantly%2520enhanced%2520action%2520prediction%2520across%250Aa%2520wide%2520array%2520of%2520manipulation%2520challenges.%2520Extensive%2520experiments%2520on%2520RLBench%2520tasks%250Ashow%2520that%2520our%2520proposed%2520TAVP%2520model%2520achieves%2520superior%2520performance%2520over%250Astate-of-the-art%2520fixed-view%2520approaches.%2520Visual%2520results%2520and%2520code%2520are%2520provided%250Aat%253A%2520https%253A//hcplab-sysu.github.io/TAVP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05186v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20See%20and%20Act%3A%20Task-Aware%20View%20Planning%20for%20Robotic%0A%20%20Manipulation&entry.906535625=Yongjie%20Bai%20and%20Zhouxia%20Wang%20and%20Yang%20Liu%20and%20Weixing%20Chen%20and%20Ziliang%20Chen%20and%20Mingtong%20Dai%20and%20Yongsen%20Zheng%20and%20Lingbo%20Liu%20and%20Guanbin%20Li%20and%20Liang%20Lin&entry.1292438233=%20%20Recent%20vision-language-action%20%28VLA%29%20models%20for%20multi-task%20robotic%0Amanipulation%20commonly%20rely%20on%20static%20viewpoints%20and%20shared%20visual%20encoders%2C%0Awhich%20limit%203D%20perception%20and%20cause%20task%20interference%2C%20hindering%20robustness%20and%0Ageneralization.%20In%20this%20work%2C%20we%20propose%20Task-Aware%20View%20Planning%20%28TAVP%29%2C%20a%0Aframework%20designed%20to%20overcome%20these%20challenges%20by%20integrating%20active%20view%0Aplanning%20with%20task-specific%20representation%20learning.%20TAVP%20employs%20an%20efficient%0Aexploration%20policy%2C%20accelerated%20by%20a%20novel%20pseudo-environment%2C%20to%20actively%0Aacquire%20informative%20views.%20Furthermore%2C%20we%20introduce%20a%20Mixture-of-Experts%20%28MoE%29%0Avisual%20encoder%20to%20disentangle%20features%20across%20different%20tasks%2C%20boosting%20both%0Arepresentation%20fidelity%20and%20task%20generalization.%20By%20learning%20to%20see%20the%20world%0Ain%20a%20task-aware%20way%2C%20TAVP%20generates%20more%20complete%20and%20discriminative%20visual%0Arepresentations%2C%20demonstrating%20significantly%20enhanced%20action%20prediction%20across%0Aa%20wide%20array%20of%20manipulation%20challenges.%20Extensive%20experiments%20on%20RLBench%20tasks%0Ashow%20that%20our%20proposed%20TAVP%20model%20achieves%20superior%20performance%20over%0Astate-of-the-art%20fixed-view%20approaches.%20Visual%20results%20and%20code%20are%20provided%0Aat%3A%20https%3A//hcplab-sysu.github.io/TAVP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05186v2&entry.124074799=Read"},
{"title": "PowerChain: A Verifiable Agentic AI System for Automating Distribution\n  Grid Analyses", "author": "Emmanuel O. Badmus and Peng Sang and Dimitrios Stamoulis and Amritanshu Pandey", "abstract": "  Rapid electrification and decarbonization are increasing the complexity of\ndistribution grid (DG) operation and planning, necessitating advanced\ncomputational analyses to ensure reliability and resilience. These analyses\ndepend on disparate workflows comprising complex models, function calls, and\ndata pipelines that require substantial expert knowledge and remain difficult\nto automate. Workforce and budget constraints further limit utilities' ability\nto apply such analyses at scale. To address this gap, we build an agentic\nsystem PowerChain, which is capable of autonomously performing complex grid\nanalyses. Existing agentic AI systems are typically developed in a bottom-up\nmanner with customized context for predefined analysis tasks; therefore, they\ndo not generalize to tasks that the agent has never seen. In comparison, to\ngeneralize to unseen DG analysis tasks, PowerChain dynamically generates\nstructured context by leveraging supervisory signals from self-contained power\nsystems tools (e.g., GridLAB-D) and an optimized set of expert-annotated and\nverified reasoning trajectories. For complex DG tasks defined in natural\nlanguage, empirical results on real utility data demonstrate that PowerChain\nachieves up to a 144/% improvement in performance over baselines.\n", "link": "http://arxiv.org/abs/2508.17094v3", "date": "2025-10-21", "relevancy": 1.8914, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5053}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4853}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PowerChain%3A%20A%20Verifiable%20Agentic%20AI%20System%20for%20Automating%20Distribution%0A%20%20Grid%20Analyses&body=Title%3A%20PowerChain%3A%20A%20Verifiable%20Agentic%20AI%20System%20for%20Automating%20Distribution%0A%20%20Grid%20Analyses%0AAuthor%3A%20Emmanuel%20O.%20Badmus%20and%20Peng%20Sang%20and%20Dimitrios%20Stamoulis%20and%20Amritanshu%20Pandey%0AAbstract%3A%20%20%20Rapid%20electrification%20and%20decarbonization%20are%20increasing%20the%20complexity%20of%0Adistribution%20grid%20%28DG%29%20operation%20and%20planning%2C%20necessitating%20advanced%0Acomputational%20analyses%20to%20ensure%20reliability%20and%20resilience.%20These%20analyses%0Adepend%20on%20disparate%20workflows%20comprising%20complex%20models%2C%20function%20calls%2C%20and%0Adata%20pipelines%20that%20require%20substantial%20expert%20knowledge%20and%20remain%20difficult%0Ato%20automate.%20Workforce%20and%20budget%20constraints%20further%20limit%20utilities%27%20ability%0Ato%20apply%20such%20analyses%20at%20scale.%20To%20address%20this%20gap%2C%20we%20build%20an%20agentic%0Asystem%20PowerChain%2C%20which%20is%20capable%20of%20autonomously%20performing%20complex%20grid%0Aanalyses.%20Existing%20agentic%20AI%20systems%20are%20typically%20developed%20in%20a%20bottom-up%0Amanner%20with%20customized%20context%20for%20predefined%20analysis%20tasks%3B%20therefore%2C%20they%0Ado%20not%20generalize%20to%20tasks%20that%20the%20agent%20has%20never%20seen.%20In%20comparison%2C%20to%0Ageneralize%20to%20unseen%20DG%20analysis%20tasks%2C%20PowerChain%20dynamically%20generates%0Astructured%20context%20by%20leveraging%20supervisory%20signals%20from%20self-contained%20power%0Asystems%20tools%20%28e.g.%2C%20GridLAB-D%29%20and%20an%20optimized%20set%20of%20expert-annotated%20and%0Averified%20reasoning%20trajectories.%20For%20complex%20DG%20tasks%20defined%20in%20natural%0Alanguage%2C%20empirical%20results%20on%20real%20utility%20data%20demonstrate%20that%20PowerChain%0Aachieves%20up%20to%20a%20144/%25%20improvement%20in%20performance%20over%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17094v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPowerChain%253A%2520A%2520Verifiable%2520Agentic%2520AI%2520System%2520for%2520Automating%2520Distribution%250A%2520%2520Grid%2520Analyses%26entry.906535625%3DEmmanuel%2520O.%2520Badmus%2520and%2520Peng%2520Sang%2520and%2520Dimitrios%2520Stamoulis%2520and%2520Amritanshu%2520Pandey%26entry.1292438233%3D%2520%2520Rapid%2520electrification%2520and%2520decarbonization%2520are%2520increasing%2520the%2520complexity%2520of%250Adistribution%2520grid%2520%2528DG%2529%2520operation%2520and%2520planning%252C%2520necessitating%2520advanced%250Acomputational%2520analyses%2520to%2520ensure%2520reliability%2520and%2520resilience.%2520These%2520analyses%250Adepend%2520on%2520disparate%2520workflows%2520comprising%2520complex%2520models%252C%2520function%2520calls%252C%2520and%250Adata%2520pipelines%2520that%2520require%2520substantial%2520expert%2520knowledge%2520and%2520remain%2520difficult%250Ato%2520automate.%2520Workforce%2520and%2520budget%2520constraints%2520further%2520limit%2520utilities%2527%2520ability%250Ato%2520apply%2520such%2520analyses%2520at%2520scale.%2520To%2520address%2520this%2520gap%252C%2520we%2520build%2520an%2520agentic%250Asystem%2520PowerChain%252C%2520which%2520is%2520capable%2520of%2520autonomously%2520performing%2520complex%2520grid%250Aanalyses.%2520Existing%2520agentic%2520AI%2520systems%2520are%2520typically%2520developed%2520in%2520a%2520bottom-up%250Amanner%2520with%2520customized%2520context%2520for%2520predefined%2520analysis%2520tasks%253B%2520therefore%252C%2520they%250Ado%2520not%2520generalize%2520to%2520tasks%2520that%2520the%2520agent%2520has%2520never%2520seen.%2520In%2520comparison%252C%2520to%250Ageneralize%2520to%2520unseen%2520DG%2520analysis%2520tasks%252C%2520PowerChain%2520dynamically%2520generates%250Astructured%2520context%2520by%2520leveraging%2520supervisory%2520signals%2520from%2520self-contained%2520power%250Asystems%2520tools%2520%2528e.g.%252C%2520GridLAB-D%2529%2520and%2520an%2520optimized%2520set%2520of%2520expert-annotated%2520and%250Averified%2520reasoning%2520trajectories.%2520For%2520complex%2520DG%2520tasks%2520defined%2520in%2520natural%250Alanguage%252C%2520empirical%2520results%2520on%2520real%2520utility%2520data%2520demonstrate%2520that%2520PowerChain%250Aachieves%2520up%2520to%2520a%2520144/%2525%2520improvement%2520in%2520performance%2520over%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17094v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PowerChain%3A%20A%20Verifiable%20Agentic%20AI%20System%20for%20Automating%20Distribution%0A%20%20Grid%20Analyses&entry.906535625=Emmanuel%20O.%20Badmus%20and%20Peng%20Sang%20and%20Dimitrios%20Stamoulis%20and%20Amritanshu%20Pandey&entry.1292438233=%20%20Rapid%20electrification%20and%20decarbonization%20are%20increasing%20the%20complexity%20of%0Adistribution%20grid%20%28DG%29%20operation%20and%20planning%2C%20necessitating%20advanced%0Acomputational%20analyses%20to%20ensure%20reliability%20and%20resilience.%20These%20analyses%0Adepend%20on%20disparate%20workflows%20comprising%20complex%20models%2C%20function%20calls%2C%20and%0Adata%20pipelines%20that%20require%20substantial%20expert%20knowledge%20and%20remain%20difficult%0Ato%20automate.%20Workforce%20and%20budget%20constraints%20further%20limit%20utilities%27%20ability%0Ato%20apply%20such%20analyses%20at%20scale.%20To%20address%20this%20gap%2C%20we%20build%20an%20agentic%0Asystem%20PowerChain%2C%20which%20is%20capable%20of%20autonomously%20performing%20complex%20grid%0Aanalyses.%20Existing%20agentic%20AI%20systems%20are%20typically%20developed%20in%20a%20bottom-up%0Amanner%20with%20customized%20context%20for%20predefined%20analysis%20tasks%3B%20therefore%2C%20they%0Ado%20not%20generalize%20to%20tasks%20that%20the%20agent%20has%20never%20seen.%20In%20comparison%2C%20to%0Ageneralize%20to%20unseen%20DG%20analysis%20tasks%2C%20PowerChain%20dynamically%20generates%0Astructured%20context%20by%20leveraging%20supervisory%20signals%20from%20self-contained%20power%0Asystems%20tools%20%28e.g.%2C%20GridLAB-D%29%20and%20an%20optimized%20set%20of%20expert-annotated%20and%0Averified%20reasoning%20trajectories.%20For%20complex%20DG%20tasks%20defined%20in%20natural%0Alanguage%2C%20empirical%20results%20on%20real%20utility%20data%20demonstrate%20that%20PowerChain%0Aachieves%20up%20to%20a%20144/%25%20improvement%20in%20performance%20over%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17094v3&entry.124074799=Read"},
{"title": "When LRP Diverges from Leave-One-Out in Transformers", "author": "Weiqiu You and Siqi Zeng and Yao-Hung Hubert Tsai and Makoto Yamada and Han Zhao", "abstract": "  Leave-One-Out (LOO) provides an intuitive measure of feature importance but\nis computationally prohibitive. While Layer-Wise Relevance Propagation (LRP)\noffers a potentially efficient alternative, its axiomatic soundness in modern\nTransformers remains largely under-examined. In this work, we first show that\nthe bilinear propagation rules used in recent advances of AttnLRP violate the\nimplementation invariance axiom. We prove this analytically and confirm it\nempirically in linear attention layers. Second, we also revisit CP-LRP as a\ndiagnostic baseline and find that bypassing relevance propagation through the\nsoftmax layer -- backpropagating relevance only through the value matrices --\nsignificantly improves alignment with LOO, particularly in middle-to-late\nTransformer layers. Overall, our results suggest that (i) bilinear\nfactorization sensitivity and (ii) softmax propagation error potentially\njointly undermine LRP's ability to approximate LOO in Transformers.\n", "link": "http://arxiv.org/abs/2510.18810v1", "date": "2025-10-21", "relevancy": 1.8858, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4789}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20LRP%20Diverges%20from%20Leave-One-Out%20in%20Transformers&body=Title%3A%20When%20LRP%20Diverges%20from%20Leave-One-Out%20in%20Transformers%0AAuthor%3A%20Weiqiu%20You%20and%20Siqi%20Zeng%20and%20Yao-Hung%20Hubert%20Tsai%20and%20Makoto%20Yamada%20and%20Han%20Zhao%0AAbstract%3A%20%20%20Leave-One-Out%20%28LOO%29%20provides%20an%20intuitive%20measure%20of%20feature%20importance%20but%0Ais%20computationally%20prohibitive.%20While%20Layer-Wise%20Relevance%20Propagation%20%28LRP%29%0Aoffers%20a%20potentially%20efficient%20alternative%2C%20its%20axiomatic%20soundness%20in%20modern%0ATransformers%20remains%20largely%20under-examined.%20In%20this%20work%2C%20we%20first%20show%20that%0Athe%20bilinear%20propagation%20rules%20used%20in%20recent%20advances%20of%20AttnLRP%20violate%20the%0Aimplementation%20invariance%20axiom.%20We%20prove%20this%20analytically%20and%20confirm%20it%0Aempirically%20in%20linear%20attention%20layers.%20Second%2C%20we%20also%20revisit%20CP-LRP%20as%20a%0Adiagnostic%20baseline%20and%20find%20that%20bypassing%20relevance%20propagation%20through%20the%0Asoftmax%20layer%20--%20backpropagating%20relevance%20only%20through%20the%20value%20matrices%20--%0Asignificantly%20improves%20alignment%20with%20LOO%2C%20particularly%20in%20middle-to-late%0ATransformer%20layers.%20Overall%2C%20our%20results%20suggest%20that%20%28i%29%20bilinear%0Afactorization%20sensitivity%20and%20%28ii%29%20softmax%20propagation%20error%20potentially%0Ajointly%20undermine%20LRP%27s%20ability%20to%20approximate%20LOO%20in%20Transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520LRP%2520Diverges%2520from%2520Leave-One-Out%2520in%2520Transformers%26entry.906535625%3DWeiqiu%2520You%2520and%2520Siqi%2520Zeng%2520and%2520Yao-Hung%2520Hubert%2520Tsai%2520and%2520Makoto%2520Yamada%2520and%2520Han%2520Zhao%26entry.1292438233%3D%2520%2520Leave-One-Out%2520%2528LOO%2529%2520provides%2520an%2520intuitive%2520measure%2520of%2520feature%2520importance%2520but%250Ais%2520computationally%2520prohibitive.%2520While%2520Layer-Wise%2520Relevance%2520Propagation%2520%2528LRP%2529%250Aoffers%2520a%2520potentially%2520efficient%2520alternative%252C%2520its%2520axiomatic%2520soundness%2520in%2520modern%250ATransformers%2520remains%2520largely%2520under-examined.%2520In%2520this%2520work%252C%2520we%2520first%2520show%2520that%250Athe%2520bilinear%2520propagation%2520rules%2520used%2520in%2520recent%2520advances%2520of%2520AttnLRP%2520violate%2520the%250Aimplementation%2520invariance%2520axiom.%2520We%2520prove%2520this%2520analytically%2520and%2520confirm%2520it%250Aempirically%2520in%2520linear%2520attention%2520layers.%2520Second%252C%2520we%2520also%2520revisit%2520CP-LRP%2520as%2520a%250Adiagnostic%2520baseline%2520and%2520find%2520that%2520bypassing%2520relevance%2520propagation%2520through%2520the%250Asoftmax%2520layer%2520--%2520backpropagating%2520relevance%2520only%2520through%2520the%2520value%2520matrices%2520--%250Asignificantly%2520improves%2520alignment%2520with%2520LOO%252C%2520particularly%2520in%2520middle-to-late%250ATransformer%2520layers.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520%2528i%2529%2520bilinear%250Afactorization%2520sensitivity%2520and%2520%2528ii%2529%2520softmax%2520propagation%2520error%2520potentially%250Ajointly%2520undermine%2520LRP%2527s%2520ability%2520to%2520approximate%2520LOO%2520in%2520Transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20LRP%20Diverges%20from%20Leave-One-Out%20in%20Transformers&entry.906535625=Weiqiu%20You%20and%20Siqi%20Zeng%20and%20Yao-Hung%20Hubert%20Tsai%20and%20Makoto%20Yamada%20and%20Han%20Zhao&entry.1292438233=%20%20Leave-One-Out%20%28LOO%29%20provides%20an%20intuitive%20measure%20of%20feature%20importance%20but%0Ais%20computationally%20prohibitive.%20While%20Layer-Wise%20Relevance%20Propagation%20%28LRP%29%0Aoffers%20a%20potentially%20efficient%20alternative%2C%20its%20axiomatic%20soundness%20in%20modern%0ATransformers%20remains%20largely%20under-examined.%20In%20this%20work%2C%20we%20first%20show%20that%0Athe%20bilinear%20propagation%20rules%20used%20in%20recent%20advances%20of%20AttnLRP%20violate%20the%0Aimplementation%20invariance%20axiom.%20We%20prove%20this%20analytically%20and%20confirm%20it%0Aempirically%20in%20linear%20attention%20layers.%20Second%2C%20we%20also%20revisit%20CP-LRP%20as%20a%0Adiagnostic%20baseline%20and%20find%20that%20bypassing%20relevance%20propagation%20through%20the%0Asoftmax%20layer%20--%20backpropagating%20relevance%20only%20through%20the%20value%20matrices%20--%0Asignificantly%20improves%20alignment%20with%20LOO%2C%20particularly%20in%20middle-to-late%0ATransformer%20layers.%20Overall%2C%20our%20results%20suggest%20that%20%28i%29%20bilinear%0Afactorization%20sensitivity%20and%20%28ii%29%20softmax%20propagation%20error%20potentially%0Ajointly%20undermine%20LRP%27s%20ability%20to%20approximate%20LOO%20in%20Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18810v1&entry.124074799=Read"},
{"title": "Inverse Q-Learning Done Right: Offline Imitation Learning in\n  $Q^\u03c0$-Realizable MDPs", "author": "Antoine Moulin and Gergely Neu and Luca Viano", "abstract": "  We study the problem of offline imitation learning in Markov decision\nprocesses (MDPs), where the goal is to learn a well-performing policy given a\ndataset of state-action pairs generated by an expert policy. Complementing a\nrecent line of work on this topic that assumes the expert belongs to a\ntractable class of known policies, we approach this problem from a new angle\nand leverage a different type of structural assumption about the environment.\nSpecifically, for the class of linear $Q^\\pi$-realizable MDPs, we introduce a\nnew algorithm called saddle-point offline imitation learning (\\SPOIL), which is\nguaranteed to match the performance of any expert up to an additive error\n$\\varepsilon$ with access to $\\mathcal{O}(\\varepsilon^{-2})$ samples. Moreover,\nwe extend this result to possibly non-linear $Q^\\pi$-realizable MDPs at the\ncost of a worse sample complexity of order $\\mathcal{O}(\\varepsilon^{-4})$.\nFinally, our analysis suggests a new loss function for training critic networks\nfrom expert data in deep imitation learning. Empirical evaluations on standard\nbenchmarks demonstrate that the neural net implementation of \\SPOIL is superior\nto behavior cloning and competitive with state-of-the-art algorithms.\n", "link": "http://arxiv.org/abs/2505.19946v3", "date": "2025-10-21", "relevancy": 1.8701, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4842}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4679}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Q-Learning%20Done%20Right%3A%20Offline%20Imitation%20Learning%20in%0A%20%20%24Q%5E%CF%80%24-Realizable%20MDPs&body=Title%3A%20Inverse%20Q-Learning%20Done%20Right%3A%20Offline%20Imitation%20Learning%20in%0A%20%20%24Q%5E%CF%80%24-Realizable%20MDPs%0AAuthor%3A%20Antoine%20Moulin%20and%20Gergely%20Neu%20and%20Luca%20Viano%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20offline%20imitation%20learning%20in%20Markov%20decision%0Aprocesses%20%28MDPs%29%2C%20where%20the%20goal%20is%20to%20learn%20a%20well-performing%20policy%20given%20a%0Adataset%20of%20state-action%20pairs%20generated%20by%20an%20expert%20policy.%20Complementing%20a%0Arecent%20line%20of%20work%20on%20this%20topic%20that%20assumes%20the%20expert%20belongs%20to%20a%0Atractable%20class%20of%20known%20policies%2C%20we%20approach%20this%20problem%20from%20a%20new%20angle%0Aand%20leverage%20a%20different%20type%20of%20structural%20assumption%20about%20the%20environment.%0ASpecifically%2C%20for%20the%20class%20of%20linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%2C%20we%20introduce%20a%0Anew%20algorithm%20called%20saddle-point%20offline%20imitation%20learning%20%28%5CSPOIL%29%2C%20which%20is%0Aguaranteed%20to%20match%20the%20performance%20of%20any%20expert%20up%20to%20an%20additive%20error%0A%24%5Cvarepsilon%24%20with%20access%20to%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-2%7D%29%24%20samples.%20Moreover%2C%0Awe%20extend%20this%20result%20to%20possibly%20non-linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%20at%20the%0Acost%20of%20a%20worse%20sample%20complexity%20of%20order%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-4%7D%29%24.%0AFinally%2C%20our%20analysis%20suggests%20a%20new%20loss%20function%20for%20training%20critic%20networks%0Afrom%20expert%20data%20in%20deep%20imitation%20learning.%20Empirical%20evaluations%20on%20standard%0Abenchmarks%20demonstrate%20that%20the%20neural%20net%20implementation%20of%20%5CSPOIL%20is%20superior%0Ato%20behavior%20cloning%20and%20competitive%20with%20state-of-the-art%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19946v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Q-Learning%2520Done%2520Right%253A%2520Offline%2520Imitation%2520Learning%2520in%250A%2520%2520%2524Q%255E%25CF%2580%2524-Realizable%2520MDPs%26entry.906535625%3DAntoine%2520Moulin%2520and%2520Gergely%2520Neu%2520and%2520Luca%2520Viano%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520offline%2520imitation%2520learning%2520in%2520Markov%2520decision%250Aprocesses%2520%2528MDPs%2529%252C%2520where%2520the%2520goal%2520is%2520to%2520learn%2520a%2520well-performing%2520policy%2520given%2520a%250Adataset%2520of%2520state-action%2520pairs%2520generated%2520by%2520an%2520expert%2520policy.%2520Complementing%2520a%250Arecent%2520line%2520of%2520work%2520on%2520this%2520topic%2520that%2520assumes%2520the%2520expert%2520belongs%2520to%2520a%250Atractable%2520class%2520of%2520known%2520policies%252C%2520we%2520approach%2520this%2520problem%2520from%2520a%2520new%2520angle%250Aand%2520leverage%2520a%2520different%2520type%2520of%2520structural%2520assumption%2520about%2520the%2520environment.%250ASpecifically%252C%2520for%2520the%2520class%2520of%2520linear%2520%2524Q%255E%255Cpi%2524-realizable%2520MDPs%252C%2520we%2520introduce%2520a%250Anew%2520algorithm%2520called%2520saddle-point%2520offline%2520imitation%2520learning%2520%2528%255CSPOIL%2529%252C%2520which%2520is%250Aguaranteed%2520to%2520match%2520the%2520performance%2520of%2520any%2520expert%2520up%2520to%2520an%2520additive%2520error%250A%2524%255Cvarepsilon%2524%2520with%2520access%2520to%2520%2524%255Cmathcal%257BO%257D%2528%255Cvarepsilon%255E%257B-2%257D%2529%2524%2520samples.%2520Moreover%252C%250Awe%2520extend%2520this%2520result%2520to%2520possibly%2520non-linear%2520%2524Q%255E%255Cpi%2524-realizable%2520MDPs%2520at%2520the%250Acost%2520of%2520a%2520worse%2520sample%2520complexity%2520of%2520order%2520%2524%255Cmathcal%257BO%257D%2528%255Cvarepsilon%255E%257B-4%257D%2529%2524.%250AFinally%252C%2520our%2520analysis%2520suggests%2520a%2520new%2520loss%2520function%2520for%2520training%2520critic%2520networks%250Afrom%2520expert%2520data%2520in%2520deep%2520imitation%2520learning.%2520Empirical%2520evaluations%2520on%2520standard%250Abenchmarks%2520demonstrate%2520that%2520the%2520neural%2520net%2520implementation%2520of%2520%255CSPOIL%2520is%2520superior%250Ato%2520behavior%2520cloning%2520and%2520competitive%2520with%2520state-of-the-art%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19946v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Q-Learning%20Done%20Right%3A%20Offline%20Imitation%20Learning%20in%0A%20%20%24Q%5E%CF%80%24-Realizable%20MDPs&entry.906535625=Antoine%20Moulin%20and%20Gergely%20Neu%20and%20Luca%20Viano&entry.1292438233=%20%20We%20study%20the%20problem%20of%20offline%20imitation%20learning%20in%20Markov%20decision%0Aprocesses%20%28MDPs%29%2C%20where%20the%20goal%20is%20to%20learn%20a%20well-performing%20policy%20given%20a%0Adataset%20of%20state-action%20pairs%20generated%20by%20an%20expert%20policy.%20Complementing%20a%0Arecent%20line%20of%20work%20on%20this%20topic%20that%20assumes%20the%20expert%20belongs%20to%20a%0Atractable%20class%20of%20known%20policies%2C%20we%20approach%20this%20problem%20from%20a%20new%20angle%0Aand%20leverage%20a%20different%20type%20of%20structural%20assumption%20about%20the%20environment.%0ASpecifically%2C%20for%20the%20class%20of%20linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%2C%20we%20introduce%20a%0Anew%20algorithm%20called%20saddle-point%20offline%20imitation%20learning%20%28%5CSPOIL%29%2C%20which%20is%0Aguaranteed%20to%20match%20the%20performance%20of%20any%20expert%20up%20to%20an%20additive%20error%0A%24%5Cvarepsilon%24%20with%20access%20to%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-2%7D%29%24%20samples.%20Moreover%2C%0Awe%20extend%20this%20result%20to%20possibly%20non-linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%20at%20the%0Acost%20of%20a%20worse%20sample%20complexity%20of%20order%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-4%7D%29%24.%0AFinally%2C%20our%20analysis%20suggests%20a%20new%20loss%20function%20for%20training%20critic%20networks%0Afrom%20expert%20data%20in%20deep%20imitation%20learning.%20Empirical%20evaluations%20on%20standard%0Abenchmarks%20demonstrate%20that%20the%20neural%20net%20implementation%20of%20%5CSPOIL%20is%20superior%0Ato%20behavior%20cloning%20and%20competitive%20with%20state-of-the-art%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19946v3&entry.124074799=Read"},
{"title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn\n  LLM Jailbreaks", "author": "Javad Rafiei Asl and Sidhant Narula and Mohammad Ghasemigol and Eduardo Blanco and Daniel Takabi", "abstract": "  Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS\n", "link": "http://arxiv.org/abs/2510.03417v2", "date": "2025-10-21", "relevancy": 1.8611, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4689}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NEXUS%3A%20Network%20Exploration%20for%20eXploiting%20Unsafe%20Sequences%20in%20Multi-Turn%0A%20%20LLM%20Jailbreaks&body=Title%3A%20NEXUS%3A%20Network%20Exploration%20for%20eXploiting%20Unsafe%20Sequences%20in%20Multi-Turn%0A%20%20LLM%20Jailbreaks%0AAuthor%3A%20Javad%20Rafiei%20Asl%20and%20Sidhant%20Narula%20and%20Mohammad%20Ghasemigol%20and%20Eduardo%20Blanco%20and%20Daniel%20Takabi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20natural%20language%20processing%0Abut%20remain%20vulnerable%20to%20jailbreak%20attacks%2C%20especially%20multi-turn%20jailbreaks%0Athat%20distribute%20malicious%20intent%20across%20benign%20exchanges%20and%20bypass%20alignment%0Amechanisms.%20Existing%20approaches%20often%20explore%20the%20adversarial%20space%20poorly%2C%0Arely%20on%20hand-crafted%20heuristics%2C%20or%20lack%20systematic%20query%20refinement.%20We%0Apresent%20NEXUS%20%28Network%20Exploration%20for%20eXploiting%20Unsafe%20Sequences%29%2C%20a%20modular%0Aframework%20for%20constructing%2C%20refining%2C%20and%20executing%20optimized%20multi-turn%0Aattacks.%20NEXUS%20comprises%3A%20%281%29%20ThoughtNet%2C%20which%20hierarchically%20expands%20a%0Aharmful%20intent%20into%20a%20structured%20semantic%20network%20of%20topics%2C%20entities%2C%20and%0Aquery%20chains%3B%20%282%29%20a%20feedback-driven%20Simulator%20that%20iteratively%20refines%20and%0Aprunes%20these%20chains%20through%20attacker-victim-judge%20LLM%20collaboration%20using%0Aharmfulness%20and%20semantic-similarity%20benchmarks%3B%20and%20%283%29%20a%20Network%20Traverser%0Athat%20adaptively%20navigates%20the%20refined%20query%20space%20for%20real-time%20attacks.%20This%0Apipeline%20uncovers%20stealthy%2C%20high-success%20adversarial%20paths%20across%20LLMs.%20On%0Aseveral%20closed-source%20and%20open-source%20LLMs%2C%20NEXUS%20increases%20attack%20success%20rate%0Aby%202.1%25%20to%2019.4%25%20over%20prior%20methods.%20Code%3A%20https%3A//github.com/inspire-lab/NEXUS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNEXUS%253A%2520Network%2520Exploration%2520for%2520eXploiting%2520Unsafe%2520Sequences%2520in%2520Multi-Turn%250A%2520%2520LLM%2520Jailbreaks%26entry.906535625%3DJavad%2520Rafiei%2520Asl%2520and%2520Sidhant%2520Narula%2520and%2520Mohammad%2520Ghasemigol%2520and%2520Eduardo%2520Blanco%2520and%2520Daniel%2520Takabi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520natural%2520language%2520processing%250Abut%2520remain%2520vulnerable%2520to%2520jailbreak%2520attacks%252C%2520especially%2520multi-turn%2520jailbreaks%250Athat%2520distribute%2520malicious%2520intent%2520across%2520benign%2520exchanges%2520and%2520bypass%2520alignment%250Amechanisms.%2520Existing%2520approaches%2520often%2520explore%2520the%2520adversarial%2520space%2520poorly%252C%250Arely%2520on%2520hand-crafted%2520heuristics%252C%2520or%2520lack%2520systematic%2520query%2520refinement.%2520We%250Apresent%2520NEXUS%2520%2528Network%2520Exploration%2520for%2520eXploiting%2520Unsafe%2520Sequences%2529%252C%2520a%2520modular%250Aframework%2520for%2520constructing%252C%2520refining%252C%2520and%2520executing%2520optimized%2520multi-turn%250Aattacks.%2520NEXUS%2520comprises%253A%2520%25281%2529%2520ThoughtNet%252C%2520which%2520hierarchically%2520expands%2520a%250Aharmful%2520intent%2520into%2520a%2520structured%2520semantic%2520network%2520of%2520topics%252C%2520entities%252C%2520and%250Aquery%2520chains%253B%2520%25282%2529%2520a%2520feedback-driven%2520Simulator%2520that%2520iteratively%2520refines%2520and%250Aprunes%2520these%2520chains%2520through%2520attacker-victim-judge%2520LLM%2520collaboration%2520using%250Aharmfulness%2520and%2520semantic-similarity%2520benchmarks%253B%2520and%2520%25283%2529%2520a%2520Network%2520Traverser%250Athat%2520adaptively%2520navigates%2520the%2520refined%2520query%2520space%2520for%2520real-time%2520attacks.%2520This%250Apipeline%2520uncovers%2520stealthy%252C%2520high-success%2520adversarial%2520paths%2520across%2520LLMs.%2520On%250Aseveral%2520closed-source%2520and%2520open-source%2520LLMs%252C%2520NEXUS%2520increases%2520attack%2520success%2520rate%250Aby%25202.1%2525%2520to%252019.4%2525%2520over%2520prior%2520methods.%2520Code%253A%2520https%253A//github.com/inspire-lab/NEXUS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NEXUS%3A%20Network%20Exploration%20for%20eXploiting%20Unsafe%20Sequences%20in%20Multi-Turn%0A%20%20LLM%20Jailbreaks&entry.906535625=Javad%20Rafiei%20Asl%20and%20Sidhant%20Narula%20and%20Mohammad%20Ghasemigol%20and%20Eduardo%20Blanco%20and%20Daniel%20Takabi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20natural%20language%20processing%0Abut%20remain%20vulnerable%20to%20jailbreak%20attacks%2C%20especially%20multi-turn%20jailbreaks%0Athat%20distribute%20malicious%20intent%20across%20benign%20exchanges%20and%20bypass%20alignment%0Amechanisms.%20Existing%20approaches%20often%20explore%20the%20adversarial%20space%20poorly%2C%0Arely%20on%20hand-crafted%20heuristics%2C%20or%20lack%20systematic%20query%20refinement.%20We%0Apresent%20NEXUS%20%28Network%20Exploration%20for%20eXploiting%20Unsafe%20Sequences%29%2C%20a%20modular%0Aframework%20for%20constructing%2C%20refining%2C%20and%20executing%20optimized%20multi-turn%0Aattacks.%20NEXUS%20comprises%3A%20%281%29%20ThoughtNet%2C%20which%20hierarchically%20expands%20a%0Aharmful%20intent%20into%20a%20structured%20semantic%20network%20of%20topics%2C%20entities%2C%20and%0Aquery%20chains%3B%20%282%29%20a%20feedback-driven%20Simulator%20that%20iteratively%20refines%20and%0Aprunes%20these%20chains%20through%20attacker-victim-judge%20LLM%20collaboration%20using%0Aharmfulness%20and%20semantic-similarity%20benchmarks%3B%20and%20%283%29%20a%20Network%20Traverser%0Athat%20adaptively%20navigates%20the%20refined%20query%20space%20for%20real-time%20attacks.%20This%0Apipeline%20uncovers%20stealthy%2C%20high-success%20adversarial%20paths%20across%20LLMs.%20On%0Aseveral%20closed-source%20and%20open-source%20LLMs%2C%20NEXUS%20increases%20attack%20success%20rate%0Aby%202.1%25%20to%2019.4%25%20over%20prior%20methods.%20Code%3A%20https%3A//github.com/inspire-lab/NEXUS%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03417v2&entry.124074799=Read"},
{"title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons:\n  Benefits of Multiple Options", "author": "Joongkyu Lee and Seouh-won Yi and Min-hwan Oh", "abstract": "  We study online preference-based reinforcement learning (PbRL) with the goal\nof improving sample efficiency. While a growing body of theoretical work has\nemerged-motivated by PbRL's recent empirical success, particularly in aligning\nlarge language models (LLMs)-most existing studies focus only on pairwise\ncomparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,\nThekumparampil et al., 2024) have explored using multiple comparisons and\nranking feedback, but their performance guarantees fail to improve-and can even\ndeteriorate-as the feedback length increases, despite the richer information\navailable. To address this gap, we adopt the Plackett-Luce (PL) model for\nranking feedback over action subsets and propose M-AUPO, an algorithm that\nselects multiple actions by maximizing the average uncertainty within the\noffered subset. We prove that M-AUPO achieves a suboptimality gap of\n$\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}}\n\\right)$, where $T$ is the total number of rounds, $d$ is the feature\ndimension, and $|S_t|$ is the size of the subset at round $t$. This result\nshows that larger subsets directly lead to improved performance and, notably,\nthe bound avoids the exponential dependence on the unknown parameter's norm,\nwhich was a fundamental limitation in most previous works. Moreover, we\nestablish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}}\n\\right)$, where $K$ is the maximum subset size. To the best of our knowledge,\nthis is the first theoretical result in PbRL with ranking feedback that\nexplicitly shows improved sample efficiency as a function of the subset size.\n", "link": "http://arxiv.org/abs/2510.18713v1", "date": "2025-10-21", "relevancy": 1.8507, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4742}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4633}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preference-based%20Reinforcement%20Learning%20beyond%20Pairwise%20Comparisons%3A%0A%20%20Benefits%20of%20Multiple%20Options&body=Title%3A%20Preference-based%20Reinforcement%20Learning%20beyond%20Pairwise%20Comparisons%3A%0A%20%20Benefits%20of%20Multiple%20Options%0AAuthor%3A%20Joongkyu%20Lee%20and%20Seouh-won%20Yi%20and%20Min-hwan%20Oh%0AAbstract%3A%20%20%20We%20study%20online%20preference-based%20reinforcement%20learning%20%28PbRL%29%20with%20the%20goal%0Aof%20improving%20sample%20efficiency.%20While%20a%20growing%20body%20of%20theoretical%20work%20has%0Aemerged-motivated%20by%20PbRL%27s%20recent%20empirical%20success%2C%20particularly%20in%20aligning%0Alarge%20language%20models%20%28LLMs%29-most%20existing%20studies%20focus%20only%20on%20pairwise%0Acomparisons.%20A%20few%20recent%20works%20%28Zhu%20et%20al.%2C%202023%2C%20Mukherjee%20et%20al.%2C%202024%2C%0AThekumparampil%20et%20al.%2C%202024%29%20have%20explored%20using%20multiple%20comparisons%20and%0Aranking%20feedback%2C%20but%20their%20performance%20guarantees%20fail%20to%20improve-and%20can%20even%0Adeteriorate-as%20the%20feedback%20length%20increases%2C%20despite%20the%20richer%20information%0Aavailable.%20To%20address%20this%20gap%2C%20we%20adopt%20the%20Plackett-Luce%20%28PL%29%20model%20for%0Aranking%20feedback%20over%20action%20subsets%20and%20propose%20M-AUPO%2C%20an%20algorithm%20that%0Aselects%20multiple%20actions%20by%20maximizing%20the%20average%20uncertainty%20within%20the%0Aoffered%20subset.%20We%20prove%20that%20M-AUPO%20achieves%20a%20suboptimality%20gap%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%5Cleft%28%20%5Cfrac%7Bd%7D%7BT%7D%20%5Csqrt%7B%20%5Csum_%7Bt%3D1%7D%5ET%20%5Cfrac%7B1%7D%7B%7CS_t%7C%7D%7D%0A%5Cright%29%24%2C%20where%20%24T%24%20is%20the%20total%20number%20of%20rounds%2C%20%24d%24%20is%20the%20feature%0Adimension%2C%20and%20%24%7CS_t%7C%24%20is%20the%20size%20of%20the%20subset%20at%20round%20%24t%24.%20This%20result%0Ashows%20that%20larger%20subsets%20directly%20lead%20to%20improved%20performance%20and%2C%20notably%2C%0Athe%20bound%20avoids%20the%20exponential%20dependence%20on%20the%20unknown%20parameter%27s%20norm%2C%0Awhich%20was%20a%20fundamental%20limitation%20in%20most%20previous%20works.%20Moreover%2C%20we%0Aestablish%20a%20near-matching%20lower%20bound%20of%20%24%5COmega%20%5Cleft%28%20%5Cfrac%7Bd%7D%7BK%20%5Csqrt%7BT%7D%7D%0A%5Cright%29%24%2C%20where%20%24K%24%20is%20the%20maximum%20subset%20size.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20is%20the%20first%20theoretical%20result%20in%20PbRL%20with%20ranking%20feedback%20that%0Aexplicitly%20shows%20improved%20sample%20efficiency%20as%20a%20function%20of%20the%20subset%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreference-based%2520Reinforcement%2520Learning%2520beyond%2520Pairwise%2520Comparisons%253A%250A%2520%2520Benefits%2520of%2520Multiple%2520Options%26entry.906535625%3DJoongkyu%2520Lee%2520and%2520Seouh-won%2520Yi%2520and%2520Min-hwan%2520Oh%26entry.1292438233%3D%2520%2520We%2520study%2520online%2520preference-based%2520reinforcement%2520learning%2520%2528PbRL%2529%2520with%2520the%2520goal%250Aof%2520improving%2520sample%2520efficiency.%2520While%2520a%2520growing%2520body%2520of%2520theoretical%2520work%2520has%250Aemerged-motivated%2520by%2520PbRL%2527s%2520recent%2520empirical%2520success%252C%2520particularly%2520in%2520aligning%250Alarge%2520language%2520models%2520%2528LLMs%2529-most%2520existing%2520studies%2520focus%2520only%2520on%2520pairwise%250Acomparisons.%2520A%2520few%2520recent%2520works%2520%2528Zhu%2520et%2520al.%252C%25202023%252C%2520Mukherjee%2520et%2520al.%252C%25202024%252C%250AThekumparampil%2520et%2520al.%252C%25202024%2529%2520have%2520explored%2520using%2520multiple%2520comparisons%2520and%250Aranking%2520feedback%252C%2520but%2520their%2520performance%2520guarantees%2520fail%2520to%2520improve-and%2520can%2520even%250Adeteriorate-as%2520the%2520feedback%2520length%2520increases%252C%2520despite%2520the%2520richer%2520information%250Aavailable.%2520To%2520address%2520this%2520gap%252C%2520we%2520adopt%2520the%2520Plackett-Luce%2520%2528PL%2529%2520model%2520for%250Aranking%2520feedback%2520over%2520action%2520subsets%2520and%2520propose%2520M-AUPO%252C%2520an%2520algorithm%2520that%250Aselects%2520multiple%2520actions%2520by%2520maximizing%2520the%2520average%2520uncertainty%2520within%2520the%250Aoffered%2520subset.%2520We%2520prove%2520that%2520M-AUPO%2520achieves%2520a%2520suboptimality%2520gap%2520of%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%255Cleft%2528%2520%255Cfrac%257Bd%257D%257BT%257D%2520%255Csqrt%257B%2520%255Csum_%257Bt%253D1%257D%255ET%2520%255Cfrac%257B1%257D%257B%257CS_t%257C%257D%257D%250A%255Cright%2529%2524%252C%2520where%2520%2524T%2524%2520is%2520the%2520total%2520number%2520of%2520rounds%252C%2520%2524d%2524%2520is%2520the%2520feature%250Adimension%252C%2520and%2520%2524%257CS_t%257C%2524%2520is%2520the%2520size%2520of%2520the%2520subset%2520at%2520round%2520%2524t%2524.%2520This%2520result%250Ashows%2520that%2520larger%2520subsets%2520directly%2520lead%2520to%2520improved%2520performance%2520and%252C%2520notably%252C%250Athe%2520bound%2520avoids%2520the%2520exponential%2520dependence%2520on%2520the%2520unknown%2520parameter%2527s%2520norm%252C%250Awhich%2520was%2520a%2520fundamental%2520limitation%2520in%2520most%2520previous%2520works.%2520Moreover%252C%2520we%250Aestablish%2520a%2520near-matching%2520lower%2520bound%2520of%2520%2524%255COmega%2520%255Cleft%2528%2520%255Cfrac%257Bd%257D%257BK%2520%255Csqrt%257BT%257D%257D%250A%255Cright%2529%2524%252C%2520where%2520%2524K%2524%2520is%2520the%2520maximum%2520subset%2520size.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250Athis%2520is%2520the%2520first%2520theoretical%2520result%2520in%2520PbRL%2520with%2520ranking%2520feedback%2520that%250Aexplicitly%2520shows%2520improved%2520sample%2520efficiency%2520as%2520a%2520function%2520of%2520the%2520subset%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preference-based%20Reinforcement%20Learning%20beyond%20Pairwise%20Comparisons%3A%0A%20%20Benefits%20of%20Multiple%20Options&entry.906535625=Joongkyu%20Lee%20and%20Seouh-won%20Yi%20and%20Min-hwan%20Oh&entry.1292438233=%20%20We%20study%20online%20preference-based%20reinforcement%20learning%20%28PbRL%29%20with%20the%20goal%0Aof%20improving%20sample%20efficiency.%20While%20a%20growing%20body%20of%20theoretical%20work%20has%0Aemerged-motivated%20by%20PbRL%27s%20recent%20empirical%20success%2C%20particularly%20in%20aligning%0Alarge%20language%20models%20%28LLMs%29-most%20existing%20studies%20focus%20only%20on%20pairwise%0Acomparisons.%20A%20few%20recent%20works%20%28Zhu%20et%20al.%2C%202023%2C%20Mukherjee%20et%20al.%2C%202024%2C%0AThekumparampil%20et%20al.%2C%202024%29%20have%20explored%20using%20multiple%20comparisons%20and%0Aranking%20feedback%2C%20but%20their%20performance%20guarantees%20fail%20to%20improve-and%20can%20even%0Adeteriorate-as%20the%20feedback%20length%20increases%2C%20despite%20the%20richer%20information%0Aavailable.%20To%20address%20this%20gap%2C%20we%20adopt%20the%20Plackett-Luce%20%28PL%29%20model%20for%0Aranking%20feedback%20over%20action%20subsets%20and%20propose%20M-AUPO%2C%20an%20algorithm%20that%0Aselects%20multiple%20actions%20by%20maximizing%20the%20average%20uncertainty%20within%20the%0Aoffered%20subset.%20We%20prove%20that%20M-AUPO%20achieves%20a%20suboptimality%20gap%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%5Cleft%28%20%5Cfrac%7Bd%7D%7BT%7D%20%5Csqrt%7B%20%5Csum_%7Bt%3D1%7D%5ET%20%5Cfrac%7B1%7D%7B%7CS_t%7C%7D%7D%0A%5Cright%29%24%2C%20where%20%24T%24%20is%20the%20total%20number%20of%20rounds%2C%20%24d%24%20is%20the%20feature%0Adimension%2C%20and%20%24%7CS_t%7C%24%20is%20the%20size%20of%20the%20subset%20at%20round%20%24t%24.%20This%20result%0Ashows%20that%20larger%20subsets%20directly%20lead%20to%20improved%20performance%20and%2C%20notably%2C%0Athe%20bound%20avoids%20the%20exponential%20dependence%20on%20the%20unknown%20parameter%27s%20norm%2C%0Awhich%20was%20a%20fundamental%20limitation%20in%20most%20previous%20works.%20Moreover%2C%20we%0Aestablish%20a%20near-matching%20lower%20bound%20of%20%24%5COmega%20%5Cleft%28%20%5Cfrac%7Bd%7D%7BK%20%5Csqrt%7BT%7D%7D%0A%5Cright%29%24%2C%20where%20%24K%24%20is%20the%20maximum%20subset%20size.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20is%20the%20first%20theoretical%20result%20in%20PbRL%20with%20ranking%20feedback%20that%0Aexplicitly%20shows%20improved%20sample%20efficiency%20as%20a%20function%20of%20the%20subset%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18713v1&entry.124074799=Read"},
{"title": "Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning\n  without Rewards", "author": "Mengqi Li and Lei Zhao and Anthony Man-Cho So and Ruoyu Sun and Xiao Li", "abstract": "  We present a simple, self-help online supervised finetuning (OSFT) paradigm\nfor LLM reasoning. In this paradigm, the model generates its own responses and\nis immediately finetuned on this self-generated data. OSFT is a highly\nefficient training strategy for LLM reasoning, as it is reward-free and uses\njust one rollout by default. Experiment results show that OSFT achieves\ndownstream performance on challenging mathematical reasoning tasks comparable\nto strong reinforcement learning with verifiable rewards (RLVR) methods such as\nGRPO. Our ablation study further demonstrates the efficiency and robustness of\nOSFT. The major mechanism of OSFT lies in facilitating the model's own existing\npreference (latent knowledge) learned from pretraining, which leads to\nreasoning ability improvement. We believe that OSFT offers an efficient and\npromising alternative to more complex, reward-based training paradigms. Our\ncode is available at https://github.com/ElementQi/OnlineSFT.\n", "link": "http://arxiv.org/abs/2510.18814v1", "date": "2025-10-21", "relevancy": 1.8359, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4664}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4583}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20SFT%20for%20LLM%20Reasoning%3A%20Surprising%20Effectiveness%20of%20Self-Tuning%0A%20%20without%20Rewards&body=Title%3A%20Online%20SFT%20for%20LLM%20Reasoning%3A%20Surprising%20Effectiveness%20of%20Self-Tuning%0A%20%20without%20Rewards%0AAuthor%3A%20Mengqi%20Li%20and%20Lei%20Zhao%20and%20Anthony%20Man-Cho%20So%20and%20Ruoyu%20Sun%20and%20Xiao%20Li%0AAbstract%3A%20%20%20We%20present%20a%20simple%2C%20self-help%20online%20supervised%20finetuning%20%28OSFT%29%20paradigm%0Afor%20LLM%20reasoning.%20In%20this%20paradigm%2C%20the%20model%20generates%20its%20own%20responses%20and%0Ais%20immediately%20finetuned%20on%20this%20self-generated%20data.%20OSFT%20is%20a%20highly%0Aefficient%20training%20strategy%20for%20LLM%20reasoning%2C%20as%20it%20is%20reward-free%20and%20uses%0Ajust%20one%20rollout%20by%20default.%20Experiment%20results%20show%20that%20OSFT%20achieves%0Adownstream%20performance%20on%20challenging%20mathematical%20reasoning%20tasks%20comparable%0Ato%20strong%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20methods%20such%20as%0AGRPO.%20Our%20ablation%20study%20further%20demonstrates%20the%20efficiency%20and%20robustness%20of%0AOSFT.%20The%20major%20mechanism%20of%20OSFT%20lies%20in%20facilitating%20the%20model%27s%20own%20existing%0Apreference%20%28latent%20knowledge%29%20learned%20from%20pretraining%2C%20which%20leads%20to%0Areasoning%20ability%20improvement.%20We%20believe%20that%20OSFT%20offers%20an%20efficient%20and%0Apromising%20alternative%20to%20more%20complex%2C%20reward-based%20training%20paradigms.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/ElementQi/OnlineSFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520SFT%2520for%2520LLM%2520Reasoning%253A%2520Surprising%2520Effectiveness%2520of%2520Self-Tuning%250A%2520%2520without%2520Rewards%26entry.906535625%3DMengqi%2520Li%2520and%2520Lei%2520Zhao%2520and%2520Anthony%2520Man-Cho%2520So%2520and%2520Ruoyu%2520Sun%2520and%2520Xiao%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520simple%252C%2520self-help%2520online%2520supervised%2520finetuning%2520%2528OSFT%2529%2520paradigm%250Afor%2520LLM%2520reasoning.%2520In%2520this%2520paradigm%252C%2520the%2520model%2520generates%2520its%2520own%2520responses%2520and%250Ais%2520immediately%2520finetuned%2520on%2520this%2520self-generated%2520data.%2520OSFT%2520is%2520a%2520highly%250Aefficient%2520training%2520strategy%2520for%2520LLM%2520reasoning%252C%2520as%2520it%2520is%2520reward-free%2520and%2520uses%250Ajust%2520one%2520rollout%2520by%2520default.%2520Experiment%2520results%2520show%2520that%2520OSFT%2520achieves%250Adownstream%2520performance%2520on%2520challenging%2520mathematical%2520reasoning%2520tasks%2520comparable%250Ato%2520strong%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520methods%2520such%2520as%250AGRPO.%2520Our%2520ablation%2520study%2520further%2520demonstrates%2520the%2520efficiency%2520and%2520robustness%2520of%250AOSFT.%2520The%2520major%2520mechanism%2520of%2520OSFT%2520lies%2520in%2520facilitating%2520the%2520model%2527s%2520own%2520existing%250Apreference%2520%2528latent%2520knowledge%2529%2520learned%2520from%2520pretraining%252C%2520which%2520leads%2520to%250Areasoning%2520ability%2520improvement.%2520We%2520believe%2520that%2520OSFT%2520offers%2520an%2520efficient%2520and%250Apromising%2520alternative%2520to%2520more%2520complex%252C%2520reward-based%2520training%2520paradigms.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/ElementQi/OnlineSFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20SFT%20for%20LLM%20Reasoning%3A%20Surprising%20Effectiveness%20of%20Self-Tuning%0A%20%20without%20Rewards&entry.906535625=Mengqi%20Li%20and%20Lei%20Zhao%20and%20Anthony%20Man-Cho%20So%20and%20Ruoyu%20Sun%20and%20Xiao%20Li&entry.1292438233=%20%20We%20present%20a%20simple%2C%20self-help%20online%20supervised%20finetuning%20%28OSFT%29%20paradigm%0Afor%20LLM%20reasoning.%20In%20this%20paradigm%2C%20the%20model%20generates%20its%20own%20responses%20and%0Ais%20immediately%20finetuned%20on%20this%20self-generated%20data.%20OSFT%20is%20a%20highly%0Aefficient%20training%20strategy%20for%20LLM%20reasoning%2C%20as%20it%20is%20reward-free%20and%20uses%0Ajust%20one%20rollout%20by%20default.%20Experiment%20results%20show%20that%20OSFT%20achieves%0Adownstream%20performance%20on%20challenging%20mathematical%20reasoning%20tasks%20comparable%0Ato%20strong%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20methods%20such%20as%0AGRPO.%20Our%20ablation%20study%20further%20demonstrates%20the%20efficiency%20and%20robustness%20of%0AOSFT.%20The%20major%20mechanism%20of%20OSFT%20lies%20in%20facilitating%20the%20model%27s%20own%20existing%0Apreference%20%28latent%20knowledge%29%20learned%20from%20pretraining%2C%20which%20leads%20to%0Areasoning%20ability%20improvement.%20We%20believe%20that%20OSFT%20offers%20an%20efficient%20and%0Apromising%20alternative%20to%20more%20complex%2C%20reward-based%20training%20paradigms.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/ElementQi/OnlineSFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18814v1&entry.124074799=Read"},
{"title": "H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical\n  Imaging", "author": "Zhen Huang and Tao Tang and Ronghao Xu and Yangbo Wei and Wenkai Yang and Suhua Wang and Xiaoxin Sun and Han Li and Qingsong Yao", "abstract": "  3D landmark detection is a critical task in medical image analysis, and\naccurately detecting anatomical landmarks is essential for subsequent medical\nimaging tasks. However, mainstream deep learning methods in this field struggle\nto simultaneously capture fine-grained local features and model global spatial\nrelationships, while maintaining a balance between accuracy and computational\nefficiency. Local feature extraction requires capturing fine-grained anatomical\ndetails, while global modeling requires understanding the spatial relationships\nwithin complex anatomical structures. The high-dimensional nature of 3D volume\nfurther exacerbates these challenges, as landmarks are sparsely distributed,\nleading to significant computational costs. Therefore, achieving efficient and\nprecise 3D landmark detection remains a pressing challenge in medical image\nanalysis.\n  In this work, We propose a \\textbf{H}ybrid \\textbf{3}D \\textbf{DE}tection\n\\textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature\nextraction with a lightweight attention mechanism designed to efficiently\ncapture global dependencies in 3D volumetric data. This mechanism employs a\nhierarchical routing strategy to reduce computational cost while maintaining\nglobal context modeling. To our knowledge, H3DE-Net is the first 3D landmark\ndetection model that integrates such a lightweight attention mechanism with\nCNNs. Additionally, integrating multi-scale feature fusion further enhances\ndetection accuracy and robustness. Experimental results on a public CT dataset\ndemonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance,\nsignificantly improving accuracy and robustness, particularly in scenarios with\nmissing landmarks or complex anatomical variations. We aready open-source our\nproject, including code, data and model weights.\n", "link": "http://arxiv.org/abs/2502.14221v3", "date": "2025-10-21", "relevancy": 1.8178, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.581}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H3DE-Net%3A%20Efficient%20and%20Accurate%203D%20Landmark%20Detection%20in%20Medical%0A%20%20Imaging&body=Title%3A%20H3DE-Net%3A%20Efficient%20and%20Accurate%203D%20Landmark%20Detection%20in%20Medical%0A%20%20Imaging%0AAuthor%3A%20Zhen%20Huang%20and%20Tao%20Tang%20and%20Ronghao%20Xu%20and%20Yangbo%20Wei%20and%20Wenkai%20Yang%20and%20Suhua%20Wang%20and%20Xiaoxin%20Sun%20and%20Han%20Li%20and%20Qingsong%20Yao%0AAbstract%3A%20%20%203D%20landmark%20detection%20is%20a%20critical%20task%20in%20medical%20image%20analysis%2C%20and%0Aaccurately%20detecting%20anatomical%20landmarks%20is%20essential%20for%20subsequent%20medical%0Aimaging%20tasks.%20However%2C%20mainstream%20deep%20learning%20methods%20in%20this%20field%20struggle%0Ato%20simultaneously%20capture%20fine-grained%20local%20features%20and%20model%20global%20spatial%0Arelationships%2C%20while%20maintaining%20a%20balance%20between%20accuracy%20and%20computational%0Aefficiency.%20Local%20feature%20extraction%20requires%20capturing%20fine-grained%20anatomical%0Adetails%2C%20while%20global%20modeling%20requires%20understanding%20the%20spatial%20relationships%0Awithin%20complex%20anatomical%20structures.%20The%20high-dimensional%20nature%20of%203D%20volume%0Afurther%20exacerbates%20these%20challenges%2C%20as%20landmarks%20are%20sparsely%20distributed%2C%0Aleading%20to%20significant%20computational%20costs.%20Therefore%2C%20achieving%20efficient%20and%0Aprecise%203D%20landmark%20detection%20remains%20a%20pressing%20challenge%20in%20medical%20image%0Aanalysis.%0A%20%20In%20this%20work%2C%20We%20propose%20a%20%5Ctextbf%7BH%7Dybrid%20%5Ctextbf%7B3%7DD%20%5Ctextbf%7BDE%7Dtection%0A%5Ctextbf%7BNet%7D%28H3DE-Net%29%2C%20a%20novel%20framework%20that%20combines%20CNNs%20for%20local%20feature%0Aextraction%20with%20a%20lightweight%20attention%20mechanism%20designed%20to%20efficiently%0Acapture%20global%20dependencies%20in%203D%20volumetric%20data.%20This%20mechanism%20employs%20a%0Ahierarchical%20routing%20strategy%20to%20reduce%20computational%20cost%20while%20maintaining%0Aglobal%20context%20modeling.%20To%20our%20knowledge%2C%20H3DE-Net%20is%20the%20first%203D%20landmark%0Adetection%20model%20that%20integrates%20such%20a%20lightweight%20attention%20mechanism%20with%0ACNNs.%20Additionally%2C%20integrating%20multi-scale%20feature%20fusion%20further%20enhances%0Adetection%20accuracy%20and%20robustness.%20Experimental%20results%20on%20a%20public%20CT%20dataset%0Ademonstrate%20that%20H3DE-Net%20achieves%20state-of-the-art%28SOTA%29%20performance%2C%0Asignificantly%20improving%20accuracy%20and%20robustness%2C%20particularly%20in%20scenarios%20with%0Amissing%20landmarks%20or%20complex%20anatomical%20variations.%20We%20aready%20open-source%20our%0Aproject%2C%20including%20code%2C%20data%20and%20model%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14221v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH3DE-Net%253A%2520Efficient%2520and%2520Accurate%25203D%2520Landmark%2520Detection%2520in%2520Medical%250A%2520%2520Imaging%26entry.906535625%3DZhen%2520Huang%2520and%2520Tao%2520Tang%2520and%2520Ronghao%2520Xu%2520and%2520Yangbo%2520Wei%2520and%2520Wenkai%2520Yang%2520and%2520Suhua%2520Wang%2520and%2520Xiaoxin%2520Sun%2520and%2520Han%2520Li%2520and%2520Qingsong%2520Yao%26entry.1292438233%3D%2520%25203D%2520landmark%2520detection%2520is%2520a%2520critical%2520task%2520in%2520medical%2520image%2520analysis%252C%2520and%250Aaccurately%2520detecting%2520anatomical%2520landmarks%2520is%2520essential%2520for%2520subsequent%2520medical%250Aimaging%2520tasks.%2520However%252C%2520mainstream%2520deep%2520learning%2520methods%2520in%2520this%2520field%2520struggle%250Ato%2520simultaneously%2520capture%2520fine-grained%2520local%2520features%2520and%2520model%2520global%2520spatial%250Arelationships%252C%2520while%2520maintaining%2520a%2520balance%2520between%2520accuracy%2520and%2520computational%250Aefficiency.%2520Local%2520feature%2520extraction%2520requires%2520capturing%2520fine-grained%2520anatomical%250Adetails%252C%2520while%2520global%2520modeling%2520requires%2520understanding%2520the%2520spatial%2520relationships%250Awithin%2520complex%2520anatomical%2520structures.%2520The%2520high-dimensional%2520nature%2520of%25203D%2520volume%250Afurther%2520exacerbates%2520these%2520challenges%252C%2520as%2520landmarks%2520are%2520sparsely%2520distributed%252C%250Aleading%2520to%2520significant%2520computational%2520costs.%2520Therefore%252C%2520achieving%2520efficient%2520and%250Aprecise%25203D%2520landmark%2520detection%2520remains%2520a%2520pressing%2520challenge%2520in%2520medical%2520image%250Aanalysis.%250A%2520%2520In%2520this%2520work%252C%2520We%2520propose%2520a%2520%255Ctextbf%257BH%257Dybrid%2520%255Ctextbf%257B3%257DD%2520%255Ctextbf%257BDE%257Dtection%250A%255Ctextbf%257BNet%257D%2528H3DE-Net%2529%252C%2520a%2520novel%2520framework%2520that%2520combines%2520CNNs%2520for%2520local%2520feature%250Aextraction%2520with%2520a%2520lightweight%2520attention%2520mechanism%2520designed%2520to%2520efficiently%250Acapture%2520global%2520dependencies%2520in%25203D%2520volumetric%2520data.%2520This%2520mechanism%2520employs%2520a%250Ahierarchical%2520routing%2520strategy%2520to%2520reduce%2520computational%2520cost%2520while%2520maintaining%250Aglobal%2520context%2520modeling.%2520To%2520our%2520knowledge%252C%2520H3DE-Net%2520is%2520the%2520first%25203D%2520landmark%250Adetection%2520model%2520that%2520integrates%2520such%2520a%2520lightweight%2520attention%2520mechanism%2520with%250ACNNs.%2520Additionally%252C%2520integrating%2520multi-scale%2520feature%2520fusion%2520further%2520enhances%250Adetection%2520accuracy%2520and%2520robustness.%2520Experimental%2520results%2520on%2520a%2520public%2520CT%2520dataset%250Ademonstrate%2520that%2520H3DE-Net%2520achieves%2520state-of-the-art%2528SOTA%2529%2520performance%252C%250Asignificantly%2520improving%2520accuracy%2520and%2520robustness%252C%2520particularly%2520in%2520scenarios%2520with%250Amissing%2520landmarks%2520or%2520complex%2520anatomical%2520variations.%2520We%2520aready%2520open-source%2520our%250Aproject%252C%2520including%2520code%252C%2520data%2520and%2520model%2520weights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14221v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H3DE-Net%3A%20Efficient%20and%20Accurate%203D%20Landmark%20Detection%20in%20Medical%0A%20%20Imaging&entry.906535625=Zhen%20Huang%20and%20Tao%20Tang%20and%20Ronghao%20Xu%20and%20Yangbo%20Wei%20and%20Wenkai%20Yang%20and%20Suhua%20Wang%20and%20Xiaoxin%20Sun%20and%20Han%20Li%20and%20Qingsong%20Yao&entry.1292438233=%20%203D%20landmark%20detection%20is%20a%20critical%20task%20in%20medical%20image%20analysis%2C%20and%0Aaccurately%20detecting%20anatomical%20landmarks%20is%20essential%20for%20subsequent%20medical%0Aimaging%20tasks.%20However%2C%20mainstream%20deep%20learning%20methods%20in%20this%20field%20struggle%0Ato%20simultaneously%20capture%20fine-grained%20local%20features%20and%20model%20global%20spatial%0Arelationships%2C%20while%20maintaining%20a%20balance%20between%20accuracy%20and%20computational%0Aefficiency.%20Local%20feature%20extraction%20requires%20capturing%20fine-grained%20anatomical%0Adetails%2C%20while%20global%20modeling%20requires%20understanding%20the%20spatial%20relationships%0Awithin%20complex%20anatomical%20structures.%20The%20high-dimensional%20nature%20of%203D%20volume%0Afurther%20exacerbates%20these%20challenges%2C%20as%20landmarks%20are%20sparsely%20distributed%2C%0Aleading%20to%20significant%20computational%20costs.%20Therefore%2C%20achieving%20efficient%20and%0Aprecise%203D%20landmark%20detection%20remains%20a%20pressing%20challenge%20in%20medical%20image%0Aanalysis.%0A%20%20In%20this%20work%2C%20We%20propose%20a%20%5Ctextbf%7BH%7Dybrid%20%5Ctextbf%7B3%7DD%20%5Ctextbf%7BDE%7Dtection%0A%5Ctextbf%7BNet%7D%28H3DE-Net%29%2C%20a%20novel%20framework%20that%20combines%20CNNs%20for%20local%20feature%0Aextraction%20with%20a%20lightweight%20attention%20mechanism%20designed%20to%20efficiently%0Acapture%20global%20dependencies%20in%203D%20volumetric%20data.%20This%20mechanism%20employs%20a%0Ahierarchical%20routing%20strategy%20to%20reduce%20computational%20cost%20while%20maintaining%0Aglobal%20context%20modeling.%20To%20our%20knowledge%2C%20H3DE-Net%20is%20the%20first%203D%20landmark%0Adetection%20model%20that%20integrates%20such%20a%20lightweight%20attention%20mechanism%20with%0ACNNs.%20Additionally%2C%20integrating%20multi-scale%20feature%20fusion%20further%20enhances%0Adetection%20accuracy%20and%20robustness.%20Experimental%20results%20on%20a%20public%20CT%20dataset%0Ademonstrate%20that%20H3DE-Net%20achieves%20state-of-the-art%28SOTA%29%20performance%2C%0Asignificantly%20improving%20accuracy%20and%20robustness%2C%20particularly%20in%20scenarios%20with%0Amissing%20landmarks%20or%20complex%20anatomical%20variations.%20We%20aready%20open-source%20our%0Aproject%2C%20including%20code%2C%20data%20and%20model%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14221v3&entry.124074799=Read"},
{"title": "Enhancing Fractional Gradient Descent with Learned Optimizers", "author": "Jan Sobotka and Petr \u0160im\u00e1nek and Pavel Kord\u00edk", "abstract": "  Fractional Gradient Descent (FGD) offers a novel and promising way to\naccelerate optimization by incorporating fractional calculus into machine\nlearning. Although FGD has shown encouraging initial results across various\noptimization tasks, it faces significant challenges with convergence behavior\nand hyperparameter selection. Moreover, the impact of its hyperparameters is\nnot fully understood, and scheduling them is particularly difficult in\nnon-convex settings such as neural network training. To address these issues,\nwe propose a novel approach called Learning to Optimize Caputo Fractional\nGradient Descent (L2O-CFGD), which meta-learns how to dynamically tune the\nhyperparameters of Caputo FGD (CFGD). Our method's meta-learned schedule\noutperforms CFGD with static hyperparameters found through an extensive search\nand, in some tasks, achieves performance comparable to a fully black-box\nmeta-learned optimizer. L2O-CFGD can thus serve as a powerful tool for\nresearchers to identify high-performing hyperparameters and gain insights on\nhow to leverage the history-dependence of the fractional differential in\noptimization.\n", "link": "http://arxiv.org/abs/2510.18783v1", "date": "2025-10-21", "relevancy": 1.8051, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4543}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4508}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Fractional%20Gradient%20Descent%20with%20Learned%20Optimizers&body=Title%3A%20Enhancing%20Fractional%20Gradient%20Descent%20with%20Learned%20Optimizers%0AAuthor%3A%20Jan%20Sobotka%20and%20Petr%20%C5%A0im%C3%A1nek%20and%20Pavel%20Kord%C3%ADk%0AAbstract%3A%20%20%20Fractional%20Gradient%20Descent%20%28FGD%29%20offers%20a%20novel%20and%20promising%20way%20to%0Aaccelerate%20optimization%20by%20incorporating%20fractional%20calculus%20into%20machine%0Alearning.%20Although%20FGD%20has%20shown%20encouraging%20initial%20results%20across%20various%0Aoptimization%20tasks%2C%20it%20faces%20significant%20challenges%20with%20convergence%20behavior%0Aand%20hyperparameter%20selection.%20Moreover%2C%20the%20impact%20of%20its%20hyperparameters%20is%0Anot%20fully%20understood%2C%20and%20scheduling%20them%20is%20particularly%20difficult%20in%0Anon-convex%20settings%20such%20as%20neural%20network%20training.%20To%20address%20these%20issues%2C%0Awe%20propose%20a%20novel%20approach%20called%20Learning%20to%20Optimize%20Caputo%20Fractional%0AGradient%20Descent%20%28L2O-CFGD%29%2C%20which%20meta-learns%20how%20to%20dynamically%20tune%20the%0Ahyperparameters%20of%20Caputo%20FGD%20%28CFGD%29.%20Our%20method%27s%20meta-learned%20schedule%0Aoutperforms%20CFGD%20with%20static%20hyperparameters%20found%20through%20an%20extensive%20search%0Aand%2C%20in%20some%20tasks%2C%20achieves%20performance%20comparable%20to%20a%20fully%20black-box%0Ameta-learned%20optimizer.%20L2O-CFGD%20can%20thus%20serve%20as%20a%20powerful%20tool%20for%0Aresearchers%20to%20identify%20high-performing%20hyperparameters%20and%20gain%20insights%20on%0Ahow%20to%20leverage%20the%20history-dependence%20of%20the%20fractional%20differential%20in%0Aoptimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Fractional%2520Gradient%2520Descent%2520with%2520Learned%2520Optimizers%26entry.906535625%3DJan%2520Sobotka%2520and%2520Petr%2520%25C5%25A0im%25C3%25A1nek%2520and%2520Pavel%2520Kord%25C3%25ADk%26entry.1292438233%3D%2520%2520Fractional%2520Gradient%2520Descent%2520%2528FGD%2529%2520offers%2520a%2520novel%2520and%2520promising%2520way%2520to%250Aaccelerate%2520optimization%2520by%2520incorporating%2520fractional%2520calculus%2520into%2520machine%250Alearning.%2520Although%2520FGD%2520has%2520shown%2520encouraging%2520initial%2520results%2520across%2520various%250Aoptimization%2520tasks%252C%2520it%2520faces%2520significant%2520challenges%2520with%2520convergence%2520behavior%250Aand%2520hyperparameter%2520selection.%2520Moreover%252C%2520the%2520impact%2520of%2520its%2520hyperparameters%2520is%250Anot%2520fully%2520understood%252C%2520and%2520scheduling%2520them%2520is%2520particularly%2520difficult%2520in%250Anon-convex%2520settings%2520such%2520as%2520neural%2520network%2520training.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520a%2520novel%2520approach%2520called%2520Learning%2520to%2520Optimize%2520Caputo%2520Fractional%250AGradient%2520Descent%2520%2528L2O-CFGD%2529%252C%2520which%2520meta-learns%2520how%2520to%2520dynamically%2520tune%2520the%250Ahyperparameters%2520of%2520Caputo%2520FGD%2520%2528CFGD%2529.%2520Our%2520method%2527s%2520meta-learned%2520schedule%250Aoutperforms%2520CFGD%2520with%2520static%2520hyperparameters%2520found%2520through%2520an%2520extensive%2520search%250Aand%252C%2520in%2520some%2520tasks%252C%2520achieves%2520performance%2520comparable%2520to%2520a%2520fully%2520black-box%250Ameta-learned%2520optimizer.%2520L2O-CFGD%2520can%2520thus%2520serve%2520as%2520a%2520powerful%2520tool%2520for%250Aresearchers%2520to%2520identify%2520high-performing%2520hyperparameters%2520and%2520gain%2520insights%2520on%250Ahow%2520to%2520leverage%2520the%2520history-dependence%2520of%2520the%2520fractional%2520differential%2520in%250Aoptimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Fractional%20Gradient%20Descent%20with%20Learned%20Optimizers&entry.906535625=Jan%20Sobotka%20and%20Petr%20%C5%A0im%C3%A1nek%20and%20Pavel%20Kord%C3%ADk&entry.1292438233=%20%20Fractional%20Gradient%20Descent%20%28FGD%29%20offers%20a%20novel%20and%20promising%20way%20to%0Aaccelerate%20optimization%20by%20incorporating%20fractional%20calculus%20into%20machine%0Alearning.%20Although%20FGD%20has%20shown%20encouraging%20initial%20results%20across%20various%0Aoptimization%20tasks%2C%20it%20faces%20significant%20challenges%20with%20convergence%20behavior%0Aand%20hyperparameter%20selection.%20Moreover%2C%20the%20impact%20of%20its%20hyperparameters%20is%0Anot%20fully%20understood%2C%20and%20scheduling%20them%20is%20particularly%20difficult%20in%0Anon-convex%20settings%20such%20as%20neural%20network%20training.%20To%20address%20these%20issues%2C%0Awe%20propose%20a%20novel%20approach%20called%20Learning%20to%20Optimize%20Caputo%20Fractional%0AGradient%20Descent%20%28L2O-CFGD%29%2C%20which%20meta-learns%20how%20to%20dynamically%20tune%20the%0Ahyperparameters%20of%20Caputo%20FGD%20%28CFGD%29.%20Our%20method%27s%20meta-learned%20schedule%0Aoutperforms%20CFGD%20with%20static%20hyperparameters%20found%20through%20an%20extensive%20search%0Aand%2C%20in%20some%20tasks%2C%20achieves%20performance%20comparable%20to%20a%20fully%20black-box%0Ameta-learned%20optimizer.%20L2O-CFGD%20can%20thus%20serve%20as%20a%20powerful%20tool%20for%0Aresearchers%20to%20identify%20high-performing%20hyperparameters%20and%20gain%20insights%20on%0Ahow%20to%20leverage%20the%20history-dependence%20of%20the%20fractional%20differential%20in%0Aoptimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18783v1&entry.124074799=Read"},
{"title": "Online Object-Level Semantic Mapping for Quadrupeds in Real-World\n  Environments", "author": "Emad Razavi and Angelo Bratta and Jo\u00e3o Carlos Virgolino Soares and Carmine Recchiuto and Claudio Semini", "abstract": "  We present an online semantic object mapping system for a quadruped robot\noperating in real indoor environments, turning sensor detections into named\nobjects in a global map. During a run, the mapper integrates range geometry\nwith camera detections, merges co-located detections within a frame, and\nassociates repeated detections into persistent object instances across frames.\nObjects remain in the map when they are out of view, and repeated sightings\nupdate the same instance rather than creating duplicates. The output is a\ncompact object layer that can be queried (class, pose, and confidence), is\nintegrated with the occupancy map and readable by a planner. In on-robot tests,\nthe layer remained stable across viewpoint changes.\n", "link": "http://arxiv.org/abs/2510.18776v1", "date": "2025-10-21", "relevancy": 1.7887, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6436}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5956}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Object-Level%20Semantic%20Mapping%20for%20Quadrupeds%20in%20Real-World%0A%20%20Environments&body=Title%3A%20Online%20Object-Level%20Semantic%20Mapping%20for%20Quadrupeds%20in%20Real-World%0A%20%20Environments%0AAuthor%3A%20Emad%20Razavi%20and%20Angelo%20Bratta%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Carmine%20Recchiuto%20and%20Claudio%20Semini%0AAbstract%3A%20%20%20We%20present%20an%20online%20semantic%20object%20mapping%20system%20for%20a%20quadruped%20robot%0Aoperating%20in%20real%20indoor%20environments%2C%20turning%20sensor%20detections%20into%20named%0Aobjects%20in%20a%20global%20map.%20During%20a%20run%2C%20the%20mapper%20integrates%20range%20geometry%0Awith%20camera%20detections%2C%20merges%20co-located%20detections%20within%20a%20frame%2C%20and%0Aassociates%20repeated%20detections%20into%20persistent%20object%20instances%20across%20frames.%0AObjects%20remain%20in%20the%20map%20when%20they%20are%20out%20of%20view%2C%20and%20repeated%20sightings%0Aupdate%20the%20same%20instance%20rather%20than%20creating%20duplicates.%20The%20output%20is%20a%0Acompact%20object%20layer%20that%20can%20be%20queried%20%28class%2C%20pose%2C%20and%20confidence%29%2C%20is%0Aintegrated%20with%20the%20occupancy%20map%20and%20readable%20by%20a%20planner.%20In%20on-robot%20tests%2C%0Athe%20layer%20remained%20stable%20across%20viewpoint%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Object-Level%2520Semantic%2520Mapping%2520for%2520Quadrupeds%2520in%2520Real-World%250A%2520%2520Environments%26entry.906535625%3DEmad%2520Razavi%2520and%2520Angelo%2520Bratta%2520and%2520Jo%25C3%25A3o%2520Carlos%2520Virgolino%2520Soares%2520and%2520Carmine%2520Recchiuto%2520and%2520Claudio%2520Semini%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520online%2520semantic%2520object%2520mapping%2520system%2520for%2520a%2520quadruped%2520robot%250Aoperating%2520in%2520real%2520indoor%2520environments%252C%2520turning%2520sensor%2520detections%2520into%2520named%250Aobjects%2520in%2520a%2520global%2520map.%2520During%2520a%2520run%252C%2520the%2520mapper%2520integrates%2520range%2520geometry%250Awith%2520camera%2520detections%252C%2520merges%2520co-located%2520detections%2520within%2520a%2520frame%252C%2520and%250Aassociates%2520repeated%2520detections%2520into%2520persistent%2520object%2520instances%2520across%2520frames.%250AObjects%2520remain%2520in%2520the%2520map%2520when%2520they%2520are%2520out%2520of%2520view%252C%2520and%2520repeated%2520sightings%250Aupdate%2520the%2520same%2520instance%2520rather%2520than%2520creating%2520duplicates.%2520The%2520output%2520is%2520a%250Acompact%2520object%2520layer%2520that%2520can%2520be%2520queried%2520%2528class%252C%2520pose%252C%2520and%2520confidence%2529%252C%2520is%250Aintegrated%2520with%2520the%2520occupancy%2520map%2520and%2520readable%2520by%2520a%2520planner.%2520In%2520on-robot%2520tests%252C%250Athe%2520layer%2520remained%2520stable%2520across%2520viewpoint%2520changes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Object-Level%20Semantic%20Mapping%20for%20Quadrupeds%20in%20Real-World%0A%20%20Environments&entry.906535625=Emad%20Razavi%20and%20Angelo%20Bratta%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Carmine%20Recchiuto%20and%20Claudio%20Semini&entry.1292438233=%20%20We%20present%20an%20online%20semantic%20object%20mapping%20system%20for%20a%20quadruped%20robot%0Aoperating%20in%20real%20indoor%20environments%2C%20turning%20sensor%20detections%20into%20named%0Aobjects%20in%20a%20global%20map.%20During%20a%20run%2C%20the%20mapper%20integrates%20range%20geometry%0Awith%20camera%20detections%2C%20merges%20co-located%20detections%20within%20a%20frame%2C%20and%0Aassociates%20repeated%20detections%20into%20persistent%20object%20instances%20across%20frames.%0AObjects%20remain%20in%20the%20map%20when%20they%20are%20out%20of%20view%2C%20and%20repeated%20sightings%0Aupdate%20the%20same%20instance%20rather%20than%20creating%20duplicates.%20The%20output%20is%20a%0Acompact%20object%20layer%20that%20can%20be%20queried%20%28class%2C%20pose%2C%20and%20confidence%29%2C%20is%0Aintegrated%20with%20the%20occupancy%20map%20and%20readable%20by%20a%20planner.%20In%20on-robot%20tests%2C%0Athe%20layer%20remained%20stable%20across%20viewpoint%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18776v1&entry.124074799=Read"},
{"title": "DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World\n  Image Super-Resolution", "author": "Rongyuan Wu and Lingchen Sun and Zhengqiang Zhang and Shihao Wang and Tianhe Wu and Qiaosi Yi and Shuai Li and Lei Zhang", "abstract": "  Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world\nimage super-resolution (Real-ISR) methods can synthesize rich and realistic\ndetails. However, due to the inherent stochasticity of T2I models, different\nnoise inputs often lead to outputs with varying perceptual quality. Although\nthis randomness is sometimes seen as a limitation, it also introduces a wider\nperceptual quality range, which can be exploited to improve Real-ISR\nperformance. To this end, we introduce Direct Perceptual Preference\nOptimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative\nmodels with perceptual preferences without requiring costly human annotations.\nWe construct a hybrid reward signal by combining full-reference and\nno-reference image quality assessment (IQA) models trained on large-scale human\npreference datasets. This reward encourages both structural fidelity and\nnatural appearance. To better utilize perceptual diversity, we move beyond the\nstandard best-vs-worst selection and construct multiple preference pairs from\noutputs of the same model. Our analysis reveals that the optimal selection\nratio depends on model capacity: smaller models benefit from broader coverage,\nwhile larger models respond better to stronger contrast in supervision.\nFurthermore, we propose hierarchical preference optimization, which adaptively\nweights training pairs based on intra-group reward gaps and inter-group\ndiversity, enabling more efficient and stable learning. Extensive experiments\nacross both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR\nsignificantly improves perceptual quality and generalizes well to real-world\nbenchmarks.\n", "link": "http://arxiv.org/abs/2510.18851v1", "date": "2025-10-21", "relevancy": 1.7692, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6152}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6096}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP%24%5E2%24O-SR%3A%20Direct%20Perceptual%20Preference%20Optimization%20for%20Real-World%0A%20%20Image%20Super-Resolution&body=Title%3A%20DP%24%5E2%24O-SR%3A%20Direct%20Perceptual%20Preference%20Optimization%20for%20Real-World%0A%20%20Image%20Super-Resolution%0AAuthor%3A%20Rongyuan%20Wu%20and%20Lingchen%20Sun%20and%20Zhengqiang%20Zhang%20and%20Shihao%20Wang%20and%20Tianhe%20Wu%20and%20Qiaosi%20Yi%20and%20Shuai%20Li%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Benefiting%20from%20pre-trained%20text-to-image%20%28T2I%29%20diffusion%20models%2C%20real-world%0Aimage%20super-resolution%20%28Real-ISR%29%20methods%20can%20synthesize%20rich%20and%20realistic%0Adetails.%20However%2C%20due%20to%20the%20inherent%20stochasticity%20of%20T2I%20models%2C%20different%0Anoise%20inputs%20often%20lead%20to%20outputs%20with%20varying%20perceptual%20quality.%20Although%0Athis%20randomness%20is%20sometimes%20seen%20as%20a%20limitation%2C%20it%20also%20introduces%20a%20wider%0Aperceptual%20quality%20range%2C%20which%20can%20be%20exploited%20to%20improve%20Real-ISR%0Aperformance.%20To%20this%20end%2C%20we%20introduce%20Direct%20Perceptual%20Preference%0AOptimization%20for%20Real-ISR%20%28DP%24%5E2%24O-SR%29%2C%20a%20framework%20that%20aligns%20generative%0Amodels%20with%20perceptual%20preferences%20without%20requiring%20costly%20human%20annotations.%0AWe%20construct%20a%20hybrid%20reward%20signal%20by%20combining%20full-reference%20and%0Ano-reference%20image%20quality%20assessment%20%28IQA%29%20models%20trained%20on%20large-scale%20human%0Apreference%20datasets.%20This%20reward%20encourages%20both%20structural%20fidelity%20and%0Anatural%20appearance.%20To%20better%20utilize%20perceptual%20diversity%2C%20we%20move%20beyond%20the%0Astandard%20best-vs-worst%20selection%20and%20construct%20multiple%20preference%20pairs%20from%0Aoutputs%20of%20the%20same%20model.%20Our%20analysis%20reveals%20that%20the%20optimal%20selection%0Aratio%20depends%20on%20model%20capacity%3A%20smaller%20models%20benefit%20from%20broader%20coverage%2C%0Awhile%20larger%20models%20respond%20better%20to%20stronger%20contrast%20in%20supervision.%0AFurthermore%2C%20we%20propose%20hierarchical%20preference%20optimization%2C%20which%20adaptively%0Aweights%20training%20pairs%20based%20on%20intra-group%20reward%20gaps%20and%20inter-group%0Adiversity%2C%20enabling%20more%20efficient%20and%20stable%20learning.%20Extensive%20experiments%0Aacross%20both%20diffusion-%20and%20flow-based%20T2I%20backbones%20demonstrate%20that%20DP%24%5E2%24O-SR%0Asignificantly%20improves%20perceptual%20quality%20and%20generalizes%20well%20to%20real-world%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP%2524%255E2%2524O-SR%253A%2520Direct%2520Perceptual%2520Preference%2520Optimization%2520for%2520Real-World%250A%2520%2520Image%2520Super-Resolution%26entry.906535625%3DRongyuan%2520Wu%2520and%2520Lingchen%2520Sun%2520and%2520Zhengqiang%2520Zhang%2520and%2520Shihao%2520Wang%2520and%2520Tianhe%2520Wu%2520and%2520Qiaosi%2520Yi%2520and%2520Shuai%2520Li%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Benefiting%2520from%2520pre-trained%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%252C%2520real-world%250Aimage%2520super-resolution%2520%2528Real-ISR%2529%2520methods%2520can%2520synthesize%2520rich%2520and%2520realistic%250Adetails.%2520However%252C%2520due%2520to%2520the%2520inherent%2520stochasticity%2520of%2520T2I%2520models%252C%2520different%250Anoise%2520inputs%2520often%2520lead%2520to%2520outputs%2520with%2520varying%2520perceptual%2520quality.%2520Although%250Athis%2520randomness%2520is%2520sometimes%2520seen%2520as%2520a%2520limitation%252C%2520it%2520also%2520introduces%2520a%2520wider%250Aperceptual%2520quality%2520range%252C%2520which%2520can%2520be%2520exploited%2520to%2520improve%2520Real-ISR%250Aperformance.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Direct%2520Perceptual%2520Preference%250AOptimization%2520for%2520Real-ISR%2520%2528DP%2524%255E2%2524O-SR%2529%252C%2520a%2520framework%2520that%2520aligns%2520generative%250Amodels%2520with%2520perceptual%2520preferences%2520without%2520requiring%2520costly%2520human%2520annotations.%250AWe%2520construct%2520a%2520hybrid%2520reward%2520signal%2520by%2520combining%2520full-reference%2520and%250Ano-reference%2520image%2520quality%2520assessment%2520%2528IQA%2529%2520models%2520trained%2520on%2520large-scale%2520human%250Apreference%2520datasets.%2520This%2520reward%2520encourages%2520both%2520structural%2520fidelity%2520and%250Anatural%2520appearance.%2520To%2520better%2520utilize%2520perceptual%2520diversity%252C%2520we%2520move%2520beyond%2520the%250Astandard%2520best-vs-worst%2520selection%2520and%2520construct%2520multiple%2520preference%2520pairs%2520from%250Aoutputs%2520of%2520the%2520same%2520model.%2520Our%2520analysis%2520reveals%2520that%2520the%2520optimal%2520selection%250Aratio%2520depends%2520on%2520model%2520capacity%253A%2520smaller%2520models%2520benefit%2520from%2520broader%2520coverage%252C%250Awhile%2520larger%2520models%2520respond%2520better%2520to%2520stronger%2520contrast%2520in%2520supervision.%250AFurthermore%252C%2520we%2520propose%2520hierarchical%2520preference%2520optimization%252C%2520which%2520adaptively%250Aweights%2520training%2520pairs%2520based%2520on%2520intra-group%2520reward%2520gaps%2520and%2520inter-group%250Adiversity%252C%2520enabling%2520more%2520efficient%2520and%2520stable%2520learning.%2520Extensive%2520experiments%250Aacross%2520both%2520diffusion-%2520and%2520flow-based%2520T2I%2520backbones%2520demonstrate%2520that%2520DP%2524%255E2%2524O-SR%250Asignificantly%2520improves%2520perceptual%2520quality%2520and%2520generalizes%2520well%2520to%2520real-world%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP%24%5E2%24O-SR%3A%20Direct%20Perceptual%20Preference%20Optimization%20for%20Real-World%0A%20%20Image%20Super-Resolution&entry.906535625=Rongyuan%20Wu%20and%20Lingchen%20Sun%20and%20Zhengqiang%20Zhang%20and%20Shihao%20Wang%20and%20Tianhe%20Wu%20and%20Qiaosi%20Yi%20and%20Shuai%20Li%20and%20Lei%20Zhang&entry.1292438233=%20%20Benefiting%20from%20pre-trained%20text-to-image%20%28T2I%29%20diffusion%20models%2C%20real-world%0Aimage%20super-resolution%20%28Real-ISR%29%20methods%20can%20synthesize%20rich%20and%20realistic%0Adetails.%20However%2C%20due%20to%20the%20inherent%20stochasticity%20of%20T2I%20models%2C%20different%0Anoise%20inputs%20often%20lead%20to%20outputs%20with%20varying%20perceptual%20quality.%20Although%0Athis%20randomness%20is%20sometimes%20seen%20as%20a%20limitation%2C%20it%20also%20introduces%20a%20wider%0Aperceptual%20quality%20range%2C%20which%20can%20be%20exploited%20to%20improve%20Real-ISR%0Aperformance.%20To%20this%20end%2C%20we%20introduce%20Direct%20Perceptual%20Preference%0AOptimization%20for%20Real-ISR%20%28DP%24%5E2%24O-SR%29%2C%20a%20framework%20that%20aligns%20generative%0Amodels%20with%20perceptual%20preferences%20without%20requiring%20costly%20human%20annotations.%0AWe%20construct%20a%20hybrid%20reward%20signal%20by%20combining%20full-reference%20and%0Ano-reference%20image%20quality%20assessment%20%28IQA%29%20models%20trained%20on%20large-scale%20human%0Apreference%20datasets.%20This%20reward%20encourages%20both%20structural%20fidelity%20and%0Anatural%20appearance.%20To%20better%20utilize%20perceptual%20diversity%2C%20we%20move%20beyond%20the%0Astandard%20best-vs-worst%20selection%20and%20construct%20multiple%20preference%20pairs%20from%0Aoutputs%20of%20the%20same%20model.%20Our%20analysis%20reveals%20that%20the%20optimal%20selection%0Aratio%20depends%20on%20model%20capacity%3A%20smaller%20models%20benefit%20from%20broader%20coverage%2C%0Awhile%20larger%20models%20respond%20better%20to%20stronger%20contrast%20in%20supervision.%0AFurthermore%2C%20we%20propose%20hierarchical%20preference%20optimization%2C%20which%20adaptively%0Aweights%20training%20pairs%20based%20on%20intra-group%20reward%20gaps%20and%20inter-group%0Adiversity%2C%20enabling%20more%20efficient%20and%20stable%20learning.%20Extensive%20experiments%0Aacross%20both%20diffusion-%20and%20flow-based%20T2I%20backbones%20demonstrate%20that%20DP%24%5E2%24O-SR%0Asignificantly%20improves%20perceptual%20quality%20and%20generalizes%20well%20to%20real-world%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18851v1&entry.124074799=Read"},
{"title": "Dynamic object goal pushing with mobile manipulators through model-free\n  constrained reinforcement learning", "author": "Ioannis Dadiotis and Mayank Mittal and Nikos Tsagarakis and Marco Hutter", "abstract": "  Non-prehensile pushing to move and reorient objects to a goal is a versatile\nloco-manipulation skill. In the real world, the object's physical properties\nand friction with the floor contain significant uncertainties, which makes the\ntask challenging for a mobile manipulator. In this paper, we develop a\nlearning-based controller for a mobile manipulator to move an unknown object to\na desired position and yaw orientation through a sequence of pushing actions.\nThe proposed controller for the robotic arm and the mobile base motion is\ntrained using a constrained Reinforcement Learning (RL) formulation. We\ndemonstrate its capability in experiments with a quadrupedal robot equipped\nwith an arm. The learned policy achieves a success rate of 91.35% in simulation\nand at least 80% on hardware in challenging scenarios. Through our extensive\nhardware experiments, we show that the approach demonstrates high robustness\nagainst unknown objects of different masses, materials, sizes, and shapes. It\nreactively discovers the pushing location and direction, thus achieving\ncontact-rich behavior while observing only the pose of the object.\nAdditionally, we demonstrate the adaptive behavior of the learned policy\ntowards preventing the object from toppling.\n", "link": "http://arxiv.org/abs/2502.01546v2", "date": "2025-10-21", "relevancy": 1.7211, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6097}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5947}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20object%20goal%20pushing%20with%20mobile%20manipulators%20through%20model-free%0A%20%20constrained%20reinforcement%20learning&body=Title%3A%20Dynamic%20object%20goal%20pushing%20with%20mobile%20manipulators%20through%20model-free%0A%20%20constrained%20reinforcement%20learning%0AAuthor%3A%20Ioannis%20Dadiotis%20and%20Mayank%20Mittal%20and%20Nikos%20Tsagarakis%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Non-prehensile%20pushing%20to%20move%20and%20reorient%20objects%20to%20a%20goal%20is%20a%20versatile%0Aloco-manipulation%20skill.%20In%20the%20real%20world%2C%20the%20object%27s%20physical%20properties%0Aand%20friction%20with%20the%20floor%20contain%20significant%20uncertainties%2C%20which%20makes%20the%0Atask%20challenging%20for%20a%20mobile%20manipulator.%20In%20this%20paper%2C%20we%20develop%20a%0Alearning-based%20controller%20for%20a%20mobile%20manipulator%20to%20move%20an%20unknown%20object%20to%0Aa%20desired%20position%20and%20yaw%20orientation%20through%20a%20sequence%20of%20pushing%20actions.%0AThe%20proposed%20controller%20for%20the%20robotic%20arm%20and%20the%20mobile%20base%20motion%20is%0Atrained%20using%20a%20constrained%20Reinforcement%20Learning%20%28RL%29%20formulation.%20We%0Ademonstrate%20its%20capability%20in%20experiments%20with%20a%20quadrupedal%20robot%20equipped%0Awith%20an%20arm.%20The%20learned%20policy%20achieves%20a%20success%20rate%20of%2091.35%25%20in%20simulation%0Aand%20at%20least%2080%25%20on%20hardware%20in%20challenging%20scenarios.%20Through%20our%20extensive%0Ahardware%20experiments%2C%20we%20show%20that%20the%20approach%20demonstrates%20high%20robustness%0Aagainst%20unknown%20objects%20of%20different%20masses%2C%20materials%2C%20sizes%2C%20and%20shapes.%20It%0Areactively%20discovers%20the%20pushing%20location%20and%20direction%2C%20thus%20achieving%0Acontact-rich%20behavior%20while%20observing%20only%20the%20pose%20of%20the%20object.%0AAdditionally%2C%20we%20demonstrate%20the%20adaptive%20behavior%20of%20the%20learned%20policy%0Atowards%20preventing%20the%20object%20from%20toppling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520object%2520goal%2520pushing%2520with%2520mobile%2520manipulators%2520through%2520model-free%250A%2520%2520constrained%2520reinforcement%2520learning%26entry.906535625%3DIoannis%2520Dadiotis%2520and%2520Mayank%2520Mittal%2520and%2520Nikos%2520Tsagarakis%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Non-prehensile%2520pushing%2520to%2520move%2520and%2520reorient%2520objects%2520to%2520a%2520goal%2520is%2520a%2520versatile%250Aloco-manipulation%2520skill.%2520In%2520the%2520real%2520world%252C%2520the%2520object%2527s%2520physical%2520properties%250Aand%2520friction%2520with%2520the%2520floor%2520contain%2520significant%2520uncertainties%252C%2520which%2520makes%2520the%250Atask%2520challenging%2520for%2520a%2520mobile%2520manipulator.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%250Alearning-based%2520controller%2520for%2520a%2520mobile%2520manipulator%2520to%2520move%2520an%2520unknown%2520object%2520to%250Aa%2520desired%2520position%2520and%2520yaw%2520orientation%2520through%2520a%2520sequence%2520of%2520pushing%2520actions.%250AThe%2520proposed%2520controller%2520for%2520the%2520robotic%2520arm%2520and%2520the%2520mobile%2520base%2520motion%2520is%250Atrained%2520using%2520a%2520constrained%2520Reinforcement%2520Learning%2520%2528RL%2529%2520formulation.%2520We%250Ademonstrate%2520its%2520capability%2520in%2520experiments%2520with%2520a%2520quadrupedal%2520robot%2520equipped%250Awith%2520an%2520arm.%2520The%2520learned%2520policy%2520achieves%2520a%2520success%2520rate%2520of%252091.35%2525%2520in%2520simulation%250Aand%2520at%2520least%252080%2525%2520on%2520hardware%2520in%2520challenging%2520scenarios.%2520Through%2520our%2520extensive%250Ahardware%2520experiments%252C%2520we%2520show%2520that%2520the%2520approach%2520demonstrates%2520high%2520robustness%250Aagainst%2520unknown%2520objects%2520of%2520different%2520masses%252C%2520materials%252C%2520sizes%252C%2520and%2520shapes.%2520It%250Areactively%2520discovers%2520the%2520pushing%2520location%2520and%2520direction%252C%2520thus%2520achieving%250Acontact-rich%2520behavior%2520while%2520observing%2520only%2520the%2520pose%2520of%2520the%2520object.%250AAdditionally%252C%2520we%2520demonstrate%2520the%2520adaptive%2520behavior%2520of%2520the%2520learned%2520policy%250Atowards%2520preventing%2520the%2520object%2520from%2520toppling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20object%20goal%20pushing%20with%20mobile%20manipulators%20through%20model-free%0A%20%20constrained%20reinforcement%20learning&entry.906535625=Ioannis%20Dadiotis%20and%20Mayank%20Mittal%20and%20Nikos%20Tsagarakis%20and%20Marco%20Hutter&entry.1292438233=%20%20Non-prehensile%20pushing%20to%20move%20and%20reorient%20objects%20to%20a%20goal%20is%20a%20versatile%0Aloco-manipulation%20skill.%20In%20the%20real%20world%2C%20the%20object%27s%20physical%20properties%0Aand%20friction%20with%20the%20floor%20contain%20significant%20uncertainties%2C%20which%20makes%20the%0Atask%20challenging%20for%20a%20mobile%20manipulator.%20In%20this%20paper%2C%20we%20develop%20a%0Alearning-based%20controller%20for%20a%20mobile%20manipulator%20to%20move%20an%20unknown%20object%20to%0Aa%20desired%20position%20and%20yaw%20orientation%20through%20a%20sequence%20of%20pushing%20actions.%0AThe%20proposed%20controller%20for%20the%20robotic%20arm%20and%20the%20mobile%20base%20motion%20is%0Atrained%20using%20a%20constrained%20Reinforcement%20Learning%20%28RL%29%20formulation.%20We%0Ademonstrate%20its%20capability%20in%20experiments%20with%20a%20quadrupedal%20robot%20equipped%0Awith%20an%20arm.%20The%20learned%20policy%20achieves%20a%20success%20rate%20of%2091.35%25%20in%20simulation%0Aand%20at%20least%2080%25%20on%20hardware%20in%20challenging%20scenarios.%20Through%20our%20extensive%0Ahardware%20experiments%2C%20we%20show%20that%20the%20approach%20demonstrates%20high%20robustness%0Aagainst%20unknown%20objects%20of%20different%20masses%2C%20materials%2C%20sizes%2C%20and%20shapes.%20It%0Areactively%20discovers%20the%20pushing%20location%20and%20direction%2C%20thus%20achieving%0Acontact-rich%20behavior%20while%20observing%20only%20the%20pose%20of%20the%20object.%0AAdditionally%2C%20we%20demonstrate%20the%20adaptive%20behavior%20of%20the%20learned%20policy%0Atowards%20preventing%20the%20object%20from%20toppling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01546v2&entry.124074799=Read"},
{"title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and\n  Reducing Hallucination in Generative Models", "author": "Yiqi Tian and Pengfei Jin and Mingze Yuan and Na Li and Bo Zeng and Quanzheng Li", "abstract": "  Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to\nhallucinations-often stemming from inaccuracies in score approximation. In this\nwork, we reinterpret diffusion sampling through the lens of optimization and\nintroduce RODS (Robust Optimization-inspired Diffusion Sampler), a novel method\nthat detects and corrects high-risk sampling steps using geometric cues from\nthe loss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS maintains comparable image quality and preserves\ngeneration diversity. More importantly, it improves both sampling fidelity and\nrobustness, detecting over 70% of hallucinated samples and correcting more than\n25%, all while avoiding the introduction of new artifacts. We release our code\nat https://github.com/Yiqi-Verna-Tian/RODS.\n", "link": "http://arxiv.org/abs/2507.12201v2", "date": "2025-10-21", "relevancy": 1.6944, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5712}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5676}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RODS%3A%20Robust%20Optimization%20Inspired%20Diffusion%20Sampling%20for%20Detecting%20and%0A%20%20Reducing%20Hallucination%20in%20Generative%20Models&body=Title%3A%20RODS%3A%20Robust%20Optimization%20Inspired%20Diffusion%20Sampling%20for%20Detecting%20and%0A%20%20Reducing%20Hallucination%20in%20Generative%20Models%0AAuthor%3A%20Yiqi%20Tian%20and%20Pengfei%20Jin%20and%20Mingze%20Yuan%20and%20Na%20Li%20and%20Bo%20Zeng%20and%20Quanzheng%20Li%0AAbstract%3A%20%20%20Diffusion%20models%20have%20achieved%20state-of-the-art%20performance%20in%20generative%0Amodeling%2C%20yet%20their%20sampling%20procedures%20remain%20vulnerable%20to%0Ahallucinations-often%20stemming%20from%20inaccuracies%20in%20score%20approximation.%20In%20this%0Awork%2C%20we%20reinterpret%20diffusion%20sampling%20through%20the%20lens%20of%20optimization%20and%0Aintroduce%20RODS%20%28Robust%20Optimization-inspired%20Diffusion%20Sampler%29%2C%20a%20novel%20method%0Athat%20detects%20and%20corrects%20high-risk%20sampling%20steps%20using%20geometric%20cues%20from%0Athe%20loss%20landscape.%20RODS%20enforces%20smoother%20sampling%20trajectories%20and%20adaptively%0Aadjusts%20perturbations%2C%20reducing%20hallucinations%20without%20retraining%20and%20at%0Aminimal%20additional%20inference%20cost.%20Experiments%20on%20AFHQv2%2C%20FFHQ%2C%20and%2011k-hands%0Ademonstrate%20that%20RODS%20maintains%20comparable%20image%20quality%20and%20preserves%0Ageneration%20diversity.%20More%20importantly%2C%20it%20improves%20both%20sampling%20fidelity%20and%0Arobustness%2C%20detecting%20over%2070%25%20of%20hallucinated%20samples%20and%20correcting%20more%20than%0A25%25%2C%20all%20while%20avoiding%20the%20introduction%20of%20new%20artifacts.%20We%20release%20our%20code%0Aat%20https%3A//github.com/Yiqi-Verna-Tian/RODS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12201v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRODS%253A%2520Robust%2520Optimization%2520Inspired%2520Diffusion%2520Sampling%2520for%2520Detecting%2520and%250A%2520%2520Reducing%2520Hallucination%2520in%2520Generative%2520Models%26entry.906535625%3DYiqi%2520Tian%2520and%2520Pengfei%2520Jin%2520and%2520Mingze%2520Yuan%2520and%2520Na%2520Li%2520and%2520Bo%2520Zeng%2520and%2520Quanzheng%2520Li%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520achieved%2520state-of-the-art%2520performance%2520in%2520generative%250Amodeling%252C%2520yet%2520their%2520sampling%2520procedures%2520remain%2520vulnerable%2520to%250Ahallucinations-often%2520stemming%2520from%2520inaccuracies%2520in%2520score%2520approximation.%2520In%2520this%250Awork%252C%2520we%2520reinterpret%2520diffusion%2520sampling%2520through%2520the%2520lens%2520of%2520optimization%2520and%250Aintroduce%2520RODS%2520%2528Robust%2520Optimization-inspired%2520Diffusion%2520Sampler%2529%252C%2520a%2520novel%2520method%250Athat%2520detects%2520and%2520corrects%2520high-risk%2520sampling%2520steps%2520using%2520geometric%2520cues%2520from%250Athe%2520loss%2520landscape.%2520RODS%2520enforces%2520smoother%2520sampling%2520trajectories%2520and%2520adaptively%250Aadjusts%2520perturbations%252C%2520reducing%2520hallucinations%2520without%2520retraining%2520and%2520at%250Aminimal%2520additional%2520inference%2520cost.%2520Experiments%2520on%2520AFHQv2%252C%2520FFHQ%252C%2520and%252011k-hands%250Ademonstrate%2520that%2520RODS%2520maintains%2520comparable%2520image%2520quality%2520and%2520preserves%250Ageneration%2520diversity.%2520More%2520importantly%252C%2520it%2520improves%2520both%2520sampling%2520fidelity%2520and%250Arobustness%252C%2520detecting%2520over%252070%2525%2520of%2520hallucinated%2520samples%2520and%2520correcting%2520more%2520than%250A25%2525%252C%2520all%2520while%2520avoiding%2520the%2520introduction%2520of%2520new%2520artifacts.%2520We%2520release%2520our%2520code%250Aat%2520https%253A//github.com/Yiqi-Verna-Tian/RODS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12201v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RODS%3A%20Robust%20Optimization%20Inspired%20Diffusion%20Sampling%20for%20Detecting%20and%0A%20%20Reducing%20Hallucination%20in%20Generative%20Models&entry.906535625=Yiqi%20Tian%20and%20Pengfei%20Jin%20and%20Mingze%20Yuan%20and%20Na%20Li%20and%20Bo%20Zeng%20and%20Quanzheng%20Li&entry.1292438233=%20%20Diffusion%20models%20have%20achieved%20state-of-the-art%20performance%20in%20generative%0Amodeling%2C%20yet%20their%20sampling%20procedures%20remain%20vulnerable%20to%0Ahallucinations-often%20stemming%20from%20inaccuracies%20in%20score%20approximation.%20In%20this%0Awork%2C%20we%20reinterpret%20diffusion%20sampling%20through%20the%20lens%20of%20optimization%20and%0Aintroduce%20RODS%20%28Robust%20Optimization-inspired%20Diffusion%20Sampler%29%2C%20a%20novel%20method%0Athat%20detects%20and%20corrects%20high-risk%20sampling%20steps%20using%20geometric%20cues%20from%0Athe%20loss%20landscape.%20RODS%20enforces%20smoother%20sampling%20trajectories%20and%20adaptively%0Aadjusts%20perturbations%2C%20reducing%20hallucinations%20without%20retraining%20and%20at%0Aminimal%20additional%20inference%20cost.%20Experiments%20on%20AFHQv2%2C%20FFHQ%2C%20and%2011k-hands%0Ademonstrate%20that%20RODS%20maintains%20comparable%20image%20quality%20and%20preserves%0Ageneration%20diversity.%20More%20importantly%2C%20it%20improves%20both%20sampling%20fidelity%20and%0Arobustness%2C%20detecting%20over%2070%25%20of%20hallucinated%20samples%20and%20correcting%20more%20than%0A25%25%2C%20all%20while%20avoiding%20the%20introduction%20of%20new%20artifacts.%20We%20release%20our%20code%0Aat%20https%3A//github.com/Yiqi-Verna-Tian/RODS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12201v2&entry.124074799=Read"},
{"title": "Unifying and Enhancing Graph Transformers via a Hierarchical Mask\n  Framework", "author": "Yujie Xing and Xiao Wang and Bin Wu and Hai Huang and Chuan Shi", "abstract": "  Graph Transformers (GTs) have emerged as a powerful paradigm for graph\nrepresentation learning due to their ability to model diverse node\ninteractions. However, existing GTs often rely on intricate architectural\ndesigns tailored to specific interactions, limiting their flexibility. To\naddress this, we propose a unified hierarchical mask framework that reveals an\nunderlying equivalence between model architecture and attention mask\nconstruction. This framework enables a consistent modeling paradigm by\ncapturing diverse interactions through carefully designed attention masks.\nTheoretical analysis under this framework demonstrates that the probability of\ncorrect classification positively correlates with the receptive field size and\nlabel consistency, leading to a fundamental design principle: an effective\nattention mask should ensure both a sufficiently large receptive field and a\nhigh level of label consistency. While no single existing mask satisfies this\nprinciple across all scenarios, our analysis reveals that hierarchical masks\noffer complementary strengths, motivating their effective integration. Then, we\nintroduce M3Dphormer, a Mixture-of-Experts-based Graph Transformer with\nMulti-Level Masking and Dual Attention Computation. M3Dphormer incorporates\nthree theoretically grounded hierarchical masks and employs a bi-level expert\nrouting mechanism to adaptively integrate multi-level interaction information.\nTo ensure scalability, we further introduce a dual attention computation scheme\nthat dynamically switches between dense and sparse modes based on local mask\nsparsity. Extensive experiments across multiple benchmarks demonstrate that\nM3Dphormer achieves state-of-the-art performance, validating the effectiveness\nof our unified framework and model design.\n", "link": "http://arxiv.org/abs/2510.18825v1", "date": "2025-10-21", "relevancy": 1.6744, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5797}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20and%20Enhancing%20Graph%20Transformers%20via%20a%20Hierarchical%20Mask%0A%20%20Framework&body=Title%3A%20Unifying%20and%20Enhancing%20Graph%20Transformers%20via%20a%20Hierarchical%20Mask%0A%20%20Framework%0AAuthor%3A%20Yujie%20Xing%20and%20Xiao%20Wang%20and%20Bin%20Wu%20and%20Hai%20Huang%20and%20Chuan%20Shi%0AAbstract%3A%20%20%20Graph%20Transformers%20%28GTs%29%20have%20emerged%20as%20a%20powerful%20paradigm%20for%20graph%0Arepresentation%20learning%20due%20to%20their%20ability%20to%20model%20diverse%20node%0Ainteractions.%20However%2C%20existing%20GTs%20often%20rely%20on%20intricate%20architectural%0Adesigns%20tailored%20to%20specific%20interactions%2C%20limiting%20their%20flexibility.%20To%0Aaddress%20this%2C%20we%20propose%20a%20unified%20hierarchical%20mask%20framework%20that%20reveals%20an%0Aunderlying%20equivalence%20between%20model%20architecture%20and%20attention%20mask%0Aconstruction.%20This%20framework%20enables%20a%20consistent%20modeling%20paradigm%20by%0Acapturing%20diverse%20interactions%20through%20carefully%20designed%20attention%20masks.%0ATheoretical%20analysis%20under%20this%20framework%20demonstrates%20that%20the%20probability%20of%0Acorrect%20classification%20positively%20correlates%20with%20the%20receptive%20field%20size%20and%0Alabel%20consistency%2C%20leading%20to%20a%20fundamental%20design%20principle%3A%20an%20effective%0Aattention%20mask%20should%20ensure%20both%20a%20sufficiently%20large%20receptive%20field%20and%20a%0Ahigh%20level%20of%20label%20consistency.%20While%20no%20single%20existing%20mask%20satisfies%20this%0Aprinciple%20across%20all%20scenarios%2C%20our%20analysis%20reveals%20that%20hierarchical%20masks%0Aoffer%20complementary%20strengths%2C%20motivating%20their%20effective%20integration.%20Then%2C%20we%0Aintroduce%20M3Dphormer%2C%20a%20Mixture-of-Experts-based%20Graph%20Transformer%20with%0AMulti-Level%20Masking%20and%20Dual%20Attention%20Computation.%20M3Dphormer%20incorporates%0Athree%20theoretically%20grounded%20hierarchical%20masks%20and%20employs%20a%20bi-level%20expert%0Arouting%20mechanism%20to%20adaptively%20integrate%20multi-level%20interaction%20information.%0ATo%20ensure%20scalability%2C%20we%20further%20introduce%20a%20dual%20attention%20computation%20scheme%0Athat%20dynamically%20switches%20between%20dense%20and%20sparse%20modes%20based%20on%20local%20mask%0Asparsity.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%0AM3Dphormer%20achieves%20state-of-the-art%20performance%2C%20validating%20the%20effectiveness%0Aof%20our%20unified%20framework%20and%20model%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520and%2520Enhancing%2520Graph%2520Transformers%2520via%2520a%2520Hierarchical%2520Mask%250A%2520%2520Framework%26entry.906535625%3DYujie%2520Xing%2520and%2520Xiao%2520Wang%2520and%2520Bin%2520Wu%2520and%2520Hai%2520Huang%2520and%2520Chuan%2520Shi%26entry.1292438233%3D%2520%2520Graph%2520Transformers%2520%2528GTs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520graph%250Arepresentation%2520learning%2520due%2520to%2520their%2520ability%2520to%2520model%2520diverse%2520node%250Ainteractions.%2520However%252C%2520existing%2520GTs%2520often%2520rely%2520on%2520intricate%2520architectural%250Adesigns%2520tailored%2520to%2520specific%2520interactions%252C%2520limiting%2520their%2520flexibility.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520unified%2520hierarchical%2520mask%2520framework%2520that%2520reveals%2520an%250Aunderlying%2520equivalence%2520between%2520model%2520architecture%2520and%2520attention%2520mask%250Aconstruction.%2520This%2520framework%2520enables%2520a%2520consistent%2520modeling%2520paradigm%2520by%250Acapturing%2520diverse%2520interactions%2520through%2520carefully%2520designed%2520attention%2520masks.%250ATheoretical%2520analysis%2520under%2520this%2520framework%2520demonstrates%2520that%2520the%2520probability%2520of%250Acorrect%2520classification%2520positively%2520correlates%2520with%2520the%2520receptive%2520field%2520size%2520and%250Alabel%2520consistency%252C%2520leading%2520to%2520a%2520fundamental%2520design%2520principle%253A%2520an%2520effective%250Aattention%2520mask%2520should%2520ensure%2520both%2520a%2520sufficiently%2520large%2520receptive%2520field%2520and%2520a%250Ahigh%2520level%2520of%2520label%2520consistency.%2520While%2520no%2520single%2520existing%2520mask%2520satisfies%2520this%250Aprinciple%2520across%2520all%2520scenarios%252C%2520our%2520analysis%2520reveals%2520that%2520hierarchical%2520masks%250Aoffer%2520complementary%2520strengths%252C%2520motivating%2520their%2520effective%2520integration.%2520Then%252C%2520we%250Aintroduce%2520M3Dphormer%252C%2520a%2520Mixture-of-Experts-based%2520Graph%2520Transformer%2520with%250AMulti-Level%2520Masking%2520and%2520Dual%2520Attention%2520Computation.%2520M3Dphormer%2520incorporates%250Athree%2520theoretically%2520grounded%2520hierarchical%2520masks%2520and%2520employs%2520a%2520bi-level%2520expert%250Arouting%2520mechanism%2520to%2520adaptively%2520integrate%2520multi-level%2520interaction%2520information.%250ATo%2520ensure%2520scalability%252C%2520we%2520further%2520introduce%2520a%2520dual%2520attention%2520computation%2520scheme%250Athat%2520dynamically%2520switches%2520between%2520dense%2520and%2520sparse%2520modes%2520based%2520on%2520local%2520mask%250Asparsity.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%250AM3Dphormer%2520achieves%2520state-of-the-art%2520performance%252C%2520validating%2520the%2520effectiveness%250Aof%2520our%2520unified%2520framework%2520and%2520model%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20and%20Enhancing%20Graph%20Transformers%20via%20a%20Hierarchical%20Mask%0A%20%20Framework&entry.906535625=Yujie%20Xing%20and%20Xiao%20Wang%20and%20Bin%20Wu%20and%20Hai%20Huang%20and%20Chuan%20Shi&entry.1292438233=%20%20Graph%20Transformers%20%28GTs%29%20have%20emerged%20as%20a%20powerful%20paradigm%20for%20graph%0Arepresentation%20learning%20due%20to%20their%20ability%20to%20model%20diverse%20node%0Ainteractions.%20However%2C%20existing%20GTs%20often%20rely%20on%20intricate%20architectural%0Adesigns%20tailored%20to%20specific%20interactions%2C%20limiting%20their%20flexibility.%20To%0Aaddress%20this%2C%20we%20propose%20a%20unified%20hierarchical%20mask%20framework%20that%20reveals%20an%0Aunderlying%20equivalence%20between%20model%20architecture%20and%20attention%20mask%0Aconstruction.%20This%20framework%20enables%20a%20consistent%20modeling%20paradigm%20by%0Acapturing%20diverse%20interactions%20through%20carefully%20designed%20attention%20masks.%0ATheoretical%20analysis%20under%20this%20framework%20demonstrates%20that%20the%20probability%20of%0Acorrect%20classification%20positively%20correlates%20with%20the%20receptive%20field%20size%20and%0Alabel%20consistency%2C%20leading%20to%20a%20fundamental%20design%20principle%3A%20an%20effective%0Aattention%20mask%20should%20ensure%20both%20a%20sufficiently%20large%20receptive%20field%20and%20a%0Ahigh%20level%20of%20label%20consistency.%20While%20no%20single%20existing%20mask%20satisfies%20this%0Aprinciple%20across%20all%20scenarios%2C%20our%20analysis%20reveals%20that%20hierarchical%20masks%0Aoffer%20complementary%20strengths%2C%20motivating%20their%20effective%20integration.%20Then%2C%20we%0Aintroduce%20M3Dphormer%2C%20a%20Mixture-of-Experts-based%20Graph%20Transformer%20with%0AMulti-Level%20Masking%20and%20Dual%20Attention%20Computation.%20M3Dphormer%20incorporates%0Athree%20theoretically%20grounded%20hierarchical%20masks%20and%20employs%20a%20bi-level%20expert%0Arouting%20mechanism%20to%20adaptively%20integrate%20multi-level%20interaction%20information.%0ATo%20ensure%20scalability%2C%20we%20further%20introduce%20a%20dual%20attention%20computation%20scheme%0Athat%20dynamically%20switches%20between%20dense%20and%20sparse%20modes%20based%20on%20local%20mask%0Asparsity.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%0AM3Dphormer%20achieves%20state-of-the-art%20performance%2C%20validating%20the%20effectiveness%0Aof%20our%20unified%20framework%20and%20model%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18825v1&entry.124074799=Read"},
{"title": "A Statistical Theory of Contrastive Pre-training and Multimodal\n  Generative AI", "author": "Kazusato Oko and Licong Lin and Yuhang Cai and Song Mei", "abstract": "  Multi-modal generative AI systems, such as those combining vision and\nlanguage, rely on contrastive pre-training to learn representations across\ndifferent modalities. While their practical benefits are widely acknowledged, a\nrigorous theoretical understanding of the contrastive pre-training framework\nremains limited. This paper develops a theoretical framework to explain the\nsuccess of contrastive pre-training in downstream tasks, such as zero-shot\nclassification, conditional diffusion models, and vision-language models. We\nintroduce the concept of approximate sufficient statistics, a generalization of\nthe classical sufficient statistics, and show that near-minimizers of the\ncontrastive pre-training loss are approximately sufficient, making them\nadaptable to diverse downstream tasks. We further propose the Joint Generative\nHierarchical Model for the joint distribution of images and text, showing that\ntransformers can efficiently approximate relevant functions within this model\nvia belief propagation. Building on this framework, we derive sample complexity\nguarantees for multi-modal learning based on contrastive pre-trained\nrepresentations. Numerical simulations validate these theoretical findings,\ndemonstrating the strong generalization performance of contrastively\npre-trained transformers in various multi-modal tasks.\n", "link": "http://arxiv.org/abs/2501.04641v2", "date": "2025-10-21", "relevancy": 1.6672, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5629}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5551}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Statistical%20Theory%20of%20Contrastive%20Pre-training%20and%20Multimodal%0A%20%20Generative%20AI&body=Title%3A%20A%20Statistical%20Theory%20of%20Contrastive%20Pre-training%20and%20Multimodal%0A%20%20Generative%20AI%0AAuthor%3A%20Kazusato%20Oko%20and%20Licong%20Lin%20and%20Yuhang%20Cai%20and%20Song%20Mei%0AAbstract%3A%20%20%20Multi-modal%20generative%20AI%20systems%2C%20such%20as%20those%20combining%20vision%20and%0Alanguage%2C%20rely%20on%20contrastive%20pre-training%20to%20learn%20representations%20across%0Adifferent%20modalities.%20While%20their%20practical%20benefits%20are%20widely%20acknowledged%2C%20a%0Arigorous%20theoretical%20understanding%20of%20the%20contrastive%20pre-training%20framework%0Aremains%20limited.%20This%20paper%20develops%20a%20theoretical%20framework%20to%20explain%20the%0Asuccess%20of%20contrastive%20pre-training%20in%20downstream%20tasks%2C%20such%20as%20zero-shot%0Aclassification%2C%20conditional%20diffusion%20models%2C%20and%20vision-language%20models.%20We%0Aintroduce%20the%20concept%20of%20approximate%20sufficient%20statistics%2C%20a%20generalization%20of%0Athe%20classical%20sufficient%20statistics%2C%20and%20show%20that%20near-minimizers%20of%20the%0Acontrastive%20pre-training%20loss%20are%20approximately%20sufficient%2C%20making%20them%0Aadaptable%20to%20diverse%20downstream%20tasks.%20We%20further%20propose%20the%20Joint%20Generative%0AHierarchical%20Model%20for%20the%20joint%20distribution%20of%20images%20and%20text%2C%20showing%20that%0Atransformers%20can%20efficiently%20approximate%20relevant%20functions%20within%20this%20model%0Avia%20belief%20propagation.%20Building%20on%20this%20framework%2C%20we%20derive%20sample%20complexity%0Aguarantees%20for%20multi-modal%20learning%20based%20on%20contrastive%20pre-trained%0Arepresentations.%20Numerical%20simulations%20validate%20these%20theoretical%20findings%2C%0Ademonstrating%20the%20strong%20generalization%20performance%20of%20contrastively%0Apre-trained%20transformers%20in%20various%20multi-modal%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Statistical%2520Theory%2520of%2520Contrastive%2520Pre-training%2520and%2520Multimodal%250A%2520%2520Generative%2520AI%26entry.906535625%3DKazusato%2520Oko%2520and%2520Licong%2520Lin%2520and%2520Yuhang%2520Cai%2520and%2520Song%2520Mei%26entry.1292438233%3D%2520%2520Multi-modal%2520generative%2520AI%2520systems%252C%2520such%2520as%2520those%2520combining%2520vision%2520and%250Alanguage%252C%2520rely%2520on%2520contrastive%2520pre-training%2520to%2520learn%2520representations%2520across%250Adifferent%2520modalities.%2520While%2520their%2520practical%2520benefits%2520are%2520widely%2520acknowledged%252C%2520a%250Arigorous%2520theoretical%2520understanding%2520of%2520the%2520contrastive%2520pre-training%2520framework%250Aremains%2520limited.%2520This%2520paper%2520develops%2520a%2520theoretical%2520framework%2520to%2520explain%2520the%250Asuccess%2520of%2520contrastive%2520pre-training%2520in%2520downstream%2520tasks%252C%2520such%2520as%2520zero-shot%250Aclassification%252C%2520conditional%2520diffusion%2520models%252C%2520and%2520vision-language%2520models.%2520We%250Aintroduce%2520the%2520concept%2520of%2520approximate%2520sufficient%2520statistics%252C%2520a%2520generalization%2520of%250Athe%2520classical%2520sufficient%2520statistics%252C%2520and%2520show%2520that%2520near-minimizers%2520of%2520the%250Acontrastive%2520pre-training%2520loss%2520are%2520approximately%2520sufficient%252C%2520making%2520them%250Aadaptable%2520to%2520diverse%2520downstream%2520tasks.%2520We%2520further%2520propose%2520the%2520Joint%2520Generative%250AHierarchical%2520Model%2520for%2520the%2520joint%2520distribution%2520of%2520images%2520and%2520text%252C%2520showing%2520that%250Atransformers%2520can%2520efficiently%2520approximate%2520relevant%2520functions%2520within%2520this%2520model%250Avia%2520belief%2520propagation.%2520Building%2520on%2520this%2520framework%252C%2520we%2520derive%2520sample%2520complexity%250Aguarantees%2520for%2520multi-modal%2520learning%2520based%2520on%2520contrastive%2520pre-trained%250Arepresentations.%2520Numerical%2520simulations%2520validate%2520these%2520theoretical%2520findings%252C%250Ademonstrating%2520the%2520strong%2520generalization%2520performance%2520of%2520contrastively%250Apre-trained%2520transformers%2520in%2520various%2520multi-modal%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Statistical%20Theory%20of%20Contrastive%20Pre-training%20and%20Multimodal%0A%20%20Generative%20AI&entry.906535625=Kazusato%20Oko%20and%20Licong%20Lin%20and%20Yuhang%20Cai%20and%20Song%20Mei&entry.1292438233=%20%20Multi-modal%20generative%20AI%20systems%2C%20such%20as%20those%20combining%20vision%20and%0Alanguage%2C%20rely%20on%20contrastive%20pre-training%20to%20learn%20representations%20across%0Adifferent%20modalities.%20While%20their%20practical%20benefits%20are%20widely%20acknowledged%2C%20a%0Arigorous%20theoretical%20understanding%20of%20the%20contrastive%20pre-training%20framework%0Aremains%20limited.%20This%20paper%20develops%20a%20theoretical%20framework%20to%20explain%20the%0Asuccess%20of%20contrastive%20pre-training%20in%20downstream%20tasks%2C%20such%20as%20zero-shot%0Aclassification%2C%20conditional%20diffusion%20models%2C%20and%20vision-language%20models.%20We%0Aintroduce%20the%20concept%20of%20approximate%20sufficient%20statistics%2C%20a%20generalization%20of%0Athe%20classical%20sufficient%20statistics%2C%20and%20show%20that%20near-minimizers%20of%20the%0Acontrastive%20pre-training%20loss%20are%20approximately%20sufficient%2C%20making%20them%0Aadaptable%20to%20diverse%20downstream%20tasks.%20We%20further%20propose%20the%20Joint%20Generative%0AHierarchical%20Model%20for%20the%20joint%20distribution%20of%20images%20and%20text%2C%20showing%20that%0Atransformers%20can%20efficiently%20approximate%20relevant%20functions%20within%20this%20model%0Avia%20belief%20propagation.%20Building%20on%20this%20framework%2C%20we%20derive%20sample%20complexity%0Aguarantees%20for%20multi-modal%20learning%20based%20on%20contrastive%20pre-trained%0Arepresentations.%20Numerical%20simulations%20validate%20these%20theoretical%20findings%2C%0Ademonstrating%20the%20strong%20generalization%20performance%20of%20contrastively%0Apre-trained%20transformers%20in%20various%20multi-modal%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04641v2&entry.124074799=Read"},
{"title": "Interpretable Decision-Making for End-to-End Autonomous Driving", "author": "Mona Mirzaie and Bodo Rosenhahn", "abstract": "  Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.\nAlthough end-to-end approaches derive control commands directly from raw data,\ninterpreting these decisions remains challenging, especially in complex urban\nscenarios. This is mainly attributed to very deep neural networks with\nnon-linear decision boundaries, making it challenging to grasp the logic behind\nAI-driven decisions. This paper presents a method to enhance interpretability\nwhile optimizing control commands in autonomous driving. To address this, we\npropose loss functions that promote the interpretability of our model by\ngenerating sparse and localized feature maps. The feature activations allow us\nto explain which image regions contribute to the predicted control command. We\nconduct comprehensive ablation studies on the feature extraction step and\nvalidate our method on the CARLA benchmarks. We also demonstrate that our\napproach improves interpretability, which correlates with reducing infractions,\nyielding a safer, high-performance driving model. Notably, our monocular,\nnon-ensemble model surpasses the top-performing approaches from the CARLA\nLeaderboard by achieving lower infraction scores and the highest route\ncompletion rate, all while ensuring interpretability.\n", "link": "http://arxiv.org/abs/2508.18898v3", "date": "2025-10-21", "relevancy": 1.6387, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5317}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Decision-Making%20for%20End-to-End%20Autonomous%20Driving&body=Title%3A%20Interpretable%20Decision-Making%20for%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Mona%20Mirzaie%20and%20Bodo%20Rosenhahn%0AAbstract%3A%20%20%20Trustworthy%20AI%20is%20mandatory%20for%20the%20broad%20deployment%20of%20autonomous%20vehicles.%0AAlthough%20end-to-end%20approaches%20derive%20control%20commands%20directly%20from%20raw%20data%2C%0Ainterpreting%20these%20decisions%20remains%20challenging%2C%20especially%20in%20complex%20urban%0Ascenarios.%20This%20is%20mainly%20attributed%20to%20very%20deep%20neural%20networks%20with%0Anon-linear%20decision%20boundaries%2C%20making%20it%20challenging%20to%20grasp%20the%20logic%20behind%0AAI-driven%20decisions.%20This%20paper%20presents%20a%20method%20to%20enhance%20interpretability%0Awhile%20optimizing%20control%20commands%20in%20autonomous%20driving.%20To%20address%20this%2C%20we%0Apropose%20loss%20functions%20that%20promote%20the%20interpretability%20of%20our%20model%20by%0Agenerating%20sparse%20and%20localized%20feature%20maps.%20The%20feature%20activations%20allow%20us%0Ato%20explain%20which%20image%20regions%20contribute%20to%20the%20predicted%20control%20command.%20We%0Aconduct%20comprehensive%20ablation%20studies%20on%20the%20feature%20extraction%20step%20and%0Avalidate%20our%20method%20on%20the%20CARLA%20benchmarks.%20We%20also%20demonstrate%20that%20our%0Aapproach%20improves%20interpretability%2C%20which%20correlates%20with%20reducing%20infractions%2C%0Ayielding%20a%20safer%2C%20high-performance%20driving%20model.%20Notably%2C%20our%20monocular%2C%0Anon-ensemble%20model%20surpasses%20the%20top-performing%20approaches%20from%20the%20CARLA%0ALeaderboard%20by%20achieving%20lower%20infraction%20scores%20and%20the%20highest%20route%0Acompletion%20rate%2C%20all%20while%20ensuring%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18898v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Decision-Making%2520for%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DMona%2520Mirzaie%2520and%2520Bodo%2520Rosenhahn%26entry.1292438233%3D%2520%2520Trustworthy%2520AI%2520is%2520mandatory%2520for%2520the%2520broad%2520deployment%2520of%2520autonomous%2520vehicles.%250AAlthough%2520end-to-end%2520approaches%2520derive%2520control%2520commands%2520directly%2520from%2520raw%2520data%252C%250Ainterpreting%2520these%2520decisions%2520remains%2520challenging%252C%2520especially%2520in%2520complex%2520urban%250Ascenarios.%2520This%2520is%2520mainly%2520attributed%2520to%2520very%2520deep%2520neural%2520networks%2520with%250Anon-linear%2520decision%2520boundaries%252C%2520making%2520it%2520challenging%2520to%2520grasp%2520the%2520logic%2520behind%250AAI-driven%2520decisions.%2520This%2520paper%2520presents%2520a%2520method%2520to%2520enhance%2520interpretability%250Awhile%2520optimizing%2520control%2520commands%2520in%2520autonomous%2520driving.%2520To%2520address%2520this%252C%2520we%250Apropose%2520loss%2520functions%2520that%2520promote%2520the%2520interpretability%2520of%2520our%2520model%2520by%250Agenerating%2520sparse%2520and%2520localized%2520feature%2520maps.%2520The%2520feature%2520activations%2520allow%2520us%250Ato%2520explain%2520which%2520image%2520regions%2520contribute%2520to%2520the%2520predicted%2520control%2520command.%2520We%250Aconduct%2520comprehensive%2520ablation%2520studies%2520on%2520the%2520feature%2520extraction%2520step%2520and%250Avalidate%2520our%2520method%2520on%2520the%2520CARLA%2520benchmarks.%2520We%2520also%2520demonstrate%2520that%2520our%250Aapproach%2520improves%2520interpretability%252C%2520which%2520correlates%2520with%2520reducing%2520infractions%252C%250Ayielding%2520a%2520safer%252C%2520high-performance%2520driving%2520model.%2520Notably%252C%2520our%2520monocular%252C%250Anon-ensemble%2520model%2520surpasses%2520the%2520top-performing%2520approaches%2520from%2520the%2520CARLA%250ALeaderboard%2520by%2520achieving%2520lower%2520infraction%2520scores%2520and%2520the%2520highest%2520route%250Acompletion%2520rate%252C%2520all%2520while%2520ensuring%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18898v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Decision-Making%20for%20End-to-End%20Autonomous%20Driving&entry.906535625=Mona%20Mirzaie%20and%20Bodo%20Rosenhahn&entry.1292438233=%20%20Trustworthy%20AI%20is%20mandatory%20for%20the%20broad%20deployment%20of%20autonomous%20vehicles.%0AAlthough%20end-to-end%20approaches%20derive%20control%20commands%20directly%20from%20raw%20data%2C%0Ainterpreting%20these%20decisions%20remains%20challenging%2C%20especially%20in%20complex%20urban%0Ascenarios.%20This%20is%20mainly%20attributed%20to%20very%20deep%20neural%20networks%20with%0Anon-linear%20decision%20boundaries%2C%20making%20it%20challenging%20to%20grasp%20the%20logic%20behind%0AAI-driven%20decisions.%20This%20paper%20presents%20a%20method%20to%20enhance%20interpretability%0Awhile%20optimizing%20control%20commands%20in%20autonomous%20driving.%20To%20address%20this%2C%20we%0Apropose%20loss%20functions%20that%20promote%20the%20interpretability%20of%20our%20model%20by%0Agenerating%20sparse%20and%20localized%20feature%20maps.%20The%20feature%20activations%20allow%20us%0Ato%20explain%20which%20image%20regions%20contribute%20to%20the%20predicted%20control%20command.%20We%0Aconduct%20comprehensive%20ablation%20studies%20on%20the%20feature%20extraction%20step%20and%0Avalidate%20our%20method%20on%20the%20CARLA%20benchmarks.%20We%20also%20demonstrate%20that%20our%0Aapproach%20improves%20interpretability%2C%20which%20correlates%20with%20reducing%20infractions%2C%0Ayielding%20a%20safer%2C%20high-performance%20driving%20model.%20Notably%2C%20our%20monocular%2C%0Anon-ensemble%20model%20surpasses%20the%20top-performing%20approaches%20from%20the%20CARLA%0ALeaderboard%20by%20achieving%20lower%20infraction%20scores%20and%20the%20highest%20route%0Acompletion%20rate%2C%20all%20while%20ensuring%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18898v3&entry.124074799=Read"},
{"title": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent\n  Contextual Calibration", "author": "Yiyuan Pan and Zhe Liu and Hesheng Wang", "abstract": "  Autonomous exploration in complex multi-agent reinforcement learning (MARL)\nwith sparse rewards critically depends on providing agents with effective\nintrinsic motivation. While artificial curiosity offers a powerful\nself-supervised signal, it often confuses environmental stochasticity with\nmeaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform\nnovelty bias, treating all unexpected observations equally. However, peer\nbehavior novelty, which encode latent task dynamics, are often overlooked,\nresulting in suboptimal exploration in decentralized, communication-free MARL\nsettings. To this end, inspired by how human children adaptively calibrate\ntheir own exploratory behaviors via observing peers, we propose a novel\napproach to enhance multi-agent exploration. We introduce CERMIC, a principled\nframework that empowers agents to robustly filter noisy surprise signals and\nguide exploration by dynamically calibrating their intrinsic curiosity with\ninferred multi-agent context. Additionally, CERMIC generates\ntheoretically-grounded intrinsic rewards, encouraging agents to explore state\ntransitions with high information gain. We evaluate CERMIC on benchmark suites\nincluding VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that\nexploration with CERMIC significantly outperforms SoTA algorithms in\nsparse-reward environments.\n", "link": "http://arxiv.org/abs/2509.20648v2", "date": "2025-10-21", "relevancy": 1.6141, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5975}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5582}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wonder%20Wins%20Ways%3A%20Curiosity-Driven%20Exploration%20through%20Multi-Agent%0A%20%20Contextual%20Calibration&body=Title%3A%20Wonder%20Wins%20Ways%3A%20Curiosity-Driven%20Exploration%20through%20Multi-Agent%0A%20%20Contextual%20Calibration%0AAuthor%3A%20Yiyuan%20Pan%20and%20Zhe%20Liu%20and%20Hesheng%20Wang%0AAbstract%3A%20%20%20Autonomous%20exploration%20in%20complex%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Awith%20sparse%20rewards%20critically%20depends%20on%20providing%20agents%20with%20effective%0Aintrinsic%20motivation.%20While%20artificial%20curiosity%20offers%20a%20powerful%0Aself-supervised%20signal%2C%20it%20often%20confuses%20environmental%20stochasticity%20with%0Ameaningful%20novelty.%20Moreover%2C%20existing%20curiosity%20mechanisms%20exhibit%20a%20uniform%0Anovelty%20bias%2C%20treating%20all%20unexpected%20observations%20equally.%20However%2C%20peer%0Abehavior%20novelty%2C%20which%20encode%20latent%20task%20dynamics%2C%20are%20often%20overlooked%2C%0Aresulting%20in%20suboptimal%20exploration%20in%20decentralized%2C%20communication-free%20MARL%0Asettings.%20To%20this%20end%2C%20inspired%20by%20how%20human%20children%20adaptively%20calibrate%0Atheir%20own%20exploratory%20behaviors%20via%20observing%20peers%2C%20we%20propose%20a%20novel%0Aapproach%20to%20enhance%20multi-agent%20exploration.%20We%20introduce%20CERMIC%2C%20a%20principled%0Aframework%20that%20empowers%20agents%20to%20robustly%20filter%20noisy%20surprise%20signals%20and%0Aguide%20exploration%20by%20dynamically%20calibrating%20their%20intrinsic%20curiosity%20with%0Ainferred%20multi-agent%20context.%20Additionally%2C%20CERMIC%20generates%0Atheoretically-grounded%20intrinsic%20rewards%2C%20encouraging%20agents%20to%20explore%20state%0Atransitions%20with%20high%20information%20gain.%20We%20evaluate%20CERMIC%20on%20benchmark%20suites%0Aincluding%20VMAS%2C%20Meltingpot%2C%20and%20SMACv2.%20Empirical%20results%20demonstrate%20that%0Aexploration%20with%20CERMIC%20significantly%20outperforms%20SoTA%20algorithms%20in%0Asparse-reward%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20648v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWonder%2520Wins%2520Ways%253A%2520Curiosity-Driven%2520Exploration%2520through%2520Multi-Agent%250A%2520%2520Contextual%2520Calibration%26entry.906535625%3DYiyuan%2520Pan%2520and%2520Zhe%2520Liu%2520and%2520Hesheng%2520Wang%26entry.1292438233%3D%2520%2520Autonomous%2520exploration%2520in%2520complex%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%250Awith%2520sparse%2520rewards%2520critically%2520depends%2520on%2520providing%2520agents%2520with%2520effective%250Aintrinsic%2520motivation.%2520While%2520artificial%2520curiosity%2520offers%2520a%2520powerful%250Aself-supervised%2520signal%252C%2520it%2520often%2520confuses%2520environmental%2520stochasticity%2520with%250Ameaningful%2520novelty.%2520Moreover%252C%2520existing%2520curiosity%2520mechanisms%2520exhibit%2520a%2520uniform%250Anovelty%2520bias%252C%2520treating%2520all%2520unexpected%2520observations%2520equally.%2520However%252C%2520peer%250Abehavior%2520novelty%252C%2520which%2520encode%2520latent%2520task%2520dynamics%252C%2520are%2520often%2520overlooked%252C%250Aresulting%2520in%2520suboptimal%2520exploration%2520in%2520decentralized%252C%2520communication-free%2520MARL%250Asettings.%2520To%2520this%2520end%252C%2520inspired%2520by%2520how%2520human%2520children%2520adaptively%2520calibrate%250Atheir%2520own%2520exploratory%2520behaviors%2520via%2520observing%2520peers%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520to%2520enhance%2520multi-agent%2520exploration.%2520We%2520introduce%2520CERMIC%252C%2520a%2520principled%250Aframework%2520that%2520empowers%2520agents%2520to%2520robustly%2520filter%2520noisy%2520surprise%2520signals%2520and%250Aguide%2520exploration%2520by%2520dynamically%2520calibrating%2520their%2520intrinsic%2520curiosity%2520with%250Ainferred%2520multi-agent%2520context.%2520Additionally%252C%2520CERMIC%2520generates%250Atheoretically-grounded%2520intrinsic%2520rewards%252C%2520encouraging%2520agents%2520to%2520explore%2520state%250Atransitions%2520with%2520high%2520information%2520gain.%2520We%2520evaluate%2520CERMIC%2520on%2520benchmark%2520suites%250Aincluding%2520VMAS%252C%2520Meltingpot%252C%2520and%2520SMACv2.%2520Empirical%2520results%2520demonstrate%2520that%250Aexploration%2520with%2520CERMIC%2520significantly%2520outperforms%2520SoTA%2520algorithms%2520in%250Asparse-reward%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20648v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wonder%20Wins%20Ways%3A%20Curiosity-Driven%20Exploration%20through%20Multi-Agent%0A%20%20Contextual%20Calibration&entry.906535625=Yiyuan%20Pan%20and%20Zhe%20Liu%20and%20Hesheng%20Wang&entry.1292438233=%20%20Autonomous%20exploration%20in%20complex%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Awith%20sparse%20rewards%20critically%20depends%20on%20providing%20agents%20with%20effective%0Aintrinsic%20motivation.%20While%20artificial%20curiosity%20offers%20a%20powerful%0Aself-supervised%20signal%2C%20it%20often%20confuses%20environmental%20stochasticity%20with%0Ameaningful%20novelty.%20Moreover%2C%20existing%20curiosity%20mechanisms%20exhibit%20a%20uniform%0Anovelty%20bias%2C%20treating%20all%20unexpected%20observations%20equally.%20However%2C%20peer%0Abehavior%20novelty%2C%20which%20encode%20latent%20task%20dynamics%2C%20are%20often%20overlooked%2C%0Aresulting%20in%20suboptimal%20exploration%20in%20decentralized%2C%20communication-free%20MARL%0Asettings.%20To%20this%20end%2C%20inspired%20by%20how%20human%20children%20adaptively%20calibrate%0Atheir%20own%20exploratory%20behaviors%20via%20observing%20peers%2C%20we%20propose%20a%20novel%0Aapproach%20to%20enhance%20multi-agent%20exploration.%20We%20introduce%20CERMIC%2C%20a%20principled%0Aframework%20that%20empowers%20agents%20to%20robustly%20filter%20noisy%20surprise%20signals%20and%0Aguide%20exploration%20by%20dynamically%20calibrating%20their%20intrinsic%20curiosity%20with%0Ainferred%20multi-agent%20context.%20Additionally%2C%20CERMIC%20generates%0Atheoretically-grounded%20intrinsic%20rewards%2C%20encouraging%20agents%20to%20explore%20state%0Atransitions%20with%20high%20information%20gain.%20We%20evaluate%20CERMIC%20on%20benchmark%20suites%0Aincluding%20VMAS%2C%20Meltingpot%2C%20and%20SMACv2.%20Empirical%20results%20demonstrate%20that%0Aexploration%20with%20CERMIC%20significantly%20outperforms%20SoTA%20algorithms%20in%0Asparse-reward%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20648v2&entry.124074799=Read"},
{"title": "Stochastic Path Planning in Correlated Obstacle Fields", "author": "Li Zhou and Elvan Ceyhan", "abstract": "  We introduce the Stochastic Correlated Obstacle Scene (SCOS) problem, a\nnavigation setting with spatially correlated obstacles of uncertain blockage\nstatus, realistically constrained sensors that provide noisy readings and\ncostly disambiguation. Modeling the spatial correlation with Gaussian Random\nField (GRF), we develop Bayesian belief updates that refine blockage\nprobabilities, and use the posteriors to reduce search space for efficiency. To\nfind the optimal traversal policy, we propose a novel two-stage learning\nframework. An offline phase learns a robust base policy via optimistic policy\niteration augmented with information bonus to encourage exploration in\ninformative regions, followed by an online rollout policy with periodic base\nupdates via a Bayesian mechanism for information adaptation. This framework\nsupports both Monte Carlo point estimation and distributional reinforcement\nlearning (RL) to learn full cost distributions, leading to stronger uncertainty\nquantification. We establish theoretical benefits of correlation-aware updating\nand convergence property under posterior sampling. Comprehensive empirical\nevaluations across varying obstacle densities, sensor capabilities demonstrate\nconsistent performance gains over baselines. This framework addresses\nnavigation challenges in environments with adversarial interruptions or\nclustered natural hazards.\n", "link": "http://arxiv.org/abs/2509.19559v2", "date": "2025-10-21", "relevancy": 1.6106, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5622}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5548}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Path%20Planning%20in%20Correlated%20Obstacle%20Fields&body=Title%3A%20Stochastic%20Path%20Planning%20in%20Correlated%20Obstacle%20Fields%0AAuthor%3A%20Li%20Zhou%20and%20Elvan%20Ceyhan%0AAbstract%3A%20%20%20We%20introduce%20the%20Stochastic%20Correlated%20Obstacle%20Scene%20%28SCOS%29%20problem%2C%20a%0Anavigation%20setting%20with%20spatially%20correlated%20obstacles%20of%20uncertain%20blockage%0Astatus%2C%20realistically%20constrained%20sensors%20that%20provide%20noisy%20readings%20and%0Acostly%20disambiguation.%20Modeling%20the%20spatial%20correlation%20with%20Gaussian%20Random%0AField%20%28GRF%29%2C%20we%20develop%20Bayesian%20belief%20updates%20that%20refine%20blockage%0Aprobabilities%2C%20and%20use%20the%20posteriors%20to%20reduce%20search%20space%20for%20efficiency.%20To%0Afind%20the%20optimal%20traversal%20policy%2C%20we%20propose%20a%20novel%20two-stage%20learning%0Aframework.%20An%20offline%20phase%20learns%20a%20robust%20base%20policy%20via%20optimistic%20policy%0Aiteration%20augmented%20with%20information%20bonus%20to%20encourage%20exploration%20in%0Ainformative%20regions%2C%20followed%20by%20an%20online%20rollout%20policy%20with%20periodic%20base%0Aupdates%20via%20a%20Bayesian%20mechanism%20for%20information%20adaptation.%20This%20framework%0Asupports%20both%20Monte%20Carlo%20point%20estimation%20and%20distributional%20reinforcement%0Alearning%20%28RL%29%20to%20learn%20full%20cost%20distributions%2C%20leading%20to%20stronger%20uncertainty%0Aquantification.%20We%20establish%20theoretical%20benefits%20of%20correlation-aware%20updating%0Aand%20convergence%20property%20under%20posterior%20sampling.%20Comprehensive%20empirical%0Aevaluations%20across%20varying%20obstacle%20densities%2C%20sensor%20capabilities%20demonstrate%0Aconsistent%20performance%20gains%20over%20baselines.%20This%20framework%20addresses%0Anavigation%20challenges%20in%20environments%20with%20adversarial%20interruptions%20or%0Aclustered%20natural%20hazards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Path%2520Planning%2520in%2520Correlated%2520Obstacle%2520Fields%26entry.906535625%3DLi%2520Zhou%2520and%2520Elvan%2520Ceyhan%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Stochastic%2520Correlated%2520Obstacle%2520Scene%2520%2528SCOS%2529%2520problem%252C%2520a%250Anavigation%2520setting%2520with%2520spatially%2520correlated%2520obstacles%2520of%2520uncertain%2520blockage%250Astatus%252C%2520realistically%2520constrained%2520sensors%2520that%2520provide%2520noisy%2520readings%2520and%250Acostly%2520disambiguation.%2520Modeling%2520the%2520spatial%2520correlation%2520with%2520Gaussian%2520Random%250AField%2520%2528GRF%2529%252C%2520we%2520develop%2520Bayesian%2520belief%2520updates%2520that%2520refine%2520blockage%250Aprobabilities%252C%2520and%2520use%2520the%2520posteriors%2520to%2520reduce%2520search%2520space%2520for%2520efficiency.%2520To%250Afind%2520the%2520optimal%2520traversal%2520policy%252C%2520we%2520propose%2520a%2520novel%2520two-stage%2520learning%250Aframework.%2520An%2520offline%2520phase%2520learns%2520a%2520robust%2520base%2520policy%2520via%2520optimistic%2520policy%250Aiteration%2520augmented%2520with%2520information%2520bonus%2520to%2520encourage%2520exploration%2520in%250Ainformative%2520regions%252C%2520followed%2520by%2520an%2520online%2520rollout%2520policy%2520with%2520periodic%2520base%250Aupdates%2520via%2520a%2520Bayesian%2520mechanism%2520for%2520information%2520adaptation.%2520This%2520framework%250Asupports%2520both%2520Monte%2520Carlo%2520point%2520estimation%2520and%2520distributional%2520reinforcement%250Alearning%2520%2528RL%2529%2520to%2520learn%2520full%2520cost%2520distributions%252C%2520leading%2520to%2520stronger%2520uncertainty%250Aquantification.%2520We%2520establish%2520theoretical%2520benefits%2520of%2520correlation-aware%2520updating%250Aand%2520convergence%2520property%2520under%2520posterior%2520sampling.%2520Comprehensive%2520empirical%250Aevaluations%2520across%2520varying%2520obstacle%2520densities%252C%2520sensor%2520capabilities%2520demonstrate%250Aconsistent%2520performance%2520gains%2520over%2520baselines.%2520This%2520framework%2520addresses%250Anavigation%2520challenges%2520in%2520environments%2520with%2520adversarial%2520interruptions%2520or%250Aclustered%2520natural%2520hazards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Path%20Planning%20in%20Correlated%20Obstacle%20Fields&entry.906535625=Li%20Zhou%20and%20Elvan%20Ceyhan&entry.1292438233=%20%20We%20introduce%20the%20Stochastic%20Correlated%20Obstacle%20Scene%20%28SCOS%29%20problem%2C%20a%0Anavigation%20setting%20with%20spatially%20correlated%20obstacles%20of%20uncertain%20blockage%0Astatus%2C%20realistically%20constrained%20sensors%20that%20provide%20noisy%20readings%20and%0Acostly%20disambiguation.%20Modeling%20the%20spatial%20correlation%20with%20Gaussian%20Random%0AField%20%28GRF%29%2C%20we%20develop%20Bayesian%20belief%20updates%20that%20refine%20blockage%0Aprobabilities%2C%20and%20use%20the%20posteriors%20to%20reduce%20search%20space%20for%20efficiency.%20To%0Afind%20the%20optimal%20traversal%20policy%2C%20we%20propose%20a%20novel%20two-stage%20learning%0Aframework.%20An%20offline%20phase%20learns%20a%20robust%20base%20policy%20via%20optimistic%20policy%0Aiteration%20augmented%20with%20information%20bonus%20to%20encourage%20exploration%20in%0Ainformative%20regions%2C%20followed%20by%20an%20online%20rollout%20policy%20with%20periodic%20base%0Aupdates%20via%20a%20Bayesian%20mechanism%20for%20information%20adaptation.%20This%20framework%0Asupports%20both%20Monte%20Carlo%20point%20estimation%20and%20distributional%20reinforcement%0Alearning%20%28RL%29%20to%20learn%20full%20cost%20distributions%2C%20leading%20to%20stronger%20uncertainty%0Aquantification.%20We%20establish%20theoretical%20benefits%20of%20correlation-aware%20updating%0Aand%20convergence%20property%20under%20posterior%20sampling.%20Comprehensive%20empirical%0Aevaluations%20across%20varying%20obstacle%20densities%2C%20sensor%20capabilities%20demonstrate%0Aconsistent%20performance%20gains%20over%20baselines.%20This%20framework%20addresses%0Anavigation%20challenges%20in%20environments%20with%20adversarial%20interruptions%20or%0Aclustered%20natural%20hazards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19559v2&entry.124074799=Read"},
{"title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and\n  Segmentation", "author": "Patterson Hsieh and Jerry Yeh and Mao-Chi He and Wen-Han Hsieh and Elvis Hsieh", "abstract": "  Climate change is intensifying the occurrence of harmful algal bloom (HAB),\nparticularly cyanobacteria, which threaten aquatic ecosystems and human health\nthrough oxygen depletion, toxin release, and disruption of marine biodiversity.\nTraditional monitoring approaches, such as manual water sampling, remain\nlabor-intensive and limited in spatial and temporal coverage. Recent advances\nin vision-language models (VLMs) for remote sensing have shown potential for\nscalable AI-driven solutions, yet challenges remain in reasoning over imagery\nand quantifying bloom severity. In this work, we introduce ALGae Observation\nand Segmentation (ALGOS), a segmentation-and-reasoning system for HAB\nmonitoring that combines remote sensing image understanding with severity\nestimation. Our approach integrates GeoSAM-assisted human evaluation for\nhigh-quality segmentation mask curation and fine-tunes vision language model on\nseverity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)\nfrom NASA. Experiments demonstrate that ALGOS achieves robust performance on\nboth segmentation and severity-level estimation, paving the way toward\npractical and automated cyanobacterial monitoring systems.\n", "link": "http://arxiv.org/abs/2510.18751v1", "date": "2025-10-21", "relevancy": 1.583, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5362}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5211}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seg%20the%20HAB%3A%20Language-Guided%20Geospatial%20Algae%20Bloom%20Reasoning%20and%0A%20%20Segmentation&body=Title%3A%20Seg%20the%20HAB%3A%20Language-Guided%20Geospatial%20Algae%20Bloom%20Reasoning%20and%0A%20%20Segmentation%0AAuthor%3A%20Patterson%20Hsieh%20and%20Jerry%20Yeh%20and%20Mao-Chi%20He%20and%20Wen-Han%20Hsieh%20and%20Elvis%20Hsieh%0AAbstract%3A%20%20%20Climate%20change%20is%20intensifying%20the%20occurrence%20of%20harmful%20algal%20bloom%20%28HAB%29%2C%0Aparticularly%20cyanobacteria%2C%20which%20threaten%20aquatic%20ecosystems%20and%20human%20health%0Athrough%20oxygen%20depletion%2C%20toxin%20release%2C%20and%20disruption%20of%20marine%20biodiversity.%0ATraditional%20monitoring%20approaches%2C%20such%20as%20manual%20water%20sampling%2C%20remain%0Alabor-intensive%20and%20limited%20in%20spatial%20and%20temporal%20coverage.%20Recent%20advances%0Ain%20vision-language%20models%20%28VLMs%29%20for%20remote%20sensing%20have%20shown%20potential%20for%0Ascalable%20AI-driven%20solutions%2C%20yet%20challenges%20remain%20in%20reasoning%20over%20imagery%0Aand%20quantifying%20bloom%20severity.%20In%20this%20work%2C%20we%20introduce%20ALGae%20Observation%0Aand%20Segmentation%20%28ALGOS%29%2C%20a%20segmentation-and-reasoning%20system%20for%20HAB%0Amonitoring%20that%20combines%20remote%20sensing%20image%20understanding%20with%20severity%0Aestimation.%20Our%20approach%20integrates%20GeoSAM-assisted%20human%20evaluation%20for%0Ahigh-quality%20segmentation%20mask%20curation%20and%20fine-tunes%20vision%20language%20model%20on%0Aseverity%20prediction%20using%20the%20Cyanobacteria%20Aggregated%20Manual%20Labels%20%28CAML%29%0Afrom%20NASA.%20Experiments%20demonstrate%20that%20ALGOS%20achieves%20robust%20performance%20on%0Aboth%20segmentation%20and%20severity-level%20estimation%2C%20paving%20the%20way%20toward%0Apractical%20and%20automated%20cyanobacterial%20monitoring%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeg%2520the%2520HAB%253A%2520Language-Guided%2520Geospatial%2520Algae%2520Bloom%2520Reasoning%2520and%250A%2520%2520Segmentation%26entry.906535625%3DPatterson%2520Hsieh%2520and%2520Jerry%2520Yeh%2520and%2520Mao-Chi%2520He%2520and%2520Wen-Han%2520Hsieh%2520and%2520Elvis%2520Hsieh%26entry.1292438233%3D%2520%2520Climate%2520change%2520is%2520intensifying%2520the%2520occurrence%2520of%2520harmful%2520algal%2520bloom%2520%2528HAB%2529%252C%250Aparticularly%2520cyanobacteria%252C%2520which%2520threaten%2520aquatic%2520ecosystems%2520and%2520human%2520health%250Athrough%2520oxygen%2520depletion%252C%2520toxin%2520release%252C%2520and%2520disruption%2520of%2520marine%2520biodiversity.%250ATraditional%2520monitoring%2520approaches%252C%2520such%2520as%2520manual%2520water%2520sampling%252C%2520remain%250Alabor-intensive%2520and%2520limited%2520in%2520spatial%2520and%2520temporal%2520coverage.%2520Recent%2520advances%250Ain%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520remote%2520sensing%2520have%2520shown%2520potential%2520for%250Ascalable%2520AI-driven%2520solutions%252C%2520yet%2520challenges%2520remain%2520in%2520reasoning%2520over%2520imagery%250Aand%2520quantifying%2520bloom%2520severity.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ALGae%2520Observation%250Aand%2520Segmentation%2520%2528ALGOS%2529%252C%2520a%2520segmentation-and-reasoning%2520system%2520for%2520HAB%250Amonitoring%2520that%2520combines%2520remote%2520sensing%2520image%2520understanding%2520with%2520severity%250Aestimation.%2520Our%2520approach%2520integrates%2520GeoSAM-assisted%2520human%2520evaluation%2520for%250Ahigh-quality%2520segmentation%2520mask%2520curation%2520and%2520fine-tunes%2520vision%2520language%2520model%2520on%250Aseverity%2520prediction%2520using%2520the%2520Cyanobacteria%2520Aggregated%2520Manual%2520Labels%2520%2528CAML%2529%250Afrom%2520NASA.%2520Experiments%2520demonstrate%2520that%2520ALGOS%2520achieves%2520robust%2520performance%2520on%250Aboth%2520segmentation%2520and%2520severity-level%2520estimation%252C%2520paving%2520the%2520way%2520toward%250Apractical%2520and%2520automated%2520cyanobacterial%2520monitoring%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seg%20the%20HAB%3A%20Language-Guided%20Geospatial%20Algae%20Bloom%20Reasoning%20and%0A%20%20Segmentation&entry.906535625=Patterson%20Hsieh%20and%20Jerry%20Yeh%20and%20Mao-Chi%20He%20and%20Wen-Han%20Hsieh%20and%20Elvis%20Hsieh&entry.1292438233=%20%20Climate%20change%20is%20intensifying%20the%20occurrence%20of%20harmful%20algal%20bloom%20%28HAB%29%2C%0Aparticularly%20cyanobacteria%2C%20which%20threaten%20aquatic%20ecosystems%20and%20human%20health%0Athrough%20oxygen%20depletion%2C%20toxin%20release%2C%20and%20disruption%20of%20marine%20biodiversity.%0ATraditional%20monitoring%20approaches%2C%20such%20as%20manual%20water%20sampling%2C%20remain%0Alabor-intensive%20and%20limited%20in%20spatial%20and%20temporal%20coverage.%20Recent%20advances%0Ain%20vision-language%20models%20%28VLMs%29%20for%20remote%20sensing%20have%20shown%20potential%20for%0Ascalable%20AI-driven%20solutions%2C%20yet%20challenges%20remain%20in%20reasoning%20over%20imagery%0Aand%20quantifying%20bloom%20severity.%20In%20this%20work%2C%20we%20introduce%20ALGae%20Observation%0Aand%20Segmentation%20%28ALGOS%29%2C%20a%20segmentation-and-reasoning%20system%20for%20HAB%0Amonitoring%20that%20combines%20remote%20sensing%20image%20understanding%20with%20severity%0Aestimation.%20Our%20approach%20integrates%20GeoSAM-assisted%20human%20evaluation%20for%0Ahigh-quality%20segmentation%20mask%20curation%20and%20fine-tunes%20vision%20language%20model%20on%0Aseverity%20prediction%20using%20the%20Cyanobacteria%20Aggregated%20Manual%20Labels%20%28CAML%29%0Afrom%20NASA.%20Experiments%20demonstrate%20that%20ALGOS%20achieves%20robust%20performance%20on%0Aboth%20segmentation%20and%20severity-level%20estimation%2C%20paving%20the%20way%20toward%0Apractical%20and%20automated%20cyanobacterial%20monitoring%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18751v1&entry.124074799=Read"},
{"title": "In-Context Learning of Stochastic Differential Equations with Foundation\n  Inference Models", "author": "Patrick Seifner and Kostadin Cvejoski and David Berghaus and Cesar Ojeda and Ramses J. Sanchez", "abstract": "  Stochastic differential equations (SDEs) describe dynamical systems where\ndeterministic flows, governed by a drift function, are superimposed with random\nfluctuations, dictated by a diffusion function. The accurate estimation (or\ndiscovery) of these functions from data is a central problem in machine\nlearning, with wide application across the natural and social sciences. Yet\ncurrent solutions either rely heavily on prior knowledge of the dynamics or\ninvolve intricate training procedures. We introduce FIM-SDE (Foundation\nInference Model for SDEs), a pretrained recognition model that delivers\naccurate in-context (or zero-shot) estimation of the drift and diffusion\nfunctions of low-dimensional SDEs, from noisy time series data, and allows\nrapid finetuning to target datasets. Leveraging concepts from amortized\ninference and neural operators, we (pre)train FIM-SDE in a supervised fashion\nto map a large set of noisy, discretely observed SDE paths onto the space of\ndrift and diffusion functions. We demonstrate that FIM-SDE achieves robust\nin-context function estimation across a wide range of synthetic and real-world\nprocesses -- from canonical SDE systems (e.g., double-well dynamics or weakly\nperturbed Lorenz attractors) to stock price recordings and oil-price and\nwind-speed fluctuations -- while matching the performance of symbolic, Gaussian\nprocess and Neural SDE baselines trained on the target datasets. When finetuned\nto the target processes, we show that FIM-SDE consistently outperforms all\nthese baselines.\n", "link": "http://arxiv.org/abs/2502.19049v2", "date": "2025-10-21", "relevancy": 1.5628, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5566}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5219}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Learning%20of%20Stochastic%20Differential%20Equations%20with%20Foundation%0A%20%20Inference%20Models&body=Title%3A%20In-Context%20Learning%20of%20Stochastic%20Differential%20Equations%20with%20Foundation%0A%20%20Inference%20Models%0AAuthor%3A%20Patrick%20Seifner%20and%20Kostadin%20Cvejoski%20and%20David%20Berghaus%20and%20Cesar%20Ojeda%20and%20Ramses%20J.%20Sanchez%0AAbstract%3A%20%20%20Stochastic%20differential%20equations%20%28SDEs%29%20describe%20dynamical%20systems%20where%0Adeterministic%20flows%2C%20governed%20by%20a%20drift%20function%2C%20are%20superimposed%20with%20random%0Afluctuations%2C%20dictated%20by%20a%20diffusion%20function.%20The%20accurate%20estimation%20%28or%0Adiscovery%29%20of%20these%20functions%20from%20data%20is%20a%20central%20problem%20in%20machine%0Alearning%2C%20with%20wide%20application%20across%20the%20natural%20and%20social%20sciences.%20Yet%0Acurrent%20solutions%20either%20rely%20heavily%20on%20prior%20knowledge%20of%20the%20dynamics%20or%0Ainvolve%20intricate%20training%20procedures.%20We%20introduce%20FIM-SDE%20%28Foundation%0AInference%20Model%20for%20SDEs%29%2C%20a%20pretrained%20recognition%20model%20that%20delivers%0Aaccurate%20in-context%20%28or%20zero-shot%29%20estimation%20of%20the%20drift%20and%20diffusion%0Afunctions%20of%20low-dimensional%20SDEs%2C%20from%20noisy%20time%20series%20data%2C%20and%20allows%0Arapid%20finetuning%20to%20target%20datasets.%20Leveraging%20concepts%20from%20amortized%0Ainference%20and%20neural%20operators%2C%20we%20%28pre%29train%20FIM-SDE%20in%20a%20supervised%20fashion%0Ato%20map%20a%20large%20set%20of%20noisy%2C%20discretely%20observed%20SDE%20paths%20onto%20the%20space%20of%0Adrift%20and%20diffusion%20functions.%20We%20demonstrate%20that%20FIM-SDE%20achieves%20robust%0Ain-context%20function%20estimation%20across%20a%20wide%20range%20of%20synthetic%20and%20real-world%0Aprocesses%20--%20from%20canonical%20SDE%20systems%20%28e.g.%2C%20double-well%20dynamics%20or%20weakly%0Aperturbed%20Lorenz%20attractors%29%20to%20stock%20price%20recordings%20and%20oil-price%20and%0Awind-speed%20fluctuations%20--%20while%20matching%20the%20performance%20of%20symbolic%2C%20Gaussian%0Aprocess%20and%20Neural%20SDE%20baselines%20trained%20on%20the%20target%20datasets.%20When%20finetuned%0Ato%20the%20target%20processes%2C%20we%20show%20that%20FIM-SDE%20consistently%20outperforms%20all%0Athese%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Learning%2520of%2520Stochastic%2520Differential%2520Equations%2520with%2520Foundation%250A%2520%2520Inference%2520Models%26entry.906535625%3DPatrick%2520Seifner%2520and%2520Kostadin%2520Cvejoski%2520and%2520David%2520Berghaus%2520and%2520Cesar%2520Ojeda%2520and%2520Ramses%2520J.%2520Sanchez%26entry.1292438233%3D%2520%2520Stochastic%2520differential%2520equations%2520%2528SDEs%2529%2520describe%2520dynamical%2520systems%2520where%250Adeterministic%2520flows%252C%2520governed%2520by%2520a%2520drift%2520function%252C%2520are%2520superimposed%2520with%2520random%250Afluctuations%252C%2520dictated%2520by%2520a%2520diffusion%2520function.%2520The%2520accurate%2520estimation%2520%2528or%250Adiscovery%2529%2520of%2520these%2520functions%2520from%2520data%2520is%2520a%2520central%2520problem%2520in%2520machine%250Alearning%252C%2520with%2520wide%2520application%2520across%2520the%2520natural%2520and%2520social%2520sciences.%2520Yet%250Acurrent%2520solutions%2520either%2520rely%2520heavily%2520on%2520prior%2520knowledge%2520of%2520the%2520dynamics%2520or%250Ainvolve%2520intricate%2520training%2520procedures.%2520We%2520introduce%2520FIM-SDE%2520%2528Foundation%250AInference%2520Model%2520for%2520SDEs%2529%252C%2520a%2520pretrained%2520recognition%2520model%2520that%2520delivers%250Aaccurate%2520in-context%2520%2528or%2520zero-shot%2529%2520estimation%2520of%2520the%2520drift%2520and%2520diffusion%250Afunctions%2520of%2520low-dimensional%2520SDEs%252C%2520from%2520noisy%2520time%2520series%2520data%252C%2520and%2520allows%250Arapid%2520finetuning%2520to%2520target%2520datasets.%2520Leveraging%2520concepts%2520from%2520amortized%250Ainference%2520and%2520neural%2520operators%252C%2520we%2520%2528pre%2529train%2520FIM-SDE%2520in%2520a%2520supervised%2520fashion%250Ato%2520map%2520a%2520large%2520set%2520of%2520noisy%252C%2520discretely%2520observed%2520SDE%2520paths%2520onto%2520the%2520space%2520of%250Adrift%2520and%2520diffusion%2520functions.%2520We%2520demonstrate%2520that%2520FIM-SDE%2520achieves%2520robust%250Ain-context%2520function%2520estimation%2520across%2520a%2520wide%2520range%2520of%2520synthetic%2520and%2520real-world%250Aprocesses%2520--%2520from%2520canonical%2520SDE%2520systems%2520%2528e.g.%252C%2520double-well%2520dynamics%2520or%2520weakly%250Aperturbed%2520Lorenz%2520attractors%2529%2520to%2520stock%2520price%2520recordings%2520and%2520oil-price%2520and%250Awind-speed%2520fluctuations%2520--%2520while%2520matching%2520the%2520performance%2520of%2520symbolic%252C%2520Gaussian%250Aprocess%2520and%2520Neural%2520SDE%2520baselines%2520trained%2520on%2520the%2520target%2520datasets.%2520When%2520finetuned%250Ato%2520the%2520target%2520processes%252C%2520we%2520show%2520that%2520FIM-SDE%2520consistently%2520outperforms%2520all%250Athese%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Learning%20of%20Stochastic%20Differential%20Equations%20with%20Foundation%0A%20%20Inference%20Models&entry.906535625=Patrick%20Seifner%20and%20Kostadin%20Cvejoski%20and%20David%20Berghaus%20and%20Cesar%20Ojeda%20and%20Ramses%20J.%20Sanchez&entry.1292438233=%20%20Stochastic%20differential%20equations%20%28SDEs%29%20describe%20dynamical%20systems%20where%0Adeterministic%20flows%2C%20governed%20by%20a%20drift%20function%2C%20are%20superimposed%20with%20random%0Afluctuations%2C%20dictated%20by%20a%20diffusion%20function.%20The%20accurate%20estimation%20%28or%0Adiscovery%29%20of%20these%20functions%20from%20data%20is%20a%20central%20problem%20in%20machine%0Alearning%2C%20with%20wide%20application%20across%20the%20natural%20and%20social%20sciences.%20Yet%0Acurrent%20solutions%20either%20rely%20heavily%20on%20prior%20knowledge%20of%20the%20dynamics%20or%0Ainvolve%20intricate%20training%20procedures.%20We%20introduce%20FIM-SDE%20%28Foundation%0AInference%20Model%20for%20SDEs%29%2C%20a%20pretrained%20recognition%20model%20that%20delivers%0Aaccurate%20in-context%20%28or%20zero-shot%29%20estimation%20of%20the%20drift%20and%20diffusion%0Afunctions%20of%20low-dimensional%20SDEs%2C%20from%20noisy%20time%20series%20data%2C%20and%20allows%0Arapid%20finetuning%20to%20target%20datasets.%20Leveraging%20concepts%20from%20amortized%0Ainference%20and%20neural%20operators%2C%20we%20%28pre%29train%20FIM-SDE%20in%20a%20supervised%20fashion%0Ato%20map%20a%20large%20set%20of%20noisy%2C%20discretely%20observed%20SDE%20paths%20onto%20the%20space%20of%0Adrift%20and%20diffusion%20functions.%20We%20demonstrate%20that%20FIM-SDE%20achieves%20robust%0Ain-context%20function%20estimation%20across%20a%20wide%20range%20of%20synthetic%20and%20real-world%0Aprocesses%20--%20from%20canonical%20SDE%20systems%20%28e.g.%2C%20double-well%20dynamics%20or%20weakly%0Aperturbed%20Lorenz%20attractors%29%20to%20stock%20price%20recordings%20and%20oil-price%20and%0Awind-speed%20fluctuations%20--%20while%20matching%20the%20performance%20of%20symbolic%2C%20Gaussian%0Aprocess%20and%20Neural%20SDE%20baselines%20trained%20on%20the%20target%20datasets.%20When%20finetuned%0Ato%20the%20target%20processes%2C%20we%20show%20that%20FIM-SDE%20consistently%20outperforms%20all%0Athese%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19049v2&entry.124074799=Read"},
{"title": "Viability of perturbative expansion for quantum field theories on\n  neurons", "author": "Srimoyee Sen and Varun Vaidya", "abstract": "  Neural Network (NN) architectures that break statistical independence of\nparameters have been proposed as a new approach for simulating local quantum\nfield theories (QFTs). In the infinite neuron number limit, single-layer NNs\ncan exactly reproduce QFT results. This paper examines the viability of this\narchitecture for perturbative calculations of local QFTs for finite neuron\nnumber $N$ using scalar $\\phi^4$ theory in $d$ Euclidean dimensions as an\nexample. We find that the renormalized $O(1/N)$ corrections to two- and\nfour-point correlators yield perturbative series which are sensitive to the\nultraviolet cut-off and therefore have a weak convergence. We propose a\nmodification to the architecture to improve this convergence and discuss\nconstraints on the parameters of the theory and the scaling of N which allow us\nto extract accurate field theory results.\n", "link": "http://arxiv.org/abs/2508.03810v3", "date": "2025-10-21", "relevancy": 1.5398, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4052}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3823}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Viability%20of%20perturbative%20expansion%20for%20quantum%20field%20theories%20on%0A%20%20neurons&body=Title%3A%20Viability%20of%20perturbative%20expansion%20for%20quantum%20field%20theories%20on%0A%20%20neurons%0AAuthor%3A%20Srimoyee%20Sen%20and%20Varun%20Vaidya%0AAbstract%3A%20%20%20Neural%20Network%20%28NN%29%20architectures%20that%20break%20statistical%20independence%20of%0Aparameters%20have%20been%20proposed%20as%20a%20new%20approach%20for%20simulating%20local%20quantum%0Afield%20theories%20%28QFTs%29.%20In%20the%20infinite%20neuron%20number%20limit%2C%20single-layer%20NNs%0Acan%20exactly%20reproduce%20QFT%20results.%20This%20paper%20examines%20the%20viability%20of%20this%0Aarchitecture%20for%20perturbative%20calculations%20of%20local%20QFTs%20for%20finite%20neuron%0Anumber%20%24N%24%20using%20scalar%20%24%5Cphi%5E4%24%20theory%20in%20%24d%24%20Euclidean%20dimensions%20as%20an%0Aexample.%20We%20find%20that%20the%20renormalized%20%24O%281/N%29%24%20corrections%20to%20two-%20and%0Afour-point%20correlators%20yield%20perturbative%20series%20which%20are%20sensitive%20to%20the%0Aultraviolet%20cut-off%20and%20therefore%20have%20a%20weak%20convergence.%20We%20propose%20a%0Amodification%20to%20the%20architecture%20to%20improve%20this%20convergence%20and%20discuss%0Aconstraints%20on%20the%20parameters%20of%20the%20theory%20and%20the%20scaling%20of%20N%20which%20allow%20us%0Ato%20extract%20accurate%20field%20theory%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03810v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViability%2520of%2520perturbative%2520expansion%2520for%2520quantum%2520field%2520theories%2520on%250A%2520%2520neurons%26entry.906535625%3DSrimoyee%2520Sen%2520and%2520Varun%2520Vaidya%26entry.1292438233%3D%2520%2520Neural%2520Network%2520%2528NN%2529%2520architectures%2520that%2520break%2520statistical%2520independence%2520of%250Aparameters%2520have%2520been%2520proposed%2520as%2520a%2520new%2520approach%2520for%2520simulating%2520local%2520quantum%250Afield%2520theories%2520%2528QFTs%2529.%2520In%2520the%2520infinite%2520neuron%2520number%2520limit%252C%2520single-layer%2520NNs%250Acan%2520exactly%2520reproduce%2520QFT%2520results.%2520This%2520paper%2520examines%2520the%2520viability%2520of%2520this%250Aarchitecture%2520for%2520perturbative%2520calculations%2520of%2520local%2520QFTs%2520for%2520finite%2520neuron%250Anumber%2520%2524N%2524%2520using%2520scalar%2520%2524%255Cphi%255E4%2524%2520theory%2520in%2520%2524d%2524%2520Euclidean%2520dimensions%2520as%2520an%250Aexample.%2520We%2520find%2520that%2520the%2520renormalized%2520%2524O%25281/N%2529%2524%2520corrections%2520to%2520two-%2520and%250Afour-point%2520correlators%2520yield%2520perturbative%2520series%2520which%2520are%2520sensitive%2520to%2520the%250Aultraviolet%2520cut-off%2520and%2520therefore%2520have%2520a%2520weak%2520convergence.%2520We%2520propose%2520a%250Amodification%2520to%2520the%2520architecture%2520to%2520improve%2520this%2520convergence%2520and%2520discuss%250Aconstraints%2520on%2520the%2520parameters%2520of%2520the%2520theory%2520and%2520the%2520scaling%2520of%2520N%2520which%2520allow%2520us%250Ato%2520extract%2520accurate%2520field%2520theory%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03810v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Viability%20of%20perturbative%20expansion%20for%20quantum%20field%20theories%20on%0A%20%20neurons&entry.906535625=Srimoyee%20Sen%20and%20Varun%20Vaidya&entry.1292438233=%20%20Neural%20Network%20%28NN%29%20architectures%20that%20break%20statistical%20independence%20of%0Aparameters%20have%20been%20proposed%20as%20a%20new%20approach%20for%20simulating%20local%20quantum%0Afield%20theories%20%28QFTs%29.%20In%20the%20infinite%20neuron%20number%20limit%2C%20single-layer%20NNs%0Acan%20exactly%20reproduce%20QFT%20results.%20This%20paper%20examines%20the%20viability%20of%20this%0Aarchitecture%20for%20perturbative%20calculations%20of%20local%20QFTs%20for%20finite%20neuron%0Anumber%20%24N%24%20using%20scalar%20%24%5Cphi%5E4%24%20theory%20in%20%24d%24%20Euclidean%20dimensions%20as%20an%0Aexample.%20We%20find%20that%20the%20renormalized%20%24O%281/N%29%24%20corrections%20to%20two-%20and%0Afour-point%20correlators%20yield%20perturbative%20series%20which%20are%20sensitive%20to%20the%0Aultraviolet%20cut-off%20and%20therefore%20have%20a%20weak%20convergence.%20We%20propose%20a%0Amodification%20to%20the%20architecture%20to%20improve%20this%20convergence%20and%20discuss%0Aconstraints%20on%20the%20parameters%20of%20the%20theory%20and%20the%20scaling%20of%20N%20which%20allow%20us%0Ato%20extract%20accurate%20field%20theory%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03810v3&entry.124074799=Read"},
{"title": "BO4Mob: Bayesian Optimization Benchmarks for High-Dimensional Urban\n  Mobility Problem", "author": "Seunghee Ryu and Donghoon Kwon and Seongjin Choi and Aryan Deshwal and Seungmo Kang and Carolina Osorio", "abstract": "  We introduce \\textbf{BO4Mob}, a new benchmark framework for high-dimensional\nBayesian Optimization (BO), driven by the challenge of origin-destination (OD)\ntravel demand estimation in large urban road networks. Estimating OD travel\ndemand from limited traffic sensor data is a difficult inverse optimization\nproblem, particularly in real-world, large-scale transportation networks. This\nproblem involves optimizing over high-dimensional continuous spaces where each\nobjective evaluation is computationally expensive, stochastic, and\nnon-differentiable. BO4Mob comprises five scenarios based on real-world San\nJose, CA road networks, with input dimensions scaling up to 10,100. These\nscenarios utilize high-resolution, open-source traffic simulations that\nincorporate realistic nonlinear and stochastic dynamics. We demonstrate the\nbenchmark's utility by evaluating five optimization methods: three\nstate-of-the-art BO algorithms and two non-BO baselines. This benchmark is\ndesigned to support both the development of scalable optimization algorithms\nand their application for the design of data-driven urban mobility models,\nincluding high-resolution digital twins of metropolitan road networks. Code and\ndocumentation are available at https://github.com/UMN-Choi-Lab/BO4Mob.\n", "link": "http://arxiv.org/abs/2510.18824v1", "date": "2025-10-21", "relevancy": 1.5283, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5528}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5085}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BO4Mob%3A%20Bayesian%20Optimization%20Benchmarks%20for%20High-Dimensional%20Urban%0A%20%20Mobility%20Problem&body=Title%3A%20BO4Mob%3A%20Bayesian%20Optimization%20Benchmarks%20for%20High-Dimensional%20Urban%0A%20%20Mobility%20Problem%0AAuthor%3A%20Seunghee%20Ryu%20and%20Donghoon%20Kwon%20and%20Seongjin%20Choi%20and%20Aryan%20Deshwal%20and%20Seungmo%20Kang%20and%20Carolina%20Osorio%0AAbstract%3A%20%20%20We%20introduce%20%5Ctextbf%7BBO4Mob%7D%2C%20a%20new%20benchmark%20framework%20for%20high-dimensional%0ABayesian%20Optimization%20%28BO%29%2C%20driven%20by%20the%20challenge%20of%20origin-destination%20%28OD%29%0Atravel%20demand%20estimation%20in%20large%20urban%20road%20networks.%20Estimating%20OD%20travel%0Ademand%20from%20limited%20traffic%20sensor%20data%20is%20a%20difficult%20inverse%20optimization%0Aproblem%2C%20particularly%20in%20real-world%2C%20large-scale%20transportation%20networks.%20This%0Aproblem%20involves%20optimizing%20over%20high-dimensional%20continuous%20spaces%20where%20each%0Aobjective%20evaluation%20is%20computationally%20expensive%2C%20stochastic%2C%20and%0Anon-differentiable.%20BO4Mob%20comprises%20five%20scenarios%20based%20on%20real-world%20San%0AJose%2C%20CA%20road%20networks%2C%20with%20input%20dimensions%20scaling%20up%20to%2010%2C100.%20These%0Ascenarios%20utilize%20high-resolution%2C%20open-source%20traffic%20simulations%20that%0Aincorporate%20realistic%20nonlinear%20and%20stochastic%20dynamics.%20We%20demonstrate%20the%0Abenchmark%27s%20utility%20by%20evaluating%20five%20optimization%20methods%3A%20three%0Astate-of-the-art%20BO%20algorithms%20and%20two%20non-BO%20baselines.%20This%20benchmark%20is%0Adesigned%20to%20support%20both%20the%20development%20of%20scalable%20optimization%20algorithms%0Aand%20their%20application%20for%20the%20design%20of%20data-driven%20urban%20mobility%20models%2C%0Aincluding%20high-resolution%20digital%20twins%20of%20metropolitan%20road%20networks.%20Code%20and%0Adocumentation%20are%20available%20at%20https%3A//github.com/UMN-Choi-Lab/BO4Mob.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBO4Mob%253A%2520Bayesian%2520Optimization%2520Benchmarks%2520for%2520High-Dimensional%2520Urban%250A%2520%2520Mobility%2520Problem%26entry.906535625%3DSeunghee%2520Ryu%2520and%2520Donghoon%2520Kwon%2520and%2520Seongjin%2520Choi%2520and%2520Aryan%2520Deshwal%2520and%2520Seungmo%2520Kang%2520and%2520Carolina%2520Osorio%26entry.1292438233%3D%2520%2520We%2520introduce%2520%255Ctextbf%257BBO4Mob%257D%252C%2520a%2520new%2520benchmark%2520framework%2520for%2520high-dimensional%250ABayesian%2520Optimization%2520%2528BO%2529%252C%2520driven%2520by%2520the%2520challenge%2520of%2520origin-destination%2520%2528OD%2529%250Atravel%2520demand%2520estimation%2520in%2520large%2520urban%2520road%2520networks.%2520Estimating%2520OD%2520travel%250Ademand%2520from%2520limited%2520traffic%2520sensor%2520data%2520is%2520a%2520difficult%2520inverse%2520optimization%250Aproblem%252C%2520particularly%2520in%2520real-world%252C%2520large-scale%2520transportation%2520networks.%2520This%250Aproblem%2520involves%2520optimizing%2520over%2520high-dimensional%2520continuous%2520spaces%2520where%2520each%250Aobjective%2520evaluation%2520is%2520computationally%2520expensive%252C%2520stochastic%252C%2520and%250Anon-differentiable.%2520BO4Mob%2520comprises%2520five%2520scenarios%2520based%2520on%2520real-world%2520San%250AJose%252C%2520CA%2520road%2520networks%252C%2520with%2520input%2520dimensions%2520scaling%2520up%2520to%252010%252C100.%2520These%250Ascenarios%2520utilize%2520high-resolution%252C%2520open-source%2520traffic%2520simulations%2520that%250Aincorporate%2520realistic%2520nonlinear%2520and%2520stochastic%2520dynamics.%2520We%2520demonstrate%2520the%250Abenchmark%2527s%2520utility%2520by%2520evaluating%2520five%2520optimization%2520methods%253A%2520three%250Astate-of-the-art%2520BO%2520algorithms%2520and%2520two%2520non-BO%2520baselines.%2520This%2520benchmark%2520is%250Adesigned%2520to%2520support%2520both%2520the%2520development%2520of%2520scalable%2520optimization%2520algorithms%250Aand%2520their%2520application%2520for%2520the%2520design%2520of%2520data-driven%2520urban%2520mobility%2520models%252C%250Aincluding%2520high-resolution%2520digital%2520twins%2520of%2520metropolitan%2520road%2520networks.%2520Code%2520and%250Adocumentation%2520are%2520available%2520at%2520https%253A//github.com/UMN-Choi-Lab/BO4Mob.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BO4Mob%3A%20Bayesian%20Optimization%20Benchmarks%20for%20High-Dimensional%20Urban%0A%20%20Mobility%20Problem&entry.906535625=Seunghee%20Ryu%20and%20Donghoon%20Kwon%20and%20Seongjin%20Choi%20and%20Aryan%20Deshwal%20and%20Seungmo%20Kang%20and%20Carolina%20Osorio&entry.1292438233=%20%20We%20introduce%20%5Ctextbf%7BBO4Mob%7D%2C%20a%20new%20benchmark%20framework%20for%20high-dimensional%0ABayesian%20Optimization%20%28BO%29%2C%20driven%20by%20the%20challenge%20of%20origin-destination%20%28OD%29%0Atravel%20demand%20estimation%20in%20large%20urban%20road%20networks.%20Estimating%20OD%20travel%0Ademand%20from%20limited%20traffic%20sensor%20data%20is%20a%20difficult%20inverse%20optimization%0Aproblem%2C%20particularly%20in%20real-world%2C%20large-scale%20transportation%20networks.%20This%0Aproblem%20involves%20optimizing%20over%20high-dimensional%20continuous%20spaces%20where%20each%0Aobjective%20evaluation%20is%20computationally%20expensive%2C%20stochastic%2C%20and%0Anon-differentiable.%20BO4Mob%20comprises%20five%20scenarios%20based%20on%20real-world%20San%0AJose%2C%20CA%20road%20networks%2C%20with%20input%20dimensions%20scaling%20up%20to%2010%2C100.%20These%0Ascenarios%20utilize%20high-resolution%2C%20open-source%20traffic%20simulations%20that%0Aincorporate%20realistic%20nonlinear%20and%20stochastic%20dynamics.%20We%20demonstrate%20the%0Abenchmark%27s%20utility%20by%20evaluating%20five%20optimization%20methods%3A%20three%0Astate-of-the-art%20BO%20algorithms%20and%20two%20non-BO%20baselines.%20This%20benchmark%20is%0Adesigned%20to%20support%20both%20the%20development%20of%20scalable%20optimization%20algorithms%0Aand%20their%20application%20for%20the%20design%20of%20data-driven%20urban%20mobility%20models%2C%0Aincluding%20high-resolution%20digital%20twins%20of%20metropolitan%20road%20networks.%20Code%20and%0Adocumentation%20are%20available%20at%20https%3A//github.com/UMN-Choi-Lab/BO4Mob.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18824v1&entry.124074799=Read"},
{"title": "Actor-Free Continuous Control via Structurally Maximizable Q-Functions", "author": "Yigit Korkmaz and Urvi Bhuwania and Ayush Jain and Erdem B\u0131y\u0131k", "abstract": "  Value-based algorithms are a cornerstone of off-policy reinforcement learning\ndue to their simplicity and training stability. However, their use has\ntraditionally been restricted to discrete action spaces, as they rely on\nestimating Q-values for individual state-action pairs. In continuous action\nspaces, evaluating the Q-value over the entire action space becomes\ncomputationally infeasible. To address this, actor-critic methods are typically\nemployed, where a critic is trained on off-policy data to estimate Q-values,\nand an actor is trained to maximize the critic's output. Despite their\npopularity, these methods often suffer from instability during training. In\nthis work, we propose a purely value-based framework for continuous control\nthat revisits structural maximization of Q-functions, introducing a set of key\narchitectural and algorithmic choices to enable efficient and stable learning.\nWe evaluate the proposed actor-free Q-learning approach on a range of standard\nsimulation tasks, demonstrating performance and sample efficiency on par with\nstate-of-the-art baselines, without the cost of learning a separate actor.\nParticularly, in environments with constrained action spaces, where the value\nfunctions are typically non-smooth, our method with structural maximization\noutperforms traditional actor-critic methods with gradient-based maximization.\nWe have released our code at https://github.com/USC-Lira/Q3C.\n", "link": "http://arxiv.org/abs/2510.18828v1", "date": "2025-10-21", "relevancy": 1.5187, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5149}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5118}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Actor-Free%20Continuous%20Control%20via%20Structurally%20Maximizable%20Q-Functions&body=Title%3A%20Actor-Free%20Continuous%20Control%20via%20Structurally%20Maximizable%20Q-Functions%0AAuthor%3A%20Yigit%20Korkmaz%20and%20Urvi%20Bhuwania%20and%20Ayush%20Jain%20and%20Erdem%20B%C4%B1y%C4%B1k%0AAbstract%3A%20%20%20Value-based%20algorithms%20are%20a%20cornerstone%20of%20off-policy%20reinforcement%20learning%0Adue%20to%20their%20simplicity%20and%20training%20stability.%20However%2C%20their%20use%20has%0Atraditionally%20been%20restricted%20to%20discrete%20action%20spaces%2C%20as%20they%20rely%20on%0Aestimating%20Q-values%20for%20individual%20state-action%20pairs.%20In%20continuous%20action%0Aspaces%2C%20evaluating%20the%20Q-value%20over%20the%20entire%20action%20space%20becomes%0Acomputationally%20infeasible.%20To%20address%20this%2C%20actor-critic%20methods%20are%20typically%0Aemployed%2C%20where%20a%20critic%20is%20trained%20on%20off-policy%20data%20to%20estimate%20Q-values%2C%0Aand%20an%20actor%20is%20trained%20to%20maximize%20the%20critic%27s%20output.%20Despite%20their%0Apopularity%2C%20these%20methods%20often%20suffer%20from%20instability%20during%20training.%20In%0Athis%20work%2C%20we%20propose%20a%20purely%20value-based%20framework%20for%20continuous%20control%0Athat%20revisits%20structural%20maximization%20of%20Q-functions%2C%20introducing%20a%20set%20of%20key%0Aarchitectural%20and%20algorithmic%20choices%20to%20enable%20efficient%20and%20stable%20learning.%0AWe%20evaluate%20the%20proposed%20actor-free%20Q-learning%20approach%20on%20a%20range%20of%20standard%0Asimulation%20tasks%2C%20demonstrating%20performance%20and%20sample%20efficiency%20on%20par%20with%0Astate-of-the-art%20baselines%2C%20without%20the%20cost%20of%20learning%20a%20separate%20actor.%0AParticularly%2C%20in%20environments%20with%20constrained%20action%20spaces%2C%20where%20the%20value%0Afunctions%20are%20typically%20non-smooth%2C%20our%20method%20with%20structural%20maximization%0Aoutperforms%20traditional%20actor-critic%20methods%20with%20gradient-based%20maximization.%0AWe%20have%20released%20our%20code%20at%20https%3A//github.com/USC-Lira/Q3C.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActor-Free%2520Continuous%2520Control%2520via%2520Structurally%2520Maximizable%2520Q-Functions%26entry.906535625%3DYigit%2520Korkmaz%2520and%2520Urvi%2520Bhuwania%2520and%2520Ayush%2520Jain%2520and%2520Erdem%2520B%25C4%25B1y%25C4%25B1k%26entry.1292438233%3D%2520%2520Value-based%2520algorithms%2520are%2520a%2520cornerstone%2520of%2520off-policy%2520reinforcement%2520learning%250Adue%2520to%2520their%2520simplicity%2520and%2520training%2520stability.%2520However%252C%2520their%2520use%2520has%250Atraditionally%2520been%2520restricted%2520to%2520discrete%2520action%2520spaces%252C%2520as%2520they%2520rely%2520on%250Aestimating%2520Q-values%2520for%2520individual%2520state-action%2520pairs.%2520In%2520continuous%2520action%250Aspaces%252C%2520evaluating%2520the%2520Q-value%2520over%2520the%2520entire%2520action%2520space%2520becomes%250Acomputationally%2520infeasible.%2520To%2520address%2520this%252C%2520actor-critic%2520methods%2520are%2520typically%250Aemployed%252C%2520where%2520a%2520critic%2520is%2520trained%2520on%2520off-policy%2520data%2520to%2520estimate%2520Q-values%252C%250Aand%2520an%2520actor%2520is%2520trained%2520to%2520maximize%2520the%2520critic%2527s%2520output.%2520Despite%2520their%250Apopularity%252C%2520these%2520methods%2520often%2520suffer%2520from%2520instability%2520during%2520training.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520purely%2520value-based%2520framework%2520for%2520continuous%2520control%250Athat%2520revisits%2520structural%2520maximization%2520of%2520Q-functions%252C%2520introducing%2520a%2520set%2520of%2520key%250Aarchitectural%2520and%2520algorithmic%2520choices%2520to%2520enable%2520efficient%2520and%2520stable%2520learning.%250AWe%2520evaluate%2520the%2520proposed%2520actor-free%2520Q-learning%2520approach%2520on%2520a%2520range%2520of%2520standard%250Asimulation%2520tasks%252C%2520demonstrating%2520performance%2520and%2520sample%2520efficiency%2520on%2520par%2520with%250Astate-of-the-art%2520baselines%252C%2520without%2520the%2520cost%2520of%2520learning%2520a%2520separate%2520actor.%250AParticularly%252C%2520in%2520environments%2520with%2520constrained%2520action%2520spaces%252C%2520where%2520the%2520value%250Afunctions%2520are%2520typically%2520non-smooth%252C%2520our%2520method%2520with%2520structural%2520maximization%250Aoutperforms%2520traditional%2520actor-critic%2520methods%2520with%2520gradient-based%2520maximization.%250AWe%2520have%2520released%2520our%2520code%2520at%2520https%253A//github.com/USC-Lira/Q3C.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Actor-Free%20Continuous%20Control%20via%20Structurally%20Maximizable%20Q-Functions&entry.906535625=Yigit%20Korkmaz%20and%20Urvi%20Bhuwania%20and%20Ayush%20Jain%20and%20Erdem%20B%C4%B1y%C4%B1k&entry.1292438233=%20%20Value-based%20algorithms%20are%20a%20cornerstone%20of%20off-policy%20reinforcement%20learning%0Adue%20to%20their%20simplicity%20and%20training%20stability.%20However%2C%20their%20use%20has%0Atraditionally%20been%20restricted%20to%20discrete%20action%20spaces%2C%20as%20they%20rely%20on%0Aestimating%20Q-values%20for%20individual%20state-action%20pairs.%20In%20continuous%20action%0Aspaces%2C%20evaluating%20the%20Q-value%20over%20the%20entire%20action%20space%20becomes%0Acomputationally%20infeasible.%20To%20address%20this%2C%20actor-critic%20methods%20are%20typically%0Aemployed%2C%20where%20a%20critic%20is%20trained%20on%20off-policy%20data%20to%20estimate%20Q-values%2C%0Aand%20an%20actor%20is%20trained%20to%20maximize%20the%20critic%27s%20output.%20Despite%20their%0Apopularity%2C%20these%20methods%20often%20suffer%20from%20instability%20during%20training.%20In%0Athis%20work%2C%20we%20propose%20a%20purely%20value-based%20framework%20for%20continuous%20control%0Athat%20revisits%20structural%20maximization%20of%20Q-functions%2C%20introducing%20a%20set%20of%20key%0Aarchitectural%20and%20algorithmic%20choices%20to%20enable%20efficient%20and%20stable%20learning.%0AWe%20evaluate%20the%20proposed%20actor-free%20Q-learning%20approach%20on%20a%20range%20of%20standard%0Asimulation%20tasks%2C%20demonstrating%20performance%20and%20sample%20efficiency%20on%20par%20with%0Astate-of-the-art%20baselines%2C%20without%20the%20cost%20of%20learning%20a%20separate%20actor.%0AParticularly%2C%20in%20environments%20with%20constrained%20action%20spaces%2C%20where%20the%20value%0Afunctions%20are%20typically%20non-smooth%2C%20our%20method%20with%20structural%20maximization%0Aoutperforms%20traditional%20actor-critic%20methods%20with%20gradient-based%20maximization.%0AWe%20have%20released%20our%20code%20at%20https%3A//github.com/USC-Lira/Q3C.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18828v1&entry.124074799=Read"},
{"title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations", "author": "Jinkun Chen and Sher Badshah and Xuemin Yu and Sijia Han", "abstract": "  What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. We call on the community to move beyond static paradigms and help\nshape the next generation of adaptive, socially-aware multi-agent simulations.\n", "link": "http://arxiv.org/abs/2510.13982v3", "date": "2025-10-21", "relevancy": 1.5153, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5292}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5132}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Static%20Sandboxes%20Are%20Inadequate%3A%20Modeling%20Societal%20Complexity%20Requires%0A%20%20Open-Ended%20Co-Evolution%20in%20LLM-Based%20Multi-Agent%20Simulations&body=Title%3A%20Static%20Sandboxes%20Are%20Inadequate%3A%20Modeling%20Societal%20Complexity%20Requires%0A%20%20Open-Ended%20Co-Evolution%20in%20LLM-Based%20Multi-Agent%20Simulations%0AAuthor%3A%20Jinkun%20Chen%20and%20Sher%20Badshah%20and%20Xuemin%20Yu%20and%20Sijia%20Han%0AAbstract%3A%20%20%20What%20if%20artificial%20agents%20could%20not%20just%20communicate%2C%20but%20also%20evolve%2C%20adapt%2C%0Aand%20reshape%20their%20worlds%20in%20ways%20we%20cannot%20fully%20predict%3F%20With%20llm%20now%20powering%0Amulti-agent%20systems%20and%20social%20simulations%2C%20we%20are%20witnessing%20new%20possibilities%0Afor%20modeling%20open-ended%2C%20ever-changing%20environments.%20Yet%2C%20most%20current%0Asimulations%20remain%20constrained%20within%20static%20sandboxes%2C%20characterized%20by%0Apredefined%20tasks%2C%20limited%20dynamics%2C%20and%20rigid%20evaluation%20criteria.%20These%0Alimitations%20prevent%20them%20from%20capturing%20the%20complexity%20of%20real-world%20societies.%0AIn%20this%20paper%2C%20we%20argue%20that%20static%2C%20task-specific%20benchmarks%20are%20fundamentally%0Ainadequate%20and%20must%20be%20rethought.%20We%20critically%20review%20emerging%20architectures%0Athat%20blend%20llm%20with%20multi-agent%20dynamics%2C%20highlight%20key%20hurdles%20such%20as%0Abalancing%20stability%20and%20diversity%2C%20evaluating%20unexpected%20behaviors%2C%20and%20scaling%0Ato%20greater%20complexity%2C%20and%20introduce%20a%20fresh%20taxonomy%20for%20this%20rapidly%20evolving%0Afield.%20Finally%2C%20we%20present%20a%20research%20roadmap%20centered%20on%20open-endedness%2C%0Acontinuous%20co-evolution%2C%20and%20the%20development%20of%20resilient%2C%20socially%20aligned%20AI%0Aecosystems.%20We%20call%20on%20the%20community%20to%20move%20beyond%20static%20paradigms%20and%20help%0Ashape%20the%20next%20generation%20of%20adaptive%2C%20socially-aware%20multi-agent%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13982v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatic%2520Sandboxes%2520Are%2520Inadequate%253A%2520Modeling%2520Societal%2520Complexity%2520Requires%250A%2520%2520Open-Ended%2520Co-Evolution%2520in%2520LLM-Based%2520Multi-Agent%2520Simulations%26entry.906535625%3DJinkun%2520Chen%2520and%2520Sher%2520Badshah%2520and%2520Xuemin%2520Yu%2520and%2520Sijia%2520Han%26entry.1292438233%3D%2520%2520What%2520if%2520artificial%2520agents%2520could%2520not%2520just%2520communicate%252C%2520but%2520also%2520evolve%252C%2520adapt%252C%250Aand%2520reshape%2520their%2520worlds%2520in%2520ways%2520we%2520cannot%2520fully%2520predict%253F%2520With%2520llm%2520now%2520powering%250Amulti-agent%2520systems%2520and%2520social%2520simulations%252C%2520we%2520are%2520witnessing%2520new%2520possibilities%250Afor%2520modeling%2520open-ended%252C%2520ever-changing%2520environments.%2520Yet%252C%2520most%2520current%250Asimulations%2520remain%2520constrained%2520within%2520static%2520sandboxes%252C%2520characterized%2520by%250Apredefined%2520tasks%252C%2520limited%2520dynamics%252C%2520and%2520rigid%2520evaluation%2520criteria.%2520These%250Alimitations%2520prevent%2520them%2520from%2520capturing%2520the%2520complexity%2520of%2520real-world%2520societies.%250AIn%2520this%2520paper%252C%2520we%2520argue%2520that%2520static%252C%2520task-specific%2520benchmarks%2520are%2520fundamentally%250Ainadequate%2520and%2520must%2520be%2520rethought.%2520We%2520critically%2520review%2520emerging%2520architectures%250Athat%2520blend%2520llm%2520with%2520multi-agent%2520dynamics%252C%2520highlight%2520key%2520hurdles%2520such%2520as%250Abalancing%2520stability%2520and%2520diversity%252C%2520evaluating%2520unexpected%2520behaviors%252C%2520and%2520scaling%250Ato%2520greater%2520complexity%252C%2520and%2520introduce%2520a%2520fresh%2520taxonomy%2520for%2520this%2520rapidly%2520evolving%250Afield.%2520Finally%252C%2520we%2520present%2520a%2520research%2520roadmap%2520centered%2520on%2520open-endedness%252C%250Acontinuous%2520co-evolution%252C%2520and%2520the%2520development%2520of%2520resilient%252C%2520socially%2520aligned%2520AI%250Aecosystems.%2520We%2520call%2520on%2520the%2520community%2520to%2520move%2520beyond%2520static%2520paradigms%2520and%2520help%250Ashape%2520the%2520next%2520generation%2520of%2520adaptive%252C%2520socially-aware%2520multi-agent%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13982v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Static%20Sandboxes%20Are%20Inadequate%3A%20Modeling%20Societal%20Complexity%20Requires%0A%20%20Open-Ended%20Co-Evolution%20in%20LLM-Based%20Multi-Agent%20Simulations&entry.906535625=Jinkun%20Chen%20and%20Sher%20Badshah%20and%20Xuemin%20Yu%20and%20Sijia%20Han&entry.1292438233=%20%20What%20if%20artificial%20agents%20could%20not%20just%20communicate%2C%20but%20also%20evolve%2C%20adapt%2C%0Aand%20reshape%20their%20worlds%20in%20ways%20we%20cannot%20fully%20predict%3F%20With%20llm%20now%20powering%0Amulti-agent%20systems%20and%20social%20simulations%2C%20we%20are%20witnessing%20new%20possibilities%0Afor%20modeling%20open-ended%2C%20ever-changing%20environments.%20Yet%2C%20most%20current%0Asimulations%20remain%20constrained%20within%20static%20sandboxes%2C%20characterized%20by%0Apredefined%20tasks%2C%20limited%20dynamics%2C%20and%20rigid%20evaluation%20criteria.%20These%0Alimitations%20prevent%20them%20from%20capturing%20the%20complexity%20of%20real-world%20societies.%0AIn%20this%20paper%2C%20we%20argue%20that%20static%2C%20task-specific%20benchmarks%20are%20fundamentally%0Ainadequate%20and%20must%20be%20rethought.%20We%20critically%20review%20emerging%20architectures%0Athat%20blend%20llm%20with%20multi-agent%20dynamics%2C%20highlight%20key%20hurdles%20such%20as%0Abalancing%20stability%20and%20diversity%2C%20evaluating%20unexpected%20behaviors%2C%20and%20scaling%0Ato%20greater%20complexity%2C%20and%20introduce%20a%20fresh%20taxonomy%20for%20this%20rapidly%20evolving%0Afield.%20Finally%2C%20we%20present%20a%20research%20roadmap%20centered%20on%20open-endedness%2C%0Acontinuous%20co-evolution%2C%20and%20the%20development%20of%20resilient%2C%20socially%20aligned%20AI%0Aecosystems.%20We%20call%20on%20the%20community%20to%20move%20beyond%20static%20paradigms%20and%20help%0Ashape%20the%20next%20generation%20of%20adaptive%2C%20socially-aware%20multi-agent%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13982v3&entry.124074799=Read"},
{"title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning", "author": "Chenghao Zhu and Meiling Tao and Tiannan Wang and Dongyi Ding and Yuchen Eleanor Jiang and Wangchunshu Zhou", "abstract": "  Faithfully personalizing large language models (LLMs) to align with\nindividual user preferences is a critical but challenging task. While\nsupervised fine-tuning (SFT) quickly reaches a performance plateau, standard\nreinforcement learning from human feedback (RLHF) also struggles with the\nnuances of personalization. Scalar-based reward models are prone to reward\nhacking which leads to verbose and superficially personalized responses. To\naddress these limitations, we propose Critique-Post-Edit, a robust\nreinforcement learning framework that enables more faithful and controllable\npersonalization. Our framework integrates two key components: (1) a\nPersonalized Generative Reward Model (GRM) that provides multi-dimensional\nscores and textual critiques to resist reward hacking, and (2) a\nCritique-Post-Edit mechanism where the policy model revises its own outputs\nbased on these critiques for more targeted and efficient learning. Under a\nrigorous length-controlled evaluation, our method substantially outperforms\nstandard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an\naverage 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses\nthe performance of GPT-4.1. These results demonstrate a practical path to\nfaithful, efficient, and controllable personalization.\n", "link": "http://arxiv.org/abs/2510.18849v1", "date": "2025-10-21", "relevancy": 1.4883, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5317}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.488}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Faithful%20and%20Controllable%20Personalization%20via%20Critique-Post-Edit%0A%20%20Reinforcement%20Learning&body=Title%3A%20Towards%20Faithful%20and%20Controllable%20Personalization%20via%20Critique-Post-Edit%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Chenghao%20Zhu%20and%20Meiling%20Tao%20and%20Tiannan%20Wang%20and%20Dongyi%20Ding%20and%20Yuchen%20Eleanor%20Jiang%20and%20Wangchunshu%20Zhou%0AAbstract%3A%20%20%20Faithfully%20personalizing%20large%20language%20models%20%28LLMs%29%20to%20align%20with%0Aindividual%20user%20preferences%20is%20a%20critical%20but%20challenging%20task.%20While%0Asupervised%20fine-tuning%20%28SFT%29%20quickly%20reaches%20a%20performance%20plateau%2C%20standard%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20also%20struggles%20with%20the%0Anuances%20of%20personalization.%20Scalar-based%20reward%20models%20are%20prone%20to%20reward%0Ahacking%20which%20leads%20to%20verbose%20and%20superficially%20personalized%20responses.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20Critique-Post-Edit%2C%20a%20robust%0Areinforcement%20learning%20framework%20that%20enables%20more%20faithful%20and%20controllable%0Apersonalization.%20Our%20framework%20integrates%20two%20key%20components%3A%20%281%29%20a%0APersonalized%20Generative%20Reward%20Model%20%28GRM%29%20that%20provides%20multi-dimensional%0Ascores%20and%20textual%20critiques%20to%20resist%20reward%20hacking%2C%20and%20%282%29%20a%0ACritique-Post-Edit%20mechanism%20where%20the%20policy%20model%20revises%20its%20own%20outputs%0Abased%20on%20these%20critiques%20for%20more%20targeted%20and%20efficient%20learning.%20Under%20a%0Arigorous%20length-controlled%20evaluation%2C%20our%20method%20substantially%20outperforms%0Astandard%20PPO%20on%20personalization%20benchmarks.%20Personalized%20Qwen2.5-7B%20achieves%20an%0Aaverage%2011%5C%25%20win-rate%20improvement%2C%20and%20personalized%20Qwen2.5-14B%20model%20surpasses%0Athe%20performance%20of%20GPT-4.1.%20These%20results%20demonstrate%20a%20practical%20path%20to%0Afaithful%2C%20efficient%2C%20and%20controllable%20personalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Faithful%2520and%2520Controllable%2520Personalization%2520via%2520Critique-Post-Edit%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DChenghao%2520Zhu%2520and%2520Meiling%2520Tao%2520and%2520Tiannan%2520Wang%2520and%2520Dongyi%2520Ding%2520and%2520Yuchen%2520Eleanor%2520Jiang%2520and%2520Wangchunshu%2520Zhou%26entry.1292438233%3D%2520%2520Faithfully%2520personalizing%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520align%2520with%250Aindividual%2520user%2520preferences%2520is%2520a%2520critical%2520but%2520challenging%2520task.%2520While%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520quickly%2520reaches%2520a%2520performance%2520plateau%252C%2520standard%250Areinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%2520also%2520struggles%2520with%2520the%250Anuances%2520of%2520personalization.%2520Scalar-based%2520reward%2520models%2520are%2520prone%2520to%2520reward%250Ahacking%2520which%2520leads%2520to%2520verbose%2520and%2520superficially%2520personalized%2520responses.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520Critique-Post-Edit%252C%2520a%2520robust%250Areinforcement%2520learning%2520framework%2520that%2520enables%2520more%2520faithful%2520and%2520controllable%250Apersonalization.%2520Our%2520framework%2520integrates%2520two%2520key%2520components%253A%2520%25281%2529%2520a%250APersonalized%2520Generative%2520Reward%2520Model%2520%2528GRM%2529%2520that%2520provides%2520multi-dimensional%250Ascores%2520and%2520textual%2520critiques%2520to%2520resist%2520reward%2520hacking%252C%2520and%2520%25282%2529%2520a%250ACritique-Post-Edit%2520mechanism%2520where%2520the%2520policy%2520model%2520revises%2520its%2520own%2520outputs%250Abased%2520on%2520these%2520critiques%2520for%2520more%2520targeted%2520and%2520efficient%2520learning.%2520Under%2520a%250Arigorous%2520length-controlled%2520evaluation%252C%2520our%2520method%2520substantially%2520outperforms%250Astandard%2520PPO%2520on%2520personalization%2520benchmarks.%2520Personalized%2520Qwen2.5-7B%2520achieves%2520an%250Aaverage%252011%255C%2525%2520win-rate%2520improvement%252C%2520and%2520personalized%2520Qwen2.5-14B%2520model%2520surpasses%250Athe%2520performance%2520of%2520GPT-4.1.%2520These%2520results%2520demonstrate%2520a%2520practical%2520path%2520to%250Afaithful%252C%2520efficient%252C%2520and%2520controllable%2520personalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Faithful%20and%20Controllable%20Personalization%20via%20Critique-Post-Edit%0A%20%20Reinforcement%20Learning&entry.906535625=Chenghao%20Zhu%20and%20Meiling%20Tao%20and%20Tiannan%20Wang%20and%20Dongyi%20Ding%20and%20Yuchen%20Eleanor%20Jiang%20and%20Wangchunshu%20Zhou&entry.1292438233=%20%20Faithfully%20personalizing%20large%20language%20models%20%28LLMs%29%20to%20align%20with%0Aindividual%20user%20preferences%20is%20a%20critical%20but%20challenging%20task.%20While%0Asupervised%20fine-tuning%20%28SFT%29%20quickly%20reaches%20a%20performance%20plateau%2C%20standard%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20also%20struggles%20with%20the%0Anuances%20of%20personalization.%20Scalar-based%20reward%20models%20are%20prone%20to%20reward%0Ahacking%20which%20leads%20to%20verbose%20and%20superficially%20personalized%20responses.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20Critique-Post-Edit%2C%20a%20robust%0Areinforcement%20learning%20framework%20that%20enables%20more%20faithful%20and%20controllable%0Apersonalization.%20Our%20framework%20integrates%20two%20key%20components%3A%20%281%29%20a%0APersonalized%20Generative%20Reward%20Model%20%28GRM%29%20that%20provides%20multi-dimensional%0Ascores%20and%20textual%20critiques%20to%20resist%20reward%20hacking%2C%20and%20%282%29%20a%0ACritique-Post-Edit%20mechanism%20where%20the%20policy%20model%20revises%20its%20own%20outputs%0Abased%20on%20these%20critiques%20for%20more%20targeted%20and%20efficient%20learning.%20Under%20a%0Arigorous%20length-controlled%20evaluation%2C%20our%20method%20substantially%20outperforms%0Astandard%20PPO%20on%20personalization%20benchmarks.%20Personalized%20Qwen2.5-7B%20achieves%20an%0Aaverage%2011%5C%25%20win-rate%20improvement%2C%20and%20personalized%20Qwen2.5-14B%20model%20surpasses%0Athe%20performance%20of%20GPT-4.1.%20These%20results%20demonstrate%20a%20practical%20path%20to%0Afaithful%2C%20efficient%2C%20and%20controllable%20personalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18849v1&entry.124074799=Read"},
{"title": "One-Pass Learning via Bridging Orthogonal Gradient Descent and Recursive\n  Least-Squares", "author": "Youngjae Min and Namhoon Cho and Navid Azizan", "abstract": "  While large machine learning models have shown remarkable performance in\nvarious domains, their training typically requires iterating for many passes\nover the training data. However, due to computational and memory constraints\nand potential privacy concerns, storing and accessing all the data is\nimpractical in many real-world scenarios where the data arrives in a stream. In\nthis paper, we investigate the problem of one-pass learning, in which a model\nis trained on sequentially arriving data without retraining on previous\ndatapoints. Motivated by the demonstrated effectiveness of overparameterized\nmodels and the phenomenon of benign overfitting, we propose Orthogonal\nRecursive Fitting (ORFit), an algorithm for one-pass learning which seeks to\nperfectly fit each new datapoint while minimally altering the predictions on\nprevious datapoints. ORFit updates the parameters in a direction orthogonal to\npast gradients, similar to orthogonal gradient descent (OGD) in continual\nlearning. We show that, interestingly, ORFit's update leads to an operation\nsimilar to the recursive least-squares (RLS) algorithm in adaptive filtering\nbut with significantly improved memory and computational efficiency, i.e.,\nlinear, instead of quadratic, in the number of parameters. To further reduce\nmemory usage, we leverage the structure of the streaming data via an\nincremental principal component analysis (IPCA). We show that using the\nprincipal components is minimax optimal, i.e., it minimizes the worst-case\nforgetting of previous predictions for unknown future updates. Further, we\nprove that, for overparameterized linear models, the parameter vector obtained\nby ORFit matches what the standard multi-pass stochastic gradient descent (SGD)\nwould converge to. Finally, we extend our results to the nonlinear setting for\nhighly overparameterized models, relevant for deep learning.\n", "link": "http://arxiv.org/abs/2207.13853v2", "date": "2025-10-21", "relevancy": 1.4817, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4998}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4948}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Pass%20Learning%20via%20Bridging%20Orthogonal%20Gradient%20Descent%20and%20Recursive%0A%20%20Least-Squares&body=Title%3A%20One-Pass%20Learning%20via%20Bridging%20Orthogonal%20Gradient%20Descent%20and%20Recursive%0A%20%20Least-Squares%0AAuthor%3A%20Youngjae%20Min%20and%20Namhoon%20Cho%20and%20Navid%20Azizan%0AAbstract%3A%20%20%20While%20large%20machine%20learning%20models%20have%20shown%20remarkable%20performance%20in%0Avarious%20domains%2C%20their%20training%20typically%20requires%20iterating%20for%20many%20passes%0Aover%20the%20training%20data.%20However%2C%20due%20to%20computational%20and%20memory%20constraints%0Aand%20potential%20privacy%20concerns%2C%20storing%20and%20accessing%20all%20the%20data%20is%0Aimpractical%20in%20many%20real-world%20scenarios%20where%20the%20data%20arrives%20in%20a%20stream.%20In%0Athis%20paper%2C%20we%20investigate%20the%20problem%20of%20one-pass%20learning%2C%20in%20which%20a%20model%0Ais%20trained%20on%20sequentially%20arriving%20data%20without%20retraining%20on%20previous%0Adatapoints.%20Motivated%20by%20the%20demonstrated%20effectiveness%20of%20overparameterized%0Amodels%20and%20the%20phenomenon%20of%20benign%20overfitting%2C%20we%20propose%20Orthogonal%0ARecursive%20Fitting%20%28ORFit%29%2C%20an%20algorithm%20for%20one-pass%20learning%20which%20seeks%20to%0Aperfectly%20fit%20each%20new%20datapoint%20while%20minimally%20altering%20the%20predictions%20on%0Aprevious%20datapoints.%20ORFit%20updates%20the%20parameters%20in%20a%20direction%20orthogonal%20to%0Apast%20gradients%2C%20similar%20to%20orthogonal%20gradient%20descent%20%28OGD%29%20in%20continual%0Alearning.%20We%20show%20that%2C%20interestingly%2C%20ORFit%27s%20update%20leads%20to%20an%20operation%0Asimilar%20to%20the%20recursive%20least-squares%20%28RLS%29%20algorithm%20in%20adaptive%20filtering%0Abut%20with%20significantly%20improved%20memory%20and%20computational%20efficiency%2C%20i.e.%2C%0Alinear%2C%20instead%20of%20quadratic%2C%20in%20the%20number%20of%20parameters.%20To%20further%20reduce%0Amemory%20usage%2C%20we%20leverage%20the%20structure%20of%20the%20streaming%20data%20via%20an%0Aincremental%20principal%20component%20analysis%20%28IPCA%29.%20We%20show%20that%20using%20the%0Aprincipal%20components%20is%20minimax%20optimal%2C%20i.e.%2C%20it%20minimizes%20the%20worst-case%0Aforgetting%20of%20previous%20predictions%20for%20unknown%20future%20updates.%20Further%2C%20we%0Aprove%20that%2C%20for%20overparameterized%20linear%20models%2C%20the%20parameter%20vector%20obtained%0Aby%20ORFit%20matches%20what%20the%20standard%20multi-pass%20stochastic%20gradient%20descent%20%28SGD%29%0Awould%20converge%20to.%20Finally%2C%20we%20extend%20our%20results%20to%20the%20nonlinear%20setting%20for%0Ahighly%20overparameterized%20models%2C%20relevant%20for%20deep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.13853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Pass%2520Learning%2520via%2520Bridging%2520Orthogonal%2520Gradient%2520Descent%2520and%2520Recursive%250A%2520%2520Least-Squares%26entry.906535625%3DYoungjae%2520Min%2520and%2520Namhoon%2520Cho%2520and%2520Navid%2520Azizan%26entry.1292438233%3D%2520%2520While%2520large%2520machine%2520learning%2520models%2520have%2520shown%2520remarkable%2520performance%2520in%250Avarious%2520domains%252C%2520their%2520training%2520typically%2520requires%2520iterating%2520for%2520many%2520passes%250Aover%2520the%2520training%2520data.%2520However%252C%2520due%2520to%2520computational%2520and%2520memory%2520constraints%250Aand%2520potential%2520privacy%2520concerns%252C%2520storing%2520and%2520accessing%2520all%2520the%2520data%2520is%250Aimpractical%2520in%2520many%2520real-world%2520scenarios%2520where%2520the%2520data%2520arrives%2520in%2520a%2520stream.%2520In%250Athis%2520paper%252C%2520we%2520investigate%2520the%2520problem%2520of%2520one-pass%2520learning%252C%2520in%2520which%2520a%2520model%250Ais%2520trained%2520on%2520sequentially%2520arriving%2520data%2520without%2520retraining%2520on%2520previous%250Adatapoints.%2520Motivated%2520by%2520the%2520demonstrated%2520effectiveness%2520of%2520overparameterized%250Amodels%2520and%2520the%2520phenomenon%2520of%2520benign%2520overfitting%252C%2520we%2520propose%2520Orthogonal%250ARecursive%2520Fitting%2520%2528ORFit%2529%252C%2520an%2520algorithm%2520for%2520one-pass%2520learning%2520which%2520seeks%2520to%250Aperfectly%2520fit%2520each%2520new%2520datapoint%2520while%2520minimally%2520altering%2520the%2520predictions%2520on%250Aprevious%2520datapoints.%2520ORFit%2520updates%2520the%2520parameters%2520in%2520a%2520direction%2520orthogonal%2520to%250Apast%2520gradients%252C%2520similar%2520to%2520orthogonal%2520gradient%2520descent%2520%2528OGD%2529%2520in%2520continual%250Alearning.%2520We%2520show%2520that%252C%2520interestingly%252C%2520ORFit%2527s%2520update%2520leads%2520to%2520an%2520operation%250Asimilar%2520to%2520the%2520recursive%2520least-squares%2520%2528RLS%2529%2520algorithm%2520in%2520adaptive%2520filtering%250Abut%2520with%2520significantly%2520improved%2520memory%2520and%2520computational%2520efficiency%252C%2520i.e.%252C%250Alinear%252C%2520instead%2520of%2520quadratic%252C%2520in%2520the%2520number%2520of%2520parameters.%2520To%2520further%2520reduce%250Amemory%2520usage%252C%2520we%2520leverage%2520the%2520structure%2520of%2520the%2520streaming%2520data%2520via%2520an%250Aincremental%2520principal%2520component%2520analysis%2520%2528IPCA%2529.%2520We%2520show%2520that%2520using%2520the%250Aprincipal%2520components%2520is%2520minimax%2520optimal%252C%2520i.e.%252C%2520it%2520minimizes%2520the%2520worst-case%250Aforgetting%2520of%2520previous%2520predictions%2520for%2520unknown%2520future%2520updates.%2520Further%252C%2520we%250Aprove%2520that%252C%2520for%2520overparameterized%2520linear%2520models%252C%2520the%2520parameter%2520vector%2520obtained%250Aby%2520ORFit%2520matches%2520what%2520the%2520standard%2520multi-pass%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%250Awould%2520converge%2520to.%2520Finally%252C%2520we%2520extend%2520our%2520results%2520to%2520the%2520nonlinear%2520setting%2520for%250Ahighly%2520overparameterized%2520models%252C%2520relevant%2520for%2520deep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.13853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Pass%20Learning%20via%20Bridging%20Orthogonal%20Gradient%20Descent%20and%20Recursive%0A%20%20Least-Squares&entry.906535625=Youngjae%20Min%20and%20Namhoon%20Cho%20and%20Navid%20Azizan&entry.1292438233=%20%20While%20large%20machine%20learning%20models%20have%20shown%20remarkable%20performance%20in%0Avarious%20domains%2C%20their%20training%20typically%20requires%20iterating%20for%20many%20passes%0Aover%20the%20training%20data.%20However%2C%20due%20to%20computational%20and%20memory%20constraints%0Aand%20potential%20privacy%20concerns%2C%20storing%20and%20accessing%20all%20the%20data%20is%0Aimpractical%20in%20many%20real-world%20scenarios%20where%20the%20data%20arrives%20in%20a%20stream.%20In%0Athis%20paper%2C%20we%20investigate%20the%20problem%20of%20one-pass%20learning%2C%20in%20which%20a%20model%0Ais%20trained%20on%20sequentially%20arriving%20data%20without%20retraining%20on%20previous%0Adatapoints.%20Motivated%20by%20the%20demonstrated%20effectiveness%20of%20overparameterized%0Amodels%20and%20the%20phenomenon%20of%20benign%20overfitting%2C%20we%20propose%20Orthogonal%0ARecursive%20Fitting%20%28ORFit%29%2C%20an%20algorithm%20for%20one-pass%20learning%20which%20seeks%20to%0Aperfectly%20fit%20each%20new%20datapoint%20while%20minimally%20altering%20the%20predictions%20on%0Aprevious%20datapoints.%20ORFit%20updates%20the%20parameters%20in%20a%20direction%20orthogonal%20to%0Apast%20gradients%2C%20similar%20to%20orthogonal%20gradient%20descent%20%28OGD%29%20in%20continual%0Alearning.%20We%20show%20that%2C%20interestingly%2C%20ORFit%27s%20update%20leads%20to%20an%20operation%0Asimilar%20to%20the%20recursive%20least-squares%20%28RLS%29%20algorithm%20in%20adaptive%20filtering%0Abut%20with%20significantly%20improved%20memory%20and%20computational%20efficiency%2C%20i.e.%2C%0Alinear%2C%20instead%20of%20quadratic%2C%20in%20the%20number%20of%20parameters.%20To%20further%20reduce%0Amemory%20usage%2C%20we%20leverage%20the%20structure%20of%20the%20streaming%20data%20via%20an%0Aincremental%20principal%20component%20analysis%20%28IPCA%29.%20We%20show%20that%20using%20the%0Aprincipal%20components%20is%20minimax%20optimal%2C%20i.e.%2C%20it%20minimizes%20the%20worst-case%0Aforgetting%20of%20previous%20predictions%20for%20unknown%20future%20updates.%20Further%2C%20we%0Aprove%20that%2C%20for%20overparameterized%20linear%20models%2C%20the%20parameter%20vector%20obtained%0Aby%20ORFit%20matches%20what%20the%20standard%20multi-pass%20stochastic%20gradient%20descent%20%28SGD%29%0Awould%20converge%20to.%20Finally%2C%20we%20extend%20our%20results%20to%20the%20nonlinear%20setting%20for%0Ahighly%20overparameterized%20models%2C%20relevant%20for%20deep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.13853v2&entry.124074799=Read"},
{"title": "A Frequentist Statistical Introduction to Variational Inference,\n  Autoencoders, and Diffusion Models", "author": "Yen-Chi Chen", "abstract": "  While Variational Inference (VI) is central to modern generative models like\nVariational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its\npedagogical treatment is split across disciplines. In statistics, VI is\ntypically framed as a Bayesian method for posterior approximation. In machine\nlearning, however, VAEs and DDMs are developed from a Frequentist viewpoint,\nwhere VI is used to approximate a maximum likelihood estimator. This creates a\nbarrier for statisticians, as the principles behind VAEs and DDMs are hard to\ncontextualize without a corresponding Frequentist introduction to VI. This\npaper provides that introduction: we explain the theory for VI, VAEs, and DDMs\nfrom a purely Frequentist perspective, starting with the classical\nExpectation-Maximization (EM) algorithm. We show how VI arises as a scalable\nsolution for intractable E-steps and how VAEs and DDMs are natural,\ndeep-learning-based extensions of this framework, thereby bridging the gap\nbetween classical statistical inference and modern generative AI.\n", "link": "http://arxiv.org/abs/2510.18777v1", "date": "2025-10-21", "relevancy": 1.4709, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Frequentist%20Statistical%20Introduction%20to%20Variational%20Inference%2C%0A%20%20Autoencoders%2C%20and%20Diffusion%20Models&body=Title%3A%20A%20Frequentist%20Statistical%20Introduction%20to%20Variational%20Inference%2C%0A%20%20Autoencoders%2C%20and%20Diffusion%20Models%0AAuthor%3A%20Yen-Chi%20Chen%0AAbstract%3A%20%20%20While%20Variational%20Inference%20%28VI%29%20is%20central%20to%20modern%20generative%20models%20like%0AVariational%20Autoencoders%20%28VAEs%29%20and%20Denoising%20Diffusion%20Models%20%28DDMs%29%2C%20its%0Apedagogical%20treatment%20is%20split%20across%20disciplines.%20In%20statistics%2C%20VI%20is%0Atypically%20framed%20as%20a%20Bayesian%20method%20for%20posterior%20approximation.%20In%20machine%0Alearning%2C%20however%2C%20VAEs%20and%20DDMs%20are%20developed%20from%20a%20Frequentist%20viewpoint%2C%0Awhere%20VI%20is%20used%20to%20approximate%20a%20maximum%20likelihood%20estimator.%20This%20creates%20a%0Abarrier%20for%20statisticians%2C%20as%20the%20principles%20behind%20VAEs%20and%20DDMs%20are%20hard%20to%0Acontextualize%20without%20a%20corresponding%20Frequentist%20introduction%20to%20VI.%20This%0Apaper%20provides%20that%20introduction%3A%20we%20explain%20the%20theory%20for%20VI%2C%20VAEs%2C%20and%20DDMs%0Afrom%20a%20purely%20Frequentist%20perspective%2C%20starting%20with%20the%20classical%0AExpectation-Maximization%20%28EM%29%20algorithm.%20We%20show%20how%20VI%20arises%20as%20a%20scalable%0Asolution%20for%20intractable%20E-steps%20and%20how%20VAEs%20and%20DDMs%20are%20natural%2C%0Adeep-learning-based%20extensions%20of%20this%20framework%2C%20thereby%20bridging%20the%20gap%0Abetween%20classical%20statistical%20inference%20and%20modern%20generative%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Frequentist%2520Statistical%2520Introduction%2520to%2520Variational%2520Inference%252C%250A%2520%2520Autoencoders%252C%2520and%2520Diffusion%2520Models%26entry.906535625%3DYen-Chi%2520Chen%26entry.1292438233%3D%2520%2520While%2520Variational%2520Inference%2520%2528VI%2529%2520is%2520central%2520to%2520modern%2520generative%2520models%2520like%250AVariational%2520Autoencoders%2520%2528VAEs%2529%2520and%2520Denoising%2520Diffusion%2520Models%2520%2528DDMs%2529%252C%2520its%250Apedagogical%2520treatment%2520is%2520split%2520across%2520disciplines.%2520In%2520statistics%252C%2520VI%2520is%250Atypically%2520framed%2520as%2520a%2520Bayesian%2520method%2520for%2520posterior%2520approximation.%2520In%2520machine%250Alearning%252C%2520however%252C%2520VAEs%2520and%2520DDMs%2520are%2520developed%2520from%2520a%2520Frequentist%2520viewpoint%252C%250Awhere%2520VI%2520is%2520used%2520to%2520approximate%2520a%2520maximum%2520likelihood%2520estimator.%2520This%2520creates%2520a%250Abarrier%2520for%2520statisticians%252C%2520as%2520the%2520principles%2520behind%2520VAEs%2520and%2520DDMs%2520are%2520hard%2520to%250Acontextualize%2520without%2520a%2520corresponding%2520Frequentist%2520introduction%2520to%2520VI.%2520This%250Apaper%2520provides%2520that%2520introduction%253A%2520we%2520explain%2520the%2520theory%2520for%2520VI%252C%2520VAEs%252C%2520and%2520DDMs%250Afrom%2520a%2520purely%2520Frequentist%2520perspective%252C%2520starting%2520with%2520the%2520classical%250AExpectation-Maximization%2520%2528EM%2529%2520algorithm.%2520We%2520show%2520how%2520VI%2520arises%2520as%2520a%2520scalable%250Asolution%2520for%2520intractable%2520E-steps%2520and%2520how%2520VAEs%2520and%2520DDMs%2520are%2520natural%252C%250Adeep-learning-based%2520extensions%2520of%2520this%2520framework%252C%2520thereby%2520bridging%2520the%2520gap%250Abetween%2520classical%2520statistical%2520inference%2520and%2520modern%2520generative%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Frequentist%20Statistical%20Introduction%20to%20Variational%20Inference%2C%0A%20%20Autoencoders%2C%20and%20Diffusion%20Models&entry.906535625=Yen-Chi%20Chen&entry.1292438233=%20%20While%20Variational%20Inference%20%28VI%29%20is%20central%20to%20modern%20generative%20models%20like%0AVariational%20Autoencoders%20%28VAEs%29%20and%20Denoising%20Diffusion%20Models%20%28DDMs%29%2C%20its%0Apedagogical%20treatment%20is%20split%20across%20disciplines.%20In%20statistics%2C%20VI%20is%0Atypically%20framed%20as%20a%20Bayesian%20method%20for%20posterior%20approximation.%20In%20machine%0Alearning%2C%20however%2C%20VAEs%20and%20DDMs%20are%20developed%20from%20a%20Frequentist%20viewpoint%2C%0Awhere%20VI%20is%20used%20to%20approximate%20a%20maximum%20likelihood%20estimator.%20This%20creates%20a%0Abarrier%20for%20statisticians%2C%20as%20the%20principles%20behind%20VAEs%20and%20DDMs%20are%20hard%20to%0Acontextualize%20without%20a%20corresponding%20Frequentist%20introduction%20to%20VI.%20This%0Apaper%20provides%20that%20introduction%3A%20we%20explain%20the%20theory%20for%20VI%2C%20VAEs%2C%20and%20DDMs%0Afrom%20a%20purely%20Frequentist%20perspective%2C%20starting%20with%20the%20classical%0AExpectation-Maximization%20%28EM%29%20algorithm.%20We%20show%20how%20VI%20arises%20as%20a%20scalable%0Asolution%20for%20intractable%20E-steps%20and%20how%20VAEs%20and%20DDMs%20are%20natural%2C%0Adeep-learning-based%20extensions%20of%20this%20framework%2C%20thereby%20bridging%20the%20gap%0Abetween%20classical%20statistical%20inference%20and%20modern%20generative%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18777v1&entry.124074799=Read"},
{"title": "Fourier Transform Multiple Instance Learning for Whole Slide Image\n  Classification", "author": "Anthony Bilic and Guangyu Sun and Ming Li and Md Sanzid Bin Hossain and Yu Tian and Wei Zhang and Laura Brattain and Dexter Hadley and Chen Chen", "abstract": "  Whole Slide Image (WSI) classification relies on Multiple Instance Learning\n(MIL) with spatial patch features, yet existing methods struggle to capture\nglobal dependencies due to the immense size of WSIs and the local nature of\npatch embeddings. This limitation hinders the modeling of coarse structures\nessential for robust diagnostic prediction. We propose Fourier Transform\nMultiple Instance Learning (FFT-MIL), a framework that augments MIL with a\nfrequency-domain branch to provide compact global context. Low-frequency crops\nare extracted from WSIs via the Fast Fourier Transform and processed through a\nmodular FFT-Block composed of convolutional layers and Min-Max normalization to\nmitigate the high variance of frequency data. The learned global frequency\nfeature is fused with spatial patch features through lightweight integration\nstrategies, enabling compatibility with diverse MIL architectures. FFT-MIL was\nevaluated across six state-of-the-art MIL methods on three public datasets\n(BRACS, LUAD, and IMP). Integration of the FFT-Block improved macro F1 scores\nby an average of 3.51% and AUC by 1.51%, demonstrating consistent gains across\narchitectures and datasets. These results establish frequency-domain learning\nas an effective and efficient mechanism for capturing global dependencies in\nWSI classification, complementing spatial features and advancing the\nscalability and accuracy of MIL-based computational pathology.\n", "link": "http://arxiv.org/abs/2510.15138v2", "date": "2025-10-21", "relevancy": 1.4621, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4974}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4855}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fourier%20Transform%20Multiple%20Instance%20Learning%20for%20Whole%20Slide%20Image%0A%20%20Classification&body=Title%3A%20Fourier%20Transform%20Multiple%20Instance%20Learning%20for%20Whole%20Slide%20Image%0A%20%20Classification%0AAuthor%3A%20Anthony%20Bilic%20and%20Guangyu%20Sun%20and%20Ming%20Li%20and%20Md%20Sanzid%20Bin%20Hossain%20and%20Yu%20Tian%20and%20Wei%20Zhang%20and%20Laura%20Brattain%20and%20Dexter%20Hadley%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Whole%20Slide%20Image%20%28WSI%29%20classification%20relies%20on%20Multiple%20Instance%20Learning%0A%28MIL%29%20with%20spatial%20patch%20features%2C%20yet%20existing%20methods%20struggle%20to%20capture%0Aglobal%20dependencies%20due%20to%20the%20immense%20size%20of%20WSIs%20and%20the%20local%20nature%20of%0Apatch%20embeddings.%20This%20limitation%20hinders%20the%20modeling%20of%20coarse%20structures%0Aessential%20for%20robust%20diagnostic%20prediction.%20We%20propose%20Fourier%20Transform%0AMultiple%20Instance%20Learning%20%28FFT-MIL%29%2C%20a%20framework%20that%20augments%20MIL%20with%20a%0Afrequency-domain%20branch%20to%20provide%20compact%20global%20context.%20Low-frequency%20crops%0Aare%20extracted%20from%20WSIs%20via%20the%20Fast%20Fourier%20Transform%20and%20processed%20through%20a%0Amodular%20FFT-Block%20composed%20of%20convolutional%20layers%20and%20Min-Max%20normalization%20to%0Amitigate%20the%20high%20variance%20of%20frequency%20data.%20The%20learned%20global%20frequency%0Afeature%20is%20fused%20with%20spatial%20patch%20features%20through%20lightweight%20integration%0Astrategies%2C%20enabling%20compatibility%20with%20diverse%20MIL%20architectures.%20FFT-MIL%20was%0Aevaluated%20across%20six%20state-of-the-art%20MIL%20methods%20on%20three%20public%20datasets%0A%28BRACS%2C%20LUAD%2C%20and%20IMP%29.%20Integration%20of%20the%20FFT-Block%20improved%20macro%20F1%20scores%0Aby%20an%20average%20of%203.51%25%20and%20AUC%20by%201.51%25%2C%20demonstrating%20consistent%20gains%20across%0Aarchitectures%20and%20datasets.%20These%20results%20establish%20frequency-domain%20learning%0Aas%20an%20effective%20and%20efficient%20mechanism%20for%20capturing%20global%20dependencies%20in%0AWSI%20classification%2C%20complementing%20spatial%20features%20and%20advancing%20the%0Ascalability%20and%20accuracy%20of%20MIL-based%20computational%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15138v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFourier%2520Transform%2520Multiple%2520Instance%2520Learning%2520for%2520Whole%2520Slide%2520Image%250A%2520%2520Classification%26entry.906535625%3DAnthony%2520Bilic%2520and%2520Guangyu%2520Sun%2520and%2520Ming%2520Li%2520and%2520Md%2520Sanzid%2520Bin%2520Hossain%2520and%2520Yu%2520Tian%2520and%2520Wei%2520Zhang%2520and%2520Laura%2520Brattain%2520and%2520Dexter%2520Hadley%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Whole%2520Slide%2520Image%2520%2528WSI%2529%2520classification%2520relies%2520on%2520Multiple%2520Instance%2520Learning%250A%2528MIL%2529%2520with%2520spatial%2520patch%2520features%252C%2520yet%2520existing%2520methods%2520struggle%2520to%2520capture%250Aglobal%2520dependencies%2520due%2520to%2520the%2520immense%2520size%2520of%2520WSIs%2520and%2520the%2520local%2520nature%2520of%250Apatch%2520embeddings.%2520This%2520limitation%2520hinders%2520the%2520modeling%2520of%2520coarse%2520structures%250Aessential%2520for%2520robust%2520diagnostic%2520prediction.%2520We%2520propose%2520Fourier%2520Transform%250AMultiple%2520Instance%2520Learning%2520%2528FFT-MIL%2529%252C%2520a%2520framework%2520that%2520augments%2520MIL%2520with%2520a%250Afrequency-domain%2520branch%2520to%2520provide%2520compact%2520global%2520context.%2520Low-frequency%2520crops%250Aare%2520extracted%2520from%2520WSIs%2520via%2520the%2520Fast%2520Fourier%2520Transform%2520and%2520processed%2520through%2520a%250Amodular%2520FFT-Block%2520composed%2520of%2520convolutional%2520layers%2520and%2520Min-Max%2520normalization%2520to%250Amitigate%2520the%2520high%2520variance%2520of%2520frequency%2520data.%2520The%2520learned%2520global%2520frequency%250Afeature%2520is%2520fused%2520with%2520spatial%2520patch%2520features%2520through%2520lightweight%2520integration%250Astrategies%252C%2520enabling%2520compatibility%2520with%2520diverse%2520MIL%2520architectures.%2520FFT-MIL%2520was%250Aevaluated%2520across%2520six%2520state-of-the-art%2520MIL%2520methods%2520on%2520three%2520public%2520datasets%250A%2528BRACS%252C%2520LUAD%252C%2520and%2520IMP%2529.%2520Integration%2520of%2520the%2520FFT-Block%2520improved%2520macro%2520F1%2520scores%250Aby%2520an%2520average%2520of%25203.51%2525%2520and%2520AUC%2520by%25201.51%2525%252C%2520demonstrating%2520consistent%2520gains%2520across%250Aarchitectures%2520and%2520datasets.%2520These%2520results%2520establish%2520frequency-domain%2520learning%250Aas%2520an%2520effective%2520and%2520efficient%2520mechanism%2520for%2520capturing%2520global%2520dependencies%2520in%250AWSI%2520classification%252C%2520complementing%2520spatial%2520features%2520and%2520advancing%2520the%250Ascalability%2520and%2520accuracy%2520of%2520MIL-based%2520computational%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15138v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fourier%20Transform%20Multiple%20Instance%20Learning%20for%20Whole%20Slide%20Image%0A%20%20Classification&entry.906535625=Anthony%20Bilic%20and%20Guangyu%20Sun%20and%20Ming%20Li%20and%20Md%20Sanzid%20Bin%20Hossain%20and%20Yu%20Tian%20and%20Wei%20Zhang%20and%20Laura%20Brattain%20and%20Dexter%20Hadley%20and%20Chen%20Chen&entry.1292438233=%20%20Whole%20Slide%20Image%20%28WSI%29%20classification%20relies%20on%20Multiple%20Instance%20Learning%0A%28MIL%29%20with%20spatial%20patch%20features%2C%20yet%20existing%20methods%20struggle%20to%20capture%0Aglobal%20dependencies%20due%20to%20the%20immense%20size%20of%20WSIs%20and%20the%20local%20nature%20of%0Apatch%20embeddings.%20This%20limitation%20hinders%20the%20modeling%20of%20coarse%20structures%0Aessential%20for%20robust%20diagnostic%20prediction.%20We%20propose%20Fourier%20Transform%0AMultiple%20Instance%20Learning%20%28FFT-MIL%29%2C%20a%20framework%20that%20augments%20MIL%20with%20a%0Afrequency-domain%20branch%20to%20provide%20compact%20global%20context.%20Low-frequency%20crops%0Aare%20extracted%20from%20WSIs%20via%20the%20Fast%20Fourier%20Transform%20and%20processed%20through%20a%0Amodular%20FFT-Block%20composed%20of%20convolutional%20layers%20and%20Min-Max%20normalization%20to%0Amitigate%20the%20high%20variance%20of%20frequency%20data.%20The%20learned%20global%20frequency%0Afeature%20is%20fused%20with%20spatial%20patch%20features%20through%20lightweight%20integration%0Astrategies%2C%20enabling%20compatibility%20with%20diverse%20MIL%20architectures.%20FFT-MIL%20was%0Aevaluated%20across%20six%20state-of-the-art%20MIL%20methods%20on%20three%20public%20datasets%0A%28BRACS%2C%20LUAD%2C%20and%20IMP%29.%20Integration%20of%20the%20FFT-Block%20improved%20macro%20F1%20scores%0Aby%20an%20average%20of%203.51%25%20and%20AUC%20by%201.51%25%2C%20demonstrating%20consistent%20gains%20across%0Aarchitectures%20and%20datasets.%20These%20results%20establish%20frequency-domain%20learning%0Aas%20an%20effective%20and%20efficient%20mechanism%20for%20capturing%20global%20dependencies%20in%0AWSI%20classification%2C%20complementing%20spatial%20features%20and%20advancing%20the%0Ascalability%20and%20accuracy%20of%20MIL-based%20computational%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15138v2&entry.124074799=Read"},
{"title": "Inductive Domain Transfer In Misspecified Simulation-Based Inference", "author": "Ortal Senouf and Antoine Wehenkel and C\u00e9dric Vincent-Cuaz and Emmanuel Abb\u00e9 and Pascal Frossard", "abstract": "  Simulation-based inference (SBI) is a statistical inference approach for\nestimating latent parameters of a physical system when the likelihood is\nintractable but simulations are available. In practice, SBI is often hindered\nby model misspecification--the mismatch between simulated and real-world\nobservations caused by inherent modeling simplifications. RoPE, a recent SBI\napproach, addresses this challenge through a two-stage domain transfer process\nthat combines semi-supervised calibration with optimal transport (OT)-based\ndistribution alignment. However, RoPE operates in a fully transductive setting,\nrequiring access to a batch of test samples at inference time, which limits\nscalability and generalization. We propose here a fully inductive and amortized\nSBI framework that integrates calibration and distributional alignment into a\nsingle, end-to-end trainable model. Our method leverages mini-batch OT with a\nclosed-form coupling to align real and simulated observations that correspond\nto the same latent parameters, using both paired calibration data and unpaired\nsamples. A conditional normalizing flow is then trained to approximate the\nOT-induced posterior, enabling efficient inference without simulation access at\ntest time. Across a range of synthetic and real-world benchmarks--including\ncomplex medical biomarker estimation--our approach matches or surpasses the\nperformance of RoPE, as well as other standard SBI and non-SBI estimators,\nwhile offering improved scalability and applicability in challenging,\nmisspecified environments.\n", "link": "http://arxiv.org/abs/2508.15593v3", "date": "2025-10-21", "relevancy": 1.4576, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5655}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4702}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inductive%20Domain%20Transfer%20In%20Misspecified%20Simulation-Based%20Inference&body=Title%3A%20Inductive%20Domain%20Transfer%20In%20Misspecified%20Simulation-Based%20Inference%0AAuthor%3A%20Ortal%20Senouf%20and%20Antoine%20Wehenkel%20and%20C%C3%A9dric%20Vincent-Cuaz%20and%20Emmanuel%20Abb%C3%A9%20and%20Pascal%20Frossard%0AAbstract%3A%20%20%20Simulation-based%20inference%20%28SBI%29%20is%20a%20statistical%20inference%20approach%20for%0Aestimating%20latent%20parameters%20of%20a%20physical%20system%20when%20the%20likelihood%20is%0Aintractable%20but%20simulations%20are%20available.%20In%20practice%2C%20SBI%20is%20often%20hindered%0Aby%20model%20misspecification--the%20mismatch%20between%20simulated%20and%20real-world%0Aobservations%20caused%20by%20inherent%20modeling%20simplifications.%20RoPE%2C%20a%20recent%20SBI%0Aapproach%2C%20addresses%20this%20challenge%20through%20a%20two-stage%20domain%20transfer%20process%0Athat%20combines%20semi-supervised%20calibration%20with%20optimal%20transport%20%28OT%29-based%0Adistribution%20alignment.%20However%2C%20RoPE%20operates%20in%20a%20fully%20transductive%20setting%2C%0Arequiring%20access%20to%20a%20batch%20of%20test%20samples%20at%20inference%20time%2C%20which%20limits%0Ascalability%20and%20generalization.%20We%20propose%20here%20a%20fully%20inductive%20and%20amortized%0ASBI%20framework%20that%20integrates%20calibration%20and%20distributional%20alignment%20into%20a%0Asingle%2C%20end-to-end%20trainable%20model.%20Our%20method%20leverages%20mini-batch%20OT%20with%20a%0Aclosed-form%20coupling%20to%20align%20real%20and%20simulated%20observations%20that%20correspond%0Ato%20the%20same%20latent%20parameters%2C%20using%20both%20paired%20calibration%20data%20and%20unpaired%0Asamples.%20A%20conditional%20normalizing%20flow%20is%20then%20trained%20to%20approximate%20the%0AOT-induced%20posterior%2C%20enabling%20efficient%20inference%20without%20simulation%20access%20at%0Atest%20time.%20Across%20a%20range%20of%20synthetic%20and%20real-world%20benchmarks--including%0Acomplex%20medical%20biomarker%20estimation--our%20approach%20matches%20or%20surpasses%20the%0Aperformance%20of%20RoPE%2C%20as%20well%20as%20other%20standard%20SBI%20and%20non-SBI%20estimators%2C%0Awhile%20offering%20improved%20scalability%20and%20applicability%20in%20challenging%2C%0Amisspecified%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15593v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInductive%2520Domain%2520Transfer%2520In%2520Misspecified%2520Simulation-Based%2520Inference%26entry.906535625%3DOrtal%2520Senouf%2520and%2520Antoine%2520Wehenkel%2520and%2520C%25C3%25A9dric%2520Vincent-Cuaz%2520and%2520Emmanuel%2520Abb%25C3%25A9%2520and%2520Pascal%2520Frossard%26entry.1292438233%3D%2520%2520Simulation-based%2520inference%2520%2528SBI%2529%2520is%2520a%2520statistical%2520inference%2520approach%2520for%250Aestimating%2520latent%2520parameters%2520of%2520a%2520physical%2520system%2520when%2520the%2520likelihood%2520is%250Aintractable%2520but%2520simulations%2520are%2520available.%2520In%2520practice%252C%2520SBI%2520is%2520often%2520hindered%250Aby%2520model%2520misspecification--the%2520mismatch%2520between%2520simulated%2520and%2520real-world%250Aobservations%2520caused%2520by%2520inherent%2520modeling%2520simplifications.%2520RoPE%252C%2520a%2520recent%2520SBI%250Aapproach%252C%2520addresses%2520this%2520challenge%2520through%2520a%2520two-stage%2520domain%2520transfer%2520process%250Athat%2520combines%2520semi-supervised%2520calibration%2520with%2520optimal%2520transport%2520%2528OT%2529-based%250Adistribution%2520alignment.%2520However%252C%2520RoPE%2520operates%2520in%2520a%2520fully%2520transductive%2520setting%252C%250Arequiring%2520access%2520to%2520a%2520batch%2520of%2520test%2520samples%2520at%2520inference%2520time%252C%2520which%2520limits%250Ascalability%2520and%2520generalization.%2520We%2520propose%2520here%2520a%2520fully%2520inductive%2520and%2520amortized%250ASBI%2520framework%2520that%2520integrates%2520calibration%2520and%2520distributional%2520alignment%2520into%2520a%250Asingle%252C%2520end-to-end%2520trainable%2520model.%2520Our%2520method%2520leverages%2520mini-batch%2520OT%2520with%2520a%250Aclosed-form%2520coupling%2520to%2520align%2520real%2520and%2520simulated%2520observations%2520that%2520correspond%250Ato%2520the%2520same%2520latent%2520parameters%252C%2520using%2520both%2520paired%2520calibration%2520data%2520and%2520unpaired%250Asamples.%2520A%2520conditional%2520normalizing%2520flow%2520is%2520then%2520trained%2520to%2520approximate%2520the%250AOT-induced%2520posterior%252C%2520enabling%2520efficient%2520inference%2520without%2520simulation%2520access%2520at%250Atest%2520time.%2520Across%2520a%2520range%2520of%2520synthetic%2520and%2520real-world%2520benchmarks--including%250Acomplex%2520medical%2520biomarker%2520estimation--our%2520approach%2520matches%2520or%2520surpasses%2520the%250Aperformance%2520of%2520RoPE%252C%2520as%2520well%2520as%2520other%2520standard%2520SBI%2520and%2520non-SBI%2520estimators%252C%250Awhile%2520offering%2520improved%2520scalability%2520and%2520applicability%2520in%2520challenging%252C%250Amisspecified%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15593v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inductive%20Domain%20Transfer%20In%20Misspecified%20Simulation-Based%20Inference&entry.906535625=Ortal%20Senouf%20and%20Antoine%20Wehenkel%20and%20C%C3%A9dric%20Vincent-Cuaz%20and%20Emmanuel%20Abb%C3%A9%20and%20Pascal%20Frossard&entry.1292438233=%20%20Simulation-based%20inference%20%28SBI%29%20is%20a%20statistical%20inference%20approach%20for%0Aestimating%20latent%20parameters%20of%20a%20physical%20system%20when%20the%20likelihood%20is%0Aintractable%20but%20simulations%20are%20available.%20In%20practice%2C%20SBI%20is%20often%20hindered%0Aby%20model%20misspecification--the%20mismatch%20between%20simulated%20and%20real-world%0Aobservations%20caused%20by%20inherent%20modeling%20simplifications.%20RoPE%2C%20a%20recent%20SBI%0Aapproach%2C%20addresses%20this%20challenge%20through%20a%20two-stage%20domain%20transfer%20process%0Athat%20combines%20semi-supervised%20calibration%20with%20optimal%20transport%20%28OT%29-based%0Adistribution%20alignment.%20However%2C%20RoPE%20operates%20in%20a%20fully%20transductive%20setting%2C%0Arequiring%20access%20to%20a%20batch%20of%20test%20samples%20at%20inference%20time%2C%20which%20limits%0Ascalability%20and%20generalization.%20We%20propose%20here%20a%20fully%20inductive%20and%20amortized%0ASBI%20framework%20that%20integrates%20calibration%20and%20distributional%20alignment%20into%20a%0Asingle%2C%20end-to-end%20trainable%20model.%20Our%20method%20leverages%20mini-batch%20OT%20with%20a%0Aclosed-form%20coupling%20to%20align%20real%20and%20simulated%20observations%20that%20correspond%0Ato%20the%20same%20latent%20parameters%2C%20using%20both%20paired%20calibration%20data%20and%20unpaired%0Asamples.%20A%20conditional%20normalizing%20flow%20is%20then%20trained%20to%20approximate%20the%0AOT-induced%20posterior%2C%20enabling%20efficient%20inference%20without%20simulation%20access%20at%0Atest%20time.%20Across%20a%20range%20of%20synthetic%20and%20real-world%20benchmarks--including%0Acomplex%20medical%20biomarker%20estimation--our%20approach%20matches%20or%20surpasses%20the%0Aperformance%20of%20RoPE%2C%20as%20well%20as%20other%20standard%20SBI%20and%20non-SBI%20estimators%2C%0Awhile%20offering%20improved%20scalability%20and%20applicability%20in%20challenging%2C%0Amisspecified%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15593v3&entry.124074799=Read"},
{"title": "On Biologically Plausible Learning in Continuous Time", "author": "Marc Gong Bacvanski and Liu Ziyin and Tomaso Poggio", "abstract": "  Biological learning unfolds continuously in time, yet most algorithmic models\nrely on discrete updates and separate inference and learning phases. We study a\ncontinuous-time neural model that unifies several biologically plausible\nlearning algorithms and removes the need for phase separation. Rules including\nstochastic gradient descent (SGD), feedback alignment (FA), direct feedback\nalignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of\nthe dynamics. Simulations show that these continuous-time networks stably learn\nat biological timescales, even under temporal mismatches and integration noise.\nThrough analysis and simulation, we show that learning depends on temporal\noverlap: a synapse updates correctly only when its input and the corresponding\nerror signal coincide in time. When inputs are held constant, learning strength\ndeclines linearly as the delay between input and error approaches the stimulus\nduration, explaining observed robustness and failure across network depths.\nCritically, robust learning requires the synaptic plasticity timescale to\nexceed the stimulus duration by one to two orders of magnitude. For typical\ncortical stimuli (tens of milliseconds), this places the functional plasticity\nwindow in the few-second range, a testable prediction that identifies\nseconds-scale eligibility traces as necessary for error-driven learning in\nbiological circuits.\n", "link": "http://arxiv.org/abs/2510.18808v1", "date": "2025-10-21", "relevancy": 1.4488, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5102}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Biologically%20Plausible%20Learning%20in%20Continuous%20Time&body=Title%3A%20On%20Biologically%20Plausible%20Learning%20in%20Continuous%20Time%0AAuthor%3A%20Marc%20Gong%20Bacvanski%20and%20Liu%20Ziyin%20and%20Tomaso%20Poggio%0AAbstract%3A%20%20%20Biological%20learning%20unfolds%20continuously%20in%20time%2C%20yet%20most%20algorithmic%20models%0Arely%20on%20discrete%20updates%20and%20separate%20inference%20and%20learning%20phases.%20We%20study%20a%0Acontinuous-time%20neural%20model%20that%20unifies%20several%20biologically%20plausible%0Alearning%20algorithms%20and%20removes%20the%20need%20for%20phase%20separation.%20Rules%20including%0Astochastic%20gradient%20descent%20%28SGD%29%2C%20feedback%20alignment%20%28FA%29%2C%20direct%20feedback%0Aalignment%20%28DFA%29%2C%20and%20Kolen-Pollack%20%28KP%29%20emerge%20naturally%20as%20limiting%20cases%20of%0Athe%20dynamics.%20Simulations%20show%20that%20these%20continuous-time%20networks%20stably%20learn%0Aat%20biological%20timescales%2C%20even%20under%20temporal%20mismatches%20and%20integration%20noise.%0AThrough%20analysis%20and%20simulation%2C%20we%20show%20that%20learning%20depends%20on%20temporal%0Aoverlap%3A%20a%20synapse%20updates%20correctly%20only%20when%20its%20input%20and%20the%20corresponding%0Aerror%20signal%20coincide%20in%20time.%20When%20inputs%20are%20held%20constant%2C%20learning%20strength%0Adeclines%20linearly%20as%20the%20delay%20between%20input%20and%20error%20approaches%20the%20stimulus%0Aduration%2C%20explaining%20observed%20robustness%20and%20failure%20across%20network%20depths.%0ACritically%2C%20robust%20learning%20requires%20the%20synaptic%20plasticity%20timescale%20to%0Aexceed%20the%20stimulus%20duration%20by%20one%20to%20two%20orders%20of%20magnitude.%20For%20typical%0Acortical%20stimuli%20%28tens%20of%20milliseconds%29%2C%20this%20places%20the%20functional%20plasticity%0Awindow%20in%20the%20few-second%20range%2C%20a%20testable%20prediction%20that%20identifies%0Aseconds-scale%20eligibility%20traces%20as%20necessary%20for%20error-driven%20learning%20in%0Abiological%20circuits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Biologically%2520Plausible%2520Learning%2520in%2520Continuous%2520Time%26entry.906535625%3DMarc%2520Gong%2520Bacvanski%2520and%2520Liu%2520Ziyin%2520and%2520Tomaso%2520Poggio%26entry.1292438233%3D%2520%2520Biological%2520learning%2520unfolds%2520continuously%2520in%2520time%252C%2520yet%2520most%2520algorithmic%2520models%250Arely%2520on%2520discrete%2520updates%2520and%2520separate%2520inference%2520and%2520learning%2520phases.%2520We%2520study%2520a%250Acontinuous-time%2520neural%2520model%2520that%2520unifies%2520several%2520biologically%2520plausible%250Alearning%2520algorithms%2520and%2520removes%2520the%2520need%2520for%2520phase%2520separation.%2520Rules%2520including%250Astochastic%2520gradient%2520descent%2520%2528SGD%2529%252C%2520feedback%2520alignment%2520%2528FA%2529%252C%2520direct%2520feedback%250Aalignment%2520%2528DFA%2529%252C%2520and%2520Kolen-Pollack%2520%2528KP%2529%2520emerge%2520naturally%2520as%2520limiting%2520cases%2520of%250Athe%2520dynamics.%2520Simulations%2520show%2520that%2520these%2520continuous-time%2520networks%2520stably%2520learn%250Aat%2520biological%2520timescales%252C%2520even%2520under%2520temporal%2520mismatches%2520and%2520integration%2520noise.%250AThrough%2520analysis%2520and%2520simulation%252C%2520we%2520show%2520that%2520learning%2520depends%2520on%2520temporal%250Aoverlap%253A%2520a%2520synapse%2520updates%2520correctly%2520only%2520when%2520its%2520input%2520and%2520the%2520corresponding%250Aerror%2520signal%2520coincide%2520in%2520time.%2520When%2520inputs%2520are%2520held%2520constant%252C%2520learning%2520strength%250Adeclines%2520linearly%2520as%2520the%2520delay%2520between%2520input%2520and%2520error%2520approaches%2520the%2520stimulus%250Aduration%252C%2520explaining%2520observed%2520robustness%2520and%2520failure%2520across%2520network%2520depths.%250ACritically%252C%2520robust%2520learning%2520requires%2520the%2520synaptic%2520plasticity%2520timescale%2520to%250Aexceed%2520the%2520stimulus%2520duration%2520by%2520one%2520to%2520two%2520orders%2520of%2520magnitude.%2520For%2520typical%250Acortical%2520stimuli%2520%2528tens%2520of%2520milliseconds%2529%252C%2520this%2520places%2520the%2520functional%2520plasticity%250Awindow%2520in%2520the%2520few-second%2520range%252C%2520a%2520testable%2520prediction%2520that%2520identifies%250Aseconds-scale%2520eligibility%2520traces%2520as%2520necessary%2520for%2520error-driven%2520learning%2520in%250Abiological%2520circuits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Biologically%20Plausible%20Learning%20in%20Continuous%20Time&entry.906535625=Marc%20Gong%20Bacvanski%20and%20Liu%20Ziyin%20and%20Tomaso%20Poggio&entry.1292438233=%20%20Biological%20learning%20unfolds%20continuously%20in%20time%2C%20yet%20most%20algorithmic%20models%0Arely%20on%20discrete%20updates%20and%20separate%20inference%20and%20learning%20phases.%20We%20study%20a%0Acontinuous-time%20neural%20model%20that%20unifies%20several%20biologically%20plausible%0Alearning%20algorithms%20and%20removes%20the%20need%20for%20phase%20separation.%20Rules%20including%0Astochastic%20gradient%20descent%20%28SGD%29%2C%20feedback%20alignment%20%28FA%29%2C%20direct%20feedback%0Aalignment%20%28DFA%29%2C%20and%20Kolen-Pollack%20%28KP%29%20emerge%20naturally%20as%20limiting%20cases%20of%0Athe%20dynamics.%20Simulations%20show%20that%20these%20continuous-time%20networks%20stably%20learn%0Aat%20biological%20timescales%2C%20even%20under%20temporal%20mismatches%20and%20integration%20noise.%0AThrough%20analysis%20and%20simulation%2C%20we%20show%20that%20learning%20depends%20on%20temporal%0Aoverlap%3A%20a%20synapse%20updates%20correctly%20only%20when%20its%20input%20and%20the%20corresponding%0Aerror%20signal%20coincide%20in%20time.%20When%20inputs%20are%20held%20constant%2C%20learning%20strength%0Adeclines%20linearly%20as%20the%20delay%20between%20input%20and%20error%20approaches%20the%20stimulus%0Aduration%2C%20explaining%20observed%20robustness%20and%20failure%20across%20network%20depths.%0ACritically%2C%20robust%20learning%20requires%20the%20synaptic%20plasticity%20timescale%20to%0Aexceed%20the%20stimulus%20duration%20by%20one%20to%20two%20orders%20of%20magnitude.%20For%20typical%0Acortical%20stimuli%20%28tens%20of%20milliseconds%29%2C%20this%20places%20the%20functional%20plasticity%0Awindow%20in%20the%20few-second%20range%2C%20a%20testable%20prediction%20that%20identifies%0Aseconds-scale%20eligibility%20traces%20as%20necessary%20for%20error-driven%20learning%20in%0Abiological%20circuits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18808v1&entry.124074799=Read"},
{"title": "The Shift Towards Preprints in AI Policy Research: A Comparative Study\n  of Preprint Trends in the U.S., Europe, and South Korea", "author": "Simon Suh", "abstract": "  The adoption of open science has quickly changed how artificial intelligence\n(AI) policy research is distributed globally. This study examines the regional\ntrends in the citation of preprints, specifically focusing on the impact of two\nmajor disruptive events: the COVID-19 pandemic and the release of ChatGPT, on\nresearch dissemination patterns in the United States, Europe, and South Korea\nfrom 2015 to 2024. Using bibliometrics data from the Web of Science, this study\ntracks how global disruptive events influenced the adoption of preprints in AI\npolicy research and how such shifts vary by region. By marking the timing of\nthese disruptive events, the analysis reveals that while all regions\nexperienced growth in preprint citations, the magnitude and trajectory of\nchange varied significantly. The United States exhibited sharp, event-driven\nincreases; Europe demonstrated institutional growth; and South Korea maintained\nconsistent, linear growth in preprint adoption. These findings suggest that\nglobal disruptions may have accelerated preprint adoption, but the extent and\ntrajectory are shaped by local research cultures, policy environments, and\nlevels of open science maturity. This paper emphasizes the need for future AI\ngovernance strategies to consider regional variability in research\ndissemination and highlights opportunities for further longitudinal and\ncomparative research to deepen our understanding of open-access adoption in AI\npolicy development.\n", "link": "http://arxiv.org/abs/2505.03835v2", "date": "2025-10-21", "relevancy": 1.4439, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3622}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.362}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Shift%20Towards%20Preprints%20in%20AI%20Policy%20Research%3A%20A%20Comparative%20Study%0A%20%20of%20Preprint%20Trends%20in%20the%20U.S.%2C%20Europe%2C%20and%20South%20Korea&body=Title%3A%20The%20Shift%20Towards%20Preprints%20in%20AI%20Policy%20Research%3A%20A%20Comparative%20Study%0A%20%20of%20Preprint%20Trends%20in%20the%20U.S.%2C%20Europe%2C%20and%20South%20Korea%0AAuthor%3A%20Simon%20Suh%0AAbstract%3A%20%20%20The%20adoption%20of%20open%20science%20has%20quickly%20changed%20how%20artificial%20intelligence%0A%28AI%29%20policy%20research%20is%20distributed%20globally.%20This%20study%20examines%20the%20regional%0Atrends%20in%20the%20citation%20of%20preprints%2C%20specifically%20focusing%20on%20the%20impact%20of%20two%0Amajor%20disruptive%20events%3A%20the%20COVID-19%20pandemic%20and%20the%20release%20of%20ChatGPT%2C%20on%0Aresearch%20dissemination%20patterns%20in%20the%20United%20States%2C%20Europe%2C%20and%20South%20Korea%0Afrom%202015%20to%202024.%20Using%20bibliometrics%20data%20from%20the%20Web%20of%20Science%2C%20this%20study%0Atracks%20how%20global%20disruptive%20events%20influenced%20the%20adoption%20of%20preprints%20in%20AI%0Apolicy%20research%20and%20how%20such%20shifts%20vary%20by%20region.%20By%20marking%20the%20timing%20of%0Athese%20disruptive%20events%2C%20the%20analysis%20reveals%20that%20while%20all%20regions%0Aexperienced%20growth%20in%20preprint%20citations%2C%20the%20magnitude%20and%20trajectory%20of%0Achange%20varied%20significantly.%20The%20United%20States%20exhibited%20sharp%2C%20event-driven%0Aincreases%3B%20Europe%20demonstrated%20institutional%20growth%3B%20and%20South%20Korea%20maintained%0Aconsistent%2C%20linear%20growth%20in%20preprint%20adoption.%20These%20findings%20suggest%20that%0Aglobal%20disruptions%20may%20have%20accelerated%20preprint%20adoption%2C%20but%20the%20extent%20and%0Atrajectory%20are%20shaped%20by%20local%20research%20cultures%2C%20policy%20environments%2C%20and%0Alevels%20of%20open%20science%20maturity.%20This%20paper%20emphasizes%20the%20need%20for%20future%20AI%0Agovernance%20strategies%20to%20consider%20regional%20variability%20in%20research%0Adissemination%20and%20highlights%20opportunities%20for%20further%20longitudinal%20and%0Acomparative%20research%20to%20deepen%20our%20understanding%20of%20open-access%20adoption%20in%20AI%0Apolicy%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Shift%2520Towards%2520Preprints%2520in%2520AI%2520Policy%2520Research%253A%2520A%2520Comparative%2520Study%250A%2520%2520of%2520Preprint%2520Trends%2520in%2520the%2520U.S.%252C%2520Europe%252C%2520and%2520South%2520Korea%26entry.906535625%3DSimon%2520Suh%26entry.1292438233%3D%2520%2520The%2520adoption%2520of%2520open%2520science%2520has%2520quickly%2520changed%2520how%2520artificial%2520intelligence%250A%2528AI%2529%2520policy%2520research%2520is%2520distributed%2520globally.%2520This%2520study%2520examines%2520the%2520regional%250Atrends%2520in%2520the%2520citation%2520of%2520preprints%252C%2520specifically%2520focusing%2520on%2520the%2520impact%2520of%2520two%250Amajor%2520disruptive%2520events%253A%2520the%2520COVID-19%2520pandemic%2520and%2520the%2520release%2520of%2520ChatGPT%252C%2520on%250Aresearch%2520dissemination%2520patterns%2520in%2520the%2520United%2520States%252C%2520Europe%252C%2520and%2520South%2520Korea%250Afrom%25202015%2520to%25202024.%2520Using%2520bibliometrics%2520data%2520from%2520the%2520Web%2520of%2520Science%252C%2520this%2520study%250Atracks%2520how%2520global%2520disruptive%2520events%2520influenced%2520the%2520adoption%2520of%2520preprints%2520in%2520AI%250Apolicy%2520research%2520and%2520how%2520such%2520shifts%2520vary%2520by%2520region.%2520By%2520marking%2520the%2520timing%2520of%250Athese%2520disruptive%2520events%252C%2520the%2520analysis%2520reveals%2520that%2520while%2520all%2520regions%250Aexperienced%2520growth%2520in%2520preprint%2520citations%252C%2520the%2520magnitude%2520and%2520trajectory%2520of%250Achange%2520varied%2520significantly.%2520The%2520United%2520States%2520exhibited%2520sharp%252C%2520event-driven%250Aincreases%253B%2520Europe%2520demonstrated%2520institutional%2520growth%253B%2520and%2520South%2520Korea%2520maintained%250Aconsistent%252C%2520linear%2520growth%2520in%2520preprint%2520adoption.%2520These%2520findings%2520suggest%2520that%250Aglobal%2520disruptions%2520may%2520have%2520accelerated%2520preprint%2520adoption%252C%2520but%2520the%2520extent%2520and%250Atrajectory%2520are%2520shaped%2520by%2520local%2520research%2520cultures%252C%2520policy%2520environments%252C%2520and%250Alevels%2520of%2520open%2520science%2520maturity.%2520This%2520paper%2520emphasizes%2520the%2520need%2520for%2520future%2520AI%250Agovernance%2520strategies%2520to%2520consider%2520regional%2520variability%2520in%2520research%250Adissemination%2520and%2520highlights%2520opportunities%2520for%2520further%2520longitudinal%2520and%250Acomparative%2520research%2520to%2520deepen%2520our%2520understanding%2520of%2520open-access%2520adoption%2520in%2520AI%250Apolicy%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Shift%20Towards%20Preprints%20in%20AI%20Policy%20Research%3A%20A%20Comparative%20Study%0A%20%20of%20Preprint%20Trends%20in%20the%20U.S.%2C%20Europe%2C%20and%20South%20Korea&entry.906535625=Simon%20Suh&entry.1292438233=%20%20The%20adoption%20of%20open%20science%20has%20quickly%20changed%20how%20artificial%20intelligence%0A%28AI%29%20policy%20research%20is%20distributed%20globally.%20This%20study%20examines%20the%20regional%0Atrends%20in%20the%20citation%20of%20preprints%2C%20specifically%20focusing%20on%20the%20impact%20of%20two%0Amajor%20disruptive%20events%3A%20the%20COVID-19%20pandemic%20and%20the%20release%20of%20ChatGPT%2C%20on%0Aresearch%20dissemination%20patterns%20in%20the%20United%20States%2C%20Europe%2C%20and%20South%20Korea%0Afrom%202015%20to%202024.%20Using%20bibliometrics%20data%20from%20the%20Web%20of%20Science%2C%20this%20study%0Atracks%20how%20global%20disruptive%20events%20influenced%20the%20adoption%20of%20preprints%20in%20AI%0Apolicy%20research%20and%20how%20such%20shifts%20vary%20by%20region.%20By%20marking%20the%20timing%20of%0Athese%20disruptive%20events%2C%20the%20analysis%20reveals%20that%20while%20all%20regions%0Aexperienced%20growth%20in%20preprint%20citations%2C%20the%20magnitude%20and%20trajectory%20of%0Achange%20varied%20significantly.%20The%20United%20States%20exhibited%20sharp%2C%20event-driven%0Aincreases%3B%20Europe%20demonstrated%20institutional%20growth%3B%20and%20South%20Korea%20maintained%0Aconsistent%2C%20linear%20growth%20in%20preprint%20adoption.%20These%20findings%20suggest%20that%0Aglobal%20disruptions%20may%20have%20accelerated%20preprint%20adoption%2C%20but%20the%20extent%20and%0Atrajectory%20are%20shaped%20by%20local%20research%20cultures%2C%20policy%20environments%2C%20and%0Alevels%20of%20open%20science%20maturity.%20This%20paper%20emphasizes%20the%20need%20for%20future%20AI%0Agovernance%20strategies%20to%20consider%20regional%20variability%20in%20research%0Adissemination%20and%20highlights%20opportunities%20for%20further%20longitudinal%20and%0Acomparative%20research%20to%20deepen%20our%20understanding%20of%20open-access%20adoption%20in%20AI%0Apolicy%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03835v2&entry.124074799=Read"},
{"title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model", "author": " Ling Team and Anqi Shen and Baihui Li and Bin Hu and Bin Jing and Cai Chen and Chao Huang and Chao Zhang and Chaokun Yang and Cheng Lin and Chengyao Wen and Congqi Li and Deng Zhao and Dingbo Yuan and Donghai You and Fagui Mao and Fanzhuang Meng and Feng Xu and Guojie Li and Guowei Wang and Hao Dai and Haonan Zheng and Hong Liu and Jia Guo and Jiaming Liu and Jian Liu and Jianhao Fu and Jiannan Shi and Jianwen Wang and Jianxin Lai and Jin Yang and Jun Mei and Jun Zhou and Junbo Zhao and Junping Zhao and Kuan Xu and Le Su and Lei Chen and Li Tang and Liang Jiang and Liangcheng Fu and Lianhao Xu and Linfeng Shi and Lisha Liao and Longfei Zheng and Meng Li and Mingchun Chen and Qi Zuo and Qiang Cheng and Qianggang Cao and Qitao Shi and Quanrui Guo and Senlin Zhu and Shaofei Wang and Shaomian Zheng and Shuaicheng Li and Shuwei Gu and Siba Chen and Tao Wu and Tao Zhang and Tianyu Zhang and Tianyu Zhou and Tiwei Bie and Tongkai Yang and Wang Hong and Wang Ren and Weihua Chen and Wenbo Yu and Wengang Zheng and Xiangchun Wang and Xiaodong Yan and Xiaopei Wan and Xin Zhao and Xinyu Kong and Xinyu Tang and Xudong Han and Xudong Wang and Xuemin Yang and Xueyu Hu and Yalin Zhang and Yan Sun and Yicheng Shan and Yilong Wang and Yingying Xu and Yongkang Liu and Yongzhen Guo and Yuanyuan Wang and Yuchen Yan and Yuefan Wang and Yuhong Guo and Zehuan Li and Zhankai Xu and Zhe Li and Zhenduo Zhang and Zhengke Gui and Zhenxuan Pan and Zhenyu Huang and Zhenzhong Lan and Zhiqiang Ding and Zhiqiang Zhang and Zhixun Li and Zhizhen Liu and Zihao Wang and Zujie Wen", "abstract": "  We present Ring-1T, the first open-source, state-of-the-art thinking model\nwith a trillion-scale parameter. It features 1 trillion total parameters and\nactivates approximately 50 billion per token. Training such models at a\ntrillion-parameter scale introduces unprecedented challenges, including\ntrain-inference misalignment, inefficiencies in rollout processing, and\nbottlenecks in the RL system. To address these, we pioneer three interconnected\ninnovations: (1) IcePop stabilizes RL training via token-level discrepancy\nmasking and clipping, resolving instability from training-inference mismatches;\n(2) C3PO++ improves resource utilization for long rollouts under a token budget\nby dynamically partitioning them, thereby obtaining high time efficiency; and\n(3) ASystem, a high-performance RL framework designed to overcome the systemic\nbottlenecks that impede trillion-parameter model training. Ring-1T delivers\nbreakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on\nHMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a\nsilver medal-level result on the IMO-2025, underscoring its exceptional\nreasoning capabilities. By releasing the complete 1T parameter MoE model to the\ncommunity, we provide the research community with direct access to cutting-edge\nreasoning capabilities. This contribution marks a significant milestone in\ndemocratizing large-scale reasoning intelligence and establishes a new baseline\nfor open-source model performance.\n", "link": "http://arxiv.org/abs/2510.18855v1", "date": "2025-10-21", "relevancy": 1.4426, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5056}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4792}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Every%20Step%20Evolves%3A%20Scaling%20Reinforcement%20Learning%20for%20Trillion-Scale%0A%20%20Thinking%20Model&body=Title%3A%20Every%20Step%20Evolves%3A%20Scaling%20Reinforcement%20Learning%20for%20Trillion-Scale%0A%20%20Thinking%20Model%0AAuthor%3A%20%20Ling%20Team%20and%20Anqi%20Shen%20and%20Baihui%20Li%20and%20Bin%20Hu%20and%20Bin%20Jing%20and%20Cai%20Chen%20and%20Chao%20Huang%20and%20Chao%20Zhang%20and%20Chaokun%20Yang%20and%20Cheng%20Lin%20and%20Chengyao%20Wen%20and%20Congqi%20Li%20and%20Deng%20Zhao%20and%20Dingbo%20Yuan%20and%20Donghai%20You%20and%20Fagui%20Mao%20and%20Fanzhuang%20Meng%20and%20Feng%20Xu%20and%20Guojie%20Li%20and%20Guowei%20Wang%20and%20Hao%20Dai%20and%20Haonan%20Zheng%20and%20Hong%20Liu%20and%20Jia%20Guo%20and%20Jiaming%20Liu%20and%20Jian%20Liu%20and%20Jianhao%20Fu%20and%20Jiannan%20Shi%20and%20Jianwen%20Wang%20and%20Jianxin%20Lai%20and%20Jin%20Yang%20and%20Jun%20Mei%20and%20Jun%20Zhou%20and%20Junbo%20Zhao%20and%20Junping%20Zhao%20and%20Kuan%20Xu%20and%20Le%20Su%20and%20Lei%20Chen%20and%20Li%20Tang%20and%20Liang%20Jiang%20and%20Liangcheng%20Fu%20and%20Lianhao%20Xu%20and%20Linfeng%20Shi%20and%20Lisha%20Liao%20and%20Longfei%20Zheng%20and%20Meng%20Li%20and%20Mingchun%20Chen%20and%20Qi%20Zuo%20and%20Qiang%20Cheng%20and%20Qianggang%20Cao%20and%20Qitao%20Shi%20and%20Quanrui%20Guo%20and%20Senlin%20Zhu%20and%20Shaofei%20Wang%20and%20Shaomian%20Zheng%20and%20Shuaicheng%20Li%20and%20Shuwei%20Gu%20and%20Siba%20Chen%20and%20Tao%20Wu%20and%20Tao%20Zhang%20and%20Tianyu%20Zhang%20and%20Tianyu%20Zhou%20and%20Tiwei%20Bie%20and%20Tongkai%20Yang%20and%20Wang%20Hong%20and%20Wang%20Ren%20and%20Weihua%20Chen%20and%20Wenbo%20Yu%20and%20Wengang%20Zheng%20and%20Xiangchun%20Wang%20and%20Xiaodong%20Yan%20and%20Xiaopei%20Wan%20and%20Xin%20Zhao%20and%20Xinyu%20Kong%20and%20Xinyu%20Tang%20and%20Xudong%20Han%20and%20Xudong%20Wang%20and%20Xuemin%20Yang%20and%20Xueyu%20Hu%20and%20Yalin%20Zhang%20and%20Yan%20Sun%20and%20Yicheng%20Shan%20and%20Yilong%20Wang%20and%20Yingying%20Xu%20and%20Yongkang%20Liu%20and%20Yongzhen%20Guo%20and%20Yuanyuan%20Wang%20and%20Yuchen%20Yan%20and%20Yuefan%20Wang%20and%20Yuhong%20Guo%20and%20Zehuan%20Li%20and%20Zhankai%20Xu%20and%20Zhe%20Li%20and%20Zhenduo%20Zhang%20and%20Zhengke%20Gui%20and%20Zhenxuan%20Pan%20and%20Zhenyu%20Huang%20and%20Zhenzhong%20Lan%20and%20Zhiqiang%20Ding%20and%20Zhiqiang%20Zhang%20and%20Zhixun%20Li%20and%20Zhizhen%20Liu%20and%20Zihao%20Wang%20and%20Zujie%20Wen%0AAbstract%3A%20%20%20We%20present%20Ring-1T%2C%20the%20first%20open-source%2C%20state-of-the-art%20thinking%20model%0Awith%20a%20trillion-scale%20parameter.%20It%20features%201%20trillion%20total%20parameters%20and%0Aactivates%20approximately%2050%20billion%20per%20token.%20Training%20such%20models%20at%20a%0Atrillion-parameter%20scale%20introduces%20unprecedented%20challenges%2C%20including%0Atrain-inference%20misalignment%2C%20inefficiencies%20in%20rollout%20processing%2C%20and%0Abottlenecks%20in%20the%20RL%20system.%20To%20address%20these%2C%20we%20pioneer%20three%20interconnected%0Ainnovations%3A%20%281%29%20IcePop%20stabilizes%20RL%20training%20via%20token-level%20discrepancy%0Amasking%20and%20clipping%2C%20resolving%20instability%20from%20training-inference%20mismatches%3B%0A%282%29%20C3PO%2B%2B%20improves%20resource%20utilization%20for%20long%20rollouts%20under%20a%20token%20budget%0Aby%20dynamically%20partitioning%20them%2C%20thereby%20obtaining%20high%20time%20efficiency%3B%20and%0A%283%29%20ASystem%2C%20a%20high-performance%20RL%20framework%20designed%20to%20overcome%20the%20systemic%0Abottlenecks%20that%20impede%20trillion-parameter%20model%20training.%20Ring-1T%20delivers%0Abreakthrough%20results%20across%20critical%20benchmarks%3A%2093.4%20on%20AIME-2025%2C%2086.72%20on%0AHMMT-2025%2C%202088%20on%20CodeForces%2C%20and%2055.94%20on%20ARC-AGI-v1.%20Notably%2C%20it%20attains%20a%0Asilver%20medal-level%20result%20on%20the%20IMO-2025%2C%20underscoring%20its%20exceptional%0Areasoning%20capabilities.%20By%20releasing%20the%20complete%201T%20parameter%20MoE%20model%20to%20the%0Acommunity%2C%20we%20provide%20the%20research%20community%20with%20direct%20access%20to%20cutting-edge%0Areasoning%20capabilities.%20This%20contribution%20marks%20a%20significant%20milestone%20in%0Ademocratizing%20large-scale%20reasoning%20intelligence%20and%20establishes%20a%20new%20baseline%0Afor%20open-source%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvery%2520Step%2520Evolves%253A%2520Scaling%2520Reinforcement%2520Learning%2520for%2520Trillion-Scale%250A%2520%2520Thinking%2520Model%26entry.906535625%3D%2520Ling%2520Team%2520and%2520Anqi%2520Shen%2520and%2520Baihui%2520Li%2520and%2520Bin%2520Hu%2520and%2520Bin%2520Jing%2520and%2520Cai%2520Chen%2520and%2520Chao%2520Huang%2520and%2520Chao%2520Zhang%2520and%2520Chaokun%2520Yang%2520and%2520Cheng%2520Lin%2520and%2520Chengyao%2520Wen%2520and%2520Congqi%2520Li%2520and%2520Deng%2520Zhao%2520and%2520Dingbo%2520Yuan%2520and%2520Donghai%2520You%2520and%2520Fagui%2520Mao%2520and%2520Fanzhuang%2520Meng%2520and%2520Feng%2520Xu%2520and%2520Guojie%2520Li%2520and%2520Guowei%2520Wang%2520and%2520Hao%2520Dai%2520and%2520Haonan%2520Zheng%2520and%2520Hong%2520Liu%2520and%2520Jia%2520Guo%2520and%2520Jiaming%2520Liu%2520and%2520Jian%2520Liu%2520and%2520Jianhao%2520Fu%2520and%2520Jiannan%2520Shi%2520and%2520Jianwen%2520Wang%2520and%2520Jianxin%2520Lai%2520and%2520Jin%2520Yang%2520and%2520Jun%2520Mei%2520and%2520Jun%2520Zhou%2520and%2520Junbo%2520Zhao%2520and%2520Junping%2520Zhao%2520and%2520Kuan%2520Xu%2520and%2520Le%2520Su%2520and%2520Lei%2520Chen%2520and%2520Li%2520Tang%2520and%2520Liang%2520Jiang%2520and%2520Liangcheng%2520Fu%2520and%2520Lianhao%2520Xu%2520and%2520Linfeng%2520Shi%2520and%2520Lisha%2520Liao%2520and%2520Longfei%2520Zheng%2520and%2520Meng%2520Li%2520and%2520Mingchun%2520Chen%2520and%2520Qi%2520Zuo%2520and%2520Qiang%2520Cheng%2520and%2520Qianggang%2520Cao%2520and%2520Qitao%2520Shi%2520and%2520Quanrui%2520Guo%2520and%2520Senlin%2520Zhu%2520and%2520Shaofei%2520Wang%2520and%2520Shaomian%2520Zheng%2520and%2520Shuaicheng%2520Li%2520and%2520Shuwei%2520Gu%2520and%2520Siba%2520Chen%2520and%2520Tao%2520Wu%2520and%2520Tao%2520Zhang%2520and%2520Tianyu%2520Zhang%2520and%2520Tianyu%2520Zhou%2520and%2520Tiwei%2520Bie%2520and%2520Tongkai%2520Yang%2520and%2520Wang%2520Hong%2520and%2520Wang%2520Ren%2520and%2520Weihua%2520Chen%2520and%2520Wenbo%2520Yu%2520and%2520Wengang%2520Zheng%2520and%2520Xiangchun%2520Wang%2520and%2520Xiaodong%2520Yan%2520and%2520Xiaopei%2520Wan%2520and%2520Xin%2520Zhao%2520and%2520Xinyu%2520Kong%2520and%2520Xinyu%2520Tang%2520and%2520Xudong%2520Han%2520and%2520Xudong%2520Wang%2520and%2520Xuemin%2520Yang%2520and%2520Xueyu%2520Hu%2520and%2520Yalin%2520Zhang%2520and%2520Yan%2520Sun%2520and%2520Yicheng%2520Shan%2520and%2520Yilong%2520Wang%2520and%2520Yingying%2520Xu%2520and%2520Yongkang%2520Liu%2520and%2520Yongzhen%2520Guo%2520and%2520Yuanyuan%2520Wang%2520and%2520Yuchen%2520Yan%2520and%2520Yuefan%2520Wang%2520and%2520Yuhong%2520Guo%2520and%2520Zehuan%2520Li%2520and%2520Zhankai%2520Xu%2520and%2520Zhe%2520Li%2520and%2520Zhenduo%2520Zhang%2520and%2520Zhengke%2520Gui%2520and%2520Zhenxuan%2520Pan%2520and%2520Zhenyu%2520Huang%2520and%2520Zhenzhong%2520Lan%2520and%2520Zhiqiang%2520Ding%2520and%2520Zhiqiang%2520Zhang%2520and%2520Zhixun%2520Li%2520and%2520Zhizhen%2520Liu%2520and%2520Zihao%2520Wang%2520and%2520Zujie%2520Wen%26entry.1292438233%3D%2520%2520We%2520present%2520Ring-1T%252C%2520the%2520first%2520open-source%252C%2520state-of-the-art%2520thinking%2520model%250Awith%2520a%2520trillion-scale%2520parameter.%2520It%2520features%25201%2520trillion%2520total%2520parameters%2520and%250Aactivates%2520approximately%252050%2520billion%2520per%2520token.%2520Training%2520such%2520models%2520at%2520a%250Atrillion-parameter%2520scale%2520introduces%2520unprecedented%2520challenges%252C%2520including%250Atrain-inference%2520misalignment%252C%2520inefficiencies%2520in%2520rollout%2520processing%252C%2520and%250Abottlenecks%2520in%2520the%2520RL%2520system.%2520To%2520address%2520these%252C%2520we%2520pioneer%2520three%2520interconnected%250Ainnovations%253A%2520%25281%2529%2520IcePop%2520stabilizes%2520RL%2520training%2520via%2520token-level%2520discrepancy%250Amasking%2520and%2520clipping%252C%2520resolving%2520instability%2520from%2520training-inference%2520mismatches%253B%250A%25282%2529%2520C3PO%252B%252B%2520improves%2520resource%2520utilization%2520for%2520long%2520rollouts%2520under%2520a%2520token%2520budget%250Aby%2520dynamically%2520partitioning%2520them%252C%2520thereby%2520obtaining%2520high%2520time%2520efficiency%253B%2520and%250A%25283%2529%2520ASystem%252C%2520a%2520high-performance%2520RL%2520framework%2520designed%2520to%2520overcome%2520the%2520systemic%250Abottlenecks%2520that%2520impede%2520trillion-parameter%2520model%2520training.%2520Ring-1T%2520delivers%250Abreakthrough%2520results%2520across%2520critical%2520benchmarks%253A%252093.4%2520on%2520AIME-2025%252C%252086.72%2520on%250AHMMT-2025%252C%25202088%2520on%2520CodeForces%252C%2520and%252055.94%2520on%2520ARC-AGI-v1.%2520Notably%252C%2520it%2520attains%2520a%250Asilver%2520medal-level%2520result%2520on%2520the%2520IMO-2025%252C%2520underscoring%2520its%2520exceptional%250Areasoning%2520capabilities.%2520By%2520releasing%2520the%2520complete%25201T%2520parameter%2520MoE%2520model%2520to%2520the%250Acommunity%252C%2520we%2520provide%2520the%2520research%2520community%2520with%2520direct%2520access%2520to%2520cutting-edge%250Areasoning%2520capabilities.%2520This%2520contribution%2520marks%2520a%2520significant%2520milestone%2520in%250Ademocratizing%2520large-scale%2520reasoning%2520intelligence%2520and%2520establishes%2520a%2520new%2520baseline%250Afor%2520open-source%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Every%20Step%20Evolves%3A%20Scaling%20Reinforcement%20Learning%20for%20Trillion-Scale%0A%20%20Thinking%20Model&entry.906535625=%20Ling%20Team%20and%20Anqi%20Shen%20and%20Baihui%20Li%20and%20Bin%20Hu%20and%20Bin%20Jing%20and%20Cai%20Chen%20and%20Chao%20Huang%20and%20Chao%20Zhang%20and%20Chaokun%20Yang%20and%20Cheng%20Lin%20and%20Chengyao%20Wen%20and%20Congqi%20Li%20and%20Deng%20Zhao%20and%20Dingbo%20Yuan%20and%20Donghai%20You%20and%20Fagui%20Mao%20and%20Fanzhuang%20Meng%20and%20Feng%20Xu%20and%20Guojie%20Li%20and%20Guowei%20Wang%20and%20Hao%20Dai%20and%20Haonan%20Zheng%20and%20Hong%20Liu%20and%20Jia%20Guo%20and%20Jiaming%20Liu%20and%20Jian%20Liu%20and%20Jianhao%20Fu%20and%20Jiannan%20Shi%20and%20Jianwen%20Wang%20and%20Jianxin%20Lai%20and%20Jin%20Yang%20and%20Jun%20Mei%20and%20Jun%20Zhou%20and%20Junbo%20Zhao%20and%20Junping%20Zhao%20and%20Kuan%20Xu%20and%20Le%20Su%20and%20Lei%20Chen%20and%20Li%20Tang%20and%20Liang%20Jiang%20and%20Liangcheng%20Fu%20and%20Lianhao%20Xu%20and%20Linfeng%20Shi%20and%20Lisha%20Liao%20and%20Longfei%20Zheng%20and%20Meng%20Li%20and%20Mingchun%20Chen%20and%20Qi%20Zuo%20and%20Qiang%20Cheng%20and%20Qianggang%20Cao%20and%20Qitao%20Shi%20and%20Quanrui%20Guo%20and%20Senlin%20Zhu%20and%20Shaofei%20Wang%20and%20Shaomian%20Zheng%20and%20Shuaicheng%20Li%20and%20Shuwei%20Gu%20and%20Siba%20Chen%20and%20Tao%20Wu%20and%20Tao%20Zhang%20and%20Tianyu%20Zhang%20and%20Tianyu%20Zhou%20and%20Tiwei%20Bie%20and%20Tongkai%20Yang%20and%20Wang%20Hong%20and%20Wang%20Ren%20and%20Weihua%20Chen%20and%20Wenbo%20Yu%20and%20Wengang%20Zheng%20and%20Xiangchun%20Wang%20and%20Xiaodong%20Yan%20and%20Xiaopei%20Wan%20and%20Xin%20Zhao%20and%20Xinyu%20Kong%20and%20Xinyu%20Tang%20and%20Xudong%20Han%20and%20Xudong%20Wang%20and%20Xuemin%20Yang%20and%20Xueyu%20Hu%20and%20Yalin%20Zhang%20and%20Yan%20Sun%20and%20Yicheng%20Shan%20and%20Yilong%20Wang%20and%20Yingying%20Xu%20and%20Yongkang%20Liu%20and%20Yongzhen%20Guo%20and%20Yuanyuan%20Wang%20and%20Yuchen%20Yan%20and%20Yuefan%20Wang%20and%20Yuhong%20Guo%20and%20Zehuan%20Li%20and%20Zhankai%20Xu%20and%20Zhe%20Li%20and%20Zhenduo%20Zhang%20and%20Zhengke%20Gui%20and%20Zhenxuan%20Pan%20and%20Zhenyu%20Huang%20and%20Zhenzhong%20Lan%20and%20Zhiqiang%20Ding%20and%20Zhiqiang%20Zhang%20and%20Zhixun%20Li%20and%20Zhizhen%20Liu%20and%20Zihao%20Wang%20and%20Zujie%20Wen&entry.1292438233=%20%20We%20present%20Ring-1T%2C%20the%20first%20open-source%2C%20state-of-the-art%20thinking%20model%0Awith%20a%20trillion-scale%20parameter.%20It%20features%201%20trillion%20total%20parameters%20and%0Aactivates%20approximately%2050%20billion%20per%20token.%20Training%20such%20models%20at%20a%0Atrillion-parameter%20scale%20introduces%20unprecedented%20challenges%2C%20including%0Atrain-inference%20misalignment%2C%20inefficiencies%20in%20rollout%20processing%2C%20and%0Abottlenecks%20in%20the%20RL%20system.%20To%20address%20these%2C%20we%20pioneer%20three%20interconnected%0Ainnovations%3A%20%281%29%20IcePop%20stabilizes%20RL%20training%20via%20token-level%20discrepancy%0Amasking%20and%20clipping%2C%20resolving%20instability%20from%20training-inference%20mismatches%3B%0A%282%29%20C3PO%2B%2B%20improves%20resource%20utilization%20for%20long%20rollouts%20under%20a%20token%20budget%0Aby%20dynamically%20partitioning%20them%2C%20thereby%20obtaining%20high%20time%20efficiency%3B%20and%0A%283%29%20ASystem%2C%20a%20high-performance%20RL%20framework%20designed%20to%20overcome%20the%20systemic%0Abottlenecks%20that%20impede%20trillion-parameter%20model%20training.%20Ring-1T%20delivers%0Abreakthrough%20results%20across%20critical%20benchmarks%3A%2093.4%20on%20AIME-2025%2C%2086.72%20on%0AHMMT-2025%2C%202088%20on%20CodeForces%2C%20and%2055.94%20on%20ARC-AGI-v1.%20Notably%2C%20it%20attains%20a%0Asilver%20medal-level%20result%20on%20the%20IMO-2025%2C%20underscoring%20its%20exceptional%0Areasoning%20capabilities.%20By%20releasing%20the%20complete%201T%20parameter%20MoE%20model%20to%20the%0Acommunity%2C%20we%20provide%20the%20research%20community%20with%20direct%20access%20to%20cutting-edge%0Areasoning%20capabilities.%20This%20contribution%20marks%20a%20significant%20milestone%20in%0Ademocratizing%20large-scale%20reasoning%20intelligence%20and%20establishes%20a%20new%20baseline%0Afor%20open-source%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18855v1&entry.124074799=Read"},
{"title": "Rethink Repeatable Measures of Robot Performance with Statistical Query", "author": "Bowen Weng and Linda Capito and Guillermo A. Castillo and Dylan Khor", "abstract": "  For a general standardized testing algorithm designed to evaluate a specific\naspect of a robot's performance, several key expectations are commonly imposed.\nBeyond accuracy (i.e., closeness to a typically unknown ground-truth reference)\nand efficiency (i.e., feasibility within acceptable testing costs and equipment\nconstraints), one particularly important attribute is repeatability.\nRepeatability refers to the ability to consistently obtain the same testing\noutcome when similar testing algorithms are executed on the same subject robot\nby different stakeholders, across different times or locations. However,\nachieving repeatable testing has become increasingly challenging as the\ncomponents involved grow more complex, intelligent, diverse, and, most\nimportantly, stochastic. While related efforts have addressed repeatability at\nethical, hardware, and procedural levels, this study focuses specifically on\nrepeatable testing at the algorithmic level. Specifically, we target the\nwell-adopted class of testing algorithms in standardized evaluation:\nstatistical query (SQ) algorithms (i.e., algorithms that estimate the expected\nvalue of a bounded function over a distribution using sampled data). We propose\na lightweight, parameterized, and adaptive modification applicable to any SQ\nroutine, whether based on Monte Carlo sampling, importance sampling, or\nadaptive importance sampling, that makes it provably repeatable, with\nguaranteed bounds on both accuracy and efficiency. We demonstrate the\neffectiveness of the proposed approach across three representative scenarios:\n(i) established and widely adopted standardized testing of manipulators, (ii)\nemerging intelligent testing algorithms for operational risk assessment in\nautomated vehicles, and (iii) developing use cases involving command tracking\nperformance evaluation of humanoid robots in locomotion tasks.\n", "link": "http://arxiv.org/abs/2505.08216v3", "date": "2025-10-21", "relevancy": 1.414, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5113}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4774}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethink%20Repeatable%20Measures%20of%20Robot%20Performance%20with%20Statistical%20Query&body=Title%3A%20Rethink%20Repeatable%20Measures%20of%20Robot%20Performance%20with%20Statistical%20Query%0AAuthor%3A%20Bowen%20Weng%20and%20Linda%20Capito%20and%20Guillermo%20A.%20Castillo%20and%20Dylan%20Khor%0AAbstract%3A%20%20%20For%20a%20general%20standardized%20testing%20algorithm%20designed%20to%20evaluate%20a%20specific%0Aaspect%20of%20a%20robot%27s%20performance%2C%20several%20key%20expectations%20are%20commonly%20imposed.%0ABeyond%20accuracy%20%28i.e.%2C%20closeness%20to%20a%20typically%20unknown%20ground-truth%20reference%29%0Aand%20efficiency%20%28i.e.%2C%20feasibility%20within%20acceptable%20testing%20costs%20and%20equipment%0Aconstraints%29%2C%20one%20particularly%20important%20attribute%20is%20repeatability.%0ARepeatability%20refers%20to%20the%20ability%20to%20consistently%20obtain%20the%20same%20testing%0Aoutcome%20when%20similar%20testing%20algorithms%20are%20executed%20on%20the%20same%20subject%20robot%0Aby%20different%20stakeholders%2C%20across%20different%20times%20or%20locations.%20However%2C%0Aachieving%20repeatable%20testing%20has%20become%20increasingly%20challenging%20as%20the%0Acomponents%20involved%20grow%20more%20complex%2C%20intelligent%2C%20diverse%2C%20and%2C%20most%0Aimportantly%2C%20stochastic.%20While%20related%20efforts%20have%20addressed%20repeatability%20at%0Aethical%2C%20hardware%2C%20and%20procedural%20levels%2C%20this%20study%20focuses%20specifically%20on%0Arepeatable%20testing%20at%20the%20algorithmic%20level.%20Specifically%2C%20we%20target%20the%0Awell-adopted%20class%20of%20testing%20algorithms%20in%20standardized%20evaluation%3A%0Astatistical%20query%20%28SQ%29%20algorithms%20%28i.e.%2C%20algorithms%20that%20estimate%20the%20expected%0Avalue%20of%20a%20bounded%20function%20over%20a%20distribution%20using%20sampled%20data%29.%20We%20propose%0Aa%20lightweight%2C%20parameterized%2C%20and%20adaptive%20modification%20applicable%20to%20any%20SQ%0Aroutine%2C%20whether%20based%20on%20Monte%20Carlo%20sampling%2C%20importance%20sampling%2C%20or%0Aadaptive%20importance%20sampling%2C%20that%20makes%20it%20provably%20repeatable%2C%20with%0Aguaranteed%20bounds%20on%20both%20accuracy%20and%20efficiency.%20We%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approach%20across%20three%20representative%20scenarios%3A%0A%28i%29%20established%20and%20widely%20adopted%20standardized%20testing%20of%20manipulators%2C%20%28ii%29%0Aemerging%20intelligent%20testing%20algorithms%20for%20operational%20risk%20assessment%20in%0Aautomated%20vehicles%2C%20and%20%28iii%29%20developing%20use%20cases%20involving%20command%20tracking%0Aperformance%20evaluation%20of%20humanoid%20robots%20in%20locomotion%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08216v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethink%2520Repeatable%2520Measures%2520of%2520Robot%2520Performance%2520with%2520Statistical%2520Query%26entry.906535625%3DBowen%2520Weng%2520and%2520Linda%2520Capito%2520and%2520Guillermo%2520A.%2520Castillo%2520and%2520Dylan%2520Khor%26entry.1292438233%3D%2520%2520For%2520a%2520general%2520standardized%2520testing%2520algorithm%2520designed%2520to%2520evaluate%2520a%2520specific%250Aaspect%2520of%2520a%2520robot%2527s%2520performance%252C%2520several%2520key%2520expectations%2520are%2520commonly%2520imposed.%250ABeyond%2520accuracy%2520%2528i.e.%252C%2520closeness%2520to%2520a%2520typically%2520unknown%2520ground-truth%2520reference%2529%250Aand%2520efficiency%2520%2528i.e.%252C%2520feasibility%2520within%2520acceptable%2520testing%2520costs%2520and%2520equipment%250Aconstraints%2529%252C%2520one%2520particularly%2520important%2520attribute%2520is%2520repeatability.%250ARepeatability%2520refers%2520to%2520the%2520ability%2520to%2520consistently%2520obtain%2520the%2520same%2520testing%250Aoutcome%2520when%2520similar%2520testing%2520algorithms%2520are%2520executed%2520on%2520the%2520same%2520subject%2520robot%250Aby%2520different%2520stakeholders%252C%2520across%2520different%2520times%2520or%2520locations.%2520However%252C%250Aachieving%2520repeatable%2520testing%2520has%2520become%2520increasingly%2520challenging%2520as%2520the%250Acomponents%2520involved%2520grow%2520more%2520complex%252C%2520intelligent%252C%2520diverse%252C%2520and%252C%2520most%250Aimportantly%252C%2520stochastic.%2520While%2520related%2520efforts%2520have%2520addressed%2520repeatability%2520at%250Aethical%252C%2520hardware%252C%2520and%2520procedural%2520levels%252C%2520this%2520study%2520focuses%2520specifically%2520on%250Arepeatable%2520testing%2520at%2520the%2520algorithmic%2520level.%2520Specifically%252C%2520we%2520target%2520the%250Awell-adopted%2520class%2520of%2520testing%2520algorithms%2520in%2520standardized%2520evaluation%253A%250Astatistical%2520query%2520%2528SQ%2529%2520algorithms%2520%2528i.e.%252C%2520algorithms%2520that%2520estimate%2520the%2520expected%250Avalue%2520of%2520a%2520bounded%2520function%2520over%2520a%2520distribution%2520using%2520sampled%2520data%2529.%2520We%2520propose%250Aa%2520lightweight%252C%2520parameterized%252C%2520and%2520adaptive%2520modification%2520applicable%2520to%2520any%2520SQ%250Aroutine%252C%2520whether%2520based%2520on%2520Monte%2520Carlo%2520sampling%252C%2520importance%2520sampling%252C%2520or%250Aadaptive%2520importance%2520sampling%252C%2520that%2520makes%2520it%2520provably%2520repeatable%252C%2520with%250Aguaranteed%2520bounds%2520on%2520both%2520accuracy%2520and%2520efficiency.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520approach%2520across%2520three%2520representative%2520scenarios%253A%250A%2528i%2529%2520established%2520and%2520widely%2520adopted%2520standardized%2520testing%2520of%2520manipulators%252C%2520%2528ii%2529%250Aemerging%2520intelligent%2520testing%2520algorithms%2520for%2520operational%2520risk%2520assessment%2520in%250Aautomated%2520vehicles%252C%2520and%2520%2528iii%2529%2520developing%2520use%2520cases%2520involving%2520command%2520tracking%250Aperformance%2520evaluation%2520of%2520humanoid%2520robots%2520in%2520locomotion%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08216v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethink%20Repeatable%20Measures%20of%20Robot%20Performance%20with%20Statistical%20Query&entry.906535625=Bowen%20Weng%20and%20Linda%20Capito%20and%20Guillermo%20A.%20Castillo%20and%20Dylan%20Khor&entry.1292438233=%20%20For%20a%20general%20standardized%20testing%20algorithm%20designed%20to%20evaluate%20a%20specific%0Aaspect%20of%20a%20robot%27s%20performance%2C%20several%20key%20expectations%20are%20commonly%20imposed.%0ABeyond%20accuracy%20%28i.e.%2C%20closeness%20to%20a%20typically%20unknown%20ground-truth%20reference%29%0Aand%20efficiency%20%28i.e.%2C%20feasibility%20within%20acceptable%20testing%20costs%20and%20equipment%0Aconstraints%29%2C%20one%20particularly%20important%20attribute%20is%20repeatability.%0ARepeatability%20refers%20to%20the%20ability%20to%20consistently%20obtain%20the%20same%20testing%0Aoutcome%20when%20similar%20testing%20algorithms%20are%20executed%20on%20the%20same%20subject%20robot%0Aby%20different%20stakeholders%2C%20across%20different%20times%20or%20locations.%20However%2C%0Aachieving%20repeatable%20testing%20has%20become%20increasingly%20challenging%20as%20the%0Acomponents%20involved%20grow%20more%20complex%2C%20intelligent%2C%20diverse%2C%20and%2C%20most%0Aimportantly%2C%20stochastic.%20While%20related%20efforts%20have%20addressed%20repeatability%20at%0Aethical%2C%20hardware%2C%20and%20procedural%20levels%2C%20this%20study%20focuses%20specifically%20on%0Arepeatable%20testing%20at%20the%20algorithmic%20level.%20Specifically%2C%20we%20target%20the%0Awell-adopted%20class%20of%20testing%20algorithms%20in%20standardized%20evaluation%3A%0Astatistical%20query%20%28SQ%29%20algorithms%20%28i.e.%2C%20algorithms%20that%20estimate%20the%20expected%0Avalue%20of%20a%20bounded%20function%20over%20a%20distribution%20using%20sampled%20data%29.%20We%20propose%0Aa%20lightweight%2C%20parameterized%2C%20and%20adaptive%20modification%20applicable%20to%20any%20SQ%0Aroutine%2C%20whether%20based%20on%20Monte%20Carlo%20sampling%2C%20importance%20sampling%2C%20or%0Aadaptive%20importance%20sampling%2C%20that%20makes%20it%20provably%20repeatable%2C%20with%0Aguaranteed%20bounds%20on%20both%20accuracy%20and%20efficiency.%20We%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approach%20across%20three%20representative%20scenarios%3A%0A%28i%29%20established%20and%20widely%20adopted%20standardized%20testing%20of%20manipulators%2C%20%28ii%29%0Aemerging%20intelligent%20testing%20algorithms%20for%20operational%20risk%20assessment%20in%0Aautomated%20vehicles%2C%20and%20%28iii%29%20developing%20use%20cases%20involving%20command%20tracking%0Aperformance%20evaluation%20of%20humanoid%20robots%20in%20locomotion%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08216v3&entry.124074799=Read"},
{"title": "An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom\n  Detection", "author": "Neel Patel and Alexander Wong and Ashkan Ebadi", "abstract": "  Tuberculosis remains a critical global health issue, particularly in\nresource-limited and remote areas. Early detection is vital for treatment, yet\nthe lack of skilled radiologists underscores the need for artificial\nintelligence (AI)-driven screening tools. Developing reliable AI models is\nchallenging due to the necessity for large, high-quality datasets, which are\ncostly to obtain. To tackle this, we propose a teacher--student framework which\nenhances both disease and symptom detection on chest X-rays by integrating two\nsupervised heads and a self-supervised head. Our model achieves an accuracy of\n98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and\na macro-F1 score of 90.09% for multilabel symptom detection, significantly\noutperforming baselines. The explainability assessments also show the model\nbases its predictions on relevant anatomical features, demonstrating promise\nfor deployment in clinical screening and triage settings.\n", "link": "http://arxiv.org/abs/2510.18819v1", "date": "2025-10-21", "relevancy": 1.4138, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4894}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4673}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Explainable%20Hybrid%20AI%20Framework%20for%20Enhanced%20Tuberculosis%20and%20Symptom%0A%20%20Detection&body=Title%3A%20An%20Explainable%20Hybrid%20AI%20Framework%20for%20Enhanced%20Tuberculosis%20and%20Symptom%0A%20%20Detection%0AAuthor%3A%20Neel%20Patel%20and%20Alexander%20Wong%20and%20Ashkan%20Ebadi%0AAbstract%3A%20%20%20Tuberculosis%20remains%20a%20critical%20global%20health%20issue%2C%20particularly%20in%0Aresource-limited%20and%20remote%20areas.%20Early%20detection%20is%20vital%20for%20treatment%2C%20yet%0Athe%20lack%20of%20skilled%20radiologists%20underscores%20the%20need%20for%20artificial%0Aintelligence%20%28AI%29-driven%20screening%20tools.%20Developing%20reliable%20AI%20models%20is%0Achallenging%20due%20to%20the%20necessity%20for%20large%2C%20high-quality%20datasets%2C%20which%20are%0Acostly%20to%20obtain.%20To%20tackle%20this%2C%20we%20propose%20a%20teacher--student%20framework%20which%0Aenhances%20both%20disease%20and%20symptom%20detection%20on%20chest%20X-rays%20by%20integrating%20two%0Asupervised%20heads%20and%20a%20self-supervised%20head.%20Our%20model%20achieves%20an%20accuracy%20of%0A98.85%25%20for%20distinguishing%20between%20COVID-19%2C%20tuberculosis%2C%20and%20normal%20cases%2C%20and%0Aa%20macro-F1%20score%20of%2090.09%25%20for%20multilabel%20symptom%20detection%2C%20significantly%0Aoutperforming%20baselines.%20The%20explainability%20assessments%20also%20show%20the%20model%0Abases%20its%20predictions%20on%20relevant%20anatomical%20features%2C%20demonstrating%20promise%0Afor%20deployment%20in%20clinical%20screening%20and%20triage%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Explainable%2520Hybrid%2520AI%2520Framework%2520for%2520Enhanced%2520Tuberculosis%2520and%2520Symptom%250A%2520%2520Detection%26entry.906535625%3DNeel%2520Patel%2520and%2520Alexander%2520Wong%2520and%2520Ashkan%2520Ebadi%26entry.1292438233%3D%2520%2520Tuberculosis%2520remains%2520a%2520critical%2520global%2520health%2520issue%252C%2520particularly%2520in%250Aresource-limited%2520and%2520remote%2520areas.%2520Early%2520detection%2520is%2520vital%2520for%2520treatment%252C%2520yet%250Athe%2520lack%2520of%2520skilled%2520radiologists%2520underscores%2520the%2520need%2520for%2520artificial%250Aintelligence%2520%2528AI%2529-driven%2520screening%2520tools.%2520Developing%2520reliable%2520AI%2520models%2520is%250Achallenging%2520due%2520to%2520the%2520necessity%2520for%2520large%252C%2520high-quality%2520datasets%252C%2520which%2520are%250Acostly%2520to%2520obtain.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520a%2520teacher--student%2520framework%2520which%250Aenhances%2520both%2520disease%2520and%2520symptom%2520detection%2520on%2520chest%2520X-rays%2520by%2520integrating%2520two%250Asupervised%2520heads%2520and%2520a%2520self-supervised%2520head.%2520Our%2520model%2520achieves%2520an%2520accuracy%2520of%250A98.85%2525%2520for%2520distinguishing%2520between%2520COVID-19%252C%2520tuberculosis%252C%2520and%2520normal%2520cases%252C%2520and%250Aa%2520macro-F1%2520score%2520of%252090.09%2525%2520for%2520multilabel%2520symptom%2520detection%252C%2520significantly%250Aoutperforming%2520baselines.%2520The%2520explainability%2520assessments%2520also%2520show%2520the%2520model%250Abases%2520its%2520predictions%2520on%2520relevant%2520anatomical%2520features%252C%2520demonstrating%2520promise%250Afor%2520deployment%2520in%2520clinical%2520screening%2520and%2520triage%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Explainable%20Hybrid%20AI%20Framework%20for%20Enhanced%20Tuberculosis%20and%20Symptom%0A%20%20Detection&entry.906535625=Neel%20Patel%20and%20Alexander%20Wong%20and%20Ashkan%20Ebadi&entry.1292438233=%20%20Tuberculosis%20remains%20a%20critical%20global%20health%20issue%2C%20particularly%20in%0Aresource-limited%20and%20remote%20areas.%20Early%20detection%20is%20vital%20for%20treatment%2C%20yet%0Athe%20lack%20of%20skilled%20radiologists%20underscores%20the%20need%20for%20artificial%0Aintelligence%20%28AI%29-driven%20screening%20tools.%20Developing%20reliable%20AI%20models%20is%0Achallenging%20due%20to%20the%20necessity%20for%20large%2C%20high-quality%20datasets%2C%20which%20are%0Acostly%20to%20obtain.%20To%20tackle%20this%2C%20we%20propose%20a%20teacher--student%20framework%20which%0Aenhances%20both%20disease%20and%20symptom%20detection%20on%20chest%20X-rays%20by%20integrating%20two%0Asupervised%20heads%20and%20a%20self-supervised%20head.%20Our%20model%20achieves%20an%20accuracy%20of%0A98.85%25%20for%20distinguishing%20between%20COVID-19%2C%20tuberculosis%2C%20and%20normal%20cases%2C%20and%0Aa%20macro-F1%20score%20of%2090.09%25%20for%20multilabel%20symptom%20detection%2C%20significantly%0Aoutperforming%20baselines.%20The%20explainability%20assessments%20also%20show%20the%20model%0Abases%20its%20predictions%20on%20relevant%20anatomical%20features%2C%20demonstrating%20promise%0Afor%20deployment%20in%20clinical%20screening%20and%20triage%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18819v1&entry.124074799=Read"},
{"title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned\n  Geospatial Foundation Model for Microclimate Impact Prediction", "author": "Jannis Fleckenstein and David Kreismann and Tamara Rosemary Govindasamy and Thomas Brunschwiler and Etienne Vos and Mattia Rigotti", "abstract": "  As urbanization and climate change progress, urban heat island effects are\nbecoming more frequent and severe. To formulate effective mitigation plans,\ncities require detailed air temperature data, yet conventional machine learning\nmodels with limited data often produce inaccurate predictions, particularly in\nunderserved areas. Geospatial foundation models trained on global unstructured\ndata offer a promising alternative by demonstrating strong generalization and\nrequiring only minimal fine-tuning. In this study, an empirical ground truth of\nurban heat patterns is established by quantifying cooling effects from green\nspaces and benchmarking them against model predictions to evaluate the model's\naccuracy. The foundation model is subsequently fine-tuned to predict land\nsurface temperatures under future climate scenarios, and its practical value is\ndemonstrated through a simulated inpainting that highlights its role for\nmitigation support. The results indicate that foundation models offer a\npowerful way for evaluating urban heat island mitigation strategies in\ndata-scarce regions to support more climate-resilient cities.\n", "link": "http://arxiv.org/abs/2510.18773v1", "date": "2025-10-21", "relevancy": 1.4028, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4853}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4749}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20and%20Simulation%20of%20Urban%20Heat%20Islands%20Using%20a%20Fine-Tuned%0A%20%20Geospatial%20Foundation%20Model%20for%20Microclimate%20Impact%20Prediction&body=Title%3A%20Detection%20and%20Simulation%20of%20Urban%20Heat%20Islands%20Using%20a%20Fine-Tuned%0A%20%20Geospatial%20Foundation%20Model%20for%20Microclimate%20Impact%20Prediction%0AAuthor%3A%20Jannis%20Fleckenstein%20and%20David%20Kreismann%20and%20Tamara%20Rosemary%20Govindasamy%20and%20Thomas%20Brunschwiler%20and%20Etienne%20Vos%20and%20Mattia%20Rigotti%0AAbstract%3A%20%20%20As%20urbanization%20and%20climate%20change%20progress%2C%20urban%20heat%20island%20effects%20are%0Abecoming%20more%20frequent%20and%20severe.%20To%20formulate%20effective%20mitigation%20plans%2C%0Acities%20require%20detailed%20air%20temperature%20data%2C%20yet%20conventional%20machine%20learning%0Amodels%20with%20limited%20data%20often%20produce%20inaccurate%20predictions%2C%20particularly%20in%0Aunderserved%20areas.%20Geospatial%20foundation%20models%20trained%20on%20global%20unstructured%0Adata%20offer%20a%20promising%20alternative%20by%20demonstrating%20strong%20generalization%20and%0Arequiring%20only%20minimal%20fine-tuning.%20In%20this%20study%2C%20an%20empirical%20ground%20truth%20of%0Aurban%20heat%20patterns%20is%20established%20by%20quantifying%20cooling%20effects%20from%20green%0Aspaces%20and%20benchmarking%20them%20against%20model%20predictions%20to%20evaluate%20the%20model%27s%0Aaccuracy.%20The%20foundation%20model%20is%20subsequently%20fine-tuned%20to%20predict%20land%0Asurface%20temperatures%20under%20future%20climate%20scenarios%2C%20and%20its%20practical%20value%20is%0Ademonstrated%20through%20a%20simulated%20inpainting%20that%20highlights%20its%20role%20for%0Amitigation%20support.%20The%20results%20indicate%20that%20foundation%20models%20offer%20a%0Apowerful%20way%20for%20evaluating%20urban%20heat%20island%20mitigation%20strategies%20in%0Adata-scarce%20regions%20to%20support%20more%20climate-resilient%20cities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520and%2520Simulation%2520of%2520Urban%2520Heat%2520Islands%2520Using%2520a%2520Fine-Tuned%250A%2520%2520Geospatial%2520Foundation%2520Model%2520for%2520Microclimate%2520Impact%2520Prediction%26entry.906535625%3DJannis%2520Fleckenstein%2520and%2520David%2520Kreismann%2520and%2520Tamara%2520Rosemary%2520Govindasamy%2520and%2520Thomas%2520Brunschwiler%2520and%2520Etienne%2520Vos%2520and%2520Mattia%2520Rigotti%26entry.1292438233%3D%2520%2520As%2520urbanization%2520and%2520climate%2520change%2520progress%252C%2520urban%2520heat%2520island%2520effects%2520are%250Abecoming%2520more%2520frequent%2520and%2520severe.%2520To%2520formulate%2520effective%2520mitigation%2520plans%252C%250Acities%2520require%2520detailed%2520air%2520temperature%2520data%252C%2520yet%2520conventional%2520machine%2520learning%250Amodels%2520with%2520limited%2520data%2520often%2520produce%2520inaccurate%2520predictions%252C%2520particularly%2520in%250Aunderserved%2520areas.%2520Geospatial%2520foundation%2520models%2520trained%2520on%2520global%2520unstructured%250Adata%2520offer%2520a%2520promising%2520alternative%2520by%2520demonstrating%2520strong%2520generalization%2520and%250Arequiring%2520only%2520minimal%2520fine-tuning.%2520In%2520this%2520study%252C%2520an%2520empirical%2520ground%2520truth%2520of%250Aurban%2520heat%2520patterns%2520is%2520established%2520by%2520quantifying%2520cooling%2520effects%2520from%2520green%250Aspaces%2520and%2520benchmarking%2520them%2520against%2520model%2520predictions%2520to%2520evaluate%2520the%2520model%2527s%250Aaccuracy.%2520The%2520foundation%2520model%2520is%2520subsequently%2520fine-tuned%2520to%2520predict%2520land%250Asurface%2520temperatures%2520under%2520future%2520climate%2520scenarios%252C%2520and%2520its%2520practical%2520value%2520is%250Ademonstrated%2520through%2520a%2520simulated%2520inpainting%2520that%2520highlights%2520its%2520role%2520for%250Amitigation%2520support.%2520The%2520results%2520indicate%2520that%2520foundation%2520models%2520offer%2520a%250Apowerful%2520way%2520for%2520evaluating%2520urban%2520heat%2520island%2520mitigation%2520strategies%2520in%250Adata-scarce%2520regions%2520to%2520support%2520more%2520climate-resilient%2520cities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20and%20Simulation%20of%20Urban%20Heat%20Islands%20Using%20a%20Fine-Tuned%0A%20%20Geospatial%20Foundation%20Model%20for%20Microclimate%20Impact%20Prediction&entry.906535625=Jannis%20Fleckenstein%20and%20David%20Kreismann%20and%20Tamara%20Rosemary%20Govindasamy%20and%20Thomas%20Brunschwiler%20and%20Etienne%20Vos%20and%20Mattia%20Rigotti&entry.1292438233=%20%20As%20urbanization%20and%20climate%20change%20progress%2C%20urban%20heat%20island%20effects%20are%0Abecoming%20more%20frequent%20and%20severe.%20To%20formulate%20effective%20mitigation%20plans%2C%0Acities%20require%20detailed%20air%20temperature%20data%2C%20yet%20conventional%20machine%20learning%0Amodels%20with%20limited%20data%20often%20produce%20inaccurate%20predictions%2C%20particularly%20in%0Aunderserved%20areas.%20Geospatial%20foundation%20models%20trained%20on%20global%20unstructured%0Adata%20offer%20a%20promising%20alternative%20by%20demonstrating%20strong%20generalization%20and%0Arequiring%20only%20minimal%20fine-tuning.%20In%20this%20study%2C%20an%20empirical%20ground%20truth%20of%0Aurban%20heat%20patterns%20is%20established%20by%20quantifying%20cooling%20effects%20from%20green%0Aspaces%20and%20benchmarking%20them%20against%20model%20predictions%20to%20evaluate%20the%20model%27s%0Aaccuracy.%20The%20foundation%20model%20is%20subsequently%20fine-tuned%20to%20predict%20land%0Asurface%20temperatures%20under%20future%20climate%20scenarios%2C%20and%20its%20practical%20value%20is%0Ademonstrated%20through%20a%20simulated%20inpainting%20that%20highlights%20its%20role%20for%0Amitigation%20support.%20The%20results%20indicate%20that%20foundation%20models%20offer%20a%0Apowerful%20way%20for%20evaluating%20urban%20heat%20island%20mitigation%20strategies%20in%0Adata-scarce%20regions%20to%20support%20more%20climate-resilient%20cities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18773v1&entry.124074799=Read"},
{"title": "A Hybrid Enumeration Framework for Optimal Counterfactual Generation in\n  Post-Acute COVID-19 Heart Failure", "author": "Jingya Cheng and Alaleh Azhir and Jiazi Tian and Hossein Estiri", "abstract": "  Counterfactual inference provides a mathematical framework for reasoning\nabout hypothetical outcomes under alternative interventions, bridging causal\nreasoning and predictive modeling. We present a counterfactual inference\nframework for individualized risk estimation and intervention analysis,\nillustrated through a clinical application to post-acute sequelae of COVID-19\n(PASC) among patients with pre-existing heart failure (HF). Using longitudinal\ndiagnosis, laboratory, and medication data from a large health-system cohort,\nwe integrate regularized predictive modeling with counterfactual search to\nidentify actionable pathways to PASC-related HF hospital admissions. The\nframework combines exact enumeration with optimization-based methods, including\nthe Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective\nCounterfactuals (MOC) algorithms, to efficiently explore high-dimensional\nintervention spaces. Applied to more than 2700 individuals with confirmed\nSARS-CoV-2 infection and prior HF, the model achieved strong discriminative\nperformance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable,\npatient-specific counterfactuals that quantify how modifying comorbidity\npatterns or treatment factors could alter predicted outcomes. This work\ndemonstrates how counterfactual reasoning can be formalized as an optimization\nproblem over predictive functions, offering a rigorous, interpretable, and\ncomputationally efficient approach to personalized inference in complex\nbiomedical systems.\n", "link": "http://arxiv.org/abs/2510.18841v1", "date": "2025-10-21", "relevancy": 1.3005, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4493}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4384}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Enumeration%20Framework%20for%20Optimal%20Counterfactual%20Generation%20in%0A%20%20Post-Acute%20COVID-19%20Heart%20Failure&body=Title%3A%20A%20Hybrid%20Enumeration%20Framework%20for%20Optimal%20Counterfactual%20Generation%20in%0A%20%20Post-Acute%20COVID-19%20Heart%20Failure%0AAuthor%3A%20Jingya%20Cheng%20and%20Alaleh%20Azhir%20and%20Jiazi%20Tian%20and%20Hossein%20Estiri%0AAbstract%3A%20%20%20Counterfactual%20inference%20provides%20a%20mathematical%20framework%20for%20reasoning%0Aabout%20hypothetical%20outcomes%20under%20alternative%20interventions%2C%20bridging%20causal%0Areasoning%20and%20predictive%20modeling.%20We%20present%20a%20counterfactual%20inference%0Aframework%20for%20individualized%20risk%20estimation%20and%20intervention%20analysis%2C%0Aillustrated%20through%20a%20clinical%20application%20to%20post-acute%20sequelae%20of%20COVID-19%0A%28PASC%29%20among%20patients%20with%20pre-existing%20heart%20failure%20%28HF%29.%20Using%20longitudinal%0Adiagnosis%2C%20laboratory%2C%20and%20medication%20data%20from%20a%20large%20health-system%20cohort%2C%0Awe%20integrate%20regularized%20predictive%20modeling%20with%20counterfactual%20search%20to%0Aidentify%20actionable%20pathways%20to%20PASC-related%20HF%20hospital%20admissions.%20The%0Aframework%20combines%20exact%20enumeration%20with%20optimization-based%20methods%2C%20including%0Athe%20Nearest%20Instance%20Counterfactual%20Explanations%20%28NICE%29%20and%20Multi-Objective%0ACounterfactuals%20%28MOC%29%20algorithms%2C%20to%20efficiently%20explore%20high-dimensional%0Aintervention%20spaces.%20Applied%20to%20more%20than%202700%20individuals%20with%20confirmed%0ASARS-CoV-2%20infection%20and%20prior%20HF%2C%20the%20model%20achieved%20strong%20discriminative%0Aperformance%20%28AUROC%3A%200.88%2C%2095%25%20CI%3A%200.84-0.91%29%20and%20generated%20interpretable%2C%0Apatient-specific%20counterfactuals%20that%20quantify%20how%20modifying%20comorbidity%0Apatterns%20or%20treatment%20factors%20could%20alter%20predicted%20outcomes.%20This%20work%0Ademonstrates%20how%20counterfactual%20reasoning%20can%20be%20formalized%20as%20an%20optimization%0Aproblem%20over%20predictive%20functions%2C%20offering%20a%20rigorous%2C%20interpretable%2C%20and%0Acomputationally%20efficient%20approach%20to%20personalized%20inference%20in%20complex%0Abiomedical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Enumeration%2520Framework%2520for%2520Optimal%2520Counterfactual%2520Generation%2520in%250A%2520%2520Post-Acute%2520COVID-19%2520Heart%2520Failure%26entry.906535625%3DJingya%2520Cheng%2520and%2520Alaleh%2520Azhir%2520and%2520Jiazi%2520Tian%2520and%2520Hossein%2520Estiri%26entry.1292438233%3D%2520%2520Counterfactual%2520inference%2520provides%2520a%2520mathematical%2520framework%2520for%2520reasoning%250Aabout%2520hypothetical%2520outcomes%2520under%2520alternative%2520interventions%252C%2520bridging%2520causal%250Areasoning%2520and%2520predictive%2520modeling.%2520We%2520present%2520a%2520counterfactual%2520inference%250Aframework%2520for%2520individualized%2520risk%2520estimation%2520and%2520intervention%2520analysis%252C%250Aillustrated%2520through%2520a%2520clinical%2520application%2520to%2520post-acute%2520sequelae%2520of%2520COVID-19%250A%2528PASC%2529%2520among%2520patients%2520with%2520pre-existing%2520heart%2520failure%2520%2528HF%2529.%2520Using%2520longitudinal%250Adiagnosis%252C%2520laboratory%252C%2520and%2520medication%2520data%2520from%2520a%2520large%2520health-system%2520cohort%252C%250Awe%2520integrate%2520regularized%2520predictive%2520modeling%2520with%2520counterfactual%2520search%2520to%250Aidentify%2520actionable%2520pathways%2520to%2520PASC-related%2520HF%2520hospital%2520admissions.%2520The%250Aframework%2520combines%2520exact%2520enumeration%2520with%2520optimization-based%2520methods%252C%2520including%250Athe%2520Nearest%2520Instance%2520Counterfactual%2520Explanations%2520%2528NICE%2529%2520and%2520Multi-Objective%250ACounterfactuals%2520%2528MOC%2529%2520algorithms%252C%2520to%2520efficiently%2520explore%2520high-dimensional%250Aintervention%2520spaces.%2520Applied%2520to%2520more%2520than%25202700%2520individuals%2520with%2520confirmed%250ASARS-CoV-2%2520infection%2520and%2520prior%2520HF%252C%2520the%2520model%2520achieved%2520strong%2520discriminative%250Aperformance%2520%2528AUROC%253A%25200.88%252C%252095%2525%2520CI%253A%25200.84-0.91%2529%2520and%2520generated%2520interpretable%252C%250Apatient-specific%2520counterfactuals%2520that%2520quantify%2520how%2520modifying%2520comorbidity%250Apatterns%2520or%2520treatment%2520factors%2520could%2520alter%2520predicted%2520outcomes.%2520This%2520work%250Ademonstrates%2520how%2520counterfactual%2520reasoning%2520can%2520be%2520formalized%2520as%2520an%2520optimization%250Aproblem%2520over%2520predictive%2520functions%252C%2520offering%2520a%2520rigorous%252C%2520interpretable%252C%2520and%250Acomputationally%2520efficient%2520approach%2520to%2520personalized%2520inference%2520in%2520complex%250Abiomedical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Enumeration%20Framework%20for%20Optimal%20Counterfactual%20Generation%20in%0A%20%20Post-Acute%20COVID-19%20Heart%20Failure&entry.906535625=Jingya%20Cheng%20and%20Alaleh%20Azhir%20and%20Jiazi%20Tian%20and%20Hossein%20Estiri&entry.1292438233=%20%20Counterfactual%20inference%20provides%20a%20mathematical%20framework%20for%20reasoning%0Aabout%20hypothetical%20outcomes%20under%20alternative%20interventions%2C%20bridging%20causal%0Areasoning%20and%20predictive%20modeling.%20We%20present%20a%20counterfactual%20inference%0Aframework%20for%20individualized%20risk%20estimation%20and%20intervention%20analysis%2C%0Aillustrated%20through%20a%20clinical%20application%20to%20post-acute%20sequelae%20of%20COVID-19%0A%28PASC%29%20among%20patients%20with%20pre-existing%20heart%20failure%20%28HF%29.%20Using%20longitudinal%0Adiagnosis%2C%20laboratory%2C%20and%20medication%20data%20from%20a%20large%20health-system%20cohort%2C%0Awe%20integrate%20regularized%20predictive%20modeling%20with%20counterfactual%20search%20to%0Aidentify%20actionable%20pathways%20to%20PASC-related%20HF%20hospital%20admissions.%20The%0Aframework%20combines%20exact%20enumeration%20with%20optimization-based%20methods%2C%20including%0Athe%20Nearest%20Instance%20Counterfactual%20Explanations%20%28NICE%29%20and%20Multi-Objective%0ACounterfactuals%20%28MOC%29%20algorithms%2C%20to%20efficiently%20explore%20high-dimensional%0Aintervention%20spaces.%20Applied%20to%20more%20than%202700%20individuals%20with%20confirmed%0ASARS-CoV-2%20infection%20and%20prior%20HF%2C%20the%20model%20achieved%20strong%20discriminative%0Aperformance%20%28AUROC%3A%200.88%2C%2095%25%20CI%3A%200.84-0.91%29%20and%20generated%20interpretable%2C%0Apatient-specific%20counterfactuals%20that%20quantify%20how%20modifying%20comorbidity%0Apatterns%20or%20treatment%20factors%20could%20alter%20predicted%20outcomes.%20This%20work%0Ademonstrates%20how%20counterfactual%20reasoning%20can%20be%20formalized%20as%20an%20optimization%0Aproblem%20over%20predictive%20functions%2C%20offering%20a%20rigorous%2C%20interpretable%2C%20and%0Acomputationally%20efficient%20approach%20to%20personalized%20inference%20in%20complex%0Abiomedical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18841v1&entry.124074799=Read"},
{"title": "Bayesian Low-Rank Factorization for Robust Model Adaptation", "author": "Enes Yavuz Ugan and Ngoc-Quan Pham and Alexander Waibel", "abstract": "  Large speech foundation models achieve strong performance across many\ndomains, but they often require adaptation to handle local needs such as\ncode-switching, where speakers mix languages within the same utterance. Direct\nfine-tuning of these models risks overfitting to the target domain and\noverwriting the broad capabilities of the base model. To address this\nchallenge, we explore Bayesian factorized adapters for speech foundation\nmodels, which place priors near zero to achieve sparser adaptation matrices and\nthereby retain general performance while adapting to specific domains. We apply\nour approach to the Whisper model and evaluate on different multilingual\ncode-switching scenarios. Our results show only minimal adaptation loss while\nsignificantly reducing catastrophic forgetting of the base model. Compared to\nLoRA, our method achieves a backward gain of 54% with only a 4% drop on the new\ndomain. These findings highlight the effectiveness of Bayesian adaptation for\nfine-tuning speech foundation models without sacrificing generalization.\n", "link": "http://arxiv.org/abs/2510.18723v1", "date": "2025-10-21", "relevancy": 0.9519, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4899}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4699}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Low-Rank%20Factorization%20for%20Robust%20Model%20Adaptation&body=Title%3A%20Bayesian%20Low-Rank%20Factorization%20for%20Robust%20Model%20Adaptation%0AAuthor%3A%20Enes%20Yavuz%20Ugan%20and%20Ngoc-Quan%20Pham%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20Large%20speech%20foundation%20models%20achieve%20strong%20performance%20across%20many%0Adomains%2C%20but%20they%20often%20require%20adaptation%20to%20handle%20local%20needs%20such%20as%0Acode-switching%2C%20where%20speakers%20mix%20languages%20within%20the%20same%20utterance.%20Direct%0Afine-tuning%20of%20these%20models%20risks%20overfitting%20to%20the%20target%20domain%20and%0Aoverwriting%20the%20broad%20capabilities%20of%20the%20base%20model.%20To%20address%20this%0Achallenge%2C%20we%20explore%20Bayesian%20factorized%20adapters%20for%20speech%20foundation%0Amodels%2C%20which%20place%20priors%20near%20zero%20to%20achieve%20sparser%20adaptation%20matrices%20and%0Athereby%20retain%20general%20performance%20while%20adapting%20to%20specific%20domains.%20We%20apply%0Aour%20approach%20to%20the%20Whisper%20model%20and%20evaluate%20on%20different%20multilingual%0Acode-switching%20scenarios.%20Our%20results%20show%20only%20minimal%20adaptation%20loss%20while%0Asignificantly%20reducing%20catastrophic%20forgetting%20of%20the%20base%20model.%20Compared%20to%0ALoRA%2C%20our%20method%20achieves%20a%20backward%20gain%20of%2054%25%20with%20only%20a%204%25%20drop%20on%20the%20new%0Adomain.%20These%20findings%20highlight%20the%20effectiveness%20of%20Bayesian%20adaptation%20for%0Afine-tuning%20speech%20foundation%20models%20without%20sacrificing%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Low-Rank%2520Factorization%2520for%2520Robust%2520Model%2520Adaptation%26entry.906535625%3DEnes%2520Yavuz%2520Ugan%2520and%2520Ngoc-Quan%2520Pham%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520Large%2520speech%2520foundation%2520models%2520achieve%2520strong%2520performance%2520across%2520many%250Adomains%252C%2520but%2520they%2520often%2520require%2520adaptation%2520to%2520handle%2520local%2520needs%2520such%2520as%250Acode-switching%252C%2520where%2520speakers%2520mix%2520languages%2520within%2520the%2520same%2520utterance.%2520Direct%250Afine-tuning%2520of%2520these%2520models%2520risks%2520overfitting%2520to%2520the%2520target%2520domain%2520and%250Aoverwriting%2520the%2520broad%2520capabilities%2520of%2520the%2520base%2520model.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520explore%2520Bayesian%2520factorized%2520adapters%2520for%2520speech%2520foundation%250Amodels%252C%2520which%2520place%2520priors%2520near%2520zero%2520to%2520achieve%2520sparser%2520adaptation%2520matrices%2520and%250Athereby%2520retain%2520general%2520performance%2520while%2520adapting%2520to%2520specific%2520domains.%2520We%2520apply%250Aour%2520approach%2520to%2520the%2520Whisper%2520model%2520and%2520evaluate%2520on%2520different%2520multilingual%250Acode-switching%2520scenarios.%2520Our%2520results%2520show%2520only%2520minimal%2520adaptation%2520loss%2520while%250Asignificantly%2520reducing%2520catastrophic%2520forgetting%2520of%2520the%2520base%2520model.%2520Compared%2520to%250ALoRA%252C%2520our%2520method%2520achieves%2520a%2520backward%2520gain%2520of%252054%2525%2520with%2520only%2520a%25204%2525%2520drop%2520on%2520the%2520new%250Adomain.%2520These%2520findings%2520highlight%2520the%2520effectiveness%2520of%2520Bayesian%2520adaptation%2520for%250Afine-tuning%2520speech%2520foundation%2520models%2520without%2520sacrificing%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Low-Rank%20Factorization%20for%20Robust%20Model%20Adaptation&entry.906535625=Enes%20Yavuz%20Ugan%20and%20Ngoc-Quan%20Pham%20and%20Alexander%20Waibel&entry.1292438233=%20%20Large%20speech%20foundation%20models%20achieve%20strong%20performance%20across%20many%0Adomains%2C%20but%20they%20often%20require%20adaptation%20to%20handle%20local%20needs%20such%20as%0Acode-switching%2C%20where%20speakers%20mix%20languages%20within%20the%20same%20utterance.%20Direct%0Afine-tuning%20of%20these%20models%20risks%20overfitting%20to%20the%20target%20domain%20and%0Aoverwriting%20the%20broad%20capabilities%20of%20the%20base%20model.%20To%20address%20this%0Achallenge%2C%20we%20explore%20Bayesian%20factorized%20adapters%20for%20speech%20foundation%0Amodels%2C%20which%20place%20priors%20near%20zero%20to%20achieve%20sparser%20adaptation%20matrices%20and%0Athereby%20retain%20general%20performance%20while%20adapting%20to%20specific%20domains.%20We%20apply%0Aour%20approach%20to%20the%20Whisper%20model%20and%20evaluate%20on%20different%20multilingual%0Acode-switching%20scenarios.%20Our%20results%20show%20only%20minimal%20adaptation%20loss%20while%0Asignificantly%20reducing%20catastrophic%20forgetting%20of%20the%20base%20model.%20Compared%20to%0ALoRA%2C%20our%20method%20achieves%20a%20backward%20gain%20of%2054%25%20with%20only%20a%204%25%20drop%20on%20the%20new%0Adomain.%20These%20findings%20highlight%20the%20effectiveness%20of%20Bayesian%20adaptation%20for%0Afine-tuning%20speech%20foundation%20models%20without%20sacrificing%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18723v1&entry.124074799=Read"},
{"title": "Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers", "author": "Wenhan Ma and Hailin Zhang and Liang Zhao and Yifan Song and Yudong Wang and Zhifang Sui and Fuli Luo", "abstract": "  Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.\n", "link": "http://arxiv.org/abs/2510.11370v2", "date": "2025-10-21", "relevancy": 0.9002, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4569}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4517}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilizing%20MoE%20Reinforcement%20Learning%20by%20Aligning%20Training%20and%0A%20%20Inference%20Routers&body=Title%3A%20Stabilizing%20MoE%20Reinforcement%20Learning%20by%20Aligning%20Training%20and%0A%20%20Inference%20Routers%0AAuthor%3A%20Wenhan%20Ma%20and%20Hailin%20Zhang%20and%20Liang%20Zhao%20and%20Yifan%20Song%20and%20Yudong%20Wang%20and%20Zhifang%20Sui%20and%20Fuli%20Luo%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20crucial%20approach%20for%20enhancing%0Athe%20capabilities%20of%20large%20language%20models.%20However%2C%20in%20Mixture-of-Experts%20%28MoE%29%0Amodels%2C%20the%20routing%20mechanism%20often%20introduces%20instability%2C%20even%20leading%20to%0Acatastrophic%20RL%20training%20collapse.%20We%20analyze%20the%20training-inference%0Aconsistency%20of%20MoE%20models%20and%20identify%20a%20notable%20discrepancy%20in%20routing%0Abehaviors%20between%20the%20two%20phases.%20Moreover%2C%20even%20under%20identical%20conditions%2C%0Athe%20routing%20framework%20can%20yield%20divergent%20expert%20selections%20across%20repeated%0Aforward%20passes.%20To%20address%20this%20foundational%20inconsistency%2C%20we%20propose%20Rollout%0ARouting%20Replay%20%28R3%29%2C%20a%20method%20that%20records%20routing%20distributions%20from%20the%0Ainference%20engine%20and%20replays%20them%20during%20training.%20R3%20significantly%20reduces%0Atraining-inference%20policy%20KL%20divergence%20and%20mitigates%20extreme%20discrepancies%0Awithout%20compromising%20training%20speed.%20Extensive%20experiments%20on%20various%20settings%0Aconfirm%20that%20R3%20succeeds%20in%20stabilizing%20RL%20training%2C%20preventing%20collapse%20and%0Aoutperforming%20methods%20such%20as%20GSPO%20and%20TIS.%20We%20believe%20this%20work%20can%20offer%20a%0Anew%20solution%20for%20stabilizing%20RL%20in%20MoE%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilizing%2520MoE%2520Reinforcement%2520Learning%2520by%2520Aligning%2520Training%2520and%250A%2520%2520Inference%2520Routers%26entry.906535625%3DWenhan%2520Ma%2520and%2520Hailin%2520Zhang%2520and%2520Liang%2520Zhao%2520and%2520Yifan%2520Song%2520and%2520Yudong%2520Wang%2520and%2520Zhifang%2520Sui%2520and%2520Fuli%2520Luo%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520crucial%2520approach%2520for%2520enhancing%250Athe%2520capabilities%2520of%2520large%2520language%2520models.%2520However%252C%2520in%2520Mixture-of-Experts%2520%2528MoE%2529%250Amodels%252C%2520the%2520routing%2520mechanism%2520often%2520introduces%2520instability%252C%2520even%2520leading%2520to%250Acatastrophic%2520RL%2520training%2520collapse.%2520We%2520analyze%2520the%2520training-inference%250Aconsistency%2520of%2520MoE%2520models%2520and%2520identify%2520a%2520notable%2520discrepancy%2520in%2520routing%250Abehaviors%2520between%2520the%2520two%2520phases.%2520Moreover%252C%2520even%2520under%2520identical%2520conditions%252C%250Athe%2520routing%2520framework%2520can%2520yield%2520divergent%2520expert%2520selections%2520across%2520repeated%250Aforward%2520passes.%2520To%2520address%2520this%2520foundational%2520inconsistency%252C%2520we%2520propose%2520Rollout%250ARouting%2520Replay%2520%2528R3%2529%252C%2520a%2520method%2520that%2520records%2520routing%2520distributions%2520from%2520the%250Ainference%2520engine%2520and%2520replays%2520them%2520during%2520training.%2520R3%2520significantly%2520reduces%250Atraining-inference%2520policy%2520KL%2520divergence%2520and%2520mitigates%2520extreme%2520discrepancies%250Awithout%2520compromising%2520training%2520speed.%2520Extensive%2520experiments%2520on%2520various%2520settings%250Aconfirm%2520that%2520R3%2520succeeds%2520in%2520stabilizing%2520RL%2520training%252C%2520preventing%2520collapse%2520and%250Aoutperforming%2520methods%2520such%2520as%2520GSPO%2520and%2520TIS.%2520We%2520believe%2520this%2520work%2520can%2520offer%2520a%250Anew%2520solution%2520for%2520stabilizing%2520RL%2520in%2520MoE%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilizing%20MoE%20Reinforcement%20Learning%20by%20Aligning%20Training%20and%0A%20%20Inference%20Routers&entry.906535625=Wenhan%20Ma%20and%20Hailin%20Zhang%20and%20Liang%20Zhao%20and%20Yifan%20Song%20and%20Yudong%20Wang%20and%20Zhifang%20Sui%20and%20Fuli%20Luo&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20crucial%20approach%20for%20enhancing%0Athe%20capabilities%20of%20large%20language%20models.%20However%2C%20in%20Mixture-of-Experts%20%28MoE%29%0Amodels%2C%20the%20routing%20mechanism%20often%20introduces%20instability%2C%20even%20leading%20to%0Acatastrophic%20RL%20training%20collapse.%20We%20analyze%20the%20training-inference%0Aconsistency%20of%20MoE%20models%20and%20identify%20a%20notable%20discrepancy%20in%20routing%0Abehaviors%20between%20the%20two%20phases.%20Moreover%2C%20even%20under%20identical%20conditions%2C%0Athe%20routing%20framework%20can%20yield%20divergent%20expert%20selections%20across%20repeated%0Aforward%20passes.%20To%20address%20this%20foundational%20inconsistency%2C%20we%20propose%20Rollout%0ARouting%20Replay%20%28R3%29%2C%20a%20method%20that%20records%20routing%20distributions%20from%20the%0Ainference%20engine%20and%20replays%20them%20during%20training.%20R3%20significantly%20reduces%0Atraining-inference%20policy%20KL%20divergence%20and%20mitigates%20extreme%20discrepancies%0Awithout%20compromising%20training%20speed.%20Extensive%20experiments%20on%20various%20settings%0Aconfirm%20that%20R3%20succeeds%20in%20stabilizing%20RL%20training%2C%20preventing%20collapse%20and%0Aoutperforming%20methods%20such%20as%20GSPO%20and%20TIS.%20We%20believe%20this%20work%20can%20offer%20a%0Anew%20solution%20for%20stabilizing%20RL%20in%20MoE%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11370v2&entry.124074799=Read"},
{"title": "Symbolic Emulators for Cosmology: Accelerating Cosmological Analyses\n  Without Sacrificing Precision", "author": "Deaglan J. Bartlett and Shivam Pandey", "abstract": "  In cosmology, emulators play a crucial role by providing fast and accurate\npredictions of complex physical models, enabling efficient exploration of\nhigh-dimensional parameter spaces that would be computationally prohibitive\nwith direct numerical simulations. Symbolic emulators have emerged as promising\nalternatives to numerical approaches, delivering comparable accuracy with\nsignificantly faster evaluation times. While previous symbolic emulators were\nlimited to relatively narrow prior ranges, we expand these to cover the\nparameter space relevant for current cosmological analyses. We introduce\napproximations to hypergeometric functions used for the $\\Lambda$CDM comoving\ndistance and linear growth factor which are accurate to better than 0.001% and\n0.05%, respectively, for all redshifts and for $\\Omega_{\\rm m} \\in [0.1, 0.5]$.\nWe show that integrating symbolic emulators into a Dark Energy Survey-like\n$3\\times2$pt analysis produces cosmological constraints consistent with those\nobtained using standard numerical methods. Our symbolic emulators offer\nsubstantial improvements in speed and memory usage, demonstrating their\npractical potential for scalable, likelihood-based inference.\n", "link": "http://arxiv.org/abs/2510.18749v1", "date": "2025-10-21", "relevancy": 1.2484, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4227}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4183}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symbolic%20Emulators%20for%20Cosmology%3A%20Accelerating%20Cosmological%20Analyses%0A%20%20Without%20Sacrificing%20Precision&body=Title%3A%20Symbolic%20Emulators%20for%20Cosmology%3A%20Accelerating%20Cosmological%20Analyses%0A%20%20Without%20Sacrificing%20Precision%0AAuthor%3A%20Deaglan%20J.%20Bartlett%20and%20Shivam%20Pandey%0AAbstract%3A%20%20%20In%20cosmology%2C%20emulators%20play%20a%20crucial%20role%20by%20providing%20fast%20and%20accurate%0Apredictions%20of%20complex%20physical%20models%2C%20enabling%20efficient%20exploration%20of%0Ahigh-dimensional%20parameter%20spaces%20that%20would%20be%20computationally%20prohibitive%0Awith%20direct%20numerical%20simulations.%20Symbolic%20emulators%20have%20emerged%20as%20promising%0Aalternatives%20to%20numerical%20approaches%2C%20delivering%20comparable%20accuracy%20with%0Asignificantly%20faster%20evaluation%20times.%20While%20previous%20symbolic%20emulators%20were%0Alimited%20to%20relatively%20narrow%20prior%20ranges%2C%20we%20expand%20these%20to%20cover%20the%0Aparameter%20space%20relevant%20for%20current%20cosmological%20analyses.%20We%20introduce%0Aapproximations%20to%20hypergeometric%20functions%20used%20for%20the%20%24%5CLambda%24CDM%20comoving%0Adistance%20and%20linear%20growth%20factor%20which%20are%20accurate%20to%20better%20than%200.001%25%20and%0A0.05%25%2C%20respectively%2C%20for%20all%20redshifts%20and%20for%20%24%5COmega_%7B%5Crm%20m%7D%20%5Cin%20%5B0.1%2C%200.5%5D%24.%0AWe%20show%20that%20integrating%20symbolic%20emulators%20into%20a%20Dark%20Energy%20Survey-like%0A%243%5Ctimes2%24pt%20analysis%20produces%20cosmological%20constraints%20consistent%20with%20those%0Aobtained%20using%20standard%20numerical%20methods.%20Our%20symbolic%20emulators%20offer%0Asubstantial%20improvements%20in%20speed%20and%20memory%20usage%2C%20demonstrating%20their%0Apractical%20potential%20for%20scalable%2C%20likelihood-based%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymbolic%2520Emulators%2520for%2520Cosmology%253A%2520Accelerating%2520Cosmological%2520Analyses%250A%2520%2520Without%2520Sacrificing%2520Precision%26entry.906535625%3DDeaglan%2520J.%2520Bartlett%2520and%2520Shivam%2520Pandey%26entry.1292438233%3D%2520%2520In%2520cosmology%252C%2520emulators%2520play%2520a%2520crucial%2520role%2520by%2520providing%2520fast%2520and%2520accurate%250Apredictions%2520of%2520complex%2520physical%2520models%252C%2520enabling%2520efficient%2520exploration%2520of%250Ahigh-dimensional%2520parameter%2520spaces%2520that%2520would%2520be%2520computationally%2520prohibitive%250Awith%2520direct%2520numerical%2520simulations.%2520Symbolic%2520emulators%2520have%2520emerged%2520as%2520promising%250Aalternatives%2520to%2520numerical%2520approaches%252C%2520delivering%2520comparable%2520accuracy%2520with%250Asignificantly%2520faster%2520evaluation%2520times.%2520While%2520previous%2520symbolic%2520emulators%2520were%250Alimited%2520to%2520relatively%2520narrow%2520prior%2520ranges%252C%2520we%2520expand%2520these%2520to%2520cover%2520the%250Aparameter%2520space%2520relevant%2520for%2520current%2520cosmological%2520analyses.%2520We%2520introduce%250Aapproximations%2520to%2520hypergeometric%2520functions%2520used%2520for%2520the%2520%2524%255CLambda%2524CDM%2520comoving%250Adistance%2520and%2520linear%2520growth%2520factor%2520which%2520are%2520accurate%2520to%2520better%2520than%25200.001%2525%2520and%250A0.05%2525%252C%2520respectively%252C%2520for%2520all%2520redshifts%2520and%2520for%2520%2524%255COmega_%257B%255Crm%2520m%257D%2520%255Cin%2520%255B0.1%252C%25200.5%255D%2524.%250AWe%2520show%2520that%2520integrating%2520symbolic%2520emulators%2520into%2520a%2520Dark%2520Energy%2520Survey-like%250A%25243%255Ctimes2%2524pt%2520analysis%2520produces%2520cosmological%2520constraints%2520consistent%2520with%2520those%250Aobtained%2520using%2520standard%2520numerical%2520methods.%2520Our%2520symbolic%2520emulators%2520offer%250Asubstantial%2520improvements%2520in%2520speed%2520and%2520memory%2520usage%252C%2520demonstrating%2520their%250Apractical%2520potential%2520for%2520scalable%252C%2520likelihood-based%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolic%20Emulators%20for%20Cosmology%3A%20Accelerating%20Cosmological%20Analyses%0A%20%20Without%20Sacrificing%20Precision&entry.906535625=Deaglan%20J.%20Bartlett%20and%20Shivam%20Pandey&entry.1292438233=%20%20In%20cosmology%2C%20emulators%20play%20a%20crucial%20role%20by%20providing%20fast%20and%20accurate%0Apredictions%20of%20complex%20physical%20models%2C%20enabling%20efficient%20exploration%20of%0Ahigh-dimensional%20parameter%20spaces%20that%20would%20be%20computationally%20prohibitive%0Awith%20direct%20numerical%20simulations.%20Symbolic%20emulators%20have%20emerged%20as%20promising%0Aalternatives%20to%20numerical%20approaches%2C%20delivering%20comparable%20accuracy%20with%0Asignificantly%20faster%20evaluation%20times.%20While%20previous%20symbolic%20emulators%20were%0Alimited%20to%20relatively%20narrow%20prior%20ranges%2C%20we%20expand%20these%20to%20cover%20the%0Aparameter%20space%20relevant%20for%20current%20cosmological%20analyses.%20We%20introduce%0Aapproximations%20to%20hypergeometric%20functions%20used%20for%20the%20%24%5CLambda%24CDM%20comoving%0Adistance%20and%20linear%20growth%20factor%20which%20are%20accurate%20to%20better%20than%200.001%25%20and%0A0.05%25%2C%20respectively%2C%20for%20all%20redshifts%20and%20for%20%24%5COmega_%7B%5Crm%20m%7D%20%5Cin%20%5B0.1%2C%200.5%5D%24.%0AWe%20show%20that%20integrating%20symbolic%20emulators%20into%20a%20Dark%20Energy%20Survey-like%0A%243%5Ctimes2%24pt%20analysis%20produces%20cosmological%20constraints%20consistent%20with%20those%0Aobtained%20using%20standard%20numerical%20methods.%20Our%20symbolic%20emulators%20offer%0Asubstantial%20improvements%20in%20speed%20and%20memory%20usage%2C%20demonstrating%20their%0Apractical%20potential%20for%20scalable%2C%20likelihood-based%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18749v1&entry.124074799=Read"},
{"title": "Who cuts emissions, who turns up the heat? causal machine learning\n  estimates of energy efficiency interventions", "author": "Bernardino D'Amico and Francesco Pomponi and Jay H. Arehart and Lina Khaddour", "abstract": "  Reducing domestic energy demand is central to climate mitigation and fuel\npoverty strategies, yet the impact of energy efficiency interventions is highly\nheterogeneous. Using a causal machine learning model trained on nationally\nrepresentative data of the English housing stock, we estimate average and\nconditional treatment effects of wall insulation on gas consumption, focusing\non distributional effects across energy burden subgroups. While interventions\nreduce gas demand on average (by as much as 19 percent), low energy burden\ngroups achieve substantial savings, whereas those experiencing high energy\nburdens see little to no reduction. This pattern reflects a\nbehaviourally-driven mechanism: households constrained by high costs-to-income\nratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort\nrather than lowering consumption. Far from wasteful, such responses represent\nrational adjustments in contexts of prior deprivation, with potential\nco-benefits for health and well-being. These findings call for a broader\nevaluation framework that accounts for both climate impacts and the equity\nimplications of domestic energy policy.\n", "link": "http://arxiv.org/abs/2508.04478v2", "date": "2025-10-21", "relevancy": 0.7824, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4078}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4043}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20cuts%20emissions%2C%20who%20turns%20up%20the%20heat%3F%20causal%20machine%20learning%0A%20%20estimates%20of%20energy%20efficiency%20interventions&body=Title%3A%20Who%20cuts%20emissions%2C%20who%20turns%20up%20the%20heat%3F%20causal%20machine%20learning%0A%20%20estimates%20of%20energy%20efficiency%20interventions%0AAuthor%3A%20Bernardino%20D%27Amico%20and%20Francesco%20Pomponi%20and%20Jay%20H.%20Arehart%20and%20Lina%20Khaddour%0AAbstract%3A%20%20%20Reducing%20domestic%20energy%20demand%20is%20central%20to%20climate%20mitigation%20and%20fuel%0Apoverty%20strategies%2C%20yet%20the%20impact%20of%20energy%20efficiency%20interventions%20is%20highly%0Aheterogeneous.%20Using%20a%20causal%20machine%20learning%20model%20trained%20on%20nationally%0Arepresentative%20data%20of%20the%20English%20housing%20stock%2C%20we%20estimate%20average%20and%0Aconditional%20treatment%20effects%20of%20wall%20insulation%20on%20gas%20consumption%2C%20focusing%0Aon%20distributional%20effects%20across%20energy%20burden%20subgroups.%20While%20interventions%0Areduce%20gas%20demand%20on%20average%20%28by%20as%20much%20as%2019%20percent%29%2C%20low%20energy%20burden%0Agroups%20achieve%20substantial%20savings%2C%20whereas%20those%20experiencing%20high%20energy%0Aburdens%20see%20little%20to%20no%20reduction.%20This%20pattern%20reflects%20a%0Abehaviourally-driven%20mechanism%3A%20households%20constrained%20by%20high%20costs-to-income%0Aratios%20%28e.g.%20more%20than%200.1%29%20reallocate%20savings%20toward%20improved%20thermal%20comfort%0Arather%20than%20lowering%20consumption.%20Far%20from%20wasteful%2C%20such%20responses%20represent%0Arational%20adjustments%20in%20contexts%20of%20prior%20deprivation%2C%20with%20potential%0Aco-benefits%20for%20health%20and%20well-being.%20These%20findings%20call%20for%20a%20broader%0Aevaluation%20framework%20that%20accounts%20for%20both%20climate%20impacts%20and%20the%20equity%0Aimplications%20of%20domestic%20energy%20policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04478v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520cuts%2520emissions%252C%2520who%2520turns%2520up%2520the%2520heat%253F%2520causal%2520machine%2520learning%250A%2520%2520estimates%2520of%2520energy%2520efficiency%2520interventions%26entry.906535625%3DBernardino%2520D%2527Amico%2520and%2520Francesco%2520Pomponi%2520and%2520Jay%2520H.%2520Arehart%2520and%2520Lina%2520Khaddour%26entry.1292438233%3D%2520%2520Reducing%2520domestic%2520energy%2520demand%2520is%2520central%2520to%2520climate%2520mitigation%2520and%2520fuel%250Apoverty%2520strategies%252C%2520yet%2520the%2520impact%2520of%2520energy%2520efficiency%2520interventions%2520is%2520highly%250Aheterogeneous.%2520Using%2520a%2520causal%2520machine%2520learning%2520model%2520trained%2520on%2520nationally%250Arepresentative%2520data%2520of%2520the%2520English%2520housing%2520stock%252C%2520we%2520estimate%2520average%2520and%250Aconditional%2520treatment%2520effects%2520of%2520wall%2520insulation%2520on%2520gas%2520consumption%252C%2520focusing%250Aon%2520distributional%2520effects%2520across%2520energy%2520burden%2520subgroups.%2520While%2520interventions%250Areduce%2520gas%2520demand%2520on%2520average%2520%2528by%2520as%2520much%2520as%252019%2520percent%2529%252C%2520low%2520energy%2520burden%250Agroups%2520achieve%2520substantial%2520savings%252C%2520whereas%2520those%2520experiencing%2520high%2520energy%250Aburdens%2520see%2520little%2520to%2520no%2520reduction.%2520This%2520pattern%2520reflects%2520a%250Abehaviourally-driven%2520mechanism%253A%2520households%2520constrained%2520by%2520high%2520costs-to-income%250Aratios%2520%2528e.g.%2520more%2520than%25200.1%2529%2520reallocate%2520savings%2520toward%2520improved%2520thermal%2520comfort%250Arather%2520than%2520lowering%2520consumption.%2520Far%2520from%2520wasteful%252C%2520such%2520responses%2520represent%250Arational%2520adjustments%2520in%2520contexts%2520of%2520prior%2520deprivation%252C%2520with%2520potential%250Aco-benefits%2520for%2520health%2520and%2520well-being.%2520These%2520findings%2520call%2520for%2520a%2520broader%250Aevaluation%2520framework%2520that%2520accounts%2520for%2520both%2520climate%2520impacts%2520and%2520the%2520equity%250Aimplications%2520of%2520domestic%2520energy%2520policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04478v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20cuts%20emissions%2C%20who%20turns%20up%20the%20heat%3F%20causal%20machine%20learning%0A%20%20estimates%20of%20energy%20efficiency%20interventions&entry.906535625=Bernardino%20D%27Amico%20and%20Francesco%20Pomponi%20and%20Jay%20H.%20Arehart%20and%20Lina%20Khaddour&entry.1292438233=%20%20Reducing%20domestic%20energy%20demand%20is%20central%20to%20climate%20mitigation%20and%20fuel%0Apoverty%20strategies%2C%20yet%20the%20impact%20of%20energy%20efficiency%20interventions%20is%20highly%0Aheterogeneous.%20Using%20a%20causal%20machine%20learning%20model%20trained%20on%20nationally%0Arepresentative%20data%20of%20the%20English%20housing%20stock%2C%20we%20estimate%20average%20and%0Aconditional%20treatment%20effects%20of%20wall%20insulation%20on%20gas%20consumption%2C%20focusing%0Aon%20distributional%20effects%20across%20energy%20burden%20subgroups.%20While%20interventions%0Areduce%20gas%20demand%20on%20average%20%28by%20as%20much%20as%2019%20percent%29%2C%20low%20energy%20burden%0Agroups%20achieve%20substantial%20savings%2C%20whereas%20those%20experiencing%20high%20energy%0Aburdens%20see%20little%20to%20no%20reduction.%20This%20pattern%20reflects%20a%0Abehaviourally-driven%20mechanism%3A%20households%20constrained%20by%20high%20costs-to-income%0Aratios%20%28e.g.%20more%20than%200.1%29%20reallocate%20savings%20toward%20improved%20thermal%20comfort%0Arather%20than%20lowering%20consumption.%20Far%20from%20wasteful%2C%20such%20responses%20represent%0Arational%20adjustments%20in%20contexts%20of%20prior%20deprivation%2C%20with%20potential%0Aco-benefits%20for%20health%20and%20well-being.%20These%20findings%20call%20for%20a%20broader%0Aevaluation%20framework%20that%20accounts%20for%20both%20climate%20impacts%20and%20the%20equity%0Aimplications%20of%20domestic%20energy%20policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04478v2&entry.124074799=Read"},
{"title": "Computational Foundations for Strategic Coopetition: Formalizing\n  Interdependence and Complementarity", "author": "Vik Pant and Eric Yu", "abstract": "  Modern socio-technical systems are characterized by strategic coopetition\nwhere actors simultaneously cooperate to create value and compete to capture\nit. While conceptual modeling languages like i* provide rich qualitative\nrepresentations of strategic dependencies, they lack mechanisms for\nquantitative analysis of dynamic trade-offs. Conversely, classical game theory\noffers mathematical rigor but strips away contextual richness. This technical\nreport bridges this gap by developing computational foundations that formalize\ntwo critical dimensions of coopetition: interdependence and complementarity. We\nground interdependence in i* structural dependency analysis, translating\ndepender-dependee-dependum relationships into quantitative interdependence\ncoefficients through a structured translation framework. We formalize\ncomplementarity following Brandenburger and Nalebuff's Added Value concept,\nmodeling synergistic value creation with validated parameterization. We\nintegrate structural dependencies with bargaining power in value appropriation\nand introduce a game-theoretic formulation where Nash Equilibrium incorporates\nstructural interdependence. Validation combines comprehensive experimental\ntesting across power and logarithmic value function specifications,\ndemonstrating functional form robustness, with empirical application to the\nSamsung-Sony S-LCD joint venture (2004-2011), where logarithmic specifications\nachieve superior empirical fit (validation score 45/60) while power functions\nprovide theoretical tractability. This technical report serves as the\nfoundational reference for a coordinated research program examining strategic\ncoopetition in requirements engineering and multi-agent systems, with companion\nwork addressing trust dynamics, team production, and reciprocity mechanisms.\n", "link": "http://arxiv.org/abs/2510.18802v1", "date": "2025-10-21", "relevancy": 0.9058, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4575}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4508}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computational%20Foundations%20for%20Strategic%20Coopetition%3A%20Formalizing%0A%20%20Interdependence%20and%20Complementarity&body=Title%3A%20Computational%20Foundations%20for%20Strategic%20Coopetition%3A%20Formalizing%0A%20%20Interdependence%20and%20Complementarity%0AAuthor%3A%20Vik%20Pant%20and%20Eric%20Yu%0AAbstract%3A%20%20%20Modern%20socio-technical%20systems%20are%20characterized%20by%20strategic%20coopetition%0Awhere%20actors%20simultaneously%20cooperate%20to%20create%20value%20and%20compete%20to%20capture%0Ait.%20While%20conceptual%20modeling%20languages%20like%20i%2A%20provide%20rich%20qualitative%0Arepresentations%20of%20strategic%20dependencies%2C%20they%20lack%20mechanisms%20for%0Aquantitative%20analysis%20of%20dynamic%20trade-offs.%20Conversely%2C%20classical%20game%20theory%0Aoffers%20mathematical%20rigor%20but%20strips%20away%20contextual%20richness.%20This%20technical%0Areport%20bridges%20this%20gap%20by%20developing%20computational%20foundations%20that%20formalize%0Atwo%20critical%20dimensions%20of%20coopetition%3A%20interdependence%20and%20complementarity.%20We%0Aground%20interdependence%20in%20i%2A%20structural%20dependency%20analysis%2C%20translating%0Adepender-dependee-dependum%20relationships%20into%20quantitative%20interdependence%0Acoefficients%20through%20a%20structured%20translation%20framework.%20We%20formalize%0Acomplementarity%20following%20Brandenburger%20and%20Nalebuff%27s%20Added%20Value%20concept%2C%0Amodeling%20synergistic%20value%20creation%20with%20validated%20parameterization.%20We%0Aintegrate%20structural%20dependencies%20with%20bargaining%20power%20in%20value%20appropriation%0Aand%20introduce%20a%20game-theoretic%20formulation%20where%20Nash%20Equilibrium%20incorporates%0Astructural%20interdependence.%20Validation%20combines%20comprehensive%20experimental%0Atesting%20across%20power%20and%20logarithmic%20value%20function%20specifications%2C%0Ademonstrating%20functional%20form%20robustness%2C%20with%20empirical%20application%20to%20the%0ASamsung-Sony%20S-LCD%20joint%20venture%20%282004-2011%29%2C%20where%20logarithmic%20specifications%0Aachieve%20superior%20empirical%20fit%20%28validation%20score%2045/60%29%20while%20power%20functions%0Aprovide%20theoretical%20tractability.%20This%20technical%20report%20serves%20as%20the%0Afoundational%20reference%20for%20a%20coordinated%20research%20program%20examining%20strategic%0Acoopetition%20in%20requirements%20engineering%20and%20multi-agent%20systems%2C%20with%20companion%0Awork%20addressing%20trust%20dynamics%2C%20team%20production%2C%20and%20reciprocity%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputational%2520Foundations%2520for%2520Strategic%2520Coopetition%253A%2520Formalizing%250A%2520%2520Interdependence%2520and%2520Complementarity%26entry.906535625%3DVik%2520Pant%2520and%2520Eric%2520Yu%26entry.1292438233%3D%2520%2520Modern%2520socio-technical%2520systems%2520are%2520characterized%2520by%2520strategic%2520coopetition%250Awhere%2520actors%2520simultaneously%2520cooperate%2520to%2520create%2520value%2520and%2520compete%2520to%2520capture%250Ait.%2520While%2520conceptual%2520modeling%2520languages%2520like%2520i%252A%2520provide%2520rich%2520qualitative%250Arepresentations%2520of%2520strategic%2520dependencies%252C%2520they%2520lack%2520mechanisms%2520for%250Aquantitative%2520analysis%2520of%2520dynamic%2520trade-offs.%2520Conversely%252C%2520classical%2520game%2520theory%250Aoffers%2520mathematical%2520rigor%2520but%2520strips%2520away%2520contextual%2520richness.%2520This%2520technical%250Areport%2520bridges%2520this%2520gap%2520by%2520developing%2520computational%2520foundations%2520that%2520formalize%250Atwo%2520critical%2520dimensions%2520of%2520coopetition%253A%2520interdependence%2520and%2520complementarity.%2520We%250Aground%2520interdependence%2520in%2520i%252A%2520structural%2520dependency%2520analysis%252C%2520translating%250Adepender-dependee-dependum%2520relationships%2520into%2520quantitative%2520interdependence%250Acoefficients%2520through%2520a%2520structured%2520translation%2520framework.%2520We%2520formalize%250Acomplementarity%2520following%2520Brandenburger%2520and%2520Nalebuff%2527s%2520Added%2520Value%2520concept%252C%250Amodeling%2520synergistic%2520value%2520creation%2520with%2520validated%2520parameterization.%2520We%250Aintegrate%2520structural%2520dependencies%2520with%2520bargaining%2520power%2520in%2520value%2520appropriation%250Aand%2520introduce%2520a%2520game-theoretic%2520formulation%2520where%2520Nash%2520Equilibrium%2520incorporates%250Astructural%2520interdependence.%2520Validation%2520combines%2520comprehensive%2520experimental%250Atesting%2520across%2520power%2520and%2520logarithmic%2520value%2520function%2520specifications%252C%250Ademonstrating%2520functional%2520form%2520robustness%252C%2520with%2520empirical%2520application%2520to%2520the%250ASamsung-Sony%2520S-LCD%2520joint%2520venture%2520%25282004-2011%2529%252C%2520where%2520logarithmic%2520specifications%250Aachieve%2520superior%2520empirical%2520fit%2520%2528validation%2520score%252045/60%2529%2520while%2520power%2520functions%250Aprovide%2520theoretical%2520tractability.%2520This%2520technical%2520report%2520serves%2520as%2520the%250Afoundational%2520reference%2520for%2520a%2520coordinated%2520research%2520program%2520examining%2520strategic%250Acoopetition%2520in%2520requirements%2520engineering%2520and%2520multi-agent%2520systems%252C%2520with%2520companion%250Awork%2520addressing%2520trust%2520dynamics%252C%2520team%2520production%252C%2520and%2520reciprocity%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%20Foundations%20for%20Strategic%20Coopetition%3A%20Formalizing%0A%20%20Interdependence%20and%20Complementarity&entry.906535625=Vik%20Pant%20and%20Eric%20Yu&entry.1292438233=%20%20Modern%20socio-technical%20systems%20are%20characterized%20by%20strategic%20coopetition%0Awhere%20actors%20simultaneously%20cooperate%20to%20create%20value%20and%20compete%20to%20capture%0Ait.%20While%20conceptual%20modeling%20languages%20like%20i%2A%20provide%20rich%20qualitative%0Arepresentations%20of%20strategic%20dependencies%2C%20they%20lack%20mechanisms%20for%0Aquantitative%20analysis%20of%20dynamic%20trade-offs.%20Conversely%2C%20classical%20game%20theory%0Aoffers%20mathematical%20rigor%20but%20strips%20away%20contextual%20richness.%20This%20technical%0Areport%20bridges%20this%20gap%20by%20developing%20computational%20foundations%20that%20formalize%0Atwo%20critical%20dimensions%20of%20coopetition%3A%20interdependence%20and%20complementarity.%20We%0Aground%20interdependence%20in%20i%2A%20structural%20dependency%20analysis%2C%20translating%0Adepender-dependee-dependum%20relationships%20into%20quantitative%20interdependence%0Acoefficients%20through%20a%20structured%20translation%20framework.%20We%20formalize%0Acomplementarity%20following%20Brandenburger%20and%20Nalebuff%27s%20Added%20Value%20concept%2C%0Amodeling%20synergistic%20value%20creation%20with%20validated%20parameterization.%20We%0Aintegrate%20structural%20dependencies%20with%20bargaining%20power%20in%20value%20appropriation%0Aand%20introduce%20a%20game-theoretic%20formulation%20where%20Nash%20Equilibrium%20incorporates%0Astructural%20interdependence.%20Validation%20combines%20comprehensive%20experimental%0Atesting%20across%20power%20and%20logarithmic%20value%20function%20specifications%2C%0Ademonstrating%20functional%20form%20robustness%2C%20with%20empirical%20application%20to%20the%0ASamsung-Sony%20S-LCD%20joint%20venture%20%282004-2011%29%2C%20where%20logarithmic%20specifications%0Aachieve%20superior%20empirical%20fit%20%28validation%20score%2045/60%29%20while%20power%20functions%0Aprovide%20theoretical%20tractability.%20This%20technical%20report%20serves%20as%20the%0Afoundational%20reference%20for%20a%20coordinated%20research%20program%20examining%20strategic%0Acoopetition%20in%20requirements%20engineering%20and%20multi-agent%20systems%2C%20with%20companion%0Awork%20addressing%20trust%20dynamics%2C%20team%20production%2C%20and%20reciprocity%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18802v1&entry.124074799=Read"},
{"title": "MADR: MPC-guided Adversarial DeepReach", "author": "Ryan Teoh and Sander Tonkens and William Sharpless and Aijia Yang and Zeyuan Feng and Somil Bansal and Sylvia Herbert", "abstract": "  Hamilton-Jacobi (HJ) Reachability offers a framework for generating safe\nvalue functions and policies in the face of adversarial disturbance, but is\nlimited by the curse of dimensionality. Physics-informed deep learning is able\nto overcome this infeasibility, but itself suffers from slow and inaccurate\nconvergence, primarily due to weak PDE gradients and the complexity of\nself-supervised learning. A few works, recently, have demonstrated that\nenriching the self-supervision process with regular supervision (based on the\nnature of the optimal control problem), greatly accelerates convergence and\nsolution quality, however, these have been limited to single player problems\nand simple games. In this work, we introduce MADR: MPC-guided Adversarial\nDeepReach, a general framework to robustly approximate the two-player, zero-sum\ndifferential game value function. In doing so, MADR yields the corresponding\noptimal strategies for both players in zero-sum games as well as safe policies\nfor worst-case robustness. We test MADR on a multitude of high-dimensional\nsimulated and real robotic agents with varying dynamics and games, finding that\nour approach significantly out-performs state-of-the-art baselines in\nsimulation and produces impressive results in hardware.\n", "link": "http://arxiv.org/abs/2510.18845v1", "date": "2025-10-21", "relevancy": 1.018, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5374}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5001}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MADR%3A%20MPC-guided%20Adversarial%20DeepReach&body=Title%3A%20MADR%3A%20MPC-guided%20Adversarial%20DeepReach%0AAuthor%3A%20Ryan%20Teoh%20and%20Sander%20Tonkens%20and%20William%20Sharpless%20and%20Aijia%20Yang%20and%20Zeyuan%20Feng%20and%20Somil%20Bansal%20and%20Sylvia%20Herbert%0AAbstract%3A%20%20%20Hamilton-Jacobi%20%28HJ%29%20Reachability%20offers%20a%20framework%20for%20generating%20safe%0Avalue%20functions%20and%20policies%20in%20the%20face%20of%20adversarial%20disturbance%2C%20but%20is%0Alimited%20by%20the%20curse%20of%20dimensionality.%20Physics-informed%20deep%20learning%20is%20able%0Ato%20overcome%20this%20infeasibility%2C%20but%20itself%20suffers%20from%20slow%20and%20inaccurate%0Aconvergence%2C%20primarily%20due%20to%20weak%20PDE%20gradients%20and%20the%20complexity%20of%0Aself-supervised%20learning.%20A%20few%20works%2C%20recently%2C%20have%20demonstrated%20that%0Aenriching%20the%20self-supervision%20process%20with%20regular%20supervision%20%28based%20on%20the%0Anature%20of%20the%20optimal%20control%20problem%29%2C%20greatly%20accelerates%20convergence%20and%0Asolution%20quality%2C%20however%2C%20these%20have%20been%20limited%20to%20single%20player%20problems%0Aand%20simple%20games.%20In%20this%20work%2C%20we%20introduce%20MADR%3A%20MPC-guided%20Adversarial%0ADeepReach%2C%20a%20general%20framework%20to%20robustly%20approximate%20the%20two-player%2C%20zero-sum%0Adifferential%20game%20value%20function.%20In%20doing%20so%2C%20MADR%20yields%20the%20corresponding%0Aoptimal%20strategies%20for%20both%20players%20in%20zero-sum%20games%20as%20well%20as%20safe%20policies%0Afor%20worst-case%20robustness.%20We%20test%20MADR%20on%20a%20multitude%20of%20high-dimensional%0Asimulated%20and%20real%20robotic%20agents%20with%20varying%20dynamics%20and%20games%2C%20finding%20that%0Aour%20approach%20significantly%20out-performs%20state-of-the-art%20baselines%20in%0Asimulation%20and%20produces%20impressive%20results%20in%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMADR%253A%2520MPC-guided%2520Adversarial%2520DeepReach%26entry.906535625%3DRyan%2520Teoh%2520and%2520Sander%2520Tonkens%2520and%2520William%2520Sharpless%2520and%2520Aijia%2520Yang%2520and%2520Zeyuan%2520Feng%2520and%2520Somil%2520Bansal%2520and%2520Sylvia%2520Herbert%26entry.1292438233%3D%2520%2520Hamilton-Jacobi%2520%2528HJ%2529%2520Reachability%2520offers%2520a%2520framework%2520for%2520generating%2520safe%250Avalue%2520functions%2520and%2520policies%2520in%2520the%2520face%2520of%2520adversarial%2520disturbance%252C%2520but%2520is%250Alimited%2520by%2520the%2520curse%2520of%2520dimensionality.%2520Physics-informed%2520deep%2520learning%2520is%2520able%250Ato%2520overcome%2520this%2520infeasibility%252C%2520but%2520itself%2520suffers%2520from%2520slow%2520and%2520inaccurate%250Aconvergence%252C%2520primarily%2520due%2520to%2520weak%2520PDE%2520gradients%2520and%2520the%2520complexity%2520of%250Aself-supervised%2520learning.%2520A%2520few%2520works%252C%2520recently%252C%2520have%2520demonstrated%2520that%250Aenriching%2520the%2520self-supervision%2520process%2520with%2520regular%2520supervision%2520%2528based%2520on%2520the%250Anature%2520of%2520the%2520optimal%2520control%2520problem%2529%252C%2520greatly%2520accelerates%2520convergence%2520and%250Asolution%2520quality%252C%2520however%252C%2520these%2520have%2520been%2520limited%2520to%2520single%2520player%2520problems%250Aand%2520simple%2520games.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MADR%253A%2520MPC-guided%2520Adversarial%250ADeepReach%252C%2520a%2520general%2520framework%2520to%2520robustly%2520approximate%2520the%2520two-player%252C%2520zero-sum%250Adifferential%2520game%2520value%2520function.%2520In%2520doing%2520so%252C%2520MADR%2520yields%2520the%2520corresponding%250Aoptimal%2520strategies%2520for%2520both%2520players%2520in%2520zero-sum%2520games%2520as%2520well%2520as%2520safe%2520policies%250Afor%2520worst-case%2520robustness.%2520We%2520test%2520MADR%2520on%2520a%2520multitude%2520of%2520high-dimensional%250Asimulated%2520and%2520real%2520robotic%2520agents%2520with%2520varying%2520dynamics%2520and%2520games%252C%2520finding%2520that%250Aour%2520approach%2520significantly%2520out-performs%2520state-of-the-art%2520baselines%2520in%250Asimulation%2520and%2520produces%2520impressive%2520results%2520in%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MADR%3A%20MPC-guided%20Adversarial%20DeepReach&entry.906535625=Ryan%20Teoh%20and%20Sander%20Tonkens%20and%20William%20Sharpless%20and%20Aijia%20Yang%20and%20Zeyuan%20Feng%20and%20Somil%20Bansal%20and%20Sylvia%20Herbert&entry.1292438233=%20%20Hamilton-Jacobi%20%28HJ%29%20Reachability%20offers%20a%20framework%20for%20generating%20safe%0Avalue%20functions%20and%20policies%20in%20the%20face%20of%20adversarial%20disturbance%2C%20but%20is%0Alimited%20by%20the%20curse%20of%20dimensionality.%20Physics-informed%20deep%20learning%20is%20able%0Ato%20overcome%20this%20infeasibility%2C%20but%20itself%20suffers%20from%20slow%20and%20inaccurate%0Aconvergence%2C%20primarily%20due%20to%20weak%20PDE%20gradients%20and%20the%20complexity%20of%0Aself-supervised%20learning.%20A%20few%20works%2C%20recently%2C%20have%20demonstrated%20that%0Aenriching%20the%20self-supervision%20process%20with%20regular%20supervision%20%28based%20on%20the%0Anature%20of%20the%20optimal%20control%20problem%29%2C%20greatly%20accelerates%20convergence%20and%0Asolution%20quality%2C%20however%2C%20these%20have%20been%20limited%20to%20single%20player%20problems%0Aand%20simple%20games.%20In%20this%20work%2C%20we%20introduce%20MADR%3A%20MPC-guided%20Adversarial%0ADeepReach%2C%20a%20general%20framework%20to%20robustly%20approximate%20the%20two-player%2C%20zero-sum%0Adifferential%20game%20value%20function.%20In%20doing%20so%2C%20MADR%20yields%20the%20corresponding%0Aoptimal%20strategies%20for%20both%20players%20in%20zero-sum%20games%20as%20well%20as%20safe%20policies%0Afor%20worst-case%20robustness.%20We%20test%20MADR%20on%20a%20multitude%20of%20high-dimensional%0Asimulated%20and%20real%20robotic%20agents%20with%20varying%20dynamics%20and%20games%2C%20finding%20that%0Aour%20approach%20significantly%20out-performs%20state-of-the-art%20baselines%20in%0Asimulation%20and%20produces%20impressive%20results%20in%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18845v1&entry.124074799=Read"},
{"title": "Diffusion Buffer for Online Generative Speech Enhancement", "author": "Bunlong Lay and Rostislav Makarov and Simon Welker and Maris Hillemann and Timo Gerkmann", "abstract": "  Online Speech Enhancement was mainly reserved for predictive models. A key\nadvantage of these models is that for an incoming signal frame from a stream of\ndata, the model is called only once for enhancement. In contrast, generative\nSpeech Enhancement models often require multiple calls, resulting in a\ncomputational complexity that is too high for many online speech enhancement\napplications. This work presents the Diffusion Buffer, a generative\ndiffusion-based Speech Enhancement model which only requires one neural network\ncall per incoming signal frame from a stream of data and performs enhancement\nin an online fashion on a consumer-grade GPU. The key idea of the Diffusion\nBuffer is to align physical time with Diffusion time-steps. The approach\nprogressively denoises frames through physical time, where past frames have\nmore noise removed. Consequently, an enhanced frame is output to the listener\nwith a delay defined by the Diffusion Buffer, and the output frame has a\ncorresponding look-ahead. In this work, we extend upon our previous work by\ncarefully designing a 2D convolutional UNet architecture that specifically\naligns with the Diffusion Buffer's look-ahead. We observe that the proposed\nUNet improves performance, particularly when the algorithmic latency is low.\nMoreover, we show that using a Data Prediction loss instead of Denoising Score\nMatching loss enables flexible control over the trade-off between algorithmic\nlatency and quality during inference. The extended Diffusion Buffer equipped\nwith a novel NN and loss function drastically reduces the algorithmic latency\nfrom 320 - 960 ms to 32 - 176 ms with an even increased performance. While it\nhas been shown before that offline generative diffusion models outperform\npredictive approaches in unseen noisy speech data, we confirm that the online\nDiffusion Buffer also outperforms its predictive counterpart on unseen noisy\nspeech data.\n", "link": "http://arxiv.org/abs/2510.18744v1", "date": "2025-10-21", "relevancy": 1.1617, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6074}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5678}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Buffer%20for%20Online%20Generative%20Speech%20Enhancement&body=Title%3A%20Diffusion%20Buffer%20for%20Online%20Generative%20Speech%20Enhancement%0AAuthor%3A%20Bunlong%20Lay%20and%20Rostislav%20Makarov%20and%20Simon%20Welker%20and%20Maris%20Hillemann%20and%20Timo%20Gerkmann%0AAbstract%3A%20%20%20Online%20Speech%20Enhancement%20was%20mainly%20reserved%20for%20predictive%20models.%20A%20key%0Aadvantage%20of%20these%20models%20is%20that%20for%20an%20incoming%20signal%20frame%20from%20a%20stream%20of%0Adata%2C%20the%20model%20is%20called%20only%20once%20for%20enhancement.%20In%20contrast%2C%20generative%0ASpeech%20Enhancement%20models%20often%20require%20multiple%20calls%2C%20resulting%20in%20a%0Acomputational%20complexity%20that%20is%20too%20high%20for%20many%20online%20speech%20enhancement%0Aapplications.%20This%20work%20presents%20the%20Diffusion%20Buffer%2C%20a%20generative%0Adiffusion-based%20Speech%20Enhancement%20model%20which%20only%20requires%20one%20neural%20network%0Acall%20per%20incoming%20signal%20frame%20from%20a%20stream%20of%20data%20and%20performs%20enhancement%0Ain%20an%20online%20fashion%20on%20a%20consumer-grade%20GPU.%20The%20key%20idea%20of%20the%20Diffusion%0ABuffer%20is%20to%20align%20physical%20time%20with%20Diffusion%20time-steps.%20The%20approach%0Aprogressively%20denoises%20frames%20through%20physical%20time%2C%20where%20past%20frames%20have%0Amore%20noise%20removed.%20Consequently%2C%20an%20enhanced%20frame%20is%20output%20to%20the%20listener%0Awith%20a%20delay%20defined%20by%20the%20Diffusion%20Buffer%2C%20and%20the%20output%20frame%20has%20a%0Acorresponding%20look-ahead.%20In%20this%20work%2C%20we%20extend%20upon%20our%20previous%20work%20by%0Acarefully%20designing%20a%202D%20convolutional%20UNet%20architecture%20that%20specifically%0Aaligns%20with%20the%20Diffusion%20Buffer%27s%20look-ahead.%20We%20observe%20that%20the%20proposed%0AUNet%20improves%20performance%2C%20particularly%20when%20the%20algorithmic%20latency%20is%20low.%0AMoreover%2C%20we%20show%20that%20using%20a%20Data%20Prediction%20loss%20instead%20of%20Denoising%20Score%0AMatching%20loss%20enables%20flexible%20control%20over%20the%20trade-off%20between%20algorithmic%0Alatency%20and%20quality%20during%20inference.%20The%20extended%20Diffusion%20Buffer%20equipped%0Awith%20a%20novel%20NN%20and%20loss%20function%20drastically%20reduces%20the%20algorithmic%20latency%0Afrom%20320%20-%20960%20ms%20to%2032%20-%20176%20ms%20with%20an%20even%20increased%20performance.%20While%20it%0Ahas%20been%20shown%20before%20that%20offline%20generative%20diffusion%20models%20outperform%0Apredictive%20approaches%20in%20unseen%20noisy%20speech%20data%2C%20we%20confirm%20that%20the%20online%0ADiffusion%20Buffer%20also%20outperforms%20its%20predictive%20counterpart%20on%20unseen%20noisy%0Aspeech%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Buffer%2520for%2520Online%2520Generative%2520Speech%2520Enhancement%26entry.906535625%3DBunlong%2520Lay%2520and%2520Rostislav%2520Makarov%2520and%2520Simon%2520Welker%2520and%2520Maris%2520Hillemann%2520and%2520Timo%2520Gerkmann%26entry.1292438233%3D%2520%2520Online%2520Speech%2520Enhancement%2520was%2520mainly%2520reserved%2520for%2520predictive%2520models.%2520A%2520key%250Aadvantage%2520of%2520these%2520models%2520is%2520that%2520for%2520an%2520incoming%2520signal%2520frame%2520from%2520a%2520stream%2520of%250Adata%252C%2520the%2520model%2520is%2520called%2520only%2520once%2520for%2520enhancement.%2520In%2520contrast%252C%2520generative%250ASpeech%2520Enhancement%2520models%2520often%2520require%2520multiple%2520calls%252C%2520resulting%2520in%2520a%250Acomputational%2520complexity%2520that%2520is%2520too%2520high%2520for%2520many%2520online%2520speech%2520enhancement%250Aapplications.%2520This%2520work%2520presents%2520the%2520Diffusion%2520Buffer%252C%2520a%2520generative%250Adiffusion-based%2520Speech%2520Enhancement%2520model%2520which%2520only%2520requires%2520one%2520neural%2520network%250Acall%2520per%2520incoming%2520signal%2520frame%2520from%2520a%2520stream%2520of%2520data%2520and%2520performs%2520enhancement%250Ain%2520an%2520online%2520fashion%2520on%2520a%2520consumer-grade%2520GPU.%2520The%2520key%2520idea%2520of%2520the%2520Diffusion%250ABuffer%2520is%2520to%2520align%2520physical%2520time%2520with%2520Diffusion%2520time-steps.%2520The%2520approach%250Aprogressively%2520denoises%2520frames%2520through%2520physical%2520time%252C%2520where%2520past%2520frames%2520have%250Amore%2520noise%2520removed.%2520Consequently%252C%2520an%2520enhanced%2520frame%2520is%2520output%2520to%2520the%2520listener%250Awith%2520a%2520delay%2520defined%2520by%2520the%2520Diffusion%2520Buffer%252C%2520and%2520the%2520output%2520frame%2520has%2520a%250Acorresponding%2520look-ahead.%2520In%2520this%2520work%252C%2520we%2520extend%2520upon%2520our%2520previous%2520work%2520by%250Acarefully%2520designing%2520a%25202D%2520convolutional%2520UNet%2520architecture%2520that%2520specifically%250Aaligns%2520with%2520the%2520Diffusion%2520Buffer%2527s%2520look-ahead.%2520We%2520observe%2520that%2520the%2520proposed%250AUNet%2520improves%2520performance%252C%2520particularly%2520when%2520the%2520algorithmic%2520latency%2520is%2520low.%250AMoreover%252C%2520we%2520show%2520that%2520using%2520a%2520Data%2520Prediction%2520loss%2520instead%2520of%2520Denoising%2520Score%250AMatching%2520loss%2520enables%2520flexible%2520control%2520over%2520the%2520trade-off%2520between%2520algorithmic%250Alatency%2520and%2520quality%2520during%2520inference.%2520The%2520extended%2520Diffusion%2520Buffer%2520equipped%250Awith%2520a%2520novel%2520NN%2520and%2520loss%2520function%2520drastically%2520reduces%2520the%2520algorithmic%2520latency%250Afrom%2520320%2520-%2520960%2520ms%2520to%252032%2520-%2520176%2520ms%2520with%2520an%2520even%2520increased%2520performance.%2520While%2520it%250Ahas%2520been%2520shown%2520before%2520that%2520offline%2520generative%2520diffusion%2520models%2520outperform%250Apredictive%2520approaches%2520in%2520unseen%2520noisy%2520speech%2520data%252C%2520we%2520confirm%2520that%2520the%2520online%250ADiffusion%2520Buffer%2520also%2520outperforms%2520its%2520predictive%2520counterpart%2520on%2520unseen%2520noisy%250Aspeech%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Buffer%20for%20Online%20Generative%20Speech%20Enhancement&entry.906535625=Bunlong%20Lay%20and%20Rostislav%20Makarov%20and%20Simon%20Welker%20and%20Maris%20Hillemann%20and%20Timo%20Gerkmann&entry.1292438233=%20%20Online%20Speech%20Enhancement%20was%20mainly%20reserved%20for%20predictive%20models.%20A%20key%0Aadvantage%20of%20these%20models%20is%20that%20for%20an%20incoming%20signal%20frame%20from%20a%20stream%20of%0Adata%2C%20the%20model%20is%20called%20only%20once%20for%20enhancement.%20In%20contrast%2C%20generative%0ASpeech%20Enhancement%20models%20often%20require%20multiple%20calls%2C%20resulting%20in%20a%0Acomputational%20complexity%20that%20is%20too%20high%20for%20many%20online%20speech%20enhancement%0Aapplications.%20This%20work%20presents%20the%20Diffusion%20Buffer%2C%20a%20generative%0Adiffusion-based%20Speech%20Enhancement%20model%20which%20only%20requires%20one%20neural%20network%0Acall%20per%20incoming%20signal%20frame%20from%20a%20stream%20of%20data%20and%20performs%20enhancement%0Ain%20an%20online%20fashion%20on%20a%20consumer-grade%20GPU.%20The%20key%20idea%20of%20the%20Diffusion%0ABuffer%20is%20to%20align%20physical%20time%20with%20Diffusion%20time-steps.%20The%20approach%0Aprogressively%20denoises%20frames%20through%20physical%20time%2C%20where%20past%20frames%20have%0Amore%20noise%20removed.%20Consequently%2C%20an%20enhanced%20frame%20is%20output%20to%20the%20listener%0Awith%20a%20delay%20defined%20by%20the%20Diffusion%20Buffer%2C%20and%20the%20output%20frame%20has%20a%0Acorresponding%20look-ahead.%20In%20this%20work%2C%20we%20extend%20upon%20our%20previous%20work%20by%0Acarefully%20designing%20a%202D%20convolutional%20UNet%20architecture%20that%20specifically%0Aaligns%20with%20the%20Diffusion%20Buffer%27s%20look-ahead.%20We%20observe%20that%20the%20proposed%0AUNet%20improves%20performance%2C%20particularly%20when%20the%20algorithmic%20latency%20is%20low.%0AMoreover%2C%20we%20show%20that%20using%20a%20Data%20Prediction%20loss%20instead%20of%20Denoising%20Score%0AMatching%20loss%20enables%20flexible%20control%20over%20the%20trade-off%20between%20algorithmic%0Alatency%20and%20quality%20during%20inference.%20The%20extended%20Diffusion%20Buffer%20equipped%0Awith%20a%20novel%20NN%20and%20loss%20function%20drastically%20reduces%20the%20algorithmic%20latency%0Afrom%20320%20-%20960%20ms%20to%2032%20-%20176%20ms%20with%20an%20even%20increased%20performance.%20While%20it%0Ahas%20been%20shown%20before%20that%20offline%20generative%20diffusion%20models%20outperform%0Apredictive%20approaches%20in%20unseen%20noisy%20speech%20data%2C%20we%20confirm%20that%20the%20online%0ADiffusion%20Buffer%20also%20outperforms%20its%20predictive%20counterpart%20on%20unseen%20noisy%0Aspeech%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18744v1&entry.124074799=Read"},
{"title": "Stick-Breaking Embedded Topic Model with Continuous Optimal Transport\n  for Online Analysis of Document Streams", "author": "Federica Granese and Serena Villata and Charles Bouveyron", "abstract": "  Online topic models are unsupervised algorithms to identify latent topics in\ndata streams that continuously evolve over time. Although these methods\nnaturally align with real-world scenarios, they have received considerably less\nattention from the community compared to their offline counterparts, due to\nspecific additional challenges. To tackle these issues, we present SB-SETM, an\ninnovative model extending the Embedded Topic Model (ETM) to process data\nstreams by merging models formed on successive partial document batches. To\nthis end, SB-SETM (i) leverages a truncated stick-breaking construction for the\ntopic-per-document distribution, enabling the model to automatically infer from\nthe data the appropriate number of active topics at each timestep; and (ii)\nintroduces a merging strategy for topic embeddings based on a continuous\nformulation of optimal transport adapted to the high dimensionality of the\nlatent topic space. Numerical experiments show SB-SETM outperforming baselines\non simulated scenarios. We extensively test it on a real-world corpus of news\narticles covering the Russian-Ukrainian war throughout 2022-2023.\n", "link": "http://arxiv.org/abs/2510.18786v1", "date": "2025-10-21", "relevancy": 1.3797, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4653}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.458}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stick-Breaking%20Embedded%20Topic%20Model%20with%20Continuous%20Optimal%20Transport%0A%20%20for%20Online%20Analysis%20of%20Document%20Streams&body=Title%3A%20Stick-Breaking%20Embedded%20Topic%20Model%20with%20Continuous%20Optimal%20Transport%0A%20%20for%20Online%20Analysis%20of%20Document%20Streams%0AAuthor%3A%20Federica%20Granese%20and%20Serena%20Villata%20and%20Charles%20Bouveyron%0AAbstract%3A%20%20%20Online%20topic%20models%20are%20unsupervised%20algorithms%20to%20identify%20latent%20topics%20in%0Adata%20streams%20that%20continuously%20evolve%20over%20time.%20Although%20these%20methods%0Anaturally%20align%20with%20real-world%20scenarios%2C%20they%20have%20received%20considerably%20less%0Aattention%20from%20the%20community%20compared%20to%20their%20offline%20counterparts%2C%20due%20to%0Aspecific%20additional%20challenges.%20To%20tackle%20these%20issues%2C%20we%20present%20SB-SETM%2C%20an%0Ainnovative%20model%20extending%20the%20Embedded%20Topic%20Model%20%28ETM%29%20to%20process%20data%0Astreams%20by%20merging%20models%20formed%20on%20successive%20partial%20document%20batches.%20To%0Athis%20end%2C%20SB-SETM%20%28i%29%20leverages%20a%20truncated%20stick-breaking%20construction%20for%20the%0Atopic-per-document%20distribution%2C%20enabling%20the%20model%20to%20automatically%20infer%20from%0Athe%20data%20the%20appropriate%20number%20of%20active%20topics%20at%20each%20timestep%3B%20and%20%28ii%29%0Aintroduces%20a%20merging%20strategy%20for%20topic%20embeddings%20based%20on%20a%20continuous%0Aformulation%20of%20optimal%20transport%20adapted%20to%20the%20high%20dimensionality%20of%20the%0Alatent%20topic%20space.%20Numerical%20experiments%20show%20SB-SETM%20outperforming%20baselines%0Aon%20simulated%20scenarios.%20We%20extensively%20test%20it%20on%20a%20real-world%20corpus%20of%20news%0Aarticles%20covering%20the%20Russian-Ukrainian%20war%20throughout%202022-2023.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStick-Breaking%2520Embedded%2520Topic%2520Model%2520with%2520Continuous%2520Optimal%2520Transport%250A%2520%2520for%2520Online%2520Analysis%2520of%2520Document%2520Streams%26entry.906535625%3DFederica%2520Granese%2520and%2520Serena%2520Villata%2520and%2520Charles%2520Bouveyron%26entry.1292438233%3D%2520%2520Online%2520topic%2520models%2520are%2520unsupervised%2520algorithms%2520to%2520identify%2520latent%2520topics%2520in%250Adata%2520streams%2520that%2520continuously%2520evolve%2520over%2520time.%2520Although%2520these%2520methods%250Anaturally%2520align%2520with%2520real-world%2520scenarios%252C%2520they%2520have%2520received%2520considerably%2520less%250Aattention%2520from%2520the%2520community%2520compared%2520to%2520their%2520offline%2520counterparts%252C%2520due%2520to%250Aspecific%2520additional%2520challenges.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520present%2520SB-SETM%252C%2520an%250Ainnovative%2520model%2520extending%2520the%2520Embedded%2520Topic%2520Model%2520%2528ETM%2529%2520to%2520process%2520data%250Astreams%2520by%2520merging%2520models%2520formed%2520on%2520successive%2520partial%2520document%2520batches.%2520To%250Athis%2520end%252C%2520SB-SETM%2520%2528i%2529%2520leverages%2520a%2520truncated%2520stick-breaking%2520construction%2520for%2520the%250Atopic-per-document%2520distribution%252C%2520enabling%2520the%2520model%2520to%2520automatically%2520infer%2520from%250Athe%2520data%2520the%2520appropriate%2520number%2520of%2520active%2520topics%2520at%2520each%2520timestep%253B%2520and%2520%2528ii%2529%250Aintroduces%2520a%2520merging%2520strategy%2520for%2520topic%2520embeddings%2520based%2520on%2520a%2520continuous%250Aformulation%2520of%2520optimal%2520transport%2520adapted%2520to%2520the%2520high%2520dimensionality%2520of%2520the%250Alatent%2520topic%2520space.%2520Numerical%2520experiments%2520show%2520SB-SETM%2520outperforming%2520baselines%250Aon%2520simulated%2520scenarios.%2520We%2520extensively%2520test%2520it%2520on%2520a%2520real-world%2520corpus%2520of%2520news%250Aarticles%2520covering%2520the%2520Russian-Ukrainian%2520war%2520throughout%25202022-2023.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stick-Breaking%20Embedded%20Topic%20Model%20with%20Continuous%20Optimal%20Transport%0A%20%20for%20Online%20Analysis%20of%20Document%20Streams&entry.906535625=Federica%20Granese%20and%20Serena%20Villata%20and%20Charles%20Bouveyron&entry.1292438233=%20%20Online%20topic%20models%20are%20unsupervised%20algorithms%20to%20identify%20latent%20topics%20in%0Adata%20streams%20that%20continuously%20evolve%20over%20time.%20Although%20these%20methods%0Anaturally%20align%20with%20real-world%20scenarios%2C%20they%20have%20received%20considerably%20less%0Aattention%20from%20the%20community%20compared%20to%20their%20offline%20counterparts%2C%20due%20to%0Aspecific%20additional%20challenges.%20To%20tackle%20these%20issues%2C%20we%20present%20SB-SETM%2C%20an%0Ainnovative%20model%20extending%20the%20Embedded%20Topic%20Model%20%28ETM%29%20to%20process%20data%0Astreams%20by%20merging%20models%20formed%20on%20successive%20partial%20document%20batches.%20To%0Athis%20end%2C%20SB-SETM%20%28i%29%20leverages%20a%20truncated%20stick-breaking%20construction%20for%20the%0Atopic-per-document%20distribution%2C%20enabling%20the%20model%20to%20automatically%20infer%20from%0Athe%20data%20the%20appropriate%20number%20of%20active%20topics%20at%20each%20timestep%3B%20and%20%28ii%29%0Aintroduces%20a%20merging%20strategy%20for%20topic%20embeddings%20based%20on%20a%20continuous%0Aformulation%20of%20optimal%20transport%20adapted%20to%20the%20high%20dimensionality%20of%20the%0Alatent%20topic%20space.%20Numerical%20experiments%20show%20SB-SETM%20outperforming%20baselines%0Aon%20simulated%20scenarios.%20We%20extensively%20test%20it%20on%20a%20real-world%20corpus%20of%20news%0Aarticles%20covering%20the%20Russian-Ukrainian%20war%20throughout%202022-2023.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18786v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


