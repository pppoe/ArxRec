<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Discriminative Probing and Tuning for Text-to-Image Generation", "author": "Leigang Qu and Wenjie Wang and Yongqi Li and Hanwang Zhang and Liqiang Nie and Tat-Seng Chua", "abstract": "  Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.\n", "link": "http://arxiv.org/abs/2403.04321v1", "date": "2024-03-07", "relevancy": 2.9742, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.609}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5903}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5852}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Discriminative%20Probing%20and%20Tuning%20for%20Text-to-Image%20Generation&body=Title%3A%20Discriminative%20Probing%20and%20Tuning%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Leigang%20Qu%20and%20Wenjie%20Wang%20and%20Yongqi%20Li%20and%20Hanwang%20Zhang%20and%20Liqiang%20Nie%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Despite%20advancements%20in%20text-to-image%20generation%20%28T2I%29%2C%20prior%20methods%20often%0Aface%20text-image%20misalignment%20problems%20such%20as%20relation%20confusion%20in%20generated%0Aimages.%20Existing%20solutions%20involve%20cross-attention%20manipulation%20for%20better%0Acompositional%20understanding%20or%20integrating%20large%20language%20models%20for%20improved%0Alayout%20planning.%20However%2C%20the%20inherent%20alignment%20capabilities%20of%20T2I%20models%20are%0Astill%20inadequate.%20By%20reviewing%20the%20link%20between%20generative%20and%20discriminative%0Amodeling%2C%20we%20posit%20that%20T2I%20models%27%20discriminative%20abilities%20may%20reflect%20their%0Atext-image%20alignment%20proficiency%20during%20generation.%20In%20this%20light%2C%20we%20advocate%0Abolstering%20the%20discriminative%20abilities%20of%20T2I%20models%20to%20achieve%20more%20precise%0Atext-to-image%20alignment%20for%20generation.%20We%20present%20a%20discriminative%20adapter%0Abuilt%20on%20T2I%20models%20to%20probe%20their%20discriminative%20abilities%20on%20two%0Arepresentative%20tasks%20and%20leverage%20discriminative%20fine-tuning%20to%20improve%20their%0Atext-image%20alignment.%20As%20a%20bonus%20of%20the%20discriminative%20adapter%2C%20a%0Aself-correction%20mechanism%20can%20leverage%20discriminative%20gradients%20to%20better%20align%0Agenerated%20images%20to%20text%20prompts%20during%20inference.%20Comprehensive%20evaluations%0Aacross%20three%20benchmark%20datasets%2C%20including%20both%20in-distribution%20and%0Aout-of-distribution%20scenarios%2C%20demonstrate%20our%20method%27s%20superior%20generation%0Aperformance.%20Meanwhile%2C%20it%20achieves%20state-of-the-art%20discriminative%20performance%0Aon%20the%20two%20discriminative%20tasks%20compared%20to%20other%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04321v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discriminative%20Probing%20and%20Tuning%20for%20Text-to-Image%20Generation&entry.906535625=Leigang%20Qu%20and%20Wenjie%20Wang%20and%20Yongqi%20Li%20and%20Hanwang%20Zhang%20and%20Liqiang%20Nie%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Despite%20advancements%20in%20text-to-image%20generation%20%28T2I%29%2C%20prior%20methods%20often%0Aface%20text-image%20misalignment%20problems%20such%20as%20relation%20confusion%20in%20generated%0Aimages.%20Existing%20solutions%20involve%20cross-attention%20manipulation%20for%20better%0Acompositional%20understanding%20or%20integrating%20large%20language%20models%20for%20improved%0Alayout%20planning.%20However%2C%20the%20inherent%20alignment%20capabilities%20of%20T2I%20models%20are%0Astill%20inadequate.%20By%20reviewing%20the%20link%20between%20generative%20and%20discriminative%0Amodeling%2C%20we%20posit%20that%20T2I%20models%27%20discriminative%20abilities%20may%20reflect%20their%0Atext-image%20alignment%20proficiency%20during%20generation.%20In%20this%20light%2C%20we%20advocate%0Abolstering%20the%20discriminative%20abilities%20of%20T2I%20models%20to%20achieve%20more%20precise%0Atext-to-image%20alignment%20for%20generation.%20We%20present%20a%20discriminative%20adapter%0Abuilt%20on%20T2I%20models%20to%20probe%20their%20discriminative%20abilities%20on%20two%0Arepresentative%20tasks%20and%20leverage%20discriminative%20fine-tuning%20to%20improve%20their%0Atext-image%20alignment.%20As%20a%20bonus%20of%20the%20discriminative%20adapter%2C%20a%0Aself-correction%20mechanism%20can%20leverage%20discriminative%20gradients%20to%20better%20align%0Agenerated%20images%20to%20text%20prompts%20during%20inference.%20Comprehensive%20evaluations%0Aacross%20three%20benchmark%20datasets%2C%20including%20both%20in-distribution%20and%0Aout-of-distribution%20scenarios%2C%20demonstrate%20our%20method%27s%20superior%20generation%0Aperformance.%20Meanwhile%2C%20it%20achieves%20state-of-the-art%20discriminative%20performance%0Aon%20the%20two%20discriminative%20tasks%20compared%20to%20other%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04321v1&entry.124074799=Read"},
{"title": "Control-Barrier-Aided Teleoperation with Visual-Inertial SLAM for Safe\n  MAV Navigation in Complex Environments", "author": "Siqi Zhou and Sotiris Papatheodorou and Stefan Leutenegger and Angela P. Schoellig", "abstract": "  In this paper, we consider a Micro Aerial Vehicle (MAV) system teleoperated\nby a non-expert and introduce a perceptive safety filter that leverages Control\nBarrier Functions (CBFs) in conjunction with Visual-Inertial Simultaneous\nLocalization and Mapping (VI-SLAM) and dense 3D occupancy mapping to guarantee\nsafe navigation in complex and unstructured environments. Our system relies\nsolely on onboard IMU measurements, stereo infrared images, and depth images\nand autonomously corrects teleoperated inputs when they are deemed unsafe. We\ndefine a point in 3D space as unsafe if it satisfies either of two conditions:\n(i) it is occupied by an obstacle, or (ii) it remains unmapped. At each time\nstep, an occupancy map of the environment is updated by the VI-SLAM by fusing\nthe onboard measurements, and a CBF is constructed to parameterize the (un)safe\nregion in the 3D space. Given the CBF and state feedback from the VI-SLAM\nmodule, a safety filter computes a certified reference that best matches the\nteleoperation input while satisfying the safety constraint encoded by the CBF.\nIn contrast to existing perception-based safe control frameworks, we directly\nclose the perception-action loop and demonstrate the full capability of safe\ncontrol in combination with real-time VI-SLAM without any external\ninfrastructure or prior knowledge of the environment. We verify the efficacy of\nthe perceptive safety filter in real-time MAV experiments using exclusively\nonboard sensing and computation and show that the teleoperated MAV is able to\nsafely navigate through unknown environments despite arbitrary inputs sent by\nthe teleoperator.\n", "link": "http://arxiv.org/abs/2403.04331v1", "date": "2024-03-07", "relevancy": 2.935, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6207}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5909}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5493}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Control-Barrier-Aided%20Teleoperation%20with%20Visual-Inertial%20SLAM%20for%20Safe%0A%20%20MAV%20Navigation%20in%20Complex%20Environments&body=Title%3A%20Control-Barrier-Aided%20Teleoperation%20with%20Visual-Inertial%20SLAM%20for%20Safe%0A%20%20MAV%20Navigation%20in%20Complex%20Environments%0AAuthor%3A%20Siqi%20Zhou%20and%20Sotiris%20Papatheodorou%20and%20Stefan%20Leutenegger%20and%20Angela%20P.%20Schoellig%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20a%20Micro%20Aerial%20Vehicle%20%28MAV%29%20system%20teleoperated%0Aby%20a%20non-expert%20and%20introduce%20a%20perceptive%20safety%20filter%20that%20leverages%20Control%0ABarrier%20Functions%20%28CBFs%29%20in%20conjunction%20with%20Visual-Inertial%20Simultaneous%0ALocalization%20and%20Mapping%20%28VI-SLAM%29%20and%20dense%203D%20occupancy%20mapping%20to%20guarantee%0Asafe%20navigation%20in%20complex%20and%20unstructured%20environments.%20Our%20system%20relies%0Asolely%20on%20onboard%20IMU%20measurements%2C%20stereo%20infrared%20images%2C%20and%20depth%20images%0Aand%20autonomously%20corrects%20teleoperated%20inputs%20when%20they%20are%20deemed%20unsafe.%20We%0Adefine%20a%20point%20in%203D%20space%20as%20unsafe%20if%20it%20satisfies%20either%20of%20two%20conditions%3A%0A%28i%29%20it%20is%20occupied%20by%20an%20obstacle%2C%20or%20%28ii%29%20it%20remains%20unmapped.%20At%20each%20time%0Astep%2C%20an%20occupancy%20map%20of%20the%20environment%20is%20updated%20by%20the%20VI-SLAM%20by%20fusing%0Athe%20onboard%20measurements%2C%20and%20a%20CBF%20is%20constructed%20to%20parameterize%20the%20%28un%29safe%0Aregion%20in%20the%203D%20space.%20Given%20the%20CBF%20and%20state%20feedback%20from%20the%20VI-SLAM%0Amodule%2C%20a%20safety%20filter%20computes%20a%20certified%20reference%20that%20best%20matches%20the%0Ateleoperation%20input%20while%20satisfying%20the%20safety%20constraint%20encoded%20by%20the%20CBF.%0AIn%20contrast%20to%20existing%20perception-based%20safe%20control%20frameworks%2C%20we%20directly%0Aclose%20the%20perception-action%20loop%20and%20demonstrate%20the%20full%20capability%20of%20safe%0Acontrol%20in%20combination%20with%20real-time%20VI-SLAM%20without%20any%20external%0Ainfrastructure%20or%20prior%20knowledge%20of%20the%20environment.%20We%20verify%20the%20efficacy%20of%0Athe%20perceptive%20safety%20filter%20in%20real-time%20MAV%20experiments%20using%20exclusively%0Aonboard%20sensing%20and%20computation%20and%20show%20that%20the%20teleoperated%20MAV%20is%20able%20to%0Asafely%20navigate%20through%20unknown%20environments%20despite%20arbitrary%20inputs%20sent%20by%0Athe%20teleoperator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04331v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Control-Barrier-Aided%20Teleoperation%20with%20Visual-Inertial%20SLAM%20for%20Safe%0A%20%20MAV%20Navigation%20in%20Complex%20Environments&entry.906535625=Siqi%20Zhou%20and%20Sotiris%20Papatheodorou%20and%20Stefan%20Leutenegger%20and%20Angela%20P.%20Schoellig&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20a%20Micro%20Aerial%20Vehicle%20%28MAV%29%20system%20teleoperated%0Aby%20a%20non-expert%20and%20introduce%20a%20perceptive%20safety%20filter%20that%20leverages%20Control%0ABarrier%20Functions%20%28CBFs%29%20in%20conjunction%20with%20Visual-Inertial%20Simultaneous%0ALocalization%20and%20Mapping%20%28VI-SLAM%29%20and%20dense%203D%20occupancy%20mapping%20to%20guarantee%0Asafe%20navigation%20in%20complex%20and%20unstructured%20environments.%20Our%20system%20relies%0Asolely%20on%20onboard%20IMU%20measurements%2C%20stereo%20infrared%20images%2C%20and%20depth%20images%0Aand%20autonomously%20corrects%20teleoperated%20inputs%20when%20they%20are%20deemed%20unsafe.%20We%0Adefine%20a%20point%20in%203D%20space%20as%20unsafe%20if%20it%20satisfies%20either%20of%20two%20conditions%3A%0A%28i%29%20it%20is%20occupied%20by%20an%20obstacle%2C%20or%20%28ii%29%20it%20remains%20unmapped.%20At%20each%20time%0Astep%2C%20an%20occupancy%20map%20of%20the%20environment%20is%20updated%20by%20the%20VI-SLAM%20by%20fusing%0Athe%20onboard%20measurements%2C%20and%20a%20CBF%20is%20constructed%20to%20parameterize%20the%20%28un%29safe%0Aregion%20in%20the%203D%20space.%20Given%20the%20CBF%20and%20state%20feedback%20from%20the%20VI-SLAM%0Amodule%2C%20a%20safety%20filter%20computes%20a%20certified%20reference%20that%20best%20matches%20the%0Ateleoperation%20input%20while%20satisfying%20the%20safety%20constraint%20encoded%20by%20the%20CBF.%0AIn%20contrast%20to%20existing%20perception-based%20safe%20control%20frameworks%2C%20we%20directly%0Aclose%20the%20perception-action%20loop%20and%20demonstrate%20the%20full%20capability%20of%20safe%0Acontrol%20in%20combination%20with%20real-time%20VI-SLAM%20without%20any%20external%0Ainfrastructure%20or%20prior%20knowledge%20of%20the%20environment.%20We%20verify%20the%20efficacy%20of%0Athe%20perceptive%20safety%20filter%20in%20real-time%20MAV%20experiments%20using%20exclusively%0Aonboard%20sensing%20and%20computation%20and%20show%20that%20the%20teleoperated%20MAV%20is%20able%20to%0Asafely%20navigate%20through%20unknown%20environments%20despite%20arbitrary%20inputs%20sent%20by%0Athe%20teleoperator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04331v1&entry.124074799=Read"},
{"title": "Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation", "author": "Ruicong Liu and Takehiko Ohkawa and Mingfang Zhang and Yoichi Sato", "abstract": "  The pursuit of accurate 3D hand pose estimation stands as a keystone for\nunderstanding human activity in the realm of egocentric vision. The majority of\nexisting estimation methods still rely on single-view images as input, leading\nto potential limitations, e.g., limited field-of-view and ambiguity in depth.\nTo address these problems, adding another camera to better capture the shape of\nhands is a practical direction. However, existing multi-view hand pose\nestimation methods suffer from two main drawbacks: 1) Requiring multi-view\nannotations for training, which are expensive. 2) During testing, the model\nbecomes inapplicable if camera parameters/layout are not the same as those used\nin training. In this paper, we propose a novel Single-to-Dual-view adaptation\n(S2DHand) solution that adapts a pre-trained single-view estimator to dual\nviews. Compared with existing multi-view training methods, 1) our adaptation\nprocess is unsupervised, eliminating the need for multi-view annotation. 2)\nMoreover, our method can handle arbitrary dual-view pairs with unknown camera\nparameters, making the model applicable to diverse camera settings.\nSpecifically, S2DHand is built on certain stereo constraints, including\npair-wise cross-view consensus and invariance of transformation between both\nviews. These two stereo constraints are used in a complementary manner to\ngenerate pseudo-labels, allowing reliable adaptation. Evaluation results reveal\nthat S2DHand achieves significant improvements on arbitrary camera pairs under\nboth in-dataset and cross-dataset settings, and outperforms existing adaptation\nmethods with leading performance. Project page:\nhttps://github.com/MickeyLLG/S2DHand.\n", "link": "http://arxiv.org/abs/2403.04381v1", "date": "2024-03-07", "relevancy": 2.8439, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5756}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5672}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5636}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Single-to-Dual-View%20Adaptation%20for%20Egocentric%203D%20Hand%20Pose%20Estimation&body=Title%3A%20Single-to-Dual-View%20Adaptation%20for%20Egocentric%203D%20Hand%20Pose%20Estimation%0AAuthor%3A%20Ruicong%20Liu%20and%20Takehiko%20Ohkawa%20and%20Mingfang%20Zhang%20and%20Yoichi%20Sato%0AAbstract%3A%20%20%20The%20pursuit%20of%20accurate%203D%20hand%20pose%20estimation%20stands%20as%20a%20keystone%20for%0Aunderstanding%20human%20activity%20in%20the%20realm%20of%20egocentric%20vision.%20The%20majority%20of%0Aexisting%20estimation%20methods%20still%20rely%20on%20single-view%20images%20as%20input%2C%20leading%0Ato%20potential%20limitations%2C%20e.g.%2C%20limited%20field-of-view%20and%20ambiguity%20in%20depth.%0ATo%20address%20these%20problems%2C%20adding%20another%20camera%20to%20better%20capture%20the%20shape%20of%0Ahands%20is%20a%20practical%20direction.%20However%2C%20existing%20multi-view%20hand%20pose%0Aestimation%20methods%20suffer%20from%20two%20main%20drawbacks%3A%201%29%20Requiring%20multi-view%0Aannotations%20for%20training%2C%20which%20are%20expensive.%202%29%20During%20testing%2C%20the%20model%0Abecomes%20inapplicable%20if%20camera%20parameters/layout%20are%20not%20the%20same%20as%20those%20used%0Ain%20training.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Single-to-Dual-view%20adaptation%0A%28S2DHand%29%20solution%20that%20adapts%20a%20pre-trained%20single-view%20estimator%20to%20dual%0Aviews.%20Compared%20with%20existing%20multi-view%20training%20methods%2C%201%29%20our%20adaptation%0Aprocess%20is%20unsupervised%2C%20eliminating%20the%20need%20for%20multi-view%20annotation.%202%29%0AMoreover%2C%20our%20method%20can%20handle%20arbitrary%20dual-view%20pairs%20with%20unknown%20camera%0Aparameters%2C%20making%20the%20model%20applicable%20to%20diverse%20camera%20settings.%0ASpecifically%2C%20S2DHand%20is%20built%20on%20certain%20stereo%20constraints%2C%20including%0Apair-wise%20cross-view%20consensus%20and%20invariance%20of%20transformation%20between%20both%0Aviews.%20These%20two%20stereo%20constraints%20are%20used%20in%20a%20complementary%20manner%20to%0Agenerate%20pseudo-labels%2C%20allowing%20reliable%20adaptation.%20Evaluation%20results%20reveal%0Athat%20S2DHand%20achieves%20significant%20improvements%20on%20arbitrary%20camera%20pairs%20under%0Aboth%20in-dataset%20and%20cross-dataset%20settings%2C%20and%20outperforms%20existing%20adaptation%0Amethods%20with%20leading%20performance.%20Project%20page%3A%0Ahttps%3A//github.com/MickeyLLG/S2DHand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04381v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-to-Dual-View%20Adaptation%20for%20Egocentric%203D%20Hand%20Pose%20Estimation&entry.906535625=Ruicong%20Liu%20and%20Takehiko%20Ohkawa%20and%20Mingfang%20Zhang%20and%20Yoichi%20Sato&entry.1292438233=%20%20The%20pursuit%20of%20accurate%203D%20hand%20pose%20estimation%20stands%20as%20a%20keystone%20for%0Aunderstanding%20human%20activity%20in%20the%20realm%20of%20egocentric%20vision.%20The%20majority%20of%0Aexisting%20estimation%20methods%20still%20rely%20on%20single-view%20images%20as%20input%2C%20leading%0Ato%20potential%20limitations%2C%20e.g.%2C%20limited%20field-of-view%20and%20ambiguity%20in%20depth.%0ATo%20address%20these%20problems%2C%20adding%20another%20camera%20to%20better%20capture%20the%20shape%20of%0Ahands%20is%20a%20practical%20direction.%20However%2C%20existing%20multi-view%20hand%20pose%0Aestimation%20methods%20suffer%20from%20two%20main%20drawbacks%3A%201%29%20Requiring%20multi-view%0Aannotations%20for%20training%2C%20which%20are%20expensive.%202%29%20During%20testing%2C%20the%20model%0Abecomes%20inapplicable%20if%20camera%20parameters/layout%20are%20not%20the%20same%20as%20those%20used%0Ain%20training.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Single-to-Dual-view%20adaptation%0A%28S2DHand%29%20solution%20that%20adapts%20a%20pre-trained%20single-view%20estimator%20to%20dual%0Aviews.%20Compared%20with%20existing%20multi-view%20training%20methods%2C%201%29%20our%20adaptation%0Aprocess%20is%20unsupervised%2C%20eliminating%20the%20need%20for%20multi-view%20annotation.%202%29%0AMoreover%2C%20our%20method%20can%20handle%20arbitrary%20dual-view%20pairs%20with%20unknown%20camera%0Aparameters%2C%20making%20the%20model%20applicable%20to%20diverse%20camera%20settings.%0ASpecifically%2C%20S2DHand%20is%20built%20on%20certain%20stereo%20constraints%2C%20including%0Apair-wise%20cross-view%20consensus%20and%20invariance%20of%20transformation%20between%20both%0Aviews.%20These%20two%20stereo%20constraints%20are%20used%20in%20a%20complementary%20manner%20to%0Agenerate%20pseudo-labels%2C%20allowing%20reliable%20adaptation.%20Evaluation%20results%20reveal%0Athat%20S2DHand%20achieves%20significant%20improvements%20on%20arbitrary%20camera%20pairs%20under%0Aboth%20in-dataset%20and%20cross-dataset%20settings%2C%20and%20outperforms%20existing%20adaptation%0Amethods%20with%20leading%20performance.%20Project%20page%3A%0Ahttps%3A//github.com/MickeyLLG/S2DHand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04381v1&entry.124074799=Read"},
{"title": "DAMSDet: Dynamic Adaptive Multispectral Detection Transformer with\n  Competitive Query Selection and Adaptive Feature Fusion", "author": "Junjie Guo and Chenqiang Gao and Fangcen Liu and Deyu Meng and Xinbo Gao", "abstract": "  Infrared-visible object detection aims to achieve robust even full-day object\ndetection by fusing the complementary information of infrared and visible\nimages. However, highly dynamically variable complementary characteristics and\ncommonly existing modality misalignment make the fusion of complementary\ninformation difficult. In this paper, we propose a Dynamic Adaptive\nMultispectral Detection Transformer (DAMSDet) to simultaneously address these\ntwo challenges. Specifically, we propose a Modality Competitive Query Selection\nstrategy to provide useful prior information. This strategy can dynamically\nselect basic salient modality feature representation for each object. To\neffectively mine the complementary information and adapt to misalignment\nsituations, we propose a Multispectral Deformable Cross-attention module to\nadaptively sample and aggregate multi-semantic level features of infrared and\nvisible images for each object. In addition, we further adopt the cascade\nstructure of DETR to better mine complementary information. Experiments on four\npublic datasets of different scenes demonstrate significant improvements\ncompared to other state-of-the-art methods. The code will be released at\nhttps://github.com/gjj45/DAMSDet.\n", "link": "http://arxiv.org/abs/2403.00326v3", "date": "2024-03-07", "relevancy": 2.8296, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5777}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5611}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5589}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DAMSDet%3A%20Dynamic%20Adaptive%20Multispectral%20Detection%20Transformer%20with%0A%20%20Competitive%20Query%20Selection%20and%20Adaptive%20Feature%20Fusion&body=Title%3A%20DAMSDet%3A%20Dynamic%20Adaptive%20Multispectral%20Detection%20Transformer%20with%0A%20%20Competitive%20Query%20Selection%20and%20Adaptive%20Feature%20Fusion%0AAuthor%3A%20Junjie%20Guo%20and%20Chenqiang%20Gao%20and%20Fangcen%20Liu%20and%20Deyu%20Meng%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Infrared-visible%20object%20detection%20aims%20to%20achieve%20robust%20even%20full-day%20object%0Adetection%20by%20fusing%20the%20complementary%20information%20of%20infrared%20and%20visible%0Aimages.%20However%2C%20highly%20dynamically%20variable%20complementary%20characteristics%20and%0Acommonly%20existing%20modality%20misalignment%20make%20the%20fusion%20of%20complementary%0Ainformation%20difficult.%20In%20this%20paper%2C%20we%20propose%20a%20Dynamic%20Adaptive%0AMultispectral%20Detection%20Transformer%20%28DAMSDet%29%20to%20simultaneously%20address%20these%0Atwo%20challenges.%20Specifically%2C%20we%20propose%20a%20Modality%20Competitive%20Query%20Selection%0Astrategy%20to%20provide%20useful%20prior%20information.%20This%20strategy%20can%20dynamically%0Aselect%20basic%20salient%20modality%20feature%20representation%20for%20each%20object.%20To%0Aeffectively%20mine%20the%20complementary%20information%20and%20adapt%20to%20misalignment%0Asituations%2C%20we%20propose%20a%20Multispectral%20Deformable%20Cross-attention%20module%20to%0Aadaptively%20sample%20and%20aggregate%20multi-semantic%20level%20features%20of%20infrared%20and%0Avisible%20images%20for%20each%20object.%20In%20addition%2C%20we%20further%20adopt%20the%20cascade%0Astructure%20of%20DETR%20to%20better%20mine%20complementary%20information.%20Experiments%20on%20four%0Apublic%20datasets%20of%20different%20scenes%20demonstrate%20significant%20improvements%0Acompared%20to%20other%20state-of-the-art%20methods.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/gjj45/DAMSDet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00326v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAMSDet%3A%20Dynamic%20Adaptive%20Multispectral%20Detection%20Transformer%20with%0A%20%20Competitive%20Query%20Selection%20and%20Adaptive%20Feature%20Fusion&entry.906535625=Junjie%20Guo%20and%20Chenqiang%20Gao%20and%20Fangcen%20Liu%20and%20Deyu%20Meng%20and%20Xinbo%20Gao&entry.1292438233=%20%20Infrared-visible%20object%20detection%20aims%20to%20achieve%20robust%20even%20full-day%20object%0Adetection%20by%20fusing%20the%20complementary%20information%20of%20infrared%20and%20visible%0Aimages.%20However%2C%20highly%20dynamically%20variable%20complementary%20characteristics%20and%0Acommonly%20existing%20modality%20misalignment%20make%20the%20fusion%20of%20complementary%0Ainformation%20difficult.%20In%20this%20paper%2C%20we%20propose%20a%20Dynamic%20Adaptive%0AMultispectral%20Detection%20Transformer%20%28DAMSDet%29%20to%20simultaneously%20address%20these%0Atwo%20challenges.%20Specifically%2C%20we%20propose%20a%20Modality%20Competitive%20Query%20Selection%0Astrategy%20to%20provide%20useful%20prior%20information.%20This%20strategy%20can%20dynamically%0Aselect%20basic%20salient%20modality%20feature%20representation%20for%20each%20object.%20To%0Aeffectively%20mine%20the%20complementary%20information%20and%20adapt%20to%20misalignment%0Asituations%2C%20we%20propose%20a%20Multispectral%20Deformable%20Cross-attention%20module%20to%0Aadaptively%20sample%20and%20aggregate%20multi-semantic%20level%20features%20of%20infrared%20and%0Avisible%20images%20for%20each%20object.%20In%20addition%2C%20we%20further%20adopt%20the%20cascade%0Astructure%20of%20DETR%20to%20better%20mine%20complementary%20information.%20Experiments%20on%20four%0Apublic%20datasets%20of%20different%20scenes%20demonstrate%20significant%20improvements%0Acompared%20to%20other%20state-of-the-art%20methods.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/gjj45/DAMSDet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00326v3&entry.124074799=Read"},
{"title": "PatchCraft: Exploring Texture Patch for Efficient AI-generated Image\n  Detection", "author": "Nan Zhong and Yiran Xu and Sheng Li and Zhenxing Qian and Xinpeng Zhang", "abstract": "  Recent generative models show impressive performance in generating\nphotographic images. Humans can hardly distinguish such incredibly\nrealistic-looking AI-generated images from real ones. AI-generated images may\nlead to ubiquitous disinformation dissemination. Therefore, it is of utmost\nurgency to develop a detector to identify AI generated images. Most existing\ndetectors suffer from sharp performance drops over unseen generative models. In\nthis paper, we propose a novel AI-generated image detector capable of\nidentifying fake images created by a wide range of generative models. We\nobserve that the texture patches of images tend to reveal more traces left by\ngenerative models compared to the global semantic information of the images. A\nnovel Smash&Reconstruction preprocessing is proposed to erase the global\nsemantic information and enhance texture patches. Furthermore, pixels in rich\ntexture regions exhibit more significant fluctuations than those in poor\ntexture regions. Synthesizing realistic rich texture regions proves to be more\nchallenging for existing generative models. Based on this principle, we\nleverage the inter-pixel correlation contrast between rich and poor texture\nregions within an image to further boost the detection performance.\n  In addition, we build a comprehensive AI-generated image detection benchmark,\nwhich includes 17 kinds of prevalent generative models, to evaluate the\neffectiveness of existing baselines and our approach. Our benchmark provides a\nleaderboard for follow-up studies. Extensive experimental results show that our\napproach outperforms state-of-the-art baselines by a significant margin. Our\nproject: https://fdmas.github.io/AIGCDetect\n", "link": "http://arxiv.org/abs/2311.12397v3", "date": "2024-03-07", "relevancy": 2.8275, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5863}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5706}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5396}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20PatchCraft%3A%20Exploring%20Texture%20Patch%20for%20Efficient%20AI-generated%20Image%0A%20%20Detection&body=Title%3A%20PatchCraft%3A%20Exploring%20Texture%20Patch%20for%20Efficient%20AI-generated%20Image%0A%20%20Detection%0AAuthor%3A%20Nan%20Zhong%20and%20Yiran%20Xu%20and%20Sheng%20Li%20and%20Zhenxing%20Qian%20and%20Xinpeng%20Zhang%0AAbstract%3A%20%20%20Recent%20generative%20models%20show%20impressive%20performance%20in%20generating%0Aphotographic%20images.%20Humans%20can%20hardly%20distinguish%20such%20incredibly%0Arealistic-looking%20AI-generated%20images%20from%20real%20ones.%20AI-generated%20images%20may%0Alead%20to%20ubiquitous%20disinformation%20dissemination.%20Therefore%2C%20it%20is%20of%20utmost%0Aurgency%20to%20develop%20a%20detector%20to%20identify%20AI%20generated%20images.%20Most%20existing%0Adetectors%20suffer%20from%20sharp%20performance%20drops%20over%20unseen%20generative%20models.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20AI-generated%20image%20detector%20capable%20of%0Aidentifying%20fake%20images%20created%20by%20a%20wide%20range%20of%20generative%20models.%20We%0Aobserve%20that%20the%20texture%20patches%20of%20images%20tend%20to%20reveal%20more%20traces%20left%20by%0Agenerative%20models%20compared%20to%20the%20global%20semantic%20information%20of%20the%20images.%20A%0Anovel%20Smash%26Reconstruction%20preprocessing%20is%20proposed%20to%20erase%20the%20global%0Asemantic%20information%20and%20enhance%20texture%20patches.%20Furthermore%2C%20pixels%20in%20rich%0Atexture%20regions%20exhibit%20more%20significant%20fluctuations%20than%20those%20in%20poor%0Atexture%20regions.%20Synthesizing%20realistic%20rich%20texture%20regions%20proves%20to%20be%20more%0Achallenging%20for%20existing%20generative%20models.%20Based%20on%20this%20principle%2C%20we%0Aleverage%20the%20inter-pixel%20correlation%20contrast%20between%20rich%20and%20poor%20texture%0Aregions%20within%20an%20image%20to%20further%20boost%20the%20detection%20performance.%0A%20%20In%20addition%2C%20we%20build%20a%20comprehensive%20AI-generated%20image%20detection%20benchmark%2C%0Awhich%20includes%2017%20kinds%20of%20prevalent%20generative%20models%2C%20to%20evaluate%20the%0Aeffectiveness%20of%20existing%20baselines%20and%20our%20approach.%20Our%20benchmark%20provides%20a%0Aleaderboard%20for%20follow-up%20studies.%20Extensive%20experimental%20results%20show%20that%20our%0Aapproach%20outperforms%20state-of-the-art%20baselines%20by%20a%20significant%20margin.%20Our%0Aproject%3A%20https%3A//fdmas.github.io/AIGCDetect%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12397v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PatchCraft%3A%20Exploring%20Texture%20Patch%20for%20Efficient%20AI-generated%20Image%0A%20%20Detection&entry.906535625=Nan%20Zhong%20and%20Yiran%20Xu%20and%20Sheng%20Li%20and%20Zhenxing%20Qian%20and%20Xinpeng%20Zhang&entry.1292438233=%20%20Recent%20generative%20models%20show%20impressive%20performance%20in%20generating%0Aphotographic%20images.%20Humans%20can%20hardly%20distinguish%20such%20incredibly%0Arealistic-looking%20AI-generated%20images%20from%20real%20ones.%20AI-generated%20images%20may%0Alead%20to%20ubiquitous%20disinformation%20dissemination.%20Therefore%2C%20it%20is%20of%20utmost%0Aurgency%20to%20develop%20a%20detector%20to%20identify%20AI%20generated%20images.%20Most%20existing%0Adetectors%20suffer%20from%20sharp%20performance%20drops%20over%20unseen%20generative%20models.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20AI-generated%20image%20detector%20capable%20of%0Aidentifying%20fake%20images%20created%20by%20a%20wide%20range%20of%20generative%20models.%20We%0Aobserve%20that%20the%20texture%20patches%20of%20images%20tend%20to%20reveal%20more%20traces%20left%20by%0Agenerative%20models%20compared%20to%20the%20global%20semantic%20information%20of%20the%20images.%20A%0Anovel%20Smash%26Reconstruction%20preprocessing%20is%20proposed%20to%20erase%20the%20global%0Asemantic%20information%20and%20enhance%20texture%20patches.%20Furthermore%2C%20pixels%20in%20rich%0Atexture%20regions%20exhibit%20more%20significant%20fluctuations%20than%20those%20in%20poor%0Atexture%20regions.%20Synthesizing%20realistic%20rich%20texture%20regions%20proves%20to%20be%20more%0Achallenging%20for%20existing%20generative%20models.%20Based%20on%20this%20principle%2C%20we%0Aleverage%20the%20inter-pixel%20correlation%20contrast%20between%20rich%20and%20poor%20texture%0Aregions%20within%20an%20image%20to%20further%20boost%20the%20detection%20performance.%0A%20%20In%20addition%2C%20we%20build%20a%20comprehensive%20AI-generated%20image%20detection%20benchmark%2C%0Awhich%20includes%2017%20kinds%20of%20prevalent%20generative%20models%2C%20to%20evaluate%20the%0Aeffectiveness%20of%20existing%20baselines%20and%20our%20approach.%20Our%20benchmark%20provides%20a%0Aleaderboard%20for%20follow-up%20studies.%20Extensive%20experimental%20results%20show%20that%20our%0Aapproach%20outperforms%20state-of-the-art%20baselines%20by%20a%20significant%20margin.%20Our%0Aproject%3A%20https%3A//fdmas.github.io/AIGCDetect%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12397v3&entry.124074799=Read"},
{"title": "Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like\n  Speed", "author": "Yifan Wang and Xingyi He and Sida Peng and Dongli Tan and Xiaowei Zhou", "abstract": "  We present a novel method for efficiently producing semi-dense matches across\nimages. Previous detector-free matcher LoFTR has shown remarkable matching\ncapability in handling large-viewpoint change and texture-poor scenarios but\nsuffers from low efficiency. We revisit its design choices and derive multiple\nimprovements for both efficiency and accuracy. One key observation is that\nperforming the transformer over the entire feature map is redundant due to\nshared local information, therefore we propose an aggregated attention\nmechanism with adaptive token selection for efficiency. Furthermore, we find\nspatial variance exists in LoFTR's fine correlation module, which is adverse to\nmatching accuracy. A novel two-stage correlation layer is proposed to achieve\naccurate subpixel correspondences for accuracy improvement. Our efficiency\noptimized model is $\\sim 2.5\\times$ faster than LoFTR which can even surpass\nstate-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue.\nMoreover, extensive experiments show that our method can achieve higher\naccuracy compared with competitive semi-dense matchers, with considerable\nefficiency benefits. This opens up exciting prospects for large-scale or\nlatency-sensitive applications such as image retrieval and 3D reconstruction.\nProject page: https://zju3dv.github.io/efficientloftr.\n", "link": "http://arxiv.org/abs/2403.04765v1", "date": "2024-03-07", "relevancy": 2.7729, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6033}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5243}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Efficient%20LoFTR%3A%20Semi-Dense%20Local%20Feature%20Matching%20with%20Sparse-Like%0A%20%20Speed&body=Title%3A%20Efficient%20LoFTR%3A%20Semi-Dense%20Local%20Feature%20Matching%20with%20Sparse-Like%0A%20%20Speed%0AAuthor%3A%20Yifan%20Wang%20and%20Xingyi%20He%20and%20Sida%20Peng%20and%20Dongli%20Tan%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20for%20efficiently%20producing%20semi-dense%20matches%20across%0Aimages.%20Previous%20detector-free%20matcher%20LoFTR%20has%20shown%20remarkable%20matching%0Acapability%20in%20handling%20large-viewpoint%20change%20and%20texture-poor%20scenarios%20but%0Asuffers%20from%20low%20efficiency.%20We%20revisit%20its%20design%20choices%20and%20derive%20multiple%0Aimprovements%20for%20both%20efficiency%20and%20accuracy.%20One%20key%20observation%20is%20that%0Aperforming%20the%20transformer%20over%20the%20entire%20feature%20map%20is%20redundant%20due%20to%0Ashared%20local%20information%2C%20therefore%20we%20propose%20an%20aggregated%20attention%0Amechanism%20with%20adaptive%20token%20selection%20for%20efficiency.%20Furthermore%2C%20we%20find%0Aspatial%20variance%20exists%20in%20LoFTR%27s%20fine%20correlation%20module%2C%20which%20is%20adverse%20to%0Amatching%20accuracy.%20A%20novel%20two-stage%20correlation%20layer%20is%20proposed%20to%20achieve%0Aaccurate%20subpixel%20correspondences%20for%20accuracy%20improvement.%20Our%20efficiency%0Aoptimized%20model%20is%20%24%5Csim%202.5%5Ctimes%24%20faster%20than%20LoFTR%20which%20can%20even%20surpass%0Astate-of-the-art%20efficient%20sparse%20matching%20pipeline%20SuperPoint%20%2B%20LightGlue.%0AMoreover%2C%20extensive%20experiments%20show%20that%20our%20method%20can%20achieve%20higher%0Aaccuracy%20compared%20with%20competitive%20semi-dense%20matchers%2C%20with%20considerable%0Aefficiency%20benefits.%20This%20opens%20up%20exciting%20prospects%20for%20large-scale%20or%0Alatency-sensitive%20applications%20such%20as%20image%20retrieval%20and%203D%20reconstruction.%0AProject%20page%3A%20https%3A//zju3dv.github.io/efficientloftr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04765v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20LoFTR%3A%20Semi-Dense%20Local%20Feature%20Matching%20with%20Sparse-Like%0A%20%20Speed&entry.906535625=Yifan%20Wang%20and%20Xingyi%20He%20and%20Sida%20Peng%20and%20Dongli%20Tan%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20We%20present%20a%20novel%20method%20for%20efficiently%20producing%20semi-dense%20matches%20across%0Aimages.%20Previous%20detector-free%20matcher%20LoFTR%20has%20shown%20remarkable%20matching%0Acapability%20in%20handling%20large-viewpoint%20change%20and%20texture-poor%20scenarios%20but%0Asuffers%20from%20low%20efficiency.%20We%20revisit%20its%20design%20choices%20and%20derive%20multiple%0Aimprovements%20for%20both%20efficiency%20and%20accuracy.%20One%20key%20observation%20is%20that%0Aperforming%20the%20transformer%20over%20the%20entire%20feature%20map%20is%20redundant%20due%20to%0Ashared%20local%20information%2C%20therefore%20we%20propose%20an%20aggregated%20attention%0Amechanism%20with%20adaptive%20token%20selection%20for%20efficiency.%20Furthermore%2C%20we%20find%0Aspatial%20variance%20exists%20in%20LoFTR%27s%20fine%20correlation%20module%2C%20which%20is%20adverse%20to%0Amatching%20accuracy.%20A%20novel%20two-stage%20correlation%20layer%20is%20proposed%20to%20achieve%0Aaccurate%20subpixel%20correspondences%20for%20accuracy%20improvement.%20Our%20efficiency%0Aoptimized%20model%20is%20%24%5Csim%202.5%5Ctimes%24%20faster%20than%20LoFTR%20which%20can%20even%20surpass%0Astate-of-the-art%20efficient%20sparse%20matching%20pipeline%20SuperPoint%20%2B%20LightGlue.%0AMoreover%2C%20extensive%20experiments%20show%20that%20our%20method%20can%20achieve%20higher%0Aaccuracy%20compared%20with%20competitive%20semi-dense%20matchers%2C%20with%20considerable%0Aefficiency%20benefits.%20This%20opens%20up%20exciting%20prospects%20for%20large-scale%20or%0Alatency-sensitive%20applications%20such%20as%20image%20retrieval%20and%203D%20reconstruction.%0AProject%20page%3A%20https%3A//zju3dv.github.io/efficientloftr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04765v1&entry.124074799=Read"},
{"title": "Self-supervised Trajectory Representation Learning with Temporal\n  Regularities and Travel Semantics", "author": "Jiawei Jiang and Dayan Pan and Houxing Ren and Xiaohan Jiang and Chao Li and Jingyuan Wang", "abstract": "  Trajectory Representation Learning (TRL) is a powerful tool for\nspatial-temporal data analysis and management. TRL aims to convert complicated\nraw trajectories into low-dimensional representation vectors, which can be\napplied to various downstream tasks, such as trajectory classification,\nclustering, and similarity computation. Existing TRL works usually treat\ntrajectories as ordinary sequence data, while some important spatial-temporal\ncharacteristics, such as temporal regularities and travel semantics, are not\nfully exploited. To fill this gap, we propose a novel Self-supervised\ntrajectory representation learning framework with TemporAl Regularities and\nTravel semantics, namely START. The proposed method consists of two stages. The\nfirst stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT),\nwhich converts the road network features and travel semantics into\nrepresentation vectors of road segments. The second stage is a Time-Aware\nTrajectory Encoder (TAT-Enc), which encodes representation vectors of road\nsegments in the same trajectory as a trajectory representation vector,\nmeanwhile incorporating temporal regularities with the trajectory\nrepresentation. Moreover, we also design two self-supervised tasks, i.e.,\nspan-masked trajectory recovery and trajectory contrastive learning, to\nintroduce spatial-temporal characteristics of trajectories into the training\nprocess of our START framework. The effectiveness of the proposed method is\nverified by extensive experiments on two large-scale real-world datasets for\nthree downstream tasks. The experiments also demonstrate that our method can be\ntransferred across different cities to adapt heterogeneous trajectory datasets.\n", "link": "http://arxiv.org/abs/2211.09510v4", "date": "2024-03-07", "relevancy": 2.7722, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5585}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5554}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5494}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Trajectory%20Representation%20Learning%20with%20Temporal%0A%20%20Regularities%20and%20Travel%20Semantics&body=Title%3A%20Self-supervised%20Trajectory%20Representation%20Learning%20with%20Temporal%0A%20%20Regularities%20and%20Travel%20Semantics%0AAuthor%3A%20Jiawei%20Jiang%20and%20Dayan%20Pan%20and%20Houxing%20Ren%20and%20Xiaohan%20Jiang%20and%20Chao%20Li%20and%20Jingyuan%20Wang%0AAbstract%3A%20%20%20Trajectory%20Representation%20Learning%20%28TRL%29%20is%20a%20powerful%20tool%20for%0Aspatial-temporal%20data%20analysis%20and%20management.%20TRL%20aims%20to%20convert%20complicated%0Araw%20trajectories%20into%20low-dimensional%20representation%20vectors%2C%20which%20can%20be%0Aapplied%20to%20various%20downstream%20tasks%2C%20such%20as%20trajectory%20classification%2C%0Aclustering%2C%20and%20similarity%20computation.%20Existing%20TRL%20works%20usually%20treat%0Atrajectories%20as%20ordinary%20sequence%20data%2C%20while%20some%20important%20spatial-temporal%0Acharacteristics%2C%20such%20as%20temporal%20regularities%20and%20travel%20semantics%2C%20are%20not%0Afully%20exploited.%20To%20fill%20this%20gap%2C%20we%20propose%20a%20novel%20Self-supervised%0Atrajectory%20representation%20learning%20framework%20with%20TemporAl%20Regularities%20and%0ATravel%20semantics%2C%20namely%20START.%20The%20proposed%20method%20consists%20of%20two%20stages.%20The%0Afirst%20stage%20is%20a%20Trajectory%20Pattern-Enhanced%20Graph%20Attention%20Network%20%28TPE-GAT%29%2C%0Awhich%20converts%20the%20road%20network%20features%20and%20travel%20semantics%20into%0Arepresentation%20vectors%20of%20road%20segments.%20The%20second%20stage%20is%20a%20Time-Aware%0ATrajectory%20Encoder%20%28TAT-Enc%29%2C%20which%20encodes%20representation%20vectors%20of%20road%0Asegments%20in%20the%20same%20trajectory%20as%20a%20trajectory%20representation%20vector%2C%0Ameanwhile%20incorporating%20temporal%20regularities%20with%20the%20trajectory%0Arepresentation.%20Moreover%2C%20we%20also%20design%20two%20self-supervised%20tasks%2C%20i.e.%2C%0Aspan-masked%20trajectory%20recovery%20and%20trajectory%20contrastive%20learning%2C%20to%0Aintroduce%20spatial-temporal%20characteristics%20of%20trajectories%20into%20the%20training%0Aprocess%20of%20our%20START%20framework.%20The%20effectiveness%20of%20the%20proposed%20method%20is%0Averified%20by%20extensive%20experiments%20on%20two%20large-scale%20real-world%20datasets%20for%0Athree%20downstream%20tasks.%20The%20experiments%20also%20demonstrate%20that%20our%20method%20can%20be%0Atransferred%20across%20different%20cities%20to%20adapt%20heterogeneous%20trajectory%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.09510v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Trajectory%20Representation%20Learning%20with%20Temporal%0A%20%20Regularities%20and%20Travel%20Semantics&entry.906535625=Jiawei%20Jiang%20and%20Dayan%20Pan%20and%20Houxing%20Ren%20and%20Xiaohan%20Jiang%20and%20Chao%20Li%20and%20Jingyuan%20Wang&entry.1292438233=%20%20Trajectory%20Representation%20Learning%20%28TRL%29%20is%20a%20powerful%20tool%20for%0Aspatial-temporal%20data%20analysis%20and%20management.%20TRL%20aims%20to%20convert%20complicated%0Araw%20trajectories%20into%20low-dimensional%20representation%20vectors%2C%20which%20can%20be%0Aapplied%20to%20various%20downstream%20tasks%2C%20such%20as%20trajectory%20classification%2C%0Aclustering%2C%20and%20similarity%20computation.%20Existing%20TRL%20works%20usually%20treat%0Atrajectories%20as%20ordinary%20sequence%20data%2C%20while%20some%20important%20spatial-temporal%0Acharacteristics%2C%20such%20as%20temporal%20regularities%20and%20travel%20semantics%2C%20are%20not%0Afully%20exploited.%20To%20fill%20this%20gap%2C%20we%20propose%20a%20novel%20Self-supervised%0Atrajectory%20representation%20learning%20framework%20with%20TemporAl%20Regularities%20and%0ATravel%20semantics%2C%20namely%20START.%20The%20proposed%20method%20consists%20of%20two%20stages.%20The%0Afirst%20stage%20is%20a%20Trajectory%20Pattern-Enhanced%20Graph%20Attention%20Network%20%28TPE-GAT%29%2C%0Awhich%20converts%20the%20road%20network%20features%20and%20travel%20semantics%20into%0Arepresentation%20vectors%20of%20road%20segments.%20The%20second%20stage%20is%20a%20Time-Aware%0ATrajectory%20Encoder%20%28TAT-Enc%29%2C%20which%20encodes%20representation%20vectors%20of%20road%0Asegments%20in%20the%20same%20trajectory%20as%20a%20trajectory%20representation%20vector%2C%0Ameanwhile%20incorporating%20temporal%20regularities%20with%20the%20trajectory%0Arepresentation.%20Moreover%2C%20we%20also%20design%20two%20self-supervised%20tasks%2C%20i.e.%2C%0Aspan-masked%20trajectory%20recovery%20and%20trajectory%20contrastive%20learning%2C%20to%0Aintroduce%20spatial-temporal%20characteristics%20of%20trajectories%20into%20the%20training%0Aprocess%20of%20our%20START%20framework.%20The%20effectiveness%20of%20the%20proposed%20method%20is%0Averified%20by%20extensive%20experiments%20on%20two%20large-scale%20real-world%20datasets%20for%0Athree%20downstream%20tasks.%20The%20experiments%20also%20demonstrate%20that%20our%20method%20can%20be%0Atransferred%20across%20different%20cities%20to%20adapt%20heterogeneous%20trajectory%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.09510v4&entry.124074799=Read"},
{"title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "author": "Jun Zhan and Junqi Dai and Jiasheng Ye and Yunhua Zhou and Dong Zhang and Zhigeng Liu and Xin Zhang and Ruibin Yuan and Ge Zhang and Linyang Li and Hang Yan and Jie Fu and Tao Gui and Tianxiang Sun and Yugang Jiang and Xipeng Qiu", "abstract": "  We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/\n", "link": "http://arxiv.org/abs/2402.12226v3", "date": "2024-03-07", "relevancy": 2.7164, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5361}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5227}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20AnyGPT%3A%20Unified%20Multimodal%20LLM%20with%20Discrete%20Sequence%20Modeling&body=Title%3A%20AnyGPT%3A%20Unified%20Multimodal%20LLM%20with%20Discrete%20Sequence%20Modeling%0AAuthor%3A%20Jun%20Zhan%20and%20Junqi%20Dai%20and%20Jiasheng%20Ye%20and%20Yunhua%20Zhou%20and%20Dong%20Zhang%20and%20Zhigeng%20Liu%20and%20Xin%20Zhang%20and%20Ruibin%20Yuan%20and%20Ge%20Zhang%20and%20Linyang%20Li%20and%20Hang%20Yan%20and%20Jie%20Fu%20and%20Tao%20Gui%20and%20Tianxiang%20Sun%20and%20Yugang%20Jiang%20and%20Xipeng%20Qiu%0AAbstract%3A%20%20%20We%20introduce%20AnyGPT%2C%20an%20any-to-any%20multimodal%20language%20model%20that%20utilizes%0Adiscrete%20representations%20for%20the%20unified%20processing%20of%20various%20modalities%2C%0Aincluding%20speech%2C%20text%2C%20images%2C%20and%20music.%20AnyGPT%20can%20be%20trained%20stably%20without%0Aany%20alterations%20to%20the%20current%20large%20language%20model%20%28LLM%29%20architecture%20or%0Atraining%20paradigms.%20Instead%2C%20it%20relies%20exclusively%20on%20data-level%20preprocessing%2C%0Afacilitating%20the%20seamless%20integration%20of%20new%20modalities%20into%20LLMs%2C%20akin%20to%20the%0Aincorporation%20of%20new%20languages.%20We%20build%20a%20multimodal%20text-centric%20dataset%20for%0Amultimodal%20alignment%20pre-training.%20Utilizing%20generative%20models%2C%20we%20synthesize%0Athe%20first%20large-scale%20any-to-any%20multimodal%20instruction%20dataset.%20It%20consists%20of%0A108k%20samples%20of%20multi-turn%20conversations%20that%20intricately%20interweave%20various%0Amodalities%2C%20thus%20equipping%20the%20model%20to%20handle%20arbitrary%20combinations%20of%0Amultimodal%20inputs%20and%20outputs.%20Experimental%20results%20demonstrate%20that%20AnyGPT%20is%0Acapable%20of%20facilitating%20any-to-any%20multimodal%20conversation%20while%20achieving%0Aperformance%20comparable%20to%20specialized%20models%20across%20all%20modalities%2C%20proving%0Athat%20discrete%20representations%20can%20effectively%20and%20conveniently%20unify%20multiple%0Amodalities%20within%20a%20language%20model.%20Demos%20are%20shown%20in%0Ahttps%3A//junzhan2000.github.io/AnyGPT.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12226v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyGPT%3A%20Unified%20Multimodal%20LLM%20with%20Discrete%20Sequence%20Modeling&entry.906535625=Jun%20Zhan%20and%20Junqi%20Dai%20and%20Jiasheng%20Ye%20and%20Yunhua%20Zhou%20and%20Dong%20Zhang%20and%20Zhigeng%20Liu%20and%20Xin%20Zhang%20and%20Ruibin%20Yuan%20and%20Ge%20Zhang%20and%20Linyang%20Li%20and%20Hang%20Yan%20and%20Jie%20Fu%20and%20Tao%20Gui%20and%20Tianxiang%20Sun%20and%20Yugang%20Jiang%20and%20Xipeng%20Qiu&entry.1292438233=%20%20We%20introduce%20AnyGPT%2C%20an%20any-to-any%20multimodal%20language%20model%20that%20utilizes%0Adiscrete%20representations%20for%20the%20unified%20processing%20of%20various%20modalities%2C%0Aincluding%20speech%2C%20text%2C%20images%2C%20and%20music.%20AnyGPT%20can%20be%20trained%20stably%20without%0Aany%20alterations%20to%20the%20current%20large%20language%20model%20%28LLM%29%20architecture%20or%0Atraining%20paradigms.%20Instead%2C%20it%20relies%20exclusively%20on%20data-level%20preprocessing%2C%0Afacilitating%20the%20seamless%20integration%20of%20new%20modalities%20into%20LLMs%2C%20akin%20to%20the%0Aincorporation%20of%20new%20languages.%20We%20build%20a%20multimodal%20text-centric%20dataset%20for%0Amultimodal%20alignment%20pre-training.%20Utilizing%20generative%20models%2C%20we%20synthesize%0Athe%20first%20large-scale%20any-to-any%20multimodal%20instruction%20dataset.%20It%20consists%20of%0A108k%20samples%20of%20multi-turn%20conversations%20that%20intricately%20interweave%20various%0Amodalities%2C%20thus%20equipping%20the%20model%20to%20handle%20arbitrary%20combinations%20of%0Amultimodal%20inputs%20and%20outputs.%20Experimental%20results%20demonstrate%20that%20AnyGPT%20is%0Acapable%20of%20facilitating%20any-to-any%20multimodal%20conversation%20while%20achieving%0Aperformance%20comparable%20to%20specialized%20models%20across%20all%20modalities%2C%20proving%0Athat%20discrete%20representations%20can%20effectively%20and%20conveniently%20unify%20multiple%0Amodalities%20within%20a%20language%20model.%20Demos%20are%20shown%20in%0Ahttps%3A//junzhan2000.github.io/AnyGPT.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12226v3&entry.124074799=Read"},
{"title": "On-demand Quantization for Green Federated Generative Diffusion in\n  Mobile Edge Networks", "author": "Bingkun Lai and Jiayi He and Jiawen Kang and Gaolei Li and Minrui Xu and Tao zhang and Shengli Xie", "abstract": "  Generative Artificial Intelligence (GAI) shows remarkable productivity and\ncreativity in Mobile Edge Networks, such as the metaverse and the Industrial\nInternet of Things. Federated learning is a promising technique for effectively\ntraining GAI models in mobile edge networks due to its data distribution.\nHowever, there is a notable issue with communication consumption when training\nlarge GAI models like generative diffusion models in mobile edge networks.\nAdditionally, the substantial energy consumption associated with training\ndiffusion-based models, along with the limited resources of edge devices and\ncomplexities of network environments, pose challenges for improving the\ntraining efficiency of GAI models. To address this challenge, we propose an\non-demand quantized energy-efficient federated diffusion approach for mobile\nedge networks. Specifically, we first design a dynamic quantized federated\ndiffusion training scheme considering various demands from the edge devices.\nThen, we study an energy efficiency problem based on specific quantization\nrequirements. Numerical results show that our proposed method significantly\nreduces system energy consumption and transmitted model size compared to both\nbaseline federated diffusion and fixed quantized federated diffusion methods\nwhile effectively maintaining reasonable quality and diversity of generated\ndata.\n", "link": "http://arxiv.org/abs/2403.04430v1", "date": "2024-03-07", "relevancy": 2.7094, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5442}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.541}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5405}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20On-demand%20Quantization%20for%20Green%20Federated%20Generative%20Diffusion%20in%0A%20%20Mobile%20Edge%20Networks&body=Title%3A%20On-demand%20Quantization%20for%20Green%20Federated%20Generative%20Diffusion%20in%0A%20%20Mobile%20Edge%20Networks%0AAuthor%3A%20Bingkun%20Lai%20and%20Jiayi%20He%20and%20Jiawen%20Kang%20and%20Gaolei%20Li%20and%20Minrui%20Xu%20and%20Tao%20zhang%20and%20Shengli%20Xie%0AAbstract%3A%20%20%20Generative%20Artificial%20Intelligence%20%28GAI%29%20shows%20remarkable%20productivity%20and%0Acreativity%20in%20Mobile%20Edge%20Networks%2C%20such%20as%20the%20metaverse%20and%20the%20Industrial%0AInternet%20of%20Things.%20Federated%20learning%20is%20a%20promising%20technique%20for%20effectively%0Atraining%20GAI%20models%20in%20mobile%20edge%20networks%20due%20to%20its%20data%20distribution.%0AHowever%2C%20there%20is%20a%20notable%20issue%20with%20communication%20consumption%20when%20training%0Alarge%20GAI%20models%20like%20generative%20diffusion%20models%20in%20mobile%20edge%20networks.%0AAdditionally%2C%20the%20substantial%20energy%20consumption%20associated%20with%20training%0Adiffusion-based%20models%2C%20along%20with%20the%20limited%20resources%20of%20edge%20devices%20and%0Acomplexities%20of%20network%20environments%2C%20pose%20challenges%20for%20improving%20the%0Atraining%20efficiency%20of%20GAI%20models.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0Aon-demand%20quantized%20energy-efficient%20federated%20diffusion%20approach%20for%20mobile%0Aedge%20networks.%20Specifically%2C%20we%20first%20design%20a%20dynamic%20quantized%20federated%0Adiffusion%20training%20scheme%20considering%20various%20demands%20from%20the%20edge%20devices.%0AThen%2C%20we%20study%20an%20energy%20efficiency%20problem%20based%20on%20specific%20quantization%0Arequirements.%20Numerical%20results%20show%20that%20our%20proposed%20method%20significantly%0Areduces%20system%20energy%20consumption%20and%20transmitted%20model%20size%20compared%20to%20both%0Abaseline%20federated%20diffusion%20and%20fixed%20quantized%20federated%20diffusion%20methods%0Awhile%20effectively%20maintaining%20reasonable%20quality%20and%20diversity%20of%20generated%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04430v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-demand%20Quantization%20for%20Green%20Federated%20Generative%20Diffusion%20in%0A%20%20Mobile%20Edge%20Networks&entry.906535625=Bingkun%20Lai%20and%20Jiayi%20He%20and%20Jiawen%20Kang%20and%20Gaolei%20Li%20and%20Minrui%20Xu%20and%20Tao%20zhang%20and%20Shengli%20Xie&entry.1292438233=%20%20Generative%20Artificial%20Intelligence%20%28GAI%29%20shows%20remarkable%20productivity%20and%0Acreativity%20in%20Mobile%20Edge%20Networks%2C%20such%20as%20the%20metaverse%20and%20the%20Industrial%0AInternet%20of%20Things.%20Federated%20learning%20is%20a%20promising%20technique%20for%20effectively%0Atraining%20GAI%20models%20in%20mobile%20edge%20networks%20due%20to%20its%20data%20distribution.%0AHowever%2C%20there%20is%20a%20notable%20issue%20with%20communication%20consumption%20when%20training%0Alarge%20GAI%20models%20like%20generative%20diffusion%20models%20in%20mobile%20edge%20networks.%0AAdditionally%2C%20the%20substantial%20energy%20consumption%20associated%20with%20training%0Adiffusion-based%20models%2C%20along%20with%20the%20limited%20resources%20of%20edge%20devices%20and%0Acomplexities%20of%20network%20environments%2C%20pose%20challenges%20for%20improving%20the%0Atraining%20efficiency%20of%20GAI%20models.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0Aon-demand%20quantized%20energy-efficient%20federated%20diffusion%20approach%20for%20mobile%0Aedge%20networks.%20Specifically%2C%20we%20first%20design%20a%20dynamic%20quantized%20federated%0Adiffusion%20training%20scheme%20considering%20various%20demands%20from%20the%20edge%20devices.%0AThen%2C%20we%20study%20an%20energy%20efficiency%20problem%20based%20on%20specific%20quantization%0Arequirements.%20Numerical%20results%20show%20that%20our%20proposed%20method%20significantly%0Areduces%20system%20energy%20consumption%20and%20transmitted%20model%20size%20compared%20to%20both%0Abaseline%20federated%20diffusion%20and%20fixed%20quantized%20federated%20diffusion%20methods%0Awhile%20effectively%20maintaining%20reasonable%20quality%20and%20diversity%20of%20generated%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04430v1&entry.124074799=Read"},
{"title": "Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with\n  Sparse Views", "author": "Jiawei Yao and Chen Wang and Tong Wu and Chuming Li", "abstract": "  In this paper, we propose a novel method for 3D scene and object\nreconstruction from sparse multi-view images. Different from previous methods\nthat leverage extra information such as depth or generalizable features across\nscenes, our approach leverages the scene properties embedded in the multi-view\ninputs to create precise pseudo-labels for optimization without any prior\ntraining. Specifically, we introduce a geometry-guided approach that improves\nsurface reconstruction accuracy from sparse views by leveraging spherical\nharmonics to predict the novel radiance while holistically considering all\ncolor observations for a point in the scene. Also, our pipeline exploits proxy\ngeometry and correctly handles the occlusion in generating the pseudo-labels of\nradiance, which previous image-warping methods fail to avoid. Our method,\ndubbed Ray Augmentation (RayAug), achieves superior results on DTU and Blender\ndatasets without requiring prior training, demonstrating its effectiveness in\naddressing the problem of sparse view reconstruction. Our pipeline is flexible\nand can be integrated into other implicit neural reconstruction methods for\nsparse views.\n", "link": "http://arxiv.org/abs/2310.05483v4", "date": "2024-03-07", "relevancy": 2.6398, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5313}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5269}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5257}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Geometry-Guided%20Ray%20Augmentation%20for%20Neural%20Surface%20Reconstruction%20with%0A%20%20Sparse%20Views&body=Title%3A%20Geometry-Guided%20Ray%20Augmentation%20for%20Neural%20Surface%20Reconstruction%20with%0A%20%20Sparse%20Views%0AAuthor%3A%20Jiawei%20Yao%20and%20Chen%20Wang%20and%20Tong%20Wu%20and%20Chuming%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%203D%20scene%20and%20object%0Areconstruction%20from%20sparse%20multi-view%20images.%20Different%20from%20previous%20methods%0Athat%20leverage%20extra%20information%20such%20as%20depth%20or%20generalizable%20features%20across%0Ascenes%2C%20our%20approach%20leverages%20the%20scene%20properties%20embedded%20in%20the%20multi-view%0Ainputs%20to%20create%20precise%20pseudo-labels%20for%20optimization%20without%20any%20prior%0Atraining.%20Specifically%2C%20we%20introduce%20a%20geometry-guided%20approach%20that%20improves%0Asurface%20reconstruction%20accuracy%20from%20sparse%20views%20by%20leveraging%20spherical%0Aharmonics%20to%20predict%20the%20novel%20radiance%20while%20holistically%20considering%20all%0Acolor%20observations%20for%20a%20point%20in%20the%20scene.%20Also%2C%20our%20pipeline%20exploits%20proxy%0Ageometry%20and%20correctly%20handles%20the%20occlusion%20in%20generating%20the%20pseudo-labels%20of%0Aradiance%2C%20which%20previous%20image-warping%20methods%20fail%20to%20avoid.%20Our%20method%2C%0Adubbed%20Ray%20Augmentation%20%28RayAug%29%2C%20achieves%20superior%20results%20on%20DTU%20and%20Blender%0Adatasets%20without%20requiring%20prior%20training%2C%20demonstrating%20its%20effectiveness%20in%0Aaddressing%20the%20problem%20of%20sparse%20view%20reconstruction.%20Our%20pipeline%20is%20flexible%0Aand%20can%20be%20integrated%20into%20other%20implicit%20neural%20reconstruction%20methods%20for%0Asparse%20views.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05483v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Guided%20Ray%20Augmentation%20for%20Neural%20Surface%20Reconstruction%20with%0A%20%20Sparse%20Views&entry.906535625=Jiawei%20Yao%20and%20Chen%20Wang%20and%20Tong%20Wu%20and%20Chuming%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%203D%20scene%20and%20object%0Areconstruction%20from%20sparse%20multi-view%20images.%20Different%20from%20previous%20methods%0Athat%20leverage%20extra%20information%20such%20as%20depth%20or%20generalizable%20features%20across%0Ascenes%2C%20our%20approach%20leverages%20the%20scene%20properties%20embedded%20in%20the%20multi-view%0Ainputs%20to%20create%20precise%20pseudo-labels%20for%20optimization%20without%20any%20prior%0Atraining.%20Specifically%2C%20we%20introduce%20a%20geometry-guided%20approach%20that%20improves%0Asurface%20reconstruction%20accuracy%20from%20sparse%20views%20by%20leveraging%20spherical%0Aharmonics%20to%20predict%20the%20novel%20radiance%20while%20holistically%20considering%20all%0Acolor%20observations%20for%20a%20point%20in%20the%20scene.%20Also%2C%20our%20pipeline%20exploits%20proxy%0Ageometry%20and%20correctly%20handles%20the%20occlusion%20in%20generating%20the%20pseudo-labels%20of%0Aradiance%2C%20which%20previous%20image-warping%20methods%20fail%20to%20avoid.%20Our%20method%2C%0Adubbed%20Ray%20Augmentation%20%28RayAug%29%2C%20achieves%20superior%20results%20on%20DTU%20and%20Blender%0Adatasets%20without%20requiring%20prior%20training%2C%20demonstrating%20its%20effectiveness%20in%0Aaddressing%20the%20problem%20of%20sparse%20view%20reconstruction.%20Our%20pipeline%20is%20flexible%0Aand%20can%20be%20integrated%20into%20other%20implicit%20neural%20reconstruction%20methods%20for%0Asparse%20views.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05483v4&entry.124074799=Read"},
{"title": "GenTKG: Generative Forecasting on Temporal Knowledge Graph", "author": "Ruotong Liao and Xu Jia and Yunpu Ma and Yangzhe Li and Volker Tresp", "abstract": "  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs.\n", "link": "http://arxiv.org/abs/2310.07793v3", "date": "2024-03-07", "relevancy": 2.628, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5564}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5267}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4937}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20GenTKG%3A%20Generative%20Forecasting%20on%20Temporal%20Knowledge%20Graph&body=Title%3A%20GenTKG%3A%20Generative%20Forecasting%20on%20Temporal%20Knowledge%20Graph%0AAuthor%3A%20Ruotong%20Liao%20and%20Xu%20Jia%20and%20Yunpu%20Ma%20and%20Yangzhe%20Li%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20The%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20ignited%20interest%0Ain%20the%20temporal%20knowledge%20graph%20%28tKG%29%20domain%2C%20where%20conventional%0Aembedding-based%20and%20rule-based%20methods%20dominate.%20The%20question%20remains%20open%20of%0Awhether%20pre-trained%20LLMs%20can%20understand%20structured%20temporal%20relational%20data%20and%0Areplace%20them%20as%20the%20foundation%20model%20for%20temporal%20relational%20forecasting.%0ATherefore%2C%20we%20bring%20temporal%20knowledge%20forecasting%20into%20the%20generative%20setting.%0AHowever%2C%20challenges%20occur%20in%20the%20huge%20chasms%20between%20complex%20temporal%20graph%0Adata%20structure%20and%20sequential%20natural%20expressions%20LLMs%20can%20handle%2C%20and%20between%0Athe%20enormous%20data%20sizes%20of%20tKGs%20and%20heavy%20computation%20costs%20of%20finetuning%20LLMs.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20retrieval-augmented%20generation%0Aframework%20named%20GenTKG%20combining%20a%20temporal%20logical%20rule-based%20retrieval%0Astrategy%20and%20few-shot%20parameter-efficient%20instruction%20tuning%20to%20solve%20the%20above%0Achallenges%2C%20respectively.%20Extensive%20experiments%20have%20shown%20that%20GenTKG%0Aoutperforms%20conventional%20methods%20of%20temporal%20relational%20forecasting%20with%20low%0Acomputation%20resources%20using%20extremely%20limited%20training%20data%20as%20few%20as%2016%0Asamples.%20GenTKG%20also%20highlights%20remarkable%20cross-domain%20generalizability%20with%0Aoutperforming%20performance%20on%20unseen%20datasets%20without%20re-training%2C%20and%20in-domain%0Ageneralizability%20regardless%20of%20time%20split%20in%20the%20same%20dataset.%20Our%20work%20reveals%0Athe%20huge%20potential%20of%20LLMs%20in%20the%20tKG%20domain%20and%20opens%20a%20new%20frontier%20for%0Agenerative%20forecasting%20on%20tKGs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07793v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenTKG%3A%20Generative%20Forecasting%20on%20Temporal%20Knowledge%20Graph&entry.906535625=Ruotong%20Liao%20and%20Xu%20Jia%20and%20Yunpu%20Ma%20and%20Yangzhe%20Li%20and%20Volker%20Tresp&entry.1292438233=%20%20The%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20ignited%20interest%0Ain%20the%20temporal%20knowledge%20graph%20%28tKG%29%20domain%2C%20where%20conventional%0Aembedding-based%20and%20rule-based%20methods%20dominate.%20The%20question%20remains%20open%20of%0Awhether%20pre-trained%20LLMs%20can%20understand%20structured%20temporal%20relational%20data%20and%0Areplace%20them%20as%20the%20foundation%20model%20for%20temporal%20relational%20forecasting.%0ATherefore%2C%20we%20bring%20temporal%20knowledge%20forecasting%20into%20the%20generative%20setting.%0AHowever%2C%20challenges%20occur%20in%20the%20huge%20chasms%20between%20complex%20temporal%20graph%0Adata%20structure%20and%20sequential%20natural%20expressions%20LLMs%20can%20handle%2C%20and%20between%0Athe%20enormous%20data%20sizes%20of%20tKGs%20and%20heavy%20computation%20costs%20of%20finetuning%20LLMs.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20retrieval-augmented%20generation%0Aframework%20named%20GenTKG%20combining%20a%20temporal%20logical%20rule-based%20retrieval%0Astrategy%20and%20few-shot%20parameter-efficient%20instruction%20tuning%20to%20solve%20the%20above%0Achallenges%2C%20respectively.%20Extensive%20experiments%20have%20shown%20that%20GenTKG%0Aoutperforms%20conventional%20methods%20of%20temporal%20relational%20forecasting%20with%20low%0Acomputation%20resources%20using%20extremely%20limited%20training%20data%20as%20few%20as%2016%0Asamples.%20GenTKG%20also%20highlights%20remarkable%20cross-domain%20generalizability%20with%0Aoutperforming%20performance%20on%20unseen%20datasets%20without%20re-training%2C%20and%20in-domain%0Ageneralizability%20regardless%20of%20time%20split%20in%20the%20same%20dataset.%20Our%20work%20reveals%0Athe%20huge%20potential%20of%20LLMs%20in%20the%20tKG%20domain%20and%20opens%20a%20new%20frontier%20for%0Agenerative%20forecasting%20on%20tKGs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07793v3&entry.124074799=Read"},
{"title": "Discriminative Sample-Guided and Parameter-Efficient Feature Space\n  Adaptation for Cross-Domain Few-Shot Learning", "author": "Rashindrie Perera and Saman Halgamuge", "abstract": "  In this paper, we look at cross-domain few-shot classification which presents\nthe challenging task of learning new classes in unseen domains with few\nlabelled examples. Existing methods, though somewhat effective, encounter\nseveral limitations, which we address in this work through two significant\nimprovements. First, to address overfitting associated with fine-tuning a large\nnumber of parameters on small datasets, we introduce a lightweight\nparameter-efficient adaptation strategy. This strategy employs a linear\ntransformation of pre-trained features, significantly reducing the trainable\nparameter count. Second, we replace the traditional nearest centroid classifier\nwith a variance-aware loss function, enhancing the model's sensitivity to the\ninter- and intra-class variances within the training set for improved\nclustering in feature space. Empirical evaluations on the Meta-Dataset\nbenchmark showcase that our approach not only improves accuracy up to 7.7% and\n5.3% on seen and unseen datasets respectively but also achieves this\nperformance while being at least ~3x more parameter-efficient than existing\nmethods, establishing a new state-of-the-art in cross-domain few-shot learning.\nOur code can be found at https://github.com/rashindrie/DIPA.\n", "link": "http://arxiv.org/abs/2403.04492v1", "date": "2024-03-07", "relevancy": 2.6274, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5397}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5239}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5128}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Discriminative%20Sample-Guided%20and%20Parameter-Efficient%20Feature%20Space%0A%20%20Adaptation%20for%20Cross-Domain%20Few-Shot%20Learning&body=Title%3A%20Discriminative%20Sample-Guided%20and%20Parameter-Efficient%20Feature%20Space%0A%20%20Adaptation%20for%20Cross-Domain%20Few-Shot%20Learning%0AAuthor%3A%20Rashindrie%20Perera%20and%20Saman%20Halgamuge%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20look%20at%20cross-domain%20few-shot%20classification%20which%20presents%0Athe%20challenging%20task%20of%20learning%20new%20classes%20in%20unseen%20domains%20with%20few%0Alabelled%20examples.%20Existing%20methods%2C%20though%20somewhat%20effective%2C%20encounter%0Aseveral%20limitations%2C%20which%20we%20address%20in%20this%20work%20through%20two%20significant%0Aimprovements.%20First%2C%20to%20address%20overfitting%20associated%20with%20fine-tuning%20a%20large%0Anumber%20of%20parameters%20on%20small%20datasets%2C%20we%20introduce%20a%20lightweight%0Aparameter-efficient%20adaptation%20strategy.%20This%20strategy%20employs%20a%20linear%0Atransformation%20of%20pre-trained%20features%2C%20significantly%20reducing%20the%20trainable%0Aparameter%20count.%20Second%2C%20we%20replace%20the%20traditional%20nearest%20centroid%20classifier%0Awith%20a%20variance-aware%20loss%20function%2C%20enhancing%20the%20model%27s%20sensitivity%20to%20the%0Ainter-%20and%20intra-class%20variances%20within%20the%20training%20set%20for%20improved%0Aclustering%20in%20feature%20space.%20Empirical%20evaluations%20on%20the%20Meta-Dataset%0Abenchmark%20showcase%20that%20our%20approach%20not%20only%20improves%20accuracy%20up%20to%207.7%25%20and%0A5.3%25%20on%20seen%20and%20unseen%20datasets%20respectively%20but%20also%20achieves%20this%0Aperformance%20while%20being%20at%20least%20~3x%20more%20parameter-efficient%20than%20existing%0Amethods%2C%20establishing%20a%20new%20state-of-the-art%20in%20cross-domain%20few-shot%20learning.%0AOur%20code%20can%20be%20found%20at%20https%3A//github.com/rashindrie/DIPA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04492v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discriminative%20Sample-Guided%20and%20Parameter-Efficient%20Feature%20Space%0A%20%20Adaptation%20for%20Cross-Domain%20Few-Shot%20Learning&entry.906535625=Rashindrie%20Perera%20and%20Saman%20Halgamuge&entry.1292438233=%20%20In%20this%20paper%2C%20we%20look%20at%20cross-domain%20few-shot%20classification%20which%20presents%0Athe%20challenging%20task%20of%20learning%20new%20classes%20in%20unseen%20domains%20with%20few%0Alabelled%20examples.%20Existing%20methods%2C%20though%20somewhat%20effective%2C%20encounter%0Aseveral%20limitations%2C%20which%20we%20address%20in%20this%20work%20through%20two%20significant%0Aimprovements.%20First%2C%20to%20address%20overfitting%20associated%20with%20fine-tuning%20a%20large%0Anumber%20of%20parameters%20on%20small%20datasets%2C%20we%20introduce%20a%20lightweight%0Aparameter-efficient%20adaptation%20strategy.%20This%20strategy%20employs%20a%20linear%0Atransformation%20of%20pre-trained%20features%2C%20significantly%20reducing%20the%20trainable%0Aparameter%20count.%20Second%2C%20we%20replace%20the%20traditional%20nearest%20centroid%20classifier%0Awith%20a%20variance-aware%20loss%20function%2C%20enhancing%20the%20model%27s%20sensitivity%20to%20the%0Ainter-%20and%20intra-class%20variances%20within%20the%20training%20set%20for%20improved%0Aclustering%20in%20feature%20space.%20Empirical%20evaluations%20on%20the%20Meta-Dataset%0Abenchmark%20showcase%20that%20our%20approach%20not%20only%20improves%20accuracy%20up%20to%207.7%25%20and%0A5.3%25%20on%20seen%20and%20unseen%20datasets%20respectively%20but%20also%20achieves%20this%0Aperformance%20while%20being%20at%20least%20~3x%20more%20parameter-efficient%20than%20existing%0Amethods%2C%20establishing%20a%20new%20state-of-the-art%20in%20cross-domain%20few-shot%20learning.%0AOur%20code%20can%20be%20found%20at%20https%3A//github.com/rashindrie/DIPA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04492v1&entry.124074799=Read"},
{"title": "Contrastive Continual Learning with Importance Sampling and\n  Prototype-Instance Relation Distillation", "author": "Jiyong Li and Dilshod Azizov and Yang Li and Shangsong Liang", "abstract": "  Recently, because of the high-quality representations of contrastive learning\nmethods, rehearsal-based contrastive continual learning has been proposed to\nexplore how to continually learn transferable representation embeddings to\navoid the catastrophic forgetting issue in traditional continual settings.\nBased on this framework, we propose Contrastive Continual Learning via\nImportance Sampling (CCLIS) to preserve knowledge by recovering previous data\ndistributions with a new strategy for Replay Buffer Selection (RBS), which\nminimize estimated variance to save hard negative samples for representation\nlearning with high quality. Furthermore, we present the Prototype-instance\nRelation Distillation (PRD) loss, a technique designed to maintain the\nrelationship between prototypes and sample representations using a\nself-distillation process. Experiments on standard continual learning\nbenchmarks reveal that our method notably outperforms existing baselines in\nterms of knowledge preservation and thereby effectively counteracts\ncatastrophic forgetting in online contexts. The code is available at\nhttps://github.com/lijy373/CCLIS.\n", "link": "http://arxiv.org/abs/2403.04599v1", "date": "2024-03-07", "relevancy": 2.6043, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5237}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5222}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5167}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Continual%20Learning%20with%20Importance%20Sampling%20and%0A%20%20Prototype-Instance%20Relation%20Distillation&body=Title%3A%20Contrastive%20Continual%20Learning%20with%20Importance%20Sampling%20and%0A%20%20Prototype-Instance%20Relation%20Distillation%0AAuthor%3A%20Jiyong%20Li%20and%20Dilshod%20Azizov%20and%20Yang%20Li%20and%20Shangsong%20Liang%0AAbstract%3A%20%20%20Recently%2C%20because%20of%20the%20high-quality%20representations%20of%20contrastive%20learning%0Amethods%2C%20rehearsal-based%20contrastive%20continual%20learning%20has%20been%20proposed%20to%0Aexplore%20how%20to%20continually%20learn%20transferable%20representation%20embeddings%20to%0Aavoid%20the%20catastrophic%20forgetting%20issue%20in%20traditional%20continual%20settings.%0ABased%20on%20this%20framework%2C%20we%20propose%20Contrastive%20Continual%20Learning%20via%0AImportance%20Sampling%20%28CCLIS%29%20to%20preserve%20knowledge%20by%20recovering%20previous%20data%0Adistributions%20with%20a%20new%20strategy%20for%20Replay%20Buffer%20Selection%20%28RBS%29%2C%20which%0Aminimize%20estimated%20variance%20to%20save%20hard%20negative%20samples%20for%20representation%0Alearning%20with%20high%20quality.%20Furthermore%2C%20we%20present%20the%20Prototype-instance%0ARelation%20Distillation%20%28PRD%29%20loss%2C%20a%20technique%20designed%20to%20maintain%20the%0Arelationship%20between%20prototypes%20and%20sample%20representations%20using%20a%0Aself-distillation%20process.%20Experiments%20on%20standard%20continual%20learning%0Abenchmarks%20reveal%20that%20our%20method%20notably%20outperforms%20existing%20baselines%20in%0Aterms%20of%20knowledge%20preservation%20and%20thereby%20effectively%20counteracts%0Acatastrophic%20forgetting%20in%20online%20contexts.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lijy373/CCLIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04599v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Continual%20Learning%20with%20Importance%20Sampling%20and%0A%20%20Prototype-Instance%20Relation%20Distillation&entry.906535625=Jiyong%20Li%20and%20Dilshod%20Azizov%20and%20Yang%20Li%20and%20Shangsong%20Liang&entry.1292438233=%20%20Recently%2C%20because%20of%20the%20high-quality%20representations%20of%20contrastive%20learning%0Amethods%2C%20rehearsal-based%20contrastive%20continual%20learning%20has%20been%20proposed%20to%0Aexplore%20how%20to%20continually%20learn%20transferable%20representation%20embeddings%20to%0Aavoid%20the%20catastrophic%20forgetting%20issue%20in%20traditional%20continual%20settings.%0ABased%20on%20this%20framework%2C%20we%20propose%20Contrastive%20Continual%20Learning%20via%0AImportance%20Sampling%20%28CCLIS%29%20to%20preserve%20knowledge%20by%20recovering%20previous%20data%0Adistributions%20with%20a%20new%20strategy%20for%20Replay%20Buffer%20Selection%20%28RBS%29%2C%20which%0Aminimize%20estimated%20variance%20to%20save%20hard%20negative%20samples%20for%20representation%0Alearning%20with%20high%20quality.%20Furthermore%2C%20we%20present%20the%20Prototype-instance%0ARelation%20Distillation%20%28PRD%29%20loss%2C%20a%20technique%20designed%20to%20maintain%20the%0Arelationship%20between%20prototypes%20and%20sample%20representations%20using%20a%0Aself-distillation%20process.%20Experiments%20on%20standard%20continual%20learning%0Abenchmarks%20reveal%20that%20our%20method%20notably%20outperforms%20existing%20baselines%20in%0Aterms%20of%20knowledge%20preservation%20and%20thereby%20effectively%20counteracts%0Acatastrophic%20forgetting%20in%20online%20contexts.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lijy373/CCLIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04599v1&entry.124074799=Read"},
{"title": "FriendNet: Detection-Friendly Dehazing Network", "author": "Yihua Fan and Yongzhen Wang and Mingqiang Wei and Fu Lee Wang and Haoran Xie", "abstract": "  Adverse weather conditions often impair the quality of captured images,\ninevitably inducing cutting-edge object detection models for advanced driver\nassistance systems (ADAS) and autonomous driving. In this paper, we raise an\nintriguing question: can the combination of image restoration and object\ndetection enhance detection performance in adverse weather conditions? To\nanswer it, we propose an effective architecture that bridges image dehazing and\nobject detection together via guidance information and task-driven learning to\nachieve detection-friendly dehazing, termed FriendNet. FriendNet aims to\ndeliver both high-quality perception and high detection capacity. Different\nfrom existing efforts that intuitively treat image dehazing as pre-processing,\nFriendNet establishes a positive correlation between these two tasks. Clean\nfeatures generated by the dehazing network potentially contribute to\nimprovements in object detection performance. Conversely, object detection\ncrucially guides the learning process of the image dehazing network under the\ntask-driven learning scheme. We shed light on how downstream tasks can guide\nupstream dehazing processes, considering both network architecture and learning\nobjectives. We design Guidance Fusion Block (GFB) and Guidance Attention Block\n(GAB) to facilitate the integration of detection information into the network.\nFurthermore, the incorporation of the detection task loss aids in refining the\noptimization process. Additionally, we introduce a new Physics-aware Feature\nEnhancement Block (PFEB), which integrates physics-based priors to enhance the\nfeature extraction and representation capabilities. Extensive experiments on\nsynthetic and real-world datasets demonstrate the superiority of our method\nover state-of-the-art methods on both image quality and detection precision.\nOur source code is available at https://github.com/fanyihua0309/FriendNet.\n", "link": "http://arxiv.org/abs/2403.04443v1", "date": "2024-03-07", "relevancy": 2.6041, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5401}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5157}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5066}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20FriendNet%3A%20Detection-Friendly%20Dehazing%20Network&body=Title%3A%20FriendNet%3A%20Detection-Friendly%20Dehazing%20Network%0AAuthor%3A%20Yihua%20Fan%20and%20Yongzhen%20Wang%20and%20Mingqiang%20Wei%20and%20Fu%20Lee%20Wang%20and%20Haoran%20Xie%0AAbstract%3A%20%20%20Adverse%20weather%20conditions%20often%20impair%20the%20quality%20of%20captured%20images%2C%0Ainevitably%20inducing%20cutting-edge%20object%20detection%20models%20for%20advanced%20driver%0Aassistance%20systems%20%28ADAS%29%20and%20autonomous%20driving.%20In%20this%20paper%2C%20we%20raise%20an%0Aintriguing%20question%3A%20can%20the%20combination%20of%20image%20restoration%20and%20object%0Adetection%20enhance%20detection%20performance%20in%20adverse%20weather%20conditions%3F%20To%0Aanswer%20it%2C%20we%20propose%20an%20effective%20architecture%20that%20bridges%20image%20dehazing%20and%0Aobject%20detection%20together%20via%20guidance%20information%20and%20task-driven%20learning%20to%0Aachieve%20detection-friendly%20dehazing%2C%20termed%20FriendNet.%20FriendNet%20aims%20to%0Adeliver%20both%20high-quality%20perception%20and%20high%20detection%20capacity.%20Different%0Afrom%20existing%20efforts%20that%20intuitively%20treat%20image%20dehazing%20as%20pre-processing%2C%0AFriendNet%20establishes%20a%20positive%20correlation%20between%20these%20two%20tasks.%20Clean%0Afeatures%20generated%20by%20the%20dehazing%20network%20potentially%20contribute%20to%0Aimprovements%20in%20object%20detection%20performance.%20Conversely%2C%20object%20detection%0Acrucially%20guides%20the%20learning%20process%20of%20the%20image%20dehazing%20network%20under%20the%0Atask-driven%20learning%20scheme.%20We%20shed%20light%20on%20how%20downstream%20tasks%20can%20guide%0Aupstream%20dehazing%20processes%2C%20considering%20both%20network%20architecture%20and%20learning%0Aobjectives.%20We%20design%20Guidance%20Fusion%20Block%20%28GFB%29%20and%20Guidance%20Attention%20Block%0A%28GAB%29%20to%20facilitate%20the%20integration%20of%20detection%20information%20into%20the%20network.%0AFurthermore%2C%20the%20incorporation%20of%20the%20detection%20task%20loss%20aids%20in%20refining%20the%0Aoptimization%20process.%20Additionally%2C%20we%20introduce%20a%20new%20Physics-aware%20Feature%0AEnhancement%20Block%20%28PFEB%29%2C%20which%20integrates%20physics-based%20priors%20to%20enhance%20the%0Afeature%20extraction%20and%20representation%20capabilities.%20Extensive%20experiments%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20our%20method%0Aover%20state-of-the-art%20methods%20on%20both%20image%20quality%20and%20detection%20precision.%0AOur%20source%20code%20is%20available%20at%20https%3A//github.com/fanyihua0309/FriendNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04443v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FriendNet%3A%20Detection-Friendly%20Dehazing%20Network&entry.906535625=Yihua%20Fan%20and%20Yongzhen%20Wang%20and%20Mingqiang%20Wei%20and%20Fu%20Lee%20Wang%20and%20Haoran%20Xie&entry.1292438233=%20%20Adverse%20weather%20conditions%20often%20impair%20the%20quality%20of%20captured%20images%2C%0Ainevitably%20inducing%20cutting-edge%20object%20detection%20models%20for%20advanced%20driver%0Aassistance%20systems%20%28ADAS%29%20and%20autonomous%20driving.%20In%20this%20paper%2C%20we%20raise%20an%0Aintriguing%20question%3A%20can%20the%20combination%20of%20image%20restoration%20and%20object%0Adetection%20enhance%20detection%20performance%20in%20adverse%20weather%20conditions%3F%20To%0Aanswer%20it%2C%20we%20propose%20an%20effective%20architecture%20that%20bridges%20image%20dehazing%20and%0Aobject%20detection%20together%20via%20guidance%20information%20and%20task-driven%20learning%20to%0Aachieve%20detection-friendly%20dehazing%2C%20termed%20FriendNet.%20FriendNet%20aims%20to%0Adeliver%20both%20high-quality%20perception%20and%20high%20detection%20capacity.%20Different%0Afrom%20existing%20efforts%20that%20intuitively%20treat%20image%20dehazing%20as%20pre-processing%2C%0AFriendNet%20establishes%20a%20positive%20correlation%20between%20these%20two%20tasks.%20Clean%0Afeatures%20generated%20by%20the%20dehazing%20network%20potentially%20contribute%20to%0Aimprovements%20in%20object%20detection%20performance.%20Conversely%2C%20object%20detection%0Acrucially%20guides%20the%20learning%20process%20of%20the%20image%20dehazing%20network%20under%20the%0Atask-driven%20learning%20scheme.%20We%20shed%20light%20on%20how%20downstream%20tasks%20can%20guide%0Aupstream%20dehazing%20processes%2C%20considering%20both%20network%20architecture%20and%20learning%0Aobjectives.%20We%20design%20Guidance%20Fusion%20Block%20%28GFB%29%20and%20Guidance%20Attention%20Block%0A%28GAB%29%20to%20facilitate%20the%20integration%20of%20detection%20information%20into%20the%20network.%0AFurthermore%2C%20the%20incorporation%20of%20the%20detection%20task%20loss%20aids%20in%20refining%20the%0Aoptimization%20process.%20Additionally%2C%20we%20introduce%20a%20new%20Physics-aware%20Feature%0AEnhancement%20Block%20%28PFEB%29%2C%20which%20integrates%20physics-based%20priors%20to%20enhance%20the%0Afeature%20extraction%20and%20representation%20capabilities.%20Extensive%20experiments%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20our%20method%0Aover%20state-of-the-art%20methods%20on%20both%20image%20quality%20and%20detection%20precision.%0AOur%20source%20code%20is%20available%20at%20https%3A//github.com/fanyihua0309/FriendNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04443v1&entry.124074799=Read"},
{"title": "The Selected-completely-at-random Complementary Label is a Practical\n  Weak Supervision for Multi-class Classification", "author": "Wei Wang and Takashi Ishida and Yu-Jie Zhang and Gang Niu and Masashi Sugiyama", "abstract": "  Complementary-label learning is a weakly supervised learning problem in which\neach training example is associated with one or multiple complementary labels\nindicating the classes to which it does not belong. Existing consistent\napproaches have relied on the uniform distribution assumption to model the\ngeneration of complementary labels, or on an ordinary-label training set to\nestimate the transition matrix in non-uniform cases. However, either condition\nmay not be satisfied in real-world scenarios. In this paper, we propose a novel\nconsistent approach that does not rely on these conditions. Inspired by the\npositive-unlabeled (PU) learning literature, we propose an unbiased risk\nestimator based on the Selected Completely At Random assumption for\ncomplementary-label learning. We then introduce a risk-correction approach to\naddress overfitting problems. Furthermore, we find that complementary-label\nlearning can be expressed as a set of negative-unlabeled binary classification\nproblems when using the one-versus-rest strategy. Extensive experimental\nresults on both synthetic and real-world benchmark datasets validate the\nsuperiority of our proposed approach over state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2311.15502v2", "date": "2024-03-07", "relevancy": 2.597, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5328}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5232}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5023}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20The%20Selected-completely-at-random%20Complementary%20Label%20is%20a%20Practical%0A%20%20Weak%20Supervision%20for%20Multi-class%20Classification&body=Title%3A%20The%20Selected-completely-at-random%20Complementary%20Label%20is%20a%20Practical%0A%20%20Weak%20Supervision%20for%20Multi-class%20Classification%0AAuthor%3A%20Wei%20Wang%20and%20Takashi%20Ishida%20and%20Yu-Jie%20Zhang%20and%20Gang%20Niu%20and%20Masashi%20Sugiyama%0AAbstract%3A%20%20%20Complementary-label%20learning%20is%20a%20weakly%20supervised%20learning%20problem%20in%20which%0Aeach%20training%20example%20is%20associated%20with%20one%20or%20multiple%20complementary%20labels%0Aindicating%20the%20classes%20to%20which%20it%20does%20not%20belong.%20Existing%20consistent%0Aapproaches%20have%20relied%20on%20the%20uniform%20distribution%20assumption%20to%20model%20the%0Ageneration%20of%20complementary%20labels%2C%20or%20on%20an%20ordinary-label%20training%20set%20to%0Aestimate%20the%20transition%20matrix%20in%20non-uniform%20cases.%20However%2C%20either%20condition%0Amay%20not%20be%20satisfied%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aconsistent%20approach%20that%20does%20not%20rely%20on%20these%20conditions.%20Inspired%20by%20the%0Apositive-unlabeled%20%28PU%29%20learning%20literature%2C%20we%20propose%20an%20unbiased%20risk%0Aestimator%20based%20on%20the%20Selected%20Completely%20At%20Random%20assumption%20for%0Acomplementary-label%20learning.%20We%20then%20introduce%20a%20risk-correction%20approach%20to%0Aaddress%20overfitting%20problems.%20Furthermore%2C%20we%20find%20that%20complementary-label%0Alearning%20can%20be%20expressed%20as%20a%20set%20of%20negative-unlabeled%20binary%20classification%0Aproblems%20when%20using%20the%20one-versus-rest%20strategy.%20Extensive%20experimental%0Aresults%20on%20both%20synthetic%20and%20real-world%20benchmark%20datasets%20validate%20the%0Asuperiority%20of%20our%20proposed%20approach%20over%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15502v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Selected-completely-at-random%20Complementary%20Label%20is%20a%20Practical%0A%20%20Weak%20Supervision%20for%20Multi-class%20Classification&entry.906535625=Wei%20Wang%20and%20Takashi%20Ishida%20and%20Yu-Jie%20Zhang%20and%20Gang%20Niu%20and%20Masashi%20Sugiyama&entry.1292438233=%20%20Complementary-label%20learning%20is%20a%20weakly%20supervised%20learning%20problem%20in%20which%0Aeach%20training%20example%20is%20associated%20with%20one%20or%20multiple%20complementary%20labels%0Aindicating%20the%20classes%20to%20which%20it%20does%20not%20belong.%20Existing%20consistent%0Aapproaches%20have%20relied%20on%20the%20uniform%20distribution%20assumption%20to%20model%20the%0Ageneration%20of%20complementary%20labels%2C%20or%20on%20an%20ordinary-label%20training%20set%20to%0Aestimate%20the%20transition%20matrix%20in%20non-uniform%20cases.%20However%2C%20either%20condition%0Amay%20not%20be%20satisfied%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aconsistent%20approach%20that%20does%20not%20rely%20on%20these%20conditions.%20Inspired%20by%20the%0Apositive-unlabeled%20%28PU%29%20learning%20literature%2C%20we%20propose%20an%20unbiased%20risk%0Aestimator%20based%20on%20the%20Selected%20Completely%20At%20Random%20assumption%20for%0Acomplementary-label%20learning.%20We%20then%20introduce%20a%20risk-correction%20approach%20to%0Aaddress%20overfitting%20problems.%20Furthermore%2C%20we%20find%20that%20complementary-label%0Alearning%20can%20be%20expressed%20as%20a%20set%20of%20negative-unlabeled%20binary%20classification%0Aproblems%20when%20using%20the%20one-versus-rest%20strategy.%20Extensive%20experimental%0Aresults%20on%20both%20synthetic%20and%20real-world%20benchmark%20datasets%20validate%20the%0Asuperiority%20of%20our%20proposed%20approach%20over%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15502v2&entry.124074799=Read"},
{"title": "Revitalizing Multivariate Time Series Forecasting: Learnable\n  Decomposition with Inter-Series Dependencies and Intra-Series Variations\n  Modeling", "author": "Guoqi Yu and Jing Zou and Xiaowei Hu and Angelica I. Aviles-Rivero and Jing Qin and Shujun Wang", "abstract": "  Predicting multivariate time series is crucial, demanding precise modeling of\nintricate patterns, including inter-series dependencies and intra-series\nvariations. Distinctive trend characteristics in each time series pose\nchallenges, and existing methods, relying on basic moving average kernels, may\nstruggle with the non-linear structure and complex trends in real-world data.\nGiven that, we introduce a learnable decomposition strategy to capture dynamic\ntrend information more reasonably. Additionally, we propose a dual attention\nmodule tailored to capture inter-series dependencies and intra-series\nvariations simultaneously for better time series forecasting, which is\nimplemented by channel-wise self-attention and autoregressive self-attention.\nTo evaluate the effectiveness of our method, we conducted experiments across\neight open-source datasets and compared it with the state-of-the-art methods.\nThrough the comparison results, our Leddam (LEarnable Decomposition and Dual\nAttention Module) not only demonstrates significant advancements in predictive\nperformance, but also the proposed decomposition strategy can be plugged into\nother methods with a large performance-boosting, from 11.87% to 48.56% MSE\nerror degradation.\n", "link": "http://arxiv.org/abs/2402.12694v3", "date": "2024-03-07", "relevancy": 2.5944, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5316}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5174}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5076}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Revitalizing%20Multivariate%20Time%20Series%20Forecasting%3A%20Learnable%0A%20%20Decomposition%20with%20Inter-Series%20Dependencies%20and%20Intra-Series%20Variations%0A%20%20Modeling&body=Title%3A%20Revitalizing%20Multivariate%20Time%20Series%20Forecasting%3A%20Learnable%0A%20%20Decomposition%20with%20Inter-Series%20Dependencies%20and%20Intra-Series%20Variations%0A%20%20Modeling%0AAuthor%3A%20Guoqi%20Yu%20and%20Jing%20Zou%20and%20Xiaowei%20Hu%20and%20Angelica%20I.%20Aviles-Rivero%20and%20Jing%20Qin%20and%20Shujun%20Wang%0AAbstract%3A%20%20%20Predicting%20multivariate%20time%20series%20is%20crucial%2C%20demanding%20precise%20modeling%20of%0Aintricate%20patterns%2C%20including%20inter-series%20dependencies%20and%20intra-series%0Avariations.%20Distinctive%20trend%20characteristics%20in%20each%20time%20series%20pose%0Achallenges%2C%20and%20existing%20methods%2C%20relying%20on%20basic%20moving%20average%20kernels%2C%20may%0Astruggle%20with%20the%20non-linear%20structure%20and%20complex%20trends%20in%20real-world%20data.%0AGiven%20that%2C%20we%20introduce%20a%20learnable%20decomposition%20strategy%20to%20capture%20dynamic%0Atrend%20information%20more%20reasonably.%20Additionally%2C%20we%20propose%20a%20dual%20attention%0Amodule%20tailored%20to%20capture%20inter-series%20dependencies%20and%20intra-series%0Avariations%20simultaneously%20for%20better%20time%20series%20forecasting%2C%20which%20is%0Aimplemented%20by%20channel-wise%20self-attention%20and%20autoregressive%20self-attention.%0ATo%20evaluate%20the%20effectiveness%20of%20our%20method%2C%20we%20conducted%20experiments%20across%0Aeight%20open-source%20datasets%20and%20compared%20it%20with%20the%20state-of-the-art%20methods.%0AThrough%20the%20comparison%20results%2C%20our%20Leddam%20%28LEarnable%20Decomposition%20and%20Dual%0AAttention%20Module%29%20not%20only%20demonstrates%20significant%20advancements%20in%20predictive%0Aperformance%2C%20but%20also%20the%20proposed%20decomposition%20strategy%20can%20be%20plugged%20into%0Aother%20methods%20with%20a%20large%20performance-boosting%2C%20from%2011.87%25%20to%2048.56%25%20MSE%0Aerror%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12694v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revitalizing%20Multivariate%20Time%20Series%20Forecasting%3A%20Learnable%0A%20%20Decomposition%20with%20Inter-Series%20Dependencies%20and%20Intra-Series%20Variations%0A%20%20Modeling&entry.906535625=Guoqi%20Yu%20and%20Jing%20Zou%20and%20Xiaowei%20Hu%20and%20Angelica%20I.%20Aviles-Rivero%20and%20Jing%20Qin%20and%20Shujun%20Wang&entry.1292438233=%20%20Predicting%20multivariate%20time%20series%20is%20crucial%2C%20demanding%20precise%20modeling%20of%0Aintricate%20patterns%2C%20including%20inter-series%20dependencies%20and%20intra-series%0Avariations.%20Distinctive%20trend%20characteristics%20in%20each%20time%20series%20pose%0Achallenges%2C%20and%20existing%20methods%2C%20relying%20on%20basic%20moving%20average%20kernels%2C%20may%0Astruggle%20with%20the%20non-linear%20structure%20and%20complex%20trends%20in%20real-world%20data.%0AGiven%20that%2C%20we%20introduce%20a%20learnable%20decomposition%20strategy%20to%20capture%20dynamic%0Atrend%20information%20more%20reasonably.%20Additionally%2C%20we%20propose%20a%20dual%20attention%0Amodule%20tailored%20to%20capture%20inter-series%20dependencies%20and%20intra-series%0Avariations%20simultaneously%20for%20better%20time%20series%20forecasting%2C%20which%20is%0Aimplemented%20by%20channel-wise%20self-attention%20and%20autoregressive%20self-attention.%0ATo%20evaluate%20the%20effectiveness%20of%20our%20method%2C%20we%20conducted%20experiments%20across%0Aeight%20open-source%20datasets%20and%20compared%20it%20with%20the%20state-of-the-art%20methods.%0AThrough%20the%20comparison%20results%2C%20our%20Leddam%20%28LEarnable%20Decomposition%20and%20Dual%0AAttention%20Module%29%20not%20only%20demonstrates%20significant%20advancements%20in%20predictive%0Aperformance%2C%20but%20also%20the%20proposed%20decomposition%20strategy%20can%20be%20plugged%20into%0Aother%20methods%20with%20a%20large%20performance-boosting%2C%20from%2011.87%25%20to%2048.56%25%20MSE%0Aerror%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12694v3&entry.124074799=Read"},
{"title": "Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing", "author": "Xiaofan Yu and Anthony Thomas and Ivannia Gomez Moreno and Louis Gutierrez and Tajana Rosing", "abstract": "  On-device learning has emerged as a prevailing trend that avoids the slow\nresponse time and costly communication of cloud-based learning. The ability to\nlearn continuously and indefinitely in a changing environment, and with\nresource constraints, is critical for real sensor deployments. However,\nexisting designs are inadequate for practical scenarios with (i) streaming data\ninput, (ii) lack of supervision and (iii) limited on-board resources. In this\npaper, we design and deploy the first on-device lifelong learning system called\nLifeHD for general IoT applications with limited supervision. LifeHD is\ndesigned based on a novel neurally-inspired and lightweight learning paradigm\ncalled Hyperdimensional Computing (HDC). We utilize a two-tier associative\nmemory organization to intelligently store and manage high-dimensional,\nlow-precision vectors, which represent the historical patterns as cluster\ncentroids. We additionally propose two variants of LifeHD to cope with scarce\nlabeled inputs and power constraints. We implement LifeHD on off-the-shelf edge\nplatforms and perform extensive evaluations across three scenarios. Our\nmeasurements show that LifeHD improves the unsupervised clustering accuracy by\nup to 74.8% compared to the state-of-the-art NN-based unsupervised lifelong\nlearning baselines with as much as 34.3x better energy efficiency. Our code is\navailable at https://github.com/Orienfish/LifeHD.\n", "link": "http://arxiv.org/abs/2403.04759v1", "date": "2024-03-07", "relevancy": 2.5839, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5231}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5164}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5107}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Lifelong%20Intelligence%20Beyond%20the%20Edge%20using%20Hyperdimensional%20Computing&body=Title%3A%20Lifelong%20Intelligence%20Beyond%20the%20Edge%20using%20Hyperdimensional%20Computing%0AAuthor%3A%20Xiaofan%20Yu%20and%20Anthony%20Thomas%20and%20Ivannia%20Gomez%20Moreno%20and%20Louis%20Gutierrez%20and%20Tajana%20Rosing%0AAbstract%3A%20%20%20On-device%20learning%20has%20emerged%20as%20a%20prevailing%20trend%20that%20avoids%20the%20slow%0Aresponse%20time%20and%20costly%20communication%20of%20cloud-based%20learning.%20The%20ability%20to%0Alearn%20continuously%20and%20indefinitely%20in%20a%20changing%20environment%2C%20and%20with%0Aresource%20constraints%2C%20is%20critical%20for%20real%20sensor%20deployments.%20However%2C%0Aexisting%20designs%20are%20inadequate%20for%20practical%20scenarios%20with%20%28i%29%20streaming%20data%0Ainput%2C%20%28ii%29%20lack%20of%20supervision%20and%20%28iii%29%20limited%20on-board%20resources.%20In%20this%0Apaper%2C%20we%20design%20and%20deploy%20the%20first%20on-device%20lifelong%20learning%20system%20called%0ALifeHD%20for%20general%20IoT%20applications%20with%20limited%20supervision.%20LifeHD%20is%0Adesigned%20based%20on%20a%20novel%20neurally-inspired%20and%20lightweight%20learning%20paradigm%0Acalled%20Hyperdimensional%20Computing%20%28HDC%29.%20We%20utilize%20a%20two-tier%20associative%0Amemory%20organization%20to%20intelligently%20store%20and%20manage%20high-dimensional%2C%0Alow-precision%20vectors%2C%20which%20represent%20the%20historical%20patterns%20as%20cluster%0Acentroids.%20We%20additionally%20propose%20two%20variants%20of%20LifeHD%20to%20cope%20with%20scarce%0Alabeled%20inputs%20and%20power%20constraints.%20We%20implement%20LifeHD%20on%20off-the-shelf%20edge%0Aplatforms%20and%20perform%20extensive%20evaluations%20across%20three%20scenarios.%20Our%0Ameasurements%20show%20that%20LifeHD%20improves%20the%20unsupervised%20clustering%20accuracy%20by%0Aup%20to%2074.8%25%20compared%20to%20the%20state-of-the-art%20NN-based%20unsupervised%20lifelong%0Alearning%20baselines%20with%20as%20much%20as%2034.3x%20better%20energy%20efficiency.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Orienfish/LifeHD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04759v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lifelong%20Intelligence%20Beyond%20the%20Edge%20using%20Hyperdimensional%20Computing&entry.906535625=Xiaofan%20Yu%20and%20Anthony%20Thomas%20and%20Ivannia%20Gomez%20Moreno%20and%20Louis%20Gutierrez%20and%20Tajana%20Rosing&entry.1292438233=%20%20On-device%20learning%20has%20emerged%20as%20a%20prevailing%20trend%20that%20avoids%20the%20slow%0Aresponse%20time%20and%20costly%20communication%20of%20cloud-based%20learning.%20The%20ability%20to%0Alearn%20continuously%20and%20indefinitely%20in%20a%20changing%20environment%2C%20and%20with%0Aresource%20constraints%2C%20is%20critical%20for%20real%20sensor%20deployments.%20However%2C%0Aexisting%20designs%20are%20inadequate%20for%20practical%20scenarios%20with%20%28i%29%20streaming%20data%0Ainput%2C%20%28ii%29%20lack%20of%20supervision%20and%20%28iii%29%20limited%20on-board%20resources.%20In%20this%0Apaper%2C%20we%20design%20and%20deploy%20the%20first%20on-device%20lifelong%20learning%20system%20called%0ALifeHD%20for%20general%20IoT%20applications%20with%20limited%20supervision.%20LifeHD%20is%0Adesigned%20based%20on%20a%20novel%20neurally-inspired%20and%20lightweight%20learning%20paradigm%0Acalled%20Hyperdimensional%20Computing%20%28HDC%29.%20We%20utilize%20a%20two-tier%20associative%0Amemory%20organization%20to%20intelligently%20store%20and%20manage%20high-dimensional%2C%0Alow-precision%20vectors%2C%20which%20represent%20the%20historical%20patterns%20as%20cluster%0Acentroids.%20We%20additionally%20propose%20two%20variants%20of%20LifeHD%20to%20cope%20with%20scarce%0Alabeled%20inputs%20and%20power%20constraints.%20We%20implement%20LifeHD%20on%20off-the-shelf%20edge%0Aplatforms%20and%20perform%20extensive%20evaluations%20across%20three%20scenarios.%20Our%0Ameasurements%20show%20that%20LifeHD%20improves%20the%20unsupervised%20clustering%20accuracy%20by%0Aup%20to%2074.8%25%20compared%20to%20the%20state-of-the-art%20NN-based%20unsupervised%20lifelong%0Alearning%20baselines%20with%20as%20much%20as%2034.3x%20better%20energy%20efficiency.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Orienfish/LifeHD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04759v1&entry.124074799=Read"},
{"title": "Q&A Prompts: Discovering Rich Visual Clues through Mining\n  Question-Answer Prompts for VQA requiring Diverse World Knowledge", "author": "Haibi Wang and Weifeng Ge", "abstract": "  With the breakthrough of multi-modal large language models, answering complex\nvisual questions that demand advanced reasoning abilities and world knowledge\nhas become a much more important testbed for developing AI models than ever.\nHowever, equipping AI models with robust cross-modality reasoning ability\nremains challenging since the cognition scheme of humans has not been\nunderstood systematically. In this paper, we believe that if we can collect\nvisual clues in the given image as much as possible, we will recognize the\nimage more accurately, understand the question better, recall relevant\nknowledge more easily, and finally reason out the answer. We discover these\nrich visual clues by mining question-answer pairs in images and sending them\ninto multi-modal large language models as prompts. We call the proposed method\nQ&A Prompts. Specifically, we first use the image-answer pairs and the\ncorresponding questions in the training set as inputs and outputs to train a\nvisual question generation model. Then, we use an image tagging model to\nidentify various instances and send packaged image-tag pairs into the visual\nquestion generation model to generate relevant questions with the extracted\nimage tags as answers. Finally, we encode these generated question-answer pairs\nas prompts with a visual-aware prompting module and send them into pre-trained\nmulti-modal large language models to reason out the final answers. Experimental\nresults show that, compared with state-of-the-art methods, our Q&A Prompts\nachieves substantial improvements on the challenging visual question answering\ndatasets requiring reasoning over diverse world knowledge, such as OK-VQA and\nA-OKVQA.\n", "link": "http://arxiv.org/abs/2401.10712v3", "date": "2024-03-07", "relevancy": 2.5684, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5162}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5073}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Q%26A%20Prompts%3A%20Discovering%20Rich%20Visual%20Clues%20through%20Mining%0A%20%20Question-Answer%20Prompts%20for%20VQA%20requiring%20Diverse%20World%20Knowledge&body=Title%3A%20Q%26A%20Prompts%3A%20Discovering%20Rich%20Visual%20Clues%20through%20Mining%0A%20%20Question-Answer%20Prompts%20for%20VQA%20requiring%20Diverse%20World%20Knowledge%0AAuthor%3A%20Haibi%20Wang%20and%20Weifeng%20Ge%0AAbstract%3A%20%20%20With%20the%20breakthrough%20of%20multi-modal%20large%20language%20models%2C%20answering%20complex%0Avisual%20questions%20that%20demand%20advanced%20reasoning%20abilities%20and%20world%20knowledge%0Ahas%20become%20a%20much%20more%20important%20testbed%20for%20developing%20AI%20models%20than%20ever.%0AHowever%2C%20equipping%20AI%20models%20with%20robust%20cross-modality%20reasoning%20ability%0Aremains%20challenging%20since%20the%20cognition%20scheme%20of%20humans%20has%20not%20been%0Aunderstood%20systematically.%20In%20this%20paper%2C%20we%20believe%20that%20if%20we%20can%20collect%0Avisual%20clues%20in%20the%20given%20image%20as%20much%20as%20possible%2C%20we%20will%20recognize%20the%0Aimage%20more%20accurately%2C%20understand%20the%20question%20better%2C%20recall%20relevant%0Aknowledge%20more%20easily%2C%20and%20finally%20reason%20out%20the%20answer.%20We%20discover%20these%0Arich%20visual%20clues%20by%20mining%20question-answer%20pairs%20in%20images%20and%20sending%20them%0Ainto%20multi-modal%20large%20language%20models%20as%20prompts.%20We%20call%20the%20proposed%20method%0AQ%26A%20Prompts.%20Specifically%2C%20we%20first%20use%20the%20image-answer%20pairs%20and%20the%0Acorresponding%20questions%20in%20the%20training%20set%20as%20inputs%20and%20outputs%20to%20train%20a%0Avisual%20question%20generation%20model.%20Then%2C%20we%20use%20an%20image%20tagging%20model%20to%0Aidentify%20various%20instances%20and%20send%20packaged%20image-tag%20pairs%20into%20the%20visual%0Aquestion%20generation%20model%20to%20generate%20relevant%20questions%20with%20the%20extracted%0Aimage%20tags%20as%20answers.%20Finally%2C%20we%20encode%20these%20generated%20question-answer%20pairs%0Aas%20prompts%20with%20a%20visual-aware%20prompting%20module%20and%20send%20them%20into%20pre-trained%0Amulti-modal%20large%20language%20models%20to%20reason%20out%20the%20final%20answers.%20Experimental%0Aresults%20show%20that%2C%20compared%20with%20state-of-the-art%20methods%2C%20our%20Q%26A%20Prompts%0Aachieves%20substantial%20improvements%20on%20the%20challenging%20visual%20question%20answering%0Adatasets%20requiring%20reasoning%20over%20diverse%20world%20knowledge%2C%20such%20as%20OK-VQA%20and%0AA-OKVQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10712v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q%26A%20Prompts%3A%20Discovering%20Rich%20Visual%20Clues%20through%20Mining%0A%20%20Question-Answer%20Prompts%20for%20VQA%20requiring%20Diverse%20World%20Knowledge&entry.906535625=Haibi%20Wang%20and%20Weifeng%20Ge&entry.1292438233=%20%20With%20the%20breakthrough%20of%20multi-modal%20large%20language%20models%2C%20answering%20complex%0Avisual%20questions%20that%20demand%20advanced%20reasoning%20abilities%20and%20world%20knowledge%0Ahas%20become%20a%20much%20more%20important%20testbed%20for%20developing%20AI%20models%20than%20ever.%0AHowever%2C%20equipping%20AI%20models%20with%20robust%20cross-modality%20reasoning%20ability%0Aremains%20challenging%20since%20the%20cognition%20scheme%20of%20humans%20has%20not%20been%0Aunderstood%20systematically.%20In%20this%20paper%2C%20we%20believe%20that%20if%20we%20can%20collect%0Avisual%20clues%20in%20the%20given%20image%20as%20much%20as%20possible%2C%20we%20will%20recognize%20the%0Aimage%20more%20accurately%2C%20understand%20the%20question%20better%2C%20recall%20relevant%0Aknowledge%20more%20easily%2C%20and%20finally%20reason%20out%20the%20answer.%20We%20discover%20these%0Arich%20visual%20clues%20by%20mining%20question-answer%20pairs%20in%20images%20and%20sending%20them%0Ainto%20multi-modal%20large%20language%20models%20as%20prompts.%20We%20call%20the%20proposed%20method%0AQ%26A%20Prompts.%20Specifically%2C%20we%20first%20use%20the%20image-answer%20pairs%20and%20the%0Acorresponding%20questions%20in%20the%20training%20set%20as%20inputs%20and%20outputs%20to%20train%20a%0Avisual%20question%20generation%20model.%20Then%2C%20we%20use%20an%20image%20tagging%20model%20to%0Aidentify%20various%20instances%20and%20send%20packaged%20image-tag%20pairs%20into%20the%20visual%0Aquestion%20generation%20model%20to%20generate%20relevant%20questions%20with%20the%20extracted%0Aimage%20tags%20as%20answers.%20Finally%2C%20we%20encode%20these%20generated%20question-answer%20pairs%0Aas%20prompts%20with%20a%20visual-aware%20prompting%20module%20and%20send%20them%20into%20pre-trained%0Amulti-modal%20large%20language%20models%20to%20reason%20out%20the%20final%20answers.%20Experimental%0Aresults%20show%20that%2C%20compared%20with%20state-of-the-art%20methods%2C%20our%20Q%26A%20Prompts%0Aachieves%20substantial%20improvements%20on%20the%20challenging%20visual%20question%20answering%0Adatasets%20requiring%20reasoning%20over%20diverse%20world%20knowledge%2C%20such%20as%20OK-VQA%20and%0AA-OKVQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10712v3&entry.124074799=Read"},
{"title": "Explainable Face Verification via Feature-Guided Gradient\n  Backpropagation", "author": "Yuhang Lu and Zewei Xu and Touradj Ebrahimi", "abstract": "  Recent years have witnessed significant advancement in face recognition (FR)\ntechniques, with their applications widely spread in people's lives and\nsecurity-sensitive areas. There is a growing need for reliable interpretations\nof decisions of such systems. Existing studies relying on various mechanisms\nhave investigated the usage of saliency maps as an explanation approach, but\nsuffer from different limitations. This paper first explores the spatial\nrelationship between face image and its deep representation via gradient\nbackpropagation. Then a new explanation approach FGGB has been conceived, which\nprovides precise and insightful similarity and dissimilarity saliency maps to\nexplain the \"Accept\" and \"Reject\" decision of an FR system. Extensive visual\npresentation and quantitative measurement have shown that FGGB achieves\nsuperior performance in both similarity and dissimilarity maps when compared to\ncurrent state-of-the-art explainable face verification approaches.\n", "link": "http://arxiv.org/abs/2403.04549v1", "date": "2024-03-07", "relevancy": 2.5629, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5247}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5208}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4922}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Explainable%20Face%20Verification%20via%20Feature-Guided%20Gradient%0A%20%20Backpropagation&body=Title%3A%20Explainable%20Face%20Verification%20via%20Feature-Guided%20Gradient%0A%20%20Backpropagation%0AAuthor%3A%20Yuhang%20Lu%20and%20Zewei%20Xu%20and%20Touradj%20Ebrahimi%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20significant%20advancement%20in%20face%20recognition%20%28FR%29%0Atechniques%2C%20with%20their%20applications%20widely%20spread%20in%20people%27s%20lives%20and%0Asecurity-sensitive%20areas.%20There%20is%20a%20growing%20need%20for%20reliable%20interpretations%0Aof%20decisions%20of%20such%20systems.%20Existing%20studies%20relying%20on%20various%20mechanisms%0Ahave%20investigated%20the%20usage%20of%20saliency%20maps%20as%20an%20explanation%20approach%2C%20but%0Asuffer%20from%20different%20limitations.%20This%20paper%20first%20explores%20the%20spatial%0Arelationship%20between%20face%20image%20and%20its%20deep%20representation%20via%20gradient%0Abackpropagation.%20Then%20a%20new%20explanation%20approach%20FGGB%20has%20been%20conceived%2C%20which%0Aprovides%20precise%20and%20insightful%20similarity%20and%20dissimilarity%20saliency%20maps%20to%0Aexplain%20the%20%22Accept%22%20and%20%22Reject%22%20decision%20of%20an%20FR%20system.%20Extensive%20visual%0Apresentation%20and%20quantitative%20measurement%20have%20shown%20that%20FGGB%20achieves%0Asuperior%20performance%20in%20both%20similarity%20and%20dissimilarity%20maps%20when%20compared%20to%0Acurrent%20state-of-the-art%20explainable%20face%20verification%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04549v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Face%20Verification%20via%20Feature-Guided%20Gradient%0A%20%20Backpropagation&entry.906535625=Yuhang%20Lu%20and%20Zewei%20Xu%20and%20Touradj%20Ebrahimi&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20significant%20advancement%20in%20face%20recognition%20%28FR%29%0Atechniques%2C%20with%20their%20applications%20widely%20spread%20in%20people%27s%20lives%20and%0Asecurity-sensitive%20areas.%20There%20is%20a%20growing%20need%20for%20reliable%20interpretations%0Aof%20decisions%20of%20such%20systems.%20Existing%20studies%20relying%20on%20various%20mechanisms%0Ahave%20investigated%20the%20usage%20of%20saliency%20maps%20as%20an%20explanation%20approach%2C%20but%0Asuffer%20from%20different%20limitations.%20This%20paper%20first%20explores%20the%20spatial%0Arelationship%20between%20face%20image%20and%20its%20deep%20representation%20via%20gradient%0Abackpropagation.%20Then%20a%20new%20explanation%20approach%20FGGB%20has%20been%20conceived%2C%20which%0Aprovides%20precise%20and%20insightful%20similarity%20and%20dissimilarity%20saliency%20maps%20to%0Aexplain%20the%20%22Accept%22%20and%20%22Reject%22%20decision%20of%20an%20FR%20system.%20Extensive%20visual%0Apresentation%20and%20quantitative%20measurement%20have%20shown%20that%20FGGB%20achieves%0Asuperior%20performance%20in%20both%20similarity%20and%20dissimilarity%20maps%20when%20compared%20to%0Acurrent%20state-of-the-art%20explainable%20face%20verification%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04549v1&entry.124074799=Read"},
{"title": "Detect Everything with Few Examples", "author": "Xinyu Zhang and Yuting Wang and Abdeslam Boularias", "abstract": "  Few-shot object detection aims at detecting novel categories given a few\nexample images. Recent methods focus on finetuning strategies, with complicated\nprocedures that prohibit a wider application. In this paper, we introduce\nDE-ViT, a few-shot object detector without the need for finetuning. DE-ViT's\nnovel architecture is based on a new region-propagation mechanism for\nlocalization. The propagated region masks are transformed into bounding boxes\nthrough a learnable spatial integral layer. Instead of training prototype\nclassifiers, we propose to use prototypes to project ViT features into a\nsubspace that is robust to overfitting on base classes. We evaluate DE-ViT on\nfew-shot, and one-shot object detection benchmarks with Pascal VOC, COCO, and\nLVIS. DE-ViT establishes new state-of-the-art results on all benchmarks.\nNotably, for COCO, DE-ViT surpasses the few-shot SoTA by 15 mAP on 10-shot and\n7.2 mAP on 30-shot and one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms\nfew-shot SoTA by 20 box APr.\n", "link": "http://arxiv.org/abs/2309.12969v3", "date": "2024-03-07", "relevancy": 2.5575, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5335}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5006}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5003}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Detect%20Everything%20with%20Few%20Examples&body=Title%3A%20Detect%20Everything%20with%20Few%20Examples%0AAuthor%3A%20Xinyu%20Zhang%20and%20Yuting%20Wang%20and%20Abdeslam%20Boularias%0AAbstract%3A%20%20%20Few-shot%20object%20detection%20aims%20at%20detecting%20novel%20categories%20given%20a%20few%0Aexample%20images.%20Recent%20methods%20focus%20on%20finetuning%20strategies%2C%20with%20complicated%0Aprocedures%20that%20prohibit%20a%20wider%20application.%20In%20this%20paper%2C%20we%20introduce%0ADE-ViT%2C%20a%20few-shot%20object%20detector%20without%20the%20need%20for%20finetuning.%20DE-ViT%27s%0Anovel%20architecture%20is%20based%20on%20a%20new%20region-propagation%20mechanism%20for%0Alocalization.%20The%20propagated%20region%20masks%20are%20transformed%20into%20bounding%20boxes%0Athrough%20a%20learnable%20spatial%20integral%20layer.%20Instead%20of%20training%20prototype%0Aclassifiers%2C%20we%20propose%20to%20use%20prototypes%20to%20project%20ViT%20features%20into%20a%0Asubspace%20that%20is%20robust%20to%20overfitting%20on%20base%20classes.%20We%20evaluate%20DE-ViT%20on%0Afew-shot%2C%20and%20one-shot%20object%20detection%20benchmarks%20with%20Pascal%20VOC%2C%20COCO%2C%20and%0ALVIS.%20DE-ViT%20establishes%20new%20state-of-the-art%20results%20on%20all%20benchmarks.%0ANotably%2C%20for%20COCO%2C%20DE-ViT%20surpasses%20the%20few-shot%20SoTA%20by%2015%20mAP%20on%2010-shot%20and%0A7.2%20mAP%20on%2030-shot%20and%20one-shot%20SoTA%20by%202.8%20AP50.%20For%20LVIS%2C%20DE-ViT%20outperforms%0Afew-shot%20SoTA%20by%2020%20box%20APr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12969v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detect%20Everything%20with%20Few%20Examples&entry.906535625=Xinyu%20Zhang%20and%20Yuting%20Wang%20and%20Abdeslam%20Boularias&entry.1292438233=%20%20Few-shot%20object%20detection%20aims%20at%20detecting%20novel%20categories%20given%20a%20few%0Aexample%20images.%20Recent%20methods%20focus%20on%20finetuning%20strategies%2C%20with%20complicated%0Aprocedures%20that%20prohibit%20a%20wider%20application.%20In%20this%20paper%2C%20we%20introduce%0ADE-ViT%2C%20a%20few-shot%20object%20detector%20without%20the%20need%20for%20finetuning.%20DE-ViT%27s%0Anovel%20architecture%20is%20based%20on%20a%20new%20region-propagation%20mechanism%20for%0Alocalization.%20The%20propagated%20region%20masks%20are%20transformed%20into%20bounding%20boxes%0Athrough%20a%20learnable%20spatial%20integral%20layer.%20Instead%20of%20training%20prototype%0Aclassifiers%2C%20we%20propose%20to%20use%20prototypes%20to%20project%20ViT%20features%20into%20a%0Asubspace%20that%20is%20robust%20to%20overfitting%20on%20base%20classes.%20We%20evaluate%20DE-ViT%20on%0Afew-shot%2C%20and%20one-shot%20object%20detection%20benchmarks%20with%20Pascal%20VOC%2C%20COCO%2C%20and%0ALVIS.%20DE-ViT%20establishes%20new%20state-of-the-art%20results%20on%20all%20benchmarks.%0ANotably%2C%20for%20COCO%2C%20DE-ViT%20surpasses%20the%20few-shot%20SoTA%20by%2015%20mAP%20on%2010-shot%20and%0A7.2%20mAP%20on%2030-shot%20and%20one-shot%20SoTA%20by%202.8%20AP50.%20For%20LVIS%2C%20DE-ViT%20outperforms%0Afew-shot%20SoTA%20by%2020%20box%20APr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12969v3&entry.124074799=Read"},
{"title": "Value Explicit Pretraining for Learning Transferable Representations", "author": "Kiran Lekkala and Henghui Bao and Sumedh Sontakke and Laurent Itti", "abstract": "  We propose Value Explicit Pretraining (VEP), a method that learns\ngeneralizable representations for transfer reinforcement learning. VEP enables\nlearning of new tasks that share similar objectives as previously learned\ntasks, by learning an encoder for objective-conditioned representations,\nirrespective of appearance changes and environment dynamics. To pre-train the\nencoder from a sequence of observations, we use a self-supervised contrastive\nloss that results in learning temporally smooth representations. VEP learns to\nrelate states across different tasks based on the Bellman return estimate that\nis reflective of task progress. Experiments using a realistic navigation\nsimulator and Atari benchmark show that the pretrained encoder produced by our\nmethod outperforms current SoTA pretraining methods on the ability to\ngeneralize to unseen tasks. VEP achieves up to a 2 times improvement in rewards\non Atari and visual navigation, and up to a 3 times improvement in sample\nefficiency. For videos of policy performance visit our\nhttps://sites.google.com/view/value-explicit-pretraining/\n", "link": "http://arxiv.org/abs/2312.12339v2", "date": "2024-03-07", "relevancy": 2.5486, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5134}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5085}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5073}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Value%20Explicit%20Pretraining%20for%20Learning%20Transferable%20Representations&body=Title%3A%20Value%20Explicit%20Pretraining%20for%20Learning%20Transferable%20Representations%0AAuthor%3A%20Kiran%20Lekkala%20and%20Henghui%20Bao%20and%20Sumedh%20Sontakke%20and%20Laurent%20Itti%0AAbstract%3A%20%20%20We%20propose%20Value%20Explicit%20Pretraining%20%28VEP%29%2C%20a%20method%20that%20learns%0Ageneralizable%20representations%20for%20transfer%20reinforcement%20learning.%20VEP%20enables%0Alearning%20of%20new%20tasks%20that%20share%20similar%20objectives%20as%20previously%20learned%0Atasks%2C%20by%20learning%20an%20encoder%20for%20objective-conditioned%20representations%2C%0Airrespective%20of%20appearance%20changes%20and%20environment%20dynamics.%20To%20pre-train%20the%0Aencoder%20from%20a%20sequence%20of%20observations%2C%20we%20use%20a%20self-supervised%20contrastive%0Aloss%20that%20results%20in%20learning%20temporally%20smooth%20representations.%20VEP%20learns%20to%0Arelate%20states%20across%20different%20tasks%20based%20on%20the%20Bellman%20return%20estimate%20that%0Ais%20reflective%20of%20task%20progress.%20Experiments%20using%20a%20realistic%20navigation%0Asimulator%20and%20Atari%20benchmark%20show%20that%20the%20pretrained%20encoder%20produced%20by%20our%0Amethod%20outperforms%20current%20SoTA%20pretraining%20methods%20on%20the%20ability%20to%0Ageneralize%20to%20unseen%20tasks.%20VEP%20achieves%20up%20to%20a%202%20times%20improvement%20in%20rewards%0Aon%20Atari%20and%20visual%20navigation%2C%20and%20up%20to%20a%203%20times%20improvement%20in%20sample%0Aefficiency.%20For%20videos%20of%20policy%20performance%20visit%20our%0Ahttps%3A//sites.google.com/view/value-explicit-pretraining/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12339v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Value%20Explicit%20Pretraining%20for%20Learning%20Transferable%20Representations&entry.906535625=Kiran%20Lekkala%20and%20Henghui%20Bao%20and%20Sumedh%20Sontakke%20and%20Laurent%20Itti&entry.1292438233=%20%20We%20propose%20Value%20Explicit%20Pretraining%20%28VEP%29%2C%20a%20method%20that%20learns%0Ageneralizable%20representations%20for%20transfer%20reinforcement%20learning.%20VEP%20enables%0Alearning%20of%20new%20tasks%20that%20share%20similar%20objectives%20as%20previously%20learned%0Atasks%2C%20by%20learning%20an%20encoder%20for%20objective-conditioned%20representations%2C%0Airrespective%20of%20appearance%20changes%20and%20environment%20dynamics.%20To%20pre-train%20the%0Aencoder%20from%20a%20sequence%20of%20observations%2C%20we%20use%20a%20self-supervised%20contrastive%0Aloss%20that%20results%20in%20learning%20temporally%20smooth%20representations.%20VEP%20learns%20to%0Arelate%20states%20across%20different%20tasks%20based%20on%20the%20Bellman%20return%20estimate%20that%0Ais%20reflective%20of%20task%20progress.%20Experiments%20using%20a%20realistic%20navigation%0Asimulator%20and%20Atari%20benchmark%20show%20that%20the%20pretrained%20encoder%20produced%20by%20our%0Amethod%20outperforms%20current%20SoTA%20pretraining%20methods%20on%20the%20ability%20to%0Ageneralize%20to%20unseen%20tasks.%20VEP%20achieves%20up%20to%20a%202%20times%20improvement%20in%20rewards%0Aon%20Atari%20and%20visual%20navigation%2C%20and%20up%20to%20a%203%20times%20improvement%20in%20sample%0Aefficiency.%20For%20videos%20of%20policy%20performance%20visit%20our%0Ahttps%3A//sites.google.com/view/value-explicit-pretraining/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12339v2&entry.124074799=Read"},
{"title": "Rethinking of Encoder-based Warm-start Methods in Hyperparameter\n  Optimization", "author": "Dawid P\u0142udowski and Antoni Zajko and Anna Kozak and Katarzyna Wo\u017anica", "abstract": "  Effectively representing heterogeneous tabular datasets for meta-learning\nremains an open problem. Previous approaches rely on predefined meta-features,\nfor example, statistical measures or landmarkers. Encoder-based models, such as\nDataset2Vec, allow us to extract significant meta-features automatically\nwithout human intervention. This research introduces a novel encoder-based\nrepresentation of tabular datasets implemented within the liltab package\navailable on GitHub https://github.com/azoz01/liltab. Our package is based on\nan established model for heterogeneous tabular data proposed in [Iwata and\nKumagai, 2020]. The proposed approach employs a different model for encoding\nfeature relationships, generating alternative representations compared to\nexisting methods like Dataset2Vec. Both of them leverage the fundamental\nassumption of dataset similarity learning. In this work, we evaluate\nDataset2Vec and liltab on two common meta-tasks - representing entire datasets\nand hyperparameter optimization warm-start. However, validation on an\nindependent metaMIMIC dataset highlights the nuanced challenges in\nrepresentation learning. We show that general representations may not suffice\nfor some meta-tasks where requirements are not explicitly considered during\nextraction.\n  [Iwata and Kumagai, 2020] Tomoharu Iwata and Atsutoshi Kumagai. Meta-learning\nfrom Tasks with Heterogeneous Attribute Spaces. In Advances in Neural\nInformation Processing Systems, 2020.\n", "link": "http://arxiv.org/abs/2403.04720v1", "date": "2024-03-07", "relevancy": 2.5266, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5058}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5043}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Rethinking%20of%20Encoder-based%20Warm-start%20Methods%20in%20Hyperparameter%0A%20%20Optimization&body=Title%3A%20Rethinking%20of%20Encoder-based%20Warm-start%20Methods%20in%20Hyperparameter%0A%20%20Optimization%0AAuthor%3A%20Dawid%20P%C5%82udowski%20and%20Antoni%20Zajko%20and%20Anna%20Kozak%20and%20Katarzyna%20Wo%C5%BAnica%0AAbstract%3A%20%20%20Effectively%20representing%20heterogeneous%20tabular%20datasets%20for%20meta-learning%0Aremains%20an%20open%20problem.%20Previous%20approaches%20rely%20on%20predefined%20meta-features%2C%0Afor%20example%2C%20statistical%20measures%20or%20landmarkers.%20Encoder-based%20models%2C%20such%20as%0ADataset2Vec%2C%20allow%20us%20to%20extract%20significant%20meta-features%20automatically%0Awithout%20human%20intervention.%20This%20research%20introduces%20a%20novel%20encoder-based%0Arepresentation%20of%20tabular%20datasets%20implemented%20within%20the%20liltab%20package%0Aavailable%20on%20GitHub%20https%3A//github.com/azoz01/liltab.%20Our%20package%20is%20based%20on%0Aan%20established%20model%20for%20heterogeneous%20tabular%20data%20proposed%20in%20%5BIwata%20and%0AKumagai%2C%202020%5D.%20The%20proposed%20approach%20employs%20a%20different%20model%20for%20encoding%0Afeature%20relationships%2C%20generating%20alternative%20representations%20compared%20to%0Aexisting%20methods%20like%20Dataset2Vec.%20Both%20of%20them%20leverage%20the%20fundamental%0Aassumption%20of%20dataset%20similarity%20learning.%20In%20this%20work%2C%20we%20evaluate%0ADataset2Vec%20and%20liltab%20on%20two%20common%20meta-tasks%20-%20representing%20entire%20datasets%0Aand%20hyperparameter%20optimization%20warm-start.%20However%2C%20validation%20on%20an%0Aindependent%20metaMIMIC%20dataset%20highlights%20the%20nuanced%20challenges%20in%0Arepresentation%20learning.%20We%20show%20that%20general%20representations%20may%20not%20suffice%0Afor%20some%20meta-tasks%20where%20requirements%20are%20not%20explicitly%20considered%20during%0Aextraction.%0A%20%20%5BIwata%20and%20Kumagai%2C%202020%5D%20Tomoharu%20Iwata%20and%20Atsutoshi%20Kumagai.%20Meta-learning%0Afrom%20Tasks%20with%20Heterogeneous%20Attribute%20Spaces.%20In%20Advances%20in%20Neural%0AInformation%20Processing%20Systems%2C%202020.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04720v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20of%20Encoder-based%20Warm-start%20Methods%20in%20Hyperparameter%0A%20%20Optimization&entry.906535625=Dawid%20P%C5%82udowski%20and%20Antoni%20Zajko%20and%20Anna%20Kozak%20and%20Katarzyna%20Wo%C5%BAnica&entry.1292438233=%20%20Effectively%20representing%20heterogeneous%20tabular%20datasets%20for%20meta-learning%0Aremains%20an%20open%20problem.%20Previous%20approaches%20rely%20on%20predefined%20meta-features%2C%0Afor%20example%2C%20statistical%20measures%20or%20landmarkers.%20Encoder-based%20models%2C%20such%20as%0ADataset2Vec%2C%20allow%20us%20to%20extract%20significant%20meta-features%20automatically%0Awithout%20human%20intervention.%20This%20research%20introduces%20a%20novel%20encoder-based%0Arepresentation%20of%20tabular%20datasets%20implemented%20within%20the%20liltab%20package%0Aavailable%20on%20GitHub%20https%3A//github.com/azoz01/liltab.%20Our%20package%20is%20based%20on%0Aan%20established%20model%20for%20heterogeneous%20tabular%20data%20proposed%20in%20%5BIwata%20and%0AKumagai%2C%202020%5D.%20The%20proposed%20approach%20employs%20a%20different%20model%20for%20encoding%0Afeature%20relationships%2C%20generating%20alternative%20representations%20compared%20to%0Aexisting%20methods%20like%20Dataset2Vec.%20Both%20of%20them%20leverage%20the%20fundamental%0Aassumption%20of%20dataset%20similarity%20learning.%20In%20this%20work%2C%20we%20evaluate%0ADataset2Vec%20and%20liltab%20on%20two%20common%20meta-tasks%20-%20representing%20entire%20datasets%0Aand%20hyperparameter%20optimization%20warm-start.%20However%2C%20validation%20on%20an%0Aindependent%20metaMIMIC%20dataset%20highlights%20the%20nuanced%20challenges%20in%0Arepresentation%20learning.%20We%20show%20that%20general%20representations%20may%20not%20suffice%0Afor%20some%20meta-tasks%20where%20requirements%20are%20not%20explicitly%20considered%20during%0Aextraction.%0A%20%20%5BIwata%20and%20Kumagai%2C%202020%5D%20Tomoharu%20Iwata%20and%20Atsutoshi%20Kumagai.%20Meta-learning%0Afrom%20Tasks%20with%20Heterogeneous%20Attribute%20Spaces.%20In%20Advances%20in%20Neural%0AInformation%20Processing%20Systems%2C%202020.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04720v1&entry.124074799=Read"},
{"title": "Masked Capsule Autoencoders", "author": "Miles Everett and Mingjun Zhong and Georgios Leontidis", "abstract": "  We propose Masked Capsule Autoencoders (MCAE), the first Capsule Network that\nutilises pretraining in a self-supervised manner. Capsule Networks have emerged\nas a powerful alternative to Convolutional Neural Networks (CNNs), and have\nshown favourable properties when compared to Vision Transformers (ViT), but\nhave struggled to effectively learn when presented with more complex data,\nleading to Capsule Network models that do not scale to modern tasks. Our\nproposed MCAE model alleviates this issue by reformulating the Capsule Network\nto use masked image modelling as a pretraining stage before finetuning in a\nsupervised manner. Across several experiments and ablations studies we\ndemonstrate that similarly to CNNs and ViTs, Capsule Networks can also benefit\nfrom self-supervised pretraining, paving the way for further advancements in\nthis neural network domain. For instance, pretraining on the Imagenette\ndataset, a dataset of 10 classes of Imagenet-sized images, we achieve not only\nstate-of-the-art results for Capsule Networks but also a 9% improvement\ncompared to purely supervised training. Thus we propose that Capsule Networks\nbenefit from and should be trained within a masked image modelling framework,\nwith a novel capsule decoder, to improve a Capsule Network's performance on\nrealistic-sized images.\n", "link": "http://arxiv.org/abs/2403.04724v1", "date": "2024-03-07", "relevancy": 2.5245, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5433}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4916}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4797}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Masked%20Capsule%20Autoencoders&body=Title%3A%20Masked%20Capsule%20Autoencoders%0AAuthor%3A%20Miles%20Everett%20and%20Mingjun%20Zhong%20and%20Georgios%20Leontidis%0AAbstract%3A%20%20%20We%20propose%20Masked%20Capsule%20Autoencoders%20%28MCAE%29%2C%20the%20first%20Capsule%20Network%20that%0Autilises%20pretraining%20in%20a%20self-supervised%20manner.%20Capsule%20Networks%20have%20emerged%0Aas%20a%20powerful%20alternative%20to%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20and%20have%0Ashown%20favourable%20properties%20when%20compared%20to%20Vision%20Transformers%20%28ViT%29%2C%20but%0Ahave%20struggled%20to%20effectively%20learn%20when%20presented%20with%20more%20complex%20data%2C%0Aleading%20to%20Capsule%20Network%20models%20that%20do%20not%20scale%20to%20modern%20tasks.%20Our%0Aproposed%20MCAE%20model%20alleviates%20this%20issue%20by%20reformulating%20the%20Capsule%20Network%0Ato%20use%20masked%20image%20modelling%20as%20a%20pretraining%20stage%20before%20finetuning%20in%20a%0Asupervised%20manner.%20Across%20several%20experiments%20and%20ablations%20studies%20we%0Ademonstrate%20that%20similarly%20to%20CNNs%20and%20ViTs%2C%20Capsule%20Networks%20can%20also%20benefit%0Afrom%20self-supervised%20pretraining%2C%20paving%20the%20way%20for%20further%20advancements%20in%0Athis%20neural%20network%20domain.%20For%20instance%2C%20pretraining%20on%20the%20Imagenette%0Adataset%2C%20a%20dataset%20of%2010%20classes%20of%20Imagenet-sized%20images%2C%20we%20achieve%20not%20only%0Astate-of-the-art%20results%20for%20Capsule%20Networks%20but%20also%20a%209%25%20improvement%0Acompared%20to%20purely%20supervised%20training.%20Thus%20we%20propose%20that%20Capsule%20Networks%0Abenefit%20from%20and%20should%20be%20trained%20within%20a%20masked%20image%20modelling%20framework%2C%0Awith%20a%20novel%20capsule%20decoder%2C%20to%20improve%20a%20Capsule%20Network%27s%20performance%20on%0Arealistic-sized%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04724v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Capsule%20Autoencoders&entry.906535625=Miles%20Everett%20and%20Mingjun%20Zhong%20and%20Georgios%20Leontidis&entry.1292438233=%20%20We%20propose%20Masked%20Capsule%20Autoencoders%20%28MCAE%29%2C%20the%20first%20Capsule%20Network%20that%0Autilises%20pretraining%20in%20a%20self-supervised%20manner.%20Capsule%20Networks%20have%20emerged%0Aas%20a%20powerful%20alternative%20to%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20and%20have%0Ashown%20favourable%20properties%20when%20compared%20to%20Vision%20Transformers%20%28ViT%29%2C%20but%0Ahave%20struggled%20to%20effectively%20learn%20when%20presented%20with%20more%20complex%20data%2C%0Aleading%20to%20Capsule%20Network%20models%20that%20do%20not%20scale%20to%20modern%20tasks.%20Our%0Aproposed%20MCAE%20model%20alleviates%20this%20issue%20by%20reformulating%20the%20Capsule%20Network%0Ato%20use%20masked%20image%20modelling%20as%20a%20pretraining%20stage%20before%20finetuning%20in%20a%0Asupervised%20manner.%20Across%20several%20experiments%20and%20ablations%20studies%20we%0Ademonstrate%20that%20similarly%20to%20CNNs%20and%20ViTs%2C%20Capsule%20Networks%20can%20also%20benefit%0Afrom%20self-supervised%20pretraining%2C%20paving%20the%20way%20for%20further%20advancements%20in%0Athis%20neural%20network%20domain.%20For%20instance%2C%20pretraining%20on%20the%20Imagenette%0Adataset%2C%20a%20dataset%20of%2010%20classes%20of%20Imagenet-sized%20images%2C%20we%20achieve%20not%20only%0Astate-of-the-art%20results%20for%20Capsule%20Networks%20but%20also%20a%209%25%20improvement%0Acompared%20to%20purely%20supervised%20training.%20Thus%20we%20propose%20that%20Capsule%20Networks%0Abenefit%20from%20and%20should%20be%20trained%20within%20a%20masked%20image%20modelling%20framework%2C%0Awith%20a%20novel%20capsule%20decoder%2C%20to%20improve%20a%20Capsule%20Network%27s%20performance%20on%0Arealistic-sized%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04724v1&entry.124074799=Read"},
{"title": "Enhancing Court View Generation with Knowledge Injection and Guidance", "author": "Ang Li and Yiquan Wu and Yifei Liu and Fei Wu and Ming Cai and Kun Kuang", "abstract": "  Court View Generation (CVG) is a challenging task in the field of Legal\nArtificial Intelligence (LegalAI), which aims to generate court views based on\nthe plaintiff claims and the fact descriptions. While Pretrained Language\nModels (PLMs) have showcased their prowess in natural language generation,\ntheir application to the complex, knowledge-intensive domain of CVG often\nreveals inherent limitations. In this paper, we present a novel approach, named\nKnowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To\nefficiently incorporate domain knowledge during the training stage, we\nintroduce a knowledge-injected prompt encoder for prompt tuning, thereby\nreducing computational overhead. Moreover, to further enhance the model's\nability to utilize domain knowledge, we employ a generating navigator, which\ndynamically guides the text generation process in the inference stage without\naltering the model's architecture, making it readily transferable.\nComprehensive experiments on real-world data demonstrate the effectiveness of\nour approach compared to several established baselines, especially in the\nresponsivity of claims, where it outperforms the best baseline by 11.87%.\n", "link": "http://arxiv.org/abs/2403.04366v1", "date": "2024-03-07", "relevancy": 2.5166, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5104}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5038}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4957}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Court%20View%20Generation%20with%20Knowledge%20Injection%20and%20Guidance&body=Title%3A%20Enhancing%20Court%20View%20Generation%20with%20Knowledge%20Injection%20and%20Guidance%0AAuthor%3A%20Ang%20Li%20and%20Yiquan%20Wu%20and%20Yifei%20Liu%20and%20Fei%20Wu%20and%20Ming%20Cai%20and%20Kun%20Kuang%0AAbstract%3A%20%20%20Court%20View%20Generation%20%28CVG%29%20is%20a%20challenging%20task%20in%20the%20field%20of%20Legal%0AArtificial%20Intelligence%20%28LegalAI%29%2C%20which%20aims%20to%20generate%20court%20views%20based%20on%0Athe%20plaintiff%20claims%20and%20the%20fact%20descriptions.%20While%20Pretrained%20Language%0AModels%20%28PLMs%29%20have%20showcased%20their%20prowess%20in%20natural%20language%20generation%2C%0Atheir%20application%20to%20the%20complex%2C%20knowledge-intensive%20domain%20of%20CVG%20often%0Areveals%20inherent%20limitations.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%2C%20named%0AKnowledge%20Injection%20and%20Guidance%20%28KIG%29%2C%20designed%20to%20bolster%20CVG%20using%20PLMs.%20To%0Aefficiently%20incorporate%20domain%20knowledge%20during%20the%20training%20stage%2C%20we%0Aintroduce%20a%20knowledge-injected%20prompt%20encoder%20for%20prompt%20tuning%2C%20thereby%0Areducing%20computational%20overhead.%20Moreover%2C%20to%20further%20enhance%20the%20model%27s%0Aability%20to%20utilize%20domain%20knowledge%2C%20we%20employ%20a%20generating%20navigator%2C%20which%0Adynamically%20guides%20the%20text%20generation%20process%20in%20the%20inference%20stage%20without%0Aaltering%20the%20model%27s%20architecture%2C%20making%20it%20readily%20transferable.%0AComprehensive%20experiments%20on%20real-world%20data%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%20compared%20to%20several%20established%20baselines%2C%20especially%20in%20the%0Aresponsivity%20of%20claims%2C%20where%20it%20outperforms%20the%20best%20baseline%20by%2011.87%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04366v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Court%20View%20Generation%20with%20Knowledge%20Injection%20and%20Guidance&entry.906535625=Ang%20Li%20and%20Yiquan%20Wu%20and%20Yifei%20Liu%20and%20Fei%20Wu%20and%20Ming%20Cai%20and%20Kun%20Kuang&entry.1292438233=%20%20Court%20View%20Generation%20%28CVG%29%20is%20a%20challenging%20task%20in%20the%20field%20of%20Legal%0AArtificial%20Intelligence%20%28LegalAI%29%2C%20which%20aims%20to%20generate%20court%20views%20based%20on%0Athe%20plaintiff%20claims%20and%20the%20fact%20descriptions.%20While%20Pretrained%20Language%0AModels%20%28PLMs%29%20have%20showcased%20their%20prowess%20in%20natural%20language%20generation%2C%0Atheir%20application%20to%20the%20complex%2C%20knowledge-intensive%20domain%20of%20CVG%20often%0Areveals%20inherent%20limitations.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%2C%20named%0AKnowledge%20Injection%20and%20Guidance%20%28KIG%29%2C%20designed%20to%20bolster%20CVG%20using%20PLMs.%20To%0Aefficiently%20incorporate%20domain%20knowledge%20during%20the%20training%20stage%2C%20we%0Aintroduce%20a%20knowledge-injected%20prompt%20encoder%20for%20prompt%20tuning%2C%20thereby%0Areducing%20computational%20overhead.%20Moreover%2C%20to%20further%20enhance%20the%20model%27s%0Aability%20to%20utilize%20domain%20knowledge%2C%20we%20employ%20a%20generating%20navigator%2C%20which%0Adynamically%20guides%20the%20text%20generation%20process%20in%20the%20inference%20stage%20without%0Aaltering%20the%20model%27s%20architecture%2C%20making%20it%20readily%20transferable.%0AComprehensive%20experiments%20on%20real-world%20data%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%20compared%20to%20several%20established%20baselines%2C%20especially%20in%20the%0Aresponsivity%20of%20claims%2C%20where%20it%20outperforms%20the%20best%20baseline%20by%2011.87%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04366v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning for Dynamic Algorithm Selection: A\n  Proof-of-Principle Study on Differential Evolution", "author": "Hongshu Guo and Yining Ma and Zeyuan Ma and Jiacheng Chen and Xinglin Zhang and Zhiguang Cao and Jun Zhang and Yue-Jiao Gong", "abstract": "  Evolutionary algorithms, such as Differential Evolution, excel in solving\nreal-parameter optimization challenges. However, the effectiveness of a single\nalgorithm varies across different problem instances, necessitating considerable\nefforts in algorithm selection or configuration. This paper aims to address the\nlimitation by leveraging the complementary strengths of a group of algorithms\nand dynamically scheduling them throughout the optimization progress for\nspecific problems. We propose a deep reinforcement learning-based dynamic\nalgorithm selection framework to accomplish this task. Our approach models the\ndynamic algorithm selection a Markov Decision Process, training an agent in a\npolicy gradient manner to select the most suitable algorithm according to the\nfeatures observed during the optimization process. To empower the agent with\nthe necessary information, our framework incorporates a thoughtful design of\nlandscape and algorithmic features. Meanwhile, we employ a sophisticated deep\nneural network model to infer the optimal action, ensuring informed algorithm\nselections. Additionally, an algorithm context restoration mechanism is\nembedded to facilitate smooth switching among different algorithms. These\nmechanisms together enable our framework to seamlessly select and switch\nalgorithms in a dynamic online fashion. Notably, the proposed framework is\nsimple and generic, offering potential improvements across a broad spectrum of\nevolutionary algorithms. As a proof-of-principle study, we apply this framework\nto a group of Differential Evolution algorithms. The experimental results\nshowcase the remarkable effectiveness of the proposed framework, not only\nenhancing the overall optimization performance but also demonstrating favorable\ngeneralization ability across different problem classes.\n", "link": "http://arxiv.org/abs/2403.02131v3", "date": "2024-03-07", "relevancy": 2.5165, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5453}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4914}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4733}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%20for%20Dynamic%20Algorithm%20Selection%3A%20A%0A%20%20Proof-of-Principle%20Study%20on%20Differential%20Evolution&body=Title%3A%20Deep%20Reinforcement%20Learning%20for%20Dynamic%20Algorithm%20Selection%3A%20A%0A%20%20Proof-of-Principle%20Study%20on%20Differential%20Evolution%0AAuthor%3A%20Hongshu%20Guo%20and%20Yining%20Ma%20and%20Zeyuan%20Ma%20and%20Jiacheng%20Chen%20and%20Xinglin%20Zhang%20and%20Zhiguang%20Cao%20and%20Jun%20Zhang%20and%20Yue-Jiao%20Gong%0AAbstract%3A%20%20%20Evolutionary%20algorithms%2C%20such%20as%20Differential%20Evolution%2C%20excel%20in%20solving%0Areal-parameter%20optimization%20challenges.%20However%2C%20the%20effectiveness%20of%20a%20single%0Aalgorithm%20varies%20across%20different%20problem%20instances%2C%20necessitating%20considerable%0Aefforts%20in%20algorithm%20selection%20or%20configuration.%20This%20paper%20aims%20to%20address%20the%0Alimitation%20by%20leveraging%20the%20complementary%20strengths%20of%20a%20group%20of%20algorithms%0Aand%20dynamically%20scheduling%20them%20throughout%20the%20optimization%20progress%20for%0Aspecific%20problems.%20We%20propose%20a%20deep%20reinforcement%20learning-based%20dynamic%0Aalgorithm%20selection%20framework%20to%20accomplish%20this%20task.%20Our%20approach%20models%20the%0Adynamic%20algorithm%20selection%20a%20Markov%20Decision%20Process%2C%20training%20an%20agent%20in%20a%0Apolicy%20gradient%20manner%20to%20select%20the%20most%20suitable%20algorithm%20according%20to%20the%0Afeatures%20observed%20during%20the%20optimization%20process.%20To%20empower%20the%20agent%20with%0Athe%20necessary%20information%2C%20our%20framework%20incorporates%20a%20thoughtful%20design%20of%0Alandscape%20and%20algorithmic%20features.%20Meanwhile%2C%20we%20employ%20a%20sophisticated%20deep%0Aneural%20network%20model%20to%20infer%20the%20optimal%20action%2C%20ensuring%20informed%20algorithm%0Aselections.%20Additionally%2C%20an%20algorithm%20context%20restoration%20mechanism%20is%0Aembedded%20to%20facilitate%20smooth%20switching%20among%20different%20algorithms.%20These%0Amechanisms%20together%20enable%20our%20framework%20to%20seamlessly%20select%20and%20switch%0Aalgorithms%20in%20a%20dynamic%20online%20fashion.%20Notably%2C%20the%20proposed%20framework%20is%0Asimple%20and%20generic%2C%20offering%20potential%20improvements%20across%20a%20broad%20spectrum%20of%0Aevolutionary%20algorithms.%20As%20a%20proof-of-principle%20study%2C%20we%20apply%20this%20framework%0Ato%20a%20group%20of%20Differential%20Evolution%20algorithms.%20The%20experimental%20results%0Ashowcase%20the%20remarkable%20effectiveness%20of%20the%20proposed%20framework%2C%20not%20only%0Aenhancing%20the%20overall%20optimization%20performance%20but%20also%20demonstrating%20favorable%0Ageneralization%20ability%20across%20different%20problem%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02131v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20for%20Dynamic%20Algorithm%20Selection%3A%20A%0A%20%20Proof-of-Principle%20Study%20on%20Differential%20Evolution&entry.906535625=Hongshu%20Guo%20and%20Yining%20Ma%20and%20Zeyuan%20Ma%20and%20Jiacheng%20Chen%20and%20Xinglin%20Zhang%20and%20Zhiguang%20Cao%20and%20Jun%20Zhang%20and%20Yue-Jiao%20Gong&entry.1292438233=%20%20Evolutionary%20algorithms%2C%20such%20as%20Differential%20Evolution%2C%20excel%20in%20solving%0Areal-parameter%20optimization%20challenges.%20However%2C%20the%20effectiveness%20of%20a%20single%0Aalgorithm%20varies%20across%20different%20problem%20instances%2C%20necessitating%20considerable%0Aefforts%20in%20algorithm%20selection%20or%20configuration.%20This%20paper%20aims%20to%20address%20the%0Alimitation%20by%20leveraging%20the%20complementary%20strengths%20of%20a%20group%20of%20algorithms%0Aand%20dynamically%20scheduling%20them%20throughout%20the%20optimization%20progress%20for%0Aspecific%20problems.%20We%20propose%20a%20deep%20reinforcement%20learning-based%20dynamic%0Aalgorithm%20selection%20framework%20to%20accomplish%20this%20task.%20Our%20approach%20models%20the%0Adynamic%20algorithm%20selection%20a%20Markov%20Decision%20Process%2C%20training%20an%20agent%20in%20a%0Apolicy%20gradient%20manner%20to%20select%20the%20most%20suitable%20algorithm%20according%20to%20the%0Afeatures%20observed%20during%20the%20optimization%20process.%20To%20empower%20the%20agent%20with%0Athe%20necessary%20information%2C%20our%20framework%20incorporates%20a%20thoughtful%20design%20of%0Alandscape%20and%20algorithmic%20features.%20Meanwhile%2C%20we%20employ%20a%20sophisticated%20deep%0Aneural%20network%20model%20to%20infer%20the%20optimal%20action%2C%20ensuring%20informed%20algorithm%0Aselections.%20Additionally%2C%20an%20algorithm%20context%20restoration%20mechanism%20is%0Aembedded%20to%20facilitate%20smooth%20switching%20among%20different%20algorithms.%20These%0Amechanisms%20together%20enable%20our%20framework%20to%20seamlessly%20select%20and%20switch%0Aalgorithms%20in%20a%20dynamic%20online%20fashion.%20Notably%2C%20the%20proposed%20framework%20is%0Asimple%20and%20generic%2C%20offering%20potential%20improvements%20across%20a%20broad%20spectrum%20of%0Aevolutionary%20algorithms.%20As%20a%20proof-of-principle%20study%2C%20we%20apply%20this%20framework%0Ato%20a%20group%20of%20Differential%20Evolution%20algorithms.%20The%20experimental%20results%0Ashowcase%20the%20remarkable%20effectiveness%20of%20the%20proposed%20framework%2C%20not%20only%0Aenhancing%20the%20overall%20optimization%20performance%20but%20also%20demonstrating%20favorable%0Ageneralization%20ability%20across%20different%20problem%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02131v3&entry.124074799=Read"},
{"title": "Active Generalized Category Discovery", "author": "Shijie Ma and Fei Zhu and Zhun Zhong and Xu-Yao Zhang and Cheng-Lin Liu", "abstract": "  Generalized Category Discovery (GCD) is a pragmatic and challenging\nopen-world task, which endeavors to cluster unlabeled samples from both novel\nand old classes, leveraging some labeled data of old classes. Given that\nknowledge learned from old classes is not fully transferable to new classes,\nand that novel categories are fully unlabeled, GCD inherently faces intractable\nproblems, including imbalanced classification performance and inconsistent\nconfidence between old and new classes, especially in the low-labeling regime.\nHence, some annotations of new classes are deemed necessary. However, labeling\nnew classes is extremely costly. To address this issue, we take the spirit of\nactive learning and propose a new setting called Active Generalized Category\nDiscovery (AGCD). The goal is to improve the performance of GCD by actively\nselecting a limited amount of valuable samples for labeling from the oracle. To\nsolve this problem, we devise an adaptive sampling strategy, which jointly\nconsiders novelty, informativeness and diversity to adaptively select novel\nsamples with proper uncertainty. However, owing to the varied orderings of\nlabel indices caused by the clustering of novel classes, the queried labels are\nnot directly applicable to subsequent training. To overcome this issue, we\nfurther propose a stable label mapping algorithm that transforms ground truth\nlabels to the label space of the classifier, thereby ensuring consistent\ntraining across different active selection stages. Our method achieves\nstate-of-the-art performance on both generic and fine-grained datasets. Our\ncode is available at https://github.com/mashijie1028/ActiveGCD\n", "link": "http://arxiv.org/abs/2403.04272v1", "date": "2024-03-07", "relevancy": 2.4817, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5008}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4944}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4938}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Active%20Generalized%20Category%20Discovery&body=Title%3A%20Active%20Generalized%20Category%20Discovery%0AAuthor%3A%20Shijie%20Ma%20and%20Fei%20Zhu%20and%20Zhun%20Zhong%20and%20Xu-Yao%20Zhang%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20Generalized%20Category%20Discovery%20%28GCD%29%20is%20a%20pragmatic%20and%20challenging%0Aopen-world%20task%2C%20which%20endeavors%20to%20cluster%20unlabeled%20samples%20from%20both%20novel%0Aand%20old%20classes%2C%20leveraging%20some%20labeled%20data%20of%20old%20classes.%20Given%20that%0Aknowledge%20learned%20from%20old%20classes%20is%20not%20fully%20transferable%20to%20new%20classes%2C%0Aand%20that%20novel%20categories%20are%20fully%20unlabeled%2C%20GCD%20inherently%20faces%20intractable%0Aproblems%2C%20including%20imbalanced%20classification%20performance%20and%20inconsistent%0Aconfidence%20between%20old%20and%20new%20classes%2C%20especially%20in%20the%20low-labeling%20regime.%0AHence%2C%20some%20annotations%20of%20new%20classes%20are%20deemed%20necessary.%20However%2C%20labeling%0Anew%20classes%20is%20extremely%20costly.%20To%20address%20this%20issue%2C%20we%20take%20the%20spirit%20of%0Aactive%20learning%20and%20propose%20a%20new%20setting%20called%20Active%20Generalized%20Category%0ADiscovery%20%28AGCD%29.%20The%20goal%20is%20to%20improve%20the%20performance%20of%20GCD%20by%20actively%0Aselecting%20a%20limited%20amount%20of%20valuable%20samples%20for%20labeling%20from%20the%20oracle.%20To%0Asolve%20this%20problem%2C%20we%20devise%20an%20adaptive%20sampling%20strategy%2C%20which%20jointly%0Aconsiders%20novelty%2C%20informativeness%20and%20diversity%20to%20adaptively%20select%20novel%0Asamples%20with%20proper%20uncertainty.%20However%2C%20owing%20to%20the%20varied%20orderings%20of%0Alabel%20indices%20caused%20by%20the%20clustering%20of%20novel%20classes%2C%20the%20queried%20labels%20are%0Anot%20directly%20applicable%20to%20subsequent%20training.%20To%20overcome%20this%20issue%2C%20we%0Afurther%20propose%20a%20stable%20label%20mapping%20algorithm%20that%20transforms%20ground%20truth%0Alabels%20to%20the%20label%20space%20of%20the%20classifier%2C%20thereby%20ensuring%20consistent%0Atraining%20across%20different%20active%20selection%20stages.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20on%20both%20generic%20and%20fine-grained%20datasets.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/mashijie1028/ActiveGCD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04272v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Generalized%20Category%20Discovery&entry.906535625=Shijie%20Ma%20and%20Fei%20Zhu%20and%20Zhun%20Zhong%20and%20Xu-Yao%20Zhang%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Generalized%20Category%20Discovery%20%28GCD%29%20is%20a%20pragmatic%20and%20challenging%0Aopen-world%20task%2C%20which%20endeavors%20to%20cluster%20unlabeled%20samples%20from%20both%20novel%0Aand%20old%20classes%2C%20leveraging%20some%20labeled%20data%20of%20old%20classes.%20Given%20that%0Aknowledge%20learned%20from%20old%20classes%20is%20not%20fully%20transferable%20to%20new%20classes%2C%0Aand%20that%20novel%20categories%20are%20fully%20unlabeled%2C%20GCD%20inherently%20faces%20intractable%0Aproblems%2C%20including%20imbalanced%20classification%20performance%20and%20inconsistent%0Aconfidence%20between%20old%20and%20new%20classes%2C%20especially%20in%20the%20low-labeling%20regime.%0AHence%2C%20some%20annotations%20of%20new%20classes%20are%20deemed%20necessary.%20However%2C%20labeling%0Anew%20classes%20is%20extremely%20costly.%20To%20address%20this%20issue%2C%20we%20take%20the%20spirit%20of%0Aactive%20learning%20and%20propose%20a%20new%20setting%20called%20Active%20Generalized%20Category%0ADiscovery%20%28AGCD%29.%20The%20goal%20is%20to%20improve%20the%20performance%20of%20GCD%20by%20actively%0Aselecting%20a%20limited%20amount%20of%20valuable%20samples%20for%20labeling%20from%20the%20oracle.%20To%0Asolve%20this%20problem%2C%20we%20devise%20an%20adaptive%20sampling%20strategy%2C%20which%20jointly%0Aconsiders%20novelty%2C%20informativeness%20and%20diversity%20to%20adaptively%20select%20novel%0Asamples%20with%20proper%20uncertainty.%20However%2C%20owing%20to%20the%20varied%20orderings%20of%0Alabel%20indices%20caused%20by%20the%20clustering%20of%20novel%20classes%2C%20the%20queried%20labels%20are%0Anot%20directly%20applicable%20to%20subsequent%20training.%20To%20overcome%20this%20issue%2C%20we%0Afurther%20propose%20a%20stable%20label%20mapping%20algorithm%20that%20transforms%20ground%20truth%0Alabels%20to%20the%20label%20space%20of%20the%20classifier%2C%20thereby%20ensuring%20consistent%0Atraining%20across%20different%20active%20selection%20stages.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20on%20both%20generic%20and%20fine-grained%20datasets.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/mashijie1028/ActiveGCD%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04272v1&entry.124074799=Read"},
{"title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise,\n  Privacy and OOD Challenges", "author": "Wei Ju and Siyu Yi and Yifan Wang and Zhiping Xiao and Zhengyang Mao and Hourun Li and Yiyang Gu and Yifang Qin and Nan Yin and Senzhang Wang and Xinwang Liu and Xiao Luo and Philip S. Yu and Ming Zhang", "abstract": "  Graph-structured data exhibits universality and widespread applicability\nacross diverse domains, such as social network analysis, biochemistry,\nfinancial fraud detection, and network security. Significant strides have been\nmade in leveraging Graph Neural Networks (GNNs) to achieve remarkable success\nin these areas. However, in real-world scenarios, the training environment for\nmodels is often far from ideal, leading to substantial performance degradation\nof GNN models due to various unfavorable factors, including imbalance in data\ndistribution, the presence of noise in erroneous data, privacy protection of\nsensitive information, and generalization capability for out-of-distribution\n(OOD) scenarios. To tackle these issues, substantial efforts have been devoted\nto improving the performance of GNN models in practical real-world scenarios,\nas well as enhancing their reliability and robustness. In this paper, we\npresent a comprehensive survey that systematically reviews existing GNN models,\nfocusing on solutions to the four mentioned real-world challenges including\nimbalance, noise, privacy, and OOD in practical scenarios that many existing\nreviews have not considered. Specifically, we first highlight the four key\nchallenges faced by existing GNNs, paving the way for our exploration of\nreal-world GNN models. Subsequently, we provide detailed discussions on these\nfour aspects, dissecting how these solutions contribute to enhancing the\nreliability and robustness of GNN models. Last but not least, we outline\npromising directions and offer future perspectives in the field.\n", "link": "http://arxiv.org/abs/2403.04468v1", "date": "2024-03-07", "relevancy": 2.4806, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5032}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4975}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Graph%20Neural%20Networks%20in%20Real%20world%3A%20Imbalance%2C%20Noise%2C%0A%20%20Privacy%20and%20OOD%20Challenges&body=Title%3A%20A%20Survey%20of%20Graph%20Neural%20Networks%20in%20Real%20world%3A%20Imbalance%2C%20Noise%2C%0A%20%20Privacy%20and%20OOD%20Challenges%0AAuthor%3A%20Wei%20Ju%20and%20Siyu%20Yi%20and%20Yifan%20Wang%20and%20Zhiping%20Xiao%20and%20Zhengyang%20Mao%20and%20Hourun%20Li%20and%20Yiyang%20Gu%20and%20Yifang%20Qin%20and%20Nan%20Yin%20and%20Senzhang%20Wang%20and%20Xinwang%20Liu%20and%20Xiao%20Luo%20and%20Philip%20S.%20Yu%20and%20Ming%20Zhang%0AAbstract%3A%20%20%20Graph-structured%20data%20exhibits%20universality%20and%20widespread%20applicability%0Aacross%20diverse%20domains%2C%20such%20as%20social%20network%20analysis%2C%20biochemistry%2C%0Afinancial%20fraud%20detection%2C%20and%20network%20security.%20Significant%20strides%20have%20been%0Amade%20in%20leveraging%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20achieve%20remarkable%20success%0Ain%20these%20areas.%20However%2C%20in%20real-world%20scenarios%2C%20the%20training%20environment%20for%0Amodels%20is%20often%20far%20from%20ideal%2C%20leading%20to%20substantial%20performance%20degradation%0Aof%20GNN%20models%20due%20to%20various%20unfavorable%20factors%2C%20including%20imbalance%20in%20data%0Adistribution%2C%20the%20presence%20of%20noise%20in%20erroneous%20data%2C%20privacy%20protection%20of%0Asensitive%20information%2C%20and%20generalization%20capability%20for%20out-of-distribution%0A%28OOD%29%20scenarios.%20To%20tackle%20these%20issues%2C%20substantial%20efforts%20have%20been%20devoted%0Ato%20improving%20the%20performance%20of%20GNN%20models%20in%20practical%20real-world%20scenarios%2C%0Aas%20well%20as%20enhancing%20their%20reliability%20and%20robustness.%20In%20this%20paper%2C%20we%0Apresent%20a%20comprehensive%20survey%20that%20systematically%20reviews%20existing%20GNN%20models%2C%0Afocusing%20on%20solutions%20to%20the%20four%20mentioned%20real-world%20challenges%20including%0Aimbalance%2C%20noise%2C%20privacy%2C%20and%20OOD%20in%20practical%20scenarios%20that%20many%20existing%0Areviews%20have%20not%20considered.%20Specifically%2C%20we%20first%20highlight%20the%20four%20key%0Achallenges%20faced%20by%20existing%20GNNs%2C%20paving%20the%20way%20for%20our%20exploration%20of%0Areal-world%20GNN%20models.%20Subsequently%2C%20we%20provide%20detailed%20discussions%20on%20these%0Afour%20aspects%2C%20dissecting%20how%20these%20solutions%20contribute%20to%20enhancing%20the%0Areliability%20and%20robustness%20of%20GNN%20models.%20Last%20but%20not%20least%2C%20we%20outline%0Apromising%20directions%20and%20offer%20future%20perspectives%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04468v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Graph%20Neural%20Networks%20in%20Real%20world%3A%20Imbalance%2C%20Noise%2C%0A%20%20Privacy%20and%20OOD%20Challenges&entry.906535625=Wei%20Ju%20and%20Siyu%20Yi%20and%20Yifan%20Wang%20and%20Zhiping%20Xiao%20and%20Zhengyang%20Mao%20and%20Hourun%20Li%20and%20Yiyang%20Gu%20and%20Yifang%20Qin%20and%20Nan%20Yin%20and%20Senzhang%20Wang%20and%20Xinwang%20Liu%20and%20Xiao%20Luo%20and%20Philip%20S.%20Yu%20and%20Ming%20Zhang&entry.1292438233=%20%20Graph-structured%20data%20exhibits%20universality%20and%20widespread%20applicability%0Aacross%20diverse%20domains%2C%20such%20as%20social%20network%20analysis%2C%20biochemistry%2C%0Afinancial%20fraud%20detection%2C%20and%20network%20security.%20Significant%20strides%20have%20been%0Amade%20in%20leveraging%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20achieve%20remarkable%20success%0Ain%20these%20areas.%20However%2C%20in%20real-world%20scenarios%2C%20the%20training%20environment%20for%0Amodels%20is%20often%20far%20from%20ideal%2C%20leading%20to%20substantial%20performance%20degradation%0Aof%20GNN%20models%20due%20to%20various%20unfavorable%20factors%2C%20including%20imbalance%20in%20data%0Adistribution%2C%20the%20presence%20of%20noise%20in%20erroneous%20data%2C%20privacy%20protection%20of%0Asensitive%20information%2C%20and%20generalization%20capability%20for%20out-of-distribution%0A%28OOD%29%20scenarios.%20To%20tackle%20these%20issues%2C%20substantial%20efforts%20have%20been%20devoted%0Ato%20improving%20the%20performance%20of%20GNN%20models%20in%20practical%20real-world%20scenarios%2C%0Aas%20well%20as%20enhancing%20their%20reliability%20and%20robustness.%20In%20this%20paper%2C%20we%0Apresent%20a%20comprehensive%20survey%20that%20systematically%20reviews%20existing%20GNN%20models%2C%0Afocusing%20on%20solutions%20to%20the%20four%20mentioned%20real-world%20challenges%20including%0Aimbalance%2C%20noise%2C%20privacy%2C%20and%20OOD%20in%20practical%20scenarios%20that%20many%20existing%0Areviews%20have%20not%20considered.%20Specifically%2C%20we%20first%20highlight%20the%20four%20key%0Achallenges%20faced%20by%20existing%20GNNs%2C%20paving%20the%20way%20for%20our%20exploration%20of%0Areal-world%20GNN%20models.%20Subsequently%2C%20we%20provide%20detailed%20discussions%20on%20these%0Afour%20aspects%2C%20dissecting%20how%20these%20solutions%20contribute%20to%20enhancing%20the%0Areliability%20and%20robustness%20of%20GNN%20models.%20Last%20but%20not%20least%2C%20we%20outline%0Apromising%20directions%20and%20offer%20future%20perspectives%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04468v1&entry.124074799=Read"},
{"title": "Implicit regularization of multi-task learning and finetuning in\n  overparameterized neural networks", "author": "Jack W. Lindsey and Samuel Lippl", "abstract": "  In this work, we investigate the inductive biases that result from learning\nmultiple tasks, either simultaneously (multi-task learning, MTL) or\nsequentially (pretraining and subsequent finetuning, PT+FT). In the simplified\nsetting of two-layer diagonal linear networks trained with gradient descent, we\napply prior theoretical results to describe novel implicit regularization\npenalties associated with MTL and PT+FT, both of which incentivize feature\nsharing between tasks and sparsity in learned task-specific features. Notably,\nthese results imply that during finetuning, networks operate in a hybrid of the\nkernel (or \"lazy\") regime and the feature learning (\"rich\") regime identified\nin prior work. Moreover, we show that PT+FT can exhibit a novel \"nested feature\nselection\" behavior not captured by either regime, which biases it to extract a\nsparse subset of the features learned during pretraining. In ReLU networks, we\nreproduce all of these qualitative behaviors empirically, in particular\nverifying that analogues of the sparsity biases predicted by the linear theory\nhold in the nonlinear case. Our findings hold qualitatively for a deep\narchitecture trained on image classification tasks, and our characterization of\nthe nested feature selection regime motivates a modification to PT+FT that we\nfind empirically improves performance. We also observe that PT+FT (but not MTL)\nis biased to learn features that are correlated with (but distinct from) those\nneeded for the auxiliary task, while MTL is biased toward using identical\nfeatures for both tasks, which can lead to a tradeoff in performance as a\nfunction of the number of finetuning samples. Our results shed light on the\nimpact of auxiliary task learning and suggest ways to leverage it more\neffectively.\n", "link": "http://arxiv.org/abs/2310.02396v2", "date": "2024-03-07", "relevancy": 2.4779, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4846}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Implicit%20regularization%20of%20multi-task%20learning%20and%20finetuning%20in%0A%20%20overparameterized%20neural%20networks&body=Title%3A%20Implicit%20regularization%20of%20multi-task%20learning%20and%20finetuning%20in%0A%20%20overparameterized%20neural%20networks%0AAuthor%3A%20Jack%20W.%20Lindsey%20and%20Samuel%20Lippl%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20investigate%20the%20inductive%20biases%20that%20result%20from%20learning%0Amultiple%20tasks%2C%20either%20simultaneously%20%28multi-task%20learning%2C%20MTL%29%20or%0Asequentially%20%28pretraining%20and%20subsequent%20finetuning%2C%20PT%2BFT%29.%20In%20the%20simplified%0Asetting%20of%20two-layer%20diagonal%20linear%20networks%20trained%20with%20gradient%20descent%2C%20we%0Aapply%20prior%20theoretical%20results%20to%20describe%20novel%20implicit%20regularization%0Apenalties%20associated%20with%20MTL%20and%20PT%2BFT%2C%20both%20of%20which%20incentivize%20feature%0Asharing%20between%20tasks%20and%20sparsity%20in%20learned%20task-specific%20features.%20Notably%2C%0Athese%20results%20imply%20that%20during%20finetuning%2C%20networks%20operate%20in%20a%20hybrid%20of%20the%0Akernel%20%28or%20%22lazy%22%29%20regime%20and%20the%20feature%20learning%20%28%22rich%22%29%20regime%20identified%0Ain%20prior%20work.%20Moreover%2C%20we%20show%20that%20PT%2BFT%20can%20exhibit%20a%20novel%20%22nested%20feature%0Aselection%22%20behavior%20not%20captured%20by%20either%20regime%2C%20which%20biases%20it%20to%20extract%20a%0Asparse%20subset%20of%20the%20features%20learned%20during%20pretraining.%20In%20ReLU%20networks%2C%20we%0Areproduce%20all%20of%20these%20qualitative%20behaviors%20empirically%2C%20in%20particular%0Averifying%20that%20analogues%20of%20the%20sparsity%20biases%20predicted%20by%20the%20linear%20theory%0Ahold%20in%20the%20nonlinear%20case.%20Our%20findings%20hold%20qualitatively%20for%20a%20deep%0Aarchitecture%20trained%20on%20image%20classification%20tasks%2C%20and%20our%20characterization%20of%0Athe%20nested%20feature%20selection%20regime%20motivates%20a%20modification%20to%20PT%2BFT%20that%20we%0Afind%20empirically%20improves%20performance.%20We%20also%20observe%20that%20PT%2BFT%20%28but%20not%20MTL%29%0Ais%20biased%20to%20learn%20features%20that%20are%20correlated%20with%20%28but%20distinct%20from%29%20those%0Aneeded%20for%20the%20auxiliary%20task%2C%20while%20MTL%20is%20biased%20toward%20using%20identical%0Afeatures%20for%20both%20tasks%2C%20which%20can%20lead%20to%20a%20tradeoff%20in%20performance%20as%20a%0Afunction%20of%20the%20number%20of%20finetuning%20samples.%20Our%20results%20shed%20light%20on%20the%0Aimpact%20of%20auxiliary%20task%20learning%20and%20suggest%20ways%20to%20leverage%20it%20more%0Aeffectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02396v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20regularization%20of%20multi-task%20learning%20and%20finetuning%20in%0A%20%20overparameterized%20neural%20networks&entry.906535625=Jack%20W.%20Lindsey%20and%20Samuel%20Lippl&entry.1292438233=%20%20In%20this%20work%2C%20we%20investigate%20the%20inductive%20biases%20that%20result%20from%20learning%0Amultiple%20tasks%2C%20either%20simultaneously%20%28multi-task%20learning%2C%20MTL%29%20or%0Asequentially%20%28pretraining%20and%20subsequent%20finetuning%2C%20PT%2BFT%29.%20In%20the%20simplified%0Asetting%20of%20two-layer%20diagonal%20linear%20networks%20trained%20with%20gradient%20descent%2C%20we%0Aapply%20prior%20theoretical%20results%20to%20describe%20novel%20implicit%20regularization%0Apenalties%20associated%20with%20MTL%20and%20PT%2BFT%2C%20both%20of%20which%20incentivize%20feature%0Asharing%20between%20tasks%20and%20sparsity%20in%20learned%20task-specific%20features.%20Notably%2C%0Athese%20results%20imply%20that%20during%20finetuning%2C%20networks%20operate%20in%20a%20hybrid%20of%20the%0Akernel%20%28or%20%22lazy%22%29%20regime%20and%20the%20feature%20learning%20%28%22rich%22%29%20regime%20identified%0Ain%20prior%20work.%20Moreover%2C%20we%20show%20that%20PT%2BFT%20can%20exhibit%20a%20novel%20%22nested%20feature%0Aselection%22%20behavior%20not%20captured%20by%20either%20regime%2C%20which%20biases%20it%20to%20extract%20a%0Asparse%20subset%20of%20the%20features%20learned%20during%20pretraining.%20In%20ReLU%20networks%2C%20we%0Areproduce%20all%20of%20these%20qualitative%20behaviors%20empirically%2C%20in%20particular%0Averifying%20that%20analogues%20of%20the%20sparsity%20biases%20predicted%20by%20the%20linear%20theory%0Ahold%20in%20the%20nonlinear%20case.%20Our%20findings%20hold%20qualitatively%20for%20a%20deep%0Aarchitecture%20trained%20on%20image%20classification%20tasks%2C%20and%20our%20characterization%20of%0Athe%20nested%20feature%20selection%20regime%20motivates%20a%20modification%20to%20PT%2BFT%20that%20we%0Afind%20empirically%20improves%20performance.%20We%20also%20observe%20that%20PT%2BFT%20%28but%20not%20MTL%29%0Ais%20biased%20to%20learn%20features%20that%20are%20correlated%20with%20%28but%20distinct%20from%29%20those%0Aneeded%20for%20the%20auxiliary%20task%2C%20while%20MTL%20is%20biased%20toward%20using%20identical%0Afeatures%20for%20both%20tasks%2C%20which%20can%20lead%20to%20a%20tradeoff%20in%20performance%20as%20a%0Afunction%20of%20the%20number%20of%20finetuning%20samples.%20Our%20results%20shed%20light%20on%20the%0Aimpact%20of%20auxiliary%20task%20learning%20and%20suggest%20ways%20to%20leverage%20it%20more%0Aeffectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02396v2&entry.124074799=Read"},
{"title": "Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level", "author": "Ali Hassani and Wen-Mei Hwu and Humphrey Shi", "abstract": "  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\nfirst show that neighborhood attention can be represented as a batched GEMM\nproblem, similar to standard attention, and implement it for 1-D and 2-D\nneighborhood attention. These kernels on average provide 895% and 272%\nimprovement in full precision latency compared to existing naive kernels for\n1-D and 2-D neighborhood attention respectively. We find certain inherent\ninefficiencies in all unfused neighborhood attention kernels that bound their\nperformance and lower-precision scalability. We also developed fused\nneighborhood attention; an adaptation of fused dot-product attention kernels\nthat allow fine-grained control over attention across different spatial axes.\nKnown for reducing the quadratic time complexity of self attention to a linear\ncomplexity, neighborhood attention can now enjoy a reduced and constant memory\nfootprint, and record-breaking half precision latency. We observe that our\nfused kernels successfully circumvent some of the unavoidable inefficiencies in\nunfused implementations. While our unfused GEMM-based kernels only improve half\nprecision performance compared to naive kernels by an average of 496% and 113%\nin 1-D and 2-D problems respectively, our fused kernels improve naive kernels\nby an average of 1607% and 581% in 1-D and 2-D problems respectively.\n", "link": "http://arxiv.org/abs/2403.04690v1", "date": "2024-03-07", "relevancy": 2.47, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5755}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4564}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4501}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Faster%20Neighborhood%20Attention%3A%20Reducing%20the%20O%28n%5E2%29%20Cost%20of%20Self%0A%20%20Attention%20at%20the%20Threadblock%20Level&body=Title%3A%20Faster%20Neighborhood%20Attention%3A%20Reducing%20the%20O%28n%5E2%29%20Cost%20of%20Self%0A%20%20Attention%20at%20the%20Threadblock%20Level%0AAuthor%3A%20Ali%20Hassani%20and%20Wen-Mei%20Hwu%20and%20Humphrey%20Shi%0AAbstract%3A%20%20%20Neighborhood%20attention%20reduces%20the%20cost%20of%20self%20attention%20by%20restricting%20each%0Atoken%27s%20attention%20span%20to%20its%20nearest%20neighbors.%20This%20restriction%2C%0Aparameterized%20by%20a%20window%20size%20and%20dilation%20factor%2C%20draws%20a%20spectrum%20of%0Apossible%20attention%20patterns%20between%20linear%20projection%20and%20self%20attention.%0ANeighborhood%20attention%2C%20and%20more%20generally%20sliding%20window%20attention%20patterns%2C%0Ahave%20long%20been%20bounded%20by%20infrastructure%2C%20particularly%20in%20higher-rank%20spaces%0A%282-D%20and%203-D%29%2C%20calling%20for%20the%20development%20of%20custom%20kernels%2C%20which%20have%20been%0Alimited%20in%20either%20functionality%2C%20or%20performance%2C%20if%20not%20both.%20In%20this%20work%2C%20we%0Afirst%20show%20that%20neighborhood%20attention%20can%20be%20represented%20as%20a%20batched%20GEMM%0Aproblem%2C%20similar%20to%20standard%20attention%2C%20and%20implement%20it%20for%201-D%20and%202-D%0Aneighborhood%20attention.%20These%20kernels%20on%20average%20provide%20895%25%20and%20272%25%0Aimprovement%20in%20full%20precision%20latency%20compared%20to%20existing%20naive%20kernels%20for%0A1-D%20and%202-D%20neighborhood%20attention%20respectively.%20We%20find%20certain%20inherent%0Ainefficiencies%20in%20all%20unfused%20neighborhood%20attention%20kernels%20that%20bound%20their%0Aperformance%20and%20lower-precision%20scalability.%20We%20also%20developed%20fused%0Aneighborhood%20attention%3B%20an%20adaptation%20of%20fused%20dot-product%20attention%20kernels%0Athat%20allow%20fine-grained%20control%20over%20attention%20across%20different%20spatial%20axes.%0AKnown%20for%20reducing%20the%20quadratic%20time%20complexity%20of%20self%20attention%20to%20a%20linear%0Acomplexity%2C%20neighborhood%20attention%20can%20now%20enjoy%20a%20reduced%20and%20constant%20memory%0Afootprint%2C%20and%20record-breaking%20half%20precision%20latency.%20We%20observe%20that%20our%0Afused%20kernels%20successfully%20circumvent%20some%20of%20the%20unavoidable%20inefficiencies%20in%0Aunfused%20implementations.%20While%20our%20unfused%20GEMM-based%20kernels%20only%20improve%20half%0Aprecision%20performance%20compared%20to%20naive%20kernels%20by%20an%20average%20of%20496%25%20and%20113%25%0Ain%201-D%20and%202-D%20problems%20respectively%2C%20our%20fused%20kernels%20improve%20naive%20kernels%0Aby%20an%20average%20of%201607%25%20and%20581%25%20in%201-D%20and%202-D%20problems%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04690v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Neighborhood%20Attention%3A%20Reducing%20the%20O%28n%5E2%29%20Cost%20of%20Self%0A%20%20Attention%20at%20the%20Threadblock%20Level&entry.906535625=Ali%20Hassani%20and%20Wen-Mei%20Hwu%20and%20Humphrey%20Shi&entry.1292438233=%20%20Neighborhood%20attention%20reduces%20the%20cost%20of%20self%20attention%20by%20restricting%20each%0Atoken%27s%20attention%20span%20to%20its%20nearest%20neighbors.%20This%20restriction%2C%0Aparameterized%20by%20a%20window%20size%20and%20dilation%20factor%2C%20draws%20a%20spectrum%20of%0Apossible%20attention%20patterns%20between%20linear%20projection%20and%20self%20attention.%0ANeighborhood%20attention%2C%20and%20more%20generally%20sliding%20window%20attention%20patterns%2C%0Ahave%20long%20been%20bounded%20by%20infrastructure%2C%20particularly%20in%20higher-rank%20spaces%0A%282-D%20and%203-D%29%2C%20calling%20for%20the%20development%20of%20custom%20kernels%2C%20which%20have%20been%0Alimited%20in%20either%20functionality%2C%20or%20performance%2C%20if%20not%20both.%20In%20this%20work%2C%20we%0Afirst%20show%20that%20neighborhood%20attention%20can%20be%20represented%20as%20a%20batched%20GEMM%0Aproblem%2C%20similar%20to%20standard%20attention%2C%20and%20implement%20it%20for%201-D%20and%202-D%0Aneighborhood%20attention.%20These%20kernels%20on%20average%20provide%20895%25%20and%20272%25%0Aimprovement%20in%20full%20precision%20latency%20compared%20to%20existing%20naive%20kernels%20for%0A1-D%20and%202-D%20neighborhood%20attention%20respectively.%20We%20find%20certain%20inherent%0Ainefficiencies%20in%20all%20unfused%20neighborhood%20attention%20kernels%20that%20bound%20their%0Aperformance%20and%20lower-precision%20scalability.%20We%20also%20developed%20fused%0Aneighborhood%20attention%3B%20an%20adaptation%20of%20fused%20dot-product%20attention%20kernels%0Athat%20allow%20fine-grained%20control%20over%20attention%20across%20different%20spatial%20axes.%0AKnown%20for%20reducing%20the%20quadratic%20time%20complexity%20of%20self%20attention%20to%20a%20linear%0Acomplexity%2C%20neighborhood%20attention%20can%20now%20enjoy%20a%20reduced%20and%20constant%20memory%0Afootprint%2C%20and%20record-breaking%20half%20precision%20latency.%20We%20observe%20that%20our%0Afused%20kernels%20successfully%20circumvent%20some%20of%20the%20unavoidable%20inefficiencies%20in%0Aunfused%20implementations.%20While%20our%20unfused%20GEMM-based%20kernels%20only%20improve%20half%0Aprecision%20performance%20compared%20to%20naive%20kernels%20by%20an%20average%20of%20496%25%20and%20113%25%0Ain%201-D%20and%202-D%20problems%20respectively%2C%20our%20fused%20kernels%20improve%20naive%20kernels%0Aby%20an%20average%20of%201607%25%20and%20581%25%20in%201-D%20and%202-D%20problems%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04690v1&entry.124074799=Read"},
{"title": "PixArt-\u03a3: Weak-to-Strong Training of Diffusion Transformer for 4K\n  Text-to-Image Generation", "author": "Junsong Chen and Chongjian Ge and Enze Xie and Yue Wu and Lewei Yao and Xiaozhe Ren and Zhongdao Wang and Ping Luo and Huchuan Lu and Zhenguo Li", "abstract": "  In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer\nmodel~(DiT) capable of directly generating images at 4K resolution.\nPixArt-\\Sigma represents a significant advancement over its predecessor,\nPixArt-\\alpha, offering images of markedly higher fidelity and improved\nalignment with text prompts. A key feature of PixArt-\\Sigma is its training\nefficiency. Leveraging the foundational pre-training of PixArt-\\alpha, it\nevolves from the `weaker' baseline to a `stronger' model via incorporating\nhigher quality data, a process we term \"weak-to-strong training\". The\nadvancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data:\nPixArt-\\Sigma incorporates superior-quality image data, paired with more\nprecise and detailed image captions. (2) Efficient Token Compression: we\npropose a novel attention module within the DiT framework that compresses both\nkeys and values, significantly improving efficiency and facilitating\nultra-high-resolution image generation. Thanks to these improvements,\nPixArt-\\Sigma achieves superior image quality and user prompt adherence\ncapabilities with significantly smaller model size (0.6B parameters) than\nexisting text-to-image diffusion models, such as SDXL (2.6B parameters) and SD\nCascade (5.1B parameters). Moreover, PixArt-\\Sigma's capability to generate 4K\nimages supports the creation of high-resolution posters and wallpapers,\nefficiently bolstering the production of high-quality visual content in\nindustries such as film and gaming.\n", "link": "http://arxiv.org/abs/2403.04692v1", "date": "2024-03-07", "relevancy": 2.4533, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6783}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6184}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5823}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20PixArt-%CE%A3%3A%20Weak-to-Strong%20Training%20of%20Diffusion%20Transformer%20for%204K%0A%20%20Text-to-Image%20Generation&body=Title%3A%20PixArt-%CE%A3%3A%20Weak-to-Strong%20Training%20of%20Diffusion%20Transformer%20for%204K%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Junsong%20Chen%20and%20Chongjian%20Ge%20and%20Enze%20Xie%20and%20Yue%20Wu%20and%20Lewei%20Yao%20and%20Xiaozhe%20Ren%20and%20Zhongdao%20Wang%20and%20Ping%20Luo%20and%20Huchuan%20Lu%20and%20Zhenguo%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20PixArt-%5CSigma%2C%20a%20Diffusion%20Transformer%0Amodel~%28DiT%29%20capable%20of%20directly%20generating%20images%20at%204K%20resolution.%0APixArt-%5CSigma%20represents%20a%20significant%20advancement%20over%20its%20predecessor%2C%0APixArt-%5Calpha%2C%20offering%20images%20of%20markedly%20higher%20fidelity%20and%20improved%0Aalignment%20with%20text%20prompts.%20A%20key%20feature%20of%20PixArt-%5CSigma%20is%20its%20training%0Aefficiency.%20Leveraging%20the%20foundational%20pre-training%20of%20PixArt-%5Calpha%2C%20it%0Aevolves%20from%20the%20%60weaker%27%20baseline%20to%20a%20%60stronger%27%20model%20via%20incorporating%0Ahigher%20quality%20data%2C%20a%20process%20we%20term%20%22weak-to-strong%20training%22.%20The%0Aadvancements%20in%20PixArt-%5CSigma%20are%20twofold%3A%20%281%29%20High-Quality%20Training%20Data%3A%0APixArt-%5CSigma%20incorporates%20superior-quality%20image%20data%2C%20paired%20with%20more%0Aprecise%20and%20detailed%20image%20captions.%20%282%29%20Efficient%20Token%20Compression%3A%20we%0Apropose%20a%20novel%20attention%20module%20within%20the%20DiT%20framework%20that%20compresses%20both%0Akeys%20and%20values%2C%20significantly%20improving%20efficiency%20and%20facilitating%0Aultra-high-resolution%20image%20generation.%20Thanks%20to%20these%20improvements%2C%0APixArt-%5CSigma%20achieves%20superior%20image%20quality%20and%20user%20prompt%20adherence%0Acapabilities%20with%20significantly%20smaller%20model%20size%20%280.6B%20parameters%29%20than%0Aexisting%20text-to-image%20diffusion%20models%2C%20such%20as%20SDXL%20%282.6B%20parameters%29%20and%20SD%0ACascade%20%285.1B%20parameters%29.%20Moreover%2C%20PixArt-%5CSigma%27s%20capability%20to%20generate%204K%0Aimages%20supports%20the%20creation%20of%20high-resolution%20posters%20and%20wallpapers%2C%0Aefficiently%20bolstering%20the%20production%20of%20high-quality%20visual%20content%20in%0Aindustries%20such%20as%20film%20and%20gaming.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04692v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PixArt-%CE%A3%3A%20Weak-to-Strong%20Training%20of%20Diffusion%20Transformer%20for%204K%0A%20%20Text-to-Image%20Generation&entry.906535625=Junsong%20Chen%20and%20Chongjian%20Ge%20and%20Enze%20Xie%20and%20Yue%20Wu%20and%20Lewei%20Yao%20and%20Xiaozhe%20Ren%20and%20Zhongdao%20Wang%20and%20Ping%20Luo%20and%20Huchuan%20Lu%20and%20Zhenguo%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20PixArt-%5CSigma%2C%20a%20Diffusion%20Transformer%0Amodel~%28DiT%29%20capable%20of%20directly%20generating%20images%20at%204K%20resolution.%0APixArt-%5CSigma%20represents%20a%20significant%20advancement%20over%20its%20predecessor%2C%0APixArt-%5Calpha%2C%20offering%20images%20of%20markedly%20higher%20fidelity%20and%20improved%0Aalignment%20with%20text%20prompts.%20A%20key%20feature%20of%20PixArt-%5CSigma%20is%20its%20training%0Aefficiency.%20Leveraging%20the%20foundational%20pre-training%20of%20PixArt-%5Calpha%2C%20it%0Aevolves%20from%20the%20%60weaker%27%20baseline%20to%20a%20%60stronger%27%20model%20via%20incorporating%0Ahigher%20quality%20data%2C%20a%20process%20we%20term%20%22weak-to-strong%20training%22.%20The%0Aadvancements%20in%20PixArt-%5CSigma%20are%20twofold%3A%20%281%29%20High-Quality%20Training%20Data%3A%0APixArt-%5CSigma%20incorporates%20superior-quality%20image%20data%2C%20paired%20with%20more%0Aprecise%20and%20detailed%20image%20captions.%20%282%29%20Efficient%20Token%20Compression%3A%20we%0Apropose%20a%20novel%20attention%20module%20within%20the%20DiT%20framework%20that%20compresses%20both%0Akeys%20and%20values%2C%20significantly%20improving%20efficiency%20and%20facilitating%0Aultra-high-resolution%20image%20generation.%20Thanks%20to%20these%20improvements%2C%0APixArt-%5CSigma%20achieves%20superior%20image%20quality%20and%20user%20prompt%20adherence%0Acapabilities%20with%20significantly%20smaller%20model%20size%20%280.6B%20parameters%29%20than%0Aexisting%20text-to-image%20diffusion%20models%2C%20such%20as%20SDXL%20%282.6B%20parameters%29%20and%20SD%0ACascade%20%285.1B%20parameters%29.%20Moreover%2C%20PixArt-%5CSigma%27s%20capability%20to%20generate%204K%0Aimages%20supports%20the%20creation%20of%20high-resolution%20posters%20and%20wallpapers%2C%0Aefficiently%20bolstering%20the%20production%20of%20high-quality%20visual%20content%20in%0Aindustries%20such%20as%20film%20and%20gaming.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04692v1&entry.124074799=Read"},
{"title": "Tackling the Non-IID Issue in Heterogeneous Federated Learning by\n  Gradient Harmonization", "author": "Xinyu Zhang and Weiyu Sun and Ying Chen", "abstract": "  Federated learning (FL) is a privacy-preserving paradigm for collaboratively\ntraining a global model from decentralized clients. However, the performance of\nFL is hindered by non-independent and identically distributed (non-IID) data\nand device heterogeneity. In this work, we revisit this key challenge through\nthe lens of gradient conflicts on the server side. Specifically, we first\ninvestigate the gradient conflict phenomenon among multiple clients and reveal\nthat stronger heterogeneity leads to more severe gradient conflicts. To tackle\nthis issue, we propose FedGH, a simple yet effective method that mitigates\nlocal drifts through Gradient Harmonization. This technique projects one\ngradient vector onto the orthogonal plane of the other within conflicting\nclient pairs. Extensive experiments demonstrate that FedGH consistently\nenhances multiple state-of-the-art FL baselines across diverse benchmarks and\nnon-IID scenarios. Notably, FedGH yields more significant improvements in\nscenarios with stronger heterogeneity. As a plug-and-play module, FedGH can be\nseamlessly integrated into any FL framework without requiring hyperparameter\ntuning.\n", "link": "http://arxiv.org/abs/2309.06692v2", "date": "2024-03-07", "relevancy": 2.4498, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4914}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4845}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Tackling%20the%20Non-IID%20Issue%20in%20Heterogeneous%20Federated%20Learning%20by%0A%20%20Gradient%20Harmonization&body=Title%3A%20Tackling%20the%20Non-IID%20Issue%20in%20Heterogeneous%20Federated%20Learning%20by%0A%20%20Gradient%20Harmonization%0AAuthor%3A%20Xinyu%20Zhang%20and%20Weiyu%20Sun%20and%20Ying%20Chen%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20privacy-preserving%20paradigm%20for%20collaboratively%0Atraining%20a%20global%20model%20from%20decentralized%20clients.%20However%2C%20the%20performance%20of%0AFL%20is%20hindered%20by%20non-independent%20and%20identically%20distributed%20%28non-IID%29%20data%0Aand%20device%20heterogeneity.%20In%20this%20work%2C%20we%20revisit%20this%20key%20challenge%20through%0Athe%20lens%20of%20gradient%20conflicts%20on%20the%20server%20side.%20Specifically%2C%20we%20first%0Ainvestigate%20the%20gradient%20conflict%20phenomenon%20among%20multiple%20clients%20and%20reveal%0Athat%20stronger%20heterogeneity%20leads%20to%20more%20severe%20gradient%20conflicts.%20To%20tackle%0Athis%20issue%2C%20we%20propose%20FedGH%2C%20a%20simple%20yet%20effective%20method%20that%20mitigates%0Alocal%20drifts%20through%20Gradient%20Harmonization.%20This%20technique%20projects%20one%0Agradient%20vector%20onto%20the%20orthogonal%20plane%20of%20the%20other%20within%20conflicting%0Aclient%20pairs.%20Extensive%20experiments%20demonstrate%20that%20FedGH%20consistently%0Aenhances%20multiple%20state-of-the-art%20FL%20baselines%20across%20diverse%20benchmarks%20and%0Anon-IID%20scenarios.%20Notably%2C%20FedGH%20yields%20more%20significant%20improvements%20in%0Ascenarios%20with%20stronger%20heterogeneity.%20As%20a%20plug-and-play%20module%2C%20FedGH%20can%20be%0Aseamlessly%20integrated%20into%20any%20FL%20framework%20without%20requiring%20hyperparameter%0Atuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06692v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20the%20Non-IID%20Issue%20in%20Heterogeneous%20Federated%20Learning%20by%0A%20%20Gradient%20Harmonization&entry.906535625=Xinyu%20Zhang%20and%20Weiyu%20Sun%20and%20Ying%20Chen&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20privacy-preserving%20paradigm%20for%20collaboratively%0Atraining%20a%20global%20model%20from%20decentralized%20clients.%20However%2C%20the%20performance%20of%0AFL%20is%20hindered%20by%20non-independent%20and%20identically%20distributed%20%28non-IID%29%20data%0Aand%20device%20heterogeneity.%20In%20this%20work%2C%20we%20revisit%20this%20key%20challenge%20through%0Athe%20lens%20of%20gradient%20conflicts%20on%20the%20server%20side.%20Specifically%2C%20we%20first%0Ainvestigate%20the%20gradient%20conflict%20phenomenon%20among%20multiple%20clients%20and%20reveal%0Athat%20stronger%20heterogeneity%20leads%20to%20more%20severe%20gradient%20conflicts.%20To%20tackle%0Athis%20issue%2C%20we%20propose%20FedGH%2C%20a%20simple%20yet%20effective%20method%20that%20mitigates%0Alocal%20drifts%20through%20Gradient%20Harmonization.%20This%20technique%20projects%20one%0Agradient%20vector%20onto%20the%20orthogonal%20plane%20of%20the%20other%20within%20conflicting%0Aclient%20pairs.%20Extensive%20experiments%20demonstrate%20that%20FedGH%20consistently%0Aenhances%20multiple%20state-of-the-art%20FL%20baselines%20across%20diverse%20benchmarks%20and%0Anon-IID%20scenarios.%20Notably%2C%20FedGH%20yields%20more%20significant%20improvements%20in%0Ascenarios%20with%20stronger%20heterogeneity.%20As%20a%20plug-and-play%20module%2C%20FedGH%20can%20be%0Aseamlessly%20integrated%20into%20any%20FL%20framework%20without%20requiring%20hyperparameter%0Atuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06692v2&entry.124074799=Read"},
{"title": "Model-Free Load Frequency Control of Nonlinear Power Systems Based on\n  Deep Reinforcement Learning", "author": "Xiaodi Chen and Meng Zhang and Zhengguang Wu and Ligang Wu and Xiaohong Guan", "abstract": "  Load frequency control (LFC) is widely employed in power systems to stabilize\nfrequency fluctuation and guarantee power quality. However, most existing LFC\nmethods rely on accurate power system modeling and usually ignore the nonlinear\ncharacteristics of the system, limiting controllers' performance. To solve\nthese problems, this paper proposes a model-free LFC method for nonlinear power\nsystems based on deep deterministic policy gradient (DDPG) framework. The\nproposed method establishes an emulator network to emulate power system\ndynamics. After defining the action-value function, the emulator network is\napplied for control actions evaluation instead of the critic network. Then the\nactor network controller is effectively optimized by estimating the policy\ngradient based on zeroth-order optimization (ZOO) and backpropagation\nalgorithm. Simulation results and corresponding comparisons demonstrate the\ndesigned controller can generate appropriate control actions and has strong\nadaptability for nonlinear power systems.\n", "link": "http://arxiv.org/abs/2403.04374v1", "date": "2024-03-07", "relevancy": 2.442, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5211}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4834}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4607}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Model-Free%20Load%20Frequency%20Control%20of%20Nonlinear%20Power%20Systems%20Based%20on%0A%20%20Deep%20Reinforcement%20Learning&body=Title%3A%20Model-Free%20Load%20Frequency%20Control%20of%20Nonlinear%20Power%20Systems%20Based%20on%0A%20%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Xiaodi%20Chen%20and%20Meng%20Zhang%20and%20Zhengguang%20Wu%20and%20Ligang%20Wu%20and%20Xiaohong%20Guan%0AAbstract%3A%20%20%20Load%20frequency%20control%20%28LFC%29%20is%20widely%20employed%20in%20power%20systems%20to%20stabilize%0Afrequency%20fluctuation%20and%20guarantee%20power%20quality.%20However%2C%20most%20existing%20LFC%0Amethods%20rely%20on%20accurate%20power%20system%20modeling%20and%20usually%20ignore%20the%20nonlinear%0Acharacteristics%20of%20the%20system%2C%20limiting%20controllers%27%20performance.%20To%20solve%0Athese%20problems%2C%20this%20paper%20proposes%20a%20model-free%20LFC%20method%20for%20nonlinear%20power%0Asystems%20based%20on%20deep%20deterministic%20policy%20gradient%20%28DDPG%29%20framework.%20The%0Aproposed%20method%20establishes%20an%20emulator%20network%20to%20emulate%20power%20system%0Adynamics.%20After%20defining%20the%20action-value%20function%2C%20the%20emulator%20network%20is%0Aapplied%20for%20control%20actions%20evaluation%20instead%20of%20the%20critic%20network.%20Then%20the%0Aactor%20network%20controller%20is%20effectively%20optimized%20by%20estimating%20the%20policy%0Agradient%20based%20on%20zeroth-order%20optimization%20%28ZOO%29%20and%20backpropagation%0Aalgorithm.%20Simulation%20results%20and%20corresponding%20comparisons%20demonstrate%20the%0Adesigned%20controller%20can%20generate%20appropriate%20control%20actions%20and%20has%20strong%0Aadaptability%20for%20nonlinear%20power%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04374v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-Free%20Load%20Frequency%20Control%20of%20Nonlinear%20Power%20Systems%20Based%20on%0A%20%20Deep%20Reinforcement%20Learning&entry.906535625=Xiaodi%20Chen%20and%20Meng%20Zhang%20and%20Zhengguang%20Wu%20and%20Ligang%20Wu%20and%20Xiaohong%20Guan&entry.1292438233=%20%20Load%20frequency%20control%20%28LFC%29%20is%20widely%20employed%20in%20power%20systems%20to%20stabilize%0Afrequency%20fluctuation%20and%20guarantee%20power%20quality.%20However%2C%20most%20existing%20LFC%0Amethods%20rely%20on%20accurate%20power%20system%20modeling%20and%20usually%20ignore%20the%20nonlinear%0Acharacteristics%20of%20the%20system%2C%20limiting%20controllers%27%20performance.%20To%20solve%0Athese%20problems%2C%20this%20paper%20proposes%20a%20model-free%20LFC%20method%20for%20nonlinear%20power%0Asystems%20based%20on%20deep%20deterministic%20policy%20gradient%20%28DDPG%29%20framework.%20The%0Aproposed%20method%20establishes%20an%20emulator%20network%20to%20emulate%20power%20system%0Adynamics.%20After%20defining%20the%20action-value%20function%2C%20the%20emulator%20network%20is%0Aapplied%20for%20control%20actions%20evaluation%20instead%20of%20the%20critic%20network.%20Then%20the%0Aactor%20network%20controller%20is%20effectively%20optimized%20by%20estimating%20the%20policy%0Agradient%20based%20on%20zeroth-order%20optimization%20%28ZOO%29%20and%20backpropagation%0Aalgorithm.%20Simulation%20results%20and%20corresponding%20comparisons%20demonstrate%20the%0Adesigned%20controller%20can%20generate%20appropriate%20control%20actions%20and%20has%20strong%0Aadaptability%20for%20nonlinear%20power%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04374v1&entry.124074799=Read"},
{"title": "That's My Point: Compact Object-centric LiDAR Pose Estimation for\n  Large-scale Outdoor Localisation", "author": "Georgi Pramatarov and Matthew Gadd and Paul Newman and Daniele De Martini", "abstract": "  This paper is about 3D pose estimation on LiDAR scans with extremely minimal\nstorage requirements to enable scalable mapping and localisation. We achieve\nthis by clustering all points of segmented scans into semantic objects and\nrepresenting them only with their respective centroid and semantic class. In\nthis way, each LiDAR scan is reduced to a compact collection of four-number\nvectors. This abstracts away important structural information from the scenes,\nwhich is crucial for traditional registration approaches. To mitigate this, we\nintroduce an object-matching network based on self- and cross-correlation that\ncaptures geometric and semantic relationships between entities. The respective\nmatches allow us to recover the relative transformation between scans through\nweighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus\n(RANSAC). We demonstrate that such representation is sufficient for metric\nlocalisation by registering point clouds taken under different viewpoints on\nthe KITTI dataset, and at different periods of time localising between KITTI\nand KITTI-360. We achieve accurate metric estimates comparable with\nstate-of-the-art methods with almost half the representation size, specifically\n1.33 kB on average.\n", "link": "http://arxiv.org/abs/2403.04755v1", "date": "2024-03-07", "relevancy": 2.4391, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.634}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6114}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5452}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20That%27s%20My%20Point%3A%20Compact%20Object-centric%20LiDAR%20Pose%20Estimation%20for%0A%20%20Large-scale%20Outdoor%20Localisation&body=Title%3A%20That%27s%20My%20Point%3A%20Compact%20Object-centric%20LiDAR%20Pose%20Estimation%20for%0A%20%20Large-scale%20Outdoor%20Localisation%0AAuthor%3A%20Georgi%20Pramatarov%20and%20Matthew%20Gadd%20and%20Paul%20Newman%20and%20Daniele%20De%20Martini%0AAbstract%3A%20%20%20This%20paper%20is%20about%203D%20pose%20estimation%20on%20LiDAR%20scans%20with%20extremely%20minimal%0Astorage%20requirements%20to%20enable%20scalable%20mapping%20and%20localisation.%20We%20achieve%0Athis%20by%20clustering%20all%20points%20of%20segmented%20scans%20into%20semantic%20objects%20and%0Arepresenting%20them%20only%20with%20their%20respective%20centroid%20and%20semantic%20class.%20In%0Athis%20way%2C%20each%20LiDAR%20scan%20is%20reduced%20to%20a%20compact%20collection%20of%20four-number%0Avectors.%20This%20abstracts%20away%20important%20structural%20information%20from%20the%20scenes%2C%0Awhich%20is%20crucial%20for%20traditional%20registration%20approaches.%20To%20mitigate%20this%2C%20we%0Aintroduce%20an%20object-matching%20network%20based%20on%20self-%20and%20cross-correlation%20that%0Acaptures%20geometric%20and%20semantic%20relationships%20between%20entities.%20The%20respective%0Amatches%20allow%20us%20to%20recover%20the%20relative%20transformation%20between%20scans%20through%0Aweighted%20Singular%20Value%20Decomposition%20%28SVD%29%20and%20RANdom%20SAmple%20Consensus%0A%28RANSAC%29.%20We%20demonstrate%20that%20such%20representation%20is%20sufficient%20for%20metric%0Alocalisation%20by%20registering%20point%20clouds%20taken%20under%20different%20viewpoints%20on%0Athe%20KITTI%20dataset%2C%20and%20at%20different%20periods%20of%20time%20localising%20between%20KITTI%0Aand%20KITTI-360.%20We%20achieve%20accurate%20metric%20estimates%20comparable%20with%0Astate-of-the-art%20methods%20with%20almost%20half%20the%20representation%20size%2C%20specifically%0A1.33%20kB%20on%20average.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04755v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=That%27s%20My%20Point%3A%20Compact%20Object-centric%20LiDAR%20Pose%20Estimation%20for%0A%20%20Large-scale%20Outdoor%20Localisation&entry.906535625=Georgi%20Pramatarov%20and%20Matthew%20Gadd%20and%20Paul%20Newman%20and%20Daniele%20De%20Martini&entry.1292438233=%20%20This%20paper%20is%20about%203D%20pose%20estimation%20on%20LiDAR%20scans%20with%20extremely%20minimal%0Astorage%20requirements%20to%20enable%20scalable%20mapping%20and%20localisation.%20We%20achieve%0Athis%20by%20clustering%20all%20points%20of%20segmented%20scans%20into%20semantic%20objects%20and%0Arepresenting%20them%20only%20with%20their%20respective%20centroid%20and%20semantic%20class.%20In%0Athis%20way%2C%20each%20LiDAR%20scan%20is%20reduced%20to%20a%20compact%20collection%20of%20four-number%0Avectors.%20This%20abstracts%20away%20important%20structural%20information%20from%20the%20scenes%2C%0Awhich%20is%20crucial%20for%20traditional%20registration%20approaches.%20To%20mitigate%20this%2C%20we%0Aintroduce%20an%20object-matching%20network%20based%20on%20self-%20and%20cross-correlation%20that%0Acaptures%20geometric%20and%20semantic%20relationships%20between%20entities.%20The%20respective%0Amatches%20allow%20us%20to%20recover%20the%20relative%20transformation%20between%20scans%20through%0Aweighted%20Singular%20Value%20Decomposition%20%28SVD%29%20and%20RANdom%20SAmple%20Consensus%0A%28RANSAC%29.%20We%20demonstrate%20that%20such%20representation%20is%20sufficient%20for%20metric%0Alocalisation%20by%20registering%20point%20clouds%20taken%20under%20different%20viewpoints%20on%0Athe%20KITTI%20dataset%2C%20and%20at%20different%20periods%20of%20time%20localising%20between%20KITTI%0Aand%20KITTI-360.%20We%20achieve%20accurate%20metric%20estimates%20comparable%20with%0Astate-of-the-art%20methods%20with%20almost%20half%20the%20representation%20size%2C%20specifically%0A1.33%20kB%20on%20average.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04755v1&entry.124074799=Read"},
{"title": "Boosting Fairness and Robustness in Over-the-Air Federated Learning", "author": "Halil Yigit Oksuz and Fabio Molinari and Henning Sprekeler and Joerg Raisch", "abstract": "  Over-the-Air Computation is a beyond-5G communication strategy that has\nrecently been shown to be useful for the decentralized training of machine\nlearning models due to its efficiency. In this paper, we propose an\nOver-the-Air federated learning algorithm that aims to provide fairness and\nrobustness through minmax optimization. By using the epigraph form of the\nproblem at hand, we show that the proposed algorithm converges to the optimal\nsolution of the minmax problem. Moreover, the proposed approach does not\nrequire reconstructing channel coefficients by complex encoding-decoding\nschemes as opposed to state-of-the-art approaches. This improves both\nefficiency and privacy.\n", "link": "http://arxiv.org/abs/2403.04431v1", "date": "2024-03-07", "relevancy": 2.4362, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4961}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4852}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4804}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Boosting%20Fairness%20and%20Robustness%20in%20Over-the-Air%20Federated%20Learning&body=Title%3A%20Boosting%20Fairness%20and%20Robustness%20in%20Over-the-Air%20Federated%20Learning%0AAuthor%3A%20Halil%20Yigit%20Oksuz%20and%20Fabio%20Molinari%20and%20Henning%20Sprekeler%20and%20Joerg%20Raisch%0AAbstract%3A%20%20%20Over-the-Air%20Computation%20is%20a%20beyond-5G%20communication%20strategy%20that%20has%0Arecently%20been%20shown%20to%20be%20useful%20for%20the%20decentralized%20training%20of%20machine%0Alearning%20models%20due%20to%20its%20efficiency.%20In%20this%20paper%2C%20we%20propose%20an%0AOver-the-Air%20federated%20learning%20algorithm%20that%20aims%20to%20provide%20fairness%20and%0Arobustness%20through%20minmax%20optimization.%20By%20using%20the%20epigraph%20form%20of%20the%0Aproblem%20at%20hand%2C%20we%20show%20that%20the%20proposed%20algorithm%20converges%20to%20the%20optimal%0Asolution%20of%20the%20minmax%20problem.%20Moreover%2C%20the%20proposed%20approach%20does%20not%0Arequire%20reconstructing%20channel%20coefficients%20by%20complex%20encoding-decoding%0Aschemes%20as%20opposed%20to%20state-of-the-art%20approaches.%20This%20improves%20both%0Aefficiency%20and%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04431v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Fairness%20and%20Robustness%20in%20Over-the-Air%20Federated%20Learning&entry.906535625=Halil%20Yigit%20Oksuz%20and%20Fabio%20Molinari%20and%20Henning%20Sprekeler%20and%20Joerg%20Raisch&entry.1292438233=%20%20Over-the-Air%20Computation%20is%20a%20beyond-5G%20communication%20strategy%20that%20has%0Arecently%20been%20shown%20to%20be%20useful%20for%20the%20decentralized%20training%20of%20machine%0Alearning%20models%20due%20to%20its%20efficiency.%20In%20this%20paper%2C%20we%20propose%20an%0AOver-the-Air%20federated%20learning%20algorithm%20that%20aims%20to%20provide%20fairness%20and%0Arobustness%20through%20minmax%20optimization.%20By%20using%20the%20epigraph%20form%20of%20the%0Aproblem%20at%20hand%2C%20we%20show%20that%20the%20proposed%20algorithm%20converges%20to%20the%20optimal%0Asolution%20of%20the%20minmax%20problem.%20Moreover%2C%20the%20proposed%20approach%20does%20not%0Arequire%20reconstructing%20channel%20coefficients%20by%20complex%20encoding-decoding%0Aschemes%20as%20opposed%20to%20state-of-the-art%20approaches.%20This%20improves%20both%0Aefficiency%20and%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04431v1&entry.124074799=Read"},
{"title": "Pix2Gif: Motion-Guided Diffusion for GIF Generation", "author": "Hitesh Kandala and Jianfeng Gao and Jianwei Yang", "abstract": "  We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)\ngeneration. We tackle this problem differently by formulating the task as an\nimage translation problem steered by text and motion magnitude prompts, as\nshown in teaser fig. To ensure that the model adheres to motion guidance, we\npropose a new motion-guided warping module to spatially transform the features\nof the source image conditioned on the two types of prompts. Furthermore, we\nintroduce a perceptual loss to ensure the transformed feature map remains\nwithin the same space as the target image, ensuring content consistency and\ncoherence. In preparation for the model training, we meticulously curated data\nby extracting coherent image frames from the TGIF video-caption dataset, which\nprovides rich information about the temporal changes of subjects. After\npretraining, we apply our model in a zero-shot manner to a number of video\ndatasets. Extensive qualitative and quantitative experiments demonstrate the\neffectiveness of our model -- it not only captures the semantic prompt from\ntext but also the spatial ones from motion guidance. We train all our models\nusing a single node of 16xV100 GPUs. Code, dataset and models are made public\nat: https://hiteshk03.github.io/Pix2Gif/.\n", "link": "http://arxiv.org/abs/2403.04634v1", "date": "2024-03-07", "relevancy": 2.4317, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6401}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6215}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5814}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Pix2Gif%3A%20Motion-Guided%20Diffusion%20for%20GIF%20Generation&body=Title%3A%20Pix2Gif%3A%20Motion-Guided%20Diffusion%20for%20GIF%20Generation%0AAuthor%3A%20Hitesh%20Kandala%20and%20Jianfeng%20Gao%20and%20Jianwei%20Yang%0AAbstract%3A%20%20%20We%20present%20Pix2Gif%2C%20a%20motion-guided%20diffusion%20model%20for%20image-to-GIF%20%28video%29%0Ageneration.%20We%20tackle%20this%20problem%20differently%20by%20formulating%20the%20task%20as%20an%0Aimage%20translation%20problem%20steered%20by%20text%20and%20motion%20magnitude%20prompts%2C%20as%0Ashown%20in%20teaser%20fig.%20To%20ensure%20that%20the%20model%20adheres%20to%20motion%20guidance%2C%20we%0Apropose%20a%20new%20motion-guided%20warping%20module%20to%20spatially%20transform%20the%20features%0Aof%20the%20source%20image%20conditioned%20on%20the%20two%20types%20of%20prompts.%20Furthermore%2C%20we%0Aintroduce%20a%20perceptual%20loss%20to%20ensure%20the%20transformed%20feature%20map%20remains%0Awithin%20the%20same%20space%20as%20the%20target%20image%2C%20ensuring%20content%20consistency%20and%0Acoherence.%20In%20preparation%20for%20the%20model%20training%2C%20we%20meticulously%20curated%20data%0Aby%20extracting%20coherent%20image%20frames%20from%20the%20TGIF%20video-caption%20dataset%2C%20which%0Aprovides%20rich%20information%20about%20the%20temporal%20changes%20of%20subjects.%20After%0Apretraining%2C%20we%20apply%20our%20model%20in%20a%20zero-shot%20manner%20to%20a%20number%20of%20video%0Adatasets.%20Extensive%20qualitative%20and%20quantitative%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%20--%20it%20not%20only%20captures%20the%20semantic%20prompt%20from%0Atext%20but%20also%20the%20spatial%20ones%20from%20motion%20guidance.%20We%20train%20all%20our%20models%0Ausing%20a%20single%20node%20of%2016xV100%20GPUs.%20Code%2C%20dataset%20and%20models%20are%20made%20public%0Aat%3A%20https%3A//hiteshk03.github.io/Pix2Gif/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04634v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pix2Gif%3A%20Motion-Guided%20Diffusion%20for%20GIF%20Generation&entry.906535625=Hitesh%20Kandala%20and%20Jianfeng%20Gao%20and%20Jianwei%20Yang&entry.1292438233=%20%20We%20present%20Pix2Gif%2C%20a%20motion-guided%20diffusion%20model%20for%20image-to-GIF%20%28video%29%0Ageneration.%20We%20tackle%20this%20problem%20differently%20by%20formulating%20the%20task%20as%20an%0Aimage%20translation%20problem%20steered%20by%20text%20and%20motion%20magnitude%20prompts%2C%20as%0Ashown%20in%20teaser%20fig.%20To%20ensure%20that%20the%20model%20adheres%20to%20motion%20guidance%2C%20we%0Apropose%20a%20new%20motion-guided%20warping%20module%20to%20spatially%20transform%20the%20features%0Aof%20the%20source%20image%20conditioned%20on%20the%20two%20types%20of%20prompts.%20Furthermore%2C%20we%0Aintroduce%20a%20perceptual%20loss%20to%20ensure%20the%20transformed%20feature%20map%20remains%0Awithin%20the%20same%20space%20as%20the%20target%20image%2C%20ensuring%20content%20consistency%20and%0Acoherence.%20In%20preparation%20for%20the%20model%20training%2C%20we%20meticulously%20curated%20data%0Aby%20extracting%20coherent%20image%20frames%20from%20the%20TGIF%20video-caption%20dataset%2C%20which%0Aprovides%20rich%20information%20about%20the%20temporal%20changes%20of%20subjects.%20After%0Apretraining%2C%20we%20apply%20our%20model%20in%20a%20zero-shot%20manner%20to%20a%20number%20of%20video%0Adatasets.%20Extensive%20qualitative%20and%20quantitative%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%20--%20it%20not%20only%20captures%20the%20semantic%20prompt%20from%0Atext%20but%20also%20the%20spatial%20ones%20from%20motion%20guidance.%20We%20train%20all%20our%20models%0Ausing%20a%20single%20node%20of%2016xV100%20GPUs.%20Code%2C%20dataset%20and%20models%20are%20made%20public%0Aat%3A%20https%3A//hiteshk03.github.io/Pix2Gif/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04634v1&entry.124074799=Read"},
{"title": "On the Topology Awareness and Generalization Performance of Graph Neural\n  Networks", "author": "Junwei Su and Chuan Wu", "abstract": "  Many computer vision and machine learning problems are modelled as learning\ntasks on graphs, where graph neural networks (GNNs) have emerged as a dominant\ntool for learning representations of graph-structured data. A key feature of\nGNNs is their use of graph structures as input, enabling them to exploit the\ngraphs' inherent topological properties-known as the topology awareness of\nGNNs. Despite the empirical successes of GNNs, the influence of topology\nawareness on generalization performance remains unexplored, particularly for\nnode-level tasks that diverge from the assumption of data being independent and\nidentically distributed (I.I.D.). The precise definition and characterization\nof the topology awareness of GNNs, especially concerning different topological\nfeatures, are still unclear. This paper introduces a comprehensive framework to\ncharacterize the topology awareness of GNNs across any topological feature.\nUsing this framework, we investigate the effects of topology awareness on GNN\ngeneralization performance. Contrary to the prevailing belief that enhancing\nthe topology awareness of GNNs is always advantageous, our analysis reveals a\ncritical insight: improving the topology awareness of GNNs may inadvertently\nlead to unfair generalization across structural groups, which might not be\ndesired in some scenarios. Additionally, we conduct a case study using the\nintrinsic graph metric, the shortest path distance, on various benchmark\ndatasets. The empirical results of this case study confirm our theoretical\ninsights. Moreover, we demonstrate the practical applicability of our framework\nby using it to tackle the cold start problem in graph active learning.\n", "link": "http://arxiv.org/abs/2403.04482v1", "date": "2024-03-07", "relevancy": 2.4307, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4931}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4928}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4725}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20On%20the%20Topology%20Awareness%20and%20Generalization%20Performance%20of%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20On%20the%20Topology%20Awareness%20and%20Generalization%20Performance%20of%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Junwei%20Su%20and%20Chuan%20Wu%0AAbstract%3A%20%20%20Many%20computer%20vision%20and%20machine%20learning%20problems%20are%20modelled%20as%20learning%0Atasks%20on%20graphs%2C%20where%20graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20a%20dominant%0Atool%20for%20learning%20representations%20of%20graph-structured%20data.%20A%20key%20feature%20of%0AGNNs%20is%20their%20use%20of%20graph%20structures%20as%20input%2C%20enabling%20them%20to%20exploit%20the%0Agraphs%27%20inherent%20topological%20properties-known%20as%20the%20topology%20awareness%20of%0AGNNs.%20Despite%20the%20empirical%20successes%20of%20GNNs%2C%20the%20influence%20of%20topology%0Aawareness%20on%20generalization%20performance%20remains%20unexplored%2C%20particularly%20for%0Anode-level%20tasks%20that%20diverge%20from%20the%20assumption%20of%20data%20being%20independent%20and%0Aidentically%20distributed%20%28I.I.D.%29.%20The%20precise%20definition%20and%20characterization%0Aof%20the%20topology%20awareness%20of%20GNNs%2C%20especially%20concerning%20different%20topological%0Afeatures%2C%20are%20still%20unclear.%20This%20paper%20introduces%20a%20comprehensive%20framework%20to%0Acharacterize%20the%20topology%20awareness%20of%20GNNs%20across%20any%20topological%20feature.%0AUsing%20this%20framework%2C%20we%20investigate%20the%20effects%20of%20topology%20awareness%20on%20GNN%0Ageneralization%20performance.%20Contrary%20to%20the%20prevailing%20belief%20that%20enhancing%0Athe%20topology%20awareness%20of%20GNNs%20is%20always%20advantageous%2C%20our%20analysis%20reveals%20a%0Acritical%20insight%3A%20improving%20the%20topology%20awareness%20of%20GNNs%20may%20inadvertently%0Alead%20to%20unfair%20generalization%20across%20structural%20groups%2C%20which%20might%20not%20be%0Adesired%20in%20some%20scenarios.%20Additionally%2C%20we%20conduct%20a%20case%20study%20using%20the%0Aintrinsic%20graph%20metric%2C%20the%20shortest%20path%20distance%2C%20on%20various%20benchmark%0Adatasets.%20The%20empirical%20results%20of%20this%20case%20study%20confirm%20our%20theoretical%0Ainsights.%20Moreover%2C%20we%20demonstrate%20the%20practical%20applicability%20of%20our%20framework%0Aby%20using%20it%20to%20tackle%20the%20cold%20start%20problem%20in%20graph%20active%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04482v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Topology%20Awareness%20and%20Generalization%20Performance%20of%20Graph%20Neural%0A%20%20Networks&entry.906535625=Junwei%20Su%20and%20Chuan%20Wu&entry.1292438233=%20%20Many%20computer%20vision%20and%20machine%20learning%20problems%20are%20modelled%20as%20learning%0Atasks%20on%20graphs%2C%20where%20graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20a%20dominant%0Atool%20for%20learning%20representations%20of%20graph-structured%20data.%20A%20key%20feature%20of%0AGNNs%20is%20their%20use%20of%20graph%20structures%20as%20input%2C%20enabling%20them%20to%20exploit%20the%0Agraphs%27%20inherent%20topological%20properties-known%20as%20the%20topology%20awareness%20of%0AGNNs.%20Despite%20the%20empirical%20successes%20of%20GNNs%2C%20the%20influence%20of%20topology%0Aawareness%20on%20generalization%20performance%20remains%20unexplored%2C%20particularly%20for%0Anode-level%20tasks%20that%20diverge%20from%20the%20assumption%20of%20data%20being%20independent%20and%0Aidentically%20distributed%20%28I.I.D.%29.%20The%20precise%20definition%20and%20characterization%0Aof%20the%20topology%20awareness%20of%20GNNs%2C%20especially%20concerning%20different%20topological%0Afeatures%2C%20are%20still%20unclear.%20This%20paper%20introduces%20a%20comprehensive%20framework%20to%0Acharacterize%20the%20topology%20awareness%20of%20GNNs%20across%20any%20topological%20feature.%0AUsing%20this%20framework%2C%20we%20investigate%20the%20effects%20of%20topology%20awareness%20on%20GNN%0Ageneralization%20performance.%20Contrary%20to%20the%20prevailing%20belief%20that%20enhancing%0Athe%20topology%20awareness%20of%20GNNs%20is%20always%20advantageous%2C%20our%20analysis%20reveals%20a%0Acritical%20insight%3A%20improving%20the%20topology%20awareness%20of%20GNNs%20may%20inadvertently%0Alead%20to%20unfair%20generalization%20across%20structural%20groups%2C%20which%20might%20not%20be%0Adesired%20in%20some%20scenarios.%20Additionally%2C%20we%20conduct%20a%20case%20study%20using%20the%0Aintrinsic%20graph%20metric%2C%20the%20shortest%20path%20distance%2C%20on%20various%20benchmark%0Adatasets.%20The%20empirical%20results%20of%20this%20case%20study%20confirm%20our%20theoretical%0Ainsights.%20Moreover%2C%20we%20demonstrate%20the%20practical%20applicability%20of%20our%20framework%0Aby%20using%20it%20to%20tackle%20the%20cold%20start%20problem%20in%20graph%20active%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04482v1&entry.124074799=Read"},
{"title": "The Social Impact of Generative AI: An Analysis on ChatGPT", "author": "Maria T. Baldassarre and Danilo Caivano and Berenice Fernandez Nieto and Domenico Gigante and Azzurra Ragone", "abstract": "  In recent months, the social impact of Artificial Intelligence (AI) has\ngained considerable public interest, driven by the emergence of Generative AI\nmodels, ChatGPT in particular. The rapid development of these models has\nsparked heated discussions regarding their benefits, limitations, and\nassociated risks. Generative models hold immense promise across multiple\ndomains, such as healthcare, finance, and education, to cite a few, presenting\ndiverse practical applications. Nevertheless, concerns about potential adverse\neffects have elicited divergent perspectives, ranging from privacy risks to\nescalating social inequality. This paper adopts a methodology to delve into the\nsocietal implications of Generative AI tools, focusing primarily on the case of\nChatGPT. It evaluates the potential impact on several social sectors and\nillustrates the findings of a comprehensive literature review of both positive\nand negative effects, emerging trends, and areas of opportunity of Generative\nAI models. This analysis aims to facilitate an in-depth discussion by providing\ninsights that can inspire policy, regulation, and responsible development\npractices to foster a human-centered AI.\n", "link": "http://arxiv.org/abs/2403.04667v1", "date": "2024-03-07", "relevancy": 2.416, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5534}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.454}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4423}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20The%20Social%20Impact%20of%20Generative%20AI%3A%20An%20Analysis%20on%20ChatGPT&body=Title%3A%20The%20Social%20Impact%20of%20Generative%20AI%3A%20An%20Analysis%20on%20ChatGPT%0AAuthor%3A%20Maria%20T.%20Baldassarre%20and%20Danilo%20Caivano%20and%20Berenice%20Fernandez%20Nieto%20and%20Domenico%20Gigante%20and%20Azzurra%20Ragone%0AAbstract%3A%20%20%20In%20recent%20months%2C%20the%20social%20impact%20of%20Artificial%20Intelligence%20%28AI%29%20has%0Agained%20considerable%20public%20interest%2C%20driven%20by%20the%20emergence%20of%20Generative%20AI%0Amodels%2C%20ChatGPT%20in%20particular.%20The%20rapid%20development%20of%20these%20models%20has%0Asparked%20heated%20discussions%20regarding%20their%20benefits%2C%20limitations%2C%20and%0Aassociated%20risks.%20Generative%20models%20hold%20immense%20promise%20across%20multiple%0Adomains%2C%20such%20as%20healthcare%2C%20finance%2C%20and%20education%2C%20to%20cite%20a%20few%2C%20presenting%0Adiverse%20practical%20applications.%20Nevertheless%2C%20concerns%20about%20potential%20adverse%0Aeffects%20have%20elicited%20divergent%20perspectives%2C%20ranging%20from%20privacy%20risks%20to%0Aescalating%20social%20inequality.%20This%20paper%20adopts%20a%20methodology%20to%20delve%20into%20the%0Asocietal%20implications%20of%20Generative%20AI%20tools%2C%20focusing%20primarily%20on%20the%20case%20of%0AChatGPT.%20It%20evaluates%20the%20potential%20impact%20on%20several%20social%20sectors%20and%0Aillustrates%20the%20findings%20of%20a%20comprehensive%20literature%20review%20of%20both%20positive%0Aand%20negative%20effects%2C%20emerging%20trends%2C%20and%20areas%20of%20opportunity%20of%20Generative%0AAI%20models.%20This%20analysis%20aims%20to%20facilitate%20an%20in-depth%20discussion%20by%20providing%0Ainsights%20that%20can%20inspire%20policy%2C%20regulation%2C%20and%20responsible%20development%0Apractices%20to%20foster%20a%20human-centered%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04667v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Social%20Impact%20of%20Generative%20AI%3A%20An%20Analysis%20on%20ChatGPT&entry.906535625=Maria%20T.%20Baldassarre%20and%20Danilo%20Caivano%20and%20Berenice%20Fernandez%20Nieto%20and%20Domenico%20Gigante%20and%20Azzurra%20Ragone&entry.1292438233=%20%20In%20recent%20months%2C%20the%20social%20impact%20of%20Artificial%20Intelligence%20%28AI%29%20has%0Agained%20considerable%20public%20interest%2C%20driven%20by%20the%20emergence%20of%20Generative%20AI%0Amodels%2C%20ChatGPT%20in%20particular.%20The%20rapid%20development%20of%20these%20models%20has%0Asparked%20heated%20discussions%20regarding%20their%20benefits%2C%20limitations%2C%20and%0Aassociated%20risks.%20Generative%20models%20hold%20immense%20promise%20across%20multiple%0Adomains%2C%20such%20as%20healthcare%2C%20finance%2C%20and%20education%2C%20to%20cite%20a%20few%2C%20presenting%0Adiverse%20practical%20applications.%20Nevertheless%2C%20concerns%20about%20potential%20adverse%0Aeffects%20have%20elicited%20divergent%20perspectives%2C%20ranging%20from%20privacy%20risks%20to%0Aescalating%20social%20inequality.%20This%20paper%20adopts%20a%20methodology%20to%20delve%20into%20the%0Asocietal%20implications%20of%20Generative%20AI%20tools%2C%20focusing%20primarily%20on%20the%20case%20of%0AChatGPT.%20It%20evaluates%20the%20potential%20impact%20on%20several%20social%20sectors%20and%0Aillustrates%20the%20findings%20of%20a%20comprehensive%20literature%20review%20of%20both%20positive%0Aand%20negative%20effects%2C%20emerging%20trends%2C%20and%20areas%20of%20opportunity%20of%20Generative%0AAI%20models.%20This%20analysis%20aims%20to%20facilitate%20an%20in-depth%20discussion%20by%20providing%0Ainsights%20that%20can%20inspire%20policy%2C%20regulation%2C%20and%20responsible%20development%0Apractices%20to%20foster%20a%20human-centered%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04667v1&entry.124074799=Read"},
{"title": "BloomGML: Graph Machine Learning through the Lens of Bilevel\n  Optimization", "author": "Amber Yijia Zheng and Tong He and Yixuan Qiu and Minjie Wang and David Wipf", "abstract": "  Bilevel optimization refers to scenarios whereby the optimal solution of a\nlower-level energy function serves as input features to an upper-level\nobjective of interest. These optimal features typically depend on tunable\nparameters of the lower-level energy in such a way that the entire bilevel\npipeline can be trained end-to-end. Although not generally presented as such,\nthis paper demonstrates how a variety of graph learning techniques can be\nrecast as special cases of bilevel optimization or simplifications thereof. In\nbrief, building on prior work we first derive a more flexible class of energy\nfunctions that, when paired with various descent steps (e.g., gradient descent,\nproximal methods, momentum, etc.), form graph neural network (GNN)\nmessage-passing layers; critically, we also carefully unpack where any residual\napproximation error lies with respect to the underlying constituent\nmessage-passing functions. We then probe several simplifications of this\nframework to derive close connections with non-GNN-based graph learning\napproaches, including knowledge graph embeddings, various forms of label\npropagation, and efficient graph-regularized MLP models. And finally, we\npresent supporting empirical results that demonstrate the versatility of the\nproposed bilevel lens, which we refer to as BloomGML, referencing that BiLevel\nOptimization Offers More Graph Machine Learning. Our code is available at\nhttps://github.com/amberyzheng/BloomGML. Let graph ML bloom.\n", "link": "http://arxiv.org/abs/2403.04763v1", "date": "2024-03-07", "relevancy": 2.4064, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4872}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.481}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4757}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20BloomGML%3A%20Graph%20Machine%20Learning%20through%20the%20Lens%20of%20Bilevel%0A%20%20Optimization&body=Title%3A%20BloomGML%3A%20Graph%20Machine%20Learning%20through%20the%20Lens%20of%20Bilevel%0A%20%20Optimization%0AAuthor%3A%20Amber%20Yijia%20Zheng%20and%20Tong%20He%20and%20Yixuan%20Qiu%20and%20Minjie%20Wang%20and%20David%20Wipf%0AAbstract%3A%20%20%20Bilevel%20optimization%20refers%20to%20scenarios%20whereby%20the%20optimal%20solution%20of%20a%0Alower-level%20energy%20function%20serves%20as%20input%20features%20to%20an%20upper-level%0Aobjective%20of%20interest.%20These%20optimal%20features%20typically%20depend%20on%20tunable%0Aparameters%20of%20the%20lower-level%20energy%20in%20such%20a%20way%20that%20the%20entire%20bilevel%0Apipeline%20can%20be%20trained%20end-to-end.%20Although%20not%20generally%20presented%20as%20such%2C%0Athis%20paper%20demonstrates%20how%20a%20variety%20of%20graph%20learning%20techniques%20can%20be%0Arecast%20as%20special%20cases%20of%20bilevel%20optimization%20or%20simplifications%20thereof.%20In%0Abrief%2C%20building%20on%20prior%20work%20we%20first%20derive%20a%20more%20flexible%20class%20of%20energy%0Afunctions%20that%2C%20when%20paired%20with%20various%20descent%20steps%20%28e.g.%2C%20gradient%20descent%2C%0Aproximal%20methods%2C%20momentum%2C%20etc.%29%2C%20form%20graph%20neural%20network%20%28GNN%29%0Amessage-passing%20layers%3B%20critically%2C%20we%20also%20carefully%20unpack%20where%20any%20residual%0Aapproximation%20error%20lies%20with%20respect%20to%20the%20underlying%20constituent%0Amessage-passing%20functions.%20We%20then%20probe%20several%20simplifications%20of%20this%0Aframework%20to%20derive%20close%20connections%20with%20non-GNN-based%20graph%20learning%0Aapproaches%2C%20including%20knowledge%20graph%20embeddings%2C%20various%20forms%20of%20label%0Apropagation%2C%20and%20efficient%20graph-regularized%20MLP%20models.%20And%20finally%2C%20we%0Apresent%20supporting%20empirical%20results%20that%20demonstrate%20the%20versatility%20of%20the%0Aproposed%20bilevel%20lens%2C%20which%20we%20refer%20to%20as%20BloomGML%2C%20referencing%20that%20BiLevel%0AOptimization%20Offers%20More%20Graph%20Machine%20Learning.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/amberyzheng/BloomGML.%20Let%20graph%20ML%20bloom.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04763v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BloomGML%3A%20Graph%20Machine%20Learning%20through%20the%20Lens%20of%20Bilevel%0A%20%20Optimization&entry.906535625=Amber%20Yijia%20Zheng%20and%20Tong%20He%20and%20Yixuan%20Qiu%20and%20Minjie%20Wang%20and%20David%20Wipf&entry.1292438233=%20%20Bilevel%20optimization%20refers%20to%20scenarios%20whereby%20the%20optimal%20solution%20of%20a%0Alower-level%20energy%20function%20serves%20as%20input%20features%20to%20an%20upper-level%0Aobjective%20of%20interest.%20These%20optimal%20features%20typically%20depend%20on%20tunable%0Aparameters%20of%20the%20lower-level%20energy%20in%20such%20a%20way%20that%20the%20entire%20bilevel%0Apipeline%20can%20be%20trained%20end-to-end.%20Although%20not%20generally%20presented%20as%20such%2C%0Athis%20paper%20demonstrates%20how%20a%20variety%20of%20graph%20learning%20techniques%20can%20be%0Arecast%20as%20special%20cases%20of%20bilevel%20optimization%20or%20simplifications%20thereof.%20In%0Abrief%2C%20building%20on%20prior%20work%20we%20first%20derive%20a%20more%20flexible%20class%20of%20energy%0Afunctions%20that%2C%20when%20paired%20with%20various%20descent%20steps%20%28e.g.%2C%20gradient%20descent%2C%0Aproximal%20methods%2C%20momentum%2C%20etc.%29%2C%20form%20graph%20neural%20network%20%28GNN%29%0Amessage-passing%20layers%3B%20critically%2C%20we%20also%20carefully%20unpack%20where%20any%20residual%0Aapproximation%20error%20lies%20with%20respect%20to%20the%20underlying%20constituent%0Amessage-passing%20functions.%20We%20then%20probe%20several%20simplifications%20of%20this%0Aframework%20to%20derive%20close%20connections%20with%20non-GNN-based%20graph%20learning%0Aapproaches%2C%20including%20knowledge%20graph%20embeddings%2C%20various%20forms%20of%20label%0Apropagation%2C%20and%20efficient%20graph-regularized%20MLP%20models.%20And%20finally%2C%20we%0Apresent%20supporting%20empirical%20results%20that%20demonstrate%20the%20versatility%20of%20the%0Aproposed%20bilevel%20lens%2C%20which%20we%20refer%20to%20as%20BloomGML%2C%20referencing%20that%20BiLevel%0AOptimization%20Offers%20More%20Graph%20Machine%20Learning.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/amberyzheng/BloomGML.%20Let%20graph%20ML%20bloom.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04763v1&entry.124074799=Read"},
{"title": "Reducing self-supervised learning complexity improves weakly-supervised\n  classification performance in computational pathology", "author": "Tim Lenz and Omar S. M. El Nahhas and Marta Ligero and Jakob Nikolas Kather", "abstract": "  Deep Learning models have been successfully utilized to extract clinically\nactionable insights from routinely available histology data. Generally, these\nmodels require annotations performed by clinicians, which are scarce and costly\nto generate. The emergence of self-supervised learning (SSL) methods remove\nthis barrier, allowing for large-scale analyses on non-annotated data. However,\nrecent SSL approaches apply increasingly expansive model architectures and\nlarger datasets, causing the rapid escalation of data volumes, hardware\nprerequisites, and overall expenses, limiting access to these resources to few\ninstitutions. Therefore, we investigated the complexity of contrastive SSL in\ncomputational pathology in relation to classification performance with the\nutilization of consumer-grade hardware. Specifically, we analyzed the effects\nof adaptations in data volume, architecture, and algorithms on downstream\nclassification tasks, emphasizing their impact on computational resources. We\ntrained breast cancer foundation models on a large public patient cohort and\nvalidated them on various downstream classification tasks in a weakly\nsupervised manner on two external public patient cohorts. Our experiments\ndemonstrate that we can improve downstream classification performance whilst\nreducing SSL training duration by 90%. In summary, we propose a set of\nadaptations which enable the utilization of SSL in computational pathology in\nnon-resource abundant environments.\n", "link": "http://arxiv.org/abs/2403.04558v1", "date": "2024-03-07", "relevancy": 2.4007, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5106}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4799}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4499}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Reducing%20self-supervised%20learning%20complexity%20improves%20weakly-supervised%0A%20%20classification%20performance%20in%20computational%20pathology&body=Title%3A%20Reducing%20self-supervised%20learning%20complexity%20improves%20weakly-supervised%0A%20%20classification%20performance%20in%20computational%20pathology%0AAuthor%3A%20Tim%20Lenz%20and%20Omar%20S.%20M.%20El%20Nahhas%20and%20Marta%20Ligero%20and%20Jakob%20Nikolas%20Kather%0AAbstract%3A%20%20%20Deep%20Learning%20models%20have%20been%20successfully%20utilized%20to%20extract%20clinically%0Aactionable%20insights%20from%20routinely%20available%20histology%20data.%20Generally%2C%20these%0Amodels%20require%20annotations%20performed%20by%20clinicians%2C%20which%20are%20scarce%20and%20costly%0Ato%20generate.%20The%20emergence%20of%20self-supervised%20learning%20%28SSL%29%20methods%20remove%0Athis%20barrier%2C%20allowing%20for%20large-scale%20analyses%20on%20non-annotated%20data.%20However%2C%0Arecent%20SSL%20approaches%20apply%20increasingly%20expansive%20model%20architectures%20and%0Alarger%20datasets%2C%20causing%20the%20rapid%20escalation%20of%20data%20volumes%2C%20hardware%0Aprerequisites%2C%20and%20overall%20expenses%2C%20limiting%20access%20to%20these%20resources%20to%20few%0Ainstitutions.%20Therefore%2C%20we%20investigated%20the%20complexity%20of%20contrastive%20SSL%20in%0Acomputational%20pathology%20in%20relation%20to%20classification%20performance%20with%20the%0Autilization%20of%20consumer-grade%20hardware.%20Specifically%2C%20we%20analyzed%20the%20effects%0Aof%20adaptations%20in%20data%20volume%2C%20architecture%2C%20and%20algorithms%20on%20downstream%0Aclassification%20tasks%2C%20emphasizing%20their%20impact%20on%20computational%20resources.%20We%0Atrained%20breast%20cancer%20foundation%20models%20on%20a%20large%20public%20patient%20cohort%20and%0Avalidated%20them%20on%20various%20downstream%20classification%20tasks%20in%20a%20weakly%0Asupervised%20manner%20on%20two%20external%20public%20patient%20cohorts.%20Our%20experiments%0Ademonstrate%20that%20we%20can%20improve%20downstream%20classification%20performance%20whilst%0Areducing%20SSL%20training%20duration%20by%2090%25.%20In%20summary%2C%20we%20propose%20a%20set%20of%0Aadaptations%20which%20enable%20the%20utilization%20of%20SSL%20in%20computational%20pathology%20in%0Anon-resource%20abundant%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04558v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20self-supervised%20learning%20complexity%20improves%20weakly-supervised%0A%20%20classification%20performance%20in%20computational%20pathology&entry.906535625=Tim%20Lenz%20and%20Omar%20S.%20M.%20El%20Nahhas%20and%20Marta%20Ligero%20and%20Jakob%20Nikolas%20Kather&entry.1292438233=%20%20Deep%20Learning%20models%20have%20been%20successfully%20utilized%20to%20extract%20clinically%0Aactionable%20insights%20from%20routinely%20available%20histology%20data.%20Generally%2C%20these%0Amodels%20require%20annotations%20performed%20by%20clinicians%2C%20which%20are%20scarce%20and%20costly%0Ato%20generate.%20The%20emergence%20of%20self-supervised%20learning%20%28SSL%29%20methods%20remove%0Athis%20barrier%2C%20allowing%20for%20large-scale%20analyses%20on%20non-annotated%20data.%20However%2C%0Arecent%20SSL%20approaches%20apply%20increasingly%20expansive%20model%20architectures%20and%0Alarger%20datasets%2C%20causing%20the%20rapid%20escalation%20of%20data%20volumes%2C%20hardware%0Aprerequisites%2C%20and%20overall%20expenses%2C%20limiting%20access%20to%20these%20resources%20to%20few%0Ainstitutions.%20Therefore%2C%20we%20investigated%20the%20complexity%20of%20contrastive%20SSL%20in%0Acomputational%20pathology%20in%20relation%20to%20classification%20performance%20with%20the%0Autilization%20of%20consumer-grade%20hardware.%20Specifically%2C%20we%20analyzed%20the%20effects%0Aof%20adaptations%20in%20data%20volume%2C%20architecture%2C%20and%20algorithms%20on%20downstream%0Aclassification%20tasks%2C%20emphasizing%20their%20impact%20on%20computational%20resources.%20We%0Atrained%20breast%20cancer%20foundation%20models%20on%20a%20large%20public%20patient%20cohort%20and%0Avalidated%20them%20on%20various%20downstream%20classification%20tasks%20in%20a%20weakly%0Asupervised%20manner%20on%20two%20external%20public%20patient%20cohorts.%20Our%20experiments%0Ademonstrate%20that%20we%20can%20improve%20downstream%20classification%20performance%20whilst%0Areducing%20SSL%20training%20duration%20by%2090%25.%20In%20summary%2C%20we%20propose%20a%20set%20of%0Aadaptations%20which%20enable%20the%20utilization%20of%20SSL%20in%20computational%20pathology%20in%0Anon-resource%20abundant%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04558v1&entry.124074799=Read"},
{"title": "ProFSA: Self-supervised Pocket Pretraining via Protein\n  Fragment-Surroundings Alignment", "author": "Bowen Gao and Yinjun Jia and Yuanle Mo and Yuyan Ni and Weiying Ma and Zhiming Ma and Yanyan Lan", "abstract": "  Pocket representations play a vital role in various biomedical applications,\nsuch as druggability estimation, ligand affinity prediction, and de novo drug\ndesign. While existing geometric features and pretrained representations have\ndemonstrated promising results, they usually treat pockets independent of\nligands, neglecting the fundamental interactions between them. However, the\nlimited pocket-ligand complex structures available in the PDB database (less\nthan 100 thousand non-redundant pairs) hampers large-scale pretraining\nendeavors for interaction modeling. To address this constraint, we propose a\nnovel pocket pretraining approach that leverages knowledge from high-resolution\natomic protein structures, assisted by highly effective pretrained small\nmolecule representations. By segmenting protein structures into drug-like\nfragments and their corresponding pockets, we obtain a reasonable simulation of\nligand-receptor interactions, resulting in the generation of over 5 million\ncomplexes. Subsequently, the pocket encoder is trained in a contrastive manner\nto align with the representation of pseudo-ligand furnished by some pretrained\nsmall molecule encoders. Our method, named ProFSA, achieves state-of-the-art\nperformance across various tasks, including pocket druggability prediction,\npocket matching, and ligand binding affinity prediction. Notably, ProFSA\nsurpasses other pretraining methods by a substantial margin. Moreover, our work\nopens up a new avenue for mitigating the scarcity of protein-ligand complex\ndata through the utilization of high-quality and diverse protein structure\ndatabases.\n", "link": "http://arxiv.org/abs/2310.07229v2", "date": "2024-03-07", "relevancy": 2.3997, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5035}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4549}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20ProFSA%3A%20Self-supervised%20Pocket%20Pretraining%20via%20Protein%0A%20%20Fragment-Surroundings%20Alignment&body=Title%3A%20ProFSA%3A%20Self-supervised%20Pocket%20Pretraining%20via%20Protein%0A%20%20Fragment-Surroundings%20Alignment%0AAuthor%3A%20Bowen%20Gao%20and%20Yinjun%20Jia%20and%20Yuanle%20Mo%20and%20Yuyan%20Ni%20and%20Weiying%20Ma%20and%20Zhiming%20Ma%20and%20Yanyan%20Lan%0AAbstract%3A%20%20%20Pocket%20representations%20play%20a%20vital%20role%20in%20various%20biomedical%20applications%2C%0Asuch%20as%20druggability%20estimation%2C%20ligand%20affinity%20prediction%2C%20and%20de%20novo%20drug%0Adesign.%20While%20existing%20geometric%20features%20and%20pretrained%20representations%20have%0Ademonstrated%20promising%20results%2C%20they%20usually%20treat%20pockets%20independent%20of%0Aligands%2C%20neglecting%20the%20fundamental%20interactions%20between%20them.%20However%2C%20the%0Alimited%20pocket-ligand%20complex%20structures%20available%20in%20the%20PDB%20database%20%28less%0Athan%20100%20thousand%20non-redundant%20pairs%29%20hampers%20large-scale%20pretraining%0Aendeavors%20for%20interaction%20modeling.%20To%20address%20this%20constraint%2C%20we%20propose%20a%0Anovel%20pocket%20pretraining%20approach%20that%20leverages%20knowledge%20from%20high-resolution%0Aatomic%20protein%20structures%2C%20assisted%20by%20highly%20effective%20pretrained%20small%0Amolecule%20representations.%20By%20segmenting%20protein%20structures%20into%20drug-like%0Afragments%20and%20their%20corresponding%20pockets%2C%20we%20obtain%20a%20reasonable%20simulation%20of%0Aligand-receptor%20interactions%2C%20resulting%20in%20the%20generation%20of%20over%205%20million%0Acomplexes.%20Subsequently%2C%20the%20pocket%20encoder%20is%20trained%20in%20a%20contrastive%20manner%0Ato%20align%20with%20the%20representation%20of%20pseudo-ligand%20furnished%20by%20some%20pretrained%0Asmall%20molecule%20encoders.%20Our%20method%2C%20named%20ProFSA%2C%20achieves%20state-of-the-art%0Aperformance%20across%20various%20tasks%2C%20including%20pocket%20druggability%20prediction%2C%0Apocket%20matching%2C%20and%20ligand%20binding%20affinity%20prediction.%20Notably%2C%20ProFSA%0Asurpasses%20other%20pretraining%20methods%20by%20a%20substantial%20margin.%20Moreover%2C%20our%20work%0Aopens%20up%20a%20new%20avenue%20for%20mitigating%20the%20scarcity%20of%20protein-ligand%20complex%0Adata%20through%20the%20utilization%20of%20high-quality%20and%20diverse%20protein%20structure%0Adatabases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07229v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProFSA%3A%20Self-supervised%20Pocket%20Pretraining%20via%20Protein%0A%20%20Fragment-Surroundings%20Alignment&entry.906535625=Bowen%20Gao%20and%20Yinjun%20Jia%20and%20Yuanle%20Mo%20and%20Yuyan%20Ni%20and%20Weiying%20Ma%20and%20Zhiming%20Ma%20and%20Yanyan%20Lan&entry.1292438233=%20%20Pocket%20representations%20play%20a%20vital%20role%20in%20various%20biomedical%20applications%2C%0Asuch%20as%20druggability%20estimation%2C%20ligand%20affinity%20prediction%2C%20and%20de%20novo%20drug%0Adesign.%20While%20existing%20geometric%20features%20and%20pretrained%20representations%20have%0Ademonstrated%20promising%20results%2C%20they%20usually%20treat%20pockets%20independent%20of%0Aligands%2C%20neglecting%20the%20fundamental%20interactions%20between%20them.%20However%2C%20the%0Alimited%20pocket-ligand%20complex%20structures%20available%20in%20the%20PDB%20database%20%28less%0Athan%20100%20thousand%20non-redundant%20pairs%29%20hampers%20large-scale%20pretraining%0Aendeavors%20for%20interaction%20modeling.%20To%20address%20this%20constraint%2C%20we%20propose%20a%0Anovel%20pocket%20pretraining%20approach%20that%20leverages%20knowledge%20from%20high-resolution%0Aatomic%20protein%20structures%2C%20assisted%20by%20highly%20effective%20pretrained%20small%0Amolecule%20representations.%20By%20segmenting%20protein%20structures%20into%20drug-like%0Afragments%20and%20their%20corresponding%20pockets%2C%20we%20obtain%20a%20reasonable%20simulation%20of%0Aligand-receptor%20interactions%2C%20resulting%20in%20the%20generation%20of%20over%205%20million%0Acomplexes.%20Subsequently%2C%20the%20pocket%20encoder%20is%20trained%20in%20a%20contrastive%20manner%0Ato%20align%20with%20the%20representation%20of%20pseudo-ligand%20furnished%20by%20some%20pretrained%0Asmall%20molecule%20encoders.%20Our%20method%2C%20named%20ProFSA%2C%20achieves%20state-of-the-art%0Aperformance%20across%20various%20tasks%2C%20including%20pocket%20druggability%20prediction%2C%0Apocket%20matching%2C%20and%20ligand%20binding%20affinity%20prediction.%20Notably%2C%20ProFSA%0Asurpasses%20other%20pretraining%20methods%20by%20a%20substantial%20margin.%20Moreover%2C%20our%20work%0Aopens%20up%20a%20new%20avenue%20for%20mitigating%20the%20scarcity%20of%20protein-ligand%20complex%0Adata%20through%20the%20utilization%20of%20high-quality%20and%20diverse%20protein%20structure%0Adatabases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07229v2&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning: A Convex Optimization Approach", "author": "Ather Gattami", "abstract": "  In this paper, we consider reinforcement learning of nonlinear systems with\ncontinuous state and action spaces. We present an episodic learning algorithm,\nwhere we for each episode use convex optimization to find a two-layer neural\nnetwork approximation of the optimal $Q$-function. The convex optimization\napproach guarantees that the weights calculated at each episode are optimal,\nwith respect to the given sampled states and actions of the current episode.\nFor stable nonlinear systems, we show that the algorithm converges and that the\nconverging parameters of the trained neural network can be made arbitrarily\nclose to the optimal neural network parameters. In particular, if the\nregularization parameter is $\\rho$ and the time horizon is $T$, then the\nparameters of the trained neural network converge to $w$, where the distance\nbetween $w$ from the optimal parameters $w^\\star$ is bounded by\n$\\mathcal{O}(\\rho T^{-1})$. That is, when the number of episodes goes to\ninfinity, there exists a constant $C$ such that \\[\\|w-w^\\star\\| \\le\nC\\cdot\\frac{\\rho}{T}.\\] In particular, our algorithm converges arbitrarily\nclose to the optimal neural network parameters as the time horizon increases or\nas the regularization parameter decreases.\n", "link": "http://arxiv.org/abs/2402.19212v2", "date": "2024-03-07", "relevancy": 2.3944, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4991}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4849}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4527}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&body=Title%3A%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach%0AAuthor%3A%20Ather%20Gattami%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20is%20%24%5Crho%24%20and%20the%20time%20horizon%20is%20%24T%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20from%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%20T%5E%7B-1%7D%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%0Ainfinity%2C%20there%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%0AC%5Ccdot%5Cfrac%7B%5Crho%7D%7BT%7D.%5C%5D%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters%20as%20the%20time%20horizon%20increases%20or%0Aas%20the%20regularization%20parameter%20decreases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19212v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&entry.906535625=Ather%20Gattami&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20is%20%24%5Crho%24%20and%20the%20time%20horizon%20is%20%24T%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20from%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%20T%5E%7B-1%7D%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%0Ainfinity%2C%20there%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%0AC%5Ccdot%5Cfrac%7B%5Crho%7D%7BT%7D.%5C%5D%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters%20as%20the%20time%20horizon%20increases%20or%0Aas%20the%20regularization%20parameter%20decreases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19212v2&entry.124074799=Read"},
{"title": "Unbiased Estimator for Distorted Conics in Camera Calibration", "author": "Chaehyeon Song and Jaeho Shin and Myung-Hwan Jeon and Jongwoo Lim and Ayoung Kim", "abstract": "  In the literature, points and conics have been major features for camera\ngeometric calibration. Although conics are more informative features than\npoints, the loss of the conic property under distortion has critically limited\nthe utility of conic features in camera calibration. Many existing approaches\naddressed conic-based calibration by ignoring distortion or introducing 3D\nspherical targets to circumvent this limitation. In this paper, we present a\nnovel formulation for conic-based calibration using moments. Our derivation is\nbased on the mathematical finding that the first moment can be estimated\nwithout bias even under distortion. This allows us to track moment changes\nduring projection and distortion, ensuring the preservation of the first moment\nof the distorted conic. With an unbiased estimator, the circular patterns can\nbe accurately detected at the sub-pixel level and can now be fully exploited\nfor an entire calibration pipeline, resulting in significantly improved\ncalibration. The entire code is readily available from\ngithub.com/ChaehyeonSong/discocal.\n", "link": "http://arxiv.org/abs/2403.04583v1", "date": "2024-03-07", "relevancy": 2.3916, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5019}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4846}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4485}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Unbiased%20Estimator%20for%20Distorted%20Conics%20in%20Camera%20Calibration&body=Title%3A%20Unbiased%20Estimator%20for%20Distorted%20Conics%20in%20Camera%20Calibration%0AAuthor%3A%20Chaehyeon%20Song%20and%20Jaeho%20Shin%20and%20Myung-Hwan%20Jeon%20and%20Jongwoo%20Lim%20and%20Ayoung%20Kim%0AAbstract%3A%20%20%20In%20the%20literature%2C%20points%20and%20conics%20have%20been%20major%20features%20for%20camera%0Ageometric%20calibration.%20Although%20conics%20are%20more%20informative%20features%20than%0Apoints%2C%20the%20loss%20of%20the%20conic%20property%20under%20distortion%20has%20critically%20limited%0Athe%20utility%20of%20conic%20features%20in%20camera%20calibration.%20Many%20existing%20approaches%0Aaddressed%20conic-based%20calibration%20by%20ignoring%20distortion%20or%20introducing%203D%0Aspherical%20targets%20to%20circumvent%20this%20limitation.%20In%20this%20paper%2C%20we%20present%20a%0Anovel%20formulation%20for%20conic-based%20calibration%20using%20moments.%20Our%20derivation%20is%0Abased%20on%20the%20mathematical%20finding%20that%20the%20first%20moment%20can%20be%20estimated%0Awithout%20bias%20even%20under%20distortion.%20This%20allows%20us%20to%20track%20moment%20changes%0Aduring%20projection%20and%20distortion%2C%20ensuring%20the%20preservation%20of%20the%20first%20moment%0Aof%20the%20distorted%20conic.%20With%20an%20unbiased%20estimator%2C%20the%20circular%20patterns%20can%0Abe%20accurately%20detected%20at%20the%20sub-pixel%20level%20and%20can%20now%20be%20fully%20exploited%0Afor%20an%20entire%20calibration%20pipeline%2C%20resulting%20in%20significantly%20improved%0Acalibration.%20The%20entire%20code%20is%20readily%20available%20from%0Agithub.com/ChaehyeonSong/discocal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04583v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unbiased%20Estimator%20for%20Distorted%20Conics%20in%20Camera%20Calibration&entry.906535625=Chaehyeon%20Song%20and%20Jaeho%20Shin%20and%20Myung-Hwan%20Jeon%20and%20Jongwoo%20Lim%20and%20Ayoung%20Kim&entry.1292438233=%20%20In%20the%20literature%2C%20points%20and%20conics%20have%20been%20major%20features%20for%20camera%0Ageometric%20calibration.%20Although%20conics%20are%20more%20informative%20features%20than%0Apoints%2C%20the%20loss%20of%20the%20conic%20property%20under%20distortion%20has%20critically%20limited%0Athe%20utility%20of%20conic%20features%20in%20camera%20calibration.%20Many%20existing%20approaches%0Aaddressed%20conic-based%20calibration%20by%20ignoring%20distortion%20or%20introducing%203D%0Aspherical%20targets%20to%20circumvent%20this%20limitation.%20In%20this%20paper%2C%20we%20present%20a%0Anovel%20formulation%20for%20conic-based%20calibration%20using%20moments.%20Our%20derivation%20is%0Abased%20on%20the%20mathematical%20finding%20that%20the%20first%20moment%20can%20be%20estimated%0Awithout%20bias%20even%20under%20distortion.%20This%20allows%20us%20to%20track%20moment%20changes%0Aduring%20projection%20and%20distortion%2C%20ensuring%20the%20preservation%20of%20the%20first%20moment%0Aof%20the%20distorted%20conic.%20With%20an%20unbiased%20estimator%2C%20the%20circular%20patterns%20can%0Abe%20accurately%20detected%20at%20the%20sub-pixel%20level%20and%20can%20now%20be%20fully%20exploited%0Afor%20an%20entire%20calibration%20pipeline%2C%20resulting%20in%20significantly%20improved%0Acalibration.%20The%20entire%20code%20is%20readily%20available%20from%0Agithub.com/ChaehyeonSong/discocal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04583v1&entry.124074799=Read"},
{"title": "LoCoDL: Communication-Efficient Distributed Learning with Local Training\n  and Compression", "author": "Laurent Condat and Artavazd Maranjyan and Peter Richt\u00e1rik", "abstract": "  In Distributed optimization and Learning, and even more in the modern\nframework of federated learning, communication, which is slow and costly, is\ncritical. We introduce LoCoDL, a communication-efficient algorithm that\nleverages the two popular and effective techniques of Local training, which\nreduces the communication frequency, and Compression, in which short bitstreams\nare sent instead of full-dimensional vectors of floats. LoCoDL works with a\nlarge class of unbiased compressors that includes widely-used sparsification\nand quantization methods. LoCoDL provably benefits from local training and\ncompression and enjoys a doubly-accelerated communication complexity, with\nrespect to the condition number of the functions and the model dimension, in\nthe general heterogenous regime with strongly convex functions. This is\nconfirmed in practice, with LoCoDL outperforming existing algorithms.\n", "link": "http://arxiv.org/abs/2403.04348v1", "date": "2024-03-07", "relevancy": 2.3848, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4865}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4738}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4706}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20LoCoDL%3A%20Communication-Efficient%20Distributed%20Learning%20with%20Local%20Training%0A%20%20and%20Compression&body=Title%3A%20LoCoDL%3A%20Communication-Efficient%20Distributed%20Learning%20with%20Local%20Training%0A%20%20and%20Compression%0AAuthor%3A%20Laurent%20Condat%20and%20Artavazd%20Maranjyan%20and%20Peter%20Richt%C3%A1rik%0AAbstract%3A%20%20%20In%20Distributed%20optimization%20and%20Learning%2C%20and%20even%20more%20in%20the%20modern%0Aframework%20of%20federated%20learning%2C%20communication%2C%20which%20is%20slow%20and%20costly%2C%20is%0Acritical.%20We%20introduce%20LoCoDL%2C%20a%20communication-efficient%20algorithm%20that%0Aleverages%20the%20two%20popular%20and%20effective%20techniques%20of%20Local%20training%2C%20which%0Areduces%20the%20communication%20frequency%2C%20and%20Compression%2C%20in%20which%20short%20bitstreams%0Aare%20sent%20instead%20of%20full-dimensional%20vectors%20of%20floats.%20LoCoDL%20works%20with%20a%0Alarge%20class%20of%20unbiased%20compressors%20that%20includes%20widely-used%20sparsification%0Aand%20quantization%20methods.%20LoCoDL%20provably%20benefits%20from%20local%20training%20and%0Acompression%20and%20enjoys%20a%20doubly-accelerated%20communication%20complexity%2C%20with%0Arespect%20to%20the%20condition%20number%20of%20the%20functions%20and%20the%20model%20dimension%2C%20in%0Athe%20general%20heterogenous%20regime%20with%20strongly%20convex%20functions.%20This%20is%0Aconfirmed%20in%20practice%2C%20with%20LoCoDL%20outperforming%20existing%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04348v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoCoDL%3A%20Communication-Efficient%20Distributed%20Learning%20with%20Local%20Training%0A%20%20and%20Compression&entry.906535625=Laurent%20Condat%20and%20Artavazd%20Maranjyan%20and%20Peter%20Richt%C3%A1rik&entry.1292438233=%20%20In%20Distributed%20optimization%20and%20Learning%2C%20and%20even%20more%20in%20the%20modern%0Aframework%20of%20federated%20learning%2C%20communication%2C%20which%20is%20slow%20and%20costly%2C%20is%0Acritical.%20We%20introduce%20LoCoDL%2C%20a%20communication-efficient%20algorithm%20that%0Aleverages%20the%20two%20popular%20and%20effective%20techniques%20of%20Local%20training%2C%20which%0Areduces%20the%20communication%20frequency%2C%20and%20Compression%2C%20in%20which%20short%20bitstreams%0Aare%20sent%20instead%20of%20full-dimensional%20vectors%20of%20floats.%20LoCoDL%20works%20with%20a%0Alarge%20class%20of%20unbiased%20compressors%20that%20includes%20widely-used%20sparsification%0Aand%20quantization%20methods.%20LoCoDL%20provably%20benefits%20from%20local%20training%20and%0Acompression%20and%20enjoys%20a%20doubly-accelerated%20communication%20complexity%2C%20with%0Arespect%20to%20the%20condition%20number%20of%20the%20functions%20and%20the%20model%20dimension%2C%20in%0Athe%20general%20heterogenous%20regime%20with%20strongly%20convex%20functions.%20This%20is%0Aconfirmed%20in%20practice%2C%20with%20LoCoDL%20outperforming%20existing%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04348v1&entry.124074799=Read"},
{"title": "Diffusion Noise Feature: Accurate and Fast Generated Image Detection", "author": "Yichi Zhang and Xiaogang Xu", "abstract": "  Generative models have reached an advanced stage where they can produce\nremarkably realistic images. However, this remarkable generative capability\nalso introduces the risk of disseminating false or misleading information.\nNotably, existing image detectors for generated images encounter challenges\nsuch as low accuracy and limited generalization. This paper seeks to address\nthis issue by seeking a representation with strong generalization capabilities\nto enhance the detection of generated images. Our investigation has revealed\nthat real and generated images display distinct latent Gaussian representations\nwhen subjected to an inverse diffusion process within a pre-trained diffusion\nmodel. Exploiting this disparity, we can amplify subtle artifacts in generated\nimages. Building upon this insight, we introduce a novel image representation\nknown as Diffusion Noise Feature (DNF). DNF is extracted from the estimated\nnoise generated during the inverse diffusion process. A simple classifier,\ne.g., ResNet50, trained on DNF achieves high accuracy, robustness, and\ngeneralization capabilities for detecting generated images (even the\ncorresponding generator is built with datasets/structures that are not seen\nduring the classifier's training). We conducted experiments using four training\ndatasets and five testsets, achieving state-of-the-art detection performance.\n", "link": "http://arxiv.org/abs/2312.02625v2", "date": "2024-03-07", "relevancy": 2.3778, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6321}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5904}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5584}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Noise%20Feature%3A%20Accurate%20and%20Fast%20Generated%20Image%20Detection&body=Title%3A%20Diffusion%20Noise%20Feature%3A%20Accurate%20and%20Fast%20Generated%20Image%20Detection%0AAuthor%3A%20Yichi%20Zhang%20and%20Xiaogang%20Xu%0AAbstract%3A%20%20%20Generative%20models%20have%20reached%20an%20advanced%20stage%20where%20they%20can%20produce%0Aremarkably%20realistic%20images.%20However%2C%20this%20remarkable%20generative%20capability%0Aalso%20introduces%20the%20risk%20of%20disseminating%20false%20or%20misleading%20information.%0ANotably%2C%20existing%20image%20detectors%20for%20generated%20images%20encounter%20challenges%0Asuch%20as%20low%20accuracy%20and%20limited%20generalization.%20This%20paper%20seeks%20to%20address%0Athis%20issue%20by%20seeking%20a%20representation%20with%20strong%20generalization%20capabilities%0Ato%20enhance%20the%20detection%20of%20generated%20images.%20Our%20investigation%20has%20revealed%0Athat%20real%20and%20generated%20images%20display%20distinct%20latent%20Gaussian%20representations%0Awhen%20subjected%20to%20an%20inverse%20diffusion%20process%20within%20a%20pre-trained%20diffusion%0Amodel.%20Exploiting%20this%20disparity%2C%20we%20can%20amplify%20subtle%20artifacts%20in%20generated%0Aimages.%20Building%20upon%20this%20insight%2C%20we%20introduce%20a%20novel%20image%20representation%0Aknown%20as%20Diffusion%20Noise%20Feature%20%28DNF%29.%20DNF%20is%20extracted%20from%20the%20estimated%0Anoise%20generated%20during%20the%20inverse%20diffusion%20process.%20A%20simple%20classifier%2C%0Ae.g.%2C%20ResNet50%2C%20trained%20on%20DNF%20achieves%20high%20accuracy%2C%20robustness%2C%20and%0Ageneralization%20capabilities%20for%20detecting%20generated%20images%20%28even%20the%0Acorresponding%20generator%20is%20built%20with%20datasets/structures%20that%20are%20not%20seen%0Aduring%20the%20classifier%27s%20training%29.%20We%20conducted%20experiments%20using%20four%20training%0Adatasets%20and%20five%20testsets%2C%20achieving%20state-of-the-art%20detection%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02625v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Noise%20Feature%3A%20Accurate%20and%20Fast%20Generated%20Image%20Detection&entry.906535625=Yichi%20Zhang%20and%20Xiaogang%20Xu&entry.1292438233=%20%20Generative%20models%20have%20reached%20an%20advanced%20stage%20where%20they%20can%20produce%0Aremarkably%20realistic%20images.%20However%2C%20this%20remarkable%20generative%20capability%0Aalso%20introduces%20the%20risk%20of%20disseminating%20false%20or%20misleading%20information.%0ANotably%2C%20existing%20image%20detectors%20for%20generated%20images%20encounter%20challenges%0Asuch%20as%20low%20accuracy%20and%20limited%20generalization.%20This%20paper%20seeks%20to%20address%0Athis%20issue%20by%20seeking%20a%20representation%20with%20strong%20generalization%20capabilities%0Ato%20enhance%20the%20detection%20of%20generated%20images.%20Our%20investigation%20has%20revealed%0Athat%20real%20and%20generated%20images%20display%20distinct%20latent%20Gaussian%20representations%0Awhen%20subjected%20to%20an%20inverse%20diffusion%20process%20within%20a%20pre-trained%20diffusion%0Amodel.%20Exploiting%20this%20disparity%2C%20we%20can%20amplify%20subtle%20artifacts%20in%20generated%0Aimages.%20Building%20upon%20this%20insight%2C%20we%20introduce%20a%20novel%20image%20representation%0Aknown%20as%20Diffusion%20Noise%20Feature%20%28DNF%29.%20DNF%20is%20extracted%20from%20the%20estimated%0Anoise%20generated%20during%20the%20inverse%20diffusion%20process.%20A%20simple%20classifier%2C%0Ae.g.%2C%20ResNet50%2C%20trained%20on%20DNF%20achieves%20high%20accuracy%2C%20robustness%2C%20and%0Ageneralization%20capabilities%20for%20detecting%20generated%20images%20%28even%20the%0Acorresponding%20generator%20is%20built%20with%20datasets/structures%20that%20are%20not%20seen%0Aduring%20the%20classifier%27s%20training%29.%20We%20conducted%20experiments%20using%20four%20training%0Adatasets%20and%20five%20testsets%2C%20achieving%20state-of-the-art%20detection%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02625v2&entry.124074799=Read"},
{"title": "OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable\n  Virtual Try-on", "author": "Yuhao Xu and Tao Gu and Weifeng Chen and Chengcai Chen", "abstract": "  We present OOTDiffusion, a novel network architecture for realistic and\ncontrollable image-based virtual try-on (VTON). We leverage the power of\npretrained latent diffusion models, designing an outfitting UNet to learn the\ngarment detail features. Without a redundant warping process, the garment\nfeatures are precisely aligned with the target human body via the proposed\noutfitting fusion in the self-attention layers of the denoising UNet. In order\nto further enhance the controllability, we introduce outfitting dropout to the\ntraining process, which enables us to adjust the strength of the garment\nfeatures through classifier-free guidance. Our comprehensive experiments on the\nVITON-HD and Dress Code datasets demonstrate that OOTDiffusion efficiently\ngenerates high-quality try-on results for arbitrary human and garment images,\nwhich outperforms other VTON methods in both realism and controllability,\nindicating an impressive breakthrough in virtual try-on. Our source code is\navailable at https://github.com/levihsu/OOTDiffusion.\n", "link": "http://arxiv.org/abs/2403.01779v2", "date": "2024-03-07", "relevancy": 2.3709, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6434}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5908}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5744}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20OOTDiffusion%3A%20Outfitting%20Fusion%20based%20Latent%20Diffusion%20for%20Controllable%0A%20%20Virtual%20Try-on&body=Title%3A%20OOTDiffusion%3A%20Outfitting%20Fusion%20based%20Latent%20Diffusion%20for%20Controllable%0A%20%20Virtual%20Try-on%0AAuthor%3A%20Yuhao%20Xu%20and%20Tao%20Gu%20and%20Weifeng%20Chen%20and%20Chengcai%20Chen%0AAbstract%3A%20%20%20We%20present%20OOTDiffusion%2C%20a%20novel%20network%20architecture%20for%20realistic%20and%0Acontrollable%20image-based%20virtual%20try-on%20%28VTON%29.%20We%20leverage%20the%20power%20of%0Apretrained%20latent%20diffusion%20models%2C%20designing%20an%20outfitting%20UNet%20to%20learn%20the%0Agarment%20detail%20features.%20Without%20a%20redundant%20warping%20process%2C%20the%20garment%0Afeatures%20are%20precisely%20aligned%20with%20the%20target%20human%20body%20via%20the%20proposed%0Aoutfitting%20fusion%20in%20the%20self-attention%20layers%20of%20the%20denoising%20UNet.%20In%20order%0Ato%20further%20enhance%20the%20controllability%2C%20we%20introduce%20outfitting%20dropout%20to%20the%0Atraining%20process%2C%20which%20enables%20us%20to%20adjust%20the%20strength%20of%20the%20garment%0Afeatures%20through%20classifier-free%20guidance.%20Our%20comprehensive%20experiments%20on%20the%0AVITON-HD%20and%20Dress%20Code%20datasets%20demonstrate%20that%20OOTDiffusion%20efficiently%0Agenerates%20high-quality%20try-on%20results%20for%20arbitrary%20human%20and%20garment%20images%2C%0Awhich%20outperforms%20other%20VTON%20methods%20in%20both%20realism%20and%20controllability%2C%0Aindicating%20an%20impressive%20breakthrough%20in%20virtual%20try-on.%20Our%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/levihsu/OOTDiffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01779v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OOTDiffusion%3A%20Outfitting%20Fusion%20based%20Latent%20Diffusion%20for%20Controllable%0A%20%20Virtual%20Try-on&entry.906535625=Yuhao%20Xu%20and%20Tao%20Gu%20and%20Weifeng%20Chen%20and%20Chengcai%20Chen&entry.1292438233=%20%20We%20present%20OOTDiffusion%2C%20a%20novel%20network%20architecture%20for%20realistic%20and%0Acontrollable%20image-based%20virtual%20try-on%20%28VTON%29.%20We%20leverage%20the%20power%20of%0Apretrained%20latent%20diffusion%20models%2C%20designing%20an%20outfitting%20UNet%20to%20learn%20the%0Agarment%20detail%20features.%20Without%20a%20redundant%20warping%20process%2C%20the%20garment%0Afeatures%20are%20precisely%20aligned%20with%20the%20target%20human%20body%20via%20the%20proposed%0Aoutfitting%20fusion%20in%20the%20self-attention%20layers%20of%20the%20denoising%20UNet.%20In%20order%0Ato%20further%20enhance%20the%20controllability%2C%20we%20introduce%20outfitting%20dropout%20to%20the%0Atraining%20process%2C%20which%20enables%20us%20to%20adjust%20the%20strength%20of%20the%20garment%0Afeatures%20through%20classifier-free%20guidance.%20Our%20comprehensive%20experiments%20on%20the%0AVITON-HD%20and%20Dress%20Code%20datasets%20demonstrate%20that%20OOTDiffusion%20efficiently%0Agenerates%20high-quality%20try-on%20results%20for%20arbitrary%20human%20and%20garment%20images%2C%0Awhich%20outperforms%20other%20VTON%20methods%20in%20both%20realism%20and%20controllability%2C%0Aindicating%20an%20impressive%20breakthrough%20in%20virtual%20try-on.%20Our%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/levihsu/OOTDiffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01779v2&entry.124074799=Read"},
{"title": "Federated Recommendation via Hybrid Retrieval Augmented Generation", "author": "Huimin Zeng and Zhenrui Yue and Qian Jiang and Dong Wang", "abstract": "  Federated Recommendation (FR) emerges as a novel paradigm that enables\nprivacy-preserving recommendations. However, traditional FR systems usually\nrepresent users/items with discrete identities (IDs), suffering from\nperformance degradation due to the data sparsity and heterogeneity in FR. On\nthe other hand, Large Language Models (LLMs) as recommenders have proven\neffective across various recommendation scenarios. Yet, LLM-based recommenders\nencounter challenges such as low inference efficiency and potential\nhallucination, compromising their performance in real-world scenarios. To this\nend, we propose GPT-FedRec, a federated recommendation framework leveraging\nChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.\nGPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval\nprocess, mining ID-based user patterns and text-based item features. Next, the\nretrieved results are converted into text prompts and fed into GPT for\nre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims\nto extract generalized features from data and exploit pretrained knowledge\nwithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, the\nRAG approach also prevents LLM hallucination, improving the recommendation\nperformance for real-world users. Experimental results on diverse benchmark\ndatasets demonstrate the superior performance of GPT-FedRec against\nstate-of-the-art baseline methods.\n", "link": "http://arxiv.org/abs/2403.04256v1", "date": "2024-03-07", "relevancy": 2.3704, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4904}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4671}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4648}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Federated%20Recommendation%20via%20Hybrid%20Retrieval%20Augmented%20Generation&body=Title%3A%20Federated%20Recommendation%20via%20Hybrid%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Huimin%20Zeng%20and%20Zhenrui%20Yue%20and%20Qian%20Jiang%20and%20Dong%20Wang%0AAbstract%3A%20%20%20Federated%20Recommendation%20%28FR%29%20emerges%20as%20a%20novel%20paradigm%20that%20enables%0Aprivacy-preserving%20recommendations.%20However%2C%20traditional%20FR%20systems%20usually%0Arepresent%20users/items%20with%20discrete%20identities%20%28IDs%29%2C%20suffering%20from%0Aperformance%20degradation%20due%20to%20the%20data%20sparsity%20and%20heterogeneity%20in%20FR.%20On%0Athe%20other%20hand%2C%20Large%20Language%20Models%20%28LLMs%29%20as%20recommenders%20have%20proven%0Aeffective%20across%20various%20recommendation%20scenarios.%20Yet%2C%20LLM-based%20recommenders%0Aencounter%20challenges%20such%20as%20low%20inference%20efficiency%20and%20potential%0Ahallucination%2C%20compromising%20their%20performance%20in%20real-world%20scenarios.%20To%20this%0Aend%2C%20we%20propose%20GPT-FedRec%2C%20a%20federated%20recommendation%20framework%20leveraging%0AChatGPT%20and%20a%20novel%20hybrid%20Retrieval%20Augmented%20Generation%20%28RAG%29%20mechanism.%0AGPT-FedRec%20is%20a%20two-stage%20solution.%20The%20first%20stage%20is%20a%20hybrid%20retrieval%0Aprocess%2C%20mining%20ID-based%20user%20patterns%20and%20text-based%20item%20features.%20Next%2C%20the%0Aretrieved%20results%20are%20converted%20into%20text%20prompts%20and%20fed%20into%20GPT%20for%0Are-ranking.%20Our%20proposed%20hybrid%20retrieval%20mechanism%20and%20LLM-based%20re-rank%20aims%0Ato%20extract%20generalized%20features%20from%20data%20and%20exploit%20pretrained%20knowledge%0Awithin%20LLM%2C%20overcoming%20data%20sparsity%20and%20heterogeneity%20in%20FR.%20In%20addition%2C%20the%0ARAG%20approach%20also%20prevents%20LLM%20hallucination%2C%20improving%20the%20recommendation%0Aperformance%20for%20real-world%20users.%20Experimental%20results%20on%20diverse%20benchmark%0Adatasets%20demonstrate%20the%20superior%20performance%20of%20GPT-FedRec%20against%0Astate-of-the-art%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04256v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Recommendation%20via%20Hybrid%20Retrieval%20Augmented%20Generation&entry.906535625=Huimin%20Zeng%20and%20Zhenrui%20Yue%20and%20Qian%20Jiang%20and%20Dong%20Wang&entry.1292438233=%20%20Federated%20Recommendation%20%28FR%29%20emerges%20as%20a%20novel%20paradigm%20that%20enables%0Aprivacy-preserving%20recommendations.%20However%2C%20traditional%20FR%20systems%20usually%0Arepresent%20users/items%20with%20discrete%20identities%20%28IDs%29%2C%20suffering%20from%0Aperformance%20degradation%20due%20to%20the%20data%20sparsity%20and%20heterogeneity%20in%20FR.%20On%0Athe%20other%20hand%2C%20Large%20Language%20Models%20%28LLMs%29%20as%20recommenders%20have%20proven%0Aeffective%20across%20various%20recommendation%20scenarios.%20Yet%2C%20LLM-based%20recommenders%0Aencounter%20challenges%20such%20as%20low%20inference%20efficiency%20and%20potential%0Ahallucination%2C%20compromising%20their%20performance%20in%20real-world%20scenarios.%20To%20this%0Aend%2C%20we%20propose%20GPT-FedRec%2C%20a%20federated%20recommendation%20framework%20leveraging%0AChatGPT%20and%20a%20novel%20hybrid%20Retrieval%20Augmented%20Generation%20%28RAG%29%20mechanism.%0AGPT-FedRec%20is%20a%20two-stage%20solution.%20The%20first%20stage%20is%20a%20hybrid%20retrieval%0Aprocess%2C%20mining%20ID-based%20user%20patterns%20and%20text-based%20item%20features.%20Next%2C%20the%0Aretrieved%20results%20are%20converted%20into%20text%20prompts%20and%20fed%20into%20GPT%20for%0Are-ranking.%20Our%20proposed%20hybrid%20retrieval%20mechanism%20and%20LLM-based%20re-rank%20aims%0Ato%20extract%20generalized%20features%20from%20data%20and%20exploit%20pretrained%20knowledge%0Awithin%20LLM%2C%20overcoming%20data%20sparsity%20and%20heterogeneity%20in%20FR.%20In%20addition%2C%20the%0ARAG%20approach%20also%20prevents%20LLM%20hallucination%2C%20improving%20the%20recommendation%0Aperformance%20for%20real-world%20users.%20Experimental%20results%20on%20diverse%20benchmark%0Adatasets%20demonstrate%20the%20superior%20performance%20of%20GPT-FedRec%20against%0Astate-of-the-art%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04256v1&entry.124074799=Read"},
{"title": "A Domain Translation Framework with an Adversarial Denoising Diffusion\n  Model to Generate Synthetic Datasets of Echocardiography Images", "author": "Cristiana Tiago and Sten Roar Snare and Jurica Sprem and Kristin McLeod", "abstract": "  Currently, medical image domain translation operations show a high demand\nfrom researchers and clinicians. Amongst other capabilities, this task allows\nthe generation of new medical images with sufficiently high image quality,\nmaking them clinically relevant. Deep Learning (DL) architectures, most\nspecifically deep generative models, are widely used to generate and translate\nimages from one domain to another. The proposed framework relies on an\nadversarial Denoising Diffusion Model (DDM) to synthesize echocardiography\nimages and perform domain translation. Contrary to Generative Adversarial\nNetworks (GANs), DDMs are able to generate high quality image samples with a\nlarge diversity. If a DDM is combined with a GAN, this ability to generate new\ndata is completed at an even faster sampling time. In this work we trained an\nadversarial DDM combined with a GAN to learn the reverse denoising process,\nrelying on a guide image, making sure relevant anatomical structures of each\nechocardiography image were kept and represented on the generated image\nsamples. For several domain translation operations, the results verified that\nsuch generative model was able to synthesize high quality image samples: MSE:\n11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03. The proposed\nmethod showed high generalization ability, introducing a framework to create\nechocardiography images suitable to be used for clinical research purposes.\n", "link": "http://arxiv.org/abs/2403.04612v1", "date": "2024-03-07", "relevancy": 2.3652, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6102}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6099}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5651}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Domain%20Translation%20Framework%20with%20an%20Adversarial%20Denoising%20Diffusion%0A%20%20Model%20to%20Generate%20Synthetic%20Datasets%20of%20Echocardiography%20Images&body=Title%3A%20A%20Domain%20Translation%20Framework%20with%20an%20Adversarial%20Denoising%20Diffusion%0A%20%20Model%20to%20Generate%20Synthetic%20Datasets%20of%20Echocardiography%20Images%0AAuthor%3A%20Cristiana%20Tiago%20and%20Sten%20Roar%20Snare%20and%20Jurica%20Sprem%20and%20Kristin%20McLeod%0AAbstract%3A%20%20%20Currently%2C%20medical%20image%20domain%20translation%20operations%20show%20a%20high%20demand%0Afrom%20researchers%20and%20clinicians.%20Amongst%20other%20capabilities%2C%20this%20task%20allows%0Athe%20generation%20of%20new%20medical%20images%20with%20sufficiently%20high%20image%20quality%2C%0Amaking%20them%20clinically%20relevant.%20Deep%20Learning%20%28DL%29%20architectures%2C%20most%0Aspecifically%20deep%20generative%20models%2C%20are%20widely%20used%20to%20generate%20and%20translate%0Aimages%20from%20one%20domain%20to%20another.%20The%20proposed%20framework%20relies%20on%20an%0Aadversarial%20Denoising%20Diffusion%20Model%20%28DDM%29%20to%20synthesize%20echocardiography%0Aimages%20and%20perform%20domain%20translation.%20Contrary%20to%20Generative%20Adversarial%0ANetworks%20%28GANs%29%2C%20DDMs%20are%20able%20to%20generate%20high%20quality%20image%20samples%20with%20a%0Alarge%20diversity.%20If%20a%20DDM%20is%20combined%20with%20a%20GAN%2C%20this%20ability%20to%20generate%20new%0Adata%20is%20completed%20at%20an%20even%20faster%20sampling%20time.%20In%20this%20work%20we%20trained%20an%0Aadversarial%20DDM%20combined%20with%20a%20GAN%20to%20learn%20the%20reverse%20denoising%20process%2C%0Arelying%20on%20a%20guide%20image%2C%20making%20sure%20relevant%20anatomical%20structures%20of%20each%0Aechocardiography%20image%20were%20kept%20and%20represented%20on%20the%20generated%20image%0Asamples.%20For%20several%20domain%20translation%20operations%2C%20the%20results%20verified%20that%0Asuch%20generative%20model%20was%20able%20to%20synthesize%20high%20quality%20image%20samples%3A%20MSE%3A%0A11.50%20%2B/-%203.69%2C%20PSNR%20%28dB%29%3A%2030.48%20%2B/-%200.09%2C%20SSIM%3A%200.47%20%2B/-%200.03.%20The%20proposed%0Amethod%20showed%20high%20generalization%20ability%2C%20introducing%20a%20framework%20to%20create%0Aechocardiography%20images%20suitable%20to%20be%20used%20for%20clinical%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04612v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Domain%20Translation%20Framework%20with%20an%20Adversarial%20Denoising%20Diffusion%0A%20%20Model%20to%20Generate%20Synthetic%20Datasets%20of%20Echocardiography%20Images&entry.906535625=Cristiana%20Tiago%20and%20Sten%20Roar%20Snare%20and%20Jurica%20Sprem%20and%20Kristin%20McLeod&entry.1292438233=%20%20Currently%2C%20medical%20image%20domain%20translation%20operations%20show%20a%20high%20demand%0Afrom%20researchers%20and%20clinicians.%20Amongst%20other%20capabilities%2C%20this%20task%20allows%0Athe%20generation%20of%20new%20medical%20images%20with%20sufficiently%20high%20image%20quality%2C%0Amaking%20them%20clinically%20relevant.%20Deep%20Learning%20%28DL%29%20architectures%2C%20most%0Aspecifically%20deep%20generative%20models%2C%20are%20widely%20used%20to%20generate%20and%20translate%0Aimages%20from%20one%20domain%20to%20another.%20The%20proposed%20framework%20relies%20on%20an%0Aadversarial%20Denoising%20Diffusion%20Model%20%28DDM%29%20to%20synthesize%20echocardiography%0Aimages%20and%20perform%20domain%20translation.%20Contrary%20to%20Generative%20Adversarial%0ANetworks%20%28GANs%29%2C%20DDMs%20are%20able%20to%20generate%20high%20quality%20image%20samples%20with%20a%0Alarge%20diversity.%20If%20a%20DDM%20is%20combined%20with%20a%20GAN%2C%20this%20ability%20to%20generate%20new%0Adata%20is%20completed%20at%20an%20even%20faster%20sampling%20time.%20In%20this%20work%20we%20trained%20an%0Aadversarial%20DDM%20combined%20with%20a%20GAN%20to%20learn%20the%20reverse%20denoising%20process%2C%0Arelying%20on%20a%20guide%20image%2C%20making%20sure%20relevant%20anatomical%20structures%20of%20each%0Aechocardiography%20image%20were%20kept%20and%20represented%20on%20the%20generated%20image%0Asamples.%20For%20several%20domain%20translation%20operations%2C%20the%20results%20verified%20that%0Asuch%20generative%20model%20was%20able%20to%20synthesize%20high%20quality%20image%20samples%3A%20MSE%3A%0A11.50%20%2B/-%203.69%2C%20PSNR%20%28dB%29%3A%2030.48%20%2B/-%200.09%2C%20SSIM%3A%200.47%20%2B/-%200.03.%20The%20proposed%0Amethod%20showed%20high%20generalization%20ability%2C%20introducing%20a%20framework%20to%20create%0Aechocardiography%20images%20suitable%20to%20be%20used%20for%20clinical%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04612v1&entry.124074799=Read"},
{"title": "MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided\n  Diffusion with Visual Invariant", "author": "Chenlu Zhan and Yu Lin and Gaoang Wang and Hongwei Wang and Jian Wu", "abstract": "  Medical generative models, acknowledged for their high-quality sample\ngeneration ability, have accelerated the fast growth of medical applications.\nHowever, recent works concentrate on separate medical generation models for\ndistinct medical tasks and are restricted to inadequate medical multi-modal\nknowledge, constraining medical comprehensive diagnosis. In this paper, we\npropose MedM2G, a Medical Multi-Modal Generative framework, with the key\ninnovation to align, extract, and generate medical multi-modal within a unified\nmodel. Extending beyond single or two medical modalities, we efficiently align\nmedical multi-modal through the central alignment approach in the unified\nspace. Significantly, our framework extracts valuable clinical knowledge by\npreserving the medical visual invariant of each imaging modal, thereby\nenhancing specific medical information for multi-modal generation. By\nconditioning the adaptive cross-guided parameters into the multi-flow diffusion\nframework, our model promotes flexible interactions among medical multi-modal\nfor generation. MedM2G is the first medical generative model that unifies\nmedical generation tasks of text-to-image, image-to-text, and unified\ngeneration of medical modalities (CT, MRI, X-ray). It performs 5 medical\ngeneration tasks across 10 datasets, consistently outperforming various\nstate-of-the-art works.\n", "link": "http://arxiv.org/abs/2403.04290v1", "date": "2024-03-07", "relevancy": 2.3478, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6036}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5819}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5581}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MedM2G%3A%20Unifying%20Medical%20Multi-Modal%20Generation%20via%20Cross-Guided%0A%20%20Diffusion%20with%20Visual%20Invariant&body=Title%3A%20MedM2G%3A%20Unifying%20Medical%20Multi-Modal%20Generation%20via%20Cross-Guided%0A%20%20Diffusion%20with%20Visual%20Invariant%0AAuthor%3A%20Chenlu%20Zhan%20and%20Yu%20Lin%20and%20Gaoang%20Wang%20and%20Hongwei%20Wang%20and%20Jian%20Wu%0AAbstract%3A%20%20%20Medical%20generative%20models%2C%20acknowledged%20for%20their%20high-quality%20sample%0Ageneration%20ability%2C%20have%20accelerated%20the%20fast%20growth%20of%20medical%20applications.%0AHowever%2C%20recent%20works%20concentrate%20on%20separate%20medical%20generation%20models%20for%0Adistinct%20medical%20tasks%20and%20are%20restricted%20to%20inadequate%20medical%20multi-modal%0Aknowledge%2C%20constraining%20medical%20comprehensive%20diagnosis.%20In%20this%20paper%2C%20we%0Apropose%20MedM2G%2C%20a%20Medical%20Multi-Modal%20Generative%20framework%2C%20with%20the%20key%0Ainnovation%20to%20align%2C%20extract%2C%20and%20generate%20medical%20multi-modal%20within%20a%20unified%0Amodel.%20Extending%20beyond%20single%20or%20two%20medical%20modalities%2C%20we%20efficiently%20align%0Amedical%20multi-modal%20through%20the%20central%20alignment%20approach%20in%20the%20unified%0Aspace.%20Significantly%2C%20our%20framework%20extracts%20valuable%20clinical%20knowledge%20by%0Apreserving%20the%20medical%20visual%20invariant%20of%20each%20imaging%20modal%2C%20thereby%0Aenhancing%20specific%20medical%20information%20for%20multi-modal%20generation.%20By%0Aconditioning%20the%20adaptive%20cross-guided%20parameters%20into%20the%20multi-flow%20diffusion%0Aframework%2C%20our%20model%20promotes%20flexible%20interactions%20among%20medical%20multi-modal%0Afor%20generation.%20MedM2G%20is%20the%20first%20medical%20generative%20model%20that%20unifies%0Amedical%20generation%20tasks%20of%20text-to-image%2C%20image-to-text%2C%20and%20unified%0Ageneration%20of%20medical%20modalities%20%28CT%2C%20MRI%2C%20X-ray%29.%20It%20performs%205%20medical%0Ageneration%20tasks%20across%2010%20datasets%2C%20consistently%20outperforming%20various%0Astate-of-the-art%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04290v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedM2G%3A%20Unifying%20Medical%20Multi-Modal%20Generation%20via%20Cross-Guided%0A%20%20Diffusion%20with%20Visual%20Invariant&entry.906535625=Chenlu%20Zhan%20and%20Yu%20Lin%20and%20Gaoang%20Wang%20and%20Hongwei%20Wang%20and%20Jian%20Wu&entry.1292438233=%20%20Medical%20generative%20models%2C%20acknowledged%20for%20their%20high-quality%20sample%0Ageneration%20ability%2C%20have%20accelerated%20the%20fast%20growth%20of%20medical%20applications.%0AHowever%2C%20recent%20works%20concentrate%20on%20separate%20medical%20generation%20models%20for%0Adistinct%20medical%20tasks%20and%20are%20restricted%20to%20inadequate%20medical%20multi-modal%0Aknowledge%2C%20constraining%20medical%20comprehensive%20diagnosis.%20In%20this%20paper%2C%20we%0Apropose%20MedM2G%2C%20a%20Medical%20Multi-Modal%20Generative%20framework%2C%20with%20the%20key%0Ainnovation%20to%20align%2C%20extract%2C%20and%20generate%20medical%20multi-modal%20within%20a%20unified%0Amodel.%20Extending%20beyond%20single%20or%20two%20medical%20modalities%2C%20we%20efficiently%20align%0Amedical%20multi-modal%20through%20the%20central%20alignment%20approach%20in%20the%20unified%0Aspace.%20Significantly%2C%20our%20framework%20extracts%20valuable%20clinical%20knowledge%20by%0Apreserving%20the%20medical%20visual%20invariant%20of%20each%20imaging%20modal%2C%20thereby%0Aenhancing%20specific%20medical%20information%20for%20multi-modal%20generation.%20By%0Aconditioning%20the%20adaptive%20cross-guided%20parameters%20into%20the%20multi-flow%20diffusion%0Aframework%2C%20our%20model%20promotes%20flexible%20interactions%20among%20medical%20multi-modal%0Afor%20generation.%20MedM2G%20is%20the%20first%20medical%20generative%20model%20that%20unifies%0Amedical%20generation%20tasks%20of%20text-to-image%2C%20image-to-text%2C%20and%20unified%0Ageneration%20of%20medical%20modalities%20%28CT%2C%20MRI%2C%20X-ray%29.%20It%20performs%205%20medical%0Ageneration%20tasks%20across%2010%20datasets%2C%20consistently%20outperforming%20various%0Astate-of-the-art%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04290v1&entry.124074799=Read"},
{"title": "Depth-aware Test-Time Training for Zero-shot Video Object Segmentation", "author": "Weihuang Liu and Xi Shen and Haolun Li and Xiuli Bi and Bo Liu and Chi-Man Pun and Xiaodong Cun", "abstract": "  Zero-shot Video Object Segmentation (ZSVOS) aims at segmenting the primary\nmoving object without any human annotations. Mainstream solutions mainly focus\non learning a single model on large-scale video datasets, which struggle to\ngeneralize to unseen videos. In this work, we introduce a test-time training\n(TTT) strategy to address the problem. Our key insight is to enforce the model\nto predict consistent depth during the TTT process. In detail, we first train a\nsingle network to perform both segmentation and depth prediction tasks. This\ncan be effectively learned with our specifically designed depth modulation\nlayer. Then, for the TTT process, the model is updated by predicting consistent\ndepth maps for the same frame under different data augmentations. In addition,\nwe explore different TTT weight updating strategies. Our empirical results\nsuggest that the momentum-based weight initialization and looping-based\ntraining scheme lead to more stable improvements. Experiments show that the\nproposed method achieves clear improvements on ZSVOS. Our proposed video TTT\nstrategy provides significant superiority over state-of-the-art TTT methods.\nOur code is available at: https://nifangbaage.github.io/DATTT.\n", "link": "http://arxiv.org/abs/2403.04258v1", "date": "2024-03-07", "relevancy": 2.3304, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5863}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5838}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5799}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Depth-aware%20Test-Time%20Training%20for%20Zero-shot%20Video%20Object%20Segmentation&body=Title%3A%20Depth-aware%20Test-Time%20Training%20for%20Zero-shot%20Video%20Object%20Segmentation%0AAuthor%3A%20Weihuang%20Liu%20and%20Xi%20Shen%20and%20Haolun%20Li%20and%20Xiuli%20Bi%20and%20Bo%20Liu%20and%20Chi-Man%20Pun%20and%20Xiaodong%20Cun%0AAbstract%3A%20%20%20Zero-shot%20Video%20Object%20Segmentation%20%28ZSVOS%29%20aims%20at%20segmenting%20the%20primary%0Amoving%20object%20without%20any%20human%20annotations.%20Mainstream%20solutions%20mainly%20focus%0Aon%20learning%20a%20single%20model%20on%20large-scale%20video%20datasets%2C%20which%20struggle%20to%0Ageneralize%20to%20unseen%20videos.%20In%20this%20work%2C%20we%20introduce%20a%20test-time%20training%0A%28TTT%29%20strategy%20to%20address%20the%20problem.%20Our%20key%20insight%20is%20to%20enforce%20the%20model%0Ato%20predict%20consistent%20depth%20during%20the%20TTT%20process.%20In%20detail%2C%20we%20first%20train%20a%0Asingle%20network%20to%20perform%20both%20segmentation%20and%20depth%20prediction%20tasks.%20This%0Acan%20be%20effectively%20learned%20with%20our%20specifically%20designed%20depth%20modulation%0Alayer.%20Then%2C%20for%20the%20TTT%20process%2C%20the%20model%20is%20updated%20by%20predicting%20consistent%0Adepth%20maps%20for%20the%20same%20frame%20under%20different%20data%20augmentations.%20In%20addition%2C%0Awe%20explore%20different%20TTT%20weight%20updating%20strategies.%20Our%20empirical%20results%0Asuggest%20that%20the%20momentum-based%20weight%20initialization%20and%20looping-based%0Atraining%20scheme%20lead%20to%20more%20stable%20improvements.%20Experiments%20show%20that%20the%0Aproposed%20method%20achieves%20clear%20improvements%20on%20ZSVOS.%20Our%20proposed%20video%20TTT%0Astrategy%20provides%20significant%20superiority%20over%20state-of-the-art%20TTT%20methods.%0AOur%20code%20is%20available%20at%3A%20https%3A//nifangbaage.github.io/DATTT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04258v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-aware%20Test-Time%20Training%20for%20Zero-shot%20Video%20Object%20Segmentation&entry.906535625=Weihuang%20Liu%20and%20Xi%20Shen%20and%20Haolun%20Li%20and%20Xiuli%20Bi%20and%20Bo%20Liu%20and%20Chi-Man%20Pun%20and%20Xiaodong%20Cun&entry.1292438233=%20%20Zero-shot%20Video%20Object%20Segmentation%20%28ZSVOS%29%20aims%20at%20segmenting%20the%20primary%0Amoving%20object%20without%20any%20human%20annotations.%20Mainstream%20solutions%20mainly%20focus%0Aon%20learning%20a%20single%20model%20on%20large-scale%20video%20datasets%2C%20which%20struggle%20to%0Ageneralize%20to%20unseen%20videos.%20In%20this%20work%2C%20we%20introduce%20a%20test-time%20training%0A%28TTT%29%20strategy%20to%20address%20the%20problem.%20Our%20key%20insight%20is%20to%20enforce%20the%20model%0Ato%20predict%20consistent%20depth%20during%20the%20TTT%20process.%20In%20detail%2C%20we%20first%20train%20a%0Asingle%20network%20to%20perform%20both%20segmentation%20and%20depth%20prediction%20tasks.%20This%0Acan%20be%20effectively%20learned%20with%20our%20specifically%20designed%20depth%20modulation%0Alayer.%20Then%2C%20for%20the%20TTT%20process%2C%20the%20model%20is%20updated%20by%20predicting%20consistent%0Adepth%20maps%20for%20the%20same%20frame%20under%20different%20data%20augmentations.%20In%20addition%2C%0Awe%20explore%20different%20TTT%20weight%20updating%20strategies.%20Our%20empirical%20results%0Asuggest%20that%20the%20momentum-based%20weight%20initialization%20and%20looping-based%0Atraining%20scheme%20lead%20to%20more%20stable%20improvements.%20Experiments%20show%20that%20the%0Aproposed%20method%20achieves%20clear%20improvements%20on%20ZSVOS.%20Our%20proposed%20video%20TTT%0Astrategy%20provides%20significant%20superiority%20over%20state-of-the-art%20TTT%20methods.%0AOur%20code%20is%20available%20at%3A%20https%3A//nifangbaage.github.io/DATTT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04258v1&entry.124074799=Read"},
{"title": "EasyPortrait -- Face Parsing and Portrait Segmentation Dataset", "author": "Karina Kvanchiani and Elizaveta Petrova and Karen Efremyan and Alexander Sautin and Alexander Kapitanov", "abstract": "  Recently, video conferencing apps have become functional by accomplishing\nsuch computer vision-based features as real-time background removal and face\nbeautification. Limited variability in existing portrait segmentation and face\nparsing datasets, including head poses, ethnicity, scenes, and occlusions\nspecific to video conferencing, motivated us to create a new dataset,\nEasyPortrait, for these tasks simultaneously. It contains 40,000 primarily\nindoor photos repeating video meeting scenarios with 13,705 unique users and\nfine-grained segmentation masks separated into 9 classes. Inappropriate\nannotation masks from other datasets caused a revision of annotator guidelines,\nresulting in EasyPortrait's ability to process cases, such as teeth whitening\nand skin smoothing. The pipeline for data mining and high-quality mask\nannotation via crowdsourcing is also proposed in this paper. In the ablation\nstudy experiments, we proved the importance of data quantity and diversity in\nhead poses in our dataset for the effective learning of the model. The\ncross-dataset evaluation experiments confirmed the best domain generalization\nability among portrait segmentation datasets. Moreover, we demonstrate the\nsimplicity of training segmentation models on EasyPortrait without extra\ntraining tricks. The proposed dataset and trained models are publicly\navailable.\n", "link": "http://arxiv.org/abs/2304.13509v3", "date": "2024-03-07", "relevancy": 2.3139, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4749}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4568}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4566}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20EasyPortrait%20--%20Face%20Parsing%20and%20Portrait%20Segmentation%20Dataset&body=Title%3A%20EasyPortrait%20--%20Face%20Parsing%20and%20Portrait%20Segmentation%20Dataset%0AAuthor%3A%20Karina%20Kvanchiani%20and%20Elizaveta%20Petrova%20and%20Karen%20Efremyan%20and%20Alexander%20Sautin%20and%20Alexander%20Kapitanov%0AAbstract%3A%20%20%20Recently%2C%20video%20conferencing%20apps%20have%20become%20functional%20by%20accomplishing%0Asuch%20computer%20vision-based%20features%20as%20real-time%20background%20removal%20and%20face%0Abeautification.%20Limited%20variability%20in%20existing%20portrait%20segmentation%20and%20face%0Aparsing%20datasets%2C%20including%20head%20poses%2C%20ethnicity%2C%20scenes%2C%20and%20occlusions%0Aspecific%20to%20video%20conferencing%2C%20motivated%20us%20to%20create%20a%20new%20dataset%2C%0AEasyPortrait%2C%20for%20these%20tasks%20simultaneously.%20It%20contains%2040%2C000%20primarily%0Aindoor%20photos%20repeating%20video%20meeting%20scenarios%20with%2013%2C705%20unique%20users%20and%0Afine-grained%20segmentation%20masks%20separated%20into%209%20classes.%20Inappropriate%0Aannotation%20masks%20from%20other%20datasets%20caused%20a%20revision%20of%20annotator%20guidelines%2C%0Aresulting%20in%20EasyPortrait%27s%20ability%20to%20process%20cases%2C%20such%20as%20teeth%20whitening%0Aand%20skin%20smoothing.%20The%20pipeline%20for%20data%20mining%20and%20high-quality%20mask%0Aannotation%20via%20crowdsourcing%20is%20also%20proposed%20in%20this%20paper.%20In%20the%20ablation%0Astudy%20experiments%2C%20we%20proved%20the%20importance%20of%20data%20quantity%20and%20diversity%20in%0Ahead%20poses%20in%20our%20dataset%20for%20the%20effective%20learning%20of%20the%20model.%20The%0Across-dataset%20evaluation%20experiments%20confirmed%20the%20best%20domain%20generalization%0Aability%20among%20portrait%20segmentation%20datasets.%20Moreover%2C%20we%20demonstrate%20the%0Asimplicity%20of%20training%20segmentation%20models%20on%20EasyPortrait%20without%20extra%0Atraining%20tricks.%20The%20proposed%20dataset%20and%20trained%20models%20are%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.13509v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyPortrait%20--%20Face%20Parsing%20and%20Portrait%20Segmentation%20Dataset&entry.906535625=Karina%20Kvanchiani%20and%20Elizaveta%20Petrova%20and%20Karen%20Efremyan%20and%20Alexander%20Sautin%20and%20Alexander%20Kapitanov&entry.1292438233=%20%20Recently%2C%20video%20conferencing%20apps%20have%20become%20functional%20by%20accomplishing%0Asuch%20computer%20vision-based%20features%20as%20real-time%20background%20removal%20and%20face%0Abeautification.%20Limited%20variability%20in%20existing%20portrait%20segmentation%20and%20face%0Aparsing%20datasets%2C%20including%20head%20poses%2C%20ethnicity%2C%20scenes%2C%20and%20occlusions%0Aspecific%20to%20video%20conferencing%2C%20motivated%20us%20to%20create%20a%20new%20dataset%2C%0AEasyPortrait%2C%20for%20these%20tasks%20simultaneously.%20It%20contains%2040%2C000%20primarily%0Aindoor%20photos%20repeating%20video%20meeting%20scenarios%20with%2013%2C705%20unique%20users%20and%0Afine-grained%20segmentation%20masks%20separated%20into%209%20classes.%20Inappropriate%0Aannotation%20masks%20from%20other%20datasets%20caused%20a%20revision%20of%20annotator%20guidelines%2C%0Aresulting%20in%20EasyPortrait%27s%20ability%20to%20process%20cases%2C%20such%20as%20teeth%20whitening%0Aand%20skin%20smoothing.%20The%20pipeline%20for%20data%20mining%20and%20high-quality%20mask%0Aannotation%20via%20crowdsourcing%20is%20also%20proposed%20in%20this%20paper.%20In%20the%20ablation%0Astudy%20experiments%2C%20we%20proved%20the%20importance%20of%20data%20quantity%20and%20diversity%20in%0Ahead%20poses%20in%20our%20dataset%20for%20the%20effective%20learning%20of%20the%20model.%20The%0Across-dataset%20evaluation%20experiments%20confirmed%20the%20best%20domain%20generalization%0Aability%20among%20portrait%20segmentation%20datasets.%20Moreover%2C%20we%20demonstrate%20the%0Asimplicity%20of%20training%20segmentation%20models%20on%20EasyPortrait%20without%20extra%0Atraining%20tricks.%20The%20proposed%20dataset%20and%20trained%20models%20are%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.13509v3&entry.124074799=Read"},
{"title": "MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training\n  with Masked Autoencoder", "author": "Lei Li and Tianfang Zhang and Xinglin Zhang and Jiaqi Liu and Bingqi Ma and Yan Luo and Tao Chen", "abstract": "  Within the domain of medical analysis, extensive research has explored the\npotential of mutual learning between Masked Autoencoders(MAEs) and multimodal\ndata. However, the impact of MAEs on intermodality remains a key challenge. We\nintroduce MedFLIP, a Fast Language-Image Pre-training method for Medical\nanalysis. We explore MAEs for zero-shot learning with crossed domains, which\nenhances the model ability to learn from limited data, a common scenario in\nmedical diagnostics. We verify that masking an image does not affect intermodal\nlearning. Furthermore, we propose the SVD loss to enhance the representation\nlearning for characteristics of medical images, aiming to improve\nclassification accuracy by leveraging the structural intricacies of such data.\nLastly, we validate using language will improve the zero-shot performance for\nthe medical image analysis. MedFLIP scaling of the masking process marks an\nadvancement in the field, offering a pathway to rapid and precise medical image\nanalysis without the traditional computational bottlenecks. Through experiments\nand validation, MedFLIP demonstrates efficient performance improvements,\nsetting an explored standard for future research and application in medical\ndiagnostics.\n", "link": "http://arxiv.org/abs/2403.04626v1", "date": "2024-03-07", "relevancy": 2.298, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6104}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.576}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.538}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MedFLIP%3A%20Medical%20Vision-and-Language%20Self-supervised%20Fast%20Pre-Training%0A%20%20with%20Masked%20Autoencoder&body=Title%3A%20MedFLIP%3A%20Medical%20Vision-and-Language%20Self-supervised%20Fast%20Pre-Training%0A%20%20with%20Masked%20Autoencoder%0AAuthor%3A%20Lei%20Li%20and%20Tianfang%20Zhang%20and%20Xinglin%20Zhang%20and%20Jiaqi%20Liu%20and%20Bingqi%20Ma%20and%20Yan%20Luo%20and%20Tao%20Chen%0AAbstract%3A%20%20%20Within%20the%20domain%20of%20medical%20analysis%2C%20extensive%20research%20has%20explored%20the%0Apotential%20of%20mutual%20learning%20between%20Masked%20Autoencoders%28MAEs%29%20and%20multimodal%0Adata.%20However%2C%20the%20impact%20of%20MAEs%20on%20intermodality%20remains%20a%20key%20challenge.%20We%0Aintroduce%20MedFLIP%2C%20a%20Fast%20Language-Image%20Pre-training%20method%20for%20Medical%0Aanalysis.%20We%20explore%20MAEs%20for%20zero-shot%20learning%20with%20crossed%20domains%2C%20which%0Aenhances%20the%20model%20ability%20to%20learn%20from%20limited%20data%2C%20a%20common%20scenario%20in%0Amedical%20diagnostics.%20We%20verify%20that%20masking%20an%20image%20does%20not%20affect%20intermodal%0Alearning.%20Furthermore%2C%20we%20propose%20the%20SVD%20loss%20to%20enhance%20the%20representation%0Alearning%20for%20characteristics%20of%20medical%20images%2C%20aiming%20to%20improve%0Aclassification%20accuracy%20by%20leveraging%20the%20structural%20intricacies%20of%20such%20data.%0ALastly%2C%20we%20validate%20using%20language%20will%20improve%20the%20zero-shot%20performance%20for%0Athe%20medical%20image%20analysis.%20MedFLIP%20scaling%20of%20the%20masking%20process%20marks%20an%0Aadvancement%20in%20the%20field%2C%20offering%20a%20pathway%20to%20rapid%20and%20precise%20medical%20image%0Aanalysis%20without%20the%20traditional%20computational%20bottlenecks.%20Through%20experiments%0Aand%20validation%2C%20MedFLIP%20demonstrates%20efficient%20performance%20improvements%2C%0Asetting%20an%20explored%20standard%20for%20future%20research%20and%20application%20in%20medical%0Adiagnostics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04626v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedFLIP%3A%20Medical%20Vision-and-Language%20Self-supervised%20Fast%20Pre-Training%0A%20%20with%20Masked%20Autoencoder&entry.906535625=Lei%20Li%20and%20Tianfang%20Zhang%20and%20Xinglin%20Zhang%20and%20Jiaqi%20Liu%20and%20Bingqi%20Ma%20and%20Yan%20Luo%20and%20Tao%20Chen&entry.1292438233=%20%20Within%20the%20domain%20of%20medical%20analysis%2C%20extensive%20research%20has%20explored%20the%0Apotential%20of%20mutual%20learning%20between%20Masked%20Autoencoders%28MAEs%29%20and%20multimodal%0Adata.%20However%2C%20the%20impact%20of%20MAEs%20on%20intermodality%20remains%20a%20key%20challenge.%20We%0Aintroduce%20MedFLIP%2C%20a%20Fast%20Language-Image%20Pre-training%20method%20for%20Medical%0Aanalysis.%20We%20explore%20MAEs%20for%20zero-shot%20learning%20with%20crossed%20domains%2C%20which%0Aenhances%20the%20model%20ability%20to%20learn%20from%20limited%20data%2C%20a%20common%20scenario%20in%0Amedical%20diagnostics.%20We%20verify%20that%20masking%20an%20image%20does%20not%20affect%20intermodal%0Alearning.%20Furthermore%2C%20we%20propose%20the%20SVD%20loss%20to%20enhance%20the%20representation%0Alearning%20for%20characteristics%20of%20medical%20images%2C%20aiming%20to%20improve%0Aclassification%20accuracy%20by%20leveraging%20the%20structural%20intricacies%20of%20such%20data.%0ALastly%2C%20we%20validate%20using%20language%20will%20improve%20the%20zero-shot%20performance%20for%0Athe%20medical%20image%20analysis.%20MedFLIP%20scaling%20of%20the%20masking%20process%20marks%20an%0Aadvancement%20in%20the%20field%2C%20offering%20a%20pathway%20to%20rapid%20and%20precise%20medical%20image%0Aanalysis%20without%20the%20traditional%20computational%20bottlenecks.%20Through%20experiments%0Aand%20validation%2C%20MedFLIP%20demonstrates%20efficient%20performance%20improvements%2C%0Asetting%20an%20explored%20standard%20for%20future%20research%20and%20application%20in%20medical%0Adiagnostics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04626v1&entry.124074799=Read"},
{"title": "Video-Driven Animation of Neural Head Avatars", "author": "Wolfgang Paier and Paul Hinzer and Anna Hilsmann and Peter Eisert", "abstract": "  We present a new approach for video-driven animation of high-quality neural\n3D head models, addressing the challenge of person-independent animation from\nvideo input. Typically, high-quality generative models are learned for specific\nindividuals from multi-view video footage, resulting in person-specific latent\nrepresentations that drive the generation process. In order to achieve\nperson-independent animation from video input, we introduce an LSTM-based\nanimation network capable of translating person-independent expression features\ninto personalized animation parameters of person-specific 3D head models. Our\napproach combines the advantages of personalized head models (high quality and\nrealism) with the convenience of video-driven animation employing multi-person\nfacial performance capture. We demonstrate the effectiveness of our approach on\nsynthesized animations with high quality based on different source videos as\nwell as an ablation study.\n", "link": "http://arxiv.org/abs/2403.04380v1", "date": "2024-03-07", "relevancy": 2.2532, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5898}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5697}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5464}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Video-Driven%20Animation%20of%20Neural%20Head%20Avatars&body=Title%3A%20Video-Driven%20Animation%20of%20Neural%20Head%20Avatars%0AAuthor%3A%20Wolfgang%20Paier%20and%20Paul%20Hinzer%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%20for%20video-driven%20animation%20of%20high-quality%20neural%0A3D%20head%20models%2C%20addressing%20the%20challenge%20of%20person-independent%20animation%20from%0Avideo%20input.%20Typically%2C%20high-quality%20generative%20models%20are%20learned%20for%20specific%0Aindividuals%20from%20multi-view%20video%20footage%2C%20resulting%20in%20person-specific%20latent%0Arepresentations%20that%20drive%20the%20generation%20process.%20In%20order%20to%20achieve%0Aperson-independent%20animation%20from%20video%20input%2C%20we%20introduce%20an%20LSTM-based%0Aanimation%20network%20capable%20of%20translating%20person-independent%20expression%20features%0Ainto%20personalized%20animation%20parameters%20of%20person-specific%203D%20head%20models.%20Our%0Aapproach%20combines%20the%20advantages%20of%20personalized%20head%20models%20%28high%20quality%20and%0Arealism%29%20with%20the%20convenience%20of%20video-driven%20animation%20employing%20multi-person%0Afacial%20performance%20capture.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%0Asynthesized%20animations%20with%20high%20quality%20based%20on%20different%20source%20videos%20as%0Awell%20as%20an%20ablation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04380v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-Driven%20Animation%20of%20Neural%20Head%20Avatars&entry.906535625=Wolfgang%20Paier%20and%20Paul%20Hinzer%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert&entry.1292438233=%20%20We%20present%20a%20new%20approach%20for%20video-driven%20animation%20of%20high-quality%20neural%0A3D%20head%20models%2C%20addressing%20the%20challenge%20of%20person-independent%20animation%20from%0Avideo%20input.%20Typically%2C%20high-quality%20generative%20models%20are%20learned%20for%20specific%0Aindividuals%20from%20multi-view%20video%20footage%2C%20resulting%20in%20person-specific%20latent%0Arepresentations%20that%20drive%20the%20generation%20process.%20In%20order%20to%20achieve%0Aperson-independent%20animation%20from%20video%20input%2C%20we%20introduce%20an%20LSTM-based%0Aanimation%20network%20capable%20of%20translating%20person-independent%20expression%20features%0Ainto%20personalized%20animation%20parameters%20of%20person-specific%203D%20head%20models.%20Our%0Aapproach%20combines%20the%20advantages%20of%20personalized%20head%20models%20%28high%20quality%20and%0Arealism%29%20with%20the%20convenience%20of%20video-driven%20animation%20employing%20multi-person%0Afacial%20performance%20capture.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%0Asynthesized%20animations%20with%20high%20quality%20based%20on%20different%20source%20videos%20as%0Awell%20as%20an%20ablation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04380v1&entry.124074799=Read"},
{"title": "Towards General Computer Control: A Multimodal Agent for Red Dead\n  Redemption II as a Case Study", "author": "Weihao Tan and Ziluo Ding and Wentao Zhang and Boyu Li and Bohan Zhou and Junpeng Yue and Haochong Xia and Jiechuan Jiang and Longtao Zheng and Xinrun Xu and Yifei Bi and Pengjie Gu and Xinrun Wang and B\u00f6rje F. Karlsson and Bo An and Zongqing Lu", "abstract": "  Despite the success in specific tasks and scenarios, existing foundation\nagents, empowered by large models (LMs) and advanced tools, still cannot\ngeneralize to different scenarios, mainly due to dramatic differences in the\nobservations and actions across scenarios. In this work, we propose the General\nComputer Control (GCC) setting: building foundation agents that can master any\ncomputer task by taking only screen images (and possibly audio) of the computer\nas input, and producing keyboard and mouse operations as output, similar to\nhuman-computer interaction. The main challenges of achieving GCC are: 1) the\nmultimodal observations for decision-making, 2) the requirements of accurate\ncontrol of keyboard and mouse, 3) the need for long-term memory and reasoning,\nand 4) the abilities of efficient exploration and self-improvement. To target\nGCC, we introduce Cradle, an agent framework with six main modules, including:\n1) information gathering to extract multi-modality information, 2)\nself-reflection to rethink past experiences, 3) task inference to choose the\nbest next task, 4) skill curation for generating and updating relevant skills\nfor given tasks, 5) action planning to generate specific operations for\nkeyboard and mouse control, and 6) memory for storage and retrieval of past\nexperiences and known skills. To demonstrate the capabilities of generalization\nand self-improvement of Cradle, we deploy it in the complex AAA game Red Dead\nRedemption II, serving as a preliminary attempt towards GCC with a challenging\ntarget. To our best knowledge, our work is the first to enable LMM-based agents\nto follow the main storyline and finish real missions in complex AAA games,\nwith minimal reliance on prior knowledge or resources. The project website is\nat https://baai-agents.github.io/Cradle/.\n", "link": "http://arxiv.org/abs/2403.03186v2", "date": "2024-03-07", "relevancy": 2.2529, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5946}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5707}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5432}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Towards%20General%20Computer%20Control%3A%20A%20Multimodal%20Agent%20for%20Red%20Dead%0A%20%20Redemption%20II%20as%20a%20Case%20Study&body=Title%3A%20Towards%20General%20Computer%20Control%3A%20A%20Multimodal%20Agent%20for%20Red%20Dead%0A%20%20Redemption%20II%20as%20a%20Case%20Study%0AAuthor%3A%20Weihao%20Tan%20and%20Ziluo%20Ding%20and%20Wentao%20Zhang%20and%20Boyu%20Li%20and%20Bohan%20Zhou%20and%20Junpeng%20Yue%20and%20Haochong%20Xia%20and%20Jiechuan%20Jiang%20and%20Longtao%20Zheng%20and%20Xinrun%20Xu%20and%20Yifei%20Bi%20and%20Pengjie%20Gu%20and%20Xinrun%20Wang%20and%20B%C3%B6rje%20F.%20Karlsson%20and%20Bo%20An%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20Despite%20the%20success%20in%20specific%20tasks%20and%20scenarios%2C%20existing%20foundation%0Aagents%2C%20empowered%20by%20large%20models%20%28LMs%29%20and%20advanced%20tools%2C%20still%20cannot%0Ageneralize%20to%20different%20scenarios%2C%20mainly%20due%20to%20dramatic%20differences%20in%20the%0Aobservations%20and%20actions%20across%20scenarios.%20In%20this%20work%2C%20we%20propose%20the%20General%0AComputer%20Control%20%28GCC%29%20setting%3A%20building%20foundation%20agents%20that%20can%20master%20any%0Acomputer%20task%20by%20taking%20only%20screen%20images%20%28and%20possibly%20audio%29%20of%20the%20computer%0Aas%20input%2C%20and%20producing%20keyboard%20and%20mouse%20operations%20as%20output%2C%20similar%20to%0Ahuman-computer%20interaction.%20The%20main%20challenges%20of%20achieving%20GCC%20are%3A%201%29%20the%0Amultimodal%20observations%20for%20decision-making%2C%202%29%20the%20requirements%20of%20accurate%0Acontrol%20of%20keyboard%20and%20mouse%2C%203%29%20the%20need%20for%20long-term%20memory%20and%20reasoning%2C%0Aand%204%29%20the%20abilities%20of%20efficient%20exploration%20and%20self-improvement.%20To%20target%0AGCC%2C%20we%20introduce%20Cradle%2C%20an%20agent%20framework%20with%20six%20main%20modules%2C%20including%3A%0A1%29%20information%20gathering%20to%20extract%20multi-modality%20information%2C%202%29%0Aself-reflection%20to%20rethink%20past%20experiences%2C%203%29%20task%20inference%20to%20choose%20the%0Abest%20next%20task%2C%204%29%20skill%20curation%20for%20generating%20and%20updating%20relevant%20skills%0Afor%20given%20tasks%2C%205%29%20action%20planning%20to%20generate%20specific%20operations%20for%0Akeyboard%20and%20mouse%20control%2C%20and%206%29%20memory%20for%20storage%20and%20retrieval%20of%20past%0Aexperiences%20and%20known%20skills.%20To%20demonstrate%20the%20capabilities%20of%20generalization%0Aand%20self-improvement%20of%20Cradle%2C%20we%20deploy%20it%20in%20the%20complex%20AAA%20game%20Red%20Dead%0ARedemption%20II%2C%20serving%20as%20a%20preliminary%20attempt%20towards%20GCC%20with%20a%20challenging%0Atarget.%20To%20our%20best%20knowledge%2C%20our%20work%20is%20the%20first%20to%20enable%20LMM-based%20agents%0Ato%20follow%20the%20main%20storyline%20and%20finish%20real%20missions%20in%20complex%20AAA%20games%2C%0Awith%20minimal%20reliance%20on%20prior%20knowledge%20or%20resources.%20The%20project%20website%20is%0Aat%20https%3A//baai-agents.github.io/Cradle/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03186v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20General%20Computer%20Control%3A%20A%20Multimodal%20Agent%20for%20Red%20Dead%0A%20%20Redemption%20II%20as%20a%20Case%20Study&entry.906535625=Weihao%20Tan%20and%20Ziluo%20Ding%20and%20Wentao%20Zhang%20and%20Boyu%20Li%20and%20Bohan%20Zhou%20and%20Junpeng%20Yue%20and%20Haochong%20Xia%20and%20Jiechuan%20Jiang%20and%20Longtao%20Zheng%20and%20Xinrun%20Xu%20and%20Yifei%20Bi%20and%20Pengjie%20Gu%20and%20Xinrun%20Wang%20and%20B%C3%B6rje%20F.%20Karlsson%20and%20Bo%20An%20and%20Zongqing%20Lu&entry.1292438233=%20%20Despite%20the%20success%20in%20specific%20tasks%20and%20scenarios%2C%20existing%20foundation%0Aagents%2C%20empowered%20by%20large%20models%20%28LMs%29%20and%20advanced%20tools%2C%20still%20cannot%0Ageneralize%20to%20different%20scenarios%2C%20mainly%20due%20to%20dramatic%20differences%20in%20the%0Aobservations%20and%20actions%20across%20scenarios.%20In%20this%20work%2C%20we%20propose%20the%20General%0AComputer%20Control%20%28GCC%29%20setting%3A%20building%20foundation%20agents%20that%20can%20master%20any%0Acomputer%20task%20by%20taking%20only%20screen%20images%20%28and%20possibly%20audio%29%20of%20the%20computer%0Aas%20input%2C%20and%20producing%20keyboard%20and%20mouse%20operations%20as%20output%2C%20similar%20to%0Ahuman-computer%20interaction.%20The%20main%20challenges%20of%20achieving%20GCC%20are%3A%201%29%20the%0Amultimodal%20observations%20for%20decision-making%2C%202%29%20the%20requirements%20of%20accurate%0Acontrol%20of%20keyboard%20and%20mouse%2C%203%29%20the%20need%20for%20long-term%20memory%20and%20reasoning%2C%0Aand%204%29%20the%20abilities%20of%20efficient%20exploration%20and%20self-improvement.%20To%20target%0AGCC%2C%20we%20introduce%20Cradle%2C%20an%20agent%20framework%20with%20six%20main%20modules%2C%20including%3A%0A1%29%20information%20gathering%20to%20extract%20multi-modality%20information%2C%202%29%0Aself-reflection%20to%20rethink%20past%20experiences%2C%203%29%20task%20inference%20to%20choose%20the%0Abest%20next%20task%2C%204%29%20skill%20curation%20for%20generating%20and%20updating%20relevant%20skills%0Afor%20given%20tasks%2C%205%29%20action%20planning%20to%20generate%20specific%20operations%20for%0Akeyboard%20and%20mouse%20control%2C%20and%206%29%20memory%20for%20storage%20and%20retrieval%20of%20past%0Aexperiences%20and%20known%20skills.%20To%20demonstrate%20the%20capabilities%20of%20generalization%0Aand%20self-improvement%20of%20Cradle%2C%20we%20deploy%20it%20in%20the%20complex%20AAA%20game%20Red%20Dead%0ARedemption%20II%2C%20serving%20as%20a%20preliminary%20attempt%20towards%20GCC%20with%20a%20challenging%0Atarget.%20To%20our%20best%20knowledge%2C%20our%20work%20is%20the%20first%20to%20enable%20LMM-based%20agents%0Ato%20follow%20the%20main%20storyline%20and%20finish%20real%20missions%20in%20complex%20AAA%20games%2C%0Awith%20minimal%20reliance%20on%20prior%20knowledge%20or%20resources.%20The%20project%20website%20is%0Aat%20https%3A//baai-agents.github.io/Cradle/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03186v2&entry.124074799=Read"},
{"title": "RankDVQA-mini: Knowledge Distillation-Driven Deep Video Quality\n  Assessment", "author": "Chen Feng and Duolikun Danier and Haoran Wang and Fan Zhang and Benoit Vallade and Alex Mackin and David Bull", "abstract": "  Deep learning-based video quality assessment (deep VQA) has demonstrated\nsignificant potential in surpassing conventional metrics, with promising\nimprovements in terms of correlation with human perception. However, the\npractical deployment of such deep VQA models is often limited due to their high\ncomputational complexity and large memory requirements. To address this issue,\nwe aim to significantly reduce the model size and runtime of one of the\nstate-of-the-art deep VQA methods, RankDVQA, by employing a two-phase workflow\nthat integrates pruning-driven model compression with multi-level knowledge\ndistillation. The resulting lightweight full reference quality metric,\nRankDVQA-mini, requires less than 10% of the model parameters compared to its\nfull version (14% in terms of FLOPs), while still retaining a quality\nprediction performance that is superior to most existing deep VQA methods. The\nsource code of the RankDVQA-mini has been released at\nhttps://chenfeng-bristol.github.io/RankDVQA-mini/ for public evaluation.\n", "link": "http://arxiv.org/abs/2312.08864v2", "date": "2024-03-07", "relevancy": 2.2474, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5646}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5619}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5607}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20RankDVQA-mini%3A%20Knowledge%20Distillation-Driven%20Deep%20Video%20Quality%0A%20%20Assessment&body=Title%3A%20RankDVQA-mini%3A%20Knowledge%20Distillation-Driven%20Deep%20Video%20Quality%0A%20%20Assessment%0AAuthor%3A%20Chen%20Feng%20and%20Duolikun%20Danier%20and%20Haoran%20Wang%20and%20Fan%20Zhang%20and%20Benoit%20Vallade%20and%20Alex%20Mackin%20and%20David%20Bull%0AAbstract%3A%20%20%20Deep%20learning-based%20video%20quality%20assessment%20%28deep%20VQA%29%20has%20demonstrated%0Asignificant%20potential%20in%20surpassing%20conventional%20metrics%2C%20with%20promising%0Aimprovements%20in%20terms%20of%20correlation%20with%20human%20perception.%20However%2C%20the%0Apractical%20deployment%20of%20such%20deep%20VQA%20models%20is%20often%20limited%20due%20to%20their%20high%0Acomputational%20complexity%20and%20large%20memory%20requirements.%20To%20address%20this%20issue%2C%0Awe%20aim%20to%20significantly%20reduce%20the%20model%20size%20and%20runtime%20of%20one%20of%20the%0Astate-of-the-art%20deep%20VQA%20methods%2C%20RankDVQA%2C%20by%20employing%20a%20two-phase%20workflow%0Athat%20integrates%20pruning-driven%20model%20compression%20with%20multi-level%20knowledge%0Adistillation.%20The%20resulting%20lightweight%20full%20reference%20quality%20metric%2C%0ARankDVQA-mini%2C%20requires%20less%20than%2010%25%20of%20the%20model%20parameters%20compared%20to%20its%0Afull%20version%20%2814%25%20in%20terms%20of%20FLOPs%29%2C%20while%20still%20retaining%20a%20quality%0Aprediction%20performance%20that%20is%20superior%20to%20most%20existing%20deep%20VQA%20methods.%20The%0Asource%20code%20of%20the%20RankDVQA-mini%20has%20been%20released%20at%0Ahttps%3A//chenfeng-bristol.github.io/RankDVQA-mini/%20for%20public%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08864v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RankDVQA-mini%3A%20Knowledge%20Distillation-Driven%20Deep%20Video%20Quality%0A%20%20Assessment&entry.906535625=Chen%20Feng%20and%20Duolikun%20Danier%20and%20Haoran%20Wang%20and%20Fan%20Zhang%20and%20Benoit%20Vallade%20and%20Alex%20Mackin%20and%20David%20Bull&entry.1292438233=%20%20Deep%20learning-based%20video%20quality%20assessment%20%28deep%20VQA%29%20has%20demonstrated%0Asignificant%20potential%20in%20surpassing%20conventional%20metrics%2C%20with%20promising%0Aimprovements%20in%20terms%20of%20correlation%20with%20human%20perception.%20However%2C%20the%0Apractical%20deployment%20of%20such%20deep%20VQA%20models%20is%20often%20limited%20due%20to%20their%20high%0Acomputational%20complexity%20and%20large%20memory%20requirements.%20To%20address%20this%20issue%2C%0Awe%20aim%20to%20significantly%20reduce%20the%20model%20size%20and%20runtime%20of%20one%20of%20the%0Astate-of-the-art%20deep%20VQA%20methods%2C%20RankDVQA%2C%20by%20employing%20a%20two-phase%20workflow%0Athat%20integrates%20pruning-driven%20model%20compression%20with%20multi-level%20knowledge%0Adistillation.%20The%20resulting%20lightweight%20full%20reference%20quality%20metric%2C%0ARankDVQA-mini%2C%20requires%20less%20than%2010%25%20of%20the%20model%20parameters%20compared%20to%20its%0Afull%20version%20%2814%25%20in%20terms%20of%20FLOPs%29%2C%20while%20still%20retaining%20a%20quality%0Aprediction%20performance%20that%20is%20superior%20to%20most%20existing%20deep%20VQA%20methods.%20The%0Asource%20code%20of%20the%20RankDVQA-mini%20has%20been%20released%20at%0Ahttps%3A//chenfeng-bristol.github.io/RankDVQA-mini/%20for%20public%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08864v2&entry.124074799=Read"},
{"title": "False Positive Sampling-based Data Augmentation for Enhanced 3D Object\n  Detection Accuracy", "author": "Jiyong Oh and Junhaeng Lee and Woongchan Byun and Minsang Kong and Sang Hun Lee", "abstract": "  Recent studies have focused on enhancing the performance of 3D object\ndetection models. Among various approaches, ground-truth sampling has been\nproposed as an augmentation technique to address the challenges posed by\nlimited ground-truth data. However, an inherent issue with ground-truth\nsampling is its tendency to increase false positives. Therefore, this study\naims to overcome the limitations of ground-truth sampling and improve the\nperformance of 3D object detection models by developing a new augmentation\ntechnique called false-positive sampling. False-positive sampling involves\nretraining the model using point clouds that are identified as false positives\nin the model's predictions. We propose an algorithm that utilizes both\nground-truth and false-positive sampling and an algorithm for building the\nfalse-positive sample database. Additionally, we analyze the principles behind\nthe performance enhancement due to false-positive sampling and propose a\ntechnique that applies the concept of curriculum learning to the sampling\nstrategy that encompasses both false-positive and ground-truth sampling\ntechniques. Our experiments demonstrate that models utilizing false-positive\nsampling show a reduction in false positives and exhibit improved object\ndetection performance. On the KITTI and Waymo Open datasets, models with\nfalse-positive sampling surpass the baseline models by a large margin.\n", "link": "http://arxiv.org/abs/2403.02639v2", "date": "2024-03-07", "relevancy": 2.235, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5892}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.564}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5262}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20False%20Positive%20Sampling-based%20Data%20Augmentation%20for%20Enhanced%203D%20Object%0A%20%20Detection%20Accuracy&body=Title%3A%20False%20Positive%20Sampling-based%20Data%20Augmentation%20for%20Enhanced%203D%20Object%0A%20%20Detection%20Accuracy%0AAuthor%3A%20Jiyong%20Oh%20and%20Junhaeng%20Lee%20and%20Woongchan%20Byun%20and%20Minsang%20Kong%20and%20Sang%20Hun%20Lee%0AAbstract%3A%20%20%20Recent%20studies%20have%20focused%20on%20enhancing%20the%20performance%20of%203D%20object%0Adetection%20models.%20Among%20various%20approaches%2C%20ground-truth%20sampling%20has%20been%0Aproposed%20as%20an%20augmentation%20technique%20to%20address%20the%20challenges%20posed%20by%0Alimited%20ground-truth%20data.%20However%2C%20an%20inherent%20issue%20with%20ground-truth%0Asampling%20is%20its%20tendency%20to%20increase%20false%20positives.%20Therefore%2C%20this%20study%0Aaims%20to%20overcome%20the%20limitations%20of%20ground-truth%20sampling%20and%20improve%20the%0Aperformance%20of%203D%20object%20detection%20models%20by%20developing%20a%20new%20augmentation%0Atechnique%20called%20false-positive%20sampling.%20False-positive%20sampling%20involves%0Aretraining%20the%20model%20using%20point%20clouds%20that%20are%20identified%20as%20false%20positives%0Ain%20the%20model%27s%20predictions.%20We%20propose%20an%20algorithm%20that%20utilizes%20both%0Aground-truth%20and%20false-positive%20sampling%20and%20an%20algorithm%20for%20building%20the%0Afalse-positive%20sample%20database.%20Additionally%2C%20we%20analyze%20the%20principles%20behind%0Athe%20performance%20enhancement%20due%20to%20false-positive%20sampling%20and%20propose%20a%0Atechnique%20that%20applies%20the%20concept%20of%20curriculum%20learning%20to%20the%20sampling%0Astrategy%20that%20encompasses%20both%20false-positive%20and%20ground-truth%20sampling%0Atechniques.%20Our%20experiments%20demonstrate%20that%20models%20utilizing%20false-positive%0Asampling%20show%20a%20reduction%20in%20false%20positives%20and%20exhibit%20improved%20object%0Adetection%20performance.%20On%20the%20KITTI%20and%20Waymo%20Open%20datasets%2C%20models%20with%0Afalse-positive%20sampling%20surpass%20the%20baseline%20models%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02639v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=False%20Positive%20Sampling-based%20Data%20Augmentation%20for%20Enhanced%203D%20Object%0A%20%20Detection%20Accuracy&entry.906535625=Jiyong%20Oh%20and%20Junhaeng%20Lee%20and%20Woongchan%20Byun%20and%20Minsang%20Kong%20and%20Sang%20Hun%20Lee&entry.1292438233=%20%20Recent%20studies%20have%20focused%20on%20enhancing%20the%20performance%20of%203D%20object%0Adetection%20models.%20Among%20various%20approaches%2C%20ground-truth%20sampling%20has%20been%0Aproposed%20as%20an%20augmentation%20technique%20to%20address%20the%20challenges%20posed%20by%0Alimited%20ground-truth%20data.%20However%2C%20an%20inherent%20issue%20with%20ground-truth%0Asampling%20is%20its%20tendency%20to%20increase%20false%20positives.%20Therefore%2C%20this%20study%0Aaims%20to%20overcome%20the%20limitations%20of%20ground-truth%20sampling%20and%20improve%20the%0Aperformance%20of%203D%20object%20detection%20models%20by%20developing%20a%20new%20augmentation%0Atechnique%20called%20false-positive%20sampling.%20False-positive%20sampling%20involves%0Aretraining%20the%20model%20using%20point%20clouds%20that%20are%20identified%20as%20false%20positives%0Ain%20the%20model%27s%20predictions.%20We%20propose%20an%20algorithm%20that%20utilizes%20both%0Aground-truth%20and%20false-positive%20sampling%20and%20an%20algorithm%20for%20building%20the%0Afalse-positive%20sample%20database.%20Additionally%2C%20we%20analyze%20the%20principles%20behind%0Athe%20performance%20enhancement%20due%20to%20false-positive%20sampling%20and%20propose%20a%0Atechnique%20that%20applies%20the%20concept%20of%20curriculum%20learning%20to%20the%20sampling%0Astrategy%20that%20encompasses%20both%20false-positive%20and%20ground-truth%20sampling%0Atechniques.%20Our%20experiments%20demonstrate%20that%20models%20utilizing%20false-positive%0Asampling%20show%20a%20reduction%20in%20false%20positives%20and%20exhibit%20improved%20object%0Adetection%20performance.%20On%20the%20KITTI%20and%20Waymo%20Open%20datasets%2C%20models%20with%0Afalse-positive%20sampling%20surpass%20the%20baseline%20models%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02639v2&entry.124074799=Read"},
{"title": "A$^{3}$lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment\n  for Dynamic Facial Expression Recognition with CLIP", "author": "Zeng Tao and Yan Wang and Junxiong Lin and Haoran Wang and Xinji Mai and Jiawen Yu and Xuan Tong and Ziheng Zhou and Shaoqi Yan and Qing Zhao and Liyuan Han and Wenqiang Zhang", "abstract": "  The performance of CLIP in dynamic facial expression recognition (DFER) task\ndoesn't yield exceptional results as observed in other CLIP-based\nclassification tasks. While CLIP's primary objective is to achieve alignment\nbetween images and text in the feature space, DFER poses challenges due to the\nabstract nature of text and the dynamic nature of video, making label\nrepresentation limited and perfect alignment difficult. To address this issue,\nwe have designed A$^{3}$lign-DFER, which introduces a new DFER labeling\nparadigm to comprehensively achieve alignment, thus enhancing CLIP's\nsuitability for the DFER task. Specifically, our A$^{3}$lign-DFER method is\ndesigned with multiple modules that work together to obtain the most suitable\nexpanded-dimensional embeddings for classification and to achieve alignment in\nthree key aspects: affective, dynamic, and bidirectional. We replace the input\nlabel text with a learnable Multi-Dimensional Alignment Token (MAT), enabling\nalignment of text to facial expression video samples in both affective and\ndynamic dimensions. After CLIP feature extraction, we introduce the Joint\nDynamic Alignment Synchronizer (JAS), further facilitating synchronization and\nalignment in the temporal dimension. Additionally, we implement a Bidirectional\nAlignment Training Paradigm (BAP) to ensure gradual and steady training of\nparameters for both modalities. Our insightful and concise A$^{3}$lign-DFER\nmethod achieves state-of-the-art results on multiple DFER datasets, including\nDFEW, FERV39k, and MAFW. Extensive ablation experiments and visualization\nstudies demonstrate the effectiveness of A$^{3}$lign-DFER. The code will be\navailable in the future.\n", "link": "http://arxiv.org/abs/2403.04294v1", "date": "2024-03-07", "relevancy": 2.2323, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5782}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5475}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5422}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%24%5E%7B3%7D%24lign-DFER%3A%20Pioneering%20Comprehensive%20Dynamic%20Affective%20Alignment%0A%20%20for%20Dynamic%20Facial%20Expression%20Recognition%20with%20CLIP&body=Title%3A%20A%24%5E%7B3%7D%24lign-DFER%3A%20Pioneering%20Comprehensive%20Dynamic%20Affective%20Alignment%0A%20%20for%20Dynamic%20Facial%20Expression%20Recognition%20with%20CLIP%0AAuthor%3A%20Zeng%20Tao%20and%20Yan%20Wang%20and%20Junxiong%20Lin%20and%20Haoran%20Wang%20and%20Xinji%20Mai%20and%20Jiawen%20Yu%20and%20Xuan%20Tong%20and%20Ziheng%20Zhou%20and%20Shaoqi%20Yan%20and%20Qing%20Zhao%20and%20Liyuan%20Han%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20The%20performance%20of%20CLIP%20in%20dynamic%20facial%20expression%20recognition%20%28DFER%29%20task%0Adoesn%27t%20yield%20exceptional%20results%20as%20observed%20in%20other%20CLIP-based%0Aclassification%20tasks.%20While%20CLIP%27s%20primary%20objective%20is%20to%20achieve%20alignment%0Abetween%20images%20and%20text%20in%20the%20feature%20space%2C%20DFER%20poses%20challenges%20due%20to%20the%0Aabstract%20nature%20of%20text%20and%20the%20dynamic%20nature%20of%20video%2C%20making%20label%0Arepresentation%20limited%20and%20perfect%20alignment%20difficult.%20To%20address%20this%20issue%2C%0Awe%20have%20designed%20A%24%5E%7B3%7D%24lign-DFER%2C%20which%20introduces%20a%20new%20DFER%20labeling%0Aparadigm%20to%20comprehensively%20achieve%20alignment%2C%20thus%20enhancing%20CLIP%27s%0Asuitability%20for%20the%20DFER%20task.%20Specifically%2C%20our%20A%24%5E%7B3%7D%24lign-DFER%20method%20is%0Adesigned%20with%20multiple%20modules%20that%20work%20together%20to%20obtain%20the%20most%20suitable%0Aexpanded-dimensional%20embeddings%20for%20classification%20and%20to%20achieve%20alignment%20in%0Athree%20key%20aspects%3A%20affective%2C%20dynamic%2C%20and%20bidirectional.%20We%20replace%20the%20input%0Alabel%20text%20with%20a%20learnable%20Multi-Dimensional%20Alignment%20Token%20%28MAT%29%2C%20enabling%0Aalignment%20of%20text%20to%20facial%20expression%20video%20samples%20in%20both%20affective%20and%0Adynamic%20dimensions.%20After%20CLIP%20feature%20extraction%2C%20we%20introduce%20the%20Joint%0ADynamic%20Alignment%20Synchronizer%20%28JAS%29%2C%20further%20facilitating%20synchronization%20and%0Aalignment%20in%20the%20temporal%20dimension.%20Additionally%2C%20we%20implement%20a%20Bidirectional%0AAlignment%20Training%20Paradigm%20%28BAP%29%20to%20ensure%20gradual%20and%20steady%20training%20of%0Aparameters%20for%20both%20modalities.%20Our%20insightful%20and%20concise%20A%24%5E%7B3%7D%24lign-DFER%0Amethod%20achieves%20state-of-the-art%20results%20on%20multiple%20DFER%20datasets%2C%20including%0ADFEW%2C%20FERV39k%2C%20and%20MAFW.%20Extensive%20ablation%20experiments%20and%20visualization%0Astudies%20demonstrate%20the%20effectiveness%20of%20A%24%5E%7B3%7D%24lign-DFER.%20The%20code%20will%20be%0Aavailable%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04294v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%24%5E%7B3%7D%24lign-DFER%3A%20Pioneering%20Comprehensive%20Dynamic%20Affective%20Alignment%0A%20%20for%20Dynamic%20Facial%20Expression%20Recognition%20with%20CLIP&entry.906535625=Zeng%20Tao%20and%20Yan%20Wang%20and%20Junxiong%20Lin%20and%20Haoran%20Wang%20and%20Xinji%20Mai%20and%20Jiawen%20Yu%20and%20Xuan%20Tong%20and%20Ziheng%20Zhou%20and%20Shaoqi%20Yan%20and%20Qing%20Zhao%20and%20Liyuan%20Han%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20The%20performance%20of%20CLIP%20in%20dynamic%20facial%20expression%20recognition%20%28DFER%29%20task%0Adoesn%27t%20yield%20exceptional%20results%20as%20observed%20in%20other%20CLIP-based%0Aclassification%20tasks.%20While%20CLIP%27s%20primary%20objective%20is%20to%20achieve%20alignment%0Abetween%20images%20and%20text%20in%20the%20feature%20space%2C%20DFER%20poses%20challenges%20due%20to%20the%0Aabstract%20nature%20of%20text%20and%20the%20dynamic%20nature%20of%20video%2C%20making%20label%0Arepresentation%20limited%20and%20perfect%20alignment%20difficult.%20To%20address%20this%20issue%2C%0Awe%20have%20designed%20A%24%5E%7B3%7D%24lign-DFER%2C%20which%20introduces%20a%20new%20DFER%20labeling%0Aparadigm%20to%20comprehensively%20achieve%20alignment%2C%20thus%20enhancing%20CLIP%27s%0Asuitability%20for%20the%20DFER%20task.%20Specifically%2C%20our%20A%24%5E%7B3%7D%24lign-DFER%20method%20is%0Adesigned%20with%20multiple%20modules%20that%20work%20together%20to%20obtain%20the%20most%20suitable%0Aexpanded-dimensional%20embeddings%20for%20classification%20and%20to%20achieve%20alignment%20in%0Athree%20key%20aspects%3A%20affective%2C%20dynamic%2C%20and%20bidirectional.%20We%20replace%20the%20input%0Alabel%20text%20with%20a%20learnable%20Multi-Dimensional%20Alignment%20Token%20%28MAT%29%2C%20enabling%0Aalignment%20of%20text%20to%20facial%20expression%20video%20samples%20in%20both%20affective%20and%0Adynamic%20dimensions.%20After%20CLIP%20feature%20extraction%2C%20we%20introduce%20the%20Joint%0ADynamic%20Alignment%20Synchronizer%20%28JAS%29%2C%20further%20facilitating%20synchronization%20and%0Aalignment%20in%20the%20temporal%20dimension.%20Additionally%2C%20we%20implement%20a%20Bidirectional%0AAlignment%20Training%20Paradigm%20%28BAP%29%20to%20ensure%20gradual%20and%20steady%20training%20of%0Aparameters%20for%20both%20modalities.%20Our%20insightful%20and%20concise%20A%24%5E%7B3%7D%24lign-DFER%0Amethod%20achieves%20state-of-the-art%20results%20on%20multiple%20DFER%20datasets%2C%20including%0ADFEW%2C%20FERV39k%2C%20and%20MAFW.%20Extensive%20ablation%20experiments%20and%20visualization%0Astudies%20demonstrate%20the%20effectiveness%20of%20A%24%5E%7B3%7D%24lign-DFER.%20The%20code%20will%20be%0Aavailable%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04294v1&entry.124074799=Read"},
{"title": "Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical\n  Spatial and Temporal Denoiser", "author": "Qingyuan Cai and Xuecai Hu and Saihui Hou and Li Yao and Yongzhen Huang", "abstract": "  Recently, diffusion-based methods for monocular 3D human pose estimation have\nachieved state-of-the-art (SOTA) performance by directly regressing the 3D\njoint coordinates from the 2D pose sequence. Although some methods decompose\nthe task into bone length and bone direction prediction based on the human\nanatomical skeleton to explicitly incorporate more human body prior\nconstraints, the performance of these methods is significantly lower than that\nof the SOTA diffusion-based methods. This can be attributed to the tree\nstructure of the human skeleton. Direct application of the disentangled method\ncould amplify the accumulation of hierarchical errors, propagating through each\nhierarchy. Meanwhile, the hierarchical information has not been fully explored\nby the previous methods. To address these problems, a Disentangled\nDiffusion-based 3D Human Pose Estimation method with Hierarchical Spatial and\nTemporal Denoiser is proposed, termed DDHPose. In our approach: (1) We\ndisentangle the 3D pose and diffuse the bone length and bone direction during\nthe forward process of the diffusion model to effectively model the human pose\nprior. A disentanglement loss is proposed to supervise diffusion model\nlearning. (2) For the reverse process, we propose Hierarchical Spatial and\nTemporal Denoiser (HSTDenoiser) to improve the hierarchical modeling of each\njoint. Our HSTDenoiser comprises two components: the Hierarchical-Related\nSpatial Transformer (HRST) and the Hierarchical-Related Temporal Transformer\n(HRTT). HRST exploits joint spatial information and the influence of the parent\njoint on each joint for spatial modeling, while HRTT utilizes information from\nboth the joint and its hierarchical adjacent joints to explore the hierarchical\ntemporal correlations among joints.\n", "link": "http://arxiv.org/abs/2403.04444v1", "date": "2024-03-07", "relevancy": 2.2299, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5869}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5575}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.528}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Diffusion-Based%203D%20Human%20Pose%20Estimation%20with%20Hierarchical%0A%20%20Spatial%20and%20Temporal%20Denoiser&body=Title%3A%20Disentangled%20Diffusion-Based%203D%20Human%20Pose%20Estimation%20with%20Hierarchical%0A%20%20Spatial%20and%20Temporal%20Denoiser%0AAuthor%3A%20Qingyuan%20Cai%20and%20Xuecai%20Hu%20and%20Saihui%20Hou%20and%20Li%20Yao%20and%20Yongzhen%20Huang%0AAbstract%3A%20%20%20Recently%2C%20diffusion-based%20methods%20for%20monocular%203D%20human%20pose%20estimation%20have%0Aachieved%20state-of-the-art%20%28SOTA%29%20performance%20by%20directly%20regressing%20the%203D%0Ajoint%20coordinates%20from%20the%202D%20pose%20sequence.%20Although%20some%20methods%20decompose%0Athe%20task%20into%20bone%20length%20and%20bone%20direction%20prediction%20based%20on%20the%20human%0Aanatomical%20skeleton%20to%20explicitly%20incorporate%20more%20human%20body%20prior%0Aconstraints%2C%20the%20performance%20of%20these%20methods%20is%20significantly%20lower%20than%20that%0Aof%20the%20SOTA%20diffusion-based%20methods.%20This%20can%20be%20attributed%20to%20the%20tree%0Astructure%20of%20the%20human%20skeleton.%20Direct%20application%20of%20the%20disentangled%20method%0Acould%20amplify%20the%20accumulation%20of%20hierarchical%20errors%2C%20propagating%20through%20each%0Ahierarchy.%20Meanwhile%2C%20the%20hierarchical%20information%20has%20not%20been%20fully%20explored%0Aby%20the%20previous%20methods.%20To%20address%20these%20problems%2C%20a%20Disentangled%0ADiffusion-based%203D%20Human%20Pose%20Estimation%20method%20with%20Hierarchical%20Spatial%20and%0ATemporal%20Denoiser%20is%20proposed%2C%20termed%20DDHPose.%20In%20our%20approach%3A%20%281%29%20We%0Adisentangle%20the%203D%20pose%20and%20diffuse%20the%20bone%20length%20and%20bone%20direction%20during%0Athe%20forward%20process%20of%20the%20diffusion%20model%20to%20effectively%20model%20the%20human%20pose%0Aprior.%20A%20disentanglement%20loss%20is%20proposed%20to%20supervise%20diffusion%20model%0Alearning.%20%282%29%20For%20the%20reverse%20process%2C%20we%20propose%20Hierarchical%20Spatial%20and%0ATemporal%20Denoiser%20%28HSTDenoiser%29%20to%20improve%20the%20hierarchical%20modeling%20of%20each%0Ajoint.%20Our%20HSTDenoiser%20comprises%20two%20components%3A%20the%20Hierarchical-Related%0ASpatial%20Transformer%20%28HRST%29%20and%20the%20Hierarchical-Related%20Temporal%20Transformer%0A%28HRTT%29.%20HRST%20exploits%20joint%20spatial%20information%20and%20the%20influence%20of%20the%20parent%0Ajoint%20on%20each%20joint%20for%20spatial%20modeling%2C%20while%20HRTT%20utilizes%20information%20from%0Aboth%20the%20joint%20and%20its%20hierarchical%20adjacent%20joints%20to%20explore%20the%20hierarchical%0Atemporal%20correlations%20among%20joints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04444v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Diffusion-Based%203D%20Human%20Pose%20Estimation%20with%20Hierarchical%0A%20%20Spatial%20and%20Temporal%20Denoiser&entry.906535625=Qingyuan%20Cai%20and%20Xuecai%20Hu%20and%20Saihui%20Hou%20and%20Li%20Yao%20and%20Yongzhen%20Huang&entry.1292438233=%20%20Recently%2C%20diffusion-based%20methods%20for%20monocular%203D%20human%20pose%20estimation%20have%0Aachieved%20state-of-the-art%20%28SOTA%29%20performance%20by%20directly%20regressing%20the%203D%0Ajoint%20coordinates%20from%20the%202D%20pose%20sequence.%20Although%20some%20methods%20decompose%0Athe%20task%20into%20bone%20length%20and%20bone%20direction%20prediction%20based%20on%20the%20human%0Aanatomical%20skeleton%20to%20explicitly%20incorporate%20more%20human%20body%20prior%0Aconstraints%2C%20the%20performance%20of%20these%20methods%20is%20significantly%20lower%20than%20that%0Aof%20the%20SOTA%20diffusion-based%20methods.%20This%20can%20be%20attributed%20to%20the%20tree%0Astructure%20of%20the%20human%20skeleton.%20Direct%20application%20of%20the%20disentangled%20method%0Acould%20amplify%20the%20accumulation%20of%20hierarchical%20errors%2C%20propagating%20through%20each%0Ahierarchy.%20Meanwhile%2C%20the%20hierarchical%20information%20has%20not%20been%20fully%20explored%0Aby%20the%20previous%20methods.%20To%20address%20these%20problems%2C%20a%20Disentangled%0ADiffusion-based%203D%20Human%20Pose%20Estimation%20method%20with%20Hierarchical%20Spatial%20and%0ATemporal%20Denoiser%20is%20proposed%2C%20termed%20DDHPose.%20In%20our%20approach%3A%20%281%29%20We%0Adisentangle%20the%203D%20pose%20and%20diffuse%20the%20bone%20length%20and%20bone%20direction%20during%0Athe%20forward%20process%20of%20the%20diffusion%20model%20to%20effectively%20model%20the%20human%20pose%0Aprior.%20A%20disentanglement%20loss%20is%20proposed%20to%20supervise%20diffusion%20model%0Alearning.%20%282%29%20For%20the%20reverse%20process%2C%20we%20propose%20Hierarchical%20Spatial%20and%0ATemporal%20Denoiser%20%28HSTDenoiser%29%20to%20improve%20the%20hierarchical%20modeling%20of%20each%0Ajoint.%20Our%20HSTDenoiser%20comprises%20two%20components%3A%20the%20Hierarchical-Related%0ASpatial%20Transformer%20%28HRST%29%20and%20the%20Hierarchical-Related%20Temporal%20Transformer%0A%28HRTT%29.%20HRST%20exploits%20joint%20spatial%20information%20and%20the%20influence%20of%20the%20parent%0Ajoint%20on%20each%20joint%20for%20spatial%20modeling%2C%20while%20HRTT%20utilizes%20information%20from%0Aboth%20the%20joint%20and%20its%20hierarchical%20adjacent%20joints%20to%20explore%20the%20hierarchical%0Atemporal%20correlations%20among%20joints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04444v1&entry.124074799=Read"},
{"title": "High-Level Parallelism and Nested Features for Dynamic Inference Cost\n  and Top-Down Attention", "author": "Andr\u00e9 Peter Kelm and Niels Hannemann and Bruno Heberle and Lucas Schmidt and Tim Rolff and Christian Wilms and Ehsan Yaghoubi and Simone Frintrop", "abstract": "  This paper introduces a novel network topology that seamlessly integrates\ndynamic inference cost with a top-down attention mechanism, addressing two\nsignificant gaps in traditional deep learning models. Drawing inspiration from\nhuman perception, we combine sequential processing of generic low-level\nfeatures with parallelism and nesting of high-level features. This design not\nonly reflects a finding from recent neuroscience research regarding - spatially\nand contextually distinct neural activations - in human cortex, but also\nintroduces a novel \"cutout\" technique: the ability to selectively activate\n%segments of the network for task-relevant only network segments of\ntask-relevant categories to optimize inference cost and eliminate the need for\nre-training. We believe this paves the way for future network designs that are\nlightweight and adaptable, making them suitable for a wide range of\napplications, from compact edge devices to large-scale clouds. Our proposed\ntopology also comes with a built-in top-down attention mechanism, which allows\nprocessing to be directly influenced by either enhancing or inhibiting\ncategory-specific high-level features, drawing parallels to the selective\nattention mechanism observed in human cognition. Using targeted external\nsignals, we experimentally enhanced predictions across all tested models. In\nterms of dynamic inference cost our methodology can achieve an exclusion of up\nto $73.48\\,\\%$ of parameters and $84.41\\,\\%$ fewer giga-multiply-accumulate\n(GMAC) operations, analysis against comparative baselines show an average\nreduction of $40\\,\\%$ in parameters and $8\\,\\%$ in GMACs across the cases we\nevaluated.\n", "link": "http://arxiv.org/abs/2308.05128v2", "date": "2024-03-07", "relevancy": 2.2242, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.576}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5436}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5372}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20High-Level%20Parallelism%20and%20Nested%20Features%20for%20Dynamic%20Inference%20Cost%0A%20%20and%20Top-Down%20Attention&body=Title%3A%20High-Level%20Parallelism%20and%20Nested%20Features%20for%20Dynamic%20Inference%20Cost%0A%20%20and%20Top-Down%20Attention%0AAuthor%3A%20Andr%C3%A9%20Peter%20Kelm%20and%20Niels%20Hannemann%20and%20Bruno%20Heberle%20and%20Lucas%20Schmidt%20and%20Tim%20Rolff%20and%20Christian%20Wilms%20and%20Ehsan%20Yaghoubi%20and%20Simone%20Frintrop%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20network%20topology%20that%20seamlessly%20integrates%0Adynamic%20inference%20cost%20with%20a%20top-down%20attention%20mechanism%2C%20addressing%20two%0Asignificant%20gaps%20in%20traditional%20deep%20learning%20models.%20Drawing%20inspiration%20from%0Ahuman%20perception%2C%20we%20combine%20sequential%20processing%20of%20generic%20low-level%0Afeatures%20with%20parallelism%20and%20nesting%20of%20high-level%20features.%20This%20design%20not%0Aonly%20reflects%20a%20finding%20from%20recent%20neuroscience%20research%20regarding%20-%20spatially%0Aand%20contextually%20distinct%20neural%20activations%20-%20in%20human%20cortex%2C%20but%20also%0Aintroduces%20a%20novel%20%22cutout%22%20technique%3A%20the%20ability%20to%20selectively%20activate%0A%25segments%20of%20the%20network%20for%20task-relevant%20only%20network%20segments%20of%0Atask-relevant%20categories%20to%20optimize%20inference%20cost%20and%20eliminate%20the%20need%20for%0Are-training.%20We%20believe%20this%20paves%20the%20way%20for%20future%20network%20designs%20that%20are%0Alightweight%20and%20adaptable%2C%20making%20them%20suitable%20for%20a%20wide%20range%20of%0Aapplications%2C%20from%20compact%20edge%20devices%20to%20large-scale%20clouds.%20Our%20proposed%0Atopology%20also%20comes%20with%20a%20built-in%20top-down%20attention%20mechanism%2C%20which%20allows%0Aprocessing%20to%20be%20directly%20influenced%20by%20either%20enhancing%20or%20inhibiting%0Acategory-specific%20high-level%20features%2C%20drawing%20parallels%20to%20the%20selective%0Aattention%20mechanism%20observed%20in%20human%20cognition.%20Using%20targeted%20external%0Asignals%2C%20we%20experimentally%20enhanced%20predictions%20across%20all%20tested%20models.%20In%0Aterms%20of%20dynamic%20inference%20cost%20our%20methodology%20can%20achieve%20an%20exclusion%20of%20up%0Ato%20%2473.48%5C%2C%5C%25%24%20of%20parameters%20and%20%2484.41%5C%2C%5C%25%24%20fewer%20giga-multiply-accumulate%0A%28GMAC%29%20operations%2C%20analysis%20against%20comparative%20baselines%20show%20an%20average%0Areduction%20of%20%2440%5C%2C%5C%25%24%20in%20parameters%20and%20%248%5C%2C%5C%25%24%20in%20GMACs%20across%20the%20cases%20we%0Aevaluated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05128v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Level%20Parallelism%20and%20Nested%20Features%20for%20Dynamic%20Inference%20Cost%0A%20%20and%20Top-Down%20Attention&entry.906535625=Andr%C3%A9%20Peter%20Kelm%20and%20Niels%20Hannemann%20and%20Bruno%20Heberle%20and%20Lucas%20Schmidt%20and%20Tim%20Rolff%20and%20Christian%20Wilms%20and%20Ehsan%20Yaghoubi%20and%20Simone%20Frintrop&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20network%20topology%20that%20seamlessly%20integrates%0Adynamic%20inference%20cost%20with%20a%20top-down%20attention%20mechanism%2C%20addressing%20two%0Asignificant%20gaps%20in%20traditional%20deep%20learning%20models.%20Drawing%20inspiration%20from%0Ahuman%20perception%2C%20we%20combine%20sequential%20processing%20of%20generic%20low-level%0Afeatures%20with%20parallelism%20and%20nesting%20of%20high-level%20features.%20This%20design%20not%0Aonly%20reflects%20a%20finding%20from%20recent%20neuroscience%20research%20regarding%20-%20spatially%0Aand%20contextually%20distinct%20neural%20activations%20-%20in%20human%20cortex%2C%20but%20also%0Aintroduces%20a%20novel%20%22cutout%22%20technique%3A%20the%20ability%20to%20selectively%20activate%0A%25segments%20of%20the%20network%20for%20task-relevant%20only%20network%20segments%20of%0Atask-relevant%20categories%20to%20optimize%20inference%20cost%20and%20eliminate%20the%20need%20for%0Are-training.%20We%20believe%20this%20paves%20the%20way%20for%20future%20network%20designs%20that%20are%0Alightweight%20and%20adaptable%2C%20making%20them%20suitable%20for%20a%20wide%20range%20of%0Aapplications%2C%20from%20compact%20edge%20devices%20to%20large-scale%20clouds.%20Our%20proposed%0Atopology%20also%20comes%20with%20a%20built-in%20top-down%20attention%20mechanism%2C%20which%20allows%0Aprocessing%20to%20be%20directly%20influenced%20by%20either%20enhancing%20or%20inhibiting%0Acategory-specific%20high-level%20features%2C%20drawing%20parallels%20to%20the%20selective%0Aattention%20mechanism%20observed%20in%20human%20cognition.%20Using%20targeted%20external%0Asignals%2C%20we%20experimentally%20enhanced%20predictions%20across%20all%20tested%20models.%20In%0Aterms%20of%20dynamic%20inference%20cost%20our%20methodology%20can%20achieve%20an%20exclusion%20of%20up%0Ato%20%2473.48%5C%2C%5C%25%24%20of%20parameters%20and%20%2484.41%5C%2C%5C%25%24%20fewer%20giga-multiply-accumulate%0A%28GMAC%29%20operations%2C%20analysis%20against%20comparative%20baselines%20show%20an%20average%0Areduction%20of%20%2440%5C%2C%5C%25%24%20in%20parameters%20and%20%248%5C%2C%5C%25%24%20in%20GMACs%20across%20the%20cases%20we%0Aevaluated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05128v2&entry.124074799=Read"},
{"title": "VeCLIP: Improving CLIP Training via Visual-enriched Captions", "author": "Zhengfeng Lai and Haotian Zhang and Bowen Zhang and Wentao Wu and Haoping Bai and Aleksei Timofeev and Xianzhi Du and Zhe Gan and Jiulong Shan and Chen-Nee Chuah and Yinfei Yang and Meng Cao", "abstract": "  Large-scale web-crawled datasets are fundamental for the success of\npre-training vision-language models, such as CLIP. However, the inherent noise\nand potential irrelevance of web-crawled AltTexts pose challenges in achieving\nprecise image-text alignment. Existing methods utilizing large language models\n(LLMs) for caption rewriting have shown promise on small, curated datasets like\nCC3M and CC12M. This study introduces a scalable pipeline for noisy caption\nrewriting. Unlike recent LLM rewriting techniques, we emphasize the\nincorporation of visual concepts into captions, termed as Visual-enriched\nCaptions (VeCap). To ensure data diversity, we propose a novel mixed training\nscheme that optimizes the utilization of AltTexts alongside newly generated\nVeCap. We showcase the adaptation of this method for training CLIP on\nlarge-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective\npipeline, we effortlessly scale our dataset up to 300 million samples named\nVeCap dataset. Our results show significant advantages in image-text alignment\nand overall model performance. For example, VeCLIP achieves up to +25.2% gain\nin COCO and Flickr30k retrieval tasks under the 12M setting. For data\nefficiency, VeCLIP achieves +3% gain while only using 14% of the data employed\nin the vanilla CLIP and 11% in ALIGN. We also note the VeCap data is\ncomplementary with other well curated datasets good for zero-shot\nclassification tasks. When combining VeCap and DFN, our model can achieve\nstrong performance on both of image-text retrieval and zero-shot classification\ntasks, e.g. 83.1% accuracy@1 on ImageNet zero-shot for a H/14 model. We release\nthe pre-trained models at https://github.com/apple/ml-veclip.\n", "link": "http://arxiv.org/abs/2310.07699v2", "date": "2024-03-07", "relevancy": 2.2225, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5942}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5329}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5157}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20VeCLIP%3A%20Improving%20CLIP%20Training%20via%20Visual-enriched%20Captions&body=Title%3A%20VeCLIP%3A%20Improving%20CLIP%20Training%20via%20Visual-enriched%20Captions%0AAuthor%3A%20Zhengfeng%20Lai%20and%20Haotian%20Zhang%20and%20Bowen%20Zhang%20and%20Wentao%20Wu%20and%20Haoping%20Bai%20and%20Aleksei%20Timofeev%20and%20Xianzhi%20Du%20and%20Zhe%20Gan%20and%20Jiulong%20Shan%20and%20Chen-Nee%20Chuah%20and%20Yinfei%20Yang%20and%20Meng%20Cao%0AAbstract%3A%20%20%20Large-scale%20web-crawled%20datasets%20are%20fundamental%20for%20the%20success%20of%0Apre-training%20vision-language%20models%2C%20such%20as%20CLIP.%20However%2C%20the%20inherent%20noise%0Aand%20potential%20irrelevance%20of%20web-crawled%20AltTexts%20pose%20challenges%20in%20achieving%0Aprecise%20image-text%20alignment.%20Existing%20methods%20utilizing%20large%20language%20models%0A%28LLMs%29%20for%20caption%20rewriting%20have%20shown%20promise%20on%20small%2C%20curated%20datasets%20like%0ACC3M%20and%20CC12M.%20This%20study%20introduces%20a%20scalable%20pipeline%20for%20noisy%20caption%0Arewriting.%20Unlike%20recent%20LLM%20rewriting%20techniques%2C%20we%20emphasize%20the%0Aincorporation%20of%20visual%20concepts%20into%20captions%2C%20termed%20as%20Visual-enriched%0ACaptions%20%28VeCap%29.%20To%20ensure%20data%20diversity%2C%20we%20propose%20a%20novel%20mixed%20training%0Ascheme%20that%20optimizes%20the%20utilization%20of%20AltTexts%20alongside%20newly%20generated%0AVeCap.%20We%20showcase%20the%20adaptation%20of%20this%20method%20for%20training%20CLIP%20on%0Alarge-scale%20web-crawled%20datasets%2C%20termed%20VeCLIP.%20Employing%20this%20cost-effective%0Apipeline%2C%20we%20effortlessly%20scale%20our%20dataset%20up%20to%20300%20million%20samples%20named%0AVeCap%20dataset.%20Our%20results%20show%20significant%20advantages%20in%20image-text%20alignment%0Aand%20overall%20model%20performance.%20For%20example%2C%20VeCLIP%20achieves%20up%20to%20%2B25.2%25%20gain%0Ain%20COCO%20and%20Flickr30k%20retrieval%20tasks%20under%20the%2012M%20setting.%20For%20data%0Aefficiency%2C%20VeCLIP%20achieves%20%2B3%25%20gain%20while%20only%20using%2014%25%20of%20the%20data%20employed%0Ain%20the%20vanilla%20CLIP%20and%2011%25%20in%20ALIGN.%20We%20also%20note%20the%20VeCap%20data%20is%0Acomplementary%20with%20other%20well%20curated%20datasets%20good%20for%20zero-shot%0Aclassification%20tasks.%20When%20combining%20VeCap%20and%20DFN%2C%20our%20model%20can%20achieve%0Astrong%20performance%20on%20both%20of%20image-text%20retrieval%20and%20zero-shot%20classification%0Atasks%2C%20e.g.%2083.1%25%20accuracy%401%20on%20ImageNet%20zero-shot%20for%20a%20H/14%20model.%20We%20release%0Athe%20pre-trained%20models%20at%20https%3A//github.com/apple/ml-veclip.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07699v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VeCLIP%3A%20Improving%20CLIP%20Training%20via%20Visual-enriched%20Captions&entry.906535625=Zhengfeng%20Lai%20and%20Haotian%20Zhang%20and%20Bowen%20Zhang%20and%20Wentao%20Wu%20and%20Haoping%20Bai%20and%20Aleksei%20Timofeev%20and%20Xianzhi%20Du%20and%20Zhe%20Gan%20and%20Jiulong%20Shan%20and%20Chen-Nee%20Chuah%20and%20Yinfei%20Yang%20and%20Meng%20Cao&entry.1292438233=%20%20Large-scale%20web-crawled%20datasets%20are%20fundamental%20for%20the%20success%20of%0Apre-training%20vision-language%20models%2C%20such%20as%20CLIP.%20However%2C%20the%20inherent%20noise%0Aand%20potential%20irrelevance%20of%20web-crawled%20AltTexts%20pose%20challenges%20in%20achieving%0Aprecise%20image-text%20alignment.%20Existing%20methods%20utilizing%20large%20language%20models%0A%28LLMs%29%20for%20caption%20rewriting%20have%20shown%20promise%20on%20small%2C%20curated%20datasets%20like%0ACC3M%20and%20CC12M.%20This%20study%20introduces%20a%20scalable%20pipeline%20for%20noisy%20caption%0Arewriting.%20Unlike%20recent%20LLM%20rewriting%20techniques%2C%20we%20emphasize%20the%0Aincorporation%20of%20visual%20concepts%20into%20captions%2C%20termed%20as%20Visual-enriched%0ACaptions%20%28VeCap%29.%20To%20ensure%20data%20diversity%2C%20we%20propose%20a%20novel%20mixed%20training%0Ascheme%20that%20optimizes%20the%20utilization%20of%20AltTexts%20alongside%20newly%20generated%0AVeCap.%20We%20showcase%20the%20adaptation%20of%20this%20method%20for%20training%20CLIP%20on%0Alarge-scale%20web-crawled%20datasets%2C%20termed%20VeCLIP.%20Employing%20this%20cost-effective%0Apipeline%2C%20we%20effortlessly%20scale%20our%20dataset%20up%20to%20300%20million%20samples%20named%0AVeCap%20dataset.%20Our%20results%20show%20significant%20advantages%20in%20image-text%20alignment%0Aand%20overall%20model%20performance.%20For%20example%2C%20VeCLIP%20achieves%20up%20to%20%2B25.2%25%20gain%0Ain%20COCO%20and%20Flickr30k%20retrieval%20tasks%20under%20the%2012M%20setting.%20For%20data%0Aefficiency%2C%20VeCLIP%20achieves%20%2B3%25%20gain%20while%20only%20using%2014%25%20of%20the%20data%20employed%0Ain%20the%20vanilla%20CLIP%20and%2011%25%20in%20ALIGN.%20We%20also%20note%20the%20VeCap%20data%20is%0Acomplementary%20with%20other%20well%20curated%20datasets%20good%20for%20zero-shot%0Aclassification%20tasks.%20When%20combining%20VeCap%20and%20DFN%2C%20our%20model%20can%20achieve%0Astrong%20performance%20on%20both%20of%20image-text%20retrieval%20and%20zero-shot%20classification%0Atasks%2C%20e.g.%2083.1%25%20accuracy%401%20on%20ImageNet%20zero-shot%20for%20a%20H/14%20model.%20We%20release%0Athe%20pre-trained%20models%20at%20https%3A//github.com/apple/ml-veclip.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07699v2&entry.124074799=Read"},
{"title": "Exploring the Influence of Dimensionality Reduction on Anomaly Detection\n  Performance in Multivariate Time Series", "author": "Mahsun Altin and Altan Cakir", "abstract": "  This paper presents an extensive empirical study on the integration of\ndimensionality reduction techniques with advanced unsupervised time series\nanomaly detection models, focusing on the MUTANT and Anomaly-Transformer\nmodels. The study involves a comprehensive evaluation across three different\ndatasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing\nfor a robust assessment of the models' capabilities in varied contexts. The\ndimensionality reduction techniques examined include PCA, UMAP, Random\nProjection, and t-SNE, each offering distinct advantages in simplifying\nhigh-dimensional data. Our findings reveal that dimensionality reduction not\nonly aids in reducing computational complexity but also significantly enhances\nanomaly detection performance in certain scenarios. Moreover, a remarkable\nreduction in training times was observed, with reductions by approximately\n300\\% and 650\\% when dimensionality was halved and minimized to the lowest\ndimensions, respectively. This efficiency gain underscores the dual benefit of\ndimensionality reduction in both performance enhancement and operational\nefficiency. The MUTANT model exhibits notable adaptability, especially with\nUMAP reduction, while the Anomaly-Transformer demonstrates versatility across\nvarious reduction techniques. These insights provide a deeper understanding of\nthe synergistic effects of dimensionality reduction and anomaly detection,\ncontributing valuable perspectives to the field of time series analysis. The\nstudy underscores the importance of selecting appropriate dimensionality\nreduction strategies based on specific model requirements and dataset\ncharacteristics, paving the way for more efficient, accurate, and scalable\nsolutions in anomaly detection.\n", "link": "http://arxiv.org/abs/2403.04429v1", "date": "2024-03-07", "relevancy": 2.2156, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4505}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4401}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4387}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Influence%20of%20Dimensionality%20Reduction%20on%20Anomaly%20Detection%0A%20%20Performance%20in%20Multivariate%20Time%20Series&body=Title%3A%20Exploring%20the%20Influence%20of%20Dimensionality%20Reduction%20on%20Anomaly%20Detection%0A%20%20Performance%20in%20Multivariate%20Time%20Series%0AAuthor%3A%20Mahsun%20Altin%20and%20Altan%20Cakir%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20extensive%20empirical%20study%20on%20the%20integration%20of%0Adimensionality%20reduction%20techniques%20with%20advanced%20unsupervised%20time%20series%0Aanomaly%20detection%20models%2C%20focusing%20on%20the%20MUTANT%20and%20Anomaly-Transformer%0Amodels.%20The%20study%20involves%20a%20comprehensive%20evaluation%20across%20three%20different%0Adatasets%3A%20MSL%2C%20SMAP%2C%20and%20SWaT.%20Each%20dataset%20poses%20unique%20challenges%2C%20allowing%0Afor%20a%20robust%20assessment%20of%20the%20models%27%20capabilities%20in%20varied%20contexts.%20The%0Adimensionality%20reduction%20techniques%20examined%20include%20PCA%2C%20UMAP%2C%20Random%0AProjection%2C%20and%20t-SNE%2C%20each%20offering%20distinct%20advantages%20in%20simplifying%0Ahigh-dimensional%20data.%20Our%20findings%20reveal%20that%20dimensionality%20reduction%20not%0Aonly%20aids%20in%20reducing%20computational%20complexity%20but%20also%20significantly%20enhances%0Aanomaly%20detection%20performance%20in%20certain%20scenarios.%20Moreover%2C%20a%20remarkable%0Areduction%20in%20training%20times%20was%20observed%2C%20with%20reductions%20by%20approximately%0A300%5C%25%20and%20650%5C%25%20when%20dimensionality%20was%20halved%20and%20minimized%20to%20the%20lowest%0Adimensions%2C%20respectively.%20This%20efficiency%20gain%20underscores%20the%20dual%20benefit%20of%0Adimensionality%20reduction%20in%20both%20performance%20enhancement%20and%20operational%0Aefficiency.%20The%20MUTANT%20model%20exhibits%20notable%20adaptability%2C%20especially%20with%0AUMAP%20reduction%2C%20while%20the%20Anomaly-Transformer%20demonstrates%20versatility%20across%0Avarious%20reduction%20techniques.%20These%20insights%20provide%20a%20deeper%20understanding%20of%0Athe%20synergistic%20effects%20of%20dimensionality%20reduction%20and%20anomaly%20detection%2C%0Acontributing%20valuable%20perspectives%20to%20the%20field%20of%20time%20series%20analysis.%20The%0Astudy%20underscores%20the%20importance%20of%20selecting%20appropriate%20dimensionality%0Areduction%20strategies%20based%20on%20specific%20model%20requirements%20and%20dataset%0Acharacteristics%2C%20paving%20the%20way%20for%20more%20efficient%2C%20accurate%2C%20and%20scalable%0Asolutions%20in%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04429v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Influence%20of%20Dimensionality%20Reduction%20on%20Anomaly%20Detection%0A%20%20Performance%20in%20Multivariate%20Time%20Series&entry.906535625=Mahsun%20Altin%20and%20Altan%20Cakir&entry.1292438233=%20%20This%20paper%20presents%20an%20extensive%20empirical%20study%20on%20the%20integration%20of%0Adimensionality%20reduction%20techniques%20with%20advanced%20unsupervised%20time%20series%0Aanomaly%20detection%20models%2C%20focusing%20on%20the%20MUTANT%20and%20Anomaly-Transformer%0Amodels.%20The%20study%20involves%20a%20comprehensive%20evaluation%20across%20three%20different%0Adatasets%3A%20MSL%2C%20SMAP%2C%20and%20SWaT.%20Each%20dataset%20poses%20unique%20challenges%2C%20allowing%0Afor%20a%20robust%20assessment%20of%20the%20models%27%20capabilities%20in%20varied%20contexts.%20The%0Adimensionality%20reduction%20techniques%20examined%20include%20PCA%2C%20UMAP%2C%20Random%0AProjection%2C%20and%20t-SNE%2C%20each%20offering%20distinct%20advantages%20in%20simplifying%0Ahigh-dimensional%20data.%20Our%20findings%20reveal%20that%20dimensionality%20reduction%20not%0Aonly%20aids%20in%20reducing%20computational%20complexity%20but%20also%20significantly%20enhances%0Aanomaly%20detection%20performance%20in%20certain%20scenarios.%20Moreover%2C%20a%20remarkable%0Areduction%20in%20training%20times%20was%20observed%2C%20with%20reductions%20by%20approximately%0A300%5C%25%20and%20650%5C%25%20when%20dimensionality%20was%20halved%20and%20minimized%20to%20the%20lowest%0Adimensions%2C%20respectively.%20This%20efficiency%20gain%20underscores%20the%20dual%20benefit%20of%0Adimensionality%20reduction%20in%20both%20performance%20enhancement%20and%20operational%0Aefficiency.%20The%20MUTANT%20model%20exhibits%20notable%20adaptability%2C%20especially%20with%0AUMAP%20reduction%2C%20while%20the%20Anomaly-Transformer%20demonstrates%20versatility%20across%0Avarious%20reduction%20techniques.%20These%20insights%20provide%20a%20deeper%20understanding%20of%0Athe%20synergistic%20effects%20of%20dimensionality%20reduction%20and%20anomaly%20detection%2C%0Acontributing%20valuable%20perspectives%20to%20the%20field%20of%20time%20series%20analysis.%20The%0Astudy%20underscores%20the%20importance%20of%20selecting%20appropriate%20dimensionality%0Areduction%20strategies%20based%20on%20specific%20model%20requirements%20and%20dataset%0Acharacteristics%2C%20paving%20the%20way%20for%20more%20efficient%2C%20accurate%2C%20and%20scalable%0Asolutions%20in%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04429v1&entry.124074799=Read"},
{"title": "Signature Isolation Forest", "author": "Guillaume Staerman and Marta Campi and Gareth W. Peters", "abstract": "  Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly\nDetection (AD) algorithm designed for functional data. It relies on a tree\npartition procedure where an abnormality score is computed by projecting each\ncurve observation on a drawn dictionary through a linear inner product. Such\nlinear inner product and the dictionary are a priori choices that highly\ninfluence the algorithm's performances and might lead to unreliable results,\nparticularly with complex datasets. This work addresses these challenges by\nintroducing \\textit{Signature Isolation Forest}, a novel AD algorithm class\nleveraging the rough path theory's signature transform. Our objective is to\nremove the constraints imposed by FIF through the proposition of two algorithms\nwhich specifically target the linearity of the FIF inner product and the choice\nof the dictionary. We provide several numerical experiments, including a\nreal-world applications benchmark showing the relevance of our methods.\n", "link": "http://arxiv.org/abs/2403.04405v1", "date": "2024-03-07", "relevancy": 2.21, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4481}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4439}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.434}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Signature%20Isolation%20Forest&body=Title%3A%20Signature%20Isolation%20Forest%0AAuthor%3A%20Guillaume%20Staerman%20and%20Marta%20Campi%20and%20Gareth%20W.%20Peters%0AAbstract%3A%20%20%20Functional%20Isolation%20Forest%20%28FIF%29%20is%20a%20recent%20state-of-the-art%20Anomaly%0ADetection%20%28AD%29%20algorithm%20designed%20for%20functional%20data.%20It%20relies%20on%20a%20tree%0Apartition%20procedure%20where%20an%20abnormality%20score%20is%20computed%20by%20projecting%20each%0Acurve%20observation%20on%20a%20drawn%20dictionary%20through%20a%20linear%20inner%20product.%20Such%0Alinear%20inner%20product%20and%20the%20dictionary%20are%20a%20priori%20choices%20that%20highly%0Ainfluence%20the%20algorithm%27s%20performances%20and%20might%20lead%20to%20unreliable%20results%2C%0Aparticularly%20with%20complex%20datasets.%20This%20work%20addresses%20these%20challenges%20by%0Aintroducing%20%5Ctextit%7BSignature%20Isolation%20Forest%7D%2C%20a%20novel%20AD%20algorithm%20class%0Aleveraging%20the%20rough%20path%20theory%27s%20signature%20transform.%20Our%20objective%20is%20to%0Aremove%20the%20constraints%20imposed%20by%20FIF%20through%20the%20proposition%20of%20two%20algorithms%0Awhich%20specifically%20target%20the%20linearity%20of%20the%20FIF%20inner%20product%20and%20the%20choice%0Aof%20the%20dictionary.%20We%20provide%20several%20numerical%20experiments%2C%20including%20a%0Areal-world%20applications%20benchmark%20showing%20the%20relevance%20of%20our%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04405v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signature%20Isolation%20Forest&entry.906535625=Guillaume%20Staerman%20and%20Marta%20Campi%20and%20Gareth%20W.%20Peters&entry.1292438233=%20%20Functional%20Isolation%20Forest%20%28FIF%29%20is%20a%20recent%20state-of-the-art%20Anomaly%0ADetection%20%28AD%29%20algorithm%20designed%20for%20functional%20data.%20It%20relies%20on%20a%20tree%0Apartition%20procedure%20where%20an%20abnormality%20score%20is%20computed%20by%20projecting%20each%0Acurve%20observation%20on%20a%20drawn%20dictionary%20through%20a%20linear%20inner%20product.%20Such%0Alinear%20inner%20product%20and%20the%20dictionary%20are%20a%20priori%20choices%20that%20highly%0Ainfluence%20the%20algorithm%27s%20performances%20and%20might%20lead%20to%20unreliable%20results%2C%0Aparticularly%20with%20complex%20datasets.%20This%20work%20addresses%20these%20challenges%20by%0Aintroducing%20%5Ctextit%7BSignature%20Isolation%20Forest%7D%2C%20a%20novel%20AD%20algorithm%20class%0Aleveraging%20the%20rough%20path%20theory%27s%20signature%20transform.%20Our%20objective%20is%20to%0Aremove%20the%20constraints%20imposed%20by%20FIF%20through%20the%20proposition%20of%20two%20algorithms%0Awhich%20specifically%20target%20the%20linearity%20of%20the%20FIF%20inner%20product%20and%20the%20choice%0Aof%20the%20dictionary.%20We%20provide%20several%20numerical%20experiments%2C%20including%20a%0Areal-world%20applications%20benchmark%20showing%20the%20relevance%20of%20our%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04405v1&entry.124074799=Read"},
{"title": "Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", "author": "Tairan He and Zhengyi Luo and Wenli Xiao and Chong Zhang and Kris Kitani and Changliu Liu and Guanya Shi", "abstract": "  We present Human to Humanoid (H2O), a reinforcement learning (RL) based\nframework that enables real-time whole-body teleoperation of a full-sized\nhumanoid robot with only an RGB camera. To create a large-scale retargeted\nmotion dataset of human movements for humanoid robots, we propose a scalable\n\"sim-to-data\" process to filter and pick feasible motions using a privileged\nmotion imitator. Afterwards, we train a robust real-time humanoid motion\nimitator in simulation using these refined motions and transfer it to the real\nhumanoid robot in a zero-shot manner. We successfully achieve teleoperation of\ndynamic whole-body motions in real-world scenarios, including walking, back\njumping, kicking, turning, waving, pushing, boxing, etc. To the best of our\nknowledge, this is the first demonstration to achieve learning-based real-time\nwhole-body humanoid teleoperation.\n", "link": "http://arxiv.org/abs/2403.04436v1", "date": "2024-03-07", "relevancy": 2.2086, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5602}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5349}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Learning%20Human-to-Humanoid%20Real-Time%20Whole-Body%20Teleoperation&body=Title%3A%20Learning%20Human-to-Humanoid%20Real-Time%20Whole-Body%20Teleoperation%0AAuthor%3A%20Tairan%20He%20and%20Zhengyi%20Luo%20and%20Wenli%20Xiao%20and%20Chong%20Zhang%20and%20Kris%20Kitani%20and%20Changliu%20Liu%20and%20Guanya%20Shi%0AAbstract%3A%20%20%20We%20present%20Human%20to%20Humanoid%20%28H2O%29%2C%20a%20reinforcement%20learning%20%28RL%29%20based%0Aframework%20that%20enables%20real-time%20whole-body%20teleoperation%20of%20a%20full-sized%0Ahumanoid%20robot%20with%20only%20an%20RGB%20camera.%20To%20create%20a%20large-scale%20retargeted%0Amotion%20dataset%20of%20human%20movements%20for%20humanoid%20robots%2C%20we%20propose%20a%20scalable%0A%22sim-to-data%22%20process%20to%20filter%20and%20pick%20feasible%20motions%20using%20a%20privileged%0Amotion%20imitator.%20Afterwards%2C%20we%20train%20a%20robust%20real-time%20humanoid%20motion%0Aimitator%20in%20simulation%20using%20these%20refined%20motions%20and%20transfer%20it%20to%20the%20real%0Ahumanoid%20robot%20in%20a%20zero-shot%20manner.%20We%20successfully%20achieve%20teleoperation%20of%0Adynamic%20whole-body%20motions%20in%20real-world%20scenarios%2C%20including%20walking%2C%20back%0Ajumping%2C%20kicking%2C%20turning%2C%20waving%2C%20pushing%2C%20boxing%2C%20etc.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20demonstration%20to%20achieve%20learning-based%20real-time%0Awhole-body%20humanoid%20teleoperation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04436v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Human-to-Humanoid%20Real-Time%20Whole-Body%20Teleoperation&entry.906535625=Tairan%20He%20and%20Zhengyi%20Luo%20and%20Wenli%20Xiao%20and%20Chong%20Zhang%20and%20Kris%20Kitani%20and%20Changliu%20Liu%20and%20Guanya%20Shi&entry.1292438233=%20%20We%20present%20Human%20to%20Humanoid%20%28H2O%29%2C%20a%20reinforcement%20learning%20%28RL%29%20based%0Aframework%20that%20enables%20real-time%20whole-body%20teleoperation%20of%20a%20full-sized%0Ahumanoid%20robot%20with%20only%20an%20RGB%20camera.%20To%20create%20a%20large-scale%20retargeted%0Amotion%20dataset%20of%20human%20movements%20for%20humanoid%20robots%2C%20we%20propose%20a%20scalable%0A%22sim-to-data%22%20process%20to%20filter%20and%20pick%20feasible%20motions%20using%20a%20privileged%0Amotion%20imitator.%20Afterwards%2C%20we%20train%20a%20robust%20real-time%20humanoid%20motion%0Aimitator%20in%20simulation%20using%20these%20refined%20motions%20and%20transfer%20it%20to%20the%20real%0Ahumanoid%20robot%20in%20a%20zero-shot%20manner.%20We%20successfully%20achieve%20teleoperation%20of%0Adynamic%20whole-body%20motions%20in%20real-world%20scenarios%2C%20including%20walking%2C%20back%0Ajumping%2C%20kicking%2C%20turning%2C%20waving%2C%20pushing%2C%20boxing%2C%20etc.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20demonstration%20to%20achieve%20learning-based%20real-time%0Awhole-body%20humanoid%20teleoperation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04436v1&entry.124074799=Read"},
{"title": "StableDrag: Stable Dragging for Point-based Image Editing", "author": "Yutao Cui and Xiaotong Zhao and Guozhen Zhang and Shengming Cao and Kai Ma and Limin Wang", "abstract": "  Point-based image editing has attracted remarkable attention since the\nemergence of DragGAN. Recently, DragDiffusion further pushes forward the\ngenerative quality via adapting this dragging technique to diffusion models.\nDespite these great success, this dragging scheme exhibits two major drawbacks,\nnamely inaccurate point tracking and incomplete motion supervision, which may\nresult in unsatisfactory dragging outcomes. To tackle these issues, we build a\nstable and precise drag-based editing framework, coined as StableDrag, by\ndesigning a discirminative point tracking method and a confidence-based latent\nenhancement strategy for motion supervision. The former allows us to precisely\nlocate the updated handle points, thereby boosting the stability of long-range\nmanipulation, while the latter is responsible for guaranteeing the optimized\nlatent as high-quality as possible across all the manipulation steps. Thanks to\nthese unique designs, we instantiate two types of image editing models\nincluding StableDrag-GAN and StableDrag-Diff, which attains more stable\ndragging performance, through extensive qualitative experiments and\nquantitative assessment on DragBench.\n", "link": "http://arxiv.org/abs/2403.04437v1", "date": "2024-03-07", "relevancy": 2.2031, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5895}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5345}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5186}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20StableDrag%3A%20Stable%20Dragging%20for%20Point-based%20Image%20Editing&body=Title%3A%20StableDrag%3A%20Stable%20Dragging%20for%20Point-based%20Image%20Editing%0AAuthor%3A%20Yutao%20Cui%20and%20Xiaotong%20Zhao%20and%20Guozhen%20Zhang%20and%20Shengming%20Cao%20and%20Kai%20Ma%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Point-based%20image%20editing%20has%20attracted%20remarkable%20attention%20since%20the%0Aemergence%20of%20DragGAN.%20Recently%2C%20DragDiffusion%20further%20pushes%20forward%20the%0Agenerative%20quality%20via%20adapting%20this%20dragging%20technique%20to%20diffusion%20models.%0ADespite%20these%20great%20success%2C%20this%20dragging%20scheme%20exhibits%20two%20major%20drawbacks%2C%0Anamely%20inaccurate%20point%20tracking%20and%20incomplete%20motion%20supervision%2C%20which%20may%0Aresult%20in%20unsatisfactory%20dragging%20outcomes.%20To%20tackle%20these%20issues%2C%20we%20build%20a%0Astable%20and%20precise%20drag-based%20editing%20framework%2C%20coined%20as%20StableDrag%2C%20by%0Adesigning%20a%20discirminative%20point%20tracking%20method%20and%20a%20confidence-based%20latent%0Aenhancement%20strategy%20for%20motion%20supervision.%20The%20former%20allows%20us%20to%20precisely%0Alocate%20the%20updated%20handle%20points%2C%20thereby%20boosting%20the%20stability%20of%20long-range%0Amanipulation%2C%20while%20the%20latter%20is%20responsible%20for%20guaranteeing%20the%20optimized%0Alatent%20as%20high-quality%20as%20possible%20across%20all%20the%20manipulation%20steps.%20Thanks%20to%0Athese%20unique%20designs%2C%20we%20instantiate%20two%20types%20of%20image%20editing%20models%0Aincluding%20StableDrag-GAN%20and%20StableDrag-Diff%2C%20which%20attains%20more%20stable%0Adragging%20performance%2C%20through%20extensive%20qualitative%20experiments%20and%0Aquantitative%20assessment%20on%20DragBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04437v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableDrag%3A%20Stable%20Dragging%20for%20Point-based%20Image%20Editing&entry.906535625=Yutao%20Cui%20and%20Xiaotong%20Zhao%20and%20Guozhen%20Zhang%20and%20Shengming%20Cao%20and%20Kai%20Ma%20and%20Limin%20Wang&entry.1292438233=%20%20Point-based%20image%20editing%20has%20attracted%20remarkable%20attention%20since%20the%0Aemergence%20of%20DragGAN.%20Recently%2C%20DragDiffusion%20further%20pushes%20forward%20the%0Agenerative%20quality%20via%20adapting%20this%20dragging%20technique%20to%20diffusion%20models.%0ADespite%20these%20great%20success%2C%20this%20dragging%20scheme%20exhibits%20two%20major%20drawbacks%2C%0Anamely%20inaccurate%20point%20tracking%20and%20incomplete%20motion%20supervision%2C%20which%20may%0Aresult%20in%20unsatisfactory%20dragging%20outcomes.%20To%20tackle%20these%20issues%2C%20we%20build%20a%0Astable%20and%20precise%20drag-based%20editing%20framework%2C%20coined%20as%20StableDrag%2C%20by%0Adesigning%20a%20discirminative%20point%20tracking%20method%20and%20a%20confidence-based%20latent%0Aenhancement%20strategy%20for%20motion%20supervision.%20The%20former%20allows%20us%20to%20precisely%0Alocate%20the%20updated%20handle%20points%2C%20thereby%20boosting%20the%20stability%20of%20long-range%0Amanipulation%2C%20while%20the%20latter%20is%20responsible%20for%20guaranteeing%20the%20optimized%0Alatent%20as%20high-quality%20as%20possible%20across%20all%20the%20manipulation%20steps.%20Thanks%20to%0Athese%20unique%20designs%2C%20we%20instantiate%20two%20types%20of%20image%20editing%20models%0Aincluding%20StableDrag-GAN%20and%20StableDrag-Diff%2C%20which%20attains%20more%20stable%0Adragging%20performance%2C%20through%20extensive%20qualitative%20experiments%20and%0Aquantitative%20assessment%20on%20DragBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04437v1&entry.124074799=Read"},
{"title": "Dynamic Inertial Poser (DynaIP): Part-Based Motion Dynamics Learning for\n  Enhanced Human Pose Estimation with Sparse Inertial Sensors", "author": "Yu Zhang and Songpengcheng Xia and Lei Chu and Jiarui Yang and Qi Wu and Ling Pei", "abstract": "  This paper introduces a novel human pose estimation approach using sparse\ninertial sensors, addressing the shortcomings of previous methods reliant on\nsynthetic data. It leverages a diverse array of real inertial motion capture\ndata from different skeleton formats to improve motion diversity and model\ngeneralization. This method features two innovative components: a\npseudo-velocity regression model for dynamic motion capture with inertial\nsensors, and a part-based model dividing the body and sensor data into three\nregions, each focusing on their unique characteristics. The approach\ndemonstrates superior performance over state-of-the-art models across five\npublic datasets, notably reducing pose error by 19\\% on the DIP-IMU dataset,\nthus representing a significant improvement in inertial sensor-based human pose\nestimation. Our codes are available at {\\url{https://github.com/dx118/dynaip}}.\n", "link": "http://arxiv.org/abs/2312.02196v2", "date": "2024-03-07", "relevancy": 2.1992, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.58}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.563}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5245}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Inertial%20Poser%20%28DynaIP%29%3A%20Part-Based%20Motion%20Dynamics%20Learning%20for%0A%20%20Enhanced%20Human%20Pose%20Estimation%20with%20Sparse%20Inertial%20Sensors&body=Title%3A%20Dynamic%20Inertial%20Poser%20%28DynaIP%29%3A%20Part-Based%20Motion%20Dynamics%20Learning%20for%0A%20%20Enhanced%20Human%20Pose%20Estimation%20with%20Sparse%20Inertial%20Sensors%0AAuthor%3A%20Yu%20Zhang%20and%20Songpengcheng%20Xia%20and%20Lei%20Chu%20and%20Jiarui%20Yang%20and%20Qi%20Wu%20and%20Ling%20Pei%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20human%20pose%20estimation%20approach%20using%20sparse%0Ainertial%20sensors%2C%20addressing%20the%20shortcomings%20of%20previous%20methods%20reliant%20on%0Asynthetic%20data.%20It%20leverages%20a%20diverse%20array%20of%20real%20inertial%20motion%20capture%0Adata%20from%20different%20skeleton%20formats%20to%20improve%20motion%20diversity%20and%20model%0Ageneralization.%20This%20method%20features%20two%20innovative%20components%3A%20a%0Apseudo-velocity%20regression%20model%20for%20dynamic%20motion%20capture%20with%20inertial%0Asensors%2C%20and%20a%20part-based%20model%20dividing%20the%20body%20and%20sensor%20data%20into%20three%0Aregions%2C%20each%20focusing%20on%20their%20unique%20characteristics.%20The%20approach%0Ademonstrates%20superior%20performance%20over%20state-of-the-art%20models%20across%20five%0Apublic%20datasets%2C%20notably%20reducing%20pose%20error%20by%2019%5C%25%20on%20the%20DIP-IMU%20dataset%2C%0Athus%20representing%20a%20significant%20improvement%20in%20inertial%20sensor-based%20human%20pose%0Aestimation.%20Our%20codes%20are%20available%20at%20%7B%5Curl%7Bhttps%3A//github.com/dx118/dynaip%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02196v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Inertial%20Poser%20%28DynaIP%29%3A%20Part-Based%20Motion%20Dynamics%20Learning%20for%0A%20%20Enhanced%20Human%20Pose%20Estimation%20with%20Sparse%20Inertial%20Sensors&entry.906535625=Yu%20Zhang%20and%20Songpengcheng%20Xia%20and%20Lei%20Chu%20and%20Jiarui%20Yang%20and%20Qi%20Wu%20and%20Ling%20Pei&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20human%20pose%20estimation%20approach%20using%20sparse%0Ainertial%20sensors%2C%20addressing%20the%20shortcomings%20of%20previous%20methods%20reliant%20on%0Asynthetic%20data.%20It%20leverages%20a%20diverse%20array%20of%20real%20inertial%20motion%20capture%0Adata%20from%20different%20skeleton%20formats%20to%20improve%20motion%20diversity%20and%20model%0Ageneralization.%20This%20method%20features%20two%20innovative%20components%3A%20a%0Apseudo-velocity%20regression%20model%20for%20dynamic%20motion%20capture%20with%20inertial%0Asensors%2C%20and%20a%20part-based%20model%20dividing%20the%20body%20and%20sensor%20data%20into%20three%0Aregions%2C%20each%20focusing%20on%20their%20unique%20characteristics.%20The%20approach%0Ademonstrates%20superior%20performance%20over%20state-of-the-art%20models%20across%20five%0Apublic%20datasets%2C%20notably%20reducing%20pose%20error%20by%2019%5C%25%20on%20the%20DIP-IMU%20dataset%2C%0Athus%20representing%20a%20significant%20improvement%20in%20inertial%20sensor-based%20human%20pose%0Aestimation.%20Our%20codes%20are%20available%20at%20%7B%5Curl%7Bhttps%3A//github.com/dx118/dynaip%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02196v2&entry.124074799=Read"},
{"title": "RankED: Addressing Imbalance and Uncertainty in Edge Detection Using\n  Ranking-based Losses", "author": "Bedrettin Cetinkaya and Sinan Kalkan and Emre Akbas", "abstract": "  Detecting edges in images suffers from the problems of (P1) heavy imbalance\nbetween positive and negative classes as well as (P2) label uncertainty owing\nto disagreement between different annotators. Existing solutions address P1\nusing class-balanced cross-entropy loss and dice loss and P2 by only predicting\nedges agreed upon by most annotators. In this paper, we propose RankED, a\nunified ranking-based approach that addresses both the imbalance problem (P1)\nand the uncertainty problem (P2). RankED tackles these two problems with two\ncomponents: One component which ranks positive pixels over negative pixels, and\nthe second which promotes high confidence edge pixels to have more label\ncertainty. We show that RankED outperforms previous studies and sets a new\nstate-of-the-art on NYUD-v2, BSDS500 and Multi-cue datasets. Code is available\nat https://ranked-cvpr24.github.io.\n", "link": "http://arxiv.org/abs/2403.01795v2", "date": "2024-03-07", "relevancy": 2.1952, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5935}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5419}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5378}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20RankED%3A%20Addressing%20Imbalance%20and%20Uncertainty%20in%20Edge%20Detection%20Using%0A%20%20Ranking-based%20Losses&body=Title%3A%20RankED%3A%20Addressing%20Imbalance%20and%20Uncertainty%20in%20Edge%20Detection%20Using%0A%20%20Ranking-based%20Losses%0AAuthor%3A%20Bedrettin%20Cetinkaya%20and%20Sinan%20Kalkan%20and%20Emre%20Akbas%0AAbstract%3A%20%20%20Detecting%20edges%20in%20images%20suffers%20from%20the%20problems%20of%20%28P1%29%20heavy%20imbalance%0Abetween%20positive%20and%20negative%20classes%20as%20well%20as%20%28P2%29%20label%20uncertainty%20owing%0Ato%20disagreement%20between%20different%20annotators.%20Existing%20solutions%20address%20P1%0Ausing%20class-balanced%20cross-entropy%20loss%20and%20dice%20loss%20and%20P2%20by%20only%20predicting%0Aedges%20agreed%20upon%20by%20most%20annotators.%20In%20this%20paper%2C%20we%20propose%20RankED%2C%20a%0Aunified%20ranking-based%20approach%20that%20addresses%20both%20the%20imbalance%20problem%20%28P1%29%0Aand%20the%20uncertainty%20problem%20%28P2%29.%20RankED%20tackles%20these%20two%20problems%20with%20two%0Acomponents%3A%20One%20component%20which%20ranks%20positive%20pixels%20over%20negative%20pixels%2C%20and%0Athe%20second%20which%20promotes%20high%20confidence%20edge%20pixels%20to%20have%20more%20label%0Acertainty.%20We%20show%20that%20RankED%20outperforms%20previous%20studies%20and%20sets%20a%20new%0Astate-of-the-art%20on%20NYUD-v2%2C%20BSDS500%20and%20Multi-cue%20datasets.%20Code%20is%20available%0Aat%20https%3A//ranked-cvpr24.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01795v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RankED%3A%20Addressing%20Imbalance%20and%20Uncertainty%20in%20Edge%20Detection%20Using%0A%20%20Ranking-based%20Losses&entry.906535625=Bedrettin%20Cetinkaya%20and%20Sinan%20Kalkan%20and%20Emre%20Akbas&entry.1292438233=%20%20Detecting%20edges%20in%20images%20suffers%20from%20the%20problems%20of%20%28P1%29%20heavy%20imbalance%0Abetween%20positive%20and%20negative%20classes%20as%20well%20as%20%28P2%29%20label%20uncertainty%20owing%0Ato%20disagreement%20between%20different%20annotators.%20Existing%20solutions%20address%20P1%0Ausing%20class-balanced%20cross-entropy%20loss%20and%20dice%20loss%20and%20P2%20by%20only%20predicting%0Aedges%20agreed%20upon%20by%20most%20annotators.%20In%20this%20paper%2C%20we%20propose%20RankED%2C%20a%0Aunified%20ranking-based%20approach%20that%20addresses%20both%20the%20imbalance%20problem%20%28P1%29%0Aand%20the%20uncertainty%20problem%20%28P2%29.%20RankED%20tackles%20these%20two%20problems%20with%20two%0Acomponents%3A%20One%20component%20which%20ranks%20positive%20pixels%20over%20negative%20pixels%2C%20and%0Athe%20second%20which%20promotes%20high%20confidence%20edge%20pixels%20to%20have%20more%20label%0Acertainty.%20We%20show%20that%20RankED%20outperforms%20previous%20studies%20and%20sets%20a%20new%0Astate-of-the-art%20on%20NYUD-v2%2C%20BSDS500%20and%20Multi-cue%20datasets.%20Code%20is%20available%0Aat%20https%3A//ranked-cvpr24.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01795v2&entry.124074799=Read"},
{"title": "Finding Waldo: Towards Efficient Exploration of NeRF Scene Space", "author": "Evangelos Skartados and Mehmet Kerim Yucel and Bruno Manganelli and Anastasios Drosou and Albert Sa\u00e0-Garriga", "abstract": "  Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D\nreconstruction and novel view synthesis in recent years due to their remarkable\nperformance. Despite the huge interest in NeRF methods, a practical use case of\nNeRFs has largely been ignored; the exploration of the scene space modelled by\na NeRF. In this paper, for the first time in the literature, we propose and\nformally define the scene exploration framework as the efficient discovery of\nNeRF model inputs (i.e. coordinates and viewing angles), using which one can\nrender novel views that adhere to user-selected criteria. To remedy the lack of\napproaches addressing scene exploration, we first propose two baseline methods\ncalled Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).\nWe then cast scene exploration as an optimization problem, and propose the\ncriteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient\nexploration. We test all three approaches with various criteria (e.g. saliency\nmaximization, image quality maximization, photo-composition quality\nimprovement) and show that our EGPS performs more favourably than other\nbaselines. We finally highlight key points and limitations, and outline\ndirections for future research in scene exploration.\n", "link": "http://arxiv.org/abs/2403.04508v1", "date": "2024-03-07", "relevancy": 2.1713, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6324}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5212}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Finding%20Waldo%3A%20Towards%20Efficient%20Exploration%20of%20NeRF%20Scene%20Space&body=Title%3A%20Finding%20Waldo%3A%20Towards%20Efficient%20Exploration%20of%20NeRF%20Scene%20Space%0AAuthor%3A%20Evangelos%20Skartados%20and%20Mehmet%20Kerim%20Yucel%20and%20Bruno%20Manganelli%20and%20Anastasios%20Drosou%20and%20Albert%20Sa%C3%A0-Garriga%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20quickly%20become%20the%20primary%20approach%20for%203D%0Areconstruction%20and%20novel%20view%20synthesis%20in%20recent%20years%20due%20to%20their%20remarkable%0Aperformance.%20Despite%20the%20huge%20interest%20in%20NeRF%20methods%2C%20a%20practical%20use%20case%20of%0ANeRFs%20has%20largely%20been%20ignored%3B%20the%20exploration%20of%20the%20scene%20space%20modelled%20by%0Aa%20NeRF.%20In%20this%20paper%2C%20for%20the%20first%20time%20in%20the%20literature%2C%20we%20propose%20and%0Aformally%20define%20the%20scene%20exploration%20framework%20as%20the%20efficient%20discovery%20of%0ANeRF%20model%20inputs%20%28i.e.%20coordinates%20and%20viewing%20angles%29%2C%20using%20which%20one%20can%0Arender%20novel%20views%20that%20adhere%20to%20user-selected%20criteria.%20To%20remedy%20the%20lack%20of%0Aapproaches%20addressing%20scene%20exploration%2C%20we%20first%20propose%20two%20baseline%20methods%0Acalled%20Guided-Random%20Search%20%28GRS%29%20and%20Pose%20Interpolation-based%20Search%20%28PIBS%29.%0AWe%20then%20cast%20scene%20exploration%20as%20an%20optimization%20problem%2C%20and%20propose%20the%0Acriteria-agnostic%20Evolution-Guided%20Pose%20Search%20%28EGPS%29%20for%20efficient%0Aexploration.%20We%20test%20all%20three%20approaches%20with%20various%20criteria%20%28e.g.%20saliency%0Amaximization%2C%20image%20quality%20maximization%2C%20photo-composition%20quality%0Aimprovement%29%20and%20show%20that%20our%20EGPS%20performs%20more%20favourably%20than%20other%0Abaselines.%20We%20finally%20highlight%20key%20points%20and%20limitations%2C%20and%20outline%0Adirections%20for%20future%20research%20in%20scene%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04508v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20Waldo%3A%20Towards%20Efficient%20Exploration%20of%20NeRF%20Scene%20Space&entry.906535625=Evangelos%20Skartados%20and%20Mehmet%20Kerim%20Yucel%20and%20Bruno%20Manganelli%20and%20Anastasios%20Drosou%20and%20Albert%20Sa%C3%A0-Garriga&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20quickly%20become%20the%20primary%20approach%20for%203D%0Areconstruction%20and%20novel%20view%20synthesis%20in%20recent%20years%20due%20to%20their%20remarkable%0Aperformance.%20Despite%20the%20huge%20interest%20in%20NeRF%20methods%2C%20a%20practical%20use%20case%20of%0ANeRFs%20has%20largely%20been%20ignored%3B%20the%20exploration%20of%20the%20scene%20space%20modelled%20by%0Aa%20NeRF.%20In%20this%20paper%2C%20for%20the%20first%20time%20in%20the%20literature%2C%20we%20propose%20and%0Aformally%20define%20the%20scene%20exploration%20framework%20as%20the%20efficient%20discovery%20of%0ANeRF%20model%20inputs%20%28i.e.%20coordinates%20and%20viewing%20angles%29%2C%20using%20which%20one%20can%0Arender%20novel%20views%20that%20adhere%20to%20user-selected%20criteria.%20To%20remedy%20the%20lack%20of%0Aapproaches%20addressing%20scene%20exploration%2C%20we%20first%20propose%20two%20baseline%20methods%0Acalled%20Guided-Random%20Search%20%28GRS%29%20and%20Pose%20Interpolation-based%20Search%20%28PIBS%29.%0AWe%20then%20cast%20scene%20exploration%20as%20an%20optimization%20problem%2C%20and%20propose%20the%0Acriteria-agnostic%20Evolution-Guided%20Pose%20Search%20%28EGPS%29%20for%20efficient%0Aexploration.%20We%20test%20all%20three%20approaches%20with%20various%20criteria%20%28e.g.%20saliency%0Amaximization%2C%20image%20quality%20maximization%2C%20photo-composition%20quality%0Aimprovement%29%20and%20show%20that%20our%20EGPS%20performs%20more%20favourably%20than%20other%0Abaselines.%20We%20finally%20highlight%20key%20points%20and%20limitations%2C%20and%20outline%0Adirections%20for%20future%20research%20in%20scene%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04508v1&entry.124074799=Read"},
{"title": "CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?", "author": "Ibrahim Alabdulmohsin and Xiao Wang and Andreas Steiner and Priya Goyal and Alexander D'Amour and Xiaohua Zhai", "abstract": "  We study the effectiveness of data-balancing for mitigating biases in\ncontrastive language-image pretraining (CLIP), identifying areas of strength\nand limitation. First, we reaffirm prior conclusions that CLIP models can\ninadvertently absorb societal stereotypes. To counter this, we present a novel\nalgorithm, called Multi-Modal Moment Matching (M4), designed to reduce both\nrepresentation and association biases (i.e. in first- and second-order\nstatistics) in multimodal data. We use M4 to conduct an in-depth analysis\ntaking into account various factors, such as the model, representation, and\ndata size. Our study also explores the dynamic nature of how CLIP learns and\nunlearns biases. In particular, we find that fine-tuning is effective in\ncountering representation biases, though its impact diminishes for association\nbiases. Also, data balancing has a mixed impact on quality: it tends to improve\nclassification but can hurt retrieval. Interestingly, data and architectural\nimprovements seem to mitigate the negative impact of data balancing on\nperformance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves\nCOCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and\nImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with\nrecommendations for improving the efficacy of data balancing in multimodal\nsystems.\n", "link": "http://arxiv.org/abs/2403.04547v1", "date": "2024-03-07", "relevancy": 2.159, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5206}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4878}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20CLIP%20the%20Bias%3A%20How%20Useful%20is%20Balancing%20Data%20in%20Multimodal%20Learning%3F&body=Title%3A%20CLIP%20the%20Bias%3A%20How%20Useful%20is%20Balancing%20Data%20in%20Multimodal%20Learning%3F%0AAuthor%3A%20Ibrahim%20Alabdulmohsin%20and%20Xiao%20Wang%20and%20Andreas%20Steiner%20and%20Priya%20Goyal%20and%20Alexander%20D%27Amour%20and%20Xiaohua%20Zhai%0AAbstract%3A%20%20%20We%20study%20the%20effectiveness%20of%20data-balancing%20for%20mitigating%20biases%20in%0Acontrastive%20language-image%20pretraining%20%28CLIP%29%2C%20identifying%20areas%20of%20strength%0Aand%20limitation.%20First%2C%20we%20reaffirm%20prior%20conclusions%20that%20CLIP%20models%20can%0Ainadvertently%20absorb%20societal%20stereotypes.%20To%20counter%20this%2C%20we%20present%20a%20novel%0Aalgorithm%2C%20called%20Multi-Modal%20Moment%20Matching%20%28M4%29%2C%20designed%20to%20reduce%20both%0Arepresentation%20and%20association%20biases%20%28i.e.%20in%20first-%20and%20second-order%0Astatistics%29%20in%20multimodal%20data.%20We%20use%20M4%20to%20conduct%20an%20in-depth%20analysis%0Ataking%20into%20account%20various%20factors%2C%20such%20as%20the%20model%2C%20representation%2C%20and%0Adata%20size.%20Our%20study%20also%20explores%20the%20dynamic%20nature%20of%20how%20CLIP%20learns%20and%0Aunlearns%20biases.%20In%20particular%2C%20we%20find%20that%20fine-tuning%20is%20effective%20in%0Acountering%20representation%20biases%2C%20though%20its%20impact%20diminishes%20for%20association%0Abiases.%20Also%2C%20data%20balancing%20has%20a%20mixed%20impact%20on%20quality%3A%20it%20tends%20to%20improve%0Aclassification%20but%20can%20hurt%20retrieval.%20Interestingly%2C%20data%20and%20architectural%0Aimprovements%20seem%20to%20mitigate%20the%20negative%20impact%20of%20data%20balancing%20on%0Aperformance%3B%20e.g.%20applying%20M4%20to%20SigLIP-B/16%20with%20data%20quality%20filters%20improves%0ACOCO%20image-to-text%20retrieval%20%405%20from%2086%25%20%28without%20data%20balancing%29%20to%2087%25%20and%0AImageNet%200-shot%20classification%20from%2077%25%20to%2077.5%25%21%20Finally%2C%20we%20conclude%20with%0Arecommendations%20for%20improving%20the%20efficacy%20of%20data%20balancing%20in%20multimodal%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04547v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%20the%20Bias%3A%20How%20Useful%20is%20Balancing%20Data%20in%20Multimodal%20Learning%3F&entry.906535625=Ibrahim%20Alabdulmohsin%20and%20Xiao%20Wang%20and%20Andreas%20Steiner%20and%20Priya%20Goyal%20and%20Alexander%20D%27Amour%20and%20Xiaohua%20Zhai&entry.1292438233=%20%20We%20study%20the%20effectiveness%20of%20data-balancing%20for%20mitigating%20biases%20in%0Acontrastive%20language-image%20pretraining%20%28CLIP%29%2C%20identifying%20areas%20of%20strength%0Aand%20limitation.%20First%2C%20we%20reaffirm%20prior%20conclusions%20that%20CLIP%20models%20can%0Ainadvertently%20absorb%20societal%20stereotypes.%20To%20counter%20this%2C%20we%20present%20a%20novel%0Aalgorithm%2C%20called%20Multi-Modal%20Moment%20Matching%20%28M4%29%2C%20designed%20to%20reduce%20both%0Arepresentation%20and%20association%20biases%20%28i.e.%20in%20first-%20and%20second-order%0Astatistics%29%20in%20multimodal%20data.%20We%20use%20M4%20to%20conduct%20an%20in-depth%20analysis%0Ataking%20into%20account%20various%20factors%2C%20such%20as%20the%20model%2C%20representation%2C%20and%0Adata%20size.%20Our%20study%20also%20explores%20the%20dynamic%20nature%20of%20how%20CLIP%20learns%20and%0Aunlearns%20biases.%20In%20particular%2C%20we%20find%20that%20fine-tuning%20is%20effective%20in%0Acountering%20representation%20biases%2C%20though%20its%20impact%20diminishes%20for%20association%0Abiases.%20Also%2C%20data%20balancing%20has%20a%20mixed%20impact%20on%20quality%3A%20it%20tends%20to%20improve%0Aclassification%20but%20can%20hurt%20retrieval.%20Interestingly%2C%20data%20and%20architectural%0Aimprovements%20seem%20to%20mitigate%20the%20negative%20impact%20of%20data%20balancing%20on%0Aperformance%3B%20e.g.%20applying%20M4%20to%20SigLIP-B/16%20with%20data%20quality%20filters%20improves%0ACOCO%20image-to-text%20retrieval%20%405%20from%2086%25%20%28without%20data%20balancing%29%20to%2087%25%20and%0AImageNet%200-shot%20classification%20from%2077%25%20to%2077.5%25%21%20Finally%2C%20we%20conclude%20with%0Arecommendations%20for%20improving%20the%20efficacy%20of%20data%20balancing%20in%20multimodal%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04547v1&entry.124074799=Read"},
{"title": "Multi-step Temporal Modeling for UAV Tracking", "author": "Xiaoying Yuan and Tingfa Xu and Xincong Liu and Ying Wang and Haolin Qin and Yuqiang Fang and Jianan Li", "abstract": "  In the realm of unmanned aerial vehicle (UAV) tracking, Siamese-based\napproaches have gained traction due to their optimal balance between efficiency\nand precision. However, UAV scenarios often present challenges such as\ninsufficient sampling resolution, fast motion and small objects with limited\nfeature information. As a result, temporal context in UAV tracking tasks plays\na pivotal role in target location, overshadowing the target's precise features.\nIn this paper, we introduce MT-Track, a streamlined and efficient multi-step\ntemporal modeling framework designed to harness the temporal context from\nhistorical frames for enhanced UAV tracking. This temporal integration occurs\nin two steps: correlation map generation and correlation map refinement.\nSpecifically, we unveil a unique temporal correlation module that dynamically\nassesses the interplay between the template and search region features. This\nmodule leverages temporal information to refresh the template feature, yielding\na more precise correlation map. Subsequently, we propose a mutual transformer\nmodule to refine the correlation maps of historical and current frames by\nmodeling the temporal knowledge in the tracking sequence. This method\nsignificantly trims computational demands compared to the raw transformer. The\ncompact yet potent nature of our tracking framework ensures commendable\ntracking outcomes, particularly in extended tracking scenarios.\n", "link": "http://arxiv.org/abs/2403.04363v1", "date": "2024-03-07", "relevancy": 2.1582, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.517}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Multi-step%20Temporal%20Modeling%20for%20UAV%20Tracking&body=Title%3A%20Multi-step%20Temporal%20Modeling%20for%20UAV%20Tracking%0AAuthor%3A%20Xiaoying%20Yuan%20and%20Tingfa%20Xu%20and%20Xincong%20Liu%20and%20Ying%20Wang%20and%20Haolin%20Qin%20and%20Yuqiang%20Fang%20and%20Jianan%20Li%0AAbstract%3A%20%20%20In%20the%20realm%20of%20unmanned%20aerial%20vehicle%20%28UAV%29%20tracking%2C%20Siamese-based%0Aapproaches%20have%20gained%20traction%20due%20to%20their%20optimal%20balance%20between%20efficiency%0Aand%20precision.%20However%2C%20UAV%20scenarios%20often%20present%20challenges%20such%20as%0Ainsufficient%20sampling%20resolution%2C%20fast%20motion%20and%20small%20objects%20with%20limited%0Afeature%20information.%20As%20a%20result%2C%20temporal%20context%20in%20UAV%20tracking%20tasks%20plays%0Aa%20pivotal%20role%20in%20target%20location%2C%20overshadowing%20the%20target%27s%20precise%20features.%0AIn%20this%20paper%2C%20we%20introduce%20MT-Track%2C%20a%20streamlined%20and%20efficient%20multi-step%0Atemporal%20modeling%20framework%20designed%20to%20harness%20the%20temporal%20context%20from%0Ahistorical%20frames%20for%20enhanced%20UAV%20tracking.%20This%20temporal%20integration%20occurs%0Ain%20two%20steps%3A%20correlation%20map%20generation%20and%20correlation%20map%20refinement.%0ASpecifically%2C%20we%20unveil%20a%20unique%20temporal%20correlation%20module%20that%20dynamically%0Aassesses%20the%20interplay%20between%20the%20template%20and%20search%20region%20features.%20This%0Amodule%20leverages%20temporal%20information%20to%20refresh%20the%20template%20feature%2C%20yielding%0Aa%20more%20precise%20correlation%20map.%20Subsequently%2C%20we%20propose%20a%20mutual%20transformer%0Amodule%20to%20refine%20the%20correlation%20maps%20of%20historical%20and%20current%20frames%20by%0Amodeling%20the%20temporal%20knowledge%20in%20the%20tracking%20sequence.%20This%20method%0Asignificantly%20trims%20computational%20demands%20compared%20to%20the%20raw%20transformer.%20The%0Acompact%20yet%20potent%20nature%20of%20our%20tracking%20framework%20ensures%20commendable%0Atracking%20outcomes%2C%20particularly%20in%20extended%20tracking%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04363v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-step%20Temporal%20Modeling%20for%20UAV%20Tracking&entry.906535625=Xiaoying%20Yuan%20and%20Tingfa%20Xu%20and%20Xincong%20Liu%20and%20Ying%20Wang%20and%20Haolin%20Qin%20and%20Yuqiang%20Fang%20and%20Jianan%20Li&entry.1292438233=%20%20In%20the%20realm%20of%20unmanned%20aerial%20vehicle%20%28UAV%29%20tracking%2C%20Siamese-based%0Aapproaches%20have%20gained%20traction%20due%20to%20their%20optimal%20balance%20between%20efficiency%0Aand%20precision.%20However%2C%20UAV%20scenarios%20often%20present%20challenges%20such%20as%0Ainsufficient%20sampling%20resolution%2C%20fast%20motion%20and%20small%20objects%20with%20limited%0Afeature%20information.%20As%20a%20result%2C%20temporal%20context%20in%20UAV%20tracking%20tasks%20plays%0Aa%20pivotal%20role%20in%20target%20location%2C%20overshadowing%20the%20target%27s%20precise%20features.%0AIn%20this%20paper%2C%20we%20introduce%20MT-Track%2C%20a%20streamlined%20and%20efficient%20multi-step%0Atemporal%20modeling%20framework%20designed%20to%20harness%20the%20temporal%20context%20from%0Ahistorical%20frames%20for%20enhanced%20UAV%20tracking.%20This%20temporal%20integration%20occurs%0Ain%20two%20steps%3A%20correlation%20map%20generation%20and%20correlation%20map%20refinement.%0ASpecifically%2C%20we%20unveil%20a%20unique%20temporal%20correlation%20module%20that%20dynamically%0Aassesses%20the%20interplay%20between%20the%20template%20and%20search%20region%20features.%20This%0Amodule%20leverages%20temporal%20information%20to%20refresh%20the%20template%20feature%2C%20yielding%0Aa%20more%20precise%20correlation%20map.%20Subsequently%2C%20we%20propose%20a%20mutual%20transformer%0Amodule%20to%20refine%20the%20correlation%20maps%20of%20historical%20and%20current%20frames%20by%0Amodeling%20the%20temporal%20knowledge%20in%20the%20tracking%20sequence.%20This%20method%0Asignificantly%20trims%20computational%20demands%20compared%20to%20the%20raw%20transformer.%20The%0Acompact%20yet%20potent%20nature%20of%20our%20tracking%20framework%20ensures%20commendable%0Atracking%20outcomes%2C%20particularly%20in%20extended%20tracking%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04363v1&entry.124074799=Read"},
{"title": "On the Out-of-Distribution Coverage of Combining Split Conformal\n  Prediction and Bayesian Deep Learning", "author": "Paul Scemama and Ariel Kapusta", "abstract": "  Bayesian deep learning and conformal prediction are two methods that have\nbeen used to convey uncertainty and increase safety in machine learning\nsystems. We focus on combining Bayesian deep learning with split conformal\nprediction and how this combination effects out-of-distribution coverage;\nparticularly in the case of multiclass image classification. We suggest that if\nthe model is generally underconfident on the calibration set, then the\nresultant conformal sets may exhibit worse out-of-distribution coverage\ncompared to simple predictive credible sets. Conversely, if the model is\noverconfident on the calibration set, the use of conformal prediction may\nimprove out-of-distribution coverage. We evaluate prediction sets as a result\nof combining split conformal methods and neural networks trained with (i)\nstochastic gradient descent, (ii) deep ensembles, and (iii) mean-field\nvariational inference. Our results suggest that combining Bayesian deep\nlearning models with split conformal prediction can, in some cases, cause\nunintended consequences such as reducing out-of-distribution coverage.\n", "link": "http://arxiv.org/abs/2311.12688v2", "date": "2024-03-07", "relevancy": 2.1533, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5446}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5206}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20On%20the%20Out-of-Distribution%20Coverage%20of%20Combining%20Split%20Conformal%0A%20%20Prediction%20and%20Bayesian%20Deep%20Learning&body=Title%3A%20On%20the%20Out-of-Distribution%20Coverage%20of%20Combining%20Split%20Conformal%0A%20%20Prediction%20and%20Bayesian%20Deep%20Learning%0AAuthor%3A%20Paul%20Scemama%20and%20Ariel%20Kapusta%0AAbstract%3A%20%20%20Bayesian%20deep%20learning%20and%20conformal%20prediction%20are%20two%20methods%20that%20have%0Abeen%20used%20to%20convey%20uncertainty%20and%20increase%20safety%20in%20machine%20learning%0Asystems.%20We%20focus%20on%20combining%20Bayesian%20deep%20learning%20with%20split%20conformal%0Aprediction%20and%20how%20this%20combination%20effects%20out-of-distribution%20coverage%3B%0Aparticularly%20in%20the%20case%20of%20multiclass%20image%20classification.%20We%20suggest%20that%20if%0Athe%20model%20is%20generally%20underconfident%20on%20the%20calibration%20set%2C%20then%20the%0Aresultant%20conformal%20sets%20may%20exhibit%20worse%20out-of-distribution%20coverage%0Acompared%20to%20simple%20predictive%20credible%20sets.%20Conversely%2C%20if%20the%20model%20is%0Aoverconfident%20on%20the%20calibration%20set%2C%20the%20use%20of%20conformal%20prediction%20may%0Aimprove%20out-of-distribution%20coverage.%20We%20evaluate%20prediction%20sets%20as%20a%20result%0Aof%20combining%20split%20conformal%20methods%20and%20neural%20networks%20trained%20with%20%28i%29%0Astochastic%20gradient%20descent%2C%20%28ii%29%20deep%20ensembles%2C%20and%20%28iii%29%20mean-field%0Avariational%20inference.%20Our%20results%20suggest%20that%20combining%20Bayesian%20deep%0Alearning%20models%20with%20split%20conformal%20prediction%20can%2C%20in%20some%20cases%2C%20cause%0Aunintended%20consequences%20such%20as%20reducing%20out-of-distribution%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12688v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Out-of-Distribution%20Coverage%20of%20Combining%20Split%20Conformal%0A%20%20Prediction%20and%20Bayesian%20Deep%20Learning&entry.906535625=Paul%20Scemama%20and%20Ariel%20Kapusta&entry.1292438233=%20%20Bayesian%20deep%20learning%20and%20conformal%20prediction%20are%20two%20methods%20that%20have%0Abeen%20used%20to%20convey%20uncertainty%20and%20increase%20safety%20in%20machine%20learning%0Asystems.%20We%20focus%20on%20combining%20Bayesian%20deep%20learning%20with%20split%20conformal%0Aprediction%20and%20how%20this%20combination%20effects%20out-of-distribution%20coverage%3B%0Aparticularly%20in%20the%20case%20of%20multiclass%20image%20classification.%20We%20suggest%20that%20if%0Athe%20model%20is%20generally%20underconfident%20on%20the%20calibration%20set%2C%20then%20the%0Aresultant%20conformal%20sets%20may%20exhibit%20worse%20out-of-distribution%20coverage%0Acompared%20to%20simple%20predictive%20credible%20sets.%20Conversely%2C%20if%20the%20model%20is%0Aoverconfident%20on%20the%20calibration%20set%2C%20the%20use%20of%20conformal%20prediction%20may%0Aimprove%20out-of-distribution%20coverage.%20We%20evaluate%20prediction%20sets%20as%20a%20result%0Aof%20combining%20split%20conformal%20methods%20and%20neural%20networks%20trained%20with%20%28i%29%0Astochastic%20gradient%20descent%2C%20%28ii%29%20deep%20ensembles%2C%20and%20%28iii%29%20mean-field%0Avariational%20inference.%20Our%20results%20suggest%20that%20combining%20Bayesian%20deep%0Alearning%20models%20with%20split%20conformal%20prediction%20can%2C%20in%20some%20cases%2C%20cause%0Aunintended%20consequences%20such%20as%20reducing%20out-of-distribution%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12688v2&entry.124074799=Read"},
{"title": "mmPlace: Robust Place Recognition with Intermediate Frequency Signal of\n  Low-cost Single-chip Millimeter Wave Radar", "author": "Chengzhen Meng and Yifan Duan and Chenming He and Dequan Wang and Xiaoran Fan and Yanyong Zhang", "abstract": "  Place recognition is crucial for tasks like loop-closure detection and\nre-localization. Single-chip millimeter wave radar (single-chip radar in short)\nemerges as a low-cost sensor option for place recognition, with the advantage\nof insensitivity to degraded visual environments. However, it encounters two\nchallenges. Firstly, sparse point cloud from single-chip radar leads to poor\nperformance when using current place recognition methods, which assume much\ndenser data. Secondly, its performance significantly declines in scenarios\ninvolving rotational and lateral variations, due to limited overlap in its\nfield of view (FOV). We propose mmPlace, a robust place recognition system to\naddress these challenges. Specifically, mmPlace transforms intermediate\nfrequency (IF) signal into range azimuth heatmap and employs a spatial encoder\nto extract features. Additionally, to improve the performance in scenarios\ninvolving rotational and lateral variations, mmPlace employs a rotating\nplatform and concatenates heatmaps in a rotation cycle, effectively expanding\nthe system's FOV. We evaluate mmPlace's performance on the milliSonic dataset,\nwhich is collected on the University of Science and Technology of China (USTC)\ncampus, the city roads surrounding the campus, and an underground parking\ngarage. The results demonstrate that mmPlace outperforms point cloud-based\nmethods and achieves 87.37% recall@1 in scenarios involving rotational and\nlateral variations.\n", "link": "http://arxiv.org/abs/2403.04703v1", "date": "2024-03-07", "relevancy": 2.1512, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5637}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5247}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5057}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20mmPlace%3A%20Robust%20Place%20Recognition%20with%20Intermediate%20Frequency%20Signal%20of%0A%20%20Low-cost%20Single-chip%20Millimeter%20Wave%20Radar&body=Title%3A%20mmPlace%3A%20Robust%20Place%20Recognition%20with%20Intermediate%20Frequency%20Signal%20of%0A%20%20Low-cost%20Single-chip%20Millimeter%20Wave%20Radar%0AAuthor%3A%20Chengzhen%20Meng%20and%20Yifan%20Duan%20and%20Chenming%20He%20and%20Dequan%20Wang%20and%20Xiaoran%20Fan%20and%20Yanyong%20Zhang%0AAbstract%3A%20%20%20Place%20recognition%20is%20crucial%20for%20tasks%20like%20loop-closure%20detection%20and%0Are-localization.%20Single-chip%20millimeter%20wave%20radar%20%28single-chip%20radar%20in%20short%29%0Aemerges%20as%20a%20low-cost%20sensor%20option%20for%20place%20recognition%2C%20with%20the%20advantage%0Aof%20insensitivity%20to%20degraded%20visual%20environments.%20However%2C%20it%20encounters%20two%0Achallenges.%20Firstly%2C%20sparse%20point%20cloud%20from%20single-chip%20radar%20leads%20to%20poor%0Aperformance%20when%20using%20current%20place%20recognition%20methods%2C%20which%20assume%20much%0Adenser%20data.%20Secondly%2C%20its%20performance%20significantly%20declines%20in%20scenarios%0Ainvolving%20rotational%20and%20lateral%20variations%2C%20due%20to%20limited%20overlap%20in%20its%0Afield%20of%20view%20%28FOV%29.%20We%20propose%20mmPlace%2C%20a%20robust%20place%20recognition%20system%20to%0Aaddress%20these%20challenges.%20Specifically%2C%20mmPlace%20transforms%20intermediate%0Afrequency%20%28IF%29%20signal%20into%20range%20azimuth%20heatmap%20and%20employs%20a%20spatial%20encoder%0Ato%20extract%20features.%20Additionally%2C%20to%20improve%20the%20performance%20in%20scenarios%0Ainvolving%20rotational%20and%20lateral%20variations%2C%20mmPlace%20employs%20a%20rotating%0Aplatform%20and%20concatenates%20heatmaps%20in%20a%20rotation%20cycle%2C%20effectively%20expanding%0Athe%20system%27s%20FOV.%20We%20evaluate%20mmPlace%27s%20performance%20on%20the%20milliSonic%20dataset%2C%0Awhich%20is%20collected%20on%20the%20University%20of%20Science%20and%20Technology%20of%20China%20%28USTC%29%0Acampus%2C%20the%20city%20roads%20surrounding%20the%20campus%2C%20and%20an%20underground%20parking%0Agarage.%20The%20results%20demonstrate%20that%20mmPlace%20outperforms%20point%20cloud-based%0Amethods%20and%20achieves%2087.37%25%20recall%401%20in%20scenarios%20involving%20rotational%20and%0Alateral%20variations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04703v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mmPlace%3A%20Robust%20Place%20Recognition%20with%20Intermediate%20Frequency%20Signal%20of%0A%20%20Low-cost%20Single-chip%20Millimeter%20Wave%20Radar&entry.906535625=Chengzhen%20Meng%20and%20Yifan%20Duan%20and%20Chenming%20He%20and%20Dequan%20Wang%20and%20Xiaoran%20Fan%20and%20Yanyong%20Zhang&entry.1292438233=%20%20Place%20recognition%20is%20crucial%20for%20tasks%20like%20loop-closure%20detection%20and%0Are-localization.%20Single-chip%20millimeter%20wave%20radar%20%28single-chip%20radar%20in%20short%29%0Aemerges%20as%20a%20low-cost%20sensor%20option%20for%20place%20recognition%2C%20with%20the%20advantage%0Aof%20insensitivity%20to%20degraded%20visual%20environments.%20However%2C%20it%20encounters%20two%0Achallenges.%20Firstly%2C%20sparse%20point%20cloud%20from%20single-chip%20radar%20leads%20to%20poor%0Aperformance%20when%20using%20current%20place%20recognition%20methods%2C%20which%20assume%20much%0Adenser%20data.%20Secondly%2C%20its%20performance%20significantly%20declines%20in%20scenarios%0Ainvolving%20rotational%20and%20lateral%20variations%2C%20due%20to%20limited%20overlap%20in%20its%0Afield%20of%20view%20%28FOV%29.%20We%20propose%20mmPlace%2C%20a%20robust%20place%20recognition%20system%20to%0Aaddress%20these%20challenges.%20Specifically%2C%20mmPlace%20transforms%20intermediate%0Afrequency%20%28IF%29%20signal%20into%20range%20azimuth%20heatmap%20and%20employs%20a%20spatial%20encoder%0Ato%20extract%20features.%20Additionally%2C%20to%20improve%20the%20performance%20in%20scenarios%0Ainvolving%20rotational%20and%20lateral%20variations%2C%20mmPlace%20employs%20a%20rotating%0Aplatform%20and%20concatenates%20heatmaps%20in%20a%20rotation%20cycle%2C%20effectively%20expanding%0Athe%20system%27s%20FOV.%20We%20evaluate%20mmPlace%27s%20performance%20on%20the%20milliSonic%20dataset%2C%0Awhich%20is%20collected%20on%20the%20University%20of%20Science%20and%20Technology%20of%20China%20%28USTC%29%0Acampus%2C%20the%20city%20roads%20surrounding%20the%20campus%2C%20and%20an%20underground%20parking%0Agarage.%20The%20results%20demonstrate%20that%20mmPlace%20outperforms%20point%20cloud-based%0Amethods%20and%20achieves%2087.37%25%20recall%401%20in%20scenarios%20involving%20rotational%20and%0Alateral%20variations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04703v1&entry.124074799=Read"},
{"title": "TextMonkey: An OCR-Free Large Multimodal Model for Understanding\n  Document", "author": "Yuliang Liu and Biao Yang and Qiang Liu and Zhang Li and Zhiyin Ma and Shuo Zhang and Xiang Bai", "abstract": "  We present TextMonkey, a large multimodal model (LMM) tailored for\ntext-centric tasks, including document question answering (DocVQA) and scene\ntext analysis. Our approach introduces enhancement across several dimensions:\nby adopting Shifted Window Attention with zero-initialization, we achieve\ncross-window connectivity at higher input resolutions and stabilize early\ntraining; We hypothesize that images may contain redundant tokens, and by using\nsimilarity to filter out significant tokens, we can not only streamline the\ntoken length but also enhance the model's performance. Moreover, by expanding\nour model's capabilities to encompass text spotting and grounding, and\nincorporating positional information into responses, we enhance\ninterpretability and minimize hallucinations. Additionally, TextMonkey can be\nfinetuned to gain the ability to comprehend commands for clicking screenshots.\nOverall, our method notably boosts performance across various benchmark\ndatasets, achieving increases of 5.2%, 6.9%, and 2.8% in Scene Text-Centric\nVQA, Document Oriented VQA, and KIE, respectively, especially with a score of\n561 on OCRBench, surpassing prior open-sourced large multimodal models for\ndocument understanding. Code will be released at\nhttps://github.com/Yuliang-Liu/Monkey.\n", "link": "http://arxiv.org/abs/2403.04473v1", "date": "2024-03-07", "relevancy": 2.1446, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5158}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5137}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20TextMonkey%3A%20An%20OCR-Free%20Large%20Multimodal%20Model%20for%20Understanding%0A%20%20Document&body=Title%3A%20TextMonkey%3A%20An%20OCR-Free%20Large%20Multimodal%20Model%20for%20Understanding%0A%20%20Document%0AAuthor%3A%20Yuliang%20Liu%20and%20Biao%20Yang%20and%20Qiang%20Liu%20and%20Zhang%20Li%20and%20Zhiyin%20Ma%20and%20Shuo%20Zhang%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20We%20present%20TextMonkey%2C%20a%20large%20multimodal%20model%20%28LMM%29%20tailored%20for%0Atext-centric%20tasks%2C%20including%20document%20question%20answering%20%28DocVQA%29%20and%20scene%0Atext%20analysis.%20Our%20approach%20introduces%20enhancement%20across%20several%20dimensions%3A%0Aby%20adopting%20Shifted%20Window%20Attention%20with%20zero-initialization%2C%20we%20achieve%0Across-window%20connectivity%20at%20higher%20input%20resolutions%20and%20stabilize%20early%0Atraining%3B%20We%20hypothesize%20that%20images%20may%20contain%20redundant%20tokens%2C%20and%20by%20using%0Asimilarity%20to%20filter%20out%20significant%20tokens%2C%20we%20can%20not%20only%20streamline%20the%0Atoken%20length%20but%20also%20enhance%20the%20model%27s%20performance.%20Moreover%2C%20by%20expanding%0Aour%20model%27s%20capabilities%20to%20encompass%20text%20spotting%20and%20grounding%2C%20and%0Aincorporating%20positional%20information%20into%20responses%2C%20we%20enhance%0Ainterpretability%20and%20minimize%20hallucinations.%20Additionally%2C%20TextMonkey%20can%20be%0Afinetuned%20to%20gain%20the%20ability%20to%20comprehend%20commands%20for%20clicking%20screenshots.%0AOverall%2C%20our%20method%20notably%20boosts%20performance%20across%20various%20benchmark%0Adatasets%2C%20achieving%20increases%20of%205.2%25%2C%206.9%25%2C%20and%202.8%25%20in%20Scene%20Text-Centric%0AVQA%2C%20Document%20Oriented%20VQA%2C%20and%20KIE%2C%20respectively%2C%20especially%20with%20a%20score%20of%0A561%20on%20OCRBench%2C%20surpassing%20prior%20open-sourced%20large%20multimodal%20models%20for%0Adocument%20understanding.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/Yuliang-Liu/Monkey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04473v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextMonkey%3A%20An%20OCR-Free%20Large%20Multimodal%20Model%20for%20Understanding%0A%20%20Document&entry.906535625=Yuliang%20Liu%20and%20Biao%20Yang%20and%20Qiang%20Liu%20and%20Zhang%20Li%20and%20Zhiyin%20Ma%20and%20Shuo%20Zhang%20and%20Xiang%20Bai&entry.1292438233=%20%20We%20present%20TextMonkey%2C%20a%20large%20multimodal%20model%20%28LMM%29%20tailored%20for%0Atext-centric%20tasks%2C%20including%20document%20question%20answering%20%28DocVQA%29%20and%20scene%0Atext%20analysis.%20Our%20approach%20introduces%20enhancement%20across%20several%20dimensions%3A%0Aby%20adopting%20Shifted%20Window%20Attention%20with%20zero-initialization%2C%20we%20achieve%0Across-window%20connectivity%20at%20higher%20input%20resolutions%20and%20stabilize%20early%0Atraining%3B%20We%20hypothesize%20that%20images%20may%20contain%20redundant%20tokens%2C%20and%20by%20using%0Asimilarity%20to%20filter%20out%20significant%20tokens%2C%20we%20can%20not%20only%20streamline%20the%0Atoken%20length%20but%20also%20enhance%20the%20model%27s%20performance.%20Moreover%2C%20by%20expanding%0Aour%20model%27s%20capabilities%20to%20encompass%20text%20spotting%20and%20grounding%2C%20and%0Aincorporating%20positional%20information%20into%20responses%2C%20we%20enhance%0Ainterpretability%20and%20minimize%20hallucinations.%20Additionally%2C%20TextMonkey%20can%20be%0Afinetuned%20to%20gain%20the%20ability%20to%20comprehend%20commands%20for%20clicking%20screenshots.%0AOverall%2C%20our%20method%20notably%20boosts%20performance%20across%20various%20benchmark%0Adatasets%2C%20achieving%20increases%20of%205.2%25%2C%206.9%25%2C%20and%202.8%25%20in%20Scene%20Text-Centric%0AVQA%2C%20Document%20Oriented%20VQA%2C%20and%20KIE%2C%20respectively%2C%20especially%20with%20a%20score%20of%0A561%20on%20OCRBench%2C%20surpassing%20prior%20open-sourced%20large%20multimodal%20models%20for%0Adocument%20understanding.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/Yuliang-Liu/Monkey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04473v1&entry.124074799=Read"},
{"title": "Articulated Object Manipulation with Coarse-to-fine Affordance for\n  Mitigating the Effect of Point Cloud Noise", "author": "Suhan Ling and Yian Wang and Shiguang Wu and Yuzheng Zhuang and Tianyi Xu and Yu Li and Chang Liu and Hao Dong", "abstract": "  3D articulated objects are inherently challenging for manipulation due to the\nvaried geometries and intricate functionalities associated with articulated\nobjects.Point-level affordance, which predicts the per-point actionable score\nand thus proposes the best point to interact with, has demonstrated excellent\nperformance and generalization capabilities in articulated object manipulation.\nHowever, a significant challenge remains: while previous works use perfect\npoint cloud generated in simulation, the models cannot directly apply to the\nnoisy point cloud in the real-world. To tackle this challenge, we leverage the\nproperty of real-world scanned point cloud that, the point cloud becomes less\nnoisy when the camera is closer to the object. Therefore, we propose a novel\ncoarse-to-fine affordance learning pipeline to mitigate the effect of point\ncloud noise in two stages. In the first stage, we learn the affordance on the\nnoisy far point cloud which includes the whole object to propose the\napproximated place to manipulate. Then, we move the camera in front of the\napproximated place, scan a less noisy point cloud containing precise local\ngeometries for manipulation, and learn affordance on such point cloud to\npropose fine-grained final actions. The proposed method is thoroughly evaluated\nboth using large-scale simulated noisy point clouds mimicking real-world scans,\nand in the real world scenarios, with superiority over existing methods,\ndemonstrating the effectiveness in tackling the noisy real-world point cloud\nproblem.\n", "link": "http://arxiv.org/abs/2402.18699v2", "date": "2024-03-07", "relevancy": 2.1408, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5536}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5332}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5298}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Articulated%20Object%20Manipulation%20with%20Coarse-to-fine%20Affordance%20for%0A%20%20Mitigating%20the%20Effect%20of%20Point%20Cloud%20Noise&body=Title%3A%20Articulated%20Object%20Manipulation%20with%20Coarse-to-fine%20Affordance%20for%0A%20%20Mitigating%20the%20Effect%20of%20Point%20Cloud%20Noise%0AAuthor%3A%20Suhan%20Ling%20and%20Yian%20Wang%20and%20Shiguang%20Wu%20and%20Yuzheng%20Zhuang%20and%20Tianyi%20Xu%20and%20Yu%20Li%20and%20Chang%20Liu%20and%20Hao%20Dong%0AAbstract%3A%20%20%203D%20articulated%20objects%20are%20inherently%20challenging%20for%20manipulation%20due%20to%20the%0Avaried%20geometries%20and%20intricate%20functionalities%20associated%20with%20articulated%0Aobjects.Point-level%20affordance%2C%20which%20predicts%20the%20per-point%20actionable%20score%0Aand%20thus%20proposes%20the%20best%20point%20to%20interact%20with%2C%20has%20demonstrated%20excellent%0Aperformance%20and%20generalization%20capabilities%20in%20articulated%20object%20manipulation.%0AHowever%2C%20a%20significant%20challenge%20remains%3A%20while%20previous%20works%20use%20perfect%0Apoint%20cloud%20generated%20in%20simulation%2C%20the%20models%20cannot%20directly%20apply%20to%20the%0Anoisy%20point%20cloud%20in%20the%20real-world.%20To%20tackle%20this%20challenge%2C%20we%20leverage%20the%0Aproperty%20of%20real-world%20scanned%20point%20cloud%20that%2C%20the%20point%20cloud%20becomes%20less%0Anoisy%20when%20the%20camera%20is%20closer%20to%20the%20object.%20Therefore%2C%20we%20propose%20a%20novel%0Acoarse-to-fine%20affordance%20learning%20pipeline%20to%20mitigate%20the%20effect%20of%20point%0Acloud%20noise%20in%20two%20stages.%20In%20the%20first%20stage%2C%20we%20learn%20the%20affordance%20on%20the%0Anoisy%20far%20point%20cloud%20which%20includes%20the%20whole%20object%20to%20propose%20the%0Aapproximated%20place%20to%20manipulate.%20Then%2C%20we%20move%20the%20camera%20in%20front%20of%20the%0Aapproximated%20place%2C%20scan%20a%20less%20noisy%20point%20cloud%20containing%20precise%20local%0Ageometries%20for%20manipulation%2C%20and%20learn%20affordance%20on%20such%20point%20cloud%20to%0Apropose%20fine-grained%20final%20actions.%20The%20proposed%20method%20is%20thoroughly%20evaluated%0Aboth%20using%20large-scale%20simulated%20noisy%20point%20clouds%20mimicking%20real-world%20scans%2C%0Aand%20in%20the%20real%20world%20scenarios%2C%20with%20superiority%20over%20existing%20methods%2C%0Ademonstrating%20the%20effectiveness%20in%20tackling%20the%20noisy%20real-world%20point%20cloud%0Aproblem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18699v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Articulated%20Object%20Manipulation%20with%20Coarse-to-fine%20Affordance%20for%0A%20%20Mitigating%20the%20Effect%20of%20Point%20Cloud%20Noise&entry.906535625=Suhan%20Ling%20and%20Yian%20Wang%20and%20Shiguang%20Wu%20and%20Yuzheng%20Zhuang%20and%20Tianyi%20Xu%20and%20Yu%20Li%20and%20Chang%20Liu%20and%20Hao%20Dong&entry.1292438233=%20%203D%20articulated%20objects%20are%20inherently%20challenging%20for%20manipulation%20due%20to%20the%0Avaried%20geometries%20and%20intricate%20functionalities%20associated%20with%20articulated%0Aobjects.Point-level%20affordance%2C%20which%20predicts%20the%20per-point%20actionable%20score%0Aand%20thus%20proposes%20the%20best%20point%20to%20interact%20with%2C%20has%20demonstrated%20excellent%0Aperformance%20and%20generalization%20capabilities%20in%20articulated%20object%20manipulation.%0AHowever%2C%20a%20significant%20challenge%20remains%3A%20while%20previous%20works%20use%20perfect%0Apoint%20cloud%20generated%20in%20simulation%2C%20the%20models%20cannot%20directly%20apply%20to%20the%0Anoisy%20point%20cloud%20in%20the%20real-world.%20To%20tackle%20this%20challenge%2C%20we%20leverage%20the%0Aproperty%20of%20real-world%20scanned%20point%20cloud%20that%2C%20the%20point%20cloud%20becomes%20less%0Anoisy%20when%20the%20camera%20is%20closer%20to%20the%20object.%20Therefore%2C%20we%20propose%20a%20novel%0Acoarse-to-fine%20affordance%20learning%20pipeline%20to%20mitigate%20the%20effect%20of%20point%0Acloud%20noise%20in%20two%20stages.%20In%20the%20first%20stage%2C%20we%20learn%20the%20affordance%20on%20the%0Anoisy%20far%20point%20cloud%20which%20includes%20the%20whole%20object%20to%20propose%20the%0Aapproximated%20place%20to%20manipulate.%20Then%2C%20we%20move%20the%20camera%20in%20front%20of%20the%0Aapproximated%20place%2C%20scan%20a%20less%20noisy%20point%20cloud%20containing%20precise%20local%0Ageometries%20for%20manipulation%2C%20and%20learn%20affordance%20on%20such%20point%20cloud%20to%0Apropose%20fine-grained%20final%20actions.%20The%20proposed%20method%20is%20thoroughly%20evaluated%0Aboth%20using%20large-scale%20simulated%20noisy%20point%20clouds%20mimicking%20real-world%20scans%2C%0Aand%20in%20the%20real%20world%20scenarios%2C%20with%20superiority%20over%20existing%20methods%2C%0Ademonstrating%20the%20effectiveness%20in%20tackling%20the%20noisy%20real-world%20point%20cloud%0Aproblem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18699v2&entry.124074799=Read"},
{"title": "Trust-Region Neural Moving Horizon Estimation for Robots", "author": "Bingheng Wang and Xuyang Chen and Lin Zhao", "abstract": "  Accurate disturbance estimation is essential for safe robot operations. The\nrecently proposed neural moving horizon estimation (NeuroMHE), which uses a\nportable neural network to model the MHE's weightings, has shown promise in\nfurther pushing the accuracy and efficiency boundary. Currently, NeuroMHE is\ntrained through gradient descent, with its gradient computed recursively using\na Kalman filter. This paper proposes a trust-region policy optimization method\nfor training NeuroMHE. We achieve this by providing the second-order\nderivatives of MHE, referred to as the MHE Hessian. Remarkably, we show that\nmuch of computation already used to obtain the gradient, especially the Kalman\nfilter, can be efficiently reused to compute the MHE Hessian. This offers\nlinear computational complexity relative to the MHE horizon. As a case study,\nwe evaluate the proposed trust region NeuroMHE on real quadrotor flight data\nfor disturbance estimation. Our approach demonstrates highly efficient training\nin under 5 min using only 100 data points. It outperforms a state-of-the-art\nneural estimator by up to 68.1% in force estimation accuracy, utilizing only\n1.4% of its network parameters. Furthermore, our method showcases enhanced\nrobustness to network initialization compared to the gradient descent\ncounterpart.\n", "link": "http://arxiv.org/abs/2309.05955v4", "date": "2024-03-07", "relevancy": 2.1345, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5546}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5217}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Trust-Region%20Neural%20Moving%20Horizon%20Estimation%20for%20Robots&body=Title%3A%20Trust-Region%20Neural%20Moving%20Horizon%20Estimation%20for%20Robots%0AAuthor%3A%20Bingheng%20Wang%20and%20Xuyang%20Chen%20and%20Lin%20Zhao%0AAbstract%3A%20%20%20Accurate%20disturbance%20estimation%20is%20essential%20for%20safe%20robot%20operations.%20The%0Arecently%20proposed%20neural%20moving%20horizon%20estimation%20%28NeuroMHE%29%2C%20which%20uses%20a%0Aportable%20neural%20network%20to%20model%20the%20MHE%27s%20weightings%2C%20has%20shown%20promise%20in%0Afurther%20pushing%20the%20accuracy%20and%20efficiency%20boundary.%20Currently%2C%20NeuroMHE%20is%0Atrained%20through%20gradient%20descent%2C%20with%20its%20gradient%20computed%20recursively%20using%0Aa%20Kalman%20filter.%20This%20paper%20proposes%20a%20trust-region%20policy%20optimization%20method%0Afor%20training%20NeuroMHE.%20We%20achieve%20this%20by%20providing%20the%20second-order%0Aderivatives%20of%20MHE%2C%20referred%20to%20as%20the%20MHE%20Hessian.%20Remarkably%2C%20we%20show%20that%0Amuch%20of%20computation%20already%20used%20to%20obtain%20the%20gradient%2C%20especially%20the%20Kalman%0Afilter%2C%20can%20be%20efficiently%20reused%20to%20compute%20the%20MHE%20Hessian.%20This%20offers%0Alinear%20computational%20complexity%20relative%20to%20the%20MHE%20horizon.%20As%20a%20case%20study%2C%0Awe%20evaluate%20the%20proposed%20trust%20region%20NeuroMHE%20on%20real%20quadrotor%20flight%20data%0Afor%20disturbance%20estimation.%20Our%20approach%20demonstrates%20highly%20efficient%20training%0Ain%20under%205%20min%20using%20only%20100%20data%20points.%20It%20outperforms%20a%20state-of-the-art%0Aneural%20estimator%20by%20up%20to%2068.1%25%20in%20force%20estimation%20accuracy%2C%20utilizing%20only%0A1.4%25%20of%20its%20network%20parameters.%20Furthermore%2C%20our%20method%20showcases%20enhanced%0Arobustness%20to%20network%20initialization%20compared%20to%20the%20gradient%20descent%0Acounterpart.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05955v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust-Region%20Neural%20Moving%20Horizon%20Estimation%20for%20Robots&entry.906535625=Bingheng%20Wang%20and%20Xuyang%20Chen%20and%20Lin%20Zhao&entry.1292438233=%20%20Accurate%20disturbance%20estimation%20is%20essential%20for%20safe%20robot%20operations.%20The%0Arecently%20proposed%20neural%20moving%20horizon%20estimation%20%28NeuroMHE%29%2C%20which%20uses%20a%0Aportable%20neural%20network%20to%20model%20the%20MHE%27s%20weightings%2C%20has%20shown%20promise%20in%0Afurther%20pushing%20the%20accuracy%20and%20efficiency%20boundary.%20Currently%2C%20NeuroMHE%20is%0Atrained%20through%20gradient%20descent%2C%20with%20its%20gradient%20computed%20recursively%20using%0Aa%20Kalman%20filter.%20This%20paper%20proposes%20a%20trust-region%20policy%20optimization%20method%0Afor%20training%20NeuroMHE.%20We%20achieve%20this%20by%20providing%20the%20second-order%0Aderivatives%20of%20MHE%2C%20referred%20to%20as%20the%20MHE%20Hessian.%20Remarkably%2C%20we%20show%20that%0Amuch%20of%20computation%20already%20used%20to%20obtain%20the%20gradient%2C%20especially%20the%20Kalman%0Afilter%2C%20can%20be%20efficiently%20reused%20to%20compute%20the%20MHE%20Hessian.%20This%20offers%0Alinear%20computational%20complexity%20relative%20to%20the%20MHE%20horizon.%20As%20a%20case%20study%2C%0Awe%20evaluate%20the%20proposed%20trust%20region%20NeuroMHE%20on%20real%20quadrotor%20flight%20data%0Afor%20disturbance%20estimation.%20Our%20approach%20demonstrates%20highly%20efficient%20training%0Ain%20under%205%20min%20using%20only%20100%20data%20points.%20It%20outperforms%20a%20state-of-the-art%0Aneural%20estimator%20by%20up%20to%2068.1%25%20in%20force%20estimation%20accuracy%2C%20utilizing%20only%0A1.4%25%20of%20its%20network%20parameters.%20Furthermore%2C%20our%20method%20showcases%20enhanced%0Arobustness%20to%20network%20initialization%20compared%20to%20the%20gradient%20descent%0Acounterpart.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05955v4&entry.124074799=Read"},
{"title": "Effectiveness Assessment of Recent Large Vision-Language Models", "author": "Yao Jiang and Xinyu Yan and Ge-Peng Ji and Keren Fu and Meijun Sun and Huan Xiong and Deng-Ping Fan and Fahad Shahbaz Khan", "abstract": "  The advent of large vision-language models (LVLMs) represents a noteworthy\nadvancement towards the pursuit of artificial general intelligence. However,\nthe extent of their efficacy across both specialized and general tasks warrants\nfurther investigation. This article endeavors to evaluate the competency of\npopular LVLMs in specialized and general tasks, respectively, aiming to offer a\ncomprehensive comprehension of these innovative methodologies. To gauge their\nefficacy in specialized tasks, we tailor a comprehensive testbed comprising\nthree distinct scenarios: natural, healthcare, and industrial, encompassing six\nchallenging tasks. These tasks include salient, camouflaged, and transparent\nobject detection, as well as polyp and skin lesion detection, alongside\nindustrial anomaly detection. We examine the performance of three recent\nopen-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of\nvisual recognition and localization. Moreover, we conduct empirical\ninvestigations utilizing the aforementioned models alongside GPT-4V, assessing\ntheir multi-modal understanding capacities in general tasks such as object\ncounting, absurd question answering, affordance reasoning, attribute\nrecognition, and spatial relation reasoning. Our investigations reveal that\nthese models demonstrate limited proficiency not only in specialized tasks but\nalso in general tasks. We delve deeper into this inadequacy and suggest several\npotential factors, including limited cognition in specialized tasks, object\nhallucination, text-to-image interference, and decreased robustness in complex\nproblems. We hope this study would provide valuable insights for the future\ndevelopment of LVLMs, augmenting their power in coping with both general and\nspecialized applications.\n", "link": "http://arxiv.org/abs/2403.04306v1", "date": "2024-03-07", "relevancy": 2.1271, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5364}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5332}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Effectiveness%20Assessment%20of%20Recent%20Large%20Vision-Language%20Models&body=Title%3A%20Effectiveness%20Assessment%20of%20Recent%20Large%20Vision-Language%20Models%0AAuthor%3A%20Yao%20Jiang%20and%20Xinyu%20Yan%20and%20Ge-Peng%20Ji%20and%20Keren%20Fu%20and%20Meijun%20Sun%20and%20Huan%20Xiong%20and%20Deng-Ping%20Fan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20The%20advent%20of%20large%20vision-language%20models%20%28LVLMs%29%20represents%20a%20noteworthy%0Aadvancement%20towards%20the%20pursuit%20of%20artificial%20general%20intelligence.%20However%2C%0Athe%20extent%20of%20their%20efficacy%20across%20both%20specialized%20and%20general%20tasks%20warrants%0Afurther%20investigation.%20This%20article%20endeavors%20to%20evaluate%20the%20competency%20of%0Apopular%20LVLMs%20in%20specialized%20and%20general%20tasks%2C%20respectively%2C%20aiming%20to%20offer%20a%0Acomprehensive%20comprehension%20of%20these%20innovative%20methodologies.%20To%20gauge%20their%0Aefficacy%20in%20specialized%20tasks%2C%20we%20tailor%20a%20comprehensive%20testbed%20comprising%0Athree%20distinct%20scenarios%3A%20natural%2C%20healthcare%2C%20and%20industrial%2C%20encompassing%20six%0Achallenging%20tasks.%20These%20tasks%20include%20salient%2C%20camouflaged%2C%20and%20transparent%0Aobject%20detection%2C%20as%20well%20as%20polyp%20and%20skin%20lesion%20detection%2C%20alongside%0Aindustrial%20anomaly%20detection.%20We%20examine%20the%20performance%20of%20three%20recent%0Aopen-source%20LVLMs%20--%20MiniGPT-v2%2C%20LLaVA-1.5%2C%20and%20Shikra%20--%20in%20the%20realm%20of%0Avisual%20recognition%20and%20localization.%20Moreover%2C%20we%20conduct%20empirical%0Ainvestigations%20utilizing%20the%20aforementioned%20models%20alongside%20GPT-4V%2C%20assessing%0Atheir%20multi-modal%20understanding%20capacities%20in%20general%20tasks%20such%20as%20object%0Acounting%2C%20absurd%20question%20answering%2C%20affordance%20reasoning%2C%20attribute%0Arecognition%2C%20and%20spatial%20relation%20reasoning.%20Our%20investigations%20reveal%20that%0Athese%20models%20demonstrate%20limited%20proficiency%20not%20only%20in%20specialized%20tasks%20but%0Aalso%20in%20general%20tasks.%20We%20delve%20deeper%20into%20this%20inadequacy%20and%20suggest%20several%0Apotential%20factors%2C%20including%20limited%20cognition%20in%20specialized%20tasks%2C%20object%0Ahallucination%2C%20text-to-image%20interference%2C%20and%20decreased%20robustness%20in%20complex%0Aproblems.%20We%20hope%20this%20study%20would%20provide%20valuable%20insights%20for%20the%20future%0Adevelopment%20of%20LVLMs%2C%20augmenting%20their%20power%20in%20coping%20with%20both%20general%20and%0Aspecialized%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04306v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effectiveness%20Assessment%20of%20Recent%20Large%20Vision-Language%20Models&entry.906535625=Yao%20Jiang%20and%20Xinyu%20Yan%20and%20Ge-Peng%20Ji%20and%20Keren%20Fu%20and%20Meijun%20Sun%20and%20Huan%20Xiong%20and%20Deng-Ping%20Fan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20The%20advent%20of%20large%20vision-language%20models%20%28LVLMs%29%20represents%20a%20noteworthy%0Aadvancement%20towards%20the%20pursuit%20of%20artificial%20general%20intelligence.%20However%2C%0Athe%20extent%20of%20their%20efficacy%20across%20both%20specialized%20and%20general%20tasks%20warrants%0Afurther%20investigation.%20This%20article%20endeavors%20to%20evaluate%20the%20competency%20of%0Apopular%20LVLMs%20in%20specialized%20and%20general%20tasks%2C%20respectively%2C%20aiming%20to%20offer%20a%0Acomprehensive%20comprehension%20of%20these%20innovative%20methodologies.%20To%20gauge%20their%0Aefficacy%20in%20specialized%20tasks%2C%20we%20tailor%20a%20comprehensive%20testbed%20comprising%0Athree%20distinct%20scenarios%3A%20natural%2C%20healthcare%2C%20and%20industrial%2C%20encompassing%20six%0Achallenging%20tasks.%20These%20tasks%20include%20salient%2C%20camouflaged%2C%20and%20transparent%0Aobject%20detection%2C%20as%20well%20as%20polyp%20and%20skin%20lesion%20detection%2C%20alongside%0Aindustrial%20anomaly%20detection.%20We%20examine%20the%20performance%20of%20three%20recent%0Aopen-source%20LVLMs%20--%20MiniGPT-v2%2C%20LLaVA-1.5%2C%20and%20Shikra%20--%20in%20the%20realm%20of%0Avisual%20recognition%20and%20localization.%20Moreover%2C%20we%20conduct%20empirical%0Ainvestigations%20utilizing%20the%20aforementioned%20models%20alongside%20GPT-4V%2C%20assessing%0Atheir%20multi-modal%20understanding%20capacities%20in%20general%20tasks%20such%20as%20object%0Acounting%2C%20absurd%20question%20answering%2C%20affordance%20reasoning%2C%20attribute%0Arecognition%2C%20and%20spatial%20relation%20reasoning.%20Our%20investigations%20reveal%20that%0Athese%20models%20demonstrate%20limited%20proficiency%20not%20only%20in%20specialized%20tasks%20but%0Aalso%20in%20general%20tasks.%20We%20delve%20deeper%20into%20this%20inadequacy%20and%20suggest%20several%0Apotential%20factors%2C%20including%20limited%20cognition%20in%20specialized%20tasks%2C%20object%0Ahallucination%2C%20text-to-image%20interference%2C%20and%20decreased%20robustness%20in%20complex%0Aproblems.%20We%20hope%20this%20study%20would%20provide%20valuable%20insights%20for%20the%20future%0Adevelopment%20of%20LVLMs%2C%20augmenting%20their%20power%20in%20coping%20with%20both%20general%20and%0Aspecialized%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04306v1&entry.124074799=Read"},
{"title": "How Far Are We from Intelligent Visual Deductive Reasoning?", "author": "Yizhe Zhang and He Bai and Ruixiang Zhang and Jiatao Gu and Shuangfei Zhai and Josh Susskind and Navdeep Jaitly", "abstract": "  Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.\n", "link": "http://arxiv.org/abs/2403.04732v1", "date": "2024-03-07", "relevancy": 2.1251, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5295}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5168}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20How%20Far%20Are%20We%20from%20Intelligent%20Visual%20Deductive%20Reasoning%3F&body=Title%3A%20How%20Far%20Are%20We%20from%20Intelligent%20Visual%20Deductive%20Reasoning%3F%0AAuthor%3A%20Yizhe%20Zhang%20and%20He%20Bai%20and%20Ruixiang%20Zhang%20and%20Jiatao%20Gu%20and%20Shuangfei%20Zhai%20and%20Josh%20Susskind%20and%20Navdeep%20Jaitly%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20such%20as%20GPT-4V%20have%20recently%20demonstrated%0Aincredible%20strides%20on%20diverse%20vision%20language%20tasks.%20We%20dig%20into%20vision-based%0Adeductive%20reasoning%2C%20a%20more%20sophisticated%20but%20less%20explored%20realm%2C%20and%20find%0Apreviously%20unexposed%20blindspots%20in%20the%20current%20SOTA%20VLMs.%20Specifically%2C%20we%0Aleverage%20Raven%27s%20Progressive%20Matrices%20%28RPMs%29%2C%20to%20assess%20VLMs%27%20abilities%20to%0Aperform%20multi-hop%20relational%20and%20deductive%20reasoning%20relying%20solely%20on%20visual%0Aclues.%20We%20perform%20comprehensive%20evaluations%20of%20several%20popular%20VLMs%20employing%0Astandard%20strategies%20such%20as%20in-context%20learning%2C%20self-consistency%2C%20and%0AChain-of-thoughts%20%28CoT%29%20on%20three%20diverse%20datasets%2C%20including%20the%20Mensa%20IQ%20test%2C%0AIntelligenceTest%2C%20and%20RAVEN.%20The%20results%20reveal%20that%20despite%20the%20impressive%0Acapabilities%20of%20LLMs%20in%20text-based%20reasoning%2C%20we%20are%20still%20far%20from%20achieving%0Acomparable%20proficiency%20in%20visual%20deductive%20reasoning.%20We%20found%20that%20certain%0Astandard%20strategies%20that%20are%20effective%20when%20applied%20to%20LLMs%20do%20not%20seamlessly%0Atranslate%20to%20the%20challenges%20presented%20by%20visual%20reasoning%20tasks.%20Moreover%2C%20a%0Adetailed%20analysis%20reveals%20that%20VLMs%20struggle%20to%20solve%20these%20tasks%20mainly%0Abecause%20they%20are%20unable%20to%20perceive%20and%20comprehend%20multiple%2C%20confounding%0Aabstract%20patterns%20in%20RPM%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04732v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Far%20Are%20We%20from%20Intelligent%20Visual%20Deductive%20Reasoning%3F&entry.906535625=Yizhe%20Zhang%20and%20He%20Bai%20and%20Ruixiang%20Zhang%20and%20Jiatao%20Gu%20and%20Shuangfei%20Zhai%20and%20Josh%20Susskind%20and%20Navdeep%20Jaitly&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20such%20as%20GPT-4V%20have%20recently%20demonstrated%0Aincredible%20strides%20on%20diverse%20vision%20language%20tasks.%20We%20dig%20into%20vision-based%0Adeductive%20reasoning%2C%20a%20more%20sophisticated%20but%20less%20explored%20realm%2C%20and%20find%0Apreviously%20unexposed%20blindspots%20in%20the%20current%20SOTA%20VLMs.%20Specifically%2C%20we%0Aleverage%20Raven%27s%20Progressive%20Matrices%20%28RPMs%29%2C%20to%20assess%20VLMs%27%20abilities%20to%0Aperform%20multi-hop%20relational%20and%20deductive%20reasoning%20relying%20solely%20on%20visual%0Aclues.%20We%20perform%20comprehensive%20evaluations%20of%20several%20popular%20VLMs%20employing%0Astandard%20strategies%20such%20as%20in-context%20learning%2C%20self-consistency%2C%20and%0AChain-of-thoughts%20%28CoT%29%20on%20three%20diverse%20datasets%2C%20including%20the%20Mensa%20IQ%20test%2C%0AIntelligenceTest%2C%20and%20RAVEN.%20The%20results%20reveal%20that%20despite%20the%20impressive%0Acapabilities%20of%20LLMs%20in%20text-based%20reasoning%2C%20we%20are%20still%20far%20from%20achieving%0Acomparable%20proficiency%20in%20visual%20deductive%20reasoning.%20We%20found%20that%20certain%0Astandard%20strategies%20that%20are%20effective%20when%20applied%20to%20LLMs%20do%20not%20seamlessly%0Atranslate%20to%20the%20challenges%20presented%20by%20visual%20reasoning%20tasks.%20Moreover%2C%20a%0Adetailed%20analysis%20reveals%20that%20VLMs%20struggle%20to%20solve%20these%20tasks%20mainly%0Abecause%20they%20are%20unable%20to%20perceive%20and%20comprehend%20multiple%2C%20confounding%0Aabstract%20patterns%20in%20RPM%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04732v1&entry.124074799=Read"},
{"title": "AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit\n  Detectors", "author": "Kaishen Yuan and Zitong Yu and Xin Liu and Weicheng Xie and Huanjing Yue and Jingyu Yang", "abstract": "  Facial Action Units (AU) is a vital concept in the realm of affective\ncomputing, and AU detection has always been a hot research topic. Existing\nmethods suffer from overfitting issues due to the utilization of a large number\nof learnable parameters on scarce AU-annotated datasets or heavy reliance on\nsubstantial additional relevant data. Parameter-Efficient Transfer Learning\n(PETL) provides a promising paradigm to address these challenges, whereas its\nexisting methods lack design for AU characteristics. Therefore, we innovatively\ninvestigate PETL paradigm to AU detection, introducing AUFormer and proposing a\nnovel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual\nMoKE specific to a certain AU with minimal learnable parameters first\nintegrates personalized multi-scale and correlation knowledge. Then the MoKE\ncollaborates with other MoKEs in the expert group to obtain aggregated\ninformation and inject it into the frozen Vision Transformer (ViT) to achieve\nparameter-efficient AU detection. Additionally, we design a Margin-truncated\nDifficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the\nmodel to focus more on activated AUs, differentiate the difficulty of\nunactivated AUs, and discard potential mislabeled samples. Extensive\nexperiments from various perspectives, including within-domain, cross-domain,\ndata efficiency, and micro-expression domain, demonstrate AUFormer's\nstate-of-the-art performance and robust generalization abilities without\nrelying on additional relevant data. The code for AUFormer is available at\nhttps://github.com/yuankaishen2001/AUFormer.\n", "link": "http://arxiv.org/abs/2403.04697v1", "date": "2024-03-07", "relevancy": 2.124, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5518}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5227}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20AUFormer%3A%20Vision%20Transformers%20are%20Parameter-Efficient%20Facial%20Action%20Unit%0A%20%20Detectors&body=Title%3A%20AUFormer%3A%20Vision%20Transformers%20are%20Parameter-Efficient%20Facial%20Action%20Unit%0A%20%20Detectors%0AAuthor%3A%20Kaishen%20Yuan%20and%20Zitong%20Yu%20and%20Xin%20Liu%20and%20Weicheng%20Xie%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang%0AAbstract%3A%20%20%20Facial%20Action%20Units%20%28AU%29%20is%20a%20vital%20concept%20in%20the%20realm%20of%20affective%0Acomputing%2C%20and%20AU%20detection%20has%20always%20been%20a%20hot%20research%20topic.%20Existing%0Amethods%20suffer%20from%20overfitting%20issues%20due%20to%20the%20utilization%20of%20a%20large%20number%0Aof%20learnable%20parameters%20on%20scarce%20AU-annotated%20datasets%20or%20heavy%20reliance%20on%0Asubstantial%20additional%20relevant%20data.%20Parameter-Efficient%20Transfer%20Learning%0A%28PETL%29%20provides%20a%20promising%20paradigm%20to%20address%20these%20challenges%2C%20whereas%20its%0Aexisting%20methods%20lack%20design%20for%20AU%20characteristics.%20Therefore%2C%20we%20innovatively%0Ainvestigate%20PETL%20paradigm%20to%20AU%20detection%2C%20introducing%20AUFormer%20and%20proposing%20a%0Anovel%20Mixture-of-Knowledge%20Expert%20%28MoKE%29%20collaboration%20mechanism.%20An%20individual%0AMoKE%20specific%20to%20a%20certain%20AU%20with%20minimal%20learnable%20parameters%20first%0Aintegrates%20personalized%20multi-scale%20and%20correlation%20knowledge.%20Then%20the%20MoKE%0Acollaborates%20with%20other%20MoKEs%20in%20the%20expert%20group%20to%20obtain%20aggregated%0Ainformation%20and%20inject%20it%20into%20the%20frozen%20Vision%20Transformer%20%28ViT%29%20to%20achieve%0Aparameter-efficient%20AU%20detection.%20Additionally%2C%20we%20design%20a%20Margin-truncated%0ADifficulty-aware%20Weighted%20Asymmetric%20Loss%20%28MDWA-Loss%29%2C%20which%20can%20encourage%20the%0Amodel%20to%20focus%20more%20on%20activated%20AUs%2C%20differentiate%20the%20difficulty%20of%0Aunactivated%20AUs%2C%20and%20discard%20potential%20mislabeled%20samples.%20Extensive%0Aexperiments%20from%20various%20perspectives%2C%20including%20within-domain%2C%20cross-domain%2C%0Adata%20efficiency%2C%20and%20micro-expression%20domain%2C%20demonstrate%20AUFormer%27s%0Astate-of-the-art%20performance%20and%20robust%20generalization%20abilities%20without%0Arelying%20on%20additional%20relevant%20data.%20The%20code%20for%20AUFormer%20is%20available%20at%0Ahttps%3A//github.com/yuankaishen2001/AUFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04697v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AUFormer%3A%20Vision%20Transformers%20are%20Parameter-Efficient%20Facial%20Action%20Unit%0A%20%20Detectors&entry.906535625=Kaishen%20Yuan%20and%20Zitong%20Yu%20and%20Xin%20Liu%20and%20Weicheng%20Xie%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang&entry.1292438233=%20%20Facial%20Action%20Units%20%28AU%29%20is%20a%20vital%20concept%20in%20the%20realm%20of%20affective%0Acomputing%2C%20and%20AU%20detection%20has%20always%20been%20a%20hot%20research%20topic.%20Existing%0Amethods%20suffer%20from%20overfitting%20issues%20due%20to%20the%20utilization%20of%20a%20large%20number%0Aof%20learnable%20parameters%20on%20scarce%20AU-annotated%20datasets%20or%20heavy%20reliance%20on%0Asubstantial%20additional%20relevant%20data.%20Parameter-Efficient%20Transfer%20Learning%0A%28PETL%29%20provides%20a%20promising%20paradigm%20to%20address%20these%20challenges%2C%20whereas%20its%0Aexisting%20methods%20lack%20design%20for%20AU%20characteristics.%20Therefore%2C%20we%20innovatively%0Ainvestigate%20PETL%20paradigm%20to%20AU%20detection%2C%20introducing%20AUFormer%20and%20proposing%20a%0Anovel%20Mixture-of-Knowledge%20Expert%20%28MoKE%29%20collaboration%20mechanism.%20An%20individual%0AMoKE%20specific%20to%20a%20certain%20AU%20with%20minimal%20learnable%20parameters%20first%0Aintegrates%20personalized%20multi-scale%20and%20correlation%20knowledge.%20Then%20the%20MoKE%0Acollaborates%20with%20other%20MoKEs%20in%20the%20expert%20group%20to%20obtain%20aggregated%0Ainformation%20and%20inject%20it%20into%20the%20frozen%20Vision%20Transformer%20%28ViT%29%20to%20achieve%0Aparameter-efficient%20AU%20detection.%20Additionally%2C%20we%20design%20a%20Margin-truncated%0ADifficulty-aware%20Weighted%20Asymmetric%20Loss%20%28MDWA-Loss%29%2C%20which%20can%20encourage%20the%0Amodel%20to%20focus%20more%20on%20activated%20AUs%2C%20differentiate%20the%20difficulty%20of%0Aunactivated%20AUs%2C%20and%20discard%20potential%20mislabeled%20samples.%20Extensive%0Aexperiments%20from%20various%20perspectives%2C%20including%20within-domain%2C%20cross-domain%2C%0Adata%20efficiency%2C%20and%20micro-expression%20domain%2C%20demonstrate%20AUFormer%27s%0Astate-of-the-art%20performance%20and%20robust%20generalization%20abilities%20without%0Arelying%20on%20additional%20relevant%20data.%20The%20code%20for%20AUFormer%20is%20available%20at%0Ahttps%3A//github.com/yuankaishen2001/AUFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04697v1&entry.124074799=Read"},
{"title": "Bridging the Gap between Chemical Reaction Pretraining and Conditional\n  Molecule Generation with a Unified Model", "author": "Bo Qiang and Yiran Zhou and Yuheng Ding and Ningfeng Liu and Song Song and Liangren Zhang and Bo Huang and Zhenming Liu", "abstract": "  Chemical reactions are the fundamental building blocks of drug design and\norganic chemistry research. In recent years, there has been a growing need for\na large-scale deep-learning framework that can efficiently capture the basic\nrules of chemical reactions. In this paper, we have proposed a unified\nframework that addresses both the reaction representation learning and molecule\ngeneration tasks, which allows for a more holistic approach. Inspired by the\norganic chemistry mechanism, we develop a novel pretraining framework that\nenables us to incorporate inductive biases into the model. Our framework\nachieves state-of-the-art results on challenging downstream tasks. By\npossessing chemical knowledge, our generative framework overcome the\nlimitations of current molecule generation models that rely on a small number\nof reaction templates. In the extensive experiments, our model generates\nsynthesizable drug-like structures of high quality. Overall, our work presents\na significant step toward a large-scale deep-learning framework for a variety\nof reaction-based applications.\n", "link": "http://arxiv.org/abs/2303.06965v5", "date": "2024-03-07", "relevancy": 2.1033, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5483}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.518}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4892}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Gap%20between%20Chemical%20Reaction%20Pretraining%20and%20Conditional%0A%20%20Molecule%20Generation%20with%20a%20Unified%20Model&body=Title%3A%20Bridging%20the%20Gap%20between%20Chemical%20Reaction%20Pretraining%20and%20Conditional%0A%20%20Molecule%20Generation%20with%20a%20Unified%20Model%0AAuthor%3A%20Bo%20Qiang%20and%20Yiran%20Zhou%20and%20Yuheng%20Ding%20and%20Ningfeng%20Liu%20and%20Song%20Song%20and%20Liangren%20Zhang%20and%20Bo%20Huang%20and%20Zhenming%20Liu%0AAbstract%3A%20%20%20Chemical%20reactions%20are%20the%20fundamental%20building%20blocks%20of%20drug%20design%20and%0Aorganic%20chemistry%20research.%20In%20recent%20years%2C%20there%20has%20been%20a%20growing%20need%20for%0Aa%20large-scale%20deep-learning%20framework%20that%20can%20efficiently%20capture%20the%20basic%0Arules%20of%20chemical%20reactions.%20In%20this%20paper%2C%20we%20have%20proposed%20a%20unified%0Aframework%20that%20addresses%20both%20the%20reaction%20representation%20learning%20and%20molecule%0Ageneration%20tasks%2C%20which%20allows%20for%20a%20more%20holistic%20approach.%20Inspired%20by%20the%0Aorganic%20chemistry%20mechanism%2C%20we%20develop%20a%20novel%20pretraining%20framework%20that%0Aenables%20us%20to%20incorporate%20inductive%20biases%20into%20the%20model.%20Our%20framework%0Aachieves%20state-of-the-art%20results%20on%20challenging%20downstream%20tasks.%20By%0Apossessing%20chemical%20knowledge%2C%20our%20generative%20framework%20overcome%20the%0Alimitations%20of%20current%20molecule%20generation%20models%20that%20rely%20on%20a%20small%20number%0Aof%20reaction%20templates.%20In%20the%20extensive%20experiments%2C%20our%20model%20generates%0Asynthesizable%20drug-like%20structures%20of%20high%20quality.%20Overall%2C%20our%20work%20presents%0Aa%20significant%20step%20toward%20a%20large-scale%20deep-learning%20framework%20for%20a%20variety%0Aof%20reaction-based%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06965v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Gap%20between%20Chemical%20Reaction%20Pretraining%20and%20Conditional%0A%20%20Molecule%20Generation%20with%20a%20Unified%20Model&entry.906535625=Bo%20Qiang%20and%20Yiran%20Zhou%20and%20Yuheng%20Ding%20and%20Ningfeng%20Liu%20and%20Song%20Song%20and%20Liangren%20Zhang%20and%20Bo%20Huang%20and%20Zhenming%20Liu&entry.1292438233=%20%20Chemical%20reactions%20are%20the%20fundamental%20building%20blocks%20of%20drug%20design%20and%0Aorganic%20chemistry%20research.%20In%20recent%20years%2C%20there%20has%20been%20a%20growing%20need%20for%0Aa%20large-scale%20deep-learning%20framework%20that%20can%20efficiently%20capture%20the%20basic%0Arules%20of%20chemical%20reactions.%20In%20this%20paper%2C%20we%20have%20proposed%20a%20unified%0Aframework%20that%20addresses%20both%20the%20reaction%20representation%20learning%20and%20molecule%0Ageneration%20tasks%2C%20which%20allows%20for%20a%20more%20holistic%20approach.%20Inspired%20by%20the%0Aorganic%20chemistry%20mechanism%2C%20we%20develop%20a%20novel%20pretraining%20framework%20that%0Aenables%20us%20to%20incorporate%20inductive%20biases%20into%20the%20model.%20Our%20framework%0Aachieves%20state-of-the-art%20results%20on%20challenging%20downstream%20tasks.%20By%0Apossessing%20chemical%20knowledge%2C%20our%20generative%20framework%20overcome%20the%0Alimitations%20of%20current%20molecule%20generation%20models%20that%20rely%20on%20a%20small%20number%0Aof%20reaction%20templates.%20In%20the%20extensive%20experiments%2C%20our%20model%20generates%0Asynthesizable%20drug-like%20structures%20of%20high%20quality.%20Overall%2C%20our%20work%20presents%0Aa%20significant%20step%20toward%20a%20large-scale%20deep-learning%20framework%20for%20a%20variety%0Aof%20reaction-based%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06965v5&entry.124074799=Read"},
{"title": "Label Alignment Regularization for Distribution Shift", "author": "Ehsan Imani and Guojun Zhang and Runjia Li and Jun Luo and Pascal Poupart and Philip H. S. Torr and Yangchen Pan", "abstract": "  Recent work has highlighted the label alignment property (LAP) in supervised\nlearning, where the vector of all labels in the dataset is mostly in the span\nof the top few singular vectors of the data matrix. Drawing inspiration from\nthis observation, we propose a regularization method for unsupervised domain\nadaptation that encourages alignment between the predictions in the target\ndomain and its top singular vectors. Unlike conventional domain adaptation\napproaches that focus on regularizing representations, we instead regularize\nthe classifier to align with the unsupervised target data, guided by the LAP in\nboth the source and target domains. Theoretical analysis demonstrates that,\nunder certain assumptions, our solution resides within the span of the top\nright singular vectors of the target domain data and aligns with the optimal\nsolution. By removing the reliance on the commonly used optimal joint risk\nassumption found in classic domain adaptation theory, we showcase the\neffectiveness of our method on addressing problems where traditional domain\nadaptation methods often fall short due to high joint error. Additionally, we\nreport improved performance over domain adaptation baselines in well-known\ntasks such as MNIST-USPS domain adaptation and cross-lingual sentiment\nanalysis.\n", "link": "http://arxiv.org/abs/2211.14960v3", "date": "2024-03-07", "relevancy": 2.1029, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5119}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5103}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Label%20Alignment%20Regularization%20for%20Distribution%20Shift&body=Title%3A%20Label%20Alignment%20Regularization%20for%20Distribution%20Shift%0AAuthor%3A%20Ehsan%20Imani%20and%20Guojun%20Zhang%20and%20Runjia%20Li%20and%20Jun%20Luo%20and%20Pascal%20Poupart%20and%20Philip%20H.%20S.%20Torr%20and%20Yangchen%20Pan%0AAbstract%3A%20%20%20Recent%20work%20has%20highlighted%20the%20label%20alignment%20property%20%28LAP%29%20in%20supervised%0Alearning%2C%20where%20the%20vector%20of%20all%20labels%20in%20the%20dataset%20is%20mostly%20in%20the%20span%0Aof%20the%20top%20few%20singular%20vectors%20of%20the%20data%20matrix.%20Drawing%20inspiration%20from%0Athis%20observation%2C%20we%20propose%20a%20regularization%20method%20for%20unsupervised%20domain%0Aadaptation%20that%20encourages%20alignment%20between%20the%20predictions%20in%20the%20target%0Adomain%20and%20its%20top%20singular%20vectors.%20Unlike%20conventional%20domain%20adaptation%0Aapproaches%20that%20focus%20on%20regularizing%20representations%2C%20we%20instead%20regularize%0Athe%20classifier%20to%20align%20with%20the%20unsupervised%20target%20data%2C%20guided%20by%20the%20LAP%20in%0Aboth%20the%20source%20and%20target%20domains.%20Theoretical%20analysis%20demonstrates%20that%2C%0Aunder%20certain%20assumptions%2C%20our%20solution%20resides%20within%20the%20span%20of%20the%20top%0Aright%20singular%20vectors%20of%20the%20target%20domain%20data%20and%20aligns%20with%20the%20optimal%0Asolution.%20By%20removing%20the%20reliance%20on%20the%20commonly%20used%20optimal%20joint%20risk%0Aassumption%20found%20in%20classic%20domain%20adaptation%20theory%2C%20we%20showcase%20the%0Aeffectiveness%20of%20our%20method%20on%20addressing%20problems%20where%20traditional%20domain%0Aadaptation%20methods%20often%20fall%20short%20due%20to%20high%20joint%20error.%20Additionally%2C%20we%0Areport%20improved%20performance%20over%20domain%20adaptation%20baselines%20in%20well-known%0Atasks%20such%20as%20MNIST-USPS%20domain%20adaptation%20and%20cross-lingual%20sentiment%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.14960v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Alignment%20Regularization%20for%20Distribution%20Shift&entry.906535625=Ehsan%20Imani%20and%20Guojun%20Zhang%20and%20Runjia%20Li%20and%20Jun%20Luo%20and%20Pascal%20Poupart%20and%20Philip%20H.%20S.%20Torr%20and%20Yangchen%20Pan&entry.1292438233=%20%20Recent%20work%20has%20highlighted%20the%20label%20alignment%20property%20%28LAP%29%20in%20supervised%0Alearning%2C%20where%20the%20vector%20of%20all%20labels%20in%20the%20dataset%20is%20mostly%20in%20the%20span%0Aof%20the%20top%20few%20singular%20vectors%20of%20the%20data%20matrix.%20Drawing%20inspiration%20from%0Athis%20observation%2C%20we%20propose%20a%20regularization%20method%20for%20unsupervised%20domain%0Aadaptation%20that%20encourages%20alignment%20between%20the%20predictions%20in%20the%20target%0Adomain%20and%20its%20top%20singular%20vectors.%20Unlike%20conventional%20domain%20adaptation%0Aapproaches%20that%20focus%20on%20regularizing%20representations%2C%20we%20instead%20regularize%0Athe%20classifier%20to%20align%20with%20the%20unsupervised%20target%20data%2C%20guided%20by%20the%20LAP%20in%0Aboth%20the%20source%20and%20target%20domains.%20Theoretical%20analysis%20demonstrates%20that%2C%0Aunder%20certain%20assumptions%2C%20our%20solution%20resides%20within%20the%20span%20of%20the%20top%0Aright%20singular%20vectors%20of%20the%20target%20domain%20data%20and%20aligns%20with%20the%20optimal%0Asolution.%20By%20removing%20the%20reliance%20on%20the%20commonly%20used%20optimal%20joint%20risk%0Aassumption%20found%20in%20classic%20domain%20adaptation%20theory%2C%20we%20showcase%20the%0Aeffectiveness%20of%20our%20method%20on%20addressing%20problems%20where%20traditional%20domain%0Aadaptation%20methods%20often%20fall%20short%20due%20to%20high%20joint%20error.%20Additionally%2C%20we%0Areport%20improved%20performance%20over%20domain%20adaptation%20baselines%20in%20well-known%0Atasks%20such%20as%20MNIST-USPS%20domain%20adaptation%20and%20cross-lingual%20sentiment%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.14960v3&entry.124074799=Read"},
{"title": "Dynamic Cross Attention for Audio-Visual Person Verification", "author": "R. Gnana Praveen and Jahangir Alam", "abstract": "  Although person or identity verification has been predominantly explored\nusing individual modalities such as face and voice, audio-visual fusion has\nrecently shown immense potential to outperform unimodal approaches. Audio and\nvisual modalities are often expected to pose strong complementary\nrelationships, which plays a crucial role in effective audio-visual fusion.\nHowever, they may not always strongly complement each other, they may also\nexhibit weak complementary relationships, resulting in poor audio-visual\nfeature representations. In this paper, we propose a Dynamic Cross-Attention\n(DCA) model that can dynamically select the cross-attended or unattended\nfeatures on the fly based on the strong or weak complementary relationships,\nrespectively, across audio and visual modalities. In particular, a conditional\ngating layer is designed to evaluate the contribution of the cross-attention\nmechanism and choose cross-attended features only when they exhibit strong\ncomplementary relationships, otherwise unattended features. Extensive\nexperiments are conducted on the Voxceleb1 dataset to demonstrate the\nrobustness of the proposed model. Results indicate that the proposed model\nconsistently improves the performance on multiple variants of cross-attention\nwhile outperforming the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.04661v1", "date": "2024-03-07", "relevancy": 2.0997, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5427}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.518}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Cross%20Attention%20for%20Audio-Visual%20Person%20Verification&body=Title%3A%20Dynamic%20Cross%20Attention%20for%20Audio-Visual%20Person%20Verification%0AAuthor%3A%20R.%20Gnana%20Praveen%20and%20Jahangir%20Alam%0AAbstract%3A%20%20%20Although%20person%20or%20identity%20verification%20has%20been%20predominantly%20explored%0Ausing%20individual%20modalities%20such%20as%20face%20and%20voice%2C%20audio-visual%20fusion%20has%0Arecently%20shown%20immense%20potential%20to%20outperform%20unimodal%20approaches.%20Audio%20and%0Avisual%20modalities%20are%20often%20expected%20to%20pose%20strong%20complementary%0Arelationships%2C%20which%20plays%20a%20crucial%20role%20in%20effective%20audio-visual%20fusion.%0AHowever%2C%20they%20may%20not%20always%20strongly%20complement%20each%20other%2C%20they%20may%20also%0Aexhibit%20weak%20complementary%20relationships%2C%20resulting%20in%20poor%20audio-visual%0Afeature%20representations.%20In%20this%20paper%2C%20we%20propose%20a%20Dynamic%20Cross-Attention%0A%28DCA%29%20model%20that%20can%20dynamically%20select%20the%20cross-attended%20or%20unattended%0Afeatures%20on%20the%20fly%20based%20on%20the%20strong%20or%20weak%20complementary%20relationships%2C%0Arespectively%2C%20across%20audio%20and%20visual%20modalities.%20In%20particular%2C%20a%20conditional%0Agating%20layer%20is%20designed%20to%20evaluate%20the%20contribution%20of%20the%20cross-attention%0Amechanism%20and%20choose%20cross-attended%20features%20only%20when%20they%20exhibit%20strong%0Acomplementary%20relationships%2C%20otherwise%20unattended%20features.%20Extensive%0Aexperiments%20are%20conducted%20on%20the%20Voxceleb1%20dataset%20to%20demonstrate%20the%0Arobustness%20of%20the%20proposed%20model.%20Results%20indicate%20that%20the%20proposed%20model%0Aconsistently%20improves%20the%20performance%20on%20multiple%20variants%20of%20cross-attention%0Awhile%20outperforming%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04661v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Cross%20Attention%20for%20Audio-Visual%20Person%20Verification&entry.906535625=R.%20Gnana%20Praveen%20and%20Jahangir%20Alam&entry.1292438233=%20%20Although%20person%20or%20identity%20verification%20has%20been%20predominantly%20explored%0Ausing%20individual%20modalities%20such%20as%20face%20and%20voice%2C%20audio-visual%20fusion%20has%0Arecently%20shown%20immense%20potential%20to%20outperform%20unimodal%20approaches.%20Audio%20and%0Avisual%20modalities%20are%20often%20expected%20to%20pose%20strong%20complementary%0Arelationships%2C%20which%20plays%20a%20crucial%20role%20in%20effective%20audio-visual%20fusion.%0AHowever%2C%20they%20may%20not%20always%20strongly%20complement%20each%20other%2C%20they%20may%20also%0Aexhibit%20weak%20complementary%20relationships%2C%20resulting%20in%20poor%20audio-visual%0Afeature%20representations.%20In%20this%20paper%2C%20we%20propose%20a%20Dynamic%20Cross-Attention%0A%28DCA%29%20model%20that%20can%20dynamically%20select%20the%20cross-attended%20or%20unattended%0Afeatures%20on%20the%20fly%20based%20on%20the%20strong%20or%20weak%20complementary%20relationships%2C%0Arespectively%2C%20across%20audio%20and%20visual%20modalities.%20In%20particular%2C%20a%20conditional%0Agating%20layer%20is%20designed%20to%20evaluate%20the%20contribution%20of%20the%20cross-attention%0Amechanism%20and%20choose%20cross-attended%20features%20only%20when%20they%20exhibit%20strong%0Acomplementary%20relationships%2C%20otherwise%20unattended%20features.%20Extensive%0Aexperiments%20are%20conducted%20on%20the%20Voxceleb1%20dataset%20to%20demonstrate%20the%0Arobustness%20of%20the%20proposed%20model.%20Results%20indicate%20that%20the%20proposed%20model%0Aconsistently%20improves%20the%20performance%20on%20multiple%20variants%20of%20cross-attention%0Awhile%20outperforming%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04661v1&entry.124074799=Read"},
{"title": "IRConStyle: Image Restoration Framework Using Contrastive Learning and\n  Style Transfer", "author": "Dongqi Fan and Xin Zhao and Liang Chang", "abstract": "  Recently, the contrastive learning paradigm has achieved remarkable success\nin high-level tasks such as classification, detection, and segmentation.\nHowever, contrastive learning applied in low-level tasks, like image\nrestoration, is limited, and its effectiveness is uncertain. This raises a\nquestion: Why does the contrastive learning paradigm not yield satisfactory\nresults in image restoration? In this paper, we conduct in-depth analyses and\npropose three guidelines to address the above question. In addition, inspired\nby style transfer and based on contrastive learning, we propose a novel module\nfor image restoration called \\textbf{ConStyle}, which can be efficiently\nintegrated into any U-Net structure network. By leveraging the flexibility of\nConStyle, we develop a \\textbf{general restoration network} for image\nrestoration. ConStyle and the general restoration network together form an\nimage restoration framework, namely \\textbf{IRConStyle}. To demonstrate the\ncapability and compatibility of ConStyle, we replace the general restoration\nnetwork with transformer-based, CNN-based, and MLP-based networks,\nrespectively. We perform extensive experiments on various image restoration\ntasks, including denoising, deblurring, deraining, and dehazing. The results on\n19 benchmarks demonstrate that ConStyle can be integrated with any U-Net-based\nnetwork and significantly enhance performance. For instance, ConStyle NAFNet\nsignificantly outperforms the original NAFNet on SOTS outdoor (dehazing) and\nRain100H (deraining) datasets, with PSNR improvements of 4.16 dB and 3.58 dB\nwith 85% fewer parameters.\n", "link": "http://arxiv.org/abs/2402.15784v3", "date": "2024-03-07", "relevancy": 2.0884, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5386}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5281}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5032}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20IRConStyle%3A%20Image%20Restoration%20Framework%20Using%20Contrastive%20Learning%20and%0A%20%20Style%20Transfer&body=Title%3A%20IRConStyle%3A%20Image%20Restoration%20Framework%20Using%20Contrastive%20Learning%20and%0A%20%20Style%20Transfer%0AAuthor%3A%20Dongqi%20Fan%20and%20Xin%20Zhao%20and%20Liang%20Chang%0AAbstract%3A%20%20%20Recently%2C%20the%20contrastive%20learning%20paradigm%20has%20achieved%20remarkable%20success%0Ain%20high-level%20tasks%20such%20as%20classification%2C%20detection%2C%20and%20segmentation.%0AHowever%2C%20contrastive%20learning%20applied%20in%20low-level%20tasks%2C%20like%20image%0Arestoration%2C%20is%20limited%2C%20and%20its%20effectiveness%20is%20uncertain.%20This%20raises%20a%0Aquestion%3A%20Why%20does%20the%20contrastive%20learning%20paradigm%20not%20yield%20satisfactory%0Aresults%20in%20image%20restoration%3F%20In%20this%20paper%2C%20we%20conduct%20in-depth%20analyses%20and%0Apropose%20three%20guidelines%20to%20address%20the%20above%20question.%20In%20addition%2C%20inspired%0Aby%20style%20transfer%20and%20based%20on%20contrastive%20learning%2C%20we%20propose%20a%20novel%20module%0Afor%20image%20restoration%20called%20%5Ctextbf%7BConStyle%7D%2C%20which%20can%20be%20efficiently%0Aintegrated%20into%20any%20U-Net%20structure%20network.%20By%20leveraging%20the%20flexibility%20of%0AConStyle%2C%20we%20develop%20a%20%5Ctextbf%7Bgeneral%20restoration%20network%7D%20for%20image%0Arestoration.%20ConStyle%20and%20the%20general%20restoration%20network%20together%20form%20an%0Aimage%20restoration%20framework%2C%20namely%20%5Ctextbf%7BIRConStyle%7D.%20To%20demonstrate%20the%0Acapability%20and%20compatibility%20of%20ConStyle%2C%20we%20replace%20the%20general%20restoration%0Anetwork%20with%20transformer-based%2C%20CNN-based%2C%20and%20MLP-based%20networks%2C%0Arespectively.%20We%20perform%20extensive%20experiments%20on%20various%20image%20restoration%0Atasks%2C%20including%20denoising%2C%20deblurring%2C%20deraining%2C%20and%20dehazing.%20The%20results%20on%0A19%20benchmarks%20demonstrate%20that%20ConStyle%20can%20be%20integrated%20with%20any%20U-Net-based%0Anetwork%20and%20significantly%20enhance%20performance.%20For%20instance%2C%20ConStyle%20NAFNet%0Asignificantly%20outperforms%20the%20original%20NAFNet%20on%20SOTS%20outdoor%20%28dehazing%29%20and%0ARain100H%20%28deraining%29%20datasets%2C%20with%20PSNR%20improvements%20of%204.16%20dB%20and%203.58%20dB%0Awith%2085%25%20fewer%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15784v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRConStyle%3A%20Image%20Restoration%20Framework%20Using%20Contrastive%20Learning%20and%0A%20%20Style%20Transfer&entry.906535625=Dongqi%20Fan%20and%20Xin%20Zhao%20and%20Liang%20Chang&entry.1292438233=%20%20Recently%2C%20the%20contrastive%20learning%20paradigm%20has%20achieved%20remarkable%20success%0Ain%20high-level%20tasks%20such%20as%20classification%2C%20detection%2C%20and%20segmentation.%0AHowever%2C%20contrastive%20learning%20applied%20in%20low-level%20tasks%2C%20like%20image%0Arestoration%2C%20is%20limited%2C%20and%20its%20effectiveness%20is%20uncertain.%20This%20raises%20a%0Aquestion%3A%20Why%20does%20the%20contrastive%20learning%20paradigm%20not%20yield%20satisfactory%0Aresults%20in%20image%20restoration%3F%20In%20this%20paper%2C%20we%20conduct%20in-depth%20analyses%20and%0Apropose%20three%20guidelines%20to%20address%20the%20above%20question.%20In%20addition%2C%20inspired%0Aby%20style%20transfer%20and%20based%20on%20contrastive%20learning%2C%20we%20propose%20a%20novel%20module%0Afor%20image%20restoration%20called%20%5Ctextbf%7BConStyle%7D%2C%20which%20can%20be%20efficiently%0Aintegrated%20into%20any%20U-Net%20structure%20network.%20By%20leveraging%20the%20flexibility%20of%0AConStyle%2C%20we%20develop%20a%20%5Ctextbf%7Bgeneral%20restoration%20network%7D%20for%20image%0Arestoration.%20ConStyle%20and%20the%20general%20restoration%20network%20together%20form%20an%0Aimage%20restoration%20framework%2C%20namely%20%5Ctextbf%7BIRConStyle%7D.%20To%20demonstrate%20the%0Acapability%20and%20compatibility%20of%20ConStyle%2C%20we%20replace%20the%20general%20restoration%0Anetwork%20with%20transformer-based%2C%20CNN-based%2C%20and%20MLP-based%20networks%2C%0Arespectively.%20We%20perform%20extensive%20experiments%20on%20various%20image%20restoration%0Atasks%2C%20including%20denoising%2C%20deblurring%2C%20deraining%2C%20and%20dehazing.%20The%20results%20on%0A19%20benchmarks%20demonstrate%20that%20ConStyle%20can%20be%20integrated%20with%20any%20U-Net-based%0Anetwork%20and%20significantly%20enhance%20performance.%20For%20instance%2C%20ConStyle%20NAFNet%0Asignificantly%20outperforms%20the%20original%20NAFNet%20on%20SOTS%20outdoor%20%28dehazing%29%20and%0ARain100H%20%28deraining%29%20datasets%2C%20with%20PSNR%20improvements%20of%204.16%20dB%20and%203.58%20dB%0Awith%2085%25%20fewer%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15784v3&entry.124074799=Read"},
{"title": "TensoIR: Tensorial Inverse Rendering", "author": "Haian Jin and Isabella Liu and Peijia Xu and Xiaoshuai Zhang and Songfang Han and Sai Bi and Xiaowei Zhou and Zexiang Xu and Hao Su", "abstract": "  We propose TensoIR, a novel inverse rendering approach based on tensor\nfactorization and neural fields. Unlike previous works that use purely\nMLP-based neural fields, thus suffering from low capacity and high computation\ncosts, we extend TensoRF, a state-of-the-art approach for radiance field\nmodeling, to estimate scene geometry, surface reflectance, and environment\nillumination from multi-view images captured under unknown lighting conditions.\nOur approach jointly achieves radiance field reconstruction and\nphysically-based model estimation, leading to photo-realistic novel view\nsynthesis and relighting results. Benefiting from the efficiency and\nextensibility of the TensoRF-based representation, our method can accurately\nmodel secondary shading effects (like shadows and indirect lighting) and\ngenerally support input images captured under single or multiple unknown\nlighting conditions. The low-rank tensor representation allows us to not only\nachieve fast and compact reconstruction but also better exploit shared\ninformation under an arbitrary number of capturing lighting conditions. We\ndemonstrate the superiority of our method to baseline methods qualitatively and\nquantitatively on various challenging synthetic and real-world scenes.\n", "link": "http://arxiv.org/abs/2304.12461v2", "date": "2024-03-07", "relevancy": 2.0881, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5591}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5448}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4759}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20TensoIR%3A%20Tensorial%20Inverse%20Rendering&body=Title%3A%20TensoIR%3A%20Tensorial%20Inverse%20Rendering%0AAuthor%3A%20Haian%20Jin%20and%20Isabella%20Liu%20and%20Peijia%20Xu%20and%20Xiaoshuai%20Zhang%20and%20Songfang%20Han%20and%20Sai%20Bi%20and%20Xiaowei%20Zhou%20and%20Zexiang%20Xu%20and%20Hao%20Su%0AAbstract%3A%20%20%20We%20propose%20TensoIR%2C%20a%20novel%20inverse%20rendering%20approach%20based%20on%20tensor%0Afactorization%20and%20neural%20fields.%20Unlike%20previous%20works%20that%20use%20purely%0AMLP-based%20neural%20fields%2C%20thus%20suffering%20from%20low%20capacity%20and%20high%20computation%0Acosts%2C%20we%20extend%20TensoRF%2C%20a%20state-of-the-art%20approach%20for%20radiance%20field%0Amodeling%2C%20to%20estimate%20scene%20geometry%2C%20surface%20reflectance%2C%20and%20environment%0Aillumination%20from%20multi-view%20images%20captured%20under%20unknown%20lighting%20conditions.%0AOur%20approach%20jointly%20achieves%20radiance%20field%20reconstruction%20and%0Aphysically-based%20model%20estimation%2C%20leading%20to%20photo-realistic%20novel%20view%0Asynthesis%20and%20relighting%20results.%20Benefiting%20from%20the%20efficiency%20and%0Aextensibility%20of%20the%20TensoRF-based%20representation%2C%20our%20method%20can%20accurately%0Amodel%20secondary%20shading%20effects%20%28like%20shadows%20and%20indirect%20lighting%29%20and%0Agenerally%20support%20input%20images%20captured%20under%20single%20or%20multiple%20unknown%0Alighting%20conditions.%20The%20low-rank%20tensor%20representation%20allows%20us%20to%20not%20only%0Aachieve%20fast%20and%20compact%20reconstruction%20but%20also%20better%20exploit%20shared%0Ainformation%20under%20an%20arbitrary%20number%20of%20capturing%20lighting%20conditions.%20We%0Ademonstrate%20the%20superiority%20of%20our%20method%20to%20baseline%20methods%20qualitatively%20and%0Aquantitatively%20on%20various%20challenging%20synthetic%20and%20real-world%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.12461v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TensoIR%3A%20Tensorial%20Inverse%20Rendering&entry.906535625=Haian%20Jin%20and%20Isabella%20Liu%20and%20Peijia%20Xu%20and%20Xiaoshuai%20Zhang%20and%20Songfang%20Han%20and%20Sai%20Bi%20and%20Xiaowei%20Zhou%20and%20Zexiang%20Xu%20and%20Hao%20Su&entry.1292438233=%20%20We%20propose%20TensoIR%2C%20a%20novel%20inverse%20rendering%20approach%20based%20on%20tensor%0Afactorization%20and%20neural%20fields.%20Unlike%20previous%20works%20that%20use%20purely%0AMLP-based%20neural%20fields%2C%20thus%20suffering%20from%20low%20capacity%20and%20high%20computation%0Acosts%2C%20we%20extend%20TensoRF%2C%20a%20state-of-the-art%20approach%20for%20radiance%20field%0Amodeling%2C%20to%20estimate%20scene%20geometry%2C%20surface%20reflectance%2C%20and%20environment%0Aillumination%20from%20multi-view%20images%20captured%20under%20unknown%20lighting%20conditions.%0AOur%20approach%20jointly%20achieves%20radiance%20field%20reconstruction%20and%0Aphysically-based%20model%20estimation%2C%20leading%20to%20photo-realistic%20novel%20view%0Asynthesis%20and%20relighting%20results.%20Benefiting%20from%20the%20efficiency%20and%0Aextensibility%20of%20the%20TensoRF-based%20representation%2C%20our%20method%20can%20accurately%0Amodel%20secondary%20shading%20effects%20%28like%20shadows%20and%20indirect%20lighting%29%20and%0Agenerally%20support%20input%20images%20captured%20under%20single%20or%20multiple%20unknown%0Alighting%20conditions.%20The%20low-rank%20tensor%20representation%20allows%20us%20to%20not%20only%0Aachieve%20fast%20and%20compact%20reconstruction%20but%20also%20better%20exploit%20shared%0Ainformation%20under%20an%20arbitrary%20number%20of%20capturing%20lighting%20conditions.%20We%0Ademonstrate%20the%20superiority%20of%20our%20method%20to%20baseline%20methods%20qualitatively%20and%0Aquantitatively%20on%20various%20challenging%20synthetic%20and%20real-world%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.12461v2&entry.124074799=Read"},
{"title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with\n  Retrieval Augmented Multimodal LLM", "author": "Jielin Qiu and Andrea Madotto and Zhaojiang Lin and Paul A. Crook and Yifan Ethan Xu and Xin Luna Dong and Christos Faloutsos and Lei Li and Babak Damavandi and Seungwhan Moon", "abstract": "  Vision-extended LLMs have made significant strides in Visual Question\nAnswering (VQA). Despite these advancements, VLLMs still encounter substantial\ndifficulties in handling queries involving long-tail entities, with a tendency\nto produce erroneous or hallucinated responses. In this work, we introduce a\nnovel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for\nentity-centric VQA. This task aims to test the models' capabilities in\nidentifying entities and providing detailed, entity-specific knowledge. We have\ndeveloped the \\textbf{SnapNTell Dataset}, distinct from traditional VQA\ndatasets: (1) It encompasses a wide range of categorized entities, each\nrepresented by images and explicitly named in the answers; (2) It features QA\npairs that require extensive knowledge for accurate responses. The dataset is\norganized into 22 major categories, containing 7,568 unique entities in total.\nFor each entity, we curated 10 illustrative images and crafted 10\nknowledge-intensive QA pairs. To address this novel task, we devised a\nscalable, efficient, and transparent retrieval-augmented multimodal LLM. Our\napproach markedly outperforms existing methods on the SnapNTell dataset,\nachieving a 66.5\\% improvement in the BELURT score. We will soon make the\ndataset and the source code publicly accessible.\n", "link": "http://arxiv.org/abs/2403.04735v1", "date": "2024-03-07", "relevancy": 2.0815, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5424}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5352}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4925}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20SnapNTell%3A%20Enhancing%20Entity-Centric%20Visual%20Question%20Answering%20with%0A%20%20Retrieval%20Augmented%20Multimodal%20LLM&body=Title%3A%20SnapNTell%3A%20Enhancing%20Entity-Centric%20Visual%20Question%20Answering%20with%0A%20%20Retrieval%20Augmented%20Multimodal%20LLM%0AAuthor%3A%20Jielin%20Qiu%20and%20Andrea%20Madotto%20and%20Zhaojiang%20Lin%20and%20Paul%20A.%20Crook%20and%20Yifan%20Ethan%20Xu%20and%20Xin%20Luna%20Dong%20and%20Christos%20Faloutsos%20and%20Lei%20Li%20and%20Babak%20Damavandi%20and%20Seungwhan%20Moon%0AAbstract%3A%20%20%20Vision-extended%20LLMs%20have%20made%20significant%20strides%20in%20Visual%20Question%0AAnswering%20%28VQA%29.%20Despite%20these%20advancements%2C%20VLLMs%20still%20encounter%20substantial%0Adifficulties%20in%20handling%20queries%20involving%20long-tail%20entities%2C%20with%20a%20tendency%0Ato%20produce%20erroneous%20or%20hallucinated%20responses.%20In%20this%20work%2C%20we%20introduce%20a%0Anovel%20evaluative%20benchmark%20named%20%5Ctextbf%7BSnapNTell%7D%2C%20specifically%20tailored%20for%0Aentity-centric%20VQA.%20This%20task%20aims%20to%20test%20the%20models%27%20capabilities%20in%0Aidentifying%20entities%20and%20providing%20detailed%2C%20entity-specific%20knowledge.%20We%20have%0Adeveloped%20the%20%5Ctextbf%7BSnapNTell%20Dataset%7D%2C%20distinct%20from%20traditional%20VQA%0Adatasets%3A%20%281%29%20It%20encompasses%20a%20wide%20range%20of%20categorized%20entities%2C%20each%0Arepresented%20by%20images%20and%20explicitly%20named%20in%20the%20answers%3B%20%282%29%20It%20features%20QA%0Apairs%20that%20require%20extensive%20knowledge%20for%20accurate%20responses.%20The%20dataset%20is%0Aorganized%20into%2022%20major%20categories%2C%20containing%207%2C568%20unique%20entities%20in%20total.%0AFor%20each%20entity%2C%20we%20curated%2010%20illustrative%20images%20and%20crafted%2010%0Aknowledge-intensive%20QA%20pairs.%20To%20address%20this%20novel%20task%2C%20we%20devised%20a%0Ascalable%2C%20efficient%2C%20and%20transparent%20retrieval-augmented%20multimodal%20LLM.%20Our%0Aapproach%20markedly%20outperforms%20existing%20methods%20on%20the%20SnapNTell%20dataset%2C%0Aachieving%20a%2066.5%5C%25%20improvement%20in%20the%20BELURT%20score.%20We%20will%20soon%20make%20the%0Adataset%20and%20the%20source%20code%20publicly%20accessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04735v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SnapNTell%3A%20Enhancing%20Entity-Centric%20Visual%20Question%20Answering%20with%0A%20%20Retrieval%20Augmented%20Multimodal%20LLM&entry.906535625=Jielin%20Qiu%20and%20Andrea%20Madotto%20and%20Zhaojiang%20Lin%20and%20Paul%20A.%20Crook%20and%20Yifan%20Ethan%20Xu%20and%20Xin%20Luna%20Dong%20and%20Christos%20Faloutsos%20and%20Lei%20Li%20and%20Babak%20Damavandi%20and%20Seungwhan%20Moon&entry.1292438233=%20%20Vision-extended%20LLMs%20have%20made%20significant%20strides%20in%20Visual%20Question%0AAnswering%20%28VQA%29.%20Despite%20these%20advancements%2C%20VLLMs%20still%20encounter%20substantial%0Adifficulties%20in%20handling%20queries%20involving%20long-tail%20entities%2C%20with%20a%20tendency%0Ato%20produce%20erroneous%20or%20hallucinated%20responses.%20In%20this%20work%2C%20we%20introduce%20a%0Anovel%20evaluative%20benchmark%20named%20%5Ctextbf%7BSnapNTell%7D%2C%20specifically%20tailored%20for%0Aentity-centric%20VQA.%20This%20task%20aims%20to%20test%20the%20models%27%20capabilities%20in%0Aidentifying%20entities%20and%20providing%20detailed%2C%20entity-specific%20knowledge.%20We%20have%0Adeveloped%20the%20%5Ctextbf%7BSnapNTell%20Dataset%7D%2C%20distinct%20from%20traditional%20VQA%0Adatasets%3A%20%281%29%20It%20encompasses%20a%20wide%20range%20of%20categorized%20entities%2C%20each%0Arepresented%20by%20images%20and%20explicitly%20named%20in%20the%20answers%3B%20%282%29%20It%20features%20QA%0Apairs%20that%20require%20extensive%20knowledge%20for%20accurate%20responses.%20The%20dataset%20is%0Aorganized%20into%2022%20major%20categories%2C%20containing%207%2C568%20unique%20entities%20in%20total.%0AFor%20each%20entity%2C%20we%20curated%2010%20illustrative%20images%20and%20crafted%2010%0Aknowledge-intensive%20QA%20pairs.%20To%20address%20this%20novel%20task%2C%20we%20devised%20a%0Ascalable%2C%20efficient%2C%20and%20transparent%20retrieval-augmented%20multimodal%20LLM.%20Our%0Aapproach%20markedly%20outperforms%20existing%20methods%20on%20the%20SnapNTell%20dataset%2C%0Aachieving%20a%2066.5%5C%25%20improvement%20in%20the%20BELURT%20score.%20We%20will%20soon%20make%20the%0Adataset%20and%20the%20source%20code%20publicly%20accessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04735v1&entry.124074799=Read"},
{"title": "LORS: Low-rank Residual Structure for Parameter-Efficient Network\n  Stacking", "author": "Jialin Li and Qiang Nie and Weifu Fu and Yuhuan Lin and Guangpin Tao and Yong Liu and Chengjie Wang", "abstract": "  Deep learning models, particularly those based on transformers, often employ\nnumerous stacked structures, which possess identical architectures and perform\nsimilar functions. While effective, this stacking paradigm leads to a\nsubstantial increase in the number of parameters, posing challenges for\npractical applications. In today's landscape of increasingly large models,\nstacking depth can even reach dozens, further exacerbating this issue. To\nmitigate this problem, we introduce LORS (LOw-rank Residual Structure). LORS\nallows stacked modules to share the majority of parameters, requiring a much\nsmaller number of unique ones per module to match or even surpass the\nperformance of using entirely distinct ones, thereby significantly reducing\nparameter usage. We validate our method by applying it to the stacked decoders\nof a query-based object detector, and conduct extensive experiments on the\nwidely used MS COCO dataset. Experimental results demonstrate the effectiveness\nof our method, as even with a 70\\% reduction in the parameters of the decoder,\nour method still enables the model to achieve comparable or\n", "link": "http://arxiv.org/abs/2403.04303v1", "date": "2024-03-07", "relevancy": 2.0812, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5447}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5066}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5013}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20LORS%3A%20Low-rank%20Residual%20Structure%20for%20Parameter-Efficient%20Network%0A%20%20Stacking&body=Title%3A%20LORS%3A%20Low-rank%20Residual%20Structure%20for%20Parameter-Efficient%20Network%0A%20%20Stacking%0AAuthor%3A%20Jialin%20Li%20and%20Qiang%20Nie%20and%20Weifu%20Fu%20and%20Yuhuan%20Lin%20and%20Guangpin%20Tao%20and%20Yong%20Liu%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Deep%20learning%20models%2C%20particularly%20those%20based%20on%20transformers%2C%20often%20employ%0Anumerous%20stacked%20structures%2C%20which%20possess%20identical%20architectures%20and%20perform%0Asimilar%20functions.%20While%20effective%2C%20this%20stacking%20paradigm%20leads%20to%20a%0Asubstantial%20increase%20in%20the%20number%20of%20parameters%2C%20posing%20challenges%20for%0Apractical%20applications.%20In%20today%27s%20landscape%20of%20increasingly%20large%20models%2C%0Astacking%20depth%20can%20even%20reach%20dozens%2C%20further%20exacerbating%20this%20issue.%20To%0Amitigate%20this%20problem%2C%20we%20introduce%20LORS%20%28LOw-rank%20Residual%20Structure%29.%20LORS%0Aallows%20stacked%20modules%20to%20share%20the%20majority%20of%20parameters%2C%20requiring%20a%20much%0Asmaller%20number%20of%20unique%20ones%20per%20module%20to%20match%20or%20even%20surpass%20the%0Aperformance%20of%20using%20entirely%20distinct%20ones%2C%20thereby%20significantly%20reducing%0Aparameter%20usage.%20We%20validate%20our%20method%20by%20applying%20it%20to%20the%20stacked%20decoders%0Aof%20a%20query-based%20object%20detector%2C%20and%20conduct%20extensive%20experiments%20on%20the%0Awidely%20used%20MS%20COCO%20dataset.%20Experimental%20results%20demonstrate%20the%20effectiveness%0Aof%20our%20method%2C%20as%20even%20with%20a%2070%5C%25%20reduction%20in%20the%20parameters%20of%20the%20decoder%2C%0Aour%20method%20still%20enables%20the%20model%20to%20achieve%20comparable%20or%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04303v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LORS%3A%20Low-rank%20Residual%20Structure%20for%20Parameter-Efficient%20Network%0A%20%20Stacking&entry.906535625=Jialin%20Li%20and%20Qiang%20Nie%20and%20Weifu%20Fu%20and%20Yuhuan%20Lin%20and%20Guangpin%20Tao%20and%20Yong%20Liu%20and%20Chengjie%20Wang&entry.1292438233=%20%20Deep%20learning%20models%2C%20particularly%20those%20based%20on%20transformers%2C%20often%20employ%0Anumerous%20stacked%20structures%2C%20which%20possess%20identical%20architectures%20and%20perform%0Asimilar%20functions.%20While%20effective%2C%20this%20stacking%20paradigm%20leads%20to%20a%0Asubstantial%20increase%20in%20the%20number%20of%20parameters%2C%20posing%20challenges%20for%0Apractical%20applications.%20In%20today%27s%20landscape%20of%20increasingly%20large%20models%2C%0Astacking%20depth%20can%20even%20reach%20dozens%2C%20further%20exacerbating%20this%20issue.%20To%0Amitigate%20this%20problem%2C%20we%20introduce%20LORS%20%28LOw-rank%20Residual%20Structure%29.%20LORS%0Aallows%20stacked%20modules%20to%20share%20the%20majority%20of%20parameters%2C%20requiring%20a%20much%0Asmaller%20number%20of%20unique%20ones%20per%20module%20to%20match%20or%20even%20surpass%20the%0Aperformance%20of%20using%20entirely%20distinct%20ones%2C%20thereby%20significantly%20reducing%0Aparameter%20usage.%20We%20validate%20our%20method%20by%20applying%20it%20to%20the%20stacked%20decoders%0Aof%20a%20query-based%20object%20detector%2C%20and%20conduct%20extensive%20experiments%20on%20the%0Awidely%20used%20MS%20COCO%20dataset.%20Experimental%20results%20demonstrate%20the%20effectiveness%0Aof%20our%20method%2C%20as%20even%20with%20a%2070%5C%25%20reduction%20in%20the%20parameters%20of%20the%20decoder%2C%0Aour%20method%20still%20enables%20the%20model%20to%20achieve%20comparable%20or%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04303v1&entry.124074799=Read"},
{"title": "Each Test Image Deserves A Specific Prompt: Continual Test-Time\n  Adaptation for 2D Medical Image Segmentation", "author": "Ziyang Chen and Yiwen Ye and Mengkang Lu and Yongsheng Pan and Yong Xia", "abstract": "  Distribution shift widely exists in medical images acquired from different\nmedical centres and poses a significant obstacle to deploying the pre-trained\nsemantic segmentation model in real-world applications. Test-time adaptation\nhas proven its effectiveness in tackling the cross-domain distribution shift\nduring inference. However, most existing methods achieve adaptation by updating\nthe pre-trained models, rendering them susceptible to error accumulation and\ncatastrophic forgetting when encountering a series of distribution shifts\n(i.e., under the continual test-time adaptation setup). To overcome these\nchallenges caused by updating the models, in this paper, we freeze the\npre-trained model and propose the Visual Prompt-based Test-Time Adaptation\n(VPTTA) method to train a specific prompt for each test image to align the\nstatistics in the batch normalization layers. Specifically, we present the\nlow-frequency prompt, which is lightweight with only a few parameters and can\nbe effectively trained in a single iteration. To enhance prompt initialization,\nwe equip VPTTA with a memory bank to benefit the current prompt from previous\nones. Additionally, we design a warm-up mechanism, which mixes source and\ntarget statistics to construct warm-up statistics, thereby facilitating the\ntraining process. Extensive experiments demonstrate the superiority of our\nVPTTA over other state-of-the-art methods on two medical image segmentation\nbenchmark tasks. The code and weights of pre-trained source models are\navailable at https://github.com/Chen-Ziyang/VPTTA.\n", "link": "http://arxiv.org/abs/2311.18363v2", "date": "2024-03-07", "relevancy": 2.076, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5075}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5007}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Each%20Test%20Image%20Deserves%20A%20Specific%20Prompt%3A%20Continual%20Test-Time%0A%20%20Adaptation%20for%202D%20Medical%20Image%20Segmentation&body=Title%3A%20Each%20Test%20Image%20Deserves%20A%20Specific%20Prompt%3A%20Continual%20Test-Time%0A%20%20Adaptation%20for%202D%20Medical%20Image%20Segmentation%0AAuthor%3A%20Ziyang%20Chen%20and%20Yiwen%20Ye%20and%20Mengkang%20Lu%20and%20Yongsheng%20Pan%20and%20Yong%20Xia%0AAbstract%3A%20%20%20Distribution%20shift%20widely%20exists%20in%20medical%20images%20acquired%20from%20different%0Amedical%20centres%20and%20poses%20a%20significant%20obstacle%20to%20deploying%20the%20pre-trained%0Asemantic%20segmentation%20model%20in%20real-world%20applications.%20Test-time%20adaptation%0Ahas%20proven%20its%20effectiveness%20in%20tackling%20the%20cross-domain%20distribution%20shift%0Aduring%20inference.%20However%2C%20most%20existing%20methods%20achieve%20adaptation%20by%20updating%0Athe%20pre-trained%20models%2C%20rendering%20them%20susceptible%20to%20error%20accumulation%20and%0Acatastrophic%20forgetting%20when%20encountering%20a%20series%20of%20distribution%20shifts%0A%28i.e.%2C%20under%20the%20continual%20test-time%20adaptation%20setup%29.%20To%20overcome%20these%0Achallenges%20caused%20by%20updating%20the%20models%2C%20in%20this%20paper%2C%20we%20freeze%20the%0Apre-trained%20model%20and%20propose%20the%20Visual%20Prompt-based%20Test-Time%20Adaptation%0A%28VPTTA%29%20method%20to%20train%20a%20specific%20prompt%20for%20each%20test%20image%20to%20align%20the%0Astatistics%20in%20the%20batch%20normalization%20layers.%20Specifically%2C%20we%20present%20the%0Alow-frequency%20prompt%2C%20which%20is%20lightweight%20with%20only%20a%20few%20parameters%20and%20can%0Abe%20effectively%20trained%20in%20a%20single%20iteration.%20To%20enhance%20prompt%20initialization%2C%0Awe%20equip%20VPTTA%20with%20a%20memory%20bank%20to%20benefit%20the%20current%20prompt%20from%20previous%0Aones.%20Additionally%2C%20we%20design%20a%20warm-up%20mechanism%2C%20which%20mixes%20source%20and%0Atarget%20statistics%20to%20construct%20warm-up%20statistics%2C%20thereby%20facilitating%20the%0Atraining%20process.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%0AVPTTA%20over%20other%20state-of-the-art%20methods%20on%20two%20medical%20image%20segmentation%0Abenchmark%20tasks.%20The%20code%20and%20weights%20of%20pre-trained%20source%20models%20are%0Aavailable%20at%20https%3A//github.com/Chen-Ziyang/VPTTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18363v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Each%20Test%20Image%20Deserves%20A%20Specific%20Prompt%3A%20Continual%20Test-Time%0A%20%20Adaptation%20for%202D%20Medical%20Image%20Segmentation&entry.906535625=Ziyang%20Chen%20and%20Yiwen%20Ye%20and%20Mengkang%20Lu%20and%20Yongsheng%20Pan%20and%20Yong%20Xia&entry.1292438233=%20%20Distribution%20shift%20widely%20exists%20in%20medical%20images%20acquired%20from%20different%0Amedical%20centres%20and%20poses%20a%20significant%20obstacle%20to%20deploying%20the%20pre-trained%0Asemantic%20segmentation%20model%20in%20real-world%20applications.%20Test-time%20adaptation%0Ahas%20proven%20its%20effectiveness%20in%20tackling%20the%20cross-domain%20distribution%20shift%0Aduring%20inference.%20However%2C%20most%20existing%20methods%20achieve%20adaptation%20by%20updating%0Athe%20pre-trained%20models%2C%20rendering%20them%20susceptible%20to%20error%20accumulation%20and%0Acatastrophic%20forgetting%20when%20encountering%20a%20series%20of%20distribution%20shifts%0A%28i.e.%2C%20under%20the%20continual%20test-time%20adaptation%20setup%29.%20To%20overcome%20these%0Achallenges%20caused%20by%20updating%20the%20models%2C%20in%20this%20paper%2C%20we%20freeze%20the%0Apre-trained%20model%20and%20propose%20the%20Visual%20Prompt-based%20Test-Time%20Adaptation%0A%28VPTTA%29%20method%20to%20train%20a%20specific%20prompt%20for%20each%20test%20image%20to%20align%20the%0Astatistics%20in%20the%20batch%20normalization%20layers.%20Specifically%2C%20we%20present%20the%0Alow-frequency%20prompt%2C%20which%20is%20lightweight%20with%20only%20a%20few%20parameters%20and%20can%0Abe%20effectively%20trained%20in%20a%20single%20iteration.%20To%20enhance%20prompt%20initialization%2C%0Awe%20equip%20VPTTA%20with%20a%20memory%20bank%20to%20benefit%20the%20current%20prompt%20from%20previous%0Aones.%20Additionally%2C%20we%20design%20a%20warm-up%20mechanism%2C%20which%20mixes%20source%20and%0Atarget%20statistics%20to%20construct%20warm-up%20statistics%2C%20thereby%20facilitating%20the%0Atraining%20process.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%0AVPTTA%20over%20other%20state-of-the-art%20methods%20on%20two%20medical%20image%20segmentation%0Abenchmark%20tasks.%20The%20code%20and%20weights%20of%20pre-trained%20source%20models%20are%0Aavailable%20at%20https%3A//github.com/Chen-Ziyang/VPTTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18363v2&entry.124074799=Read"},
{"title": "In-n-Out: Calibrating Graph Neural Networks for Link Prediction", "author": "Erik Nascimento and Diego Mesquita and Samuel Kaskio and Amauri H Souza", "abstract": "  Deep neural networks are notoriously miscalibrated, i.e., their outputs do\nnot reflect the true probability of the event we aim to predict. While networks\nfor tabular or image data are usually overconfident, recent works have shown\nthat graph neural networks (GNNs) show the opposite behavior for node-level\nclassification. But what happens when we are predicting links? We show that, in\nthis case, GNNs often exhibit a mixed behavior. More specifically, they may be\noverconfident in negative predictions while being underconfident in positive\nones. Based on this observation, we propose IN-N-OUT, the first-ever method to\ncalibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions:\ni) attributing true/false labels to an edge while respecting a GNNs prediction\nshould cause but small fluctuations in that edge's embedding; and, conversely,\nii) if we label that same edge contradicting our GNN, embeddings should change\nmore substantially. An extensive experimental campaign shows that IN-N-OUT\nsignificantly improves the calibration of GNNs in link prediction, consistently\noutperforming the baselines available -- which are not designed for this\nspecific task.\n", "link": "http://arxiv.org/abs/2403.04605v1", "date": "2024-03-07", "relevancy": 2.0722, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5705}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4736}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20In-n-Out%3A%20Calibrating%20Graph%20Neural%20Networks%20for%20Link%20Prediction&body=Title%3A%20In-n-Out%3A%20Calibrating%20Graph%20Neural%20Networks%20for%20Link%20Prediction%0AAuthor%3A%20Erik%20Nascimento%20and%20Diego%20Mesquita%20and%20Samuel%20Kaskio%20and%20Amauri%20H%20Souza%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20notoriously%20miscalibrated%2C%20i.e.%2C%20their%20outputs%20do%0Anot%20reflect%20the%20true%20probability%20of%20the%20event%20we%20aim%20to%20predict.%20While%20networks%0Afor%20tabular%20or%20image%20data%20are%20usually%20overconfident%2C%20recent%20works%20have%20shown%0Athat%20graph%20neural%20networks%20%28GNNs%29%20show%20the%20opposite%20behavior%20for%20node-level%0Aclassification.%20But%20what%20happens%20when%20we%20are%20predicting%20links%3F%20We%20show%20that%2C%20in%0Athis%20case%2C%20GNNs%20often%20exhibit%20a%20mixed%20behavior.%20More%20specifically%2C%20they%20may%20be%0Aoverconfident%20in%20negative%20predictions%20while%20being%20underconfident%20in%20positive%0Aones.%20Based%20on%20this%20observation%2C%20we%20propose%20IN-N-OUT%2C%20the%20first-ever%20method%20to%0Acalibrate%20GNNs%20for%20link%20prediction.%20IN-N-OUT%20is%20based%20on%20two%20simple%20intuitions%3A%0Ai%29%20attributing%20true/false%20labels%20to%20an%20edge%20while%20respecting%20a%20GNNs%20prediction%0Ashould%20cause%20but%20small%20fluctuations%20in%20that%20edge%27s%20embedding%3B%20and%2C%20conversely%2C%0Aii%29%20if%20we%20label%20that%20same%20edge%20contradicting%20our%20GNN%2C%20embeddings%20should%20change%0Amore%20substantially.%20An%20extensive%20experimental%20campaign%20shows%20that%20IN-N-OUT%0Asignificantly%20improves%20the%20calibration%20of%20GNNs%20in%20link%20prediction%2C%20consistently%0Aoutperforming%20the%20baselines%20available%20--%20which%20are%20not%20designed%20for%20this%0Aspecific%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04605v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-n-Out%3A%20Calibrating%20Graph%20Neural%20Networks%20for%20Link%20Prediction&entry.906535625=Erik%20Nascimento%20and%20Diego%20Mesquita%20and%20Samuel%20Kaskio%20and%20Amauri%20H%20Souza&entry.1292438233=%20%20Deep%20neural%20networks%20are%20notoriously%20miscalibrated%2C%20i.e.%2C%20their%20outputs%20do%0Anot%20reflect%20the%20true%20probability%20of%20the%20event%20we%20aim%20to%20predict.%20While%20networks%0Afor%20tabular%20or%20image%20data%20are%20usually%20overconfident%2C%20recent%20works%20have%20shown%0Athat%20graph%20neural%20networks%20%28GNNs%29%20show%20the%20opposite%20behavior%20for%20node-level%0Aclassification.%20But%20what%20happens%20when%20we%20are%20predicting%20links%3F%20We%20show%20that%2C%20in%0Athis%20case%2C%20GNNs%20often%20exhibit%20a%20mixed%20behavior.%20More%20specifically%2C%20they%20may%20be%0Aoverconfident%20in%20negative%20predictions%20while%20being%20underconfident%20in%20positive%0Aones.%20Based%20on%20this%20observation%2C%20we%20propose%20IN-N-OUT%2C%20the%20first-ever%20method%20to%0Acalibrate%20GNNs%20for%20link%20prediction.%20IN-N-OUT%20is%20based%20on%20two%20simple%20intuitions%3A%0Ai%29%20attributing%20true/false%20labels%20to%20an%20edge%20while%20respecting%20a%20GNNs%20prediction%0Ashould%20cause%20but%20small%20fluctuations%20in%20that%20edge%27s%20embedding%3B%20and%2C%20conversely%2C%0Aii%29%20if%20we%20label%20that%20same%20edge%20contradicting%20our%20GNN%2C%20embeddings%20should%20change%0Amore%20substantially.%20An%20extensive%20experimental%20campaign%20shows%20that%20IN-N-OUT%0Asignificantly%20improves%20the%20calibration%20of%20GNNs%20in%20link%20prediction%2C%20consistently%0Aoutperforming%20the%20baselines%20available%20--%20which%20are%20not%20designed%20for%20this%0Aspecific%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04605v1&entry.124074799=Read"},
{"title": "RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit\n  Neural Representations", "author": "Jiajun He and Gergely Flamich and Zongyu Guo and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  COMpression with Bayesian Implicit NEural Representations (COMBINER) is a\nrecent data compression method that addresses a key inefficiency of previous\nImplicit Neural Representation (INR)-based approaches: it avoids quantization\nand enables direct optimization of the rate-distortion performance. However,\nCOMBINER still has significant limitations: 1) it uses factorized priors and\nposterior approximations that lack flexibility; 2) it cannot effectively adapt\nto local deviations from global patterns in the data; and 3) its performance\ncan be susceptible to modeling choices and the variational parameters'\ninitializations. Our proposed method, Robust and Enhanced COMBINER\n(RECOMBINER), addresses these issues by 1) enriching the variational\napproximation while retaining a low computational cost via a linear\nreparameterization of the INR weights, 2) augmenting our INRs with learnable\npositional encodings that enable them to adapt to local details and 3)\nsplitting high-resolution data into patches to increase robustness and\nutilizing expressive hierarchical priors to capture dependency across patches.\nWe conduct extensive experiments across several data modalities, showcasing\nthat RECOMBINER achieves competitive results with the best INR-based methods\nand even outperforms autoencoder-based codecs on low-resolution images at low\nbitrates. Our PyTorch implementation is available at\nhttps://github.com/cambridge-mlg/RECOMBINER/.\n", "link": "http://arxiv.org/abs/2309.17182v2", "date": "2024-03-07", "relevancy": 2.0686, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5487}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4954}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4943}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20RECOMBINER%3A%20Robust%20and%20Enhanced%20Compression%20with%20Bayesian%20Implicit%0A%20%20Neural%20Representations&body=Title%3A%20RECOMBINER%3A%20Robust%20and%20Enhanced%20Compression%20with%20Bayesian%20Implicit%0A%20%20Neural%20Representations%0AAuthor%3A%20Jiajun%20He%20and%20Gergely%20Flamich%20and%20Zongyu%20Guo%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20COMpression%20with%20Bayesian%20Implicit%20NEural%20Representations%20%28COMBINER%29%20is%20a%0Arecent%20data%20compression%20method%20that%20addresses%20a%20key%20inefficiency%20of%20previous%0AImplicit%20Neural%20Representation%20%28INR%29-based%20approaches%3A%20it%20avoids%20quantization%0Aand%20enables%20direct%20optimization%20of%20the%20rate-distortion%20performance.%20However%2C%0ACOMBINER%20still%20has%20significant%20limitations%3A%201%29%20it%20uses%20factorized%20priors%20and%0Aposterior%20approximations%20that%20lack%20flexibility%3B%202%29%20it%20cannot%20effectively%20adapt%0Ato%20local%20deviations%20from%20global%20patterns%20in%20the%20data%3B%20and%203%29%20its%20performance%0Acan%20be%20susceptible%20to%20modeling%20choices%20and%20the%20variational%20parameters%27%0Ainitializations.%20Our%20proposed%20method%2C%20Robust%20and%20Enhanced%20COMBINER%0A%28RECOMBINER%29%2C%20addresses%20these%20issues%20by%201%29%20enriching%20the%20variational%0Aapproximation%20while%20retaining%20a%20low%20computational%20cost%20via%20a%20linear%0Areparameterization%20of%20the%20INR%20weights%2C%202%29%20augmenting%20our%20INRs%20with%20learnable%0Apositional%20encodings%20that%20enable%20them%20to%20adapt%20to%20local%20details%20and%203%29%0Asplitting%20high-resolution%20data%20into%20patches%20to%20increase%20robustness%20and%0Autilizing%20expressive%20hierarchical%20priors%20to%20capture%20dependency%20across%20patches.%0AWe%20conduct%20extensive%20experiments%20across%20several%20data%20modalities%2C%20showcasing%0Athat%20RECOMBINER%20achieves%20competitive%20results%20with%20the%20best%20INR-based%20methods%0Aand%20even%20outperforms%20autoencoder-based%20codecs%20on%20low-resolution%20images%20at%20low%0Abitrates.%20Our%20PyTorch%20implementation%20is%20available%20at%0Ahttps%3A//github.com/cambridge-mlg/RECOMBINER/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17182v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RECOMBINER%3A%20Robust%20and%20Enhanced%20Compression%20with%20Bayesian%20Implicit%0A%20%20Neural%20Representations&entry.906535625=Jiajun%20He%20and%20Gergely%20Flamich%20and%20Zongyu%20Guo%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20COMpression%20with%20Bayesian%20Implicit%20NEural%20Representations%20%28COMBINER%29%20is%20a%0Arecent%20data%20compression%20method%20that%20addresses%20a%20key%20inefficiency%20of%20previous%0AImplicit%20Neural%20Representation%20%28INR%29-based%20approaches%3A%20it%20avoids%20quantization%0Aand%20enables%20direct%20optimization%20of%20the%20rate-distortion%20performance.%20However%2C%0ACOMBINER%20still%20has%20significant%20limitations%3A%201%29%20it%20uses%20factorized%20priors%20and%0Aposterior%20approximations%20that%20lack%20flexibility%3B%202%29%20it%20cannot%20effectively%20adapt%0Ato%20local%20deviations%20from%20global%20patterns%20in%20the%20data%3B%20and%203%29%20its%20performance%0Acan%20be%20susceptible%20to%20modeling%20choices%20and%20the%20variational%20parameters%27%0Ainitializations.%20Our%20proposed%20method%2C%20Robust%20and%20Enhanced%20COMBINER%0A%28RECOMBINER%29%2C%20addresses%20these%20issues%20by%201%29%20enriching%20the%20variational%0Aapproximation%20while%20retaining%20a%20low%20computational%20cost%20via%20a%20linear%0Areparameterization%20of%20the%20INR%20weights%2C%202%29%20augmenting%20our%20INRs%20with%20learnable%0Apositional%20encodings%20that%20enable%20them%20to%20adapt%20to%20local%20details%20and%203%29%0Asplitting%20high-resolution%20data%20into%20patches%20to%20increase%20robustness%20and%0Autilizing%20expressive%20hierarchical%20priors%20to%20capture%20dependency%20across%20patches.%0AWe%20conduct%20extensive%20experiments%20across%20several%20data%20modalities%2C%20showcasing%0Athat%20RECOMBINER%20achieves%20competitive%20results%20with%20the%20best%20INR-based%20methods%0Aand%20even%20outperforms%20autoencoder-based%20codecs%20on%20low-resolution%20images%20at%20low%0Abitrates.%20Our%20PyTorch%20implementation%20is%20available%20at%0Ahttps%3A//github.com/cambridge-mlg/RECOMBINER/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17182v2&entry.124074799=Read"},
{"title": "GraphInstruct: Empowering Large Language Models with Graph Understanding\n  and Reasoning Capability", "author": "Zihan Luo and Xiran Song and Hong Huang and Jianxun Lian and Chenhao Zhang and Jinqi Jiang and Xing Xie and Hai Jin", "abstract": "  Evaluating and enhancing the general capabilities of large language models\n(LLMs) has been an important research topic. Graph is a common data structure\nin the real world, and understanding graph data is a crucial part for advancing\ngeneral intelligence. To evaluate and enhance the graph understanding abilities\nof LLMs, in this paper, we propose a benchmark named GraphInstruct, which\ncomprehensively includes 21 classical graph reasoning tasks, providing diverse\ngraph generation pipelines and detailed reasoning steps. Based on\nGraphInstruct, we further construct GraphLM through efficient\ninstruction-tuning, which shows prominent graph understanding capability. In\norder to enhance the LLM with graph reasoning capability as well, we propose a\nstep mask training strategy, and construct a model named GraphLM+. As one of\nthe pioneering efforts to enhance the graph understanding and reasoning\nabilities of LLMs, extensive experiments have demonstrated the superiority of\nGraphLM and GraphLM+ over other LLMs. We look forward to more researchers\nexploring the potential of LLMs in the graph data mining domain through\nGraphInstruct. Our code for generating GraphInstruct is released publicly at:\nhttps://github.com/CGCL-codes/GraphInstruct.\n", "link": "http://arxiv.org/abs/2403.04483v1", "date": "2024-03-07", "relevancy": 2.0682, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5521}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5296}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.477}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20GraphInstruct%3A%20Empowering%20Large%20Language%20Models%20with%20Graph%20Understanding%0A%20%20and%20Reasoning%20Capability&body=Title%3A%20GraphInstruct%3A%20Empowering%20Large%20Language%20Models%20with%20Graph%20Understanding%0A%20%20and%20Reasoning%20Capability%0AAuthor%3A%20Zihan%20Luo%20and%20Xiran%20Song%20and%20Hong%20Huang%20and%20Jianxun%20Lian%20and%20Chenhao%20Zhang%20and%20Jinqi%20Jiang%20and%20Xing%20Xie%20and%20Hai%20Jin%0AAbstract%3A%20%20%20Evaluating%20and%20enhancing%20the%20general%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20has%20been%20an%20important%20research%20topic.%20Graph%20is%20a%20common%20data%20structure%0Ain%20the%20real%20world%2C%20and%20understanding%20graph%20data%20is%20a%20crucial%20part%20for%20advancing%0Ageneral%20intelligence.%20To%20evaluate%20and%20enhance%20the%20graph%20understanding%20abilities%0Aof%20LLMs%2C%20in%20this%20paper%2C%20we%20propose%20a%20benchmark%20named%20GraphInstruct%2C%20which%0Acomprehensively%20includes%2021%20classical%20graph%20reasoning%20tasks%2C%20providing%20diverse%0Agraph%20generation%20pipelines%20and%20detailed%20reasoning%20steps.%20Based%20on%0AGraphInstruct%2C%20we%20further%20construct%20GraphLM%20through%20efficient%0Ainstruction-tuning%2C%20which%20shows%20prominent%20graph%20understanding%20capability.%20In%0Aorder%20to%20enhance%20the%20LLM%20with%20graph%20reasoning%20capability%20as%20well%2C%20we%20propose%20a%0Astep%20mask%20training%20strategy%2C%20and%20construct%20a%20model%20named%20GraphLM%2B.%20As%20one%20of%0Athe%20pioneering%20efforts%20to%20enhance%20the%20graph%20understanding%20and%20reasoning%0Aabilities%20of%20LLMs%2C%20extensive%20experiments%20have%20demonstrated%20the%20superiority%20of%0AGraphLM%20and%20GraphLM%2B%20over%20other%20LLMs.%20We%20look%20forward%20to%20more%20researchers%0Aexploring%20the%20potential%20of%20LLMs%20in%20the%20graph%20data%20mining%20domain%20through%0AGraphInstruct.%20Our%20code%20for%20generating%20GraphInstruct%20is%20released%20publicly%20at%3A%0Ahttps%3A//github.com/CGCL-codes/GraphInstruct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04483v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphInstruct%3A%20Empowering%20Large%20Language%20Models%20with%20Graph%20Understanding%0A%20%20and%20Reasoning%20Capability&entry.906535625=Zihan%20Luo%20and%20Xiran%20Song%20and%20Hong%20Huang%20and%20Jianxun%20Lian%20and%20Chenhao%20Zhang%20and%20Jinqi%20Jiang%20and%20Xing%20Xie%20and%20Hai%20Jin&entry.1292438233=%20%20Evaluating%20and%20enhancing%20the%20general%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20has%20been%20an%20important%20research%20topic.%20Graph%20is%20a%20common%20data%20structure%0Ain%20the%20real%20world%2C%20and%20understanding%20graph%20data%20is%20a%20crucial%20part%20for%20advancing%0Ageneral%20intelligence.%20To%20evaluate%20and%20enhance%20the%20graph%20understanding%20abilities%0Aof%20LLMs%2C%20in%20this%20paper%2C%20we%20propose%20a%20benchmark%20named%20GraphInstruct%2C%20which%0Acomprehensively%20includes%2021%20classical%20graph%20reasoning%20tasks%2C%20providing%20diverse%0Agraph%20generation%20pipelines%20and%20detailed%20reasoning%20steps.%20Based%20on%0AGraphInstruct%2C%20we%20further%20construct%20GraphLM%20through%20efficient%0Ainstruction-tuning%2C%20which%20shows%20prominent%20graph%20understanding%20capability.%20In%0Aorder%20to%20enhance%20the%20LLM%20with%20graph%20reasoning%20capability%20as%20well%2C%20we%20propose%20a%0Astep%20mask%20training%20strategy%2C%20and%20construct%20a%20model%20named%20GraphLM%2B.%20As%20one%20of%0Athe%20pioneering%20efforts%20to%20enhance%20the%20graph%20understanding%20and%20reasoning%0Aabilities%20of%20LLMs%2C%20extensive%20experiments%20have%20demonstrated%20the%20superiority%20of%0AGraphLM%20and%20GraphLM%2B%20over%20other%20LLMs.%20We%20look%20forward%20to%20more%20researchers%0Aexploring%20the%20potential%20of%20LLMs%20in%20the%20graph%20data%20mining%20domain%20through%0AGraphInstruct.%20Our%20code%20for%20generating%20GraphInstruct%20is%20released%20publicly%20at%3A%0Ahttps%3A//github.com/CGCL-codes/GraphInstruct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04483v1&entry.124074799=Read"},
{"title": "A General Calibrated Regret Metric for Detecting and Mitigating\n  Human-Robot Interaction Failures", "author": "Kensuke Nakamura and Ran Tian and Andrea Bajcsy", "abstract": "  Robot decision-making increasingly relies on expressive data-driven human\nprediction models when operating around people. While these models are known to\nsuffer from prediction errors in out-of-distribution interactions, not all\nprediction errors equally impact downstream robot performance. We identify that\nthe mathematical notion of regret precisely characterizes the degree to which\nincorrect predictions of future interaction outcomes degraded closed-loop robot\nperformance. However, canonical regret measures are poorly calibrated across\ndiverse deployment interactions. We extend the canonical notion of regret by\nderiving a calibrated regret metric that generalizes from absolute reward space\nto probability space. With this transformation, our metric removes the need for\nexplicit reward functions to calculate the robot's regret, enables fairer\ncomparison of interaction anomalies across disparate deployment contexts, and\nfacilitates targetted dataset construction of \"system-level\" prediction\nfailures. We experimentally quantify the value of this high-regret interaction\ndata for aiding the robot in improving its downstream decision-making. In a\nsuite of closed-loop autonomous driving simulations, we find that fine-tuning\nego-conditioned behavior predictors exclusively on high-regret human-robot\ninteraction data can improve the robot's overall re-deployment performance with\nsignificantly (77%) less data.\n", "link": "http://arxiv.org/abs/2403.04745v1", "date": "2024-03-07", "relevancy": 2.0672, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.577}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.509}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5005}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20General%20Calibrated%20Regret%20Metric%20for%20Detecting%20and%20Mitigating%0A%20%20Human-Robot%20Interaction%20Failures&body=Title%3A%20A%20General%20Calibrated%20Regret%20Metric%20for%20Detecting%20and%20Mitigating%0A%20%20Human-Robot%20Interaction%20Failures%0AAuthor%3A%20Kensuke%20Nakamura%20and%20Ran%20Tian%20and%20Andrea%20Bajcsy%0AAbstract%3A%20%20%20Robot%20decision-making%20increasingly%20relies%20on%20expressive%20data-driven%20human%0Aprediction%20models%20when%20operating%20around%20people.%20While%20these%20models%20are%20known%20to%0Asuffer%20from%20prediction%20errors%20in%20out-of-distribution%20interactions%2C%20not%20all%0Aprediction%20errors%20equally%20impact%20downstream%20robot%20performance.%20We%20identify%20that%0Athe%20mathematical%20notion%20of%20regret%20precisely%20characterizes%20the%20degree%20to%20which%0Aincorrect%20predictions%20of%20future%20interaction%20outcomes%20degraded%20closed-loop%20robot%0Aperformance.%20However%2C%20canonical%20regret%20measures%20are%20poorly%20calibrated%20across%0Adiverse%20deployment%20interactions.%20We%20extend%20the%20canonical%20notion%20of%20regret%20by%0Aderiving%20a%20calibrated%20regret%20metric%20that%20generalizes%20from%20absolute%20reward%20space%0Ato%20probability%20space.%20With%20this%20transformation%2C%20our%20metric%20removes%20the%20need%20for%0Aexplicit%20reward%20functions%20to%20calculate%20the%20robot%27s%20regret%2C%20enables%20fairer%0Acomparison%20of%20interaction%20anomalies%20across%20disparate%20deployment%20contexts%2C%20and%0Afacilitates%20targetted%20dataset%20construction%20of%20%22system-level%22%20prediction%0Afailures.%20We%20experimentally%20quantify%20the%20value%20of%20this%20high-regret%20interaction%0Adata%20for%20aiding%20the%20robot%20in%20improving%20its%20downstream%20decision-making.%20In%20a%0Asuite%20of%20closed-loop%20autonomous%20driving%20simulations%2C%20we%20find%20that%20fine-tuning%0Aego-conditioned%20behavior%20predictors%20exclusively%20on%20high-regret%20human-robot%0Ainteraction%20data%20can%20improve%20the%20robot%27s%20overall%20re-deployment%20performance%20with%0Asignificantly%20%2877%25%29%20less%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04745v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Calibrated%20Regret%20Metric%20for%20Detecting%20and%20Mitigating%0A%20%20Human-Robot%20Interaction%20Failures&entry.906535625=Kensuke%20Nakamura%20and%20Ran%20Tian%20and%20Andrea%20Bajcsy&entry.1292438233=%20%20Robot%20decision-making%20increasingly%20relies%20on%20expressive%20data-driven%20human%0Aprediction%20models%20when%20operating%20around%20people.%20While%20these%20models%20are%20known%20to%0Asuffer%20from%20prediction%20errors%20in%20out-of-distribution%20interactions%2C%20not%20all%0Aprediction%20errors%20equally%20impact%20downstream%20robot%20performance.%20We%20identify%20that%0Athe%20mathematical%20notion%20of%20regret%20precisely%20characterizes%20the%20degree%20to%20which%0Aincorrect%20predictions%20of%20future%20interaction%20outcomes%20degraded%20closed-loop%20robot%0Aperformance.%20However%2C%20canonical%20regret%20measures%20are%20poorly%20calibrated%20across%0Adiverse%20deployment%20interactions.%20We%20extend%20the%20canonical%20notion%20of%20regret%20by%0Aderiving%20a%20calibrated%20regret%20metric%20that%20generalizes%20from%20absolute%20reward%20space%0Ato%20probability%20space.%20With%20this%20transformation%2C%20our%20metric%20removes%20the%20need%20for%0Aexplicit%20reward%20functions%20to%20calculate%20the%20robot%27s%20regret%2C%20enables%20fairer%0Acomparison%20of%20interaction%20anomalies%20across%20disparate%20deployment%20contexts%2C%20and%0Afacilitates%20targetted%20dataset%20construction%20of%20%22system-level%22%20prediction%0Afailures.%20We%20experimentally%20quantify%20the%20value%20of%20this%20high-regret%20interaction%0Adata%20for%20aiding%20the%20robot%20in%20improving%20its%20downstream%20decision-making.%20In%20a%0Asuite%20of%20closed-loop%20autonomous%20driving%20simulations%2C%20we%20find%20that%20fine-tuning%0Aego-conditioned%20behavior%20predictors%20exclusively%20on%20high-regret%20human-robot%0Ainteraction%20data%20can%20improve%20the%20robot%27s%20overall%20re-deployment%20performance%20with%0Asignificantly%20%2877%25%29%20less%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04745v1&entry.124074799=Read"},
{"title": "Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition\n  Learning", "author": "Jiahao Ji and Jingyuan Wang and Yu Mou and Cheng Long", "abstract": "  Spatio-temporal (ST) prediction is an important and widely used technique in\ndata mining and analytics, especially for ST data in urban systems such as\ntransportation data. In practice, the ST data generation is usually influenced\nby various latent factors tied to natural phenomena or human socioeconomic\nactivities, impacting specific spatial areas selectively. However, existing ST\nprediction methods usually do not refine the impacts of different factors, but\ndirectly model the entangled impacts of multiple factors. This amplifies the\nmodeling complexity of ST data and compromises model interpretability. To this\nend, we propose a multi-factor ST prediction task that predicts partial ST data\nevolution under different factors, and combines them for a final prediction. We\nmake two contributions to this task: an effective theoretical solution and a\nportable instantiation framework. Specifically, we first propose a theoretical\nsolution called decomposed prediction strategy and prove its effectiveness from\nthe perspective of information entropy theory. On top of that, we instantiate a\nnovel model-agnostic framework, named spatio-temporal graph decomposition\nlearning (STGDL), for multi-factor ST prediction. The framework consists of two\nmain components: an automatic graph decomposition module that decomposes the\noriginal graph structure inherent in ST data into subgraphs corresponding to\ndifferent factors, and a decomposed learning network that learns the partial ST\ndata on each subgraph separately and integrates them for the final prediction.\nWe conduct extensive experiments on four real-world ST datasets of two types of\ngraphs, i.e., grid graph and network graph. Results show that our framework\nsignificantly reduces prediction errors of various ST models by 9.41% on\naverage (35.36% at most). Furthermore, a case study reveals the\ninterpretability potential of our framework.\n", "link": "http://arxiv.org/abs/2310.10374v2", "date": "2024-03-07", "relevancy": 2.0468, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5329}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5141}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5008}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Multi-Factor%20Spatio-Temporal%20Prediction%20based%20on%20Graph%20Decomposition%0A%20%20Learning&body=Title%3A%20Multi-Factor%20Spatio-Temporal%20Prediction%20based%20on%20Graph%20Decomposition%0A%20%20Learning%0AAuthor%3A%20Jiahao%20Ji%20and%20Jingyuan%20Wang%20and%20Yu%20Mou%20and%20Cheng%20Long%0AAbstract%3A%20%20%20Spatio-temporal%20%28ST%29%20prediction%20is%20an%20important%20and%20widely%20used%20technique%20in%0Adata%20mining%20and%20analytics%2C%20especially%20for%20ST%20data%20in%20urban%20systems%20such%20as%0Atransportation%20data.%20In%20practice%2C%20the%20ST%20data%20generation%20is%20usually%20influenced%0Aby%20various%20latent%20factors%20tied%20to%20natural%20phenomena%20or%20human%20socioeconomic%0Aactivities%2C%20impacting%20specific%20spatial%20areas%20selectively.%20However%2C%20existing%20ST%0Aprediction%20methods%20usually%20do%20not%20refine%20the%20impacts%20of%20different%20factors%2C%20but%0Adirectly%20model%20the%20entangled%20impacts%20of%20multiple%20factors.%20This%20amplifies%20the%0Amodeling%20complexity%20of%20ST%20data%20and%20compromises%20model%20interpretability.%20To%20this%0Aend%2C%20we%20propose%20a%20multi-factor%20ST%20prediction%20task%20that%20predicts%20partial%20ST%20data%0Aevolution%20under%20different%20factors%2C%20and%20combines%20them%20for%20a%20final%20prediction.%20We%0Amake%20two%20contributions%20to%20this%20task%3A%20an%20effective%20theoretical%20solution%20and%20a%0Aportable%20instantiation%20framework.%20Specifically%2C%20we%20first%20propose%20a%20theoretical%0Asolution%20called%20decomposed%20prediction%20strategy%20and%20prove%20its%20effectiveness%20from%0Athe%20perspective%20of%20information%20entropy%20theory.%20On%20top%20of%20that%2C%20we%20instantiate%20a%0Anovel%20model-agnostic%20framework%2C%20named%20spatio-temporal%20graph%20decomposition%0Alearning%20%28STGDL%29%2C%20for%20multi-factor%20ST%20prediction.%20The%20framework%20consists%20of%20two%0Amain%20components%3A%20an%20automatic%20graph%20decomposition%20module%20that%20decomposes%20the%0Aoriginal%20graph%20structure%20inherent%20in%20ST%20data%20into%20subgraphs%20corresponding%20to%0Adifferent%20factors%2C%20and%20a%20decomposed%20learning%20network%20that%20learns%20the%20partial%20ST%0Adata%20on%20each%20subgraph%20separately%20and%20integrates%20them%20for%20the%20final%20prediction.%0AWe%20conduct%20extensive%20experiments%20on%20four%20real-world%20ST%20datasets%20of%20two%20types%20of%0Agraphs%2C%20i.e.%2C%20grid%20graph%20and%20network%20graph.%20Results%20show%20that%20our%20framework%0Asignificantly%20reduces%20prediction%20errors%20of%20various%20ST%20models%20by%209.41%25%20on%0Aaverage%20%2835.36%25%20at%20most%29.%20Furthermore%2C%20a%20case%20study%20reveals%20the%0Ainterpretability%20potential%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10374v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Factor%20Spatio-Temporal%20Prediction%20based%20on%20Graph%20Decomposition%0A%20%20Learning&entry.906535625=Jiahao%20Ji%20and%20Jingyuan%20Wang%20and%20Yu%20Mou%20and%20Cheng%20Long&entry.1292438233=%20%20Spatio-temporal%20%28ST%29%20prediction%20is%20an%20important%20and%20widely%20used%20technique%20in%0Adata%20mining%20and%20analytics%2C%20especially%20for%20ST%20data%20in%20urban%20systems%20such%20as%0Atransportation%20data.%20In%20practice%2C%20the%20ST%20data%20generation%20is%20usually%20influenced%0Aby%20various%20latent%20factors%20tied%20to%20natural%20phenomena%20or%20human%20socioeconomic%0Aactivities%2C%20impacting%20specific%20spatial%20areas%20selectively.%20However%2C%20existing%20ST%0Aprediction%20methods%20usually%20do%20not%20refine%20the%20impacts%20of%20different%20factors%2C%20but%0Adirectly%20model%20the%20entangled%20impacts%20of%20multiple%20factors.%20This%20amplifies%20the%0Amodeling%20complexity%20of%20ST%20data%20and%20compromises%20model%20interpretability.%20To%20this%0Aend%2C%20we%20propose%20a%20multi-factor%20ST%20prediction%20task%20that%20predicts%20partial%20ST%20data%0Aevolution%20under%20different%20factors%2C%20and%20combines%20them%20for%20a%20final%20prediction.%20We%0Amake%20two%20contributions%20to%20this%20task%3A%20an%20effective%20theoretical%20solution%20and%20a%0Aportable%20instantiation%20framework.%20Specifically%2C%20we%20first%20propose%20a%20theoretical%0Asolution%20called%20decomposed%20prediction%20strategy%20and%20prove%20its%20effectiveness%20from%0Athe%20perspective%20of%20information%20entropy%20theory.%20On%20top%20of%20that%2C%20we%20instantiate%20a%0Anovel%20model-agnostic%20framework%2C%20named%20spatio-temporal%20graph%20decomposition%0Alearning%20%28STGDL%29%2C%20for%20multi-factor%20ST%20prediction.%20The%20framework%20consists%20of%20two%0Amain%20components%3A%20an%20automatic%20graph%20decomposition%20module%20that%20decomposes%20the%0Aoriginal%20graph%20structure%20inherent%20in%20ST%20data%20into%20subgraphs%20corresponding%20to%0Adifferent%20factors%2C%20and%20a%20decomposed%20learning%20network%20that%20learns%20the%20partial%20ST%0Adata%20on%20each%20subgraph%20separately%20and%20integrates%20them%20for%20the%20final%20prediction.%0AWe%20conduct%20extensive%20experiments%20on%20four%20real-world%20ST%20datasets%20of%20two%20types%20of%0Agraphs%2C%20i.e.%2C%20grid%20graph%20and%20network%20graph.%20Results%20show%20that%20our%20framework%0Asignificantly%20reduces%20prediction%20errors%20of%20various%20ST%20models%20by%209.41%25%20on%0Aaverage%20%2835.36%25%20at%20most%29.%20Furthermore%2C%20a%20case%20study%20reveals%20the%0Ainterpretability%20potential%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10374v2&entry.124074799=Read"},
{"title": "Enhancing Data Quality in Federated Fine-Tuning of Foundation Models", "author": "Wanru Zhao and Yaxin Du and Nicholas Donald Lane and Siheng Chen and Yanfeng Wang", "abstract": "  In the current landscape of foundation model training, there is a significant\nreliance on public domain data, which is nearing exhaustion according to recent\nresearch. To further scale up, it is crucial to incorporate collaboration among\nmultiple specialized and high-quality private domain data sources. However, the\nchallenge of training models locally without sharing private data presents\nnumerous obstacles in data quality control. To tackle this issue, we propose a\ndata quality control pipeline for federated fine-tuning of foundation models.\nThis pipeline computes scores reflecting the quality of training data and\ndetermines a global threshold for a unified standard, aiming for improved\nglobal performance. Our experiments show that the proposed quality control\npipeline facilitates the effectiveness and reliability of the model training,\nleading to better performance.\n", "link": "http://arxiv.org/abs/2403.04529v1", "date": "2024-03-07", "relevancy": 2.0433, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5277}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5074}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4953}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Data%20Quality%20in%20Federated%20Fine-Tuning%20of%20Foundation%20Models&body=Title%3A%20Enhancing%20Data%20Quality%20in%20Federated%20Fine-Tuning%20of%20Foundation%20Models%0AAuthor%3A%20Wanru%20Zhao%20and%20Yaxin%20Du%20and%20Nicholas%20Donald%20Lane%20and%20Siheng%20Chen%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20In%20the%20current%20landscape%20of%20foundation%20model%20training%2C%20there%20is%20a%20significant%0Areliance%20on%20public%20domain%20data%2C%20which%20is%20nearing%20exhaustion%20according%20to%20recent%0Aresearch.%20To%20further%20scale%20up%2C%20it%20is%20crucial%20to%20incorporate%20collaboration%20among%0Amultiple%20specialized%20and%20high-quality%20private%20domain%20data%20sources.%20However%2C%20the%0Achallenge%20of%20training%20models%20locally%20without%20sharing%20private%20data%20presents%0Anumerous%20obstacles%20in%20data%20quality%20control.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%0Adata%20quality%20control%20pipeline%20for%20federated%20fine-tuning%20of%20foundation%20models.%0AThis%20pipeline%20computes%20scores%20reflecting%20the%20quality%20of%20training%20data%20and%0Adetermines%20a%20global%20threshold%20for%20a%20unified%20standard%2C%20aiming%20for%20improved%0Aglobal%20performance.%20Our%20experiments%20show%20that%20the%20proposed%20quality%20control%0Apipeline%20facilitates%20the%20effectiveness%20and%20reliability%20of%20the%20model%20training%2C%0Aleading%20to%20better%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04529v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Data%20Quality%20in%20Federated%20Fine-Tuning%20of%20Foundation%20Models&entry.906535625=Wanru%20Zhao%20and%20Yaxin%20Du%20and%20Nicholas%20Donald%20Lane%20and%20Siheng%20Chen%20and%20Yanfeng%20Wang&entry.1292438233=%20%20In%20the%20current%20landscape%20of%20foundation%20model%20training%2C%20there%20is%20a%20significant%0Areliance%20on%20public%20domain%20data%2C%20which%20is%20nearing%20exhaustion%20according%20to%20recent%0Aresearch.%20To%20further%20scale%20up%2C%20it%20is%20crucial%20to%20incorporate%20collaboration%20among%0Amultiple%20specialized%20and%20high-quality%20private%20domain%20data%20sources.%20However%2C%20the%0Achallenge%20of%20training%20models%20locally%20without%20sharing%20private%20data%20presents%0Anumerous%20obstacles%20in%20data%20quality%20control.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%0Adata%20quality%20control%20pipeline%20for%20federated%20fine-tuning%20of%20foundation%20models.%0AThis%20pipeline%20computes%20scores%20reflecting%20the%20quality%20of%20training%20data%20and%0Adetermines%20a%20global%20threshold%20for%20a%20unified%20standard%2C%20aiming%20for%20improved%0Aglobal%20performance.%20Our%20experiments%20show%20that%20the%20proposed%20quality%20control%0Apipeline%20facilitates%20the%20effectiveness%20and%20reliability%20of%20the%20model%20training%2C%0Aleading%20to%20better%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04529v1&entry.124074799=Read"},
{"title": "Predicting small molecules solubilities on endpoint devices using deep\n  ensemble neural networks", "author": "Mayk Caldas Ramos and Andrew D. White", "abstract": "  Aqueous solubility is a valuable yet challenging property to predict.\nComputing solubility using first-principles methods requires accounting for the\ncompeting effects of entropy and enthalpy, resulting in long computations for\nrelatively poor accuracy. Data-driven approaches, such as deep learning, offer\nimproved accuracy and computational efficiency but typically lack uncertainty\nquantification. Additionally, ease of use remains a concern for any\ncomputational technique, resulting in the sustained popularity of group-based\ncontribution methods. In this work, we addressed these problems with a deep\nlearning model with predictive uncertainty that runs on a static website\n(without a server). This approach moves computing needs onto the website\nvisitor without requiring installation, removing the need to pay for and\nmaintain servers. Our model achieves satisfactory results in solubility\nprediction. Furthermore, we demonstrate how to create molecular property\nprediction models that balance uncertainty and ease of use. The code is\navailable at https://github.com/ur-whitelab/mol.dev, and the model is usable at\nhttps://mol.dev.\n", "link": "http://arxiv.org/abs/2307.05318v4", "date": "2024-03-07", "relevancy": 1.3083, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4818}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4442}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4145}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Predicting%20small%20molecules%20solubilities%20on%20endpoint%20devices%20using%20deep%0A%20%20ensemble%20neural%20networks&body=Title%3A%20Predicting%20small%20molecules%20solubilities%20on%20endpoint%20devices%20using%20deep%0A%20%20ensemble%20neural%20networks%0AAuthor%3A%20Mayk%20Caldas%20Ramos%20and%20Andrew%20D.%20White%0AAbstract%3A%20%20%20Aqueous%20solubility%20is%20a%20valuable%20yet%20challenging%20property%20to%20predict.%0AComputing%20solubility%20using%20first-principles%20methods%20requires%20accounting%20for%20the%0Acompeting%20effects%20of%20entropy%20and%20enthalpy%2C%20resulting%20in%20long%20computations%20for%0Arelatively%20poor%20accuracy.%20Data-driven%20approaches%2C%20such%20as%20deep%20learning%2C%20offer%0Aimproved%20accuracy%20and%20computational%20efficiency%20but%20typically%20lack%20uncertainty%0Aquantification.%20Additionally%2C%20ease%20of%20use%20remains%20a%20concern%20for%20any%0Acomputational%20technique%2C%20resulting%20in%20the%20sustained%20popularity%20of%20group-based%0Acontribution%20methods.%20In%20this%20work%2C%20we%20addressed%20these%20problems%20with%20a%20deep%0Alearning%20model%20with%20predictive%20uncertainty%20that%20runs%20on%20a%20static%20website%0A%28without%20a%20server%29.%20This%20approach%20moves%20computing%20needs%20onto%20the%20website%0Avisitor%20without%20requiring%20installation%2C%20removing%20the%20need%20to%20pay%20for%20and%0Amaintain%20servers.%20Our%20model%20achieves%20satisfactory%20results%20in%20solubility%0Aprediction.%20Furthermore%2C%20we%20demonstrate%20how%20to%20create%20molecular%20property%0Aprediction%20models%20that%20balance%20uncertainty%20and%20ease%20of%20use.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/ur-whitelab/mol.dev%2C%20and%20the%20model%20is%20usable%20at%0Ahttps%3A//mol.dev.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.05318v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20small%20molecules%20solubilities%20on%20endpoint%20devices%20using%20deep%0A%20%20ensemble%20neural%20networks&entry.906535625=Mayk%20Caldas%20Ramos%20and%20Andrew%20D.%20White&entry.1292438233=%20%20Aqueous%20solubility%20is%20a%20valuable%20yet%20challenging%20property%20to%20predict.%0AComputing%20solubility%20using%20first-principles%20methods%20requires%20accounting%20for%20the%0Acompeting%20effects%20of%20entropy%20and%20enthalpy%2C%20resulting%20in%20long%20computations%20for%0Arelatively%20poor%20accuracy.%20Data-driven%20approaches%2C%20such%20as%20deep%20learning%2C%20offer%0Aimproved%20accuracy%20and%20computational%20efficiency%20but%20typically%20lack%20uncertainty%0Aquantification.%20Additionally%2C%20ease%20of%20use%20remains%20a%20concern%20for%20any%0Acomputational%20technique%2C%20resulting%20in%20the%20sustained%20popularity%20of%20group-based%0Acontribution%20methods.%20In%20this%20work%2C%20we%20addressed%20these%20problems%20with%20a%20deep%0Alearning%20model%20with%20predictive%20uncertainty%20that%20runs%20on%20a%20static%20website%0A%28without%20a%20server%29.%20This%20approach%20moves%20computing%20needs%20onto%20the%20website%0Avisitor%20without%20requiring%20installation%2C%20removing%20the%20need%20to%20pay%20for%20and%0Amaintain%20servers.%20Our%20model%20achieves%20satisfactory%20results%20in%20solubility%0Aprediction.%20Furthermore%2C%20we%20demonstrate%20how%20to%20create%20molecular%20property%0Aprediction%20models%20that%20balance%20uncertainty%20and%20ease%20of%20use.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/ur-whitelab/mol.dev%2C%20and%20the%20model%20is%20usable%20at%0Ahttps%3A//mol.dev.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.05318v4&entry.124074799=Read"},
{"title": "Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation\n  for Complex Scenes", "author": "Stamatios Georgoulis and Weining Ren and Alfredo Bochicchio and Daniel Eckert and Yuanyou Li and Abel Gawel", "abstract": "  Rapid and reliable identification of dynamic scene parts, also known as\nmotion segmentation, is a key challenge for mobile sensors. Contemporary RGB\ncamera-based methods rely on modeling camera and scene properties however, are\noften under-constrained and fall short in unknown categories. Event cameras\nhave the potential to overcome these limitations, but corresponding methods\nhave only been demonstrated in smaller-scale indoor environments with\nsimplified dynamic objects. This work presents an event-based method for\nclass-agnostic motion segmentation that can successfully be deployed across\ncomplex large-scale outdoor environments too. To this end, we introduce a novel\ndivide-and-conquer pipeline that combines: (a) ego-motion compensated events,\ncomputed via a scene understanding module that predicts monocular depth and\ncamera pose as auxiliary tasks, and (b) optical flow from a dedicated optical\nflow module. These intermediate representations are then fed into a\nsegmentation module that predicts motion segmentation masks. A novel\ntransformer-based temporal attention module in the segmentation module builds\ncorrelations across adjacent 'frames' to get temporally consistent segmentation\nmasks. Our method sets the new state-of-the-art on the classic EV-IMO benchmark\n(indoors), where we achieve improvements of 2.19 moving object IoU (2.22 mIoU)\nand 4.52 point IoU respectively, as well as on a newly-generated motion\nsegmentation and tracking benchmark (outdoors) based on the DSEC event dataset,\ntermed DSEC-MOTS, where we show improvement of 12.91 moving object IoU.\n", "link": "http://arxiv.org/abs/2403.04562v1", "date": "2024-03-07", "relevancy": 1.7426, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5897}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5874}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5747}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Out%20of%20the%20Room%3A%20Generalizing%20Event-Based%20Dynamic%20Motion%20Segmentation%0A%20%20for%20Complex%20Scenes&body=Title%3A%20Out%20of%20the%20Room%3A%20Generalizing%20Event-Based%20Dynamic%20Motion%20Segmentation%0A%20%20for%20Complex%20Scenes%0AAuthor%3A%20Stamatios%20Georgoulis%20and%20Weining%20Ren%20and%20Alfredo%20Bochicchio%20and%20Daniel%20Eckert%20and%20Yuanyou%20Li%20and%20Abel%20Gawel%0AAbstract%3A%20%20%20Rapid%20and%20reliable%20identification%20of%20dynamic%20scene%20parts%2C%20also%20known%20as%0Amotion%20segmentation%2C%20is%20a%20key%20challenge%20for%20mobile%20sensors.%20Contemporary%20RGB%0Acamera-based%20methods%20rely%20on%20modeling%20camera%20and%20scene%20properties%20however%2C%20are%0Aoften%20under-constrained%20and%20fall%20short%20in%20unknown%20categories.%20Event%20cameras%0Ahave%20the%20potential%20to%20overcome%20these%20limitations%2C%20but%20corresponding%20methods%0Ahave%20only%20been%20demonstrated%20in%20smaller-scale%20indoor%20environments%20with%0Asimplified%20dynamic%20objects.%20This%20work%20presents%20an%20event-based%20method%20for%0Aclass-agnostic%20motion%20segmentation%20that%20can%20successfully%20be%20deployed%20across%0Acomplex%20large-scale%20outdoor%20environments%20too.%20To%20this%20end%2C%20we%20introduce%20a%20novel%0Adivide-and-conquer%20pipeline%20that%20combines%3A%20%28a%29%20ego-motion%20compensated%20events%2C%0Acomputed%20via%20a%20scene%20understanding%20module%20that%20predicts%20monocular%20depth%20and%0Acamera%20pose%20as%20auxiliary%20tasks%2C%20and%20%28b%29%20optical%20flow%20from%20a%20dedicated%20optical%0Aflow%20module.%20These%20intermediate%20representations%20are%20then%20fed%20into%20a%0Asegmentation%20module%20that%20predicts%20motion%20segmentation%20masks.%20A%20novel%0Atransformer-based%20temporal%20attention%20module%20in%20the%20segmentation%20module%20builds%0Acorrelations%20across%20adjacent%20%27frames%27%20to%20get%20temporally%20consistent%20segmentation%0Amasks.%20Our%20method%20sets%20the%20new%20state-of-the-art%20on%20the%20classic%20EV-IMO%20benchmark%0A%28indoors%29%2C%20where%20we%20achieve%20improvements%20of%202.19%20moving%20object%20IoU%20%282.22%20mIoU%29%0Aand%204.52%20point%20IoU%20respectively%2C%20as%20well%20as%20on%20a%20newly-generated%20motion%0Asegmentation%20and%20tracking%20benchmark%20%28outdoors%29%20based%20on%20the%20DSEC%20event%20dataset%2C%0Atermed%20DSEC-MOTS%2C%20where%20we%20show%20improvement%20of%2012.91%20moving%20object%20IoU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04562v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out%20of%20the%20Room%3A%20Generalizing%20Event-Based%20Dynamic%20Motion%20Segmentation%0A%20%20for%20Complex%20Scenes&entry.906535625=Stamatios%20Georgoulis%20and%20Weining%20Ren%20and%20Alfredo%20Bochicchio%20and%20Daniel%20Eckert%20and%20Yuanyou%20Li%20and%20Abel%20Gawel&entry.1292438233=%20%20Rapid%20and%20reliable%20identification%20of%20dynamic%20scene%20parts%2C%20also%20known%20as%0Amotion%20segmentation%2C%20is%20a%20key%20challenge%20for%20mobile%20sensors.%20Contemporary%20RGB%0Acamera-based%20methods%20rely%20on%20modeling%20camera%20and%20scene%20properties%20however%2C%20are%0Aoften%20under-constrained%20and%20fall%20short%20in%20unknown%20categories.%20Event%20cameras%0Ahave%20the%20potential%20to%20overcome%20these%20limitations%2C%20but%20corresponding%20methods%0Ahave%20only%20been%20demonstrated%20in%20smaller-scale%20indoor%20environments%20with%0Asimplified%20dynamic%20objects.%20This%20work%20presents%20an%20event-based%20method%20for%0Aclass-agnostic%20motion%20segmentation%20that%20can%20successfully%20be%20deployed%20across%0Acomplex%20large-scale%20outdoor%20environments%20too.%20To%20this%20end%2C%20we%20introduce%20a%20novel%0Adivide-and-conquer%20pipeline%20that%20combines%3A%20%28a%29%20ego-motion%20compensated%20events%2C%0Acomputed%20via%20a%20scene%20understanding%20module%20that%20predicts%20monocular%20depth%20and%0Acamera%20pose%20as%20auxiliary%20tasks%2C%20and%20%28b%29%20optical%20flow%20from%20a%20dedicated%20optical%0Aflow%20module.%20These%20intermediate%20representations%20are%20then%20fed%20into%20a%0Asegmentation%20module%20that%20predicts%20motion%20segmentation%20masks.%20A%20novel%0Atransformer-based%20temporal%20attention%20module%20in%20the%20segmentation%20module%20builds%0Acorrelations%20across%20adjacent%20%27frames%27%20to%20get%20temporally%20consistent%20segmentation%0Amasks.%20Our%20method%20sets%20the%20new%20state-of-the-art%20on%20the%20classic%20EV-IMO%20benchmark%0A%28indoors%29%2C%20where%20we%20achieve%20improvements%20of%202.19%20moving%20object%20IoU%20%282.22%20mIoU%29%0Aand%204.52%20point%20IoU%20respectively%2C%20as%20well%20as%20on%20a%20newly-generated%20motion%0Asegmentation%20and%20tracking%20benchmark%20%28outdoors%29%20based%20on%20the%20DSEC%20event%20dataset%2C%0Atermed%20DSEC-MOTS%2C%20where%20we%20show%20improvement%20of%2012.91%20moving%20object%20IoU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04562v1&entry.124074799=Read"},
{"title": "Scalable, Simulation-Guided Compliant Tactile Finger Design", "author": "Yuxiang Ma and Arpit Agarwal and Sandra Q. Liu and Wenzhen Yuan and Edward H. Adelson", "abstract": "  Compliant grippers enable robots to work with humans in unstructured\nenvironments. In general, these grippers can improve with tactile sensing to\nestimate the state of objects around them to precisely manipulate objects.\nHowever, co-designing compliant structures with high-resolution tactile sensing\nis a challenging task. We propose a simulation framework for the end-to-end\nforward design of GelSight Fin Ray sensors. Our simulation framework consists\nof mechanical simulation using the finite element method (FEM) and optical\nsimulation including physically based rendering (PBR). To simulate the\nfluorescent paint used in these GelSight Fin Rays, we propose an efficient\nmethod that can be directly integrated in PBR. Using the simulation framework,\nwe investigate design choices available in the compliant grippers, namely gel\npad shapes, illumination conditions, Fin Ray gripper sizes, and Fin Ray\nstiffness. This infrastructure enables faster design and prototype time frames\nof new Fin Ray sensors that have various sensing areas, ranging from 48 mm\n$\\times$ \\18 mm to 70 mm $\\times$ 35 mm. Given the parameters we choose, we can\nthus optimize different Fin Ray designs and show their utility in grasping\nday-to-day objects.\n", "link": "http://arxiv.org/abs/2403.04638v1", "date": "2024-03-07", "relevancy": 1.6253, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5344}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5181}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Scalable%2C%20Simulation-Guided%20Compliant%20Tactile%20Finger%20Design&body=Title%3A%20Scalable%2C%20Simulation-Guided%20Compliant%20Tactile%20Finger%20Design%0AAuthor%3A%20Yuxiang%20Ma%20and%20Arpit%20Agarwal%20and%20Sandra%20Q.%20Liu%20and%20Wenzhen%20Yuan%20and%20Edward%20H.%20Adelson%0AAbstract%3A%20%20%20Compliant%20grippers%20enable%20robots%20to%20work%20with%20humans%20in%20unstructured%0Aenvironments.%20In%20general%2C%20these%20grippers%20can%20improve%20with%20tactile%20sensing%20to%0Aestimate%20the%20state%20of%20objects%20around%20them%20to%20precisely%20manipulate%20objects.%0AHowever%2C%20co-designing%20compliant%20structures%20with%20high-resolution%20tactile%20sensing%0Ais%20a%20challenging%20task.%20We%20propose%20a%20simulation%20framework%20for%20the%20end-to-end%0Aforward%20design%20of%20GelSight%20Fin%20Ray%20sensors.%20Our%20simulation%20framework%20consists%0Aof%20mechanical%20simulation%20using%20the%20finite%20element%20method%20%28FEM%29%20and%20optical%0Asimulation%20including%20physically%20based%20rendering%20%28PBR%29.%20To%20simulate%20the%0Afluorescent%20paint%20used%20in%20these%20GelSight%20Fin%20Rays%2C%20we%20propose%20an%20efficient%0Amethod%20that%20can%20be%20directly%20integrated%20in%20PBR.%20Using%20the%20simulation%20framework%2C%0Awe%20investigate%20design%20choices%20available%20in%20the%20compliant%20grippers%2C%20namely%20gel%0Apad%20shapes%2C%20illumination%20conditions%2C%20Fin%20Ray%20gripper%20sizes%2C%20and%20Fin%20Ray%0Astiffness.%20This%20infrastructure%20enables%20faster%20design%20and%20prototype%20time%20frames%0Aof%20new%20Fin%20Ray%20sensors%20that%20have%20various%20sensing%20areas%2C%20ranging%20from%2048%20mm%0A%24%5Ctimes%24%20%5C18%20mm%20to%2070%20mm%20%24%5Ctimes%24%2035%20mm.%20Given%20the%20parameters%20we%20choose%2C%20we%20can%0Athus%20optimize%20different%20Fin%20Ray%20designs%20and%20show%20their%20utility%20in%20grasping%0Aday-to-day%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04638v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%2C%20Simulation-Guided%20Compliant%20Tactile%20Finger%20Design&entry.906535625=Yuxiang%20Ma%20and%20Arpit%20Agarwal%20and%20Sandra%20Q.%20Liu%20and%20Wenzhen%20Yuan%20and%20Edward%20H.%20Adelson&entry.1292438233=%20%20Compliant%20grippers%20enable%20robots%20to%20work%20with%20humans%20in%20unstructured%0Aenvironments.%20In%20general%2C%20these%20grippers%20can%20improve%20with%20tactile%20sensing%20to%0Aestimate%20the%20state%20of%20objects%20around%20them%20to%20precisely%20manipulate%20objects.%0AHowever%2C%20co-designing%20compliant%20structures%20with%20high-resolution%20tactile%20sensing%0Ais%20a%20challenging%20task.%20We%20propose%20a%20simulation%20framework%20for%20the%20end-to-end%0Aforward%20design%20of%20GelSight%20Fin%20Ray%20sensors.%20Our%20simulation%20framework%20consists%0Aof%20mechanical%20simulation%20using%20the%20finite%20element%20method%20%28FEM%29%20and%20optical%0Asimulation%20including%20physically%20based%20rendering%20%28PBR%29.%20To%20simulate%20the%0Afluorescent%20paint%20used%20in%20these%20GelSight%20Fin%20Rays%2C%20we%20propose%20an%20efficient%0Amethod%20that%20can%20be%20directly%20integrated%20in%20PBR.%20Using%20the%20simulation%20framework%2C%0Awe%20investigate%20design%20choices%20available%20in%20the%20compliant%20grippers%2C%20namely%20gel%0Apad%20shapes%2C%20illumination%20conditions%2C%20Fin%20Ray%20gripper%20sizes%2C%20and%20Fin%20Ray%0Astiffness.%20This%20infrastructure%20enables%20faster%20design%20and%20prototype%20time%20frames%0Aof%20new%20Fin%20Ray%20sensors%20that%20have%20various%20sensing%20areas%2C%20ranging%20from%2048%20mm%0A%24%5Ctimes%24%20%5C18%20mm%20to%2070%20mm%20%24%5Ctimes%24%2035%20mm.%20Given%20the%20parameters%20we%20choose%2C%20we%20can%0Athus%20optimize%20different%20Fin%20Ray%20designs%20and%20show%20their%20utility%20in%20grasping%0Aday-to-day%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04638v1&entry.124074799=Read"},
{"title": "Membership Inference Attacks and Privacy in Topic Modeling", "author": "Nico Manzonelli and Wanrong Zhang and Salil Vadhan", "abstract": "  Recent research shows that large language models are susceptible to privacy\nattacks that infer aspects of the training data. However, it is unclear if\nsimpler generative models, like topic models, share similar vulnerabilities. In\nthis work, we propose an attack against topic models that can confidently\nidentify members of the training data in Latent Dirichlet Allocation. Our\nresults suggest that the privacy risks associated with generative modeling are\nnot restricted to large neural models. Additionally, to mitigate these\nvulnerabilities, we explore differentially private (DP) topic modeling. We\npropose a framework for private topic modeling that incorporates DP vocabulary\nselection as a pre-processing step, and show that it improves privacy while\nhaving limited effects on practical utility.\n", "link": "http://arxiv.org/abs/2403.04451v1", "date": "2024-03-07", "relevancy": 1.3043, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4436}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4262}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4213}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Membership%20Inference%20Attacks%20and%20Privacy%20in%20Topic%20Modeling&body=Title%3A%20Membership%20Inference%20Attacks%20and%20Privacy%20in%20Topic%20Modeling%0AAuthor%3A%20Nico%20Manzonelli%20and%20Wanrong%20Zhang%20and%20Salil%20Vadhan%0AAbstract%3A%20%20%20Recent%20research%20shows%20that%20large%20language%20models%20are%20susceptible%20to%20privacy%0Aattacks%20that%20infer%20aspects%20of%20the%20training%20data.%20However%2C%20it%20is%20unclear%20if%0Asimpler%20generative%20models%2C%20like%20topic%20models%2C%20share%20similar%20vulnerabilities.%20In%0Athis%20work%2C%20we%20propose%20an%20attack%20against%20topic%20models%20that%20can%20confidently%0Aidentify%20members%20of%20the%20training%20data%20in%20Latent%20Dirichlet%20Allocation.%20Our%0Aresults%20suggest%20that%20the%20privacy%20risks%20associated%20with%20generative%20modeling%20are%0Anot%20restricted%20to%20large%20neural%20models.%20Additionally%2C%20to%20mitigate%20these%0Avulnerabilities%2C%20we%20explore%20differentially%20private%20%28DP%29%20topic%20modeling.%20We%0Apropose%20a%20framework%20for%20private%20topic%20modeling%20that%20incorporates%20DP%20vocabulary%0Aselection%20as%20a%20pre-processing%20step%2C%20and%20show%20that%20it%20improves%20privacy%20while%0Ahaving%20limited%20effects%20on%20practical%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04451v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Membership%20Inference%20Attacks%20and%20Privacy%20in%20Topic%20Modeling&entry.906535625=Nico%20Manzonelli%20and%20Wanrong%20Zhang%20and%20Salil%20Vadhan&entry.1292438233=%20%20Recent%20research%20shows%20that%20large%20language%20models%20are%20susceptible%20to%20privacy%0Aattacks%20that%20infer%20aspects%20of%20the%20training%20data.%20However%2C%20it%20is%20unclear%20if%0Asimpler%20generative%20models%2C%20like%20topic%20models%2C%20share%20similar%20vulnerabilities.%20In%0Athis%20work%2C%20we%20propose%20an%20attack%20against%20topic%20models%20that%20can%20confidently%0Aidentify%20members%20of%20the%20training%20data%20in%20Latent%20Dirichlet%20Allocation.%20Our%0Aresults%20suggest%20that%20the%20privacy%20risks%20associated%20with%20generative%20modeling%20are%0Anot%20restricted%20to%20large%20neural%20models.%20Additionally%2C%20to%20mitigate%20these%0Avulnerabilities%2C%20we%20explore%20differentially%20private%20%28DP%29%20topic%20modeling.%20We%0Apropose%20a%20framework%20for%20private%20topic%20modeling%20that%20incorporates%20DP%20vocabulary%0Aselection%20as%20a%20pre-processing%20step%2C%20and%20show%20that%20it%20improves%20privacy%20while%0Ahaving%20limited%20effects%20on%20practical%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04451v1&entry.124074799=Read"},
{"title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error", "author": "Boshi Wang and Hao Fang and Jason Eisner and Benjamin Van Durme and Yu Su", "abstract": "  Tools are essential for large language models (LLMs) to acquire up-to-date\ninformation and take consequential actions in external environments. Existing\nwork on tool-augmented LLMs primarily focuses on the broad coverage of tools\nand the flexibility of adding new tools. However, a critical aspect that has\nsurprisingly been understudied is simply how accurately an LLM uses tools for\nwhich it has been trained. We find that existing LLMs, including GPT-4 and\nopen-source LLMs specifically fine-tuned for tool use, only reach a correctness\nrate in the range of 30% to 60%, far from reliable use in practice. We propose\na biologically inspired method for tool-augmented LLMs, simulated trial and\nerror (STE), that orchestrates three key mechanisms for successful tool use\nbehaviors in the biological system: trial and error, imagination, and memory.\nSpecifically, STE leverages an LLM's 'imagination' to simulate plausible\nscenarios for using a tool, after which the LLM interacts with the tool to\nlearn from its execution feedback. Both short-term and long-term memory are\nemployed to improve the depth and breadth of the exploration, respectively.\nComprehensive experiments on ToolBench show that STE substantially improves\ntool learning for LLMs under both in-context learning and fine-tuning settings,\nbringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform\nGPT-4. We also show effective continual learning of tools via a simple\nexperience replay strategy.\n", "link": "http://arxiv.org/abs/2403.04746v1", "date": "2024-03-07", "relevancy": 1.5623, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5298}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5214}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5102}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20LLMs%20in%20the%20Imaginarium%3A%20Tool%20Learning%20through%20Simulated%20Trial%20and%20Error&body=Title%3A%20LLMs%20in%20the%20Imaginarium%3A%20Tool%20Learning%20through%20Simulated%20Trial%20and%20Error%0AAuthor%3A%20Boshi%20Wang%20and%20Hao%20Fang%20and%20Jason%20Eisner%20and%20Benjamin%20Van%20Durme%20and%20Yu%20Su%0AAbstract%3A%20%20%20Tools%20are%20essential%20for%20large%20language%20models%20%28LLMs%29%20to%20acquire%20up-to-date%0Ainformation%20and%20take%20consequential%20actions%20in%20external%20environments.%20Existing%0Awork%20on%20tool-augmented%20LLMs%20primarily%20focuses%20on%20the%20broad%20coverage%20of%20tools%0Aand%20the%20flexibility%20of%20adding%20new%20tools.%20However%2C%20a%20critical%20aspect%20that%20has%0Asurprisingly%20been%20understudied%20is%20simply%20how%20accurately%20an%20LLM%20uses%20tools%20for%0Awhich%20it%20has%20been%20trained.%20We%20find%20that%20existing%20LLMs%2C%20including%20GPT-4%20and%0Aopen-source%20LLMs%20specifically%20fine-tuned%20for%20tool%20use%2C%20only%20reach%20a%20correctness%0Arate%20in%20the%20range%20of%2030%25%20to%2060%25%2C%20far%20from%20reliable%20use%20in%20practice.%20We%20propose%0Aa%20biologically%20inspired%20method%20for%20tool-augmented%20LLMs%2C%20simulated%20trial%20and%0Aerror%20%28STE%29%2C%20that%20orchestrates%20three%20key%20mechanisms%20for%20successful%20tool%20use%0Abehaviors%20in%20the%20biological%20system%3A%20trial%20and%20error%2C%20imagination%2C%20and%20memory.%0ASpecifically%2C%20STE%20leverages%20an%20LLM%27s%20%27imagination%27%20to%20simulate%20plausible%0Ascenarios%20for%20using%20a%20tool%2C%20after%20which%20the%20LLM%20interacts%20with%20the%20tool%20to%0Alearn%20from%20its%20execution%20feedback.%20Both%20short-term%20and%20long-term%20memory%20are%0Aemployed%20to%20improve%20the%20depth%20and%20breadth%20of%20the%20exploration%2C%20respectively.%0AComprehensive%20experiments%20on%20ToolBench%20show%20that%20STE%20substantially%20improves%0Atool%20learning%20for%20LLMs%20under%20both%20in-context%20learning%20and%20fine-tuning%20settings%2C%0Abringing%20a%20boost%20of%2046.7%25%20to%20Mistral-Instruct-7B%20and%20enabling%20it%20to%20outperform%0AGPT-4.%20We%20also%20show%20effective%20continual%20learning%20of%20tools%20via%20a%20simple%0Aexperience%20replay%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04746v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20in%20the%20Imaginarium%3A%20Tool%20Learning%20through%20Simulated%20Trial%20and%20Error&entry.906535625=Boshi%20Wang%20and%20Hao%20Fang%20and%20Jason%20Eisner%20and%20Benjamin%20Van%20Durme%20and%20Yu%20Su&entry.1292438233=%20%20Tools%20are%20essential%20for%20large%20language%20models%20%28LLMs%29%20to%20acquire%20up-to-date%0Ainformation%20and%20take%20consequential%20actions%20in%20external%20environments.%20Existing%0Awork%20on%20tool-augmented%20LLMs%20primarily%20focuses%20on%20the%20broad%20coverage%20of%20tools%0Aand%20the%20flexibility%20of%20adding%20new%20tools.%20However%2C%20a%20critical%20aspect%20that%20has%0Asurprisingly%20been%20understudied%20is%20simply%20how%20accurately%20an%20LLM%20uses%20tools%20for%0Awhich%20it%20has%20been%20trained.%20We%20find%20that%20existing%20LLMs%2C%20including%20GPT-4%20and%0Aopen-source%20LLMs%20specifically%20fine-tuned%20for%20tool%20use%2C%20only%20reach%20a%20correctness%0Arate%20in%20the%20range%20of%2030%25%20to%2060%25%2C%20far%20from%20reliable%20use%20in%20practice.%20We%20propose%0Aa%20biologically%20inspired%20method%20for%20tool-augmented%20LLMs%2C%20simulated%20trial%20and%0Aerror%20%28STE%29%2C%20that%20orchestrates%20three%20key%20mechanisms%20for%20successful%20tool%20use%0Abehaviors%20in%20the%20biological%20system%3A%20trial%20and%20error%2C%20imagination%2C%20and%20memory.%0ASpecifically%2C%20STE%20leverages%20an%20LLM%27s%20%27imagination%27%20to%20simulate%20plausible%0Ascenarios%20for%20using%20a%20tool%2C%20after%20which%20the%20LLM%20interacts%20with%20the%20tool%20to%0Alearn%20from%20its%20execution%20feedback.%20Both%20short-term%20and%20long-term%20memory%20are%0Aemployed%20to%20improve%20the%20depth%20and%20breadth%20of%20the%20exploration%2C%20respectively.%0AComprehensive%20experiments%20on%20ToolBench%20show%20that%20STE%20substantially%20improves%0Atool%20learning%20for%20LLMs%20under%20both%20in-context%20learning%20and%20fine-tuning%20settings%2C%0Abringing%20a%20boost%20of%2046.7%25%20to%20Mistral-Instruct-7B%20and%20enabling%20it%20to%20outperform%0AGPT-4.%20We%20also%20show%20effective%20continual%20learning%20of%20tools%20via%20a%20simple%0Aexperience%20replay%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04746v1&entry.124074799=Read"},
{"title": "A New Benchmark for Evaluating Automatic Speech Recognition in the\n  Arabic Call Domain", "author": "Qusai Abo Obaidah and Muhy Eddin Zater and Adnan Jaljuli and Ali Mahboub and Asma Hakouz and Bashar Alfrou and Yazan Estaitia", "abstract": "  This work is an attempt to introduce a comprehensive benchmark for Arabic\nspeech recognition, specifically tailored to address the challenges of\ntelephone conversations in Arabic language. Arabic, characterized by its rich\ndialectal diversity and phonetic complexity, presents a number of unique\nchallenges for automatic speech recognition (ASR) systems. These challenges are\nfurther amplified in the domain of telephone calls, where audio quality,\nbackground noise, and conversational speech styles negatively affect\nrecognition accuracy. Our work aims to establish a robust benchmark that not\nonly encompasses the broad spectrum of Arabic dialects but also emulates the\nreal-world conditions of call-based communications. By incorporating diverse\ndialectical expressions and accounting for the variable quality of call\nrecordings, this benchmark seeks to provide a rigorous testing ground for the\ndevelopment and evaluation of ASR systems capable of navigating the\ncomplexities of Arabic speech in telephonic contexts. This work also attempts\nto establish a baseline performance evaluation using state-of-the-art ASR\ntechnologies.\n", "link": "http://arxiv.org/abs/2403.04280v1", "date": "2024-03-07", "relevancy": 1.5359, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4073}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3853}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3601}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20New%20Benchmark%20for%20Evaluating%20Automatic%20Speech%20Recognition%20in%20the%0A%20%20Arabic%20Call%20Domain&body=Title%3A%20A%20New%20Benchmark%20for%20Evaluating%20Automatic%20Speech%20Recognition%20in%20the%0A%20%20Arabic%20Call%20Domain%0AAuthor%3A%20Qusai%20Abo%20Obaidah%20and%20Muhy%20Eddin%20Zater%20and%20Adnan%20Jaljuli%20and%20Ali%20Mahboub%20and%20Asma%20Hakouz%20and%20Bashar%20Alfrou%20and%20Yazan%20Estaitia%0AAbstract%3A%20%20%20This%20work%20is%20an%20attempt%20to%20introduce%20a%20comprehensive%20benchmark%20for%20Arabic%0Aspeech%20recognition%2C%20specifically%20tailored%20to%20address%20the%20challenges%20of%0Atelephone%20conversations%20in%20Arabic%20language.%20Arabic%2C%20characterized%20by%20its%20rich%0Adialectal%20diversity%20and%20phonetic%20complexity%2C%20presents%20a%20number%20of%20unique%0Achallenges%20for%20automatic%20speech%20recognition%20%28ASR%29%20systems.%20These%20challenges%20are%0Afurther%20amplified%20in%20the%20domain%20of%20telephone%20calls%2C%20where%20audio%20quality%2C%0Abackground%20noise%2C%20and%20conversational%20speech%20styles%20negatively%20affect%0Arecognition%20accuracy.%20Our%20work%20aims%20to%20establish%20a%20robust%20benchmark%20that%20not%0Aonly%20encompasses%20the%20broad%20spectrum%20of%20Arabic%20dialects%20but%20also%20emulates%20the%0Areal-world%20conditions%20of%20call-based%20communications.%20By%20incorporating%20diverse%0Adialectical%20expressions%20and%20accounting%20for%20the%20variable%20quality%20of%20call%0Arecordings%2C%20this%20benchmark%20seeks%20to%20provide%20a%20rigorous%20testing%20ground%20for%20the%0Adevelopment%20and%20evaluation%20of%20ASR%20systems%20capable%20of%20navigating%20the%0Acomplexities%20of%20Arabic%20speech%20in%20telephonic%20contexts.%20This%20work%20also%20attempts%0Ato%20establish%20a%20baseline%20performance%20evaluation%20using%20state-of-the-art%20ASR%0Atechnologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04280v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Benchmark%20for%20Evaluating%20Automatic%20Speech%20Recognition%20in%20the%0A%20%20Arabic%20Call%20Domain&entry.906535625=Qusai%20Abo%20Obaidah%20and%20Muhy%20Eddin%20Zater%20and%20Adnan%20Jaljuli%20and%20Ali%20Mahboub%20and%20Asma%20Hakouz%20and%20Bashar%20Alfrou%20and%20Yazan%20Estaitia&entry.1292438233=%20%20This%20work%20is%20an%20attempt%20to%20introduce%20a%20comprehensive%20benchmark%20for%20Arabic%0Aspeech%20recognition%2C%20specifically%20tailored%20to%20address%20the%20challenges%20of%0Atelephone%20conversations%20in%20Arabic%20language.%20Arabic%2C%20characterized%20by%20its%20rich%0Adialectal%20diversity%20and%20phonetic%20complexity%2C%20presents%20a%20number%20of%20unique%0Achallenges%20for%20automatic%20speech%20recognition%20%28ASR%29%20systems.%20These%20challenges%20are%0Afurther%20amplified%20in%20the%20domain%20of%20telephone%20calls%2C%20where%20audio%20quality%2C%0Abackground%20noise%2C%20and%20conversational%20speech%20styles%20negatively%20affect%0Arecognition%20accuracy.%20Our%20work%20aims%20to%20establish%20a%20robust%20benchmark%20that%20not%0Aonly%20encompasses%20the%20broad%20spectrum%20of%20Arabic%20dialects%20but%20also%20emulates%20the%0Areal-world%20conditions%20of%20call-based%20communications.%20By%20incorporating%20diverse%0Adialectical%20expressions%20and%20accounting%20for%20the%20variable%20quality%20of%20call%0Arecordings%2C%20this%20benchmark%20seeks%20to%20provide%20a%20rigorous%20testing%20ground%20for%20the%0Adevelopment%20and%20evaluation%20of%20ASR%20systems%20capable%20of%20navigating%20the%0Acomplexities%20of%20Arabic%20speech%20in%20telephonic%20contexts.%20This%20work%20also%20attempts%0Ato%20establish%20a%20baseline%20performance%20evaluation%20using%20state-of-the-art%20ASR%0Atechnologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04280v1&entry.124074799=Read"},
{"title": "Analysis of Systems' Performance in Natural Language Processing\n  Competitions", "author": "Sergio Nava-Mu\u00f1oz and Mario Graff and Hugo Jair Escalante", "abstract": "  Collaborative competitions have gained popularity in the scientific and\ntechnological fields. These competitions involve defining tasks, selecting\nevaluation scores, and devising result verification methods. In the standard\nscenario, participants receive a training set and are expected to provide a\nsolution for a held-out dataset kept by organizers. An essential challenge for\norganizers arises when comparing algorithms' performance, assessing multiple\nparticipants, and ranking them. Statistical tools are often used for this\npurpose; however, traditional statistical methods often fail to capture\ndecisive differences between systems' performance. This manuscript describes an\nevaluation methodology for statistically analyzing competition results and\ncompetition. The methodology is designed to be universally applicable; however,\nit is illustrated using eight natural language competitions as case studies\ninvolving classification and regression problems. The proposed methodology\noffers several advantages, including off-the-shell comparisons with correction\nmechanisms and the inclusion of confidence intervals. Furthermore, we introduce\nmetrics that allow organizers to assess the difficulty of competitions. Our\nanalysis shows the potential usefulness of our methodology for effectively\nevaluating competition results.\n", "link": "http://arxiv.org/abs/2403.04693v1", "date": "2024-03-07", "relevancy": 1.3284, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4543}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4438}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4378}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Systems%27%20Performance%20in%20Natural%20Language%20Processing%0A%20%20Competitions&body=Title%3A%20Analysis%20of%20Systems%27%20Performance%20in%20Natural%20Language%20Processing%0A%20%20Competitions%0AAuthor%3A%20Sergio%20Nava-Mu%C3%B1oz%20and%20Mario%20Graff%20and%20Hugo%20Jair%20Escalante%0AAbstract%3A%20%20%20Collaborative%20competitions%20have%20gained%20popularity%20in%20the%20scientific%20and%0Atechnological%20fields.%20These%20competitions%20involve%20defining%20tasks%2C%20selecting%0Aevaluation%20scores%2C%20and%20devising%20result%20verification%20methods.%20In%20the%20standard%0Ascenario%2C%20participants%20receive%20a%20training%20set%20and%20are%20expected%20to%20provide%20a%0Asolution%20for%20a%20held-out%20dataset%20kept%20by%20organizers.%20An%20essential%20challenge%20for%0Aorganizers%20arises%20when%20comparing%20algorithms%27%20performance%2C%20assessing%20multiple%0Aparticipants%2C%20and%20ranking%20them.%20Statistical%20tools%20are%20often%20used%20for%20this%0Apurpose%3B%20however%2C%20traditional%20statistical%20methods%20often%20fail%20to%20capture%0Adecisive%20differences%20between%20systems%27%20performance.%20This%20manuscript%20describes%20an%0Aevaluation%20methodology%20for%20statistically%20analyzing%20competition%20results%20and%0Acompetition.%20The%20methodology%20is%20designed%20to%20be%20universally%20applicable%3B%20however%2C%0Ait%20is%20illustrated%20using%20eight%20natural%20language%20competitions%20as%20case%20studies%0Ainvolving%20classification%20and%20regression%20problems.%20The%20proposed%20methodology%0Aoffers%20several%20advantages%2C%20including%20off-the-shell%20comparisons%20with%20correction%0Amechanisms%20and%20the%20inclusion%20of%20confidence%20intervals.%20Furthermore%2C%20we%20introduce%0Ametrics%20that%20allow%20organizers%20to%20assess%20the%20difficulty%20of%20competitions.%20Our%0Aanalysis%20shows%20the%20potential%20usefulness%20of%20our%20methodology%20for%20effectively%0Aevaluating%20competition%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04693v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Systems%27%20Performance%20in%20Natural%20Language%20Processing%0A%20%20Competitions&entry.906535625=Sergio%20Nava-Mu%C3%B1oz%20and%20Mario%20Graff%20and%20Hugo%20Jair%20Escalante&entry.1292438233=%20%20Collaborative%20competitions%20have%20gained%20popularity%20in%20the%20scientific%20and%0Atechnological%20fields.%20These%20competitions%20involve%20defining%20tasks%2C%20selecting%0Aevaluation%20scores%2C%20and%20devising%20result%20verification%20methods.%20In%20the%20standard%0Ascenario%2C%20participants%20receive%20a%20training%20set%20and%20are%20expected%20to%20provide%20a%0Asolution%20for%20a%20held-out%20dataset%20kept%20by%20organizers.%20An%20essential%20challenge%20for%0Aorganizers%20arises%20when%20comparing%20algorithms%27%20performance%2C%20assessing%20multiple%0Aparticipants%2C%20and%20ranking%20them.%20Statistical%20tools%20are%20often%20used%20for%20this%0Apurpose%3B%20however%2C%20traditional%20statistical%20methods%20often%20fail%20to%20capture%0Adecisive%20differences%20between%20systems%27%20performance.%20This%20manuscript%20describes%20an%0Aevaluation%20methodology%20for%20statistically%20analyzing%20competition%20results%20and%0Acompetition.%20The%20methodology%20is%20designed%20to%20be%20universally%20applicable%3B%20however%2C%0Ait%20is%20illustrated%20using%20eight%20natural%20language%20competitions%20as%20case%20studies%0Ainvolving%20classification%20and%20regression%20problems.%20The%20proposed%20methodology%0Aoffers%20several%20advantages%2C%20including%20off-the-shell%20comparisons%20with%20correction%0Amechanisms%20and%20the%20inclusion%20of%20confidence%20intervals.%20Furthermore%2C%20we%20introduce%0Ametrics%20that%20allow%20organizers%20to%20assess%20the%20difficulty%20of%20competitions.%20Our%0Aanalysis%20shows%20the%20potential%20usefulness%20of%20our%20methodology%20for%20effectively%0Aevaluating%20competition%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04693v1&entry.124074799=Read"},
{"title": "Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness\n  Characterization Methods for Data-Centric AI", "author": "Nabeel Seedat and Fergus Imrie and Mihaela van der Schaar", "abstract": "  Characterizing samples that are difficult to learn from is crucial to\ndeveloping highly performant ML models. This has led to numerous Hardness\nCharacterization Methods (HCMs) that aim to identify \"hard\" samples. However,\nthere is a lack of consensus regarding the definition and evaluation of\n\"hardness\". Unfortunately, current HCMs have only been evaluated on specific\ntypes of hardness and often only qualitatively or with respect to downstream\nperformance, overlooking the fundamental quantitative identification task. We\naddress this gap by presenting a fine-grained taxonomy of hardness types.\nAdditionally, we propose the Hardness Characterization Analysis Toolkit\n(H-CAT), which supports comprehensive and quantitative benchmarking of HCMs\nacross the hardness taxonomy and can easily be extended to new HCMs, hardness\ntypes, and datasets. We use H-CAT to evaluate 13 different HCMs across 8\nhardness types. This comprehensive evaluation encompassing over 14K setups\nuncovers strengths and weaknesses of different HCMs, leading to practical tips\nto guide HCM selection and future development. Our findings highlight the need\nfor more comprehensive HCM evaluation, while we hope our hardness taxonomy and\ntoolkit will advance the principled evaluation and uptake of data-centric AI\nmethods.\n", "link": "http://arxiv.org/abs/2403.04551v1", "date": "2024-03-07", "relevancy": 1.7354, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4256}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4092}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Dissecting%20Sample%20Hardness%3A%20A%20Fine-Grained%20Analysis%20of%20Hardness%0A%20%20Characterization%20Methods%20for%20Data-Centric%20AI&body=Title%3A%20Dissecting%20Sample%20Hardness%3A%20A%20Fine-Grained%20Analysis%20of%20Hardness%0A%20%20Characterization%20Methods%20for%20Data-Centric%20AI%0AAuthor%3A%20Nabeel%20Seedat%20and%20Fergus%20Imrie%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20%20%20Characterizing%20samples%20that%20are%20difficult%20to%20learn%20from%20is%20crucial%20to%0Adeveloping%20highly%20performant%20ML%20models.%20This%20has%20led%20to%20numerous%20Hardness%0ACharacterization%20Methods%20%28HCMs%29%20that%20aim%20to%20identify%20%22hard%22%20samples.%20However%2C%0Athere%20is%20a%20lack%20of%20consensus%20regarding%20the%20definition%20and%20evaluation%20of%0A%22hardness%22.%20Unfortunately%2C%20current%20HCMs%20have%20only%20been%20evaluated%20on%20specific%0Atypes%20of%20hardness%20and%20often%20only%20qualitatively%20or%20with%20respect%20to%20downstream%0Aperformance%2C%20overlooking%20the%20fundamental%20quantitative%20identification%20task.%20We%0Aaddress%20this%20gap%20by%20presenting%20a%20fine-grained%20taxonomy%20of%20hardness%20types.%0AAdditionally%2C%20we%20propose%20the%20Hardness%20Characterization%20Analysis%20Toolkit%0A%28H-CAT%29%2C%20which%20supports%20comprehensive%20and%20quantitative%20benchmarking%20of%20HCMs%0Aacross%20the%20hardness%20taxonomy%20and%20can%20easily%20be%20extended%20to%20new%20HCMs%2C%20hardness%0Atypes%2C%20and%20datasets.%20We%20use%20H-CAT%20to%20evaluate%2013%20different%20HCMs%20across%208%0Ahardness%20types.%20This%20comprehensive%20evaluation%20encompassing%20over%2014K%20setups%0Auncovers%20strengths%20and%20weaknesses%20of%20different%20HCMs%2C%20leading%20to%20practical%20tips%0Ato%20guide%20HCM%20selection%20and%20future%20development.%20Our%20findings%20highlight%20the%20need%0Afor%20more%20comprehensive%20HCM%20evaluation%2C%20while%20we%20hope%20our%20hardness%20taxonomy%20and%0Atoolkit%20will%20advance%20the%20principled%20evaluation%20and%20uptake%20of%20data-centric%20AI%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04551v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20Sample%20Hardness%3A%20A%20Fine-Grained%20Analysis%20of%20Hardness%0A%20%20Characterization%20Methods%20for%20Data-Centric%20AI&entry.906535625=Nabeel%20Seedat%20and%20Fergus%20Imrie%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=%20%20Characterizing%20samples%20that%20are%20difficult%20to%20learn%20from%20is%20crucial%20to%0Adeveloping%20highly%20performant%20ML%20models.%20This%20has%20led%20to%20numerous%20Hardness%0ACharacterization%20Methods%20%28HCMs%29%20that%20aim%20to%20identify%20%22hard%22%20samples.%20However%2C%0Athere%20is%20a%20lack%20of%20consensus%20regarding%20the%20definition%20and%20evaluation%20of%0A%22hardness%22.%20Unfortunately%2C%20current%20HCMs%20have%20only%20been%20evaluated%20on%20specific%0Atypes%20of%20hardness%20and%20often%20only%20qualitatively%20or%20with%20respect%20to%20downstream%0Aperformance%2C%20overlooking%20the%20fundamental%20quantitative%20identification%20task.%20We%0Aaddress%20this%20gap%20by%20presenting%20a%20fine-grained%20taxonomy%20of%20hardness%20types.%0AAdditionally%2C%20we%20propose%20the%20Hardness%20Characterization%20Analysis%20Toolkit%0A%28H-CAT%29%2C%20which%20supports%20comprehensive%20and%20quantitative%20benchmarking%20of%20HCMs%0Aacross%20the%20hardness%20taxonomy%20and%20can%20easily%20be%20extended%20to%20new%20HCMs%2C%20hardness%0Atypes%2C%20and%20datasets.%20We%20use%20H-CAT%20to%20evaluate%2013%20different%20HCMs%20across%208%0Ahardness%20types.%20This%20comprehensive%20evaluation%20encompassing%20over%2014K%20setups%0Auncovers%20strengths%20and%20weaknesses%20of%20different%20HCMs%2C%20leading%20to%20practical%20tips%0Ato%20guide%20HCM%20selection%20and%20future%20development.%20Our%20findings%20highlight%20the%20need%0Afor%20more%20comprehensive%20HCM%20evaluation%2C%20while%20we%20hope%20our%20hardness%20taxonomy%20and%0Atoolkit%20will%20advance%20the%20principled%20evaluation%20and%20uptake%20of%20data-centric%20AI%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04551v1&entry.124074799=Read"},
{"title": "The Shutdown Problem: Three Theorems", "author": "Elliott Thornley", "abstract": "  I explain the shutdown problem: the problem of designing artificial agents\nthat (1) shut down when a shutdown button is pressed, (2) don't try to prevent\nor cause the pressing of the shutdown button, and (3) otherwise pursue goals\ncompetently. I prove three theorems that make the difficulty precise. These\ntheorems show that agents satisfying some innocuous-seeming conditions will\noften try to prevent or cause the pressing of the shutdown button, even in\ncases where it's costly to do so. And patience trades off against\nshutdownability: the more patient an agent, the greater the costs that agent is\nwilling to incur to manipulate the shutdown button. I end by noting that these\ntheorems can guide our search for solutions.\n", "link": "http://arxiv.org/abs/2403.04471v1", "date": "2024-03-07", "relevancy": 1.4675, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4129}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3495}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3278}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20The%20Shutdown%20Problem%3A%20Three%20Theorems&body=Title%3A%20The%20Shutdown%20Problem%3A%20Three%20Theorems%0AAuthor%3A%20Elliott%20Thornley%0AAbstract%3A%20%20%20I%20explain%20the%20shutdown%20problem%3A%20the%20problem%20of%20designing%20artificial%20agents%0Athat%20%281%29%20shut%20down%20when%20a%20shutdown%20button%20is%20pressed%2C%20%282%29%20don%27t%20try%20to%20prevent%0Aor%20cause%20the%20pressing%20of%20the%20shutdown%20button%2C%20and%20%283%29%20otherwise%20pursue%20goals%0Acompetently.%20I%20prove%20three%20theorems%20that%20make%20the%20difficulty%20precise.%20These%0Atheorems%20show%20that%20agents%20satisfying%20some%20innocuous-seeming%20conditions%20will%0Aoften%20try%20to%20prevent%20or%20cause%20the%20pressing%20of%20the%20shutdown%20button%2C%20even%20in%0Acases%20where%20it%27s%20costly%20to%20do%20so.%20And%20patience%20trades%20off%20against%0Ashutdownability%3A%20the%20more%20patient%20an%20agent%2C%20the%20greater%20the%20costs%20that%20agent%20is%0Awilling%20to%20incur%20to%20manipulate%20the%20shutdown%20button.%20I%20end%20by%20noting%20that%20these%0Atheorems%20can%20guide%20our%20search%20for%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04471v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Shutdown%20Problem%3A%20Three%20Theorems&entry.906535625=Elliott%20Thornley&entry.1292438233=%20%20I%20explain%20the%20shutdown%20problem%3A%20the%20problem%20of%20designing%20artificial%20agents%0Athat%20%281%29%20shut%20down%20when%20a%20shutdown%20button%20is%20pressed%2C%20%282%29%20don%27t%20try%20to%20prevent%0Aor%20cause%20the%20pressing%20of%20the%20shutdown%20button%2C%20and%20%283%29%20otherwise%20pursue%20goals%0Acompetently.%20I%20prove%20three%20theorems%20that%20make%20the%20difficulty%20precise.%20These%0Atheorems%20show%20that%20agents%20satisfying%20some%20innocuous-seeming%20conditions%20will%0Aoften%20try%20to%20prevent%20or%20cause%20the%20pressing%20of%20the%20shutdown%20button%2C%20even%20in%0Acases%20where%20it%27s%20costly%20to%20do%20so.%20And%20patience%20trades%20off%20against%0Ashutdownability%3A%20the%20more%20patient%20an%20agent%2C%20the%20greater%20the%20costs%20that%20agent%20is%0Awilling%20to%20incur%20to%20manipulate%20the%20shutdown%20button.%20I%20end%20by%20noting%20that%20these%0Atheorems%20can%20guide%20our%20search%20for%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04471v1&entry.124074799=Read"},
{"title": "ObjectCompose: Evaluating Resilience of Vision-Based Models on\n  Object-to-Background Compositional Changes", "author": "Hashmat Shadab Malik and Muhammad Huzaifa and Muzammal Naseer and Salman Khan and Fahad Shahbaz Khan", "abstract": "  Given the large-scale multi-modal training of recent vision-based models and\ntheir generalization capabilities, understanding the extent of their robustness\nis critical for their real-world deployment. In this work, we evaluate the\nresilience of current vision-based models against diverse object-to-background\ncontext variations. The majority of robustness evaluation methods have\nintroduced synthetic datasets to induce changes to object characteristics\n(viewpoints, scale, color) or utilized image transformation techniques\n(adversarial changes, common corruptions) on real images to simulate shifts in\ndistributions. Recent works have explored leveraging large language models and\ndiffusion models to generate changes in the background. However, these methods\neither lack in offering control over the changes to be made or distort the\nobject semantics, making them unsuitable for the task. Our method, on the other\nhand, can induce diverse object-to-background changes while preserving the\noriginal semantics and appearance of the object. To achieve this goal, we\nharness the generative capabilities of text-to-image, image-to-text, and\nimage-to-segment models to automatically generate a broad spectrum of\nobject-to-background changes. We induce both natural and adversarial background\nchanges by either modifying the textual prompts or optimizing the latents and\ntextual embedding of text-to-image models. This allows us to quantify the role\nof background context in understanding the robustness and generalization of\ndeep neural networks. We produce various versions of standard vision datasets\n(ImageNet, COCO), incorporating either diverse and realistic backgrounds into\nthe images or introducing color, texture, and adversarial changes in the\nbackground. We conduct extensive experiment to analyze the robustness of\nvision-based models against object-to-background context variations across\ndiverse tasks.\n", "link": "http://arxiv.org/abs/2403.04701v1", "date": "2024-03-07", "relevancy": 2.0202, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5158}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5022}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4954}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20ObjectCompose%3A%20Evaluating%20Resilience%20of%20Vision-Based%20Models%20on%0A%20%20Object-to-Background%20Compositional%20Changes&body=Title%3A%20ObjectCompose%3A%20Evaluating%20Resilience%20of%20Vision-Based%20Models%20on%0A%20%20Object-to-Background%20Compositional%20Changes%0AAuthor%3A%20Hashmat%20Shadab%20Malik%20and%20Muhammad%20Huzaifa%20and%20Muzammal%20Naseer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Given%20the%20large-scale%20multi-modal%20training%20of%20recent%20vision-based%20models%20and%0Atheir%20generalization%20capabilities%2C%20understanding%20the%20extent%20of%20their%20robustness%0Ais%20critical%20for%20their%20real-world%20deployment.%20In%20this%20work%2C%20we%20evaluate%20the%0Aresilience%20of%20current%20vision-based%20models%20against%20diverse%20object-to-background%0Acontext%20variations.%20The%20majority%20of%20robustness%20evaluation%20methods%20have%0Aintroduced%20synthetic%20datasets%20to%20induce%20changes%20to%20object%20characteristics%0A%28viewpoints%2C%20scale%2C%20color%29%20or%20utilized%20image%20transformation%20techniques%0A%28adversarial%20changes%2C%20common%20corruptions%29%20on%20real%20images%20to%20simulate%20shifts%20in%0Adistributions.%20Recent%20works%20have%20explored%20leveraging%20large%20language%20models%20and%0Adiffusion%20models%20to%20generate%20changes%20in%20the%20background.%20However%2C%20these%20methods%0Aeither%20lack%20in%20offering%20control%20over%20the%20changes%20to%20be%20made%20or%20distort%20the%0Aobject%20semantics%2C%20making%20them%20unsuitable%20for%20the%20task.%20Our%20method%2C%20on%20the%20other%0Ahand%2C%20can%20induce%20diverse%20object-to-background%20changes%20while%20preserving%20the%0Aoriginal%20semantics%20and%20appearance%20of%20the%20object.%20To%20achieve%20this%20goal%2C%20we%0Aharness%20the%20generative%20capabilities%20of%20text-to-image%2C%20image-to-text%2C%20and%0Aimage-to-segment%20models%20to%20automatically%20generate%20a%20broad%20spectrum%20of%0Aobject-to-background%20changes.%20We%20induce%20both%20natural%20and%20adversarial%20background%0Achanges%20by%20either%20modifying%20the%20textual%20prompts%20or%20optimizing%20the%20latents%20and%0Atextual%20embedding%20of%20text-to-image%20models.%20This%20allows%20us%20to%20quantify%20the%20role%0Aof%20background%20context%20in%20understanding%20the%20robustness%20and%20generalization%20of%0Adeep%20neural%20networks.%20We%20produce%20various%20versions%20of%20standard%20vision%20datasets%0A%28ImageNet%2C%20COCO%29%2C%20incorporating%20either%20diverse%20and%20realistic%20backgrounds%20into%0Athe%20images%20or%20introducing%20color%2C%20texture%2C%20and%20adversarial%20changes%20in%20the%0Abackground.%20We%20conduct%20extensive%20experiment%20to%20analyze%20the%20robustness%20of%0Avision-based%20models%20against%20object-to-background%20context%20variations%20across%0Adiverse%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04701v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObjectCompose%3A%20Evaluating%20Resilience%20of%20Vision-Based%20Models%20on%0A%20%20Object-to-Background%20Compositional%20Changes&entry.906535625=Hashmat%20Shadab%20Malik%20and%20Muhammad%20Huzaifa%20and%20Muzammal%20Naseer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Given%20the%20large-scale%20multi-modal%20training%20of%20recent%20vision-based%20models%20and%0Atheir%20generalization%20capabilities%2C%20understanding%20the%20extent%20of%20their%20robustness%0Ais%20critical%20for%20their%20real-world%20deployment.%20In%20this%20work%2C%20we%20evaluate%20the%0Aresilience%20of%20current%20vision-based%20models%20against%20diverse%20object-to-background%0Acontext%20variations.%20The%20majority%20of%20robustness%20evaluation%20methods%20have%0Aintroduced%20synthetic%20datasets%20to%20induce%20changes%20to%20object%20characteristics%0A%28viewpoints%2C%20scale%2C%20color%29%20or%20utilized%20image%20transformation%20techniques%0A%28adversarial%20changes%2C%20common%20corruptions%29%20on%20real%20images%20to%20simulate%20shifts%20in%0Adistributions.%20Recent%20works%20have%20explored%20leveraging%20large%20language%20models%20and%0Adiffusion%20models%20to%20generate%20changes%20in%20the%20background.%20However%2C%20these%20methods%0Aeither%20lack%20in%20offering%20control%20over%20the%20changes%20to%20be%20made%20or%20distort%20the%0Aobject%20semantics%2C%20making%20them%20unsuitable%20for%20the%20task.%20Our%20method%2C%20on%20the%20other%0Ahand%2C%20can%20induce%20diverse%20object-to-background%20changes%20while%20preserving%20the%0Aoriginal%20semantics%20and%20appearance%20of%20the%20object.%20To%20achieve%20this%20goal%2C%20we%0Aharness%20the%20generative%20capabilities%20of%20text-to-image%2C%20image-to-text%2C%20and%0Aimage-to-segment%20models%20to%20automatically%20generate%20a%20broad%20spectrum%20of%0Aobject-to-background%20changes.%20We%20induce%20both%20natural%20and%20adversarial%20background%0Achanges%20by%20either%20modifying%20the%20textual%20prompts%20or%20optimizing%20the%20latents%20and%0Atextual%20embedding%20of%20text-to-image%20models.%20This%20allows%20us%20to%20quantify%20the%20role%0Aof%20background%20context%20in%20understanding%20the%20robustness%20and%20generalization%20of%0Adeep%20neural%20networks.%20We%20produce%20various%20versions%20of%20standard%20vision%20datasets%0A%28ImageNet%2C%20COCO%29%2C%20incorporating%20either%20diverse%20and%20realistic%20backgrounds%20into%0Athe%20images%20or%20introducing%20color%2C%20texture%2C%20and%20adversarial%20changes%20in%20the%0Abackground.%20We%20conduct%20extensive%20experiment%20to%20analyze%20the%20robustness%20of%0Avision-based%20models%20against%20object-to-background%20context%20variations%20across%0Adiverse%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04701v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


