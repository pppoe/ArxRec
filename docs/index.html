<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MGE: A Training-Free and Efficient Model Generation and Enhancement\n  Scheme", "author": "Xuan Wang and Zeshan Pang and Yuliang Lu and Xuehu Yan", "abstract": "  To provide a foundation for the research of deep learning models, the\nconstruction of model pool is an essential step. This paper proposes a\nTraining-Free and Efficient Model Generation and Enhancement Scheme (MGE). This\nscheme primarily considers two aspects during the model generation process: the\ndistribution of model parameters and model performance. Experiments result\nshows that generated models are comparable to models obtained through normal\ntraining, and even superior in some cases. Moreover, the time consumed in\ngenerating models accounts for only 1\\% of the time required for normal model\ntraining. More importantly, with the enhancement of Evolution-MGE, generated\nmodels exhibits competitive generalization ability in few-shot tasks. And the\nbehavioral dissimilarity of generated models has the potential of adversarial\ndefense.\n", "link": "http://arxiv.org/abs/2402.17486v1", "date": "2024-02-27", "relevancy": 2.803, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.588}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.575}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5188}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGE%3A%20A%20Training-Free%20and%20Efficient%20Model%20Generation%20and%20Enhancement%0A%20%20Scheme&entry.906535625=Xuan%20Wang%20and%20Zeshan%20Pang%20and%20Yuliang%20Lu%20and%20Xuehu%20Yan&entry.1292438233=%20%20To%20provide%20a%20foundation%20for%20the%20research%20of%20deep%20learning%20models%2C%20the%0Aconstruction%20of%20model%20pool%20is%20an%20essential%20step.%20This%20paper%20proposes%20a%0ATraining-Free%20and%20Efficient%20Model%20Generation%20and%20Enhancement%20Scheme%20%28MGE%29.%20This%0Ascheme%20primarily%20considers%20two%20aspects%20during%20the%20model%20generation%20process%3A%20the%0Adistribution%20of%20model%20parameters%20and%20model%20performance.%20Experiments%20result%0Ashows%20that%20generated%20models%20are%20comparable%20to%20models%20obtained%20through%20normal%0Atraining%2C%20and%20even%20superior%20in%20some%20cases.%20Moreover%2C%20the%20time%20consumed%20in%0Agenerating%20models%20accounts%20for%20only%201%5C%25%20of%20the%20time%20required%20for%20normal%20model%0Atraining.%20More%20importantly%2C%20with%20the%20enhancement%20of%20Evolution-MGE%2C%20generated%0Amodels%20exhibits%20competitive%20generalization%20ability%20in%20few-shot%20tasks.%20And%20the%0Abehavioral%20dissimilarity%20of%20generated%20models%20has%20the%20potential%20of%20adversarial%0Adefense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17486v1&entry.124074799=Read"},
{"title": "AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera\n  Joint Synthesis", "author": "Tao Tang and Guangrun Wang and Yixing Lao and Peng Chen and Jie Liu and Liang Lin and Kaicheng Yu and Xiaodan Liang", "abstract": "  Neural implicit fields have been a de facto standard in novel view synthesis.\nRecently, there exist some methods exploring fusing multiple modalities within\na single field, aiming to share implicit features from different modalities to\nenhance reconstruction performance. However, these modalities often exhibit\nmisaligned behaviors: optimizing for one modality, such as LiDAR, can adversely\naffect another, like camera performance, and vice versa. In this work, we\nconduct comprehensive analyses on the multimodal implicit field of LiDAR-camera\njoint synthesis, revealing the underlying issue lies in the misalignment of\ndifferent sensors. Furthermore, we introduce AlignMiF, a geometrically aligned\nmultimodal implicit field with two proposed modules: Geometry-Aware Alignment\n(GAA) and Shared Geometry Initialization (SGI). These modules effectively align\nthe coarse geometry across different modalities, significantly enhancing the\nfusion process between LiDAR and camera data. Through extensive experiments\nacross various datasets and scenes, we demonstrate the effectiveness of our\napproach in facilitating better interaction between LiDAR and camera modalities\nwithin a unified neural field. Specifically, our proposed AlignMiF, achieves\nremarkable improvement over recent implicit fusion methods (+2.01 and +3.11\nimage PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses\nsingle modality performance (13.8% and 14.2% reduction in LiDAR Chamfer\nDistance on the respective datasets).\n", "link": "http://arxiv.org/abs/2402.17483v1", "date": "2024-02-27", "relevancy": 2.7732, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5683}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.547}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignMiF%3A%20Geometry-Aligned%20Multimodal%20Implicit%20Field%20for%20LiDAR-Camera%0A%20%20Joint%20Synthesis&entry.906535625=Tao%20Tang%20and%20Guangrun%20Wang%20and%20Yixing%20Lao%20and%20Peng%20Chen%20and%20Jie%20Liu%20and%20Liang%20Lin%20and%20Kaicheng%20Yu%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Neural%20implicit%20fields%20have%20been%20a%20de%20facto%20standard%20in%20novel%20view%20synthesis.%0ARecently%2C%20there%20exist%20some%20methods%20exploring%20fusing%20multiple%20modalities%20within%0Aa%20single%20field%2C%20aiming%20to%20share%20implicit%20features%20from%20different%20modalities%20to%0Aenhance%20reconstruction%20performance.%20However%2C%20these%20modalities%20often%20exhibit%0Amisaligned%20behaviors%3A%20optimizing%20for%20one%20modality%2C%20such%20as%20LiDAR%2C%20can%20adversely%0Aaffect%20another%2C%20like%20camera%20performance%2C%20and%20vice%20versa.%20In%20this%20work%2C%20we%0Aconduct%20comprehensive%20analyses%20on%20the%20multimodal%20implicit%20field%20of%20LiDAR-camera%0Ajoint%20synthesis%2C%20revealing%20the%20underlying%20issue%20lies%20in%20the%20misalignment%20of%0Adifferent%20sensors.%20Furthermore%2C%20we%20introduce%20AlignMiF%2C%20a%20geometrically%20aligned%0Amultimodal%20implicit%20field%20with%20two%20proposed%20modules%3A%20Geometry-Aware%20Alignment%0A%28GAA%29%20and%20Shared%20Geometry%20Initialization%20%28SGI%29.%20These%20modules%20effectively%20align%0Athe%20coarse%20geometry%20across%20different%20modalities%2C%20significantly%20enhancing%20the%0Afusion%20process%20between%20LiDAR%20and%20camera%20data.%20Through%20extensive%20experiments%0Aacross%20various%20datasets%20and%20scenes%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20in%20facilitating%20better%20interaction%20between%20LiDAR%20and%20camera%20modalities%0Awithin%20a%20unified%20neural%20field.%20Specifically%2C%20our%20proposed%20AlignMiF%2C%20achieves%0Aremarkable%20improvement%20over%20recent%20implicit%20fusion%20methods%20%28%2B2.01%20and%20%2B3.11%0Aimage%20PSNR%20on%20the%20KITTI-360%20and%20Waymo%20datasets%29%20and%20consistently%20surpasses%0Asingle%20modality%20performance%20%2813.8%25%20and%2014.2%25%20reduction%20in%20LiDAR%20Chamfer%0ADistance%20on%20the%20respective%20datasets%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17483v1&entry.124074799=Read"},
{"title": "Robust Unsupervised Crowd Counting and Localization with Adaptive\n  Resolution SAM", "author": "Jia Wan and Qiangqiang Wu and Wei Lin and Antoni B. Chan", "abstract": "  The existing crowd counting models require extensive training data, which is\ntime-consuming to annotate. To tackle this issue, we propose a simple yet\neffective crowd counting method by utilizing the Segment-Everything-Everywhere\nModel (SEEM), an adaptation of the Segmentation Anything Model (SAM), to\ngenerate pseudo-labels for training crowd counting models. However, our initial\ninvestigation reveals that SEEM's performance in dense crowd scenes is limited,\nprimarily due to the omission of many persons in high-density areas. To\novercome this limitation, we propose an adaptive resolution SEEM to handle the\nscale variations, occlusions, and overlapping of people within crowd scenes.\nAlongside this, we introduce a robust localization method, based on Gaussian\nMixture Models, for predicting the head positions in the predicted people\nmasks. Given the mask and point pseudo-labels, we propose a robust loss\nfunction, which is designed to exclude uncertain regions based on SEEM's\npredictions, thereby enhancing the training process of the counting networks.\nFinally, we propose an iterative method for generating pseudo-labels. This\nmethod aims at improving the quality of the segmentation masks by identifying\nmore tiny persons in high-density regions, which are often missed in the first\npseudo-labeling stage. Overall, our proposed method achieves the best\nunsupervised performance in crowd counting, while also being comparable results\nto some supervised methods. This makes it a highly effective and versatile tool\nfor crowd counting, especially in situations where labeled data is not\navailable.\n", "link": "http://arxiv.org/abs/2402.17514v1", "date": "2024-02-27", "relevancy": 2.7605, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5737}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5512}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5314}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Unsupervised%20Crowd%20Counting%20and%20Localization%20with%20Adaptive%0A%20%20Resolution%20SAM&entry.906535625=Jia%20Wan%20and%20Qiangqiang%20Wu%20and%20Wei%20Lin%20and%20Antoni%20B.%20Chan&entry.1292438233=%20%20The%20existing%20crowd%20counting%20models%20require%20extensive%20training%20data%2C%20which%20is%0Atime-consuming%20to%20annotate.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20crowd%20counting%20method%20by%20utilizing%20the%20Segment-Everything-Everywhere%0AModel%20%28SEEM%29%2C%20an%20adaptation%20of%20the%20Segmentation%20Anything%20Model%20%28SAM%29%2C%20to%0Agenerate%20pseudo-labels%20for%20training%20crowd%20counting%20models.%20However%2C%20our%20initial%0Ainvestigation%20reveals%20that%20SEEM%27s%20performance%20in%20dense%20crowd%20scenes%20is%20limited%2C%0Aprimarily%20due%20to%20the%20omission%20of%20many%20persons%20in%20high-density%20areas.%20To%0Aovercome%20this%20limitation%2C%20we%20propose%20an%20adaptive%20resolution%20SEEM%20to%20handle%20the%0Ascale%20variations%2C%20occlusions%2C%20and%20overlapping%20of%20people%20within%20crowd%20scenes.%0AAlongside%20this%2C%20we%20introduce%20a%20robust%20localization%20method%2C%20based%20on%20Gaussian%0AMixture%20Models%2C%20for%20predicting%20the%20head%20positions%20in%20the%20predicted%20people%0Amasks.%20Given%20the%20mask%20and%20point%20pseudo-labels%2C%20we%20propose%20a%20robust%20loss%0Afunction%2C%20which%20is%20designed%20to%20exclude%20uncertain%20regions%20based%20on%20SEEM%27s%0Apredictions%2C%20thereby%20enhancing%20the%20training%20process%20of%20the%20counting%20networks.%0AFinally%2C%20we%20propose%20an%20iterative%20method%20for%20generating%20pseudo-labels.%20This%0Amethod%20aims%20at%20improving%20the%20quality%20of%20the%20segmentation%20masks%20by%20identifying%0Amore%20tiny%20persons%20in%20high-density%20regions%2C%20which%20are%20often%20missed%20in%20the%20first%0Apseudo-labeling%20stage.%20Overall%2C%20our%20proposed%20method%20achieves%20the%20best%0Aunsupervised%20performance%20in%20crowd%20counting%2C%20while%20also%20being%20comparable%20results%0Ato%20some%20supervised%20methods.%20This%20makes%20it%20a%20highly%20effective%20and%20versatile%20tool%0Afor%20crowd%20counting%2C%20especially%20in%20situations%20where%20labeled%20data%20is%20not%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17514v1&entry.124074799=Read"},
{"title": "Leveraging Enhanced Queries of Point Sets for Vectorized Map\n  Construction", "author": "Zihao Liu and Xiaoyu Zhang and Guangwei Liu and Ji Zhao and Ningyi Xu", "abstract": "  In autonomous driving, the high-definition (HD) map plays a crucial role in\nlocalization and planning. Recently, several methods have facilitated\nend-to-end online map construction in DETR-like frameworks. However, little\nattention has been paid to the potential capabilities of exploring the query\nmechanism. This paper introduces MapQR, an end-to-end method with an emphasis\non enhancing query capabilities for constructing online vectorized maps.\nAlthough the map construction is essentially a point set prediction task, MapQR\nutilizes instance queries rather than point queries. These instance queries are\nscattered for the prediction of point sets and subsequently gathered for the\nfinal matching. This query design, called the scatter-and-gather query, shares\ncontent information in the same map element and avoids possible inconsistency\nof content information in point queries. We further exploit prior information\nto enhance an instance query by adding positional information embedded from\ntheir reference points. Together with a simple and effective improvement of a\nBEV encoder, the proposed MapQR achieves the best mean average precision (mAP)\nand maintains good efficiency on both nuScenes and Argoverse 2. In addition,\nintegrating our query design into other models can boost their performance\nsignificantly. The code will be available at https://github.com/HXMap/MapQR.\n", "link": "http://arxiv.org/abs/2402.17430v1", "date": "2024-02-27", "relevancy": 2.7504, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5804}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5727}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4971}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Enhanced%20Queries%20of%20Point%20Sets%20for%20Vectorized%20Map%0A%20%20Construction&entry.906535625=Zihao%20Liu%20and%20Xiaoyu%20Zhang%20and%20Guangwei%20Liu%20and%20Ji%20Zhao%20and%20Ningyi%20Xu&entry.1292438233=%20%20In%20autonomous%20driving%2C%20the%20high-definition%20%28HD%29%20map%20plays%20a%20crucial%20role%20in%0Alocalization%20and%20planning.%20Recently%2C%20several%20methods%20have%20facilitated%0Aend-to-end%20online%20map%20construction%20in%20DETR-like%20frameworks.%20However%2C%20little%0Aattention%20has%20been%20paid%20to%20the%20potential%20capabilities%20of%20exploring%20the%20query%0Amechanism.%20This%20paper%20introduces%20MapQR%2C%20an%20end-to-end%20method%20with%20an%20emphasis%0Aon%20enhancing%20query%20capabilities%20for%20constructing%20online%20vectorized%20maps.%0AAlthough%20the%20map%20construction%20is%20essentially%20a%20point%20set%20prediction%20task%2C%20MapQR%0Autilizes%20instance%20queries%20rather%20than%20point%20queries.%20These%20instance%20queries%20are%0Ascattered%20for%20the%20prediction%20of%20point%20sets%20and%20subsequently%20gathered%20for%20the%0Afinal%20matching.%20This%20query%20design%2C%20called%20the%20scatter-and-gather%20query%2C%20shares%0Acontent%20information%20in%20the%20same%20map%20element%20and%20avoids%20possible%20inconsistency%0Aof%20content%20information%20in%20point%20queries.%20We%20further%20exploit%20prior%20information%0Ato%20enhance%20an%20instance%20query%20by%20adding%20positional%20information%20embedded%20from%0Atheir%20reference%20points.%20Together%20with%20a%20simple%20and%20effective%20improvement%20of%20a%0ABEV%20encoder%2C%20the%20proposed%20MapQR%20achieves%20the%20best%20mean%20average%20precision%20%28mAP%29%0Aand%20maintains%20good%20efficiency%20on%20both%20nuScenes%20and%20Argoverse%202.%20In%20addition%2C%0Aintegrating%20our%20query%20design%20into%20other%20models%20can%20boost%20their%20performance%0Asignificantly.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/HXMap/MapQR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17430v1&entry.124074799=Read"},
{"title": "Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing", "author": "Bi'an Du and Xiang Gao and Wei Hu and Renjie Liao", "abstract": "  Generative 3D part assembly involves understanding part relationships and\npredicting their 6-DoF poses for assembling a realistic 3D shape. Prior work\noften focus on the geometry of individual parts, neglecting part-whole\nhierarchies of objects. Leveraging two key observations: 1) super-part poses\nprovide strong hints about part poses, and 2) predicting super-part poses is\neasier due to fewer superparts, we propose a part-whole-hierarchy message\npassing network for efficient 3D part assembly. We first introduce super-parts\nby grouping geometrically similar parts without any semantic labels. Then we\nemploy a part-whole hierarchical encoder, wherein a super-part encoder predicts\nlatent super-part poses based on input parts. Subsequently, we transform the\npoint cloud using the latent poses, feeding it to the part encoder for\naggregating super-part information and reasoning about part relationships to\npredict all part poses. In training, only ground-truth part poses are required.\nDuring inference, the predicted latent poses of super-parts enhance\ninterpretability. Experimental results on the PartNet dataset show that our\nmethod achieves state-of-the-art performance in part and connectivity accuracy\nand enables an interpretable hierarchical part assembly.\n", "link": "http://arxiv.org/abs/2402.17464v1", "date": "2024-02-27", "relevancy": 2.7312, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5533}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5441}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5413}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%203D%20Part%20Assembly%20via%20Part-Whole-Hierarchy%20Message%20Passing&entry.906535625=Bi%27an%20Du%20and%20Xiang%20Gao%20and%20Wei%20Hu%20and%20Renjie%20Liao&entry.1292438233=%20%20Generative%203D%20part%20assembly%20involves%20understanding%20part%20relationships%20and%0Apredicting%20their%206-DoF%20poses%20for%20assembling%20a%20realistic%203D%20shape.%20Prior%20work%0Aoften%20focus%20on%20the%20geometry%20of%20individual%20parts%2C%20neglecting%20part-whole%0Ahierarchies%20of%20objects.%20Leveraging%20two%20key%20observations%3A%201%29%20super-part%20poses%0Aprovide%20strong%20hints%20about%20part%20poses%2C%20and%202%29%20predicting%20super-part%20poses%20is%0Aeasier%20due%20to%20fewer%20superparts%2C%20we%20propose%20a%20part-whole-hierarchy%20message%0Apassing%20network%20for%20efficient%203D%20part%20assembly.%20We%20first%20introduce%20super-parts%0Aby%20grouping%20geometrically%20similar%20parts%20without%20any%20semantic%20labels.%20Then%20we%0Aemploy%20a%20part-whole%20hierarchical%20encoder%2C%20wherein%20a%20super-part%20encoder%20predicts%0Alatent%20super-part%20poses%20based%20on%20input%20parts.%20Subsequently%2C%20we%20transform%20the%0Apoint%20cloud%20using%20the%20latent%20poses%2C%20feeding%20it%20to%20the%20part%20encoder%20for%0Aaggregating%20super-part%20information%20and%20reasoning%20about%20part%20relationships%20to%0Apredict%20all%20part%20poses.%20In%20training%2C%20only%20ground-truth%20part%20poses%20are%20required.%0ADuring%20inference%2C%20the%20predicted%20latent%20poses%20of%20super-parts%20enhance%0Ainterpretability.%20Experimental%20results%20on%20the%20PartNet%20dataset%20show%20that%20our%0Amethod%20achieves%20state-of-the-art%20performance%20in%20part%20and%20connectivity%20accuracy%0Aand%20enables%20an%20interpretable%20hierarchical%20part%20assembly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17464v1&entry.124074799=Read"},
{"title": "EFLNet: Enhancing Feature Learning for Infrared Small Target Detection", "author": "Bo Yang and Xinyu Zhang and Jian Zhang and Jun Luo and Mingliang Zhou and Yangjun Pi", "abstract": "  Single-frame infrared small target detection is considered to be a\nchallenging task, due to the extreme imbalance between target and background,\nbounding box regression is extremely sensitive to infrared small target, and\ntarget information is easy to lose in the high-level semantic layer. In this\narticle, we propose an enhancing feature learning network (EFLNet) to address\nthese problems. First, we notice that there is an extremely imbalance between\nthe target and the background in the infrared image, which makes the model pay\nmore attention to the background features rather than target features. To\naddress this problem, we propose a new adaptive threshold focal loss (ATFL)\nfunction that decouples the target and the background, and utilizes the\nadaptive mechanism to adjust the loss weight to force the model to allocate\nmore attention to target features. Second, we introduce the normalized Gaussian\nWasserstein distance (NWD) to alleviate the difficulty of convergence caused by\nthe extreme sensitivity of the bounding box regression to infrared small\ntarget. Finally, we incorporate a dynamic head mechanism into the network to\nenable adaptive learning of the relative importance of each semantic layer.\nExperimental results demonstrate our method can achieve better performance in\nthe detection performance of infrared small target compared to the\nstate-of-the-art (SOTA) deep-learning-based methods. The source codes and\nbounding box annotated datasets are available at\nhttps://github.com/YangBo0411/infrared-small-target.\n", "link": "http://arxiv.org/abs/2307.14723v2", "date": "2024-02-27", "relevancy": 2.7138, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5773}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5285}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5225}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EFLNet%3A%20Enhancing%20Feature%20Learning%20for%20Infrared%20Small%20Target%20Detection&entry.906535625=Bo%20Yang%20and%20Xinyu%20Zhang%20and%20Jian%20Zhang%20and%20Jun%20Luo%20and%20Mingliang%20Zhou%20and%20Yangjun%20Pi&entry.1292438233=%20%20Single-frame%20infrared%20small%20target%20detection%20is%20considered%20to%20be%20a%0Achallenging%20task%2C%20due%20to%20the%20extreme%20imbalance%20between%20target%20and%20background%2C%0Abounding%20box%20regression%20is%20extremely%20sensitive%20to%20infrared%20small%20target%2C%20and%0Atarget%20information%20is%20easy%20to%20lose%20in%20the%20high-level%20semantic%20layer.%20In%20this%0Aarticle%2C%20we%20propose%20an%20enhancing%20feature%20learning%20network%20%28EFLNet%29%20to%20address%0Athese%20problems.%20First%2C%20we%20notice%20that%20there%20is%20an%20extremely%20imbalance%20between%0Athe%20target%20and%20the%20background%20in%20the%20infrared%20image%2C%20which%20makes%20the%20model%20pay%0Amore%20attention%20to%20the%20background%20features%20rather%20than%20target%20features.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20a%20new%20adaptive%20threshold%20focal%20loss%20%28ATFL%29%0Afunction%20that%20decouples%20the%20target%20and%20the%20background%2C%20and%20utilizes%20the%0Aadaptive%20mechanism%20to%20adjust%20the%20loss%20weight%20to%20force%20the%20model%20to%20allocate%0Amore%20attention%20to%20target%20features.%20Second%2C%20we%20introduce%20the%20normalized%20Gaussian%0AWasserstein%20distance%20%28NWD%29%20to%20alleviate%20the%20difficulty%20of%20convergence%20caused%20by%0Athe%20extreme%20sensitivity%20of%20the%20bounding%20box%20regression%20to%20infrared%20small%0Atarget.%20Finally%2C%20we%20incorporate%20a%20dynamic%20head%20mechanism%20into%20the%20network%20to%0Aenable%20adaptive%20learning%20of%20the%20relative%20importance%20of%20each%20semantic%20layer.%0AExperimental%20results%20demonstrate%20our%20method%20can%20achieve%20better%20performance%20in%0Athe%20detection%20performance%20of%20infrared%20small%20target%20compared%20to%20the%0Astate-of-the-art%20%28SOTA%29%20deep-learning-based%20methods.%20The%20source%20codes%20and%0Abounding%20box%20annotated%20datasets%20are%20available%20at%0Ahttps%3A//github.com/YangBo0411/infrared-small-target.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.14723v2&entry.124074799=Read"},
{"title": "Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning", "author": "Xiaoyu Zhang and Matthew Chang and Pranav Kumar and Saurabh Gupta", "abstract": "  A common failure mode for policies trained with imitation is compounding\nexecution errors at test time. When the learned policy encounters states that\nwere not present in the expert demonstrations, the policy fails, leading to\ndegenerate behavior. The Dataset Aggregation, or DAgger approach to this\nproblem simply collects more data to cover these failure states. However, in\npractice, this is often prohibitively expensive. In this work, we propose\nDiffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without\nthe cost for eye-in-hand imitation learning problems. Instead of collecting new\nsamples to cover out-of-distribution states, DMD uses recent advances in\ndiffusion models to create these samples with diffusion models. This leads to\nrobust performance from few demonstrations. In experiments conducted for\nnon-prehensile pushing on a Franka Research 3, we show that DMD can achieve a\nsuccess rate of 80% with as few as 8 expert demonstrations, where naive\nbehavior cloning reaches only 20%. DMD also outperform competing NeRF-based\naugmentation schemes by 50%.\n", "link": "http://arxiv.org/abs/2402.17768v1", "date": "2024-02-27", "relevancy": 2.6919, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5648}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5314}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.519}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Meets%20DAgger%3A%20Supercharging%20Eye-in-hand%20Imitation%20Learning&entry.906535625=Xiaoyu%20Zhang%20and%20Matthew%20Chang%20and%20Pranav%20Kumar%20and%20Saurabh%20Gupta&entry.1292438233=%20%20A%20common%20failure%20mode%20for%20policies%20trained%20with%20imitation%20is%20compounding%0Aexecution%20errors%20at%20test%20time.%20When%20the%20learned%20policy%20encounters%20states%20that%0Awere%20not%20present%20in%20the%20expert%20demonstrations%2C%20the%20policy%20fails%2C%20leading%20to%0Adegenerate%20behavior.%20The%20Dataset%20Aggregation%2C%20or%20DAgger%20approach%20to%20this%0Aproblem%20simply%20collects%20more%20data%20to%20cover%20these%20failure%20states.%20However%2C%20in%0Apractice%2C%20this%20is%20often%20prohibitively%20expensive.%20In%20this%20work%2C%20we%20propose%0ADiffusion%20Meets%20DAgger%20%28DMD%29%2C%20a%20method%20to%20reap%20the%20benefits%20of%20DAgger%20without%0Athe%20cost%20for%20eye-in-hand%20imitation%20learning%20problems.%20Instead%20of%20collecting%20new%0Asamples%20to%20cover%20out-of-distribution%20states%2C%20DMD%20uses%20recent%20advances%20in%0Adiffusion%20models%20to%20create%20these%20samples%20with%20diffusion%20models.%20This%20leads%20to%0Arobust%20performance%20from%20few%20demonstrations.%20In%20experiments%20conducted%20for%0Anon-prehensile%20pushing%20on%20a%20Franka%20Research%203%2C%20we%20show%20that%20DMD%20can%20achieve%20a%0Asuccess%20rate%20of%2080%25%20with%20as%20few%20as%208%20expert%20demonstrations%2C%20where%20naive%0Abehavior%20cloning%20reaches%20only%2020%25.%20DMD%20also%20outperform%20competing%20NeRF-based%0Aaugmentation%20schemes%20by%2050%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17768v1&entry.124074799=Read"},
{"title": "AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud\n  Analysis", "author": "Hongcheng Yang and Dingkang Liang and Dingyuan Zhang and Xingyu Jiang and Zhe Liu and Zhikang Zou and Yingying Zhu", "abstract": "  Efficient downsampling plays a crucial role in point cloud learning,\nparticularly for large-scale 3D scenes. Existing downsampling methods either\nrequire a huge computational burden or sacrifice fine-grained geometric\ninformation. This paper presents an advanced sampler that achieves both high\naccuracy and efficiency. The proposed method utilizes voxel-based sampling as a\nfoundation, but effectively addresses the challenges regarding voxel size\ndetermination and the preservation of critical geometric cues. Specifically, we\npropose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the\nreference of point-based downsampling ratio. This ensures the sampling results\nexhibit a favorable distribution for comprehending various 3D objects or\nscenes. Additionally, we introduce a network compatible with arbitrary voxel\nsizes for sampling and feature extraction while maintaining high efficiency.\nOur method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet\nbenchmarks with promising efficiency. Code will be available at\nhttps://github.com/yhc2021/AVS-Net.\n", "link": "http://arxiv.org/abs/2402.17521v1", "date": "2024-02-27", "relevancy": 2.6671, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5456}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5437}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.511}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AVS-Net%3A%20Point%20Sampling%20with%20Adaptive%20Voxel%20Size%20for%203D%20Point%20Cloud%0A%20%20Analysis&entry.906535625=Hongcheng%20Yang%20and%20Dingkang%20Liang%20and%20Dingyuan%20Zhang%20and%20Xingyu%20Jiang%20and%20Zhe%20Liu%20and%20Zhikang%20Zou%20and%20Yingying%20Zhu&entry.1292438233=%20%20Efficient%20downsampling%20plays%20a%20crucial%20role%20in%20point%20cloud%20learning%2C%0Aparticularly%20for%20large-scale%203D%20scenes.%20Existing%20downsampling%20methods%20either%0Arequire%20a%20huge%20computational%20burden%20or%20sacrifice%20fine-grained%20geometric%0Ainformation.%20This%20paper%20presents%20an%20advanced%20sampler%20that%20achieves%20both%20high%0Aaccuracy%20and%20efficiency.%20The%20proposed%20method%20utilizes%20voxel-based%20sampling%20as%20a%0Afoundation%2C%20but%20effectively%20addresses%20the%20challenges%20regarding%20voxel%20size%0Adetermination%20and%20the%20preservation%20of%20critical%20geometric%20cues.%20Specifically%2C%20we%0Apropose%20a%20Voxel%20Adaptation%20Module%20that%20adaptively%20adjusts%20voxel%20sizes%20with%20the%0Areference%20of%20point-based%20downsampling%20ratio.%20This%20ensures%20the%20sampling%20results%0Aexhibit%20a%20favorable%20distribution%20for%20comprehending%20various%203D%20objects%20or%0Ascenes.%20Additionally%2C%20we%20introduce%20a%20network%20compatible%20with%20arbitrary%20voxel%0Asizes%20for%20sampling%20and%20feature%20extraction%20while%20maintaining%20high%20efficiency.%0AOur%20method%20achieves%20state-of-the-art%20accuracy%20on%20the%20ShapeNetPart%20and%20ScanNet%0Abenchmarks%20with%20promising%20efficiency.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/yhc2021/AVS-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17521v1&entry.124074799=Read"},
{"title": "Adaptive Perturbation for Adversarial Attack", "author": "Zheng Yuan and Jie Zhang and Zhaoyan Jiang and Liangliang Li and Shiguang Shan", "abstract": "  In recent years, the security of deep learning models achieves more and more\nattentions with the rapid development of neural networks, which are vulnerable\nto adversarial examples. Almost all existing gradient-based attack methods use\nthe sign function in the generation to meet the requirement of perturbation\nbudget on $L_\\infty$ norm. However, we find that the sign function may be\nimproper for generating adversarial examples since it modifies the exact\ngradient direction. Instead of using the sign function, we propose to directly\nutilize the exact gradient direction with a scaling factor for generating\nadversarial perturbations, which improves the attack success rates of\nadversarial examples even with fewer perturbations. At the same time, we also\ntheoretically prove that this method can achieve better black-box\ntransferability. Moreover, considering that the best scaling factor varies\nacross different images, we propose an adaptive scaling factor generator to\nseek an appropriate scaling factor for each image, which avoids the\ncomputational cost for manually searching the scaling factor. Our method can be\nintegrated with almost all existing gradient-based attack methods to further\nimprove their attack success rates. Extensive experiments on the CIFAR10 and\nImageNet datasets show that our method exhibits higher transferability and\noutperforms the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2111.13841v3", "date": "2024-02-27", "relevancy": 2.6024, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5351}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5145}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5118}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Perturbation%20for%20Adversarial%20Attack&entry.906535625=Zheng%20Yuan%20and%20Jie%20Zhang%20and%20Zhaoyan%20Jiang%20and%20Liangliang%20Li%20and%20Shiguang%20Shan&entry.1292438233=%20%20In%20recent%20years%2C%20the%20security%20of%20deep%20learning%20models%20achieves%20more%20and%20more%0Aattentions%20with%20the%20rapid%20development%20of%20neural%20networks%2C%20which%20are%20vulnerable%0Ato%20adversarial%20examples.%20Almost%20all%20existing%20gradient-based%20attack%20methods%20use%0Athe%20sign%20function%20in%20the%20generation%20to%20meet%20the%20requirement%20of%20perturbation%0Abudget%20on%20%24L_%5Cinfty%24%20norm.%20However%2C%20we%20find%20that%20the%20sign%20function%20may%20be%0Aimproper%20for%20generating%20adversarial%20examples%20since%20it%20modifies%20the%20exact%0Agradient%20direction.%20Instead%20of%20using%20the%20sign%20function%2C%20we%20propose%20to%20directly%0Autilize%20the%20exact%20gradient%20direction%20with%20a%20scaling%20factor%20for%20generating%0Aadversarial%20perturbations%2C%20which%20improves%20the%20attack%20success%20rates%20of%0Aadversarial%20examples%20even%20with%20fewer%20perturbations.%20At%20the%20same%20time%2C%20we%20also%0Atheoretically%20prove%20that%20this%20method%20can%20achieve%20better%20black-box%0Atransferability.%20Moreover%2C%20considering%20that%20the%20best%20scaling%20factor%20varies%0Aacross%20different%20images%2C%20we%20propose%20an%20adaptive%20scaling%20factor%20generator%20to%0Aseek%20an%20appropriate%20scaling%20factor%20for%20each%20image%2C%20which%20avoids%20the%0Acomputational%20cost%20for%20manually%20searching%20the%20scaling%20factor.%20Our%20method%20can%20be%0Aintegrated%20with%20almost%20all%20existing%20gradient-based%20attack%20methods%20to%20further%0Aimprove%20their%20attack%20success%20rates.%20Extensive%20experiments%20on%20the%20CIFAR10%20and%0AImageNet%20datasets%20show%20that%20our%20method%20exhibits%20higher%20transferability%20and%0Aoutperforms%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2111.13841v3&entry.124074799=Read"},
{"title": "An Efficient MLP-based Point-guided Segmentation Network for Ore Images\n  with Ambiguous Boundary", "author": "Guodong Sun and Yuting Peng and Le Cheng and Mengya Xu and An Wang and Bo Wu and Hongliang Ren and Yang Zhang", "abstract": "  The precise segmentation of ore images is critical to the successful\nexecution of the beneficiation process. Due to the homogeneous appearance of\nthe ores, which leads to low contrast and unclear boundaries, accurate\nsegmentation becomes challenging, and recognition becomes problematic. This\npaper proposes a lightweight framework based on Multi-Layer Perceptron (MLP),\nwhich focuses on solving the problem of edge burring. Specifically, we\nintroduce a lightweight backbone better suited for efficiently extracting\nlow-level features. Besides, we design a feature pyramid network consisting of\ntwo MLP structures that balance local and global information thus enhancing\ndetection accuracy. Furthermore, we propose a novel loss function that guides\nthe prediction points to match the instance edge points to achieve clear object\nboundaries. We have conducted extensive experiments to validate the efficacy of\nour proposed method. Our approach achieves a remarkable processing speed of\nover 27 frames per second (FPS) with a model size of only 73 MB. Moreover, our\nmethod delivers a consistently high level of accuracy, with impressive\nperformance scores of 60.4 and 48.9 in~$AP_{50}^{box}$ and~$AP_{50}^{mask}$\nrespectively, as compared to the currently available state-of-the-art\ntechniques, when tested on the ore image dataset. The source code will be\nreleased at \\url{https://github.com/MVME-HBUT/ORENEXT}.\n", "link": "http://arxiv.org/abs/2402.17370v1", "date": "2024-02-27", "relevancy": 2.5772, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.539}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5075}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4998}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20MLP-based%20Point-guided%20Segmentation%20Network%20for%20Ore%20Images%0A%20%20with%20Ambiguous%20Boundary&entry.906535625=Guodong%20Sun%20and%20Yuting%20Peng%20and%20Le%20Cheng%20and%20Mengya%20Xu%20and%20An%20Wang%20and%20Bo%20Wu%20and%20Hongliang%20Ren%20and%20Yang%20Zhang&entry.1292438233=%20%20The%20precise%20segmentation%20of%20ore%20images%20is%20critical%20to%20the%20successful%0Aexecution%20of%20the%20beneficiation%20process.%20Due%20to%20the%20homogeneous%20appearance%20of%0Athe%20ores%2C%20which%20leads%20to%20low%20contrast%20and%20unclear%20boundaries%2C%20accurate%0Asegmentation%20becomes%20challenging%2C%20and%20recognition%20becomes%20problematic.%20This%0Apaper%20proposes%20a%20lightweight%20framework%20based%20on%20Multi-Layer%20Perceptron%20%28MLP%29%2C%0Awhich%20focuses%20on%20solving%20the%20problem%20of%20edge%20burring.%20Specifically%2C%20we%0Aintroduce%20a%20lightweight%20backbone%20better%20suited%20for%20efficiently%20extracting%0Alow-level%20features.%20Besides%2C%20we%20design%20a%20feature%20pyramid%20network%20consisting%20of%0Atwo%20MLP%20structures%20that%20balance%20local%20and%20global%20information%20thus%20enhancing%0Adetection%20accuracy.%20Furthermore%2C%20we%20propose%20a%20novel%20loss%20function%20that%20guides%0Athe%20prediction%20points%20to%20match%20the%20instance%20edge%20points%20to%20achieve%20clear%20object%0Aboundaries.%20We%20have%20conducted%20extensive%20experiments%20to%20validate%20the%20efficacy%20of%0Aour%20proposed%20method.%20Our%20approach%20achieves%20a%20remarkable%20processing%20speed%20of%0Aover%2027%20frames%20per%20second%20%28FPS%29%20with%20a%20model%20size%20of%20only%2073%20MB.%20Moreover%2C%20our%0Amethod%20delivers%20a%20consistently%20high%20level%20of%20accuracy%2C%20with%20impressive%0Aperformance%20scores%20of%2060.4%20and%2048.9%20in~%24AP_%7B50%7D%5E%7Bbox%7D%24%20and~%24AP_%7B50%7D%5E%7Bmask%7D%24%0Arespectively%2C%20as%20compared%20to%20the%20currently%20available%20state-of-the-art%0Atechniques%2C%20when%20tested%20on%20the%20ore%20image%20dataset.%20The%20source%20code%20will%20be%0Areleased%20at%20%5Curl%7Bhttps%3A//github.com/MVME-HBUT/ORENEXT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17370v1&entry.124074799=Read"},
{"title": "Removal and Selection: Improving RGB-Infrared Object Detection via\n  Coarse-to-Fine Fusion", "author": "Tianyi Zhao and Maoxun Yuan and Xingxing Wei", "abstract": "  Object detection in visible (RGB) and infrared (IR) images has been widely\napplied in recent years. Leveraging the complementary characteristics of RGB\nand IR images, the object detector provides reliable and robust object\nlocalization from day to night. Existing fusion strategies directly inject RGB\nand IR images into convolution neural networks, leading to inferior detection\nperformance. Since the RGB and IR features have modality-specific noise, these\nstrategies will worsen the fused features along with the propagation. Inspired\nby the mechanism of human brain processing multimodal information, this work\nintroduces a new coarse-to-fine perspective to purify and fuse two modality\nfeatures. Specifically, following this perspective, we design a Redundant\nSpectrum Removal module to coarsely remove interfering information within each\nmodality and a Dynamic Feature Selection module to finely select the desired\nfeatures for feature fusion. To verify the effectiveness of the coarse-to-fine\nfusion strategy, we construct a new object detector called Removal and\nSelection Detector (RSDet). Extensive experiments on three RGB-IR object\ndetection datasets verify the superior performance of our method.\n", "link": "http://arxiv.org/abs/2401.10731v2", "date": "2024-02-27", "relevancy": 2.546, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5178}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5157}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4942}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Removal%20and%20Selection%3A%20Improving%20RGB-Infrared%20Object%20Detection%20via%0A%20%20Coarse-to-Fine%20Fusion&entry.906535625=Tianyi%20Zhao%20and%20Maoxun%20Yuan%20and%20Xingxing%20Wei&entry.1292438233=%20%20Object%20detection%20in%20visible%20%28RGB%29%20and%20infrared%20%28IR%29%20images%20has%20been%20widely%0Aapplied%20in%20recent%20years.%20Leveraging%20the%20complementary%20characteristics%20of%20RGB%0Aand%20IR%20images%2C%20the%20object%20detector%20provides%20reliable%20and%20robust%20object%0Alocalization%20from%20day%20to%20night.%20Existing%20fusion%20strategies%20directly%20inject%20RGB%0Aand%20IR%20images%20into%20convolution%20neural%20networks%2C%20leading%20to%20inferior%20detection%0Aperformance.%20Since%20the%20RGB%20and%20IR%20features%20have%20modality-specific%20noise%2C%20these%0Astrategies%20will%20worsen%20the%20fused%20features%20along%20with%20the%20propagation.%20Inspired%0Aby%20the%20mechanism%20of%20human%20brain%20processing%20multimodal%20information%2C%20this%20work%0Aintroduces%20a%20new%20coarse-to-fine%20perspective%20to%20purify%20and%20fuse%20two%20modality%0Afeatures.%20Specifically%2C%20following%20this%20perspective%2C%20we%20design%20a%20Redundant%0ASpectrum%20Removal%20module%20to%20coarsely%20remove%20interfering%20information%20within%20each%0Amodality%20and%20a%20Dynamic%20Feature%20Selection%20module%20to%20finely%20select%20the%20desired%0Afeatures%20for%20feature%20fusion.%20To%20verify%20the%20effectiveness%20of%20the%20coarse-to-fine%0Afusion%20strategy%2C%20we%20construct%20a%20new%20object%20detector%20called%20Removal%20and%0ASelection%20Detector%20%28RSDet%29.%20Extensive%20experiments%20on%20three%20RGB-IR%20object%0Adetection%20datasets%20verify%20the%20superior%20performance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10731v2&entry.124074799=Read"},
{"title": "A Generalized Neural Diffusion Framework on Graphs", "author": "Yibo Li and Xiao Wang and Hongrui Liu and Chuan Shi", "abstract": "  Recent studies reveal the connection between GNNs and the diffusion process,\nwhich motivates many diffusion-based GNNs to be proposed. However, since these\ntwo mechanisms are closely related, one fundamental question naturally arises:\nIs there a general diffusion framework that can formally unify these GNNs? The\nanswer to this question can not only deepen our understanding of the learning\nprocess of GNNs, but also may open a new door to design a broad new class of\nGNNs. In this paper, we propose a general diffusion equation framework with the\nfidelity term, which formally establishes the relationship between the\ndiffusion process with more GNNs. Meanwhile, with this framework, we identify\none characteristic of graph diffusion networks, i.e., the current neural\ndiffusion process only corresponds to the first-order diffusion equation.\nHowever, by an experimental investigation, we show that the labels of\nhigh-order neighbors actually exhibit monophily property, which induces the\nsimilarity based on labels among high-order neighbors without requiring the\nsimilarity among first-order neighbors. This discovery motives to design a new\nhigh-order neighbor-aware diffusion equation, and derive a new type of graph\ndiffusion network (HiD-Net) based on the framework. With the high-order\ndiffusion equation, HiD-Net is more robust against attacks and works on both\nhomophily and heterophily graphs. We not only theoretically analyze the\nrelation between HiD-Net with high-order random walk, but also provide a\ntheoretical convergence guarantee. Extensive experimental results well\ndemonstrate the effectiveness of HiD-Net over state-of-the-art graph diffusion\nnetworks.\n", "link": "http://arxiv.org/abs/2312.08616v3", "date": "2024-02-27", "relevancy": 2.5046, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5457}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4817}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4754}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generalized%20Neural%20Diffusion%20Framework%20on%20Graphs&entry.906535625=Yibo%20Li%20and%20Xiao%20Wang%20and%20Hongrui%20Liu%20and%20Chuan%20Shi&entry.1292438233=%20%20Recent%20studies%20reveal%20the%20connection%20between%20GNNs%20and%20the%20diffusion%20process%2C%0Awhich%20motivates%20many%20diffusion-based%20GNNs%20to%20be%20proposed.%20However%2C%20since%20these%0Atwo%20mechanisms%20are%20closely%20related%2C%20one%20fundamental%20question%20naturally%20arises%3A%0AIs%20there%20a%20general%20diffusion%20framework%20that%20can%20formally%20unify%20these%20GNNs%3F%20The%0Aanswer%20to%20this%20question%20can%20not%20only%20deepen%20our%20understanding%20of%20the%20learning%0Aprocess%20of%20GNNs%2C%20but%20also%20may%20open%20a%20new%20door%20to%20design%20a%20broad%20new%20class%20of%0AGNNs.%20In%20this%20paper%2C%20we%20propose%20a%20general%20diffusion%20equation%20framework%20with%20the%0Afidelity%20term%2C%20which%20formally%20establishes%20the%20relationship%20between%20the%0Adiffusion%20process%20with%20more%20GNNs.%20Meanwhile%2C%20with%20this%20framework%2C%20we%20identify%0Aone%20characteristic%20of%20graph%20diffusion%20networks%2C%20i.e.%2C%20the%20current%20neural%0Adiffusion%20process%20only%20corresponds%20to%20the%20first-order%20diffusion%20equation.%0AHowever%2C%20by%20an%20experimental%20investigation%2C%20we%20show%20that%20the%20labels%20of%0Ahigh-order%20neighbors%20actually%20exhibit%20monophily%20property%2C%20which%20induces%20the%0Asimilarity%20based%20on%20labels%20among%20high-order%20neighbors%20without%20requiring%20the%0Asimilarity%20among%20first-order%20neighbors.%20This%20discovery%20motives%20to%20design%20a%20new%0Ahigh-order%20neighbor-aware%20diffusion%20equation%2C%20and%20derive%20a%20new%20type%20of%20graph%0Adiffusion%20network%20%28HiD-Net%29%20based%20on%20the%20framework.%20With%20the%20high-order%0Adiffusion%20equation%2C%20HiD-Net%20is%20more%20robust%20against%20attacks%20and%20works%20on%20both%0Ahomophily%20and%20heterophily%20graphs.%20We%20not%20only%20theoretically%20analyze%20the%0Arelation%20between%20HiD-Net%20with%20high-order%20random%20walk%2C%20but%20also%20provide%20a%0Atheoretical%20convergence%20guarantee.%20Extensive%20experimental%20results%20well%0Ademonstrate%20the%20effectiveness%20of%20HiD-Net%20over%20state-of-the-art%20graph%20diffusion%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08616v3&entry.124074799=Read"},
{"title": "Gradient-based Discrete Sampling with Automatic Cyclical Scheduling", "author": "Patrick Pynadath and Riddhiman Bhattacharya and Arun Hariharan and Ruqi Zhang", "abstract": "  Discrete distributions, particularly in high-dimensional deep models, are\noften highly multimodal due to inherent discontinuities. While gradient-based\ndiscrete sampling has proven effective, it is susceptible to becoming trapped\nin local modes due to the gradient information. To tackle this challenge, we\npropose an automatic cyclical scheduling, designed for efficient and accurate\nsampling in multimodal discrete distributions. Our method contains three key\ncomponents: (1) a cyclical step size schedule where large steps discover new\nmodes and small steps exploit each mode; (2) a cyclical balancing schedule,\nensuring ``balanced\" proposals for given step sizes and high efficiency of the\nMarkov chain; and (3) an automatic tuning scheme for adjusting the\nhyperparameters in the cyclical schedules, allowing adaptability across diverse\ndatasets with minimal tuning. We prove the non-asymptotic convergence and\ninference guarantee for our method in general discrete distributions. Extensive\nexperiments demonstrate the superiority of our method in sampling complex\nmultimodal discrete distributions.\n", "link": "http://arxiv.org/abs/2402.17699v1", "date": "2024-02-27", "relevancy": 2.4182, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5208}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4752}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4548}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-based%20Discrete%20Sampling%20with%20Automatic%20Cyclical%20Scheduling&entry.906535625=Patrick%20Pynadath%20and%20Riddhiman%20Bhattacharya%20and%20Arun%20Hariharan%20and%20Ruqi%20Zhang&entry.1292438233=%20%20Discrete%20distributions%2C%20particularly%20in%20high-dimensional%20deep%20models%2C%20are%0Aoften%20highly%20multimodal%20due%20to%20inherent%20discontinuities.%20While%20gradient-based%0Adiscrete%20sampling%20has%20proven%20effective%2C%20it%20is%20susceptible%20to%20becoming%20trapped%0Ain%20local%20modes%20due%20to%20the%20gradient%20information.%20To%20tackle%20this%20challenge%2C%20we%0Apropose%20an%20automatic%20cyclical%20scheduling%2C%20designed%20for%20efficient%20and%20accurate%0Asampling%20in%20multimodal%20discrete%20distributions.%20Our%20method%20contains%20three%20key%0Acomponents%3A%20%281%29%20a%20cyclical%20step%20size%20schedule%20where%20large%20steps%20discover%20new%0Amodes%20and%20small%20steps%20exploit%20each%20mode%3B%20%282%29%20a%20cyclical%20balancing%20schedule%2C%0Aensuring%20%60%60balanced%22%20proposals%20for%20given%20step%20sizes%20and%20high%20efficiency%20of%20the%0AMarkov%20chain%3B%20and%20%283%29%20an%20automatic%20tuning%20scheme%20for%20adjusting%20the%0Ahyperparameters%20in%20the%20cyclical%20schedules%2C%20allowing%20adaptability%20across%20diverse%0Adatasets%20with%20minimal%20tuning.%20We%20prove%20the%20non-asymptotic%20convergence%20and%0Ainference%20guarantee%20for%20our%20method%20in%20general%20discrete%20distributions.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20method%20in%20sampling%20complex%0Amultimodal%20discrete%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17699v1&entry.124074799=Read"},
{"title": "PHNet: Patch-based Normalization for Portrait Harmonization", "author": "Karen Efremyan and Elizaveta Petrova and Evgeny Kaskov and Alexander Kapitanov", "abstract": "  A common problem for composite images is the incompatibility of their\nforeground and background components. Image harmonization aims to solve this\nproblem, making the whole image look more authentic and coherent. Most existing\nsolutions predict lookup tables (LUTs) or reconstruct images, utilizing various\nattributes of composite images. Recent approaches have primarily focused on\nemploying global transformations like normalization and color curve rendering\nto achieve visual consistency, and they often overlook the importance of local\nvisual coherence. We present a patch-based harmonization network consisting of\nnovel Patch-based normalization (PN) blocks and a feature extractor based on\nstatistical color transfer. Extensive experiments demonstrate the network's\nhigh generalization capability for different domains. Our network achieves\nstate-of-the-art results on the iHarmony4 dataset. Also, we created a new human\nportrait harmonization dataset based on FFHQ and checked the proposed method to\nshow the generalization ability by achieving the best metrics on it. The\nbenchmark experiments confirm that the suggested patch-based normalization\nblock and feature extractor effectively improve the network's capability to\nharmonize portraits. Our code and model baselines are publicly available.\n", "link": "http://arxiv.org/abs/2402.17561v1", "date": "2024-02-27", "relevancy": 2.4134, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4748}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.463}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PHNet%3A%20Patch-based%20Normalization%20for%20Portrait%20Harmonization&entry.906535625=Karen%20Efremyan%20and%20Elizaveta%20Petrova%20and%20Evgeny%20Kaskov%20and%20Alexander%20Kapitanov&entry.1292438233=%20%20A%20common%20problem%20for%20composite%20images%20is%20the%20incompatibility%20of%20their%0Aforeground%20and%20background%20components.%20Image%20harmonization%20aims%20to%20solve%20this%0Aproblem%2C%20making%20the%20whole%20image%20look%20more%20authentic%20and%20coherent.%20Most%20existing%0Asolutions%20predict%20lookup%20tables%20%28LUTs%29%20or%20reconstruct%20images%2C%20utilizing%20various%0Aattributes%20of%20composite%20images.%20Recent%20approaches%20have%20primarily%20focused%20on%0Aemploying%20global%20transformations%20like%20normalization%20and%20color%20curve%20rendering%0Ato%20achieve%20visual%20consistency%2C%20and%20they%20often%20overlook%20the%20importance%20of%20local%0Avisual%20coherence.%20We%20present%20a%20patch-based%20harmonization%20network%20consisting%20of%0Anovel%20Patch-based%20normalization%20%28PN%29%20blocks%20and%20a%20feature%20extractor%20based%20on%0Astatistical%20color%20transfer.%20Extensive%20experiments%20demonstrate%20the%20network%27s%0Ahigh%20generalization%20capability%20for%20different%20domains.%20Our%20network%20achieves%0Astate-of-the-art%20results%20on%20the%20iHarmony4%20dataset.%20Also%2C%20we%20created%20a%20new%20human%0Aportrait%20harmonization%20dataset%20based%20on%20FFHQ%20and%20checked%20the%20proposed%20method%20to%0Ashow%20the%20generalization%20ability%20by%20achieving%20the%20best%20metrics%20on%20it.%20The%0Abenchmark%20experiments%20confirm%20that%20the%20suggested%20patch-based%20normalization%0Ablock%20and%20feature%20extractor%20effectively%20improve%20the%20network%27s%20capability%20to%0Aharmonize%20portraits.%20Our%20code%20and%20model%20baselines%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17561v1&entry.124074799=Read"},
{"title": "Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion\n  Latent Aligners", "author": "Yazhou Xing and Yingqing He and Zeyue Tian and Xintao Wang and Qifeng Chen", "abstract": "  Video and audio content creation serves as the core technique for the movie\nindustry and professional users. Recently, existing diffusion-based methods\ntackle video and audio generation separately, which hinders the technique\ntransfer from academia to industry. In this work, we aim at filling the gap,\nwith a carefully designed optimization-based framework for cross-visual-audio\nand joint-visual-audio generation. We observe the powerful generation ability\nof off-the-shelf video or audio generation models. Thus, instead of training\nthe giant models from scratch, we propose to bridge the existing strong models\nwith a shared latent representation space. Specifically, we propose a\nmultimodality latent aligner with the pre-trained ImageBind model. Our latent\naligner shares a similar core as the classifier guidance that guides the\ndiffusion denoising process during inference time. Through carefully designed\noptimization strategy and loss functions, we show the superior performance of\nour method on joint video-audio generation, visual-steered audio generation,\nand audio-steered visual generation tasks. The project website can be found at\nhttps://yzxing87.github.io/Seeing-and-Hearing/\n", "link": "http://arxiv.org/abs/2402.17723v1", "date": "2024-02-27", "relevancy": 2.3859, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6328}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6249}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5488}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20and%20Hearing%3A%20Open-domain%20Visual-Audio%20Generation%20with%20Diffusion%0A%20%20Latent%20Aligners&entry.906535625=Yazhou%20Xing%20and%20Yingqing%20He%20and%20Zeyue%20Tian%20and%20Xintao%20Wang%20and%20Qifeng%20Chen&entry.1292438233=%20%20Video%20and%20audio%20content%20creation%20serves%20as%20the%20core%20technique%20for%20the%20movie%0Aindustry%20and%20professional%20users.%20Recently%2C%20existing%20diffusion-based%20methods%0Atackle%20video%20and%20audio%20generation%20separately%2C%20which%20hinders%20the%20technique%0Atransfer%20from%20academia%20to%20industry.%20In%20this%20work%2C%20we%20aim%20at%20filling%20the%20gap%2C%0Awith%20a%20carefully%20designed%20optimization-based%20framework%20for%20cross-visual-audio%0Aand%20joint-visual-audio%20generation.%20We%20observe%20the%20powerful%20generation%20ability%0Aof%20off-the-shelf%20video%20or%20audio%20generation%20models.%20Thus%2C%20instead%20of%20training%0Athe%20giant%20models%20from%20scratch%2C%20we%20propose%20to%20bridge%20the%20existing%20strong%20models%0Awith%20a%20shared%20latent%20representation%20space.%20Specifically%2C%20we%20propose%20a%0Amultimodality%20latent%20aligner%20with%20the%20pre-trained%20ImageBind%20model.%20Our%20latent%0Aaligner%20shares%20a%20similar%20core%20as%20the%20classifier%20guidance%20that%20guides%20the%0Adiffusion%20denoising%20process%20during%20inference%20time.%20Through%20carefully%20designed%0Aoptimization%20strategy%20and%20loss%20functions%2C%20we%20show%20the%20superior%20performance%20of%0Aour%20method%20on%20joint%20video-audio%20generation%2C%20visual-steered%20audio%20generation%2C%0Aand%20audio-steered%20visual%20generation%20tasks.%20The%20project%20website%20can%20be%20found%20at%0Ahttps%3A//yzxing87.github.io/Seeing-and-Hearing/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17723v1&entry.124074799=Read"},
{"title": "Bit Distribution Study and Implementation of Spatial Quality Map in the\n  JPEG-AI Standardization", "author": "Panqi Jia and Jue Mao and Esin Koyuncu and A. Burakhan Koyuncu and Timofey Solovyev and Alexander Karabutov and Yin Zhao and Elena Alshina and Andre Kaup", "abstract": "  Currently, there is a high demand for neural network-based image compression\ncodecs. These codecs employ non-linear transforms to create compact bit\nrepresentations and facilitate faster coding speeds on devices compared to the\nhand-crafted transforms used in classical frameworks. The scientific and\nindustrial communities are highly interested in these properties, leading to\nthe standardization effort of JPEG-AI. The JPEG-AI verification model has been\nreleased and is currently under development for standardization. Utilizing\nneural networks, it can outperform the classic codec VVC intra by over 10%\nBD-rate operating at base operation point. Researchers attribute this success\nto the flexible bit distribution in the spatial domain, in contrast to VVC\nintra's anchor that is generated with a constant quality point. However, our\nstudy reveals that VVC intra displays a more adaptable bit distribution\nstructure through the implementation of various block sizes. As a result of our\nobservations, we have proposed a spatial bit allocation method to optimize the\nJPEG-AI verification model's bit distribution and enhance the visual quality.\nFurthermore, by applying the VVC bit distribution strategy, the objective\nperformance of JPEG-AI verification mode can be further improved, resulting in\na maximum gain of 0.45 dB in PSNR-Y.\n", "link": "http://arxiv.org/abs/2402.17470v1", "date": "2024-02-27", "relevancy": 2.3819, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4718}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4584}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bit%20Distribution%20Study%20and%20Implementation%20of%20Spatial%20Quality%20Map%20in%20the%0A%20%20JPEG-AI%20Standardization&entry.906535625=Panqi%20Jia%20and%20Jue%20Mao%20and%20Esin%20Koyuncu%20and%20A.%20Burakhan%20Koyuncu%20and%20Timofey%20Solovyev%20and%20Alexander%20Karabutov%20and%20Yin%20Zhao%20and%20Elena%20Alshina%20and%20Andre%20Kaup&entry.1292438233=%20%20Currently%2C%20there%20is%20a%20high%20demand%20for%20neural%20network-based%20image%20compression%0Acodecs.%20These%20codecs%20employ%20non-linear%20transforms%20to%20create%20compact%20bit%0Arepresentations%20and%20facilitate%20faster%20coding%20speeds%20on%20devices%20compared%20to%20the%0Ahand-crafted%20transforms%20used%20in%20classical%20frameworks.%20The%20scientific%20and%0Aindustrial%20communities%20are%20highly%20interested%20in%20these%20properties%2C%20leading%20to%0Athe%20standardization%20effort%20of%20JPEG-AI.%20The%20JPEG-AI%20verification%20model%20has%20been%0Areleased%20and%20is%20currently%20under%20development%20for%20standardization.%20Utilizing%0Aneural%20networks%2C%20it%20can%20outperform%20the%20classic%20codec%20VVC%20intra%20by%20over%2010%25%0ABD-rate%20operating%20at%20base%20operation%20point.%20Researchers%20attribute%20this%20success%0Ato%20the%20flexible%20bit%20distribution%20in%20the%20spatial%20domain%2C%20in%20contrast%20to%20VVC%0Aintra%27s%20anchor%20that%20is%20generated%20with%20a%20constant%20quality%20point.%20However%2C%20our%0Astudy%20reveals%20that%20VVC%20intra%20displays%20a%20more%20adaptable%20bit%20distribution%0Astructure%20through%20the%20implementation%20of%20various%20block%20sizes.%20As%20a%20result%20of%20our%0Aobservations%2C%20we%20have%20proposed%20a%20spatial%20bit%20allocation%20method%20to%20optimize%20the%0AJPEG-AI%20verification%20model%27s%20bit%20distribution%20and%20enhance%20the%20visual%20quality.%0AFurthermore%2C%20by%20applying%20the%20VVC%20bit%20distribution%20strategy%2C%20the%20objective%0Aperformance%20of%20JPEG-AI%20verification%20mode%20can%20be%20further%20improved%2C%20resulting%20in%0Aa%20maximum%20gain%20of%200.45%20dB%20in%20PSNR-Y.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17470v1&entry.124074799=Read"},
{"title": "Fraud Detection with Binding Global and Local Relational Interaction", "author": "Haolin Li and Shuyang Jiang and Lifeng Zhang and Siyuan Du and Guangnan Ye and Hongfeng Chai", "abstract": "  Graph Neural Network has been proved to be effective for fraud detection for\nits capability to encode node interaction and aggregate features in a holistic\nview. Recently, Transformer network with great sequence encoding ability, has\nalso outperformed other GNN-based methods in literatures. However, both\nGNN-based and Transformer-based networks only encode one perspective of the\nwhole graph, while GNN encodes global features and Transformer network encodes\nlocal ones. Furthermore, previous works ignored encoding global interaction\nfeatures of the heterogeneous graph with separate networks, thus leading to\nsuboptimal performance. In this work, we present a novel framework called\nRelation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds\nlocal and global features into a target node. The simple yet effective network\napplies a modified GAGA module where each transformer layer is followed by a\ncross-relation aggregation layer, to encode local embeddings and node\ninteractions across different relations. Apart from the Transformer-based\nnetwork, we further introduce a Relation-Aware GNN module to learn global\nembeddings, which is later merged into the local embeddings by an attention\nfusion module and a skip connection. Extensive experiments on two popular\npublic datasets and an industrial dataset demonstrate that RAGFormer achieves\nthe state-of-the-art performance. Substantial analysis experiments validate the\neffectiveness of each submodule of RAGFormer and its high efficiency in\nutilizing small-scale data and low hyper-parameter sensitivity.\n", "link": "http://arxiv.org/abs/2402.17472v1", "date": "2024-02-27", "relevancy": 2.3789, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4849}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.481}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4614}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fraud%20Detection%20with%20Binding%20Global%20and%20Local%20Relational%20Interaction&entry.906535625=Haolin%20Li%20and%20Shuyang%20Jiang%20and%20Lifeng%20Zhang%20and%20Siyuan%20Du%20and%20Guangnan%20Ye%20and%20Hongfeng%20Chai&entry.1292438233=%20%20Graph%20Neural%20Network%20has%20been%20proved%20to%20be%20effective%20for%20fraud%20detection%20for%0Aits%20capability%20to%20encode%20node%20interaction%20and%20aggregate%20features%20in%20a%20holistic%0Aview.%20Recently%2C%20Transformer%20network%20with%20great%20sequence%20encoding%20ability%2C%20has%0Aalso%20outperformed%20other%20GNN-based%20methods%20in%20literatures.%20However%2C%20both%0AGNN-based%20and%20Transformer-based%20networks%20only%20encode%20one%20perspective%20of%20the%0Awhole%20graph%2C%20while%20GNN%20encodes%20global%20features%20and%20Transformer%20network%20encodes%0Alocal%20ones.%20Furthermore%2C%20previous%20works%20ignored%20encoding%20global%20interaction%0Afeatures%20of%20the%20heterogeneous%20graph%20with%20separate%20networks%2C%20thus%20leading%20to%0Asuboptimal%20performance.%20In%20this%20work%2C%20we%20present%20a%20novel%20framework%20called%0ARelation-Aware%20GNN%20with%20transFormer%20%28RAGFormer%29%20which%20simultaneously%20embeds%0Alocal%20and%20global%20features%20into%20a%20target%20node.%20The%20simple%20yet%20effective%20network%0Aapplies%20a%20modified%20GAGA%20module%20where%20each%20transformer%20layer%20is%20followed%20by%20a%0Across-relation%20aggregation%20layer%2C%20to%20encode%20local%20embeddings%20and%20node%0Ainteractions%20across%20different%20relations.%20Apart%20from%20the%20Transformer-based%0Anetwork%2C%20we%20further%20introduce%20a%20Relation-Aware%20GNN%20module%20to%20learn%20global%0Aembeddings%2C%20which%20is%20later%20merged%20into%20the%20local%20embeddings%20by%20an%20attention%0Afusion%20module%20and%20a%20skip%20connection.%20Extensive%20experiments%20on%20two%20popular%0Apublic%20datasets%20and%20an%20industrial%20dataset%20demonstrate%20that%20RAGFormer%20achieves%0Athe%20state-of-the-art%20performance.%20Substantial%20analysis%20experiments%20validate%20the%0Aeffectiveness%20of%20each%20submodule%20of%20RAGFormer%20and%20its%20high%20efficiency%20in%0Autilizing%20small-scale%20data%20and%20low%20hyper-parameter%20sensitivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17472v1&entry.124074799=Read"},
{"title": "CoDream: Exchanging dreams instead of models for federated aggregation\n  with heterogeneous models", "author": "Abhishek Singh and Gauri Gupta and Ritvik Kapila and Yichuan Shi and Alex Dang and Sheshank Shankar and Mohammed Ehab and Ramesh Raskar", "abstract": "  Federated Learning (FL) enables collaborative optimization of machine\nlearning models across decentralized data by aggregating model parameters. Our\napproach extends this concept by aggregating \"knowledge\" derived from models,\ninstead of model parameters. We present a novel framework called CoDream, where\nclients collaboratively optimize randomly initialized data using federated\noptimization in the input data space, similar to how randomly initialized model\nparameters are optimized in FL. Our key insight is that jointly optimizing this\ndata can effectively capture the properties of the global data distribution.\nSharing knowledge in data space offers numerous benefits: (1) model-agnostic\ncollaborative learning, i.e., different clients can have different model\narchitectures; (2) communication that is independent of the model size,\neliminating scalability concerns with model parameters; (3) compatibility with\nsecure aggregation, thus preserving the privacy benefits of federated learning;\n(4) allowing of adaptive optimization of knowledge shared for personalized\nlearning. We empirically validate CoDream on standard FL tasks, demonstrating\ncompetitive performance despite not sharing model parameters. Our code:\nhttps://mitmedialab.github.io/codream.github.io/\n", "link": "http://arxiv.org/abs/2402.15968v2", "date": "2024-02-27", "relevancy": 2.3743, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4809}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4742}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4694}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoDream%3A%20Exchanging%20dreams%20instead%20of%20models%20for%20federated%20aggregation%0A%20%20with%20heterogeneous%20models&entry.906535625=Abhishek%20Singh%20and%20Gauri%20Gupta%20and%20Ritvik%20Kapila%20and%20Yichuan%20Shi%20and%20Alex%20Dang%20and%20Sheshank%20Shankar%20and%20Mohammed%20Ehab%20and%20Ramesh%20Raskar&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20optimization%20of%20machine%0Alearning%20models%20across%20decentralized%20data%20by%20aggregating%20model%20parameters.%20Our%0Aapproach%20extends%20this%20concept%20by%20aggregating%20%22knowledge%22%20derived%20from%20models%2C%0Ainstead%20of%20model%20parameters.%20We%20present%20a%20novel%20framework%20called%20CoDream%2C%20where%0Aclients%20collaboratively%20optimize%20randomly%20initialized%20data%20using%20federated%0Aoptimization%20in%20the%20input%20data%20space%2C%20similar%20to%20how%20randomly%20initialized%20model%0Aparameters%20are%20optimized%20in%20FL.%20Our%20key%20insight%20is%20that%20jointly%20optimizing%20this%0Adata%20can%20effectively%20capture%20the%20properties%20of%20the%20global%20data%20distribution.%0ASharing%20knowledge%20in%20data%20space%20offers%20numerous%20benefits%3A%20%281%29%20model-agnostic%0Acollaborative%20learning%2C%20i.e.%2C%20different%20clients%20can%20have%20different%20model%0Aarchitectures%3B%20%282%29%20communication%20that%20is%20independent%20of%20the%20model%20size%2C%0Aeliminating%20scalability%20concerns%20with%20model%20parameters%3B%20%283%29%20compatibility%20with%0Asecure%20aggregation%2C%20thus%20preserving%20the%20privacy%20benefits%20of%20federated%20learning%3B%0A%284%29%20allowing%20of%20adaptive%20optimization%20of%20knowledge%20shared%20for%20personalized%0Alearning.%20We%20empirically%20validate%20CoDream%20on%20standard%20FL%20tasks%2C%20demonstrating%0Acompetitive%20performance%20despite%20not%20sharing%20model%20parameters.%20Our%20code%3A%0Ahttps%3A//mitmedialab.github.io/codream.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15968v2&entry.124074799=Read"},
{"title": "SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image\n  Classification", "author": "Mohammed Q. Alkhatib and M. Sami Zitouni and Mina Al-Saad and Nour Aburaed and Hussain Al-Ahmad", "abstract": "  Polarimetric synthetic aperture radar (PolSAR) images encompass valuable\ninformation that can facilitate extensive land cover interpretation and\ngenerate diverse output products. Extracting meaningful features from PolSAR\ndata poses challenges distinct from those encountered in optical imagery. Deep\nlearning (DL) methods offer effective solutions for overcoming these challenges\nin PolSAR feature extraction. Convolutional neural networks (CNNs) play a\ncrucial role in capturing PolSAR image characteristics by leveraging kernel\ncapabilities to consider local information and the complex-valued nature of\nPolSAR data. In this study, a novel three-branch fusion of complex-valued CNN,\nnamed the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for\nPolSAR image classification. To validate the performance of the proposed\nmethod, classification results are compared against multiple state-of-the-art\napproaches using the airborne synthetic aperture radar (AIRSAR) datasets of\nFlevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The\nresults indicate that the proposed approach demonstrates improvements in\noverallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a\n0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data\nunderscore the effectiveness of the SDF2Net model, revealing a promising\noverall accuracy of 96.01% even with only a 1% sampling ratio.\n", "link": "http://arxiv.org/abs/2402.17672v1", "date": "2024-02-27", "relevancy": 2.3738, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5064}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4643}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4535}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDF2Net%3A%20Shallow%20to%20Deep%20Feature%20Fusion%20Network%20for%20PolSAR%20Image%0A%20%20Classification&entry.906535625=Mohammed%20Q.%20Alkhatib%20and%20M.%20Sami%20Zitouni%20and%20Mina%20Al-Saad%20and%20Nour%20Aburaed%20and%20Hussain%20Al-Ahmad&entry.1292438233=%20%20Polarimetric%20synthetic%20aperture%20radar%20%28PolSAR%29%20images%20encompass%20valuable%0Ainformation%20that%20can%20facilitate%20extensive%20land%20cover%20interpretation%20and%0Agenerate%20diverse%20output%20products.%20Extracting%20meaningful%20features%20from%20PolSAR%0Adata%20poses%20challenges%20distinct%20from%20those%20encountered%20in%20optical%20imagery.%20Deep%0Alearning%20%28DL%29%20methods%20offer%20effective%20solutions%20for%20overcoming%20these%20challenges%0Ain%20PolSAR%20feature%20extraction.%20Convolutional%20neural%20networks%20%28CNNs%29%20play%20a%0Acrucial%20role%20in%20capturing%20PolSAR%20image%20characteristics%20by%20leveraging%20kernel%0Acapabilities%20to%20consider%20local%20information%20and%20the%20complex-valued%20nature%20of%0APolSAR%20data.%20In%20this%20study%2C%20a%20novel%20three-branch%20fusion%20of%20complex-valued%20CNN%2C%0Anamed%20the%20Shallow%20to%20Deep%20Feature%20Fusion%20Network%20%28SDF2Net%29%2C%20is%20proposed%20for%0APolSAR%20image%20classification.%20To%20validate%20the%20performance%20of%20the%20proposed%0Amethod%2C%20classification%20results%20are%20compared%20against%20multiple%20state-of-the-art%0Aapproaches%20using%20the%20airborne%20synthetic%20aperture%20radar%20%28AIRSAR%29%20datasets%20of%0AFlevoland%20and%20San%20Francisco%2C%20as%20well%20as%20the%20ESAR%20Oberpfaffenhofen%20dataset.%20The%0Aresults%20indicate%20that%20the%20proposed%20approach%20demonstrates%20improvements%20in%0Aoverallaccuracy%2C%20with%20a%201.3%25%20and%200.8%25%20enhancement%20for%20the%20AIRSAR%20datasets%20and%20a%0A0.5%25%20improvement%20for%20the%20ESAR%20dataset.%20Analyses%20conducted%20on%20the%20Flevoland%20data%0Aunderscore%20the%20effectiveness%20of%20the%20SDF2Net%20model%2C%20revealing%20a%20promising%0Aoverall%20accuracy%20of%2096.01%25%20even%20with%20only%20a%201%25%20sampling%20ratio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17672v1&entry.124074799=Read"},
{"title": "Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud\n  Matching", "author": "Matteo Bastico and Etienne Decenci\u00e8re and Laurent Cort\u00e9 and Yannick Tillier and David Ryckelynck", "abstract": "  Point cloud matching, a crucial technique in computer vision, medical and\nrobotics fields, is primarily concerned with finding correspondences between\npairs of point clouds or voxels. In some practical scenarios, emphasizing local\ndifferences is crucial for accurately identifying a correct match, thereby\nenhancing the overall robustness and reliability of the matching process.\nCommonly used shape descriptors have several limitations and often fail to\nprovide meaningful local insights on the paired geometries. In this work, we\npropose a new technique, based on graph Laplacian eigenmaps, to match point\nclouds by taking into account fine local structures. To deal with the order and\nsign ambiguity of Laplacian eigenmaps, we introduce a new operator, called\nCoupled Laplacian, that allows to easily generate aligned eigenspaces for\nmultiple rigidly-registered geometries. We show that the similarity between\nthose aligned high-dimensional spaces provides a locally meaningful score to\nmatch shapes. We initially evaluate the performance of the proposed technique\nin a point-wise manner, specifically focusing on the task of object anomaly\nlocalization using the MVTec 3D-AD dataset. Additionally, we define a new\nmedical task, called automatic Bone Side Estimation (BSE), which we address\nthrough a global similarity score derived from coupled eigenspaces. In order to\ntest it, we propose a benchmark collecting bone surface structures from various\npublic datasets. Our matching technique, based on Coupled Laplacian,\noutperforms other methods by reaching an impressive accuracy on both tasks. The\ncode to reproduce our experiments is publicly available at\nhttps://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary\nCode.\n", "link": "http://arxiv.org/abs/2402.17372v1", "date": "2024-02-27", "relevancy": 2.3719, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5735}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5266}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coupled%20Laplacian%20Eigenmaps%20for%20Locally-Aware%203D%20Rigid%20Point%20Cloud%0A%20%20Matching&entry.906535625=Matteo%20Bastico%20and%20Etienne%20Decenci%C3%A8re%20and%20Laurent%20Cort%C3%A9%20and%20Yannick%20Tillier%20and%20David%20Ryckelynck&entry.1292438233=%20%20Point%20cloud%20matching%2C%20a%20crucial%20technique%20in%20computer%20vision%2C%20medical%20and%0Arobotics%20fields%2C%20is%20primarily%20concerned%20with%20finding%20correspondences%20between%0Apairs%20of%20point%20clouds%20or%20voxels.%20In%20some%20practical%20scenarios%2C%20emphasizing%20local%0Adifferences%20is%20crucial%20for%20accurately%20identifying%20a%20correct%20match%2C%20thereby%0Aenhancing%20the%20overall%20robustness%20and%20reliability%20of%20the%20matching%20process.%0ACommonly%20used%20shape%20descriptors%20have%20several%20limitations%20and%20often%20fail%20to%0Aprovide%20meaningful%20local%20insights%20on%20the%20paired%20geometries.%20In%20this%20work%2C%20we%0Apropose%20a%20new%20technique%2C%20based%20on%20graph%20Laplacian%20eigenmaps%2C%20to%20match%20point%0Aclouds%20by%20taking%20into%20account%20fine%20local%20structures.%20To%20deal%20with%20the%20order%20and%0Asign%20ambiguity%20of%20Laplacian%20eigenmaps%2C%20we%20introduce%20a%20new%20operator%2C%20called%0ACoupled%20Laplacian%2C%20that%20allows%20to%20easily%20generate%20aligned%20eigenspaces%20for%0Amultiple%20rigidly-registered%20geometries.%20We%20show%20that%20the%20similarity%20between%0Athose%20aligned%20high-dimensional%20spaces%20provides%20a%20locally%20meaningful%20score%20to%0Amatch%20shapes.%20We%20initially%20evaluate%20the%20performance%20of%20the%20proposed%20technique%0Ain%20a%20point-wise%20manner%2C%20specifically%20focusing%20on%20the%20task%20of%20object%20anomaly%0Alocalization%20using%20the%20MVTec%203D-AD%20dataset.%20Additionally%2C%20we%20define%20a%20new%0Amedical%20task%2C%20called%20automatic%20Bone%20Side%20Estimation%20%28BSE%29%2C%20which%20we%20address%0Athrough%20a%20global%20similarity%20score%20derived%20from%20coupled%20eigenspaces.%20In%20order%20to%0Atest%20it%2C%20we%20propose%20a%20benchmark%20collecting%20bone%20surface%20structures%20from%20various%0Apublic%20datasets.%20Our%20matching%20technique%2C%20based%20on%20Coupled%20Laplacian%2C%0Aoutperforms%20other%20methods%20by%20reaching%20an%20impressive%20accuracy%20on%20both%20tasks.%20The%0Acode%20to%20reproduce%20our%20experiments%20is%20publicly%20available%20at%0Ahttps%3A//github.com/matteo-bastico/CoupledLaplacian%20and%20in%20the%20Supplementary%0ACode.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17372v1&entry.124074799=Read"},
{"title": "An Empirical Study of the Generalization Ability of Lidar 3D Object\n  Detectors to Unseen Domains", "author": "George Eskandar and Chongzhe Zhang and Abhishek Kaushik and Karim Guirguis and Mohamed Sayed and Bin Yang", "abstract": "  3D Object Detectors (3D-OD) are crucial for understanding the environment in\nmany robotic tasks, especially autonomous driving. Including 3D information via\nLidar sensors improves accuracy greatly. However, such detectors perform poorly\non domains they were not trained on, i.e. different locations, sensors,\nweather, etc., limiting their reliability in safety-critical applications.\nThere exist methods to adapt 3D-ODs to these domains; however, these methods\ntreat 3D-ODs as a black box, neglecting underlying architectural decisions and\nsource-domain training strategies. Instead, we dive deep into the details of\n3D-ODs, focusing our efforts on fundamental factors that influence robustness\nprior to domain adaptation.\n  We systematically investigate four design choices (and the interplay between\nthem) often overlooked in 3D-OD robustness and domain adaptation: architecture,\nvoxel encoding, data augmentations, and anchor strategies. We assess their\nimpact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks\nencompassing three types of domain gaps - sensor type, weather, and location.\n  Our main findings are: (1) transformer backbones with local point features\nare more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial\nfor adaptation across geographical locations, significantly boosting scores\nwithout retraining, (3) source-domain augmentations allow the model to\ngeneralize to low-resolution sensors, and (4) surprisingly, robustness to bad\nweather is improved when training directly on more clean weather data than on\ntraining with bad weather data. We outline our main conclusions and findings to\nprovide practical guidance on developing more robust 3D-ODs.\n", "link": "http://arxiv.org/abs/2402.17562v1", "date": "2024-02-27", "relevancy": 2.348, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5975}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5935}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5739}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20of%20the%20Generalization%20Ability%20of%20Lidar%203D%20Object%0A%20%20Detectors%20to%20Unseen%20Domains&entry.906535625=George%20Eskandar%20and%20Chongzhe%20Zhang%20and%20Abhishek%20Kaushik%20and%20Karim%20Guirguis%20and%20Mohamed%20Sayed%20and%20Bin%20Yang&entry.1292438233=%20%203D%20Object%20Detectors%20%283D-OD%29%20are%20crucial%20for%20understanding%20the%20environment%20in%0Amany%20robotic%20tasks%2C%20especially%20autonomous%20driving.%20Including%203D%20information%20via%0ALidar%20sensors%20improves%20accuracy%20greatly.%20However%2C%20such%20detectors%20perform%20poorly%0Aon%20domains%20they%20were%20not%20trained%20on%2C%20i.e.%20different%20locations%2C%20sensors%2C%0Aweather%2C%20etc.%2C%20limiting%20their%20reliability%20in%20safety-critical%20applications.%0AThere%20exist%20methods%20to%20adapt%203D-ODs%20to%20these%20domains%3B%20however%2C%20these%20methods%0Atreat%203D-ODs%20as%20a%20black%20box%2C%20neglecting%20underlying%20architectural%20decisions%20and%0Asource-domain%20training%20strategies.%20Instead%2C%20we%20dive%20deep%20into%20the%20details%20of%0A3D-ODs%2C%20focusing%20our%20efforts%20on%20fundamental%20factors%20that%20influence%20robustness%0Aprior%20to%20domain%20adaptation.%0A%20%20We%20systematically%20investigate%20four%20design%20choices%20%28and%20the%20interplay%20between%0Athem%29%20often%20overlooked%20in%203D-OD%20robustness%20and%20domain%20adaptation%3A%20architecture%2C%0Avoxel%20encoding%2C%20data%20augmentations%2C%20and%20anchor%20strategies.%20We%20assess%20their%0Aimpact%20on%20the%20robustness%20of%20nine%20state-of-the-art%203D-ODs%20across%20six%20benchmarks%0Aencompassing%20three%20types%20of%20domain%20gaps%20-%20sensor%20type%2C%20weather%2C%20and%20location.%0A%20%20Our%20main%20findings%20are%3A%20%281%29%20transformer%20backbones%20with%20local%20point%20features%0Aare%20more%20robust%20than%203D%20CNNs%2C%20%282%29%20test-time%20anchor%20size%20adjustment%20is%20crucial%0Afor%20adaptation%20across%20geographical%20locations%2C%20significantly%20boosting%20scores%0Awithout%20retraining%2C%20%283%29%20source-domain%20augmentations%20allow%20the%20model%20to%0Ageneralize%20to%20low-resolution%20sensors%2C%20and%20%284%29%20surprisingly%2C%20robustness%20to%20bad%0Aweather%20is%20improved%20when%20training%20directly%20on%20more%20clean%20weather%20data%20than%20on%0Atraining%20with%20bad%20weather%20data.%20We%20outline%20our%20main%20conclusions%20and%20findings%20to%0Aprovide%20practical%20guidance%20on%20developing%20more%20robust%203D-ODs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17562v1&entry.124074799=Read"},
{"title": "Pretrained Visual Uncertainties", "author": "Michael Kirchhof and Mark Collier and Seong Joon Oh and Enkelejda Kasneci", "abstract": "  Accurate uncertainty estimation is vital to trustworthy machine learning, yet\nuncertainties typically have to be learned for each task anew. This work\nintroduces the first pretrained uncertainty modules for vision models. Similar\nto standard pretraining this enables the zero-shot transfer of uncertainties\nlearned on a large pretraining dataset to specialized downstream datasets. We\nenable our large-scale pretraining on ImageNet-21k by solving a gradient\nconflict in previous uncertainty modules and accelerating the training by up to\n180x. We find that the pretrained uncertainties generalize to unseen datasets.\nIn scrutinizing the learned uncertainties, we find that they capture aleatoric\nuncertainty, disentangled from epistemic components. We demonstrate that this\nenables safe retrieval and uncertainty-aware dataset visualization. To\nencourage applications to further problems and domains, we release all\npretrained checkpoints and code under https://github.com/mkirchhof/url .\n", "link": "http://arxiv.org/abs/2402.16569v2", "date": "2024-02-27", "relevancy": 2.3198, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6113}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6035}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5439}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretrained%20Visual%20Uncertainties&entry.906535625=Michael%20Kirchhof%20and%20Mark%20Collier%20and%20Seong%20Joon%20Oh%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20Accurate%20uncertainty%20estimation%20is%20vital%20to%20trustworthy%20machine%20learning%2C%20yet%0Auncertainties%20typically%20have%20to%20be%20learned%20for%20each%20task%20anew.%20This%20work%0Aintroduces%20the%20first%20pretrained%20uncertainty%20modules%20for%20vision%20models.%20Similar%0Ato%20standard%20pretraining%20this%20enables%20the%20zero-shot%20transfer%20of%20uncertainties%0Alearned%20on%20a%20large%20pretraining%20dataset%20to%20specialized%20downstream%20datasets.%20We%0Aenable%20our%20large-scale%20pretraining%20on%20ImageNet-21k%20by%20solving%20a%20gradient%0Aconflict%20in%20previous%20uncertainty%20modules%20and%20accelerating%20the%20training%20by%20up%20to%0A180x.%20We%20find%20that%20the%20pretrained%20uncertainties%20generalize%20to%20unseen%20datasets.%0AIn%20scrutinizing%20the%20learned%20uncertainties%2C%20we%20find%20that%20they%20capture%20aleatoric%0Auncertainty%2C%20disentangled%20from%20epistemic%20components.%20We%20demonstrate%20that%20this%0Aenables%20safe%20retrieval%20and%20uncertainty-aware%20dataset%20visualization.%20To%0Aencourage%20applications%20to%20further%20problems%20and%20domains%2C%20we%20release%20all%0Apretrained%20checkpoints%20and%20code%20under%20https%3A//github.com/mkirchhof/url%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16569v2&entry.124074799=Read"},
{"title": "Robustly Learning Single-Index Models via Alignment Sharpness", "author": "Nikos Zarifis and Puqian Wang and Ilias Diakonikolas and Jelena Diakonikolas", "abstract": "  We study the problem of learning Single-Index Models under the $L_2^2$ loss\nin the agnostic model. We give an efficient learning algorithm, achieving a\nconstant factor approximation to the optimal loss, that succeeds under a range\nof distributions (including log-concave distributions) and a broad class of\nmonotone and Lipschitz link functions. This is the first efficient constant\nfactor approximate agnostic learner, even for Gaussian data and for any\nnontrivial class of link functions. Prior work for the case of unknown link\nfunction either works in the realizable setting or does not attain constant\nfactor approximation. The main technical ingredient enabling our algorithm and\nanalysis is a novel notion of a local error bound in optimization that we term\nalignment sharpness and that may be of broader interest.\n", "link": "http://arxiv.org/abs/2402.17756v1", "date": "2024-02-27", "relevancy": 2.3072, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4901}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4536}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4406}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustly%20Learning%20Single-Index%20Models%20via%20Alignment%20Sharpness&entry.906535625=Nikos%20Zarifis%20and%20Puqian%20Wang%20and%20Ilias%20Diakonikolas%20and%20Jelena%20Diakonikolas&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20Single-Index%20Models%20under%20the%20%24L_2%5E2%24%20loss%0Ain%20the%20agnostic%20model.%20We%20give%20an%20efficient%20learning%20algorithm%2C%20achieving%20a%0Aconstant%20factor%20approximation%20to%20the%20optimal%20loss%2C%20that%20succeeds%20under%20a%20range%0Aof%20distributions%20%28including%20log-concave%20distributions%29%20and%20a%20broad%20class%20of%0Amonotone%20and%20Lipschitz%20link%20functions.%20This%20is%20the%20first%20efficient%20constant%0Afactor%20approximate%20agnostic%20learner%2C%20even%20for%20Gaussian%20data%20and%20for%20any%0Anontrivial%20class%20of%20link%20functions.%20Prior%20work%20for%20the%20case%20of%20unknown%20link%0Afunction%20either%20works%20in%20the%20realizable%20setting%20or%20does%20not%20attain%20constant%0Afactor%20approximation.%20The%20main%20technical%20ingredient%20enabling%20our%20algorithm%20and%0Aanalysis%20is%20a%20novel%20notion%20of%20a%20local%20error%20bound%20in%20optimization%20that%20we%20term%0Aalignment%20sharpness%20and%20that%20may%20be%20of%20broader%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17756v1&entry.124074799=Read"},
{"title": "PonderV2: Pave the Way for 3D Foundation Model with A Universal\n  Pre-training Paradigm", "author": "Haoyi Zhu and Honghui Yang and Xiaoyang Wu and Di Huang and Sha Zhang and Xianglong He and Hengshuang Zhao and Chunhua Shen and Yu Qiao and Tong He and Wanli Ouyang", "abstract": "  In contrast to numerous NLP and 2D vision foundational models, learning a 3D\nfoundational model poses considerably greater challenges. This is primarily due\nto the inherent data variability and diversity of downstream tasks. In this\npaper, we introduce a novel universal 3D pre-training framework designed to\nfacilitate the acquisition of efficient 3D representation, thereby establishing\na pathway to 3D foundational models. Considering that informative 3D features\nshould encode rich geometry and appearance cues that can be utilized to render\nrealistic images, we propose to learn 3D representations by differentiable\nneural rendering. We train a 3D backbone with a devised volumetric neural\nrenderer by comparing the rendered with the real images. Notably, our approach\nseamlessly integrates the learned 3D encoder into various downstream tasks.\nThese tasks encompass not only high-level challenges such as 3D detection and\nsegmentation but also low-level objectives like 3D reconstruction and image\nsynthesis, spanning both indoor and outdoor scenarios. Besides, we also\nillustrate the capability of pre-training a 2D backbone using the proposed\nmethodology, surpassing conventional pre-training methods by a large margin.\nFor the first time, PonderV2 achieves state-of-the-art performance on 11 indoor\nand outdoor benchmarks, implying its effectiveness. Code and models are\navailable at https://github.com/OpenGVLab/PonderV2.\n", "link": "http://arxiv.org/abs/2310.08586v3", "date": "2024-02-27", "relevancy": 2.3022, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5685}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5414}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PonderV2%3A%20Pave%20the%20Way%20for%203D%20Foundation%20Model%20with%20A%20Universal%0A%20%20Pre-training%20Paradigm&entry.906535625=Haoyi%20Zhu%20and%20Honghui%20Yang%20and%20Xiaoyang%20Wu%20and%20Di%20Huang%20and%20Sha%20Zhang%20and%20Xianglong%20He%20and%20Hengshuang%20Zhao%20and%20Chunhua%20Shen%20and%20Yu%20Qiao%20and%20Tong%20He%20and%20Wanli%20Ouyang&entry.1292438233=%20%20In%20contrast%20to%20numerous%20NLP%20and%202D%20vision%20foundational%20models%2C%20learning%20a%203D%0Afoundational%20model%20poses%20considerably%20greater%20challenges.%20This%20is%20primarily%20due%0Ato%20the%20inherent%20data%20variability%20and%20diversity%20of%20downstream%20tasks.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20universal%203D%20pre-training%20framework%20designed%20to%0Afacilitate%20the%20acquisition%20of%20efficient%203D%20representation%2C%20thereby%20establishing%0Aa%20pathway%20to%203D%20foundational%20models.%20Considering%20that%20informative%203D%20features%0Ashould%20encode%20rich%20geometry%20and%20appearance%20cues%20that%20can%20be%20utilized%20to%20render%0Arealistic%20images%2C%20we%20propose%20to%20learn%203D%20representations%20by%20differentiable%0Aneural%20rendering.%20We%20train%20a%203D%20backbone%20with%20a%20devised%20volumetric%20neural%0Arenderer%20by%20comparing%20the%20rendered%20with%20the%20real%20images.%20Notably%2C%20our%20approach%0Aseamlessly%20integrates%20the%20learned%203D%20encoder%20into%20various%20downstream%20tasks.%0AThese%20tasks%20encompass%20not%20only%20high-level%20challenges%20such%20as%203D%20detection%20and%0Asegmentation%20but%20also%20low-level%20objectives%20like%203D%20reconstruction%20and%20image%0Asynthesis%2C%20spanning%20both%20indoor%20and%20outdoor%20scenarios.%20Besides%2C%20we%20also%0Aillustrate%20the%20capability%20of%20pre-training%20a%202D%20backbone%20using%20the%20proposed%0Amethodology%2C%20surpassing%20conventional%20pre-training%20methods%20by%20a%20large%20margin.%0AFor%20the%20first%20time%2C%20PonderV2%20achieves%20state-of-the-art%20performance%20on%2011%20indoor%0Aand%20outdoor%20benchmarks%2C%20implying%20its%20effectiveness.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/OpenGVLab/PonderV2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08586v3&entry.124074799=Read"},
{"title": "Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image\n  Modeling", "author": "David S. W. Williams and Matthew Gadd and Paul Newman and Daniele De Martini", "abstract": "  This work proposes a semantic segmentation network that produces high-quality\nuncertainty estimates in a single forward pass. We exploit general\nrepresentations from foundation models and unlabelled datasets through a Masked\nImage Modeling (MIM) approach, which is robust to augmentation hyper-parameters\nand simpler than previous techniques. For neural networks used in\nsafety-critical applications, bias in the training data can lead to errors;\ntherefore it is crucial to understand a network's limitations at run time and\nact accordingly. To this end, we test our proposed method on a number of test\ndomains including the SAX Segmentation benchmark, which includes labelled test\ndata from dense urban, rural and off-road driving domains. The proposed method\nconsistently outperforms uncertainty estimation and Out-of-Distribution (OoD)\ntechniques on this difficult benchmark.\n", "link": "http://arxiv.org/abs/2402.17622v1", "date": "2024-02-27", "relevancy": 2.2966, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6187}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6014}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5292}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Gamma-SSL%3A%20Learning%20Uncertainty%20Estimation%20via%20Masked%20Image%0A%20%20Modeling&entry.906535625=David%20S.%20W.%20Williams%20and%20Matthew%20Gadd%20and%20Paul%20Newman%20and%20Daniele%20De%20Martini&entry.1292438233=%20%20This%20work%20proposes%20a%20semantic%20segmentation%20network%20that%20produces%20high-quality%0Auncertainty%20estimates%20in%20a%20single%20forward%20pass.%20We%20exploit%20general%0Arepresentations%20from%20foundation%20models%20and%20unlabelled%20datasets%20through%20a%20Masked%0AImage%20Modeling%20%28MIM%29%20approach%2C%20which%20is%20robust%20to%20augmentation%20hyper-parameters%0Aand%20simpler%20than%20previous%20techniques.%20For%20neural%20networks%20used%20in%0Asafety-critical%20applications%2C%20bias%20in%20the%20training%20data%20can%20lead%20to%20errors%3B%0Atherefore%20it%20is%20crucial%20to%20understand%20a%20network%27s%20limitations%20at%20run%20time%20and%0Aact%20accordingly.%20To%20this%20end%2C%20we%20test%20our%20proposed%20method%20on%20a%20number%20of%20test%0Adomains%20including%20the%20SAX%20Segmentation%20benchmark%2C%20which%20includes%20labelled%20test%0Adata%20from%20dense%20urban%2C%20rural%20and%20off-road%20driving%20domains.%20The%20proposed%20method%0Aconsistently%20outperforms%20uncertainty%20estimation%20and%20Out-of-Distribution%20%28OoD%29%0Atechniques%20on%20this%20difficult%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17622v1&entry.124074799=Read"},
{"title": "Autonomous Shuttle Operation for Vulnerable Populations: Lessons and\n  Experiences", "author": "Ren Zhong and Zhaofeng Tian and Jinghui Liao and Weisong Shi", "abstract": "  The increasing shortage of drivers poses a significant threat to vulnerable\npopulations, particularly seniors and disabled individuals who heavily depend\non public transportation for accessing healthcare services and social events.\nAutonomous Vehicles (AVs) emerge as a promising alternative, offering potential\nimprovements in accessibility and independence for these groups. However,\ncurrent designs and studies often overlook the unique needs and experiences of\nthese populations, leading to potential accessibility barriers. This paper\npresents a detailed case study of an autonomous shuttle test specifically\ntailored for seniors and disabled individuals, conducted during the early\nstages of the COVID-19 pandemic. The service, which lasted 13 weeks, catered to\napproximately 1500 passengers in an urban setting, aiming to facilitate access\nto essential services. Drawing from the safety operator's experiences and\ndirect observations, we identify critical user experience and safety challenges\nfaced by vulnerable passengers. Based on our findings, we propose targeted\ninitiatives to enhance the safety, accessibility, and user education of AV\ntechnology for seniors and disabled individuals. These include increasing\neducational opportunities to familiarize these groups with AV technology,\ndesigning AVs with a focus on diversity and inclusion, and improving training\nprograms for AV operators to address the unique needs of vulnerable\npopulations. Through these initiatives, we aim to bridge the gap in AV\naccessibility and ensure that these technologies benefit all members of\nsociety.\n", "link": "http://arxiv.org/abs/2402.17593v1", "date": "2024-02-27", "relevancy": 2.2892, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.553}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4229}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3976}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Shuttle%20Operation%20for%20Vulnerable%20Populations%3A%20Lessons%20and%0A%20%20Experiences&entry.906535625=Ren%20Zhong%20and%20Zhaofeng%20Tian%20and%20Jinghui%20Liao%20and%20Weisong%20Shi&entry.1292438233=%20%20The%20increasing%20shortage%20of%20drivers%20poses%20a%20significant%20threat%20to%20vulnerable%0Apopulations%2C%20particularly%20seniors%20and%20disabled%20individuals%20who%20heavily%20depend%0Aon%20public%20transportation%20for%20accessing%20healthcare%20services%20and%20social%20events.%0AAutonomous%20Vehicles%20%28AVs%29%20emerge%20as%20a%20promising%20alternative%2C%20offering%20potential%0Aimprovements%20in%20accessibility%20and%20independence%20for%20these%20groups.%20However%2C%0Acurrent%20designs%20and%20studies%20often%20overlook%20the%20unique%20needs%20and%20experiences%20of%0Athese%20populations%2C%20leading%20to%20potential%20accessibility%20barriers.%20This%20paper%0Apresents%20a%20detailed%20case%20study%20of%20an%20autonomous%20shuttle%20test%20specifically%0Atailored%20for%20seniors%20and%20disabled%20individuals%2C%20conducted%20during%20the%20early%0Astages%20of%20the%20COVID-19%20pandemic.%20The%20service%2C%20which%20lasted%2013%20weeks%2C%20catered%20to%0Aapproximately%201500%20passengers%20in%20an%20urban%20setting%2C%20aiming%20to%20facilitate%20access%0Ato%20essential%20services.%20Drawing%20from%20the%20safety%20operator%27s%20experiences%20and%0Adirect%20observations%2C%20we%20identify%20critical%20user%20experience%20and%20safety%20challenges%0Afaced%20by%20vulnerable%20passengers.%20Based%20on%20our%20findings%2C%20we%20propose%20targeted%0Ainitiatives%20to%20enhance%20the%20safety%2C%20accessibility%2C%20and%20user%20education%20of%20AV%0Atechnology%20for%20seniors%20and%20disabled%20individuals.%20These%20include%20increasing%0Aeducational%20opportunities%20to%20familiarize%20these%20groups%20with%20AV%20technology%2C%0Adesigning%20AVs%20with%20a%20focus%20on%20diversity%20and%20inclusion%2C%20and%20improving%20training%0Aprograms%20for%20AV%20operators%20to%20address%20the%20unique%20needs%20of%20vulnerable%0Apopulations.%20Through%20these%20initiatives%2C%20we%20aim%20to%20bridge%20the%20gap%20in%20AV%0Aaccessibility%20and%20ensure%20that%20these%20technologies%20benefit%20all%20members%20of%0Asociety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17593v1&entry.124074799=Read"},
{"title": "Taming Nonconvex Stochastic Mirror Descent with General Bregman\n  Divergence", "author": "Ilyas Fatkhullin and Niao He", "abstract": "  This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the\ncontemporary nonconvex optimization setting. Existing results for batch-free\nnonconvex SMD restrict the choice of the distance generating function (DGF) to\nbe differentiable with Lipschitz continuous gradients, thereby excluding\nimportant setups such as Shannon entropy. In this work, we present a new\nconvergence analysis of nonconvex SMD supporting general DGF, that overcomes\nthe above limitations and relies solely on the standard assumptions. Moreover,\nour convergence is established with respect to the Bregman Forward-Backward\nenvelope, which is a stronger measure than the commonly used squared norm of\ngradient mapping. We further extend our results to guarantee high probability\nconvergence under sub-Gaussian noise and global convergence under the\ngeneralized Bregman Proximal Polyak-{\\L}ojasiewicz condition. Additionally, we\nillustrate the advantages of our improved SMD theory in various nonconvex\nmachine learning tasks by harnessing nonsmooth DGFs. Notably, in the context of\nnonconvex differentially private (DP) learning, our theory yields a simple\nalgorithm with a (nearly) dimension-independent utility bound. For the problem\nof training linear neural networks, we develop provably convergent stochastic\nalgorithms.\n", "link": "http://arxiv.org/abs/2402.17722v1", "date": "2024-02-27", "relevancy": 2.2812, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.463}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4551}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4506}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Nonconvex%20Stochastic%20Mirror%20Descent%20with%20General%20Bregman%0A%20%20Divergence&entry.906535625=Ilyas%20Fatkhullin%20and%20Niao%20He&entry.1292438233=%20%20This%20paper%20revisits%20the%20convergence%20of%20Stochastic%20Mirror%20Descent%20%28SMD%29%20in%20the%0Acontemporary%20nonconvex%20optimization%20setting.%20Existing%20results%20for%20batch-free%0Anonconvex%20SMD%20restrict%20the%20choice%20of%20the%20distance%20generating%20function%20%28DGF%29%20to%0Abe%20differentiable%20with%20Lipschitz%20continuous%20gradients%2C%20thereby%20excluding%0Aimportant%20setups%20such%20as%20Shannon%20entropy.%20In%20this%20work%2C%20we%20present%20a%20new%0Aconvergence%20analysis%20of%20nonconvex%20SMD%20supporting%20general%20DGF%2C%20that%20overcomes%0Athe%20above%20limitations%20and%20relies%20solely%20on%20the%20standard%20assumptions.%20Moreover%2C%0Aour%20convergence%20is%20established%20with%20respect%20to%20the%20Bregman%20Forward-Backward%0Aenvelope%2C%20which%20is%20a%20stronger%20measure%20than%20the%20commonly%20used%20squared%20norm%20of%0Agradient%20mapping.%20We%20further%20extend%20our%20results%20to%20guarantee%20high%20probability%0Aconvergence%20under%20sub-Gaussian%20noise%20and%20global%20convergence%20under%20the%0Ageneralized%20Bregman%20Proximal%20Polyak-%7B%5CL%7Dojasiewicz%20condition.%20Additionally%2C%20we%0Aillustrate%20the%20advantages%20of%20our%20improved%20SMD%20theory%20in%20various%20nonconvex%0Amachine%20learning%20tasks%20by%20harnessing%20nonsmooth%20DGFs.%20Notably%2C%20in%20the%20context%20of%0Anonconvex%20differentially%20private%20%28DP%29%20learning%2C%20our%20theory%20yields%20a%20simple%0Aalgorithm%20with%20a%20%28nearly%29%20dimension-independent%20utility%20bound.%20For%20the%20problem%0Aof%20training%20linear%20neural%20networks%2C%20we%20develop%20provably%20convergent%20stochastic%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17722v1&entry.124074799=Read"},
{"title": "Wisdom of Committee: Distilling from Foundation Model to Specialized\n  Application Model", "author": "Zichang Liu and Qingyun Liu and Yuening Li and Liang Liu and Anshumali Shrivastava and Shuchao Bi and Lichan Hong and Ed H. Chi and Zhe Zhao", "abstract": "  Recent advancements in foundation models have yielded impressive performance\nacross a wide range of tasks. Meanwhile, for specific applications,\npractitioners have been developing specialized application models. To enjoy the\nbenefits of both kinds of models, one natural path is to transfer the knowledge\nin foundation models into specialized application models, which are generally\nmore efficient for serving. Techniques from knowledge distillation may be\napplied here, where the application model learns to mimic the foundation model.\nHowever, specialized application models and foundation models have substantial\ngaps in capacity, employing distinct architectures, using different input\nfeatures from different modalities, and being optimized on different\ndistributions. These differences in model characteristics lead to significant\nchallenges for distillation methods. In this work, we propose creating a\nteaching committee comprising both foundation model teachers and complementary\nteachers. Complementary teachers possess model characteristics akin to the\nstudent's, aiming to bridge the gap between the foundation model and\nspecialized application models for a smoother knowledge transfer. Further, to\naccommodate the dissimilarity among the teachers in the committee, we introduce\nDiverseDistill, which allows the student to understand the expertise of each\nteacher and extract task knowledge. Our evaluations demonstrate that adding\ncomplementary teachers enhances student performance. Finally, DiverseDistill\nconsistently outperforms baseline distillation methods, regardless of the\nteacher choices, resulting in significantly improved student performance.\n", "link": "http://arxiv.org/abs/2402.14035v2", "date": "2024-02-27", "relevancy": 2.2776, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4609}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4571}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4487}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wisdom%20of%20Committee%3A%20Distilling%20from%20Foundation%20Model%20to%20Specialized%0A%20%20Application%20Model&entry.906535625=Zichang%20Liu%20and%20Qingyun%20Liu%20and%20Yuening%20Li%20and%20Liang%20Liu%20and%20Anshumali%20Shrivastava%20and%20Shuchao%20Bi%20and%20Lichan%20Hong%20and%20Ed%20H.%20Chi%20and%20Zhe%20Zhao&entry.1292438233=%20%20Recent%20advancements%20in%20foundation%20models%20have%20yielded%20impressive%20performance%0Aacross%20a%20wide%20range%20of%20tasks.%20Meanwhile%2C%20for%20specific%20applications%2C%0Apractitioners%20have%20been%20developing%20specialized%20application%20models.%20To%20enjoy%20the%0Abenefits%20of%20both%20kinds%20of%20models%2C%20one%20natural%20path%20is%20to%20transfer%20the%20knowledge%0Ain%20foundation%20models%20into%20specialized%20application%20models%2C%20which%20are%20generally%0Amore%20efficient%20for%20serving.%20Techniques%20from%20knowledge%20distillation%20may%20be%0Aapplied%20here%2C%20where%20the%20application%20model%20learns%20to%20mimic%20the%20foundation%20model.%0AHowever%2C%20specialized%20application%20models%20and%20foundation%20models%20have%20substantial%0Agaps%20in%20capacity%2C%20employing%20distinct%20architectures%2C%20using%20different%20input%0Afeatures%20from%20different%20modalities%2C%20and%20being%20optimized%20on%20different%0Adistributions.%20These%20differences%20in%20model%20characteristics%20lead%20to%20significant%0Achallenges%20for%20distillation%20methods.%20In%20this%20work%2C%20we%20propose%20creating%20a%0Ateaching%20committee%20comprising%20both%20foundation%20model%20teachers%20and%20complementary%0Ateachers.%20Complementary%20teachers%20possess%20model%20characteristics%20akin%20to%20the%0Astudent%27s%2C%20aiming%20to%20bridge%20the%20gap%20between%20the%20foundation%20model%20and%0Aspecialized%20application%20models%20for%20a%20smoother%20knowledge%20transfer.%20Further%2C%20to%0Aaccommodate%20the%20dissimilarity%20among%20the%20teachers%20in%20the%20committee%2C%20we%20introduce%0ADiverseDistill%2C%20which%20allows%20the%20student%20to%20understand%20the%20expertise%20of%20each%0Ateacher%20and%20extract%20task%20knowledge.%20Our%20evaluations%20demonstrate%20that%20adding%0Acomplementary%20teachers%20enhances%20student%20performance.%20Finally%2C%20DiverseDistill%0Aconsistently%20outperforms%20baseline%20distillation%20methods%2C%20regardless%20of%20the%0Ateacher%20choices%2C%20resulting%20in%20significantly%20improved%20student%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14035v2&entry.124074799=Read"},
{"title": "HRTF upsampling with a generative adversarial network using a gnomonic\n  equiangular projection", "author": "Aidan O. T. Hogg and Mads Jenkins and He Liu and Isaac Squires and Samuel J. Cooper and Lorenzo Picinali", "abstract": "  An individualised head-related transfer function (HRTF) is very important for\ncreating realistic virtual reality (VR) and augmented reality (AR)\nenvironments. However, acoustically measuring high-quality HRTFs requires\nexpensive equipment and an acoustic lab setting. To overcome these limitations\nand to make this measurement more efficient HRTF upsampling has been exploited\nin the past where a high-resolution HRTF is created from a low-resolution one.\nThis paper demonstrates how generative adversarial networks (GANs) can be\napplied to HRTF upsampling. We propose a novel approach that transforms the\nHRTF data for direct use with a convolutional super-resolution generative\nadversarial network (SRGAN). This new approach is benchmarked against three\nbaselines: barycentric upsampling, spherical harmonic (SH) upsampling and an\nHRTF selection approach. Experimental results show that the proposed method\noutperforms all three baselines in terms of log-spectral distortion (LSD) and\nlocalisation performance using perceptual models when the input HRTF is sparse\n(less than 20 measured positions).\n", "link": "http://arxiv.org/abs/2306.05812v2", "date": "2024-02-27", "relevancy": 2.2678, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4632}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4564}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4411}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HRTF%20upsampling%20with%20a%20generative%20adversarial%20network%20using%20a%20gnomonic%0A%20%20equiangular%20projection&entry.906535625=Aidan%20O.%20T.%20Hogg%20and%20Mads%20Jenkins%20and%20He%20Liu%20and%20Isaac%20Squires%20and%20Samuel%20J.%20Cooper%20and%20Lorenzo%20Picinali&entry.1292438233=%20%20An%20individualised%20head-related%20transfer%20function%20%28HRTF%29%20is%20very%20important%20for%0Acreating%20realistic%20virtual%20reality%20%28VR%29%20and%20augmented%20reality%20%28AR%29%0Aenvironments.%20However%2C%20acoustically%20measuring%20high-quality%20HRTFs%20requires%0Aexpensive%20equipment%20and%20an%20acoustic%20lab%20setting.%20To%20overcome%20these%20limitations%0Aand%20to%20make%20this%20measurement%20more%20efficient%20HRTF%20upsampling%20has%20been%20exploited%0Ain%20the%20past%20where%20a%20high-resolution%20HRTF%20is%20created%20from%20a%20low-resolution%20one.%0AThis%20paper%20demonstrates%20how%20generative%20adversarial%20networks%20%28GANs%29%20can%20be%0Aapplied%20to%20HRTF%20upsampling.%20We%20propose%20a%20novel%20approach%20that%20transforms%20the%0AHRTF%20data%20for%20direct%20use%20with%20a%20convolutional%20super-resolution%20generative%0Aadversarial%20network%20%28SRGAN%29.%20This%20new%20approach%20is%20benchmarked%20against%20three%0Abaselines%3A%20barycentric%20upsampling%2C%20spherical%20harmonic%20%28SH%29%20upsampling%20and%20an%0AHRTF%20selection%20approach.%20Experimental%20results%20show%20that%20the%20proposed%20method%0Aoutperforms%20all%20three%20baselines%20in%20terms%20of%20log-spectral%20distortion%20%28LSD%29%20and%0Alocalisation%20performance%20using%20perceptual%20models%20when%20the%20input%20HRTF%20is%20sparse%0A%28less%20than%2020%20measured%20positions%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05812v2&entry.124074799=Read"},
{"title": "CGGM: A conditional graph generation model with adaptive sparsity for\n  node anomaly detection in IoT networks", "author": "Xianshi Su and Munan Li and Tongbang Jiang and Hao Long", "abstract": "  Dynamic graphs are extensively employed for detecting anomalous behavior in\nnodes within the Internet of Things (IoT). Generative models are often used to\naddress the issue of imbalanced node categories in dynamic graphs.\nNevertheless, the constraints it faces include the monotonicity of adjacency\nrelationships, the difficulty in constructing multi-dimensional features for\nnodes, and the lack of a method for end-to-end generation of multiple\ncategories of nodes. This paper presents a novel graph generation model, called\nCGGM, designed specifically to generate a larger number of nodes belonging to\nthe minority class. The mechanism for generating an adjacency matrix, through\nadaptive sparsity, enhances flexibility in its structure. The feature\ngeneration module, called multidimensional features generator (MFG) to generate\nnode features along with topological information. Labels are transformed into\nembedding vectors, serving as conditional constraints to control the generation\nof synthetic data across multiple categories. Using a multi-stage loss, the\ndistribution of synthetic data is adjusted to closely resemble that of real\ndata. In extensive experiments, we show that CGGM's synthetic data outperforms\nstate-of-the-art methods across various metrics. Our results demonstrate\nefficient generation of diverse data categories, robustly enhancing\nmulti-category classification model performance.\n", "link": "http://arxiv.org/abs/2402.17363v1", "date": "2024-02-27", "relevancy": 2.2663, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.602}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5559}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5047}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CGGM%3A%20A%20conditional%20graph%20generation%20model%20with%20adaptive%20sparsity%20for%0A%20%20node%20anomaly%20detection%20in%20IoT%20networks&entry.906535625=Xianshi%20Su%20and%20Munan%20Li%20and%20Tongbang%20Jiang%20and%20Hao%20Long&entry.1292438233=%20%20Dynamic%20graphs%20are%20extensively%20employed%20for%20detecting%20anomalous%20behavior%20in%0Anodes%20within%20the%20Internet%20of%20Things%20%28IoT%29.%20Generative%20models%20are%20often%20used%20to%0Aaddress%20the%20issue%20of%20imbalanced%20node%20categories%20in%20dynamic%20graphs.%0ANevertheless%2C%20the%20constraints%20it%20faces%20include%20the%20monotonicity%20of%20adjacency%0Arelationships%2C%20the%20difficulty%20in%20constructing%20multi-dimensional%20features%20for%0Anodes%2C%20and%20the%20lack%20of%20a%20method%20for%20end-to-end%20generation%20of%20multiple%0Acategories%20of%20nodes.%20This%20paper%20presents%20a%20novel%20graph%20generation%20model%2C%20called%0ACGGM%2C%20designed%20specifically%20to%20generate%20a%20larger%20number%20of%20nodes%20belonging%20to%0Athe%20minority%20class.%20The%20mechanism%20for%20generating%20an%20adjacency%20matrix%2C%20through%0Aadaptive%20sparsity%2C%20enhances%20flexibility%20in%20its%20structure.%20The%20feature%0Ageneration%20module%2C%20called%20multidimensional%20features%20generator%20%28MFG%29%20to%20generate%0Anode%20features%20along%20with%20topological%20information.%20Labels%20are%20transformed%20into%0Aembedding%20vectors%2C%20serving%20as%20conditional%20constraints%20to%20control%20the%20generation%0Aof%20synthetic%20data%20across%20multiple%20categories.%20Using%20a%20multi-stage%20loss%2C%20the%0Adistribution%20of%20synthetic%20data%20is%20adjusted%20to%20closely%20resemble%20that%20of%20real%0Adata.%20In%20extensive%20experiments%2C%20we%20show%20that%20CGGM%27s%20synthetic%20data%20outperforms%0Astate-of-the-art%20methods%20across%20various%20metrics.%20Our%20results%20demonstrate%0Aefficient%20generation%20of%20diverse%20data%20categories%2C%20robustly%20enhancing%0Amulti-category%20classification%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17363v1&entry.124074799=Read"},
{"title": "Structure-Guided Adversarial Training of Diffusion Models", "author": "Ling Yang and Haotian Qian and Zhilong Zhang and Jingwei Liu and Bin Cui", "abstract": "  Diffusion models have demonstrated exceptional efficacy in various generative\napplications. While existing models focus on minimizing a weighted sum of\ndenoising score matching losses for data distribution modeling, their training\nprimarily emphasizes instance-level optimization, overlooking valuable\nstructural information within each mini-batch, indicative of pair-wise\nrelationships among samples. To address this limitation, we introduce\nStructure-guided Adversarial training of Diffusion Models (SADM). In this\npioneering approach, we compel the model to learn manifold structures between\nsamples in each training batch. To ensure the model captures authentic manifold\nstructures in the data distribution, we advocate adversarial training of the\ndiffusion generator against a novel structure discriminator in a minimax game,\ndistinguishing real manifold structures from the generated ones. SADM\nsubstantially improves existing diffusion transformers (DiT) and outperforms\nexisting methods in image generation and cross-domain fine-tuning tasks across\n12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on\nImageNet for class-conditional image generation at resolutions of 256x256 and\n512x512, respectively.\n", "link": "http://arxiv.org/abs/2402.17563v1", "date": "2024-02-27", "relevancy": 2.2628, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5844}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5735}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5439}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Guided%20Adversarial%20Training%20of%20Diffusion%20Models&entry.906535625=Ling%20Yang%20and%20Haotian%20Qian%20and%20Zhilong%20Zhang%20and%20Jingwei%20Liu%20and%20Bin%20Cui&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20exceptional%20efficacy%20in%20various%20generative%0Aapplications.%20While%20existing%20models%20focus%20on%20minimizing%20a%20weighted%20sum%20of%0Adenoising%20score%20matching%20losses%20for%20data%20distribution%20modeling%2C%20their%20training%0Aprimarily%20emphasizes%20instance-level%20optimization%2C%20overlooking%20valuable%0Astructural%20information%20within%20each%20mini-batch%2C%20indicative%20of%20pair-wise%0Arelationships%20among%20samples.%20To%20address%20this%20limitation%2C%20we%20introduce%0AStructure-guided%20Adversarial%20training%20of%20Diffusion%20Models%20%28SADM%29.%20In%20this%0Apioneering%20approach%2C%20we%20compel%20the%20model%20to%20learn%20manifold%20structures%20between%0Asamples%20in%20each%20training%20batch.%20To%20ensure%20the%20model%20captures%20authentic%20manifold%0Astructures%20in%20the%20data%20distribution%2C%20we%20advocate%20adversarial%20training%20of%20the%0Adiffusion%20generator%20against%20a%20novel%20structure%20discriminator%20in%20a%20minimax%20game%2C%0Adistinguishing%20real%20manifold%20structures%20from%20the%20generated%20ones.%20SADM%0Asubstantially%20improves%20existing%20diffusion%20transformers%20%28DiT%29%20and%20outperforms%0Aexisting%20methods%20in%20image%20generation%20and%20cross-domain%20fine-tuning%20tasks%20across%0A12%20datasets%2C%20establishing%20a%20new%20state-of-the-art%20FID%20of%201.58%20and%202.11%20on%0AImageNet%20for%20class-conditional%20image%20generation%20at%20resolutions%20of%20256x256%20and%0A512x512%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17563v1&entry.124074799=Read"},
{"title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and\n  Optimization", "author": "Wenqi Zhang and Ke Tang and Hai Wu and Mengna Wang and Yongliang Shen and Guiyang Hou and Zeqi Tan and Peng Li and Yueting Zhuang and Weiming Lu", "abstract": "  Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.\n", "link": "http://arxiv.org/abs/2402.17574v1", "date": "2024-02-27", "relevancy": 2.2537, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5937}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5578}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5354}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent-Pro%3A%20Learning%20to%20Evolve%20via%20Policy-Level%20Reflection%20and%0A%20%20Optimization&entry.906535625=Wenqi%20Zhang%20and%20Ke%20Tang%20and%20Hai%20Wu%20and%20Mengna%20Wang%20and%20Yongliang%20Shen%20and%20Guiyang%20Hou%20and%20Zeqi%20Tan%20and%20Peng%20Li%20and%20Yueting%20Zhuang%20and%20Weiming%20Lu&entry.1292438233=%20%20Large%20Language%20Models%20exhibit%20robust%20problem-solving%20capabilities%20for%20diverse%0Atasks.%20However%2C%20most%20LLM-based%20agents%20are%20designed%20as%20specific%20task%20solvers%0Awith%20sophisticated%20prompt%20engineering%2C%20rather%20than%20agents%20capable%20of%20learning%0Aand%20evolving%20through%20interactions.%20These%20task%20solvers%20necessitate%20manually%0Acrafted%20prompts%20to%20inform%20task%20rules%20and%20regulate%20LLM%20behaviors%2C%20inherently%0Aincapacitating%20to%20address%20complex%20dynamic%20scenarios%20e.g.%2C%20large%20interactive%0Agames.%20In%20light%20of%20this%2C%20we%20propose%20Agent-Pro%3A%20an%20LLM-based%20Agent%20with%0APolicy-level%20Reflection%20and%20Optimization%20that%20can%20learn%20a%20wealth%20of%20expertise%0Afrom%20interactive%20experiences%20and%20progressively%20elevate%20its%20behavioral%20policy.%0ASpecifically%2C%20it%20involves%20a%20dynamic%20belief%20generation%20and%20reflection%20process%0Afor%20policy%20evolution.%20Rather%20than%20action-level%20reflection%2C%20Agent-Pro%0Aiteratively%20reflects%20on%20past%20trajectories%20and%20beliefs%2C%20fine-tuning%20its%0Airrational%20beliefs%20for%20a%20better%20policy.%20Moreover%2C%20a%20depth-first%20search%20is%0Aemployed%20for%20policy%20optimization%2C%20ensuring%20continual%20enhancement%20in%20policy%0Apayoffs.%20Agent-Pro%20is%20evaluated%20across%20two%20games%3A%20Blackjack%20and%20Texas%20Hold%27em%2C%0Aoutperforming%20vanilla%20LLM%20and%20specialized%20models.%20Our%20results%20show%20Agent-Pro%0Acan%20learn%20and%20evolve%20in%20complex%20and%20dynamic%20scenes%2C%20which%20also%20benefits%0Anumerous%20LLM-based%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17574v1&entry.124074799=Read"},
{"title": "AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile\n  Platform Real-Time RGB-D Semantic Segmentation", "author": "Siqi Du and Weixi Wang and Renzhong Guo and Shengjun Tang", "abstract": "  In the realm of robotic intelligence, achieving efficient and precise RGB-D\nsemantic segmentation is a key cornerstone. State-of-the-art multimodal\nsemantic segmentation methods, primarily rooted in symmetrical skeleton\nnetworks, find it challenging to harmonize computational efficiency and\nprecision. In this work, we propose AsymFormer, a novel network for real-time\nRGB-D semantic segmentation, which targets the minimization of superfluous\nparameters by optimizing the distribution of computational resources and\nintroduces an asymmetrical backbone to allow for the effective fusion of\nmultimodal features. Furthermore, we explore techniques to bolster network\naccuracy by redefining feature selection and extracting multi-modal\nself-similarity features without a substantial increase in the parameter count,\nthereby ensuring real-time execution on robotic platforms. Additionally, a\nLocal Attention-Guided Feature Selection (LAFS) module is used to selectively\nfuse features from different modalities by leveraging their dependencies.\nSubsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding\n(CMA) module is introduced to further extract cross-modal representations. This\nmethod is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer\ndemonstrating competitive results with 54.1% mIoU on NYUv2 and 49.1% mIoU on\nSUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after\nimplementing mixed precision quantization, it attains an impressive inference\nspeed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal\nmethods, thereby demonstrating that AsymFormer can strike a balance between\nhigh accuracy and efficiency for RGB-D semantic segmentation.\n", "link": "http://arxiv.org/abs/2309.14065v5", "date": "2024-02-27", "relevancy": 2.2349, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5765}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5584}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5519}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AsymFormer%3A%20Asymmetrical%20Cross-Modal%20Representation%20Learning%20for%20Mobile%0A%20%20Platform%20Real-Time%20RGB-D%20Semantic%20Segmentation&entry.906535625=Siqi%20Du%20and%20Weixi%20Wang%20and%20Renzhong%20Guo%20and%20Shengjun%20Tang&entry.1292438233=%20%20In%20the%20realm%20of%20robotic%20intelligence%2C%20achieving%20efficient%20and%20precise%20RGB-D%0Asemantic%20segmentation%20is%20a%20key%20cornerstone.%20State-of-the-art%20multimodal%0Asemantic%20segmentation%20methods%2C%20primarily%20rooted%20in%20symmetrical%20skeleton%0Anetworks%2C%20find%20it%20challenging%20to%20harmonize%20computational%20efficiency%20and%0Aprecision.%20In%20this%20work%2C%20we%20propose%20AsymFormer%2C%20a%20novel%20network%20for%20real-time%0ARGB-D%20semantic%20segmentation%2C%20which%20targets%20the%20minimization%20of%20superfluous%0Aparameters%20by%20optimizing%20the%20distribution%20of%20computational%20resources%20and%0Aintroduces%20an%20asymmetrical%20backbone%20to%20allow%20for%20the%20effective%20fusion%20of%0Amultimodal%20features.%20Furthermore%2C%20we%20explore%20techniques%20to%20bolster%20network%0Aaccuracy%20by%20redefining%20feature%20selection%20and%20extracting%20multi-modal%0Aself-similarity%20features%20without%20a%20substantial%20increase%20in%20the%20parameter%20count%2C%0Athereby%20ensuring%20real-time%20execution%20on%20robotic%20platforms.%20Additionally%2C%20a%0ALocal%20Attention-Guided%20Feature%20Selection%20%28LAFS%29%20module%20is%20used%20to%20selectively%0Afuse%20features%20from%20different%20modalities%20by%20leveraging%20their%20dependencies.%0ASubsequently%2C%20a%20Cross-Modal%20Attention-Guided%20Feature%20Correlation%20Embedding%0A%28CMA%29%20module%20is%20introduced%20to%20further%20extract%20cross-modal%20representations.%20This%0Amethod%20is%20evaluated%20on%20NYUv2%20and%20SUNRGBD%20datasets%2C%20with%20AsymFormer%0Ademonstrating%20competitive%20results%20with%2054.1%25%20mIoU%20on%20NYUv2%20and%2049.1%25%20mIoU%20on%0ASUNRGBD.%20Notably%2C%20AsymFormer%20achieves%20an%20inference%20speed%20of%2065%20FPS%20and%20after%0Aimplementing%20mixed%20precision%20quantization%2C%20it%20attains%20an%20impressive%20inference%0Aspeed%20of%2079%20FPS%20on%20RTX3090.%20This%20significantly%20outperforms%20existing%20multi-modal%0Amethods%2C%20thereby%20demonstrating%20that%20AsymFormer%20can%20strike%20a%20balance%20between%0Ahigh%20accuracy%20and%20efficiency%20for%20RGB-D%20semantic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14065v5&entry.124074799=Read"},
{"title": "AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion\n  Models", "author": "Xuelong Dai and Kaisheng Liang and Bin Xiao", "abstract": "  Unrestricted adversarial attacks present a serious threat to deep learning\nmodels and adversarial defense techniques. They pose severe security problems\nfor deep learning applications because they can effectively bypass defense\nmechanisms. However, previous attack methods often utilize Generative\nAdversarial Networks (GANs), which are not theoretically provable and thus\ngenerate unrealistic examples by incorporating adversarial objectives,\nespecially for large-scale datasets like ImageNet. In this paper, we propose a\nnew method, called AdvDiff, to generate unrestricted adversarial examples with\ndiffusion models. We design two novel adversarial guidance techniques to\nconduct adversarial sampling in the reverse generation process of diffusion\nmodels. These two techniques are effective and stable to generate high-quality,\nrealistic adversarial examples by integrating gradients of the target\nclassifier interpretably. Experimental results on MNIST and ImageNet datasets\ndemonstrate that AdvDiff is effective to generate unrestricted adversarial\nexamples, which outperforms GAN-based methods in terms of attack performance\nand generation quality.\n", "link": "http://arxiv.org/abs/2307.12499v3", "date": "2024-02-27", "relevancy": 2.2041, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.579}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.536}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5186}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdvDiff%3A%20Generating%20Unrestricted%20Adversarial%20Examples%20using%20Diffusion%0A%20%20Models&entry.906535625=Xuelong%20Dai%20and%20Kaisheng%20Liang%20and%20Bin%20Xiao&entry.1292438233=%20%20Unrestricted%20adversarial%20attacks%20present%20a%20serious%20threat%20to%20deep%20learning%0Amodels%20and%20adversarial%20defense%20techniques.%20They%20pose%20severe%20security%20problems%0Afor%20deep%20learning%20applications%20because%20they%20can%20effectively%20bypass%20defense%0Amechanisms.%20However%2C%20previous%20attack%20methods%20often%20utilize%20Generative%0AAdversarial%20Networks%20%28GANs%29%2C%20which%20are%20not%20theoretically%20provable%20and%20thus%0Agenerate%20unrealistic%20examples%20by%20incorporating%20adversarial%20objectives%2C%0Aespecially%20for%20large-scale%20datasets%20like%20ImageNet.%20In%20this%20paper%2C%20we%20propose%20a%0Anew%20method%2C%20called%20AdvDiff%2C%20to%20generate%20unrestricted%20adversarial%20examples%20with%0Adiffusion%20models.%20We%20design%20two%20novel%20adversarial%20guidance%20techniques%20to%0Aconduct%20adversarial%20sampling%20in%20the%20reverse%20generation%20process%20of%20diffusion%0Amodels.%20These%20two%20techniques%20are%20effective%20and%20stable%20to%20generate%20high-quality%2C%0Arealistic%20adversarial%20examples%20by%20integrating%20gradients%20of%20the%20target%0Aclassifier%20interpretably.%20Experimental%20results%20on%20MNIST%20and%20ImageNet%20datasets%0Ademonstrate%20that%20AdvDiff%20is%20effective%20to%20generate%20unrestricted%20adversarial%0Aexamples%2C%20which%20outperforms%20GAN-based%20methods%20in%20terms%20of%20attack%20performance%0Aand%20generation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12499v3&entry.124074799=Read"},
{"title": "Real-time Low-latency Music Source Separation using Hybrid\n  Spectrogram-TasNet", "author": "Satvik Venkatesh and Arthur Benilov and Philip Coleman and Frederic Roskam", "abstract": "  There have been significant advances in deep learning for music demixing in\nrecent years. However, there has been little attention given to how these\nneural networks can be adapted for real-time low-latency applications, which\ncould be helpful for hearing aids, remixing audio streams and live shows. In\nthis paper, we investigate the various challenges involved in adapting current\ndemixing models in the literature for this use case. Subsequently, inspired by\nthe Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain\nAudio Separation Network HS-TasNet, which utilises the advantages of spectral\nand waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall\nsignal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases\nto 5.55 with additional training data. These results demonstrate the potential\nof efficient demixing for real-time low-latency music applications.\n", "link": "http://arxiv.org/abs/2402.17701v1", "date": "2024-02-27", "relevancy": 2.1918, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4471}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4418}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4261}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Low-latency%20Music%20Source%20Separation%20using%20Hybrid%0A%20%20Spectrogram-TasNet&entry.906535625=Satvik%20Venkatesh%20and%20Arthur%20Benilov%20and%20Philip%20Coleman%20and%20Frederic%20Roskam&entry.1292438233=%20%20There%20have%20been%20significant%20advances%20in%20deep%20learning%20for%20music%20demixing%20in%0Arecent%20years.%20However%2C%20there%20has%20been%20little%20attention%20given%20to%20how%20these%0Aneural%20networks%20can%20be%20adapted%20for%20real-time%20low-latency%20applications%2C%20which%0Acould%20be%20helpful%20for%20hearing%20aids%2C%20remixing%20audio%20streams%20and%20live%20shows.%20In%0Athis%20paper%2C%20we%20investigate%20the%20various%20challenges%20involved%20in%20adapting%20current%0Ademixing%20models%20in%20the%20literature%20for%20this%20use%20case.%20Subsequently%2C%20inspired%20by%0Athe%20Hybrid%20Demucs%20architecture%2C%20we%20propose%20the%20Hybrid%20Spectrogram%20Time-domain%0AAudio%20Separation%20Network%20HS-TasNet%2C%20which%20utilises%20the%20advantages%20of%20spectral%0Aand%20waveform%20domains.%20For%20a%20latency%20of%2023%20ms%2C%20the%20HS-TasNet%20obtains%20an%20overall%0Asignal-to-distortion%20ratio%20%28SDR%29%20of%204.65%20on%20the%20MusDB%20test%20set%2C%20and%20increases%0Ato%205.55%20with%20additional%20training%20data.%20These%20results%20demonstrate%20the%20potential%0Aof%20efficient%20demixing%20for%20real-time%20low-latency%20music%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17701v1&entry.124074799=Read"},
{"title": "Real-Time Estimation of Relative Pose for UAVs Using a Dual-Channel\n  Feature Association", "author": "Zhaoying Wang and Wei Dong", "abstract": "  Leveraging multiple cameras on Unmanned Aerial Vehicles (UAVs) to form a\nvariable-baseline stereo camera for collaborative perception is highly\npromising. The critical steps include high-rate cross-camera feature\nassociation and frame-rate relative pose estimation of multiple UAVs. To\naccelerate the feature association rate to match the frame rate, we propose a\ndual-channel structure to decouple the time-consuming feature detection and\nmatch from the high-rate image stream. The novel design of periodic guidance\nand fast prediction effectively utilizes each image frame to achieve a\nframe-rate feature association. Real-world experiments are executed using\nSuperPoint and SuperGlue on the NVIDIA NX 8G platform with a 30 Hz image\nstream. Using single-channel SuperPoint and SuperGlue can only achieve 13 Hz\nfeature association. The proposed dual-channel method can improve the rate of\nfeature association from 13 Hz to 30 Hz, supporting the frame-rate requirement.\nTo accommodate the proposed feature association, we develop a Multi-State\nConstrained Kalman Filter (MSCKF)-based relative pose estimator in the back-end\nby fusing the local odometry from two UAVs together with the measurements of\ncommon features. Experiments show that the dual-channel feature association\nimproves the rate of visual observation and enhances the real-time performance\nof back-end estimator compared to the existing methods. Video -\nhttps://youtu.be/UBAR1iP0GPk Supplementary video - https://youtu.be/nPq8EpVzJZM\n", "link": "http://arxiv.org/abs/2402.17504v1", "date": "2024-02-27", "relevancy": 2.1856, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5793}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5395}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Estimation%20of%20Relative%20Pose%20for%20UAVs%20Using%20a%20Dual-Channel%0A%20%20Feature%20Association&entry.906535625=Zhaoying%20Wang%20and%20Wei%20Dong&entry.1292438233=%20%20Leveraging%20multiple%20cameras%20on%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20to%20form%20a%0Avariable-baseline%20stereo%20camera%20for%20collaborative%20perception%20is%20highly%0Apromising.%20The%20critical%20steps%20include%20high-rate%20cross-camera%20feature%0Aassociation%20and%20frame-rate%20relative%20pose%20estimation%20of%20multiple%20UAVs.%20To%0Aaccelerate%20the%20feature%20association%20rate%20to%20match%20the%20frame%20rate%2C%20we%20propose%20a%0Adual-channel%20structure%20to%20decouple%20the%20time-consuming%20feature%20detection%20and%0Amatch%20from%20the%20high-rate%20image%20stream.%20The%20novel%20design%20of%20periodic%20guidance%0Aand%20fast%20prediction%20effectively%20utilizes%20each%20image%20frame%20to%20achieve%20a%0Aframe-rate%20feature%20association.%20Real-world%20experiments%20are%20executed%20using%0ASuperPoint%20and%20SuperGlue%20on%20the%20NVIDIA%20NX%208G%20platform%20with%20a%2030%20Hz%20image%0Astream.%20Using%20single-channel%20SuperPoint%20and%20SuperGlue%20can%20only%20achieve%2013%20Hz%0Afeature%20association.%20The%20proposed%20dual-channel%20method%20can%20improve%20the%20rate%20of%0Afeature%20association%20from%2013%20Hz%20to%2030%20Hz%2C%20supporting%20the%20frame-rate%20requirement.%0ATo%20accommodate%20the%20proposed%20feature%20association%2C%20we%20develop%20a%20Multi-State%0AConstrained%20Kalman%20Filter%20%28MSCKF%29-based%20relative%20pose%20estimator%20in%20the%20back-end%0Aby%20fusing%20the%20local%20odometry%20from%20two%20UAVs%20together%20with%20the%20measurements%20of%0Acommon%20features.%20Experiments%20show%20that%20the%20dual-channel%20feature%20association%0Aimproves%20the%20rate%20of%20visual%20observation%20and%20enhances%20the%20real-time%20performance%0Aof%20back-end%20estimator%20compared%20to%20the%20existing%20methods.%20Video%20-%0Ahttps%3A//youtu.be/UBAR1iP0GPk%20Supplementary%20video%20-%20https%3A//youtu.be/nPq8EpVzJZM%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17504v1&entry.124074799=Read"},
{"title": "SongComposer: A Large Language Model for Lyric and Melody Composition in\n  Song Generation", "author": "Shuangrui Ding and Zihan Liu and Xiaoyi Dong and Pan Zhang and Rui Qian and Conghui He and Dahua Lin and Jiaqi Wang", "abstract": "  We present SongComposer, an innovative LLM designed for song composition. It\ncould understand and generate melodies and lyrics in symbolic song\nrepresentations, by leveraging the capability of LLM. Existing music-related\nLLM treated the music as quantized audio signals, while such implicit encoding\nleads to inefficient encoding and poor flexibility. In contrast, we resort to\nsymbolic song representation, the mature and efficient way humans designed for\nmusic, and enable LLM to explicitly compose songs like humans. In practice, we\ndesign a novel tuple design to format lyric and three note attributes (pitch,\nduration, and rest duration) in the melody, which guarantees the correct LLM\nunderstanding of musical symbols and realizes precise alignment between lyrics\nand melody. To impart basic music understanding to LLM, we carefully collected\nSongCompose-PT, a large-scale song pretraining dataset that includes lyrics,\nmelodies, and paired lyrics-melodies in either Chinese or English. After\nadequate pre-training, 10K carefully crafted QA pairs are used to empower the\nLLM with the instruction-following capability and solve diverse tasks. With\nextensive experiments, SongComposer demonstrates superior performance in\nlyric-to-melody generation, melody-to-lyric generation, song continuation, and\ntext-to-song creation, outperforming advanced LLMs like GPT-4.\n", "link": "http://arxiv.org/abs/2402.17645v1", "date": "2024-02-27", "relevancy": 2.1834, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4466}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4361}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4274}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SongComposer%3A%20A%20Large%20Language%20Model%20for%20Lyric%20and%20Melody%20Composition%20in%0A%20%20Song%20Generation&entry.906535625=Shuangrui%20Ding%20and%20Zihan%20Liu%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Rui%20Qian%20and%20Conghui%20He%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20We%20present%20SongComposer%2C%20an%20innovative%20LLM%20designed%20for%20song%20composition.%20It%0Acould%20understand%20and%20generate%20melodies%20and%20lyrics%20in%20symbolic%20song%0Arepresentations%2C%20by%20leveraging%20the%20capability%20of%20LLM.%20Existing%20music-related%0ALLM%20treated%20the%20music%20as%20quantized%20audio%20signals%2C%20while%20such%20implicit%20encoding%0Aleads%20to%20inefficient%20encoding%20and%20poor%20flexibility.%20In%20contrast%2C%20we%20resort%20to%0Asymbolic%20song%20representation%2C%20the%20mature%20and%20efficient%20way%20humans%20designed%20for%0Amusic%2C%20and%20enable%20LLM%20to%20explicitly%20compose%20songs%20like%20humans.%20In%20practice%2C%20we%0Adesign%20a%20novel%20tuple%20design%20to%20format%20lyric%20and%20three%20note%20attributes%20%28pitch%2C%0Aduration%2C%20and%20rest%20duration%29%20in%20the%20melody%2C%20which%20guarantees%20the%20correct%20LLM%0Aunderstanding%20of%20musical%20symbols%20and%20realizes%20precise%20alignment%20between%20lyrics%0Aand%20melody.%20To%20impart%20basic%20music%20understanding%20to%20LLM%2C%20we%20carefully%20collected%0ASongCompose-PT%2C%20a%20large-scale%20song%20pretraining%20dataset%20that%20includes%20lyrics%2C%0Amelodies%2C%20and%20paired%20lyrics-melodies%20in%20either%20Chinese%20or%20English.%20After%0Aadequate%20pre-training%2C%2010K%20carefully%20crafted%20QA%20pairs%20are%20used%20to%20empower%20the%0ALLM%20with%20the%20instruction-following%20capability%20and%20solve%20diverse%20tasks.%20With%0Aextensive%20experiments%2C%20SongComposer%20demonstrates%20superior%20performance%20in%0Alyric-to-melody%20generation%2C%20melody-to-lyric%20generation%2C%20song%20continuation%2C%20and%0Atext-to-song%20creation%2C%20outperforming%20advanced%20LLMs%20like%20GPT-4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17645v1&entry.124074799=Read"},
{"title": "RACP: Risk-Aware Contingency Planning with Multi-Modal Predictions", "author": "Khaled A. Mustafa and Daniel Jarne Ornia and Jens Kober and Javier Alonso-Mora", "abstract": "  For an autonomous vehicle to operate reliably within real-world traffic\nscenarios, it is imperative to assess the repercussions of its prospective\nactions by anticipating the uncertain intentions exhibited by other\nparticipants in the traffic environment. Driven by the pronounced multi-modal\nnature of human driving behavior, this paper presents an approach that\nleverages Bayesian beliefs over the distribution of potential policies of other\nroad users to construct a novel risk-aware probabilistic motion planning\nframework. In particular, we propose a novel contingency planner that outputs\nlong-term contingent plans conditioned on multiple possible intents for other\nactors in the traffic scene. The Bayesian belief is incorporated into the\noptimization cost function to influence the behavior of the short-term plan\nbased on the likelihood of other agents' policies. Furthermore, a probabilistic\nrisk metric is employed to fine-tune the balance between efficiency and\nrobustness. Through a series of closed-loop safety-critical simulated traffic\nscenarios shared with human-driven vehicles, we demonstrate the practical\nefficacy of our proposed approach that can handle multi-vehicle scenarios.\n", "link": "http://arxiv.org/abs/2402.17387v1", "date": "2024-02-27", "relevancy": 2.1784, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6204}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5446}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5143}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RACP%3A%20Risk-Aware%20Contingency%20Planning%20with%20Multi-Modal%20Predictions&entry.906535625=Khaled%20A.%20Mustafa%20and%20Daniel%20Jarne%20Ornia%20and%20Jens%20Kober%20and%20Javier%20Alonso-Mora&entry.1292438233=%20%20For%20an%20autonomous%20vehicle%20to%20operate%20reliably%20within%20real-world%20traffic%0Ascenarios%2C%20it%20is%20imperative%20to%20assess%20the%20repercussions%20of%20its%20prospective%0Aactions%20by%20anticipating%20the%20uncertain%20intentions%20exhibited%20by%20other%0Aparticipants%20in%20the%20traffic%20environment.%20Driven%20by%20the%20pronounced%20multi-modal%0Anature%20of%20human%20driving%20behavior%2C%20this%20paper%20presents%20an%20approach%20that%0Aleverages%20Bayesian%20beliefs%20over%20the%20distribution%20of%20potential%20policies%20of%20other%0Aroad%20users%20to%20construct%20a%20novel%20risk-aware%20probabilistic%20motion%20planning%0Aframework.%20In%20particular%2C%20we%20propose%20a%20novel%20contingency%20planner%20that%20outputs%0Along-term%20contingent%20plans%20conditioned%20on%20multiple%20possible%20intents%20for%20other%0Aactors%20in%20the%20traffic%20scene.%20The%20Bayesian%20belief%20is%20incorporated%20into%20the%0Aoptimization%20cost%20function%20to%20influence%20the%20behavior%20of%20the%20short-term%20plan%0Abased%20on%20the%20likelihood%20of%20other%20agents%27%20policies.%20Furthermore%2C%20a%20probabilistic%0Arisk%20metric%20is%20employed%20to%20fine-tune%20the%20balance%20between%20efficiency%20and%0Arobustness.%20Through%20a%20series%20of%20closed-loop%20safety-critical%20simulated%20traffic%0Ascenarios%20shared%20with%20human-driven%20vehicles%2C%20we%20demonstrate%20the%20practical%0Aefficacy%20of%20our%20proposed%20approach%20that%20can%20handle%20multi-vehicle%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17387v1&entry.124074799=Read"},
{"title": "DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized\n  Diffusion Model", "author": "Shyam Marjit and Harshit Singh and Nityanand Mathur and Sayak Paul and Chia-Mu Yu and Pin-Yu Chen", "abstract": "  In the realm of subject-driven text-to-image (T2I) generative models, recent\ndevelopments like DreamBooth and BLIP-Diffusion have led to impressive results\nyet encounter limitations due to their intensive fine-tuning demands and\nsubstantial parameter requirements. While the low-rank adaptation (LoRA) module\nwithin DreamBooth offers a reduction in trainable parameters, it introduces a\npronounced sensitivity to hyperparameters, leading to a compromise between\nparameter efficiency and the quality of T2I personalized image synthesis.\nAddressing these constraints, we introduce \\textbf{\\textit{DiffuseKronA}}, a\nnovel Kronecker product-based adaptation module that not only significantly\nreduces the parameter count by 35\\% and 99.947\\% compared to LoRA-DreamBooth\nand the original DreamBooth, respectively, but also enhances the quality of\nimage synthesis. Crucially, \\textit{DiffuseKronA} mitigates the issue of\nhyperparameter sensitivity, delivering consistent high-quality generations\nacross a wide range of hyperparameters, thereby diminishing the necessity for\nextensive fine-tuning. Furthermore, a more controllable decomposition makes\n\\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\\%\nreduction with results comparable to LoRA-Dreambooth. Evaluated against diverse\nand complex input images and text prompts, \\textit{DiffuseKronA} consistently\noutperforms existing models, producing diverse images of higher quality with\nimproved fidelity and a more accurate color distribution of objects, all the\nwhile upholding exceptional parameter efficiency, thus presenting a substantial\nadvancement in the field of T2I generative modeling. Our project page,\nconsisting of links to the code, and pre-trained checkpoints, is available at\n\\href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}.\n", "link": "http://arxiv.org/abs/2402.17412v1", "date": "2024-02-27", "relevancy": 2.1773, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.619}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5635}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4953}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffuseKronA%3A%20A%20Parameter%20Efficient%20Fine-tuning%20Method%20for%20Personalized%0A%20%20Diffusion%20Model&entry.906535625=Shyam%20Marjit%20and%20Harshit%20Singh%20and%20Nityanand%20Mathur%20and%20Sayak%20Paul%20and%20Chia-Mu%20Yu%20and%20Pin-Yu%20Chen&entry.1292438233=%20%20In%20the%20realm%20of%20subject-driven%20text-to-image%20%28T2I%29%20generative%20models%2C%20recent%0Adevelopments%20like%20DreamBooth%20and%20BLIP-Diffusion%20have%20led%20to%20impressive%20results%0Ayet%20encounter%20limitations%20due%20to%20their%20intensive%20fine-tuning%20demands%20and%0Asubstantial%20parameter%20requirements.%20While%20the%20low-rank%20adaptation%20%28LoRA%29%20module%0Awithin%20DreamBooth%20offers%20a%20reduction%20in%20trainable%20parameters%2C%20it%20introduces%20a%0Apronounced%20sensitivity%20to%20hyperparameters%2C%20leading%20to%20a%20compromise%20between%0Aparameter%20efficiency%20and%20the%20quality%20of%20T2I%20personalized%20image%20synthesis.%0AAddressing%20these%20constraints%2C%20we%20introduce%20%5Ctextbf%7B%5Ctextit%7BDiffuseKronA%7D%7D%2C%20a%0Anovel%20Kronecker%20product-based%20adaptation%20module%20that%20not%20only%20significantly%0Areduces%20the%20parameter%20count%20by%2035%5C%25%20and%2099.947%5C%25%20compared%20to%20LoRA-DreamBooth%0Aand%20the%20original%20DreamBooth%2C%20respectively%2C%20but%20also%20enhances%20the%20quality%20of%0Aimage%20synthesis.%20Crucially%2C%20%5Ctextit%7BDiffuseKronA%7D%20mitigates%20the%20issue%20of%0Ahyperparameter%20sensitivity%2C%20delivering%20consistent%20high-quality%20generations%0Aacross%20a%20wide%20range%20of%20hyperparameters%2C%20thereby%20diminishing%20the%20necessity%20for%0Aextensive%20fine-tuning.%20Furthermore%2C%20a%20more%20controllable%20decomposition%20makes%0A%5Ctextit%7BDiffuseKronA%7D%20more%20interpretable%20and%20even%20can%20achieve%20up%20to%20a%2050%5C%25%0Areduction%20with%20results%20comparable%20to%20LoRA-Dreambooth.%20Evaluated%20against%20diverse%0Aand%20complex%20input%20images%20and%20text%20prompts%2C%20%5Ctextit%7BDiffuseKronA%7D%20consistently%0Aoutperforms%20existing%20models%2C%20producing%20diverse%20images%20of%20higher%20quality%20with%0Aimproved%20fidelity%20and%20a%20more%20accurate%20color%20distribution%20of%20objects%2C%20all%20the%0Awhile%20upholding%20exceptional%20parameter%20efficiency%2C%20thus%20presenting%20a%20substantial%0Aadvancement%20in%20the%20field%20of%20T2I%20generative%20modeling.%20Our%20project%20page%2C%0Aconsisting%20of%20links%20to%20the%20code%2C%20and%20pre-trained%20checkpoints%2C%20is%20available%20at%0A%5Chref%7Bhttps%3A//diffusekrona.github.io/%7D%7Bhttps%3A//diffusekrona.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17412v1&entry.124074799=Read"},
{"title": "Integrating Multimodal Data for Joint Generative Modeling of Complex\n  Dynamics", "author": "Manuel Brenner and Florian Hess and Georgia Koppe and Daniel Durstewitz", "abstract": "  Many, if not most, systems of interest in science are naturally described as\nnonlinear dynamical systems. Empirically, we commonly access these systems\nthrough time series measurements. Often such time series may consist of\ndiscrete random variables rather than continuous measurements, or may be\ncomposed of measurements from multiple data modalities observed simultaneously.\nFor instance, in neuroscience we may have behavioral labels in addition to\nspike counts and continuous physiological recordings. While by now there is a\nburgeoning literature on deep learning for dynamical systems reconstruction\n(DSR), multimodal data integration has hardly been considered in this context.\nHere we provide such an efficient and flexible algorithmic framework that rests\non a multimodal variational autoencoder for generating a sparse teacher signal\nthat guides training of a reconstruction model, exploiting recent advances in\nDSR training techniques. It enables to combine various sources of information\nfor optimal reconstruction, even allows for reconstruction from symbolic data\n(class labels) alone, and connects different types of observations within a\ncommon latent dynamics space. In contrast to previous multimodal data\nintegration techniques for scientific applications, our framework is fully\n\\textit{generative}, producing, after training, trajectories with the same\ngeometrical and temporal structure as those of the ground truth system.\n", "link": "http://arxiv.org/abs/2212.07892v2", "date": "2024-02-27", "relevancy": 2.171, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5658}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5533}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5229}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Multimodal%20Data%20for%20Joint%20Generative%20Modeling%20of%20Complex%0A%20%20Dynamics&entry.906535625=Manuel%20Brenner%20and%20Florian%20Hess%20and%20Georgia%20Koppe%20and%20Daniel%20Durstewitz&entry.1292438233=%20%20Many%2C%20if%20not%20most%2C%20systems%20of%20interest%20in%20science%20are%20naturally%20described%20as%0Anonlinear%20dynamical%20systems.%20Empirically%2C%20we%20commonly%20access%20these%20systems%0Athrough%20time%20series%20measurements.%20Often%20such%20time%20series%20may%20consist%20of%0Adiscrete%20random%20variables%20rather%20than%20continuous%20measurements%2C%20or%20may%20be%0Acomposed%20of%20measurements%20from%20multiple%20data%20modalities%20observed%20simultaneously.%0AFor%20instance%2C%20in%20neuroscience%20we%20may%20have%20behavioral%20labels%20in%20addition%20to%0Aspike%20counts%20and%20continuous%20physiological%20recordings.%20While%20by%20now%20there%20is%20a%0Aburgeoning%20literature%20on%20deep%20learning%20for%20dynamical%20systems%20reconstruction%0A%28DSR%29%2C%20multimodal%20data%20integration%20has%20hardly%20been%20considered%20in%20this%20context.%0AHere%20we%20provide%20such%20an%20efficient%20and%20flexible%20algorithmic%20framework%20that%20rests%0Aon%20a%20multimodal%20variational%20autoencoder%20for%20generating%20a%20sparse%20teacher%20signal%0Athat%20guides%20training%20of%20a%20reconstruction%20model%2C%20exploiting%20recent%20advances%20in%0ADSR%20training%20techniques.%20It%20enables%20to%20combine%20various%20sources%20of%20information%0Afor%20optimal%20reconstruction%2C%20even%20allows%20for%20reconstruction%20from%20symbolic%20data%0A%28class%20labels%29%20alone%2C%20and%20connects%20different%20types%20of%20observations%20within%20a%0Acommon%20latent%20dynamics%20space.%20In%20contrast%20to%20previous%20multimodal%20data%0Aintegration%20techniques%20for%20scientific%20applications%2C%20our%20framework%20is%20fully%0A%5Ctextit%7Bgenerative%7D%2C%20producing%2C%20after%20training%2C%20trajectories%20with%20the%20same%0Ageometrical%20and%20temporal%20structure%20as%20those%20of%20the%20ground%20truth%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.07892v2&entry.124074799=Read"},
{"title": "Solving PDEs on Unknown Manifolds with Machine Learning", "author": "Senwei Liang and Shixiao W. Jiang and John Harlim and Haizhao Yang", "abstract": "  This paper proposes a mesh-free computational framework and machine learning\ntheory for solving elliptic PDEs on unknown manifolds, identified with point\nclouds, based on diffusion maps (DM) and deep learning. The PDE solver is\nformulated as a supervised learning task to solve a least-squares regression\nproblem that imposes an algebraic equation approximating a PDE (and boundary\nconditions if applicable). This algebraic equation involves a graph-Laplacian\ntype matrix obtained via DM asymptotic expansion, which is a consistent\nestimator of second-order elliptic differential operators. The resulting\nnumerical method is to solve a highly non-convex empirical risk minimization\nproblem subjected to a solution from a hypothesis space of neural networks\n(NNs). In a well-posed elliptic PDE setting, when the hypothesis space consists\nof neural networks with either infinite width or depth, we show that the global\nminimizer of the empirical loss function is a consistent solution in the limit\nof large training data. When the hypothesis space is a two-layer neural\nnetwork, we show that for a sufficiently large width, gradient descent can\nidentify a global minimizer of the empirical loss function. Supporting\nnumerical examples demonstrate the convergence of the solutions, ranging from\nsimple manifolds with low and high co-dimensions, to rough surfaces with and\nwithout boundaries. We also show that the proposed NN solver can robustly\ngeneralize the PDE solution on new data points with generalization errors that\nare almost identical to the training errors, superseding a Nystrom-based\ninterpolation method.\n", "link": "http://arxiv.org/abs/2106.06682v4", "date": "2024-02-27", "relevancy": 2.1657, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5614}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5321}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5149}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20PDEs%20on%20Unknown%20Manifolds%20with%20Machine%20Learning&entry.906535625=Senwei%20Liang%20and%20Shixiao%20W.%20Jiang%20and%20John%20Harlim%20and%20Haizhao%20Yang&entry.1292438233=%20%20This%20paper%20proposes%20a%20mesh-free%20computational%20framework%20and%20machine%20learning%0Atheory%20for%20solving%20elliptic%20PDEs%20on%20unknown%20manifolds%2C%20identified%20with%20point%0Aclouds%2C%20based%20on%20diffusion%20maps%20%28DM%29%20and%20deep%20learning.%20The%20PDE%20solver%20is%0Aformulated%20as%20a%20supervised%20learning%20task%20to%20solve%20a%20least-squares%20regression%0Aproblem%20that%20imposes%20an%20algebraic%20equation%20approximating%20a%20PDE%20%28and%20boundary%0Aconditions%20if%20applicable%29.%20This%20algebraic%20equation%20involves%20a%20graph-Laplacian%0Atype%20matrix%20obtained%20via%20DM%20asymptotic%20expansion%2C%20which%20is%20a%20consistent%0Aestimator%20of%20second-order%20elliptic%20differential%20operators.%20The%20resulting%0Anumerical%20method%20is%20to%20solve%20a%20highly%20non-convex%20empirical%20risk%20minimization%0Aproblem%20subjected%20to%20a%20solution%20from%20a%20hypothesis%20space%20of%20neural%20networks%0A%28NNs%29.%20In%20a%20well-posed%20elliptic%20PDE%20setting%2C%20when%20the%20hypothesis%20space%20consists%0Aof%20neural%20networks%20with%20either%20infinite%20width%20or%20depth%2C%20we%20show%20that%20the%20global%0Aminimizer%20of%20the%20empirical%20loss%20function%20is%20a%20consistent%20solution%20in%20the%20limit%0Aof%20large%20training%20data.%20When%20the%20hypothesis%20space%20is%20a%20two-layer%20neural%0Anetwork%2C%20we%20show%20that%20for%20a%20sufficiently%20large%20width%2C%20gradient%20descent%20can%0Aidentify%20a%20global%20minimizer%20of%20the%20empirical%20loss%20function.%20Supporting%0Anumerical%20examples%20demonstrate%20the%20convergence%20of%20the%20solutions%2C%20ranging%20from%0Asimple%20manifolds%20with%20low%20and%20high%20co-dimensions%2C%20to%20rough%20surfaces%20with%20and%0Awithout%20boundaries.%20We%20also%20show%20that%20the%20proposed%20NN%20solver%20can%20robustly%0Ageneralize%20the%20PDE%20solution%20on%20new%20data%20points%20with%20generalization%20errors%20that%0Aare%20almost%20identical%20to%20the%20training%20errors%2C%20superseding%20a%20Nystrom-based%0Ainterpolation%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2106.06682v4&entry.124074799=Read"},
{"title": "Advancing sleep detection by modelling weak label sets: A novel weakly\n  supervised learning approach", "author": "Matthias Boeker and Vajira Thambawita and Michael Riegler and P\u00e5l Halvorsen and Hugo L. Hammer", "abstract": "  Understanding sleep and activity patterns plays a crucial role in physical\nand mental health. This study introduces a novel approach for sleep detection\nusing weakly supervised learning for scenarios where reliable ground truth\nlabels are unavailable. The proposed method relies on a set of weak labels,\nderived from the predictions generated by conventional sleep detection\nalgorithms. Introducing a novel approach, we suggest a novel generalised\nnon-linear statistical model in which the number of weak sleep labels is\nmodelled as outcome of a binomial distribution. The probability of sleep in the\nbinomial distribution is linked to the outcomes of neural networks trained to\ndetect sleep based on actigraphy. We show that maximizing the likelihood\nfunction of the model, is equivalent to minimizing the soft cross-entropy loss.\nAdditionally, we explored the use of the Brier score as a loss function for\nweak labels. The efficacy of the suggested modelling framework was demonstrated\nusing the Multi-Ethnic Study of Atherosclerosis dataset. A \\gls{lstm} trained\non the soft cross-entropy outperformed conventional sleep detection algorithms,\nother neural network architectures and loss functions in accuracy and model\ncalibration. This research not only advances sleep detection techniques in\nscenarios where ground truth data is scarce but also contributes to the broader\nfield of weakly supervised learning by introducing innovative approach in\nmodelling sets of weak labels.\n", "link": "http://arxiv.org/abs/2402.17601v1", "date": "2024-02-27", "relevancy": 2.1638, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4948}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.489}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20sleep%20detection%20by%20modelling%20weak%20label%20sets%3A%20A%20novel%20weakly%0A%20%20supervised%20learning%20approach&entry.906535625=Matthias%20Boeker%20and%20Vajira%20Thambawita%20and%20Michael%20Riegler%20and%20P%C3%A5l%20Halvorsen%20and%20Hugo%20L.%20Hammer&entry.1292438233=%20%20Understanding%20sleep%20and%20activity%20patterns%20plays%20a%20crucial%20role%20in%20physical%0Aand%20mental%20health.%20This%20study%20introduces%20a%20novel%20approach%20for%20sleep%20detection%0Ausing%20weakly%20supervised%20learning%20for%20scenarios%20where%20reliable%20ground%20truth%0Alabels%20are%20unavailable.%20The%20proposed%20method%20relies%20on%20a%20set%20of%20weak%20labels%2C%0Aderived%20from%20the%20predictions%20generated%20by%20conventional%20sleep%20detection%0Aalgorithms.%20Introducing%20a%20novel%20approach%2C%20we%20suggest%20a%20novel%20generalised%0Anon-linear%20statistical%20model%20in%20which%20the%20number%20of%20weak%20sleep%20labels%20is%0Amodelled%20as%20outcome%20of%20a%20binomial%20distribution.%20The%20probability%20of%20sleep%20in%20the%0Abinomial%20distribution%20is%20linked%20to%20the%20outcomes%20of%20neural%20networks%20trained%20to%0Adetect%20sleep%20based%20on%20actigraphy.%20We%20show%20that%20maximizing%20the%20likelihood%0Afunction%20of%20the%20model%2C%20is%20equivalent%20to%20minimizing%20the%20soft%20cross-entropy%20loss.%0AAdditionally%2C%20we%20explored%20the%20use%20of%20the%20Brier%20score%20as%20a%20loss%20function%20for%0Aweak%20labels.%20The%20efficacy%20of%20the%20suggested%20modelling%20framework%20was%20demonstrated%0Ausing%20the%20Multi-Ethnic%20Study%20of%20Atherosclerosis%20dataset.%20A%20%5Cgls%7Blstm%7D%20trained%0Aon%20the%20soft%20cross-entropy%20outperformed%20conventional%20sleep%20detection%20algorithms%2C%0Aother%20neural%20network%20architectures%20and%20loss%20functions%20in%20accuracy%20and%20model%0Acalibration.%20This%20research%20not%20only%20advances%20sleep%20detection%20techniques%20in%0Ascenarios%20where%20ground%20truth%20data%20is%20scarce%20but%20also%20contributes%20to%20the%20broader%0Afield%20of%20weakly%20supervised%20learning%20by%20introducing%20innovative%20approach%20in%0Amodelling%20sets%20of%20weak%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17601v1&entry.124074799=Read"},
{"title": "Snapture -- A Novel Neural Architecture for Combined Static and Dynamic\n  Hand Gesture Recognition", "author": "Hassan Ali and Doreen Jirak and Stefan Wermter", "abstract": "  As robots are expected to get more involved in people's everyday lives,\nframeworks that enable intuitive user interfaces are in demand. Hand gesture\nrecognition systems provide a natural way of communication and, thus, are an\nintegral part of seamless Human-Robot Interaction (HRI). Recent years have\nwitnessed an immense evolution of computational models powered by deep\nlearning. However, state-of-the-art models fall short in expanding across\ndifferent gesture domains, such as emblems and co-speech. In this paper, we\npropose a novel hybrid hand gesture recognition system. Our architecture\nenables learning both static and dynamic gestures: by capturing a so-called\n\"snapshot\" of the gesture performance at its peak, we integrate the hand pose\nalong with the dynamic movement. Moreover, we present a method for analyzing\nthe motion profile of a gesture to uncover its dynamic characteristics and\nwhich allows regulating a static channel based on the amount of motion. Our\nevaluation demonstrates the superiority of our approach on two gesture\nbenchmarks compared to a CNNLSTM baseline. We also provide an analysis on a\ngesture class basis that unveils the potential of our Snapture architecture for\nperformance improvements. Thanks to its modular implementation, our framework\nallows the integration of other multimodal data like facial expressions and\nhead tracking, which are important cues in HRI scenarios, into one\narchitecture. Thus, our work contributes both to gesture recognition research\nand machine learning applications for non-verbal communication with robots.\n", "link": "http://arxiv.org/abs/2205.15862v2", "date": "2024-02-27", "relevancy": 2.1549, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5535}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5299}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5238}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snapture%20--%20A%20Novel%20Neural%20Architecture%20for%20Combined%20Static%20and%20Dynamic%0A%20%20Hand%20Gesture%20Recognition&entry.906535625=Hassan%20Ali%20and%20Doreen%20Jirak%20and%20Stefan%20Wermter&entry.1292438233=%20%20As%20robots%20are%20expected%20to%20get%20more%20involved%20in%20people%27s%20everyday%20lives%2C%0Aframeworks%20that%20enable%20intuitive%20user%20interfaces%20are%20in%20demand.%20Hand%20gesture%0Arecognition%20systems%20provide%20a%20natural%20way%20of%20communication%20and%2C%20thus%2C%20are%20an%0Aintegral%20part%20of%20seamless%20Human-Robot%20Interaction%20%28HRI%29.%20Recent%20years%20have%0Awitnessed%20an%20immense%20evolution%20of%20computational%20models%20powered%20by%20deep%0Alearning.%20However%2C%20state-of-the-art%20models%20fall%20short%20in%20expanding%20across%0Adifferent%20gesture%20domains%2C%20such%20as%20emblems%20and%20co-speech.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20hybrid%20hand%20gesture%20recognition%20system.%20Our%20architecture%0Aenables%20learning%20both%20static%20and%20dynamic%20gestures%3A%20by%20capturing%20a%20so-called%0A%22snapshot%22%20of%20the%20gesture%20performance%20at%20its%20peak%2C%20we%20integrate%20the%20hand%20pose%0Aalong%20with%20the%20dynamic%20movement.%20Moreover%2C%20we%20present%20a%20method%20for%20analyzing%0Athe%20motion%20profile%20of%20a%20gesture%20to%20uncover%20its%20dynamic%20characteristics%20and%0Awhich%20allows%20regulating%20a%20static%20channel%20based%20on%20the%20amount%20of%20motion.%20Our%0Aevaluation%20demonstrates%20the%20superiority%20of%20our%20approach%20on%20two%20gesture%0Abenchmarks%20compared%20to%20a%20CNNLSTM%20baseline.%20We%20also%20provide%20an%20analysis%20on%20a%0Agesture%20class%20basis%20that%20unveils%20the%20potential%20of%20our%20Snapture%20architecture%20for%0Aperformance%20improvements.%20Thanks%20to%20its%20modular%20implementation%2C%20our%20framework%0Aallows%20the%20integration%20of%20other%20multimodal%20data%20like%20facial%20expressions%20and%0Ahead%20tracking%2C%20which%20are%20important%20cues%20in%20HRI%20scenarios%2C%20into%20one%0Aarchitecture.%20Thus%2C%20our%20work%20contributes%20both%20to%20gesture%20recognition%20research%0Aand%20machine%20learning%20applications%20for%20non-verbal%20communication%20with%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.15862v2&entry.124074799=Read"},
{"title": "MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning\n  for Multimodal Video Captioning", "author": "Huiyu Xiong and Lanxiao Wang and Heqian Qiu and Taijin Zhao and Benliu Qiu and Hongliang Li", "abstract": "  To address the problem of catastrophic forgetting due to the invisibility of\nold categories in sequential input, existing work based on relatively simple\ncategorization tasks has made some progress. In contrast, video captioning is a\nmore complex task in multimodal scenario, which has not been explored in the\nfield of incremental learning. After identifying this stability-plasticity\nproblem when analyzing video with sequential input, we originally propose a\nmethod to Mitigate Catastrophic Forgetting in class-incremental learning for\nmultimodal Video Captioning (MCF-VC). As for effectively maintaining good\nperformance on old tasks at the macro level, we design Fine-grained Sensitivity\nSelection (FgSS) based on the Mask of Linear's Parameters and Fisher\nSensitivity to pick useful knowledge from old tasks. Further, in order to\nbetter constrain the knowledge characteristics of old and new tasks at the\nspecific feature level, we have created the Two-stage Knowledge Distillation\n(TsKD), which is able to learn the new task well while weighing the old task.\nSpecifically, we design two distillation losses, which constrain the cross\nmodal semantic information of semantic attention feature map and the textual\ninformation of the final outputs respectively, so that the inter-model and\nintra-model stylized knowledge of the old class is retained while learning the\nnew class. In order to illustrate the ability of our model to resist\nforgetting, we designed a metric CIDER_t to detect the stage forgetting rate.\nOur experiments on the public dataset MSR-VTT show that the proposed method\nsignificantly resists the forgetting of previous tasks without replaying old\nsamples, and performs well on the new task.\n", "link": "http://arxiv.org/abs/2402.17680v1", "date": "2024-02-27", "relevancy": 2.1514, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5622}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5376}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5136}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCF-VC%3A%20Mitigate%20Catastrophic%20Forgetting%20in%20Class-Incremental%20Learning%0A%20%20for%20Multimodal%20Video%20Captioning&entry.906535625=Huiyu%20Xiong%20and%20Lanxiao%20Wang%20and%20Heqian%20Qiu%20and%20Taijin%20Zhao%20and%20Benliu%20Qiu%20and%20Hongliang%20Li&entry.1292438233=%20%20To%20address%20the%20problem%20of%20catastrophic%20forgetting%20due%20to%20the%20invisibility%20of%0Aold%20categories%20in%20sequential%20input%2C%20existing%20work%20based%20on%20relatively%20simple%0Acategorization%20tasks%20has%20made%20some%20progress.%20In%20contrast%2C%20video%20captioning%20is%20a%0Amore%20complex%20task%20in%20multimodal%20scenario%2C%20which%20has%20not%20been%20explored%20in%20the%0Afield%20of%20incremental%20learning.%20After%20identifying%20this%20stability-plasticity%0Aproblem%20when%20analyzing%20video%20with%20sequential%20input%2C%20we%20originally%20propose%20a%0Amethod%20to%20Mitigate%20Catastrophic%20Forgetting%20in%20class-incremental%20learning%20for%0Amultimodal%20Video%20Captioning%20%28MCF-VC%29.%20As%20for%20effectively%20maintaining%20good%0Aperformance%20on%20old%20tasks%20at%20the%20macro%20level%2C%20we%20design%20Fine-grained%20Sensitivity%0ASelection%20%28FgSS%29%20based%20on%20the%20Mask%20of%20Linear%27s%20Parameters%20and%20Fisher%0ASensitivity%20to%20pick%20useful%20knowledge%20from%20old%20tasks.%20Further%2C%20in%20order%20to%0Abetter%20constrain%20the%20knowledge%20characteristics%20of%20old%20and%20new%20tasks%20at%20the%0Aspecific%20feature%20level%2C%20we%20have%20created%20the%20Two-stage%20Knowledge%20Distillation%0A%28TsKD%29%2C%20which%20is%20able%20to%20learn%20the%20new%20task%20well%20while%20weighing%20the%20old%20task.%0ASpecifically%2C%20we%20design%20two%20distillation%20losses%2C%20which%20constrain%20the%20cross%0Amodal%20semantic%20information%20of%20semantic%20attention%20feature%20map%20and%20the%20textual%0Ainformation%20of%20the%20final%20outputs%20respectively%2C%20so%20that%20the%20inter-model%20and%0Aintra-model%20stylized%20knowledge%20of%20the%20old%20class%20is%20retained%20while%20learning%20the%0Anew%20class.%20In%20order%20to%20illustrate%20the%20ability%20of%20our%20model%20to%20resist%0Aforgetting%2C%20we%20designed%20a%20metric%20CIDER_t%20to%20detect%20the%20stage%20forgetting%20rate.%0AOur%20experiments%20on%20the%20public%20dataset%20MSR-VTT%20show%20that%20the%20proposed%20method%0Asignificantly%20resists%20the%20forgetting%20of%20previous%20tasks%20without%20replaying%20old%0Asamples%2C%20and%20performs%20well%20on%20the%20new%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17680v1&entry.124074799=Read"},
{"title": "Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control", "author": "Thong Nguyen and Mariya Hendriksen and Andrew Yates and Maarten de Rijke", "abstract": "  Learned sparse retrieval (LSR) is a family of neural methods that encode\nqueries and documents into sparse lexical vectors that can be indexed and\nretrieved efficiently with an inverted index. We explore the application of LSR\nto the multi-modal domain, with a focus on text-image retrieval. While LSR has\nseen success in text retrieval, its application in multimodal retrieval remains\nunderexplored. Current approaches like LexLIP and STAIR require complex\nmulti-step training on massive datasets. Our proposed approach efficiently\ntransforms dense vectors from a frozen dense model into sparse lexical vectors.\nWe address issues of high dimension co-activation and semantic deviation\nthrough a new training algorithm, using Bernoulli random variables to control\nquery expansion. Experiments with two dense models (BLIP, ALBEF) and two\ndatasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively\nreduces co-activation and semantic deviation. Our best-performing sparsified\nmodel outperforms state-of-the-art text-image LSR models with a shorter\ntraining time and lower GPU memory requirements. Our approach offers an\neffective solution for training LSR retrieval models in multimodal settings.\nOur code and model checkpoints are available at\ngithub.com/thongnt99/lsr-multimodal\n", "link": "http://arxiv.org/abs/2402.17535v1", "date": "2024-02-27", "relevancy": 2.1501, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5814}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5392}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5183}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Learned%20Sparse%20Retrieval%20with%20Probabilistic%20Expansion%20Control&entry.906535625=Thong%20Nguyen%20and%20Mariya%20Hendriksen%20and%20Andrew%20Yates%20and%20Maarten%20de%20Rijke&entry.1292438233=%20%20Learned%20sparse%20retrieval%20%28LSR%29%20is%20a%20family%20of%20neural%20methods%20that%20encode%0Aqueries%20and%20documents%20into%20sparse%20lexical%20vectors%20that%20can%20be%20indexed%20and%0Aretrieved%20efficiently%20with%20an%20inverted%20index.%20We%20explore%20the%20application%20of%20LSR%0Ato%20the%20multi-modal%20domain%2C%20with%20a%20focus%20on%20text-image%20retrieval.%20While%20LSR%20has%0Aseen%20success%20in%20text%20retrieval%2C%20its%20application%20in%20multimodal%20retrieval%20remains%0Aunderexplored.%20Current%20approaches%20like%20LexLIP%20and%20STAIR%20require%20complex%0Amulti-step%20training%20on%20massive%20datasets.%20Our%20proposed%20approach%20efficiently%0Atransforms%20dense%20vectors%20from%20a%20frozen%20dense%20model%20into%20sparse%20lexical%20vectors.%0AWe%20address%20issues%20of%20high%20dimension%20co-activation%20and%20semantic%20deviation%0Athrough%20a%20new%20training%20algorithm%2C%20using%20Bernoulli%20random%20variables%20to%20control%0Aquery%20expansion.%20Experiments%20with%20two%20dense%20models%20%28BLIP%2C%20ALBEF%29%20and%20two%0Adatasets%20%28MSCOCO%2C%20Flickr30k%29%20show%20that%20our%20proposed%20algorithm%20effectively%0Areduces%20co-activation%20and%20semantic%20deviation.%20Our%20best-performing%20sparsified%0Amodel%20outperforms%20state-of-the-art%20text-image%20LSR%20models%20with%20a%20shorter%0Atraining%20time%20and%20lower%20GPU%20memory%20requirements.%20Our%20approach%20offers%20an%0Aeffective%20solution%20for%20training%20LSR%20retrieval%20models%20in%20multimodal%20settings.%0AOur%20code%20and%20model%20checkpoints%20are%20available%20at%0Agithub.com/thongnt99/lsr-multimodal%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17535v1&entry.124074799=Read"},
{"title": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large\n  Language Models in Knowledge Conflicts", "author": "Jian Xie and Kai Zhang and Jiangjie Chen and Renze Lou and Yu Su", "abstract": "  By providing external information to large language models (LLMs), tool\naugmentation (including retrieval augmentation) has emerged as a promising\nsolution for addressing the limitations of LLMs' static parametric memory.\nHowever, how receptive are LLMs to such external evidence, especially when the\nevidence conflicts with their parametric memory? We present the first\ncomprehensive and controlled investigation into the behavior of LLMs when\nencountering knowledge conflicts. We propose a systematic framework to elicit\nhigh-quality parametric memory from LLMs and construct the corresponding\ncounter-memory, which enables us to conduct a series of controlled experiments.\nOur investigation reveals seemingly contradicting behaviors of LLMs. On the one\nhand, different from prior wisdom, we find that LLMs can be highly receptive to\nexternal evidence even when that conflicts with their parametric memory, given\nthat the external evidence is coherent and convincing. On the other hand, LLMs\nalso demonstrate a strong confirmation bias when the external evidence contains\nsome information that is consistent with their parametric memory, despite being\npresented with conflicting evidence at the same time. These results pose\nimportant implications that are worth careful consideration for the further\ndevelopment and deployment of tool- and retrieval-augmented LLMs. Resources are\navailable at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.\n", "link": "http://arxiv.org/abs/2305.13300v4", "date": "2024-02-27", "relevancy": 2.142, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5453}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.517}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Chameleon%20or%20Stubborn%20Sloth%3A%20Revealing%20the%20Behavior%20of%20Large%0A%20%20Language%20Models%20in%20Knowledge%20Conflicts&entry.906535625=Jian%20Xie%20and%20Kai%20Zhang%20and%20Jiangjie%20Chen%20and%20Renze%20Lou%20and%20Yu%20Su&entry.1292438233=%20%20By%20providing%20external%20information%20to%20large%20language%20models%20%28LLMs%29%2C%20tool%0Aaugmentation%20%28including%20retrieval%20augmentation%29%20has%20emerged%20as%20a%20promising%0Asolution%20for%20addressing%20the%20limitations%20of%20LLMs%27%20static%20parametric%20memory.%0AHowever%2C%20how%20receptive%20are%20LLMs%20to%20such%20external%20evidence%2C%20especially%20when%20the%0Aevidence%20conflicts%20with%20their%20parametric%20memory%3F%20We%20present%20the%20first%0Acomprehensive%20and%20controlled%20investigation%20into%20the%20behavior%20of%20LLMs%20when%0Aencountering%20knowledge%20conflicts.%20We%20propose%20a%20systematic%20framework%20to%20elicit%0Ahigh-quality%20parametric%20memory%20from%20LLMs%20and%20construct%20the%20corresponding%0Acounter-memory%2C%20which%20enables%20us%20to%20conduct%20a%20series%20of%20controlled%20experiments.%0AOur%20investigation%20reveals%20seemingly%20contradicting%20behaviors%20of%20LLMs.%20On%20the%20one%0Ahand%2C%20different%20from%20prior%20wisdom%2C%20we%20find%20that%20LLMs%20can%20be%20highly%20receptive%20to%0Aexternal%20evidence%20even%20when%20that%20conflicts%20with%20their%20parametric%20memory%2C%20given%0Athat%20the%20external%20evidence%20is%20coherent%20and%20convincing.%20On%20the%20other%20hand%2C%20LLMs%0Aalso%20demonstrate%20a%20strong%20confirmation%20bias%20when%20the%20external%20evidence%20contains%0Asome%20information%20that%20is%20consistent%20with%20their%20parametric%20memory%2C%20despite%20being%0Apresented%20with%20conflicting%20evidence%20at%20the%20same%20time.%20These%20results%20pose%0Aimportant%20implications%20that%20are%20worth%20careful%20consideration%20for%20the%20further%0Adevelopment%20and%20deployment%20of%20tool-%20and%20retrieval-augmented%20LLMs.%20Resources%20are%0Aavailable%20at%20https%3A//github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.13300v4&entry.124074799=Read"},
{"title": "The Impact of Loss Functions and Scene Representations for 3D/2D\n  Registration on Single-view Fluoroscopic X-ray Pose Estimation", "author": "Chaochao Zhou and Syed Hasib Akhter Faruqui and Abhinav Patel and Ramez N. Abdalla and Michael C. Hurley and Ali Shaibani and Matthew B. Potts and Babak S. Jahromi and Sameer A. Ansari and Donald R. Cantrell", "abstract": "  Many tasks performed in image-guided procedures can be cast as pose\nestimation problems, where specific projections are chosen to reach a target in\n3D space. In this study, we first develop a differentiable projection\n(DiffProj) rendering framework for the efficient computation of Digitally\nReconstructed Radiographs (DRRs) with automatic differentiability from either\nCone-Beam Computerized Tomography (CBCT) or neural scene representations,\nincluding two newly proposed methods, Neural Tuned Tomography (NeTT) and masked\nNeural Radiance Fields (mNeRF). We then perform pose estimation by iterative\ngradient descent using various candidate loss functions, that quantify the\nimage discrepancy of the synthesized DRR with respect to the ground-truth\nfluoroscopic X-ray image. Compared to alternative loss functions, the Mutual\nInformation loss function can significantly improve pose estimation accuracy,\nas it can effectively prevent entrapment in local optima. Using the Mutual\nInformation loss, a comprehensive evaluation of pose estimation performed on a\ntomographic X-ray dataset of 50 patients$'$ skulls shows that utilizing either\ndiscretized (CBCT) or neural (NeTT/mNeRF) scene representations in DiffProj\nleads to comparable performance in DRR appearance and pose estimation (3D angle\nerrors: mean $\\leq$ 3.2{\\deg} and 90% quantile $\\leq$ 3.4{\\deg}), despite the\nlatter often incurring considerable training expenses and time. These findings\ncould be instrumental for selecting appropriate approaches to improve the\nefficiency and effectiveness of fluoroscopic X-ray pose estimation in\nwidespread image-guided interventions.\n", "link": "http://arxiv.org/abs/2308.00214v3", "date": "2024-02-27", "relevancy": 2.1339, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5615}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5383}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5035}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Loss%20Functions%20and%20Scene%20Representations%20for%203D/2D%0A%20%20Registration%20on%20Single-view%20Fluoroscopic%20X-ray%20Pose%20Estimation&entry.906535625=Chaochao%20Zhou%20and%20Syed%20Hasib%20Akhter%20Faruqui%20and%20Abhinav%20Patel%20and%20Ramez%20N.%20Abdalla%20and%20Michael%20C.%20Hurley%20and%20Ali%20Shaibani%20and%20Matthew%20B.%20Potts%20and%20Babak%20S.%20Jahromi%20and%20Sameer%20A.%20Ansari%20and%20Donald%20R.%20Cantrell&entry.1292438233=%20%20Many%20tasks%20performed%20in%20image-guided%20procedures%20can%20be%20cast%20as%20pose%0Aestimation%20problems%2C%20where%20specific%20projections%20are%20chosen%20to%20reach%20a%20target%20in%0A3D%20space.%20In%20this%20study%2C%20we%20first%20develop%20a%20differentiable%20projection%0A%28DiffProj%29%20rendering%20framework%20for%20the%20efficient%20computation%20of%20Digitally%0AReconstructed%20Radiographs%20%28DRRs%29%20with%20automatic%20differentiability%20from%20either%0ACone-Beam%20Computerized%20Tomography%20%28CBCT%29%20or%20neural%20scene%20representations%2C%0Aincluding%20two%20newly%20proposed%20methods%2C%20Neural%20Tuned%20Tomography%20%28NeTT%29%20and%20masked%0ANeural%20Radiance%20Fields%20%28mNeRF%29.%20We%20then%20perform%20pose%20estimation%20by%20iterative%0Agradient%20descent%20using%20various%20candidate%20loss%20functions%2C%20that%20quantify%20the%0Aimage%20discrepancy%20of%20the%20synthesized%20DRR%20with%20respect%20to%20the%20ground-truth%0Afluoroscopic%20X-ray%20image.%20Compared%20to%20alternative%20loss%20functions%2C%20the%20Mutual%0AInformation%20loss%20function%20can%20significantly%20improve%20pose%20estimation%20accuracy%2C%0Aas%20it%20can%20effectively%20prevent%20entrapment%20in%20local%20optima.%20Using%20the%20Mutual%0AInformation%20loss%2C%20a%20comprehensive%20evaluation%20of%20pose%20estimation%20performed%20on%20a%0Atomographic%20X-ray%20dataset%20of%2050%20patients%24%27%24%20skulls%20shows%20that%20utilizing%20either%0Adiscretized%20%28CBCT%29%20or%20neural%20%28NeTT/mNeRF%29%20scene%20representations%20in%20DiffProj%0Aleads%20to%20comparable%20performance%20in%20DRR%20appearance%20and%20pose%20estimation%20%283D%20angle%0Aerrors%3A%20mean%20%24%5Cleq%24%203.2%7B%5Cdeg%7D%20and%2090%25%20quantile%20%24%5Cleq%24%203.4%7B%5Cdeg%7D%29%2C%20despite%20the%0Alatter%20often%20incurring%20considerable%20training%20expenses%20and%20time.%20These%20findings%0Acould%20be%20instrumental%20for%20selecting%20appropriate%20approaches%20to%20improve%20the%0Aefficiency%20and%20effectiveness%20of%20fluoroscopic%20X-ray%20pose%20estimation%20in%0Awidespread%20image-guided%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.00214v3&entry.124074799=Read"},
{"title": "Sora Generates Videos with Stunning Geometrical Consistency", "author": "Xuanyi Li and Daquan Zhou and Chenxu Zhang and Shaodong Wei and Qibin Hou and Ming-Ming Cheng", "abstract": "  The recently developed Sora model [1] has exhibited remarkable capabilities\nin video generation, sparking intense discussions regarding its ability to\nsimulate real-world phenomena. Despite its growing popularity, there is a lack\nof established metrics to evaluate its fidelity to real-world physics\nquantitatively. In this paper, we introduce a new benchmark that assesses the\nquality of the generated videos based on their adherence to real-world physics\nprinciples. We employ a method that transforms the generated videos into 3D\nmodels, leveraging the premise that the accuracy of 3D reconstruction is\nheavily contingent on the video quality. From the perspective of 3D\nreconstruction, we use the fidelity of the geometric constraints satisfied by\nthe constructed 3D models as a proxy to gauge the extent to which the generated\nvideos conform to real-world physics rules. Project page:\nhttps://sora-geometrical-consistency.github.io/\n", "link": "http://arxiv.org/abs/2402.17403v1", "date": "2024-02-27", "relevancy": 2.132, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5374}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5326}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5229}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sora%20Generates%20Videos%20with%20Stunning%20Geometrical%20Consistency&entry.906535625=Xuanyi%20Li%20and%20Daquan%20Zhou%20and%20Chenxu%20Zhang%20and%20Shaodong%20Wei%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%20The%20recently%20developed%20Sora%20model%20%5B1%5D%20has%20exhibited%20remarkable%20capabilities%0Ain%20video%20generation%2C%20sparking%20intense%20discussions%20regarding%20its%20ability%20to%0Asimulate%20real-world%20phenomena.%20Despite%20its%20growing%20popularity%2C%20there%20is%20a%20lack%0Aof%20established%20metrics%20to%20evaluate%20its%20fidelity%20to%20real-world%20physics%0Aquantitatively.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20benchmark%20that%20assesses%20the%0Aquality%20of%20the%20generated%20videos%20based%20on%20their%20adherence%20to%20real-world%20physics%0Aprinciples.%20We%20employ%20a%20method%20that%20transforms%20the%20generated%20videos%20into%203D%0Amodels%2C%20leveraging%20the%20premise%20that%20the%20accuracy%20of%203D%20reconstruction%20is%0Aheavily%20contingent%20on%20the%20video%20quality.%20From%20the%20perspective%20of%203D%0Areconstruction%2C%20we%20use%20the%20fidelity%20of%20the%20geometric%20constraints%20satisfied%20by%0Athe%20constructed%203D%20models%20as%20a%20proxy%20to%20gauge%20the%20extent%20to%20which%20the%20generated%0Avideos%20conform%20to%20real-world%20physics%20rules.%20Project%20page%3A%0Ahttps%3A//sora-geometrical-consistency.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17403v1&entry.124074799=Read"},
{"title": "FedLPPA: Learning Personalized Prompt and Aggregation for Federated\n  Weakly-supervised Medical Image Segmentation", "author": "Li Lin and Yixiang Liu and Jiewei Wu and Pujin Cheng and Zhiyuan Cai and Kenneth K. Y. Wong and Xiaoying Tang", "abstract": "  Federated learning (FL) effectively mitigates the data silo challenge brought\nabout by policies and privacy concerns, implicitly harnessing more data for\ndeep model training. However, traditional centralized FL models grapple with\ndiverse multi-center data, especially in the face of significant data\nheterogeneity, notably in medical contexts. In the realm of medical image\nsegmentation, the growing imperative to curtail annotation costs has amplified\nthe importance of weakly-supervised techniques which utilize sparse annotations\nsuch as points, scribbles, etc. A pragmatic FL paradigm shall accommodate\ndiverse annotation formats across different sites, which research topic remains\nunder-investigated. In such context, we propose a novel personalized FL\nframework with learnable prompt and aggregation (FedLPPA) to uniformly leverage\nheterogeneous weak supervision for medical image segmentation. In FedLPPA, a\nlearnable universal knowledge prompt is maintained, complemented by multiple\nlearnable personalized data distribution prompts and prompts representing the\nsupervision sparsity. Integrated with sample features through a dual-attention\nmechanism, those prompts empower each local task decoder to adeptly adjust to\nboth the local distribution and the supervision form. Concurrently, a\ndual-decoder strategy, predicated on prompt similarity, is introduced for\nenhancing the generation of pseudo-labels in weakly-supervised learning,\nalleviating overfitting and noise accumulation inherent to local data, while an\nadaptable aggregation method is employed to customize the task decoder on a\nparameter-wise basis. Extensive experiments on three distinct medical image\nsegmentation tasks involving different modalities underscore the superiority of\nFedLPPA, with its efficacy closely parallels that of fully supervised\ncentralized training. Our code and data will be available.\n", "link": "http://arxiv.org/abs/2402.17502v1", "date": "2024-02-27", "relevancy": 2.1269, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5565}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5331}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5204}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedLPPA%3A%20Learning%20Personalized%20Prompt%20and%20Aggregation%20for%20Federated%0A%20%20Weakly-supervised%20Medical%20Image%20Segmentation&entry.906535625=Li%20Lin%20and%20Yixiang%20Liu%20and%20Jiewei%20Wu%20and%20Pujin%20Cheng%20and%20Zhiyuan%20Cai%20and%20Kenneth%20K.%20Y.%20Wong%20and%20Xiaoying%20Tang&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20effectively%20mitigates%20the%20data%20silo%20challenge%20brought%0Aabout%20by%20policies%20and%20privacy%20concerns%2C%20implicitly%20harnessing%20more%20data%20for%0Adeep%20model%20training.%20However%2C%20traditional%20centralized%20FL%20models%20grapple%20with%0Adiverse%20multi-center%20data%2C%20especially%20in%20the%20face%20of%20significant%20data%0Aheterogeneity%2C%20notably%20in%20medical%20contexts.%20In%20the%20realm%20of%20medical%20image%0Asegmentation%2C%20the%20growing%20imperative%20to%20curtail%20annotation%20costs%20has%20amplified%0Athe%20importance%20of%20weakly-supervised%20techniques%20which%20utilize%20sparse%20annotations%0Asuch%20as%20points%2C%20scribbles%2C%20etc.%20A%20pragmatic%20FL%20paradigm%20shall%20accommodate%0Adiverse%20annotation%20formats%20across%20different%20sites%2C%20which%20research%20topic%20remains%0Aunder-investigated.%20In%20such%20context%2C%20we%20propose%20a%20novel%20personalized%20FL%0Aframework%20with%20learnable%20prompt%20and%20aggregation%20%28FedLPPA%29%20to%20uniformly%20leverage%0Aheterogeneous%20weak%20supervision%20for%20medical%20image%20segmentation.%20In%20FedLPPA%2C%20a%0Alearnable%20universal%20knowledge%20prompt%20is%20maintained%2C%20complemented%20by%20multiple%0Alearnable%20personalized%20data%20distribution%20prompts%20and%20prompts%20representing%20the%0Asupervision%20sparsity.%20Integrated%20with%20sample%20features%20through%20a%20dual-attention%0Amechanism%2C%20those%20prompts%20empower%20each%20local%20task%20decoder%20to%20adeptly%20adjust%20to%0Aboth%20the%20local%20distribution%20and%20the%20supervision%20form.%20Concurrently%2C%20a%0Adual-decoder%20strategy%2C%20predicated%20on%20prompt%20similarity%2C%20is%20introduced%20for%0Aenhancing%20the%20generation%20of%20pseudo-labels%20in%20weakly-supervised%20learning%2C%0Aalleviating%20overfitting%20and%20noise%20accumulation%20inherent%20to%20local%20data%2C%20while%20an%0Aadaptable%20aggregation%20method%20is%20employed%20to%20customize%20the%20task%20decoder%20on%20a%0Aparameter-wise%20basis.%20Extensive%20experiments%20on%20three%20distinct%20medical%20image%0Asegmentation%20tasks%20involving%20different%20modalities%20underscore%20the%20superiority%20of%0AFedLPPA%2C%20with%20its%20efficacy%20closely%20parallels%20that%20of%20fully%20supervised%0Acentralized%20training.%20Our%20code%20and%20data%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17502v1&entry.124074799=Read"},
{"title": "Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis", "author": "Zicheng Zhang and Ruobing Zheng and Ziwen Liu and Congying Han and Tianqi Li and Meng Wang and Tiande Guo and Jingdong Chen and Bonan Li and Ming Yang", "abstract": "  Recent works in implicit representations, such as Neural Radiance Fields\n(NeRF), have advanced the generation of realistic and animatable head avatars\nfrom video sequences. These implicit methods are still confronted by visual\nartifacts and jitters, since the lack of explicit geometric constraints poses a\nfundamental challenge in accurately modeling complex facial deformations. In\nthis paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid\nrepresentation that encodes explicit dynamic meshes by neural networks to\nensure geometric consistency across various motions and viewpoints. DynTet is\nparameterized by the coordinate-based networks which learn signed distance,\ndeformation, and material texture, anchoring the training data into a\npredefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently\ndecodes textured meshes with a consistent topology, enabling fast rendering\nthrough a differentiable rasterizer and supervision via a pixel loss. To\nenhance training efficiency, we incorporate classical 3D Morphable Models to\nfacilitate geometry learning and define a canonical space for simplifying\ntexture learning. These advantages are readily achievable owing to the\neffective geometric representation employed in DynTet. Compared with prior\nworks, DynTet demonstrates significant improvements in fidelity, lip\nsynchronization, and real-time performance according to various metrics. Beyond\nproducing stable and visually appealing synthesis videos, our method also\noutputs the dynamic meshes which is promising to enable many emerging\napplications.\n", "link": "http://arxiv.org/abs/2402.17364v1", "date": "2024-02-27", "relevancy": 2.1174, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5426}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5344}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5141}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamic%20Tetrahedra%20for%20High-Quality%20Talking%20Head%20Synthesis&entry.906535625=Zicheng%20Zhang%20and%20Ruobing%20Zheng%20and%20Ziwen%20Liu%20and%20Congying%20Han%20and%20Tianqi%20Li%20and%20Meng%20Wang%20and%20Tiande%20Guo%20and%20Jingdong%20Chen%20and%20Bonan%20Li%20and%20Ming%20Yang&entry.1292438233=%20%20Recent%20works%20in%20implicit%20representations%2C%20such%20as%20Neural%20Radiance%20Fields%0A%28NeRF%29%2C%20have%20advanced%20the%20generation%20of%20realistic%20and%20animatable%20head%20avatars%0Afrom%20video%20sequences.%20These%20implicit%20methods%20are%20still%20confronted%20by%20visual%0Aartifacts%20and%20jitters%2C%20since%20the%20lack%20of%20explicit%20geometric%20constraints%20poses%20a%0Afundamental%20challenge%20in%20accurately%20modeling%20complex%20facial%20deformations.%20In%0Athis%20paper%2C%20we%20introduce%20Dynamic%20Tetrahedra%20%28DynTet%29%2C%20a%20novel%20hybrid%0Arepresentation%20that%20encodes%20explicit%20dynamic%20meshes%20by%20neural%20networks%20to%0Aensure%20geometric%20consistency%20across%20various%20motions%20and%20viewpoints.%20DynTet%20is%0Aparameterized%20by%20the%20coordinate-based%20networks%20which%20learn%20signed%20distance%2C%0Adeformation%2C%20and%20material%20texture%2C%20anchoring%20the%20training%20data%20into%20a%0Apredefined%20tetrahedra%20grid.%20Leveraging%20Marching%20Tetrahedra%2C%20DynTet%20efficiently%0Adecodes%20textured%20meshes%20with%20a%20consistent%20topology%2C%20enabling%20fast%20rendering%0Athrough%20a%20differentiable%20rasterizer%20and%20supervision%20via%20a%20pixel%20loss.%20To%0Aenhance%20training%20efficiency%2C%20we%20incorporate%20classical%203D%20Morphable%20Models%20to%0Afacilitate%20geometry%20learning%20and%20define%20a%20canonical%20space%20for%20simplifying%0Atexture%20learning.%20These%20advantages%20are%20readily%20achievable%20owing%20to%20the%0Aeffective%20geometric%20representation%20employed%20in%20DynTet.%20Compared%20with%20prior%0Aworks%2C%20DynTet%20demonstrates%20significant%20improvements%20in%20fidelity%2C%20lip%0Asynchronization%2C%20and%20real-time%20performance%20according%20to%20various%20metrics.%20Beyond%0Aproducing%20stable%20and%20visually%20appealing%20synthesis%20videos%2C%20our%20method%20also%0Aoutputs%20the%20dynamic%20meshes%20which%20is%20promising%20to%20enable%20many%20emerging%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17364v1&entry.124074799=Read"},
{"title": "Neural Video Compression with Feature Modulation", "author": "Jiahao Li and Bin Li and Yan Lu", "abstract": "  The emerging conditional coding-based neural video codec (NVC) shows\nsuperiority over commonly-used residual coding-based codec and the latest NVC\nalready claims to outperform the best traditional codec. However, there still\nexist critical problems blocking the practicality of NVC. In this paper, we\npropose a powerful conditional coding-based NVC that solves two critical\nproblems via feature modulation. The first is how to support a wide quality\nrange in a single model. Previous NVC with this capability only supports about\n3.8 dB PSNR range on average. To tackle this limitation, we modulate the latent\nfeature of the current frame via the learnable quantization scaler. During the\ntraining, we specially design the uniform quantization parameter sampling\nmechanism to improve the harmonization of encoding and quantization. This\nresults in a better learning of the quantization scaler and helps our NVC\nsupport about 11.4 dB PSNR range. The second is how to make NVC still work\nunder a long prediction chain. We expose that the previous SOTA NVC has an\nobvious quality degradation problem when using a large intra-period setting. To\nthis end, we propose modulating the temporal feature with a periodically\nrefreshing mechanism to boost the quality. %Besides solving the above two\nproblems, we also design a single model that can support both RGB and YUV\ncolorspaces. Notably, under single intra-frame setting, our codec can achieve\n29.7\\% bitrate saving over previous SOTA NVC with 16\\% MACs reduction. Our\ncodec serves as a notable landmark in the journey of NVC evolution. The codes\nare at https://github.com/microsoft/DCVC.\n", "link": "http://arxiv.org/abs/2402.17414v1", "date": "2024-02-27", "relevancy": 2.1166, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.556}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5365}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4994}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Video%20Compression%20with%20Feature%20Modulation&entry.906535625=Jiahao%20Li%20and%20Bin%20Li%20and%20Yan%20Lu&entry.1292438233=%20%20The%20emerging%20conditional%20coding-based%20neural%20video%20codec%20%28NVC%29%20shows%0Asuperiority%20over%20commonly-used%20residual%20coding-based%20codec%20and%20the%20latest%20NVC%0Aalready%20claims%20to%20outperform%20the%20best%20traditional%20codec.%20However%2C%20there%20still%0Aexist%20critical%20problems%20blocking%20the%20practicality%20of%20NVC.%20In%20this%20paper%2C%20we%0Apropose%20a%20powerful%20conditional%20coding-based%20NVC%20that%20solves%20two%20critical%0Aproblems%20via%20feature%20modulation.%20The%20first%20is%20how%20to%20support%20a%20wide%20quality%0Arange%20in%20a%20single%20model.%20Previous%20NVC%20with%20this%20capability%20only%20supports%20about%0A3.8%20dB%20PSNR%20range%20on%20average.%20To%20tackle%20this%20limitation%2C%20we%20modulate%20the%20latent%0Afeature%20of%20the%20current%20frame%20via%20the%20learnable%20quantization%20scaler.%20During%20the%0Atraining%2C%20we%20specially%20design%20the%20uniform%20quantization%20parameter%20sampling%0Amechanism%20to%20improve%20the%20harmonization%20of%20encoding%20and%20quantization.%20This%0Aresults%20in%20a%20better%20learning%20of%20the%20quantization%20scaler%20and%20helps%20our%20NVC%0Asupport%20about%2011.4%20dB%20PSNR%20range.%20The%20second%20is%20how%20to%20make%20NVC%20still%20work%0Aunder%20a%20long%20prediction%20chain.%20We%20expose%20that%20the%20previous%20SOTA%20NVC%20has%20an%0Aobvious%20quality%20degradation%20problem%20when%20using%20a%20large%20intra-period%20setting.%20To%0Athis%20end%2C%20we%20propose%20modulating%20the%20temporal%20feature%20with%20a%20periodically%0Arefreshing%20mechanism%20to%20boost%20the%20quality.%20%25Besides%20solving%20the%20above%20two%0Aproblems%2C%20we%20also%20design%20a%20single%20model%20that%20can%20support%20both%20RGB%20and%20YUV%0Acolorspaces.%20Notably%2C%20under%20single%20intra-frame%20setting%2C%20our%20codec%20can%20achieve%0A29.7%5C%25%20bitrate%20saving%20over%20previous%20SOTA%20NVC%20with%2016%5C%25%20MACs%20reduction.%20Our%0Acodec%20serves%20as%20a%20notable%20landmark%20in%20the%20journey%20of%20NVC%20evolution.%20The%20codes%0Aare%20at%20https%3A//github.com/microsoft/DCVC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17414v1&entry.124074799=Read"},
{"title": "Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional\n  layers for 3D abdominal organ segmentation", "author": "Wenzhao Zhao and Steffen Albert and Barbara D. Wichtmann and Angelika Maurer and Ulrike Attenberger and Frank G. Z\u00f6llner and J\u00fcrgen Hesser", "abstract": "  Filter-decomposition-based group equivariant convolutional neural networks\nshow promising stability and data efficiency for 3D image feature extraction.\nHowever, the existing filter-decomposition-based 3D group equivariant neural\nnetworks rely on parameter-sharing designs and are mostly limited to rotation\ntransform groups, where the chosen spherical harmonic filter bases consider\nonly angular orthogonality. These limitations hamper its application to deep\nneural network architectures for medical image segmentation. To address these\nissues, this paper describes a non-parameter-sharing affine group equivariant\nneural network for 3D medical image segmentation based on an adaptive\naggregation of Monte Carlo augmented spherical Fourier Bessel filter bases. The\nefficiency and flexibility of the adopted non-parameter strategy enable for the\nfirst time an efficient implementation of 3D affine group equivariant\nconvolutional neural networks for volumetric data. The introduced spherical\nBessel Fourier filter basis combines both angular and radial orthogonality for\nbetter feature extraction. The 3D image segmentation experiments on two\nabdominal image sets, BTCV and the NIH Pancreas datasets, show that the\nproposed methods excel the state-of-the-art 3D neural networks with high\ntraining stability and data efficiency. The code will be available at\nhttps://github.com/ZhaoWenzhao/WVMS.\n", "link": "http://arxiv.org/abs/2402.16825v2", "date": "2024-02-27", "relevancy": 2.1118, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5388}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5259}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5061}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weighted%20Monte%20Carlo%20augmented%20spherical%20Fourier-Bessel%20convolutional%0A%20%20layers%20for%203D%20abdominal%20organ%20segmentation&entry.906535625=Wenzhao%20Zhao%20and%20Steffen%20Albert%20and%20Barbara%20D.%20Wichtmann%20and%20Angelika%20Maurer%20and%20Ulrike%20Attenberger%20and%20Frank%20G.%20Z%C3%B6llner%20and%20J%C3%BCrgen%20Hesser&entry.1292438233=%20%20Filter-decomposition-based%20group%20equivariant%20convolutional%20neural%20networks%0Ashow%20promising%20stability%20and%20data%20efficiency%20for%203D%20image%20feature%20extraction.%0AHowever%2C%20the%20existing%20filter-decomposition-based%203D%20group%20equivariant%20neural%0Anetworks%20rely%20on%20parameter-sharing%20designs%20and%20are%20mostly%20limited%20to%20rotation%0Atransform%20groups%2C%20where%20the%20chosen%20spherical%20harmonic%20filter%20bases%20consider%0Aonly%20angular%20orthogonality.%20These%20limitations%20hamper%20its%20application%20to%20deep%0Aneural%20network%20architectures%20for%20medical%20image%20segmentation.%20To%20address%20these%0Aissues%2C%20this%20paper%20describes%20a%20non-parameter-sharing%20affine%20group%20equivariant%0Aneural%20network%20for%203D%20medical%20image%20segmentation%20based%20on%20an%20adaptive%0Aaggregation%20of%20Monte%20Carlo%20augmented%20spherical%20Fourier%20Bessel%20filter%20bases.%20The%0Aefficiency%20and%20flexibility%20of%20the%20adopted%20non-parameter%20strategy%20enable%20for%20the%0Afirst%20time%20an%20efficient%20implementation%20of%203D%20affine%20group%20equivariant%0Aconvolutional%20neural%20networks%20for%20volumetric%20data.%20The%20introduced%20spherical%0ABessel%20Fourier%20filter%20basis%20combines%20both%20angular%20and%20radial%20orthogonality%20for%0Abetter%20feature%20extraction.%20The%203D%20image%20segmentation%20experiments%20on%20two%0Aabdominal%20image%20sets%2C%20BTCV%20and%20the%20NIH%20Pancreas%20datasets%2C%20show%20that%20the%0Aproposed%20methods%20excel%20the%20state-of-the-art%203D%20neural%20networks%20with%20high%0Atraining%20stability%20and%20data%20efficiency.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/ZhaoWenzhao/WVMS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16825v2&entry.124074799=Read"},
{"title": "QUCE: The Minimisation and Quantification of Path-Based Uncertainty for\n  Generative Counterfactual Explanations", "author": "Jamie Duell and Hsuan Fu and Monika Seisenberger and Xiuyi Fan", "abstract": "  Deep Neural Networks (DNNs) stand out as one of the most prominent approaches\nwithin the Machine Learning (ML) domain. The efficacy of DNNs has surged\nalongside recent increases in computational capacity, allowing these approaches\nto scale to significant complexities for addressing predictive challenges in\nbig data. However, as the complexity of DNN models rises, interpretability\ndiminishes. In response to this challenge, explainable models such as\nAdversarial Gradient Integration (AGI) leverage path-based gradients provided\nby DNNs to elucidate their decisions. Yet the performance of path-based\nexplainers can be compromised when gradients exhibit irregularities during\nout-of-distribution path traversal. In this context, we introduce Quantified\nUncertainty Counterfactual Explanations (QUCE), a method designed to mitigate\nout-of-distribution traversal by minimizing path uncertainty. QUCE not only\nquantifies uncertainty when presenting explanations but also generates more\ncertain counterfactual examples. We showcase the performance of the QUCE method\nby comparing it with competing methods for both path-based explanations and\ngenerative counterfactual examples. The code repository for the QUCE method is\navailable at: https://github.com/jamie-duell/QUCE.\n", "link": "http://arxiv.org/abs/2402.17516v1", "date": "2024-02-27", "relevancy": 2.1043, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5642}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.519}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5179}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QUCE%3A%20The%20Minimisation%20and%20Quantification%20of%20Path-Based%20Uncertainty%20for%0A%20%20Generative%20Counterfactual%20Explanations&entry.906535625=Jamie%20Duell%20and%20Hsuan%20Fu%20and%20Monika%20Seisenberger%20and%20Xiuyi%20Fan&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20stand%20out%20as%20one%20of%20the%20most%20prominent%20approaches%0Awithin%20the%20Machine%20Learning%20%28ML%29%20domain.%20The%20efficacy%20of%20DNNs%20has%20surged%0Aalongside%20recent%20increases%20in%20computational%20capacity%2C%20allowing%20these%20approaches%0Ato%20scale%20to%20significant%20complexities%20for%20addressing%20predictive%20challenges%20in%0Abig%20data.%20However%2C%20as%20the%20complexity%20of%20DNN%20models%20rises%2C%20interpretability%0Adiminishes.%20In%20response%20to%20this%20challenge%2C%20explainable%20models%20such%20as%0AAdversarial%20Gradient%20Integration%20%28AGI%29%20leverage%20path-based%20gradients%20provided%0Aby%20DNNs%20to%20elucidate%20their%20decisions.%20Yet%20the%20performance%20of%20path-based%0Aexplainers%20can%20be%20compromised%20when%20gradients%20exhibit%20irregularities%20during%0Aout-of-distribution%20path%20traversal.%20In%20this%20context%2C%20we%20introduce%20Quantified%0AUncertainty%20Counterfactual%20Explanations%20%28QUCE%29%2C%20a%20method%20designed%20to%20mitigate%0Aout-of-distribution%20traversal%20by%20minimizing%20path%20uncertainty.%20QUCE%20not%20only%0Aquantifies%20uncertainty%20when%20presenting%20explanations%20but%20also%20generates%20more%0Acertain%20counterfactual%20examples.%20We%20showcase%20the%20performance%20of%20the%20QUCE%20method%0Aby%20comparing%20it%20with%20competing%20methods%20for%20both%20path-based%20explanations%20and%0Agenerative%20counterfactual%20examples.%20The%20code%20repository%20for%20the%20QUCE%20method%20is%0Aavailable%20at%3A%20https%3A//github.com/jamie-duell/QUCE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17516v1&entry.124074799=Read"},
{"title": "Backpropagation-Based Analytical Derivatives of EKF Covariance for\n  Active Sensing", "author": "Jonas Benhamou and Silv\u00e8re Bonnabel and Camille Chapdelaine", "abstract": "  To enhance accuracy of robot state estimation, perception-aware (or active\nsensing) methods seek trajectories that minimize uncertainty. To this aim, one\npossibility is to seek trajectories that minimize the final covariance of an\nextended Kalman filter (EKF), w.r.t. its control inputs over a given horizon.\nHowever, this can be computationally demanding. In this article, we derive\nnovel backpropagation analytical formulas for the derivatives of the final\ncovariance of an EKF w.r.t. its inputs. We then leverage the obtained gradients\nas an enabling technology to derive perception-aware optimal motion plans.\nSimulations validate the approach, showcasing improvements in both estimation\naccuracy and execution time. Experimental results on a real large ground\nvehicle also support the method.\n", "link": "http://arxiv.org/abs/2402.17569v1", "date": "2024-02-27", "relevancy": 2.0992, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5825}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5229}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5036}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backpropagation-Based%20Analytical%20Derivatives%20of%20EKF%20Covariance%20for%0A%20%20Active%20Sensing&entry.906535625=Jonas%20Benhamou%20and%20Silv%C3%A8re%20Bonnabel%20and%20Camille%20Chapdelaine&entry.1292438233=%20%20To%20enhance%20accuracy%20of%20robot%20state%20estimation%2C%20perception-aware%20%28or%20active%0Asensing%29%20methods%20seek%20trajectories%20that%20minimize%20uncertainty.%20To%20this%20aim%2C%20one%0Apossibility%20is%20to%20seek%20trajectories%20that%20minimize%20the%20final%20covariance%20of%20an%0Aextended%20Kalman%20filter%20%28EKF%29%2C%20w.r.t.%20its%20control%20inputs%20over%20a%20given%20horizon.%0AHowever%2C%20this%20can%20be%20computationally%20demanding.%20In%20this%20article%2C%20we%20derive%0Anovel%20backpropagation%20analytical%20formulas%20for%20the%20derivatives%20of%20the%20final%0Acovariance%20of%20an%20EKF%20w.r.t.%20its%20inputs.%20We%20then%20leverage%20the%20obtained%20gradients%0Aas%20an%20enabling%20technology%20to%20derive%20perception-aware%20optimal%20motion%20plans.%0ASimulations%20validate%20the%20approach%2C%20showcasing%20improvements%20in%20both%20estimation%0Aaccuracy%20and%20execution%20time.%20Experimental%20results%20on%20a%20real%20large%20ground%0Avehicle%20also%20support%20the%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17569v1&entry.124074799=Read"},
{"title": "MedContext: Learning Contextual Cues for Efficient Volumetric Medical\n  Segmentation", "author": "Hanan Gani and Muzammal Naseer and Fahad Khan and Salman Khan", "abstract": "  Volumetric medical segmentation is a critical component of 3D medical image\nanalysis that delineates different semantic regions. Deep neural networks have\nsignificantly improved volumetric medical segmentation, but they generally\nrequire large-scale annotated data to achieve better performance, which can be\nexpensive and prohibitive to obtain. To address this limitation, existing works\ntypically perform transfer learning or design dedicated pretraining-finetuning\nstages to learn representative features. However, the mismatch between the\nsource and target domain can make it challenging to learn optimal\nrepresentation for volumetric data, while the multi-stage training demands\nhigher compute as well as careful selection of stage-specific design choices.\nIn contrast, we propose a universal training framework called MedContext that\nis architecture-agnostic and can be incorporated into any existing training\nframework for 3D medical segmentation. Our approach effectively learns self\nsupervised contextual cues jointly with the supervised voxel segmentation task\nwithout requiring large-scale annotated volumetric medical data or dedicated\npretraining-finetuning stages. The proposed approach induces contextual\nknowledge in the network by learning to reconstruct the missing organ or parts\nof an organ in the output segmentation space. The effectiveness of MedContext\nis validated across multiple 3D medical datasets and four state-of-the-art\nmodel architectures. Our approach demonstrates consistent gains in segmentation\nperformance across datasets and different architectures even in few-shot data\nscenarios. Our code and pretrained models are available at\nhttps://github.com/hananshafi/MedContext\n", "link": "http://arxiv.org/abs/2402.17725v1", "date": "2024-02-27", "relevancy": 2.0946, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5699}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5393}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4895}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedContext%3A%20Learning%20Contextual%20Cues%20for%20Efficient%20Volumetric%20Medical%0A%20%20Segmentation&entry.906535625=Hanan%20Gani%20and%20Muzammal%20Naseer%20and%20Fahad%20Khan%20and%20Salman%20Khan&entry.1292438233=%20%20Volumetric%20medical%20segmentation%20is%20a%20critical%20component%20of%203D%20medical%20image%0Aanalysis%20that%20delineates%20different%20semantic%20regions.%20Deep%20neural%20networks%20have%0Asignificantly%20improved%20volumetric%20medical%20segmentation%2C%20but%20they%20generally%0Arequire%20large-scale%20annotated%20data%20to%20achieve%20better%20performance%2C%20which%20can%20be%0Aexpensive%20and%20prohibitive%20to%20obtain.%20To%20address%20this%20limitation%2C%20existing%20works%0Atypically%20perform%20transfer%20learning%20or%20design%20dedicated%20pretraining-finetuning%0Astages%20to%20learn%20representative%20features.%20However%2C%20the%20mismatch%20between%20the%0Asource%20and%20target%20domain%20can%20make%20it%20challenging%20to%20learn%20optimal%0Arepresentation%20for%20volumetric%20data%2C%20while%20the%20multi-stage%20training%20demands%0Ahigher%20compute%20as%20well%20as%20careful%20selection%20of%20stage-specific%20design%20choices.%0AIn%20contrast%2C%20we%20propose%20a%20universal%20training%20framework%20called%20MedContext%20that%0Ais%20architecture-agnostic%20and%20can%20be%20incorporated%20into%20any%20existing%20training%0Aframework%20for%203D%20medical%20segmentation.%20Our%20approach%20effectively%20learns%20self%0Asupervised%20contextual%20cues%20jointly%20with%20the%20supervised%20voxel%20segmentation%20task%0Awithout%20requiring%20large-scale%20annotated%20volumetric%20medical%20data%20or%20dedicated%0Apretraining-finetuning%20stages.%20The%20proposed%20approach%20induces%20contextual%0Aknowledge%20in%20the%20network%20by%20learning%20to%20reconstruct%20the%20missing%20organ%20or%20parts%0Aof%20an%20organ%20in%20the%20output%20segmentation%20space.%20The%20effectiveness%20of%20MedContext%0Ais%20validated%20across%20multiple%203D%20medical%20datasets%20and%20four%20state-of-the-art%0Amodel%20architectures.%20Our%20approach%20demonstrates%20consistent%20gains%20in%20segmentation%0Aperformance%20across%20datasets%20and%20different%20architectures%20even%20in%20few-shot%20data%0Ascenarios.%20Our%20code%20and%20pretrained%20models%20are%20available%20at%0Ahttps%3A//github.com/hananshafi/MedContext%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17725v1&entry.124074799=Read"},
{"title": "UVDoc: Neural Grid-based Document Unwarping", "author": "Floor Verhoeven and Tanguy Magne and Olga Sorkine-Hornung", "abstract": "  Restoring the original, flat appearance of a printed document from casual\nphotographs of bent and wrinkled pages is a common everyday problem. In this\npaper we propose a novel method for grid-based single-image document unwarping.\nOur method performs geometric distortion correction via a fully convolutional\ndeep neural network that learns to predict the 3D grid mesh of the document and\nthe corresponding 2D unwarping grid in a dual-task fashion, implicitly encoding\nthe coupling between the shape of a 3D piece of paper and its 2D image. In\norder to allow unwarping models to train on data that is more realistic in\nappearance than the commonly used synthetic Doc3D dataset, we create and\npublish our own dataset, called UVDoc, which combines pseudo-photorealistic\ndocument images with physically accurate 3D shape and unwarping function\nannotations. Our dataset is labeled with all the information necessary to train\nour unwarping network, without having to engineer separate loss functions that\ncan deal with the lack of ground-truth typically found in document in the wild\ndatasets. We perform an in-depth evaluation that demonstrates that with the\ninclusion of our novel pseudo-photorealistic dataset, our relatively small\nnetwork architecture achieves state-of-the-art results on the DocUNet\nbenchmark. We show that the pseudo-photorealistic nature of our UVDoc dataset\nallows for new and better evaluation methods, such as lighting-corrected\nMS-SSIM. We provide a novel benchmark dataset that facilitates such\nevaluations, and propose a metric that quantifies line straightness after\nunwarping. Our code, results and UVDoc dataset are available at\nhttps://github.com/tanguymagne/UVDoc.\n", "link": "http://arxiv.org/abs/2302.02887v2", "date": "2024-02-27", "relevancy": 2.0917, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5353}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.529}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5081}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UVDoc%3A%20Neural%20Grid-based%20Document%20Unwarping&entry.906535625=Floor%20Verhoeven%20and%20Tanguy%20Magne%20and%20Olga%20Sorkine-Hornung&entry.1292438233=%20%20Restoring%20the%20original%2C%20flat%20appearance%20of%20a%20printed%20document%20from%20casual%0Aphotographs%20of%20bent%20and%20wrinkled%20pages%20is%20a%20common%20everyday%20problem.%20In%20this%0Apaper%20we%20propose%20a%20novel%20method%20for%20grid-based%20single-image%20document%20unwarping.%0AOur%20method%20performs%20geometric%20distortion%20correction%20via%20a%20fully%20convolutional%0Adeep%20neural%20network%20that%20learns%20to%20predict%20the%203D%20grid%20mesh%20of%20the%20document%20and%0Athe%20corresponding%202D%20unwarping%20grid%20in%20a%20dual-task%20fashion%2C%20implicitly%20encoding%0Athe%20coupling%20between%20the%20shape%20of%20a%203D%20piece%20of%20paper%20and%20its%202D%20image.%20In%0Aorder%20to%20allow%20unwarping%20models%20to%20train%20on%20data%20that%20is%20more%20realistic%20in%0Aappearance%20than%20the%20commonly%20used%20synthetic%20Doc3D%20dataset%2C%20we%20create%20and%0Apublish%20our%20own%20dataset%2C%20called%20UVDoc%2C%20which%20combines%20pseudo-photorealistic%0Adocument%20images%20with%20physically%20accurate%203D%20shape%20and%20unwarping%20function%0Aannotations.%20Our%20dataset%20is%20labeled%20with%20all%20the%20information%20necessary%20to%20train%0Aour%20unwarping%20network%2C%20without%20having%20to%20engineer%20separate%20loss%20functions%20that%0Acan%20deal%20with%20the%20lack%20of%20ground-truth%20typically%20found%20in%20document%20in%20the%20wild%0Adatasets.%20We%20perform%20an%20in-depth%20evaluation%20that%20demonstrates%20that%20with%20the%0Ainclusion%20of%20our%20novel%20pseudo-photorealistic%20dataset%2C%20our%20relatively%20small%0Anetwork%20architecture%20achieves%20state-of-the-art%20results%20on%20the%20DocUNet%0Abenchmark.%20We%20show%20that%20the%20pseudo-photorealistic%20nature%20of%20our%20UVDoc%20dataset%0Aallows%20for%20new%20and%20better%20evaluation%20methods%2C%20such%20as%20lighting-corrected%0AMS-SSIM.%20We%20provide%20a%20novel%20benchmark%20dataset%20that%20facilitates%20such%0Aevaluations%2C%20and%20propose%20a%20metric%20that%20quantifies%20line%20straightness%20after%0Aunwarping.%20Our%20code%2C%20results%20and%20UVDoc%20dataset%20are%20available%20at%0Ahttps%3A//github.com/tanguymagne/UVDoc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.02887v2&entry.124074799=Read"},
{"title": "Visual Abductive Reasoning Meets Driving Hazard Prediction", "author": "Korawat Charoenpitaks and Van-Quang Nguyen and Masanori Suganuma and Masahiro Takahashi and Ryoma Niihara and Takayuki Okatani", "abstract": "  This paper addresses the problem of predicting hazards that drivers may\nencounter while driving a car. We formulate it as a task of anticipating\nimpending accidents using a single input image captured by car dashcams. Unlike\nexisting approaches to driving hazard prediction that rely on computational\nsimulations or anomaly detection from videos, this study focuses on high-level\ninference from static images. The problem needs predicting and reasoning about\nfuture events based on uncertain observations, which falls under visual\nabductive reasoning. To enable research in this understudied area, a new\ndataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is\ncreated. The dataset consists of 15K dashcam images of street scenes, and each\nimage is associated with a tuple containing car speed, a hypothesized hazard\ndescription, and visual entities present in the scene. These are annotated by\nhuman annotators, who identify risky scenes and provide descriptions of\npotential accidents that could occur a few seconds later. We present several\nbaseline methods and evaluate their performance on our dataset, identifying\nremaining issues and discussing future directions. This study contributes to\nthe field by introducing a novel problem formulation and dataset, enabling\nresearchers to explore the potential of multi-modal AI for driving hazard\nprediction.\n", "link": "http://arxiv.org/abs/2310.04671v3", "date": "2024-02-27", "relevancy": 2.0865, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5653}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5201}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5056}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Abductive%20Reasoning%20Meets%20Driving%20Hazard%20Prediction&entry.906535625=Korawat%20Charoenpitaks%20and%20Van-Quang%20Nguyen%20and%20Masanori%20Suganuma%20and%20Masahiro%20Takahashi%20and%20Ryoma%20Niihara%20and%20Takayuki%20Okatani&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20predicting%20hazards%20that%20drivers%20may%0Aencounter%20while%20driving%20a%20car.%20We%20formulate%20it%20as%20a%20task%20of%20anticipating%0Aimpending%20accidents%20using%20a%20single%20input%20image%20captured%20by%20car%20dashcams.%20Unlike%0Aexisting%20approaches%20to%20driving%20hazard%20prediction%20that%20rely%20on%20computational%0Asimulations%20or%20anomaly%20detection%20from%20videos%2C%20this%20study%20focuses%20on%20high-level%0Ainference%20from%20static%20images.%20The%20problem%20needs%20predicting%20and%20reasoning%20about%0Afuture%20events%20based%20on%20uncertain%20observations%2C%20which%20falls%20under%20visual%0Aabductive%20reasoning.%20To%20enable%20research%20in%20this%20understudied%20area%2C%20a%20new%0Adataset%20named%20the%20DHPR%20%28Driving%20Hazard%20Prediction%20and%20Reasoning%29%20dataset%20is%0Acreated.%20The%20dataset%20consists%20of%2015K%20dashcam%20images%20of%20street%20scenes%2C%20and%20each%0Aimage%20is%20associated%20with%20a%20tuple%20containing%20car%20speed%2C%20a%20hypothesized%20hazard%0Adescription%2C%20and%20visual%20entities%20present%20in%20the%20scene.%20These%20are%20annotated%20by%0Ahuman%20annotators%2C%20who%20identify%20risky%20scenes%20and%20provide%20descriptions%20of%0Apotential%20accidents%20that%20could%20occur%20a%20few%20seconds%20later.%20We%20present%20several%0Abaseline%20methods%20and%20evaluate%20their%20performance%20on%20our%20dataset%2C%20identifying%0Aremaining%20issues%20and%20discussing%20future%20directions.%20This%20study%20contributes%20to%0Athe%20field%20by%20introducing%20a%20novel%20problem%20formulation%20and%20dataset%2C%20enabling%0Aresearchers%20to%20explore%20the%20potential%20of%20multi-modal%20AI%20for%20driving%20hazard%0Aprediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04671v3&entry.124074799=Read"},
{"title": "DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic\n  Autonomous Driving Scenes", "author": "Xiaoyu Zhou and Zhiwei Lin and Xiaojun Shan and Yongtao Wang and Deqing Sun and Ming-Hsuan Yang", "abstract": "  We present DrivingGaussian, an efficient and effective framework for\nsurrounding dynamic autonomous driving scenes. For complex scenes with moving\nobjects, we first sequentially and progressively model the static background of\nthe entire scene with incremental static 3D Gaussians. We then leverage a\ncomposite dynamic Gaussian graph to handle multiple moving objects,\nindividually reconstructing each object and restoring their accurate positions\nand occlusion relationships within the scene. We further use a LiDAR prior for\nGaussian Splatting to reconstruct scenes with greater details and maintain\npanoramic consistency. DrivingGaussian outperforms existing methods in driving\nscene reconstruction and enables photorealistic surround-view synthesis with\nhigh-fidelity and multi-camera consistency. Our project page is at:\nhttps://github.com/VDIGPKU/DrivingGaussian.\n", "link": "http://arxiv.org/abs/2312.07920v2", "date": "2024-02-27", "relevancy": 2.0814, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5267}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5195}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5068}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DrivingGaussian%3A%20Composite%20Gaussian%20Splatting%20for%20Surrounding%20Dynamic%0A%20%20Autonomous%20Driving%20Scenes&entry.906535625=Xiaoyu%20Zhou%20and%20Zhiwei%20Lin%20and%20Xiaojun%20Shan%20and%20Yongtao%20Wang%20and%20Deqing%20Sun%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20We%20present%20DrivingGaussian%2C%20an%20efficient%20and%20effective%20framework%20for%0Asurrounding%20dynamic%20autonomous%20driving%20scenes.%20For%20complex%20scenes%20with%20moving%0Aobjects%2C%20we%20first%20sequentially%20and%20progressively%20model%20the%20static%20background%20of%0Athe%20entire%20scene%20with%20incremental%20static%203D%20Gaussians.%20We%20then%20leverage%20a%0Acomposite%20dynamic%20Gaussian%20graph%20to%20handle%20multiple%20moving%20objects%2C%0Aindividually%20reconstructing%20each%20object%20and%20restoring%20their%20accurate%20positions%0Aand%20occlusion%20relationships%20within%20the%20scene.%20We%20further%20use%20a%20LiDAR%20prior%20for%0AGaussian%20Splatting%20to%20reconstruct%20scenes%20with%20greater%20details%20and%20maintain%0Apanoramic%20consistency.%20DrivingGaussian%20outperforms%20existing%20methods%20in%20driving%0Ascene%20reconstruction%20and%20enables%20photorealistic%20surround-view%20synthesis%20with%0Ahigh-fidelity%20and%20multi-camera%20consistency.%20Our%20project%20page%20is%20at%3A%0Ahttps%3A//github.com/VDIGPKU/DrivingGaussian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07920v2&entry.124074799=Read"},
{"title": "A Large-scale Evaluation of Pretraining Paradigms for the Detection of\n  Defects in Electroluminescence Solar Cell Images", "author": "David Torpey and Lawrence Pratt and Richard Klein", "abstract": "  Pretraining has been shown to improve performance in many domains, including\nsemantic segmentation, especially in domains with limited labelled data. In\nthis work, we perform a large-scale evaluation and benchmarking of various\npretraining methods for Solar Cell Defect Detection (SCDD) in\nelectroluminescence images, a field with limited labelled datasets. We cover\nsupervised training with semantic segmentation, semi-supervised learning, and\ntwo self-supervised techniques. We also experiment with both in-distribution\nand out-of-distribution (OOD) pretraining and observe how this affects\ndownstream performance. The results suggest that supervised training on a large\nOOD dataset (COCO), self-supervised pretraining on a large OOD dataset\n(ImageNet), and semi-supervised pretraining (CCT) all yield statistically\nequivalent performance for mean Intersection over Union (mIoU). We achieve a\nnew state-of-the-art for SCDD and demonstrate that certain pretraining schemes\nresult in superior performance on underrepresented classes. Additionally, we\nprovide a large-scale unlabelled EL image dataset of $22000$ images, and a\n$642$-image labelled semantic segmentation EL dataset, for further research in\ndeveloping self- and semi-supervised training techniques in this domain.\n", "link": "http://arxiv.org/abs/2402.17611v1", "date": "2024-02-27", "relevancy": 2.0662, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5573}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5041}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4809}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-scale%20Evaluation%20of%20Pretraining%20Paradigms%20for%20the%20Detection%20of%0A%20%20Defects%20in%20Electroluminescence%20Solar%20Cell%20Images&entry.906535625=David%20Torpey%20and%20Lawrence%20Pratt%20and%20Richard%20Klein&entry.1292438233=%20%20Pretraining%20has%20been%20shown%20to%20improve%20performance%20in%20many%20domains%2C%20including%0Asemantic%20segmentation%2C%20especially%20in%20domains%20with%20limited%20labelled%20data.%20In%0Athis%20work%2C%20we%20perform%20a%20large-scale%20evaluation%20and%20benchmarking%20of%20various%0Apretraining%20methods%20for%20Solar%20Cell%20Defect%20Detection%20%28SCDD%29%20in%0Aelectroluminescence%20images%2C%20a%20field%20with%20limited%20labelled%20datasets.%20We%20cover%0Asupervised%20training%20with%20semantic%20segmentation%2C%20semi-supervised%20learning%2C%20and%0Atwo%20self-supervised%20techniques.%20We%20also%20experiment%20with%20both%20in-distribution%0Aand%20out-of-distribution%20%28OOD%29%20pretraining%20and%20observe%20how%20this%20affects%0Adownstream%20performance.%20The%20results%20suggest%20that%20supervised%20training%20on%20a%20large%0AOOD%20dataset%20%28COCO%29%2C%20self-supervised%20pretraining%20on%20a%20large%20OOD%20dataset%0A%28ImageNet%29%2C%20and%20semi-supervised%20pretraining%20%28CCT%29%20all%20yield%20statistically%0Aequivalent%20performance%20for%20mean%20Intersection%20over%20Union%20%28mIoU%29.%20We%20achieve%20a%0Anew%20state-of-the-art%20for%20SCDD%20and%20demonstrate%20that%20certain%20pretraining%20schemes%0Aresult%20in%20superior%20performance%20on%20underrepresented%20classes.%20Additionally%2C%20we%0Aprovide%20a%20large-scale%20unlabelled%20EL%20image%20dataset%20of%20%2422000%24%20images%2C%20and%20a%0A%24642%24-image%20labelled%20semantic%20segmentation%20EL%20dataset%2C%20for%20further%20research%20in%0Adeveloping%20self-%20and%20semi-supervised%20training%20techniques%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17611v1&entry.124074799=Read"},
{"title": "Overcoming Dimensional Collapse in Self-supervised Contrastive Learning\n  for Medical Image Segmentation", "author": "Jamshid Hassanpour and Vinkle Srivastav and Didier Mutter and Nicolas Padoy", "abstract": "  Self-supervised learning (SSL) approaches have achieved great success when\nthe amount of labeled data is limited. Within SSL, models learn robust feature\nrepresentations by solving pretext tasks. One such pretext task is contrastive\nlearning, which involves forming pairs of similar and dissimilar input samples,\nguiding the model to distinguish between them. In this work, we investigate the\napplication of contrastive learning to the domain of medical image analysis.\nOur findings reveal that MoCo v2, a state-of-the-art contrastive learning\nmethod, encounters dimensional collapse when applied to medical images. This is\nattributed to the high degree of inter-image similarity shared between the\nmedical images. To address this, we propose two key contributions: local\nfeature learning and feature decorrelation. Local feature learning improves the\nability of the model to focus on the local regions of the image, while feature\ndecorrelation removes the linear dependence among the features. Our\nexperimental findings demonstrate that our contributions significantly enhance\nthe model's performance in the downstream task of medical segmentation, both in\nthe linear evaluation and full fine-tuning settings. This work illustrates the\nimportance of effectively adapting SSL techniques to the characteristics of\nmedical imaging tasks. The source code will be made publicly available at:\nhttps://github.com/CAMMA-public/med-moco\n", "link": "http://arxiv.org/abs/2402.14611v2", "date": "2024-02-27", "relevancy": 2.0583, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5311}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5068}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5012}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Dimensional%20Collapse%20in%20Self-supervised%20Contrastive%20Learning%0A%20%20for%20Medical%20Image%20Segmentation&entry.906535625=Jamshid%20Hassanpour%20and%20Vinkle%20Srivastav%20and%20Didier%20Mutter%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20approaches%20have%20achieved%20great%20success%20when%0Athe%20amount%20of%20labeled%20data%20is%20limited.%20Within%20SSL%2C%20models%20learn%20robust%20feature%0Arepresentations%20by%20solving%20pretext%20tasks.%20One%20such%20pretext%20task%20is%20contrastive%0Alearning%2C%20which%20involves%20forming%20pairs%20of%20similar%20and%20dissimilar%20input%20samples%2C%0Aguiding%20the%20model%20to%20distinguish%20between%20them.%20In%20this%20work%2C%20we%20investigate%20the%0Aapplication%20of%20contrastive%20learning%20to%20the%20domain%20of%20medical%20image%20analysis.%0AOur%20findings%20reveal%20that%20MoCo%20v2%2C%20a%20state-of-the-art%20contrastive%20learning%0Amethod%2C%20encounters%20dimensional%20collapse%20when%20applied%20to%20medical%20images.%20This%20is%0Aattributed%20to%20the%20high%20degree%20of%20inter-image%20similarity%20shared%20between%20the%0Amedical%20images.%20To%20address%20this%2C%20we%20propose%20two%20key%20contributions%3A%20local%0Afeature%20learning%20and%20feature%20decorrelation.%20Local%20feature%20learning%20improves%20the%0Aability%20of%20the%20model%20to%20focus%20on%20the%20local%20regions%20of%20the%20image%2C%20while%20feature%0Adecorrelation%20removes%20the%20linear%20dependence%20among%20the%20features.%20Our%0Aexperimental%20findings%20demonstrate%20that%20our%20contributions%20significantly%20enhance%0Athe%20model%27s%20performance%20in%20the%20downstream%20task%20of%20medical%20segmentation%2C%20both%20in%0Athe%20linear%20evaluation%20and%20full%20fine-tuning%20settings.%20This%20work%20illustrates%20the%0Aimportance%20of%20effectively%20adapting%20SSL%20techniques%20to%20the%20characteristics%20of%0Amedical%20imaging%20tasks.%20The%20source%20code%20will%20be%20made%20publicly%20available%20at%3A%0Ahttps%3A//github.com/CAMMA-public/med-moco%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14611v2&entry.124074799=Read"},
{"title": "CustomSketching: Sketch Concept Extraction for Sketch-based Image\n  Synthesis and Editing", "author": "Chufeng Xiao and Hongbo Fu", "abstract": "  Personalization techniques for large text-to-image (T2I) models allow users\nto incorporate new concepts from reference images. However, existing methods\nprimarily rely on textual descriptions, leading to limited control over\ncustomized images and failing to support fine-grained and local editing (e.g.,\nshape, pose, and details). In this paper, we identify sketches as an intuitive\nand versatile representation that can facilitate such control, e.g., contour\nlines capturing shape information and flow lines representing texture. This\nmotivates us to explore a novel task of sketch concept extraction: given one or\nmore sketch-image pairs, we aim to extract a special sketch concept that\nbridges the correspondence between the images and sketches, thus enabling\nsketch-based image synthesis and editing at a fine-grained level. To accomplish\nthis, we introduce CustomSketching, a two-stage framework for extracting novel\nsketch concepts. Considering that an object can often be depicted by a contour\nfor general shapes and additional strokes for internal details, we introduce a\ndual-sketch representation to reduce the inherent ambiguity in sketch\ndepiction. We employ a shape loss and a regularization loss to balance fidelity\nand editability during optimization. Through extensive experiments, a user\nstudy, and several applications, we show our method is effective and superior\nto the adapted baselines.\n", "link": "http://arxiv.org/abs/2402.17624v1", "date": "2024-02-27", "relevancy": 2.0557, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5722}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5225}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4821}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CustomSketching%3A%20Sketch%20Concept%20Extraction%20for%20Sketch-based%20Image%0A%20%20Synthesis%20and%20Editing&entry.906535625=Chufeng%20Xiao%20and%20Hongbo%20Fu&entry.1292438233=%20%20Personalization%20techniques%20for%20large%20text-to-image%20%28T2I%29%20models%20allow%20users%0Ato%20incorporate%20new%20concepts%20from%20reference%20images.%20However%2C%20existing%20methods%0Aprimarily%20rely%20on%20textual%20descriptions%2C%20leading%20to%20limited%20control%20over%0Acustomized%20images%20and%20failing%20to%20support%20fine-grained%20and%20local%20editing%20%28e.g.%2C%0Ashape%2C%20pose%2C%20and%20details%29.%20In%20this%20paper%2C%20we%20identify%20sketches%20as%20an%20intuitive%0Aand%20versatile%20representation%20that%20can%20facilitate%20such%20control%2C%20e.g.%2C%20contour%0Alines%20capturing%20shape%20information%20and%20flow%20lines%20representing%20texture.%20This%0Amotivates%20us%20to%20explore%20a%20novel%20task%20of%20sketch%20concept%20extraction%3A%20given%20one%20or%0Amore%20sketch-image%20pairs%2C%20we%20aim%20to%20extract%20a%20special%20sketch%20concept%20that%0Abridges%20the%20correspondence%20between%20the%20images%20and%20sketches%2C%20thus%20enabling%0Asketch-based%20image%20synthesis%20and%20editing%20at%20a%20fine-grained%20level.%20To%20accomplish%0Athis%2C%20we%20introduce%20CustomSketching%2C%20a%20two-stage%20framework%20for%20extracting%20novel%0Asketch%20concepts.%20Considering%20that%20an%20object%20can%20often%20be%20depicted%20by%20a%20contour%0Afor%20general%20shapes%20and%20additional%20strokes%20for%20internal%20details%2C%20we%20introduce%20a%0Adual-sketch%20representation%20to%20reduce%20the%20inherent%20ambiguity%20in%20sketch%0Adepiction.%20We%20employ%20a%20shape%20loss%20and%20a%20regularization%20loss%20to%20balance%20fidelity%0Aand%20editability%20during%20optimization.%20Through%20extensive%20experiments%2C%20a%20user%0Astudy%2C%20and%20several%20applications%2C%20we%20show%20our%20method%20is%20effective%20and%20superior%0Ato%20the%20adapted%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17624v1&entry.124074799=Read"},
{"title": "Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a\n  Bayesian Neural Network", "author": "Rebecca S Stone and Nishant Ravikumar and Andrew J Bulpitt and David C Hogg", "abstract": "  The fairness of a deep neural network is strongly affected by dataset bias\nand spurious correlations, both of which are usually present in modern\nfeature-rich and complex visual datasets. Due to the difficulty and variability\nof the task, no single de-biasing method has been universally successful. In\nparticular, implicit methods not requiring explicit knowledge of bias variables\nare especially relevant for real-world applications. We propose a novel\nimplicit mitigation method using a Bayesian neural network, allowing us to\nleverage the relationship between epistemic uncertainties and the presence of\nbias or spurious correlations in a sample. Our proposed posterior estimate\nsharpening procedure encourages the network to focus on core features that do\nnot contribute to high uncertainties. Experimental results on three benchmark\ndatasets demonstrate that Bayesian networks with sharpened posterior estimates\nperform comparably to prior existing methods and show potential worthy of\nfurther exploration.\n", "link": "http://arxiv.org/abs/2303.16564v3", "date": "2024-02-27", "relevancy": 2.0551, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4943}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Visual%20Bias%20Mitigation%20by%20Posterior%20Estimate%20Sharpening%20of%20a%0A%20%20Bayesian%20Neural%20Network&entry.906535625=Rebecca%20S%20Stone%20and%20Nishant%20Ravikumar%20and%20Andrew%20J%20Bulpitt%20and%20David%20C%20Hogg&entry.1292438233=%20%20The%20fairness%20of%20a%20deep%20neural%20network%20is%20strongly%20affected%20by%20dataset%20bias%0Aand%20spurious%20correlations%2C%20both%20of%20which%20are%20usually%20present%20in%20modern%0Afeature-rich%20and%20complex%20visual%20datasets.%20Due%20to%20the%20difficulty%20and%20variability%0Aof%20the%20task%2C%20no%20single%20de-biasing%20method%20has%20been%20universally%20successful.%20In%0Aparticular%2C%20implicit%20methods%20not%20requiring%20explicit%20knowledge%20of%20bias%20variables%0Aare%20especially%20relevant%20for%20real-world%20applications.%20We%20propose%20a%20novel%0Aimplicit%20mitigation%20method%20using%20a%20Bayesian%20neural%20network%2C%20allowing%20us%20to%0Aleverage%20the%20relationship%20between%20epistemic%20uncertainties%20and%20the%20presence%20of%0Abias%20or%20spurious%20correlations%20in%20a%20sample.%20Our%20proposed%20posterior%20estimate%0Asharpening%20procedure%20encourages%20the%20network%20to%20focus%20on%20core%20features%20that%20do%0Anot%20contribute%20to%20high%20uncertainties.%20Experimental%20results%20on%20three%20benchmark%0Adatasets%20demonstrate%20that%20Bayesian%20networks%20with%20sharpened%20posterior%20estimates%0Aperform%20comparably%20to%20prior%20existing%20methods%20and%20show%20potential%20worthy%20of%0Afurther%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.16564v3&entry.124074799=Read"},
{"title": "Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using\n  Contrastive Learning and Geometric Unfolding", "author": "Alexander Oberstrass and Jordan DeKraker and Nicola Palomero-Gallagher and Sascha E. A. Muenzing and Alan C. Evans and Markus Axer and Katrin Amunts and Timo Dickscheid", "abstract": "  Understanding the cortical organization of the human brain requires\ninterpretable descriptors for distinct structural and functional imaging data.\n3D polarized light imaging (3D-PLI) is an imaging modality for visualizing\nfiber architecture in postmortem brains with high resolution that also captures\nthe presence of cell bodies, for example, to identify hippocampal subfields.\nThe rich texture in 3D-PLI images, however, makes this modality particularly\ndifficult to analyze and best practices for characterizing architectonic\npatterns still need to be established. In this work, we demonstrate a novel\nmethod to analyze the regional organization of the human hippocampus in 3D-PLI\nby combining recent advances in unfolding methods with deep texture features\nobtained using a self-supervised contrastive learning approach. We identify\nclusters in the representations that correspond well with classical\ndescriptions of hippocampal subfields, lending validity to the developed\nmethodology.\n", "link": "http://arxiv.org/abs/2402.17744v1", "date": "2024-02-27", "relevancy": 2.0513, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5362}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5046}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4749}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Regional%20Organization%20of%20the%20Human%20Hippocampus%20in%203D-PLI%20Using%0A%20%20Contrastive%20Learning%20and%20Geometric%20Unfolding&entry.906535625=Alexander%20Oberstrass%20and%20Jordan%20DeKraker%20and%20Nicola%20Palomero-Gallagher%20and%20Sascha%20E.%20A.%20Muenzing%20and%20Alan%20C.%20Evans%20and%20Markus%20Axer%20and%20Katrin%20Amunts%20and%20Timo%20Dickscheid&entry.1292438233=%20%20Understanding%20the%20cortical%20organization%20of%20the%20human%20brain%20requires%0Ainterpretable%20descriptors%20for%20distinct%20structural%20and%20functional%20imaging%20data.%0A3D%20polarized%20light%20imaging%20%283D-PLI%29%20is%20an%20imaging%20modality%20for%20visualizing%0Afiber%20architecture%20in%20postmortem%20brains%20with%20high%20resolution%20that%20also%20captures%0Athe%20presence%20of%20cell%20bodies%2C%20for%20example%2C%20to%20identify%20hippocampal%20subfields.%0AThe%20rich%20texture%20in%203D-PLI%20images%2C%20however%2C%20makes%20this%20modality%20particularly%0Adifficult%20to%20analyze%20and%20best%20practices%20for%20characterizing%20architectonic%0Apatterns%20still%20need%20to%20be%20established.%20In%20this%20work%2C%20we%20demonstrate%20a%20novel%0Amethod%20to%20analyze%20the%20regional%20organization%20of%20the%20human%20hippocampus%20in%203D-PLI%0Aby%20combining%20recent%20advances%20in%20unfolding%20methods%20with%20deep%20texture%20features%0Aobtained%20using%20a%20self-supervised%20contrastive%20learning%20approach.%20We%20identify%0Aclusters%20in%20the%20representations%20that%20correspond%20well%20with%20classical%0Adescriptions%20of%20hippocampal%20subfields%2C%20lending%20validity%20to%20the%20developed%0Amethodology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17744v1&entry.124074799=Read"},
{"title": "CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise\n  Sketch Instance Guided Attention", "author": "Mohammad Sadil Khan and Elona Dupont and Sk Aziz Ali and Kseniya Cherenkova and Anis Kacem and Djamila Aouada", "abstract": "  Reverse engineering in the realm of Computer-Aided Design (CAD) has been a\nlongstanding aspiration, though not yet entirely realized. Its primary aim is\nto uncover the CAD process behind a physical object given its 3D scan. We\npropose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to\nrecover the design history of a CAD model represented as a sequence of\nsketch-and-extrusion from an input point cloud. Our model learns\nvisual-language representations by layer-wise cross-attention between point\ncloud and CAD language embedding. In particular, a new Sketch instance Guided\nAttention (SGA) module is proposed in order to reconstruct the fine-grained\ndetails of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not\nonly reconstructs a unique full design history of the corresponding CAD model\ngiven an input point cloud but also provides multiple plausible design choices.\nThis allows for an interactive reverse engineering scenario by providing\ndesigners with multiple next-step choices along with the design process.\nExtensive experiments on publicly available CAD datasets showcase the\neffectiveness of our approach against existing baseline models in two settings,\nnamely, full design history recovery and conditional auto-completion from point\nclouds.\n", "link": "http://arxiv.org/abs/2402.17678v1", "date": "2024-02-27", "relevancy": 2.0508, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5353}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5027}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4942}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-SIGNet%3A%20CAD%20Language%20Inference%20from%20Point%20Clouds%20using%20Layer-wise%0A%20%20Sketch%20Instance%20Guided%20Attention&entry.906535625=Mohammad%20Sadil%20Khan%20and%20Elona%20Dupont%20and%20Sk%20Aziz%20Ali%20and%20Kseniya%20Cherenkova%20and%20Anis%20Kacem%20and%20Djamila%20Aouada&entry.1292438233=%20%20Reverse%20engineering%20in%20the%20realm%20of%20Computer-Aided%20Design%20%28CAD%29%20has%20been%20a%0Alongstanding%20aspiration%2C%20though%20not%20yet%20entirely%20realized.%20Its%20primary%20aim%20is%0Ato%20uncover%20the%20CAD%20process%20behind%20a%20physical%20object%20given%20its%203D%20scan.%20We%0Apropose%20CAD-SIGNet%2C%20an%20end-to-end%20trainable%20and%20auto-regressive%20architecture%20to%0Arecover%20the%20design%20history%20of%20a%20CAD%20model%20represented%20as%20a%20sequence%20of%0Asketch-and-extrusion%20from%20an%20input%20point%20cloud.%20Our%20model%20learns%0Avisual-language%20representations%20by%20layer-wise%20cross-attention%20between%20point%0Acloud%20and%20CAD%20language%20embedding.%20In%20particular%2C%20a%20new%20Sketch%20instance%20Guided%0AAttention%20%28SGA%29%20module%20is%20proposed%20in%20order%20to%20reconstruct%20the%20fine-grained%0Adetails%20of%20the%20sketches.%20Thanks%20to%20its%20auto-regressive%20nature%2C%20CAD-SIGNet%20not%0Aonly%20reconstructs%20a%20unique%20full%20design%20history%20of%20the%20corresponding%20CAD%20model%0Agiven%20an%20input%20point%20cloud%20but%20also%20provides%20multiple%20plausible%20design%20choices.%0AThis%20allows%20for%20an%20interactive%20reverse%20engineering%20scenario%20by%20providing%0Adesigners%20with%20multiple%20next-step%20choices%20along%20with%20the%20design%20process.%0AExtensive%20experiments%20on%20publicly%20available%20CAD%20datasets%20showcase%20the%0Aeffectiveness%20of%20our%20approach%20against%20existing%20baseline%20models%20in%20two%20settings%2C%0Anamely%2C%20full%20design%20history%20recovery%20and%20conditional%20auto-completion%20from%20point%0Aclouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17678v1&entry.124074799=Read"},
{"title": "Towards Fairness-Aware Adversarial Learning", "author": "Yanghao Zhang and Tianle Zhang and Ronghui Mu and Xiaowei Huang and Wenjie Ruan", "abstract": "  Although adversarial training (AT) has proven effective in enhancing the\nmodel's robustness, the recently revealed issue of fairness in robustness has\nnot been well addressed, i.e. the robust accuracy varies significantly among\ndifferent categories. In this paper, instead of uniformly evaluating the\nmodel's average class performance, we delve into the issue of robust fairness,\nby considering the worst-case distribution across various classes. We propose a\nnovel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a\ngeneralization of conventional AT, we re-define the problem of adversarial\ntraining as a min-max-max framework, to ensure both robustness and fairness of\nthe trained model. Specifically, by taking advantage of distributional robust\noptimization, our method aims to find the worst distribution among different\ncategories, and the solution is guaranteed to obtain the upper bound\nperformance with high probability. In particular, FAAL can fine-tune an unfair\nrobust model to be fair within only two epochs, without compromising the\noverall clean and robust accuracies. Extensive experiments on various image\ndatasets validate the superior performance and efficiency of the proposed FAAL\ncompared to other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2402.17729v1", "date": "2024-02-27", "relevancy": 2.0476, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5377}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5013}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.474}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fairness-Aware%20Adversarial%20Learning&entry.906535625=Yanghao%20Zhang%20and%20Tianle%20Zhang%20and%20Ronghui%20Mu%20and%20Xiaowei%20Huang%20and%20Wenjie%20Ruan&entry.1292438233=%20%20Although%20adversarial%20training%20%28AT%29%20has%20proven%20effective%20in%20enhancing%20the%0Amodel%27s%20robustness%2C%20the%20recently%20revealed%20issue%20of%20fairness%20in%20robustness%20has%0Anot%20been%20well%20addressed%2C%20i.e.%20the%20robust%20accuracy%20varies%20significantly%20among%0Adifferent%20categories.%20In%20this%20paper%2C%20instead%20of%20uniformly%20evaluating%20the%0Amodel%27s%20average%20class%20performance%2C%20we%20delve%20into%20the%20issue%20of%20robust%20fairness%2C%0Aby%20considering%20the%20worst-case%20distribution%20across%20various%20classes.%20We%20propose%20a%0Anovel%20learning%20paradigm%2C%20named%20Fairness-Aware%20Adversarial%20Learning%20%28FAAL%29.%20As%20a%0Ageneralization%20of%20conventional%20AT%2C%20we%20re-define%20the%20problem%20of%20adversarial%0Atraining%20as%20a%20min-max-max%20framework%2C%20to%20ensure%20both%20robustness%20and%20fairness%20of%0Athe%20trained%20model.%20Specifically%2C%20by%20taking%20advantage%20of%20distributional%20robust%0Aoptimization%2C%20our%20method%20aims%20to%20find%20the%20worst%20distribution%20among%20different%0Acategories%2C%20and%20the%20solution%20is%20guaranteed%20to%20obtain%20the%20upper%20bound%0Aperformance%20with%20high%20probability.%20In%20particular%2C%20FAAL%20can%20fine-tune%20an%20unfair%0Arobust%20model%20to%20be%20fair%20within%20only%20two%20epochs%2C%20without%20compromising%20the%0Aoverall%20clean%20and%20robust%20accuracies.%20Extensive%20experiments%20on%20various%20image%0Adatasets%20validate%20the%20superior%20performance%20and%20efficiency%20of%20the%20proposed%20FAAL%0Acompared%20to%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17729v1&entry.124074799=Read"},
{"title": "Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for\n  Robust 3D Medical Image Segmentation", "author": "Jingjie Guo and Weitong Zhang and Matthew Sinclair and Daniel Rueckert and Chen Chen", "abstract": "  Convolutional neural networks (CNNs) often suffer from poor performance when\ntested on target data that differs from the training (source) data\ndistribution, particularly in medical imaging applications where variations in\nimaging protocols across different clinical sites and scanners lead to\ndifferent imaging appearances. However, re-accessing source training data for\nunsupervised domain adaptation or labeling additional test data for model\nfine-tuning can be difficult due to privacy issues and high labeling costs,\nrespectively. To solve this problem, we propose a novel atlas-guided test-time\nadaptation (TTA) method for robust 3D medical image segmentation, called\nAdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and\nadapts the segmentation network by minimizing an atlas-based loss.\nSpecifically, the network is adapted so that its prediction after registration\nis aligned with the learned atlas in the atlas space, which helps to reduce\nanatomical segmentation errors at test time. In addition, different from most\nexisting TTA methods which restrict the adaptation to batch normalization\nblocks in the segmentation network only, we further exploit the use of channel\nand spatial attention blocks for improved adaptability at test time. Extensive\nexperiments on multiple datasets from different sites show that AdaAtlas with\nattention blocks adapted (AdaAtlas-Attention) achieves superior performance\nimprovements, greatly outperforming other competitive TTA methods.\n", "link": "http://arxiv.org/abs/2307.00676v2", "date": "2024-02-27", "relevancy": 2.0467, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5176}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5133}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5051}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pay%20Attention%20to%20the%20Atlas%3A%20Atlas-Guided%20Test-Time%20Adaptation%20Method%20for%0A%20%20Robust%203D%20Medical%20Image%20Segmentation&entry.906535625=Jingjie%20Guo%20and%20Weitong%20Zhang%20and%20Matthew%20Sinclair%20and%20Daniel%20Rueckert%20and%20Chen%20Chen&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20often%20suffer%20from%20poor%20performance%20when%0Atested%20on%20target%20data%20that%20differs%20from%20the%20training%20%28source%29%20data%0Adistribution%2C%20particularly%20in%20medical%20imaging%20applications%20where%20variations%20in%0Aimaging%20protocols%20across%20different%20clinical%20sites%20and%20scanners%20lead%20to%0Adifferent%20imaging%20appearances.%20However%2C%20re-accessing%20source%20training%20data%20for%0Aunsupervised%20domain%20adaptation%20or%20labeling%20additional%20test%20data%20for%20model%0Afine-tuning%20can%20be%20difficult%20due%20to%20privacy%20issues%20and%20high%20labeling%20costs%2C%0Arespectively.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20novel%20atlas-guided%20test-time%0Aadaptation%20%28TTA%29%20method%20for%20robust%203D%20medical%20image%20segmentation%2C%20called%0AAdaAtlas.%20AdaAtlas%20only%20takes%20one%20single%20unlabeled%20test%20sample%20as%20input%20and%0Aadapts%20the%20segmentation%20network%20by%20minimizing%20an%20atlas-based%20loss.%0ASpecifically%2C%20the%20network%20is%20adapted%20so%20that%20its%20prediction%20after%20registration%0Ais%20aligned%20with%20the%20learned%20atlas%20in%20the%20atlas%20space%2C%20which%20helps%20to%20reduce%0Aanatomical%20segmentation%20errors%20at%20test%20time.%20In%20addition%2C%20different%20from%20most%0Aexisting%20TTA%20methods%20which%20restrict%20the%20adaptation%20to%20batch%20normalization%0Ablocks%20in%20the%20segmentation%20network%20only%2C%20we%20further%20exploit%20the%20use%20of%20channel%0Aand%20spatial%20attention%20blocks%20for%20improved%20adaptability%20at%20test%20time.%20Extensive%0Aexperiments%20on%20multiple%20datasets%20from%20different%20sites%20show%20that%20AdaAtlas%20with%0Aattention%20blocks%20adapted%20%28AdaAtlas-Attention%29%20achieves%20superior%20performance%0Aimprovements%2C%20greatly%20outperforming%20other%20competitive%20TTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.00676v2&entry.124074799=Read"},
{"title": "Emergency Caching: Coded Caching-based Reliable Map Transmission in\n  Emergency Networks", "author": "Zeyu Tian and Lianming Xu and Liang Li and Li Wang and Aiguo Fei", "abstract": "  Many rescue missions demand effective perception and real-time decision\nmaking, which highly rely on effective data collection and processing. In this\nstudy, we propose a three-layer architecture of emergency caching networks\nfocusing on data collection and reliable transmission, by leveraging efficient\nperception and edge caching technologies. Based on this architecture, we\npropose a disaster map collection framework that integrates coded caching\ntechnologies. Our framework strategically caches coded fragments of maps across\nunmanned aerial vehicles (UAVs), fostering collaborative uploading for\naugmented transmission reliability. Additionally, we establish a comprehensive\nprobability model to assess the effective recovery area of disaster maps.\nTowards the goal of utility maximization, we propose a deep reinforcement\nlearning (DRL) based algorithm that jointly makes decisions about cooperative\nUAVs selection, bandwidth allocation and coded caching parameter adjustment,\naccommodating the real-time map updates in a dynamic disaster situation. Our\nproposed scheme is more effective than the non-coding caching scheme, as\nvalidated by simulation.\n", "link": "http://arxiv.org/abs/2402.17550v1", "date": "2024-02-27", "relevancy": 2.0419, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5571}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4816}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4662}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergency%20Caching%3A%20Coded%20Caching-based%20Reliable%20Map%20Transmission%20in%0A%20%20Emergency%20Networks&entry.906535625=Zeyu%20Tian%20and%20Lianming%20Xu%20and%20Liang%20Li%20and%20Li%20Wang%20and%20Aiguo%20Fei&entry.1292438233=%20%20Many%20rescue%20missions%20demand%20effective%20perception%20and%20real-time%20decision%0Amaking%2C%20which%20highly%20rely%20on%20effective%20data%20collection%20and%20processing.%20In%20this%0Astudy%2C%20we%20propose%20a%20three-layer%20architecture%20of%20emergency%20caching%20networks%0Afocusing%20on%20data%20collection%20and%20reliable%20transmission%2C%20by%20leveraging%20efficient%0Aperception%20and%20edge%20caching%20technologies.%20Based%20on%20this%20architecture%2C%20we%0Apropose%20a%20disaster%20map%20collection%20framework%20that%20integrates%20coded%20caching%0Atechnologies.%20Our%20framework%20strategically%20caches%20coded%20fragments%20of%20maps%20across%0Aunmanned%20aerial%20vehicles%20%28UAVs%29%2C%20fostering%20collaborative%20uploading%20for%0Aaugmented%20transmission%20reliability.%20Additionally%2C%20we%20establish%20a%20comprehensive%0Aprobability%20model%20to%20assess%20the%20effective%20recovery%20area%20of%20disaster%20maps.%0ATowards%20the%20goal%20of%20utility%20maximization%2C%20we%20propose%20a%20deep%20reinforcement%0Alearning%20%28DRL%29%20based%20algorithm%20that%20jointly%20makes%20decisions%20about%20cooperative%0AUAVs%20selection%2C%20bandwidth%20allocation%20and%20coded%20caching%20parameter%20adjustment%2C%0Aaccommodating%20the%20real-time%20map%20updates%20in%20a%20dynamic%20disaster%20situation.%20Our%0Aproposed%20scheme%20is%20more%20effective%20than%20the%20non-coding%20caching%20scheme%2C%20as%0Avalidated%20by%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17550v1&entry.124074799=Read"},
{"title": "Language Agents as Optimizable Graphs", "author": "Mingchen Zhuge and Wenyi Wang and Louis Kirsch and Francesco Faccio and Dmitrii Khizbullin and J\u00fcrgen Schmidhuber", "abstract": "  Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm.\n", "link": "http://arxiv.org/abs/2402.16823v2", "date": "2024-02-27", "relevancy": 2.0332, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.534}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5037}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5026}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Agents%20as%20Optimizable%20Graphs&entry.906535625=Mingchen%20Zhuge%20and%20Wenyi%20Wang%20and%20Louis%20Kirsch%20and%20Francesco%20Faccio%20and%20Dmitrii%20Khizbullin%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Various%20human-designed%20prompt%20engineering%20techniques%20have%20been%20proposed%20to%0Aimprove%20problem%20solvers%20based%20on%20Large%20Language%20Models%20%28LLMs%29%2C%20yielding%20many%0Adisparate%20code%20bases.%20We%20unify%20these%20approaches%20by%20describing%20LLM-based%20agents%0Aas%20computational%20graphs.%20The%20nodes%20implement%20functions%20to%20process%20multimodal%0Adata%20or%20query%20LLMs%2C%20and%20the%20edges%20describe%20the%20information%20flow%20between%0Aoperations.%20Graphs%20can%20be%20recursively%20combined%20into%20larger%20composite%20graphs%0Arepresenting%20hierarchies%20of%20inter-agent%20collaboration%20%28where%20edges%20connect%0Aoperations%20of%20different%20agents%29.%20Our%20novel%20automatic%20graph%20optimizers%20%281%29%0Arefine%20node-level%20LLM%20prompts%20%28node%20optimization%29%20and%20%282%29%20improve%20agent%0Aorchestration%20by%20changing%20graph%20connectivity%20%28edge%20optimization%29.%20Experiments%0Ademonstrate%20that%20our%20framework%20can%20be%20used%20to%20efficiently%20develop%2C%20integrate%2C%0Aand%20automatically%20improve%20various%20LLM%20agents.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/metauto-ai/gptswarm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16823v2&entry.124074799=Read"},
{"title": "Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised\n  Semantic Segmentation with Its Class Label", "author": "Xinliang Zhang and Lei Zhu and Hangzhou He and Lujia Jin and Yanye Lu", "abstract": "  Scribble-based weakly-supervised semantic segmentation using sparse scribble\nsupervision is gaining traction as it reduces annotation costs when compared to\nfully annotated alternatives. Existing methods primarily generate pseudo-labels\nby diffusing labeled pixels to unlabeled ones with local cues for supervision.\nHowever, this diffusion process fails to exploit global semantics and\nclass-specific cues, which are important for semantic segmentation. In this\nstudy, we propose a class-driven scribble promotion network, which utilizes\nboth scribble annotations and pseudo-labels informed by image-level classes and\nglobal semantics for supervision. Directly adopting pseudo-labels might\nmisguide the segmentation model, thus we design a localization rectification\nmodule to correct foreground representations in the feature space. To further\ncombine the advantages of both supervisions, we also introduce a distance\nentropy loss for uncertainty reduction, which adapts per-pixel confidence\nweights according to the reliable region determined by the scribble and\npseudo-label's boundary. Experiments on the ScribbleSup dataset with different\nqualities of scribble annotations outperform all the previous methods,\ndemonstrating the superiority and robustness of our method.The code is\navailable at\nhttps://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.\n", "link": "http://arxiv.org/abs/2402.17555v1", "date": "2024-02-27", "relevancy": 2.0305, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5208}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5033}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4855}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scribble%20Hides%20Class%3A%20Promoting%20Scribble-Based%20Weakly-Supervised%0A%20%20Semantic%20Segmentation%20with%20Its%20Class%20Label&entry.906535625=Xinliang%20Zhang%20and%20Lei%20Zhu%20and%20Hangzhou%20He%20and%20Lujia%20Jin%20and%20Yanye%20Lu&entry.1292438233=%20%20Scribble-based%20weakly-supervised%20semantic%20segmentation%20using%20sparse%20scribble%0Asupervision%20is%20gaining%20traction%20as%20it%20reduces%20annotation%20costs%20when%20compared%20to%0Afully%20annotated%20alternatives.%20Existing%20methods%20primarily%20generate%20pseudo-labels%0Aby%20diffusing%20labeled%20pixels%20to%20unlabeled%20ones%20with%20local%20cues%20for%20supervision.%0AHowever%2C%20this%20diffusion%20process%20fails%20to%20exploit%20global%20semantics%20and%0Aclass-specific%20cues%2C%20which%20are%20important%20for%20semantic%20segmentation.%20In%20this%0Astudy%2C%20we%20propose%20a%20class-driven%20scribble%20promotion%20network%2C%20which%20utilizes%0Aboth%20scribble%20annotations%20and%20pseudo-labels%20informed%20by%20image-level%20classes%20and%0Aglobal%20semantics%20for%20supervision.%20Directly%20adopting%20pseudo-labels%20might%0Amisguide%20the%20segmentation%20model%2C%20thus%20we%20design%20a%20localization%20rectification%0Amodule%20to%20correct%20foreground%20representations%20in%20the%20feature%20space.%20To%20further%0Acombine%20the%20advantages%20of%20both%20supervisions%2C%20we%20also%20introduce%20a%20distance%0Aentropy%20loss%20for%20uncertainty%20reduction%2C%20which%20adapts%20per-pixel%20confidence%0Aweights%20according%20to%20the%20reliable%20region%20determined%20by%20the%20scribble%20and%0Apseudo-label%27s%20boundary.%20Experiments%20on%20the%20ScribbleSup%20dataset%20with%20different%0Aqualities%20of%20scribble%20annotations%20outperform%20all%20the%20previous%20methods%2C%0Ademonstrating%20the%20superiority%20and%20robustness%20of%20our%20method.The%20code%20is%0Aavailable%20at%0Ahttps%3A//github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17555v1&entry.124074799=Read"},
{"title": "DS-Agent: Automated Data Science by Empowering Large Language Models\n  with Case-Based Reasoning", "author": "Siyuan Guo and Cheng Deng and Ying Wen and Hechang Chen and Yi Chang and Jun Wang", "abstract": "  In this work, we investigate the potential of large language models (LLMs)\nbased agents to automate data science tasks, with the goal of comprehending\ntask requirements, then building and training the best-fit machine learning\nmodels. Despite their widespread success, existing LLM agents are hindered by\ngenerating unreasonable experiment plans within this scenario. To this end, we\npresent DS-Agent, a novel automatic framework that harnesses LLM agent and\ncase-based reasoning (CBR). In the development stage, DS-Agent follows the CBR\nframework to structure an automatic iteration pipeline, which can flexibly\ncapitalize on the expert knowledge from Kaggle, and facilitate consistent\nperformance improvement through the feedback mechanism. Moreover, DS-Agent\nimplements a low-resource deployment stage with a simplified CBR paradigm to\nadapt past successful solutions from the development stage for direct code\ngeneration, significantly reducing the demand on foundational capabilities of\nLLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success\nrate in the development stage, while attaining 36% improvement on average one\npass rate across alternative LLMs in the deployment stage. In both stages,\nDS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per\nrun with GPT-4, respectively.\n", "link": "http://arxiv.org/abs/2402.17453v1", "date": "2024-02-27", "relevancy": 2.0234, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5177}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5147}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4923}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DS-Agent%3A%20Automated%20Data%20Science%20by%20Empowering%20Large%20Language%20Models%0A%20%20with%20Case-Based%20Reasoning&entry.906535625=Siyuan%20Guo%20and%20Cheng%20Deng%20and%20Ying%20Wen%20and%20Hechang%20Chen%20and%20Yi%20Chang%20and%20Jun%20Wang&entry.1292438233=%20%20In%20this%20work%2C%20we%20investigate%20the%20potential%20of%20large%20language%20models%20%28LLMs%29%0Abased%20agents%20to%20automate%20data%20science%20tasks%2C%20with%20the%20goal%20of%20comprehending%0Atask%20requirements%2C%20then%20building%20and%20training%20the%20best-fit%20machine%20learning%0Amodels.%20Despite%20their%20widespread%20success%2C%20existing%20LLM%20agents%20are%20hindered%20by%0Agenerating%20unreasonable%20experiment%20plans%20within%20this%20scenario.%20To%20this%20end%2C%20we%0Apresent%20DS-Agent%2C%20a%20novel%20automatic%20framework%20that%20harnesses%20LLM%20agent%20and%0Acase-based%20reasoning%20%28CBR%29.%20In%20the%20development%20stage%2C%20DS-Agent%20follows%20the%20CBR%0Aframework%20to%20structure%20an%20automatic%20iteration%20pipeline%2C%20which%20can%20flexibly%0Acapitalize%20on%20the%20expert%20knowledge%20from%20Kaggle%2C%20and%20facilitate%20consistent%0Aperformance%20improvement%20through%20the%20feedback%20mechanism.%20Moreover%2C%20DS-Agent%0Aimplements%20a%20low-resource%20deployment%20stage%20with%20a%20simplified%20CBR%20paradigm%20to%0Aadapt%20past%20successful%20solutions%20from%20the%20development%20stage%20for%20direct%20code%0Ageneration%2C%20significantly%20reducing%20the%20demand%20on%20foundational%20capabilities%20of%0ALLMs.%20Empirically%2C%20DS-Agent%20with%20GPT-4%20achieves%20an%20unprecedented%20100%25%20success%0Arate%20in%20the%20development%20stage%2C%20while%20attaining%2036%25%20improvement%20on%20average%20one%0Apass%20rate%20across%20alternative%20LLMs%20in%20the%20deployment%20stage.%20In%20both%20stages%2C%0ADS-Agent%20achieves%20the%20best%20rank%20in%20performance%2C%20costing%20%5C%241.60%20and%20%5C%240.13%20per%0Arun%20with%20GPT-4%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17453v1&entry.124074799=Read"},
{"title": "Dynamic Neighborhood Construction for Structured Large Discrete Action\n  Spaces", "author": "Fabian Akkerman and Julius Luy and Wouter van Heeswijk and Maximilian Schiffer", "abstract": "  Large discrete action spaces (LDAS) remain a central challenge in\nreinforcement learning. Existing solution approaches can handle unstructured\nLDAS with up to a few million actions. However, many real-world applications in\nlogistics, production, and transportation systems have combinatorial action\nspaces, whose size grows well beyond millions of actions, even on small\ninstances. Fortunately, such action spaces exhibit structure, e.g., equally\nspaced discrete resource units. With this work, we focus on handling structured\nLDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we\npropose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm\nfor SLDAS. We present a scalable neighborhood exploration heuristic that\nutilizes this paradigm and efficiently explores the discrete neighborhood\naround the continuous proxy action in structured action spaces with up to\n$10^{73}$ actions. We demonstrate the performance of our method by benchmarking\nit against three state-of-the-art approaches designed for large discrete action\nspaces across two distinct environments. Our results show that DNC matches or\noutperforms state-of-the-art approaches while being computationally more\nefficient. Furthermore, our method scales to action spaces that so far remained\ncomputationally intractable for existing methodologies.\n", "link": "http://arxiv.org/abs/2305.19891v4", "date": "2024-02-27", "relevancy": 2.0225, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5983}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4917}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4824}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Neighborhood%20Construction%20for%20Structured%20Large%20Discrete%20Action%0A%20%20Spaces&entry.906535625=Fabian%20Akkerman%20and%20Julius%20Luy%20and%20Wouter%20van%20Heeswijk%20and%20Maximilian%20Schiffer&entry.1292438233=%20%20Large%20discrete%20action%20spaces%20%28LDAS%29%20remain%20a%20central%20challenge%20in%0Areinforcement%20learning.%20Existing%20solution%20approaches%20can%20handle%20unstructured%0ALDAS%20with%20up%20to%20a%20few%20million%20actions.%20However%2C%20many%20real-world%20applications%20in%0Alogistics%2C%20production%2C%20and%20transportation%20systems%20have%20combinatorial%20action%0Aspaces%2C%20whose%20size%20grows%20well%20beyond%20millions%20of%20actions%2C%20even%20on%20small%0Ainstances.%20Fortunately%2C%20such%20action%20spaces%20exhibit%20structure%2C%20e.g.%2C%20equally%0Aspaced%20discrete%20resource%20units.%20With%20this%20work%2C%20we%20focus%20on%20handling%20structured%0ALDAS%20%28SLDAS%29%20with%20sizes%20that%20cannot%20be%20handled%20by%20current%20benchmarks%3A%20we%0Apropose%20Dynamic%20Neighborhood%20Construction%20%28DNC%29%2C%20a%20novel%20exploitation%20paradigm%0Afor%20SLDAS.%20We%20present%20a%20scalable%20neighborhood%20exploration%20heuristic%20that%0Autilizes%20this%20paradigm%20and%20efficiently%20explores%20the%20discrete%20neighborhood%0Aaround%20the%20continuous%20proxy%20action%20in%20structured%20action%20spaces%20with%20up%20to%0A%2410%5E%7B73%7D%24%20actions.%20We%20demonstrate%20the%20performance%20of%20our%20method%20by%20benchmarking%0Ait%20against%20three%20state-of-the-art%20approaches%20designed%20for%20large%20discrete%20action%0Aspaces%20across%20two%20distinct%20environments.%20Our%20results%20show%20that%20DNC%20matches%20or%0Aoutperforms%20state-of-the-art%20approaches%20while%20being%20computationally%20more%0Aefficient.%20Furthermore%2C%20our%20method%20scales%20to%20action%20spaces%20that%20so%20far%20remained%0Acomputationally%20intractable%20for%20existing%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.19891v4&entry.124074799=Read"},
{"title": "Robustness-Congruent Adversarial Training for Secure Machine Learning\n  Model Updates", "author": "Daniele Angioni and Luca Demetrio and Maura Pintor and Luca Oneto and Davide Anguita and Battista Biggio and Fabio Roli", "abstract": "  Machine-learning models demand for periodic updates to improve their average\naccuracy, exploiting novel architectures and additional data. However, a\nnewly-updated model may commit mistakes that the previous model did not make.\nSuch misclassifications are referred to as negative flips, and experienced by\nusers as a regression of performance. In this work, we show that this problem\nalso affects robustness to adversarial examples, thereby hindering the\ndevelopment of secure model update practices. In particular, when updating a\nmodel to improve its adversarial robustness, some previously-ineffective\nadversarial examples may become misclassified, causing a regression in the\nperceived security of the system. We propose a novel technique, named\nrobustness-congruent adversarial training, to address this issue. It amounts to\nfine-tuning a model with adversarial training, while constraining it to retain\nhigher robustness on the adversarial examples that were correctly classified\nbefore the update. We show that our algorithm and, more generally, learning\nwith non-regression constraints, provides a theoretically-grounded framework to\ntrain consistent estimators. Our experiments on robust models for computer\nvision confirm that (i) both accuracy and robustness, even if improved after\nmodel update, can be affected by negative flips, and (ii) our\nrobustness-congruent adversarial training can mitigate the problem,\noutperforming competing baseline methods.\n", "link": "http://arxiv.org/abs/2402.17390v1", "date": "2024-02-27", "relevancy": 2.0205, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5167}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5017}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4848}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness-Congruent%20Adversarial%20Training%20for%20Secure%20Machine%20Learning%0A%20%20Model%20Updates&entry.906535625=Daniele%20Angioni%20and%20Luca%20Demetrio%20and%20Maura%20Pintor%20and%20Luca%20Oneto%20and%20Davide%20Anguita%20and%20Battista%20Biggio%20and%20Fabio%20Roli&entry.1292438233=%20%20Machine-learning%20models%20demand%20for%20periodic%20updates%20to%20improve%20their%20average%0Aaccuracy%2C%20exploiting%20novel%20architectures%20and%20additional%20data.%20However%2C%20a%0Anewly-updated%20model%20may%20commit%20mistakes%20that%20the%20previous%20model%20did%20not%20make.%0ASuch%20misclassifications%20are%20referred%20to%20as%20negative%20flips%2C%20and%20experienced%20by%0Ausers%20as%20a%20regression%20of%20performance.%20In%20this%20work%2C%20we%20show%20that%20this%20problem%0Aalso%20affects%20robustness%20to%20adversarial%20examples%2C%20thereby%20hindering%20the%0Adevelopment%20of%20secure%20model%20update%20practices.%20In%20particular%2C%20when%20updating%20a%0Amodel%20to%20improve%20its%20adversarial%20robustness%2C%20some%20previously-ineffective%0Aadversarial%20examples%20may%20become%20misclassified%2C%20causing%20a%20regression%20in%20the%0Aperceived%20security%20of%20the%20system.%20We%20propose%20a%20novel%20technique%2C%20named%0Arobustness-congruent%20adversarial%20training%2C%20to%20address%20this%20issue.%20It%20amounts%20to%0Afine-tuning%20a%20model%20with%20adversarial%20training%2C%20while%20constraining%20it%20to%20retain%0Ahigher%20robustness%20on%20the%20adversarial%20examples%20that%20were%20correctly%20classified%0Abefore%20the%20update.%20We%20show%20that%20our%20algorithm%20and%2C%20more%20generally%2C%20learning%0Awith%20non-regression%20constraints%2C%20provides%20a%20theoretically-grounded%20framework%20to%0Atrain%20consistent%20estimators.%20Our%20experiments%20on%20robust%20models%20for%20computer%0Avision%20confirm%20that%20%28i%29%20both%20accuracy%20and%20robustness%2C%20even%20if%20improved%20after%0Amodel%20update%2C%20can%20be%20affected%20by%20negative%20flips%2C%20and%20%28ii%29%20our%0Arobustness-congruent%20adversarial%20training%20can%20mitigate%20the%20problem%2C%0Aoutperforming%20competing%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17390v1&entry.124074799=Read"},
{"title": "PolypNextLSTM: A lightweight and fast polyp video segmentation network\n  using ConvNext and ConvLSTM", "author": "Debayan Bhattacharya and Konrad Reuter and Finn Behrendnt and Lennart Maack and Sarah Grube and Alexander Schlaefer", "abstract": "  Commonly employed in polyp segmentation, single image UNet architectures lack\nthe temporal insight clinicians gain from video data in diagnosing polyps. To\nmirror clinical practices more faithfully, our proposed solution,\nPolypNextLSTM, leverages video-based deep learning, harnessing temporal\ninformation for superior segmentation performance with the least parameter\noverhead, making it possibly suitable for edge devices. PolypNextLSTM employs a\nUNet-like structure with ConvNext-Tiny as its backbone, strategically omitting\nthe last two layers to reduce parameter overhead. Our temporal fusion module, a\nConvolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal\nfeatures. Our primary novelty lies in PolypNextLSTM, which stands out as the\nleanest in parameters and the fastest model, surpassing the performance of five\nstate-of-the-art image and video-based deep learning models. The evaluation of\nthe SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios,\nalong with videos containing challenging artefacts like fast motion and\nocclusion. Comparison against 5 image-based and 5 video-based models\ndemonstrates PolypNextLSTM's superiority, achieving a Dice score of 0.7898 on\nthe hard-to-detect polyp test set, surpassing image-based PraNet (0.7519) and\nvideo-based PNSPlusNet (0.7486). Notably, our model excels in videos featuring\ncomplex artefacts such as ghosting and occlusion. PolypNextLSTM, integrating\npruned ConvNext-Tiny with ConvLSTM for temporal fusion, not only exhibits\nsuperior segmentation performance but also maintains the highest frames per\nspeed among evaluated models. Access code here\nhttps://github.com/mtec-tuhh/PolypNextLSTM\n", "link": "http://arxiv.org/abs/2402.11585v2", "date": "2024-02-27", "relevancy": 2.0189, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5084}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5031}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4997}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolypNextLSTM%3A%20A%20lightweight%20and%20fast%20polyp%20video%20segmentation%20network%0A%20%20using%20ConvNext%20and%20ConvLSTM&entry.906535625=Debayan%20Bhattacharya%20and%20Konrad%20Reuter%20and%20Finn%20Behrendnt%20and%20Lennart%20Maack%20and%20Sarah%20Grube%20and%20Alexander%20Schlaefer&entry.1292438233=%20%20Commonly%20employed%20in%20polyp%20segmentation%2C%20single%20image%20UNet%20architectures%20lack%0Athe%20temporal%20insight%20clinicians%20gain%20from%20video%20data%20in%20diagnosing%20polyps.%20To%0Amirror%20clinical%20practices%20more%20faithfully%2C%20our%20proposed%20solution%2C%0APolypNextLSTM%2C%20leverages%20video-based%20deep%20learning%2C%20harnessing%20temporal%0Ainformation%20for%20superior%20segmentation%20performance%20with%20the%20least%20parameter%0Aoverhead%2C%20making%20it%20possibly%20suitable%20for%20edge%20devices.%20PolypNextLSTM%20employs%20a%0AUNet-like%20structure%20with%20ConvNext-Tiny%20as%20its%20backbone%2C%20strategically%20omitting%0Athe%20last%20two%20layers%20to%20reduce%20parameter%20overhead.%20Our%20temporal%20fusion%20module%2C%20a%0AConvolutional%20Long%20Short%20Term%20Memory%20%28ConvLSTM%29%2C%20effectively%20exploits%20temporal%0Afeatures.%20Our%20primary%20novelty%20lies%20in%20PolypNextLSTM%2C%20which%20stands%20out%20as%20the%0Aleanest%20in%20parameters%20and%20the%20fastest%20model%2C%20surpassing%20the%20performance%20of%20five%0Astate-of-the-art%20image%20and%20video-based%20deep%20learning%20models.%20The%20evaluation%20of%0Athe%20SUN-SEG%20dataset%20spans%20easy-to-detect%20and%20hard-to-detect%20polyp%20scenarios%2C%0Aalong%20with%20videos%20containing%20challenging%20artefacts%20like%20fast%20motion%20and%0Aocclusion.%20Comparison%20against%205%20image-based%20and%205%20video-based%20models%0Ademonstrates%20PolypNextLSTM%27s%20superiority%2C%20achieving%20a%20Dice%20score%20of%200.7898%20on%0Athe%20hard-to-detect%20polyp%20test%20set%2C%20surpassing%20image-based%20PraNet%20%280.7519%29%20and%0Avideo-based%20PNSPlusNet%20%280.7486%29.%20Notably%2C%20our%20model%20excels%20in%20videos%20featuring%0Acomplex%20artefacts%20such%20as%20ghosting%20and%20occlusion.%20PolypNextLSTM%2C%20integrating%0Apruned%20ConvNext-Tiny%20with%20ConvLSTM%20for%20temporal%20fusion%2C%20not%20only%20exhibits%0Asuperior%20segmentation%20performance%20but%20also%20maintains%20the%20highest%20frames%20per%0Aspeed%20among%20evaluated%20models.%20Access%20code%20here%0Ahttps%3A//github.com/mtec-tuhh/PolypNextLSTM%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11585v2&entry.124074799=Read"},
{"title": "CAPT: Category-level Articulation Estimation from a Single Point Cloud\n  Using Transformer", "author": "Lian Fu and Ryoichi Ishikawa and Yoshihiro Sato and Takeshi Oishi", "abstract": "  The ability to estimate joint parameters is essential for various\napplications in robotics and computer vision. In this paper, we propose CAPT:\ncategory-level articulation estimation from a point cloud using Transformer.\nCAPT uses an end-to-end transformer-based architecture for joint parameter and\nstate estimation of articulated objects from a single point cloud. The proposed\nCAPT methods accurately estimate joint parameters and states for various\narticulated objects with high precision and robustness. The paper also\nintroduces a motion loss approach, which improves articulation estimation\nperformance by emphasizing the dynamic features of articulated objects.\nAdditionally, the paper presents a double voting strategy to provide the\nframework with coarse-to-fine parameter estimation. Experimental results on\nseveral category datasets demonstrate that our methods outperform existing\nalternatives for articulation estimation. Our research provides a promising\nsolution for applying Transformer-based architectures in articulated object\nanalysis.\n", "link": "http://arxiv.org/abs/2402.17360v1", "date": "2024-02-27", "relevancy": 2.0154, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5043}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4969}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPT%3A%20Category-level%20Articulation%20Estimation%20from%20a%20Single%20Point%20Cloud%0A%20%20Using%20Transformer&entry.906535625=Lian%20Fu%20and%20Ryoichi%20Ishikawa%20and%20Yoshihiro%20Sato%20and%20Takeshi%20Oishi&entry.1292438233=%20%20The%20ability%20to%20estimate%20joint%20parameters%20is%20essential%20for%20various%0Aapplications%20in%20robotics%20and%20computer%20vision.%20In%20this%20paper%2C%20we%20propose%20CAPT%3A%0Acategory-level%20articulation%20estimation%20from%20a%20point%20cloud%20using%20Transformer.%0ACAPT%20uses%20an%20end-to-end%20transformer-based%20architecture%20for%20joint%20parameter%20and%0Astate%20estimation%20of%20articulated%20objects%20from%20a%20single%20point%20cloud.%20The%20proposed%0ACAPT%20methods%20accurately%20estimate%20joint%20parameters%20and%20states%20for%20various%0Aarticulated%20objects%20with%20high%20precision%20and%20robustness.%20The%20paper%20also%0Aintroduces%20a%20motion%20loss%20approach%2C%20which%20improves%20articulation%20estimation%0Aperformance%20by%20emphasizing%20the%20dynamic%20features%20of%20articulated%20objects.%0AAdditionally%2C%20the%20paper%20presents%20a%20double%20voting%20strategy%20to%20provide%20the%0Aframework%20with%20coarse-to-fine%20parameter%20estimation.%20Experimental%20results%20on%0Aseveral%20category%20datasets%20demonstrate%20that%20our%20methods%20outperform%20existing%0Aalternatives%20for%20articulation%20estimation.%20Our%20research%20provides%20a%20promising%0Asolution%20for%20applying%20Transformer-based%20architectures%20in%20articulated%20object%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17360v1&entry.124074799=Read"},
{"title": "Bayesian Differentiable Physics for Cloth Digitalization", "author": "Deshan Gong and Ningtao Mao and He Wang", "abstract": "  We propose a new method for cloth digitalization. Deviating from existing\nmethods which learn from data captured under relatively casual settings, we\npropose to learn from data captured in strictly tested measuring protocols, and\nfind plausible physical parameters of the cloths. However, such data is\ncurrently absent, so we first propose a new dataset with accurate cloth\nmeasurements. Further, the data size is considerably smaller than the ones in\ncurrent deep learning, due to the nature of the data capture process. To learn\nfrom small data, we propose a new Bayesian differentiable cloth model to\nestimate the complex material heterogeneity of real cloths. It can provide\nhighly accurate digitalization from very limited data samples. Through\nexhaustive evaluation and comparison, we show our method is accurate in cloth\ndigitalization, efficient in learning from limited data samples, and general in\ncapturing material variations. Code and data are available\nhttps://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization\n", "link": "http://arxiv.org/abs/2402.17664v1", "date": "2024-02-27", "relevancy": 2.0123, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5551}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4959}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4894}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Differentiable%20Physics%20for%20Cloth%20Digitalization&entry.906535625=Deshan%20Gong%20and%20Ningtao%20Mao%20and%20He%20Wang&entry.1292438233=%20%20We%20propose%20a%20new%20method%20for%20cloth%20digitalization.%20Deviating%20from%20existing%0Amethods%20which%20learn%20from%20data%20captured%20under%20relatively%20casual%20settings%2C%20we%0Apropose%20to%20learn%20from%20data%20captured%20in%20strictly%20tested%20measuring%20protocols%2C%20and%0Afind%20plausible%20physical%20parameters%20of%20the%20cloths.%20However%2C%20such%20data%20is%0Acurrently%20absent%2C%20so%20we%20first%20propose%20a%20new%20dataset%20with%20accurate%20cloth%0Ameasurements.%20Further%2C%20the%20data%20size%20is%20considerably%20smaller%20than%20the%20ones%20in%0Acurrent%20deep%20learning%2C%20due%20to%20the%20nature%20of%20the%20data%20capture%20process.%20To%20learn%0Afrom%20small%20data%2C%20we%20propose%20a%20new%20Bayesian%20differentiable%20cloth%20model%20to%0Aestimate%20the%20complex%20material%20heterogeneity%20of%20real%20cloths.%20It%20can%20provide%0Ahighly%20accurate%20digitalization%20from%20very%20limited%20data%20samples.%20Through%0Aexhaustive%20evaluation%20and%20comparison%2C%20we%20show%20our%20method%20is%20accurate%20in%20cloth%0Adigitalization%2C%20efficient%20in%20learning%20from%20limited%20data%20samples%2C%20and%20general%20in%0Acapturing%20material%20variations.%20Code%20and%20data%20are%20available%0Ahttps%3A//github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17664v1&entry.124074799=Read"},
{"title": "Citation-Enhanced Generation for LLM-based Chatbot", "author": "Weitao Li and Junkai Li and Weizhi Ma and Yang Liu", "abstract": "  Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable.\n", "link": "http://arxiv.org/abs/2402.16063v2", "date": "2024-02-27", "relevancy": 2.0079, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5257}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.494}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4814}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Citation-Enhanced%20Generation%20for%20LLM-based%20Chatbot&entry.906535625=Weitao%20Li%20and%20Junkai%20Li%20and%20Weizhi%20Ma%20and%20Yang%20Liu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20powerful%20general%20intelligence%20across%0Adiverse%20scenarios%2C%20including%20their%20integration%20into%20chatbots.%20However%2C%20a%20vital%0Achallenge%20of%20LLM-based%20chatbots%20is%20that%20they%20may%20produce%20hallucinated%20content%0Ain%20responses%2C%20which%20significantly%20limits%20their%20applicability.%20Various%20efforts%0Ahave%20been%20made%20to%20alleviate%20hallucination%2C%20such%20as%20retrieval%20augmented%0Ageneration%20and%20reinforcement%20learning%20with%20human%20feedback%2C%20but%20most%20of%20them%0Arequire%20additional%20training%20and%20data%20annotation.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20post-hoc%20Citation-Enhanced%20Generation%20%28CEG%29%20approach%20combined%20with%0Aretrieval%20argumentation.%20Unlike%20previous%20studies%20that%20focus%20on%20preventing%0Ahallucinations%20during%20generation%2C%20our%20method%20addresses%20this%20issue%20in%20a%20post-hoc%0Away.%20It%20incorporates%20a%20retrieval%20module%20to%20search%20for%20supporting%20documents%0Arelevant%20to%20the%20generated%20content%2C%20and%20employs%20a%20natural%20language%0Ainference-based%20citation%20generation%20module.%20Once%20the%20statements%20in%20the%0Agenerated%20content%20lack%20of%20reference%2C%20our%20model%20can%20regenerate%20responses%20until%0Aall%20statements%20are%20supported%20by%20citations.%20Note%20that%20our%20method%20is%20a%0Atraining-free%20plug-and-play%20plugin%20that%20is%20capable%20of%20various%20LLMs.%20Experiments%0Aon%20various%20hallucination-related%20datasets%20show%20our%20framework%20outperforms%0Astate-of-the-art%20methods%20in%20both%20hallucination%20detection%20and%20response%0Aregeneration%20on%20three%20benchmarks.%20Our%20codes%20and%20dataset%20will%20be%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16063v2&entry.124074799=Read"},
{"title": "Why do Learning Rates Transfer? Reconciling Optimization and Scaling\n  Limits for Deep Learning", "author": "Lorenzo Noci and Alexandru Meterez and Thomas Hofmann and Antonio Orvieto", "abstract": "  Recently, there has been growing evidence that if the width and depth of a\nneural network are scaled toward the so-called rich feature learning limit\n($\\mu$P and its depth extension), then some hyperparameters - such as the\nlearning rate - exhibit transfer from small to very large models, thus reducing\nthe cost of hyperparameter tuning. From an optimization perspective, this\nphenomenon is puzzling, as it implies that the loss landscape is remarkably\nconsistent across very different model sizes. In this work, we find empirical\nevidence that learning rate transfer can be attributed to the fact that under\n$\\mu$P and its depth extension, the largest eigenvalue of the training loss\nHessian (i.e. the sharpness) is largely independent of the width and depth of\nthe network for a sustained period of training time. On the other hand, we show\nthat under the neural tangent kernel (NTK) regime, the sharpness exhibits very\ndifferent dynamics at different scales, thus preventing learning rate transfer.\nBut what causes these differences in the sharpness dynamics? Through a\nconnection between the spectra of the Hessian and the NTK matrix, we argue that\nthe cause lies in the presence (for $\\mu$P) or progressive absence (for the NTK\nregime) of feature learning, which results in a different evolution of the NTK,\nand thus of the sharpness. We corroborate our claims with a substantial suite\nof experiments, covering a wide range of datasets and architectures: from\nResNets and Vision Transformers trained on benchmark vision datasets to\nTransformers-based language models trained on WikiText\n", "link": "http://arxiv.org/abs/2402.17457v1", "date": "2024-02-27", "relevancy": 2.0045, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.543}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4731}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4667}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20do%20Learning%20Rates%20Transfer%3F%20Reconciling%20Optimization%20and%20Scaling%0A%20%20Limits%20for%20Deep%20Learning&entry.906535625=Lorenzo%20Noci%20and%20Alexandru%20Meterez%20and%20Thomas%20Hofmann%20and%20Antonio%20Orvieto&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20growing%20evidence%20that%20if%20the%20width%20and%20depth%20of%20a%0Aneural%20network%20are%20scaled%20toward%20the%20so-called%20rich%20feature%20learning%20limit%0A%28%24%5Cmu%24P%20and%20its%20depth%20extension%29%2C%20then%20some%20hyperparameters%20-%20such%20as%20the%0Alearning%20rate%20-%20exhibit%20transfer%20from%20small%20to%20very%20large%20models%2C%20thus%20reducing%0Athe%20cost%20of%20hyperparameter%20tuning.%20From%20an%20optimization%20perspective%2C%20this%0Aphenomenon%20is%20puzzling%2C%20as%20it%20implies%20that%20the%20loss%20landscape%20is%20remarkably%0Aconsistent%20across%20very%20different%20model%20sizes.%20In%20this%20work%2C%20we%20find%20empirical%0Aevidence%20that%20learning%20rate%20transfer%20can%20be%20attributed%20to%20the%20fact%20that%20under%0A%24%5Cmu%24P%20and%20its%20depth%20extension%2C%20the%20largest%20eigenvalue%20of%20the%20training%20loss%0AHessian%20%28i.e.%20the%20sharpness%29%20is%20largely%20independent%20of%20the%20width%20and%20depth%20of%0Athe%20network%20for%20a%20sustained%20period%20of%20training%20time.%20On%20the%20other%20hand%2C%20we%20show%0Athat%20under%20the%20neural%20tangent%20kernel%20%28NTK%29%20regime%2C%20the%20sharpness%20exhibits%20very%0Adifferent%20dynamics%20at%20different%20scales%2C%20thus%20preventing%20learning%20rate%20transfer.%0ABut%20what%20causes%20these%20differences%20in%20the%20sharpness%20dynamics%3F%20Through%20a%0Aconnection%20between%20the%20spectra%20of%20the%20Hessian%20and%20the%20NTK%20matrix%2C%20we%20argue%20that%0Athe%20cause%20lies%20in%20the%20presence%20%28for%20%24%5Cmu%24P%29%20or%20progressive%20absence%20%28for%20the%20NTK%0Aregime%29%20of%20feature%20learning%2C%20which%20results%20in%20a%20different%20evolution%20of%20the%20NTK%2C%0Aand%20thus%20of%20the%20sharpness.%20We%20corroborate%20our%20claims%20with%20a%20substantial%20suite%0Aof%20experiments%2C%20covering%20a%20wide%20range%20of%20datasets%20and%20architectures%3A%20from%0AResNets%20and%20Vision%20Transformers%20trained%20on%20benchmark%20vision%20datasets%20to%0ATransformers-based%20language%20models%20trained%20on%20WikiText%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17457v1&entry.124074799=Read"},
{"title": "Securing Reliability: A Brief Overview on Enhancing In-Context Learning\n  for Foundation Models", "author": "Yunpeng Huang and Yaonan Gu and Jingwei Xu and Zhihong Zhu and Zhaorun Chen and Xiaoxing Ma", "abstract": "  As foundation models (FMs) continue to shape the landscape of AI, the\nin-context learning (ICL) paradigm thrives but also encounters issues such as\ntoxicity, hallucination, disparity, adversarial vulnerability, and\ninconsistency. Ensuring the reliability and responsibility of FMs is crucial\nfor the sustainable development of the AI ecosystem. In this concise overview,\nwe investigate recent advancements in enhancing the reliability and\ntrustworthiness of FMs within ICL frameworks, focusing on four key\nmethodologies, each with its corresponding subgoals. We sincerely hope this\npaper can provide valuable insights for researchers and practitioners\nendeavoring to build safe and dependable FMs and foster a stable and consistent\nICL environment, thereby unlocking their vast potential.\n", "link": "http://arxiv.org/abs/2402.17671v1", "date": "2024-02-27", "relevancy": 1.9997, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5311}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4958}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4704}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Securing%20Reliability%3A%20A%20Brief%20Overview%20on%20Enhancing%20In-Context%20Learning%0A%20%20for%20Foundation%20Models&entry.906535625=Yunpeng%20Huang%20and%20Yaonan%20Gu%20and%20Jingwei%20Xu%20and%20Zhihong%20Zhu%20and%20Zhaorun%20Chen%20and%20Xiaoxing%20Ma&entry.1292438233=%20%20As%20foundation%20models%20%28FMs%29%20continue%20to%20shape%20the%20landscape%20of%20AI%2C%20the%0Ain-context%20learning%20%28ICL%29%20paradigm%20thrives%20but%20also%20encounters%20issues%20such%20as%0Atoxicity%2C%20hallucination%2C%20disparity%2C%20adversarial%20vulnerability%2C%20and%0Ainconsistency.%20Ensuring%20the%20reliability%20and%20responsibility%20of%20FMs%20is%20crucial%0Afor%20the%20sustainable%20development%20of%20the%20AI%20ecosystem.%20In%20this%20concise%20overview%2C%0Awe%20investigate%20recent%20advancements%20in%20enhancing%20the%20reliability%20and%0Atrustworthiness%20of%20FMs%20within%20ICL%20frameworks%2C%20focusing%20on%20four%20key%0Amethodologies%2C%20each%20with%20its%20corresponding%20subgoals.%20We%20sincerely%20hope%20this%0Apaper%20can%20provide%20valuable%20insights%20for%20researchers%20and%20practitioners%0Aendeavoring%20to%20build%20safe%20and%20dependable%20FMs%20and%20foster%20a%20stable%20and%20consistent%0AICL%20environment%2C%20thereby%20unlocking%20their%20vast%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17671v1&entry.124074799=Read"},
{"title": "LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning", "author": "Shentong Mo and Yansen Wang and Xufang Luo and Dongsheng Li", "abstract": "  Visual Prompt Tuning (VPT) techniques have gained prominence for their\ncapacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual\ntasks using specialized learnable tokens termed as prompts. Contemporary VPT\nmethodologies, especially when employed with self-supervised vision\ntransformers, often default to the introduction of new learnable prompts or\ngated prompt tokens predominantly sourced from the model's previous block. A\npivotal oversight in such approaches is their failure to harness the potential\nof long-range previous blocks as sources of prompts within each self-supervised\nViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning\n(LSPT) - a revolutionary approach to visual representation learning. Drawing\ninspiration from the intricacies of the human brain, LSPT ingeniously\nincorporates long-term gated prompts. This feature serves as temporal coding,\ncurbing the risk of forgetting parameters acquired from earlier blocks. Further\nenhancing its prowess, LSPT brings into play patch tokens, serving as spatial\ncoding. This is strategically designed to perpetually amass class-conscious\nfeatures, thereby fortifying the model's prowess in distinguishing and\nidentifying visual categories. To validate the efficacy of our proposed method,\nwe engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.\nOur empirical findings underscore the superiority of LSPT, showcasing its\nability to set new benchmarks in visual prompt tuning performance.\n", "link": "http://arxiv.org/abs/2402.17406v1", "date": "2024-02-27", "relevancy": 1.9947, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5201}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5074}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4737}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSPT%3A%20Long-term%20Spatial%20Prompt%20Tuning%20for%20Visual%20Representation%20Learning&entry.906535625=Shentong%20Mo%20and%20Yansen%20Wang%20and%20Xufang%20Luo%20and%20Dongsheng%20Li&entry.1292438233=%20%20Visual%20Prompt%20Tuning%20%28VPT%29%20techniques%20have%20gained%20prominence%20for%20their%0Acapacity%20to%20adapt%20pre-trained%20Vision%20Transformers%20%28ViTs%29%20to%20downstream%20visual%0Atasks%20using%20specialized%20learnable%20tokens%20termed%20as%20prompts.%20Contemporary%20VPT%0Amethodologies%2C%20especially%20when%20employed%20with%20self-supervised%20vision%0Atransformers%2C%20often%20default%20to%20the%20introduction%20of%20new%20learnable%20prompts%20or%0Agated%20prompt%20tokens%20predominantly%20sourced%20from%20the%20model%27s%20previous%20block.%20A%0Apivotal%20oversight%20in%20such%20approaches%20is%20their%20failure%20to%20harness%20the%20potential%0Aof%20long-range%20previous%20blocks%20as%20sources%20of%20prompts%20within%20each%20self-supervised%0AViT.%20To%20bridge%20this%20crucial%20gap%2C%20we%20introduce%20Long-term%20Spatial%20Prompt%20Tuning%0A%28LSPT%29%20-%20a%20revolutionary%20approach%20to%20visual%20representation%20learning.%20Drawing%0Ainspiration%20from%20the%20intricacies%20of%20the%20human%20brain%2C%20LSPT%20ingeniously%0Aincorporates%20long-term%20gated%20prompts.%20This%20feature%20serves%20as%20temporal%20coding%2C%0Acurbing%20the%20risk%20of%20forgetting%20parameters%20acquired%20from%20earlier%20blocks.%20Further%0Aenhancing%20its%20prowess%2C%20LSPT%20brings%20into%20play%20patch%20tokens%2C%20serving%20as%20spatial%0Acoding.%20This%20is%20strategically%20designed%20to%20perpetually%20amass%20class-conscious%0Afeatures%2C%20thereby%20fortifying%20the%20model%27s%20prowess%20in%20distinguishing%20and%0Aidentifying%20visual%20categories.%20To%20validate%20the%20efficacy%20of%20our%20proposed%20method%2C%0Awe%20engaged%20in%20rigorous%20experimentation%20across%205%20FGVC%20and%2019%20VTAB-1K%20benchmarks.%0AOur%20empirical%20findings%20underscore%20the%20superiority%20of%20LSPT%2C%20showcasing%20its%0Aability%20to%20set%20new%20benchmarks%20in%20visual%20prompt%20tuning%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17406v1&entry.124074799=Read"},
{"title": "How Good is ChatGPT at Face Biometrics? A First Look into Recognition,\n  Soft Biometrics, and Explainability", "author": "Ivan DeAndres-Tame and Ruben Tolosana and Ruben Vera-Rodriguez and Aythami Morales and Julian Fierrez and Javier Ortega-Garcia", "abstract": "  Large Language Models (LLMs) such as GPT developed by OpenAI, have already\nshown astonishing results, introducing quick changes in our society. This has\nbeen intensified by the release of ChatGPT which allows anyone to interact in a\nsimple conversational way with LLMs, without any experience in the field\nneeded. As a result, ChatGPT has been rapidly applied to many different tasks\nsuch as code- and song-writer, education, virtual assistants, etc., showing\nimpressive results for tasks for which it was not trained (zero-shot learning).\n  The present study aims to explore the ability of ChatGPT, based on the recent\nGPT-4 multimodal LLM, for the task of face biometrics. In particular, we\nanalyze the ability of ChatGPT to perform tasks such as face verification,\nsoft-biometrics estimation, and explainability of the results. ChatGPT could be\nvery valuable to further increase the explainability and transparency of\nautomatic decisions in human scenarios. Experiments are carried out in order to\nevaluate the performance and robustness of ChatGPT, using popular public\nbenchmarks and comparing the results with state-of-the-art methods in the\nfield. The results achieved in this study show the potential of LLMs such as\nChatGPT for face biometrics, especially to enhance explainability. For\nreproducibility reasons, we release all the code in GitHub.\n", "link": "http://arxiv.org/abs/2401.13641v2", "date": "2024-02-27", "relevancy": 1.9914, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5099}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4931}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4878}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Good%20is%20ChatGPT%20at%20Face%20Biometrics%3F%20A%20First%20Look%20into%20Recognition%2C%0A%20%20Soft%20Biometrics%2C%20and%20Explainability&entry.906535625=Ivan%20DeAndres-Tame%20and%20Ruben%20Tolosana%20and%20Ruben%20Vera-Rodriguez%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Javier%20Ortega-Garcia&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20GPT%20developed%20by%20OpenAI%2C%20have%20already%0Ashown%20astonishing%20results%2C%20introducing%20quick%20changes%20in%20our%20society.%20This%20has%0Abeen%20intensified%20by%20the%20release%20of%20ChatGPT%20which%20allows%20anyone%20to%20interact%20in%20a%0Asimple%20conversational%20way%20with%20LLMs%2C%20without%20any%20experience%20in%20the%20field%0Aneeded.%20As%20a%20result%2C%20ChatGPT%20has%20been%20rapidly%20applied%20to%20many%20different%20tasks%0Asuch%20as%20code-%20and%20song-writer%2C%20education%2C%20virtual%20assistants%2C%20etc.%2C%20showing%0Aimpressive%20results%20for%20tasks%20for%20which%20it%20was%20not%20trained%20%28zero-shot%20learning%29.%0A%20%20The%20present%20study%20aims%20to%20explore%20the%20ability%20of%20ChatGPT%2C%20based%20on%20the%20recent%0AGPT-4%20multimodal%20LLM%2C%20for%20the%20task%20of%20face%20biometrics.%20In%20particular%2C%20we%0Aanalyze%20the%20ability%20of%20ChatGPT%20to%20perform%20tasks%20such%20as%20face%20verification%2C%0Asoft-biometrics%20estimation%2C%20and%20explainability%20of%20the%20results.%20ChatGPT%20could%20be%0Avery%20valuable%20to%20further%20increase%20the%20explainability%20and%20transparency%20of%0Aautomatic%20decisions%20in%20human%20scenarios.%20Experiments%20are%20carried%20out%20in%20order%20to%0Aevaluate%20the%20performance%20and%20robustness%20of%20ChatGPT%2C%20using%20popular%20public%0Abenchmarks%20and%20comparing%20the%20results%20with%20state-of-the-art%20methods%20in%20the%0Afield.%20The%20results%20achieved%20in%20this%20study%20show%20the%20potential%20of%20LLMs%20such%20as%0AChatGPT%20for%20face%20biometrics%2C%20especially%20to%20enhance%20explainability.%20For%0Areproducibility%20reasons%2C%20we%20release%20all%20the%20code%20in%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13641v2&entry.124074799=Read"},
{"title": "VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction", "author": "Jiaqi Lin and Zhihao Li and Xiao Tang and Jianzhuang Liu and Shiyong Liu and Jiayue Liu and Yangdi Lu and Xiaofei Wu and Songcen Xu and Youliang Yan and Wenming Yang", "abstract": "  Existing NeRF-based methods for large scene reconstruction often have\nlimitations in visual quality and rendering speed. While the recent 3D Gaussian\nSplatting works well on small-scale and object-centric scenes, scaling it up to\nlarge scenes poses challenges due to limited video memory, long optimization\ntime, and noticeable appearance variations. To address these challenges, we\npresent VastGaussian, the first method for high-quality reconstruction and\nreal-time rendering on large scenes based on 3D Gaussian Splatting. We propose\na progressive partitioning strategy to divide a large scene into multiple\ncells, where the training cameras and point cloud are properly distributed with\nan airspace-aware visibility criterion. These cells are merged into a complete\nscene after parallel optimization. We also introduce decoupled appearance\nmodeling into the optimization process to reduce appearance variations in the\nrendered images. Our approach outperforms existing NeRF-based methods and\nachieves state-of-the-art results on multiple large scene datasets, enabling\nfast optimization and high-fidelity real-time rendering.\n", "link": "http://arxiv.org/abs/2402.17427v1", "date": "2024-02-27", "relevancy": 1.9904, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5098}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4956}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4862}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VastGaussian%3A%20Vast%203D%20Gaussians%20for%20Large%20Scene%20Reconstruction&entry.906535625=Jiaqi%20Lin%20and%20Zhihao%20Li%20and%20Xiao%20Tang%20and%20Jianzhuang%20Liu%20and%20Shiyong%20Liu%20and%20Jiayue%20Liu%20and%20Yangdi%20Lu%20and%20Xiaofei%20Wu%20and%20Songcen%20Xu%20and%20Youliang%20Yan%20and%20Wenming%20Yang&entry.1292438233=%20%20Existing%20NeRF-based%20methods%20for%20large%20scene%20reconstruction%20often%20have%0Alimitations%20in%20visual%20quality%20and%20rendering%20speed.%20While%20the%20recent%203D%20Gaussian%0ASplatting%20works%20well%20on%20small-scale%20and%20object-centric%20scenes%2C%20scaling%20it%20up%20to%0Alarge%20scenes%20poses%20challenges%20due%20to%20limited%20video%20memory%2C%20long%20optimization%0Atime%2C%20and%20noticeable%20appearance%20variations.%20To%20address%20these%20challenges%2C%20we%0Apresent%20VastGaussian%2C%20the%20first%20method%20for%20high-quality%20reconstruction%20and%0Areal-time%20rendering%20on%20large%20scenes%20based%20on%203D%20Gaussian%20Splatting.%20We%20propose%0Aa%20progressive%20partitioning%20strategy%20to%20divide%20a%20large%20scene%20into%20multiple%0Acells%2C%20where%20the%20training%20cameras%20and%20point%20cloud%20are%20properly%20distributed%20with%0Aan%20airspace-aware%20visibility%20criterion.%20These%20cells%20are%20merged%20into%20a%20complete%0Ascene%20after%20parallel%20optimization.%20We%20also%20introduce%20decoupled%20appearance%0Amodeling%20into%20the%20optimization%20process%20to%20reduce%20appearance%20variations%20in%20the%0Arendered%20images.%20Our%20approach%20outperforms%20existing%20NeRF-based%20methods%20and%0Aachieves%20state-of-the-art%20results%20on%20multiple%20large%20scene%20datasets%2C%20enabling%0Afast%20optimization%20and%20high-fidelity%20real-time%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17427v1&entry.124074799=Read"},
{"title": "ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models", "author": "David Peer and Philemon Sch\u00f6pf and Volckmar Nebendahl and Alexander Rietzler and Sebastian Stabinger", "abstract": "  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs. This paper introduces a new metric for generative models called ANLS*\nfor evaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets and 3 different GLLMs using the\nANLS* metric is also provided, demonstrating the importance of the proposed\nmetric. We also benchmark a novel approach to generate prompts for documents,\ncalled SFT, against other prompting techniques such as LATIN. In 15 out of 21\ncases, SFT outperforms other techniques and improves the state-of-the-art,\nsometimes by as much as 15 percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric\n", "link": "http://arxiv.org/abs/2402.03848v2", "date": "2024-02-27", "relevancy": 1.987, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5213}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4994}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4843}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANLS%2A%20--%20A%20Universal%20Document%20Processing%20Metric%20for%20Generative%20Large%0A%20%20Language%20Models&entry.906535625=David%20Peer%20and%20Philemon%20Sch%C3%B6pf%20and%20Volckmar%20Nebendahl%20and%20Alexander%20Rietzler%20and%20Sebastian%20Stabinger&entry.1292438233=%20%20Traditionally%2C%20discriminative%20models%20have%20been%20the%20predominant%20choice%20for%0Atasks%20like%20document%20classification%20and%20information%20extraction.%20These%20models%0Amake%20predictions%20that%20fall%20into%20a%20limited%20number%20of%20predefined%20classes%2C%0Afacilitating%20a%20binary%20true%20or%20false%20evaluation%20and%20enabling%20the%20direct%0Acalculation%20of%20metrics%20such%20as%20the%20F1%20score.%20However%2C%20recent%20advancements%20in%0Agenerative%20large%20language%20models%20%28GLLMs%29%20have%20prompted%20a%20shift%20in%20the%20field%20due%0Ato%20their%20enhanced%20zero-shot%20capabilities%2C%20which%20eliminate%20the%20need%20for%20a%0Adownstream%20dataset%20and%20computationally%20expensive%20fine-tuning.%20However%2C%0Aevaluating%20GLLMs%20presents%20a%20challenge%20as%20the%20binary%20true%20or%20false%20evaluation%0Aused%20for%20discriminative%20models%20is%20not%20applicable%20to%20the%20predictions%20made%20by%0AGLLMs.%20This%20paper%20introduces%20a%20new%20metric%20for%20generative%20models%20called%20ANLS%2A%0Afor%20evaluating%20a%20wide%20variety%20of%20tasks%2C%20including%20information%20extraction%20and%0Aclassification%20tasks.%20The%20ANLS%2A%20metric%20extends%20existing%20ANLS%20metrics%20as%20a%0Adrop-in-replacement%20and%20is%20still%20compatible%20with%20previously%20reported%20ANLS%0Ascores.%20An%20evaluation%20of%207%20different%20datasets%20and%203%20different%20GLLMs%20using%20the%0AANLS%2A%20metric%20is%20also%20provided%2C%20demonstrating%20the%20importance%20of%20the%20proposed%0Ametric.%20We%20also%20benchmark%20a%20novel%20approach%20to%20generate%20prompts%20for%20documents%2C%0Acalled%20SFT%2C%20against%20other%20prompting%20techniques%20such%20as%20LATIN.%20In%2015%20out%20of%2021%0Acases%2C%20SFT%20outperforms%20other%20techniques%20and%20improves%20the%20state-of-the-art%2C%0Asometimes%20by%20as%20much%20as%2015%20percentage%20points.%0A%20%20Sources%20are%20available%20at%20https%3A//github.com/deepopinion/anls_star_metric%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03848v2&entry.124074799=Read"},
{"title": "LoDIP: Low light phase retrieval with deep image prior", "author": "Raunak Manekar and Elisa Negrini and Minh Pham and Daniel Jacobs and Jaideep Srivastava", "abstract": "  Phase retrieval (PR) is a fundamental challenge in scientific imaging,\nenabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging\nat low radiation doses becomes important in applications where samples are\nsusceptible to radiation damage. However, most PR methods struggle in low dose\nscenario due to the presence of very high shot noise. Advancements in the\noptical data acquisition setup, exemplified by in-situ CDI, have shown\npotential for low-dose imaging. But these depend on a time series of\nmeasurements, rendering them unsuitable for single-image applications.\nSimilarly, on the computational front, data-driven phase retrieval techniques\nare not readily adaptable to the single-image context. Deep learning based\nsingle-image methods, such as deep image prior, have been effective for various\nimaging tasks but have exhibited limited success when applied to PR. In this\nwork, we propose LoDIP which combines the in-situ CDI setup with the power of\nimplicit neural priors to tackle the problem of single-image low-dose phase\nretrieval. Quantitative evaluations demonstrate the superior performance of\nLoDIP on this task as well as applicability to real experimental scenarios.\n", "link": "http://arxiv.org/abs/2402.17745v1", "date": "2024-02-27", "relevancy": 1.9797, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4987}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4952}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4847}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoDIP%3A%20Low%20light%20phase%20retrieval%20with%20deep%20image%20prior&entry.906535625=Raunak%20Manekar%20and%20Elisa%20Negrini%20and%20Minh%20Pham%20and%20Daniel%20Jacobs%20and%20Jaideep%20Srivastava&entry.1292438233=%20%20Phase%20retrieval%20%28PR%29%20is%20a%20fundamental%20challenge%20in%20scientific%20imaging%2C%0Aenabling%20nanoscale%20techniques%20like%20coherent%20diffractive%20imaging%20%28CDI%29.%20Imaging%0Aat%20low%20radiation%20doses%20becomes%20important%20in%20applications%20where%20samples%20are%0Asusceptible%20to%20radiation%20damage.%20However%2C%20most%20PR%20methods%20struggle%20in%20low%20dose%0Ascenario%20due%20to%20the%20presence%20of%20very%20high%20shot%20noise.%20Advancements%20in%20the%0Aoptical%20data%20acquisition%20setup%2C%20exemplified%20by%20in-situ%20CDI%2C%20have%20shown%0Apotential%20for%20low-dose%20imaging.%20But%20these%20depend%20on%20a%20time%20series%20of%0Ameasurements%2C%20rendering%20them%20unsuitable%20for%20single-image%20applications.%0ASimilarly%2C%20on%20the%20computational%20front%2C%20data-driven%20phase%20retrieval%20techniques%0Aare%20not%20readily%20adaptable%20to%20the%20single-image%20context.%20Deep%20learning%20based%0Asingle-image%20methods%2C%20such%20as%20deep%20image%20prior%2C%20have%20been%20effective%20for%20various%0Aimaging%20tasks%20but%20have%20exhibited%20limited%20success%20when%20applied%20to%20PR.%20In%20this%0Awork%2C%20we%20propose%20LoDIP%20which%20combines%20the%20in-situ%20CDI%20setup%20with%20the%20power%20of%0Aimplicit%20neural%20priors%20to%20tackle%20the%20problem%20of%20single-image%20low-dose%20phase%0Aretrieval.%20Quantitative%20evaluations%20demonstrate%20the%20superior%20performance%20of%0ALoDIP%20on%20this%20task%20as%20well%20as%20applicability%20to%20real%20experimental%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17745v1&entry.124074799=Read"},
{"title": "Efficient Contextformer: Spatio-Channel Window Attention for Fast\n  Context Modeling in Learned Image Compression", "author": "A. Burakhan Koyuncu and Panqi Jia and Atanas Boev and Elena Alshina and Eckehard Steinbach", "abstract": "  Entropy estimation is essential for the performance of learned image\ncompression. It has been demonstrated that a transformer-based entropy model is\nof critical importance for achieving a high compression ratio, however, at the\nexpense of a significant computational effort. In this work, we introduce the\nEfficient Contextformer (eContextformer) - a computationally efficient\ntransformer-based autoregressive context model for learned image compression.\nThe eContextformer efficiently fuses the patch-wise, checkered, and\nchannel-wise grouping techniques for parallel context modeling, and introduces\na shifted window spatio-channel attention mechanism. We explore better training\nstrategies and architectural designs and introduce additional complexity\noptimizations. During decoding, the proposed optimization techniques\ndynamically scale the attention span and cache the previous attention\ncomputations, drastically reducing the model and runtime complexity. Compared\nto the non-parallel approach, our proposal has ~145x lower model complexity and\n~210x faster decoding speed, and achieves higher average bit savings on Kodak,\nCLIC2020, and Tecnick datasets. Additionally, the low complexity of our context\nmodel enables online rate-distortion algorithms, which further improve the\ncompression performance. We achieve up to 17% bitrate savings over the intra\ncoding of Versatile Video Coding (VVC) Test Model (VTM) 16.2 and surpass\nvarious learning-based compression models.\n", "link": "http://arxiv.org/abs/2306.14287v2", "date": "2024-02-27", "relevancy": 1.9778, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5198}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5075}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4713}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Contextformer%3A%20Spatio-Channel%20Window%20Attention%20for%20Fast%0A%20%20Context%20Modeling%20in%20Learned%20Image%20Compression&entry.906535625=A.%20Burakhan%20Koyuncu%20and%20Panqi%20Jia%20and%20Atanas%20Boev%20and%20Elena%20Alshina%20and%20Eckehard%20Steinbach&entry.1292438233=%20%20Entropy%20estimation%20is%20essential%20for%20the%20performance%20of%20learned%20image%0Acompression.%20It%20has%20been%20demonstrated%20that%20a%20transformer-based%20entropy%20model%20is%0Aof%20critical%20importance%20for%20achieving%20a%20high%20compression%20ratio%2C%20however%2C%20at%20the%0Aexpense%20of%20a%20significant%20computational%20effort.%20In%20this%20work%2C%20we%20introduce%20the%0AEfficient%20Contextformer%20%28eContextformer%29%20-%20a%20computationally%20efficient%0Atransformer-based%20autoregressive%20context%20model%20for%20learned%20image%20compression.%0AThe%20eContextformer%20efficiently%20fuses%20the%20patch-wise%2C%20checkered%2C%20and%0Achannel-wise%20grouping%20techniques%20for%20parallel%20context%20modeling%2C%20and%20introduces%0Aa%20shifted%20window%20spatio-channel%20attention%20mechanism.%20We%20explore%20better%20training%0Astrategies%20and%20architectural%20designs%20and%20introduce%20additional%20complexity%0Aoptimizations.%20During%20decoding%2C%20the%20proposed%20optimization%20techniques%0Adynamically%20scale%20the%20attention%20span%20and%20cache%20the%20previous%20attention%0Acomputations%2C%20drastically%20reducing%20the%20model%20and%20runtime%20complexity.%20Compared%0Ato%20the%20non-parallel%20approach%2C%20our%20proposal%20has%20~145x%20lower%20model%20complexity%20and%0A~210x%20faster%20decoding%20speed%2C%20and%20achieves%20higher%20average%20bit%20savings%20on%20Kodak%2C%0ACLIC2020%2C%20and%20Tecnick%20datasets.%20Additionally%2C%20the%20low%20complexity%20of%20our%20context%0Amodel%20enables%20online%20rate-distortion%20algorithms%2C%20which%20further%20improve%20the%0Acompression%20performance.%20We%20achieve%20up%20to%2017%25%20bitrate%20savings%20over%20the%20intra%0Acoding%20of%20Versatile%20Video%20Coding%20%28VVC%29%20Test%20Model%20%28VTM%29%2016.2%20and%20surpass%0Avarious%20learning-based%20compression%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.14287v2&entry.124074799=Read"},
{"title": "Multiclass Learning from Noisy Labels for Non-decomposable Performance\n  Measures", "author": "Mingyuan Zhang and Shivani Agarwal", "abstract": "  There has been much interest in recent years in learning good classifiers\nfrom data with noisy labels. Most work on learning from noisy labels has\nfocused on standard loss-based performance measures. However, many machine\nlearning problems require using non-decomposable performance measures which\ncannot be expressed as the expectation or sum of a loss on individual examples;\nthese include for example the H-mean, Q-mean and G-mean in class imbalance\nsettings, and the Micro $F_1$ in information retrieval. In this paper, we\ndesign algorithms to learn from noisy labels for two broad classes of\nmulticlass non-decomposable performance measures, namely, monotonic convex and\nratio-of-linear, which encompass all the above examples. Our work builds on the\nFrank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both\ncases, we develop noise-corrected versions of the algorithms under the widely\nstudied family of class-conditional noise models. We provide regret (excess\nrisk) bounds for our algorithms, establishing that even though they are trained\non noisy data, they are Bayes consistent in the sense that their performance\nconverges to the optimal performance w.r.t. the clean (non-noisy) distribution.\nOur experiments demonstrate the effectiveness of our algorithms in handling\nlabel noise.\n", "link": "http://arxiv.org/abs/2402.01055v2", "date": "2024-02-27", "relevancy": 1.9764, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5023}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4883}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4882}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiclass%20Learning%20from%20Noisy%20Labels%20for%20Non-decomposable%20Performance%0A%20%20Measures&entry.906535625=Mingyuan%20Zhang%20and%20Shivani%20Agarwal&entry.1292438233=%20%20There%20has%20been%20much%20interest%20in%20recent%20years%20in%20learning%20good%20classifiers%0Afrom%20data%20with%20noisy%20labels.%20Most%20work%20on%20learning%20from%20noisy%20labels%20has%0Afocused%20on%20standard%20loss-based%20performance%20measures.%20However%2C%20many%20machine%0Alearning%20problems%20require%20using%20non-decomposable%20performance%20measures%20which%0Acannot%20be%20expressed%20as%20the%20expectation%20or%20sum%20of%20a%20loss%20on%20individual%20examples%3B%0Athese%20include%20for%20example%20the%20H-mean%2C%20Q-mean%20and%20G-mean%20in%20class%20imbalance%0Asettings%2C%20and%20the%20Micro%20%24F_1%24%20in%20information%20retrieval.%20In%20this%20paper%2C%20we%0Adesign%20algorithms%20to%20learn%20from%20noisy%20labels%20for%20two%20broad%20classes%20of%0Amulticlass%20non-decomposable%20performance%20measures%2C%20namely%2C%20monotonic%20convex%20and%0Aratio-of-linear%2C%20which%20encompass%20all%20the%20above%20examples.%20Our%20work%20builds%20on%20the%0AFrank-Wolfe%20and%20Bisection%20based%20methods%20of%20Narasimhan%20et%20al.%20%282015%29.%20In%20both%0Acases%2C%20we%20develop%20noise-corrected%20versions%20of%20the%20algorithms%20under%20the%20widely%0Astudied%20family%20of%20class-conditional%20noise%20models.%20We%20provide%20regret%20%28excess%0Arisk%29%20bounds%20for%20our%20algorithms%2C%20establishing%20that%20even%20though%20they%20are%20trained%0Aon%20noisy%20data%2C%20they%20are%20Bayes%20consistent%20in%20the%20sense%20that%20their%20performance%0Aconverges%20to%20the%20optimal%20performance%20w.r.t.%20the%20clean%20%28non-noisy%29%20distribution.%0AOur%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20algorithms%20in%20handling%0Alabel%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01055v2&entry.124074799=Read"},
{"title": "Controller-Guided Partial Label Consistency Regularization with\n  Unlabeled Data", "author": "Qian-Wei Wang and Bowen Zhao and Mingyan Zhu and Tianxiang Li and Zimo Liu and Shu-Tao Xia", "abstract": "  Partial label learning (PLL) learns from training examples each associated\nwith multiple candidate labels, among which only one is valid. In recent years,\nbenefiting from the strong capability of dealing with ambiguous supervision and\nthe impetus of modern data augmentation methods, consistency\nregularization-based PLL methods have achieved a series of successes and become\nmainstream. However, as the partial annotation becomes insufficient, their\nperformances drop significantly. In this paper, we leverage easily accessible\nunlabeled examples to facilitate the partial label consistency regularization.\nIn addition to a partial supervised loss, our method performs a\ncontroller-guided consistency regularization at both the label-level and\nrepresentation-level with the help of unlabeled data. To minimize the\ndisadvantages of insufficient capabilities of the initial supervised model, we\nuse the controller to estimate the confidence of each current prediction to\nguide the subsequent consistency regularization. Furthermore, we dynamically\nadjust the confidence thresholds so that the number of samples of each class\nparticipating in consistency regularization remains roughly equal to alleviate\nthe problem of class-imbalance. Experiments show that our method achieves\nsatisfactory performances in more practical situations, and its modules can be\napplied to existing PLL methods to enhance their capabilities.\n", "link": "http://arxiv.org/abs/2210.11194v4", "date": "2024-02-27", "relevancy": 1.9732, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5449}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4868}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4792}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controller-Guided%20Partial%20Label%20Consistency%20Regularization%20with%0A%20%20Unlabeled%20Data&entry.906535625=Qian-Wei%20Wang%20and%20Bowen%20Zhao%20and%20Mingyan%20Zhu%20and%20Tianxiang%20Li%20and%20Zimo%20Liu%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Partial%20label%20learning%20%28PLL%29%20learns%20from%20training%20examples%20each%20associated%0Awith%20multiple%20candidate%20labels%2C%20among%20which%20only%20one%20is%20valid.%20In%20recent%20years%2C%0Abenefiting%20from%20the%20strong%20capability%20of%20dealing%20with%20ambiguous%20supervision%20and%0Athe%20impetus%20of%20modern%20data%20augmentation%20methods%2C%20consistency%0Aregularization-based%20PLL%20methods%20have%20achieved%20a%20series%20of%20successes%20and%20become%0Amainstream.%20However%2C%20as%20the%20partial%20annotation%20becomes%20insufficient%2C%20their%0Aperformances%20drop%20significantly.%20In%20this%20paper%2C%20we%20leverage%20easily%20accessible%0Aunlabeled%20examples%20to%20facilitate%20the%20partial%20label%20consistency%20regularization.%0AIn%20addition%20to%20a%20partial%20supervised%20loss%2C%20our%20method%20performs%20a%0Acontroller-guided%20consistency%20regularization%20at%20both%20the%20label-level%20and%0Arepresentation-level%20with%20the%20help%20of%20unlabeled%20data.%20To%20minimize%20the%0Adisadvantages%20of%20insufficient%20capabilities%20of%20the%20initial%20supervised%20model%2C%20we%0Ause%20the%20controller%20to%20estimate%20the%20confidence%20of%20each%20current%20prediction%20to%0Aguide%20the%20subsequent%20consistency%20regularization.%20Furthermore%2C%20we%20dynamically%0Aadjust%20the%20confidence%20thresholds%20so%20that%20the%20number%20of%20samples%20of%20each%20class%0Aparticipating%20in%20consistency%20regularization%20remains%20roughly%20equal%20to%20alleviate%0Athe%20problem%20of%20class-imbalance.%20Experiments%20show%20that%20our%20method%20achieves%0Asatisfactory%20performances%20in%20more%20practical%20situations%2C%20and%20its%20modules%20can%20be%0Aapplied%20to%20existing%20PLL%20methods%20to%20enhance%20their%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.11194v4&entry.124074799=Read"},
{"title": "Principled Architecture-aware Scaling of Hyperparameters", "author": "Wuyang Chen and Junru Wu and Zhangyang Wang and Boris Hanin", "abstract": "  Training a high-quality deep neural network requires choosing suitable\nhyperparameters, which is a non-trivial and expensive process. Current works\ntry to automatically optimize or design principles of hyperparameters, such\nthat they can generalize to diverse unseen scenarios. However, most designs or\noptimization methods are agnostic to the choice of network structures, and thus\nlargely ignore the impact of neural architectures on hyperparameters. In this\nwork, we precisely characterize the dependence of initializations and maximal\nlearning rates on the network architecture, which includes the network depth,\nwidth, convolutional kernel size, and connectivity patterns. By pursuing every\nparameter to be maximally updated with the same mean squared change in\npre-activations, we can generalize our initialization and learning rates across\nMLPs (multi-layer perception) and CNNs (convolutional neural network) with\nsophisticated graph topologies. We verify our principles with comprehensive\nexperiments. More importantly, our strategy further sheds light on advancing\ncurrent benchmarks for architecture design. A fair comparison of AutoML\nalgorithms requires accurate network rankings. However, we demonstrate that\nnetwork rankings can be easily changed by better training networks in\nbenchmarks with our architecture-aware learning rates and initialization.\n", "link": "http://arxiv.org/abs/2402.17440v1", "date": "2024-02-27", "relevancy": 1.9691, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5202}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4769}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4705}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Principled%20Architecture-aware%20Scaling%20of%20Hyperparameters&entry.906535625=Wuyang%20Chen%20and%20Junru%20Wu%20and%20Zhangyang%20Wang%20and%20Boris%20Hanin&entry.1292438233=%20%20Training%20a%20high-quality%20deep%20neural%20network%20requires%20choosing%20suitable%0Ahyperparameters%2C%20which%20is%20a%20non-trivial%20and%20expensive%20process.%20Current%20works%0Atry%20to%20automatically%20optimize%20or%20design%20principles%20of%20hyperparameters%2C%20such%0Athat%20they%20can%20generalize%20to%20diverse%20unseen%20scenarios.%20However%2C%20most%20designs%20or%0Aoptimization%20methods%20are%20agnostic%20to%20the%20choice%20of%20network%20structures%2C%20and%20thus%0Alargely%20ignore%20the%20impact%20of%20neural%20architectures%20on%20hyperparameters.%20In%20this%0Awork%2C%20we%20precisely%20characterize%20the%20dependence%20of%20initializations%20and%20maximal%0Alearning%20rates%20on%20the%20network%20architecture%2C%20which%20includes%20the%20network%20depth%2C%0Awidth%2C%20convolutional%20kernel%20size%2C%20and%20connectivity%20patterns.%20By%20pursuing%20every%0Aparameter%20to%20be%20maximally%20updated%20with%20the%20same%20mean%20squared%20change%20in%0Apre-activations%2C%20we%20can%20generalize%20our%20initialization%20and%20learning%20rates%20across%0AMLPs%20%28multi-layer%20perception%29%20and%20CNNs%20%28convolutional%20neural%20network%29%20with%0Asophisticated%20graph%20topologies.%20We%20verify%20our%20principles%20with%20comprehensive%0Aexperiments.%20More%20importantly%2C%20our%20strategy%20further%20sheds%20light%20on%20advancing%0Acurrent%20benchmarks%20for%20architecture%20design.%20A%20fair%20comparison%20of%20AutoML%0Aalgorithms%20requires%20accurate%20network%20rankings.%20However%2C%20we%20demonstrate%20that%0Anetwork%20rankings%20can%20be%20easily%20changed%20by%20better%20training%20networks%20in%0Abenchmarks%20with%20our%20architecture-aware%20learning%20rates%20and%20initialization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17440v1&entry.124074799=Read"},
{"title": "PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive\n  Representation Learning", "author": "Xiaoyu Liu and Beitong Zhou and Cheng Cheng", "abstract": "  Recently, the application of Contrastive Representation Learning (CRL) in\nlearning with noisy labels (LNL) has shown promising advancements due to its\nremarkable ability to learn well-distributed representations for better\ndistinguishing noisy labels. However, CRL is mainly used as a pre-training\ntechnique, leading to a complicated multi-stage training pipeline. We also\nobserved that trivially combining CRL with supervised LNL methods decreases\nperformance. Using different images from the same class as negative pairs in\nCRL creates optimization conflicts between CRL and the supervised loss. To\naddress these two issues, we propose an end-to-end PLReMix framework that\navoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR)\ncontrastive loss to alleviate the conflicts between losses. This PLR loss\nconstructs a reliable negative set of each sample by filtering out its\ninappropriate negative pairs that overlap at the top k indices of prediction\nprobabilities, leading to more compact semantic clusters than vanilla CRL.\nFurthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to\ndistinguish clean and noisy samples by leveraging semantic information and\nmodel outputs simultaneously, which is expanded on the previously widely used\none-dimensional form. The PLR loss and a semi-supervised loss are\nsimultaneously applied to train on the GMM divided clean and noisy samples.\nExperiments on multiple benchmark datasets demonstrate the effectiveness of the\nproposed method. Our proposed PLR loss is scalable, which can be easily\nintegrated into other LNL methods and boost their performance. Codes will be\navailable.\n", "link": "http://arxiv.org/abs/2402.17589v1", "date": "2024-02-27", "relevancy": 1.9605, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5024}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4984}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLReMix%3A%20Combating%20Noisy%20Labels%20with%20Pseudo-Label%20Relaxed%20Contrastive%0A%20%20Representation%20Learning&entry.906535625=Xiaoyu%20Liu%20and%20Beitong%20Zhou%20and%20Cheng%20Cheng&entry.1292438233=%20%20Recently%2C%20the%20application%20of%20Contrastive%20Representation%20Learning%20%28CRL%29%20in%0Alearning%20with%20noisy%20labels%20%28LNL%29%20has%20shown%20promising%20advancements%20due%20to%20its%0Aremarkable%20ability%20to%20learn%20well-distributed%20representations%20for%20better%0Adistinguishing%20noisy%20labels.%20However%2C%20CRL%20is%20mainly%20used%20as%20a%20pre-training%0Atechnique%2C%20leading%20to%20a%20complicated%20multi-stage%20training%20pipeline.%20We%20also%0Aobserved%20that%20trivially%20combining%20CRL%20with%20supervised%20LNL%20methods%20decreases%0Aperformance.%20Using%20different%20images%20from%20the%20same%20class%20as%20negative%20pairs%20in%0ACRL%20creates%20optimization%20conflicts%20between%20CRL%20and%20the%20supervised%20loss.%20To%0Aaddress%20these%20two%20issues%2C%20we%20propose%20an%20end-to-end%20PLReMix%20framework%20that%0Aavoids%20the%20complicated%20pipeline%20by%20introducing%20a%20Pseudo-Label%20Relaxed%20%28PLR%29%0Acontrastive%20loss%20to%20alleviate%20the%20conflicts%20between%20losses.%20This%20PLR%20loss%0Aconstructs%20a%20reliable%20negative%20set%20of%20each%20sample%20by%20filtering%20out%20its%0Ainappropriate%20negative%20pairs%20that%20overlap%20at%20the%20top%20k%20indices%20of%20prediction%0Aprobabilities%2C%20leading%20to%20more%20compact%20semantic%20clusters%20than%20vanilla%20CRL.%0AFurthermore%2C%20a%20two-dimensional%20Gaussian%20Mixture%20Model%20%28GMM%29%20is%20adopted%20to%0Adistinguish%20clean%20and%20noisy%20samples%20by%20leveraging%20semantic%20information%20and%0Amodel%20outputs%20simultaneously%2C%20which%20is%20expanded%20on%20the%20previously%20widely%20used%0Aone-dimensional%20form.%20The%20PLR%20loss%20and%20a%20semi-supervised%20loss%20are%0Asimultaneously%20applied%20to%20train%20on%20the%20GMM%20divided%20clean%20and%20noisy%20samples.%0AExperiments%20on%20multiple%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method.%20Our%20proposed%20PLR%20loss%20is%20scalable%2C%20which%20can%20be%20easily%0Aintegrated%20into%20other%20LNL%20methods%20and%20boost%20their%20performance.%20Codes%20will%20be%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17589v1&entry.124074799=Read"},
{"title": "Autonomous Vehicles: Evolution of Artificial Intelligence and Learning\n  Algorithms", "author": "Sneha Sudhir Shetiya and Divya Garikapati", "abstract": "  The advent of autonomous vehicles has heralded a transformative era in\ntransportation, reshaping the landscape of mobility through cutting-edge\ntechnologies. Central to this evolu- tion is the integration of Artificial\nIntelligence (AI) and learning algorithms, propelling vehicles into realms of\nunprecedented autonomy. This paper provides a comprehensive exploration of the\nevolutionary trajectory of AI within autonomous vehicles, tracing the journey\nfrom foundational principles to the most recent advancements. Commencing with a\ncurrent landscape overview, the paper delves into the fundamental role of AI in\nshaping the autonomous decision-making capabilities of vehicles. It elucidates\nthe steps involved in the AI-powered development life cycle in vehicles,\naddressing ethical considerations and bias in AI-driven software development\nfor autonomous vehicles. The study presents statis- tical insights into the\nusage and types of AI/learning algorithms over the years, showcasing the\nevolving research landscape within the automotive industry. Furthermore, the\npaper highlights the pivotal role of parameters in refining algorithms for both\ntrucks and cars, facilitating vehicles to adapt, learn, and improve performance\nover time. It concludes by outlining different levels of autonomy, elucidating\nthe nuanced usage of AI and learning algorithms, and automating key tasks at\neach level. Additionally, the document discusses the variation in software\npackage sizes across different autonomy levels\n", "link": "http://arxiv.org/abs/2402.17690v1", "date": "2024-02-27", "relevancy": 1.9593, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4813}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4636}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Vehicles%3A%20Evolution%20of%20Artificial%20Intelligence%20and%20Learning%0A%20%20Algorithms&entry.906535625=Sneha%20Sudhir%20Shetiya%20and%20Divya%20Garikapati&entry.1292438233=%20%20The%20advent%20of%20autonomous%20vehicles%20has%20heralded%20a%20transformative%20era%20in%0Atransportation%2C%20reshaping%20the%20landscape%20of%20mobility%20through%20cutting-edge%0Atechnologies.%20Central%20to%20this%20evolu-%20tion%20is%20the%20integration%20of%20Artificial%0AIntelligence%20%28AI%29%20and%20learning%20algorithms%2C%20propelling%20vehicles%20into%20realms%20of%0Aunprecedented%20autonomy.%20This%20paper%20provides%20a%20comprehensive%20exploration%20of%20the%0Aevolutionary%20trajectory%20of%20AI%20within%20autonomous%20vehicles%2C%20tracing%20the%20journey%0Afrom%20foundational%20principles%20to%20the%20most%20recent%20advancements.%20Commencing%20with%20a%0Acurrent%20landscape%20overview%2C%20the%20paper%20delves%20into%20the%20fundamental%20role%20of%20AI%20in%0Ashaping%20the%20autonomous%20decision-making%20capabilities%20of%20vehicles.%20It%20elucidates%0Athe%20steps%20involved%20in%20the%20AI-powered%20development%20life%20cycle%20in%20vehicles%2C%0Aaddressing%20ethical%20considerations%20and%20bias%20in%20AI-driven%20software%20development%0Afor%20autonomous%20vehicles.%20The%20study%20presents%20statis-%20tical%20insights%20into%20the%0Ausage%20and%20types%20of%20AI/learning%20algorithms%20over%20the%20years%2C%20showcasing%20the%0Aevolving%20research%20landscape%20within%20the%20automotive%20industry.%20Furthermore%2C%20the%0Apaper%20highlights%20the%20pivotal%20role%20of%20parameters%20in%20refining%20algorithms%20for%20both%0Atrucks%20and%20cars%2C%20facilitating%20vehicles%20to%20adapt%2C%20learn%2C%20and%20improve%20performance%0Aover%20time.%20It%20concludes%20by%20outlining%20different%20levels%20of%20autonomy%2C%20elucidating%0Athe%20nuanced%20usage%20of%20AI%20and%20learning%20algorithms%2C%20and%20automating%20key%20tasks%20at%0Aeach%20level.%20Additionally%2C%20the%20document%20discusses%20the%20variation%20in%20software%0Apackage%20sizes%20across%20different%20autonomy%20levels%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17690v1&entry.124074799=Read"},
{"title": "Learning Topological Representations with Bidirectional Graph Attention\n  Network for Solving Job Shop Scheduling Problem", "author": "Cong Zhang and Zhiguang Cao and Yaoxin Wu and Wen Song and Jing Sun", "abstract": "  Existing learning-based methods for solving job shop scheduling problem\n(JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and\nneglect the rich and meaningful topological structures of disjunctive graphs\n(DGs). This paper proposes the topology-aware bidirectional graph attention\nnetwork (TBGAT), a novel GNN architecture based on the attention mechanism, to\nembed the DG for solving JSSP in a local search framework. Specifically, TBGAT\nembeds the DG from a forward and a backward view, respectively, where the\nmessages are propagated by following the different topologies of the views and\naggregated via graph attention. Then, we propose a novel operator based on the\nmessage-passing mechanism to calculate the forward and backward topological\nsorts of the DG, which are the features for characterizing the topological\nstructures and exploited by our model. In addition, we theoretically and\nexperimentally show that TBGAT has linear computational complexity to the\nnumber of jobs and machines, respectively, which strengthens the practical\nvalue of our method. Besides, extensive experiments on five synthetic datasets\nand seven classic benchmarks show that TBGAT achieves new SOTA results by\noutperforming a wide range of neural methods by a large margin.\n", "link": "http://arxiv.org/abs/2402.17606v1", "date": "2024-02-27", "relevancy": 1.9568, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5012}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4953}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4783}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Topological%20Representations%20with%20Bidirectional%20Graph%20Attention%0A%20%20Network%20for%20Solving%20Job%20Shop%20Scheduling%20Problem&entry.906535625=Cong%20Zhang%20and%20Zhiguang%20Cao%20and%20Yaoxin%20Wu%20and%20Wen%20Song%20and%20Jing%20Sun&entry.1292438233=%20%20Existing%20learning-based%20methods%20for%20solving%20job%20shop%20scheduling%20problem%0A%28JSSP%29%20usually%20use%20off-the-shelf%20GNN%20models%20tailored%20to%20undirected%20graphs%20and%0Aneglect%20the%20rich%20and%20meaningful%20topological%20structures%20of%20disjunctive%20graphs%0A%28DGs%29.%20This%20paper%20proposes%20the%20topology-aware%20bidirectional%20graph%20attention%0Anetwork%20%28TBGAT%29%2C%20a%20novel%20GNN%20architecture%20based%20on%20the%20attention%20mechanism%2C%20to%0Aembed%20the%20DG%20for%20solving%20JSSP%20in%20a%20local%20search%20framework.%20Specifically%2C%20TBGAT%0Aembeds%20the%20DG%20from%20a%20forward%20and%20a%20backward%20view%2C%20respectively%2C%20where%20the%0Amessages%20are%20propagated%20by%20following%20the%20different%20topologies%20of%20the%20views%20and%0Aaggregated%20via%20graph%20attention.%20Then%2C%20we%20propose%20a%20novel%20operator%20based%20on%20the%0Amessage-passing%20mechanism%20to%20calculate%20the%20forward%20and%20backward%20topological%0Asorts%20of%20the%20DG%2C%20which%20are%20the%20features%20for%20characterizing%20the%20topological%0Astructures%20and%20exploited%20by%20our%20model.%20In%20addition%2C%20we%20theoretically%20and%0Aexperimentally%20show%20that%20TBGAT%20has%20linear%20computational%20complexity%20to%20the%0Anumber%20of%20jobs%20and%20machines%2C%20respectively%2C%20which%20strengthens%20the%20practical%0Avalue%20of%20our%20method.%20Besides%2C%20extensive%20experiments%20on%20five%20synthetic%20datasets%0Aand%20seven%20classic%20benchmarks%20show%20that%20TBGAT%20achieves%20new%20SOTA%20results%20by%0Aoutperforming%20a%20wide%20range%20of%20neural%20methods%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17606v1&entry.124074799=Read"},
{"title": "GmGM: a Fast Multi-Axis Gaussian Graphical Model", "author": "Bailey Andrew and David Westhead and Luisa Cutillo", "abstract": "  This paper introduces the Gaussian multi-Graphical Model, a model to\nconstruct sparse graph representations of matrix- and tensor-variate data. We\ngeneralize prior work in this area by simultaneously learning this\nrepresentation across several tensors that share axes, which is necessary to\nallow the analysis of multimodal datasets such as those encountered in\nmulti-omics. Our algorithm uses only a single eigendecomposition per axis,\nachieving an order of magnitude speedup over prior work in the ungeneralized\ncase. This allows the use of our methodology on large multi-modal datasets such\nas single-cell multi-omics data, which was challenging with previous\napproaches. We validate our model on synthetic data and five real-world\ndatasets.\n", "link": "http://arxiv.org/abs/2211.02920v3", "date": "2024-02-27", "relevancy": 1.917, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5155}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4788}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4431}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GmGM%3A%20a%20Fast%20Multi-Axis%20Gaussian%20Graphical%20Model&entry.906535625=Bailey%20Andrew%20and%20David%20Westhead%20and%20Luisa%20Cutillo&entry.1292438233=%20%20This%20paper%20introduces%20the%20Gaussian%20multi-Graphical%20Model%2C%20a%20model%20to%0Aconstruct%20sparse%20graph%20representations%20of%20matrix-%20and%20tensor-variate%20data.%20We%0Ageneralize%20prior%20work%20in%20this%20area%20by%20simultaneously%20learning%20this%0Arepresentation%20across%20several%20tensors%20that%20share%20axes%2C%20which%20is%20necessary%20to%0Aallow%20the%20analysis%20of%20multimodal%20datasets%20such%20as%20those%20encountered%20in%0Amulti-omics.%20Our%20algorithm%20uses%20only%20a%20single%20eigendecomposition%20per%20axis%2C%0Aachieving%20an%20order%20of%20magnitude%20speedup%20over%20prior%20work%20in%20the%20ungeneralized%0Acase.%20This%20allows%20the%20use%20of%20our%20methodology%20on%20large%20multi-modal%20datasets%20such%0Aas%20single-cell%20multi-omics%20data%2C%20which%20was%20challenging%20with%20previous%0Aapproaches.%20We%20validate%20our%20model%20on%20synthetic%20data%20and%20five%20real-world%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.02920v3&entry.124074799=Read"},
{"title": "SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation", "author": "Bin Xie and Jiale Cao and Jin Xie and Fahad Shahbaz Khan and Yanwei Pang", "abstract": "  Open-vocabulary semantic segmentation strives to distinguish pixels into\ndifferent semantic groups from an open set of categories. Most existing methods\nexplore utilizing pre-trained vision-language models, in which the key is to\nadopt the image-level model for pixel-level segmentation task. In this paper,\nwe propose a simple encoder-decoder, named SED, for open-vocabulary semantic\nsegmentation, which comprises a hierarchical encoder-based cost map generation\nand a gradual fusion decoder with category early rejection. The hierarchical\nencoder-based cost map generation employs hierarchical backbone, instead of\nplain transformer, to predict pixel-level image-text cost map. Compared to\nplain transformer, hierarchical backbone better captures local spatial\ninformation and has linear computational complexity with respect to input size.\nOur gradual fusion decoder employs a top-down structure to combine cost map and\nthe feature maps of different backbone levels for segmentation. To accelerate\ninference speed, we introduce a category early rejection scheme in the decoder\nthat rejects many no-existing categories at the early layer of decoder,\nresulting in at most 4.7 times acceleration without accuracy degradation.\nExperiments are performed on multiple open-vocabulary semantic segmentation\ndatasets, which demonstrates the efficacy of our SED method. When using\nConvNeXt-B, our SED method achieves mIoU score of 31.6\\% on ADE20K with 150\ncategories at 82 millisecond ($ms$) per image on a single A6000. We will\nrelease it at \\url{https://github.com/xb534/SED.git}.\n", "link": "http://arxiv.org/abs/2311.15537v2", "date": "2024-02-27", "relevancy": 1.5775, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5308}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5278}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5159}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SED%3A%20A%20Simple%20Encoder-Decoder%20for%20Open-Vocabulary%20Semantic%20Segmentation&entry.906535625=Bin%20Xie%20and%20Jiale%20Cao%20and%20Jin%20Xie%20and%20Fahad%20Shahbaz%20Khan%20and%20Yanwei%20Pang&entry.1292438233=%20%20Open-vocabulary%20semantic%20segmentation%20strives%20to%20distinguish%20pixels%20into%0Adifferent%20semantic%20groups%20from%20an%20open%20set%20of%20categories.%20Most%20existing%20methods%0Aexplore%20utilizing%20pre-trained%20vision-language%20models%2C%20in%20which%20the%20key%20is%20to%0Aadopt%20the%20image-level%20model%20for%20pixel-level%20segmentation%20task.%20In%20this%20paper%2C%0Awe%20propose%20a%20simple%20encoder-decoder%2C%20named%20SED%2C%20for%20open-vocabulary%20semantic%0Asegmentation%2C%20which%20comprises%20a%20hierarchical%20encoder-based%20cost%20map%20generation%0Aand%20a%20gradual%20fusion%20decoder%20with%20category%20early%20rejection.%20The%20hierarchical%0Aencoder-based%20cost%20map%20generation%20employs%20hierarchical%20backbone%2C%20instead%20of%0Aplain%20transformer%2C%20to%20predict%20pixel-level%20image-text%20cost%20map.%20Compared%20to%0Aplain%20transformer%2C%20hierarchical%20backbone%20better%20captures%20local%20spatial%0Ainformation%20and%20has%20linear%20computational%20complexity%20with%20respect%20to%20input%20size.%0AOur%20gradual%20fusion%20decoder%20employs%20a%20top-down%20structure%20to%20combine%20cost%20map%20and%0Athe%20feature%20maps%20of%20different%20backbone%20levels%20for%20segmentation.%20To%20accelerate%0Ainference%20speed%2C%20we%20introduce%20a%20category%20early%20rejection%20scheme%20in%20the%20decoder%0Athat%20rejects%20many%20no-existing%20categories%20at%20the%20early%20layer%20of%20decoder%2C%0Aresulting%20in%20at%20most%204.7%20times%20acceleration%20without%20accuracy%20degradation.%0AExperiments%20are%20performed%20on%20multiple%20open-vocabulary%20semantic%20segmentation%0Adatasets%2C%20which%20demonstrates%20the%20efficacy%20of%20our%20SED%20method.%20When%20using%0AConvNeXt-B%2C%20our%20SED%20method%20achieves%20mIoU%20score%20of%2031.6%5C%25%20on%20ADE20K%20with%20150%0Acategories%20at%2082%20millisecond%20%28%24ms%24%29%20per%20image%20on%20a%20single%20A6000.%20We%20will%0Arelease%20it%20at%20%5Curl%7Bhttps%3A//github.com/xb534/SED.git%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15537v2&entry.124074799=Read"},
{"title": "The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning\n  with Kandinsky Patterns", "author": "Luca Salvatore Lorello and Marco Lippi and Stefano Melacci", "abstract": "  Artificial intelligence is continuously seeking novel challenges and\nbenchmarks to effectively measure performance and to advance the\nstate-of-the-art. In this paper we introduce KANDY, a benchmarking framework\nthat can be used to generate a variety of learning and reasoning tasks inspired\nby Kandinsky patterns. By creating curricula of binary classification tasks\nwith increasing complexity and with sparse supervisions, KANDY can be used to\nimplement benchmarks for continual and semi-supervised learning, with a\nspecific focus on symbol compositionality. Classification rules are also\nprovided in the ground truth to enable analysis of interpretable solutions.\nTogether with the benchmark generation pipeline, we release two curricula, an\neasier and a harder one, that we propose as new challenges for the research\ncommunity. With a thorough experimental evaluation, we show how both\nstate-of-the-art neural models and purely symbolic approaches struggle with\nsolving most of the tasks, thus calling for the application of advanced\nneuro-symbolic methods trained over time.\n", "link": "http://arxiv.org/abs/2402.17431v1", "date": "2024-02-27", "relevancy": 1.4034, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4803}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4662}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4635}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20KANDY%20Benchmark%3A%20Incremental%20Neuro-Symbolic%20Learning%20and%20Reasoning%0A%20%20with%20Kandinsky%20Patterns&entry.906535625=Luca%20Salvatore%20Lorello%20and%20Marco%20Lippi%20and%20Stefano%20Melacci&entry.1292438233=%20%20Artificial%20intelligence%20is%20continuously%20seeking%20novel%20challenges%20and%0Abenchmarks%20to%20effectively%20measure%20performance%20and%20to%20advance%20the%0Astate-of-the-art.%20In%20this%20paper%20we%20introduce%20KANDY%2C%20a%20benchmarking%20framework%0Athat%20can%20be%20used%20to%20generate%20a%20variety%20of%20learning%20and%20reasoning%20tasks%20inspired%0Aby%20Kandinsky%20patterns.%20By%20creating%20curricula%20of%20binary%20classification%20tasks%0Awith%20increasing%20complexity%20and%20with%20sparse%20supervisions%2C%20KANDY%20can%20be%20used%20to%0Aimplement%20benchmarks%20for%20continual%20and%20semi-supervised%20learning%2C%20with%20a%0Aspecific%20focus%20on%20symbol%20compositionality.%20Classification%20rules%20are%20also%0Aprovided%20in%20the%20ground%20truth%20to%20enable%20analysis%20of%20interpretable%20solutions.%0ATogether%20with%20the%20benchmark%20generation%20pipeline%2C%20we%20release%20two%20curricula%2C%20an%0Aeasier%20and%20a%20harder%20one%2C%20that%20we%20propose%20as%20new%20challenges%20for%20the%20research%0Acommunity.%20With%20a%20thorough%20experimental%20evaluation%2C%20we%20show%20how%20both%0Astate-of-the-art%20neural%20models%20and%20purely%20symbolic%20approaches%20struggle%20with%0Asolving%20most%20of%20the%20tasks%2C%20thus%20calling%20for%20the%20application%20of%20advanced%0Aneuro-symbolic%20methods%20trained%20over%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17431v1&entry.124074799=Read"},
{"title": "Deep Learning Based Named Entity Recognition Models for Recipes", "author": "Mansi Goel and Ayush Agarwal and Shubham Agrawal and Janak Kapuriya and Akhil Vamshi Konam and Rishabh Gupta and Shrey Rastogi and  Niharika and Ganesh Bagler", "abstract": "  Food touches our lives through various endeavors, including flavor,\nnourishment, health, and sustainability. Recipes are cultural capsules\ntransmitted across generations via unstructured text. Automated protocols for\nrecognizing named entities, the building blocks of recipe text, are of immense\nvalue for various applications ranging from information extraction to novel\nrecipe generation. Named entity recognition is a technique for extracting\ninformation from unstructured or semi-structured data with known labels.\nStarting with manually-annotated data of 6,611 ingredient phrases, we created\nan augmented dataset of 26,445 phrases cumulatively. Simultaneously, we\nsystematically cleaned and analyzed ingredient phrases from RecipeDB, the\ngold-standard recipe data repository, and annotated them using the Stanford\nNER. Based on the analysis, we sampled a subset of 88,526 phrases using a\nclustering-based approach while preserving the diversity to create the\nmachine-annotated dataset. A thorough investigation of NER approaches on these\nthree datasets involving statistical, fine-tuning of deep learning-based\nlanguage models and few-shot prompting on large language models (LLMs) provides\ndeep insights. We conclude that few-shot prompting on LLMs has abysmal\nperformance, whereas the fine-tuned spaCy-transformer emerges as the best model\nwith macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,\naugmented, and machine-annotated datasets, respectively.\n", "link": "http://arxiv.org/abs/2402.17447v1", "date": "2024-02-27", "relevancy": 1.7807, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4665}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4524}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4209}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Based%20Named%20Entity%20Recognition%20Models%20for%20Recipes&entry.906535625=Mansi%20Goel%20and%20Ayush%20Agarwal%20and%20Shubham%20Agrawal%20and%20Janak%20Kapuriya%20and%20Akhil%20Vamshi%20Konam%20and%20Rishabh%20Gupta%20and%20Shrey%20Rastogi%20and%20%20Niharika%20and%20Ganesh%20Bagler&entry.1292438233=%20%20Food%20touches%20our%20lives%20through%20various%20endeavors%2C%20including%20flavor%2C%0Anourishment%2C%20health%2C%20and%20sustainability.%20Recipes%20are%20cultural%20capsules%0Atransmitted%20across%20generations%20via%20unstructured%20text.%20Automated%20protocols%20for%0Arecognizing%20named%20entities%2C%20the%20building%20blocks%20of%20recipe%20text%2C%20are%20of%20immense%0Avalue%20for%20various%20applications%20ranging%20from%20information%20extraction%20to%20novel%0Arecipe%20generation.%20Named%20entity%20recognition%20is%20a%20technique%20for%20extracting%0Ainformation%20from%20unstructured%20or%20semi-structured%20data%20with%20known%20labels.%0AStarting%20with%20manually-annotated%20data%20of%206%2C611%20ingredient%20phrases%2C%20we%20created%0Aan%20augmented%20dataset%20of%2026%2C445%20phrases%20cumulatively.%20Simultaneously%2C%20we%0Asystematically%20cleaned%20and%20analyzed%20ingredient%20phrases%20from%20RecipeDB%2C%20the%0Agold-standard%20recipe%20data%20repository%2C%20and%20annotated%20them%20using%20the%20Stanford%0ANER.%20Based%20on%20the%20analysis%2C%20we%20sampled%20a%20subset%20of%2088%2C526%20phrases%20using%20a%0Aclustering-based%20approach%20while%20preserving%20the%20diversity%20to%20create%20the%0Amachine-annotated%20dataset.%20A%20thorough%20investigation%20of%20NER%20approaches%20on%20these%0Athree%20datasets%20involving%20statistical%2C%20fine-tuning%20of%20deep%20learning-based%0Alanguage%20models%20and%20few-shot%20prompting%20on%20large%20language%20models%20%28LLMs%29%20provides%0Adeep%20insights.%20We%20conclude%20that%20few-shot%20prompting%20on%20LLMs%20has%20abysmal%0Aperformance%2C%20whereas%20the%20fine-tuned%20spaCy-transformer%20emerges%20as%20the%20best%20model%0Awith%20macro-F1%20scores%20of%2095.9%25%2C%2096.04%25%2C%20and%2095.71%25%20for%20the%20manually-annotated%2C%0Aaugmented%2C%20and%20machine-annotated%20datasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17447v1&entry.124074799=Read"},
{"title": "Variational Learning is Effective for Large Deep Networks", "author": "Yuesong Shen and Nico Daheim and Bai Cong and Peter Nickl and Gian Maria Marconi and Clement Bazan and Rio Yokota and Iryna Gurevych and Daniel Cremers and Mohammad Emtiyaz Khan and Thomas M\u00f6llenhoff", "abstract": "  We give extensive empirical evidence against the common belief that\nvariational learning is ineffective for large neural networks. We show that an\noptimizer called Improved Variational Online Newton (IVON) consistently matches\nor outperforms Adam for training large networks such as GPT-2 and ResNets from\nscratch. IVON's computational costs are nearly identical to Adam but its\npredictive uncertainty is better. We show several new use cases of IVON where\nwe improve fine-tuning and model merging in Large Language Models, accurately\npredict generalization error, and faithfully estimate sensitivity to data. We\nfind overwhelming evidence in support of effectiveness of variational learning.\n", "link": "http://arxiv.org/abs/2402.17641v1", "date": "2024-02-27", "relevancy": 1.4281, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5084}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4326}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Learning%20is%20Effective%20for%20Large%20Deep%20Networks&entry.906535625=Yuesong%20Shen%20and%20Nico%20Daheim%20and%20Bai%20Cong%20and%20Peter%20Nickl%20and%20Gian%20Maria%20Marconi%20and%20Clement%20Bazan%20and%20Rio%20Yokota%20and%20Iryna%20Gurevych%20and%20Daniel%20Cremers%20and%20Mohammad%20Emtiyaz%20Khan%20and%20Thomas%20M%C3%B6llenhoff&entry.1292438233=%20%20We%20give%20extensive%20empirical%20evidence%20against%20the%20common%20belief%20that%0Avariational%20learning%20is%20ineffective%20for%20large%20neural%20networks.%20We%20show%20that%20an%0Aoptimizer%20called%20Improved%20Variational%20Online%20Newton%20%28IVON%29%20consistently%20matches%0Aor%20outperforms%20Adam%20for%20training%20large%20networks%20such%20as%20GPT-2%20and%20ResNets%20from%0Ascratch.%20IVON%27s%20computational%20costs%20are%20nearly%20identical%20to%20Adam%20but%20its%0Apredictive%20uncertainty%20is%20better.%20We%20show%20several%20new%20use%20cases%20of%20IVON%20where%0Awe%20improve%20fine-tuning%20and%20model%20merging%20in%20Large%20Language%20Models%2C%20accurately%0Apredict%20generalization%20error%2C%20and%20faithfully%20estimate%20sensitivity%20to%20data.%20We%0Afind%20overwhelming%20evidence%20in%20support%20of%20effectiveness%20of%20variational%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17641v1&entry.124074799=Read"},
{"title": "Emotional Voice Messages (EMOVOME) database: emotion recognition in\n  spontaneous voice messages", "author": "Luc\u00eda G\u00f3mez Zaragoz\u00e1 and Roc\u00edo del Amor and Elena Parra Vargas and Valery Naranjo and Mariano Alca\u00f1iz Raya and Javier Mar\u00edn-Morales", "abstract": "  Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing\n999 audio messages from real conversations on a messaging app from 100 Spanish\nspeakers, gender balanced. Voice messages were produced in-the-wild conditions\nbefore participants were recruited, avoiding any conscious bias due to\nlaboratory environment. Audios were labeled in valence and arousal dimensions\nby three non-experts and two experts, which were then combined to obtain a\nfinal label per dimension. The experts also provided an extra label\ncorresponding to seven emotion categories. To set a baseline for future\ninvestigations using EMOVOME, we implemented emotion recognition models using\nboth speech and audio transcriptions. For speech, we used the standard eGeMAPS\nfeature set and support vector machines, obtaining 49.27% and 44.71% unweighted\naccuracy for valence and arousal respectively. For text, we fine-tuned a\nmultilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for\nvalence and arousal respectively. This database will significantly contribute\nto research on emotion recognition in the wild, while also providing a unique\nnatural and freely accessible resource for Spanish.\n", "link": "http://arxiv.org/abs/2402.17496v1", "date": "2024-02-27", "relevancy": 1.2032, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4198}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4063}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3915}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emotional%20Voice%20Messages%20%28EMOVOME%29%20database%3A%20emotion%20recognition%20in%0A%20%20spontaneous%20voice%20messages&entry.906535625=Luc%C3%ADa%20G%C3%B3mez%20Zaragoz%C3%A1%20and%20Roc%C3%ADo%20del%20Amor%20and%20Elena%20Parra%20Vargas%20and%20Valery%20Naranjo%20and%20Mariano%20Alca%C3%B1iz%20Raya%20and%20Javier%20Mar%C3%ADn-Morales&entry.1292438233=%20%20Emotional%20Voice%20Messages%20%28EMOVOME%29%20is%20a%20spontaneous%20speech%20dataset%20containing%0A999%20audio%20messages%20from%20real%20conversations%20on%20a%20messaging%20app%20from%20100%20Spanish%0Aspeakers%2C%20gender%20balanced.%20Voice%20messages%20were%20produced%20in-the-wild%20conditions%0Abefore%20participants%20were%20recruited%2C%20avoiding%20any%20conscious%20bias%20due%20to%0Alaboratory%20environment.%20Audios%20were%20labeled%20in%20valence%20and%20arousal%20dimensions%0Aby%20three%20non-experts%20and%20two%20experts%2C%20which%20were%20then%20combined%20to%20obtain%20a%0Afinal%20label%20per%20dimension.%20The%20experts%20also%20provided%20an%20extra%20label%0Acorresponding%20to%20seven%20emotion%20categories.%20To%20set%20a%20baseline%20for%20future%0Ainvestigations%20using%20EMOVOME%2C%20we%20implemented%20emotion%20recognition%20models%20using%0Aboth%20speech%20and%20audio%20transcriptions.%20For%20speech%2C%20we%20used%20the%20standard%20eGeMAPS%0Afeature%20set%20and%20support%20vector%20machines%2C%20obtaining%2049.27%25%20and%2044.71%25%20unweighted%0Aaccuracy%20for%20valence%20and%20arousal%20respectively.%20For%20text%2C%20we%20fine-tuned%20a%0Amultilingual%20BERT%20model%20and%20achieved%2061.15%25%20and%2047.43%25%20unweighted%20accuracy%20for%0Avalence%20and%20arousal%20respectively.%20This%20database%20will%20significantly%20contribute%0Ato%20research%20on%20emotion%20recognition%20in%20the%20wild%2C%20while%20also%20providing%20a%20unique%0Anatural%20and%20freely%20accessible%20resource%20for%20Spanish.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17496v1&entry.124074799=Read"},
{"title": "Beacon, a lightweight deep reinforcement learning benchmark library for\n  flow control", "author": "Jonathan Viquerat and Philippe Meliga and Pablo Jeken and Elie Hachem", "abstract": "  Recently, the increasing use of deep reinforcement learning for flow control\nproblems has led to a new area of research, focused on the coupling and the\nadaptation of the existing algorithms to the control of numerical fluid\ndynamics environments. Although still in its infancy, the field has seen\nmultiple successes in a short time span, and its fast development pace can\ncertainly be partly imparted to the open-source effort that drives the\nexpansion of the community. Yet, this emerging domain still misses a common\nground to (i) ensure the reproducibility of the results, and (ii) offer a\nproper ad-hoc benchmarking basis. To this end, we propose Beacon, an\nopen-source benchmark library composed of seven lightweight 1D and 2D flow\ncontrol problems with various characteristics, action and observation space\ncharacteristics, and CPU requirements. In this contribution, the seven\nconsidered problems are described, and reference control solutions are\nprovided. The sources for the following work are available at\nhttps://github.com/jviquerat/beacon.\n", "link": "http://arxiv.org/abs/2402.17402v1", "date": "2024-02-27", "relevancy": 1.8456, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4739}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4731}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4442}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beacon%2C%20a%20lightweight%20deep%20reinforcement%20learning%20benchmark%20library%20for%0A%20%20flow%20control&entry.906535625=Jonathan%20Viquerat%20and%20Philippe%20Meliga%20and%20Pablo%20Jeken%20and%20Elie%20Hachem&entry.1292438233=%20%20Recently%2C%20the%20increasing%20use%20of%20deep%20reinforcement%20learning%20for%20flow%20control%0Aproblems%20has%20led%20to%20a%20new%20area%20of%20research%2C%20focused%20on%20the%20coupling%20and%20the%0Aadaptation%20of%20the%20existing%20algorithms%20to%20the%20control%20of%20numerical%20fluid%0Adynamics%20environments.%20Although%20still%20in%20its%20infancy%2C%20the%20field%20has%20seen%0Amultiple%20successes%20in%20a%20short%20time%20span%2C%20and%20its%20fast%20development%20pace%20can%0Acertainly%20be%20partly%20imparted%20to%20the%20open-source%20effort%20that%20drives%20the%0Aexpansion%20of%20the%20community.%20Yet%2C%20this%20emerging%20domain%20still%20misses%20a%20common%0Aground%20to%20%28i%29%20ensure%20the%20reproducibility%20of%20the%20results%2C%20and%20%28ii%29%20offer%20a%0Aproper%20ad-hoc%20benchmarking%20basis.%20To%20this%20end%2C%20we%20propose%20Beacon%2C%20an%0Aopen-source%20benchmark%20library%20composed%20of%20seven%20lightweight%201D%20and%202D%20flow%0Acontrol%20problems%20with%20various%20characteristics%2C%20action%20and%20observation%20space%0Acharacteristics%2C%20and%20CPU%20requirements.%20In%20this%20contribution%2C%20the%20seven%0Aconsidered%20problems%20are%20described%2C%20and%20reference%20control%20solutions%20are%0Aprovided.%20The%20sources%20for%20the%20following%20work%20are%20available%20at%0Ahttps%3A//github.com/jviquerat/beacon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17402v1&entry.124074799=Read"},
{"title": "Multi-level protein pre-training with Vabs-Net", "author": "Jiale Zhao and Wanru Zhuang and Jia Song and Yaqi Li and Shuqi Lu", "abstract": "  In recent years, there has been a surge in the development of 3D\nstructure-based pre-trained protein models, representing a significant\nadvancement over pre-trained protein language models in various downstream\ntasks. However, most existing structure-based pre-trained models primarily\nfocus on the residue level, i.e., alpha carbon atoms, while ignoring other\natoms like side chain atoms. We argue that modeling proteins at both residue\nand atom levels is important since the side chain atoms can also be crucial for\nnumerous downstream tasks, for example, molecular docking. Nevertheless, we\nfind that naively combining residue and atom information during pre-training\ntypically fails. We identify a key reason is the information leakage caused by\nthe inclusion of atom structure in the input, which renders residue-level\npre-training tasks trivial and results in insufficiently expressive residue\nrepresentations. To address this issue, we introduce a span mask pre-training\nstrategy on 3D protein chains to learn meaningful representations of both\nresidues and atoms. This leads to a simple yet effective approach to learning\nprotein representation suitable for diverse downstream tasks. Extensive\nexperimental results on binding site prediction and function prediction tasks\ndemonstrate our proposed pre-training approach significantly outperforms other\nmethods. Our code will be made public.\n", "link": "http://arxiv.org/abs/2402.01481v3", "date": "2024-02-27", "relevancy": 1.9486, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5002}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.488}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4525}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-level%20protein%20pre-training%20with%20Vabs-Net&entry.906535625=Jiale%20Zhao%20and%20Wanru%20Zhuang%20and%20Jia%20Song%20and%20Yaqi%20Li%20and%20Shuqi%20Lu&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20a%20surge%20in%20the%20development%20of%203D%0Astructure-based%20pre-trained%20protein%20models%2C%20representing%20a%20significant%0Aadvancement%20over%20pre-trained%20protein%20language%20models%20in%20various%20downstream%0Atasks.%20However%2C%20most%20existing%20structure-based%20pre-trained%20models%20primarily%0Afocus%20on%20the%20residue%20level%2C%20i.e.%2C%20alpha%20carbon%20atoms%2C%20while%20ignoring%20other%0Aatoms%20like%20side%20chain%20atoms.%20We%20argue%20that%20modeling%20proteins%20at%20both%20residue%0Aand%20atom%20levels%20is%20important%20since%20the%20side%20chain%20atoms%20can%20also%20be%20crucial%20for%0Anumerous%20downstream%20tasks%2C%20for%20example%2C%20molecular%20docking.%20Nevertheless%2C%20we%0Afind%20that%20naively%20combining%20residue%20and%20atom%20information%20during%20pre-training%0Atypically%20fails.%20We%20identify%20a%20key%20reason%20is%20the%20information%20leakage%20caused%20by%0Athe%20inclusion%20of%20atom%20structure%20in%20the%20input%2C%20which%20renders%20residue-level%0Apre-training%20tasks%20trivial%20and%20results%20in%20insufficiently%20expressive%20residue%0Arepresentations.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20span%20mask%20pre-training%0Astrategy%20on%203D%20protein%20chains%20to%20learn%20meaningful%20representations%20of%20both%0Aresidues%20and%20atoms.%20This%20leads%20to%20a%20simple%20yet%20effective%20approach%20to%20learning%0Aprotein%20representation%20suitable%20for%20diverse%20downstream%20tasks.%20Extensive%0Aexperimental%20results%20on%20binding%20site%20prediction%20and%20function%20prediction%20tasks%0Ademonstrate%20our%20proposed%20pre-training%20approach%20significantly%20outperforms%20other%0Amethods.%20Our%20code%20will%20be%20made%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01481v3&entry.124074799=Read"},
{"title": "Evaluating Very Long-Term Conversational Memory of LLM Agents", "author": "Adyasha Maharana and Dong-Ho Lee and Sergey Tulyakov and Mohit Bansal and Francesco Barbieri and Yuwei Fang", "abstract": "  Existing works on long-term open-domain dialogues focus on evaluating model\nresponses within contexts spanning no more than five chat sessions. Despite\nadvancements in long-context large language models (LLMs) and retrieval\naugmented generation (RAG) techniques, their efficacy in very long-term\ndialogues remains unexplored. To address this research gap, we introduce a\nmachine-human pipeline to generate high-quality, very long-term dialogues by\nleveraging LLM-based agent architectures and grounding their dialogues on\npersonas and temporal event graphs. Moreover, we equip each agent with the\ncapability of sharing and reacting to images. The generated conversations are\nverified and edited by human annotators for long-range consistency and\ngrounding to the event graphs. Using this pipeline, we collect LoCoMo, a\ndataset of very long-term conversations, each encompassing 300 turns and 9K\ntokens on avg., over up to 35 sessions. Based on LoCoMo, we present a\ncomprehensive evaluation benchmark to measure long-term memory in models,\nencompassing question answering, event summarization, and multi-modal dialogue\ngeneration tasks. Our experimental results indicate that LLMs exhibit\nchallenges in understanding lengthy conversations and comprehending long-range\ntemporal and causal dynamics within dialogues. Employing strategies like\nlong-context LLMs or RAG can offer improvements but these models still\nsubstantially lag behind human performance.\n", "link": "http://arxiv.org/abs/2402.17753v1", "date": "2024-02-27", "relevancy": 1.3836, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4735}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4605}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4566}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Very%20Long-Term%20Conversational%20Memory%20of%20LLM%20Agents&entry.906535625=Adyasha%20Maharana%20and%20Dong-Ho%20Lee%20and%20Sergey%20Tulyakov%20and%20Mohit%20Bansal%20and%20Francesco%20Barbieri%20and%20Yuwei%20Fang&entry.1292438233=%20%20Existing%20works%20on%20long-term%20open-domain%20dialogues%20focus%20on%20evaluating%20model%0Aresponses%20within%20contexts%20spanning%20no%20more%20than%20five%20chat%20sessions.%20Despite%0Aadvancements%20in%20long-context%20large%20language%20models%20%28LLMs%29%20and%20retrieval%0Aaugmented%20generation%20%28RAG%29%20techniques%2C%20their%20efficacy%20in%20very%20long-term%0Adialogues%20remains%20unexplored.%20To%20address%20this%20research%20gap%2C%20we%20introduce%20a%0Amachine-human%20pipeline%20to%20generate%20high-quality%2C%20very%20long-term%20dialogues%20by%0Aleveraging%20LLM-based%20agent%20architectures%20and%20grounding%20their%20dialogues%20on%0Apersonas%20and%20temporal%20event%20graphs.%20Moreover%2C%20we%20equip%20each%20agent%20with%20the%0Acapability%20of%20sharing%20and%20reacting%20to%20images.%20The%20generated%20conversations%20are%0Averified%20and%20edited%20by%20human%20annotators%20for%20long-range%20consistency%20and%0Agrounding%20to%20the%20event%20graphs.%20Using%20this%20pipeline%2C%20we%20collect%20LoCoMo%2C%20a%0Adataset%20of%20very%20long-term%20conversations%2C%20each%20encompassing%20300%20turns%20and%209K%0Atokens%20on%20avg.%2C%20over%20up%20to%2035%20sessions.%20Based%20on%20LoCoMo%2C%20we%20present%20a%0Acomprehensive%20evaluation%20benchmark%20to%20measure%20long-term%20memory%20in%20models%2C%0Aencompassing%20question%20answering%2C%20event%20summarization%2C%20and%20multi-modal%20dialogue%0Ageneration%20tasks.%20Our%20experimental%20results%20indicate%20that%20LLMs%20exhibit%0Achallenges%20in%20understanding%20lengthy%20conversations%20and%20comprehending%20long-range%0Atemporal%20and%20causal%20dynamics%20within%20dialogues.%20Employing%20strategies%20like%0Along-context%20LLMs%20or%20RAG%20can%20offer%20improvements%20but%20these%20models%20still%0Asubstantially%20lag%20behind%20human%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17753v1&entry.124074799=Read"},
{"title": "QoS prediction in radio vehicular environments via prior user\n  information", "author": "Noor Ul Ain and Rodrigo Hernang\u00f3mez and Alexandros Palaios and Martin Kasparick and S\u0142awomir Sta\u0144czak", "abstract": "  Reliable wireless communications play an important role in the automotive\nindustry as it helps to enhance current use cases and enable new ones such as\nconnected autonomous driving, platooning, cooperative maneuvering, teleoperated\ndriving, and smart navigation. These and other use cases often rely on specific\nquality of service (QoS) levels for communication. Recently, the area of\npredictive quality of service (QoS) has received a great deal of attention as a\nkey enabler to forecast communication quality well enough in advance. However,\npredicting QoS in a reliable manner is a notoriously difficult task. In this\npaper, we evaluate ML tree-ensemble methods to predict QoS in the range of\nminutes with data collected from a cellular test network. We discuss radio\nenvironment characteristics and we showcase how these can be used to improve ML\nperformance and further support the uptake of ML in commercial networks.\nSpecifically, we use the correlations of the measurements coming from the radio\nenvironment by including information of prior vehicles to enhance the\nprediction of the target vehicles. Moreover, we are extending prior art by\nshowing how longer prediction horizons can be supported.\n", "link": "http://arxiv.org/abs/2402.17689v1", "date": "2024-02-27", "relevancy": 1.8097, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5061}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4448}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4386}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QoS%20prediction%20in%20radio%20vehicular%20environments%20via%20prior%20user%0A%20%20information&entry.906535625=Noor%20Ul%20Ain%20and%20Rodrigo%20Hernang%C3%B3mez%20and%20Alexandros%20Palaios%20and%20Martin%20Kasparick%20and%20S%C5%82awomir%20Sta%C5%84czak&entry.1292438233=%20%20Reliable%20wireless%20communications%20play%20an%20important%20role%20in%20the%20automotive%0Aindustry%20as%20it%20helps%20to%20enhance%20current%20use%20cases%20and%20enable%20new%20ones%20such%20as%0Aconnected%20autonomous%20driving%2C%20platooning%2C%20cooperative%20maneuvering%2C%20teleoperated%0Adriving%2C%20and%20smart%20navigation.%20These%20and%20other%20use%20cases%20often%20rely%20on%20specific%0Aquality%20of%20service%20%28QoS%29%20levels%20for%20communication.%20Recently%2C%20the%20area%20of%0Apredictive%20quality%20of%20service%20%28QoS%29%20has%20received%20a%20great%20deal%20of%20attention%20as%20a%0Akey%20enabler%20to%20forecast%20communication%20quality%20well%20enough%20in%20advance.%20However%2C%0Apredicting%20QoS%20in%20a%20reliable%20manner%20is%20a%20notoriously%20difficult%20task.%20In%20this%0Apaper%2C%20we%20evaluate%20ML%20tree-ensemble%20methods%20to%20predict%20QoS%20in%20the%20range%20of%0Aminutes%20with%20data%20collected%20from%20a%20cellular%20test%20network.%20We%20discuss%20radio%0Aenvironment%20characteristics%20and%20we%20showcase%20how%20these%20can%20be%20used%20to%20improve%20ML%0Aperformance%20and%20further%20support%20the%20uptake%20of%20ML%20in%20commercial%20networks.%0ASpecifically%2C%20we%20use%20the%20correlations%20of%20the%20measurements%20coming%20from%20the%20radio%0Aenvironment%20by%20including%20information%20of%20prior%20vehicles%20to%20enhance%20the%0Aprediction%20of%20the%20target%20vehicles.%20Moreover%2C%20we%20are%20extending%20prior%20art%20by%0Ashowing%20how%20longer%20prediction%20horizons%20can%20be%20supported.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17689v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


