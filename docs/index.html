<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 1200px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A Self-supervised Pressure Map human keypoint Detection Approch:\n  Optimizing Generalization and Computational Efficiency Across Datasets", "author": "Chengzhang Yu and Xianjun Yang and Wenxia Bao and Shaonan Wang and Zhiming Yao", "abstract": "  In environments where RGB images are inadequate, pressure maps is a viable\nalternative, garnering scholarly attention. This study introduces a novel\nself-supervised pressure map keypoint detection (SPMKD) method, addressing the\ncurrent gap in specialized designs for human keypoint extraction from pressure\nmaps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model,\nwhich is a robust framework that integrates a lightweight encoder for precise\nhuman keypoint detection, a fuser for efficient gradient propagation, and a\ndecoder that transforms human keypoints into reconstructed pressure maps. This\nstructure is further enhanced by the Classification-to-Regression Weight\nTransfer (CRWT) method, which fine-tunes accuracy through initial\nclassification task training. This innovation not only enhances human keypoint\ngeneralization without manual annotations but also showcases remarkable\nefficiency and generalization, evidenced by a reduction to only $5.96\\%$ in\nFLOPs and $1.11\\%$ in parameter count compared to the baseline methods.\n", "link": "http://arxiv.org/abs/2402.14241v1", "date": "2024-02-22", "relevancy": 2.8999, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6053}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5882}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5464}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Self-supervised%20Pressure%20Map%20human%20keypoint%20Detection%20Approch%3A%0A%20%20Optimizing%20Generalization%20and%20Computational%20Efficiency%20Across%20Datasets&entry.906535625=Chengzhang%20Yu%20and%20Xianjun%20Yang%20and%20Wenxia%20Bao%20and%20Shaonan%20Wang%20and%20Zhiming%20Yao&entry.1292438233=%20%20In%20environments%20where%20RGB%20images%20are%20inadequate%2C%20pressure%20maps%20is%20a%20viable%0Aalternative%2C%20garnering%20scholarly%20attention.%20This%20study%20introduces%20a%20novel%0Aself-supervised%20pressure%20map%20keypoint%20detection%20%28SPMKD%29%20method%2C%20addressing%20the%0Acurrent%20gap%20in%20specialized%20designs%20for%20human%20keypoint%20extraction%20from%20pressure%0Amaps.%20Central%20to%20our%20contribution%20is%20the%20Encoder-Fuser-Decoder%20%28EFD%29%20model%2C%0Awhich%20is%20a%20robust%20framework%20that%20integrates%20a%20lightweight%20encoder%20for%20precise%0Ahuman%20keypoint%20detection%2C%20a%20fuser%20for%20efficient%20gradient%20propagation%2C%20and%20a%0Adecoder%20that%20transforms%20human%20keypoints%20into%20reconstructed%20pressure%20maps.%20This%0Astructure%20is%20further%20enhanced%20by%20the%20Classification-to-Regression%20Weight%0ATransfer%20%28CRWT%29%20method%2C%20which%20fine-tunes%20accuracy%20through%20initial%0Aclassification%20task%20training.%20This%20innovation%20not%20only%20enhances%20human%20keypoint%0Ageneralization%20without%20manual%20annotations%20but%20also%20showcases%20remarkable%0Aefficiency%20and%20generalization%2C%20evidenced%20by%20a%20reduction%20to%20only%20%245.96%5C%25%24%20in%0AFLOPs%20and%20%241.11%5C%25%24%20in%20parameter%20count%20compared%20to%20the%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14241v1&entry.124074799=Read"},
{"title": "Secure Navigation using Landmark-based Localization in a GPS-denied\n  Environment", "author": "Ganesh Sapkota and Sanjay Madria", "abstract": "  In modern battlefield scenarios, the reliance on GPS for navigation can be a\ncritical vulnerability. Adversaries often employ tactics to deny or deceive GPS\nsignals, necessitating alternative methods for the localization and navigation\nof mobile troops. Range-free localization methods such as DV-HOP rely on\nradio-based anchors and their average hop distance which suffers from accuracy\nand stability in a dynamic and sparse network topology. Vision-based approaches\nlike SLAM and Visual Odometry use sensor fusion techniques for map generation\nand pose estimation that are more sophisticated and computationally expensive.\nThis paper proposes a novel framework that integrates landmark-based\nlocalization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the\nfuture state of moving entities along the battlefield. Our framework utilizes\nsafe trajectory information generated by the troop control center by\nconsidering identifiable landmarks and pre-defined hazard maps. It performs\npoint inclusion tests on the convex hull of the trajectory segments to ensure\nthe safety and survivability of a moving entity and determines the next point\nforward decisions. We present a simulated battlefield scenario for two\ndifferent approaches (with EKF and without EKF) that guide a moving entity\nthrough an obstacle and hazard-free path. Using the proposed method, we\nobserved a percent error of 6.51% lengthwise in safe trajectory estimation with\nan Average Displacement Error (ADE) of 2.97m and a Final Displacement Error\n(FDE) of 3.27m. The results demonstrate that our approach not only ensures the\nsafety of the mobile units by keeping them within the secure trajectory but\nalso enhances operational effectiveness by adapting to the evolving threat\nlandscape.\n", "link": "http://arxiv.org/abs/2402.14280v1", "date": "2024-02-22", "relevancy": 2.8346, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6132}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.551}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5366}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Secure%20Navigation%20using%20Landmark-based%20Localization%20in%20a%20GPS-denied%0A%20%20Environment&entry.906535625=Ganesh%20Sapkota%20and%20Sanjay%20Madria&entry.1292438233=%20%20In%20modern%20battlefield%20scenarios%2C%20the%20reliance%20on%20GPS%20for%20navigation%20can%20be%20a%0Acritical%20vulnerability.%20Adversaries%20often%20employ%20tactics%20to%20deny%20or%20deceive%20GPS%0Asignals%2C%20necessitating%20alternative%20methods%20for%20the%20localization%20and%20navigation%0Aof%20mobile%20troops.%20Range-free%20localization%20methods%20such%20as%20DV-HOP%20rely%20on%0Aradio-based%20anchors%20and%20their%20average%20hop%20distance%20which%20suffers%20from%20accuracy%0Aand%20stability%20in%20a%20dynamic%20and%20sparse%20network%20topology.%20Vision-based%20approaches%0Alike%20SLAM%20and%20Visual%20Odometry%20use%20sensor%20fusion%20techniques%20for%20map%20generation%0Aand%20pose%20estimation%20that%20are%20more%20sophisticated%20and%20computationally%20expensive.%0AThis%20paper%20proposes%20a%20novel%20framework%20that%20integrates%20landmark-based%0Alocalization%20%28LanBLoc%29%20with%20an%20Extended%20Kalman%20Filter%20%28EKF%29%20to%20predict%20the%0Afuture%20state%20of%20moving%20entities%20along%20the%20battlefield.%20Our%20framework%20utilizes%0Asafe%20trajectory%20information%20generated%20by%20the%20troop%20control%20center%20by%0Aconsidering%20identifiable%20landmarks%20and%20pre-defined%20hazard%20maps.%20It%20performs%0Apoint%20inclusion%20tests%20on%20the%20convex%20hull%20of%20the%20trajectory%20segments%20to%20ensure%0Athe%20safety%20and%20survivability%20of%20a%20moving%20entity%20and%20determines%20the%20next%20point%0Aforward%20decisions.%20We%20present%20a%20simulated%20battlefield%20scenario%20for%20two%0Adifferent%20approaches%20%28with%20EKF%20and%20without%20EKF%29%20that%20guide%20a%20moving%20entity%0Athrough%20an%20obstacle%20and%20hazard-free%20path.%20Using%20the%20proposed%20method%2C%20we%0Aobserved%20a%20percent%20error%20of%206.51%25%20lengthwise%20in%20safe%20trajectory%20estimation%20with%0Aan%20Average%20Displacement%20Error%20%28ADE%29%20of%202.97m%20and%20a%20Final%20Displacement%20Error%0A%28FDE%29%20of%203.27m.%20The%20results%20demonstrate%20that%20our%20approach%20not%20only%20ensures%20the%0Asafety%20of%20the%20mobile%20units%20by%20keeping%20them%20within%20the%20secure%20trajectory%20but%0Aalso%20enhances%20operational%20effectiveness%20by%20adapting%20to%20the%20evolving%20threat%0Alandscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14280v1&entry.124074799=Read"},
{"title": "Spatial Transform Decoupling for Oriented Object Detection", "author": "Hongtian Yu and Yunjie Tian and Qixiang Ye and Yunfan Liu", "abstract": "  Vision Transformers (ViTs) have achieved remarkable success in computer\nvision tasks. However, their potential in rotation-sensitive scenarios has not\nbeen fully explored, and this limitation may be inherently attributed to the\nlack of spatial invariance in the data-forwarding process. In this study, we\npresent a novel approach, termed Spatial Transform Decoupling (STD), providing\na simple-yet-effective solution for oriented object detection with ViTs. Built\nupon stacked ViT blocks, STD utilizes separate network branches to predict the\nposition, size, and angle of bounding boxes, effectively harnessing the spatial\ntransform potential of ViTs in a divide-and-conquer fashion. Moreover, by\naggregating cascaded activation masks (CAMs) computed upon the regressed\nparameters, STD gradually enhances features within regions of interest (RoIs),\nwhich complements the self-attention mechanism. Without bells and whistles, STD\nachieves state-of-the-art performance on the benchmark datasets including\nDOTA-v1.0 (82.24% mAP) and HRSC2016 (98.55% mAP), which demonstrates the\neffectiveness of the proposed method. Source code is available at\nhttps://github.com/yuhongtian17/Spatial-Transform-Decoupling.\n", "link": "http://arxiv.org/abs/2308.10561v2", "date": "2024-02-22", "relevancy": 2.7541, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5639}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5512}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5374}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Transform%20Decoupling%20for%20Oriented%20Object%20Detection&entry.906535625=Hongtian%20Yu%20and%20Yunjie%20Tian%20and%20Qixiang%20Ye%20and%20Yunfan%20Liu&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20remarkable%20success%20in%20computer%0Avision%20tasks.%20However%2C%20their%20potential%20in%20rotation-sensitive%20scenarios%20has%20not%0Abeen%20fully%20explored%2C%20and%20this%20limitation%20may%20be%20inherently%20attributed%20to%20the%0Alack%20of%20spatial%20invariance%20in%20the%20data-forwarding%20process.%20In%20this%20study%2C%20we%0Apresent%20a%20novel%20approach%2C%20termed%20Spatial%20Transform%20Decoupling%20%28STD%29%2C%20providing%0Aa%20simple-yet-effective%20solution%20for%20oriented%20object%20detection%20with%20ViTs.%20Built%0Aupon%20stacked%20ViT%20blocks%2C%20STD%20utilizes%20separate%20network%20branches%20to%20predict%20the%0Aposition%2C%20size%2C%20and%20angle%20of%20bounding%20boxes%2C%20effectively%20harnessing%20the%20spatial%0Atransform%20potential%20of%20ViTs%20in%20a%20divide-and-conquer%20fashion.%20Moreover%2C%20by%0Aaggregating%20cascaded%20activation%20masks%20%28CAMs%29%20computed%20upon%20the%20regressed%0Aparameters%2C%20STD%20gradually%20enhances%20features%20within%20regions%20of%20interest%20%28RoIs%29%2C%0Awhich%20complements%20the%20self-attention%20mechanism.%20Without%20bells%20and%20whistles%2C%20STD%0Aachieves%20state-of-the-art%20performance%20on%20the%20benchmark%20datasets%20including%0ADOTA-v1.0%20%2882.24%25%20mAP%29%20and%20HRSC2016%20%2898.55%25%20mAP%29%2C%20which%20demonstrates%20the%0Aeffectiveness%20of%20the%20proposed%20method.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/yuhongtian17/Spatial-Transform-Decoupling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10561v2&entry.124074799=Read"},
{"title": "WeakSAM: Segment Anything Meets Weakly-supervised Instance-level\n  Recognition", "author": "Lianghui Zhu and Junwei Zhou and Yan Liu and Xin Hao and Wenyu Liu and Xinggang Wang", "abstract": "  Weakly supervised visual recognition using inexact supervision is a critical\nyet challenging learning problem. It significantly reduces human labeling costs\nand traditionally relies on multi-instance learning and pseudo-labeling. This\npaper introduces WeakSAM and solves the weakly-supervised object detection\n(WSOD) and segmentation by utilizing the pre-learned world knowledge contained\nin a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM\naddresses two critical limitations in traditional WSOD retraining, i.e., pseudo\nground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT\ngeneration and Region of Interest (RoI) drop regularization. It also addresses\nthe SAM's problems of requiring prompts and category unawareness for automatic\nobject detection and segmentation. Our results indicate that WeakSAM\nsignificantly surpasses previous state-of-the-art methods in WSOD and WSIS\nbenchmarks with large margins, i.e. average improvements of 7.4% and 8.5%,\nrespectively. The code is available at \\url{https://github.com/hustvl/WeakSAM}.\n", "link": "http://arxiv.org/abs/2402.14812v1", "date": "2024-02-22", "relevancy": 2.7483, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5897}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5362}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5232}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeakSAM%3A%20Segment%20Anything%20Meets%20Weakly-supervised%20Instance-level%0A%20%20Recognition&entry.906535625=Lianghui%20Zhu%20and%20Junwei%20Zhou%20and%20Yan%20Liu%20and%20Xin%20Hao%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20Weakly%20supervised%20visual%20recognition%20using%20inexact%20supervision%20is%20a%20critical%0Ayet%20challenging%20learning%20problem.%20It%20significantly%20reduces%20human%20labeling%20costs%0Aand%20traditionally%20relies%20on%20multi-instance%20learning%20and%20pseudo-labeling.%20This%0Apaper%20introduces%20WeakSAM%20and%20solves%20the%20weakly-supervised%20object%20detection%0A%28WSOD%29%20and%20segmentation%20by%20utilizing%20the%20pre-learned%20world%20knowledge%20contained%0Ain%20a%20vision%20foundation%20model%2C%20i.e.%2C%20the%20Segment%20Anything%20Model%20%28SAM%29.%20WeakSAM%0Aaddresses%20two%20critical%20limitations%20in%20traditional%20WSOD%20retraining%2C%20i.e.%2C%20pseudo%0Aground%20truth%20%28PGT%29%20incompleteness%20and%20noisy%20PGT%20instances%2C%20through%20adaptive%20PGT%0Ageneration%20and%20Region%20of%20Interest%20%28RoI%29%20drop%20regularization.%20It%20also%20addresses%0Athe%20SAM%27s%20problems%20of%20requiring%20prompts%20and%20category%20unawareness%20for%20automatic%0Aobject%20detection%20and%20segmentation.%20Our%20results%20indicate%20that%20WeakSAM%0Asignificantly%20surpasses%20previous%20state-of-the-art%20methods%20in%20WSOD%20and%20WSIS%0Abenchmarks%20with%20large%20margins%2C%20i.e.%20average%20improvements%20of%207.4%25%20and%208.5%25%2C%0Arespectively.%20The%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/hustvl/WeakSAM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14812v1&entry.124074799=Read"},
{"title": "Towards Seamless Adaptation of Pre-trained Models for Visual Place\n  Recognition", "author": "Feng Lu and Lijun Zhang and Xiangyuan Lan and Shuting Dong and Yaowei Wang and Chun Yuan", "abstract": "  Recent studies show that vision models pre-trained in generic visual learning\ntasks with large-scale data can provide useful feature representations for a\nwide range of visual perception problems. However, few attempts have been made\nto exploit pre-trained foundation models in visual place recognition (VPR). Due\nto the inherent difference in training objectives and data between the tasks of\nmodel pre-training and VPR, how to bridge the gap and fully unleash the\ncapability of pre-trained models for VPR is still a key issue to address. To\nthis end, we propose a novel method to realize seamless adaptation of\npre-trained models for VPR. Specifically, to obtain both global and local\nfeatures that focus on salient landmarks for discriminating places, we design a\nhybrid adaptation method to achieve both global and local adaptation\nefficiently, in which only lightweight adapters are tuned without adjusting the\npre-trained model. Besides, to guide effective adaptation, we propose a mutual\nnearest neighbor local feature loss, which ensures proper dense local features\nare produced for local matching and avoids time-consuming spatial verification\nin re-ranking. Experimental results show that our method outperforms the\nstate-of-the-art methods with less training data and training time, and uses\nabout only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based\nspatial verification. It ranks 1st on the MSLS challenge leaderboard (at the\ntime of submission). The code is released at\nhttps://github.com/Lu-Feng/SelaVPR.\n", "link": "http://arxiv.org/abs/2402.14505v1", "date": "2024-02-22", "relevancy": 2.7326, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5467}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5373}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Seamless%20Adaptation%20of%20Pre-trained%20Models%20for%20Visual%20Place%0A%20%20Recognition&entry.906535625=Feng%20Lu%20and%20Lijun%20Zhang%20and%20Xiangyuan%20Lan%20and%20Shuting%20Dong%20and%20Yaowei%20Wang%20and%20Chun%20Yuan&entry.1292438233=%20%20Recent%20studies%20show%20that%20vision%20models%20pre-trained%20in%20generic%20visual%20learning%0Atasks%20with%20large-scale%20data%20can%20provide%20useful%20feature%20representations%20for%20a%0Awide%20range%20of%20visual%20perception%20problems.%20However%2C%20few%20attempts%20have%20been%20made%0Ato%20exploit%20pre-trained%20foundation%20models%20in%20visual%20place%20recognition%20%28VPR%29.%20Due%0Ato%20the%20inherent%20difference%20in%20training%20objectives%20and%20data%20between%20the%20tasks%20of%0Amodel%20pre-training%20and%20VPR%2C%20how%20to%20bridge%20the%20gap%20and%20fully%20unleash%20the%0Acapability%20of%20pre-trained%20models%20for%20VPR%20is%20still%20a%20key%20issue%20to%20address.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20method%20to%20realize%20seamless%20adaptation%20of%0Apre-trained%20models%20for%20VPR.%20Specifically%2C%20to%20obtain%20both%20global%20and%20local%0Afeatures%20that%20focus%20on%20salient%20landmarks%20for%20discriminating%20places%2C%20we%20design%20a%0Ahybrid%20adaptation%20method%20to%20achieve%20both%20global%20and%20local%20adaptation%0Aefficiently%2C%20in%20which%20only%20lightweight%20adapters%20are%20tuned%20without%20adjusting%20the%0Apre-trained%20model.%20Besides%2C%20to%20guide%20effective%20adaptation%2C%20we%20propose%20a%20mutual%0Anearest%20neighbor%20local%20feature%20loss%2C%20which%20ensures%20proper%20dense%20local%20features%0Aare%20produced%20for%20local%20matching%20and%20avoids%20time-consuming%20spatial%20verification%0Ain%20re-ranking.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20the%0Astate-of-the-art%20methods%20with%20less%20training%20data%20and%20training%20time%2C%20and%20uses%0Aabout%20only%203%25%20retrieval%20runtime%20of%20the%20two-stage%20VPR%20methods%20with%20RANSAC-based%0Aspatial%20verification.%20It%20ranks%201st%20on%20the%20MSLS%20challenge%20leaderboard%20%28at%20the%0Atime%20of%20submission%29.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/Lu-Feng/SelaVPR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14505v1&entry.124074799=Read"},
{"title": "Attention Disturbance and Dual-Path Constraint Network for Occluded\n  Person Re-identification", "author": "Jiaer Xia and Lei Tan and Pingyang Dai and Mingbo Zhao and Yongjian Wu and Liujuan Cao", "abstract": "  Occluded person re-identification (Re-ID) aims to address the potential\nocclusion problem when matching occluded or holistic pedestrians from different\ncamera views. Many methods use the background as artificial occlusion and rely\non attention networks to exclude noisy interference. However, the significant\ndiscrepancy between simple background occlusion and realistic occlusion can\nnegatively impact the generalization of the network. To address this issue, we\npropose a novel transformer-based Attention Disturbance and Dual-Path\nConstraint Network (ADP) to enhance the generalization of attention networks.\nFirstly, to imitate real-world obstacles, we introduce an Attention Disturbance\nMask (ADM) module that generates an offensive noise, which can distract\nattention like a realistic occluder, as a more complex form of occlusion.\nSecondly, to fully exploit these complex occluded images, we develop a\nDual-Path Constraint Module (DPC) that can obtain preferable supervision\ninformation from holistic images through dual-path interaction. With our\nproposed method, the network can effectively circumvent a wide variety of\nocclusions using the basic ViT baseline. Comprehensive experimental evaluations\nconducted on person re-ID benchmarks demonstrate the superiority of ADP over\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2303.10976v2", "date": "2024-02-22", "relevancy": 2.7075, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5528}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5415}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5302}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Disturbance%20and%20Dual-Path%20Constraint%20Network%20for%20Occluded%0A%20%20Person%20Re-identification&entry.906535625=Jiaer%20Xia%20and%20Lei%20Tan%20and%20Pingyang%20Dai%20and%20Mingbo%20Zhao%20and%20Yongjian%20Wu%20and%20Liujuan%20Cao&entry.1292438233=%20%20Occluded%20person%20re-identification%20%28Re-ID%29%20aims%20to%20address%20the%20potential%0Aocclusion%20problem%20when%20matching%20occluded%20or%20holistic%20pedestrians%20from%20different%0Acamera%20views.%20Many%20methods%20use%20the%20background%20as%20artificial%20occlusion%20and%20rely%0Aon%20attention%20networks%20to%20exclude%20noisy%20interference.%20However%2C%20the%20significant%0Adiscrepancy%20between%20simple%20background%20occlusion%20and%20realistic%20occlusion%20can%0Anegatively%20impact%20the%20generalization%20of%20the%20network.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20novel%20transformer-based%20Attention%20Disturbance%20and%20Dual-Path%0AConstraint%20Network%20%28ADP%29%20to%20enhance%20the%20generalization%20of%20attention%20networks.%0AFirstly%2C%20to%20imitate%20real-world%20obstacles%2C%20we%20introduce%20an%20Attention%20Disturbance%0AMask%20%28ADM%29%20module%20that%20generates%20an%20offensive%20noise%2C%20which%20can%20distract%0Aattention%20like%20a%20realistic%20occluder%2C%20as%20a%20more%20complex%20form%20of%20occlusion.%0ASecondly%2C%20to%20fully%20exploit%20these%20complex%20occluded%20images%2C%20we%20develop%20a%0ADual-Path%20Constraint%20Module%20%28DPC%29%20that%20can%20obtain%20preferable%20supervision%0Ainformation%20from%20holistic%20images%20through%20dual-path%20interaction.%20With%20our%0Aproposed%20method%2C%20the%20network%20can%20effectively%20circumvent%20a%20wide%20variety%20of%0Aocclusions%20using%20the%20basic%20ViT%20baseline.%20Comprehensive%20experimental%20evaluations%0Aconducted%20on%20person%20re-ID%20benchmarks%20demonstrate%20the%20superiority%20of%20ADP%20over%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10976v2&entry.124074799=Read"},
{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "author": "Yixuan Ren and Yang Zhou and Jimei Yang and Jing Shi and Difan Liu and Feng Liu and Mingi Kwon and Abhinav Shrivastava", "abstract": "  Image customization has been extensively studied in text-to-image (T2I)\ndiffusion models, leading to impressive outcomes and applications. With the\nemergence of text-to-video (T2V) diffusion models, its temporal counterpart,\nmotion customization, has not yet been well investigated. To address the\nchallenge of one-shot motion customization, we propose Customize-A-Video that\nmodels the motion from a single reference video and adapting it to new subjects\nand scenes with both spatial and temporal varieties. It leverages low-rank\nadaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V\ndiffusion model for specific motion modeling from the reference videos. To\ndisentangle the spatial and temporal information during the training pipeline,\nwe introduce a novel concept of appearance absorbers that detach the original\nappearance from the single reference video prior to motion learning. Our\nproposed method can be easily extended to various downstream tasks, including\ncustom video generation and editing, video appearance customization, and\nmultiple motion combination, in a plug-and-play fashion. Our project page can\nbe found at https://anonymous-314.github.io.\n", "link": "http://arxiv.org/abs/2402.14780v1", "date": "2024-02-22", "relevancy": 2.6751, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 1.0}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7215}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4836}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customize-A-Video%3A%20One-Shot%20Motion%20Customization%20of%20Text-to-Video%0A%20%20Diffusion%20Models&entry.906535625=Yixuan%20Ren%20and%20Yang%20Zhou%20and%20Jimei%20Yang%20and%20Jing%20Shi%20and%20Difan%20Liu%20and%20Feng%20Liu%20and%20Mingi%20Kwon%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Image%20customization%20has%20been%20extensively%20studied%20in%20text-to-image%20%28T2I%29%0Adiffusion%20models%2C%20leading%20to%20impressive%20outcomes%20and%20applications.%20With%20the%0Aemergence%20of%20text-to-video%20%28T2V%29%20diffusion%20models%2C%20its%20temporal%20counterpart%2C%0Amotion%20customization%2C%20has%20not%20yet%20been%20well%20investigated.%20To%20address%20the%0Achallenge%20of%20one-shot%20motion%20customization%2C%20we%20propose%20Customize-A-Video%20that%0Amodels%20the%20motion%20from%20a%20single%20reference%20video%20and%20adapting%20it%20to%20new%20subjects%0Aand%20scenes%20with%20both%20spatial%20and%20temporal%20varieties.%20It%20leverages%20low-rank%0Aadaptation%20%28LoRA%29%20on%20temporal%20attention%20layers%20to%20tailor%20the%20pre-trained%20T2V%0Adiffusion%20model%20for%20specific%20motion%20modeling%20from%20the%20reference%20videos.%20To%0Adisentangle%20the%20spatial%20and%20temporal%20information%20during%20the%20training%20pipeline%2C%0Awe%20introduce%20a%20novel%20concept%20of%20appearance%20absorbers%20that%20detach%20the%20original%0Aappearance%20from%20the%20single%20reference%20video%20prior%20to%20motion%20learning.%20Our%0Aproposed%20method%20can%20be%20easily%20extended%20to%20various%20downstream%20tasks%2C%20including%0Acustom%20video%20generation%20and%20editing%2C%20video%20appearance%20customization%2C%20and%0Amultiple%20motion%20combination%2C%20in%20a%20plug-and-play%20fashion.%20Our%20project%20page%20can%0Abe%20found%20at%20https%3A//anonymous-314.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14780v1&entry.124074799=Read"},
{"title": "Adaptive conformal classification with noisy labels", "author": "Matteo Sesia and Y. X. Rachel Wang and Xin Tong", "abstract": "  This paper develops novel conformal prediction methods for classification\ntasks that can automatically adapt to random label contamination in the\ncalibration sample, leading to more informative prediction sets with stronger\ncoverage guarantees compared to state-of-the-art approaches. This is made\npossible by a precise characterization of the effective coverage inflation (or\ndeflation) suffered by standard conformal inferences in the presence of label\ncontamination, which is then made actionable through new calibration\nalgorithms. Our solution is flexible and can leverage different modeling\nassumptions about the label contamination process, while requiring no knowledge\nof the underlying data distribution or of the inner workings of the\nmachine-learning classifier. The advantages of the proposed methods are\ndemonstrated through extensive simulations and an application to object\nclassification with the CIFAR-10H image data set.\n", "link": "http://arxiv.org/abs/2309.05092v2", "date": "2024-02-22", "relevancy": 2.6541, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5429}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5364}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5132}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20conformal%20classification%20with%20noisy%20labels&entry.906535625=Matteo%20Sesia%20and%20Y.%20X.%20Rachel%20Wang%20and%20Xin%20Tong&entry.1292438233=%20%20This%20paper%20develops%20novel%20conformal%20prediction%20methods%20for%20classification%0Atasks%20that%20can%20automatically%20adapt%20to%20random%20label%20contamination%20in%20the%0Acalibration%20sample%2C%20leading%20to%20more%20informative%20prediction%20sets%20with%20stronger%0Acoverage%20guarantees%20compared%20to%20state-of-the-art%20approaches.%20This%20is%20made%0Apossible%20by%20a%20precise%20characterization%20of%20the%20effective%20coverage%20inflation%20%28or%0Adeflation%29%20suffered%20by%20standard%20conformal%20inferences%20in%20the%20presence%20of%20label%0Acontamination%2C%20which%20is%20then%20made%20actionable%20through%20new%20calibration%0Aalgorithms.%20Our%20solution%20is%20flexible%20and%20can%20leverage%20different%20modeling%0Aassumptions%20about%20the%20label%20contamination%20process%2C%20while%20requiring%20no%20knowledge%0Aof%20the%20underlying%20data%20distribution%20or%20of%20the%20inner%20workings%20of%20the%0Amachine-learning%20classifier.%20The%20advantages%20of%20the%20proposed%20methods%20are%0Ademonstrated%20through%20extensive%20simulations%20and%20an%20application%20to%20object%0Aclassification%20with%20the%20CIFAR-10H%20image%20data%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05092v2&entry.124074799=Read"},
{"title": "Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP\n  Guided Reinforcement Learning", "author": "Antoine Chaffin and Ewa Kijak and Vincent Claveau", "abstract": "  Training image captioning models using teacher forcing results in very\ngeneric samples, whereas more distinctive captions can be very useful in\nretrieval applications or to produce alternative texts describing images for\naccessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval\nsimilarity score between the generated caption and the input image as reward to\nguide the training, leading to more distinctive captions. Recent studies show\nthat pre-trained cross-modal retrieval models can be used to provide this\nreward, completely eliminating the need for reference captions. However, we\nargue in this paper that Ground Truth (GT) captions can still be useful in this\nRL framework. We propose a new image captioning model training strategy that\nmakes use of GT captions in different ways. Firstly, they can be used to train\na simple MLP discriminator that serves as a regularization to prevent reward\nhacking and ensures the fluency of generated captions, resulting in a textual\nGAN setup extended for multimodal inputs. Secondly, they can serve as\nadditional trajectories in the RL strategy, resulting in a teacher forcing loss\nweighted by the similarity of the GT to the image. This objective acts as an\nadditional learning signal grounded to the distribution of the GT captions.\nThirdly, they can serve as strong baselines when added to the pool of captions\nused to compute the proposed contrastive reward to reduce the variance of\ngradient estimate. Experiments on MS-COCO demonstrate the interest of the\nproposed training strategy to produce highly distinctive captions while\nmaintaining high writing quality.\n", "link": "http://arxiv.org/abs/2402.13936v1", "date": "2024-02-21", "relevancy": 2.0433, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.519}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5095}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4937}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distinctive%20Image%20Captioning%3A%20Leveraging%20Ground%20Truth%20Captions%20in%20CLIP%0A%20%20Guided%20Reinforcement%20Learning&entry.906535625=Antoine%20Chaffin%20and%20Ewa%20Kijak%20and%20Vincent%20Claveau&entry.1292438233=%20%20Training%20image%20captioning%20models%20using%20teacher%20forcing%20results%20in%20very%0Ageneric%20samples%2C%20whereas%20more%20distinctive%20captions%20can%20be%20very%20useful%20in%0Aretrieval%20applications%20or%20to%20produce%20alternative%20texts%20describing%20images%20for%0Aaccessibility.%20Reinforcement%20Learning%20%28RL%29%20allows%20to%20use%20cross-modal%20retrieval%0Asimilarity%20score%20between%20the%20generated%20caption%20and%20the%20input%20image%20as%20reward%20to%0Aguide%20the%20training%2C%20leading%20to%20more%20distinctive%20captions.%20Recent%20studies%20show%0Athat%20pre-trained%20cross-modal%20retrieval%20models%20can%20be%20used%20to%20provide%20this%0Areward%2C%20completely%20eliminating%20the%20need%20for%20reference%20captions.%20However%2C%20we%0Aargue%20in%20this%20paper%20that%20Ground%20Truth%20%28GT%29%20captions%20can%20still%20be%20useful%20in%20this%0ARL%20framework.%20We%20propose%20a%20new%20image%20captioning%20model%20training%20strategy%20that%0Amakes%20use%20of%20GT%20captions%20in%20different%20ways.%20Firstly%2C%20they%20can%20be%20used%20to%20train%0Aa%20simple%20MLP%20discriminator%20that%20serves%20as%20a%20regularization%20to%20prevent%20reward%0Ahacking%20and%20ensures%20the%20fluency%20of%20generated%20captions%2C%20resulting%20in%20a%20textual%0AGAN%20setup%20extended%20for%20multimodal%20inputs.%20Secondly%2C%20they%20can%20serve%20as%0Aadditional%20trajectories%20in%20the%20RL%20strategy%2C%20resulting%20in%20a%20teacher%20forcing%20loss%0Aweighted%20by%20the%20similarity%20of%20the%20GT%20to%20the%20image.%20This%20objective%20acts%20as%20an%0Aadditional%20learning%20signal%20grounded%20to%20the%20distribution%20of%20the%20GT%20captions.%0AThirdly%2C%20they%20can%20serve%20as%20strong%20baselines%20when%20added%20to%20the%20pool%20of%20captions%0Aused%20to%20compute%20the%20proposed%20contrastive%20reward%20to%20reduce%20the%20variance%20of%0Agradient%20estimate.%20Experiments%20on%20MS-COCO%20demonstrate%20the%20interest%20of%20the%0Aproposed%20training%20strategy%20to%20produce%20highly%20distinctive%20captions%20while%0Amaintaining%20high%20writing%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13936v1&entry.124074799=Read"},
{"title": "A Survey on Global LiDAR Localization: Challenges, Advances and Open\n  Problems", "author": "Huan Yin and Xuecheng Xu and Sha Lu and Xieyuanli Chen and Rong Xiong and Shaojie Shen and Cyrill Stachniss and Yue Wang", "abstract": "  Knowledge about the own pose is key for all mobile robot applications. Thus\npose estimation is part of the core functionalities of mobile robots. Over the\nlast two decades, LiDAR scanners have become the standard sensor for robot\nlocalization and mapping. This article aims to provide an overview of recent\nprogress and advancements in LiDAR-based global localization. We begin by\nformulating the problem and exploring the application scope. We then present a\nreview of the methodology, including recent advancements in several topics,\nsuch as maps, descriptor extraction, and cross-robot localization. The contents\nof the article are organized under three themes. The first theme concerns the\ncombination of global place retrieval and local pose estimation. The second\ntheme is upgrading single-shot measurements to sequential ones for sequential\nglobal localization. Finally, the third theme focuses on extending single-robot\nglobal localization to cross-robot localization in multi-robot systems. We\nconclude the survey with a discussion of open challenges and promising\ndirections in global LiDAR localization. To our best knowledge, this is the\nfirst comprehensive survey on global LiDAR localization for mobile robots.\n", "link": "http://arxiv.org/abs/2302.07433v3", "date": "2024-02-22", "relevancy": 2.3495, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6158}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5784}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Global%20LiDAR%20Localization%3A%20Challenges%2C%20Advances%20and%20Open%0A%20%20Problems&entry.906535625=Huan%20Yin%20and%20Xuecheng%20Xu%20and%20Sha%20Lu%20and%20Xieyuanli%20Chen%20and%20Rong%20Xiong%20and%20Shaojie%20Shen%20and%20Cyrill%20Stachniss%20and%20Yue%20Wang&entry.1292438233=%20%20Knowledge%20about%20the%20own%20pose%20is%20key%20for%20all%20mobile%20robot%20applications.%20Thus%0Apose%20estimation%20is%20part%20of%20the%20core%20functionalities%20of%20mobile%20robots.%20Over%20the%0Alast%20two%20decades%2C%20LiDAR%20scanners%20have%20become%20the%20standard%20sensor%20for%20robot%0Alocalization%20and%20mapping.%20This%20article%20aims%20to%20provide%20an%20overview%20of%20recent%0Aprogress%20and%20advancements%20in%20LiDAR-based%20global%20localization.%20We%20begin%20by%0Aformulating%20the%20problem%20and%20exploring%20the%20application%20scope.%20We%20then%20present%20a%0Areview%20of%20the%20methodology%2C%20including%20recent%20advancements%20in%20several%20topics%2C%0Asuch%20as%20maps%2C%20descriptor%20extraction%2C%20and%20cross-robot%20localization.%20The%20contents%0Aof%20the%20article%20are%20organized%20under%20three%20themes.%20The%20first%20theme%20concerns%20the%0Acombination%20of%20global%20place%20retrieval%20and%20local%20pose%20estimation.%20The%20second%0Atheme%20is%20upgrading%20single-shot%20measurements%20to%20sequential%20ones%20for%20sequential%0Aglobal%20localization.%20Finally%2C%20the%20third%20theme%20focuses%20on%20extending%20single-robot%0Aglobal%20localization%20to%20cross-robot%20localization%20in%20multi-robot%20systems.%20We%0Aconclude%20the%20survey%20with%20a%20discussion%20of%20open%20challenges%20and%20promising%0Adirections%20in%20global%20LiDAR%20localization.%20To%20our%20best%20knowledge%2C%20this%20is%20the%0Afirst%20comprehensive%20survey%20on%20global%20LiDAR%20localization%20for%20mobile%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.07433v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


